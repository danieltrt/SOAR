file_path,api_count,code
setup.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nfrom setuptools import setup, find_packages\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname), encoding='utf-8').read()\n\nsetup(\n    name = 'nni',\n    version = '999.0.0-developing',\n    author = 'Microsoft NNI Team',\n    author_email = 'nni@microsoft.com',\n    description = 'Neural Network Intelligence project',\n    long_description = read('README.md'),\n    license = 'MIT',\n    url = 'https://github.com/Microsoft/nni',\n\n    packages = find_packages('src/sdk/pynni', exclude=['tests']) + find_packages('src/sdk/pycli') + find_packages('tools'),\n    package_dir = {\n        'nni': 'src/sdk/pynni/nni',\n        'nnicli': 'src/sdk/pycli/nnicli',\n        'nni_annotation': 'tools/nni_annotation',\n        'nni_cmd': 'tools/nni_cmd',\n        'nni_trial_tool':'tools/nni_trial_tool',\n        'nni_gpu_tool':'tools/nni_gpu_tool'\n    },\n    package_data = {'nni': ['**/requirements.txt']},\n    python_requires = '>=3.5',\n    install_requires = [\n        'astor',\n        'hyperopt==0.1.2',\n        'json_tricks',\n        'numpy',\n        'psutil',\n        'ruamel.yaml',\n        'requests',\n        'scipy',\n        'schema',\n        'PythonWebHDFS',\n        'colorama',\n        'scikit-learn>=0.20,<0.22'\n    ],\n\n    entry_points = {\n        'console_scripts' : [\n            'nnictl = nni_cmd.nnictl:parse_args'\n        ]\n    }\n)\n"""
tools/setup.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport setuptools\n\nsetuptools.setup(\n    name = 'nni-tool',\n    version = '999.0.0-developing',\n    packages = setuptools.find_packages(exclude=['*test*']),\n\n    python_requires = '>=3.5',\n    install_requires = [\n        'requests',\n        'ruamel.yaml',\n        'psutil',\n        'astor',\n        'schema',\n        'PythonWebHDFS',\n        'colorama'\n    ],\n\n    author = 'Microsoft NNI Team',\n    author_email = 'nni@microsoft.com',\n    description = 'NNI control for Neural Network Intelligence project',\n    license = 'MIT',\n    url = 'https://github.com/Microsoft/nni',\n    entry_points = {\n        'console_scripts' : [\n            'nnictl = nni_cmd.nnictl:parse_args'\n        ]\n    }\n)\n"""
deployment/pypi/setup.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport setuptools\nimport platform\nfrom os import walk, path\n\nos_type = platform.system()\nif os_type == \'Linux\':\n    os_name = \'POSIX :: Linux\'\nelif os_type == \'Darwin\':\n    os_name = \'MacOS\'\nelif os_type == \'Windows\':\n    os_name = \'Microsoft :: Windows\'\nelse:\n    raise NotImplementedError(\'current platform {} not supported\'.format(os_type))\n\ndata_files = [(\'bin\', [\'node-{}-x64/bin/node\'.format(os_type.lower())])]\nif os_type == \'Windows\':\n    data_files = [(\'.\\Scripts\', [\'node-{}/node.exe\'.format(os_type.lower())])]\n\nfor (dirpath, dirnames, filenames) in walk(\'./nni\'):\n    files = [path.normpath(path.join(dirpath, filename)) for filename in filenames]\n    data_files.append((path.normpath(dirpath), files))\n\nwith open(\'../../README.md\', \'r\', encoding=""utf-8"") as fh:\n    long_description = fh.read()\n\nsetuptools.setup(\n    name = \'nni\',\n    version = \'999.0.0-developing\',\n    author = \'Microsoft NNI team\',\n    author_email = \'nni@microsoft.com\',\n    description = \'Neural Network Intelligence package\',\n    long_description = long_description,\n    long_description_content_type = \'text/markdown\',\n    license = \'MIT\',\n    url = \'https://github.com/Microsoft/nni\',\n    packages = setuptools.find_packages(\'../../tools\') \\\n        + setuptools.find_packages(\'../../src/sdk/pynni\', exclude=[\'tests\']) \\\n        + setuptools.find_packages(\'../../src/sdk/pycli\'),\n    package_dir = {\n        \'nni_annotation\': \'../../tools/nni_annotation\',\n        \'nni_cmd\': \'../../tools/nni_cmd\',\n        \'nni_trial_tool\': \'../../tools/nni_trial_tool\',\n        \'nni_gpu_tool\': \'../../tools/nni_gpu_tool\',\n        \'nni\': \'../../src/sdk/pynni/nni\',\n        \'nnicli\': \'../../src/sdk/pycli/nnicli\'\n    },\n    package_data = {\'nni\': [\'**/requirements.txt\']},\n    python_requires = \'>=3.5\',\n    install_requires = [\n        \'schema\',\n        \'ruamel.yaml\',\n        \'psutil\',\n        \'requests\',\n        \'astor\',\n        \'PythonWebHDFS\',\n        \'hyperopt==0.1.2\',\n        \'json_tricks\',\n        \'numpy\',\n        \'scipy\',\n        \'coverage\',\n        \'colorama\',\n        \'scikit-learn>=0.20,<0.22\'\n    ],\n    classifiers = [\n        \'Programming Language :: Python :: 3\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: \' + os_name\n    ],\n    data_files = data_files,\n    entry_points = {\n        \'console_scripts\' : [\n            \'nnictl = nni_cmd.nnictl:parse_args\'\n        ]\n    }\n)\n'"
docs/en_US/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nfrom recommonmark.transform import AutoStructify\nfrom recommonmark.parser import CommonMarkParser\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../src/sdk/pynni\'))\n\n\n# -- Project information ---------------------------------------------------\n\nproject = \'NNI\'\ncopyright = \'2020, Microsoft\'\nauthor = \'Microsoft\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'v1.6\'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx_markdown_tables\',\n    \'sphinxarg.ext\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\n# Add mock modules\nautodoc_mock_imports = [\'apex\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_parsers = {\n    \'.md\': CommonMarkParser\n}\n\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\', \'Release_v1.0.md\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'../static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\nhtml_logo = \'../img/nni_logo_dark.png\'\nhtml_title = \'An open source AutoML toolkit for neural architecture search, model compression and hyper-parameter tuning (%s %s)\' % \\\n    (project, release)\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NeuralNetworkIntelligencedoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NeuralNetworkIntelligence.tex\', \'Neural Network Intelligence Documentation\',\n     \'Microsoft\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'neuralnetworkintelligence\', \'Neural Network Intelligence Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NeuralNetworkIntelligence\', \'Neural Network Intelligence Documentation\',\n     author, \'NeuralNetworkIntelligence\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\ndef setup(app):\n    app.add_config_value(\'recommonmark_config\', {\n        \'enable_eval_rst\': True,\n        \'enable_auto_toc_tree\': False,\n    }, True)\n    app.add_transform(AutoStructify)\n    app.add_stylesheet(\'css/custom.css\')\n'"
docs/zh_CN/conf.py,0,"b'# -*- coding: utf-8 -*-\n#\n# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nfrom recommonmark.transform import AutoStructify\nfrom recommonmark.parser import CommonMarkParser\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(\'../../src/sdk/pynni\'))\n\n\n# -- Project information ---------------------------------------------------\n\nproject = \'NNI\'\ncopyright = \'2020, Microsoft\'\nauthor = \'Microsoft\'\n\n# The short X.Y version\nversion = \'\'\n# The full version, including alpha/beta/rc tags\nrelease = \'v1.5\'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = \'1.0\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.mathjax\',\n    \'sphinx_markdown_tables\',\n    \'sphinxarg.ext\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.viewcode\',\n]\n\n# \xe6\xb7\xbb\xe5\x8a\xa0\xe7\xa4\xba\xe4\xbe\x8b\xe6\xa8\xa1\xe5\x9d\x97\nautodoc_mock_imports = [\'apex\']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\nsource_parsers = {\n    \'.md\': CommonMarkParser\n}\n\nsource_suffix = [\'.rst\', \'.md\']\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [\'_build\', \'Thumbs.db\', \'.DS_Store\', \'Release_v1.0.md\']\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = \'sphinx_rtd_theme\'\n\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n#\nhtml_theme_options = {\n    \'logo_only\': True,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'../static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\nhtml_logo = \'../img/nni_logo_dark.png\'\nhtml_title = \'\xe6\x94\xaf\xe6\x8c\x81\xe7\xa5\x9e\xe7\xbb\x8f\xe7\xbd\x91\xe7\xbb\x9c\xe7\xbb\x93\xe6\x9e\x84\xe6\x90\x9c\xe7\xb4\xa2\xe3\x80\x81\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x8e\x8b\xe7\xbc\xa9\xe3\x80\x81\xe8\xb6\x85\xe5\x8f\x82\xe8\xb0\x83\xe4\xbc\x98\xe7\x9a\x84\xe5\xbc\x80\xe6\xba\x90\xe8\x87\xaa\xe5\x8a\xa8\xe6\x9c\xba\xe5\x99\xa8\xe5\xad\xa6\xe4\xb9\xa0\xe5\xb7\xa5\xe5\x85\xb7 (%s %s)\' % \\\n    (project, release)\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \'NeuralNetworkIntelligencedoc\'\n\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    #\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    #\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    #\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    #\n    # \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \'NeuralNetworkIntelligence.tex\', \'Neural Network Intelligence Documentation\',\n     \'Microsoft\', \'manual\'),\n]\n\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, \'neuralnetworkintelligence\', \'Neural Network Intelligence Documentation\',\n     [author], 1)\n]\n\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, \'NeuralNetworkIntelligence\', \'Neural Network Intelligence Documentation\',\n     author, \'NeuralNetworkIntelligence\', \'One line description of project.\',\n     \'Miscellaneous\'),\n]\n\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n\n# -- Extension configuration -------------------------------------------------\ndef setup(app):\n    app.add_config_value(\'recommonmark_config\', {\n        \'enable_eval_rst\': True,\n        \'enable_auto_toc_tree\': False,\n    }, True)\n    app.add_transform(AutoStructify)\n    app.add_stylesheet(\'css/custom.css\')\n'"
examples/model_compress/BNN_quantizer_cifar10.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import BNNQuantizer\n\n\nclass VGG_Cifar10(nn.Module):\n    def __init__(self, num_classes=1000):\n        super(VGG_Cifar10, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 128, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(128, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias=False),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.BatchNorm2d(128, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(256, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True),\n\n\n            nn.Conv2d(256, 256, kernel_size=3, padding=1, bias=False),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.BatchNorm2d(256, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True),\n\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1, bias=False),\n            nn.BatchNorm2d(512, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True),\n\n\n            nn.Conv2d(512, 512, kernel_size=3, padding=1, bias=False),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.BatchNorm2d(512, eps=1e-4, momentum=0.1),\n            nn.Hardtanh(inplace=True)\n        )\n\n        self.classifier = nn.Sequential(\n            nn.Linear(512 * 4 * 4, 1024, bias=False),\n            nn.BatchNorm1d(1024),\n            nn.Hardtanh(inplace=True),\n            nn.Linear(1024, 1024, bias=False),\n            nn.BatchNorm1d(1024),\n            nn.Hardtanh(inplace=True),\n            nn.Linear(1024, num_classes), # do not quantize output\n            nn.BatchNorm1d(num_classes, affine=False)\n        )\n\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(-1, 512 * 4 * 4)\n        x = self.classifier(x)\n        return x\n\n\ndef train(model, device, train_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        for name, param in model.named_parameters():\n            if name.endswith(\'old_weight\'):\n                param = param.clamp(-1, 1)\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    acc = 100 * correct / len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, acc))\n    return acc\n\ndef adjust_learning_rate(optimizer, epoch):\n    update_list = [55, 100, 150, 200, 400, 600]\n    if epoch in update_list:\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.1\n    return\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                         transform=transforms.Compose([\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                         ])),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])),\n        batch_size=200, shuffle=False)\n\n    model = VGG_Cifar10(num_classes=10)\n    model.to(device)\n\n    configure_list = [{\n        \'quant_types\': [\'weight\'],\n        \'quant_bits\': 1,\n        \'op_types\': [\'Conv2d\', \'Linear\'],\n        \'op_names\': [\'features.3\', \'features.7\', \'features.10\', \'features.14\', \'classifier.0\', \'classifier.3\']\n    }, {\n        \'quant_types\': [\'output\'],\n        \'quant_bits\': 1,\n        \'op_types\': [\'Hardtanh\'],\n        \'op_names\': [\'features.6\', \'features.9\', \'features.13\', \'features.16\', \'features.20\', \'classifier.2\', \'classifier.5\']\n    }]\n\n    quantizer = BNNQuantizer(model, configure_list)\n    model = quantizer.compress()\n\n    print(\'=\' * 10 + \'train\' + \'=\' * 10)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n    best_top1 = 0\n    for epoch in range(400):\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer)\n        adjust_learning_rate(optimizer, epoch)\n        top1 = test(model, device, test_loader)\n        if top1 > best_top1:\n            best_top1 = top1\n    print(best_top1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/model_compress/DoReFaQuantizer_torch_mnist.py,15,"b'import torch\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import DoReFaQuantizer\n\n\nclass Mnist(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = torch.nn.Linear(500, 10)\n        self.relu1 = torch.nn.ReLU6()\n        self.relu2 = torch.nn.ReLU6()\n        self.relu3 = torch.nn.ReLU6()\n\n    def forward(self, x):\n        x = self.relu1(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = self.relu2(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = self.relu3(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(model, quantizer, device, train_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, 100 * correct / len(test_loader.dataset)))\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'data\', train=True, download=True, transform=trans),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'data\', train=False, transform=trans),\n        batch_size=1000, shuffle=True)\n\n    model = Mnist()\n    model = model.to(device)\n    configure_list = [{\n        \'quant_types\': [\'weight\'],\n        \'quant_bits\': {\n            \'weight\': 8,\n        }, # you can just use `int` here because all `quan_types` share same bits length, see config for `ReLu6` below.\n        \'op_types\':[\'Conv2d\', \'Linear\']\n    }]\n    quantizer = DoReFaQuantizer(model, configure_list)\n    quantizer.compress()\n\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.5)\n    for epoch in range(10):\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, quantizer, device, train_loader, optimizer)\n        test(model, device, test_loader)\n\n\nif __name__ == \'__main__\':\n    main()'"
examples/model_compress/L1_torch_cifar10.py,13,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import L1FilterPruner\nfrom models.cifar10.vgg import VGG\n\n\ndef train(model, device, train_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    acc = 100 * correct / len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, acc))\n    return acc\n\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                         transform=transforms.Compose([\n                             transforms.Pad(4),\n                             transforms.RandomCrop(32),\n                             transforms.RandomHorizontalFlip(),\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                         ])),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])),\n        batch_size=200, shuffle=False)\n\n    model = VGG(depth=16)\n    model.to(device)\n\n    # Train the base VGG-16 model\n    print(\'=\' * 10 + \'Train the unpruned base model\' + \'=\' * 10)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 160, 0)\n    for epoch in range(160):\n        train(model, device, train_loader, optimizer)\n        test(model, device, test_loader)\n        lr_scheduler.step(epoch)\n    torch.save(model.state_dict(), \'vgg16_cifar10.pth\')\n\n    # Test base model accuracy\n    print(\'=\' * 10 + \'Test on the original model\' + \'=\' * 10)\n    model.load_state_dict(torch.load(\'vgg16_cifar10.pth\'))\n    test(model, device, test_loader)\n    # top1 = 93.51%\n\n    # Pruning Configuration, in paper \'PRUNING FILTERS FOR EFFICIENT CONVNETS\',\n    # Conv_1, Conv_8, Conv_9, Conv_10, Conv_11, Conv_12 are pruned with 50% sparsity, as \'VGG-16-pruned-A\'\n    configure_list = [{\n        \'sparsity\': 0.5,\n        \'op_types\': [\'default\'],\n        \'op_names\': [\'feature.0\', \'feature.24\', \'feature.27\', \'feature.30\', \'feature.34\', \'feature.37\']\n    }]\n\n    # Prune model and test accuracy without fine tuning.\n    print(\'=\' * 10 + \'Test on the pruned model before fine tune\' + \'=\' * 10)\n    optimizer_finetune = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n    pruner = L1FilterPruner(model, configure_list, optimizer_finetune)\n    model = pruner.compress()\n    test(model, device, test_loader)\n    # top1 = 88.19%\n\n    # Fine tune the pruned model for 40 epochs and test accuracy\n    print(\'=\' * 10 + \'Fine tuning\' + \'=\' * 10)\n    best_top1 = 0\n    for epoch in range(40):\n        pruner.update_epoch(epoch)\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer_finetune)\n        top1 = test(model, device, test_loader)\n        if top1 > best_top1:\n            best_top1 = top1\n            # Export the best model, \'model_path\' stores state_dict of the pruned model,\n            # mask_path stores mask_dict of the pruned model\n            pruner.export_model(model_path=\'pruned_vgg16_cifar10.pth\', mask_path=\'mask_vgg16_cifar10.pth\')\n\n    # Test the exported model\n    print(\'=\' * 10 + \'Test on the pruned model after fine tune\' + \'=\' * 10)\n    new_model = VGG(depth=16)\n    new_model.to(device)\n    new_model.load_state_dict(torch.load(\'pruned_vgg16_cifar10.pth\'))\n    test(new_model, device, test_loader)\n    # top1 = 93.53%\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/model_compress/QAT_torch_quantizer.py,15,"b'import torch\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import QAT_Quantizer\n\n\nclass Mnist(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = torch.nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = torch.nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = torch.nn.Linear(500, 10)\n        self.relu1 = torch.nn.ReLU6()\n        self.relu2 = torch.nn.ReLU6()\n        self.relu3 = torch.nn.ReLU6()\n\n    def forward(self, x):\n        x = self.relu1(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = self.relu2(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 50)\n        x = self.relu3(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(model, quantizer, device, train_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, 100 * correct / len(test_loader.dataset)))\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'data\', train=True, download=True, transform=trans),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(\'data\', train=False, transform=trans),\n        batch_size=1000, shuffle=True)\n\n    model = Mnist()\n    \'\'\'you can change this to DoReFaQuantizer to implement it\n    DoReFaQuantizer(configure_list).compress(model)\n    \'\'\'\n    configure_list = [{\n        \'quant_types\': [\'weight\'],\n        \'quant_bits\': {\n            \'weight\': 8,\n        }, # you can just use `int` here because all `quan_types` share same bits length, see config for `ReLu6` below.\n        \'op_types\':[\'Conv2d\', \'Linear\']\n    }, {\n        \'quant_types\': [\'output\'],\n        \'quant_bits\': 8,\n        \'quant_start_step\': 1000,\n        \'op_types\':[\'ReLU6\']\n    }]\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n    quantizer = QAT_Quantizer(model, configure_list, optimizer)\n    quantizer.compress()\n\n    model.to(device)\n    for epoch in range(40):\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, quantizer, device, train_loader, optimizer)\n        test(model, device, test_loader)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/model_compress/fpgm_tf_mnist.py,0,"b'import tensorflow as tf\nfrom tensorflow import keras\nassert tf.__version__ >= ""2.0""\nimport numpy as np\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom nni.compression.tensorflow import FPGMPruner\n\ndef get_data():\n    (X_train_full, y_train_full), _ = keras.datasets.mnist.load_data()\n    X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n    y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n\n    X_mean = X_train.mean(axis=0, keepdims=True)\n    X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n    X_train = (X_train - X_mean) / X_std\n    X_valid = (X_valid - X_mean) / X_std\n\n    X_train = X_train[..., np.newaxis]\n    X_valid = X_valid[..., np.newaxis]\n\n    return X_train, X_valid, y_train, y_valid\n\ndef get_model():\n    model = keras.models.Sequential([\n        Conv2D(filters=32, kernel_size=7, input_shape=[28, 28, 1], activation=\'relu\', padding=""SAME""),\n        MaxPooling2D(pool_size=2),\n        Conv2D(filters=64, kernel_size=3, activation=\'relu\', padding=""SAME""),\n        MaxPooling2D(pool_size=2),\n        Flatten(),\n        Dense(units=128, activation=\'relu\'),\n        Dropout(0.5),\n        Dense(units=10, activation=\'softmax\'),\n    ])\n    model.compile(loss=""sparse_categorical_crossentropy"",\n        optimizer=keras.optimizers.SGD(lr=1e-3),\n        metrics=[""accuracy""])\n    return model\n\ndef main():\n    X_train, X_valid, y_train, y_valid = get_data()\n    model = get_model()\n\n    configure_list = [{\n        \'sparsity\': 0.5,\n        \'op_types\': [\'Conv2D\']\n    }]\n    pruner = FPGMPruner(model, configure_list)\n    pruner.compress()\n\n    update_epoch_callback = keras.callbacks.LambdaCallback(on_epoch_begin=lambda epoch, logs: pruner.update_epoch(epoch))\n\n    model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid), callbacks=[update_epoch_callback])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/model_compress/lottery_torch_mnist_fc.py,11,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.utils.data\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom nni.compression.torch import LotteryTicketPruner\n\nclass fc1(nn.Module):\n\n    def __init__(self, num_classes=10):\n        super(fc1, self).__init__()\n        self.classifier = nn.Sequential(\n            nn.Linear(28*28, 300),\n            nn.ReLU(inplace=True),\n            nn.Linear(300, 100),\n            nn.ReLU(inplace=True),\n            nn.Linear(100, num_classes),\n        )\n\n    def forward(self, x):\n        x = torch.flatten(x, 1)\n        x = self.classifier(x)\n        return x\n\ndef train(model, train_loader, optimizer, criterion):\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model.train()\n    for imgs, targets in train_loader:\n        optimizer.zero_grad()\n        imgs, targets = imgs.to(device), targets.to(device)\n        output = model(imgs)\n        train_loss = criterion(output, targets)\n        train_loss.backward()\n        optimizer.step()\n    return train_loss.item()\n\ndef test(model, test_loader, criterion):\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()  # sum up batch loss\n            pred = output.data.max(1, keepdim=True)[1]  # get the index of the max log-probability\n            correct += pred.eq(target.data.view_as(pred)).sum().item()\n        test_loss /= len(test_loader.dataset)\n        accuracy = 100. * correct / len(test_loader.dataset)\n    return accuracy\n\n\nif __name__ == \'__main__\':\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n    traindataset = datasets.MNIST(\'./data\', train=True, download=True, transform=transform)\n    testdataset = datasets.MNIST(\'./data\', train=False, transform=transform)\n    train_loader = torch.utils.data.DataLoader(traindataset, batch_size=60, shuffle=True, num_workers=0, drop_last=False)\n    test_loader = torch.utils.data.DataLoader(testdataset, batch_size=60, shuffle=False, num_workers=0, drop_last=True)\n\n    model = fc1().to(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    optimizer = torch.optim.Adam(model.parameters(), lr=1.2e-3)\n    criterion = nn.CrossEntropyLoss()\n\n    configure_list = [{\n        \'prune_iterations\': 5,\n        \'sparsity\': 0.96,\n        \'op_types\': [\'default\']\n    }]\n    pruner = LotteryTicketPruner(model, configure_list, optimizer)\n    pruner.compress()\n\n\n    for i in pruner.get_prune_iterations():\n        pruner.prune_iteration_start()\n        loss = 0\n        accuracy = 0\n        for epoch in range(10):\n            loss = train(model, train_loader, optimizer, criterion)\n            accuracy = test(model, test_loader, criterion)\n            print(\'current epoch: {0}, loss: {1}, accuracy: {2}\'.format(epoch, loss, accuracy))\n        print(\'prune iteration: {0}, loss: {1}, accuracy: {2}\'.format(i, loss, accuracy))\n    pruner.export_model(\'model.pth\', \'mask.pth\')\n'"
examples/model_compress/model_prune_torch.py,13,"b'import os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n\nfrom models.cifar10.vgg import VGG\nimport nni\nfrom nni.compression.torch import LevelPruner, SlimPruner, FPGMPruner, L1FilterPruner, \\\n    L2FilterPruner, AGP_Pruner, ActivationMeanRankFilterPruner, ActivationAPoZRankFilterPruner\n\nprune_config = {\n    \'level\': {\n        \'dataset_name\': \'mnist\',\n        \'model_name\': \'naive\',\n        \'pruner_class\': LevelPruner,\n        \'config_list\': [{\n            \'sparsity\': 0.5,\n            \'op_types\': [\'default\'],\n        }]\n    },\n    \'agp\': {\n        \'dataset_name\': \'mnist\',\n        \'model_name\': \'naive\',\n        \'pruner_class\': AGP_Pruner,\n        \'config_list\': [{\n            \'initial_sparsity\': 0.,\n            \'final_sparsity\': 0.8,\n            \'start_epoch\': 0,\n            \'end_epoch\': 10,\n            \'frequency\': 1,\n            \'op_types\': [\'default\']\n        }]\n    },\n    \'slim\': {\n        \'dataset_name\': \'cifar10\',\n        \'model_name\': \'vgg19\',\n        \'pruner_class\': SlimPruner,\n        \'config_list\': [{\n            \'sparsity\': 0.7,\n            \'op_types\': [\'BatchNorm2d\']\n        }]\n    },\n    \'fpgm\': {\n        \'dataset_name\': \'mnist\',\n        \'model_name\': \'naive\',\n        \'pruner_class\': FPGMPruner,\n        \'config_list\':[{\n            \'sparsity\': 0.5,\n            \'op_types\': [\'Conv2d\']\n        }]\n    },\n    \'l1filter\': {\n        \'dataset_name\': \'cifar10\',\n        \'model_name\': \'vgg16\',\n        \'pruner_class\': L1FilterPruner,\n        \'config_list\': [{\n            \'sparsity\': 0.5,\n            \'op_types\': [\'Conv2d\'],\n            \'op_names\': [\'feature.0\', \'feature.24\', \'feature.27\', \'feature.30\', \'feature.34\', \'feature.37\']\n        }]\n    },\n    \'mean_activation\': {\n        \'dataset_name\': \'cifar10\',\n        \'model_name\': \'vgg16\',\n        \'pruner_class\': ActivationMeanRankFilterPruner,\n        \'config_list\': [{\n            \'sparsity\': 0.5,\n            \'op_types\': [\'Conv2d\'],\n            \'op_names\': [\'feature.0\', \'feature.24\', \'feature.27\', \'feature.30\', \'feature.34\', \'feature.37\']\n        }]\n    },\n    \'apoz\': {\n        \'dataset_name\': \'cifar10\',\n        \'model_name\': \'vgg16\',\n        \'pruner_class\': ActivationAPoZRankFilterPruner,\n        \'config_list\': [{\n            \'sparsity\': 0.5,\n            \'op_types\': [\'default\'],\n            \'op_names\': [\'feature.0\', \'feature.24\', \'feature.27\', \'feature.30\', \'feature.34\', \'feature.37\']\n        }]\n    }\n}\n\ndef get_data_loaders(dataset_name=\'mnist\', batch_size=128):\n    assert dataset_name in [\'cifar10\', \'mnist\']\n\n    if dataset_name == \'cifar10\':\n        ds_class = datasets.CIFAR10 if dataset_name == \'cifar10\' else datasets.MNIST\n        MEAN, STD = (0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)\n    else:\n        ds_class = datasets.MNIST\n        MEAN, STD = (0.1307,), (0.3081,)\n\n    train_loader = DataLoader(\n        ds_class(\n            \'./data\', train=True, download=True,\n            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n        ),\n        batch_size=batch_size, shuffle=True\n    )\n    test_loader = DataLoader(\n        ds_class(\n            \'./data\', train=False, download=True,\n            transform=transforms.Compose([transforms.ToTensor(), transforms.Normalize(MEAN, STD)])\n        ),\n        batch_size=batch_size, shuffle=False\n    )\n\n    return train_loader, test_loader\n\nclass NaiveModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(x.size(0), -1)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\ndef create_model(model_name=\'naive\'):\n    assert model_name in [\'naive\', \'vgg16\', \'vgg19\']\n\n    if model_name == \'naive\':\n        return NaiveModel()\n    elif model_name == \'vgg16\':\n        return VGG(16)\n    else:\n        return VGG(19)\n\ndef create_pruner(model, pruner_name, optimizer=None):\n    pruner_class = prune_config[pruner_name][\'pruner_class\']\n    config_list = prune_config[pruner_name][\'config_list\']\n    return pruner_class(model, config_list, optimizer)\n\ndef train(model, device, train_loader, optimizer):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.cross_entropy(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    acc = 100 * correct / len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, acc))\n    return acc\n\ndef main(args):\n    device = torch.device(\'cuda\') if torch.cuda.is_available() else torch.device(\'cpu\')\n    os.makedirs(args.checkpoints_dir, exist_ok=True)\n\n    model_name = prune_config[args.pruner_name][\'model_name\']\n    dataset_name = prune_config[args.pruner_name][\'dataset_name\']\n    train_loader, test_loader = get_data_loaders(dataset_name, args.batch_size)\n    model = create_model(model_name).cuda()\n    if args.resume_from is not None and os.path.exists(args.resume_from):\n        print(\'loading checkpoint {} ...\'.format(args.resume_from))\n        model.load_state_dict(torch.load(args.resume_from))\n        test(model, device, test_loader)\n    else:\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n        if args.multi_gpu and torch.cuda.device_count():\n            model = nn.DataParallel(model)\n\n        print(\'start training\')\n        pretrain_model_path = os.path.join(\n            args.checkpoints_dir, \'pretrain_{}_{}_{}.pth\'.format(model_name, dataset_name, args.pruner_name))\n        for epoch in range(args.pretrain_epochs):\n            train(model, device, train_loader, optimizer)\n            test(model, device, test_loader)\n        torch.save(model.state_dict(), pretrain_model_path)\n\n    print(\'start model pruning...\')\n\n    model_path = os.path.join(args.checkpoints_dir, \'pruned_{}_{}_{}.pth\'.format(model_name, dataset_name, args.pruner_name))\n    mask_path = os.path.join(args.checkpoints_dir, \'mask_{}_{}_{}.pth\'.format(model_name, dataset_name, args.pruner_name))\n\n    # pruner needs to be initialized from a model not wrapped by DataParallel\n    if isinstance(model, nn.DataParallel):\n        model = model.module\n\n    optimizer_finetune = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n    best_top1 = 0\n\n    pruner = create_pruner(model, args.pruner_name, optimizer_finetune)\n    model = pruner.compress()\n\n    if args.multi_gpu and torch.cuda.device_count() > 1:\n        model = nn.DataParallel(model)\n\n    for epoch in range(args.prune_epochs):\n        pruner.update_epoch(epoch)\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer_finetune)\n        top1 = test(model, device, test_loader)\n        if top1 > best_top1:\n            best_top1 = top1\n            # Export the best model, \'model_path\' stores state_dict of the pruned model,\n            # mask_path stores mask_dict of the pruned model\n            pruner.export_model(model_path=model_path, mask_path=mask_path)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--pruner_name"", type=str, default=""level"", help=""pruner name"")\n    parser.add_argument(""--batch_size"", type=int, default=256)\n    parser.add_argument(""--pretrain_epochs"", type=int, default=10, help=""training epochs before model pruning"")\n    parser.add_argument(""--prune_epochs"", type=int, default=10, help=""training epochs for model pruning"")\n    parser.add_argument(""--checkpoints_dir"", type=str, default=""./checkpoints"", help=""checkpoints directory"")\n    parser.add_argument(""--resume_from"", type=str, default=None, help=""pretrained model weights"")\n    parser.add_argument(""--multi_gpu"", action=""store_true"", help=""Use multiple GPUs for training"")\n\n    args = parser.parse_args()\n    main(args)\n'"
examples/model_compress/model_speedup.py,7,"b'import os\nimport argparse\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom models.cifar10.vgg import VGG\nfrom nni.compression.torch import apply_compression_results, ModelSpeedup\n\ntorch.manual_seed(0)\nuse_mask = True\nuse_speedup = True\ncompare_results = True\n\nconfig = {\n    \'apoz\': {\n        \'model_name\': \'vgg16\',\n        \'device\': \'cuda\',\n        \'input_shape\': [64, 3, 32, 32],\n        \'masks_file\': \'./checkpoints/mask_vgg16_cifar10_apoz.pth\'\n    },\n    \'l1filter\': {\n        \'model_name\': \'vgg16\',\n        \'device\': \'cuda\',\n        \'input_shape\': [64, 3, 32, 32],\n        \'masks_file\': \'./checkpoints/mask_vgg16_cifar10_l1filter.pth\'\n    },\n    \'fpgm\': {\n        \'model_name\': \'naive\',\n        \'device\': \'cpu\',\n        \'input_shape\': [64, 1, 28, 28],\n        \'masks_file\': \'./checkpoints/mask_naive_mnist_fpgm.pth\'\n    },\n    \'slim\': {\n        \'model_name\': \'vgg19\',\n        \'device\': \'cuda\',\n        \'input_shape\': [64, 3, 32, 32],\n        \'masks_file\': \'./checkpoints/mask_vgg19_cifar10_slim.pth\' #\'mask_vgg19_cifar10.pth\'\n    }\n}\n\ndef model_inference(config):\n    masks_file = config[\'masks_file\']\n    device = torch.device(config[\'device\'])\n    if config[\'model_name\'] == \'vgg16\':\n        model = VGG(depth=16)\n    elif config[\'model_name\'] == \'vgg19\':\n        model = VGG(depth=19)\n    elif config[\'model_name\'] == \'naive\':\n        from model_prune_torch import NaiveModel\n        model = NaiveModel()\n    model.to(device)\n    model.eval()\n\n    dummy_input = torch.randn(config[\'input_shape\']).to(device)\n    use_mask_out = use_speedup_out = None\n    # must run use_mask before use_speedup because use_speedup modify the model\n    if use_mask:\n        apply_compression_results(model, masks_file, \'cpu\' if config[\'device\'] == \'cpu\' else None)\n        start = time.time()\n        for _ in range(32):\n            use_mask_out = model(dummy_input)\n        print(\'elapsed time when use mask: \', time.time() - start)\n    if use_speedup:\n        m_speedup = ModelSpeedup(model, dummy_input, masks_file,\n                                 \'cpu\' if config[\'device\'] == \'cpu\' else None)\n        m_speedup.speedup_model()\n        start = time.time()\n        for _ in range(32):\n            use_speedup_out = model(dummy_input)\n        print(\'elapsed time when use speedup: \', time.time() - start)\n    if compare_results:\n        if torch.allclose(use_mask_out, use_speedup_out, atol=1e-07):\n            print(\'the outputs from use_mask and use_speedup are the same\')\n        else:\n            raise RuntimeError(\'the outputs from use_mask and use_speedup are different\')\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(""speedup"")\n    parser.add_argument(""--example_name"", type=str, default=""slim"", help=""the name of pruning example"")\n    parser.add_argument(""--masks_file"", type=str, default=None, help=""the path of the masks file"")\n    args = parser.parse_args()\n\n    if args.example_name != \'all\':\n        if args.masks_file is not None:\n            config[args.example_name][\'masks_file\'] = args.masks_file\n        if not os.path.exists(config[args.example_name][\'masks_file\']):\n            msg = \'{} does not exist! You should specify masks_file correctly, \' \\\n                  \'or use default one which is generated by model_prune_torch.py\'\n            raise RuntimeError(msg.format(config[args.example_name][\'masks_file\']))\n        model_inference(config[args.example_name])\n    else:\n        model_inference(config[\'fpgm\'])\n        model_inference(config[\'slim\'])\n        model_inference(config[\'l1filter\'])\n        model_inference(config[\'apoz\'])\n'"
examples/model_compress/pruning_kd.py,14,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import L1FilterPruner\nfrom knowledge_distill.knowledge_distill import KnowledgeDistill\nfrom models.cifar10.vgg import VGG\n\n\ndef train(model, device, train_loader, optimizer, kd=None):\n    alpha = 1\n    beta = 0.8\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        student_loss = F.cross_entropy(output, target)\n        if kd is not None:\n            kd_loss = kd.loss(data=data, student_out=output)\n            loss = alpha * student_loss + beta * kd_loss\n        else:\n            loss = student_loss\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    acc = 100 * correct / len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, acc))\n    return acc\n\n\ndef main():\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                         transform=transforms.Compose([\n                             transforms.Pad(4),\n                             transforms.RandomCrop(32),\n                             transforms.RandomHorizontalFlip(),\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                         ])),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])),\n        batch_size=200, shuffle=False)\n\n    model = VGG(depth=16)\n    model.to(device)\n\n    # Train the base VGG-16 model\n    print(\'=\' * 10 + \'Train the unpruned base model\' + \'=\' * 10)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 160, 0)\n    for epoch in range(160):\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer)\n        test(model, device, test_loader)\n        lr_scheduler.step(epoch)\n    torch.save(model.state_dict(), \'vgg16_cifar10.pth\')\n\n    # Test base model accuracy\n    print(\'=\' * 10 + \'Test on the original model\' + \'=\' * 10)\n    model.load_state_dict(torch.load(\'vgg16_cifar10.pth\'))\n    test(model, device, test_loader)\n    # top1 = 93.51%\n\n    # Pruning Configuration, all convolution layers are pruned out 80% filters according to the L1 norm\n    configure_list = [{\n        \'sparsity\': 0.8,\n        \'op_types\': [\'Conv2d\'],\n    }]\n\n    # Prune model and test accuracy without fine tuning.\n    print(\'=\' * 10 + \'Test on the pruned model before fine tune\' + \'=\' * 10)\n    pruner = L1FilterPruner(model, configure_list)\n    model = pruner.compress()\n    test(model, device, test_loader)\n    # top1 = 10.00%\n\n    # Fine tune the pruned model for 40 epochs and test accuracy\n    print(\'=\' * 10 + \'Fine tuning\' + \'=\' * 10)\n    optimizer_finetune = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n    best_top1 = 0\n    kd_teacher_model = VGG(depth=16)\n    kd_teacher_model.to(device)\n    kd_teacher_model.load_state_dict(torch.load(\'vgg16_cifar10.pth\'))\n    kd = KnowledgeDistill(kd_teacher_model, kd_T=5)\n    for epoch in range(40):\n        pruner.update_epoch(epoch)\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer_finetune, kd)\n        top1 = test(model, device, test_loader)\n        if top1 > best_top1:\n            best_top1 = top1\n            # Export the best model, \'model_path\' stores state_dict of the pruned model,\n            # mask_path stores mask_dict of the pruned model\n            pruner.export_model(model_path=\'pruned_vgg16_cifar10.pth\', mask_path=\'mask_vgg16_cifar10.pth\')\n\n    # Test the exported model\n    print(\'=\' * 10 + \'Test on the pruned model after fine tune\' + \'=\' * 10)\n    new_model = VGG(depth=16)\n    new_model.to(device)\n    new_model.load_state_dict(torch.load(\'pruned_vgg16_cifar10.pth\'))\n    test(new_model, device, test_loader)\n    # top1 = 85.43% with kd, top1 = 85.04% without kd,\n\n\nif __name__ == \'__main__\':\n    main()\n'"
examples/model_compress/slim_torch_cifar10.py,15,"b'import math\nimport os\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom nni.compression.torch import SlimPruner\nfrom models.cifar10.vgg import VGG\n\ndef updateBN(model):\n    for m in model.modules():\n        if isinstance(m, nn.BatchNorm2d):\n            m.weight.grad.data.add_(0.0001 * torch.sign(m.weight.data))  # L1\n\n\ndef train(model, device, train_loader, optimizer, sparse_bn=False):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        # L1 regularization on BN layer\n        if sparse_bn:\n            updateBN(model)\n        optimizer.step()\n        if batch_idx % 100 == 0:\n            print(\'{:2.0f}%  Loss {}\'.format(100 * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n    test_loss /= len(test_loader.dataset)\n    acc = 100 * correct / len(test_loader.dataset)\n\n    print(\'Loss: {}  Accuracy: {}%)\\n\'.format(\n        test_loss, acc))\n    return acc\n\n\ndef main():\n    parser = argparse.ArgumentParser(""multiple gpu with pruning"")\n    parser.add_argument(""--epochs"", type=int, default=160)\n    parser.add_argument(""--retrain"", default=False, action=""store_true"")\n    parser.add_argument(""--parallel"", default=False, action=""store_true"")\n\n    args = parser.parse_args()\n\n    torch.manual_seed(0)\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    train_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=True, download=True,\n                         transform=transforms.Compose([\n                             transforms.Pad(4),\n                             transforms.RandomCrop(32),\n                             transforms.RandomHorizontalFlip(),\n                             transforms.ToTensor(),\n                             transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n                         ])),\n        batch_size=64, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.CIFAR10(\'./data.cifar10\', train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n        ])),\n        batch_size=200, shuffle=False)\n\n    model = VGG(depth=19)\n    model.to(device)\n    # Train the base VGG-19 model\n    if args.retrain:\n        print(\'=\' * 10 + \'Train the unpruned base model\' + \'=\' * 10)\n        epochs = args.epochs\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-4)\n        for epoch in range(epochs):\n            if epoch in [epochs * 0.5, epochs * 0.75]:\n                for param_group in optimizer.param_groups:\n                    param_group[\'lr\'] *= 0.1\n            print(""epoch {}"".format(epoch))\n            train(model, device, train_loader, optimizer, True)\n            test(model, device, test_loader)\n        torch.save(model.state_dict(), \'vgg19_cifar10.pth\')\n    else:\n        assert os.path.isfile(\'vgg19_cifar10.pth\'), ""can not find checkpoint \'vgg19_cifar10.pth\'""\n        model.load_state_dict(torch.load(\'vgg19_cifar10.pth\'))\n    # Test base model accuracy\n    print(\'=\' * 10 + \'Test the original model\' + \'=\' * 10)\n    test(model, device, test_loader)\n    # top1 = 93.60%\n\n    # Pruning Configuration, in paper \'Learning efficient convolutional networks through network slimming\',\n    configure_list = [{\n        \'sparsity\': 0.7,\n        \'op_types\': [\'BatchNorm2d\'],\n    }]\n    \n    # Prune model and test accuracy without fine tuning.\n    print(\'=\' * 10 + \'Test the pruned model before fine tune\' + \'=\' * 10)\n    optimizer_finetune = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=1e-4)\n    pruner = SlimPruner(model, configure_list, optimizer_finetune)\n    model = pruner.compress()\n    if args.parallel:\n        if torch.cuda.device_count() > 1:\n            print(""use {} gpus for pruning"".format(torch.cuda.device_count()))\n            model = nn.DataParallel(model)\n            # model = nn.DataParallel(model, device_ids=[0, 1])\n        else:\n            print(""only detect 1 gpu, fall back"")\n    model.to(device)\n    # Fine tune the pruned model for 40 epochs and test accuracy\n    print(\'=\' * 10 + \'Fine tuning\' + \'=\' * 10)\n    best_top1 = 0\n    for epoch in range(40):\n        print(\'# Epoch {} #\'.format(epoch))\n        train(model, device, train_loader, optimizer_finetune)\n        top1 = test(model, device, test_loader)\n\n        if top1 > best_top1:\n            best_top1 = top1\n            # Export the best model, \'model_path\' stores state_dict of the pruned model,\n            # mask_path stores mask_dict of the pruned model\n            pruner.export_model(model_path=\'pruned_vgg19_cifar10.pth\', mask_path=\'mask_vgg19_cifar10.pth\')\n\n    # Test the exported model\n    print(\'=\' * 10 + \'Test the export pruned model after fine tune\' + \'=\' * 10)\n    new_model = VGG(depth=19)\n    new_model.to(device)\n    new_model.load_state_dict(torch.load(\'pruned_vgg19_cifar10.pth\'))\n    test(new_model, device, test_loader)\n    # top1 = 93.74%\n\n\nif __name__ == \'__main__\':\n    main()\n'"
test/async_sharing_test/main.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nTest code for weight sharing\nneed NFS setup and mounted as `/mnt/nfs/nni`\n""""""\n\nimport hashlib\nimport os\nimport random\nimport time\n\nimport nni\n\n\ndef generate_rand_file(fl_name):\n    """"""\n    generate random file and write to `fl_name`\n    """"""\n    fl_size = random.randint(1024, 102400)\n    fl_dir = os.path.split(fl_name)[0]\n    if not os.path.exists(fl_dir):\n        os.makedirs(fl_dir)\n    with open(fl_name, \'wb\') as fout:\n        fout.write(os.urandom(fl_size))\n\n\ndef check_sum(fl_name, tid=None):\n    """"""\n    compute checksum for generated file of `fl_name`\n    """"""\n    hasher = hashlib.md5()\n    with open(fl_name, \'rb\') as fin:\n        for chunk in iter(lambda: fin.read(4096), b""""):\n            hasher.update(chunk)\n    ret = hasher.hexdigest()\n    if tid is not None:\n        ret = ret + str(tid)\n    return ret\n\n\nif __name__ == \'__main__\':\n    nfs_path = \'/mnt/nfs/nni/test\'\n    params = nni.get_next_parameter()\n    print(params)\n    if params[\'id\'] == 0:\n        model_file = os.path.join(nfs_path, str(params[\'id\']), \'model.dat\')\n        generate_rand_file(model_file)\n        time.sleep(10)\n        nni.report_final_result({\n            \'checksum\': check_sum(model_file, tid=params[\'id\']),\n            \'path\': model_file\n        })\n    else:\n        model_file = params[\'prev_path\']\n        time.sleep(10)\n        nni.report_final_result({\n            \'checksum\': check_sum(model_file, tid=params[\'prev_id\'])\n        })\n'"
test/async_sharing_test/simple_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nSimpleTuner for Weight Sharing\n""""""\n\nimport logging\n\nfrom threading import Event, Lock\nfrom nni.tuner import Tuner\n\n_logger = logging.getLogger(\'WeightSharingTuner\')\n\n\nclass SimpleTuner(Tuner):\n    """"""\n    simple tuner, test for weight sharing\n    """"""\n\n    def __init__(self):\n        super(SimpleTuner, self).__init__()\n        self.trial_meta = {}\n        self.f_id = None  # father\n        self.sig_event = Event()\n        self.thread_lock = Lock()\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        if self.f_id is None:\n            self.thread_lock.acquire()\n            self.f_id = parameter_id\n            self.trial_meta[parameter_id] = {\n                \'prev_id\': 0,\n                \'id\': parameter_id,\n                \'checksum\': None,\n                \'path\': \'\',\n            }\n            _logger.info(\'generate parameter for father trial %s\' %\n                         parameter_id)\n            self.thread_lock.release()\n            return {\n                \'prev_id\': 0,\n                \'id\': parameter_id,\n            }\n        else:\n            self.sig_event.wait()\n            self.thread_lock.acquire()\n            self.trial_meta[parameter_id] = {\n                \'id\': parameter_id,\n                \'prev_id\': self.f_id,\n                \'prev_path\': self.trial_meta[self.f_id][\'path\']\n            }\n            self.thread_lock.release()\n            return self.trial_meta[parameter_id]\n\n    def receive_trial_result(self, parameter_id, parameters, reward, **kwargs):\n        self.thread_lock.acquire()\n        if parameter_id == self.f_id:\n            self.trial_meta[parameter_id][\'checksum\'] = reward[\'checksum\']\n            self.trial_meta[parameter_id][\'path\'] = reward[\'path\']\n            self.sig_event.set()\n        else:\n            if reward[\'checksum\'] != self.trial_meta[self.f_id][\'checksum\']:\n                raise ValueError(""Inconsistency in weight sharing: {} != {}"".format(\n                    reward[\'checksum\'], self.trial_meta[self.f_id][\'checksum\']))\n        self.thread_lock.release()\n\n    def update_search_space(self, search_space):\n        pass\n'"
test/nni_test/setup.py,0,"b'from setuptools import setup, find_packages\n\nsetup(\n    name=""nnitest"",\n    version=""0.0.1"",\n    author = \'Microsoft NNI team\',\n    author_email = \'nni@microsoft.com\',\n    description = \'Neural Network Intelligence package\',\n    license = \'MIT\',\n    url = \'https://github.com/Microsoft/nni\',\n    packages=find_packages(\'nnitest\'),\n    long_description="""",\n    classifiers = [\n        \'Programming Language :: Python :: 3\',\n        \'License :: OSI Approved :: MIT License\',\n        ""Operating System :: OS Independent""\n    ],\n)\n'"
tools/nni_annotation/__init__.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport shutil\nimport json\n\nfrom . import code_generator\nfrom . import search_space_generator\nfrom . import specific_code_generator\n\n\n__all__ = [\'generate_search_space\', \'expand_annotations\']\n\nslash = \'/\'\nif sys.platform == ""win32"":\n    slash = \'\\\\\'\n\ndef generate_search_space(code_dir):\n    """"""Generate search space from Python source code.\n    Return a serializable search space object.\n    code_dir: directory path of source files (str)\n    """"""\n    search_space = {}\n\n    if code_dir.endswith(slash):\n        code_dir = code_dir[:-1]\n\n    for subdir, _, files in os.walk(code_dir):\n        # generate module name from path\n        if subdir == code_dir:\n            package = \'\'\n        else:\n            assert subdir.startswith(code_dir + slash), subdir\n            prefix_len = len(code_dir) + 1\n            package = subdir[prefix_len:].replace(slash, \'.\') + \'.\'\n\n        for file_name in files:\n            if file_name.endswith(\'.py\'):\n                path = os.path.join(subdir, file_name)\n                module = package + file_name[:-3]\n                search_space.update(_generate_file_search_space(path, module))\n\n    return search_space\n\ndef _generate_file_search_space(path, module):\n    with open(path) as src:\n        try:\n            search_space, code = search_space_generator.generate(module, src.read())\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(path + \' \' + \'\\n\'.join(exc.args))\n            else:\n                raise RuntimeError(\'Failed to generate search space for %s: %r\' % (path, exc))\n    with open(path, \'w\') as dst:\n        dst.write(code)\n    return search_space\n\n\ndef expand_annotations(src_dir, dst_dir, exp_id=\'\', trial_id=\'\', nas_mode=None):\n    """"""Expand annotations in user code.\n    Return dst_dir if annotation detected; return src_dir if not.\n    src_dir: directory path of user code (str)\n    dst_dir: directory to place generated files (str)\n    nas_mode: the mode of NAS given that NAS interface is used\n    """"""\n    if src_dir[-1] == slash:\n        src_dir = src_dir[:-1]\n\n    if dst_dir[-1] == slash:\n        dst_dir = dst_dir[:-1]\n\n    annotated = False\n\n    for src_subdir, dirs, files in os.walk(src_dir):\n        assert src_subdir.startswith(src_dir)\n        dst_subdir = src_subdir.replace(src_dir, dst_dir, 1)\n        os.makedirs(dst_subdir, exist_ok=True)\n\n        # generate module name from path\n        if src_subdir == src_dir:\n            package = \'\'\n        else:\n            assert src_subdir.startswith(src_dir + slash), src_subdir\n            prefix_len = len(src_dir) + 1\n            package = src_subdir[prefix_len:].replace(slash, \'.\') + \'.\'\n\n        for file_name in files:\n            src_path = os.path.join(src_subdir, file_name)\n            dst_path = os.path.join(dst_subdir, file_name)\n            if file_name.endswith(\'.py\'):\n                if trial_id == \'\':\n                    annotated |= _expand_file_annotations(src_path, dst_path, nas_mode)\n                else:\n                    module = package + file_name[:-3]\n                    annotated |= _generate_specific_file(src_path, dst_path, exp_id, trial_id, module)\n            else:\n                shutil.copyfile(src_path, dst_path)\n\n        for dir_name in dirs:\n            os.makedirs(os.path.join(dst_subdir, dir_name), exist_ok=True)\n\n    return dst_dir if annotated else src_dir\n\ndef _expand_file_annotations(src_path, dst_path, nas_mode):\n    with open(src_path) as src, open(dst_path, \'w\') as dst:\n        try:\n            annotated_code = code_generator.parse(src.read(), nas_mode)\n            if annotated_code is None:\n                shutil.copyfile(src_path, dst_path)\n                return False\n            dst.write(annotated_code)\n            return True\n\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(src_path + \' \' + \'\\n\'.join(str(arg) for arg in exc.args))\n            else:\n                raise RuntimeError(\'Failed to expand annotations for %s: %r\' % (src_path, exc))\n\ndef _generate_specific_file(src_path, dst_path, exp_id, trial_id, module):\n    with open(src_path) as src, open(dst_path, \'w\') as dst:\n        try:\n            with open(os.path.expanduser(\'~/nni/experiments/%s/trials/%s/parameter.cfg\'%(exp_id, trial_id))) as fd:\n                para_cfg = json.load(fd)\n            annotated_code = specific_code_generator.parse(src.read(), para_cfg[""parameters""], module)\n            if annotated_code is None:\n                shutil.copyfile(src_path, dst_path)\n                return False\n            dst.write(annotated_code)\n            return True\n\n        except Exception as exc:  # pylint: disable=broad-except\n            if exc.args:\n                raise RuntimeError(src_path + \' \' + \'\\n\'.join(str(arg) for arg in exc.args))\n            else:\n                raise RuntimeError(\'Failed to expand annotations for %s: %r\' % (src_path, exc))\n'"
tools/nni_annotation/code_generator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport ast\nimport astor\n\n\n# pylint: disable=unidiomatic-typecheck\n\ndef parse_annotation_mutable_layers(code, lineno, nas_mode):\n    """"""Parse the string of mutable layers in annotation.\n    Return a list of AST Expr nodes\n    code: annotation string (excluding \'@\')\n    nas_mode: the mode of NAS\n    """"""\n    module = ast.parse(code)\n    assert type(module) is ast.Module, \'internal error #1\'\n    assert len(module.body) == 1, \'Annotation mutable_layers contains more than one expression\'\n    assert type(module.body[0]) is ast.Expr, \'Annotation is not expression\'\n    call = module.body[0].value\n    nodes = []\n    mutable_id = \'mutable_block_\' + str(lineno)\n    mutable_layer_cnt = 0\n    for arg in call.args:\n        fields = {\'layer_choice\': False,\n                  \'fixed_inputs\': False,\n                  \'optional_inputs\': False,\n                  \'optional_input_size\': False,\n                  \'layer_output\': False}\n        for k, value in zip(arg.keys, arg.values):\n            if k.id == \'layer_choice\':\n                assert not fields[\'layer_choice\'], \'Duplicated field: layer_choice\'\n                assert type(value) is ast.List, \'Value of layer_choice should be a list\'\n                call_funcs_keys = []\n                call_funcs_values = []\n                call_kwargs_values = []\n                for call in value.elts:\n                    assert type(call) is ast.Call, \'Element in layer_choice should be function call\'\n                    call_name = astor.to_source(call).strip()\n                    call_funcs_keys.append(ast.Str(s=call_name))\n                    call_funcs_values.append(call.func)\n                    assert not call.args, \'Number of args without keyword should be zero\'\n                    kw_args = []\n                    kw_values = []\n                    for kw in call.keywords:\n                        kw_args.append(ast.Str(s=kw.arg))\n                        kw_values.append(kw.value)\n                    call_kwargs_values.append(ast.Dict(keys=kw_args, values=kw_values))\n                call_funcs = ast.Dict(keys=call_funcs_keys, values=call_funcs_values)\n                call_kwargs = ast.Dict(keys=call_funcs_keys, values=call_kwargs_values)\n                fields[\'layer_choice\'] = True\n            elif k.id == \'fixed_inputs\':\n                assert not fields[\'fixed_inputs\'], \'Duplicated field: fixed_inputs\'\n                assert type(value) is ast.List, \'Value of fixed_inputs should be a list\'\n                fixed_inputs = value\n                fields[\'fixed_inputs\'] = True\n            elif k.id == \'optional_inputs\':\n                assert not fields[\'optional_inputs\'], \'Duplicated field: optional_inputs\'\n                assert type(value) is ast.List, \'Value of optional_inputs should be a list\'\n                var_names = [ast.Str(s=astor.to_source(var).strip()) for var in value.elts]\n                optional_inputs = ast.Dict(keys=var_names, values=value.elts)\n                fields[\'optional_inputs\'] = True\n            elif k.id == \'optional_input_size\':\n                assert not fields[\'optional_input_size\'], \'Duplicated field: optional_input_size\'\n                assert type(value) is ast.Num or type(value) is ast.List, \\\n                    \'Value of optional_input_size should be a number or list\'\n                optional_input_size = value\n                fields[\'optional_input_size\'] = True\n            elif k.id == \'layer_output\':\n                assert not fields[\'layer_output\'], \'Duplicated field: layer_output\'\n                assert type(value) is ast.Name, \'Value of layer_output should be ast.Name type\'\n                layer_output = value\n                fields[\'layer_output\'] = True\n            else:\n                raise AssertionError(\'Unexpected field in mutable layer\')\n        # make call for this mutable layer\n        assert fields[\'layer_choice\'], \'layer_choice must exist\'\n        assert fields[\'layer_output\'], \'layer_output must exist\'\n        mutable_layer_id = \'mutable_layer_\' + str(mutable_layer_cnt)\n        mutable_layer_cnt += 1\n        target_call_attr = ast.Attribute(value=ast.Name(id=\'nni\', ctx=ast.Load()), attr=\'mutable_layer\', ctx=ast.Load())\n        target_call_args = [ast.Str(s=mutable_id),\n                            ast.Str(s=mutable_layer_id),\n                            call_funcs,\n                            call_kwargs]\n        if fields[\'fixed_inputs\']:\n            target_call_args.append(fixed_inputs)\n        else:\n            target_call_args.append(ast.List(elts=[]))\n        if fields[\'optional_inputs\']:\n            target_call_args.append(optional_inputs)\n            assert fields[\'optional_input_size\'], \'optional_input_size must exist when optional_inputs exists\'\n            target_call_args.append(optional_input_size)\n        else:\n            target_call_args.append(ast.Dict(keys=[], values=[]))\n            target_call_args.append(ast.Num(n=0))\n        target_call_args.append(ast.Str(s=nas_mode))\n        if nas_mode in [\'enas_mode\', \'oneshot_mode\', \'darts_mode\']:\n            target_call_args.append(ast.Name(id=\'tensorflow\'))\n        target_call = ast.Call(func=target_call_attr, args=target_call_args, keywords=[])\n        node = ast.Assign(targets=[layer_output], value=target_call)\n        nodes.append(node)\n    return nodes\n\n\ndef parse_annotation(code):\n    """"""Parse an annotation string.\n    Return an AST Expr node.\n    code: annotation string (excluding \'@\')\n    """"""\n    module = ast.parse(code)\n    assert type(module) is ast.Module, \'internal error #1\'\n    assert len(module.body) == 1, \'Annotation contains more than one expression\'\n    assert type(module.body[0]) is ast.Expr, \'Annotation is not expression\'\n    return module.body[0]\n\n\ndef parse_annotation_function(code, func_name):\n    """"""Parse an annotation function.\n    Return the value of `name` keyword argument and the AST Call node.\n    func_name: expected function name\n    """"""\n    expr = parse_annotation(code)\n    call = expr.value\n    assert type(call) is ast.Call, \'Annotation is not a function call\'\n\n    assert type(call.func) is ast.Attribute, \'Unexpected annotation function\'\n    assert type(call.func.value) is ast.Name, \'Invalid annotation function name\'\n    assert call.func.value.id == \'nni\', \'Annotation is not a NNI function\'\n    assert call.func.attr == func_name, \'internal error #2\'\n\n    assert len(call.keywords) == 1, \'Annotation function contains more than one keyword argument\'\n    assert call.keywords[0].arg == \'name\', \'Annotation keyword argument is not ""name""\'\n    name = call.keywords[0].value\n\n    return name, call\n\n\ndef parse_nni_variable(code):\n    """"""Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string\n    """"""\n    name, call = parse_annotation_function(code, \'variable\')\n\n    assert len(call.args) == 1, \'nni.variable contains more than one arguments\'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, \'Value of nni.variable is not a function call\'\n    assert type(arg.func) is ast.Attribute, \'nni.variable value is not a NNI function\'\n    assert type(arg.func.value) is ast.Name, \'nni.variable value is not a NNI function\'\n    assert arg.func.value.id == \'nni\', \'nni.variable value is not a NNI function\'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg=\'name\', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == \'choice\':\n        convert_args_to_dict(arg)\n\n    return name, arg\n\n\ndef parse_nni_function(code):\n    """"""Parse `nni.function_choice` expression.\n    Return the AST node of annotated expression and a list of dumped function call expressions.\n    code: annotation string\n    """"""\n    name, call = parse_annotation_function(code, \'function_choice\')\n    funcs = [ast.dump(func, False) for func in call.args]\n    convert_args_to_dict(call, with_lambda=True)\n\n    name_str = astor.to_source(name).strip()\n    call.keywords[0].value = ast.Str(s=name_str)\n\n    return call, funcs\n\n\ndef convert_args_to_dict(call, with_lambda=False):\n    """"""Convert all args to a dict such that every key and value in the dict is the same as the value of the arg.\n    Return the AST Call node with only one arg that is the dictionary\n    """"""\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n            # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip(\'\\n""\')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call\n\n\ndef make_lambda(call):\n    """"""Wrap an AST Call node to lambda expression node.\n    call: ast.Call node\n    """"""\n    empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[])\n    return ast.Lambda(args=empty_args, body=call)\n\n\ndef test_variable_equal(node1, node2):\n    """"""Test whether two variables are the same.""""""\n    if type(node1) is not type(node2):\n        return False\n    if isinstance(node1, ast.AST):\n        for k, v in vars(node1).items():\n            if k in (\'lineno\', \'col_offset\', \'ctx\'):\n                continue\n            if not test_variable_equal(v, getattr(node2, k)):\n                return False\n        return True\n    if isinstance(node1, list):\n        if len(node1) != len(node2):\n            return False\n        return all(test_variable_equal(n1, n2) for n1, n2 in zip(node1, node2))\n\n    return node1 == node2\n\n\ndef replace_variable_node(node, annotation):\n    """"""Replace a node annotated by `nni.variable`.\n    node: the AST node to replace\n    annotation: annotation string\n    """"""\n    assert type(node) is ast.Assign, \'nni.variable is not annotating assignment expression\'\n    assert len(node.targets) == 1, \'Annotated assignment has more than one left-hand value\'\n    name, expr = parse_nni_variable(annotation)\n    assert test_variable_equal(node.targets[0], name), \'Annotated variable has wrong name\'\n    node.value = expr\n    return node\n\n\ndef replace_function_node(node, annotation):\n    """"""Replace a node annotated by `nni.function_choice`.\n    node: the AST node to replace\n    annotation: annotation string\n    """"""\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node\n\n\nclass FuncReplacer(ast.NodeTransformer):\n    """"""To replace target function call expressions in a node annotated by `nni.function_choice`""""""\n\n    def __init__(self, funcs, target):\n        """"""Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        """"""\n        self.funcs = set(funcs)\n        self.target = target\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node\n\n\nclass Transformer(ast.NodeTransformer):\n    """"""Transform original code to annotated code""""""\n\n    def __init__(self, nas_mode=None):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False\n        self.nas_mode = nas_mode\n\n    def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, \'Annotating an annotation\'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith(\'nni.variable\'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith(\'nni.function_choice\'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)\n\n    def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith(\'@nni.\'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith(\'@nni.training_update\'):\n            expr = parse_annotation(string[1:])\n            call_node = expr.value\n            call_node.args.insert(0, ast.Str(s=self.nas_mode))\n            return expr\n\n        if string.startswith(\'@nni.report_intermediate_result\') \\\n                or string.startswith(\'@nni.report_final_result\') \\\n                or string.startswith(\'@nni.get_next_parameter\'):\n            return parse_annotation(string[1:])  # expand annotation string to code\n\n        if string.startswith(\'@nni.mutable_layers\'):\n            nodes = parse_annotation_mutable_layers(string[1:], node.lineno, self.nas_mode)\n            return nodes\n\n        if string.startswith(\'@nni.variable\') \\\n                or string.startswith(\'@nni.function_choice\'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError(\'Unexpected annotation function\')\n\n    def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, \'Annotation has no target\'\n        return node\n\n\ndef parse(code, nas_mode=None):\n    """"""Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str),\n    nas_mode: the mode of NAS given that NAS interface is used\n    """"""\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError(\'Bad Python code\')\n\n    transformer = Transformer(nas_mode)\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError(\'%d: %s\' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    last_future_import = -1\n    import_nni = ast.Import(names=[ast.alias(name=\'nni\', asname=None)])\n    nodes = ast_tree.body\n    for i, _ in enumerate(nodes):\n        if type(nodes[i]) is ast.ImportFrom and nodes[i].module == \'__future__\':\n            last_future_import = i\n    nodes.insert(last_future_import + 1, import_nni)\n    # enas, oneshot and darts modes for tensorflow need tensorflow module, so we import it here\n    if nas_mode in [\'enas_mode\', \'oneshot_mode\', \'darts_mode\']:\n        import_tf = ast.Import(names=[ast.alias(name=\'tensorflow\', asname=None)])\n        nodes.insert(last_future_import + 1, import_tf)\n\n    return astor.to_source(ast_tree)\n'"
tools/nni_annotation/search_space_generator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport ast\nimport numbers\n\nimport astor\n\n# pylint: disable=unidiomatic-typecheck\n\n\n# list of functions related to search space generating\n_ss_funcs = [\n    \'choice\',\n    \'randint\',\n    \'uniform\',\n    \'quniform\',\n    \'loguniform\',\n    \'qloguniform\',\n    \'normal\',\n    \'qnormal\',\n    \'lognormal\',\n    \'qlognormal\',\n    \'function_choice\',\n    \'mutable_layer\'\n]\n\n\nclass SearchSpaceGenerator(ast.NodeTransformer):\n    """"""Generate search space from smart parater APIs""""""\n\n    def __init__(self, module_name):\n        self.module_name = module_name\n        self.search_space = {}\n        self.last_line = 0  # last parsed line, useful for error reporting\n\n    def generate_mutable_layer_search_space(self, args):\n        mutable_block = args[0].s\n        mutable_layer = args[1].s\n        key = self.module_name + \'/\' + mutable_block\n        args[0].s = key\n        if key not in self.search_space:\n            self.search_space[key] = {\'_type\': \'mutable_layer\', \'_value\': {}}\n        self.search_space[key][\'_value\'][mutable_layer] = {\n            \'layer_choice\': [k.s for k in args[2].keys],\n            \'optional_inputs\': [k.s for k in args[5].keys],\n            \'optional_input_size\': args[6].n if isinstance(args[6], ast.Num) else [args[6].elts[0].n, args[6].elts[1].n]\n        }\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        self.generic_visit(node)\n\n        # ignore if the function is not \'nni.*\'\n        if type(node.func) is not ast.Attribute:\n            return node\n        if type(node.func.value) is not ast.Name:\n            return node\n        if node.func.value.id != \'nni\':\n            return node\n\n        # ignore if its not a search space function (e.g. `report_final_result`)\n        func = node.func.attr\n        if func not in _ss_funcs:\n            return node\n\n        self.last_line = node.lineno\n\n        if func == \'mutable_layer\':\n            self.generate_mutable_layer_search_space(node.args)\n            return node\n\n        if node.keywords:\n            # there is a `name` argument\n            assert len(node.keywords) == 1, \'Smart parameter has keyword argument other than ""name""\'\n            assert node.keywords[0].arg == \'name\', \'Smart paramater\\\'s keyword argument is not ""name""\'\n            assert type(node.keywords[0].value) is ast.Str, \'Smart parameter\\\'s name must be string literal\'\n            name = node.keywords[0].value.s\n            specified_name = True\n        else:\n            # generate the missing name automatically\n            name = \'__line\' + str(str(node.args[-1].lineno))\n            specified_name = False\n            node.keywords = list()\n\n        if func in (\'choice\', \'function_choice\'):\n            # we will use keys in the dict as the choices, which is generated by code_generator according to the args given by user\n            assert len(node.args) == 1, \'Smart parameter has arguments other than dict\'\n            # check if it is a number or a string and get its value accordingly\n            args = [key.n if type(key) is ast.Num else key.s for key in node.args[0].keys]\n        else:\n            # arguments of other functions must be literal number\n            assert all(isinstance(ast.literal_eval(astor.to_source(arg)), numbers.Real) for arg in node.args), \\\n                \'Smart parameter\\\'s arguments must be number literals\'\n            args = [ast.literal_eval(astor.to_source(arg)) for arg in node.args]\n\n        key = self.module_name + \'/\' + name + \'/\' + func\n        # store key in ast.Call\n        node.keywords.append(ast.keyword(arg=\'key\', value=ast.Str(s=key)))\n\n        if func == \'function_choice\':\n            func = \'choice\'\n        value = {\'_type\': func, \'_value\': args}\n\n        if specified_name:\n            # multiple functions with same name must have identical arguments\n            old = self.search_space.get(key)\n            assert old is None or old == value, \'Different smart parameters have same name\'\n        else:\n            # generated name must not duplicate\n            assert key not in self.search_space, \'Only one smart parameter is allowed in a line\'\n\n        self.search_space[key] = value\n\n        return node\n\n\ndef generate(module_name, code):\n    """"""Generate search space.\n    Return a serializable search space object.\n    module_name: name of the module (str)\n    code: user code (str)\n    """"""\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError(\'Bad Python code\')\n\n    visitor = SearchSpaceGenerator(module_name)\n    try:\n        visitor.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError(\'%d: %s\' % (visitor.last_line, exc.args[0]))\n    return visitor.search_space, astor.to_source(ast_tree)\n'"
tools/nni_annotation/specific_code_generator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport ast\nimport astor\nfrom nni_cmd.common_utils import print_warning\n\n# pylint: disable=unidiomatic-typecheck\n\npara_cfg = None\nprefix_name = None\n\n\ndef parse_annotation_mutable_layers(code, lineno):\n    """"""Parse the string of mutable layers in annotation.\n    Return a list of AST Expr nodes\n    code: annotation string (excluding \'@\')\n    """"""\n    module = ast.parse(code)\n    assert type(module) is ast.Module, \'internal error #1\'\n    assert len(module.body) == 1, \'Annotation mutable_layers contains more than one expression\'\n    assert type(module.body[0]) is ast.Expr, \'Annotation is not expression\'\n    call = module.body[0].value\n    nodes = []\n    mutable_id = prefix_name + \'/mutable_block_\' + str(lineno)\n    mutable_layer_cnt = 0\n    for arg in call.args:\n        fields = {\'layer_choice\': False,\n                  \'fixed_inputs\': False,\n                  \'optional_inputs\': False,\n                  \'optional_input_size\': False,\n                  \'layer_output\': False}\n        mutable_layer_id = \'mutable_layer_\' + str(mutable_layer_cnt)\n        mutable_layer_cnt += 1\n        func_call = None\n        for k, value in zip(arg.keys, arg.values):\n            if k.id == \'layer_choice\':\n                assert not fields[\'layer_choice\'], \'Duplicated field: layer_choice\'\n                assert type(value) is ast.List, \'Value of layer_choice should be a list\'\n                for call in value.elts:\n                    assert type(call) is ast.Call, \'Element in layer_choice should be function call\'\n                    call_name = astor.to_source(call).strip()\n                    if call_name == para_cfg[mutable_id][mutable_layer_id][\'chosen_layer\']:\n                        func_call = call\n                        assert not call.args, \'Number of args without keyword should be zero\'\n                        break\n                fields[\'layer_choice\'] = True\n            elif k.id == \'fixed_inputs\':\n                assert not fields[\'fixed_inputs\'], \'Duplicated field: fixed_inputs\'\n                assert type(value) is ast.List, \'Value of fixed_inputs should be a list\'\n                fixed_inputs = value\n                fields[\'fixed_inputs\'] = True\n            elif k.id == \'optional_inputs\':\n                assert not fields[\'optional_inputs\'], \'Duplicated field: optional_inputs\'\n                assert type(value) is ast.List, \'Value of optional_inputs should be a list\'\n                var_names = [astor.to_source(var).strip() for var in value.elts]\n                chosen_inputs = para_cfg[mutable_id][mutable_layer_id][\'chosen_inputs\']\n                elts = []\n                for i in chosen_inputs:\n                    index = var_names.index(i)\n                    elts.append(value.elts[index])\n                optional_inputs = ast.List(elts=elts)\n                fields[\'optional_inputs\'] = True\n            elif k.id == \'optional_input_size\':\n                pass\n            elif k.id == \'layer_output\':\n                assert not fields[\'layer_output\'], \'Duplicated field: layer_output\'\n                assert type(value) is ast.Name, \'Value of layer_output should be ast.Name type\'\n                layer_output = value\n                fields[\'layer_output\'] = True\n            else:\n                raise AssertionError(\'Unexpected field in mutable layer\')\n        # make call for this mutable layer\n        assert fields[\'layer_choice\'], \'layer_choice must exist\'\n        assert fields[\'layer_output\'], \'layer_output must exist\'\n\n        if not fields[\'fixed_inputs\']:\n            fixed_inputs = ast.List(elts=[])\n        if not fields[\'optional_inputs\']:\n            optional_inputs = ast.List(elts=[])\n        inputs = ast.List(elts=[fixed_inputs, optional_inputs])\n\n        func_call.args.append(inputs)\n        node = ast.Assign(targets=[layer_output], value=func_call)\n        nodes.append(node)\n    return nodes\n\n\ndef parse_annotation(code):\n    """"""Parse an annotation string.\n    Return an AST Expr node.\n    code: annotation string (excluding \'@\')\n    """"""\n    module = ast.parse(code)\n    assert type(module) is ast.Module, \'internal error #1\'\n    assert len(module.body) == 1, \'Annotation contains more than one expression\'\n    assert type(module.body[0]) is ast.Expr, \'Annotation is not expression\'\n    return module.body[0]\n\n\ndef parse_annotation_function(code, func_name):\n    """"""Parse an annotation function.\n    Return the value of `name` keyword argument and the AST Call node.\n    func_name: expected function name\n    """"""\n    expr = parse_annotation(code)\n    call = expr.value\n    assert type(call) is ast.Call, \'Annotation is not a function call\'\n\n    assert type(call.func) is ast.Attribute, \'Unexpected annotation function\'\n    assert type(call.func.value) is ast.Name, \'Invalid annotation function name\'\n    assert call.func.value.id == \'nni\', \'Annotation is not a NNI function\'\n    assert call.func.attr == func_name, \'internal error #2\'\n\n    assert len(call.keywords) == 1, \'Annotation function contains more than one keyword argument\'\n    assert call.keywords[0].arg == \'name\', \'Annotation keyword argument is not ""name""\'\n    name = call.keywords[0].value\n\n    return name, call\n\n\ndef parse_nni_variable(code):\n    """"""Parse `nni.variable` expression.\n    Return the name argument and AST node of annotated expression.\n    code: annotation string\n    """"""\n    name, call = parse_annotation_function(code, \'variable\')\n\n    assert len(call.args) == 1, \'nni.variable contains more than one arguments\'\n    arg = call.args[0]\n    assert type(arg) is ast.Call, \'Value of nni.variable is not a function call\'\n    assert type(arg.func) is ast.Attribute, \'nni.variable value is not a NNI function\'\n    assert type(arg.func.value) is ast.Name, \'nni.variable value is not a NNI function\'\n    assert arg.func.value.id == \'nni\', \'nni.variable value is not a NNI function\'\n\n    name_str = astor.to_source(name).strip()\n    keyword_arg = ast.keyword(arg=\'name\', value=ast.Str(s=name_str))\n    arg.keywords.append(keyword_arg)\n    if arg.func.attr == \'choice\':\n        convert_args_to_dict(arg)\n\n    return name, arg\n\n\ndef parse_nni_function(code):\n    """"""Parse `nni.function_choice` expression.\n    Return the AST node of annotated expression and a list of dumped function call expressions.\n    code: annotation string\n    """"""\n    name, call = parse_annotation_function(code, \'function_choice\')\n    funcs = [ast.dump(func, False) for func in call.args]\n    convert_args_to_dict(call, with_lambda=True)\n\n    name_str = astor.to_source(name).strip()\n    call.keywords[0].value = ast.Str(s=name_str)\n\n    return call, funcs\n\n\ndef convert_args_to_dict(call, with_lambda=False):\n    """"""Convert all args to a dict such that every key and value in the dict is the same as the value of the arg.\n    Return the AST Call node with only one arg that is the dictionary\n    """"""\n    keys, values = list(), list()\n    for arg in call.args:\n        if type(arg) in [ast.Str, ast.Num]:\n            arg_value = arg\n        else:\n            # if arg is not a string or a number, we use its source code as the key\n            arg_value = astor.to_source(arg).strip(\'\\n""\')\n            arg_value = ast.Str(str(arg_value))\n        arg = make_lambda(arg) if with_lambda else arg\n        keys.append(arg_value)\n        values.append(arg)\n    del call.args[:]\n    call.args.append(ast.Dict(keys=keys, values=values))\n\n    return call\n\n\ndef make_lambda(call):\n    """"""Wrap an AST Call node to lambda expression node.\n    call: ast.Call node\n    """"""\n    empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[])\n    return ast.Lambda(args=empty_args, body=call)\n\n\ndef test_variable_equal(node1, node2):\n    """"""Test whether two variables are the same.""""""\n    if type(node1) is not type(node2):\n        return False\n    if isinstance(node1, ast.AST):\n        for k, v in vars(node1).items():\n            if k in (\'lineno\', \'col_offset\', \'ctx\'):\n                continue\n            if not test_variable_equal(v, getattr(node2, k)):\n                return False\n        return True\n    if isinstance(node1, list):\n        if len(node1) != len(node2):\n            return False\n        return all(test_variable_equal(n1, n2) for n1, n2 in zip(node1, node2))\n\n    return node1 == node2\n\n\ndef replace_variable_node(node, annotation):\n    """"""Replace a node annotated by `nni.variable`.\n    node: the AST node to replace\n    annotation: annotation string\n    """"""\n    assert type(node) is ast.Assign, \'nni.variable is not annotating assignment expression\'\n    assert len(node.targets) == 1, \'Annotated assignment has more than one left-hand value\'\n    name, expr = parse_nni_variable(annotation)\n    assert test_variable_equal(node.targets[0], name), \'Annotated variable has wrong name\'\n    node.value = expr\n    return node\n\n\ndef replace_function_node(node, annotation):\n    """"""Replace a node annotated by `nni.function_choice`.\n    node: the AST node to replace\n    annotation: annotation string\n    """"""\n    target, funcs = parse_nni_function(annotation)\n    FuncReplacer(funcs, target).visit(node)\n    return node\n\n\nclass FuncReplacer(ast.NodeTransformer):\n    """"""To replace target function call expressions in a node annotated by `nni.function_choice`""""""\n\n    def __init__(self, funcs, target):\n        """"""Constructor.\n        funcs: list of dumped function call expressions to replace\n        target: use this AST node to replace matching expressions\n        """"""\n        self.funcs = set(funcs)\n        self.target = target\n\n    def visit_Call(self, node):  # pylint: disable=invalid-name\n        if ast.dump(node, False) in self.funcs:\n            return self.target\n        return node\n\n\nclass Transformer(ast.NodeTransformer):\n    """"""Transform original code to annotated code""""""\n\n    def __init__(self):\n        self.stack = []\n        self.last_line = 0\n        self.annotated = False\n\n    def visit(self, node):\n        if isinstance(node, (ast.expr, ast.stmt)):\n            self.last_line = node.lineno\n\n        # do nothing for root\n        if not self.stack:\n            return self._visit_children(node)\n\n        annotation = self.stack[-1]\n\n        # this is a standalone string, may be an annotation\n        if type(node) is ast.Expr and type(node.value) is ast.Str:\n            # must not annotate an annotation string\n            assert annotation is None, \'Annotating an annotation\'\n            return self._visit_string(node)\n\n        if annotation is not None:  # this expression is annotated\n            self.stack[-1] = None  # so next expression is not\n            if annotation.startswith(\'nni.variable\'):\n                return replace_variable_node(node, annotation)\n            if annotation.startswith(\'nni.function_choice\'):\n                return replace_function_node(node, annotation)\n\n        return self._visit_children(node)\n\n    def _visit_string(self, node):\n        string = node.value.s\n        if string.startswith(\'@nni.\'):\n            self.annotated = True\n        else:\n            return node  # not an annotation, ignore it\n\n        if string.startswith(\'@nni.get_next_parameter\'):\n            deprecated_message = ""\'@nni.get_next_parameter\' is deprecated in annotation due to inconvenience. "" \\\n                                 ""Please remove this line in the trial code.""\n            print_warning(deprecated_message)\n            return ast.Expr(value=ast.Call(func=ast.Name(id=\'print\', ctx=ast.Load()),\n                                           args=[ast.Str(s=\'Get next parameter here...\')], keywords=[]))\n\n        if string.startswith(\'@nni.training_update\'):\n            return ast.Expr(value=ast.Call(func=ast.Name(id=\'print\', ctx=ast.Load()),\n                                           args=[ast.Str(s=\'Training update here...\')], keywords=[]))\n\n        if string.startswith(\'@nni.report_intermediate_result\'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id=\'print\', ctx=ast.Load()),\n                                           args=[ast.Str(s=\'nni.report_intermediate_result: \'), arg], keywords=[]))\n\n        if string.startswith(\'@nni.report_final_result\'):\n            module = ast.parse(string[1:])\n            arg = module.body[0].value.args[0]\n            return ast.Expr(value=ast.Call(func=ast.Name(id=\'print\', ctx=ast.Load()),\n                                           args=[ast.Str(s=\'nni.report_final_result: \'), arg], keywords=[]))\n\n        if string.startswith(\'@nni.mutable_layers\'):\n            return parse_annotation_mutable_layers(string[1:], node.lineno)\n\n        if string.startswith(\'@nni.variable\') \\\n                or string.startswith(\'@nni.function_choice\'):\n            self.stack[-1] = string[1:]  # mark that the next expression is annotated\n            return None\n\n        raise AssertionError(\'Unexpected annotation function\')\n\n    def _visit_children(self, node):\n        self.stack.append(None)\n        self.generic_visit(node)\n        annotation = self.stack.pop()\n        assert annotation is None, \'Annotation has no target\'\n        return node\n\n\ndef parse(code, para, module):\n    """"""Annotate user code.\n    Return annotated code (str) if annotation detected; return None if not.\n    code: original user code (str)\n    """"""\n    global para_cfg\n    global prefix_name\n    para_cfg = para\n    prefix_name = module\n    try:\n        ast_tree = ast.parse(code)\n    except Exception:\n        raise RuntimeError(\'Bad Python code\')\n\n    transformer = Transformer()\n    try:\n        transformer.visit(ast_tree)\n    except AssertionError as exc:\n        raise RuntimeError(\'%d: %s\' % (ast_tree.last_line, exc.args[0]))\n\n    if not transformer.annotated:\n        return None\n\n    return astor.to_source(ast_tree)\n'"
tools/nni_annotation/test_annotation.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n# pylint: skip-file\n\nfrom .__init__ import *\n\nimport ast\nimport json\nimport os\nimport shutil\nimport tempfile\nfrom unittest import TestCase, main\n\n\nclass AnnotationTestCase(TestCase):\n    @classmethod\n    def setUpClass(cls):\n        os.chdir('nni_annotation')\n        if os.path.isdir('_generated'):\n            shutil.rmtree('_generated')\n\n    def test_search_space_generator(self):\n        shutil.copytree('testcase/annotated', '_generated/annotated')\n        search_space = generate_search_space('_generated/annotated')\n        with open('testcase/searchspace.json') as f:\n            self.assertEqual(search_space, json.load(f))\n\n    def test_code_generator(self):\n        code_dir = expand_annotations('testcase/usercode', '_generated/usercode', nas_mode='classic_mode')\n        self.assertEqual(code_dir, '_generated/usercode')\n        self._assert_source_equal('testcase/annotated/nas.py', '_generated/usercode/nas.py')\n        self._assert_source_equal('testcase/annotated/mnist.py', '_generated/usercode/mnist.py')\n        self._assert_source_equal('testcase/annotated/dir/simple.py', '_generated/usercode/dir/simple.py')\n        with open('testcase/usercode/nonpy.txt') as src, open('_generated/usercode/nonpy.txt') as dst:\n            assert src.read() == dst.read()\n\n    def test_annotation_detecting(self):\n        dir_ = 'testcase/usercode/non_annotation'\n        code_dir = expand_annotations(dir_, tempfile.mkdtemp())\n        self.assertEqual(code_dir, dir_)\n\n    def _assert_source_equal(self, src1, src2):\n        with open(src1) as f1, open(src2) as f2:\n            ast1 = ast.dump(ast.parse(f1.read()))\n            ast2 = ast.dump(ast.parse(f2.read()))\n        self.assertEqual(ast1, ast2)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/nni_cmd/__init__.py,0,b''
tools/nni_cmd/command_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom subprocess import call, check_output\nimport sys\nimport os\nimport signal\nimport psutil\nfrom .common_utils import print_error\n\n\ndef check_output_command(file_path, head=None, tail=None):\n    """"""call check_output command to read content from a file""""""\n    if os.path.exists(file_path):\n        if sys.platform == \'win32\':\n            cmds = [\'powershell.exe\', \'type\', file_path]\n            if head:\n                cmds += [\'|\', \'select\', \'-first\', str(head)]\n            elif tail:\n                cmds += [\'|\', \'select\', \'-last\', str(tail)]\n            return check_output(cmds, shell=True).decode(\'utf-8\')\n        else:\n            cmds = [\'cat\', file_path]\n            if head:\n                cmds = [\'head\', \'-\' + str(head), file_path]\n            elif tail:\n                cmds = [\'tail\', \'-\' + str(tail), file_path]\n            return check_output(cmds, shell=False).decode(\'utf-8\')\n    else:\n        print_error(\'{0} does not exist!\'.format(file_path))\n        exit(1)\n\n\ndef kill_command(pid):\n    """"""kill command""""""\n    if sys.platform == \'win32\':\n        process = psutil.Process(pid=pid)\n        process.send_signal(signal.CTRL_BREAK_EVENT)\n    else:\n        cmds = [\'kill\', str(pid)]\n        call(cmds)\n\n\ndef install_package_command(package_name):\n    """"""\n    Install python package from pip.\n\n    Parameters\n    ----------\n    package_name: str\n        The name of package to be installed.\n    """"""\n    call(_get_pip_install() + [package_name], shell=False)\n\n\ndef install_requirements_command(requirements_path):\n    """"""\n    Install packages from `requirements.txt` in `requirements_path`.\n\n    Parameters\n    ----------\n    requirements_path: str\n        Path to the directory that contains `requirements.txt`.\n    """"""\n    call(_get_pip_install() + [""-r"", os.path.join(requirements_path, ""requirements.txt"")], shell=False)\n\n\ndef _get_pip_install():\n    python = ""python"" if sys.platform == ""win32"" else ""python3""\n    ret = [python, ""-m"", ""pip"", ""install""]\n    if ""CONDA_DEFAULT_ENV"" not in os.environ and ""VIRTUAL_ENV"" not in os.environ and \\\n            (sys.platform != ""win32"" and os.getuid() != 0):  # on unix and not running in root\n        ret.append(""--user"")  # not in virtualenv or conda\n    return ret\n'"
tools/nni_cmd/common_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport site\nimport sys\nimport json\nimport socket\nfrom pathlib import Path\nimport ruamel.yaml as yaml\nimport psutil\nfrom .constants import ERROR_INFO, NORMAL_INFO, WARNING_INFO, COLOR_RED_FORMAT, COLOR_YELLOW_FORMAT\n\ndef get_yml_content(file_path):\n    \'\'\'Load yaml file content\'\'\'\n    try:\n        with open(file_path, \'r\') as file:\n            return yaml.load(file, Loader=yaml.Loader)\n    except yaml.scanner.ScannerError as err:\n        print_error(\'yaml file format error!\')\n        print_error(err)\n        exit(1)\n    except Exception as exception:\n        print_error(exception)\n        exit(1)\n\ndef get_json_content(file_path):\n    \'\'\'Load json file content\'\'\'\n    try:\n        with open(file_path, \'r\') as file:\n            return json.load(file)\n    except TypeError as err:\n        print_error(\'json file format error!\')\n        print_error(err)\n        return None\n\ndef print_error(content):\n    \'\'\'Print error information to screen\'\'\'\n    print(COLOR_RED_FORMAT % (ERROR_INFO % content))\n\ndef print_normal(content):\n    \'\'\'Print error information to screen\'\'\'\n    print(NORMAL_INFO % content)\n\ndef print_warning(content):\n    \'\'\'Print warning information to screen\'\'\'\n    print(COLOR_YELLOW_FORMAT % (WARNING_INFO % content))\n\ndef detect_process(pid):\n    \'\'\'Detect if a process is alive\'\'\'\n    try:\n        process = psutil.Process(pid)\n        return process.is_running()\n    except:\n        return False\n\ndef detect_port(port):\n    \'\'\'Detect if the port is used\'\'\'\n    socket_test = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        socket_test.connect((\'127.0.0.1\', int(port)))\n        socket_test.close()\n        return True\n    except:\n        return False\n\ndef get_user():\n    if sys.platform == \'win32\':\n        return os.environ[\'USERNAME\']\n    else:\n        return os.environ[\'USER\']\n\ndef get_python_dir(sitepackages_path):\n    if sys.platform == ""win32"":\n        return str(Path(sitepackages_path))\n    else:\n        return str(Path(sitepackages_path).parents[2])\n\ndef check_tensorboard_version():\n    try:\n        import tensorboard\n        return tensorboard.__version__\n    except:\n        print_error(\'import tensorboard error!\')\n        exit(1)\n\ndef get_nni_installation_path():\n    \'\'\' Find nni lib from the following locations in order\n    Return nni root directory if it exists\n    \'\'\'\n    def try_installation_path_sequentially(*sitepackages):\n        \'\'\'Try different installation path sequentially util nni is found.\n        Return None if nothing is found\n        \'\'\'\n        def _generate_installation_path(sitepackages_path):\n            python_dir = get_python_dir(sitepackages_path)\n            entry_file = os.path.join(python_dir, \'nni\', \'main.js\')\n            if os.path.isfile(entry_file):\n                return python_dir\n            return None\n\n        for sitepackage in sitepackages:\n            python_dir = _generate_installation_path(sitepackage)\n            if python_dir:\n                return python_dir\n        return None\n\n    if os.getenv(\'VIRTUAL_ENV\'):\n        # if \'virtualenv\' package is used, `site` has not attr getsitepackages, so we will instead use VIRTUAL_ENV\n        # Note that conda venv will not have VIRTUAL_ENV\n        python_dir = os.getenv(\'VIRTUAL_ENV\')\n    else:\n        python_sitepackage = site.getsitepackages()[0]\n        # If system-wide python is used, we will give priority to using `local sitepackage`--""usersitepackages()"" given\n        # that nni exists there\n        if python_sitepackage.startswith(\'/usr\') or python_sitepackage.startswith(\'/Library\'):\n            python_dir = try_installation_path_sequentially(site.getusersitepackages(), site.getsitepackages()[0])\n        else:\n            python_dir = try_installation_path_sequentially(site.getsitepackages()[0], site.getusersitepackages())\n\n    if python_dir:\n        entry_file = os.path.join(python_dir, \'nni\', \'main.js\')\n        if os.path.isfile(entry_file):\n            return os.path.join(python_dir, \'nni\')\n    print_error(\'Fail to find nni under python library\')\n    exit(1)'"
tools/nni_cmd/config_schema.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nfrom schema import Schema, And, Optional, Regex, Or\nfrom .constants import SCHEMA_TYPE_ERROR, SCHEMA_RANGE_ERROR, SCHEMA_PATH_ERROR\n\n\ndef setType(key, valueType):\n    \'\'\'check key type\'\'\'\n    return And(valueType, error=SCHEMA_TYPE_ERROR % (key, valueType.__name__))\n\ndef setChoice(key, *args):\n    \'\'\'check choice\'\'\'\n    return And(lambda n: n in args, error=SCHEMA_RANGE_ERROR % (key, str(args)))\n\ndef setNumberRange(key, keyType, start, end):\n    \'\'\'check number range\'\'\'\n    return And(\n        And(keyType, error=SCHEMA_TYPE_ERROR % (key, keyType.__name__)),\n        And(lambda n: start <= n <= end, error=SCHEMA_RANGE_ERROR % (key, \'(%s,%s)\' % (start, end))),\n    )\n\ndef setPathCheck(key):\n    \'\'\'check if path exist\'\'\'\n    return And(os.path.exists, error=SCHEMA_PATH_ERROR % key)\n\ncommon_schema = {\n    \'authorName\': setType(\'authorName\', str),\n    \'experimentName\': setType(\'experimentName\', str),\n    Optional(\'description\'): setType(\'description\', str),\n    \'trialConcurrency\': setNumberRange(\'trialConcurrency\', int, 1, 99999),\n    Optional(\'maxExecDuration\'): And(Regex(r\'^[1-9][0-9]*[s|m|h|d]$\', error=\'ERROR: maxExecDuration format is [digit]{s,m,h,d}\')),\n    Optional(\'maxTrialNum\'): setNumberRange(\'maxTrialNum\', int, 1, 99999),\n    \'trainingServicePlatform\': setChoice(\n        \'trainingServicePlatform\', \'remote\', \'local\', \'pai\', \'kubeflow\', \'frameworkcontroller\', \'paiYarn\', \'dlts\'),\n    Optional(\'searchSpacePath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'searchSpacePath\'),\n    Optional(\'multiPhase\'): setType(\'multiPhase\', bool),\n    Optional(\'multiThread\'): setType(\'multiThread\', bool),\n    Optional(\'nniManagerIp\'): setType(\'nniManagerIp\', str),\n    Optional(\'logDir\'): And(os.path.isdir, error=SCHEMA_PATH_ERROR % \'logDir\'),\n    Optional(\'debug\'): setType(\'debug\', bool),\n    Optional(\'versionCheck\'): setType(\'versionCheck\', bool),\n    Optional(\'logLevel\'): setChoice(\'logLevel\', \'trace\', \'debug\', \'info\', \'warning\', \'error\', \'fatal\'),\n    Optional(\'logCollection\'): setChoice(\'logCollection\', \'http\', \'none\'),\n    \'useAnnotation\': setType(\'useAnnotation\', bool),\n    Optional(\'tuner\'): dict,\n    Optional(\'advisor\'): dict,\n    Optional(\'assessor\'): dict,\n    Optional(\'localConfig\'): {\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n        Optional(\'maxTrialNumPerGpu\'): setType(\'maxTrialNumPerGpu\', int),\n        Optional(\'useActiveGpu\'): setType(\'useActiveGpu\', bool)\n    }\n}\ntuner_schema_dict = {\n    \'Anneal\': {\n        \'builtinTunerName\': \'Anneal\',\n        Optional(\'classArgs\'): {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'SMAC\': {\n        \'builtinTunerName\': \'SMAC\',\n        Optional(\'classArgs\'): {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'config_dedup\'): setType(\'config_dedup\', bool)\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    (\'Evolution\'): {\n        \'builtinTunerName\': setChoice(\'builtinTunerName\', \'Evolution\'),\n        Optional(\'classArgs\'): {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'population_size\'): setNumberRange(\'population_size\', int, 0, 99999),\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    (\'BatchTuner\', \'GridSearch\', \'Random\'): {\n        \'builtinTunerName\': setChoice(\'builtinTunerName\', \'BatchTuner\', \'GridSearch\', \'Random\'),\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'TPE\': {\n        \'builtinTunerName\': \'TPE\',\n        Optional(\'classArgs\'): {\n            Optional(\'optimize_mode\'): setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'parallel_optimize\'): setType(\'parallel_optimize\', bool),\n            Optional(\'constant_liar_type\'): setChoice(\'constant_liar_type\', \'min\', \'max\', \'mean\')\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'NetworkMorphism\': {\n        \'builtinTunerName\': \'NetworkMorphism\',\n        Optional(\'classArgs\'): {\n            Optional(\'optimize_mode\'): setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'task\'): setChoice(\'task\', \'cv\', \'nlp\', \'common\'),\n            Optional(\'input_width\'): setType(\'input_width\', int),\n            Optional(\'input_channel\'): setType(\'input_channel\', int),\n            Optional(\'n_output_node\'): setType(\'n_output_node\', int),\n            },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'MetisTuner\': {\n        \'builtinTunerName\': \'MetisTuner\',\n        Optional(\'classArgs\'): {\n            Optional(\'optimize_mode\'): setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'no_resampling\'): setType(\'no_resampling\', bool),\n            Optional(\'no_candidates\'): setType(\'no_candidates\', bool),\n            Optional(\'selection_num_starting_points\'):  setType(\'selection_num_starting_points\', int),\n            Optional(\'cold_start_num\'): setType(\'cold_start_num\', int),\n            },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'GPTuner\': {\n        \'builtinTunerName\': \'GPTuner\',\n        Optional(\'classArgs\'): {\n            Optional(\'optimize_mode\'): setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'utility\'): setChoice(\'utility\', \'ei\', \'ucb\', \'poi\'),\n            Optional(\'kappa\'): setType(\'kappa\', float),\n            Optional(\'xi\'): setType(\'xi\', float),\n            Optional(\'nu\'): setType(\'nu\', float),\n            Optional(\'alpha\'): setType(\'alpha\', float),\n            Optional(\'cold_start_num\'): setType(\'cold_start_num\', int),\n            Optional(\'selection_num_warm_up\'):  setType(\'selection_num_warm_up\', int),\n            Optional(\'selection_num_starting_points\'):  setType(\'selection_num_starting_points\', int),\n            },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'PPOTuner\': {\n        \'builtinTunerName\': \'PPOTuner\',\n        \'classArgs\': {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'trials_per_update\'): setNumberRange(\'trials_per_update\', int, 0, 99999),\n            Optional(\'epochs_per_update\'): setNumberRange(\'epochs_per_update\', int, 0, 99999),\n            Optional(\'minibatch_size\'): setNumberRange(\'minibatch_size\', int, 0, 99999),\n            Optional(\'ent_coef\'): setType(\'ent_coef\', float),\n            Optional(\'lr\'): setType(\'lr\', float),\n            Optional(\'vf_coef\'): setType(\'vf_coef\', float),\n            Optional(\'max_grad_norm\'): setType(\'max_grad_norm\', float),\n            Optional(\'gamma\'): setType(\'gamma\', float),\n            Optional(\'lam\'): setType(\'lam\', float),\n            Optional(\'cliprange\'): setType(\'cliprange\', float),\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'PBTTuner\': {\n        \'builtinTunerName\': \'PBTTuner\',\n        \'classArgs\': {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'all_checkpoint_dir\'): setType(\'all_checkpoint_dir\', str),\n            Optional(\'population_size\'): setNumberRange(\'population_size\', int, 0, 99999),\n            Optional(\'factors\'): setType(\'factors\', tuple),\n            Optional(\'fraction\'): setType(\'fraction\', float),\n        },\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'customized\': {\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'classFileName\': setType(\'classFileName\', str),\n        \'className\': setType(\'className\', str),\n        Optional(\'classArgs\'): dict,\n        Optional(\'includeIntermediateResults\'): setType(\'includeIntermediateResults\', bool),\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    }\n}\n\nadvisor_schema_dict = {\n    \'Hyperband\':{\n        \'builtinAdvisorName\': Or(\'Hyperband\'),\n        \'classArgs\': {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'R\'): setType(\'R\', int),\n            Optional(\'eta\'): setType(\'eta\', int)\n        },\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'BOHB\':{\n        \'builtinAdvisorName\': Or(\'BOHB\'),\n        \'classArgs\': {\n            \'optimize_mode\': setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'min_budget\'): setNumberRange(\'min_budget\', int, 0, 9999),\n            Optional(\'max_budget\'): setNumberRange(\'max_budget\', int, 0, 9999),\n            Optional(\'eta\'):setNumberRange(\'eta\', int, 0, 9999),\n            Optional(\'min_points_in_model\'): setNumberRange(\'min_points_in_model\', int, 0, 9999),\n            Optional(\'top_n_percent\'): setNumberRange(\'top_n_percent\', int, 1, 99),\n            Optional(\'num_samples\'): setNumberRange(\'num_samples\', int, 1, 9999),\n            Optional(\'random_fraction\'): setNumberRange(\'random_fraction\', float, 0, 9999),\n            Optional(\'bandwidth_factor\'): setNumberRange(\'bandwidth_factor\', float, 0, 9999),\n            Optional(\'min_bandwidth\'): setNumberRange(\'min_bandwidth\', float, 0, 9999),\n        },\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    },\n    \'customized\':{\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'classFileName\': setType(\'classFileName\', str),\n        \'className\': setType(\'className\', str),\n        Optional(\'classArgs\'): dict,\n        Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n    }\n}\n\nassessor_schema_dict = {\n    \'Medianstop\': {\n        \'builtinAssessorName\': \'Medianstop\',\n        Optional(\'classArgs\'): {\n            Optional(\'optimize_mode\'): setChoice(\'optimize_mode\', \'maximize\', \'minimize\'),\n            Optional(\'start_step\'): setNumberRange(\'start_step\', int, 0, 9999),\n        },\n    },\n    \'Curvefitting\': {\n        \'builtinAssessorName\': \'Curvefitting\',\n        Optional(\'classArgs\'): {\n            \'epoch_num\': setNumberRange(\'epoch_num\', int, 0, 9999),\n            Optional(\'start_step\'): setNumberRange(\'start_step\', int, 0, 9999),\n            Optional(\'threshold\'): setNumberRange(\'threshold\', float, 0, 9999),\n            Optional(\'gap\'): setNumberRange(\'gap\', int, 1, 9999),\n        },\n    },\n    \'customized\': {\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'classFileName\': setType(\'classFileName\', str),\n        \'className\': setType(\'className\', str),\n        Optional(\'classArgs\'): dict,\n    }\n}\n\ncommon_trial_schema = {\n    \'trial\':{\n        \'command\': setType(\'command\', str),\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        Optional(\'gpuNum\'): setNumberRange(\'gpuNum\', int, 0, 99999),\n        Optional(\'nasMode\'): setChoice(\'nasMode\', \'classic_mode\', \'enas_mode\', \'oneshot_mode\', \'darts_mode\')\n    }\n}\n\npai_yarn_trial_schema = {\n    \'trial\':{\n        \'command\': setType(\'command\', str),\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n        \'cpuNum\': setNumberRange(\'cpuNum\', int, 0, 99999),\n        \'memoryMB\': setType(\'memoryMB\', int),\n        \'image\': setType(\'image\', str),\n        Optional(\'authFile\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'authFile\'),\n        Optional(\'shmMB\'): setType(\'shmMB\', int),\n        Optional(\'dataDir\'): And(Regex(r\'hdfs://(([0-9]{1,3}.){3}[0-9]{1,3})(:[0-9]{2,5})?(/.*)?\'),\\\n                            error=\'ERROR: dataDir format error, dataDir format is hdfs://xxx.xxx.xxx.xxx:xxx\'),\n        Optional(\'outputDir\'): And(Regex(r\'hdfs://(([0-9]{1,3}.){3}[0-9]{1,3})(:[0-9]{2,5})?(/.*)?\'),\\\n                            error=\'ERROR: outputDir format error, outputDir format is hdfs://xxx.xxx.xxx.xxx:xxx\'),\n        Optional(\'virtualCluster\'): setType(\'virtualCluster\', str),\n        Optional(\'nasMode\'): setChoice(\'nasMode\', \'classic_mode\', \'enas_mode\', \'oneshot_mode\', \'darts_mode\'),\n        Optional(\'portList\'): [{\n            ""label"": setType(\'label\', str),\n            ""beginAt"": setType(\'beginAt\', int),\n            ""portNumber"": setType(\'portNumber\', int)\n        }]\n    }\n}\n\npai_yarn_config_schema = {\n    \'paiYarnConfig\': Or({\n        \'userName\': setType(\'userName\', str),\n        \'passWord\': setType(\'passWord\', str),\n        \'host\': setType(\'host\', str)\n    }, {\n        \'userName\': setType(\'userName\', str),\n        \'token\': setType(\'token\', str),\n        \'host\': setType(\'host\', str)\n    })\n}\n\n\npai_trial_schema = {\n    \'trial\':{\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'nniManagerNFSMountPath\': setPathCheck(\'nniManagerNFSMountPath\'),\n        \'containerNFSMountPath\': setType(\'containerNFSMountPath\', str),\n        Optional(\'command\'): setType(\'command\', str),\n        Optional(\'gpuNum\'): setNumberRange(\'gpuNum\', int, 0, 99999),\n        Optional(\'cpuNum\'): setNumberRange(\'cpuNum\', int, 0, 99999),\n        Optional(\'memoryMB\'): setType(\'memoryMB\', int),\n        Optional(\'image\'): setType(\'image\', str),\n        Optional(\'virtualCluster\'): setType(\'virtualCluster\', str),\n        Optional(\'paiStoragePlugin\'): setType(\'paiStoragePlugin\', str),\n        Optional(\'paiConfigPath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'paiConfigPath\')\n    }\n}\n\npai_config_schema = {\n    \'paiConfig\': Or({\n        \'userName\': setType(\'userName\', str),\n        \'passWord\': setType(\'passWord\', str),\n        \'host\': setType(\'host\', str)\n    }, {\n        \'userName\': setType(\'userName\', str),\n        \'token\': setType(\'token\', str),\n        \'host\': setType(\'host\', str)\n    })\n}\n\ndlts_trial_schema = {\n    \'trial\':{\n        \'command\': setType(\'command\', str),\n        \'codeDir\': setPathCheck(\'codeDir\'),\n        \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n        \'image\': setType(\'image\', str),\n    }\n}\n\ndlts_config_schema = {\n    \'dltsConfig\': {\n        \'dashboard\': setType(\'dashboard\', str),\n\n        Optional(\'cluster\'): setType(\'cluster\', str),\n        Optional(\'team\'): setType(\'team\', str),\n\n        Optional(\'email\'): setType(\'email\', str),\n        Optional(\'password\'): setType(\'password\', str),\n    }\n}\n\nkubeflow_trial_schema = {\n    \'trial\':{\n        \'codeDir\':  setPathCheck(\'codeDir\'),\n        Optional(\'nasMode\'): setChoice(\'nasMode\', \'classic_mode\', \'enas_mode\', \'oneshot_mode\', \'darts_mode\'),\n        Optional(\'ps\'): {\n            \'replicas\': setType(\'replicas\', int),\n            \'command\': setType(\'command\', str),\n            \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n            \'cpuNum\': setNumberRange(\'cpuNum\', int, 0, 99999),\n            \'memoryMB\': setType(\'memoryMB\', int),\n            \'image\': setType(\'image\', str),\n            Optional(\'privateRegistryAuthPath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'privateRegistryAuthPath\')\n        },\n        Optional(\'master\'): {\n            \'replicas\': setType(\'replicas\', int),\n            \'command\': setType(\'command\', str),\n            \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n            \'cpuNum\': setNumberRange(\'cpuNum\', int, 0, 99999),\n            \'memoryMB\': setType(\'memoryMB\', int),\n            \'image\': setType(\'image\', str),\n            Optional(\'privateRegistryAuthPath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'privateRegistryAuthPath\')\n        },\n        Optional(\'worker\'):{\n            \'replicas\': setType(\'replicas\', int),\n            \'command\': setType(\'command\', str),\n            \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n            \'cpuNum\': setNumberRange(\'cpuNum\', int, 0, 99999),\n            \'memoryMB\': setType(\'memoryMB\', int),\n            \'image\': setType(\'image\', str),\n            Optional(\'privateRegistryAuthPath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'privateRegistryAuthPath\')\n        }\n    }\n}\n\nkubeflow_config_schema = {\n    \'kubeflowConfig\':Or({\n        \'operator\': setChoice(\'operator\', \'tf-operator\', \'pytorch-operator\'),\n        \'apiVersion\': setType(\'apiVersion\', str),\n        Optional(\'storage\'): setChoice(\'storage\', \'nfs\', \'azureStorage\'),\n        \'nfs\': {\n            \'server\': setType(\'server\', str),\n            \'path\': setType(\'path\', str)\n        }\n    }, {\n        \'operator\': setChoice(\'operator\', \'tf-operator\', \'pytorch-operator\'),\n        \'apiVersion\': setType(\'apiVersion\', str),\n        Optional(\'storage\'): setChoice(\'storage\', \'nfs\', \'azureStorage\'),\n        \'keyVault\': {\n            \'vaultName\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){1,127}\'),\\\n                         error=\'ERROR: vaultName format error, vaultName support using (0-9|a-z|A-Z|-)\'),\n            \'name\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){1,127}\'),\\\n                    error=\'ERROR: name format error, name support using (0-9|a-z|A-Z|-)\')\n        },\n        \'azureStorage\': {\n            \'accountName\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){3,31}\'),\\\n                           error=\'ERROR: accountName format error, accountName support using (0-9|a-z|A-Z|-)\'),\n            \'azureShare\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){3,63}\'),\\\n                          error=\'ERROR: azureShare format error, azureShare support using (0-9|a-z|A-Z|-)\')\n        },\n        Optional(\'uploadRetryCount\'): setNumberRange(\'uploadRetryCount\', int, 1, 99999)\n    })\n}\n\nframeworkcontroller_trial_schema = {\n    \'trial\':{\n        \'codeDir\':  setPathCheck(\'codeDir\'),\n        \'taskRoles\': [{\n            \'name\': setType(\'name\', str),\n            \'taskNum\': setType(\'taskNum\', int),\n            \'frameworkAttemptCompletionPolicy\': {\n                \'minFailedTaskCount\': setType(\'minFailedTaskCount\', int),\n                \'minSucceededTaskCount\': setType(\'minSucceededTaskCount\', int),\n            },\n            \'command\': setType(\'command\', str),\n            \'gpuNum\': setNumberRange(\'gpuNum\', int, 0, 99999),\n            \'cpuNum\': setNumberRange(\'cpuNum\', int, 0, 99999),\n            \'memoryMB\': setType(\'memoryMB\', int),\n            \'image\': setType(\'image\', str),\n            Optional(\'privateRegistryAuthPath\'): And(os.path.exists, error=SCHEMA_PATH_ERROR % \'privateRegistryAuthPath\')\n        }]\n    }\n}\n\nframeworkcontroller_config_schema = {\n    \'frameworkcontrollerConfig\':Or({\n        Optional(\'storage\'): setChoice(\'storage\', \'nfs\', \'azureStorage\'),\n        Optional(\'serviceAccountName\'): setType(\'serviceAccountName\', str),\n        \'nfs\': {\n            \'server\': setType(\'server\', str),\n            \'path\': setType(\'path\', str)\n        }\n    }, {\n        Optional(\'storage\'): setChoice(\'storage\', \'nfs\', \'azureStorage\'),\n        Optional(\'serviceAccountName\'): setType(\'serviceAccountName\', str),\n        \'keyVault\': {\n            \'vaultName\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){1,127}\'),\\\n                         error=\'ERROR: vaultName format error, vaultName support using (0-9|a-z|A-Z|-)\'),\n            \'name\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){1,127}\'),\\\n                    error=\'ERROR: name format error, name support using (0-9|a-z|A-Z|-)\')\n        },\n        \'azureStorage\': {\n            \'accountName\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){3,31}\'),\\\n                           error=\'ERROR: accountName format error, accountName support using (0-9|a-z|A-Z|-)\'),\n            \'azureShare\': And(Regex(\'([0-9]|[a-z]|[A-Z]|-){3,63}\'),\\\n                          error=\'ERROR: azureShare format error, azureShare support using (0-9|a-z|A-Z|-)\')\n        },\n        Optional(\'uploadRetryCount\'): setNumberRange(\'uploadRetryCount\', int, 1, 99999)\n    })\n}\n\nmachine_list_schema = {\n    Optional(\'machineList\'):[Or(\n        {\n            \'ip\': setType(\'ip\', str),\n            Optional(\'port\'): setNumberRange(\'port\', int, 1, 65535),\n            \'username\': setType(\'username\', str),\n            \'sshKeyPath\': setPathCheck(\'sshKeyPath\'),\n            Optional(\'passphrase\'): setType(\'passphrase\', str),\n            Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n            Optional(\'maxTrialNumPerGpu\'): setType(\'maxTrialNumPerGpu\', int),\n            Optional(\'useActiveGpu\'): setType(\'useActiveGpu\', bool)\n        },\n        {\n            \'ip\': setType(\'ip\', str),\n            Optional(\'port\'): setNumberRange(\'port\', int, 1, 65535),\n            \'username\': setType(\'username\', str),\n            \'passwd\': setType(\'passwd\', str),\n            Optional(\'gpuIndices\'): Or(int, And(str, lambda x: len([int(i) for i in x.split(\',\')]) > 0), error=\'gpuIndex format error!\'),\n            Optional(\'maxTrialNumPerGpu\'): setType(\'maxTrialNumPerGpu\', int),\n            Optional(\'useActiveGpu\'): setType(\'useActiveGpu\', bool)\n        })]\n}\n\nLOCAL_CONFIG_SCHEMA = Schema({**common_schema, **common_trial_schema})\n\nREMOTE_CONFIG_SCHEMA = Schema({**common_schema, **common_trial_schema, **machine_list_schema})\n\nPAI_CONFIG_SCHEMA = Schema({**common_schema, **pai_trial_schema, **pai_config_schema})\n\nPAI_YARN_CONFIG_SCHEMA = Schema({**common_schema, **pai_yarn_trial_schema, **pai_yarn_config_schema})\n\nDLTS_CONFIG_SCHEMA = Schema({**common_schema, **dlts_trial_schema, **dlts_config_schema})\n\nKUBEFLOW_CONFIG_SCHEMA = Schema({**common_schema, **kubeflow_trial_schema, **kubeflow_config_schema})\n\nFRAMEWORKCONTROLLER_CONFIG_SCHEMA = Schema({**common_schema, **frameworkcontroller_trial_schema, **frameworkcontroller_config_schema})\n'"
tools/nni_cmd/config_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport json\nfrom .constants import NNICTL_HOME_DIR\n\nclass Config:\n    '''a util class to load and save config'''\n    def __init__(self, file_path):\n        config_path = os.path.join(NNICTL_HOME_DIR, str(file_path))\n        os.makedirs(config_path, exist_ok=True)\n        self.config_file = os.path.join(config_path, '.config')\n        self.config = self.read_file()\n\n    def get_all_config(self):\n        '''get all of config values'''\n        return json.dumps(self.config, indent=4, sort_keys=True, separators=(',', ':'))\n\n    def set_config(self, key, value):\n        '''set {key:value} paris to self.config'''\n        self.config = self.read_file()\n        self.config[key] = value\n        self.write_file()\n\n    def get_config(self, key):\n        '''get a value according to key'''\n        return self.config.get(key)\n\n    def write_file(self):\n        '''save config to local file'''\n        if self.config:\n            try:\n                with open(self.config_file, 'w') as file:\n                    json.dump(self.config, file)\n            except IOError as error:\n                print('Error:', error)\n                return\n\n    def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.config_file):\n            try:\n                with open(self.config_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}\n\nclass Experiments:\n    '''Maintain experiment list'''\n    def __init__(self):\n        os.makedirs(NNICTL_HOME_DIR, exist_ok=True)\n        self.experiment_file = os.path.join(NNICTL_HOME_DIR, '.experiment')\n        self.experiments = self.read_file()\n\n    def add_experiment(self, expId, port, time, file_name, platform, experiment_name):\n        '''set {key:value} paris to self.experiment'''\n        self.experiments[expId] = {}\n        self.experiments[expId]['port'] = port\n        self.experiments[expId]['startTime'] = time\n        self.experiments[expId]['endTime'] = 'N/A'\n        self.experiments[expId]['status'] = 'INITIALIZED'\n        self.experiments[expId]['fileName'] = file_name\n        self.experiments[expId]['platform'] = platform\n        self.experiments[expId]['experimentName'] = experiment_name\n        self.write_file()\n\n    def update_experiment(self, expId, key, value):\n        '''Update experiment'''\n        if expId not in self.experiments:\n            return False\n        self.experiments[expId][key] = value\n        self.write_file()\n        return True\n\n    def remove_experiment(self, expId):\n        '''remove an experiment by id'''\n        if expId in self.experiments:\n            self.experiments.pop(expId)\n        self.write_file()\n\n    def get_all_experiments(self):\n        '''return all of experiments'''\n        return self.experiments\n\n    def write_file(self):\n        '''save config to local file'''\n        try:\n            with open(self.experiment_file, 'w') as file:\n                json.dump(self.experiments, file)\n        except IOError as error:\n            print('Error:', error)\n            return ''\n\n    def read_file(self):\n        '''load config from local file'''\n        if os.path.exists(self.experiment_file):\n            try:\n                with open(self.experiment_file, 'r') as file:\n                    return json.load(file)\n            except ValueError:\n                return {}\n        return {}\n"""
tools/nni_cmd/constants.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nfrom colorama import Fore\n\nNNICTL_HOME_DIR = os.path.join(os.path.expanduser('~'), '.local', 'nnictl')\n\nERROR_INFO = 'ERROR: %s'\n\nNORMAL_INFO = 'INFO: %s'\n\nWARNING_INFO = 'WARNING: %s'\n\nDEFAULT_REST_PORT = 8080\n\nREST_TIME_OUT = 20\n\nEXPERIMENT_SUCCESS_INFO = Fore.GREEN + 'Successfully started experiment!\\n' + Fore.RESET + \\\n                          '------------------------------------------------------------------------------------\\n' \\\n                          'The experiment id is %s\\n'\\\n                          'The Web UI urls are: %s\\n' \\\n                          '------------------------------------------------------------------------------------\\n\\n' \\\n                          'You can use these commands to get more information about the experiment\\n' \\\n                          '------------------------------------------------------------------------------------\\n' \\\n                          '         commands                       description\\n' \\\n                          '1. nnictl experiment show        show the information of experiments\\n' \\\n                          '2. nnictl trial ls               list all of trial jobs\\n' \\\n                          '3. nnictl top                    monitor the status of running experiments\\n' \\\n                          '4. nnictl log stderr             show stderr log content\\n' \\\n                          '5. nnictl log stdout             show stdout log content\\n' \\\n                          '6. nnictl stop                   stop an experiment\\n' \\\n                          '7. nnictl trial kill             kill a trial job by id\\n' \\\n                          '8. nnictl --help                 get help information about nnictl\\n' \\\n                          '------------------------------------------------------------------------------------\\n' \\\n                          'Command reference document https://nni.readthedocs.io/en/latest/Tutorial/Nnictl.html\\n' \\\n                          '------------------------------------------------------------------------------------\\n'\n\nLOG_HEADER = '-----------------------------------------------------------------------\\n' \\\n             '                Experiment start time %s\\n' \\\n             '-----------------------------------------------------------------------\\n'\n\nEXPERIMENT_START_FAILED_INFO = 'There is an experiment running in the port %d, please stop it first or set another port!\\n' \\\n                               'You could use \\'nnictl stop --port [PORT]\\' command to stop an experiment!\\nOr you could ' \\\n                               'use \\'nnictl create --config [CONFIG_PATH] --port [PORT]\\' to set port!\\n'\n\nEXPERIMENT_INFORMATION_FORMAT = '----------------------------------------------------------------------------------------\\n' \\\n                     '                Experiment information\\n' \\\n                     '%s\\n' \\\n                     '----------------------------------------------------------------------------------------\\n'\n\nEXPERIMENT_DETAIL_FORMAT = 'Id: %s    Name: %s    Status: %s    Port: %s    Platform: %s    StartTime: %s    EndTime: %s\\n'\n\nEXPERIMENT_MONITOR_INFO = 'Id: %s    Status: %s    Port: %s    Platform: %s    \\n' \\\n                          'StartTime: %s    Duration: %s'\n\nTRIAL_MONITOR_HEAD = '-------------------------------------------------------------------------------------\\n' + \\\n                    '%-15s %-25s %-25s %-15s \\n' % ('trialId', 'startTime', 'endTime', 'status') + \\\n                     '-------------------------------------------------------------------------------------'\n\nTRIAL_MONITOR_CONTENT = '%-15s %-25s %-25s %-15s'\n\nTRIAL_MONITOR_TAIL = '-------------------------------------------------------------------------------------\\n\\n\\n'\n\nPACKAGE_REQUIREMENTS = {\n    'SMAC': 'smac_tuner',\n    'BOHB': 'bohb_advisor',\n    'PPOTuner': 'ppo_tuner'\n}\n\nTUNERS_SUPPORTING_IMPORT_DATA = {\n    'TPE',\n    'Anneal',\n    'GridSearch',\n    'MetisTuner',\n    'BOHB',\n    'SMAC',\n    'BatchTuner'\n}\n\nTUNERS_NO_NEED_TO_IMPORT_DATA = {\n    'Random',\n    'Hyperband'\n}\n\nCOLOR_RED_FORMAT = Fore.RED + '%s'\n\nCOLOR_GREEN_FORMAT = Fore.GREEN + '%s'\n\nCOLOR_YELLOW_FORMAT = Fore.YELLOW + '%s'\n\nSCHEMA_TYPE_ERROR = '%s should be %s type!'\n\nSCHEMA_RANGE_ERROR = '%s should be in range of %s!'\n\nSCHEMA_PATH_ERROR = '%s path not exist!'\n"""
tools/nni_cmd/launcher.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport os\nimport sys\nimport string\nimport random\nimport time\nimport tempfile\nfrom subprocess import Popen, check_call, CalledProcessError, PIPE, STDOUT\nfrom nni_annotation import expand_annotations, generate_search_space\nfrom nni.constants import ModuleName, AdvisorModuleName\nfrom .launcher_utils import validate_all_content\nfrom .rest_utils import rest_put, rest_post, check_rest_server, check_response\nfrom .url_utils import cluster_metadata_url, experiment_url, get_local_urls\nfrom .config_utils import Config, Experiments\nfrom .common_utils import get_yml_content, get_json_content, print_error, print_normal, \\\n                          detect_port, get_user, get_nni_installation_path\nfrom .constants import NNICTL_HOME_DIR, ERROR_INFO, REST_TIME_OUT, EXPERIMENT_SUCCESS_INFO, LOG_HEADER, PACKAGE_REQUIREMENTS\nfrom .command_utils import check_output_command, kill_command\nfrom .nnictl_utils import update_experiment\n\ndef get_log_path(config_file_name):\n    '''generate stdout and stderr log path'''\n    stdout_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stdout')\n    stderr_full_path = os.path.join(NNICTL_HOME_DIR, config_file_name, 'stderr')\n    return stdout_full_path, stderr_full_path\n\ndef print_log_content(config_file_name):\n    '''print log information'''\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    print_normal(' Stdout:')\n    print(check_output_command(stdout_full_path))\n    print('\\n\\n')\n    print_normal(' Stderr:')\n    print(check_output_command(stderr_full_path))\n\ndef start_rest_server(port, platform, mode, config_file_name, foreground=False, experiment_id=None, log_dir=None, log_level=None):\n    '''Run nni manager process'''\n    if detect_port(port):\n        print_error('Port %s is used by another process, please reset the port!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % port)\n        exit(1)\n\n    if (platform != 'local') and detect_port(int(port) + 1):\n        print_error('PAI mode need an additional adjacent port %d, and the port %d is used by another process!\\n' \\\n        'You could set another port to start experiment!\\n' \\\n        'You could use \\'nnictl create --help\\' to get help information' % ((int(port) + 1), (int(port) + 1)))\n        exit(1)\n\n    print_normal('Starting restful server...')\n\n    entry_dir = get_nni_installation_path()\n    entry_file = os.path.join(entry_dir, 'main.js')\n\n    node_command = 'node'\n    if sys.platform == 'win32':\n        node_command = os.path.join(entry_dir[:-3], 'Scripts', 'node.exe')\n    cmds = [node_command, '--max-old-space-size=4096', entry_file, '--port', str(port), '--mode', platform]\n    if mode == 'view':\n        cmds += ['--start_mode', 'resume']\n        cmds += ['--readonly', 'true']\n    else:\n        cmds += ['--start_mode', mode]\n    if log_dir is not None:\n        cmds += ['--log_dir', log_dir]\n    if log_level is not None:\n        cmds += ['--log_level', log_level]\n    if mode in ['resume', 'view']:\n        cmds += ['--experiment_id', experiment_id]\n    if foreground:\n        cmds += ['--foreground', 'true']\n    stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n    with open(stdout_full_path, 'a+') as stdout_file, open(stderr_full_path, 'a+') as stderr_file:\n        time_now = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(time.time()))\n        #add time information in the header of log files\n        log_header = LOG_HEADER % str(time_now)\n        stdout_file.write(log_header)\n        stderr_file.write(log_header)\n        if sys.platform == 'win32':\n            from subprocess import CREATE_NEW_PROCESS_GROUP\n            if foreground:\n                process = Popen(cmds, cwd=entry_dir, stdout=PIPE, stderr=STDOUT, creationflags=CREATE_NEW_PROCESS_GROUP)\n            else:\n                process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file, creationflags=CREATE_NEW_PROCESS_GROUP)\n        else:\n            if foreground:\n                process = Popen(cmds, cwd=entry_dir, stdout=PIPE, stderr=PIPE)\n            else:\n                process = Popen(cmds, cwd=entry_dir, stdout=stdout_file, stderr=stderr_file)\n    return process, str(time_now)\n\ndef set_trial_config(experiment_config, port, config_file_name):\n    '''set trial configuration'''\n    request_data = dict()\n    request_data['trial_config'] = experiment_config['trial']\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n    if check_response(response):\n        return True\n    else:\n        print('Error message is {}'.format(response.text))\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n        return False\n\ndef set_local_config(experiment_config, port, config_file_name):\n    '''set local configuration'''\n    request_data = dict()\n    if experiment_config.get('localConfig'):\n        request_data['local_config'] = experiment_config['localConfig']\n        if request_data['local_config']:\n            if request_data['local_config'].get('gpuIndices') and isinstance(request_data['local_config'].get('gpuIndices'), int):\n                request_data['local_config']['gpuIndices'] = str(request_data['local_config'].get('gpuIndices'))\n            if request_data['local_config'].get('maxTrialNumOnEachGpu'):\n                request_data['local_config']['maxTrialNumOnEachGpu'] = request_data['local_config'].get('maxTrialNumOnEachGpu')\n            if request_data['local_config'].get('useActiveGpu'):\n                request_data['local_config']['useActiveGpu'] = request_data['local_config'].get('useActiveGpu')\n        response = rest_put(cluster_metadata_url(port), json.dumps(request_data), REST_TIME_OUT)\n        err_message = ''\n        if not response or not check_response(response):\n            if response is not None:\n                err_message = response.text\n                _, stderr_full_path = get_log_path(config_file_name)\n                with open(stderr_full_path, 'a+') as fout:\n                    fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n            return False, err_message\n\n    return set_trial_config(experiment_config, port, config_file_name), None\n\ndef set_remote_config(experiment_config, port, config_file_name):\n    '''Call setClusterMetadata to pass trial'''\n    #set machine_list\n    request_data = dict()\n    request_data['machine_list'] = experiment_config['machineList']\n    if request_data['machine_list']:\n        for i in range(len(request_data['machine_list'])):\n            if isinstance(request_data['machine_list'][i].get('gpuIndices'), int):\n                request_data['machine_list'][i]['gpuIndices'] = str(request_data['machine_list'][i].get('gpuIndices'))\n    # It needs to connect all remote machines, the time out of connection is 30 seconds.\n    # So timeout of this place should be longer.\n    response = rest_put(cluster_metadata_url(port), json.dumps(request_data), 60, True)\n    err_message = ''\n    if not response or not check_response(response):\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef setNNIManagerIp(experiment_config, port, config_file_name):\n    '''set nniManagerIp'''\n    if experiment_config.get('nniManagerIp') is None:\n        return True, None\n    ip_config_dict = dict()\n    ip_config_dict['nni_manager_ip'] = {'nniManagerIp': experiment_config['nniManagerIp']}\n    response = rest_put(cluster_metadata_url(port), json.dumps(ip_config_dict), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    return True, None\n\ndef set_pai_config(experiment_config, port, config_file_name):\n    '''set pai configuration'''\n    pai_config_data = dict()\n    pai_config_data['pai_config'] = experiment_config['paiConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(pai_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef set_pai_yarn_config(experiment_config, port, config_file_name):\n    '''set paiYarn configuration'''\n    pai_yarn_config_data = dict()\n    pai_yarn_config_data['pai_yarn_config'] = experiment_config['paiYarnConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(pai_yarn_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef set_kubeflow_config(experiment_config, port, config_file_name):\n    '''set kubeflow configuration'''\n    kubeflow_config_data = dict()\n    kubeflow_config_data['kubeflow_config'] = experiment_config['kubeflowConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(kubeflow_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef set_frameworkcontroller_config(experiment_config, port, config_file_name):\n    '''set kubeflow configuration'''\n    frameworkcontroller_config_data = dict()\n    frameworkcontroller_config_data['frameworkcontroller_config'] = experiment_config['frameworkcontrollerConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(frameworkcontroller_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef set_dlts_config(experiment_config, port, config_file_name):\n    '''set dlts configuration'''\n    dlts_config_data = dict()\n    dlts_config_data['dlts_config'] = experiment_config['dltsConfig']\n    response = rest_put(cluster_metadata_url(port), json.dumps(dlts_config_data), REST_TIME_OUT)\n    err_message = None\n    if not response or not response.status_code == 200:\n        if response is not None:\n            err_message = response.text\n            _, stderr_full_path = get_log_path(config_file_name)\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(err_message), indent=4, sort_keys=True, separators=(',', ':')))\n        return False, err_message\n    result, message = setNNIManagerIp(experiment_config, port, config_file_name)\n    if not result:\n        return result, message\n    #set trial_config\n    return set_trial_config(experiment_config, port, config_file_name), err_message\n\ndef set_experiment(experiment_config, mode, port, config_file_name):\n    '''Call startExperiment (rest POST /experiment) with yaml file content'''\n    request_data = dict()\n    request_data['authorName'] = experiment_config['authorName']\n    request_data['experimentName'] = experiment_config['experimentName']\n    request_data['trialConcurrency'] = experiment_config['trialConcurrency']\n    request_data['maxExecDuration'] = experiment_config['maxExecDuration']\n    request_data['maxTrialNum'] = experiment_config['maxTrialNum']\n    request_data['searchSpace'] = experiment_config.get('searchSpace')\n    request_data['trainingServicePlatform'] = experiment_config.get('trainingServicePlatform')\n\n    if experiment_config.get('description'):\n        request_data['description'] = experiment_config['description']\n    if experiment_config.get('multiPhase'):\n        request_data['multiPhase'] = experiment_config.get('multiPhase')\n    if experiment_config.get('multiThread'):\n        request_data['multiThread'] = experiment_config.get('multiThread')\n    if experiment_config.get('advisor'):\n        request_data['advisor'] = experiment_config['advisor']\n        if request_data['advisor'].get('gpuNum'):\n            print_error('gpuNum is deprecated, please use gpuIndices instead.')\n        if request_data['advisor'].get('gpuIndices') and isinstance(request_data['advisor'].get('gpuIndices'), int):\n            request_data['advisor']['gpuIndices'] = str(request_data['advisor'].get('gpuIndices'))\n    else:\n        request_data['tuner'] = experiment_config['tuner']\n        if request_data['tuner'].get('gpuNum'):\n            print_error('gpuNum is deprecated, please use gpuIndices instead.')\n        if request_data['tuner'].get('gpuIndices') and isinstance(request_data['tuner'].get('gpuIndices'), int):\n            request_data['tuner']['gpuIndices'] = str(request_data['tuner'].get('gpuIndices'))\n        if 'assessor' in experiment_config:\n            request_data['assessor'] = experiment_config['assessor']\n            if request_data['assessor'].get('gpuNum'):\n                print_error('gpuNum is deprecated, please remove it from your config file.')\n    #debug mode should disable version check\n    if experiment_config.get('debug') is not None:\n        request_data['versionCheck'] = not experiment_config.get('debug')\n    #validate version check\n    if experiment_config.get('versionCheck') is not None:\n        request_data['versionCheck'] = experiment_config.get('versionCheck')\n    if experiment_config.get('logCollection'):\n        request_data['logCollection'] = experiment_config.get('logCollection')\n\n    request_data['clusterMetaData'] = []\n    if experiment_config['trainingServicePlatform'] == 'local':\n        request_data['clusterMetaData'].append(\n            {'key':'codeDir', 'value':experiment_config['trial']['codeDir']})\n        request_data['clusterMetaData'].append(\n            {'key': 'command', 'value': experiment_config['trial']['command']})\n    elif experiment_config['trainingServicePlatform'] == 'remote':\n        request_data['clusterMetaData'].append(\n            {'key': 'machine_list', 'value': experiment_config['machineList']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'pai':\n        request_data['clusterMetaData'].append(\n            {'key': 'pai_config', 'value': experiment_config['paiConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'paiYarn':\n        request_data['clusterMetaData'].append(\n            {'key': 'pai_yarn_config', 'value': experiment_config['paiYarnConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'kubeflow':\n        request_data['clusterMetaData'].append(\n            {'key': 'kubeflow_config', 'value': experiment_config['kubeflowConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    elif experiment_config['trainingServicePlatform'] == 'frameworkcontroller':\n        request_data['clusterMetaData'].append(\n            {'key': 'frameworkcontroller_config', 'value': experiment_config['frameworkcontrollerConfig']})\n        request_data['clusterMetaData'].append(\n            {'key': 'trial_config', 'value': experiment_config['trial']})\n    response = rest_post(experiment_url(port), json.dumps(request_data), REST_TIME_OUT, show_error=True)\n    if check_response(response):\n        return response\n    else:\n        _, stderr_full_path = get_log_path(config_file_name)\n        if response is not None:\n            with open(stderr_full_path, 'a+') as fout:\n                fout.write(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(',', ':')))\n            print_error('Setting experiment error, error message is {}'.format(response.text))\n        return None\n\ndef set_platform_config(platform, experiment_config, port, config_file_name, rest_process):\n    '''call set_cluster_metadata for specific platform'''\n    print_normal('Setting {0} config...'.format(platform))\n    config_result, err_msg = None, None\n    if platform == 'local':\n        config_result, err_msg = set_local_config(experiment_config, port, config_file_name)\n    elif platform == 'remote':\n        config_result, err_msg = set_remote_config(experiment_config, port, config_file_name)\n    elif platform == 'pai':\n        config_result, err_msg = set_pai_config(experiment_config, port, config_file_name)\n    elif platform == 'paiYarn':\n        config_result, err_msg = set_pai_yarn_config(experiment_config, port, config_file_name)\n    elif platform == 'kubeflow':\n        config_result, err_msg = set_kubeflow_config(experiment_config, port, config_file_name)\n    elif platform == 'frameworkcontroller':\n        config_result, err_msg = set_frameworkcontroller_config(experiment_config, port, config_file_name)\n    elif platform == 'dlts':\n        config_result, err_msg = set_dlts_config(experiment_config, port, config_file_name)\n    else:\n        raise Exception(ERROR_INFO % 'Unsupported platform!')\n        exit(1)\n    if config_result:\n        print_normal('Successfully set {0} config!'.format(platform))\n    else:\n        print_error('Failed! Error is: {}'.format(err_msg))\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Rest server stopped!')\n        exit(1)\n\ndef launch_experiment(args, experiment_config, mode, config_file_name, experiment_id=None):\n    '''follow steps to start rest server and start experiment'''\n    nni_config = Config(config_file_name)\n    # check packages for tuner\n    package_name, module_name = None, None\n    if experiment_config.get('tuner') and experiment_config['tuner'].get('builtinTunerName'):\n        package_name = experiment_config['tuner']['builtinTunerName']\n        module_name = ModuleName.get(package_name)\n    elif experiment_config.get('advisor') and experiment_config['advisor'].get('builtinAdvisorName'):\n        package_name = experiment_config['advisor']['builtinAdvisorName']\n        module_name = AdvisorModuleName.get(package_name)\n    if package_name and module_name:\n        try:\n            stdout_full_path, stderr_full_path = get_log_path(config_file_name)\n            with open(stdout_full_path, 'a+') as stdout_file, open(stderr_full_path, 'a+') as stderr_file:\n                check_call([sys.executable, '-c', 'import %s'%(module_name)], stdout=stdout_file, stderr=stderr_file)\n        except CalledProcessError:\n            print_error('some errors happen when import package %s.' %(package_name))\n            print_log_content(config_file_name)\n            if package_name in PACKAGE_REQUIREMENTS:\n                print_error('If %s is not installed, it should be installed through '\\\n                            '\\'nnictl package install --name %s\\''%(package_name, package_name))\n            exit(1)\n    log_dir = experiment_config['logDir'] if experiment_config.get('logDir') else None\n    log_level = experiment_config['logLevel'] if experiment_config.get('logLevel') else None\n    #view experiment mode do not need debug function, when view an experiment, there will be no new logs created\n    foreground = False\n    if mode != 'view':\n        foreground = args.foreground\n        if log_level not in ['trace', 'debug'] and (args.debug or experiment_config.get('debug') is True):\n            log_level = 'debug'\n    # start rest server\n    rest_process, start_time = start_rest_server(args.port, experiment_config['trainingServicePlatform'], \\\n                                                 mode, config_file_name, foreground, experiment_id, log_dir, log_level)\n    nni_config.set_config('restServerPid', rest_process.pid)\n    # Deal with annotation\n    if experiment_config.get('useAnnotation'):\n        path = os.path.join(tempfile.gettempdir(), get_user(), 'nni', 'annotation')\n        if not os.path.isdir(path):\n            os.makedirs(path)\n        path = tempfile.mkdtemp(dir=path)\n        nas_mode = experiment_config['trial'].get('nasMode', 'classic_mode')\n        code_dir = expand_annotations(experiment_config['trial']['codeDir'], path, nas_mode=nas_mode)\n        experiment_config['trial']['codeDir'] = code_dir\n        search_space = generate_search_space(code_dir)\n        experiment_config['searchSpace'] = json.dumps(search_space)\n        assert search_space, ERROR_INFO % 'Generated search space is empty'\n    elif experiment_config.get('searchSpacePath'):\n        search_space = get_json_content(experiment_config.get('searchSpacePath'))\n        experiment_config['searchSpace'] = json.dumps(search_space)\n    else:\n        experiment_config['searchSpace'] = json.dumps('')\n\n    # check rest server\n    running, _ = check_rest_server(args.port)\n    if running:\n        print_normal('Successfully started Restful server!')\n    else:\n        print_error('Restful server start failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Rest server stopped!')\n        exit(1)\n    if mode != 'view':\n        # set platform configuration\n        set_platform_config(experiment_config['trainingServicePlatform'], experiment_config, args.port,\\\n                            config_file_name, rest_process)\n\n    # start a new experiment\n    print_normal('Starting experiment...')\n    # set debug configuration\n    if mode != 'view' and experiment_config.get('debug') is None:\n        experiment_config['debug'] = args.debug\n    response = set_experiment(experiment_config, mode, args.port, config_file_name)\n    if response:\n        if experiment_id is None:\n            experiment_id = json.loads(response.text).get('experiment_id')\n        nni_config.set_config('experimentId', experiment_id)\n    else:\n        print_error('Start experiment failed!')\n        print_log_content(config_file_name)\n        try:\n            kill_command(rest_process.pid)\n        except Exception:\n            raise Exception(ERROR_INFO % 'Restful server stopped!')\n        exit(1)\n    if experiment_config.get('nniManagerIp'):\n        web_ui_url_list = ['{0}:{1}'.format(experiment_config['nniManagerIp'], str(args.port))]\n    else:\n        web_ui_url_list = get_local_urls(args.port)\n    nni_config.set_config('webuiUrl', web_ui_url_list)\n\n    # save experiment information\n    nnictl_experiment_config = Experiments()\n    nnictl_experiment_config.add_experiment(experiment_id, args.port, start_time, config_file_name,\n                                            experiment_config['trainingServicePlatform'],\n                                            experiment_config['experimentName'])\n\n    print_normal(EXPERIMENT_SUCCESS_INFO % (experiment_id, '   '.join(web_ui_url_list)))\n    if mode != 'view' and args.foreground:\n        try:\n            while True:\n                log_content = rest_process.stdout.readline().strip().decode('utf-8')\n                print(log_content)\n        except KeyboardInterrupt:\n            kill_command(rest_process.pid)\n            print_normal('Stopping experiment...')\n\ndef create_experiment(args):\n    '''start a new experiment'''\n    config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    nni_config = Config(config_file_name)\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error('Please set correct config path!')\n        exit(1)\n    experiment_config = get_yml_content(config_path)\n    validate_all_content(experiment_config, config_path)\n\n    nni_config.set_config('experimentConfig', experiment_config)\n    nni_config.set_config('restServerPort', args.port)\n    try:\n        launch_experiment(args, experiment_config, 'new', config_file_name)\n    except Exception as exception:\n        nni_config = Config(config_file_name)\n        restServerPid = nni_config.get_config('restServerPid')\n        if restServerPid:\n            kill_command(restServerPid)\n        print_error(exception)\n        exit(1)\n\ndef manage_stopped_experiment(args, mode):\n    '''view a stopped experiment'''\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    experiment_id = None\n    #find the latest stopped experiment\n    if not args.id:\n        print_error('Please set experiment id! \\nYou could use \\'nnictl {0} id\\' to {0} a stopped experiment!\\n' \\\n        'You could use \\'nnictl experiment list --all\\' to show all experiments!'.format(mode))\n        exit(1)\n    else:\n        if experiment_dict.get(args.id) is None:\n            print_error('Id %s not exist!' % args.id)\n            exit(1)\n        if experiment_dict[args.id]['status'] != 'STOPPED':\n            print_error('Only stopped experiments can be {0}ed!'.format(mode))\n            exit(1)\n        experiment_id = args.id\n    print_normal('{0} experiment {1}...'.format(mode, experiment_id))\n    nni_config = Config(experiment_dict[experiment_id]['fileName'])\n    experiment_config = nni_config.get_config('experimentConfig')\n    experiment_id = nni_config.get_config('experimentId')\n    new_config_file_name = ''.join(random.sample(string.ascii_letters + string.digits, 8))\n    new_nni_config = Config(new_config_file_name)\n    new_nni_config.set_config('experimentConfig', experiment_config)\n    new_nni_config.set_config('restServerPort', args.port)\n    try:\n        launch_experiment(args, experiment_config, mode, new_config_file_name, experiment_id)\n    except Exception as exception:\n        nni_config = Config(new_config_file_name)\n        restServerPid = nni_config.get_config('restServerPid')\n        if restServerPid:\n            kill_command(restServerPid)\n        print_error(exception)\n        exit(1)\n\ndef view_experiment(args):\n    '''view a stopped experiment'''\n    manage_stopped_experiment(args, 'view')\n\ndef resume_experiment(args):\n    '''resume an experiment'''\n    manage_stopped_experiment(args, 'resume')\n"""
tools/nni_cmd/launcher_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport json\nfrom schema import SchemaError\nfrom schema import Schema\nfrom .config_schema import LOCAL_CONFIG_SCHEMA, REMOTE_CONFIG_SCHEMA, PAI_CONFIG_SCHEMA, PAI_YARN_CONFIG_SCHEMA, \\\n                           DLTS_CONFIG_SCHEMA, KUBEFLOW_CONFIG_SCHEMA, FRAMEWORKCONTROLLER_CONFIG_SCHEMA, \\\n                           tuner_schema_dict, advisor_schema_dict, assessor_schema_dict\nfrom .common_utils import print_error, print_warning, print_normal, get_yml_content\n\ndef expand_path(experiment_config, key):\n    '''Change '~' to user home directory'''\n    if experiment_config.get(key):\n        experiment_config[key] = os.path.expanduser(experiment_config[key])\n\ndef parse_relative_path(root_path, experiment_config, key):\n    '''Change relative path to absolute path'''\n    if experiment_config.get(key) and not os.path.isabs(experiment_config.get(key)):\n        absolute_path = os.path.join(root_path, experiment_config.get(key))\n        print_normal('expand %s: %s to %s ' % (key, experiment_config[key], absolute_path))\n        experiment_config[key] = absolute_path\n\ndef parse_time(time):\n    '''Change the time to seconds'''\n    unit = time[-1]\n    if unit not in ['s', 'm', 'h', 'd']:\n        print_error('the unit of time could only from {s, m, h, d}')\n        exit(1)\n    time = time[:-1]\n    if not time.isdigit():\n        print_error('time format error!')\n        exit(1)\n    parse_dict = {'s':1, 'm':60, 'h':3600, 'd':86400}\n    return int(time) * parse_dict[unit]\n\ndef parse_path(experiment_config, config_path):\n    '''Parse path in config file'''\n    expand_path(experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        expand_path(experiment_config['trial'], 'codeDir')\n        if experiment_config['trial'].get('authFile'):\n            expand_path(experiment_config['trial'], 'authFile')\n        if experiment_config['trial'].get('ps'):\n            if experiment_config['trial']['ps'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['ps'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('master'):\n            if experiment_config['trial']['master'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['master'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('worker'):\n            if experiment_config['trial']['worker'].get('privateRegistryAuthPath'):\n                expand_path(experiment_config['trial']['worker'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('taskRoles'):\n            for index in range(len(experiment_config['trial']['taskRoles'])):\n                if experiment_config['trial']['taskRoles'][index].get('privateRegistryAuthPath'):\n                    expand_path(experiment_config['trial']['taskRoles'][index], 'privateRegistryAuthPath')\n    if experiment_config.get('tuner'):\n        expand_path(experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        expand_path(experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        expand_path(experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            expand_path(experiment_config['machineList'][index], 'sshKeyPath')\n    if experiment_config['trial'].get('paiConfigPath'):\n        expand_path(experiment_config['trial'], 'paiConfigPath')\n\n    #if users use relative path, convert it to absolute path\n    root_path = os.path.dirname(config_path)\n    if experiment_config.get('searchSpacePath'):\n        parse_relative_path(root_path, experiment_config, 'searchSpacePath')\n    if experiment_config.get('trial'):\n        parse_relative_path(root_path, experiment_config['trial'], 'codeDir')\n        if experiment_config['trial'].get('authFile'):\n            parse_relative_path(root_path, experiment_config['trial'], 'authFile')\n        if experiment_config['trial'].get('ps'):\n            if experiment_config['trial']['ps'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['ps'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('master'):\n            if experiment_config['trial']['master'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['master'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('worker'):\n            if experiment_config['trial']['worker'].get('privateRegistryAuthPath'):\n                parse_relative_path(root_path, experiment_config['trial']['worker'], 'privateRegistryAuthPath')\n        if experiment_config['trial'].get('taskRoles'):\n            for index in range(len(experiment_config['trial']['taskRoles'])):\n                if experiment_config['trial']['taskRoles'][index].get('privateRegistryAuthPath'):\n                    parse_relative_path(root_path, experiment_config['trial']['taskRoles'][index], 'privateRegistryAuthPath')\n    if experiment_config.get('tuner'):\n        parse_relative_path(root_path, experiment_config['tuner'], 'codeDir')\n    if experiment_config.get('assessor'):\n        parse_relative_path(root_path, experiment_config['assessor'], 'codeDir')\n    if experiment_config.get('advisor'):\n        parse_relative_path(root_path, experiment_config['advisor'], 'codeDir')\n    if experiment_config.get('machineList'):\n        for index in range(len(experiment_config['machineList'])):\n            parse_relative_path(root_path, experiment_config['machineList'][index], 'sshKeyPath')\n    if experiment_config['trial'].get('paiConfigPath'):\n        parse_relative_path(root_path, experiment_config['trial'], 'paiConfigPath')\n\ndef validate_search_space_content(experiment_config):\n    '''Validate searchspace content,\n       if the searchspace file is not json format or its values does not contain _type and _value which must be specified,\n       it will not be a valid searchspace file'''\n    try:\n        search_space_content = json.load(open(experiment_config.get('searchSpacePath'), 'r'))\n        for value in search_space_content.values():\n            if not value.get('_type') or not value.get('_value'):\n                print_error('please use _type and _value to specify searchspace!')\n                exit(1)\n    except:\n        print_error('searchspace file is not a valid json format!')\n        exit(1)\n\ndef validate_kubeflow_operators(experiment_config):\n    '''Validate whether the kubeflow operators are valid'''\n    if experiment_config.get('kubeflowConfig'):\n        if experiment_config.get('kubeflowConfig').get('operator') == 'tf-operator':\n            if experiment_config.get('trial').get('master') is not None:\n                print_error('kubeflow with tf-operator can not set master')\n                exit(1)\n            if experiment_config.get('trial').get('worker') is None:\n                print_error('kubeflow with tf-operator must set worker')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('operator') == 'pytorch-operator':\n            if experiment_config.get('trial').get('ps') is not None:\n                print_error('kubeflow with pytorch-operator can not set ps')\n                exit(1)\n            if experiment_config.get('trial').get('master') is None:\n                print_error('kubeflow with pytorch-operator must set master')\n                exit(1)\n\n        if experiment_config.get('kubeflowConfig').get('storage') == 'nfs':\n            if experiment_config.get('kubeflowConfig').get('nfs') is None:\n                print_error('please set nfs configuration!')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('storage') == 'azureStorage':\n            if experiment_config.get('kubeflowConfig').get('azureStorage') is None:\n                print_error('please set azureStorage configuration!')\n                exit(1)\n        elif experiment_config.get('kubeflowConfig').get('storage') is None:\n            if experiment_config.get('kubeflowConfig').get('azureStorage'):\n                print_error('please set storage type!')\n                exit(1)\n\ndef validate_common_content(experiment_config):\n    '''Validate whether the common values in experiment_config is valid'''\n    if not experiment_config.get('trainingServicePlatform') or \\\n        experiment_config.get('trainingServicePlatform') not in [\n                'local', 'remote', 'pai', 'kubeflow', 'frameworkcontroller', 'paiYarn', 'dlts'\n        ]:\n        print_error('Please set correct trainingServicePlatform!')\n        exit(1)\n    schema_dict = {\n        'local': LOCAL_CONFIG_SCHEMA,\n        'remote': REMOTE_CONFIG_SCHEMA,\n        'pai': PAI_CONFIG_SCHEMA,\n        'paiYarn': PAI_YARN_CONFIG_SCHEMA,\n        'kubeflow': KUBEFLOW_CONFIG_SCHEMA,\n        'frameworkcontroller': FRAMEWORKCONTROLLER_CONFIG_SCHEMA,\n        'dlts': DLTS_CONFIG_SCHEMA,\n        }\n    separate_schema_dict = {\n        'tuner': tuner_schema_dict,\n        'advisor': advisor_schema_dict,\n        'assessor': assessor_schema_dict\n    }\n    separate_builtInName_dict = {\n        'tuner': 'builtinTunerName',\n        'advisor': 'builtinAdvisorName',\n        'assessor': 'builtinAssessorName'\n    }\n    try:\n        schema_dict.get(experiment_config['trainingServicePlatform']).validate(experiment_config)\n        for separate_key in separate_schema_dict.keys():\n            if experiment_config.get(separate_key):\n                if experiment_config[separate_key].get(separate_builtInName_dict[separate_key]):\n                    validate = False\n                    for key in separate_schema_dict[separate_key].keys():\n                        if key.__contains__(experiment_config[separate_key][separate_builtInName_dict[separate_key]]):\n                            Schema({**separate_schema_dict[separate_key][key]}).validate(experiment_config[separate_key])\n                            validate = True\n                            break\n                    if not validate:\n                        print_error('%s %s error!' % (separate_key, separate_builtInName_dict[separate_key]))\n                        exit(1)\n                else:\n                    Schema({**separate_schema_dict[separate_key]['customized']}).validate(experiment_config[separate_key])\n    except SchemaError as error:\n        print_error('Your config file is not correct, please check your config file content!')\n        print_error(error.code)\n        exit(1)\n\n    #set default value\n    if experiment_config.get('maxExecDuration') is None:\n        experiment_config['maxExecDuration'] = '999d'\n    if experiment_config.get('maxTrialNum') is None:\n        experiment_config['maxTrialNum'] = 99999\n    if experiment_config['trainingServicePlatform'] == 'remote':\n        for index in range(len(experiment_config['machineList'])):\n            if experiment_config['machineList'][index].get('port') is None:\n                experiment_config['machineList'][index]['port'] = 22\n\ndef validate_customized_file(experiment_config, spec_key):\n    '''\n    check whether the file of customized tuner/assessor/advisor exists\n    spec_key: 'tuner', 'assessor', 'advisor'\n    '''\n    if experiment_config[spec_key].get('codeDir') and \\\n        experiment_config[spec_key].get('classFileName') and \\\n        experiment_config[spec_key].get('className'):\n        if not os.path.exists(os.path.join(\n                experiment_config[spec_key]['codeDir'],\n                experiment_config[spec_key]['classFileName'])):\n            print_error('%s file directory is not valid!'%(spec_key))\n            exit(1)\n    else:\n        print_error('%s file directory is not valid!'%(spec_key))\n        exit(1)\n\ndef parse_tuner_content(experiment_config):\n    '''Validate whether tuner in experiment_config is valid'''\n    if not experiment_config['tuner'].get('builtinTunerName'):\n        validate_customized_file(experiment_config, 'tuner')\n\ndef parse_assessor_content(experiment_config):\n    '''Validate whether assessor in experiment_config is valid'''\n    if experiment_config.get('assessor'):\n        if not experiment_config['assessor'].get('builtinAssessorName'):\n            validate_customized_file(experiment_config, 'assessor')\n\ndef parse_advisor_content(experiment_config):\n    '''Validate whether advisor in experiment_config is valid'''\n    if not experiment_config['advisor'].get('builtinAdvisorName'):\n        validate_customized_file(experiment_config, 'advisor')\n\ndef validate_annotation_content(experiment_config, spec_key, builtin_name):\n    '''\n    Valid whether useAnnotation and searchSpacePath is coexist\n    spec_key: 'advisor' or 'tuner'\n    builtin_name: 'builtinAdvisorName' or 'builtinTunerName'\n    '''\n    if experiment_config.get('useAnnotation'):\n        if experiment_config.get('searchSpacePath'):\n            print_error('If you set useAnnotation=true, please leave searchSpacePath empty')\n            exit(1)\n    else:\n        # validate searchSpaceFile\n        if experiment_config[spec_key].get(builtin_name) == 'NetworkMorphism':\n            return\n        if experiment_config[spec_key].get(builtin_name):\n            if experiment_config.get('searchSpacePath') is None:\n                print_error('Please set searchSpacePath!')\n                exit(1)\n            validate_search_space_content(experiment_config)\n\ndef validate_machine_list(experiment_config):\n    '''Validate machine list'''\n    if experiment_config.get('trainingServicePlatform') == 'remote' and experiment_config.get('machineList') is None:\n        print_error('Please set machineList!')\n        exit(1)\n\ndef validate_pai_config_path(experiment_config):\n    '''validate paiConfigPath field'''\n    if experiment_config.get('trainingServicePlatform') == 'pai':\n        if experiment_config.get('trial', {}).get('paiConfigPath'):\n            # validate commands\n            pai_config = get_yml_content(experiment_config['trial']['paiConfigPath'])\n            taskRoles_dict = pai_config.get('taskRoles')\n            if not taskRoles_dict:\n                print_error('Please set taskRoles in paiConfigPath config file!')\n                exit(1)\n        else:\n            pai_trial_fields_required_list = ['image', 'gpuNum', 'cpuNum', 'memoryMB', 'paiStoragePlugin', 'command']\n            for trial_field in pai_trial_fields_required_list:\n                if experiment_config['trial'].get(trial_field) is None:\n                    print_error('Please set {0} in trial configuration,\\\n                                or set additional pai configuration file path in paiConfigPath!'.format(trial_field))\n                    exit(1)\n\ndef validate_pai_trial_conifg(experiment_config):\n    '''validate the trial config in pai platform'''\n    if experiment_config.get('trainingServicePlatform') in ['pai', 'paiYarn']:\n        if experiment_config.get('trial').get('shmMB') and \\\n        experiment_config['trial']['shmMB'] > experiment_config['trial']['memoryMB']:\n            print_error('shmMB should be no more than memoryMB!')\n            exit(1)\n        #backward compatibility\n        warning_information = '{0} is not supported in NNI anymore, please remove the field in config file!\\\n        please refer https://github.com/microsoft/nni/blob/master/docs/en_US/TrainingService/PaiMode.md#run-an-experiment\\\n        for the practices of how to get data and output model in trial code'\n        if experiment_config.get('trial').get('dataDir'):\n            print_warning(warning_information.format('dataDir'))\n        if experiment_config.get('trial').get('outputDir'):\n            print_warning(warning_information.format('outputDir'))\n        validate_pai_config_path(experiment_config)\n\ndef validate_all_content(experiment_config, config_path):\n    '''Validate whether experiment_config is valid'''\n    parse_path(experiment_config, config_path)\n    validate_common_content(experiment_config)\n    validate_pai_trial_conifg(experiment_config)\n    experiment_config['maxExecDuration'] = parse_time(experiment_config['maxExecDuration'])\n    if experiment_config.get('advisor'):\n        if experiment_config.get('assessor') or experiment_config.get('tuner'):\n            print_error('advisor could not be set with assessor or tuner simultaneously!')\n            exit(1)\n        parse_advisor_content(experiment_config)\n        validate_annotation_content(experiment_config, 'advisor', 'builtinAdvisorName')\n    else:\n        if not experiment_config.get('tuner'):\n            raise Exception('Please provide tuner spec!')\n        parse_tuner_content(experiment_config)\n        parse_assessor_content(experiment_config)\n        validate_annotation_content(experiment_config, 'tuner', 'builtinTunerName')\n"""
tools/nni_cmd/nnictl.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport os\nimport pkg_resources\nfrom colorama import init\nfrom .common_utils import print_error\nfrom .launcher import create_experiment, resume_experiment, view_experiment\nfrom .updater import update_searchspace, update_concurrency, update_duration, update_trialnum, import_data\nfrom .nnictl_utils import stop_experiment, trial_ls, trial_kill, list_experiment, experiment_status,\\\n                          log_trial, experiment_clean, platform_clean, experiment_list, \\\n                          monitor_experiment, export_trials_data, trial_codegen, webui_url, \\\n                          get_config, log_stdout, log_stderr, search_space_auto_gen, webui_nas\nfrom .package_management import package_install, package_show\nfrom .constants import DEFAULT_REST_PORT\nfrom .tensorboard_utils import start_tensorboard, stop_tensorboard\ninit(autoreset=True)\n\nif os.environ.get(\'COVERAGE_PROCESS_START\'):\n    import coverage\n    coverage.process_startup()\n\ndef nni_info(*args):\n    if args[0].version:\n        try:\n            print(pkg_resources.get_distribution(\'nni\').version)\n        except pkg_resources.ResolutionError:\n            print_error(\'Get version failed, please use `pip3 list | grep nni` to check nni version!\')\n    else:\n        print(\'please run ""nnictl {positional argument} --help"" to see nnictl guidance\')\n\ndef parse_args():\n    \'\'\'Definite the arguments users need to follow and input\'\'\'\n    parser = argparse.ArgumentParser(prog=\'nnictl\', description=\'use nnictl command to control nni experiments\')\n    parser.add_argument(\'--version\', \'-v\', action=\'store_true\')\n    parser.set_defaults(func=nni_info)\n\n    # create subparsers for args with sub values\n    subparsers = parser.add_subparsers()\n\n    # parse the command of auto generating search space\n    parser_start = subparsers.add_parser(\'ss_gen\', help=\'automatically generate search space file from trial code\')\n    parser_start.add_argument(\'--trial_command\', \'-t\', required=True, dest=\'trial_command\', help=\'the command for running trial code\')\n    parser_start.add_argument(\'--trial_dir\', \'-d\', default=\'./\', dest=\'trial_dir\', help=\'the directory for running the command\')\n    parser_start.add_argument(\'--file\', \'-f\', default=\'nni_auto_gen_search_space.json\', dest=\'file\', help=\'the path of search space file\')\n    parser_start.set_defaults(func=search_space_auto_gen)\n\n    # parse start command\n    parser_start = subparsers.add_parser(\'create\', help=\'create a new experiment\')\n    parser_start.add_argument(\'--config\', \'-c\', required=True, dest=\'config\', help=\'the path of yaml config file\')\n    parser_start.add_argument(\'--port\', \'-p\', default=DEFAULT_REST_PORT, dest=\'port\', help=\'the port of restful server\')\n    parser_start.add_argument(\'--debug\', \'-d\', action=\'store_true\', help=\' set debug mode\')\n    parser_start.add_argument(\'--foreground\', \'-f\', action=\'store_true\', help=\' set foreground mode, print log content to terminal\')\n    parser_start.set_defaults(func=create_experiment)\n\n    # parse resume command\n    parser_resume = subparsers.add_parser(\'resume\', help=\'resume a new experiment\')\n    parser_resume.add_argument(\'id\', nargs=\'?\', help=\'The id of the experiment you want to resume\')\n    parser_resume.add_argument(\'--port\', \'-p\', default=DEFAULT_REST_PORT, dest=\'port\', help=\'the port of restful server\')\n    parser_resume.add_argument(\'--debug\', \'-d\', action=\'store_true\', help=\' set debug mode\')\n    parser_resume.add_argument(\'--foreground\', \'-f\', action=\'store_true\', help=\' set foreground mode, print log content to terminal\')\n    parser_resume.set_defaults(func=resume_experiment)\n\n    # parse view command\n    parser_view = subparsers.add_parser(\'view\', help=\'view a stopped experiment\')\n    parser_view.add_argument(\'id\', nargs=\'?\', help=\'The id of the experiment you want to view\')\n    parser_view.add_argument(\'--port\', \'-p\', default=DEFAULT_REST_PORT, dest=\'port\', help=\'the port of restful server\')\n    parser_view.set_defaults(func=view_experiment)\n\n    # parse update command\n    parser_updater = subparsers.add_parser(\'update\', help=\'update the experiment\')\n    #add subparsers for parser_updater\n    parser_updater_subparsers = parser_updater.add_subparsers()\n    parser_updater_searchspace = parser_updater_subparsers.add_parser(\'searchspace\', help=\'update searchspace\')\n    parser_updater_searchspace.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_updater_searchspace.add_argument(\'--filename\', \'-f\', required=True)\n    parser_updater_searchspace.set_defaults(func=update_searchspace)\n    parser_updater_concurrency = parser_updater_subparsers.add_parser(\'concurrency\', help=\'update concurrency\')\n    parser_updater_concurrency.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_updater_concurrency.add_argument(\'--value\', \'-v\', required=True)\n    parser_updater_concurrency.set_defaults(func=update_concurrency)\n    parser_updater_duration = parser_updater_subparsers.add_parser(\'duration\', help=\'update duration\')\n    parser_updater_duration.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_updater_duration.add_argument(\'--value\', \'-v\', required=True, help=\'the unit of time should in {\\\'s\\\', \\\'m\\\', \\\'h\\\', \\\'d\\\'}\')\n    parser_updater_duration.set_defaults(func=update_duration)\n    parser_updater_trialnum = parser_updater_subparsers.add_parser(\'trialnum\', help=\'update maxtrialnum\')\n    parser_updater_trialnum.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_updater_trialnum.add_argument(\'--value\', \'-v\', required=True)\n    parser_updater_trialnum.set_defaults(func=update_trialnum)\n\n    #parse stop command\n    parser_stop = subparsers.add_parser(\'stop\', help=\'stop the experiment\')\n    parser_stop.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment, use \\\'all\\\' to stop all running experiments\')\n    parser_stop.add_argument(\'--port\', \'-p\', dest=\'port\', help=\'the port of restful server\')\n    parser_stop.add_argument(\'--all\', \'-a\', action=\'store_true\', help=\'stop all of experiments\')\n    parser_stop.set_defaults(func=stop_experiment)\n\n    #parse trial command\n    parser_trial = subparsers.add_parser(\'trial\', help=\'get trial information\')\n    #add subparsers for parser_trial\n    parser_trial_subparsers = parser_trial.add_subparsers()\n    parser_trial_ls = parser_trial_subparsers.add_parser(\'ls\', help=\'list trial jobs\')\n    parser_trial_ls.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_trial_ls.set_defaults(func=trial_ls)\n    parser_trial_kill = parser_trial_subparsers.add_parser(\'kill\', help=\'kill trial jobs\')\n    parser_trial_kill.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_trial_kill.add_argument(\'--trial_id\', \'-T\', required=True, dest=\'trial_id\', help=\'the id of trial to be killed\')\n    parser_trial_kill.set_defaults(func=trial_kill)\n    parser_trial_codegen = parser_trial_subparsers.add_parser(\'codegen\', help=\'generate trial code for a specific trial\')\n    parser_trial_codegen.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_trial_codegen.add_argument(\'--trial_id\', \'-T\', required=True, dest=\'trial_id\', help=\'the id of trial to do code generation\')\n    parser_trial_codegen.set_defaults(func=trial_codegen)\n\n    #parse experiment command\n    parser_experiment = subparsers.add_parser(\'experiment\', help=\'get experiment information\')\n    #add subparsers for parser_experiment\n    parser_experiment_subparsers = parser_experiment.add_subparsers()\n    parser_experiment_show = parser_experiment_subparsers.add_parser(\'show\', help=\'show the information of experiment\')\n    parser_experiment_show.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_experiment_show.set_defaults(func=list_experiment)\n    parser_experiment_status = parser_experiment_subparsers.add_parser(\'status\', help=\'show the status of experiment\')\n    parser_experiment_status.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_experiment_status.set_defaults(func=experiment_status)\n    parser_experiment_list = parser_experiment_subparsers.add_parser(\'list\', help=\'list all of running experiment ids\')\n    parser_experiment_list.add_argument(\'--all\', action=\'store_true\', default=False, help=\'list all of experiments\')\n    parser_experiment_list.set_defaults(func=experiment_list)\n    parser_experiment_clean = parser_experiment_subparsers.add_parser(\'delete\', help=\'clean up the experiment data\')\n    parser_experiment_clean.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_experiment_clean.add_argument(\'--all\', action=\'store_true\', default=False, help=\'delete all of experiments\')\n    parser_experiment_clean.set_defaults(func=experiment_clean)\n\n    #parse experiment command\n    parser_platform = subparsers.add_parser(\'platform\', help=\'get platform information\')\n    #add subparsers for parser_experiment\n    parser_platform_subparsers = parser_platform.add_subparsers()\n    parser_platform_clean = parser_platform_subparsers.add_parser(\'clean\', help=\'clean up the platform data\')\n    parser_platform_clean.add_argument(\'--config\', \'-c\', required=True, dest=\'config\', help=\'the path of yaml config file\')\n    parser_platform_clean.set_defaults(func=platform_clean)\n\n    #import tuning data\n    parser_import_data = parser_experiment_subparsers.add_parser(\'import\', help=\'import additional data\')\n    parser_import_data.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_import_data.add_argument(\'--filename\', \'-f\', required=True)\n    parser_import_data.set_defaults(func=import_data)\n    #export trial data\n    parser_trial_export = parser_experiment_subparsers.add_parser(\'export\', help=\'export trial job results to csv or json\')\n    parser_trial_export.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_trial_export.add_argument(\'--type\', \'-t\', choices=[\'json\', \'csv\'], required=True, dest=\'type\', help=\'target file type\')\n    parser_trial_export.add_argument(\'--filename\', \'-f\', required=True, dest=\'path\', help=\'target file path\')\n    parser_trial_export.set_defaults(func=export_trials_data)\n\n    #TODO:finish webui function\n    #parse board command\n    parser_webui = subparsers.add_parser(\'webui\', help=\'get web ui information\')\n    #add subparsers for parser_board\n    parser_webui_subparsers = parser_webui.add_subparsers()\n    parser_webui_url = parser_webui_subparsers.add_parser(\'url\', help=\'show the url of web ui\')\n    parser_webui_url.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_webui_url.set_defaults(func=webui_url)\n    parser_webui_nas = parser_webui_subparsers.add_parser(\'nas\', help=\'show nas ui\')\n    parser_webui_nas.add_argument(\'--port\', default=6060, type=int, help=\'port of nas ui\')\n    parser_webui_nas.add_argument(\'--logdir\', default=\'.\', type=str, help=\'the logdir where nas ui will read data\')\n    parser_webui_nas.set_defaults(func=webui_nas)\n\n    #parse config command\n    parser_config = subparsers.add_parser(\'config\', help=\'get config information\')\n    parser_config_subparsers = parser_config.add_subparsers()\n    parser_config_show = parser_config_subparsers.add_parser(\'show\', help=\'show the information of config\')\n    parser_config_show.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_config_show.set_defaults(func=get_config)\n\n    #parse log command\n    parser_log = subparsers.add_parser(\'log\', help=\'get log information\')\n    # add subparsers for parser_log\n    parser_log_subparsers = parser_log.add_subparsers()\n    parser_log_stdout = parser_log_subparsers.add_parser(\'stdout\', help=\'get stdout information\')\n    parser_log_stdout.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_log_stdout.add_argument(\'--tail\', \'-T\', dest=\'tail\', type=int, help=\'get tail -100 content of stdout\')\n    parser_log_stdout.add_argument(\'--head\', \'-H\', dest=\'head\', type=int, help=\'get head -100 content of stdout\')\n    parser_log_stdout.add_argument(\'--path\', action=\'store_true\', default=False, help=\'get the path of stdout file\')\n    parser_log_stdout.set_defaults(func=log_stdout)\n    parser_log_stderr = parser_log_subparsers.add_parser(\'stderr\', help=\'get stderr information\')\n    parser_log_stderr.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_log_stderr.add_argument(\'--tail\', \'-T\', dest=\'tail\', type=int, help=\'get tail -100 content of stderr\')\n    parser_log_stderr.add_argument(\'--head\', \'-H\', dest=\'head\', type=int, help=\'get head -100 content of stderr\')\n    parser_log_stderr.add_argument(\'--path\', action=\'store_true\', default=False, help=\'get the path of stderr file\')\n    parser_log_stderr.set_defaults(func=log_stderr)\n    parser_log_trial = parser_log_subparsers.add_parser(\'trial\', help=\'get trial log path\')\n    parser_log_trial.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_log_trial.add_argument(\'--trial_id\', \'-T\', dest=\'trial_id\', help=\'find trial log path by id\')\n    parser_log_trial.set_defaults(func=log_trial)\n\n    #parse package command\n    parser_package = subparsers.add_parser(\'package\', help=\'control nni tuner and assessor packages\')\n    # add subparsers for parser_package\n    parser_package_subparsers = parser_package.add_subparsers()\n    parser_package_install = parser_package_subparsers.add_parser(\'install\', help=\'install packages\')\n    parser_package_install.add_argument(\'--name\', \'-n\', dest=\'name\', help=\'package name to be installed\')\n    parser_package_install.set_defaults(func=package_install)\n    parser_package_show = parser_package_subparsers.add_parser(\'show\', help=\'show the information of packages\')\n    parser_package_show.set_defaults(func=package_show)\n\n    #parse tensorboard command\n    parser_tensorboard = subparsers.add_parser(\'tensorboard\', help=\'manage tensorboard\')\n    parser_tensorboard_subparsers = parser_tensorboard.add_subparsers()\n    parser_tensorboard_start = parser_tensorboard_subparsers.add_parser(\'start\', help=\'start tensorboard\')\n    parser_tensorboard_start.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_tensorboard_start.add_argument(\'--trial_id\', \'-T\', dest=\'trial_id\', help=\'the id of trial\')\n    parser_tensorboard_start.add_argument(\'--port\', dest=\'port\', default=6006, help=\'the port to start tensorboard\')\n    parser_tensorboard_start.set_defaults(func=start_tensorboard)\n    parser_tensorboard_stop = parser_tensorboard_subparsers.add_parser(\'stop\', help=\'stop tensorboard\')\n    parser_tensorboard_stop.add_argument(\'id\', nargs=\'?\', help=\'the id of experiment\')\n    parser_tensorboard_stop.set_defaults(func=stop_tensorboard)\n\n    #parse top command\n    parser_top = subparsers.add_parser(\'top\', help=\'monitor the experiment\')\n    parser_top.add_argument(\'--time\', \'-t\', dest=\'time\', type=int, default=3, help=\'the time interval to update the experiment status, \' \\\n    \'the unit is second\')\n    parser_top.set_defaults(func=monitor_experiment)\n\n    args = parser.parse_args()\n    args.func(args)\n\nif __name__ == \'__main__\':\n    parse_args()\n'"
tools/nni_cmd/nnictl_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport csv\nimport os\nimport sys\nimport json\nimport time\nimport re\nimport shutil\nimport subprocess\nfrom datetime import datetime, timezone\nfrom pathlib import Path\nfrom subprocess import Popen\nfrom pyhdfs import HdfsClient\nfrom nni_annotation import expand_annotations\nfrom .rest_utils import rest_get, rest_delete, check_rest_server_quick, check_response\nfrom .url_utils import trial_jobs_url, experiment_url, trial_job_id_url, export_data_url\nfrom .config_utils import Config, Experiments\nfrom .constants import NNICTL_HOME_DIR, EXPERIMENT_INFORMATION_FORMAT, EXPERIMENT_DETAIL_FORMAT, \\\n     EXPERIMENT_MONITOR_INFO, TRIAL_MONITOR_HEAD, TRIAL_MONITOR_CONTENT, TRIAL_MONITOR_TAIL, REST_TIME_OUT\nfrom .common_utils import print_normal, print_error, print_warning, detect_process, get_yml_content, get_nni_installation_path\nfrom .command_utils import check_output_command, kill_command\nfrom .ssh_utils import create_ssh_sftp_client, remove_remote_directory\n\ndef get_experiment_time(port):\n    \'\'\'get the startTime and endTime of an experiment\'\'\'\n    response = rest_get(experiment_url(port), REST_TIME_OUT)\n    if response and check_response(response):\n        content = convert_time_stamp_to_date(json.loads(response.text))\n        return content.get(\'startTime\'), content.get(\'endTime\')\n    return None, None\n\ndef get_experiment_status(port):\n    \'\'\'get the status of an experiment\'\'\'\n    result, response = check_rest_server_quick(port)\n    if result:\n        return json.loads(response.text).get(\'status\')\n    return None\n\ndef update_experiment():\n    \'\'\'Update the experiment status in config file\'\'\'\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        return None\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get(\'status\') != \'STOPPED\':\n                nni_config = Config(experiment_dict[key][\'fileName\'])\n                rest_pid = nni_config.get_config(\'restServerPid\')\n                if not detect_process(rest_pid):\n                    experiment_config.update_experiment(key, \'status\', \'STOPPED\')\n                    continue\n                rest_port = nni_config.get_config(\'restServerPort\')\n                startTime, endTime = get_experiment_time(rest_port)\n                if startTime:\n                    experiment_config.update_experiment(key, \'startTime\', startTime)\n                if endTime:\n                    experiment_config.update_experiment(key, \'endTime\', endTime)\n                status = get_experiment_status(rest_port)\n                if status:\n                    experiment_config.update_experiment(key, \'status\', status)\n\ndef check_experiment_id(args, update=True):\n    \'\'\'check if the id is valid\n    \'\'\'\n    if update:\n        update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal(\'There is no experiment running...\')\n        return None\n    if not args.id:\n        running_experiment_list = []\n        for key in experiment_dict.keys():\n            if isinstance(experiment_dict[key], dict):\n                if experiment_dict[key].get(\'status\') != \'STOPPED\':\n                    running_experiment_list.append(key)\n            elif isinstance(experiment_dict[key], list):\n                # if the config file is old version, remove the configuration from file\n                experiment_config.remove_experiment(key)\n        if len(running_experiment_list) > 1:\n            print_error(\'There are multiple experiments, please set the experiment id...\')\n            experiment_information = """"\n            for key in running_experiment_list:\n                experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                                      experiment_dict[key].get(\'experimentName\', \'N/A\'),\n                                                                      experiment_dict[key][\'status\'],\n                                                                      experiment_dict[key][\'port\'],\n                                                                      experiment_dict[key].get(\'platform\'),\n                                                                      experiment_dict[key][\'startTime\'],\n                                                                      experiment_dict[key][\'endTime\'])\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        elif not running_experiment_list:\n            print_error(\'There is no experiment running.\')\n            return None\n        else:\n            return running_experiment_list[0]\n    if experiment_dict.get(args.id):\n        return args.id\n    else:\n        print_error(\'Id not correct.\')\n        return None\n\ndef parse_ids(args):\n    \'\'\'Parse the arguments for nnictl stop\n    1.If port is provided and id is not specified, return the id who owns the port\n    2.If both port and id are provided, return the id if it owns the port, otherwise fail\n    3.If there is an id specified, return the corresponding id\n    4.If there is no id specified, and there is an experiment running, return the id, or return Error\n    5.If the id matches an experiment, nnictl will return the id.\n    6.If the id ends with *, nnictl will match all ids matchs the regular\n    7.If the id does not exist but match the prefix of an experiment id, nnictl will return the matched id\n    8.If the id does not exist but match multiple prefix of the experiment ids, nnictl will give id information\n    \'\'\'\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal(\'Experiment is not running...\')\n        return None\n    result_list = []\n    running_experiment_list = []\n    for key in experiment_dict.keys():\n        if isinstance(experiment_dict[key], dict):\n            if experiment_dict[key].get(\'status\') != \'STOPPED\':\n                running_experiment_list.append(key)\n        elif isinstance(experiment_dict[key], list):\n            # if the config file is old version, remove the configuration from file\n            experiment_config.remove_experiment(key)\n    if args.all:\n        return running_experiment_list\n    if args.port is not None:\n        for key in running_experiment_list:\n            if str(experiment_dict[key][\'port\']) == args.port:\n                result_list.append(key)\n        if args.id and result_list and args.id != result_list[0]:\n            print_error(\'Experiment id and resful server port not match\')\n            exit(1)\n    elif not args.id:\n        if len(running_experiment_list) > 1:\n            print_error(\'There are multiple experiments, please set the experiment id...\')\n            experiment_information = """"\n            for key in running_experiment_list:\n                experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                                      experiment_dict[key].get(\'experimentName\', \'N/A\'),\n                                                                      experiment_dict[key][\'status\'],\n                                                                      experiment_dict[key][\'port\'],\n                                                                      experiment_dict[key].get(\'platform\'),\n                                                                      experiment_dict[key][\'startTime\'],\n                                                                      experiment_dict[key][\'endTime\'])\n            print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n            exit(1)\n        else:\n            result_list = running_experiment_list\n    elif args.id.endswith(\'*\'):\n        for expId in running_experiment_list:\n            if expId.startswith(args.id[:-1]):\n                result_list.append(expId)\n    elif args.id in running_experiment_list:\n        result_list.append(args.id)\n    else:\n        for expId in running_experiment_list:\n            if expId.startswith(args.id):\n                result_list.append(expId)\n        if len(result_list) > 1:\n            print_error(args.id + \' is ambiguous, please choose \' + \' \'.join(result_list))\n            return None\n    if not result_list and (args.id  or args.port):\n        print_error(\'There are no experiments matched, please set correct experiment id or restful server port\')\n    elif not result_list:\n        print_error(\'There is no experiment running...\')\n    return result_list\n\ndef get_config_filename(args):\n    \'\'\'get the file name of config file\'\'\'\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error(\'Please set correct experiment id.\')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id][\'fileName\']\n\ndef get_experiment_port(args):\n    \'\'\'get the port of experiment\'\'\'\n    experiment_id = check_experiment_id(args)\n    if experiment_id is None:\n        print_error(\'Please set correct experiment id.\')\n        exit(1)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    return experiment_dict[experiment_id][\'port\']\n\ndef convert_time_stamp_to_date(content):\n    \'\'\'Convert time stamp to date time format\'\'\'\n    start_time_stamp = content.get(\'startTime\')\n    end_time_stamp = content.get(\'endTime\')\n    if start_time_stamp:\n        start_time = datetime.fromtimestamp(start_time_stamp // 1000, timezone.utc).astimezone().strftime(""%Y/%m/%d %H:%M:%S"")\n        content[\'startTime\'] = str(start_time)\n    if end_time_stamp:\n        end_time = datetime.fromtimestamp(end_time_stamp // 1000, timezone.utc).astimezone().strftime(""%Y/%m/%d %H:%M:%S"")\n        content[\'endTime\'] = str(end_time)\n    return content\n\ndef check_rest(args):\n    \'\'\'check if restful server is running\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    running, _ = check_rest_server_quick(rest_port)\n    if not running:\n        print_normal(\'Restful server is running...\')\n    else:\n        print_normal(\'Restful server is not running...\')\n\ndef stop_experiment(args):\n    \'\'\'Stop the experiment which is running\'\'\'\n    if args.id and args.id == \'all\':\n        print_warning(\'\\\'nnictl stop all\\\' is abolished, please use \\\'nnictl stop --all\\\' to stop all of experiments!\')\n        exit(1)\n    experiment_id_list = parse_ids(args)\n    if experiment_id_list:\n        experiment_config = Experiments()\n        experiment_dict = experiment_config.get_all_experiments()\n        for experiment_id in experiment_id_list:\n            print_normal(\'Stopping experiment %s\' % experiment_id)\n            nni_config = Config(experiment_dict[experiment_id][\'fileName\'])\n            rest_pid = nni_config.get_config(\'restServerPid\')\n            if rest_pid:\n                kill_command(rest_pid)\n                tensorboard_pid_list = nni_config.get_config(\'tensorboardPidList\')\n                if tensorboard_pid_list:\n                    for tensorboard_pid in tensorboard_pid_list:\n                        try:\n                            kill_command(tensorboard_pid)\n                        except Exception as exception:\n                            print_error(exception)\n                    nni_config.set_config(\'tensorboardPidList\', [])\n            print_normal(\'Stop experiment success.\')\n            experiment_config.update_experiment(experiment_id, \'status\', \'STOPPED\')\n            time_now = time.strftime(\'%Y-%m-%d %H:%M:%S\', time.localtime(time.time()))\n            experiment_config.update_experiment(experiment_id, \'endTime\', str(time_now))\n\ndef trial_ls(args):\n    \'\'\'List trial\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    rest_pid = nni_config.get_config(\'restServerPid\')\n    if not detect_process(rest_pid):\n        print_error(\'Experiment is not running...\')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = json.loads(response.text)\n            for index, value in enumerate(content):\n                content[index] = convert_time_stamp_to_date(value)\n            print(json.dumps(content, indent=4, sort_keys=True, separators=(\',\', \':\')))\n        else:\n            print_error(\'List trial failed...\')\n    else:\n        print_error(\'Restful server is not running...\')\n\ndef trial_kill(args):\n    \'\'\'List trial\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    rest_pid = nni_config.get_config(\'restServerPid\')\n    if not detect_process(rest_pid):\n        print_error(\'Experiment is not running...\')\n        return\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_delete(trial_job_id_url(rest_port, args.trial_id), REST_TIME_OUT)\n        if response and check_response(response):\n            print(response.text)\n        else:\n            print_error(\'Kill trial job failed...\')\n    else:\n        print_error(\'Restful server is not running...\')\n\ndef trial_codegen(args):\n    \'\'\'Generate code for a specific trial\'\'\'\n    print_warning(\'Currently, this command is only for nni nas programming interface.\')\n    exp_id = check_experiment_id(args)\n    nni_config = Config(get_config_filename(args))\n    if not nni_config.get_config(\'experimentConfig\')[\'useAnnotation\']:\n        print_error(\'The experiment is not using annotation\')\n        exit(1)\n    code_dir = nni_config.get_config(\'experimentConfig\')[\'trial\'][\'codeDir\']\n    expand_annotations(code_dir, \'./exp_%s_trial_%s_code\'%(exp_id, args.trial_id), exp_id, args.trial_id)\n\ndef list_experiment(args):\n    \'\'\'Get experiment information\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    rest_pid = nni_config.get_config(\'restServerPid\')\n    if not detect_process(rest_pid):\n        print_error(\'Experiment is not running...\')\n        return\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(experiment_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = convert_time_stamp_to_date(json.loads(response.text))\n            print(json.dumps(content, indent=4, sort_keys=True, separators=(\',\', \':\')))\n        else:\n            print_error(\'List experiment failed...\')\n    else:\n        print_error(\'Restful server is not running...\')\n\ndef experiment_status(args):\n    \'\'\'Show the status of experiment\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    result, response = check_rest_server_quick(rest_port)\n    if not result:\n        print_normal(\'Restful server is not running...\')\n    else:\n        print(json.dumps(json.loads(response.text), indent=4, sort_keys=True, separators=(\',\', \':\')))\n\ndef log_internal(args, filetype):\n    \'\'\'internal function to call get_log_content\'\'\'\n    file_name = get_config_filename(args)\n    if filetype == \'stdout\':\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, \'stdout\')\n    else:\n        file_full_path = os.path.join(NNICTL_HOME_DIR, file_name, \'stderr\')\n    print(check_output_command(file_full_path, head=args.head, tail=args.tail))\n\ndef log_stdout(args):\n    \'\'\'get stdout log\'\'\'\n    log_internal(args, \'stdout\')\n\ndef log_stderr(args):\n    \'\'\'get stderr log\'\'\'\n    log_internal(args, \'stderr\')\n\ndef log_trial(args):\n    \'\'\'\'get trial log path\'\'\'\n    trial_id_path_dict = {}\n    trial_id_list = []\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    rest_pid = nni_config.get_config(\'restServerPid\')\n    if not detect_process(rest_pid):\n        print_error(\'Experiment is not running...\')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            content = json.loads(response.text)\n            for trial in content:\n                trial_id_list.append(trial.get(\'id\'))\n                if trial.get(\'logPath\'):\n                    trial_id_path_dict[trial.get(\'id\')] = trial[\'logPath\']\n    else:\n        print_error(\'Restful server is not running...\')\n        exit(1)\n    if args.trial_id:\n        if args.trial_id not in trial_id_list:\n            print_error(\'Trial id {0} not correct, please check your command!\'.format(args.trial_id))\n            exit(1)\n        if trial_id_path_dict.get(args.trial_id):\n            print_normal(\'id:\' + args.trial_id + \' path:\' + trial_id_path_dict[args.trial_id])\n        else:\n            print_error(\'Log path is not available yet, please wait...\')\n            exit(1)\n    else:\n        print_normal(\'All of trial log info:\')\n        for key in trial_id_path_dict:\n            print_normal(\'id:\' + key + \' path:\' + trial_id_path_dict[key])\n        if not trial_id_path_dict:\n            print_normal(\'None\')\n\ndef get_config(args):\n    \'\'\'get config info\'\'\'\n    nni_config = Config(get_config_filename(args))\n    print(nni_config.get_all_config())\n\ndef webui_url(args):\n    \'\'\'show the url of web ui\'\'\'\n    nni_config = Config(get_config_filename(args))\n    print_normal(\'{0} {1}\'.format(\'Web UI url:\', \' \'.join(nni_config.get_config(\'webuiUrl\'))))\n\ndef webui_nas(args):\n    \'\'\'launch nas ui\'\'\'\n    print_normal(\'Starting NAS UI...\')\n    try:\n        entry_dir = get_nni_installation_path()\n        entry_file = os.path.join(entry_dir, \'nasui\', \'server.js\')\n        node_command = \'node\'\n        if sys.platform == \'win32\':\n            node_command = os.path.join(entry_dir[:-3], \'Scripts\', \'node.exe\')\n        cmds = [node_command, \'--max-old-space-size=4096\', entry_file, \'--port\', str(args.port), \'--logdir\', args.logdir]\n        subprocess.run(cmds)\n    except KeyboardInterrupt:\n        pass\n\ndef local_clean(directory):\n    \'\'\'clean up local data\'\'\'\n    print_normal(\'removing folder {0}\'.format(directory))\n    try:\n        shutil.rmtree(directory)\n    except FileNotFoundError:\n        print_error(\'{0} does not exist.\'.format(directory))\n\ndef remote_clean(machine_list, experiment_id=None):\n    \'\'\'clean up remote data\'\'\'\n    for machine in machine_list:\n        passwd = machine.get(\'passwd\')\n        userName = machine.get(\'username\')\n        host = machine.get(\'ip\')\n        port = machine.get(\'port\')\n        sshKeyPath = machine.get(\'sshKeyPath\')\n        passphrase = machine.get(\'passphrase\')\n        if experiment_id:\n            remote_dir = \'/\' + \'/\'.join([\'tmp\', \'nni\', \'experiments\', experiment_id])\n        else:\n            remote_dir = \'/\' + \'/\'.join([\'tmp\', \'nni\', \'experiments\'])\n        sftp = create_ssh_sftp_client(host, port, userName, passwd, sshKeyPath, passphrase)\n        print_normal(\'removing folder {0}\'.format(host + \':\' + str(port) + remote_dir))\n        remove_remote_directory(sftp, remote_dir)\n\ndef hdfs_clean(host, user_name, output_dir, experiment_id=None):\n    \'\'\'clean up hdfs data\'\'\'\n    hdfs_client = HdfsClient(hosts=\'{0}:80\'.format(host), user_name=user_name, webhdfs_path=\'/webhdfs/api/v1\', timeout=5)\n    if experiment_id:\n        full_path = \'/\' + \'/\'.join([user_name, \'nni\', \'experiments\', experiment_id])\n    else:\n        full_path = \'/\' + \'/\'.join([user_name, \'nni\', \'experiments\'])\n    print_normal(\'removing folder {0} in hdfs\'.format(full_path))\n    hdfs_client.delete(full_path, recursive=True)\n    if output_dir:\n        pattern = re.compile(\'hdfs://(?P<host>([0-9]{1,3}.){3}[0-9]{1,3})(:[0-9]{2,5})?(?P<baseDir>/.*)?\')\n        match_result = pattern.match(output_dir)\n        if match_result:\n            output_host = match_result.group(\'host\')\n            output_dir = match_result.group(\'baseDir\')\n            #check if the host is valid\n            if output_host != host:\n                print_warning(\'The host in {0} is not consistent with {1}\'.format(output_dir, host))\n            else:\n                if experiment_id:\n                    output_dir = output_dir + \'/\' + experiment_id\n                print_normal(\'removing folder {0} in hdfs\'.format(output_dir))\n                hdfs_client.delete(output_dir, recursive=True)\n\ndef experiment_clean(args):\n    \'\'\'clean up the experiment data\'\'\'\n    experiment_id_list = []\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if args.all:\n        experiment_id_list = list(experiment_dict.keys())\n    else:\n        if args.id is None:\n            print_error(\'please set experiment id.\')\n            exit(1)\n        if args.id not in experiment_dict:\n            print_error(\'Cannot find experiment {0}.\'.format(args.id))\n            exit(1)\n        experiment_id_list.append(args.id)\n    while True:\n        print(\'INFO: This action will delete experiment {0}, and it\\\'s not recoverable.\'.format(\' \'.join(experiment_id_list)))\n        inputs = input(\'INFO: do you want to continue?[y/N]:\')\n        if not inputs.lower() or inputs.lower() in [\'n\', \'no\']:\n            exit(0)\n        elif inputs.lower() not in [\'y\', \'n\', \'yes\', \'no\']:\n            print_warning(\'please input Y or N.\')\n        else:\n            break\n    for experiment_id in experiment_id_list:\n        nni_config = Config(experiment_dict[experiment_id][\'fileName\'])\n        platform = nni_config.get_config(\'experimentConfig\').get(\'trainingServicePlatform\')\n        experiment_id = nni_config.get_config(\'experimentId\')\n        if platform == \'remote\':\n            machine_list = nni_config.get_config(\'experimentConfig\').get(\'machineList\')\n            remote_clean(machine_list, experiment_id)\n        elif platform == \'pai\':\n            host = nni_config.get_config(\'experimentConfig\').get(\'paiConfig\').get(\'host\')\n            user_name = nni_config.get_config(\'experimentConfig\').get(\'paiConfig\').get(\'userName\')\n            output_dir = nni_config.get_config(\'experimentConfig\').get(\'trial\').get(\'outputDir\')\n            hdfs_clean(host, user_name, output_dir, experiment_id)\n        elif platform != \'local\':\n            #TODO: support all platforms\n            print_warning(\'platform {0} clean up not supported yet.\'.format(platform))\n            exit(0)\n        #clean local data\n        home = str(Path.home())\n        local_dir = nni_config.get_config(\'experimentConfig\').get(\'logDir\')\n        if not local_dir:\n            local_dir = os.path.join(home, \'nni\', \'experiments\', experiment_id)\n        local_clean(local_dir)\n        experiment_config = Experiments()\n        print_normal(\'removing metadata of experiment {0}\'.format(experiment_id))\n        experiment_config.remove_experiment(experiment_id)\n        print_normal(\'Done.\')\n\ndef get_platform_dir(config_content):\n    \'\'\'get the dir list to be deleted\'\'\'\n    platform = config_content.get(\'trainingServicePlatform\')\n    dir_list = []\n    if platform == \'remote\':\n        machine_list = config_content.get(\'machineList\')\n        for machine in machine_list:\n            host = machine.get(\'ip\')\n            port = machine.get(\'port\')\n            dir_list.append(host + \':\' + str(port) + \'/tmp/nni\')\n    elif platform == \'pai\':\n        host = config_content.get(\'paiConfig\').get(\'host\')\n        user_name = config_content.get(\'paiConfig\').get(\'userName\')\n        output_dir = config_content.get(\'trial\').get(\'outputDir\')\n        dir_list.append(\'server: {0}, path: {1}/nni\'.format(host, user_name))\n        if output_dir:\n            dir_list.append(output_dir)\n    return dir_list\n\ndef platform_clean(args):\n    \'\'\'clean up the experiment data\'\'\'\n    config_path = os.path.abspath(args.config)\n    if not os.path.exists(config_path):\n        print_error(\'Please set correct config path.\')\n        exit(1)\n    config_content = get_yml_content(config_path)\n    platform = config_content.get(\'trainingServicePlatform\')\n    if platform == \'local\':\n        print_normal(\'it doesn\xe2\x80\x99t need to clean local platform.\')\n        exit(0)\n    if platform not in [\'remote\', \'pai\']:\n        print_normal(\'platform {0} not supported.\'.format(platform))\n        exit(0)\n    update_experiment()\n    dir_list = get_platform_dir(config_content)\n    if not dir_list:\n        print_normal(\'No folder of NNI caches is found.\')\n        exit(1)\n    while True:\n        print_normal(\'This command will remove below folders of NNI caches. If other users are using experiments\' \\\n                     \' on below hosts, it will be broken.\')\n        for value in dir_list:\n            print(\'       \' + value)\n        inputs = input(\'INFO: do you want to continue?[y/N]:\')\n        if not inputs.lower() or inputs.lower() in [\'n\', \'no\']:\n            exit(0)\n        elif inputs.lower() not in [\'y\', \'n\', \'yes\', \'no\']:\n            print_warning(\'please input Y or N.\')\n        else:\n            break\n    if platform == \'remote\':\n        machine_list = config_content.get(\'machineList\')\n        remote_clean(machine_list, None)\n    elif platform == \'pai\':\n        host = config_content.get(\'paiConfig\').get(\'host\')\n        user_name = config_content.get(\'paiConfig\').get(\'userName\')\n        output_dir = config_content.get(\'trial\').get(\'outputDir\')\n        hdfs_clean(host, user_name, output_dir, None)\n    print_normal(\'Done.\')\n\ndef experiment_list(args):\n    \'\'\'get the information of all experiments\'\'\'\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print_normal(\'Cannot find experiments.\')\n        exit(1)\n    experiment_id_list = []\n    if args.all:\n        for key in experiment_dict.keys():\n            experiment_id_list.append(key)\n    else:\n        for key in experiment_dict.keys():\n            if experiment_dict[key][\'status\'] != \'STOPPED\':\n                experiment_id_list.append(key)\n        if not experiment_id_list:\n            print_warning(\'There is no experiment running...\\nYou can use \\\'nnictl experiment list --all\\\' to list all experiments.\')\n    experiment_information = """"\n    for key in experiment_id_list:\n        experiment_information += EXPERIMENT_DETAIL_FORMAT % (key,\n                                                              experiment_dict[key].get(\'experimentName\', \'N/A\'),\n                                                              experiment_dict[key][\'status\'],\n                                                              experiment_dict[key][\'port\'],\n                                                              experiment_dict[key].get(\'platform\'),\n                                                              experiment_dict[key][\'startTime\'],\n                                                              experiment_dict[key][\'endTime\'])\n    print(EXPERIMENT_INFORMATION_FORMAT % experiment_information)\n\ndef get_time_interval(time1, time2):\n    \'\'\'get the interval of two times\'\'\'\n    try:\n        #convert time to timestamp\n        time1 = time.mktime(time.strptime(time1, \'%Y/%m/%d %H:%M:%S\'))\n        time2 = time.mktime(time.strptime(time2, \'%Y/%m/%d %H:%M:%S\'))\n        seconds = (datetime.fromtimestamp(time2) - datetime.fromtimestamp(time1)).seconds\n        #convert seconds to day:hour:minute:second\n        days = seconds / 86400\n        seconds %= 86400\n        hours = seconds / 3600\n        seconds %= 3600\n        minutes = seconds / 60\n        seconds %= 60\n        return \'%dd %dh %dm %ds\' % (days, hours, minutes, seconds)\n    except:\n        return \'N/A\'\n\ndef show_experiment_info():\n    \'\'\'show experiment information in monitor\'\'\'\n    update_experiment()\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    if not experiment_dict:\n        print(\'There is no experiment running...\')\n        exit(1)\n    experiment_id_list = []\n    for key in experiment_dict.keys():\n        if experiment_dict[key][\'status\'] != \'STOPPED\':\n            experiment_id_list.append(key)\n    if not experiment_id_list:\n        print_warning(\'There is no experiment running...\')\n        return\n    for key in experiment_id_list:\n        print(EXPERIMENT_MONITOR_INFO % (key, experiment_dict[key][\'status\'], experiment_dict[key][\'port\'], \\\n             experiment_dict[key].get(\'platform\'), experiment_dict[key][\'startTime\'], \\\n             get_time_interval(experiment_dict[key][\'startTime\'], experiment_dict[key][\'endTime\'])))\n        print(TRIAL_MONITOR_HEAD)\n        running, response = check_rest_server_quick(experiment_dict[key][\'port\'])\n        if running:\n            response = rest_get(trial_jobs_url(experiment_dict[key][\'port\']), REST_TIME_OUT)\n            if response and check_response(response):\n                content = json.loads(response.text)\n                for index, value in enumerate(content):\n                    content[index] = convert_time_stamp_to_date(value)\n                    print(TRIAL_MONITOR_CONTENT % (content[index].get(\'id\'), content[index].get(\'startTime\'), \\\n                          content[index].get(\'endTime\'), content[index].get(\'status\')))\n        print(TRIAL_MONITOR_TAIL)\n\ndef set_monitor(auto_exit, time_interval, port=None, pid=None):\n    \'\'\'set the experiment monitor engine\'\'\'\n    while True:\n        try:\n            if sys.platform == \'win32\':\n                os.system(\'cls\')\n            else:\n                os.system(\'clear\')\n            update_experiment()\n            show_experiment_info()\n            if auto_exit:\n                status = get_experiment_status(port)\n                if status in [\'DONE\', \'ERROR\', \'STOPPED\']:\n                    print_normal(\'Experiment status is {0}.\'.format(status))\n                    print_normal(\'Stopping experiment...\')\n                    kill_command(pid)\n                    print_normal(\'Stop experiment success.\')\n                    exit(0)\n            time.sleep(time_interval)\n        except KeyboardInterrupt:\n            if auto_exit:\n                print_normal(\'Stopping experiment...\')\n                kill_command(pid)\n                print_normal(\'Stop experiment success.\')\n            else:\n                print_normal(\'Exiting...\')\n            exit(0)\n        except Exception as exception:\n            print_error(exception)\n            exit(1)\n\ndef monitor_experiment(args):\n    \'\'\'monitor the experiment\'\'\'\n    if args.time <= 0:\n        print_error(\'please input a positive integer as time interval, the unit is second.\')\n        exit(1)\n    set_monitor(False, args.time)\n\ndef export_trials_data(args):\n    \'\'\'export experiment metadata to csv\n    \'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    rest_pid = nni_config.get_config(\'restServerPid\')\n    if not detect_process(rest_pid):\n        print_error(\'Experiment is not running...\')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(export_data_url(rest_port), 20)\n        if response is not None and check_response(response):\n            if args.type == \'json\':\n                with open(args.path, \'w\') as file:\n                    file.write(response.text)\n            elif args.type == \'csv\':\n                content = json.loads(response.text)\n                trial_records = []\n                for record in content:\n                    record_value = json.loads(record[\'value\'])\n                    if not isinstance(record_value, (float, int)):\n                        formated_record = {**record[\'parameter\'], **record_value, **{\'id\': record[\'id\']}}\n                    else:\n                        formated_record = {**record[\'parameter\'], **{\'reward\': record_value, \'id\': record[\'id\']}}\n                    trial_records.append(formated_record)\n                if not trial_records:\n                    print_error(\'No trial results collected! Please check your trial log...\')\n                    exit(0)\n                with open(args.path, \'w\', newline=\'\') as file:\n                    writer = csv.DictWriter(file, set.union(*[set(r.keys()) for r in trial_records]))\n                    writer.writeheader()\n                    writer.writerows(trial_records)\n            else:\n                print_error(\'Unknown type: %s\' % args.type)\n                exit(1)\n        else:\n            print_error(\'Export failed...\')\n    else:\n        print_error(\'Restful server is not Running\')\n\ndef search_space_auto_gen(args):\n    \'\'\'dry run trial code to generate search space file\'\'\'\n    trial_dir = os.path.expanduser(args.trial_dir)\n    file_path = os.path.expanduser(args.file)\n    if not os.path.isabs(file_path):\n        file_path = os.path.join(os.getcwd(), file_path)\n    assert os.path.exists(trial_dir)\n    if os.path.exists(file_path):\n        print_warning(\'%s already exists, will be overwritten.\' % file_path)\n    print_normal(\'Dry run to generate search space...\')\n    Popen(args.trial_command, cwd=trial_dir, env=dict(os.environ, NNI_GEN_SEARCH_SPACE=file_path), shell=True).wait()\n    if not os.path.exists(file_path):\n        print_warning(\'Expected search space file \\\'{}\\\' generated, but not found.\'.format(file_path))\n    else:\n        print_normal(\'Generate search space done: \\\'{}\\\'.\'.format(file_path))\n'"
tools/nni_cmd/package_management.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport nni\nfrom .constants import PACKAGE_REQUIREMENTS\nfrom .common_utils import print_error\nfrom .command_utils import install_requirements_command\n\ndef process_install(package_name):\n    if PACKAGE_REQUIREMENTS.get(package_name) is None:\n        print_error('{0} is not supported!' % package_name)\n    else:\n        requirements_path = os.path.join(nni.__path__[0], PACKAGE_REQUIREMENTS[package_name])\n        install_requirements_command(requirements_path)\n\ndef package_install(args):\n    '''install packages'''\n    process_install(args.name)\n\ndef package_show(args):\n    '''show all packages'''\n    print(' '.join(PACKAGE_REQUIREMENTS.keys()))\n\n"""
tools/nni_cmd/rest_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport time\nimport requests\nfrom .url_utils import check_status_url\nfrom .constants import REST_TIME_OUT\nfrom .common_utils import print_error\n\ndef rest_put(url, data, timeout, show_error=False):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None\n\ndef rest_post(url, data, timeout, show_error=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None\n\ndef rest_get(url, timeout, show_error=False):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None\n\ndef rest_delete(url, timeout, show_error=False):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as exception:\n        if show_error:\n            print_error(exception)\n        return None\n\ndef check_rest_server(rest_port):\n    '''Check if restful server is ready'''\n    retry_count = 5\n    for _ in range(retry_count):\n        response = rest_get(check_status_url(rest_port), REST_TIME_OUT)\n        if response:\n            if response.status_code == 200:\n                return True, response\n            else:\n                return False, response\n        else:\n            time.sleep(3)\n    return  False, response\n\ndef check_rest_server_quick(rest_port):\n    '''Check if restful server is ready, only check once'''\n    response = rest_get(check_status_url(rest_port), 5)\n    if response and response.status_code == 200:\n        return True, response\n    return False, None\n\ndef check_response(response):\n    '''Check if a response is success according to status_code'''\n    if response and response.status_code == 200:\n        return True\n    return False\n"""
tools/nni_cmd/ssh_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nfrom .common_utils import print_error\nfrom .command_utils import install_package_command\n\ndef check_environment():\n    '''check if paramiko is installed'''\n    try:\n        import paramiko\n    except:\n        install_package_command('paramiko')\n        import paramiko\n    return paramiko\n\ndef copy_remote_directory_to_local(sftp, remote_path, local_path):\n    '''copy remote directory to local machine'''\n    try:\n        os.makedirs(local_path, exist_ok=True)\n        files = sftp.listdir(remote_path)\n        for file in files:\n            remote_full_path = os.path.join(remote_path, file)\n            local_full_path = os.path.join(local_path, file)\n            try:\n                if sftp.listdir(remote_full_path):\n                    copy_remote_directory_to_local(sftp, remote_full_path, local_full_path)\n            except:\n                sftp.get(remote_full_path, local_full_path)\n    except Exception:\n        pass\n\ndef create_ssh_sftp_client(host_ip, port, username, password, ssh_key_path, passphrase):\n    '''create ssh client'''\n    try:\n        paramiko = check_environment()\n        conn = paramiko.Transport(host_ip, port)\n        if ssh_key_path is not None:\n            ssh_key = paramiko.RSAKey.from_private_key_file(ssh_key_path, password=passphrase)\n            conn.connect(username=username, pkey=ssh_key)\n        else:\n            conn.connect(username=username, password=password)\n        sftp = paramiko.SFTPClient.from_transport(conn)\n        return sftp\n    except Exception as exception:\n        print_error('Create ssh client error %s\\n' % exception)\n\ndef remove_remote_directory(sftp, directory):\n    '''remove a directory in remote machine'''\n    try:\n        files = sftp.listdir(directory)\n        for file in files:\n            filepath = '/'.join([directory, file])\n            try:\n                sftp.remove(filepath)\n            except IOError:\n                remove_remote_directory(sftp, filepath)\n        sftp.rmdir(directory)\n    except IOError as err:\n        print_error(err)\n"""
tools/nni_cmd/tensorboard_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport json\nimport re\nimport tempfile\nfrom subprocess import call, Popen\nfrom .rest_utils import rest_get, check_rest_server_quick, check_response\nfrom .config_utils import Config, Experiments\nfrom .url_utils import trial_jobs_url, get_local_urls\nfrom .constants import COLOR_GREEN_FORMAT, REST_TIME_OUT\nfrom .common_utils import print_normal, print_error, detect_process, detect_port, check_tensorboard_version\nfrom .nnictl_utils import check_experiment_id, check_experiment_id\nfrom .ssh_utils import create_ssh_sftp_client, copy_remote_directory_to_local\n\ndef parse_log_path(args, trial_content):\n    '''parse log path'''\n    path_list = []\n    host_list = []\n    for trial in trial_content:\n        if args.trial_id and args.trial_id != 'all' and trial.get('id') != args.trial_id:\n            continue\n        pattern = r'(?P<head>.+)://(?P<host>.+):(?P<path>.*)'\n        match = re.search(pattern, trial['logPath'])\n        if match:\n            path_list.append(match.group('path'))\n            host_list.append(match.group('host'))\n    if not path_list:\n        print_error('Trial id %s error!' % args.trial_id)\n        exit(1)\n    return path_list, host_list\n\ndef copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path):\n    '''use ssh client to copy data from remote machine to local machien'''\n    machine_list = nni_config.get_config('experimentConfig').get('machineList')\n    machine_dict = {}\n    local_path_list = []\n    for machine in machine_list:\n        machine_dict[machine['ip']] = {'port': machine['port'], 'passwd': machine['passwd'], 'username': machine['username'],\n                                       'sshKeyPath': machine.get('sshKeyPath'), 'passphrase': machine.get('passphrase')}\n    for index, host in enumerate(host_list):\n        local_path = os.path.join(temp_nni_path, trial_content[index].get('id'))\n        local_path_list.append(local_path)\n        print_normal('Copying log data from %s to %s' % (host + ':' + path_list[index], local_path))\n        sftp = create_ssh_sftp_client(host, machine_dict[host]['port'], machine_dict[host]['username'], machine_dict[host]['passwd'],\n                                      machine_dict[host]['sshKeyPath'], machine_dict[host]['passphrase'])\n        copy_remote_directory_to_local(sftp, path_list[index], local_path)\n    print_normal('Copy done!')\n    return local_path_list\n\ndef get_path_list(args, nni_config, trial_content, temp_nni_path):\n    '''get path list according to different platform'''\n    path_list, host_list = parse_log_path(args, trial_content)\n    platform = nni_config.get_config('experimentConfig').get('trainingServicePlatform')\n    if platform == 'local':\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    elif platform == 'remote':\n        path_list = copy_data_from_remote(args, nni_config, trial_content, path_list, host_list, temp_nni_path)\n        print_normal('Log path: %s' % ' '.join(path_list))\n        return path_list\n    else:\n        print_error('Not supported platform!')\n        exit(1)\n\ndef format_tensorboard_log_path(path_list):\n    new_path_list = []\n    for index, value in enumerate(path_list):\n        new_path_list.append('name%d:%s' % (index + 1, value))\n    return ','.join(new_path_list)\n\ndef start_tensorboard_process(args, nni_config, path_list, temp_nni_path):\n    '''call cmds to start tensorboard process in local machine'''\n    if detect_port(args.port):\n        print_error('Port %s is used by another process, please reset port!' % str(args.port))\n        exit(1)\n    with open(os.path.join(temp_nni_path, 'tensorboard_stdout'), 'a+') as stdout_file, \\\n         open(os.path.join(temp_nni_path, 'tensorboard_stderr'), 'a+') as stderr_file:\n        log_dir_cmd = '--logdir_spec' if check_tensorboard_version() >= '2.0' else '--logdir'\n        cmds = ['tensorboard', log_dir_cmd, format_tensorboard_log_path(path_list), '--port', str(args.port)]\n        tensorboard_process = Popen(cmds, stdout=stdout_file, stderr=stderr_file)\n    url_list = get_local_urls(args.port)\n    print_normal(COLOR_GREEN_FORMAT % 'Start tensorboard success!\\n' + 'Tensorboard urls: ' + '     '.join(url_list))\n    tensorboard_process_pid_list = nni_config.get_config('tensorboardPidList')\n    if tensorboard_process_pid_list is None:\n        tensorboard_process_pid_list = [tensorboard_process.pid]\n    else:\n        tensorboard_process_pid_list.append(tensorboard_process.pid)\n    nni_config.set_config('tensorboardPidList', tensorboard_process_pid_list)\n\ndef stop_tensorboard(args):\n    '''stop tensorboard'''\n    experiment_id = check_experiment_id(args)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    config_file_name = experiment_dict[experiment_id]['fileName']\n    nni_config = Config(config_file_name)\n    tensorboard_pid_list = nni_config.get_config('tensorboardPidList')\n    if tensorboard_pid_list:\n        for tensorboard_pid in tensorboard_pid_list:\n            try:\n                cmds = ['kill', '-9', str(tensorboard_pid)]\n                call(cmds)\n            except Exception as exception:\n                print_error(exception)\n        nni_config.set_config('tensorboardPidList', [])\n        print_normal('Stop tensorboard success!')\n    else:\n        print_error('No tensorboard configuration!')\n\n\ndef start_tensorboard(args):\n    '''start tensorboard'''\n    experiment_id = check_experiment_id(args)\n    experiment_config = Experiments()\n    experiment_dict = experiment_config.get_all_experiments()\n    config_file_name = experiment_dict[experiment_id]['fileName']\n    nni_config = Config(config_file_name)\n    rest_port = nni_config.get_config('restServerPort')\n    rest_pid = nni_config.get_config('restServerPid')\n    if not detect_process(rest_pid):\n        print_error('Experiment is not running...')\n        return\n    running, response = check_rest_server_quick(rest_port)\n    trial_content = None\n    if running:\n        response = rest_get(trial_jobs_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            trial_content = json.loads(response.text)\n        else:\n            print_error('List trial failed...')\n    else:\n        print_error('Restful server is not running...')\n    if not trial_content:\n        print_error('No trial information!')\n        exit(1)\n    if len(trial_content) > 1 and not args.trial_id:\n        print_error('There are multiple trials, please set trial id!')\n        exit(1)\n    experiment_id = nni_config.get_config('experimentId')\n    temp_nni_path = os.path.join(tempfile.gettempdir(), 'nni', experiment_id)\n    os.makedirs(temp_nni_path, exist_ok=True)\n\n    path_list = get_path_list(args, nni_config, trial_content, temp_nni_path)\n    start_tensorboard_process(args, nni_config, path_list, temp_nni_path)\n"""
tools/nni_cmd/updater.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport os\nfrom .rest_utils import rest_put, rest_post, rest_get, check_rest_server_quick, check_response\nfrom .url_utils import experiment_url, import_data_url\nfrom .config_utils import Config\nfrom .common_utils import get_json_content, print_normal, print_error, print_warning\nfrom .nnictl_utils import get_experiment_port, get_config_filename\nfrom .launcher_utils import parse_time\nfrom .constants import REST_TIME_OUT, TUNERS_SUPPORTING_IMPORT_DATA, TUNERS_NO_NEED_TO_IMPORT_DATA\n\ndef validate_digit(value, start, end):\n    \'\'\'validate if a digit is valid\'\'\'\n    if not str(value).isdigit() or int(value) < start or int(value) > end:\n        raise ValueError(\'%s must be a digit from %s to %s\' % (value, start, end))\n\ndef validate_file(path):\n    \'\'\'validate if a file exist\'\'\'\n    if not os.path.exists(path):\n        raise FileNotFoundError(\'%s is not a valid file path\' % path)\n\ndef validate_dispatcher(args):\n    \'\'\'validate if the dispatcher of the experiment supports importing data\'\'\'\n    nni_config = Config(get_config_filename(args)).get_config(\'experimentConfig\')\n    if nni_config.get(\'tuner\') and nni_config[\'tuner\'].get(\'builtinTunerName\'):\n        dispatcher_name = nni_config[\'tuner\'][\'builtinTunerName\']\n    elif nni_config.get(\'advisor\') and nni_config[\'advisor\'].get(\'builtinAdvisorName\'):\n        dispatcher_name = nni_config[\'advisor\'][\'builtinAdvisorName\']\n    else: # otherwise it should be a customized one\n        return\n    if dispatcher_name not in TUNERS_SUPPORTING_IMPORT_DATA:\n        if dispatcher_name in TUNERS_NO_NEED_TO_IMPORT_DATA:\n            print_warning(""There is no need to import data for %s"" % dispatcher_name)\n            exit(0)\n        else:\n            print_error(""%s does not support importing addtional data"" % dispatcher_name)\n            exit(1)\n\ndef load_search_space(path):\n    \'\'\'load search space content\'\'\'\n    content = json.dumps(get_json_content(path))\n    if not content:\n        raise ValueError(\'searchSpace file should not be empty\')\n    return content\n\ndef get_query_type(key):\n    \'\'\'get update query type\'\'\'\n    if key == \'trialConcurrency\':\n        return \'?update_type=TRIAL_CONCURRENCY\'\n    if key == \'maxExecDuration\':\n        return \'?update_type=MAX_EXEC_DURATION\'\n    if key == \'searchSpace\':\n        return \'?update_type=SEARCH_SPACE\'\n    if key == \'maxTrialNum\':\n        return \'?update_type=MAX_TRIAL_NUM\'\n\ndef update_experiment_profile(args, key, value):\n    \'\'\'call restful server to update experiment profile\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_get(experiment_url(rest_port), REST_TIME_OUT)\n        if response and check_response(response):\n            experiment_profile = json.loads(response.text)\n            experiment_profile[\'params\'][key] = value\n            response = rest_put(experiment_url(rest_port)+get_query_type(key), json.dumps(experiment_profile), REST_TIME_OUT)\n            if response and check_response(response):\n                return response\n    else:\n        print_error(\'Restful server is not running...\')\n    return None\n\ndef update_searchspace(args):\n    validate_file(args.filename)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, \'searchSpace\', content):\n            print_normal(\'Update %s success!\' % \'searchSpace\')\n        else:\n            print_error(\'Update %s failed!\' % \'searchSpace\')\n\n\ndef update_concurrency(args):\n    validate_digit(args.value, 1, 1000)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, \'trialConcurrency\', int(args.value)):\n            print_normal(\'Update %s success!\' % \'concurrency\')\n        else:\n            print_error(\'Update %s failed!\' % \'concurrency\')\n\ndef update_duration(args):\n    #parse time, change time unit to seconds\n    args.value = parse_time(args.value)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if update_experiment_profile(args, \'maxExecDuration\', int(args.value)):\n            print_normal(\'Update %s success!\' % \'duration\')\n        else:\n            print_error(\'Update %s failed!\' % \'duration\')\n\ndef update_trialnum(args):\n    validate_digit(args.value, 1, 999999999)\n    if update_experiment_profile(args, \'maxTrialNum\', int(args.value)):\n        print_normal(\'Update %s success!\' % \'trialnum\')\n    else:\n        print_error(\'Update %s failed!\' % \'trialnum\')\n\ndef import_data(args):\n    \'\'\'import additional data to the experiment\'\'\'\n    validate_file(args.filename)\n    validate_dispatcher(args)\n    content = load_search_space(args.filename)\n    args.port = get_experiment_port(args)\n    if args.port is not None:\n        if import_data_to_restful_server(args, content):\n            pass\n        else:\n            print_error(\'Import data failed!\')\n\ndef import_data_to_restful_server(args, content):\n    \'\'\'call restful server to import data to the experiment\'\'\'\n    nni_config = Config(get_config_filename(args))\n    rest_port = nni_config.get_config(\'restServerPort\')\n    running, _ = check_rest_server_quick(rest_port)\n    if running:\n        response = rest_post(import_data_url(rest_port), content, REST_TIME_OUT)\n        if response and check_response(response):\n            return response\n    else:\n        print_error(\'Restful server is not running...\')\n    return None\n'"
tools/nni_cmd/url_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport socket\nimport psutil\n\nBASE_URL = 'http://localhost'\n\nAPI_ROOT_URL = '/api/v1/nni'\n\nEXPERIMENT_API = '/experiment'\n\nCLUSTER_METADATA_API = '/experiment/cluster-metadata'\n\nIMPORT_DATA_API = '/experiment/import-data'\n\nCHECK_STATUS_API = '/check-status'\n\nTRIAL_JOBS_API = '/trial-jobs'\n\nEXPORT_DATA_API = '/export-data'\n\nTENSORBOARD_API = '/tensorboard'\n\n\ndef check_status_url(port):\n    '''get check_status url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, CHECK_STATUS_API)\n\n\ndef cluster_metadata_url(port):\n    '''get cluster_metadata_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, CLUSTER_METADATA_API)\n\n\ndef import_data_url(port):\n    '''get import_data_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, IMPORT_DATA_API)\n\n\ndef experiment_url(port):\n    '''get experiment_url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, EXPERIMENT_API)\n\n\ndef trial_jobs_url(port):\n    '''get trial_jobs url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, TRIAL_JOBS_API)\n\n\ndef trial_job_id_url(port, job_id):\n    '''get trial_jobs with id url'''\n    return '{0}:{1}{2}{3}/{4}'.format(BASE_URL, port, API_ROOT_URL, TRIAL_JOBS_API, job_id)\n\n\ndef export_data_url(port):\n    '''get export_data url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, EXPORT_DATA_API)\n\n\ndef tensorboard_url(port):\n    '''get tensorboard url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL, port, API_ROOT_URL, TENSORBOARD_API)\n\n\ndef get_local_urls(port):\n    '''get urls of local machine'''\n    url_list = []\n    for _, info in psutil.net_if_addrs().items():\n        for addr in info:\n            if socket.AddressFamily.AF_INET == addr.family:\n                url_list.append('http://{}:{}'.format(addr.address, port))\n    return url_list\n"""
tools/nni_gpu_tool/__init__.py,0,b''
tools/nni_gpu_tool/gpu_metrics_collector.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport os\nimport subprocess\nimport sys\nimport time\nimport traceback\n\nfrom xml.dom import minidom\n\n\ndef main(argv):\n    metrics_output_dir = os.environ[\'METRIC_OUTPUT_DIR\']\n\n    cmd = \'nvidia-smi -q -x\'.split()\n    while(True):\n        try:\n            smi_output = subprocess.check_output(cmd)\n        except Exception:\n            traceback.print_exc()\n            gen_empty_gpu_metric(metrics_output_dir)\n            break\n        parse_nvidia_smi_result(smi_output, metrics_output_dir)\n        # TODO: change to sleep time configurable via arguments\n        time.sleep(5)\n\n\ndef parse_nvidia_smi_result(smi, outputDir):\n    try:\n        old_umask = os.umask(0)\n        xmldoc = minidom.parseString(smi)\n        gpuList = xmldoc.getElementsByTagName(\'gpu\')\n        with open(os.path.join(outputDir, ""gpu_metrics""), \'a\') as outputFile:\n            outPut = {}\n            outPut[""Timestamp""] = time.asctime(time.localtime())\n            outPut[""gpuCount""] = len(gpuList)\n            outPut[""gpuInfos""] = []\n            for gpuIndex, gpu in enumerate(gpuList):\n                gpuInfo = {}\n                gpuInfo[\'index\'] = gpuIndex\n                gpuInfo[\'gpuUtil\'] = gpu.getElementsByTagName(\'utilization\')[0]\\\n                    .getElementsByTagName(\'gpu_util\')[0]\\\n                    .childNodes[0].data.replace(""%"", """").strip()\n                gpuInfo[\'gpuMemUtil\'] = gpu.getElementsByTagName(\'utilization\')[0]\\\n                    .getElementsByTagName(\'memory_util\')[0]\\\n                    .childNodes[0].data.replace(""%"", """").strip()\n                processes = gpu.getElementsByTagName(\'processes\')\n                runningProNumber = len(processes[0].getElementsByTagName(\'process_info\'))\n                gpuInfo[\'activeProcessNum\'] = runningProNumber\n\n                outPut[""gpuInfos""].append(gpuInfo)\n            print(outPut)\n            outputFile.write(""{}\\n"".format(json.dumps(outPut, sort_keys=True)))\n            outputFile.flush()\n    except Exception as error:\n        # e_info = sys.exc_info()\n        print(\'gpu_metrics_collector error: %s\' % error)\n    finally:\n        os.umask(old_umask)\n\n\ndef gen_empty_gpu_metric(outputDir):\n    try:\n        old_umask = os.umask(0)\n        with open(os.path.join(outputDir, ""gpu_metrics""), \'a\') as outputFile:\n            outPut = {}\n            outPut[""Timestamp""] = time.asctime(time.localtime())\n            outPut[""gpuCount""] = 0\n            outPut[""gpuInfos""] = []\n            print(outPut)\n            outputFile.write(""{}\\n"".format(json.dumps(outPut, sort_keys=True)))\n            outputFile.flush()\n    except Exception:\n        traceback.print_exc()\n    finally:\n        os.umask(old_umask)\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
tools/nni_trial_tool/__init__.py,0,b''
tools/nni_trial_tool/constants.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nAPI_ROOT_URL = '/api/v1/nni-pai'\n\nBASE_URL = 'http://{}'\n\nLOG_DIR = os.environ['NNI_OUTPUT_DIR']\n\nNNI_PLATFORM = os.environ['NNI_PLATFORM']\n\nSTDOUT_FULL_PATH = os.path.join(LOG_DIR, 'stdout')\n\nSTDERR_FULL_PATH = os.path.join(LOG_DIR, 'stderr')\n\nSTDOUT_API = '/stdout'\nVERSION_API = '/version'\nPARAMETER_META_API = '/parameter-file-meta'\nNNI_SYS_DIR = os.environ['NNI_SYS_DIR']\nNNI_TRIAL_JOB_ID = os.environ['NNI_TRIAL_JOB_ID']\nNNI_EXP_ID = os.environ['NNI_EXP_ID']\nMULTI_PHASE = os.environ['MULTI_PHASE']\n"""
tools/nni_trial_tool/hdfsClientUtility.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport posixpath\nfrom .log_utils import LogType, nni_log\n\ndef copyHdfsDirectoryToLocal(hdfsDirectory, localDirectory, hdfsClient):\n    '''Copy directory from HDFS to local'''\n    if not os.path.exists(localDirectory):\n        os.makedirs(localDirectory)\n    try:\n        listing = hdfsClient.list_status(hdfsDirectory)\n    except Exception as exception:\n        nni_log(LogType.Error, 'List hdfs directory {0} error: {1}'.format(hdfsDirectory, str(exception)))\n        raise exception\n\n    for f in listing:\n        if f.type == 'DIRECTORY':\n            subHdfsDirectory = posixpath.join(hdfsDirectory, f.pathSuffix)\n            subLocalDirectory = os.path.join(localDirectory, f.pathSuffix)\n            copyHdfsDirectoryToLocal(subHdfsDirectory, subLocalDirectory, hdfsClient)\n        elif f.type == 'FILE':\n            hdfsFilePath = posixpath.join(hdfsDirectory, f.pathSuffix)\n            localFilePath = os.path.join(localDirectory, f.pathSuffix)\n            copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient)\n        else:\n            raise AssertionError('unexpected type {}'.format(f.type))\n\ndef copyHdfsFileToLocal(hdfsFilePath, localFilePath, hdfsClient, override=True):\n    '''Copy file from HDFS to local'''\n    if not hdfsClient.exists(hdfsFilePath):\n        raise Exception('HDFS file {} does not exist!'.format(hdfsFilePath))\n    try:\n        file_status = hdfsClient.get_file_status(hdfsFilePath)\n        if file_status.type != 'FILE':\n            raise Exception('HDFS file path {} is not a file'.format(hdfsFilePath))\n    except Exception as exception:\n        nni_log(LogType.Error, 'Get hdfs file {0} status error: {1}'.format(hdfsFilePath, str(exception)))\n        raise exception\n\n    if os.path.exists(localFilePath) and override:\n        os.remove(localFilePath)\n    try:\n        hdfsClient.copy_to_local(hdfsFilePath, localFilePath)\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy hdfs file {0} to {1} error: {2}'.format(hdfsFilePath, localFilePath, str(exception)))\n        raise exception\n    nni_log(LogType.Info, 'Successfully copied hdfs file {0} to {1}, {2} bytes'.format(hdfsFilePath, localFilePath, file_status.length))\n\ndef copyDirectoryToHdfs(localDirectory, hdfsDirectory, hdfsClient):\n    '''Copy directory from local to HDFS'''\n    if not os.path.exists(localDirectory):\n        raise Exception('Local Directory does not exist!')\n    hdfsClient.mkdirs(hdfsDirectory)\n    result = True\n    for file in os.listdir(localDirectory):\n        file_path = os.path.join(localDirectory, file)\n        if os.path.isdir(file_path):\n            hdfs_directory = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyDirectoryToHdfs(file_path, hdfs_directory, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error,\n                        'Copy local directory {0} to hdfs directory {1} error: {2}'.format(file_path, hdfs_directory, str(exception)))\n                result = False\n        else:\n            hdfs_file_path = os.path.join(hdfsDirectory, file)\n            try:\n                result = result and copyFileToHdfs(file_path, hdfs_file_path, hdfsClient)\n            except Exception as exception:\n                nni_log(LogType.Error, 'Copy local file {0} to hdfs {1} error: {2}'.format(file_path, hdfs_file_path, str(exception)))\n                result = False\n    return result\n\ndef copyFileToHdfs(localFilePath, hdfsFilePath, hdfsClient, override=True):\n    '''Copy a local file to HDFS directory'''\n    if not os.path.exists(localFilePath):\n        raise Exception('Local file Path does not exist!')\n    if os.path.isdir(localFilePath):\n        raise Exception('localFile should not a directory!')\n    if hdfsClient.exists(hdfsFilePath):\n        if override:\n            hdfsClient.delete(hdfsFilePath)\n        else:\n            return False\n    try:\n        hdfsClient.copy_from_local(localFilePath, hdfsFilePath)\n        return True\n    except Exception as exception:\n        nni_log(LogType.Error, 'Copy local file {0} to hdfs file {1} error: {2}'.format(localFilePath, hdfsFilePath, str(exception)))\n        return False\n"""
tools/nni_trial_tool/log_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport json\nimport logging\nimport logging.handlers\nimport time\nimport threading\nimport re\n\nfrom datetime import datetime\nfrom enum import Enum, unique\nfrom logging import StreamHandler\n\nfrom queue import Queue\n\nfrom .rest_utils import rest_post\nfrom .url_utils import gen_send_stdout_url\n\n@unique\nclass LogType(Enum):\n    Trace = \'TRACE\'\n    Debug = \'DEBUG\'\n    Info = \'INFO\'\n    Warning = \'WARNING\'\n    Error = \'ERROR\'\n    Fatal = \'FATAL\'\n\n@unique\nclass StdOutputType(Enum):\n    Stdout = \'stdout\',\n    Stderr = \'stderr\'\n\ndef nni_log(log_type, log_message):\n    \'\'\'Log message into stdout\'\'\'\n    dt = datetime.now()\n    print(\'[{0}] {1} {2}\'.format(dt, log_type.value, log_message), flush=True)\n\nclass NNIRestLogHanlder(StreamHandler):\n    def __init__(self, host, port, tag, std_output_type=StdOutputType.Stdout):\n        StreamHandler.__init__(self)\n        self.host = host\n        self.port = port\n        self.tag = tag\n        self.std_output_type = std_output_type\n        self.orig_stdout = sys.__stdout__\n        self.orig_stderr = sys.__stderr__\n\n    def emit(self, record):\n        log_entry = {}\n        log_entry[\'tag\'] = self.tag\n        log_entry[\'stdOutputType\'] = self.std_output_type.name\n        log_entry[\'msg\'] = self.format(record)\n\n        try:\n            rest_post(gen_send_stdout_url(self.host, self.port), json.dumps(log_entry), 10, True)\n        except Exception as e:\n            self.orig_stderr.write(str(e) + \'\\n\')\n            self.orig_stderr.flush()\n\nclass RemoteLogger(object):\n    """"""\n    NNI remote logger\n    """"""\n    def __init__(self, syslog_host, syslog_port, tag, std_output_type, log_collection, log_level=logging.INFO):\n        \'\'\'\n        constructor\n        \'\'\'\n        self.logger = logging.getLogger(\'nni_syslog_{}\'.format(tag))\n        self.log_level = log_level\n        self.logger.setLevel(self.log_level)\n        handler = NNIRestLogHanlder(syslog_host, syslog_port, tag)\n        self.logger.addHandler(handler)\n        if std_output_type == StdOutputType.Stdout:\n            self.orig_stdout = sys.__stdout__\n        else:\n            self.orig_stdout = sys.__stderr__\n        self.log_collection = log_collection\n\n    def get_pipelog_reader(self):\n        \'\'\'\n        Get pipe for remote logger\n        \'\'\'\n        return PipeLogReader(self.logger, self.log_collection, logging.INFO)\n\n    def flush(self):\n        \'\'\'\n        Add flush in handler\n        \'\'\'\n        for handler in self.logger.handlers:\n            handler.flush()\n\n    def write(self, buf):\n        \'\'\'\n        Write buffer data into logger/stdout\n        \'\'\'\n        for line in buf.rstrip().splitlines():\n            self.orig_stdout.write(line.rstrip() + \'\\n\')\n            self.orig_stdout.flush()\n            try:\n                self.logger.log(self.log_level, line.rstrip())\n            except Exception:\n                pass\n\nclass PipeLogReader(threading.Thread):\n    """"""\n    The reader thread reads log data from pipe\n    """"""\n    def __init__(self, logger, log_collection, log_level=logging.INFO):\n        """"""Setup the object with a logger and a loglevel\n        and start the thread\n        """"""\n        threading.Thread.__init__(self)\n        self.queue = Queue()\n        self.logger = logger\n        self.daemon = False\n        self.log_level = log_level\n        self.fdRead, self.fdWrite = os.pipe()\n        self.pipeReader = os.fdopen(self.fdRead)\n        self.orig_stdout = sys.__stdout__\n        self._is_read_completed = False\n        self.process_exit = False\n        self.log_collection = log_collection\n        self.log_pattern = re.compile(r\'NNISDK_MEb\\\'.*\\\'$\')\n\n        def _populateQueue(stream, queue):\n            \'\'\'\n            Collect lines from \'stream\' and put them in \'quque\'.\n            \'\'\'\n            time.sleep(5)\n            while True:\n                cur_process_exit = self.process_exit\n                try:\n                    line = self.queue.get(True, 5)\n                    try:\n                        self.logger.log(self.log_level, line.rstrip())\n                    except Exception:\n                        pass\n                except Exception:\n                    if cur_process_exit == True:\n                        self._is_read_completed = True\n                        break\n\n        self.pip_log_reader_thread = threading.Thread(target=_populateQueue, args=(self.pipeReader, self.queue))\n        self.pip_log_reader_thread.daemon = True\n        self.start()\n        self.pip_log_reader_thread.start()\n\n    def fileno(self):\n        """"""Return the write file descriptor of the pipe\n        """"""\n        return self.fdWrite\n\n    def run(self):\n        """"""Run the thread, logging everything.\n           If the log_collection is \'none\', the log content will not be enqueued\n        """"""\n        for line in iter(self.pipeReader.readline, \'\'):\n            self.orig_stdout.write(line.rstrip() + \'\\n\')\n            self.orig_stdout.flush()\n\n            if self.log_collection == \'none\':\n                search_result = self.log_pattern.search(line)\n                if search_result:\n                    metrics = search_result.group(0)\n                    self.queue.put(metrics+\'\\n\')\n            else:\n                self.queue.put(line)\n\n        self.pipeReader.close()\n\n    def close(self):\n        """"""Close the write end of the pipe.\n        """"""\n        os.close(self.fdWrite)\n\n    @property\n    def is_read_completed(self):\n        """"""Return if read is completed\n        """"""\n        return self._is_read_completed\n\n    def set_process_exit(self):\n        self.process_exit = True\n        return self.process_exit\n'"
tools/nni_trial_tool/rest_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport requests\n\ndef rest_get(url, timeout):\n    '''Call rest get method'''\n    try:\n        response = requests.get(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http get to url {1}'.format(str(e), url))\n        return None\n\ndef rest_post(url, data, timeout, rethrow_exception=False):\n    '''Call rest post method'''\n    try:\n        response = requests.post(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                 data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        if rethrow_exception is True:\n            raise\n        print('Get exception {0} when sending http post to url {1}'.format(str(e), url))\n        return None\n\ndef rest_put(url, data, timeout):\n    '''Call rest put method'''\n    try:\n        response = requests.put(url, headers={'Accept': 'application/json', 'Content-Type': 'application/json'},\\\n                                data=data, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http put to url {1}'.format(str(e), url))\n        return None\n\ndef rest_delete(url, timeout):\n    '''Call rest delete method'''\n    try:\n        response = requests.delete(url, timeout=timeout)\n        return response\n    except Exception as e:\n        print('Get exception {0} when sending http delete to url {1}'.format(str(e), url))\n        return None\n"""
tools/nni_trial_tool/trial_keeper.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport ctypes\nimport json\nimport logging\nimport os\nimport re\nimport shlex\nimport sys\nimport threading\nimport time\nfrom subprocess import Popen\n\nimport pkg_resources\nfrom pyhdfs import HdfsClient\n\nfrom .constants import (LOG_DIR, MULTI_PHASE, NNI_EXP_ID, NNI_PLATFORM,\n                        NNI_SYS_DIR, NNI_TRIAL_JOB_ID)\nfrom .hdfsClientUtility import (copyDirectoryToHdfs, copyHdfsDirectoryToLocal,\n                                copyHdfsFileToLocal)\nfrom .log_utils import LogType, RemoteLogger, StdOutputType, nni_log\nfrom .rest_utils import rest_get, rest_post\nfrom .url_utils import gen_parameter_meta_url, gen_send_version_url\n\nlogger = logging.getLogger(\'trial_keeper\')\nregular = re.compile(\'v?(?P<version>[0-9](\\.[0-9]){0,1}).*\')\n\n_hdfs_client = None\n\n\ndef get_hdfs_client(args):\n    global _hdfs_client\n\n    if _hdfs_client is not None:\n        return _hdfs_client\n    # backward compatibility\n    hdfs_host = None\n\n    if args.hdfs_host:\n        hdfs_host = args.hdfs_host\n    elif args.pai_hdfs_host:\n        hdfs_host = args.pai_hdfs_host\n    else:\n        return None\n\n    if hdfs_host is not None and args.nni_hdfs_exp_dir is not None:\n        try:\n            if args.webhdfs_path:\n                _hdfs_client = HdfsClient(hosts=\'{0}:80\'.format(hdfs_host), user_name=args.pai_user_name,\n                                          webhdfs_path=args.webhdfs_path, timeout=5)\n            else:\n                # backward compatibility\n                _hdfs_client = HdfsClient(hosts=\'{0}:{1}\'.format(hdfs_host, \'50070\'), user_name=args.pai_user_name,\n                                          timeout=5)\n        except Exception as e:\n            nni_log(LogType.Error, \'Create HDFS client error: \' + str(e))\n            raise e\n    return _hdfs_client\n\n\ndef main_loop(args):\n    \'\'\'main loop logic for trial keeper\'\'\'\n\n    if not os.path.exists(LOG_DIR):\n        os.makedirs(LOG_DIR)\n\n    trial_keeper_syslogger = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, \'trial_keeper\',\n                                          StdOutputType.Stdout, args.log_collection)\n    # redirect trial keeper\'s stdout and stderr to syslog\n    trial_syslogger_stdout = RemoteLogger(args.nnimanager_ip, args.nnimanager_port, \'trial\', StdOutputType.Stdout,\n                                          args.log_collection)\n    sys.stdout = sys.stderr = trial_keeper_syslogger\n    hdfs_output_dir = None\n\n    if args.hdfs_output_dir:\n        hdfs_output_dir = args.hdfs_output_dir\n    elif args.pai_hdfs_output_dir:\n        hdfs_output_dir = args.pai_hdfs_output_dir\n\n    hdfs_client = get_hdfs_client(args)\n\n    if hdfs_client is not None:\n        copyHdfsDirectoryToLocal(args.nni_hdfs_exp_dir, os.getcwd(), hdfs_client)\n\n    if args.job_id_file:\n        with open(args.job_id_file, \'w\') as job_file:\n            job_file.write(""%d"" % os.getpid())\n\n    # Notice: We don\'t appoint env, which means subprocess wil inherit current environment and that is expected behavior\n    log_pipe_stdout = trial_syslogger_stdout.get_pipelog_reader()\n    process = Popen(args.trial_command, shell=True, stdout=log_pipe_stdout, stderr=log_pipe_stdout)\n    nni_log(LogType.Info, \'Trial keeper spawns a subprocess (pid {0}) to run command: {1}\'.format(process.pid,\n                                                                                                  shlex.split(\n                                                                                                      args.trial_command)))\n\n    while True:\n        retCode = process.poll()\n        # child worker process exits and all stdout data is read\n        if retCode is not None and log_pipe_stdout.set_process_exit() and log_pipe_stdout.is_read_completed == True:\n            # In Windows, the retCode -1 is 4294967295. It\'s larger than c_long, and raise OverflowError.\n            # So covert it to int32.\n            retCode = ctypes.c_long(retCode).value\n            nni_log(LogType.Info, \'subprocess terminated. Exit code is {}. Quit\'.format(retCode))\n            if hdfs_output_dir is not None:\n                # Copy local directory to hdfs for OpenPAI\n                nni_local_output_dir = os.environ[\'NNI_OUTPUT_DIR\']\n                try:\n                    if copyDirectoryToHdfs(nni_local_output_dir, hdfs_output_dir, hdfs_client):\n                        nni_log(LogType.Info,\n                                \'copy directory from {0} to {1} success!\'.format(nni_local_output_dir, hdfs_output_dir))\n                    else:\n                        nni_log(LogType.Info,\n                                \'copy directory from {0} to {1} failed!\'.format(nni_local_output_dir, hdfs_output_dir))\n                except Exception as e:\n                    nni_log(LogType.Error, \'HDFS copy directory got exception: \' + str(e))\n                    raise e\n\n            ## Exit as the retCode of subprocess(trial)\n            exit(retCode)\n            break\n\n        time.sleep(2)\n\n\ndef trial_keeper_help_info(*args):\n    print(\'please run --help to see guidance\')\n\n\ndef check_version(args):\n    try:\n        trial_keeper_version = pkg_resources.get_distribution(\'nni\').version\n    except pkg_resources.ResolutionError as err:\n        # package nni does not exist, try nni-tool package\n        nni_log(LogType.Error, \'Package nni does not exist!\')\n        os._exit(1)\n    if not args.nni_manager_version:\n        # skip version check\n        nni_log(LogType.Warning, \'Skipping version check!\')\n    else:\n        try:\n            trial_keeper_version = regular.search(trial_keeper_version).group(\'version\')\n            nni_log(LogType.Info, \'trial_keeper_version is {0}\'.format(trial_keeper_version))\n            nni_manager_version = regular.search(args.nni_manager_version).group(\'version\')\n            nni_log(LogType.Info, \'nni_manager_version is {0}\'.format(nni_manager_version))\n            log_entry = {}\n            if trial_keeper_version != nni_manager_version:\n                nni_log(LogType.Error, \'Version does not match!\')\n                error_message = \'NNIManager version is {0}, TrialKeeper version is {1}, NNI version does not match!\'.format(\n                    nni_manager_version, trial_keeper_version)\n                log_entry[\'tag\'] = \'VCFail\'\n                log_entry[\'msg\'] = error_message\n                rest_post(gen_send_version_url(args.nnimanager_ip, args.nnimanager_port), json.dumps(log_entry), 10,\n                          False)\n                os._exit(1)\n            else:\n                nni_log(LogType.Info, \'Version match!\')\n                log_entry[\'tag\'] = \'VCSuccess\'\n                rest_post(gen_send_version_url(args.nnimanager_ip, args.nnimanager_port), json.dumps(log_entry), 10,\n                          False)\n        except AttributeError as err:\n            nni_log(LogType.Error, err)\n\n\ndef is_multi_phase():\n    return MULTI_PHASE and (MULTI_PHASE in [\'True\', \'true\'])\n\n\ndef download_parameter(meta_list, args):\n    """"""\n    Download parameter file to local working directory.\n    meta_list format is defined in paiJobRestServer.ts\n    example meta_list:\n    [\n        {""experimentId"":""yWFJarYa"",""trialId"":""UpPkl"",""filePath"":""/chec/nni/experiments/yWFJarYa/trials/UpPkl/parameter_1.cfg""},\n        {""experimentId"":""yWFJarYa"",""trialId"":""aIUMA"",""filePath"":""/chec/nni/experiments/yWFJarYa/trials/aIUMA/parameter_1.cfg""}\n    ]\n    """"""\n    nni_log(LogType.Debug, str(meta_list))\n    nni_log(LogType.Debug,\n            \'NNI_SYS_DIR: {}, trial Id: {}, experiment ID: {}\'.format(NNI_SYS_DIR, NNI_TRIAL_JOB_ID, NNI_EXP_ID))\n    nni_log(LogType.Debug, \'NNI_SYS_DIR files: {}\'.format(os.listdir(NNI_SYS_DIR)))\n    for meta in meta_list:\n        if meta[\'experimentId\'] == NNI_EXP_ID and meta[\'trialId\'] == NNI_TRIAL_JOB_ID:\n            param_fp = os.path.join(NNI_SYS_DIR, os.path.basename(meta[\'filePath\']))\n            if not os.path.exists(param_fp):\n                hdfs_client = get_hdfs_client(args)\n                copyHdfsFileToLocal(meta[\'filePath\'], param_fp, hdfs_client, override=False)\n\n\ndef fetch_parameter_file(args):\n    class FetchThread(threading.Thread):\n        def __init__(self, args):\n            super(FetchThread, self).__init__()\n            self.args = args\n\n        def run(self):\n            uri = gen_parameter_meta_url(self.args.nnimanager_ip, self.args.nnimanager_port)\n            nni_log(LogType.Info, uri)\n\n            while True:\n                res = rest_get(uri, 10)\n                nni_log(LogType.Debug, \'status code: {}\'.format(res.status_code))\n                if res.status_code == 200:\n                    meta_list = res.json()\n                    download_parameter(meta_list, self.args)\n                else:\n                    nni_log(LogType.Warning, \'rest response: {}\'.format(str(res)))\n                time.sleep(5)\n\n    fetch_file_thread = FetchThread(args)\n    fetch_file_thread.start()\n\n\nif __name__ == \'__main__\':\n    \'\'\'NNI Trial Keeper main function\'\'\'\n    PARSER = argparse.ArgumentParser()\n    PARSER.set_defaults(func=trial_keeper_help_info)\n    PARSER.add_argument(\'--trial_command\', type=str, help=\'Command to launch trial process\')\n    PARSER.add_argument(\'--nnimanager_ip\', type=str, default=\'localhost\', help=\'NNI manager rest server IP\')\n    PARSER.add_argument(\'--nnimanager_port\', type=str, default=\'8081\', help=\'NNI manager rest server port\')\n    PARSER.add_argument(\'--pai_hdfs_output_dir\', type=str, help=\'the output dir of pai_hdfs\')  # backward compatibility\n    PARSER.add_argument(\'--hdfs_output_dir\', type=str, help=\'the output dir of hdfs\')\n    PARSER.add_argument(\'--pai_hdfs_host\', type=str, help=\'the host of pai_hdfs\')  # backward compatibility\n    PARSER.add_argument(\'--hdfs_host\', type=str, help=\'the host of hdfs\')\n    PARSER.add_argument(\'--pai_user_name\', type=str, help=\'the username of hdfs\')\n    PARSER.add_argument(\'--nni_hdfs_exp_dir\', type=str, help=\'nni experiment directory in hdfs\')\n    PARSER.add_argument(\'--webhdfs_path\', type=str, help=\'the webhdfs path used in webhdfs URL\')\n    PARSER.add_argument(\'--nni_manager_version\', type=str, help=\'the nni version transmitted from nniManager\')\n    PARSER.add_argument(\'--log_collection\', type=str, help=\'set the way to collect log in trialkeeper\')\n    PARSER.add_argument(\'--job_id_file\', type=str, help=\'set job id file for operating and monitoring job.\')\n    args, unknown = PARSER.parse_known_args()\n    if args.trial_command is None:\n        exit(1)\n    check_version(args)\n    try:\n        if NNI_PLATFORM == \'paiYarn\' and is_multi_phase():\n            fetch_parameter_file(args)\n        main_loop(args)\n    except SystemExit as se:\n        nni_log(LogType.Info, \'NNI trial keeper exit with code {}\'.format(se.code))\n        os._exit(se.code)\n    except Exception as e:\n        nni_log(LogType.Error, \'Exit trial keeper with code 1 because Exception: {} is catched\'.format(str(e)))\n        os._exit(1)\n'"
tools/nni_trial_tool/url_utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .constants import API_ROOT_URL, BASE_URL, STDOUT_API, NNI_TRIAL_JOB_ID, NNI_EXP_ID, VERSION_API, PARAMETER_META_API\n\ndef gen_send_stdout_url(ip, port):\n    '''Generate send stdout url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, STDOUT_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)\n\ndef gen_send_version_url(ip, port):\n    '''Generate send error url'''\n    return '{0}:{1}{2}{3}/{4}/{5}'.format(BASE_URL.format(ip), port, API_ROOT_URL, VERSION_API, NNI_EXP_ID, NNI_TRIAL_JOB_ID)\n\ndef gen_parameter_meta_url(ip, port):\n    '''Generate send error url'''\n    return '{0}:{1}{2}{3}'.format(BASE_URL.format(ip), port, API_ROOT_URL, PARAMETER_META_API)\n"""
examples/feature_engineering/gbdt_selector/gbdt_selector_test.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport bz2\nimport urllib.request\nimport numpy as np\n\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.model_selection import train_test_split\n\nfrom nni.feature_engineering.gbdt_selector import GBDTSelector\n\nurl_zip_train = \'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2\'\nurllib.request.urlretrieve(url_zip_train, filename=\'train.bz2\')\n\nf_svm = open(\'train.svm\', \'wt\')\nwith bz2.open(\'train.bz2\', \'rb\') as f_zip:\n    data = f_zip.read()\n    f_svm.write(data.decode(\'utf-8\'))\nf_svm.close()\n\nX, y = load_svmlight_file(\'train.svm\')\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nlgb_params = {\n        \'boosting_type\': \'gbdt\',\n        \'objective\': \'regression\',\n        \'metric\': {\'l2\', \'l1\'},\n        \'num_leaves\': 20,\n        \'learning_rate\': 0.05,\n        \'feature_fraction\': 0.9,\n        \'bagging_fraction\': 0.8,\n        \'bagging_freq\': 5,\n        \'verbose\': 0}\n\neval_ratio = 0.1\nearly_stopping_rounds = 10\nimportance_type = \'gain\'\nnum_boost_round = 1000\ntopk = 10\n\nselector = GBDTSelector()\nselector.fit(X_train, y_train,\n             lgb_params = lgb_params,\n             eval_ratio = eval_ratio,\n             early_stopping_rounds = early_stopping_rounds,\n             importance_type = importance_type,\n             num_boost_round = num_boost_round)\n\nprint(""selected features\\t"", selector.get_selected_features(topk=topk))\n\n'"
examples/feature_engineering/gradient_feature_selector/benchmark_test.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport bz2\nimport urllib.request\nimport numpy as np\nimport datetime\n\nimport line_profiler\nprofile = line_profiler.LineProfiler()\n\nimport os\n\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom nni.feature_engineering.gradient_selector import FeatureGradientSelector\n\n\nclass Benchmark():\n\n    def __init__(self, files=None, test_size=0.2):\n        self.files =  files\n        self.test_size = test_size\n\n\n    def run_all_test(self, pipeline):\n        for file_name in self.files:\n            file_path = self.files[file_name]\n\n            self.run_test(pipeline, file_name, file_path)\n\n\n    def run_test(self, pipeline, name, path):\n        print(""download "" + name)\n        update_name = self.download(name, path)\n        X, y = load_svmlight_file(update_name)\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=self.test_size, random_state=42)\n        \n        pipeline.fit(X_train, y_train)\n        print(""[Benchmark ""+ name + "" Score]: "", pipeline.score(X_test, y_test))\n\n\n    def download(self, name, path):\n        old_name = name + \'_train.bz2\'\n        update_name = name + \'_train.svm\'\n\n        if os.path.exists(old_name) and os.path.exists(update_name):\n            return update_name\n\n        urllib.request.urlretrieve(path, filename=old_name)\n\n        f_svm = open(update_name, \'wt\')\n        with bz2.open(old_name, \'rb\') as f_zip:\n            data = f_zip.read()\n            f_svm.write(data.decode(\'utf-8\'))\n        f_svm.close()\n\n        return update_name\n\n@profile\ndef test_memory(pipeline_name, name, path):\n    if pipeline_name == ""LR"":\n        pipeline = make_pipeline(LogisticRegression())\n\n    if pipeline_name == ""FGS"":\n        pipeline = make_pipeline(FeatureGradientSelector(), LogisticRegression())\n\n    if pipeline_name == ""Tree"":\n        pipeline = make_pipeline(SelectFromModel(ExtraTreesClassifier(n_estimators=50)), LogisticRegression())\n    \n    test_benchmark = Benchmark()\n    print(""Dataset:\\t"", name)\n    print(""Pipeline:\\t"", pipeline_name)\n    test_benchmark.run_test(pipeline, name, path)\n    print("""")\n\n\ndef test_time(pipeline_name, name, path):\n    if pipeline_name == ""LR"":\n        pipeline = make_pipeline(LogisticRegression())\n\n    if pipeline_name == ""FGS"":\n        pipeline = make_pipeline(FeatureGradientSelector(), LogisticRegression())\n\n    if pipeline_name == ""Tree"":\n        pipeline = make_pipeline(SelectFromModel(ExtraTreesClassifier(n_estimators=50)), LogisticRegression())\n    \n    test_benchmark = Benchmark()\n    print(""Dataset:\\t"", name)\n    print(""Pipeline:\\t"", pipeline_name)\n    starttime = datetime.datetime.now()\n    test_benchmark.run_test(pipeline, name, path)\n    endtime = datetime.datetime.now()\n    print(""Used time: "", (endtime - starttime).microseconds/1000)\n    print("""")\n\n\nif __name__ == ""__main__"":\n    LIBSVM_DATA = {\n        ""rcv1"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2"",\n        ""colon-cancer"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/covtype.libsvm.binary.bz2"",\n        ""gisette"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/gisette_scale.bz2"",\n        ""news20.binary"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/news20.binary.bz2"",\n        ""real-sim"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/real-sim.bz2"",\n        ""webspam"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/webspam_wc_normalized_trigram.svm.bz2"",\n        ""avazu"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2""\n    }\n\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--pipeline_name\', type=str, help=\'display pipeline_name.\')\n    parser.add_argument(\'--name\', type=str, help=\'display name.\')\n    parser.add_argument(\'--object\', type=str, help=\'display test object: time or memory.\')\n\n    args = parser.parse_args()\n    pipeline_name = args.pipeline_name\n    name = args.name\n    test_object = args.object\n    path = LIBSVM_DATA[name]\n\n    if test_object == \'time\':\n        test_time(pipeline_name, name, path)\n    elif test_object == \'memory\':\n        test_memory(pipeline_name, name, path)\n    else:\n        print(""Not support test object.\\t"", test_object)\n    \n    print(""Done."")\n'"
examples/feature_engineering/gradient_feature_selector/sklearn_test.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport bz2\nimport urllib.request\nimport numpy as np\n\nfrom sklearn.datasets import load_svmlight_file\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom nni.feature_engineering.gradient_selector import FeatureGradientSelector\n\n\ndef test():\n    url_zip_train = \'https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2\'\n    urllib.request.urlretrieve(url_zip_train, filename=\'train.bz2\')\n\n    f_svm = open(\'train.svm\', \'wt\')\n    with bz2.open(\'train.bz2\', \'rb\') as f_zip:\n        data = f_zip.read()\n        f_svm.write(data.decode(\'utf-8\'))\n    f_svm.close()\n\n\n    X, y = load_svmlight_file(\'train.svm\')\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n\n    pipeline = make_pipeline(FeatureGradientSelector(n_epochs=1, n_features=10), LogisticRegression())\n    # pipeline = make_pipeline(SelectFromModel(ExtraTreesClassifier(n_estimators=50)), LogisticRegression())\n\n    pipeline.fit(X_train, y_train)\n\n    print(""Pipeline Score: "", pipeline.score(X_train, y_train))\n\nif __name__ == ""__main__"":\n    test()'"
examples/feature_engineering/gradient_feature_selector/test_memory.py,0,"b'import os\n\nLIBSVM_DATA = {\n    ""rcv1"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2"",\n    ""colon-cancer"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/covtype.libsvm.binary.bz2"",\n    ""gisette"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/gisette_scale.bz2"",\n    ""news20.binary"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/news20.binary.bz2"",\n    ""real-sim"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/real-sim.bz2"",\n    ""avazu"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2"",\n}\n\npipeline_name = ""Tree""\ndevice = ""CUDA_VISIBLE_DEVICES=0 ""\nscript = ""setsid python -m memory_profiler benchmark_test.py ""\ntest_object = ""memory""\n\nfor name in LIBSVM_DATA:\n    log_name = ""_"".join([pipeline_name, name, test_object])\n    command = device + script + ""--pipeline_name "" + pipeline_name + "" --name "" + name + "" --object "" + test_object + "" >"" +log_name + "" 2>&1 &""\n    print(""command is\\t"", command)\n    os.system(command)\n    print(""log is here\\t"", log_name)\n\nprint(""Done."")\n\n\n'"
examples/feature_engineering/gradient_feature_selector/test_time.py,0,"b'import os\n\nLIBSVM_DATA = {\n    ""rcv1"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/rcv1_train.binary.bz2"",\n    ""colon-cancer"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/covtype.libsvm.binary.bz2"",\n    ""gisette"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/gisette_scale.bz2"",\n    ""news20.binary"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/news20.binary.bz2"",\n    ""real-sim"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/real-sim.bz2"",\n    ""avazu"" : ""https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/avazu-app.bz2"",\n}\n\npipeline_name = ""LR""\ndevice = ""CUDA_VISIBLE_DEVICES=0 ""\nscript = ""setsid python benchmark_test.py ""\ntest_object = ""time""\n\nfor name in LIBSVM_DATA:\n    log_name = ""_"".join([pipeline_name, name, test_object])\n    command = device + script + ""--pipeline_name "" + pipeline_name + "" --name "" + name + "" --object "" + test_object + "" >"" +log_name + "" 2>&1 &""\n    print(""command is\\t"", command)\n    os.system(command)\n    print(""log is here\\t"", log_name)\n\nprint(""Done."")\n\n\n'"
examples/model_compress/knowledge_distill/knowledge_distill.py,10,"b'import logging\nimport torch\nimport torch.nn.functional as F\n\n_logger = logging.getLogger(__name__)\n\n\nclass KnowledgeDistill():\n    """"""\n    Knowledge Distillaion support while fine-tuning the compressed model\n    Geoffrey Hinton, Oriol Vinyals, Jeff Dean\n    ""Distilling the Knowledge in a Neural Network""\n    https://arxiv.org/abs/1503.02531\n    """"""\n\n    def __init__(self, teacher_model, kd_T=1):\n        """"""\n        Parameters\n        ----------\n        teacher_model : pytorch model\n            the teacher_model for teaching the student model, it should be pretrained\n        kd_T: float\n            kd_T is the temperature parameter, when kd_T=1 we get the standard softmax function\n            As kd_T grows, the probability distribution generated by the softmax function becomes softer\n        """"""\n\n        self.teacher_model = teacher_model\n        self.kd_T = kd_T\n\n    def _get_kd_loss(self, data, student_out, teacher_out_preprocess=None):\n        """"""\n        Parameters\n        ----------\n        data : torch.Tensor\n            the input training data\n        student_out: torch.Tensor\n            output of the student network\n        teacher_out_preprocess: function\n            a function for pre-processing teacher_model\'s output\n            e.g. when teacher_out_preprocess=lambda x:x[0]\n            extract teacher_model\'s output (tensor1, tensor2)->tensor1\n\n        Returns\n        -------\n        torch.Tensor\n            weighted distillation loss\n        """"""\n\n        with torch.no_grad():\n            kd_out = self.teacher_model(data)\n        if teacher_out_preprocess is not None:\n            kd_out = teacher_out_preprocess(kd_out)\n        assert type(kd_out) is torch.Tensor\n        assert type(student_out) is torch.Tensor\n        assert kd_out.shape == student_out.shape\n        soft_log_out = F.log_softmax(student_out / self.kd_T, dim=1)\n        soft_t = F.softmax(kd_out / self.kd_T, dim=1)\n        loss_kd = F.kl_div(soft_log_out, soft_t.detach(), reduction=\'batchmean\')\n        return loss_kd\n\n    def loss(self, data, student_out):\n        """"""\n        Parameters\n        ----------\n        data : torch.Tensor\n            Input of the student model\n        student_out : torch.Tensor\n            Output of the student model\n\n        Returns\n        -------\n        torch.Tensor\n            Weighted loss of student loss and distillation loss\n        """"""\n        return self._get_kd_loss(data, student_out)\n'"
examples/nas/cdarts/aux_head.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\n\n\nclass DistillHeadCIFAR(nn.Module):\n\n    def __init__(self, C, size, num_classes, bn_affine=False):\n        """"""assuming input size 8x8 or 16x16""""""\n        super(DistillHeadCIFAR, self).__init__()\n        self.features = nn.Sequential(\n            nn.ReLU(),\n            nn.AvgPool2d(size, stride=2, padding=0, count_include_pad=False),  # image size = 2 x 2 / 6 x 6\n            nn.Conv2d(C, 128, 1, bias=False),\n            nn.BatchNorm2d(128, affine=bn_affine),\n            nn.ReLU(),\n            nn.Conv2d(128, 768, 2, bias=False),\n            nn.BatchNorm2d(768, affine=bn_affine),\n            nn.ReLU()\n        )\n        self.classifier = nn.Linear(768, num_classes)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.gap(x)\n        x = self.classifier(x.view(x.size(0), -1))\n        return x\n\n\nclass DistillHeadImagenet(nn.Module):\n\n    def __init__(self, C, size, num_classes, bn_affine=False):\n        """"""assuming input size 7x7 or 14x14""""""\n        super(DistillHeadImagenet, self).__init__()\n        self.features = nn.Sequential(\n            nn.ReLU(),\n            nn.AvgPool2d(size, stride=2, padding=0, count_include_pad=False),  # image size = 2 x 2 / 6 x 6\n            nn.Conv2d(C, 128, 1, bias=False),\n            nn.BatchNorm2d(128, affine=bn_affine),\n            nn.ReLU(),\n            nn.Conv2d(128, 768, 2, bias=False),\n            nn.BatchNorm2d(768, affine=bn_affine),\n            nn.ReLU()\n        )\n        self.classifier = nn.Linear(768, num_classes)\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.gap(x)\n        x = self.classifier(x.view(x.size(0), -1))\n        return x\n\n\nclass AuxiliaryHeadCIFAR(nn.Module):\n\n    def __init__(self, C, size=5, num_classes=10):\n        """"""assuming input size 8x8""""""\n        super(AuxiliaryHeadCIFAR, self).__init__()\n        self.features = nn.Sequential(\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False),  # image size = 2 x 2\n            nn.Conv2d(C, 128, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 768, 2, bias=False),\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.classifier = nn.Linear(768, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(x.size(0), -1))\n        return x\n\n\nclass AuxiliaryHeadImageNet(nn.Module):\n\n    def __init__(self, C, size=5, num_classes=1000):\n        """"""assuming input size 7x7""""""\n        super(AuxiliaryHeadImageNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(size, stride=2, padding=0, count_include_pad=False),\n            nn.Conv2d(C, 128, 1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 768, 2, bias=False),\n            # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.\n            # Commenting it out for consistency with the experiments in the paper.\n            # nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.classifier = nn.Linear(768, num_classes)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x.view(x.size(0), -1))\n        return x\n'"
examples/nas/cdarts/config.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nfrom functools import partial\n\n\ndef get_parser(name):\n    """""" make default formatted parser """"""\n    parser = argparse.ArgumentParser(name, formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n    # print default value always\n    parser.add_argument = partial(parser.add_argument, help=\' \')\n    return parser\n\n\nclass BaseConfig(argparse.Namespace):\n    def print_params(self, prtf=print):\n        prtf("""")\n        prtf(""Parameters:"")\n        for attr, value in sorted(vars(self).items()):\n            prtf(""{}={}"".format(attr.upper(), value))\n        prtf("""")\n\n    def as_markdown(self):\n        """""" Return configs as markdown format """"""\n        text = ""|name|value|  \\n|-|-|  \\n""\n        for attr, value in sorted(vars(self).items()):\n            text += ""|{}|{}|  \\n"".format(attr, value)\n\n        return text\n\n\nclass SearchConfig(BaseConfig):\n    def build_parser(self):\n        parser = get_parser(""Search config"")\n        ########### basic settings ############\n        parser.add_argument(\'--dataset\', default=\'cifar10\', choices=[\'cifar10\', \'cifar100\', \'imagenet\'])\n        parser.add_argument(\'--n_classes\', type=int, default=10)\n        parser.add_argument(\'--stem_multiplier\', type=int, default=3)\n        parser.add_argument(\'--init_channels\', type=int, default=16)\n        parser.add_argument(\'--data_dir\', type=str, default=\'data/cifar\', help=\'cifar dataset\')\n        parser.add_argument(\'--output_path\', type=str, default=\'./outputs\', help=\'\')\n        parser.add_argument(\'--batch_size\', type=int, default=128, help=\'batch size\')\n        parser.add_argument(\'--log_frequency\', type=int, default=10, help=\'print frequency\')\n        parser.add_argument(\'--seed\', type=int, default=0, help=\'random seed\')\n        parser.add_argument(\'--workers\', type=int, default=4, help=\'# of workers\')\n        parser.add_argument(\'--steps_per_epoch\', type=int, default=None, help=\'how many steps per epoch, use None for one pass of dataset\')\n\n        ########### learning rate ############\n        parser.add_argument(\'--w_lr\', type=float, default=0.05, help=\'lr for weights\')\n        parser.add_argument(\'--w_momentum\', type=float, default=0.9, help=\'momentum for weights\')\n        parser.add_argument(\'--w_weight_decay\', type=float, default=3e-4, help=\'weight decay for weights\')\n        parser.add_argument(\'--grad_clip\', type=float, default=5., help=\'gradient clipping for weights\')\n        parser.add_argument(\'--alpha_lr\', type=float, default=6e-4, help=\'lr for alpha\')\n        parser.add_argument(\'--alpha_weight_decay\', type=float, default=1e-3, help=\'weight decay for alpha\')\n        parser.add_argument(\'--nasnet_lr\', type=float, default=0.1, help=\'lr of nasnet\')\n\n        ########### alternate training ############\n        parser.add_argument(\'--epochs\', type=int, default=32, help=\'# of search epochs\')\n        parser.add_argument(\'--warmup_epochs\', type=int, default=2, help=\'# warmup epochs of super model\')\n        parser.add_argument(\'--loss_alpha\', type=float, default=1, help=\'loss alpha\')\n        parser.add_argument(\'--loss_T\', type=float, default=2, help=\'loss temperature\')\n        parser.add_argument(\'--interactive_type\', type=str, default=\'kl\', choices=[\'kl\', \'smoothl1\'])\n        parser.add_argument(\'--sync_bn\', action=\'store_true\', default=False, help=\'whether to sync bn\')\n        parser.add_argument(\'--use_apex\', action=\'store_true\', default=False, help=\'whether to use apex\')\n        parser.add_argument(\'--regular_ratio\', type=float, default=0.5, help=\'regular ratio\')\n        parser.add_argument(\'--regular_coeff\', type=float, default=5, help=\'regular coefficient\')\n        parser.add_argument(\'--fix_head\', action=\'store_true\', default=False, help=\'whether to fix head\')\n        parser.add_argument(\'--share_module\', action=\'store_true\', default=False, help=\'whether to share stem and aux head\')\n\n        ########### data augument ############\n        parser.add_argument(\'--aux_weight\', type=float, default=0.4, help=\'auxiliary loss weight\')\n        parser.add_argument(\'--cutout_length\', type=int, default=16, help=\'cutout length\')\n        parser.add_argument(\'--drop_path_prob\', type=float, default=0.2, help=\'drop path prob\')\n        parser.add_argument(\'--use_aa\', action=\'store_true\', default=False, help=\'whether to use aa\')\n        parser.add_argument(\'--mixup_alpha\', default=1., type=float, help=\'mixup interpolation coefficient (default: 1)\')\n\n        ########### distributed ############\n        parser.add_argument(""--local_rank"", default=0, type=int)\n        parser.add_argument(""--world_size"", default=1, type=int)\n        parser.add_argument(\'--dist_url\', default=\'tcp://127.0.0.1:23456\', type=str, help=\'url used to set up distributed training\')\n        parser.add_argument(\'--distributed\', action=\'store_true\', help=\'run model distributed mode\')\n\n        return parser\n\n    def __init__(self):\n        parser = self.build_parser()\n        args = parser.parse_args()\n        super().__init__(**vars(args))\n\n\nclass RetrainConfig(BaseConfig):\n    def build_parser(self):\n        parser = get_parser(""Retrain config"")\n        parser.add_argument(\'--dataset\', default=""cifar10"", choices=[\'cifar10\', \'cifar100\', \'imagenet\'])\n        parser.add_argument(\'--data_dir\', type=str, default=\'data/cifar\', help=\'cifar dataset\')\n        parser.add_argument(\'--output_path\', type=str, default=\'./outputs\', help=\'\')\n        parser.add_argument(""--arc_checkpoint"", default=""epoch_02.json"")\n        parser.add_argument(\'--log_frequency\', type=int, default=10, help=\'print frequency\')\n\n        ########### model settings ############\n        parser.add_argument(\'--n_classes\', type=int, default=10)\n        parser.add_argument(\'--input_channels\', type=int, default=3)\n        parser.add_argument(\'--stem_multiplier\', type=int, default=3)\n        parser.add_argument(\'--batch_size\', type=int, default=128, help=\'batch size\')\n        parser.add_argument(\'--eval_batch_size\', type=int, default=500, help=\'batch size for validation\')\n        parser.add_argument(\'--lr\', type=float, default=0.025, help=\'lr for weights\')\n        parser.add_argument(\'--momentum\', type=float, default=0.9, help=\'momentum\')\n        parser.add_argument(\'--grad_clip\', type=float, default=5., help=\'gradient clipping for weights\')\n        parser.add_argument(\'--weight_decay\', type=float, default=5e-4, help=\'weight decay\')\n        parser.add_argument(\'--epochs\', type=int, default=600, help=\'# of training epochs\')\n        parser.add_argument(\'--warmup_epochs\', type=int, default=5, help=\'# warmup\')\n        parser.add_argument(\'--init_channels\', type=int, default=36)\n        parser.add_argument(\'--layers\', type=int, default=20, help=\'# of layers\')\n        parser.add_argument(\'--seed\', type=int, default=0, help=\'random seed\')\n        parser.add_argument(\'--workers\', type=int, default=4, help=\'# of workers\')\n        parser.add_argument(\'--aux_weight\', type=float, default=0.4, help=\'auxiliary loss weight\')\n        parser.add_argument(\'--cutout_length\', type=int, default=16, help=\'cutout length\')\n        parser.add_argument(\'--label_smooth\', type=float, default=0.1, help=\'label smoothing\')\n        parser.add_argument(\'--drop_path_prob\', type=float, default=0.3, help=\'drop path prob\')\n\n        ########### data augmentation ############\n        parser.add_argument(\'--use_aa\', action=\'store_true\', default=False, help=\'whether to use aa\')\n        parser.add_argument(\'--mixup_alpha\', default=1., type=float, help=\'mixup interpolation coefficient\')\n\n        ########### distributed ############\n        parser.add_argument(""--local_rank"", default=0, type=int)\n        parser.add_argument(""--world_size"", default=1, type=int)\n        parser.add_argument(\'--dist_url\', default=\'tcp://127.0.0.1:23456\', type=str, help=\'url used to set up distributed training\')\n        parser.add_argument(\'--distributed\', action=\'store_true\', help=\'run model distributed mode\')\n\n        return parser\n\n    def __init__(self):\n        parser = self.build_parser()\n        args = parser.parse_args()\n        super().__init__(**vars(args))\n'"
examples/nas/cdarts/genotypes.py,4,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\n- Genotype: normal/reduce gene + normal/reduce cell output connection (concat)\n- gene: discrete ops information (w/o output connection)\n- dag: real ops (can be mixed or discrete, but Genotype has only discrete information itself)\n""""""\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport ops\nfrom ops import PRIMITIVES\n\nGenotype = namedtuple(\'Genotype\', \'normal normal_concat reduce reduce_concat\')\n\n\ndef to_dag(C_in, gene, reduction, bn_affine=True):\n    """""" generate discrete ops from gene """"""\n    dag = nn.ModuleList()\n    for edges in gene:\n        row = nn.ModuleList()\n        for op_name, s_idx in edges:\n            # reduction cell & from input nodes => stride = 2\n            stride = 2 if reduction and s_idx < 2 else 1\n            op = ops.OPS[op_name](C_in, stride, bn_affine)\n            if not isinstance(op, ops.Identity):  # Identity does not use drop path\n                op = nn.Sequential(\n                    op,\n                    ops.DropPath_()\n                )\n            op.s_idx = s_idx\n            row.append(op)\n        dag.append(row)\n\n    return dag\n\n\ndef from_str(s):\n    """""" generate genotype from string\n    e.g. ""Genotype(\n            normal=[[(\'sep_conv_3x3\', 0), (\'sep_conv_3x3\', 1)],\n                    [(\'sep_conv_3x3\', 1), (\'dil_conv_3x3\', 2)],\n                    [(\'sep_conv_3x3\', 1), (\'sep_conv_3x3\', 2)],\n                    [(\'sep_conv_3x3\', 1), (\'dil_conv_3x3\', 4)]],\n            normal_concat=range(2, 6),\n            reduce=[[(\'max_pool_3x3\', 0), (\'max_pool_3x3\', 1)],\n                    [(\'max_pool_3x3\', 0), (\'skip_connect\', 2)],\n                    [(\'max_pool_3x3\', 0), (\'skip_connect\', 2)],\n                    [(\'max_pool_3x3\', 0), (\'skip_connect\', 2)]],\n            reduce_concat=range(2, 6))""\n    """"""\n\n    genotype = eval(s)\n\n    return genotype\n\n\ndef parse(alpha, beta, k):\n    """"""\n    parse continuous alpha to discrete gene.\n    alpha is ParameterList:\n    ParameterList [\n        Parameter(n_edges1, n_ops),\n        Parameter(n_edges2, n_ops),\n        ...\n    ]\n\n    beta is ParameterList:\n    ParameterList [\n        Parameter(n_edges1),\n        Parameter(n_edges2),\n        ...\n    ]\n\n    gene is list:\n    [\n        [(\'node1_ops_1\', node_idx), ..., (\'node1_ops_k\', node_idx)],\n        [(\'node2_ops_1\', node_idx), ..., (\'node2_ops_k\', node_idx)],\n        ...\n    ]\n    each node has two edges (k=2) in CNN.\n    """"""\n\n    gene = []\n    assert PRIMITIVES[-1] == \'none\'  # \'none\' is implemented in mutator now\n\n    # 1) Convert the mixed op to discrete edge (single op) by choosing top-1 weight edge\n    # 2) Choose top-k edges per node by edge score (top-1 weight in edge)\n    # output the connect idx[(node_idx, connect_idx, op_idx).... () ()]\n    connect_idx = []\n    for edges, w in zip(alpha, beta):\n        # edges: Tensor(n_edges, n_ops)\n        edge_max, primitive_indices = torch.topk((w.view(-1, 1) * edges)[:, :-1], 1)  # ignore \'none\'\n        topk_edge_values, topk_edge_indices = torch.topk(edge_max.view(-1), k)\n        node_gene = []\n        node_idx = []\n        for edge_idx in topk_edge_indices:\n            prim_idx = primitive_indices[edge_idx]\n            prim = PRIMITIVES[prim_idx]\n            node_gene.append((prim, edge_idx.item()))\n            node_idx.append((edge_idx.item(), prim_idx.item()))\n\n        gene.append(node_gene)\n        connect_idx.append(node_idx)\n\n    return gene, connect_idx\n\n\ndef parse_gumbel(alpha, beta, k):\n    """"""\n    parse continuous alpha to discrete gene.\n    alpha is ParameterList:\n    ParameterList [\n        Parameter(n_edges1, n_ops),\n        Parameter(n_edges2, n_ops),\n        ...\n    ]\n\n    beta is ParameterList:\n    ParameterList [\n        Parameter(n_edges1),\n        Parameter(n_edges2),\n        ...\n    ]\n\n    gene is list:\n    [\n        [(\'node1_ops_1\', node_idx), ..., (\'node1_ops_k\', node_idx)],\n        [(\'node2_ops_1\', node_idx), ..., (\'node2_ops_k\', node_idx)],\n        ...\n    ]\n    each node has two edges (k=2) in CNN.\n    """"""\n\n    gene = []\n    assert PRIMITIVES[-1] == \'none\'  # assume last PRIMITIVE is \'none\'\n\n    # 1) Convert the mixed op to discrete edge (single op) by choosing top-1 weight edge\n    # 2) Choose top-k edges per node by edge score (top-1 weight in edge)\n    # output the connect idx[(node_idx, connect_idx, op_idx).... () ()]\n    connect_idx = []\n    for edges, w in zip(alpha, beta):\n        # edges: Tensor(n_edges, n_ops)\n        discrete_a = F.gumbel_softmax(edges[:, :-1].reshape(-1), tau=1, hard=True)\n        for i in range(k-1):\n            discrete_a = discrete_a + F.gumbel_softmax(edges[:, :-1].reshape(-1), tau=1, hard=True)\n        discrete_a = discrete_a.reshape(-1, len(PRIMITIVES)-1)\n        reserved_edge = (discrete_a > 0).nonzero()\n\n        node_gene = []\n        node_idx = []\n        for i in range(reserved_edge.shape[0]):\n            edge_idx = reserved_edge[i][0].item()\n            prim_idx = reserved_edge[i][1].item()\n            prim = PRIMITIVES[prim_idx]\n            node_gene.append((prim, edge_idx))\n            node_idx.append((edge_idx, prim_idx))\n\n        gene.append(node_gene)\n        connect_idx.append(node_idx)\n\n    return gene, connect_idx\n'"
examples/nas/cdarts/model.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport ops\nimport numpy as np\nfrom nni.nas.pytorch import mutables\nfrom utils import parse_results\nfrom aux_head import DistillHeadCIFAR, DistillHeadImagenet, AuxiliaryHeadCIFAR, AuxiliaryHeadImageNet\n\n\nclass Node(nn.Module):\n    def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect):\n        super().__init__()\n        self.ops = nn.ModuleList()\n        choice_keys = []\n        for i in range(num_prev_nodes):\n            stride = 2 if i < num_downsample_connect else 1\n            choice_keys.append(""{}_p{}"".format(node_id, i))\n            self.ops.append(mutables.LayerChoice([ops.OPS[k](channels, stride, False) for k in ops.PRIMITIVES],\n                                                 key=choice_keys[-1]))\n        self.drop_path = ops.DropPath()\n        self.input_switch = mutables.InputChoice(choose_from=choice_keys, n_chosen=2, key=""{}_switch"".format(node_id))\n\n    def forward(self, prev_nodes):\n        assert len(self.ops) == len(prev_nodes)\n        out = [op(node) for op, node in zip(self.ops, prev_nodes)]\n        out = [self.drop_path(o) if o is not None else None for o in out]\n        return self.input_switch(out)\n\n\nclass Cell(nn.Module):\n\n    def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction):\n        super().__init__()\n        self.reduction = reduction\n        self.n_nodes = n_nodes\n\n        # If previous cell is reduction cell, current input size does not match with\n        # output size of cell[k-2]. So the output[k-2] should be reduced by preprocessing.\n        if reduction_p:\n            self.preproc0 = ops.FactorizedReduce(channels_pp, channels, affine=False)\n        else:\n            self.preproc0 = ops.StdConv(channels_pp, channels, 1, 1, 0, affine=False)\n        self.preproc1 = ops.StdConv(channels_p, channels, 1, 1, 0, affine=False)\n\n        # generate dag\n        self.mutable_ops = nn.ModuleList()\n        for depth in range(2, self.n_nodes + 2):\n            self.mutable_ops.append(Node(""{}_n{}"".format(""reduce"" if reduction else ""normal"", depth),\n                                         depth, channels, 2 if reduction else 0))\n\n    def forward(self, s0, s1):\n        # s0, s1 are the outputs of previous previous cell and previous cell, respectively.\n        tensors = [self.preproc0(s0), self.preproc1(s1)]\n        for node in self.mutable_ops:\n            cur_tensor = node(tensors)\n            tensors.append(cur_tensor)\n\n        output = torch.cat(tensors[2:], dim=1)\n        return output\n\n\nclass Model(nn.Module):\n\n    def __init__(self, dataset, n_layers, in_channels=3, channels=16, n_nodes=4, retrain=False, shared_modules=None):\n        super().__init__()\n        assert dataset in [""cifar10"", ""imagenet""]\n        self.dataset = dataset\n        self.input_size = 32 if dataset == ""cifar"" else 224\n        self.in_channels = in_channels\n        self.channels = channels\n        self.n_nodes = n_nodes\n        self.aux_size = {2 * n_layers // 3: self.input_size // 4}\n        if dataset == ""cifar10"":\n            self.n_classes = 10\n            self.aux_head_class = AuxiliaryHeadCIFAR if retrain else DistillHeadCIFAR\n            if not retrain:\n                self.aux_size = {n_layers // 3: 6, 2 * n_layers // 3: 6}\n        elif dataset == ""imagenet"":\n            self.n_classes = 1000\n            self.aux_head_class = AuxiliaryHeadImageNet if retrain else DistillHeadImagenet\n            if not retrain:\n                self.aux_size = {n_layers // 3: 6, 2 * n_layers // 3: 5}\n        self.n_layers = n_layers\n        self.aux_head = nn.ModuleDict()\n        self.ensemble_param = nn.Parameter(torch.rand(len(self.aux_size) + 1) / (len(self.aux_size) + 1)) \\\n            if not retrain else None\n\n        stem_multiplier = 3 if dataset == ""cifar"" else 1\n        c_cur = stem_multiplier * self.channels\n        self.shared_modules = {}  # do not wrap with ModuleDict\n        if shared_modules is not None:\n            self.stem = shared_modules[""stem""]\n        else:\n            self.stem = nn.Sequential(\n                nn.Conv2d(in_channels, c_cur, 3, 1, 1, bias=False),\n                nn.BatchNorm2d(c_cur)\n            )\n            self.shared_modules[""stem""] = self.stem\n\n        # for the first cell, stem is used for both s0 and s1\n        # [!] channels_pp and channels_p is output channel size, but c_cur is input channel size.\n        channels_pp, channels_p, c_cur = c_cur, c_cur, channels\n\n        self.cells = nn.ModuleList()\n        reduction_p, reduction = False, False\n        aux_head_count = 0\n        for i in range(n_layers):\n            reduction_p, reduction = reduction, False\n            if i in [n_layers // 3, 2 * n_layers // 3]:\n                c_cur *= 2\n                reduction = True\n\n            cell = Cell(n_nodes, channels_pp, channels_p, c_cur, reduction_p, reduction)\n            self.cells.append(cell)\n            c_cur_out = c_cur * n_nodes\n            if i in self.aux_size:\n                if shared_modules is not None:\n                    self.aux_head[str(i)] = shared_modules[""aux"" + str(aux_head_count)]\n                else:\n                    self.aux_head[str(i)] = self.aux_head_class(c_cur_out, self.aux_size[i], self.n_classes)\n                    self.shared_modules[""aux"" + str(aux_head_count)] = self.aux_head[str(i)]\n                aux_head_count += 1\n            channels_pp, channels_p = channels_p, c_cur_out\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.linear = nn.Linear(channels_p, self.n_classes)\n\n    def forward(self, x):\n        s0 = s1 = self.stem(x)\n        outputs = []\n\n        for i, cell in enumerate(self.cells):\n            s0, s1 = s1, cell(s0, s1)\n            if str(i) in self.aux_head:\n                outputs.append(self.aux_head[str(i)](s1))\n\n        out = self.gap(s1)\n        out = out.view(out.size(0), -1)  # flatten\n        logits = self.linear(out)\n        outputs.append(logits)\n\n        if self.ensemble_param is None:\n            assert len(outputs) == 2\n            return outputs[1], outputs[0]\n        else:\n            em_output = torch.cat([(e * o) for e, o in zip(F.softmax(self.ensemble_param, dim=0), outputs)], 0)\n            return logits, em_output\n\n    def drop_path_prob(self, p):\n        for module in self.modules():\n            if isinstance(module, ops.DropPath):\n                module.p = p\n\n    def plot_genotype(self, results, logger):\n        genotypes = parse_results(results, self.n_nodes)\n        logger.info(genotypes)\n        return genotypes\n'"
examples/nas/cdarts/ops.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\nOPS = {\n    \'avg_pool_3x3\': lambda C, stride, affine: PoolWithoutBN(\'avg\', C, 3, stride, 1, affine=affine),\n    \'max_pool_3x3\': lambda C, stride, affine: PoolWithoutBN(\'max\', C, 3, stride, 1, affine=affine),\n    \'skip_connect\': lambda C, stride, affine: nn.Identity() if stride == 1 else FactorizedReduce(C, C, affine=affine),\n    \'sep_conv_3x3\': lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),\n    \'sep_conv_5x5\': lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),\n    \'sep_conv_7x7\': lambda C, stride, affine: SepConv(C, C, 7, stride, 3, affine=affine),\n    \'dil_conv_3x3\': lambda C, stride, affine: DilConv(C, C, 3, stride, 2, 2, affine=affine),  # 5x5\n    \'dil_conv_5x5\': lambda C, stride, affine: DilConv(C, C, 5, stride, 4, 2, affine=affine),  # 9x9\n    \'conv_7x1_1x7\': lambda C, stride, affine: FacConv(C, C, 7, stride, 3, affine=affine)\n}\n\nPRIMITIVES = [\n    \'max_pool_3x3\',\n    \'avg_pool_3x3\',\n    \'skip_connect\',  # identity\n    \'sep_conv_3x3\',\n    \'sep_conv_5x5\',\n    \'dil_conv_3x3\',\n    \'dil_conv_5x5\',\n]\n\n\nclass DropPath(nn.Module):\n    def __init__(self, p=0.):\n        """"""\n        Drop path with probability.\n\n        Parameters\n        ----------\n        p : float\n            Probability of an path to be zeroed.\n        """"""\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if self.training and self.p > 0.:\n            keep_prob = 1. - self.p\n            # per data point mask\n            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device).bernoulli_(keep_prob)\n            return x / keep_prob * mask\n\n        return x\n\n\nclass PoolWithoutBN(nn.Module):\n    """"""\n    AvgPool or MaxPool with BN. `pool_type` must be `max` or `avg`.\n    """"""\n\n    def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        if pool_type.lower() == \'max\':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == \'avg\':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise NotImplementedError(""Pool doesn\'t support pooling type other than max and avg."")\n\n    def forward(self, x):\n        out = self.pool(x)\n        return out\n\n\nclass StdConv(nn.Module):\n    """"""\n    Standard conv: ReLU - Conv - BN\n    """"""\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_out, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass FacConv(nn.Module):\n    """"""\n    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN\n    """"""\n\n    def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, (kernel_length, 1), stride, padding, bias=False),\n            nn.Conv2d(C_in, C_out, (1, kernel_length), stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DilConv(nn.Module):\n    """"""\n    (Dilated) depthwise separable conv.\n    ReLU - (Dilated) depthwise separable - Pointwise - BN.\n    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.\n    """"""\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, dilation=dilation, groups=C_in,\n                      bias=False),\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SepConv(nn.Module):\n    """"""\n    Depthwise separable conv.\n    DilConv(dilation=1) * 2.\n    """"""\n\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1, affine=affine),\n            DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass FactorizedReduce(nn.Module):\n    """"""\n    Reduce feature map size by factorized pointwise (stride=2).\n    """"""\n\n    def __init__(self, C_in, C_out, affine=True):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out\n'"
examples/nas/cdarts/retrain.py,10,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\nimport time\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\n\nimport apex  # pylint: disable=import-error\nimport datasets\nimport utils\nfrom apex.parallel import DistributedDataParallel  # pylint: disable=import-error\nfrom config import RetrainConfig\nfrom datasets.cifar import get_augment_datasets\nfrom model import Model\nfrom nni.nas.pytorch.fixed import apply_fixed_architecture\nfrom nni.nas.pytorch.utils import AverageMeterGroup\n\n\ndef train(logger, config, train_loader, model, optimizer, criterion, epoch, main_proc):\n    meters = AverageMeterGroup()\n    cur_lr = optimizer.param_groups[0][""lr""]\n    if main_proc:\n        logger.info(""Epoch %d LR %.6f"", epoch, cur_lr)\n\n    model.train()\n    for step, (x, y) in enumerate(train_loader):\n        x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n        optimizer.zero_grad()\n        logits, aux_logits = model(x)\n        loss = criterion(logits, y)\n        if config.aux_weight > 0.:\n            loss += config.aux_weight * criterion(aux_logits, y)\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n\n        prec1, prec5 = utils.accuracy(logits, y, topk=(1, 5))\n        metrics = {""prec1"": prec1, ""prec5"": prec5, ""loss"": loss}\n        metrics = utils.reduce_metrics(metrics, config.distributed)\n        meters.update(metrics)\n\n        if main_proc and (step % config.log_frequency == 0 or step + 1 == len(train_loader)):\n            logger.info(""Epoch [%d/%d] Step [%d/%d]  %s"", epoch + 1, config.epochs, step + 1, len(train_loader), meters)\n\n    if main_proc:\n        logger.info(""Train: [%d/%d] Final Prec@1 %.4f Prec@5 %.4f"", epoch + 1, config.epochs, meters.prec1.avg, meters.prec5.avg)\n\n\ndef validate(logger, config, valid_loader, model, criterion, epoch, main_proc):\n    meters = AverageMeterGroup()\n    model.eval()\n\n    with torch.no_grad():\n        for step, (x, y) in enumerate(valid_loader):\n            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n            logits, _ = model(x)\n            loss = criterion(logits, y)\n            prec1, prec5 = utils.accuracy(logits, y, topk=(1, 5))\n            metrics = {""prec1"": prec1, ""prec5"": prec5, ""loss"": loss}\n            metrics = utils.reduce_metrics(metrics, config.distributed)\n            meters.update(metrics)\n\n            if main_proc and (step % config.log_frequency == 0 or step + 1 == len(valid_loader)):\n                logger.info(""Epoch [%d/%d] Step [%d/%d]  %s"", epoch + 1, config.epochs, step + 1, len(valid_loader), meters)\n\n    if main_proc:\n        logger.info(""Train: [%d/%d] Final Prec@1 %.4f Prec@5 %.4f"", epoch + 1, config.epochs, meters.prec1.avg, meters.prec5.avg)\n    return meters.prec1.avg, meters.prec5.avg\n\n\ndef main():\n    config = RetrainConfig()\n    main_proc = not config.distributed or config.local_rank == 0\n    if config.distributed:\n        torch.cuda.set_device(config.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\', init_method=config.dist_url,\n                                             rank=config.local_rank, world_size=config.world_size)\n    if main_proc:\n        os.makedirs(config.output_path, exist_ok=True)\n    if config.distributed:\n        torch.distributed.barrier()\n    logger = utils.get_logger(os.path.join(config.output_path, \'search.log\'))\n    if main_proc:\n        config.print_params(logger.info)\n    utils.reset_seed(config.seed)\n\n    loaders, samplers = get_augment_datasets(config)\n    train_loader, valid_loader = loaders\n    train_sampler, valid_sampler = samplers\n\n    model = Model(config.dataset, config.layers, in_channels=config.input_channels, channels=config.init_channels, retrain=True).cuda()\n    if config.label_smooth > 0:\n        criterion = utils.CrossEntropyLabelSmooth(config.n_classes, config.label_smooth)\n    else:\n        criterion = nn.CrossEntropyLoss()\n\n    fixed_arc_path = os.path.join(config.output_path, config.arc_checkpoint)\n    with open(fixed_arc_path, ""r"") as f:\n        fixed_arc = json.load(f)\n    fixed_arc = utils.encode_tensor(fixed_arc, torch.device(""cuda""))\n    genotypes = utils.parse_results(fixed_arc, n_nodes=4)\n    genotypes_dict = {i: genotypes for i in range(3)}\n    apply_fixed_architecture(model, fixed_arc_path)\n    param_size = utils.param_size(model, criterion, [3, 32, 32] if \'cifar\' in config.dataset else [3, 224, 224])\n\n    if main_proc:\n        logger.info(""Param size: %.6f"", param_size)\n        logger.info(""Genotype: %s"", genotypes)\n\n    # change training hyper parameters according to cell type\n    if \'cifar\' in config.dataset:\n        if param_size < 3.0:\n            config.weight_decay = 3e-4\n            config.drop_path_prob = 0.2\n        elif 3.0 < param_size < 3.5:\n            config.weight_decay = 3e-4\n            config.drop_path_prob = 0.3\n        else:\n            config.weight_decay = 5e-4\n            config.drop_path_prob = 0.3\n\n    if config.distributed:\n        apex.parallel.convert_syncbn_model(model)\n        model = DistributedDataParallel(model, delay_allreduce=True)\n\n    optimizer = torch.optim.SGD(model.parameters(), config.lr, momentum=config.momentum, weight_decay=config.weight_decay)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, config.epochs, eta_min=1E-6)\n\n    best_top1 = best_top5 = 0.\n    for epoch in range(config.epochs):\n        drop_prob = config.drop_path_prob * epoch / config.epochs\n        if config.distributed:\n            model.module.drop_path_prob(drop_prob)\n        else:\n            model.drop_path_prob(drop_prob)\n        # training\n        if config.distributed:\n            train_sampler.set_epoch(epoch)\n        train(logger, config, train_loader, model, optimizer, criterion, epoch, main_proc)\n\n        # validation\n        top1, top5 = validate(logger, config, valid_loader, model, criterion, epoch, main_proc)\n        best_top1 = max(best_top1, top1)\n        best_top5 = max(best_top5, top5)\n        lr_scheduler.step()\n\n    logger.info(""Final best Prec@1 = %.4f Prec@5 = %.4f"", best_top1, best_top5)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
examples/nas/cdarts/search.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport os\nimport random\nimport time\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nimport utils\nfrom config import SearchConfig\nfrom datasets.cifar import get_search_datasets\nfrom model import Model\nfrom nni.nas.pytorch.cdarts import CdartsTrainer\n\nif __name__ == ""__main__"":\n    config = SearchConfig()\n    main_proc = not config.distributed or config.local_rank == 0\n    if config.distributed:\n        torch.cuda.set_device(config.local_rank)\n        torch.distributed.init_process_group(backend=\'nccl\', init_method=config.dist_url,\n                                             rank=config.local_rank, world_size=config.world_size)\n    if main_proc:\n        os.makedirs(config.output_path, exist_ok=True)\n    if config.distributed:\n        torch.distributed.barrier()\n    logger = utils.get_logger(os.path.join(config.output_path, \'search.log\'))\n    if main_proc:\n        config.print_params(logger.info)\n    utils.reset_seed(config.seed)\n\n    loaders, samplers = get_search_datasets(config)\n    model_small = Model(config.dataset, 8).cuda()\n    if config.share_module:\n        model_large = Model(config.dataset, 20, shared_modules=model_small.shared_modules).cuda()\n    else:\n        model_large = Model(config.dataset, 20).cuda()\n\n    criterion = nn.CrossEntropyLoss()\n    trainer = CdartsTrainer(model_small, model_large, criterion, loaders, samplers, logger,\n                            config.regular_coeff, config.regular_ratio, config.warmup_epochs, config.fix_head,\n                            config.epochs, config.steps_per_epoch, config.loss_alpha, config.loss_T, config.distributed,\n                            config.log_frequency, config.grad_clip, config.interactive_type, config.output_path,\n                            config.w_lr, config.w_momentum, config.w_weight_decay, config.alpha_lr, config.alpha_weight_decay,\n                            config.nasnet_lr, config.local_rank, config.share_module)\n    trainer.train()\n'"
examples/nas/cdarts/utils.py,12,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\nimport random\nfrom collections import namedtuple\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\n\nfrom genotypes import Genotype\nfrom ops import PRIMITIVES\nfrom nni.nas.pytorch.cdarts.utils import *\n\n\ndef get_logger(file_path):\n    """""" Make python logger """"""\n    logger = logging.getLogger(\'cdarts\')\n    log_format = \'%(asctime)s | %(message)s\'\n    formatter = logging.Formatter(log_format, datefmt=\'%m/%d %I:%M:%S %p\')\n    file_handler = logging.FileHandler(file_path)\n    file_handler.setFormatter(formatter)\n    # stream_handler = logging.StreamHandler()\n    # stream_handler.setFormatter(formatter)\n\n    logger.addHandler(file_handler)\n    # logger.addHandler(stream_handler)\n    logger.setLevel(logging.INFO)\n\n    return logger\n\n\nclass CyclicIterator:\n    def __init__(self, loader, sampler, distributed):\n        self.loader = loader\n        self.sampler = sampler\n        self.epoch = 0\n        self.distributed = distributed\n        self._next_epoch()\n\n    def _next_epoch(self):\n        if self.distributed:\n            self.sampler.set_epoch(self.epoch)\n        self.iterator = iter(self.loader)\n        self.epoch += 1\n\n    def __len__(self):\n        return len(self.loader)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            return next(self.iterator)\n        except StopIteration:\n            self._next_epoch()\n            return next(self.iterator)\n\n\nclass CrossEntropyLabelSmooth(nn.Module):\n\n    def __init__(self, num_classes, epsilon):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inputs, targets):\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n        loss = (-targets * log_probs).mean(0).sum()\n        return loss\n\ndef parse_results(results, n_nodes):\n    concat = range(2, 2 + n_nodes)\n    normal_gene = []\n    reduction_gene = []\n    for i in range(n_nodes):\n        normal_node = []\n        reduction_node = []\n        for j in range(2 + i):\n            normal_key = \'normal_n{}_p{}\'.format(i + 2, j)\n            reduction_key = \'reduce_n{}_p{}\'.format(i + 2, j)\n            normal_op = results[normal_key].cpu().numpy()\n            reduction_op = results[reduction_key].cpu().numpy()\n            if sum(normal_op == 1):\n                normal_index = np.argmax(normal_op)\n                normal_node.append((PRIMITIVES[normal_index], j))\n            if sum(reduction_op == 1):\n                reduction_index = np.argmax(reduction_op)\n                reduction_node.append((PRIMITIVES[reduction_index], j))\n        normal_gene.append(normal_node)\n        reduction_gene.append(reduction_node)\n\n    genotypes = Genotype(normal=normal_gene, normal_concat=concat,\n                         reduce=reduction_gene, reduce_concat=concat)\n    return genotypes\n\n\ndef param_size(model, loss_fn, input_size):\n    """"""\n    Compute parameter size in MB\n    """"""\n    x = torch.rand([2] + input_size).cuda()\n    y, _ = model(x)\n    target = torch.randint(model.n_classes, size=[2]).cuda()\n    loss = loss_fn(y, target)\n    loss.backward()\n    n_params = sum(np.prod(v.size()) for k, v in model.named_parameters() if not k.startswith(\'aux_head\') and v.grad is not None)\n    return n_params / 1e6\n\n\ndef encode_tensor(data, device):\n    if isinstance(data, list):\n        if all(map(lambda o: isinstance(o, bool), data)):\n            return torch.tensor(data, dtype=torch.bool, device=device)  # pylint: disable=not-callable\n        else:\n            return torch.tensor(data, dtype=torch.float, device=device)  # pylint: disable=not-callable\n    if isinstance(data, dict):\n        return {k: encode_tensor(v, device) for k, v in data.items()}\n    return data\n\n\ndef reset_seed(seed):\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n'"
examples/nas/classic_nas/mnist.py,13,"b'""""""\nA deep MNIST classifier using convolutional layers.\n\nThis file is a modification of the official pytorch mnist example:\nhttps://github.com/pytorch/examples/blob/master/mnist/main.py\n""""""\n\nimport os\nimport argparse\nimport logging\nfrom collections import OrderedDict\n\nimport nni\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\nfrom nni.nas.pytorch.classic_nas import get_and_apply_next_architecture\n\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass Net(nn.Module):\n    def __init__(self, hidden_size):\n        super(Net, self).__init__()\n        # two options of conv1\n        self.conv1 = LayerChoice(OrderedDict([\n            (""conv5x5"", nn.Conv2d(1, 20, 5, 1)),\n            (""conv3x3"", nn.Conv2d(1, 20, 3, 1))\n        ]), key=\'first_conv\')\n        # two options of mid_conv\n        self.mid_conv = LayerChoice([\n            nn.Conv2d(20, 20, 3, 1, padding=1),\n            nn.Conv2d(20, 20, 5, 1, padding=2)\n        ], key=\'mid_conv\')\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 10)\n        # skip connection over mid_conv\n        self.input_switch = InputChoice(n_candidates=2,\n                                        n_chosen=1,\n                                        key=\'skip\')\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        old_x = x\n        x = F.relu(self.mid_conv(x))\n        zero_x = torch.zeros_like(old_x)\n        skip_x = self.input_switch([zero_x, old_x])\n        x = torch.add(x, skip_x)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args[\'log_interval\'] == 0:\n            logger.info(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    accuracy = 100. * correct / len(test_loader.dataset)\n\n    logger.info(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset), accuracy))\n\n    return accuracy\n\n\ndef main(args):\n    use_cuda = not args[\'no_cuda\'] and torch.cuda.is_available()\n\n    torch.manual_seed(args[\'seed\'])\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if use_cuda else {}\n\n    #data_dir = os.path.join(args[\'data_dir\'], nni.get_trial_id())\n    data_dir = os.path.join(args[\'data_dir\'], \'data\')\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args[\'batch_size\'], shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=1000, shuffle=True, **kwargs)\n\n    hidden_size = args[\'hidden_size\']\n\n    model = Net(hidden_size=hidden_size).to(device)\n    get_and_apply_next_architecture(model)\n    optimizer = optim.SGD(model.parameters(), lr=args[\'lr\'],\n                          momentum=args[\'momentum\'])\n\n    for epoch in range(1, args[\'epochs\'] + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test_acc = test(args, model, device, test_loader)\n\n        if epoch < args[\'epochs\']:\n            # report intermediate result\n            nni.report_intermediate_result(test_acc)\n            logger.debug(\'test accuracy %g\', test_acc)\n            logger.debug(\'Pipe send intermediate result done.\')\n        else:\n            # report final result\n            nni.report_final_result(test_acc)\n            logger.debug(\'Final result is %g\', test_acc)\n            logger.debug(\'Send final result done.\')\n\n\ndef get_params():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\n    parser.add_argument(""--data_dir"", type=str,\n                        default=\'./data\', help=""data directory"")\n    parser.add_argument(\'--batch_size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(""--hidden_size"", type=int, default=512, metavar=\'N\',\n                        help=\'hidden layer size (default: 512)\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                        help=\'SGD momentum (default: 0.5)\')\n    parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                        help=\'number of epochs to train (default: 10)\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--log_interval\', type=int, default=1000, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n\n    args, _ = parser.parse_known_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    try:\n        params = vars(get_params())\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/nas/darts/datasets.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport torch\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\n\n\nclass Cutout(object):\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - self.length // 2, 0, h)\n        y2 = np.clip(y + self.length // 2, 0, h)\n        x1 = np.clip(x - self.length // 2, 0, w)\n        x2 = np.clip(x + self.length // 2, 0, w)\n\n        mask[y1: y2, x1: x2] = 0.\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n\n        return img\n\n\ndef get_dataset(cls, cutout_length=0):\n    MEAN = [0.49139968, 0.48215827, 0.44653124]\n    STD = [0.24703233, 0.24348505, 0.26158768]\n    transf = [\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip()\n    ]\n    normalize = [\n        transforms.ToTensor(),\n        transforms.Normalize(MEAN, STD)\n    ]\n    cutout = []\n    if cutout_length > 0:\n        cutout.append(Cutout(cutout_length))\n\n    train_transform = transforms.Compose(transf + normalize + cutout)\n    valid_transform = transforms.Compose(normalize)\n\n    if cls == ""cifar10"":\n        dataset_train = CIFAR10(root=""./data"", train=True, download=True, transform=train_transform)\n        dataset_valid = CIFAR10(root=""./data"", train=False, download=True, transform=valid_transform)\n    else:\n        raise NotImplementedError\n    return dataset_train, dataset_valid\n'"
examples/nas/darts/model.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom collections import OrderedDict\n\nimport torch\nimport torch.nn as nn\n\nimport ops\nfrom nni.nas.pytorch import mutables\n\n\nclass AuxiliaryHead(nn.Module):\n    """""" Auxiliary head in 2/3 place of network to let the gradient flow well """"""\n\n    def __init__(self, input_size, C, n_classes):\n        """""" assuming input size 7x7 or 8x8 """"""\n        assert input_size in [7, 8]\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(5, stride=input_size - 5, padding=0, count_include_pad=False),  # 2x2 out\n            nn.Conv2d(C, 128, kernel_size=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 768, kernel_size=2, bias=False),  # 1x1 out\n            nn.BatchNorm2d(768),\n            nn.ReLU(inplace=True)\n        )\n        self.linear = nn.Linear(768, n_classes)\n\n    def forward(self, x):\n        out = self.net(x)\n        out = out.view(out.size(0), -1)  # flatten\n        logits = self.linear(out)\n        return logits\n\n\nclass Node(nn.Module):\n    def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect):\n        super().__init__()\n        self.ops = nn.ModuleList()\n        choice_keys = []\n        for i in range(num_prev_nodes):\n            stride = 2 if i < num_downsample_connect else 1\n            choice_keys.append(""{}_p{}"".format(node_id, i))\n            self.ops.append(\n                mutables.LayerChoice(OrderedDict([\n                    (""maxpool"", ops.PoolBN(\'max\', channels, 3, stride, 1, affine=False)),\n                    (""avgpool"", ops.PoolBN(\'avg\', channels, 3, stride, 1, affine=False)),\n                    (""skipconnect"", nn.Identity() if stride == 1 else ops.FactorizedReduce(channels, channels, affine=False)),\n                    (""sepconv3x3"", ops.SepConv(channels, channels, 3, stride, 1, affine=False)),\n                    (""sepconv5x5"", ops.SepConv(channels, channels, 5, stride, 2, affine=False)),\n                    (""dilconv3x3"", ops.DilConv(channels, channels, 3, stride, 2, 2, affine=False)),\n                    (""dilconv5x5"", ops.DilConv(channels, channels, 5, stride, 4, 2, affine=False))\n                ]), key=choice_keys[-1]))\n        self.drop_path = ops.DropPath()\n        self.input_switch = mutables.InputChoice(choose_from=choice_keys, n_chosen=2, key=""{}_switch"".format(node_id))\n\n    def forward(self, prev_nodes):\n        assert len(self.ops) == len(prev_nodes)\n        out = [op(node) for op, node in zip(self.ops, prev_nodes)]\n        out = [self.drop_path(o) if o is not None else None for o in out]\n        return self.input_switch(out)\n\n\nclass Cell(nn.Module):\n\n    def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction):\n        super().__init__()\n        self.reduction = reduction\n        self.n_nodes = n_nodes\n\n        # If previous cell is reduction cell, current input size does not match with\n        # output size of cell[k-2]. So the output[k-2] should be reduced by preprocessing.\n        if reduction_p:\n            self.preproc0 = ops.FactorizedReduce(channels_pp, channels, affine=False)\n        else:\n            self.preproc0 = ops.StdConv(channels_pp, channels, 1, 1, 0, affine=False)\n        self.preproc1 = ops.StdConv(channels_p, channels, 1, 1, 0, affine=False)\n\n        # generate dag\n        self.mutable_ops = nn.ModuleList()\n        for depth in range(2, self.n_nodes + 2):\n            self.mutable_ops.append(Node(""{}_n{}"".format(""reduce"" if reduction else ""normal"", depth),\n                                         depth, channels, 2 if reduction else 0))\n\n    def forward(self, s0, s1):\n        # s0, s1 are the outputs of previous previous cell and previous cell, respectively.\n        tensors = [self.preproc0(s0), self.preproc1(s1)]\n        for node in self.mutable_ops:\n            cur_tensor = node(tensors)\n            tensors.append(cur_tensor)\n\n        output = torch.cat(tensors[2:], dim=1)\n        return output\n\n\nclass CNN(nn.Module):\n\n    def __init__(self, input_size, in_channels, channels, n_classes, n_layers, n_nodes=4,\n                 stem_multiplier=3, auxiliary=False):\n        super().__init__()\n        self.in_channels = in_channels\n        self.channels = channels\n        self.n_classes = n_classes\n        self.n_layers = n_layers\n        self.aux_pos = 2 * n_layers // 3 if auxiliary else -1\n\n        c_cur = stem_multiplier * self.channels\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, c_cur, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(c_cur)\n        )\n\n        # for the first cell, stem is used for both s0 and s1\n        # [!] channels_pp and channels_p is output channel size, but c_cur is input channel size.\n        channels_pp, channels_p, c_cur = c_cur, c_cur, channels\n\n        self.cells = nn.ModuleList()\n        reduction_p, reduction = False, False\n        for i in range(n_layers):\n            reduction_p, reduction = reduction, False\n            # Reduce featuremap size and double channels in 1/3 and 2/3 layer.\n            if i in [n_layers // 3, 2 * n_layers // 3]:\n                c_cur *= 2\n                reduction = True\n\n            cell = Cell(n_nodes, channels_pp, channels_p, c_cur, reduction_p, reduction)\n            self.cells.append(cell)\n            c_cur_out = c_cur * n_nodes\n            channels_pp, channels_p = channels_p, c_cur_out\n\n            if i == self.aux_pos:\n                self.aux_head = AuxiliaryHead(input_size // 4, channels_p, n_classes)\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.linear = nn.Linear(channels_p, n_classes)\n\n    def forward(self, x):\n        s0 = s1 = self.stem(x)\n\n        aux_logits = None\n        for i, cell in enumerate(self.cells):\n            s0, s1 = s1, cell(s0, s1)\n            if i == self.aux_pos and self.training:\n                aux_logits = self.aux_head(s1)\n\n        out = self.gap(s1)\n        out = out.view(out.size(0), -1)  # flatten\n        logits = self.linear(out)\n\n        if aux_logits is not None:\n            return logits, aux_logits\n        return logits\n\n    def drop_path_prob(self, p):\n        for module in self.modules():\n            if isinstance(module, ops.DropPath):\n                module.p = p\n'"
examples/nas/darts/ops.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\n\nclass DropPath(nn.Module):\n    def __init__(self, p=0.):\n        """"""\n        Drop path with probability.\n\n        Parameters\n        ----------\n        p : float\n            Probability of an path to be zeroed.\n        """"""\n        super().__init__()\n        self.p = p\n\n    def forward(self, x):\n        if self.training and self.p > 0.:\n            keep_prob = 1. - self.p\n            # per data point mask\n            mask = torch.zeros((x.size(0), 1, 1, 1), device=x.device).bernoulli_(keep_prob)\n            return x / keep_prob * mask\n\n        return x\n\n\nclass PoolBN(nn.Module):\n    """"""\n    AvgPool or MaxPool with BN. `pool_type` must be `max` or `avg`.\n    """"""\n    def __init__(self, pool_type, C, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        if pool_type.lower() == \'max\':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == \'avg\':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()\n\n        self.bn = nn.BatchNorm2d(C, affine=affine)\n\n    def forward(self, x):\n        out = self.pool(x)\n        out = self.bn(out)\n        return out\n\n\nclass StdConv(nn.Module):\n    """"""\n    Standard conv: ReLU - Conv - BN\n    """"""\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_out, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass FacConv(nn.Module):\n    """"""\n    Factorized conv: ReLU - Conv(Kx1) - Conv(1xK) - BN\n    """"""\n    def __init__(self, C_in, C_out, kernel_length, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, (kernel_length, 1), stride, padding, bias=False),\n            nn.Conv2d(C_in, C_out, (1, kernel_length), stride, padding, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass DilConv(nn.Module):\n    """"""\n    (Dilated) depthwise separable conv.\n    ReLU - (Dilated) depthwise separable - Pointwise - BN.\n    If dilation == 2, 3x3 conv => 5x5 receptive field, 5x5 conv => 9x9 receptive field.\n    """"""\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv2d(C_in, C_in, kernel_size, stride, padding, dilation=dilation, groups=C_in,\n                      bias=False),\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass SepConv(nn.Module):\n    """"""\n    Depthwise separable conv.\n    DilConv(dilation=1) * 2.\n    """"""\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n        super().__init__()\n        self.net = nn.Sequential(\n            DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1, affine=affine),\n            DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine)\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass FactorizedReduce(nn.Module):\n    """"""\n    Reduce feature map size by factorized pointwise (stride=2).\n    """"""\n    def __init__(self, C_in, C_out, affine=True):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        x = self.relu(x)\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out\n'"
examples/nas/darts/retrain.py,10,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport time\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport datasets\nimport utils\nfrom model import CNN\nfrom nni.nas.pytorch.fixed import apply_fixed_architecture\nfrom nni.nas.pytorch.utils import AverageMeter\n\nlogger = logging.getLogger(\'nni\')\n\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\nwriter = SummaryWriter()\n\n\ndef train(config, train_loader, model, optimizer, criterion, epoch):\n    top1 = AverageMeter(""top1"")\n    top5 = AverageMeter(""top5"")\n    losses = AverageMeter(""losses"")\n\n    cur_step = epoch * len(train_loader)\n    cur_lr = optimizer.param_groups[0][""lr""]\n    logger.info(""Epoch %d LR %.6f"", epoch, cur_lr)\n    writer.add_scalar(""lr"", cur_lr, global_step=cur_step)\n\n    model.train()\n\n    for step, (x, y) in enumerate(train_loader):\n        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n        bs = x.size(0)\n\n        optimizer.zero_grad()\n        logits, aux_logits = model(x)\n        loss = criterion(logits, y)\n        if config.aux_weight > 0.:\n            loss += config.aux_weight * criterion(aux_logits, y)\n        loss.backward()\n        # gradient clipping\n        nn.utils.clip_grad_norm_(model.parameters(), config.grad_clip)\n        optimizer.step()\n\n        accuracy = utils.accuracy(logits, y, topk=(1, 5))\n        losses.update(loss.item(), bs)\n        top1.update(accuracy[""acc1""], bs)\n        top5.update(accuracy[""acc5""], bs)\n        writer.add_scalar(""loss/train"", loss.item(), global_step=cur_step)\n        writer.add_scalar(""acc1/train"", accuracy[""acc1""], global_step=cur_step)\n        writer.add_scalar(""acc5/train"", accuracy[""acc5""], global_step=cur_step)\n\n        if step % config.log_frequency == 0 or step == len(train_loader) - 1:\n            logger.info(\n                ""Train: [{:3d}/{}] Step {:03d}/{:03d} Loss {losses.avg:.3f} ""\n                ""Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})"".format(\n                    epoch + 1, config.epochs, step, len(train_loader) - 1, losses=losses,\n                    top1=top1, top5=top5))\n\n        cur_step += 1\n\n    logger.info(""Train: [{:3d}/{}] Final Prec@1 {:.4%}"".format(epoch + 1, config.epochs, top1.avg))\n\n\ndef validate(config, valid_loader, model, criterion, epoch, cur_step):\n    top1 = AverageMeter(""top1"")\n    top5 = AverageMeter(""top5"")\n    losses = AverageMeter(""losses"")\n\n    model.eval()\n\n    with torch.no_grad():\n        for step, (X, y) in enumerate(valid_loader):\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            bs = X.size(0)\n\n            logits = model(X)\n            loss = criterion(logits, y)\n\n            accuracy = utils.accuracy(logits, y, topk=(1, 5))\n            losses.update(loss.item(), bs)\n            top1.update(accuracy[""acc1""], bs)\n            top5.update(accuracy[""acc5""], bs)\n\n            if step % config.log_frequency == 0 or step == len(valid_loader) - 1:\n                logger.info(\n                    ""Valid: [{:3d}/{}] Step {:03d}/{:03d} Loss {losses.avg:.3f} ""\n                    ""Prec@(1,5) ({top1.avg:.1%}, {top5.avg:.1%})"".format(\n                        epoch + 1, config.epochs, step, len(valid_loader) - 1, losses=losses,\n                        top1=top1, top5=top5))\n\n    writer.add_scalar(""loss/test"", losses.avg, global_step=cur_step)\n    writer.add_scalar(""acc1/test"", top1.avg, global_step=cur_step)\n    writer.add_scalar(""acc5/test"", top5.avg, global_step=cur_step)\n\n    logger.info(""Valid: [{:3d}/{}] Final Prec@1 {:.4%}"".format(epoch + 1, config.epochs, top1.avg))\n\n    return top1.avg\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""darts"")\n    parser.add_argument(""--layers"", default=20, type=int)\n    parser.add_argument(""--batch-size"", default=96, type=int)\n    parser.add_argument(""--log-frequency"", default=10, type=int)\n    parser.add_argument(""--epochs"", default=600, type=int)\n    parser.add_argument(""--aux-weight"", default=0.4, type=float)\n    parser.add_argument(""--drop-path-prob"", default=0.2, type=float)\n    parser.add_argument(""--workers"", default=4)\n    parser.add_argument(""--grad-clip"", default=5., type=float)\n    parser.add_argument(""--arc-checkpoint"", default=""./checkpoints/epoch_0.json"")\n\n    args = parser.parse_args()\n    dataset_train, dataset_valid = datasets.get_dataset(""cifar10"", cutout_length=16)\n\n    model = CNN(32, 3, 36, 10, args.layers, auxiliary=True)\n    apply_fixed_architecture(model, args.arc_checkpoint)\n    criterion = nn.CrossEntropyLoss()\n\n    model.to(device)\n    criterion.to(device)\n\n    optimizer = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, eta_min=1E-6)\n\n    train_loader = torch.utils.data.DataLoader(dataset_train,\n                                               batch_size=args.batch_size,\n                                               shuffle=True,\n                                               num_workers=args.workers,\n                                               pin_memory=True)\n    valid_loader = torch.utils.data.DataLoader(dataset_valid,\n                                               batch_size=args.batch_size,\n                                               shuffle=False,\n                                               num_workers=args.workers,\n                                               pin_memory=True)\n\n    best_top1 = 0.\n    for epoch in range(args.epochs):\n        drop_prob = args.drop_path_prob * epoch / args.epochs\n        model.drop_path_prob(drop_prob)\n\n        # training\n        train(args, train_loader, model, optimizer, criterion, epoch)\n\n        # validation\n        cur_step = (epoch + 1) * len(train_loader)\n        top1 = validate(args, valid_loader, model, criterion, epoch, cur_step)\n        best_top1 = max(best_top1, top1)\n\n        lr_scheduler.step()\n\n    logger.info(""Final best Prec@1 = {:.4%}"".format(best_top1))\n'"
examples/nas/darts/search.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport time\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\n\nimport datasets\nfrom model import CNN\nfrom nni.nas.pytorch.callbacks import ArchitectureCheckpoint, LRSchedulerCallback\nfrom nni.nas.pytorch.darts import DartsTrainer\nfrom utils import accuracy\n\nlogger = logging.getLogger(\'nni\')\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""darts"")\n    parser.add_argument(""--layers"", default=8, type=int)\n    parser.add_argument(""--batch-size"", default=64, type=int)\n    parser.add_argument(""--log-frequency"", default=10, type=int)\n    parser.add_argument(""--epochs"", default=50, type=int)\n    parser.add_argument(""--channels"", default=16, type=int)\n    parser.add_argument(""--unrolled"", default=False, action=""store_true"")\n    parser.add_argument(""--visualization"", default=False, action=""store_true"")\n    args = parser.parse_args()\n\n    dataset_train, dataset_valid = datasets.get_dataset(""cifar10"")\n\n    model = CNN(32, 3, args.channels, 10, args.layers)\n    criterion = nn.CrossEntropyLoss()\n\n    optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, args.epochs, eta_min=0.001)\n\n    trainer = DartsTrainer(model,\n                           loss=criterion,\n                           metrics=lambda output, target: accuracy(output, target, topk=(1,)),\n                           optimizer=optim,\n                           num_epochs=args.epochs,\n                           dataset_train=dataset_train,\n                           dataset_valid=dataset_valid,\n                           batch_size=args.batch_size,\n                           log_frequency=args.log_frequency,\n                           unrolled=args.unrolled,\n                           callbacks=[LRSchedulerCallback(lr_scheduler), ArchitectureCheckpoint(""./checkpoints"")])\n    if args.visualization:\n        trainer.enable_visualization()\n    trainer.train()\n'"
examples/nas/darts/utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\ndef accuracy(output, target, topk=(1,)):\n    """""" Computes the precision@k for the specified values of k """"""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    # one-hot case\n    if target.ndimension() > 1:\n        target = target.max(1)[1]\n\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = dict()\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res[""acc{}"".format(k)] = correct_k.mul_(1.0 / batch_size).item()\n    return res'"
examples/nas/enas-tf/datasets.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\n\ndef get_dataset():\n    (x_train, y_train), (x_valid, y_valid) = tf.keras.datasets.cifar10.load_data()\n    x_train, x_valid = x_train / 255.0, x_valid / 255.0\n    train_set = (x_train, y_train)\n    valid_set = (x_valid, y_valid)\n    return train_set, valid_set\n'"
examples/nas/enas-tf/macro.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import (\n    AveragePooling2D,\n    BatchNormalization,\n    Conv2D,\n    Dense,\n    Dropout,\n    GlobalAveragePooling2D,\n    MaxPool2D,\n    ReLU,\n    SeparableConv2D,\n)\n\nfrom nni.nas.tensorflow.mutables import InputChoice, LayerChoice, MutableScope\n\n\ndef build_conv(filters, kernel_size, name=None):\n    return Sequential([\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n        Conv2D(filters, kernel_size, padding='same'),\n        BatchNormalization(trainable=False),\n        ReLU(),\n    ], name)\n\ndef build_separable_conv(filters, kernel_size, name=None):\n    return Sequential([\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n        SeparableConv2D(filters, kernel_size, padding='same', use_bias=False),\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n    ], name)\n\ndef build_avg_pool(filters, name=None):\n    return Sequential([\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n        AveragePooling2D(pool_size=3, strides=1, padding='same'),\n        BatchNormalization(trainable=False),\n    ], name)\n\ndef build_max_pool(filters, name=None):\n    return Sequential([\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n        MaxPool2D(pool_size=3, strides=1, padding='same'),\n        BatchNormalization(trainable=False),\n    ], name)\n\n\nclass FactorizedReduce(Model):\n    def __init__(self, filters):\n        super().__init__()\n        self.conv1 = Conv2D(filters // 2, kernel_size=1, strides=2, use_bias=False)\n        self.conv2 = Conv2D(filters // 2, kernel_size=1, strides=2, use_bias=False)\n        self.bn = BatchNormalization(trainable=False)\n\n    def call(self, x):\n        out1 = self.conv1(x)\n        out2 = self.conv2(x[:, 1:, 1:, :])\n        out = tf.concat([out1, out2], axis=3)\n        out = self.bn(out)\n        return out\n\n\nclass ENASLayer(MutableScope):\n    def __init__(self, key, prev_labels, filters):\n        super().__init__(key)\n        self.mutable = LayerChoice([\n            build_conv(filters, 3, 'conv3'),\n            build_separable_conv(filters, 3, 'sepconv3'),\n            build_conv(filters, 5, 'conv5'),\n            build_separable_conv(filters, 5, 'sepconv5'),\n            build_avg_pool(filters, 'avgpool'),\n            build_max_pool(filters, 'maxpool'),\n        ])\n        if len(prev_labels) > 0:\n            self.skipconnect = InputChoice(choose_from=prev_labels, n_chosen=None)\n        else:\n            self.skipconnect = None\n        self.batch_norm = BatchNormalization(trainable=False)\n\n    def call(self, prev_layers):\n        out = self.mutable(prev_layers[-1])\n        if self.skipconnect is not None:\n            connection = self.skipconnect(prev_layers[:-1])\n            if connection is not None:\n                out += connection\n        return self.batch_norm(out)\n\n\nclass GeneralNetwork(Model):\n    def __init__(self, num_layers=12, filters=24, num_classes=10, dropout_rate=0.0):\n        super().__init__()\n        self.num_layers = num_layers\n\n        self.stem = Sequential([\n            Conv2D(filters, kernel_size=3, padding='same', use_bias=False),\n            BatchNormalization()\n        ])\n\n        labels = ['layer_{}'.format(i) for i in range(num_layers)]\n        self.enas_layers = []\n        for i in range(num_layers):\n            layer = ENASLayer(labels[i], labels[:i], filters)\n            self.enas_layers.append(layer)\n\n        pool_num = 2\n        self.pool_distance = num_layers // (pool_num + 1)\n        self.pool_layers = [FactorizedReduce(filters) for _ in range(pool_num)]\n\n        self.gap = GlobalAveragePooling2D()\n        self.dropout = Dropout(dropout_rate)\n        self.dense = Dense(num_classes)\n\n    def call(self, x):\n        cur = self.stem(x)\n        prev_outputs = [cur]\n\n        for i, layer in enumerate(self.enas_layers):\n            if i > 0 and i % self.pool_distance == 0:\n                pool = self.pool_layers[i // self.pool_distance - 1]\n                prev_outputs = [pool(tensor) for tensor in prev_outputs]\n                cur = prev_outputs[-1]\n\n            cur = layer(prev_outputs)\n            prev_outputs.append(cur)\n\n        cur = self.gap(cur)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n        return logits\n"""
examples/nas/enas-tf/micro.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model, Sequential\nfrom tensorflow.keras.layers import (\n    AveragePooling2D,\n    BatchNormalization,\n    Conv2D,\n    Dense,\n    Dropout,\n    GlobalAveragePooling2D,\n    MaxPool2D,\n    ReLU,\n    SeparableConv2D,\n)\n\nfrom nni.nas.tensorflow.mutables import InputChoice, LayerChoice, MutableScope\n\n\ndef build_conv_1x1(filters, name=None):\n    return Sequential([\n        Conv2D(filters, kernel_size=1, use_bias=False),\n        BatchNormalization(trainable=False),\n        ReLU(),\n    ], name)\n\ndef build_sep_conv(filters, kernel_size, name=None):\n    return Sequential([\n        ReLU(),\n        SeparableConv2D(filters, kernel_size, padding='same'),\n        BatchNormalization(trainable=True),\n    ], name)\n\n\nclass FactorizedReduce(Model):\n    def __init__(self, filters):\n        super().__init__()\n        self.conv1 = Conv2D(filters // 2, kernel_size=1, strides=2, use_bias=False)\n        self.conv2 = Conv2D(filters // 2, kernel_size=1, strides=2, use_bias=False)\n        self.bn = BatchNormalization(trainable=False)\n\n    def call(self, x):\n        out1 = self.conv1(x)\n        out2 = self.conv2(x[:, 1:, 1:, :])\n        out = tf.concat([out1, out2], axis=3)\n        out = self.bn(out)\n        return out\n\n\nclass ReductionLayer(Model):\n    def __init__(self, filters):\n        super().__init__()\n        self.reduce0 = FactorizedReduce(filters)\n        self.reduce1 = FactorizedReduce(filters)\n\n    def call(self, prevprev, prev):\n        return self.reduce0(prevprev), self.reduce1(prev)\n\n\nclass Calibration(Model):\n    def __init__(self, filters):\n        super().__init__()\n        self.filters = filters\n        self.process = None\n\n    def build(self, shape):\n        assert len(shape) == 4  # batch_size, width, height, filters\n        if shape[3] != self.filters:\n            self.process = build_conv_1x1(self.filters)\n\n    def call(self, x):\n        if self.process is None:\n            return x\n        return self.process(x)\n\n\nclass Cell(Model):\n    def __init__(self, cell_name, prev_labels, filters):\n        super().__init__()\n        self.input_choice = InputChoice(choose_from=prev_labels, n_chosen=1, return_mask=True, key=cell_name + '_input')\n        self.op_choice = LayerChoice([\n            build_sep_conv(filters, 3),\n            build_sep_conv(filters, 5),\n            AveragePooling2D(pool_size=3, strides=1, padding='same'),\n            MaxPool2D(pool_size=3, strides=1, padding='same'),\n            Sequential(),  # Identity\n        ], key=cell_name + '_op')\n\n    def call(self, prev_layers):\n        chosen_input, chosen_mask = self.input_choice(prev_layers)\n        cell_out = self.op_choice(chosen_input)\n        return cell_out, chosen_mask\n\n\nclass Node(MutableScope):\n    def __init__(self, node_name, prev_node_names, filters):\n        super().__init__(node_name)\n        self.cell_x = Cell(node_name + '_x', prev_node_names, filters)\n        self.cell_y = Cell(node_name + '_y', prev_node_names, filters)\n\n    def call(self, prev_layers):\n        out_x, mask_x = self.cell_x(prev_layers)\n        out_y, mask_y = self.cell_y(prev_layers)\n        return out_x + out_y, mask_x | mask_y\n\n\nclass ENASLayer(Model):\n    def __init__(self, num_nodes, filters, reduction):\n        super().__init__()\n        self.preproc0 = Calibration(filters)\n        self.preproc1 = Calibration(filters)\n\n        self.nodes = []\n        node_labels = [InputChoice.NO_KEY, InputChoice.NO_KEY]\n        name_prefix = 'reduce' if reduction else 'normal'\n        for i in range(num_nodes):\n            node_labels.append('{}_node_{}'.format(name_prefix, i))\n            self.nodes.append(Node(node_labels[-1], node_labels[:-1], filters))\n\n        self.conv_ops = [Conv2D(filters, kernel_size=1, padding='same', use_bias=False) for _ in range(num_nodes + 2)]\n        self.bn = BatchNormalization(trainable=False)\n\n    def call(self, prevprev, prev):\n        prev_nodes_out = [self.preproc0(prevprev), self.preproc1(prev)]\n        nodes_used_mask = tf.zeros(len(self.nodes) + 2, dtype=tf.bool)\n        for i, node in enumerate(self.nodes):\n            node_out, mask = node(prev_nodes_out)\n            nodes_used_mask |= tf.pad(mask, [[0, nodes_used_mask.shape[0] - mask.shape[0]]])\n            prev_nodes_out.append(node_out)\n\n        outputs = []\n        for used, out, conv in zip(nodes_used_mask.numpy(), prev_nodes_out, self.conv_ops):\n            if not used:\n                outputs.append(conv(out))\n        out = tf.add_n(outputs)\n        return prev, self.bn(out)\n\n\nclass MicroNetwork(Model):\n    def __init__(self, num_layers=6, num_nodes=5, out_channels=20, num_classes=10, dropout_rate=0.1):\n        super().__init__()\n        self.num_layers = num_layers\n        self.stem = Sequential([\n            Conv2D(out_channels * 3, kernel_size=3, padding='same', use_bias=False),\n            BatchNormalization(),\n        ])\n\n        pool_distance = num_layers // 3\n        pool_layer_indices = [pool_distance, 2 * pool_distance + 1]\n\n        self.enas_layers = []\n\n        filters = out_channels\n        for i in range(num_layers + 2):\n            if i in pool_layer_indices:\n                reduction = True\n                filters *= 2\n                self.enas_layers.append(ReductionLayer(filters))\n            else:\n                reduction = False\n            self.enas_layers.append(ENASLayer(num_nodes, filters, reduction))\n\n        self.gap = GlobalAveragePooling2D()\n        self.dropout = Dropout(dropout_rate)\n        self.dense = Dense(num_classes)\n\n    def call(self, x):\n        prev = cur = self.stem(x)\n        for layer in self.enas_layers:\n            prev, cur = layer(prev, cur)\n        cur = tf.keras.activations.relu(cur)\n        cur = self.gap(cur)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n        return logits\n"""
examples/nas/enas-tf/search.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\nfrom tensorflow.keras.losses import Reduction, SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import SGD\n\nfrom nni.nas.tensorflow import enas\n\nimport datasets\nfrom macro import GeneralNetwork\nfrom micro import MicroNetwork\nfrom utils import accuracy, accuracy_metrics\n\n\n# TODO: argparse\n\n\ndataset_train, dataset_valid = datasets.get_dataset()\n#model = GeneralNetwork()\nmodel = MicroNetwork()\n\nloss = SparseCategoricalCrossentropy(from_logits=True, reduction=Reduction.NONE)\noptimizer = SGD(learning_rate=0.05, momentum=0.9)\n\ntrainer = enas.EnasTrainer(model,\n                           loss=loss,\n                           metrics=accuracy_metrics,\n                           reward_function=accuracy,\n                           optimizer=optimizer,\n                           batch_size=64,\n                           num_epochs=310,\n                           dataset_train=dataset_train,\n                           dataset_valid=dataset_valid)\ntrainer.train()\n'"
examples/nas/enas-tf/utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\n\n\ndef accuracy_metrics(y_true, logits):\n    return {'enas_acc': accuracy(y_true, logits)}\n\ndef accuracy(y_true, logits):\n    # y_true: shape=(batch_size) or (batch_size,1), type=integer\n    # logits: shape=(batch_size, num_of_classes), type=float\n    # returns float\n    batch_size = y_true.shape[0]\n    y_true = tf.squeeze(y_true)\n    y_pred = tf.math.argmax(logits, axis=1)\n    y_pred = tf.cast(y_pred, y_true.dtype)\n    equal = tf.cast(y_pred == y_true, tf.int32)\n    return tf.math.reduce_sum(equal).numpy() / batch_size\n"""
examples/nas/enas/datasets.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom torchvision import transforms\nfrom torchvision.datasets import CIFAR10\n\n\ndef get_dataset(cls):\n    MEAN = [0.49139968, 0.48215827, 0.44653124]\n    STD = [0.24703233, 0.24348505, 0.26158768]\n    transf = [\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip()\n    ]\n    normalize = [\n        transforms.ToTensor(),\n        transforms.Normalize(MEAN, STD)\n    ]\n\n    train_transform = transforms.Compose(transf + normalize)\n    valid_transform = transforms.Compose(normalize)\n\n    if cls == ""cifar10"":\n        dataset_train = CIFAR10(root=""./data"", train=True, download=True, transform=train_transform)\n        dataset_valid = CIFAR10(root=""./data"", train=False, download=True, transform=valid_transform)\n    else:\n        raise NotImplementedError\n    return dataset_train, dataset_valid\n'"
examples/nas/enas/macro.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\n\nfrom nni.nas.pytorch import mutables\nfrom ops import FactorizedReduce, ConvBranch, PoolBranch\n\n\nclass ENASLayer(mutables.MutableScope):\n\n    def __init__(self, key, prev_labels, in_filters, out_filters):\n        super().__init__(key)\n        self.in_filters = in_filters\n        self.out_filters = out_filters\n        self.mutable = mutables.LayerChoice([\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=False),\n            ConvBranch(in_filters, out_filters, 3, 1, 1, separable=True),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=False),\n            ConvBranch(in_filters, out_filters, 5, 1, 2, separable=True),\n            PoolBranch(\'avg\', in_filters, out_filters, 3, 1, 1),\n            PoolBranch(\'max\', in_filters, out_filters, 3, 1, 1)\n        ])\n        if len(prev_labels) > 0:\n            self.skipconnect = mutables.InputChoice(choose_from=prev_labels, n_chosen=None)\n        else:\n            self.skipconnect = None\n        self.batch_norm = nn.BatchNorm2d(out_filters, affine=False)\n\n    def forward(self, prev_layers):\n        out = self.mutable(prev_layers[-1])\n        if self.skipconnect is not None:\n            connection = self.skipconnect(prev_layers[:-1])\n            if connection is not None:\n                out += connection\n        return self.batch_norm(out)\n\n\nclass GeneralNetwork(nn.Module):\n    def __init__(self, num_layers=12, out_filters=24, in_channels=3, num_classes=10,\n                 dropout_rate=0.0):\n        super().__init__()\n        self.num_layers = num_layers\n        self.num_classes = num_classes\n        self.out_filters = out_filters\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, out_filters, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_filters)\n        )\n\n        pool_distance = self.num_layers // 3\n        self.pool_layers_idx = [pool_distance - 1, 2 * pool_distance - 1]\n        self.dropout_rate = dropout_rate\n        self.dropout = nn.Dropout(self.dropout_rate)\n\n        self.layers = nn.ModuleList()\n        self.pool_layers = nn.ModuleList()\n        labels = []\n        for layer_id in range(self.num_layers):\n            labels.append(""layer_{}"".format(layer_id))\n            if layer_id in self.pool_layers_idx:\n                self.pool_layers.append(FactorizedReduce(self.out_filters, self.out_filters))\n            self.layers.append(ENASLayer(labels[-1], labels[:-1], self.out_filters, self.out_filters))\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Linear(self.out_filters, self.num_classes)\n\n    def forward(self, x):\n        bs = x.size(0)\n        cur = self.stem(x)\n\n        layers = [cur]\n\n        for layer_id in range(self.num_layers):\n            cur = self.layers[layer_id](layers)\n            layers.append(cur)\n            if layer_id in self.pool_layers_idx:\n                for i, layer in enumerate(layers):\n                    layers[i] = self.pool_layers[self.pool_layers_idx.index(layer_id)](layer)\n                cur = layers[-1]\n\n        cur = self.gap(cur).view(bs, -1)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n        return logits\n'"
examples/nas/enas/micro.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch import mutables\nfrom ops import FactorizedReduce, StdConv, SepConvBN, Pool\n\n\nclass AuxiliaryHead(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.pooling = nn.Sequential(\n            nn.ReLU(),\n            nn.AvgPool2d(5, 3, 2)\n        )\n        self.proj = nn.Sequential(\n            StdConv(in_channels, 128),\n            StdConv(128, 768)\n        )\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(768, 10, bias=False)\n\n    def forward(self, x):\n        bs = x.size(0)\n        x = self.pooling(x)\n        x = self.proj(x)\n        x = self.avg_pool(x).view(bs, -1)\n        x = self.fc(x)\n        return x\n\n\nclass Cell(nn.Module):\n    def __init__(self, cell_name, prev_labels, channels):\n        super().__init__()\n        self.input_choice = mutables.InputChoice(choose_from=prev_labels, n_chosen=1, return_mask=True,\n                                                 key=cell_name + ""_input"")\n        self.op_choice = mutables.LayerChoice([\n            SepConvBN(channels, channels, 3, 1),\n            SepConvBN(channels, channels, 5, 2),\n            Pool(""avg"", 3, 1, 1),\n            Pool(""max"", 3, 1, 1),\n            nn.Identity()\n        ], key=cell_name + ""_op"")\n\n    def forward(self, prev_layers):\n        chosen_input, chosen_mask = self.input_choice(prev_layers)\n        cell_out = self.op_choice(chosen_input)\n        return cell_out, chosen_mask\n\n\nclass Node(mutables.MutableScope):\n    def __init__(self, node_name, prev_node_names, channels):\n        super().__init__(node_name)\n        self.cell_x = Cell(node_name + ""_x"", prev_node_names, channels)\n        self.cell_y = Cell(node_name + ""_y"", prev_node_names, channels)\n\n    def forward(self, prev_layers):\n        out_x, mask_x = self.cell_x(prev_layers)\n        out_y, mask_y = self.cell_y(prev_layers)\n        return out_x + out_y, mask_x | mask_y\n\n\nclass Calibration(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.process = None\n        if in_channels != out_channels:\n            self.process = StdConv(in_channels, out_channels)\n    \n    def forward(self, x):\n        if self.process is None:\n            return x\n        return self.process(x)\n\n\nclass ReductionLayer(nn.Module):\n    def __init__(self, in_channels_pp, in_channels_p, out_channels):\n        super().__init__()\n        self.reduce0 = FactorizedReduce(in_channels_pp, out_channels, affine=False)\n        self.reduce1 = FactorizedReduce(in_channels_p, out_channels, affine=False)\n    \n    def forward(self, pprev, prev):\n        return self.reduce0(pprev), self.reduce1(prev)\n\n\nclass ENASLayer(nn.Module):\n    def __init__(self, num_nodes, in_channels_pp, in_channels_p, out_channels, reduction):\n        super().__init__()\n        self.preproc0 = Calibration(in_channels_pp, out_channels)\n        self.preproc1 = Calibration(in_channels_p, out_channels)\n\n        self.num_nodes = num_nodes\n        name_prefix = ""reduce"" if reduction else ""normal""\n        self.nodes = nn.ModuleList()\n        node_labels = [mutables.InputChoice.NO_KEY, mutables.InputChoice.NO_KEY]\n        for i in range(num_nodes):\n            node_labels.append(""{}_node_{}"".format(name_prefix, i))\n            self.nodes.append(Node(node_labels[-1], node_labels[:-1], out_channels))\n        self.final_conv_w = nn.Parameter(torch.zeros(out_channels, self.num_nodes + 2, out_channels, 1, 1), requires_grad=True)\n        self.bn = nn.BatchNorm2d(out_channels, affine=False)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.kaiming_normal_(self.final_conv_w)\n\n    def forward(self, pprev, prev):\n        pprev_, prev_ = self.preproc0(pprev), self.preproc1(prev)    \n\n        prev_nodes_out = [pprev_, prev_]\n        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool, device=prev.device)\n        for i in range(self.num_nodes):\n            node_out, mask = self.nodes[i](prev_nodes_out)\n            nodes_used_mask[:mask.size(0)] |= mask.to(node_out.device)\n            prev_nodes_out.append(node_out)\n\n        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask, prev_nodes_out) if not used], 1)\n        unused_nodes = F.relu(unused_nodes)\n        conv_weight = self.final_conv_w[:, ~nodes_used_mask, :, :, :]\n        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)\n        out = F.conv2d(unused_nodes, conv_weight)\n        return prev, self.bn(out)\n\n\nclass MicroNetwork(nn.Module):\n    def __init__(self, num_layers=2, num_nodes=5, out_channels=24, in_channels=3, num_classes=10,\n                 dropout_rate=0.0, use_aux_heads=False):\n        super().__init__()\n        self.num_layers = num_layers\n        self.use_aux_heads = use_aux_heads\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels * 3, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(out_channels * 3)\n        )\n\n        pool_distance = self.num_layers // 3\n        pool_layers = [pool_distance, 2 * pool_distance + 1]\n        self.dropout = nn.Dropout(dropout_rate)\n\n        self.layers = nn.ModuleList()\n        c_pp = c_p = out_channels * 3\n        c_cur = out_channels\n        for layer_id in range(self.num_layers + 2):\n            reduction = False\n            if layer_id in pool_layers:\n                c_cur, reduction = c_p * 2, True\n                self.layers.append(ReductionLayer(c_pp, c_p, c_cur))\n                c_pp = c_p = c_cur\n            self.layers.append(ENASLayer(num_nodes, c_pp, c_p, c_cur, reduction))\n            if self.use_aux_heads and layer_id == pool_layers[-1] + 1:\n                self.layers.append(AuxiliaryHead(c_cur, num_classes))\n            c_pp, c_p = c_p, c_cur\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Linear(c_cur, num_classes)\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n\n    def forward(self, x):\n        bs = x.size(0)\n        prev = cur = self.stem(x)\n        aux_logits = None\n\n        for layer in self.layers:\n            if isinstance(layer, AuxiliaryHead):\n                if self.training:\n                    aux_logits = layer(cur)\n            else:\n                prev, cur = layer(prev, cur)\n\n        cur = self.gap(F.relu(cur)).view(bs, -1)\n        cur = self.dropout(cur)\n        logits = self.dense(cur)\n\n        if aux_logits is not None:\n            return logits, aux_logits\n        return logits\n'"
examples/nas/enas/ops.py,2,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\n\nclass StdConv(nn.Module):\n    def __init__(self, C_in, C_out):\n        super(StdConv, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(C_in, C_out, 1, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass PoolBranch(nn.Module):\n    def __init__(self, pool_type, C_in, C_out, kernel_size, stride, padding, affine=False):\n        super().__init__()\n        self.preproc = StdConv(C_in, C_out)\n        self.pool = Pool(pool_type, kernel_size, stride, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        out = self.preproc(x)\n        out = self.pool(out)\n        out = self.bn(out)\n        return out\n\n\nclass SeparableConv(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding):\n        super(SeparableConv, self).__init__()\n        self.depthwise = nn.Conv2d(C_in, C_in, kernel_size=kernel_size, padding=padding, stride=stride,\n                                   groups=C_in, bias=False)\n        self.pointwise = nn.Conv2d(C_in, C_out, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.depthwise(x)\n        out = self.pointwise(out)\n        return out\n\n\nclass ConvBranch(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, stride, padding, separable):\n        super(ConvBranch, self).__init__()\n        self.preproc = StdConv(C_in, C_out)\n        if separable:\n            self.conv = SeparableConv(C_out, C_out, kernel_size, stride, padding)\n        else:\n            self.conv = nn.Conv2d(C_out, C_out, kernel_size, stride=stride, padding=padding)\n        self.postproc = nn.Sequential(\n            nn.BatchNorm2d(C_out, affine=False),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        out = self.preproc(x)\n        out = self.conv(out)\n        out = self.postproc(out)\n        return out\n\n\nclass FactorizedReduce(nn.Module):\n    def __init__(self, C_in, C_out, affine=False):\n        super().__init__()\n        self.conv1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.conv2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n        self.bn = nn.BatchNorm2d(C_out, affine=affine)\n\n    def forward(self, x):\n        out = torch.cat([self.conv1(x), self.conv2(x[:, :, 1:, 1:])], dim=1)\n        out = self.bn(out)\n        return out\n\n\nclass Pool(nn.Module):\n    def __init__(self, pool_type, kernel_size, stride, padding):\n        super().__init__()\n        if pool_type.lower() == 'max':\n            self.pool = nn.MaxPool2d(kernel_size, stride, padding)\n        elif pool_type.lower() == 'avg':\n            self.pool = nn.AvgPool2d(kernel_size, stride, padding, count_include_pad=False)\n        else:\n            raise ValueError()\n\n    def forward(self, x):\n        return self.pool(x)\n\n\nclass SepConvBN(nn.Module):\n    def __init__(self, C_in, C_out, kernel_size, padding):\n        super().__init__()\n        self.relu = nn.ReLU()\n        self.conv = SeparableConv(C_in, C_out, kernel_size, 1, padding)\n        self.bn = nn.BatchNorm2d(C_out, affine=True)\n\n    def forward(self, x):\n        x = self.relu(x)\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n"""
examples/nas/enas/search.py,4,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport time\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\n\nimport datasets\nfrom macro import GeneralNetwork\nfrom micro import MicroNetwork\nfrom nni.nas.pytorch import enas\nfrom nni.nas.pytorch.callbacks import (ArchitectureCheckpoint,\n                                       LRSchedulerCallback)\nfrom utils import accuracy, reward_accuracy\n\nlogger = logging.getLogger(\'nni\')\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""enas"")\n    parser.add_argument(""--batch-size"", default=128, type=int)\n    parser.add_argument(""--log-frequency"", default=10, type=int)\n    parser.add_argument(""--search-for"", choices=[""macro"", ""micro""], default=""macro"")\n    parser.add_argument(""--epochs"", default=None, type=int, help=""Number of epochs (default: macro 310, micro 150)"")\n    parser.add_argument(""--visualization"", default=False, action=""store_true"")\n    args = parser.parse_args()\n\n    dataset_train, dataset_valid = datasets.get_dataset(""cifar10"")\n    if args.search_for == ""macro"":\n        model = GeneralNetwork()\n        num_epochs = args.epochs or 310\n        mutator = None\n    elif args.search_for == ""micro"":\n        model = MicroNetwork(num_layers=6, out_channels=20, num_nodes=5, dropout_rate=0.1, use_aux_heads=True)\n        num_epochs = args.epochs or 150\n        mutator = enas.EnasMutator(model, tanh_constant=1.1, cell_exit_extra_step=True)\n    else:\n        raise AssertionError\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.SGD(model.parameters(), 0.05, momentum=0.9, weight_decay=1.0E-4)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=0.001)\n\n    trainer = enas.EnasTrainer(model,\n                               loss=criterion,\n                               metrics=accuracy,\n                               reward_function=reward_accuracy,\n                               optimizer=optimizer,\n                               callbacks=[LRSchedulerCallback(lr_scheduler), ArchitectureCheckpoint(""./checkpoints"")],\n                               batch_size=args.batch_size,\n                               num_epochs=num_epochs,\n                               dataset_train=dataset_train,\n                               dataset_valid=dataset_valid,\n                               log_frequency=args.log_frequency,\n                               mutator=mutator)\n    if args.visualization:\n        trainer.enable_visualization()\n    trainer.train()\n'"
examples/nas/enas/utils.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\n\n\ndef accuracy(output, target, topk=(1,)):\n    """""" Computes the precision@k for the specified values of k """"""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    # one-hot case\n    if target.ndimension() > 1:\n        target = target.max(1)[1]\n\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = dict()\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res[""acc{}"".format(k)] = correct_k.mul_(1.0 / batch_size).item()\n    return res\n\n\ndef reward_accuracy(output, target, topk=(1,)):\n    batch_size = target.size(0)\n    _, predicted = torch.max(output.data, 1)\n    return (predicted == target).sum().item() / batch_size\n'"
examples/nas/naive-tf/train.py,0,"b""import tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import (AveragePooling2D, BatchNormalization, Conv2D, Dense, MaxPool2D)\nfrom tensorflow.keras.losses import Reduction, SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import SGD\n\nfrom nni.nas.tensorflow.mutables import LayerChoice, InputChoice\nfrom nni.nas.tensorflow.enas import EnasTrainer\n\ntf.get_logger().setLevel('ERROR')\n\n\nclass Net(Model):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = LayerChoice([\n            Conv2D(6, 3, padding='same', activation='relu'),\n            Conv2D(6, 5, padding='same', activation='relu'),\n        ])\n        self.pool = MaxPool2D(2)\n        self.conv2 = LayerChoice([\n            Conv2D(16, 3, padding='same', activation='relu'),\n            Conv2D(16, 5, padding='same', activation='relu'),\n        ])\n        self.conv3 = Conv2D(16, 1)\n\n        self.skipconnect = InputChoice(n_candidates=1)\n        self.bn = BatchNormalization()\n\n        self.gap = AveragePooling2D(2)\n        self.fc1 = Dense(120, activation='relu')\n        self.fc2 = Dense(84, activation='relu')\n        self.fc3 = Dense(10)\n\n    def call(self, x):\n        bs = x.shape[0]\n\n        t = self.conv1(x)\n        x = self.pool(t)\n        x0 = self.conv2(x)\n        x1 = self.conv3(x0)\n\n        x0 = self.skipconnect([x0])\n        if x0 is not None:\n            x1 += x0\n        x = self.pool(self.bn(x1))\n\n        x = self.gap(x)\n        x = tf.reshape(x, [bs, -1])\n        x = self.fc1(x)\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return x\n\n\ndef accuracy(output, target):\n    bs = target.shape[0]\n    predicted = tf.cast(tf.argmax(output, 1), target.dtype)\n    target = tf.reshape(target, [-1])\n    return sum(tf.cast(predicted == target, tf.float32)) / bs\n\n\nif __name__ == '__main__':\n    cifar10 = tf.keras.datasets.cifar10\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n    split = int(len(x_train) * 0.9)\n    dataset_train = tf.data.Dataset.from_tensor_slices((x_train[:split], y_train[:split])).batch(64)\n    dataset_valid = tf.data.Dataset.from_tensor_slices((x_train[split:], y_train[split:])).batch(64)\n    dataset_test = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)\n\n    net = Net()\n    trainer = EnasTrainer(\n        net,\n        loss=SparseCategoricalCrossentropy(reduction=Reduction.SUM),\n        metrics=accuracy,\n        reward_function=accuracy,\n        optimizer=SGD(learning_rate=0.001, momentum=0.9),\n        batch_size=64,\n        num_epochs=2,\n        dataset_train=dataset_train,\n        dataset_valid=dataset_valid,\n        dataset_test=dataset_test\n    )\n\n    trainer.train()\n    #trainer.export('checkpoint')\n"""
examples/nas/naive/train.py,6,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\nfrom nni.nas.pytorch.darts import DartsTrainer\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(3, 6, 5, padding=2)])\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d(6, 16, 5, padding=2)])\n        self.conv3 = nn.Conv2d(16, 16, 1)\n\n        self.skipconnect = InputChoice(n_candidates=1)\n        self.bn = nn.BatchNorm2d(16)\n\n        self.gap = nn.AdaptiveAvgPool2d(4)\n        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        bs = x.size(0)\n\n        x = self.pool(F.relu(self.conv1(x)))\n        x0 = F.relu(self.conv2(x))\n        x1 = F.relu(self.conv3(x0))\n\n        x0 = self.skipconnect([x0])\n        if x0 is not None:\n            x1 += x0\n        x = self.pool(self.bn(x1))\n\n        x = self.gap(x).view(bs, -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\ndef accuracy(output, target):\n    batch_size = target.size(0)\n    _, predicted = torch.max(output.data, 1)\n    return {""acc1"": (predicted == target).sum().item() / batch_size}\n\n\nif __name__ == ""__main__"":\n    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n    dataset_train = torchvision.datasets.CIFAR10(root=""./data"", train=True, download=True, transform=transform)\n    dataset_valid = torchvision.datasets.CIFAR10(root=""./data"", train=False, download=True, transform=transform)\n\n    net = Net()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\n    trainer = DartsTrainer(net,\n                           loss=criterion,\n                           metrics=accuracy,\n                           optimizer=optimizer,\n                           num_epochs=2,\n                           dataset_train=dataset_train,\n                           dataset_valid=dataset_valid,\n                           batch_size=64,\n                           log_frequency=10)\n    trainer.enable_visualization()\n    trainer.train()\n    trainer.export(""checkpoint.json"")\n'"
examples/nas/pdarts/search.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport sys\nimport time\nfrom argparse import ArgumentParser\n\nimport torch\nimport torch.nn as nn\n\nfrom nni.nas.pytorch.callbacks import ArchitectureCheckpoint\nfrom nni.nas.pytorch.pdarts import PdartsTrainer\n\n# prevent it to be reordered.\nif True:\n    sys.path.append(\'../darts\')\n    from utils import accuracy\n    from model import CNN\n    import datasets\n\n\nlogger = logging.getLogger(\'nni\')\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""pdarts"")\n    parser.add_argument(\'--add_layers\', action=\'append\', type=int,\n                        help=\'add layers, default: [0, 6, 12]\')\n    parser.add_argument(\'--dropped_ops\', action=\'append\', type=int,\n                        help=\'drop ops, default: [3, 2, 1]\')\n    parser.add_argument(""--nodes"", default=4, type=int)\n    parser.add_argument(""--init_layers"", default=5, type=int)\n    parser.add_argument(""--channels"", default=16, type=int)\n    parser.add_argument(""--batch-size"", default=64, type=int)\n    parser.add_argument(""--log-frequency"", default=1, type=int)\n    parser.add_argument(""--epochs"", default=50, type=int)\n    parser.add_argument(""--unrolled"", default=False, action=""store_true"")\n    args = parser.parse_args()\n    if args.add_layers is None:\n        args.add_layers = [0, 6, 12]\n    if args.dropped_ops is None:\n        args.dropped_ops = [3, 2, 1]\n\n    logger.info(""loading data"")\n    dataset_train, dataset_valid = datasets.get_dataset(""cifar10"")\n\n    def model_creator(layers):\n        model = CNN(32, 3, args.channels, 10, layers, n_nodes=args.nodes)\n        criterion = nn.CrossEntropyLoss()\n\n        optim = torch.optim.SGD(model.parameters(), 0.025, momentum=0.9, weight_decay=3.0E-4)\n        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, args.epochs, eta_min=0.001)\n\n        return model, criterion, optim, lr_scheduler\n\n    logger.info(""initializing trainer"")\n    trainer = PdartsTrainer(model_creator,\n                            init_layers=args.init_layers,\n                            metrics=lambda output, target: accuracy(output, target, topk=(1,)),\n                            pdarts_num_layers=args.add_layers,\n                            pdarts_num_to_drop=args.dropped_ops,\n                            num_epochs=args.epochs,\n                            dataset_train=dataset_train,\n                            dataset_valid=dataset_valid,\n                            batch_size=args.batch_size,\n                            log_frequency=args.log_frequency,\n                            unrolled=args.unrolled,\n                            callbacks=[ArchitectureCheckpoint(""./checkpoints"")])\n    logger.info(""training"")\n    trainer.train()\n'"
examples/nas/proxylessnas/datasets.py,9,"b'import os\nimport numpy as np\nimport torch.utils.data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\ndef get_split_list(in_dim, child_num):\n    in_dim_list = [in_dim // child_num] * child_num\n    for _i in range(in_dim % child_num):\n        in_dim_list[_i] += 1\n    return in_dim_list\n\nclass DataProvider:\n    VALID_SEED = 0  # random seed for the validation set\n\n    @staticmethod\n    def name():\n        """""" Return name of the dataset """"""\n        raise NotImplementedError\n\n    @property\n    def data_shape(self):\n        """""" Return shape as python list of one data entry """"""\n        raise NotImplementedError\n\n    @property\n    def n_classes(self):\n        """""" Return `int` of num classes """"""\n        raise NotImplementedError\n\n    @property\n    def save_path(self):\n        """""" local path to save the data """"""\n        raise NotImplementedError\n\n    @property\n    def data_url(self):\n        """""" link to download the data """"""\n        raise NotImplementedError\n\n    @staticmethod\n    def random_sample_valid_set(train_labels, valid_size, n_classes):\n        train_size = len(train_labels)\n        assert train_size > valid_size\n\n        g = torch.Generator()\n        g.manual_seed(DataProvider.VALID_SEED)  # set random seed before sampling validation set\n        rand_indexes = torch.randperm(train_size, generator=g).tolist()\n\n        train_indexes, valid_indexes = [], []\n        per_class_remain = get_split_list(valid_size, n_classes)\n\n        for idx in rand_indexes:\n            label = train_labels[idx]\n            if isinstance(label, float):\n                label = int(label)\n            elif isinstance(label, np.ndarray):\n                label = np.argmax(label)\n            else:\n                assert isinstance(label, int)\n            if per_class_remain[label] > 0:\n                valid_indexes.append(idx)\n                per_class_remain[label] -= 1\n            else:\n                train_indexes.append(idx)\n        return train_indexes, valid_indexes\n\n\nclass ImagenetDataProvider(DataProvider):\n\n    def __init__(self, save_path=None, train_batch_size=256, test_batch_size=512, valid_size=None,\n                 n_worker=32, resize_scale=0.08, distort_color=None):\n\n        self._save_path = save_path\n        train_transforms = self.build_train_transform(distort_color, resize_scale)\n        train_dataset = datasets.ImageFolder(self.train_path, train_transforms)\n\n        if valid_size is not None:\n            if isinstance(valid_size, float):\n                valid_size = int(valid_size * len(train_dataset))\n            else:\n                assert isinstance(valid_size, int), \'invalid valid_size: %s\' % valid_size\n            train_indexes, valid_indexes = self.random_sample_valid_set(\n                [cls for _, cls in train_dataset.samples], valid_size, self.n_classes,\n            )\n            train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_indexes)\n            valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_indexes)\n\n            valid_dataset = datasets.ImageFolder(self.train_path, transforms.Compose([\n                transforms.Resize(self.resize_value),\n                transforms.CenterCrop(self.image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ]))\n\n            self.train = torch.utils.data.DataLoader(\n                train_dataset, batch_size=train_batch_size, sampler=train_sampler,\n                num_workers=n_worker, pin_memory=True,\n            )\n            self.valid = torch.utils.data.DataLoader(\n                valid_dataset, batch_size=test_batch_size, sampler=valid_sampler,\n                num_workers=n_worker, pin_memory=True,\n            )\n        else:\n            self.train = torch.utils.data.DataLoader(\n                train_dataset, batch_size=train_batch_size, shuffle=True,\n                num_workers=n_worker, pin_memory=True,\n            )\n            self.valid = None\n\n        self.test = torch.utils.data.DataLoader(\n            datasets.ImageFolder(self.valid_path, transforms.Compose([\n                transforms.Resize(self.resize_value),\n                transforms.CenterCrop(self.image_size),\n                transforms.ToTensor(),\n                self.normalize,\n            ])), batch_size=test_batch_size, shuffle=False, num_workers=n_worker, pin_memory=True,\n        )\n\n        if self.valid is None:\n            self.valid = self.test\n\n    @staticmethod\n    def name():\n        return \'imagenet\'\n\n    @property\n    def data_shape(self):\n        return 3, self.image_size, self.image_size  # C, H, W\n\n    @property\n    def n_classes(self):\n        return 1000\n\n    @property\n    def save_path(self):\n        if self._save_path is None:\n            self._save_path = \'/dataset/imagenet\'\n        return self._save_path\n\n    @property\n    def data_url(self):\n        raise ValueError(\'unable to download ImageNet\')\n\n    @property\n    def train_path(self):\n        return os.path.join(self.save_path, \'train\')\n\n    @property\n    def valid_path(self):\n        return os.path.join(self._save_path, \'val\')\n\n    @property\n    def normalize(self):\n        return transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    def build_train_transform(self, distort_color, resize_scale):\n        print(\'Color jitter: %s\' % distort_color)\n        if distort_color == \'strong\':\n            color_transform = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1)\n        elif distort_color == \'normal\':\n            color_transform = transforms.ColorJitter(brightness=32. / 255., saturation=0.5)\n        else:\n            color_transform = None\n        if color_transform is None:\n            train_transforms = transforms.Compose([\n                transforms.RandomResizedCrop(self.image_size, scale=(resize_scale, 1.0)),\n                transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                self.normalize,\n            ])\n        else:\n            train_transforms = transforms.Compose([\n                transforms.RandomResizedCrop(self.image_size, scale=(resize_scale, 1.0)),\n                transforms.RandomHorizontalFlip(),\n                color_transform,\n                transforms.ToTensor(),\n                self.normalize,\n            ])\n        return train_transforms\n\n    @property\n    def resize_value(self):\n        return 256\n\n    @property\n    def image_size(self):\n        return 224'"
examples/nas/proxylessnas/main.py,7,"b'import os\nimport sys\nimport logging\nfrom argparse import ArgumentParser\nimport torch\nimport datasets\n\nfrom putils import get_parameters\nfrom model import SearchMobileNet\nfrom nni.nas.pytorch.proxylessnas import ProxylessNasTrainer\nfrom retrain import Retrain\n\nlogger = logging.getLogger(\'nni_proxylessnas\')\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""proxylessnas"")\n    # configurations of the model\n    parser.add_argument(""--n_cell_stages"", default=\'4,4,4,4,4,1\', type=str)\n    parser.add_argument(""--stride_stages"", default=\'2,2,2,1,2,1\', type=str)\n    parser.add_argument(""--width_stages"", default=\'24,40,80,96,192,320\', type=str)\n    parser.add_argument(""--bn_momentum"", default=0.1, type=float)\n    parser.add_argument(""--bn_eps"", default=1e-3, type=float)\n    parser.add_argument(""--dropout_rate"", default=0, type=float)\n    parser.add_argument(""--no_decay_keys"", default=\'bn\', type=str, choices=[None, \'bn\', \'bn#bias\'])\n    # configurations of imagenet dataset\n    parser.add_argument(""--data_path"", default=\'/data/imagenet/\', type=str)\n    parser.add_argument(""--train_batch_size"", default=256, type=int)\n    parser.add_argument(""--test_batch_size"", default=500, type=int)\n    parser.add_argument(""--n_worker"", default=32, type=int)\n    parser.add_argument(""--resize_scale"", default=0.08, type=float)\n    parser.add_argument(""--distort_color"", default=\'normal\', type=str, choices=[\'normal\', \'strong\', \'None\'])\n    # configurations for training mode\n    parser.add_argument(""--train_mode"", default=\'search\', type=str, choices=[\'search\', \'retrain\'])\n    # configurations for search\n    parser.add_argument(""--checkpoint_path"", default=\'./search_mobile_net.pt\', type=str)\n    parser.add_argument(""--arch_path"", default=\'./arch_path.pt\', type=str)\n    parser.add_argument(""--no-warmup"", dest=\'warmup\', action=\'store_false\')\n    # configurations for retrain\n    parser.add_argument(""--exported_arch_path"", default=None, type=str)\n\n    args = parser.parse_args()\n    if args.train_mode == \'retrain\' and args.exported_arch_path is None:\n        logger.error(\'When --train_mode is retrain, --exported_arch_path must be specified.\')\n        sys.exit(-1)\n\n    model = SearchMobileNet(width_stages=[int(i) for i in args.width_stages.split(\',\')],\n                            n_cell_stages=[int(i) for i in args.n_cell_stages.split(\',\')],\n                            stride_stages=[int(i) for i in args.stride_stages.split(\',\')],\n                            n_classes=1000,\n                            dropout_rate=args.dropout_rate,\n                            bn_param=(args.bn_momentum, args.bn_eps))\n    logger.info(\'SearchMobileNet model create done\')\n    model.init_model()\n    logger.info(\'SearchMobileNet model init done\')\n\n    # move network to GPU if available\n    if torch.cuda.is_available():\n        device = torch.device(\'cuda\')\n    else:\n        device = torch.device(\'cpu\')\n\n    logger.info(\'Creating data provider...\')\n    data_provider = datasets.ImagenetDataProvider(save_path=args.data_path,\n                                                  train_batch_size=args.train_batch_size,\n                                                  test_batch_size=args.test_batch_size,\n                                                  valid_size=None,\n                                                  n_worker=args.n_worker,\n                                                  resize_scale=args.resize_scale,\n                                                  distort_color=args.distort_color)\n    logger.info(\'Creating data provider done\')\n\n    if args.no_decay_keys:\n        keys = args.no_decay_keys\n        momentum, nesterov = 0.9, True\n        optimizer = torch.optim.SGD([\n            {\'params\': get_parameters(model, keys, mode=\'exclude\'), \'weight_decay\': 4e-5},\n            {\'params\': get_parameters(model, keys, mode=\'include\'), \'weight_decay\': 0},\n        ], lr=0.05, momentum=momentum, nesterov=nesterov)\n    else:\n        optimizer = torch.optim.SGD(get_parameters(model), lr=0.05, momentum=momentum, nesterov=nesterov, weight_decay=4e-5)\n\n    if args.train_mode == \'search\':\n        # this is architecture search\n        logger.info(\'Creating ProxylessNasTrainer...\')\n        trainer = ProxylessNasTrainer(model,\n                                      model_optim=optimizer,\n                                      train_loader=data_provider.train,\n                                      valid_loader=data_provider.valid,\n                                      device=device,\n                                      warmup=args.warmup,\n                                      ckpt_path=args.checkpoint_path,\n                                      arch_path=args.arch_path)\n\n        logger.info(\'Start to train with ProxylessNasTrainer...\')\n        trainer.train()\n        logger.info(\'Training done\')\n        trainer.export(args.arch_path)\n        logger.info(\'Best architecture exported in %s\', args.arch_path)\n    elif args.train_mode == \'retrain\':\n        # this is retrain\n        from nni.nas.pytorch.fixed import apply_fixed_architecture\n        assert os.path.isfile(args.exported_arch_path), \\\n            ""exported_arch_path {} should be a file."".format(args.exported_arch_path)\n        apply_fixed_architecture(model, args.exported_arch_path)\n        trainer = Retrain(model, optimizer, device, data_provider, n_epochs=300)\n        trainer.run()\n'"
examples/nas/proxylessnas/model.py,1,"b'import torch\nimport torch.nn as nn\nimport math\n\nimport ops\nimport putils\nfrom nni.nas import pytorch as nas\n\nclass SearchMobileNet(nn.Module):\n    def __init__(self,\n                 width_stages=[24,40,80,96,192,320],\n                 n_cell_stages=[4,4,4,4,4,1],\n                 stride_stages=[2,2,2,1,2,1],\n                 width_mult=1, n_classes=1000,\n                 dropout_rate=0, bn_param=(0.1, 1e-3)):\n        """"""\n        Parameters\n        ----------\n        width_stages: str\n            width (output channels) of each cell stage in the block\n        n_cell_stages: str\n            number of cells in each cell stage\n        stride_strages: str\n            stride of each cell stage in the block\n        width_mult : int\n            the scale factor of width\n        """"""\n        super(SearchMobileNet, self).__init__()\n\n        input_channel = putils.make_divisible(32 * width_mult, 8)\n        first_cell_width = putils.make_divisible(16 * width_mult, 8)\n        for i in range(len(width_stages)):\n            width_stages[i] = putils.make_divisible(width_stages[i] * width_mult, 8)\n        # first conv\n        first_conv = ops.ConvLayer(3, input_channel, kernel_size=3, stride=2, use_bn=True, act_func=\'relu6\', ops_order=\'weight_bn_act\')\n        # first block\n        first_block_conv = ops.OPS[\'3x3_MBConv1\'](input_channel, first_cell_width, 1)\n        first_block = first_block_conv\n\n        input_channel = first_cell_width\n\n        blocks = [first_block]\n\n        stage_cnt = 0\n        for width, n_cell, s in zip(width_stages, n_cell_stages, stride_stages):\n            for i in range(n_cell):\n                if i == 0:\n                    stride = s\n                else:\n                    stride = 1\n                op_candidates = [ops.OPS[\'3x3_MBConv3\'](input_channel, width, stride),\n                                 ops.OPS[\'3x3_MBConv6\'](input_channel, width, stride),\n                                 ops.OPS[\'5x5_MBConv3\'](input_channel, width, stride),\n                                 ops.OPS[\'5x5_MBConv6\'](input_channel, width, stride),\n                                 ops.OPS[\'7x7_MBConv3\'](input_channel, width, stride),\n                                 ops.OPS[\'7x7_MBConv6\'](input_channel, width, stride)]\n                if stride == 1 and input_channel == width:\n                    # if it is not the first one\n                    op_candidates += [ops.OPS[\'Zero\'](input_channel, width, stride)]\n                    conv_op = nas.mutables.LayerChoice(op_candidates,\n                                                       return_mask=True,\n                                                       key=""s{}_c{}"".format(stage_cnt, i))\n                else:\n                    conv_op = nas.mutables.LayerChoice(op_candidates,\n                                                       return_mask=True,\n                                                       key=""s{}_c{}"".format(stage_cnt, i))\n                # shortcut\n                if stride == 1 and input_channel == width:\n                    # if not first cell\n                    shortcut = ops.IdentityLayer(input_channel, input_channel)\n                else:\n                    shortcut = None\n                inverted_residual_block = ops.MobileInvertedResidualBlock(conv_op, shortcut, op_candidates)\n                blocks.append(inverted_residual_block)\n                input_channel = width\n            stage_cnt += 1\n\n        # feature mix layer\n        last_channel = putils.make_devisible(1280 * width_mult, 8) if width_mult > 1.0 else 1280\n        feature_mix_layer = ops.ConvLayer(input_channel, last_channel, kernel_size=1, use_bn=True, act_func=\'relu6\', ops_order=\'weight_bn_act\', )\n        classifier = ops.LinearLayer(last_channel, n_classes, dropout_rate=dropout_rate)\n\n        self.first_conv = first_conv\n        self.blocks = nn.ModuleList(blocks)\n        self.feature_mix_layer = feature_mix_layer\n        self.global_avg_pooling = nn.AdaptiveAvgPool2d(1)\n        self.classifier = classifier\n\n        # set bn param\n        self.set_bn_param(momentum=bn_param[0], eps=bn_param[1])\n\n    def forward(self, x):\n        x = self.first_conv(x)\n        for block in self.blocks:\n            x = block(x)\n        x = self.feature_mix_layer(x)\n        x = self.global_avg_pooling(x)\n        x = x.view(x.size(0), -1)\n        x = self.classifier(x)\n        return x\n\n    def set_bn_param(self, momentum, eps):\n        for m in self.modules():\n            if isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.momentum = momentum\n                m.eps = eps\n        return\n\n    def init_model(self, model_init=\'he_fout\', init_div_groups=False):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if model_init == \'he_fout\':\n                    n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                    if init_div_groups:\n                        n /= m.groups\n                    m.weight.data.normal_(0, math.sqrt(2. / n))\n                elif model_init == \'he_fin\':\n                    n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n                    if init_div_groups:\n                        n /= m.groups\n                    m.weight.data.normal_(0, math.sqrt(2. / n))\n                else:\n                    raise NotImplementedError\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                stdv = 1. / math.sqrt(m.weight.size(1))\n                m.weight.data.uniform_(-stdv, stdv)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n'"
examples/nas/proxylessnas/ops.py,4,"b'from collections import OrderedDict\nimport torch\nimport torch.nn as nn\n\nfrom putils import get_same_padding, build_activation\n\n\nOPS = {\n    \'Identity\': lambda in_C, out_C, stride: IdentityLayer(in_C, out_C, ops_order=\'weight_bn_act\'),\n    \'Zero\': lambda in_C, out_C, stride: ZeroLayer(stride=stride),\n    \'3x3_MBConv1\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 1),\n    \'3x3_MBConv2\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 2),\n    \'3x3_MBConv3\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 3),\n    \'3x3_MBConv4\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 4),\n    \'3x3_MBConv5\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 5),\n    \'3x3_MBConv6\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 3, stride, 6),\n    \'5x5_MBConv1\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 1),\n    \'5x5_MBConv2\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 2),\n    \'5x5_MBConv3\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 3),\n    \'5x5_MBConv4\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 4),\n    \'5x5_MBConv5\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 5),\n    \'5x5_MBConv6\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 5, stride, 6),\n    \'7x7_MBConv1\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 1),\n    \'7x7_MBConv2\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 2),\n    \'7x7_MBConv3\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 3),\n    \'7x7_MBConv4\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 4),\n    \'7x7_MBConv5\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 5),\n    \'7x7_MBConv6\': lambda in_C, out_C, stride: MBInvertedConvLayer(in_C, out_C, 7, stride, 6)\n}\n\n\nclass MobileInvertedResidualBlock(nn.Module):\n    \n    def __init__(self, mobile_inverted_conv, shortcut, op_candidates_list):\n        super(MobileInvertedResidualBlock, self).__init__()\n\n        self.mobile_inverted_conv = mobile_inverted_conv\n        self.shortcut = shortcut\n        self.op_candidates_list = op_candidates_list\n\n    def forward(self, x):\n        out, idx = self.mobile_inverted_conv(x)\n        # TODO: unify idx format\n        if not isinstance(idx, int):\n            idx = (idx == 1).nonzero()\n        if self.op_candidates_list[idx].is_zero_layer():\n            res = x\n        elif self.shortcut is None:\n            res = out\n        else:\n            conv_x = out\n            skip_x = self.shortcut(x)\n            res = skip_x + conv_x\n        return res\n\n\nclass ShuffleLayer(nn.Module):\n    def __init__(self, groups):\n        super(ShuffleLayer, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        batchsize, num_channels, height, width = x.size()\n        channels_per_group = num_channels // self.groups\n        # reshape\n        x = x.view(batchsize, self.groups, channels_per_group, height, width)\n        # noinspection PyUnresolvedReferences\n        x = torch.transpose(x, 1, 2).contiguous()\n        # flatten\n        x = x.view(batchsize, -1, height, width)\n        return x\n\nclass Base2DLayer(nn.Module):\n    \n    def __init__(self, in_channels, out_channels,\n                 use_bn=True, act_func=\'relu\', dropout_rate=0, ops_order=\'weight_bn_act\'):\n        super(Base2DLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.use_bn = use_bn\n        self.act_func = act_func\n        self.dropout_rate = dropout_rate\n        self.ops_order = ops_order\n\n        """""" modules """"""\n        modules = {}\n        # batch norm\n        if self.use_bn:\n            if self.bn_before_weight:\n                modules[\'bn\'] = nn.BatchNorm2d(in_channels)\n            else:\n                modules[\'bn\'] = nn.BatchNorm2d(out_channels)\n        else:\n            modules[\'bn\'] = None\n        # activation\n        modules[\'act\'] = build_activation(self.act_func, self.ops_list[0] != \'act\')\n        # dropout\n        if self.dropout_rate > 0:\n            modules[\'dropout\'] = nn.Dropout2d(self.dropout_rate, inplace=True)\n        else:\n            modules[\'dropout\'] = None\n        # weight\n        modules[\'weight\'] = self.weight_op()\n\n        # add modules\n        for op in self.ops_list:\n            if modules[op] is None:\n                continue\n            elif op == \'weight\':\n                if modules[\'dropout\'] is not None:\n                    self.add_module(\'dropout\', modules[\'dropout\'])\n                for key in modules[\'weight\']:\n                    self.add_module(key, modules[\'weight\'][key])\n            else:\n                self.add_module(op, modules[op])\n\n    @property\n    def ops_list(self):\n        return self.ops_order.split(\'_\')\n\n    @property\n    def bn_before_weight(self):\n        for op in self.ops_list:\n            if op == \'bn\':\n                return True\n            elif op == \'weight\':\n                return False\n        raise ValueError(\'Invalid ops_order: %s\' % self.ops_order)\n\n    def weight_op(self):\n        raise NotImplementedError\n\n    def forward(self, x):\n        for module in self._modules.values():\n            x = module(x)\n        return x\n\n    @staticmethod\n    def is_zero_layer():\n        return False\n\n\nclass ConvLayer(Base2DLayer):\n\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, stride=1, dilation=1, groups=1, bias=False, has_shuffle=False,\n                 use_bn=True, act_func=\'relu\', dropout_rate=0, ops_order=\'weight_bn_act\'):\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.dilation = dilation\n        self.groups = groups\n        self.bias = bias\n        self.has_shuffle = has_shuffle\n\n        super(ConvLayer, self).__init__(in_channels, out_channels, use_bn, act_func, dropout_rate, ops_order)\n\n    def weight_op(self):\n        padding = get_same_padding(self.kernel_size)\n        if isinstance(padding, int):\n            padding *= self.dilation\n        else:\n            padding[0] *= self.dilation\n            padding[1] *= self.dilation\n\n        weight_dict = OrderedDict()\n        weight_dict[\'conv\'] = nn.Conv2d(\n            self.in_channels, self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=padding,\n            dilation=self.dilation, groups=self.groups, bias=self.bias\n        )\n        if self.has_shuffle and self.groups > 1:\n            weight_dict[\'shuffle\'] = ShuffleLayer(self.groups)\n\n        return weight_dict\n\n\nclass IdentityLayer(Base2DLayer):\n\n    def __init__(self, in_channels, out_channels,\n                 use_bn=False, act_func=None, dropout_rate=0, ops_order=\'weight_bn_act\'):\n        super(IdentityLayer, self).__init__(in_channels, out_channels, use_bn, act_func, dropout_rate, ops_order)\n\n    def weight_op(self):\n        return None\n\n\nclass LinearLayer(nn.Module):\n\n    def __init__(self, in_features, out_features, bias=True,\n                 use_bn=False, act_func=None, dropout_rate=0, ops_order=\'weight_bn_act\'):\n        super(LinearLayer, self).__init__()\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.bias = bias\n\n        self.use_bn = use_bn\n        self.act_func = act_func\n        self.dropout_rate = dropout_rate\n        self.ops_order = ops_order\n\n        """""" modules """"""\n        modules = {}\n        # batch norm\n        if self.use_bn:\n            if self.bn_before_weight:\n                modules[\'bn\'] = nn.BatchNorm1d(in_features)\n            else:\n                modules[\'bn\'] = nn.BatchNorm1d(out_features)\n        else:\n            modules[\'bn\'] = None\n        # activation\n        modules[\'act\'] = build_activation(self.act_func, self.ops_list[0] != \'act\')\n        # dropout\n        if self.dropout_rate > 0:\n            modules[\'dropout\'] = nn.Dropout(self.dropout_rate, inplace=True)\n        else:\n            modules[\'dropout\'] = None\n        # linear\n        modules[\'weight\'] = {\'linear\': nn.Linear(self.in_features, self.out_features, self.bias)}\n\n        # add modules\n        for op in self.ops_list:\n            if modules[op] is None:\n                continue\n            elif op == \'weight\':\n                if modules[\'dropout\'] is not None:\n                    self.add_module(\'dropout\', modules[\'dropout\'])\n                for key in modules[\'weight\']:\n                    self.add_module(key, modules[\'weight\'][key])\n            else:\n                self.add_module(op, modules[op])\n\n    @property\n    def ops_list(self):\n        return self.ops_order.split(\'_\')\n\n    @property\n    def bn_before_weight(self):\n        for op in self.ops_list:\n            if op == \'bn\':\n                return True\n            elif op == \'weight\':\n                return False\n        raise ValueError(\'Invalid ops_order: %s\' % self.ops_order)\n\n    def forward(self, x):\n        for module in self._modules.values():\n            x = module(x)\n        return x\n\n    @staticmethod\n    def is_zero_layer():\n        return False\n\n\nclass MBInvertedConvLayer(nn.Module):\n    """"""\n    This layer is introduced in section 4.2 in the paper https://arxiv.org/pdf/1812.00332.pdf\n    """"""\n    def __init__(self, in_channels, out_channels,\n                 kernel_size=3, stride=1, expand_ratio=6, mid_channels=None):\n        super(MBInvertedConvLayer, self).__init__()\n\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.expand_ratio = expand_ratio\n        self.mid_channels = mid_channels\n\n        if self.mid_channels is None:\n            feature_dim = round(self.in_channels * self.expand_ratio)\n        else:\n            feature_dim = self.mid_channels\n\n        if self.expand_ratio == 1:\n            self.inverted_bottleneck = None\n        else:\n            self.inverted_bottleneck = nn.Sequential(OrderedDict([\n                (\'conv\', nn.Conv2d(self.in_channels, feature_dim, 1, 1, 0, bias=False)),\n                (\'bn\', nn.BatchNorm2d(feature_dim)),\n                (\'act\', nn.ReLU6(inplace=True)),\n            ]))\n\n        pad = get_same_padding(self.kernel_size)\n        self.depth_conv = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(feature_dim, feature_dim, kernel_size, stride, pad, groups=feature_dim, bias=False)),\n            (\'bn\', nn.BatchNorm2d(feature_dim)),\n            (\'act\', nn.ReLU6(inplace=True)),\n        ]))\n\n        self.point_linear = nn.Sequential(OrderedDict([\n            (\'conv\', nn.Conv2d(feature_dim, out_channels, 1, 1, 0, bias=False)),\n            (\'bn\', nn.BatchNorm2d(out_channels)),\n        ]))\n\n    def forward(self, x):\n        if self.inverted_bottleneck:\n            x = self.inverted_bottleneck(x)\n        x = self.depth_conv(x)\n        x = self.point_linear(x)\n        return x\n\n    @staticmethod\n    def is_zero_layer():\n        return False\n\n\nclass ZeroLayer(nn.Module):\n\n    def __init__(self, stride):\n        super(ZeroLayer, self).__init__()\n        self.stride = stride\n\n    def forward(self, x):\n        \'\'\'n, c, h, w = x.size()\n        h //= self.stride\n        w //= self.stride\n        device = x.get_device() if x.is_cuda else torch.device(\'cpu\')\n        # noinspection PyUnresolvedReferences\n        padding = torch.zeros(n, c, h, w, device=device, requires_grad=False)\n        return padding\'\'\'\n        return x * 0\n\n    @staticmethod\n    def is_zero_layer():\n        return True\n'"
examples/nas/proxylessnas/putils.py,1,"b'import torch.nn as nn\n\ndef get_parameters(model, keys=None, mode=\'include\'):\n    if keys is None:\n        for name, param in model.named_parameters():\n            yield param\n    elif mode == \'include\':\n        for name, param in model.named_parameters():\n            flag = False\n            for key in keys:\n                if key in name:\n                    flag = True\n                    break\n            if flag:\n                yield param\n    elif mode == \'exclude\':\n        for name, param in model.named_parameters():\n            flag = True\n            for key in keys:\n                if key in name:\n                    flag = False\n                    break\n            if flag:\n                yield param\n    else:\n        raise ValueError(\'do not support: %s\' % mode)\n\n\ndef get_same_padding(kernel_size):\n    if isinstance(kernel_size, tuple):\n        assert len(kernel_size) == 2, \'invalid kernel size: %s\' % kernel_size\n        p1 = get_same_padding(kernel_size[0])\n        p2 = get_same_padding(kernel_size[1])\n        return p1, p2\n    assert isinstance(kernel_size, int), \'kernel size should be either `int` or `tuple`\'\n    assert kernel_size % 2 > 0, \'kernel size should be odd number\'\n    return kernel_size // 2\n\ndef build_activation(act_func, inplace=True):\n    if act_func == \'relu\':\n        return nn.ReLU(inplace=inplace)\n    elif act_func == \'relu6\':\n        return nn.ReLU6(inplace=inplace)\n    elif act_func == \'tanh\':\n        return nn.Tanh()\n    elif act_func == \'sigmoid\':\n        return nn.Sigmoid()\n    elif act_func is None:\n        return None\n    else:\n        raise ValueError(\'do not support: %s\' % act_func)\n\n\ndef make_divisible(v, divisor, min_val=None):\n    """"""\n    This function is taken from the original tf repo.\n    It ensures that all layers have a channel number that is divisible by 8\n    It can be seen here:\n    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n    """"""\n    if min_val is None:\n        min_val = divisor\n    new_v = max(min_val, int(v + divisor / 2) // divisor * divisor)\n    # Make sure that round down does not go down by more than 10%.\n    if new_v < 0.9 * v:\n        new_v += divisor\n    return new_v\n'"
examples/nas/proxylessnas/retrain.py,6,"b'import time\nimport math\nfrom datetime import timedelta\nimport torch\nfrom torch import nn as nn\nfrom nni.nas.pytorch.utils import AverageMeter\n\ndef cross_entropy_with_label_smoothing(pred, target, label_smoothing=0.1):\n    logsoftmax = nn.LogSoftmax()\n    n_classes = pred.size(1)\n    # convert to one-hot\n    target = torch.unsqueeze(target, 1)\n    soft_target = torch.zeros_like(pred)\n    soft_target.scatter_(1, target, 1)\n    # label smoothing\n    soft_target = soft_target * (1 - label_smoothing) + label_smoothing / n_classes\n    return torch.mean(torch.sum(- soft_target * logsoftmax(pred), 1))\n\ndef accuracy(output, target, topk=(1,)):\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\n\nclass Retrain:\n    def __init__(self, model, optimizer, device, data_provider, n_epochs):\n        self.model = model\n        self.optimizer = optimizer\n        self.device = device\n        self.train_loader = data_provider.train\n        self.valid_loader = data_provider.valid\n        self.test_loader = data_provider.test\n        self.n_epochs = n_epochs\n        self.criterion = nn.CrossEntropyLoss()\n\n    def run(self):\n        self.model = torch.nn.DataParallel(self.model)\n        self.model.to(self.device)\n        # train\n        self.train()\n        # validate\n        self.validate(is_test=False)\n        # test\n        self.validate(is_test=True)\n\n    def train_one_epoch(self, adjust_lr_func, train_log_func, label_smoothing=0.1):\n        batch_time = AverageMeter(\'batch_time\')\n        data_time = AverageMeter(\'data_time\')\n        losses = AverageMeter(\'losses\')\n        top1 = AverageMeter(\'top1\')\n        top5 = AverageMeter(\'top5\')\n        self.model.train()\n        end = time.time()\n        for i, (images, labels) in enumerate(self.train_loader):\n            data_time.update(time.time() - end)\n            new_lr = adjust_lr_func(i)\n            images, labels = images.to(self.device), labels.to(self.device)\n            output = self.model(images)\n            if label_smoothing > 0:\n                loss = cross_entropy_with_label_smoothing(output, labels, label_smoothing)\n            else:\n                loss = self.criterion(output, labels)\n            acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n            losses.update(loss, images.size(0))\n            top1.update(acc1[0], images.size(0))\n            top5.update(acc5[0], images.size(0))\n\n            # compute gradient and do SGD step\n            self.model.zero_grad()  # or self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            # measure elapsed time\n            batch_time.update(time.time() - end)\n            end = time.time()\n\n            if i % 10 == 0 or i + 1 == len(self.train_loader):\n                batch_log = train_log_func(i, batch_time, data_time, losses, top1, top5, new_lr)\n                print(batch_log)\n        return top1, top5\n\n    def train(self, validation_frequency=1):\n        best_acc = 0\n        nBatch = len(self.train_loader)\n\n        def train_log_func(epoch_, i, batch_time, data_time, losses, top1, top5, lr):\n                batch_log = \'Train [{0}][{1}/{2}]\\t\' \\\n                            \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' \\\n                            \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\' \\\n                            \'Loss {losses.val:.4f} ({losses.avg:.4f})\\t\' \\\n                            \'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\'. \\\n                    format(epoch_ + 1, i, nBatch - 1,\n                        batch_time=batch_time, data_time=data_time, losses=losses, top1=top1)\n                batch_log += \'\\tTop-5 acc {top5.val:.3f} ({top5.avg:.3f})\'.format(top5=top5)\n                batch_log += \'\\tlr {lr:.5f}\'.format(lr=lr)\n                return batch_log\n        \n        def adjust_learning_rate(n_epochs, optimizer, epoch, batch=0, nBatch=None):\n            """""" adjust learning of a given optimizer and return the new learning rate """"""\n            # cosine\n            T_total = n_epochs * nBatch\n            T_cur = epoch * nBatch + batch\n            # init_lr = 0.05\n            new_lr = 0.5 * 0.05 * (1 + math.cos(math.pi * T_cur / T_total))\n            for param_group in optimizer.param_groups:\n                param_group[\'lr\'] = new_lr\n            return new_lr\n\n        for epoch in range(self.n_epochs):\n            print(\'\\n\', \'-\' * 30, \'Train epoch: %d\' % (epoch + 1), \'-\' * 30, \'\\n\')\n            end = time.time()\n            train_top1, train_top5 = self.train_one_epoch(\n                lambda i: adjust_learning_rate(self.n_epochs, self.optimizer, epoch, i, nBatch),\n                lambda i, batch_time, data_time, losses, top1, top5, new_lr:\n                train_log_func(epoch, i, batch_time, data_time, losses, top1, top5, new_lr),\n            )\n            time_per_epoch = time.time() - end\n            seconds_left = int((self.n_epochs - epoch - 1) * time_per_epoch)\n            print(\'Time per epoch: %s, Est. complete in: %s\' % (\n                str(timedelta(seconds=time_per_epoch)),\n                str(timedelta(seconds=seconds_left))))\n            \n            if (epoch + 1) % validation_frequency == 0:\n                val_loss, val_acc, val_acc5 = self.validate(is_test=False)\n                is_best = val_acc > best_acc\n                best_acc = max(best_acc, val_acc)\n                val_log = \'Valid [{0}/{1}]\\tloss {2:.3f}\\ttop-1 acc {3:.3f} ({4:.3f})\'.\\\n                    format(epoch + 1, self.n_epochs, val_loss, val_acc, best_acc)\n                val_log += \'\\ttop-5 acc {0:.3f}\\tTrain top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}\'.\\\n                    format(val_acc5, top1=train_top1, top5=train_top5)\n                print(val_log)\n            else:\n                is_best = False\n\n    def validate(self, is_test=True):\n        if is_test:\n            data_loader = self.test_loader\n        else:\n            data_loader = self.valid_loader\n        self.model.eval()\n        batch_time = AverageMeter(\'batch_time\')\n        losses = AverageMeter(\'losses\')\n        top1 = AverageMeter(\'top1\')\n        top5 = AverageMeter(\'top5\')\n\n        end = time.time()\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(data_loader):\n                images, labels = images.to(self.device), labels.to(self.device)\n                # compute output\n                output = self.model(images)\n                loss = self.criterion(output, labels)\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == len(data_loader):\n                    if is_test:\n                        prefix = \'Test\'\n                    else:\n                        prefix = \'Valid\'\n                    test_log = prefix + \': [{0}/{1}]\\t\'\\\n                                        \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\\\n                                        \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\\\n                                        \'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\'.\\\n                        format(i, len(data_loader) - 1, batch_time=batch_time, loss=losses, top1=top1)\n                    test_log += \'\\tTop-5 acc {top5.val:.3f} ({top5.avg:.3f})\'.format(top5=top5)\n                    print(test_log)\n        return losses.avg, top1.avg, top5.avg'"
examples/nas/spos/blocks.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\n\nclass ShuffleNetBlock(nn.Module):\n    """"""\n    When stride = 1, the block receives input with 2 * inp channels. Otherwise inp channels.\n    """"""\n\n    def __init__(self, inp, oup, mid_channels, ksize, stride, sequence=""pdp""):\n        super().__init__()\n        assert stride in [1, 2]\n        assert ksize in [3, 5, 7]\n        self.channels = inp // 2 if stride == 1 else inp\n        self.inp = inp\n        self.oup = oup\n        self.mid_channels = mid_channels\n        self.ksize = ksize\n        self.stride = stride\n        self.pad = ksize // 2\n        self.oup_main = oup - self.channels\n        assert self.oup_main > 0\n\n        self.branch_main = nn.Sequential(*self._decode_point_depth_conv(sequence))\n\n        if stride == 2:\n            self.branch_proj = nn.Sequential(\n                # dw\n                nn.Conv2d(self.channels, self.channels, ksize, stride, self.pad,\n                          groups=self.channels, bias=False),\n                nn.BatchNorm2d(self.channels, affine=False),\n                # pw-linear\n                nn.Conv2d(self.channels, self.channels, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(self.channels, affine=False),\n                nn.ReLU(inplace=True)\n            )\n\n    def forward(self, x):\n        if self.stride == 2:\n            x_proj, x = self.branch_proj(x), x\n        else:\n            x_proj, x = self._channel_shuffle(x)\n        return torch.cat((x_proj, self.branch_main(x)), 1)\n\n    def _decode_point_depth_conv(self, sequence):\n        result = []\n        first_depth = first_point = True\n        pc = c = self.channels\n        for i, token in enumerate(sequence):\n            # compute output channels of this conv\n            if i + 1 == len(sequence):\n                assert token == ""p"", ""Last conv must be point-wise conv.""\n                c = self.oup_main\n            elif token == ""p"" and first_point:\n                c = self.mid_channels\n            if token == ""d"":\n                # depth-wise conv\n                assert pc == c, ""Depth-wise conv must not change channels.""\n                result.append(nn.Conv2d(pc, c, self.ksize, self.stride if first_depth else 1, self.pad,\n                                        groups=c, bias=False))\n                result.append(nn.BatchNorm2d(c, affine=False))\n                first_depth = False\n            elif token == ""p"":\n                # point-wise conv\n                result.append(nn.Conv2d(pc, c, 1, 1, 0, bias=False))\n                result.append(nn.BatchNorm2d(c, affine=False))\n                result.append(nn.ReLU(inplace=True))\n                first_point = False\n            else:\n                raise ValueError(""Conv sequence must be d and p."")\n            pc = c\n        return result\n\n    def _channel_shuffle(self, x):\n        bs, num_channels, height, width = x.data.size()\n        assert (num_channels % 4 == 0)\n        x = x.reshape(bs * num_channels // 2, 2, height * width)\n        x = x.permute(1, 0, 2)\n        x = x.reshape(2, -1, num_channels // 2, height, width)\n        return x[0], x[1]\n\n\nclass ShuffleXceptionBlock(ShuffleNetBlock):\n\n    def __init__(self, inp, oup, mid_channels, stride):\n        super().__init__(inp, oup, mid_channels, 3, stride, ""dpdpdp"")\n'"
examples/nas/spos/dataloader.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nimport nvidia.dali.ops as ops\nimport nvidia.dali.types as types\nimport torch.utils.data\nfrom nvidia.dali.pipeline import Pipeline\nfrom nvidia.dali.plugin.pytorch import DALIClassificationIterator\n\n\nclass HybridTrainPipe(Pipeline):\n    def __init__(self, batch_size, num_threads, device_id, data_dir, crop, seed=12, local_rank=0, world_size=1,\n                 spos_pre=False):\n        super(HybridTrainPipe, self).__init__(batch_size, num_threads, device_id, seed=seed + device_id)\n        color_space_type = types.BGR if spos_pre else types.RGB\n        self.input = ops.FileReader(file_root=data_dir, shard_id=local_rank, num_shards=world_size, random_shuffle=True)\n        self.decode = ops.ImageDecoder(device=""mixed"", output_type=color_space_type)\n        self.res = ops.RandomResizedCrop(device=""gpu"", size=crop,\n                                         interp_type=types.INTERP_LINEAR if spos_pre else types.INTERP_TRIANGULAR)\n        self.twist = ops.ColorTwist(device=""gpu"")\n        self.jitter_rng = ops.Uniform(range=[0.6, 1.4])\n        self.cmnp = ops.CropMirrorNormalize(device=""gpu"",\n                                            output_dtype=types.FLOAT,\n                                            output_layout=types.NCHW,\n                                            image_type=color_space_type,\n                                            mean=0. if spos_pre else [0.485 * 255, 0.456 * 255, 0.406 * 255],\n                                            std=1. if spos_pre else [0.229 * 255, 0.224 * 255, 0.225 * 255])\n        self.coin = ops.CoinFlip(probability=0.5)\n\n    def define_graph(self):\n        rng = self.coin()\n        self.jpegs, self.labels = self.input(name=""Reader"")\n        images = self.decode(self.jpegs)\n        images = self.res(images)\n        images = self.twist(images, saturation=self.jitter_rng(),\n                            contrast=self.jitter_rng(), brightness=self.jitter_rng())\n        output = self.cmnp(images, mirror=rng)\n        return [output, self.labels]\n\n\nclass HybridValPipe(Pipeline):\n    def __init__(self, batch_size, num_threads, device_id, data_dir, crop, size, seed=12, local_rank=0, world_size=1,\n                 spos_pre=False, shuffle=False):\n        super(HybridValPipe, self).__init__(batch_size, num_threads, device_id, seed=seed + device_id)\n        color_space_type = types.BGR if spos_pre else types.RGB\n        self.input = ops.FileReader(file_root=data_dir, shard_id=local_rank, num_shards=world_size,\n                                    random_shuffle=shuffle)\n        self.decode = ops.ImageDecoder(device=""mixed"", output_type=color_space_type)\n        self.res = ops.Resize(device=""gpu"", resize_shorter=size,\n                              interp_type=types.INTERP_LINEAR if spos_pre else types.INTERP_TRIANGULAR)\n        self.cmnp = ops.CropMirrorNormalize(device=""gpu"",\n                                            output_dtype=types.FLOAT,\n                                            output_layout=types.NCHW,\n                                            crop=(crop, crop),\n                                            image_type=color_space_type,\n                                            mean=0. if spos_pre else [0.485 * 255, 0.456 * 255, 0.406 * 255],\n                                            std=1. if spos_pre else [0.229 * 255, 0.224 * 255, 0.225 * 255])\n\n    def define_graph(self):\n        self.jpegs, self.labels = self.input(name=""Reader"")\n        images = self.decode(self.jpegs)\n        images = self.res(images)\n        output = self.cmnp(images)\n        return [output, self.labels]\n\n\nclass ClassificationWrapper:\n    def __init__(self, loader, size):\n        self.loader = loader\n        self.size = size\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        data = next(self.loader)\n        return data[0][""data""], data[0][""label""].view(-1).long().cuda(non_blocking=True)\n\n    def __len__(self):\n        return self.size\n\n\ndef get_imagenet_iter_dali(split, image_dir, batch_size, num_threads, crop=224, val_size=256,\n                           spos_preprocessing=False, seed=12, shuffle=False, device_id=None):\n    world_size, local_rank = 1, 0\n    if device_id is None:\n        device_id = torch.cuda.device_count() - 1  # use last gpu\n    if split == ""train"":\n        pipeline = HybridTrainPipe(batch_size=batch_size, num_threads=num_threads, device_id=device_id,\n                                   data_dir=os.path.join(image_dir, ""train""), seed=seed,\n                                   crop=crop, world_size=world_size, local_rank=local_rank,\n                                   spos_pre=spos_preprocessing)\n    elif split == ""val"":\n        pipeline = HybridValPipe(batch_size=batch_size, num_threads=num_threads, device_id=device_id,\n                                 data_dir=os.path.join(image_dir, ""val""), seed=seed,\n                                 crop=crop, size=val_size, world_size=world_size, local_rank=local_rank,\n                                 spos_pre=spos_preprocessing, shuffle=shuffle)\n    else:\n        raise AssertionError\n    pipeline.build()\n    num_samples = pipeline.epoch_size(""Reader"")\n    return ClassificationWrapper(\n        DALIClassificationIterator(pipeline, size=num_samples, fill_last_batch=split == ""train"",\n                                   auto_reset=True), (num_samples + batch_size - 1) // batch_size)\n'"
examples/nas/spos/network.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport pickle\nimport re\n\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch import mutables\n\nfrom blocks import ShuffleNetBlock, ShuffleXceptionBlock\n\n\nclass ShuffleNetV2OneShot(nn.Module):\n    block_keys = [\n        \'shufflenet_3x3\',\n        \'shufflenet_5x5\',\n        \'shufflenet_7x7\',\n        \'xception_3x3\',\n    ]\n\n    def __init__(self, input_size=224, first_conv_channels=16, last_conv_channels=1024, n_classes=1000,\n                 op_flops_path=""./data/op_flops_dict.pkl""):\n        super().__init__()\n\n        assert input_size % 32 == 0\n        with open(os.path.join(os.path.dirname(__file__), op_flops_path), ""rb"") as fp:\n            self._op_flops_dict = pickle.load(fp)\n\n        self.stage_blocks = [4, 4, 8, 4]\n        self.stage_channels = [64, 160, 320, 640]\n        self._parsed_flops = dict()\n        self._input_size = input_size\n        self._feature_map_size = input_size\n        self._first_conv_channels = first_conv_channels\n        self._last_conv_channels = last_conv_channels\n        self._n_classes = n_classes\n\n        # building first layer\n        self.first_conv = nn.Sequential(\n            nn.Conv2d(3, first_conv_channels, 3, 2, 1, bias=False),\n            nn.BatchNorm2d(first_conv_channels, affine=False),\n            nn.ReLU(inplace=True),\n        )\n        self._feature_map_size //= 2\n\n        p_channels = first_conv_channels\n        features = []\n        for num_blocks, channels in zip(self.stage_blocks, self.stage_channels):\n            features.extend(self._make_blocks(num_blocks, p_channels, channels))\n            p_channels = channels\n        self.features = nn.Sequential(*features)\n\n        self.conv_last = nn.Sequential(\n            nn.Conv2d(p_channels, last_conv_channels, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(last_conv_channels, affine=False),\n            nn.ReLU(inplace=True),\n        )\n        self.globalpool = nn.AvgPool2d(self._feature_map_size)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Sequential(\n            nn.Linear(last_conv_channels, n_classes, bias=False),\n        )\n\n        self._initialize_weights()\n\n    def _make_blocks(self, blocks, in_channels, channels):\n        result = []\n        for i in range(blocks):\n            stride = 2 if i == 0 else 1\n            inp = in_channels if i == 0 else channels\n            oup = channels\n\n            base_mid_channels = channels // 2\n            mid_channels = int(base_mid_channels)  # prepare for scale\n            choice_block = mutables.LayerChoice([\n                ShuffleNetBlock(inp, oup, mid_channels=mid_channels, ksize=3, stride=stride),\n                ShuffleNetBlock(inp, oup, mid_channels=mid_channels, ksize=5, stride=stride),\n                ShuffleNetBlock(inp, oup, mid_channels=mid_channels, ksize=7, stride=stride),\n                ShuffleXceptionBlock(inp, oup, mid_channels=mid_channels, stride=stride)\n            ])\n            result.append(choice_block)\n\n            # find the corresponding flops\n            flop_key = (inp, oup, mid_channels, self._feature_map_size, self._feature_map_size, stride)\n            self._parsed_flops[choice_block.key] = [\n                self._op_flops_dict[""{}_stride_{}"".format(k, stride)][flop_key] for k in self.block_keys\n            ]\n            if stride == 2:\n                self._feature_map_size //= 2\n        return result\n\n    def forward(self, x):\n        bs = x.size(0)\n        x = self.first_conv(x)\n        x = self.features(x)\n        x = self.conv_last(x)\n        x = self.globalpool(x)\n\n        x = self.dropout(x)\n        x = x.contiguous().view(bs, -1)\n        x = self.classifier(x)\n        return x\n\n    def get_candidate_flops(self, candidate):\n        conv1_flops = self._op_flops_dict[""conv1""][(3, self._first_conv_channels,\n                                                    self._input_size, self._input_size, 2)]\n        # Should use `last_conv_channels` here, but megvii insists that it\'s `n_classes`. Keeping it.\n        # https://github.com/megvii-model/SinglePathOneShot/blob/36eed6cf083497ffa9cfe7b8da25bb0b6ba5a452/src/Supernet/flops.py#L313\n        rest_flops = self._op_flops_dict[""rest_operation""][(self.stage_channels[-1], self._n_classes,\n                                                            self._feature_map_size, self._feature_map_size, 1)]\n        total_flops = conv1_flops + rest_flops\n        for k, m in candidate.items():\n            parsed_flops_dict = self._parsed_flops[k]\n            if isinstance(m, dict):  # to be compatible with classical nas format\n                total_flops += parsed_flops_dict[m[""_idx""]]\n            else:\n                total_flops += parsed_flops_dict[torch.max(m, 0)[1]]\n        return total_flops\n\n    def _initialize_weights(self):\n        for name, m in self.named_modules():\n            if isinstance(m, nn.Conv2d):\n                if \'first\' in name:\n                    nn.init.normal_(m.weight, 0, 0.01)\n                else:\n                    nn.init.normal_(m.weight, 0, 1.0 / m.weight.shape[1])\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.BatchNorm2d):\n                if m.weight is not None:\n                    nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0001)\n                nn.init.constant_(m.running_mean, 0)\n            elif isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0.0001)\n                nn.init.constant_(m.running_mean, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.normal_(m.weight, 0, 0.01)\n                if m.bias is not None:\n                    nn.init.constant_(m.bias, 0)\n\n\ndef load_and_parse_state_dict(filepath=""./data/checkpoint-150000.pth.tar""):\n    checkpoint = torch.load(filepath, map_location=torch.device(""cpu""))\n    if ""state_dict"" in checkpoint:\n        checkpoint = checkpoint[""state_dict""]\n    result = dict()\n    for k, v in checkpoint.items():\n        if k.startswith(""module.""):\n            k = k[len(""module.""):]\n        result[k] = v\n    return result\n'"
examples/nas/spos/scratch.py,13,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport logging\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom dataloader import get_imagenet_iter_dali\nfrom nni.nas.pytorch.fixed import apply_fixed_architecture\nfrom nni.nas.pytorch.utils import AverageMeterGroup\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom network import ShuffleNetV2OneShot\nfrom utils import CrossEntropyLabelSmooth, accuracy\n\nlogger = logging.getLogger(""nni.spos.scratch"")\n\n\ndef train(epoch, model, criterion, optimizer, loader, writer, args):\n    model.train()\n    meters = AverageMeterGroup()\n    cur_lr = optimizer.param_groups[0][""lr""]\n\n    for step, (x, y) in enumerate(loader):\n        cur_step = len(loader) * epoch + step\n        optimizer.zero_grad()\n        logits = model(x)\n        loss = criterion(logits, y)\n        loss.backward()\n        optimizer.step()\n\n        metrics = accuracy(logits, y)\n        metrics[""loss""] = loss.item()\n        meters.update(metrics)\n\n        writer.add_scalar(""lr"", cur_lr, global_step=cur_step)\n        writer.add_scalar(""loss/train"", loss.item(), global_step=cur_step)\n        writer.add_scalar(""acc1/train"", metrics[""acc1""], global_step=cur_step)\n        writer.add_scalar(""acc5/train"", metrics[""acc5""], global_step=cur_step)\n\n        if step % args.log_frequency == 0 or step + 1 == len(loader):\n            logger.info(""Epoch [%d/%d] Step [%d/%d]  %s"", epoch + 1,\n                        args.epochs, step + 1, len(loader), meters)\n\n    logger.info(""Epoch %d training summary: %s"", epoch + 1, meters)\n\n\ndef validate(epoch, model, criterion, loader, writer, args):\n    model.eval()\n    meters = AverageMeterGroup()\n    with torch.no_grad():\n        for step, (x, y) in enumerate(loader):\n            logits = model(x)\n            loss = criterion(logits, y)\n            metrics = accuracy(logits, y)\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n\n            if step % args.log_frequency == 0 or step + 1 == len(loader):\n                logger.info(""Epoch [%d/%d] Validation Step [%d/%d]  %s"", epoch + 1,\n                            args.epochs, step + 1, len(loader), meters)\n\n    writer.add_scalar(""loss/test"", meters.loss.avg, global_step=epoch)\n    writer.add_scalar(""acc1/test"", meters.acc1.avg, global_step=epoch)\n    writer.add_scalar(""acc5/test"", meters.acc5.avg, global_step=epoch)\n\n    logger.info(""Epoch %d validation: top1 = %f, top5 = %f"", epoch + 1, meters.acc1.avg, meters.acc5.avg)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""SPOS Training From Scratch"")\n    parser.add_argument(""--imagenet-dir"", type=str, default=""./data/imagenet"")\n    parser.add_argument(""--tb-dir"", type=str, default=""runs"")\n    parser.add_argument(""--architecture"", type=str, default=""architecture_final.json"")\n    parser.add_argument(""--workers"", type=int, default=12)\n    parser.add_argument(""--batch-size"", type=int, default=1024)\n    parser.add_argument(""--epochs"", type=int, default=240)\n    parser.add_argument(""--learning-rate"", type=float, default=0.5)\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--weight-decay"", type=float, default=4E-5)\n    parser.add_argument(""--label-smooth"", type=float, default=0.1)\n    parser.add_argument(""--log-frequency"", type=int, default=10)\n    parser.add_argument(""--lr-decay"", type=str, default=""linear"")\n    parser.add_argument(""--seed"", type=int, default=42)\n    parser.add_argument(""--spos-preprocessing"", default=False, action=""store_true"")\n    parser.add_argument(""--label-smoothing"", type=float, default=0.1)\n\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n    model = ShuffleNetV2OneShot()\n    model.cuda()\n    apply_fixed_architecture(model, args.architecture)\n    if torch.cuda.device_count() > 1:  # exclude last gpu, saving for data preprocessing on gpu\n        model = nn.DataParallel(model, device_ids=list(range(0, torch.cuda.device_count() - 1)))\n    criterion = CrossEntropyLabelSmooth(1000, args.label_smoothing)\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate,\n                                momentum=args.momentum, weight_decay=args.weight_decay)\n    if args.lr_decay == ""linear"":\n        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n                                                      lambda step: (1.0 - step / args.epochs)\n                                                      if step <= args.epochs else 0,\n                                                      last_epoch=-1)\n    elif args.lr_decay == ""cosine"":\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, args.epochs, 1E-3)\n    else:\n        raise ValueError(""\'%s\' not supported."" % args.lr_decay)\n    writer = SummaryWriter(log_dir=args.tb_dir)\n\n    train_loader = get_imagenet_iter_dali(""train"", args.imagenet_dir, args.batch_size, args.workers,\n                                          spos_preprocessing=args.spos_preprocessing)\n    val_loader = get_imagenet_iter_dali(""val"", args.imagenet_dir, args.batch_size, args.workers,\n                                        spos_preprocessing=args.spos_preprocessing)\n\n    for epoch in range(args.epochs):\n        train(epoch, model, criterion, optimizer, train_loader, writer, args)\n        validate(epoch, model, criterion, val_loader, writer, args)\n        scheduler.step()\n\n    writer.close()\n'"
examples/nas/spos/supernet.py,11,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport logging\nimport random\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch.callbacks import LRSchedulerCallback\nfrom nni.nas.pytorch.callbacks import ModelCheckpoint\nfrom nni.nas.pytorch.spos import SPOSSupernetTrainingMutator, SPOSSupernetTrainer\n\nfrom dataloader import get_imagenet_iter_dali\nfrom network import ShuffleNetV2OneShot, load_and_parse_state_dict\nfrom utils import CrossEntropyLabelSmooth, accuracy\n\nlogger = logging.getLogger(""nni.spos.supernet"")\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""SPOS Supernet Training"")\n    parser.add_argument(""--imagenet-dir"", type=str, default=""./data/imagenet"")\n    parser.add_argument(""--load-checkpoint"", action=""store_true"", default=False)\n    parser.add_argument(""--spos-preprocessing"", action=""store_true"", default=False,\n                        help=""When true, image values will range from 0 to 255 and use BGR ""\n                             ""(as in original repo)."")\n    parser.add_argument(""--workers"", type=int, default=4)\n    parser.add_argument(""--batch-size"", type=int, default=768)\n    parser.add_argument(""--epochs"", type=int, default=120)\n    parser.add_argument(""--learning-rate"", type=float, default=0.5)\n    parser.add_argument(""--momentum"", type=float, default=0.9)\n    parser.add_argument(""--weight-decay"", type=float, default=4E-5)\n    parser.add_argument(""--label-smooth"", type=float, default=0.1)\n    parser.add_argument(""--log-frequency"", type=int, default=10)\n    parser.add_argument(""--seed"", type=int, default=42)\n    parser.add_argument(""--label-smoothing"", type=float, default=0.1)\n\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n    model = ShuffleNetV2OneShot()\n    if args.load_checkpoint:\n        if not args.spos_preprocessing:\n            logger.warning(""You might want to use SPOS preprocessing if you are loading their checkpoints."")\n        model.load_state_dict(load_and_parse_state_dict())\n    model.cuda()\n    if torch.cuda.device_count() > 1:  # exclude last gpu, saving for data preprocessing on gpu\n        model = nn.DataParallel(model, device_ids=list(range(0, torch.cuda.device_count() - 1)))\n    mutator = SPOSSupernetTrainingMutator(model, flops_func=model.module.get_candidate_flops,\n                                          flops_lb=290E6, flops_ub=360E6)\n    criterion = CrossEntropyLabelSmooth(1000, args.label_smoothing)\n    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate,\n                                momentum=args.momentum, weight_decay=args.weight_decay)\n    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer,\n                                                  lambda step: (1.0 - step / args.epochs)\n                                                  if step <= args.epochs else 0,\n                                                  last_epoch=-1)\n    train_loader = get_imagenet_iter_dali(""train"", args.imagenet_dir, args.batch_size, args.workers,\n                                          spos_preprocessing=args.spos_preprocessing)\n    valid_loader = get_imagenet_iter_dali(""val"", args.imagenet_dir, args.batch_size, args.workers,\n                                          spos_preprocessing=args.spos_preprocessing)\n    trainer = SPOSSupernetTrainer(model, criterion, accuracy, optimizer,\n                                  args.epochs, train_loader, valid_loader,\n                                  mutator=mutator, batch_size=args.batch_size,\n                                  log_frequency=args.log_frequency, workers=args.workers,\n                                  callbacks=[LRSchedulerCallback(scheduler),\n                                             ModelCheckpoint(""./checkpoints"")])\n    trainer.train()\n'"
examples/nas/spos/tester.py,11,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport logging\nimport random\nimport time\nfrom itertools import cycle\n\nimport nni\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch.classic_nas import get_and_apply_next_architecture\nfrom nni.nas.pytorch.utils import AverageMeterGroup\n\nfrom dataloader import get_imagenet_iter_dali\nfrom network import ShuffleNetV2OneShot, load_and_parse_state_dict\nfrom utils import CrossEntropyLabelSmooth, accuracy\n\nlogger = logging.getLogger(""nni.spos.tester"")\n\n\ndef retrain_bn(model, criterion, max_iters, log_freq, loader):\n    with torch.no_grad():\n        logger.info(""Clear BN statistics..."")\n        for m in model.modules():\n            if isinstance(m, nn.BatchNorm2d):\n                m.running_mean = torch.zeros_like(m.running_mean)\n                m.running_var = torch.ones_like(m.running_var)\n\n        logger.info(""Train BN with training set (BN sanitize)..."")\n        model.train()\n        meters = AverageMeterGroup()\n        for step in range(max_iters):\n            inputs, targets = next(loader)\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            metrics = accuracy(logits, targets)\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n            if step % log_freq == 0 or step + 1 == max_iters:\n                logger.info(""Train Step [%d/%d] %s"", step + 1, max_iters, meters)\n\n\ndef test_acc(model, criterion, log_freq, loader):\n    logger.info(""Start testing..."")\n    model.eval()\n    meters = AverageMeterGroup()\n    start_time = time.time()\n    with torch.no_grad():\n        for step, (inputs, targets) in enumerate(loader):\n            logits = model(inputs)\n            loss = criterion(logits, targets)\n            metrics = accuracy(logits, targets)\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n            if step % log_freq == 0 or step + 1 == len(loader):\n                logger.info(""Valid Step [%d/%d] time %.3fs acc1 %.4f acc5 %.4f loss %.4f"",\n                            step + 1, len(loader), time.time() - start_time,\n                            meters.acc1.avg, meters.acc5.avg, meters.loss.avg)\n    return meters.acc1.avg\n\n\ndef evaluate_acc(model, criterion, args, loader_train, loader_test):\n    acc_before = test_acc(model, criterion, args.log_frequency, loader_test)\n    nni.report_intermediate_result(acc_before)\n\n    retrain_bn(model, criterion, args.train_iters, args.log_frequency, loader_train)\n    acc = test_acc(model, criterion, args.log_frequency, loader_test)\n    assert isinstance(acc, float)\n    nni.report_intermediate_result(acc)\n    nni.report_final_result(acc)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""SPOS Candidate Tester"")\n    parser.add_argument(""--imagenet-dir"", type=str, default=""./data/imagenet"")\n    parser.add_argument(""--checkpoint"", type=str, default=""./data/checkpoint-150000.pth.tar"")\n    parser.add_argument(""--spos-preprocessing"", action=""store_true"", default=False,\n                        help=""When true, image values will range from 0 to 255 and use BGR ""\n                             ""(as in original repo)."")\n    parser.add_argument(""--seed"", type=int, default=42)\n    parser.add_argument(""--workers"", type=int, default=6)\n    parser.add_argument(""--train-batch-size"", type=int, default=128)\n    parser.add_argument(""--train-iters"", type=int, default=200)\n    parser.add_argument(""--test-batch-size"", type=int, default=512)\n    parser.add_argument(""--log-frequency"", type=int, default=10)\n\n    args = parser.parse_args()\n\n    # use a fixed set of image will improve the performance\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n    assert torch.cuda.is_available()\n\n    model = ShuffleNetV2OneShot()\n    criterion = CrossEntropyLabelSmooth(1000, 0.1)\n    get_and_apply_next_architecture(model)\n    model.load_state_dict(load_and_parse_state_dict(filepath=args.checkpoint))\n    model.cuda()\n\n    train_loader = get_imagenet_iter_dali(""train"", args.imagenet_dir, args.train_batch_size, args.workers,\n                                          spos_preprocessing=args.spos_preprocessing,\n                                          seed=args.seed, device_id=0)\n    val_loader = get_imagenet_iter_dali(""val"", args.imagenet_dir, args.test_batch_size, args.workers,\n                                        spos_preprocessing=args.spos_preprocessing, shuffle=True,\n                                        seed=args.seed, device_id=0)\n    train_loader = cycle(train_loader)\n\n    evaluate_acc(model, criterion, args, train_loader, val_loader)\n'"
examples/nas/spos/tuner.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom nni.nas.pytorch.spos import SPOSEvolution\n\nfrom network import ShuffleNetV2OneShot\n\n\nclass EvolutionWithFlops(SPOSEvolution):\n    """"""\n    This tuner extends the function of evolution tuner, by limiting the flops generated by tuner.\n    Needs a function to examine the flops.\n    """"""\n\n    def __init__(self, flops_limit=330E6, **kwargs):\n        super().__init__(**kwargs)\n        self.model = ShuffleNetV2OneShot()\n        self.flops_limit = flops_limit\n\n    def _is_legal(self, cand):\n        if not super()._is_legal(cand):\n            return False\n        if self.model.get_candidate_flops(cand) > self.flops_limit:\n            return False\n        return True\n'"
examples/nas/spos/utils.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\n\nclass CrossEntropyLabelSmooth(nn.Module):\n\n    def __init__(self, num_classes, epsilon):\n        super(CrossEntropyLabelSmooth, self).__init__()\n        self.num_classes = num_classes\n        self.epsilon = epsilon\n        self.logsoftmax = nn.LogSoftmax(dim=1)\n\n    def forward(self, inputs, targets):\n        log_probs = self.logsoftmax(inputs)\n        targets = torch.zeros_like(log_probs).scatter_(1, targets.unsqueeze(1), 1)\n        targets = (1 - self.epsilon) * targets + self.epsilon / self.num_classes\n        loss = (-targets * log_probs).mean(0).sum()\n        return loss\n\n\ndef accuracy(output, target, topk=(1, 5)):\n    """""" Computes the precision@k for the specified values of k """"""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    # one-hot case\n    if target.ndimension() > 1:\n        target = target.max(1)[1]\n\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = dict()\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res[""acc{}"".format(k)] = correct_k.mul_(1.0 / batch_size).item()\n    return res\n'"
examples/nas/textnas/dataloader.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport os\nimport pickle\nfrom collections import Counter\n\nimport numpy as np\nimport torch\nfrom torch.utils import data\n\nlogger = logging.getLogger(""nni.textnas"")\n\n\nclass PTBTree:\n    WORD_TO_WORD_MAPPING = {\n        ""{"": ""-LCB-"",\n        ""}"": ""-RCB-""\n    }\n\n    def __init__(self):\n        self.subtrees = []\n        self.word = None\n        self.label = """"\n        self.parent = None\n        self.span = (-1, -1)\n        self.word_vector = None  # HOS, store dx1 RNN word vector\n        self.prediction = None  # HOS, store Kx1 prediction vector\n\n    def is_leaf(self):\n        return len(self.subtrees) == 0\n\n    def set_by_text(self, text, pos=0, left=0):\n        depth = 0\n        right = left\n        for i in range(pos + 1, len(text)):\n            char = text[i]\n            # update the depth\n            if char == ""("":\n                depth += 1\n                if depth == 1:\n                    subtree = PTBTree()\n                    subtree.parent = self\n                    subtree.set_by_text(text, i, right)\n                    right = subtree.span[1]\n                    self.span = (left, right)\n                    self.subtrees.append(subtree)\n            elif char == "")"":\n                depth -= 1\n                if len(self.subtrees) == 0:\n                    pos = i\n                    for j in range(i, 0, -1):\n                        if text[j] == "" "":\n                            pos = j\n                            break\n                    self.word = text[pos + 1:i]\n                    self.span = (left, left + 1)\n\n            # we\'ve reached the end of the category that is the root of this subtree\n            if depth == 0 and char == "" "" and self.label == """":\n                self.label = text[pos + 1:i]\n            # we\'ve reached the end of the scope for this bracket\n            if depth < 0:\n                break\n\n        # Fix some issues with variation in output, and one error in the treebank\n        # for a word with a punctuation POS\n        self.standardise_node()\n\n    def standardise_node(self):\n        if self.word in self.WORD_TO_WORD_MAPPING:\n            self.word = self.WORD_TO_WORD_MAPPING[self.word]\n\n    def __repr__(self, single_line=True, depth=0):\n        ans = """"\n        if not single_line and depth > 0:\n            ans = ""\\n"" + depth * ""\\t""\n        ans += ""("" + self.label\n        if self.word is not None:\n            ans += "" "" + self.word\n        for subtree in self.subtrees:\n            if single_line:\n                ans += "" ""\n            ans += subtree.__repr__(single_line, depth + 1)\n        ans += "")""\n        return ans\n\n\ndef read_tree(source):\n    cur_text = []\n    depth = 0\n    while True:\n        line = source.readline()\n        # Check if we are out of input\n        if line == """":\n            return None\n        # strip whitespace and only use if this contains something\n        line = line.strip()\n        if line == """":\n            continue\n        cur_text.append(line)\n        # Update depth\n        for char in line:\n            if char == ""("":\n                depth += 1\n            elif char == "")"":\n                depth -= 1\n        # At depth 0 we have a complete tree\n        if depth == 0:\n            tree = PTBTree()\n            tree.set_by_text("" "".join(cur_text))\n            return tree\n    return None\n\n\ndef read_trees(source, max_sents=-1):\n    with open(source) as fp:\n        trees = []\n        while True:\n            tree = read_tree(fp)\n            if tree is None:\n                break\n            trees.append(tree)\n            if len(trees) >= max_sents > 0:\n                break\n        return trees\n\n\nclass SSTDataset(data.Dataset):\n    def __init__(self, sents, mask, labels):\n        self.sents = sents\n        self.labels = labels\n        self.mask = mask\n\n    def __getitem__(self, index):\n        return (self.sents[index], self.mask[index]), self.labels[index]\n\n    def __len__(self):\n        return len(self.sents)\n\n\ndef sst_get_id_input(content, word_id_dict, max_input_length):\n    words = content.split("" "")\n    sentence = [word_id_dict[""<pad>""]] * max_input_length\n    mask = [0] * max_input_length\n    unknown = word_id_dict[""<unknown>""]\n    for i, word in enumerate(words[:max_input_length]):\n        sentence[i] = word_id_dict.get(word, unknown)\n        mask[i] = 1\n    return sentence, mask\n\n\ndef sst_get_phrases(trees, sample_ratio=1.0, is_binary=False, only_sentence=False):\n    all_phrases = []\n    for tree in trees:\n        if only_sentence:\n            sentence = get_sentence_by_tree(tree)\n            label = int(tree.label)\n            pair = (sentence, label)\n            all_phrases.append(pair)\n        else:\n            phrases = get_phrases_by_tree(tree)\n            sentence = get_sentence_by_tree(tree)\n            pair = (sentence, int(tree.label))\n            all_phrases.append(pair)\n            all_phrases += phrases\n    if sample_ratio < 1.:\n        np.random.shuffle(all_phrases)\n    result_phrases = []\n    for pair in all_phrases:\n        if is_binary:\n            phrase, label = pair\n            if label <= 1:\n                pair = (phrase, 0)\n            elif label >= 3:\n                pair = (phrase, 1)\n            else:\n                continue\n        if sample_ratio == 1.:\n            result_phrases.append(pair)\n        else:\n            rand_portion = np.random.random()\n            if rand_portion < sample_ratio:\n                result_phrases.append(pair)\n    return result_phrases\n\n\ndef get_phrases_by_tree(tree):\n    phrases = []\n    if tree is None:\n        return phrases\n    if tree.is_leaf():\n        pair = (tree.word, int(tree.label))\n        phrases.append(pair)\n        return phrases\n    left_child_phrases = get_phrases_by_tree(tree.subtrees[0])\n    right_child_phrases = get_phrases_by_tree(tree.subtrees[1])\n    phrases.extend(left_child_phrases)\n    phrases.extend(right_child_phrases)\n    sentence = get_sentence_by_tree(tree)\n    pair = (sentence, int(tree.label))\n    phrases.append(pair)\n    return phrases\n\n\ndef get_sentence_by_tree(tree):\n    if tree is None:\n        return """"\n    if tree.is_leaf():\n        return tree.word\n    left_sentence = get_sentence_by_tree(tree.subtrees[0])\n    right_sentence = get_sentence_by_tree(tree.subtrees[1])\n    sentence = left_sentence + "" "" + right_sentence\n    return sentence.strip()\n\n\ndef get_word_id_dict(word_num_dict, word_id_dict, min_count):\n    z = [k for k in sorted(word_num_dict.keys())]\n    for word in z:\n        count = word_num_dict[word]\n        if count >= min_count:\n            index = len(word_id_dict)\n            if word not in word_id_dict:\n                word_id_dict[word] = index\n    return word_id_dict\n\n\ndef load_word_num_dict(phrases, word_num_dict):\n    for sentence, _ in phrases:\n        words = sentence.split("" "")\n        for cur_word in words:\n            word = cur_word.strip()\n            word_num_dict[word] += 1\n    return word_num_dict\n\n\ndef init_trainable_embedding(embedding_path, word_id_dict, embed_dim=300):\n    word_embed_model = load_glove_model(embedding_path, embed_dim)\n    assert word_embed_model[""pool""].shape[1] == embed_dim\n    embedding = np.random.random([len(word_id_dict), embed_dim]).astype(np.float32) / 2.0 - 0.25\n    embedding[0] = np.zeros(embed_dim)  # PAD\n    embedding[1] = (np.random.rand(embed_dim) - 0.5) / 2  # UNK\n    for word in sorted(word_id_dict.keys()):\n        idx = word_id_dict[word]\n        if idx == 0 or idx == 1:\n            continue\n        if word in word_embed_model[""mapping""]:\n            embedding[idx] = word_embed_model[""pool""][word_embed_model[""mapping""][word]]\n        else:\n            embedding[idx] = np.random.rand(embed_dim) / 2.0 - 0.25\n    return embedding\n\n\ndef sst_get_trainable_data(phrases, word_id_dict, max_input_length):\n    texts, labels, mask = [], [], []\n\n    for phrase, label in phrases:\n        if not phrase.split():\n            continue\n        phrase_split, mask_split = sst_get_id_input(phrase, word_id_dict, max_input_length)\n        texts.append(phrase_split)\n        labels.append(int(label))\n        mask.append(mask_split)  # field_input is mask\n    labels = np.array(labels, dtype=np.int64)\n    texts = np.reshape(texts, [-1, max_input_length]).astype(np.int32)\n    mask = np.reshape(mask, [-1, max_input_length]).astype(np.int32)\n\n    return SSTDataset(texts, mask, labels)\n\n\ndef load_glove_model(filename, embed_dim):\n    if os.path.exists(filename + "".cache""):\n        logger.info(""Found cache. Loading..."")\n        with open(filename + "".cache"", ""rb"") as fp:\n            return pickle.load(fp)\n    embedding = {""mapping"": dict(), ""pool"": []}\n    with open(filename) as f:\n        for i, line in enumerate(f):\n            line = line.rstrip(""\\n"")\n            vocab_word, *vec = line.rsplit("" "", maxsplit=embed_dim)\n            assert len(vec) == 300, ""Unexpected line: \'%s\'"" % line\n            embedding[""pool""].append(np.array(list(map(float, vec)), dtype=np.float32))\n            embedding[""mapping""][vocab_word] = i\n    embedding[""pool""] = np.stack(embedding[""pool""])\n    with open(filename + "".cache"", ""wb"") as fp:\n        pickle.dump(embedding, fp)\n    return embedding\n\n\ndef read_data_sst(data_path, max_input_length=64, min_count=1, train_with_valid=False,\n                  train_ratio=1., valid_ratio=1., is_binary=False, only_sentence=False):\n    word_id_dict = dict()\n    word_num_dict = Counter()\n\n    sst_path = os.path.join(data_path, ""sst"")\n    logger.info(""Reading SST data..."")\n    train_file_name = os.path.join(sst_path, ""trees"", ""train.txt"")\n    valid_file_name = os.path.join(sst_path, ""trees"", ""dev.txt"")\n    test_file_name = os.path.join(sst_path, ""trees"", ""test.txt"")\n    train_trees = read_trees(train_file_name)\n    train_phrases = sst_get_phrases(train_trees, train_ratio, is_binary, only_sentence)\n    logger.info(""Finish load train phrases."")\n    valid_trees = read_trees(valid_file_name)\n    valid_phrases = sst_get_phrases(valid_trees, valid_ratio, is_binary, only_sentence)\n    logger.info(""Finish load valid phrases."")\n    if train_with_valid:\n        train_phrases += valid_phrases\n    test_trees = read_trees(test_file_name)\n    test_phrases = sst_get_phrases(test_trees, valid_ratio, is_binary, only_sentence=True)\n    logger.info(""Finish load test phrases."")\n\n    # get word_id_dict\n    word_id_dict[""<pad>""] = 0\n    word_id_dict[""<unknown>""] = 1\n    load_word_num_dict(train_phrases, word_num_dict)\n    logger.info(""Finish load train words: %d."", len(word_num_dict))\n    load_word_num_dict(valid_phrases, word_num_dict)\n    load_word_num_dict(test_phrases, word_num_dict)\n    logger.info(""Finish load valid+test words: %d."", len(word_num_dict))\n    word_id_dict = get_word_id_dict(word_num_dict, word_id_dict, min_count)\n    logger.info(""After trim vocab length: %d."", len(word_id_dict))\n\n    logger.info(""Loading embedding..."")\n    embedding = init_trainable_embedding(os.path.join(data_path, ""glove.840B.300d.txt""), word_id_dict)\n    logger.info(""Finish initialize word embedding."")\n\n    dataset_train = sst_get_trainable_data(train_phrases, word_id_dict, max_input_length)\n    logger.info(""Loaded %d training samples."", len(dataset_train))\n    dataset_valid = sst_get_trainable_data(valid_phrases, word_id_dict, max_input_length)\n    logger.info(""Loaded %d validation samples."", len(dataset_valid))\n    dataset_test = sst_get_trainable_data(test_phrases, word_id_dict, max_input_length)\n    logger.info(""Loaded %d test samples."", len(dataset_test))\n\n    return dataset_train, dataset_valid, dataset_test, torch.from_numpy(embedding)\n'"
examples/nas/textnas/model.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch import mutables\n\nfrom ops import ConvBN, LinearCombine, AvgPool, MaxPool, RNN, Attention, BatchNorm\nfrom utils import GlobalMaxPool, GlobalAvgPool\n\n\nclass Layer(mutables.MutableScope):\n    def __init__(self, key, prev_keys, hidden_units, choose_from_k, cnn_keep_prob, lstm_keep_prob, att_keep_prob, att_mask):\n        super(Layer, self).__init__(key)\n\n        def conv_shortcut(kernel_size):\n            return ConvBN(kernel_size, hidden_units, hidden_units, cnn_keep_prob, False, True)\n\n        self.n_candidates = len(prev_keys)\n        if self.n_candidates:\n            self.prec = mutables.InputChoice(choose_from=prev_keys[-choose_from_k:], n_chosen=1)\n        else:\n            # first layer, skip input choice\n            self.prec = None\n        self.op = mutables.LayerChoice([\n            conv_shortcut(1),\n            conv_shortcut(3),\n            conv_shortcut(5),\n            conv_shortcut(7),\n            AvgPool(3, False, True),\n            MaxPool(3, False, True),\n            RNN(hidden_units, lstm_keep_prob),\n            Attention(hidden_units, 4, att_keep_prob, att_mask)\n        ])\n        if self.n_candidates:\n            self.skipconnect = mutables.InputChoice(choose_from=prev_keys)\n        else:\n            self.skipconnect = None\n        self.bn = BatchNorm(hidden_units, False, True)\n\n    def forward(self, last_layer, prev_layers, mask):\n        # pass an extra last_layer to deal with layer 0 (prev_layers is empty)\n        if self.prec is None:\n            prec = last_layer\n        else:\n            prec = self.prec(prev_layers[-self.prec.n_candidates:])  # skip first\n        out = self.op(prec, mask)\n        if self.skipconnect is not None:\n            connection = self.skipconnect(prev_layers[-self.skipconnect.n_candidates:])\n            if connection is not None:\n                out += connection\n        out = self.bn(out, mask)\n        return out\n\n\nclass Model(nn.Module):\n    def __init__(self, embedding, hidden_units=256, num_layers=24, num_classes=5, choose_from_k=5,\n                 lstm_keep_prob=0.5, cnn_keep_prob=0.5, att_keep_prob=0.5, att_mask=True,\n                 embed_keep_prob=0.5, final_output_keep_prob=1.0, global_pool=""avg""):\n        super(Model, self).__init__()\n\n        self.embedding = nn.Embedding.from_pretrained(embedding, freeze=False)\n        self.hidden_units = hidden_units\n        self.num_layers = num_layers\n        self.num_classes = num_classes\n\n        self.init_conv = ConvBN(1, self.embedding.embedding_dim, hidden_units, cnn_keep_prob, False, True)\n\n        self.layers = nn.ModuleList()\n        candidate_keys_pool = []\n        for layer_id in range(self.num_layers):\n            k = ""layer_{}"".format(layer_id)\n            self.layers.append(Layer(k, candidate_keys_pool, hidden_units, choose_from_k,\n                                     cnn_keep_prob, lstm_keep_prob, att_keep_prob, att_mask))\n            candidate_keys_pool.append(k)\n\n        self.linear_combine = LinearCombine(self.num_layers)\n        self.linear_out = nn.Linear(self.hidden_units, self.num_classes)\n\n        self.embed_dropout = nn.Dropout(p=1 - embed_keep_prob)\n        self.output_dropout = nn.Dropout(p=1 - final_output_keep_prob)\n\n        assert global_pool in [""max"", ""avg""]\n        if global_pool == ""max"":\n            self.global_pool = GlobalMaxPool()\n        elif global_pool == ""avg"":\n            self.global_pool = GlobalAvgPool()\n\n    def forward(self, inputs):\n        sent_ids, mask = inputs\n        seq = self.embedding(sent_ids.long())\n        seq = self.embed_dropout(seq)\n\n        seq = torch.transpose(seq, 1, 2)  # from (N, L, C) -> (N, C, L)\n\n        x = self.init_conv(seq, mask)\n        prev_layers = []\n\n        for layer in self.layers:\n            x = layer(x, prev_layers, mask)\n            prev_layers.append(x)\n\n        x = self.linear_combine(torch.stack(prev_layers))\n        x = self.global_pool(x, mask)\n        x = self.output_dropout(x)\n        x = self.linear_out(x)\n        return x\n'"
examples/nas/textnas/ops.py,22,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom utils import get_length, INF\n\n\nclass Mask(nn.Module):\n    def forward(self, seq, mask):\n        # seq: (N, C, L)\n        # mask: (N, L)\n        seq_mask = torch.unsqueeze(mask, 2)\n        seq_mask = torch.transpose(seq_mask.repeat(1, 1, seq.size()[1]), 1, 2)\n        return seq.where(torch.eq(seq_mask, 1), torch.zeros_like(seq))\n\n\nclass BatchNorm(nn.Module):\n    def __init__(self, num_features, pre_mask, post_mask, eps=1e-5, decay=0.9, affine=True):\n        super(BatchNorm, self).__init__()\n        self.mask_opt = Mask()\n        self.pre_mask = pre_mask\n        self.post_mask = post_mask\n        self.bn = nn.BatchNorm1d(num_features, eps=eps, momentum=1.0 - decay, affine=affine)\n\n    def forward(self, seq, mask):\n        if self.pre_mask:\n            seq = self.mask_opt(seq, mask)\n        seq = self.bn(seq)\n        if self.post_mask:\n            seq = self.mask_opt(seq, mask)\n        return seq\n\n\nclass ConvBN(nn.Module):\n    def __init__(self, kernal_size, in_channels, out_channels, cnn_keep_prob,\n                 pre_mask, post_mask, with_bn=True, with_relu=True):\n        super(ConvBN, self).__init__()\n        self.mask_opt = Mask()\n        self.pre_mask = pre_mask\n        self.post_mask = post_mask\n        self.with_bn = with_bn\n        self.with_relu = with_relu\n        self.conv = nn.Conv1d(in_channels, out_channels, kernal_size, 1, bias=True, padding=(kernal_size - 1) // 2)\n        self.dropout = nn.Dropout(p=(1 - cnn_keep_prob))\n\n        if with_bn:\n            self.bn = BatchNorm(out_channels, not post_mask, True)\n\n        if with_relu:\n            self.relu = nn.ReLU()\n\n    def forward(self, seq, mask):\n        if self.pre_mask:\n            seq = self.mask_opt(seq, mask)\n        seq = self.conv(seq)\n        if self.post_mask:\n            seq = self.mask_opt(seq, mask)\n        if self.with_bn:\n            seq = self.bn(seq, mask)\n        if self.with_relu:\n            seq = self.relu(seq)\n        seq = self.dropout(seq)\n        return seq\n\n\nclass AvgPool(nn.Module):\n    def __init__(self, kernal_size, pre_mask, post_mask):\n        super(AvgPool, self).__init__()\n        self.avg_pool = nn.AvgPool1d(kernal_size, 1, padding=(kernal_size - 1) // 2)\n        self.pre_mask = pre_mask\n        self.post_mask = post_mask\n        self.mask_opt = Mask()\n\n    def forward(self, seq, mask):\n        if self.pre_mask:\n            seq = self.mask_opt(seq, mask)\n        seq = self.avg_pool(seq)\n        if self.post_mask:\n            seq = self.mask_opt(seq, mask)\n        return seq\n\n\nclass MaxPool(nn.Module):\n    def __init__(self, kernal_size, pre_mask, post_mask):\n        super(MaxPool, self).__init__()\n        self.max_pool = nn.MaxPool1d(kernal_size, 1, padding=(kernal_size - 1) // 2)\n        self.pre_mask = pre_mask\n        self.post_mask = post_mask\n        self.mask_opt = Mask()\n\n    def forward(self, seq, mask):\n        if self.pre_mask:\n            seq = self.mask_opt(seq, mask)\n        seq = self.max_pool(seq)\n        if self.post_mask:\n            seq = self.mask_opt(seq, mask)\n        return seq\n\n\nclass Attention(nn.Module):\n    def __init__(self, num_units, num_heads, keep_prob, is_mask):\n        super(Attention, self).__init__()\n        self.num_heads = num_heads\n        self.keep_prob = keep_prob\n\n        self.linear_q = nn.Linear(num_units, num_units)\n        self.linear_k = nn.Linear(num_units, num_units)\n        self.linear_v = nn.Linear(num_units, num_units)\n\n        self.bn = BatchNorm(num_units, True, is_mask)\n        self.dropout = nn.Dropout(p=1 - self.keep_prob)\n\n    def forward(self, seq, mask):\n        in_c = seq.size()[1]\n        seq = torch.transpose(seq, 1, 2)  # (N, L, C)\n        queries = seq\n        keys = seq\n        num_heads = self.num_heads\n\n        # T_q = T_k = L\n        Q = F.relu(self.linear_q(seq))  # (N, T_q, C)\n        K = F.relu(self.linear_k(seq))  # (N, T_k, C)\n        V = F.relu(self.linear_v(seq))  # (N, T_k, C)\n\n        # Split and concat\n        Q_ = torch.cat(torch.split(Q, in_c // num_heads, dim=2), dim=0)  # (h*N, T_q, C/h)\n        K_ = torch.cat(torch.split(K, in_c // num_heads, dim=2), dim=0)  # (h*N, T_k, C/h)\n        V_ = torch.cat(torch.split(V, in_c // num_heads, dim=2), dim=0)  # (h*N, T_k, C/h)\n\n        # Multiplication\n        outputs = torch.matmul(Q_, K_.transpose(1, 2))  # (h*N, T_q, T_k)\n        # Scale\n        outputs = outputs / (K_.size()[-1] ** 0.5)\n        # Key Masking\n        key_masks = mask.repeat(num_heads, 1)  # (h*N, T_k)\n        key_masks = torch.unsqueeze(key_masks, 1)  # (h*N, 1, T_k)\n        key_masks = key_masks.repeat(1, queries.size()[1], 1)  # (h*N, T_q, T_k)\n\n        paddings = torch.ones_like(outputs) * (-INF)  # extremely small value\n        outputs = torch.where(torch.eq(key_masks, 0), paddings, outputs)\n\n        query_masks = mask.repeat(num_heads, 1)  # (h*N, T_q)\n        query_masks = torch.unsqueeze(query_masks, -1)  # (h*N, T_q, 1)\n        query_masks = query_masks.repeat(1, 1, keys.size()[1]).float()  # (h*N, T_q, T_k)\n\n        att_scores = F.softmax(outputs, dim=-1) * query_masks  # (h*N, T_q, T_k)\n        att_scores = self.dropout(att_scores)\n\n        # Weighted sum\n        x_outputs = torch.matmul(att_scores, V_)  # (h*N, T_q, C/h)\n        # Restore shape\n        x_outputs = torch.cat(\n            torch.split(x_outputs, x_outputs.size()[0] // num_heads, dim=0),\n            dim=2)  # (N, T_q, C)\n\n        x = torch.transpose(x_outputs, 1, 2)  # (N, C, L)\n        x = self.bn(x, mask)\n\n        return x\n\n\nclass RNN(nn.Module):\n    def __init__(self, hidden_size, output_keep_prob):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.bid_rnn = nn.GRU(hidden_size, hidden_size, batch_first=True, bidirectional=True)\n        self.output_keep_prob = output_keep_prob\n\n        self.out_dropout = nn.Dropout(p=(1 - self.output_keep_prob))\n\n    def forward(self, seq, mask):\n        # seq: (N, C, L)\n        # mask: (N, L)\n        max_len = seq.size()[2]\n        length = get_length(mask)\n        seq = torch.transpose(seq, 1, 2)  # to (N, L, C)\n        packed_seq = nn.utils.rnn.pack_padded_sequence(seq, length, batch_first=True,\n                                                       enforce_sorted=False)\n        outputs, _ = self.bid_rnn(packed_seq)\n        outputs = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True,\n                                                   total_length=max_len)[0]\n        outputs = outputs.view(-1, max_len, 2, self.hidden_size).sum(2)  # (N, L, C)\n        outputs = self.out_dropout(outputs)  # output dropout\n        return torch.transpose(outputs, 1, 2)  # back to: (N, C, L)\n\n\nclass LinearCombine(nn.Module):\n    def __init__(self, layers_num, trainable=True, input_aware=False, word_level=False):\n        super(LinearCombine, self).__init__()\n        self.input_aware = input_aware\n        self.word_level = word_level\n\n        if input_aware:\n            raise NotImplementedError(""Input aware is not supported."")\n        self.w = nn.Parameter(torch.full((layers_num, 1, 1, 1), 1.0 / layers_num),\n                              requires_grad=trainable)\n\n    def forward(self, seq):\n        nw = F.softmax(self.w, dim=0)\n        seq = torch.mul(seq, nw)\n        seq = torch.sum(seq, dim=0)\n        return seq\n'"
examples/nas/textnas/retrain.py,14,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport sys\nimport os\nimport logging\nimport pickle\nimport shutil\nimport random\nimport math\n\nimport time\nimport datetime\nimport argparse\nimport distutils.util\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch import optim\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as Func\n\nfrom model import Model\nfrom nni.nas.pytorch.fixed import apply_fixed_architecture\nfrom dataloader import read_data_sst\n\n\nlogger = logging.getLogger(""nni.textnas"")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        ""--reset_output_dir"",\n        type=distutils.util.strtobool,\n        default=True,\n        help=""Whether to clean the output dir if existed. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_fixed_arc"",\n        type=str,\n        required=True,\n        help=""Architecture json file. (default: %(default)s)"")\n    parser.add_argument(\n        ""--data_path"",\n        type=str,\n        default=""data"",\n        help=""Directory containing the dataset and embedding file. (default: %(default)s)"")\n    parser.add_argument(\n        ""--output_dir"",\n        type=str,\n        default=""output"",\n        help=""The output directory. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr_decay_scheme"",\n        type=str,\n        default=""cosine"",\n        help=""Learning rate annealing strategy, only \'cosine\' supported. (default: %(default)s)"")\n    parser.add_argument(\n        ""--batch_size"",\n        type=int,\n        default=128,\n        help=""Number of samples each batch for training. (default: %(default)s)"")\n    parser.add_argument(\n        ""--eval_batch_size"",\n        type=int,\n        default=128,\n        help=""Number of samples each batch for evaluation. (default: %(default)s)"")\n    parser.add_argument(\n        ""--class_num"",\n        type=int,\n        default=5,\n        help=""The number of categories. (default: %(default)s)"")\n    parser.add_argument(\n        ""--global_seed"",\n        type=int,\n        default=1234,\n        help=""Seed for reproduction. (default: %(default)s)"")\n    parser.add_argument(\n        ""--max_input_length"",\n        type=int,\n        default=64,\n        help=""The maximum length of the sentence. (default: %(default)s)"")\n    parser.add_argument(\n        ""--num_epochs"",\n        type=int,\n        default=10,\n        help=""The number of training epochs. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_num_layers"",\n        type=int,\n        default=24,\n        help=""The layer number of the architecture. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_out_filters"",\n        type=int,\n        default=256,\n        help=""The dimension of hidden states. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_out_filters_scale"",\n        type=int,\n        default=1,\n        help=""The scale of hidden state dimension. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr_T_0"",\n        type=int,\n        default=10,\n        help=""The length of one cycle. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr_T_mul"",\n        type=int,\n        default=2,\n        help=""The multiplication factor per cycle. (default: %(default)s)"")\n    parser.add_argument(\n        ""--min_count"",\n        type=int,\n        default=1,\n        help=""The threshold to cut off low frequent words. (default: %(default)s)"")\n    parser.add_argument(\n        ""--train_ratio"",\n        type=float,\n        default=1.0,\n        help=""The sample ratio for the training set. (default: %(default)s)"")\n    parser.add_argument(\n        ""--valid_ratio"",\n        type=float,\n        default=1.0,\n        help=""The sample ratio for the dev set. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_grad_bound"",\n        type=float,\n        default=5.0,\n        help=""The threshold for gradient clipping. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr"",\n        type=float,\n        default=0.02,\n        help=""The initial learning rate. (default: %(default)s)"")\n    parser.add_argument(\n        ""--cnn_keep_prob"",\n        type=float,\n        default=0.8,\n        help=""Keep prob for cnn layer. (default: %(default)s)"")\n    parser.add_argument(\n        ""--final_output_keep_prob"",\n        type=float,\n        default=1.0,\n        help=""Keep prob for the last output layer. (default: %(default)s)"")\n    parser.add_argument(\n        ""--lstm_out_keep_prob"",\n        type=float,\n        default=0.8,\n        help=""Keep prob for the RNN layer. (default: %(default)s)"")\n    parser.add_argument(\n        ""--embed_keep_prob"",\n        type=float,\n        default=0.8,\n        help=""Keep prob for the embedding layer. (default: %(default)s)"")\n    parser.add_argument(\n        ""--attention_keep_prob"",\n        type=float,\n        default=0.8,\n        help=""Keep prob for the self-attention layer. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_l2_reg"",\n        type=float,\n        default=3e-6,\n        help=""Weight decay factor. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr_max"",\n        type=float,\n        default=0.002,\n        help=""The max learning rate. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_lr_min"",\n        type=float,\n        default=0.001,\n        help=""The min learning rate. (default: %(default)s)"")\n    parser.add_argument(\n        ""--child_optim_algo"",\n        type=str,\n        default=""adam"",\n        help=""Optimization algorithm. (default: %(default)s)"")\n    parser.add_argument(\n        ""--checkpoint_dir"",\n        type=str,\n        default=""best_checkpoint"",\n        help=""Path for saved checkpoints. (default: %(default)s)"")\n    parser.add_argument(\n        ""--output_type"",\n        type=str,\n        default=""avg"",\n        help=""Opertor type for the time steps reduction. (default: %(default)s)"")\n    parser.add_argument(\n        ""--multi_path"",\n        type=distutils.util.strtobool,\n        default=False,\n        help=""Search for multiple path in the architecture. (default: %(default)s)"")\n    parser.add_argument(\n        ""--is_binary"",\n        type=distutils.util.strtobool,\n        default=False,\n        help=""Binary label for sst dataset. (default: %(default)s)"")\n    parser.add_argument(\n        ""--is_cuda"",\n        type=distutils.util.strtobool,\n        default=True,\n        help=""Specify the device type. (default: %(default)s)"")\n    parser.add_argument(\n        ""--is_mask"",\n        type=distutils.util.strtobool,\n        default=True,\n        help=""Apply mask. (default: %(default)s)"")\n    parser.add_argument(\n        ""--fixed_seed"",\n        type=distutils.util.strtobool,\n        default=True,\n        help=""Fix the seed. (default: %(default)s)"")\n    parser.add_argument(\n        ""--load_checkpoint"",\n        type=distutils.util.strtobool,\n        default=False,\n        help=""Wether to load checkpoint. (default: %(default)s)"")\n    parser.add_argument(\n        ""--log_every"",\n        type=int,\n        default=50,\n        help=""How many steps to log. (default: %(default)s)"")\n    parser.add_argument(\n        ""--eval_every_epochs"",\n        type=int,\n        default=1,\n        help=""How many epochs to eval. (default: %(default)s)"")\n\n    global FLAGS\n\n    FLAGS = parser.parse_args()\n\n\ndef set_random_seed(seed):\n    logger.info(""set random seed for data reading: {}"".format(seed))\n    random.seed(seed)\n    os.environ[\'PYTHONHASHSEED\'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    if FLAGS.is_cuda:\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n\n\ndef get_model(embedding, num_layers):\n    logger.info(""num layers: {0}"".format(num_layers))\n    assert FLAGS.child_fixed_arc is not None, ""Architecture should be provided.""\n\n    child_model = Model(\n        embedding=embedding,\n        hidden_units=FLAGS.child_out_filters_scale * FLAGS.child_out_filters,\n        num_layers=num_layers,\n        num_classes=FLAGS.class_num,\n        choose_from_k=5 if FLAGS.multi_path else 1,\n        lstm_keep_prob=FLAGS.lstm_out_keep_prob,\n        cnn_keep_prob=FLAGS.cnn_keep_prob,\n        att_keep_prob=FLAGS.attention_keep_prob,\n        att_mask=FLAGS.is_mask,\n        embed_keep_prob=FLAGS.embed_keep_prob,\n        final_output_keep_prob=FLAGS.final_output_keep_prob,\n        global_pool=FLAGS.output_type)\n\n    apply_fixed_architecture(child_model, FLAGS.child_fixed_arc)\n    return child_model\n\n\ndef eval_once(child_model, device, eval_set, criterion, valid_dataloader=None, test_dataloader=None):\n    if eval_set == ""test"":\n        assert test_dataloader is not None\n        dataloader = test_dataloader\n    elif eval_set == ""valid"":\n        assert valid_dataloader is not None\n        dataloader = valid_dataloader\n    else:\n        raise NotImplementedError(""Unknown eval_set \'{}\'"".format(eval_set))\n\n    tot_acc = 0\n    tot = 0\n    losses = []\n\n    with torch.no_grad():  # save memory\n        for batch in dataloader:\n            (sent_ids, mask), labels = batch\n\n            sent_ids = sent_ids.to(device, non_blocking=True)\n            mask = mask.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            logits = child_model((sent_ids, mask))  # run\n\n            loss = criterion(logits, labels.long())\n            loss = loss.mean()\n            preds = logits.argmax(dim=1).long()\n            acc = torch.eq(preds, labels.long()).long().sum().item()\n\n            losses.append(loss)\n            tot_acc += acc\n            tot += len(labels)\n\n    losses = torch.tensor(losses)\n    loss = losses.mean()\n    if tot > 0:\n        final_acc = float(tot_acc) / tot\n    else:\n        final_acc = 0\n        logger.info(""Error in calculating final_acc"")\n    return final_acc, loss\n\n\ndef print_user_flags(FLAGS, line_limit=80):\n    log_strings = ""\\n"" + ""-"" * line_limit + ""\\n""\n    for flag_name in sorted(vars(FLAGS)):\n        value = ""{}"".format(getattr(FLAGS, flag_name))\n        log_string = flag_name\n        log_string += ""."" * (line_limit - len(flag_name) - len(value))\n        log_string += value\n        log_strings = log_strings + log_string\n        log_strings = log_strings + ""\\n""\n    log_strings += ""-"" * line_limit\n    logger.info(log_strings)\n\n\ndef count_model_params(trainable_params):\n    num_vars = 0\n    for var in trainable_params:\n        num_vars += np.prod([dim for dim in var.size()])\n    return num_vars\n\n\ndef update_lr(\n        optimizer,\n        epoch,\n        l2_reg=1e-4,\n        lr_warmup_val=None,\n        lr_init=0.1,\n        lr_decay_scheme=""cosine"",\n        lr_max=0.002,\n        lr_min=0.000000001,\n        lr_T_0=4,\n        lr_T_mul=1,\n        sync_replicas=False,\n        num_aggregate=None,\n        num_replicas=None):\n    if lr_decay_scheme == ""cosine"":\n        assert lr_max is not None, ""Need lr_max to use lr_cosine""\n        assert lr_min is not None, ""Need lr_min to use lr_cosine""\n        assert lr_T_0 is not None, ""Need lr_T_0 to use lr_cosine""\n        assert lr_T_mul is not None, ""Need lr_T_mul to use lr_cosine""\n\n        T_i = lr_T_0\n        t_epoch = epoch\n        last_reset = 0\n        while True:\n            t_epoch -= T_i\n            if t_epoch < 0:\n              break\n            last_reset += T_i\n            T_i *= lr_T_mul\n\n        T_curr = epoch - last_reset\n\n        def _update():\n            rate = T_curr / T_i * 3.1415926\n            lr = lr_min + 0.5 * (lr_max - lr_min) * (1.0 + math.cos(rate))\n            return lr\n\n        learning_rate = _update()\n    else:\n        raise ValueError(""Unknown learning rate decay scheme {}"".format(lr_decay_scheme))\n\n    #update lr in optimizer\n    for params_group in optimizer.param_groups:\n        params_group[\'lr\'] = learning_rate\n    return learning_rate\n\n\ndef train(device, data_path, output_dir, num_layers):\n    logger.info(""Build dataloader"")\n    train_dataset, valid_dataset, test_dataset, embedding = \\\n        read_data_sst(data_path,\n                      FLAGS.max_input_length,\n                      FLAGS.min_count,\n                      train_ratio=FLAGS.train_ratio,\n                      valid_ratio=FLAGS.valid_ratio,\n                      is_binary=FLAGS.is_binary)\n    train_dataloader = DataLoader(train_dataset, batch_size=FLAGS.batch_size, shuffle=True, pin_memory=True)\n    test_dataloader = DataLoader(test_dataset, batch_size=FLAGS.eval_batch_size, pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset, batch_size=FLAGS.eval_batch_size, pin_memory=True)\n\n    logger.info(""Build model"")\n    child_model = get_model(embedding, num_layers)\n    logger.info(""Finish build model"")\n\n    #for name, var in child_model.named_parameters():\n    #    logger.info(name, var.size(), var.requires_grad)  # output all params\n\n    num_vars = count_model_params(child_model.parameters())\n    logger.info(""Model has {} params"".format(num_vars))\n\n    for m in child_model.modules():  # initializer\n        if isinstance(m, (nn.Conv1d, nn.Linear)):\n            nn.init.xavier_uniform_(m.weight)\n\n    criterion = nn.CrossEntropyLoss()\n\n    # get optimizer\n    if FLAGS.child_optim_algo == ""adam"":\n        optimizer = optim.Adam(child_model.parameters(), eps=1e-3, weight_decay=FLAGS.child_l2_reg)  # with L2\n    else:\n        raise ValueError(""Unknown optim_algo {}"".format(FLAGS.child_optim_algo))\n\n    child_model.to(device)\n    criterion.to(device)\n\n    logger.info(""Start training"")\n    start_time = time.time()\n    step = 0\n\n    # save path\n    model_save_path = os.path.join(FLAGS.output_dir, ""model.pth"")\n    best_model_save_path = os.path.join(FLAGS.output_dir, ""best_model.pth"")\n    best_acc = 0\n    start_epoch = 0\n    if FLAGS.load_checkpoint:\n        if os.path.isfile(model_save_path):\n            checkpoint = torch.load(model_save_path, map_location = torch.device(\'cpu\'))\n            step = checkpoint[\'step\']\n            start_epoch = checkpoint[\'epoch\']\n            child_model.load_state_dict(checkpoint[\'child_model_state_dict\'])\n            optimizer.load_state_dict(checkpoint[\'optimizer_state_dict\'])\n\n    for epoch in range(start_epoch, FLAGS.num_epochs):\n        lr = update_lr(optimizer,\n                       epoch,\n                       l2_reg=FLAGS.child_l2_reg,\n                       lr_warmup_val=None,\n                       lr_init=FLAGS.child_lr,\n                       lr_decay_scheme=FLAGS.child_lr_decay_scheme,\n                       lr_max=FLAGS.child_lr_max,\n                       lr_min=FLAGS.child_lr_min,\n                       lr_T_0=FLAGS.child_lr_T_0,\n                       lr_T_mul=FLAGS.child_lr_T_mul)\n        child_model.train()\n        for batch in train_dataloader:\n            (sent_ids, mask), labels = batch\n\n            sent_ids = sent_ids.to(device, non_blocking=True)\n            mask = mask.to(device, non_blocking=True)\n            labels = labels.to(device, non_blocking=True)\n\n            step += 1\n\n            logits = child_model((sent_ids, mask))  # run\n\n            loss = criterion(logits, labels.long())\n            loss = loss.mean()\n            preds = logits.argmax(dim=1).long()\n            acc = torch.eq(preds, labels.long()).long().sum().item()\n\n            optimizer.zero_grad()\n            loss.backward()\n            grad_norm = 0\n            trainable_params = child_model.parameters()\n\n            assert FLAGS.child_grad_bound is not None, ""Need grad_bound to clip gradients.""\n            # compute the gradient norm value\n            grad_norm = nn.utils.clip_grad_norm_(trainable_params, 99999999)\n            for param in trainable_params:\n                nn.utils.clip_grad_norm_(param, FLAGS.child_grad_bound)  # clip grad\n\n            optimizer.step()\n\n            if step % FLAGS.log_every == 0:\n                curr_time = time.time()\n                log_string = """"\n                log_string += ""epoch={:<6d}"".format(epoch)\n                log_string += ""ch_step={:<6d}"".format(step)\n                log_string += "" loss={:<8.6f}"".format(loss)\n                log_string += "" lr={:<8.4f}"".format(lr)\n                log_string += "" |g|={:<8.4f}"".format(grad_norm)\n                log_string += "" tr_acc={:<3d}/{:>3d}"".format(acc, logits.size()[0])\n                log_string += "" mins={:<10.2f}"".format(float(curr_time - start_time) / 60)\n                logger.info(log_string)\n\n        epoch += 1\n        save_state = {\n            \'step\' : step,\n            \'epoch\' : epoch,\n            \'child_model_state_dict\' : child_model.state_dict(),\n            \'optimizer_state_dict\' : optimizer.state_dict()}\n        torch.save(save_state, model_save_path)\n        child_model.eval()\n        logger.info(""Epoch {}: Eval"".format(epoch))\n        eval_acc, eval_loss = eval_once(child_model, device, ""test"", criterion, test_dataloader=test_dataloader)\n        logger.info(""ch_step={} {}_accuracy={:<6.4f} {}_loss={:<6.4f}"".format(step, ""test"", eval_acc, ""test"", eval_loss))\n        if eval_acc > best_acc:\n            best_acc = eval_acc\n            logger.info(""Save best model"")\n            save_state = {\n                \'step\' : step,\n                \'epoch\' : epoch,\n                \'child_model_state_dict\' : child_model.state_dict(),\n                \'optimizer_state_dict\' : optimizer.state_dict()}\n            torch.save(save_state, best_model_save_path)\n\n    return eval_acc\n\n\ndef main():\n    parse_args()\n    if not os.path.isdir(FLAGS.output_dir):\n        logger.info(""Path {} does not exist. Creating."".format(FLAGS.output_dir))\n        os.makedirs(FLAGS.output_dir)\n    elif FLAGS.reset_output_dir:\n        logger.info(""Path {} exists. Remove and remake."".format(FLAGS.output_dir))\n        shutil.rmtree(FLAGS.output_dir, ignore_errors=True)\n        os.makedirs(FLAGS.output_dir)\n\n    print_user_flags(FLAGS)\n\n    if FLAGS.fixed_seed:\n        set_random_seed(FLAGS.global_seed)\n\n    device = torch.device(""cuda"" if FLAGS.is_cuda else ""cpu"")\n    train(device, FLAGS.data_path, FLAGS.output_dir, FLAGS.child_num_layers)\n\n\nif __name__ == ""__main__"":\n  main()\n'"
examples/nas/textnas/search.py,12,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport os\nimport random\nfrom argparse import ArgumentParser\nfrom itertools import cycle\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom nni.nas.pytorch.enas import EnasMutator, EnasTrainer\nfrom nni.nas.pytorch.callbacks import LRSchedulerCallback\n\nfrom dataloader import read_data_sst\nfrom model import Model\nfrom utils import accuracy\n\n\nlogger = logging.getLogger(""nni.textnas"")\n\n\nclass TextNASTrainer(EnasTrainer):\n    def __init__(self, *args, train_loader=None, valid_loader=None, test_loader=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.test_loader = test_loader\n\n    def init_dataloader(self):\n        pass\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser(""textnas"")\n    parser.add_argument(""--batch-size"", default=128, type=int)\n    parser.add_argument(""--log-frequency"", default=50, type=int)\n    parser.add_argument(""--seed"", default=1234, type=int)\n    parser.add_argument(""--epochs"", default=10, type=int)\n    parser.add_argument(""--lr"", default=5e-3, type=float)\n    args = parser.parse_args()\n\n    torch.manual_seed(args.seed)\n    torch.cuda.manual_seed_all(args.seed)\n    np.random.seed(args.seed)\n    random.seed(args.seed)\n    torch.backends.cudnn.deterministic = True\n\n    device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")\n    train_dataset, valid_dataset, test_dataset, embedding = read_data_sst(""data"")\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, num_workers=4, shuffle=True)\n    valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=args.batch_size, num_workers=4, shuffle=True)\n    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, num_workers=4)\n    train_loader, valid_loader = cycle(train_loader), cycle(valid_loader)\n    model = Model(embedding)\n\n    mutator = EnasMutator(model, temperature=None, tanh_constant=None, entropy_reduction=""mean"")\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, eps=1e-3, weight_decay=2e-6)\n    lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs, eta_min=1e-5)\n\n    trainer = TextNASTrainer(model,\n                             loss=criterion,\n                             metrics=lambda output, target: {""acc"": accuracy(output, target)},\n                             reward_function=accuracy,\n                             optimizer=optimizer,\n                             callbacks=[LRSchedulerCallback(lr_scheduler)],\n                             batch_size=args.batch_size,\n                             num_epochs=args.epochs,\n                             dataset_train=None,\n                             dataset_valid=None,\n                             train_loader=train_loader,\n                             valid_loader=valid_loader,\n                             test_loader=test_loader,\n                             log_frequency=args.log_frequency,\n                             mutator=mutator,\n                             mutator_lr=2e-3,\n                             mutator_steps=500,\n                             mutator_steps_aggregate=1,\n                             child_steps=3000,\n                             baseline_decay=0.99,\n                             test_arc_per_epoch=10)\n    trainer.train()\n    os.makedirs(""checkpoints"", exist_ok=True)\n    for i in range(20):\n        trainer.export(os.path.join(""checkpoints"", ""architecture_%02d.json"" % i))\n'"
examples/nas/textnas/utils.py,10,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport torch\nimport torch.nn as nn\n\nINF = 1E10\nEPS = 1E-12\n\nlogger = logging.getLogger(""nni.textnas"")\n\n\ndef get_length(mask):\n    length = torch.sum(mask, 1)\n    length = length.long()\n    return length\n\n\nclass GlobalAvgPool(nn.Module):\n    def forward(self, x, mask):\n        x = torch.sum(x, 2)\n        length = torch.sum(mask, 1, keepdim=True).float()\n        length += torch.eq(length, 0.0).float() * EPS\n        length = length.repeat(1, x.size()[1])\n        x /= length\n        return x\n\n\nclass GlobalMaxPool(nn.Module):\n    def forward(self, x, mask):\n        mask = torch.eq(mask.float(), 0.0).long()\n        mask = torch.unsqueeze(mask, dim=1).repeat(1, x.size()[1], 1)\n        mask *= -INF\n        x += mask\n        x, _ = torch.max(x + mask, 2)\n        return x\n\n\nclass IteratorWrapper:\n    def __init__(self, loader):\n        self.loader = loader\n        self.iterator = None\n\n    def __iter__(self):\n        self.iterator = iter(self.loader)\n        return self\n\n    def __len__(self):\n        return len(self.loader)\n\n    def __next__(self):\n        data = next(self.iterator)\n        text, length = data.text\n        max_length = text.size(1)\n        label = data.label - 1\n        bs = label.size(0)\n        mask = torch.arange(max_length, device=length.device).unsqueeze(0).repeat(bs, 1)\n        mask = mask < length.unsqueeze(-1).repeat(1, max_length)\n        return (text, mask), label\n\n\ndef accuracy(output, target):\n    batch_size = target.size(0)\n    _, predicted = torch.max(output.data, 1)\n    return (predicted == target).sum().item() / batch_size\n'"
examples/trials/auto-gbdt/main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\'\'\'\nThis project is for automatically tuning parameters for GBDT.\n\'\'\'\nimport logging\n\nimport lightgbm as lgb\nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\n\nimport nni\n\nLOG = logging.getLogger(\'auto-gbdt\')\n\n# specify your configurations as a dict\ndef get_default_parameters():\n    params = {\n        \'boosting_type\': \'gbdt\',\n        \'objective\': \'regression\',\n        \'metric\': {\'l2\', \'auc\'},\n        \'num_leaves\': 31,\n        \'learning_rate\': 0.05,\n        \'feature_fraction\': 0.9,\n        \'bagging_fraction\': 0.8,\n        \'bagging_freq\': 5,\n        \'verbose\': 0\n    }\n    return params\n\n\ndef load_data(train_path=\'./data/regression.train\', test_path=\'./data/regression.test\'):\n    \'\'\'\n    Load or create dataset\n    \'\'\'\n    print(\'Load data...\')\n    df_train = pd.read_csv(train_path, header=None, sep=\'\\t\')\n    df_test = pd.read_csv(test_path, header=None, sep=\'\\t\')\n    num = len(df_train)\n    split_num = int(0.9 * num)\n\n    y_train = df_train[0].values\n    y_test = df_test[0].values\n    y_eval = y_train[split_num:]\n    y_train = y_train[:split_num]\n\n    X_train = df_train.drop(0, axis=1).values\n    X_test = df_test.drop(0, axis=1).values\n    X_eval = X_train[split_num:, :]\n    X_train = X_train[:split_num, :]\n\n    # create dataset for lightgbm\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_eval, y_eval, reference=lgb_train)\n\n    return lgb_train, lgb_eval, X_test, y_test\n\ndef run(lgb_train, lgb_eval, params, X_test, y_test):\n    print(\'Start training...\')\n\n    params[\'num_leaves\'] = int(params[\'num_leaves\'])\n\n    # train\n    gbm = lgb.train(params,\n                    lgb_train,\n                    num_boost_round=20,\n                    valid_sets=lgb_eval,\n                    early_stopping_rounds=5)\n\n    print(\'Start predicting...\')\n\n    # predict\n    y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)\n\n    # eval\n    rmse = mean_squared_error(y_test, y_pred) ** 0.5\n    print(\'The rmse of prediction is:\', rmse)\n\n    nni.report_final_result(rmse)\n\nif __name__ == \'__main__\':\n    lgb_train, lgb_eval, X_test, y_test = load_data()\n\n    try:\n        # get parameters from tuner\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = get_default_parameters()\n        PARAMS.update(RECEIVED_PARAMS)\n        LOG.debug(PARAMS)\n\n        # train\n        run(lgb_train, lgb_eval, PARAMS, X_test, y_test)\n    except Exception as exception:\n        LOG.exception(exception)\n        raise'"
examples/trials/cifar10_pytorch/main.py,10,"b'\'\'\'Train CIFAR10 with PyTorch.\'\'\'\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.backends.cudnn as cudnn\n\nimport torchvision\nimport torchvision.transforms as transforms\n\nimport os\nimport argparse\nimport logging\n\nfrom models import *\nfrom utils import progress_bar\n\nimport nni\n\n_logger = logging.getLogger(""cifar10_pytorch_automl"")\n\ntrainloader = None\ntestloader = None\nnet = None\ncriterion = None\noptimizer = None\ndevice = \'cuda\' if torch.cuda.is_available() else \'cpu\'\nbest_acc = 0.0  # best test accuracy\nstart_epoch = 0  # start from epoch 0 or last checkpoint epoch\n\ndef prepare(args):\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    # Data\n    print(\'==> Preparing data..\')\n    transform_train = transforms.Compose([\n        transforms.RandomCrop(32, padding=4),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    transform_test = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n    ])\n\n    trainset = torchvision.datasets.CIFAR10(root=\'./data\', train=True, download=True, transform=transform_train)\n    trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n\n    testset = torchvision.datasets.CIFAR10(root=\'./data\', train=False, download=True, transform=transform_test)\n    testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n\n    #classes = (\'plane\', \'car\', \'bird\', \'cat\', \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    # Model\n    print(\'==> Building model..\')\n    if args[\'model\'] == \'vgg\':\n        net = VGG(\'VGG19\')\n    if args[\'model\'] == \'resnet18\':\n        net = ResNet18()\n    if args[\'model\'] == \'googlenet\':\n        net = GoogLeNet()\n    if args[\'model\'] == \'densenet121\':\n        net = DenseNet121()\n    if args[\'model\'] == \'mobilenet\':\n        net = MobileNet()\n    if args[\'model\'] == \'dpn92\':\n        net = DPN92()\n    if args[\'model\'] == \'shufflenetg2\':\n        net = ShuffleNetG2()\n    if args[\'model\'] == \'senet18\':\n        net = SENet18()\n\n    net = net.to(device)\n    if device == \'cuda\':\n        net = torch.nn.DataParallel(net)\n        cudnn.benchmark = True\n\n    criterion = nn.CrossEntropyLoss()\n    #optimizer = optim.SGD(net.parameters(), lr=args[\'lr\'], momentum=0.9, weight_decay=5e-4)\n\n    if args[\'optimizer\'] == \'SGD\':\n        optimizer = optim.SGD(net.parameters(), lr=args[\'lr\'], momentum=0.9, weight_decay=5e-4)\n    if args[\'optimizer\'] == \'Adadelta\':\n        optimizer = optim.Adadelta(net.parameters(), lr=args[\'lr\'])\n    if args[\'optimizer\'] == \'Adagrad\':\n        optimizer = optim.Adagrad(net.parameters(), lr=args[\'lr\'])\n    if args[\'optimizer\'] == \'Adam\':\n        optimizer = optim.Adam(net.parameters(), lr=args[\'lr\'])\n    if args[\'optimizer\'] == \'Adamax\':\n        optimizer = optim.Adam(net.parameters(), lr=args[\'lr\'])\n\n\n# Training\ndef train(epoch, batches=-1):\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    print(\'\\nEpoch: %d\' % epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        acc = 100.*correct/total\n\n        progress_bar(batch_idx, len(trainloader), \'Loss: %.3f | Acc: %.3f%% (%d/%d)\'\n            % (train_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n        if batches > 0 and (batch_idx+1) >= batches:\n            return\n\ndef test(epoch):\n    global best_acc\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            acc = 100.*correct/total\n\n            progress_bar(batch_idx, len(testloader), \'Loss: %.3f | Acc: %.3f%% (%d/%d)\'\n                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n\n    # Save checkpoint.\n    acc = 100.*correct/total\n    if acc > best_acc:\n        print(\'Saving..\')\n        state = {\n            \'net\': net.state_dict(),\n            \'acc\': acc,\n            \'epoch\': epoch,\n        }\n        if not os.path.isdir(\'checkpoint\'):\n            os.mkdir(\'checkpoint\')\n        torch.save(state, \'./checkpoint/ckpt.t7\')\n        best_acc = acc\n    return acc, best_acc\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--epochs"", type=int, default=200)\n\n    # Maximum mini-batches per epoch, for code testing purpose\n    parser.add_argument(""--batches"", type=int, default=-1)\n\n    args, _ = parser.parse_known_args()\n\n    try:\n        RCV_CONFIG = nni.get_next_parameter()\n        #RCV_CONFIG = {\'lr\': 0.1, \'optimizer\': \'Adam\', \'model\':\'senet18\'}\n        _logger.debug(RCV_CONFIG)\n\n        prepare(RCV_CONFIG)\n        acc = 0.0\n        best_acc = 0.0\n        for epoch in range(start_epoch, start_epoch+args.epochs):\n            train(epoch, args.batches)\n            acc, best_acc = test(epoch)\n            nni.report_intermediate_result(acc)\n\n        nni.report_final_result(best_acc)\n    except Exception as exception:\n        _logger.exception(exception)\n        raise\n'"
examples/trials/cifar10_pytorch/utils.py,5,"b""'''Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n'''\nimport os\nimport sys\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\ndef get_mean_and_std(dataset):\n    '''Compute the mean and std value of dataset.'''\n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True, num_workers=2)\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print('==> Computing mean and std..')\n    for inputs, targets in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:,i,:,:].mean()\n            std[i] += inputs[:,i,:,:].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\ndef init_params(net):\n    '''Init layer parameters.'''\n    for m in net.modules():\n        if isinstance(m, nn.Conv2d):\n            init.kaiming_normal(m.weight, mode='fan_out')\n            if m.bias:\n                init.constant(m.bias, 0)\n        elif isinstance(m, nn.BatchNorm2d):\n            init.constant(m.weight, 1)\n            init.constant(m.bias, 0)\n        elif isinstance(m, nn.Linear):\n            init.normal(m.weight, std=1e-3)\n            if m.bias:\n                init.constant(m.bias, 0)\n\nterm_width = 0\ntry:\n    term_width = os.get_terminal_size().columns\nexcept Exception as exception:\n    term_width = 200\nterm_width = int(term_width)\n\nTOTAL_BAR_LENGTH = 65.\nlast_time = time.time()\nbegin_time = last_time\ndef progress_bar(current, total, msg=None):\n    global last_time, begin_time\n    if current == 0:\n        begin_time = time.time()  # Reset for new bar.\n\n    cur_len = int(TOTAL_BAR_LENGTH*current/total)\n    rest_len = int(TOTAL_BAR_LENGTH - cur_len) - 1\n\n    sys.stdout.write(' [')\n    for i in range(cur_len):\n        sys.stdout.write('=')\n    sys.stdout.write('>')\n    for i in range(rest_len):\n        sys.stdout.write('.')\n    sys.stdout.write(']')\n\n    cur_time = time.time()\n    step_time = cur_time - last_time\n    last_time = cur_time\n    tot_time = cur_time - begin_time\n\n    L = []\n    L.append('  Step: %s' % format_time(step_time))\n    L.append(' | Tot: %s' % format_time(tot_time))\n    if msg:\n        L.append(' | ' + msg)\n\n    msg = ''.join(L)\n    sys.stdout.write(msg)\n    for i in range(term_width-int(TOTAL_BAR_LENGTH)-len(msg)-3):\n        sys.stdout.write(' ')\n\n    # Go back to the center of the bar.\n    for i in range(term_width-int(TOTAL_BAR_LENGTH/2)+2):\n        sys.stdout.write('\\b')\n    sys.stdout.write(' %d/%d ' % (current+1, total))\n\n    if current < total-1:\n        sys.stdout.write('\\r')\n    else:\n        sys.stdout.write('\\n')\n    sys.stdout.flush()\n\ndef format_time(seconds):\n    days = int(seconds / 3600/24)\n    seconds = seconds - days*3600*24\n    hours = int(seconds / 3600)\n    seconds = seconds - hours*3600\n    minutes = int(seconds / 60)\n    seconds = seconds - minutes*60\n    secondsf = int(seconds)\n    seconds = seconds - secondsf\n    millis = int(seconds*1000)\n\n    f = ''\n    i = 1\n    if days > 0:\n        f += str(days) + 'D'\n        i += 1\n    if hours > 0 and i <= 2:\n        f += str(hours) + 'h'\n        i += 1\n    if minutes > 0 and i <= 2:\n        f += str(minutes) + 'm'\n        i += 1\n    if secondsf > 0 and i <= 2:\n        f += str(secondsf) + 's'\n        i += 1\n    if millis > 0 and i <= 2:\n        f += str(millis) + 'ms'\n        i += 1\n    if f == '':\n        f = '0ms'\n    return f\n"""
examples/trials/efficientnet/tuner.py,0,"b'from nni.gridsearch_tuner.gridsearch_tuner import GridSearchTuner\n\n\nclass FixedProductTuner(GridSearchTuner):\n    """"""\n    This tuner is essentially grid search, but it guarantees all the parameters with alpha * beta^2 * gamma^2 is\n    approximately `product`.\n    """"""\n\n    def __init__(self, product):\n        """"""\n        :param product: the constant provided, should be 2 in EfficientNet-B1\n        """"""\n        super().__init__()\n        self.product = product\n\n    def _expand_parameters(self, para):\n        """"""\n        Filter out all qualified parameters\n        """"""\n        para = super()._expand_parameters(para)\n        if all([key in para[0] for key in [""alpha"", ""beta"", ""gamma""]]):  # if this is an interested set\n            ret_para = []\n            for p in para:\n                prod = p[""alpha""] * (p[""beta""] ** 2) * (p[""gamma""] ** 2)\n                if abs(prod - self.product) < 0.1:\n                    ret_para.append(p)\n            return ret_para\n        return para\n'"
examples/trials/ga_squad/attention.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\n\ndef _get_variable(variable_dict, name, shape, initializer=None, dtype=tf.float32):\n    if name not in variable_dict:\n        variable_dict[name] = tf.get_variable(\n            name=name, shape=shape, initializer=initializer, dtype=dtype)\n    return variable_dict[name]\n\nclass DotAttention:\n    \'\'\'\n    DotAttention\n    \'\'\'\n    def __init__(self, name,\n                 hidden_dim,\n                 is_vanilla=True,\n                 is_identity_transform=False,\n                 need_padding=False):\n        self._name = \'/\'.join([name, \'dot_att\'])\n        self._hidden_dim = hidden_dim\n        self._is_identity_transform = is_identity_transform\n        self._need_padding = need_padding\n        self._is_vanilla = is_vanilla\n        self._var = {}\n\n    @property\n    def is_identity_transform(self):\n        return self._is_identity_transform\n\n    @property\n    def is_vanilla(self):\n        return self._is_vanilla\n\n    @property\n    def need_padding(self):\n        return self._need_padding\n\n    @property\n    def hidden_dim(self):\n        return self._hidden_dim\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def var(self):\n        return self._var\n\n    def _get_var(self, name, shape, initializer=None):\n        with tf.variable_scope(self.name):\n            return _get_variable(self.var, name, shape, initializer)\n\n    def _define_params(self, src_dim, tgt_dim):\n        hidden_dim = self.hidden_dim\n        self._get_var(\'W\', [src_dim, hidden_dim])\n        if not self.is_vanilla:\n            self._get_var(\'V\', [src_dim, hidden_dim])\n            if self.need_padding:\n                self._get_var(\'V_s\', [src_dim, src_dim])\n                self._get_var(\'V_t\', [tgt_dim, tgt_dim])\n            if not self.is_identity_transform:\n                self._get_var(\'T\', [tgt_dim, src_dim])\n        self._get_var(\'U\', [tgt_dim, hidden_dim])\n        self._get_var(\'b\', [1, hidden_dim])\n        self._get_var(\'v\', [hidden_dim, 1])\n\n    def get_pre_compute(self, s):\n        \'\'\'\n        :param s: [src_sequence, batch_size, src_dim]\n        :return: [src_sequence, batch_size. hidden_dim]\n        \'\'\'\n        hidden_dim = self.hidden_dim\n        src_dim = s.get_shape().as_list()[-1]\n        assert src_dim is not None, \'src dim must be defined\'\n        W = self._get_var(\'W\', shape=[src_dim, hidden_dim])\n        b = self._get_var(\'b\', shape=[1, hidden_dim])\n        return tf.tensordot(s, W, [[2], [0]]) + b\n\n    def get_prob(self, src, tgt, mask, pre_compute, return_logits=False):\n        \'\'\'\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param h: [batch_size, tgt_dim] or [tgt_sequence_length, batch_size, tgt_dim]\n        :param mask: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_sizse]\n        :param pre_compute: [src_sequence_length, batch_size, hidden_dim]\n        :return: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_size]\n        \'\'\'\n        s_shape = src.get_shape().as_list()\n        h_shape = tgt.get_shape().as_list()\n        src_dim = s_shape[-1]\n        tgt_dim = h_shape[-1]\n        assert src_dim is not None, \'src dimension must be defined\'\n        assert tgt_dim is not None, \'tgt dimension must be defined\'\n\n        self._define_params(src_dim, tgt_dim)\n\n        if len(h_shape) == 2:\n            tgt = tf.expand_dims(tgt, 0)\n        if pre_compute is None:\n            pre_compute = self.get_pre_compute(src)\n\n        buf0 = pre_compute\n        buf1 = tf.tensordot(tgt, self.var[\'U\'], axes=[[2], [0]])\n        buf2 = tf.tanh(tf.expand_dims(buf0, 0) + tf.expand_dims(buf1, 1))\n\n        if not self.is_vanilla:\n            xh1 = tgt\n            xh2 = tgt\n            s1 = src\n            if self.need_padding:\n                xh1 = tf.tensordot(xh1, self.var[\'V_t\'], 1)\n                xh2 = tf.tensordot(xh2, self.var[\'S_t\'], 1)\n                s1 = tf.tensordot(s1, self.var[\'V_s\'], 1)\n            if not self.is_identity_transform:\n                xh1 = tf.tensordot(xh1, self.var[\'T\'], 1)\n                xh2 = tf.tensordot(xh2, self.var[\'T\'], 1)\n            buf3 = tf.expand_dims(s1, 0) * tf.expand_dims(xh1, 1)\n            buf3 = tf.tanh(tf.tensordot(buf3, self.var[\'V\'], axes=[[3], [0]]))\n            buf = tf.reshape(tf.tanh(buf2 + buf3), shape=tf.shape(buf3))\n        else:\n            buf = buf2\n        v = self.var[\'v\']\n        e = tf.tensordot(buf, v, [[3], [0]])\n        e = tf.squeeze(e, axis=[3])\n        tmp = tf.reshape(e + (mask - 1) * 10000.0, shape=tf.shape(e))\n        prob = tf.nn.softmax(tmp, 1)\n        if len(h_shape) == 2:\n            prob = tf.squeeze(prob, axis=[0])\n            tmp = tf.squeeze(tmp, axis=[0])\n        if return_logits:\n            return prob, tmp\n        return prob\n\n    def get_att(self, s, prob):\n        \'\'\'\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param prob: [src_sequence_length, batch_size]\\\n            or [tgt_sequence_length, src_sequence_length, batch_size]\n        :return: [batch_size, src_dim] or [tgt_sequence_length, batch_size, src_dim]\n        \'\'\'\n        buf = s * tf.expand_dims(prob, axis=-1)\n        att = tf.reduce_sum(buf, axis=-3)\n        return att'"
examples/trials/ga_squad/data.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nData processing script for the QA model.\n\'\'\'\n\nimport csv\nimport json\nfrom random import shuffle\n\nimport numpy as np\n\n\nclass WhitespaceTokenizer:\n    \'\'\'\n    Tokenizer for whitespace\n    \'\'\'\n    def tokenize(self, text):\n        \'\'\'\n        tokenize function in Tokenizer.\n        \'\'\'\n        start = -1\n        tokens = []\n        for i, character in enumerate(text):\n            if character == \' \' or character == \'\\t\':\n                if start >= 0:\n                    word = text[start:i]\n                    tokens.append({\n                        \'word\': word,\n                        \'original_text\': word,\n                        \'char_begin\': start,\n                        \'char_end\': i})\n                    start = -1\n            else:\n                if start < 0:\n                    start = i\n        if start >= 0:\n            tokens.append({\n                \'word\': text[start:len(text)],\n                \'original_text\': text[start:len(text)],\n                \'char_begin\': start,\n                \'char_end\': len(text)\n            })\n        return tokens\n\n\ndef load_from_file(path, fmt=None, is_training=True):\n    \'\'\'\n    load data from file\n    \'\'\'\n    if fmt is None:\n        fmt = \'squad\'\n    assert fmt in [\'squad\', \'csv\'], \'input format must be squad or csv\'\n    qp_pairs = []\n    if fmt == \'squad\':\n        with open(path) as data_file:\n            data = json.load(data_file)[\'data\']\n            for doc in data:\n                for paragraph in doc[\'paragraphs\']:\n                    passage = paragraph[\'context\']\n                    for qa_pair in paragraph[\'qas\']:\n                        question = qa_pair[\'question\']\n                        qa_id = qa_pair[\'id\']\n                        if not is_training:\n                            qp_pairs.append(\n                                {\'passage\': passage, \'question\': question, \'id\': qa_id})\n                        else:\n                            for answer in qa_pair[\'answers\']:\n                                answer_begin = int(answer[\'answer_start\'])\n                                answer_end = answer_begin + len(answer[\'text\'])\n                                qp_pairs.append({\'passage\': passage,\n                                                 \'question\': question,\n                                                 \'id\': qa_id,\n                                                 \'answer_begin\': answer_begin,\n                                                 \'answer_end\': answer_end})\n    else:\n        with open(path, newline=\'\') as csvfile:\n            reader = csv.reader(csvfile, delimiter=\'\\t\')\n            line_num = 0\n            for row in reader:\n                qp_pairs.append(\n                    {\'passage\': row[1], \'question\': row[0], \'id\': line_num})\n                line_num += 1\n    return qp_pairs\n\n\ndef tokenize(qp_pair, tokenizer=None, is_training=False):\n    \'\'\'\n    tokenize function.\n    \'\'\'\n    question_tokens = tokenizer.tokenize(qp_pair[\'question\'])\n    passage_tokens = tokenizer.tokenize(qp_pair[\'passage\'])\n    if is_training:\n        question_tokens = question_tokens[:300]\n        passage_tokens = passage_tokens[:300]\n    passage_tokens.insert(\n        0, {\'word\': \'<BOS>\', \'original_text\': \'<BOS>\', \'char_begin\': 0, \'char_end\': 0})\n    passage_tokens.append(\n        {\'word\': \'<EOS>\', \'original_text\': \'<EOS>\', \'char_begin\': 0, \'char_end\': 0})\n    qp_pair[\'question_tokens\'] = question_tokens\n    qp_pair[\'passage_tokens\'] = passage_tokens\n\n\ndef collect_vocab(qp_pairs):\n    \'\'\'\n    Build the vocab from corpus.\n    \'\'\'\n    vocab = set()\n    for qp_pair in qp_pairs:\n        for word in qp_pair[\'question_tokens\']:\n            vocab.add(word[\'word\'])\n        for word in qp_pair[\'passage_tokens\']:\n            vocab.add(word[\'word\'])\n    return vocab\n\n\ndef shuffle_step(entries, step):\n    \'\'\'\n    Shuffle the step\n    \'\'\'\n    answer = []\n    for i in range(0, len(entries), step):\n        sub = entries[i:i+step]\n        shuffle(sub)\n        answer += sub\n    return answer\n\n\ndef get_batches(qp_pairs, batch_size, need_sort=True):\n    \'\'\'\n    Get batches data and shuffle.\n    \'\'\'\n    if need_sort:\n        qp_pairs = sorted(qp_pairs, key=lambda qp: (\n            len(qp[\'passage_tokens\']), qp[\'id\']), reverse=True)\n    batches = [{\'qp_pairs\': qp_pairs[i:(i + batch_size)]}\n               for i in range(0, len(qp_pairs), batch_size)]\n    shuffle(batches)\n    return batches\n\n\ndef get_char_input(data, char_dict, max_char_length):\n    \'\'\'\n    Get char input.\n    \'\'\'\n    batch_size = len(data)\n    sequence_length = max(len(d) for d in data)\n    char_id = np.zeros((max_char_length, sequence_length,\n                        batch_size), dtype=np.int32)\n    char_lengths = np.zeros((sequence_length, batch_size), dtype=np.float32)\n    for batch_idx in range(0, min(len(data), batch_size)):\n        batch_data = data[batch_idx]\n        for sample_idx in range(0, min(len(batch_data), sequence_length)):\n            word = batch_data[sample_idx][\'word\']\n            char_lengths[sample_idx, batch_idx] = min(len(word), max_char_length)\n            for i in range(0, min(len(word), max_char_length)):\n                char_id[i, sample_idx, batch_idx] = get_id(char_dict, word[i])\n    return char_id, char_lengths\n\n\ndef get_word_input(data, word_dict, embed, embed_dim):\n    \'\'\'\n    Get word input.\n    \'\'\'\n    batch_size = len(data)\n    max_sequence_length = max(len(d) for d in data)\n    sequence_length = max_sequence_length\n    word_input = np.zeros((max_sequence_length, batch_size,\n                           embed_dim), dtype=np.float32)\n    ids = np.zeros((sequence_length, batch_size), dtype=np.int32)\n    masks = np.zeros((sequence_length, batch_size), dtype=np.float32)\n    lengths = np.zeros([batch_size], dtype=np.int32)\n\n    for batch_idx in range(0, min(len(data), batch_size)):\n        batch_data = data[batch_idx]\n\n        lengths[batch_idx] = len(batch_data)\n\n        for sample_idx in range(0, min(len(batch_data), sequence_length)):\n            word = batch_data[sample_idx][\'word\'].lower()\n            if word in word_dict.keys():\n                word_input[sample_idx, batch_idx] = embed[word_dict[word]]\n                ids[sample_idx, batch_idx] = word_dict[word]\n            masks[sample_idx, batch_idx] = 1\n\n    word_input = np.reshape(word_input, (-1, embed_dim))\n    return word_input, ids, masks, lengths\n\n\ndef get_word_index(tokens, char_index):\n    \'\'\'\n    Given word return word index.\n    \'\'\'\n    for (i, token) in enumerate(tokens):\n        if token[\'char_end\'] == 0:\n            continue\n        if token[\'char_begin\'] <= char_index and char_index <= token[\'char_end\']:\n            return i\n    return 0\n\n\ndef get_answer_begin_end(data):\n    \'\'\'\n    Get answer\'s index of begin and end.\n    \'\'\'\n    begin = []\n    end = []\n    for qa_pair in data:\n        tokens = qa_pair[\'passage_tokens\']\n        char_begin = qa_pair[\'answer_begin\']\n        char_end = qa_pair[\'answer_end\']\n        word_begin = get_word_index(tokens, char_begin)\n        word_end = get_word_index(tokens, char_end)\n        begin.append(word_begin)\n        end.append(word_end)\n    return np.asarray(begin), np.asarray(end)\n\n\ndef get_id(word_dict, word):\n    \'\'\'\n    Given word, return word id.\n    \'\'\'\n    if word in word_dict.keys():\n        return word_dict[word]\n    return word_dict[\'<unk>\']\n\n\ndef get_buckets(min_length, max_length, bucket_count):\n    \'\'\'\n    Get bucket by length.\n    \'\'\'\n    if bucket_count <= 0:\n        return [max_length]\n    unit_length = int((max_length - min_length) // (bucket_count))\n    buckets = [min_length + unit_length *\n               (i + 1) for i in range(0, bucket_count)]\n    buckets[-1] = max_length\n    return buckets\n\n\ndef find_bucket(length, buckets):\n    \'\'\'\n    Find bucket.\n    \'\'\'\n    for bucket in buckets:\n        if length <= bucket:\n            return bucket\n    return buckets[-1]\n'"
examples/trials/ga_squad/evaluate.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nEvaluation scripts for QA model.\n\'\'\'\n\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\ndef normalize_answer(str_input):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        \'\'\'\n        Remove ""a|an|the""\n        \'\'\'\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        \'\'\'\n        Remove unnessary whitespace\n        \'\'\'\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        \'\'\'\n        Remove punc\n        \'\'\'\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        \'\'\'\n        Change string to lower form.\n        \'\'\'\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(str_input))))\n\ndef f1_score(prediction, ground_truth):\n    \'\'\'\n    Calculate the f1 score.\n    \'\'\'\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    precision = 1.0 * num_same / len(prediction_tokens)\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1_result = (2 * precision * recall) / (precision + recall)\n    return f1_result\n\ndef exact_match_score(prediction, ground_truth):\n    \'\'\'\n    Calculate the match score with prediction and ground truth.\n    \'\'\'\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    \'\'\'\n    Metric max over the ground truths.\n    \'\'\'\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\ndef _evaluate(dataset, predictions):\n    \'\'\'\n    Evaluate function.\n    \'\'\'\n    f1_result = exact_match = total = 0\n    count = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa_pair in paragraph[\'qas\']:\n                total += 1\n                if qa_pair[\'id\'] not in predictions:\n                    count += 1\n                    continue\n                ground_truths = list(map(lambda x: x[\'text\'], qa_pair[\'answers\']))\n                prediction = predictions[qa_pair[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1_result += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n    print(\'total\', total, \'exact_match\', exact_match, \'unanswer_question \', count)\n    exact_match = 100.0 * exact_match / total\n    f1_result = 100.0 * f1_result / total\n    return {\'exact_match\': exact_match, \'f1\': f1_result}\n\ndef evaluate(data_file, pred_file):\n    \'\'\'\n    Evaluate.\n    \'\'\'\n    expected_version = \'1.1\'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json[\'version\'] != expected_version:\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(pred_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    # print(json.dumps(evaluate(dataset, predictions)))\n    result = _evaluate(dataset, predictions)\n    # print(\'em:\', result[\'exact_match\'], \'f1:\', result[\'f1\'])\n    return result[\'exact_match\']\n\ndef evaluate_with_predictions(data_file, predictions):\n    \'\'\'\n    Evalutate with predictions/\n    \'\'\'\n    expected_version = \'1.1\'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json[\'version\'] != expected_version:\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    result = _evaluate(dataset, predictions)\n    return result[\'exact_match\']\n\nif __name__ == \'__main__\':\n    EXPECT_VERSION = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + EXPECT_VERSION)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    print(evaluate(args.dataset_file, args.prediction_file))\n'"
examples/trials/ga_squad/graph.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\'\'\'\nGraph is customed-define class, this module contains related class and function about graph.\n\'\'\'\n\n\nimport copy\nimport json\nimport random\nfrom enum import Enum, unique\n\n@unique\nclass LayerType(Enum):\n    \'\'\'\n    Layer type\n    \'\'\'\n    attention = 0\n    self_attention = 1\n    rnn = 2\n    input = 3\n    output = 4\n\nclass Layer(object):\n    \'\'\'\n    Layer class, which contains the information of graph.\n    \'\'\'\n    def __init__(self, graph_type, inputs=None, output=None, size=None):\n        self.input = inputs if inputs is not None else []\n        self.output = output if output is not None else []\n        self.graph_type = graph_type\n        self.is_delete = False\n        self.size = size\n        if graph_type == LayerType.attention.value:\n            self.input_size = 2\n            self.output_size = 1\n        elif graph_type == LayerType.rnn.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.self_attention.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.input.value:\n            self.input_size = 0\n            self.output_size = 1\n        elif graph_type == LayerType.output.value:\n            self.input_size = 1\n            self.output_size = 0\n        else:\n            print(graph_type)\n    def set_size(self, graph_id, size):\n        \'\'\'\n        Set size.\n        \'\'\'\n        if self.graph_type == LayerType.attention.value:\n            if self.input[0] == graph_id:\n                self.size = size\n        if self.graph_type == LayerType.rnn.value:\n            self.size = size\n        if self.graph_type == LayerType.self_attention.value:\n            self.size = size\n        if self.graph_type == LayerType.output.value:\n            if self.size != size:\n                return False\n        return True\n\n    def clear_size(self):\n        \'\'\'\n        Clear size\n        \'\'\'\n        if self.graph_type == LayerType.attention.value or \\\n            LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None\n\n    def __str__(self):\n        return \'input:\' + str(self.input) + \' output:\' + str(self.output) + \' type:\' + str(\n            self.graph_type) + \' is_delete:\' + str(self.is_delete) + \' size:\' + str(self.size)\n\ndef graph_dumps(graph):\n    \'\'\'\n    Dump the graph.\n    \'\'\'\n    return json.dumps(graph, default=lambda obj: obj.__dict__)\n\ndef graph_loads(graph_json):\n    \'\'\'\n    Load graph\n    \'\'\'\n    layers = []\n    for layer in graph_json[\'layers\']:\n        layer_info = Layer(layer[\'type\'], layer[\'input\'], layer[\'output\'], layer[\'size\'])\n        layer_info.is_delete = layer[\'is_delete\']\n        layers.append(layer_info)\n    graph = Graph(graph_json[\'max_layer_num\'], [], [], [])\n    graph.layers = layers\n    return graph\n\nclass Graph(object):\n    \'\'\'\n    Customed Graph class.\n    \'\'\'\n    def __init__(self, max_layer_num, inputs, output, hide):\n        self.layers = []\n        self.max_layer_num = max_layer_num\n\n        for layer in inputs:\n            self.layers.append(layer)\n        for layer in output:\n            self.layers.append(layer)\n        if hide is not None:\n            for layer in hide:\n                self.layers.append(layer)\n        assert self.is_legal()\n\n    def is_topology(self, layers=None):\n        \'\'\'\n        valid the topology\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layers_nodle = []\n        result = []\n        for i, layer in enumerate(layers):\n            if layer.is_delete is False:\n                layers_nodle.append(i)\n        while True:\n            flag_break = True\n            layers_toremove = []\n            for layer1 in layers_nodle:\n                flag_arrive = True\n                for layer2 in layers[layer1].input:\n                    if layer2 in layers_nodle:\n                        flag_arrive = False\n                if flag_arrive is True:\n                    for layer2 in layers[layer1].output:\n                        # Size is error\n                        if layers[layer2].set_size(layer1, layers[layer1].size) is False:\n                            return False\n                    layers_toremove.append(layer1)\n                    result.append(layer1)\n                    flag_break = False\n            for layer in layers_toremove:\n                layers_nodle.remove(layer)\n            result.append(\'|\')\n            if flag_break:\n                break\n        # There is loop in graph || some layers can\'t to arrive\n        if layers_nodle:\n            return False\n        return result\n\n    def layer_num(self, layers=None):\n        \'\'\'\n        Reutn number of layer.\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layer_num = 0\n        for layer in layers:\n            if layer.is_delete is False and layer.graph_type != LayerType.input.value\\\n                and layer.graph_type != LayerType.output.value:\n                layer_num += 1\n        return layer_num\n\n    def is_legal(self, layers=None):\n        \'\'\'\n        Judge whether is legal for layers\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n\n        for layer in layers:\n            if layer.is_delete is False:\n                if len(layer.input) != layer.input_size:\n                    return False\n                if len(layer.output) < layer.output_size:\n                    return False\n\n        # layer_num <= max_layer_num\n        if self.layer_num(layers) > self.max_layer_num:\n            return False\n\n        # There is loop in graph || some layers can\'t to arrive\n        if self.is_topology(layers) is False:\n            return False\n\n        return True\n\n    def mutation(self, only_add=False):\n        \'\'\'\n        Mutation for a graph\n        \'\'\'\n        types = []\n        if self.layer_num() < self.max_layer_num:\n            types.append(0)\n            types.append(1)\n        if self.layer_num() > 5 and only_add is False:\n            types.append(2)\n            types.append(3)\n        # 0 : add a layer , delete a edge\n        # 1 : add a layer , change a edge\n        # 2 : delete a layer, delete a edge\n        # 3 : delete a layer, change a edge\n        graph_type = random.choice(types)\n        layer_type = random.choice([LayerType.attention.value,\\\n            LayerType.self_attention.value, LayerType.rnn.value])\n        layers = copy.deepcopy(self.layers)\n        cnt_try = 0\n        while True:\n            layers_in = []\n            layers_out = []\n            layers_del = []\n            for i, layer in enumerate(layers):\n                if layer.is_delete is False:\n                    if layer.graph_type != LayerType.output.value:\n                        layers_in.append(i)\n                    if layer.graph_type != LayerType.input.value:\n                        layers_out.append(i)\n                    if layer.graph_type != LayerType.output.value\\\n                            and layer.graph_type != LayerType.input.value:\n                        layers_del.append(i)\n            if graph_type <= 1:\n                new_id = len(layers)\n                out = random.choice(layers_out)\n                inputs = []\n                output = [out]\n                pos = random.randint(0, len(layers[out].input) - 1)\n                last_in = layers[out].input[pos]\n                layers[out].input[pos] = new_id\n                if graph_type == 0:\n                    layers[last_in].output.remove(out)\n                if graph_type == 1:\n                    layers[last_in].output.remove(out)\n                    layers[last_in].output.append(new_id)\n                    inputs = [last_in]\n                lay = Layer(graph_type=layer_type, inputs=inputs, output=output)\n                while len(inputs) < lay.input_size:\n                    layer1 = random.choice(layers_in)\n                    inputs.append(layer1)\n                    layers[layer1].output.append(new_id)\n                lay.input = inputs\n                layers.append(lay)\n            else:\n                layer1 = random.choice(layers_del)\n                for layer2 in layers[layer1].output:\n                    layers[layer2].input.remove(layer1)\n                    if graph_type == 2:\n                        random_in = random.choice(layers_in)\n                    else:\n                        random_in = random.choice(layers[layer1].input)\n                    layers[layer2].input.append(random_in)\n                    layers[random_in].output.append(layer2)\n                for layer2 in layers[layer1].input:\n                    layers[layer2].output.remove(layer1)\n                layers[layer1].is_delete = True\n\n            if self.is_legal(layers):\n                self.layers = layers\n                break\n            else:\n                layers = copy.deepcopy(self.layers)\n                cnt_try += 1\n\n    def __str__(self):\n        info = """"\n        for l_id, layer in enumerate(self.layers):\n            if layer.is_delete is False:\n                info += \'id:%d \' % l_id + str(layer) + \'\\n\'\n        return info\n'"
examples/trials/ga_squad/graph_to_tf.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport tensorflow as tf\nfrom rnn import XGRUCell\nfrom util import dropout\nfrom graph import LayerType\n\n\ndef normalize(inputs,\n              epsilon=1e-8,\n              scope=""ln""):\n    \'\'\'Applies layer normalization.\n\n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    \'\'\'\n    with tf.variable_scope(scope):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n        outputs = gamma * normalized + beta\n\n    return outputs\n\n\ndef multihead_attention(queries,\n                        keys,\n                        scope=""multihead_attention"",\n                        num_units=None,\n                        num_heads=4,\n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False):\n    \'\'\'Applies multihead attention.\n\n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A cdscalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked.\n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns\n      A 3d tensor with shape of (N, T_q, C)\n    \'\'\'\n    global look5\n    with tf.variable_scope(scope):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list()[-1]\n\n        Q_ = []\n        K_ = []\n        V_ = []\n        for _ in range(num_heads):\n            Q = tf.layers.dense(queries, num_units / num_heads,\n                                activation=tf.nn.relu)  # (N, T_q, C)\n            K = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu)  # (N, T_k, C)\n            V = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu)  # (N, T_k, C)\n            Q_.append(Q)\n            K_.append(K)\n            V_.append(V)\n\n        # Split and concat\n        Q_ = tf.concat(Q_, axis=0)  # (h*N, T_q, C/h)\n        K_ = tf.concat(K_, axis=0)  # (h*N, T_k, C/h)\n        V_ = tf.concat(V_, axis=0)  # (h*N, T_k, C/h)\n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n\n        # Scale\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n\n        # Key Masking\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1),\n                            [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n\n        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings,\n                           outputs)  # (h*N, T_q, T_k)\n\n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n            tril = tf.contrib.linalg.LinearOperatorTriL(\n                diag_vals).to_dense()  # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0),\n                            [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n            outputs = tf.where(tf.equal(masks, 0), paddings,\n                               outputs)  # (h*N, T_q, T_k)\n\n        # Activation\n        look5 = outputs\n        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = tf.sign(\n            tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(\n            query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n        outputs *= query_masks  # broadcasting. (N, T_q, C)\n\n        # Dropouts\n        outputs = dropout(outputs, dropout_rate, is_training)\n\n        # Weighted sum\n        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads,\n                                     axis=0), axis=2)  # (N, T_q, C)\n\n        # Residual connection\n        if queries.get_shape().as_list()[-1] == num_units:\n            outputs += queries\n\n        # Normalize\n        outputs = normalize(outputs, scope=scope)  # (N, T_q, C)\n\n    return outputs\n\n\ndef positional_encoding(inputs,\n                        num_units=None,\n                        zero_pad=True,\n                        scale=True,\n                        scope=""positional_encoding"",\n                        reuse=None):\n    \'\'\'\n    Return positinal embedding.\n    \'\'\'\n    Shape = tf.shape(inputs)\n    N = Shape[0]\n    T = Shape[1]\n    num_units = Shape[2]\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        #  Second part, apply the cosine to even columns and sin to odds.\n        X = tf.expand_dims(tf.cast(tf.range(T), tf.float32), axis=1)\n        Y = tf.expand_dims(\n            tf.cast(10000 ** -(2 * tf.range(num_units) / num_units), tf.float32), axis=0)\n        h1 = tf.cast((tf.range(num_units) + 1) % 2, tf.float32)\n        h2 = tf.cast((tf.range(num_units) % 2), tf.float32)\n        position_enc = tf.multiply(X, Y)\n        position_enc = tf.sin(position_enc) * tf.multiply(tf.ones_like(X), h1) + \\\n            tf.cos(position_enc) * tf.multiply(tf.ones_like(X), h2)\n\n        # Convert to a tensor\n        lookup_table = position_enc\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * tf.sqrt(tf.cast(num_units, tf.float32))\n\n        return outputs\n\n\ndef feedforward(inputs,\n                num_units,\n                scope=""multihead_attention""):\n    \'\'\'Point-wise feed forward net.\n\n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    \'\'\'\n    with tf.variable_scope(scope):\n        # Inner layer\n        params = {""inputs"": inputs, ""filters"": num_units[0], ""kernel_size"": 1,\n                  ""activation"": tf.nn.relu, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Readout layer\n        params = {""inputs"": outputs, ""filters"": num_units[1], ""kernel_size"": 1,\n                  ""activation"": None, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Residual connection\n        outputs += inputs\n\n        # Normalize\n        outputs = normalize(outputs)\n\n    return outputs\n\n\ndef rnn(input_states, sequence_lengths, dropout_rate, is_training, num_units):\n    layer_cnt = 1\n    states = []\n    xs = tf.transpose(input_states, perm=[1, 0, 2])\n    for i in range(0, layer_cnt):\n        xs = dropout(xs, dropout_rate, is_training)\n        with tf.variable_scope(\'layer_\' + str(i)):\n            cell_fw = XGRUCell(num_units)\n            cell_bw = XGRUCell(num_units)\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                dtype=tf.float32,\n                sequence_length=sequence_lengths,\n                inputs=xs,\n                time_major=True)\n\n        y_lr, y_rl = outputs\n        xs = tf.concat([y_lr, y_rl], 2)\n        states.append(xs)\n\n    return tf.transpose(dropout(tf.concat(states, axis=2),\n                                dropout_rate,\n                                is_training), perm=[1, 0, 2])\n\n\ndef graph_to_network(input1,\n                     input2,\n                     input1_lengths,\n                     input2_lengths,\n                     graph,\n                     dropout_rate,\n                     is_training,\n                     num_heads=1,\n                     rnn_units=256):\n    topology = graph.is_topology()\n    layers = dict()\n    layers_sequence_lengths = dict()\n    num_units = input1.get_shape().as_list()[-1]\n    layers[0] = input1*tf.sqrt(tf.cast(num_units, tf.float32)) + \\\n        positional_encoding(input1, scale=False, zero_pad=False)\n    layers[1] = input2*tf.sqrt(tf.cast(num_units, tf.float32))\n    layers[0] = dropout(layers[0], dropout_rate, is_training)\n    layers[1] = dropout(layers[1], dropout_rate, is_training)\n    layers_sequence_lengths[0] = input1_lengths\n    layers_sequence_lengths[1] = input2_lengths\n    for _, topo_i in enumerate(topology):\n        if topo_i == \'|\':\n            continue\n        if graph.layers[topo_i].graph_type == LayerType.input.value:\n            continue\n        elif graph.layers[topo_i].graph_type == LayerType.attention.value:\n            with tf.variable_scope(\'attation_%d\' % topo_i):\n                layer = multihead_attention(layers[graph.layers[topo_i].input[0]],\n                                            layers[graph.layers[topo_i].input[1]],\n                                            scope=""multihead_attention%d"" % topo_i,\n                                            dropout_rate=dropout_rate,\n                                            is_training=is_training,\n                                            num_heads=num_heads,\n                                            num_units=rnn_units * 2)\n                layer = feedforward(layer, scope=""feedforward%d"" % topo_i,\n                                    num_units=[rnn_units * 2 * 4, rnn_units * 2])\n            layers[topo_i] = layer\n            layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                graph.layers[topo_i].input[0]]\n        elif graph.layers[topo_i].graph_type == LayerType.self_attention.value:\n            with tf.variable_scope(\'self-attation_%d\' % topo_i):\n                layer = multihead_attention(layers[graph.layers[topo_i].input[0]],\n                                            layers[graph.layers[topo_i].input[0]],\n                                            scope=""multihead_attention%d"" % topo_i,\n                                            dropout_rate=dropout_rate,\n                                            is_training=is_training,\n                                            num_heads=num_heads,\n                                            num_units=rnn_units * 2)\n                layer = feedforward(layer, scope=""feedforward%d"" % topo_i,\n                                    num_units=[rnn_units * 2 * 4, rnn_units * 2])\n            layers[topo_i] = layer\n            layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                graph.layers[topo_i].input[0]]\n        elif graph.layers[topo_i].graph_type == LayerType.rnn.value:\n            with tf.variable_scope(\'rnn_%d\' % topo_i):\n                layer = rnn(layers[graph.layers[topo_i].input[0]],\n                            layers_sequence_lengths[graph.layers[topo_i].input[0]],\n                            dropout_rate,\n                            is_training,\n                            rnn_units)\n            layers[topo_i] = layer\n            layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                graph.layers[topo_i].input[0]]\n        elif graph.layers[topo_i].graph_type == LayerType.output.value:\n            layers[topo_i] = layers[graph.layers[topo_i].input[0]]\n            if layers[topo_i].get_shape().as_list()[-1] != rnn_units * 1 * 2:\n                with tf.variable_scope(\'add_dense\'):\n                    layers[topo_i] = tf.layers.dense(\n                        layers[topo_i], units=rnn_units*2)\n    return layers[2], layers[3]\n'"
examples/trials/ga_squad/rnn.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\n\nclass GRU:\n    \'\'\'\n    GRU class.\n    \'\'\'\n    def __init__(self, name, input_dim, hidden_dim):\n        self.name = \'/\'.join([name, \'gru\'])\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.w_matrix = None\n        self.U = None\n        self.bias = None\n\n    def define_params(self):\n        \'\'\'\n        Define parameters.\n        \'\'\'\n        input_dim = self.input_dim\n        hidden_dim = self.hidden_dim\n        prefix = self.name\n        self.w_matrix = tf.Variable(tf.random_normal([input_dim, 3 * hidden_dim], stddev=0.1),\n                                    name=\'/\'.join([prefix, \'W\']))\n        self.U = tf.Variable(tf.random_normal([hidden_dim, 3 * hidden_dim], stddev=0.1),\n                             name=\'/\'.join([prefix, \'U\']))\n        self.bias = tf.Variable(tf.random_normal([1, 3 * hidden_dim], stddev=0.1),\n                                name=\'/\'.join([prefix, \'b\']))\n        return self\n\n    def build(self, x, h, mask=None):\n        \'\'\'\n        Build the GRU cell.\n        \'\'\'\n        xw = tf.split(tf.matmul(x, self.w_matrix) + self.bias, 3, 1)\n        hu = tf.split(tf.matmul(h, self.U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = tf.tanh(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + h * z\n        if mask is not None:\n            next_h = next_h * mask + h * (1 - mask)\n        return next_h\n\n    def build_sequence(self, xs, masks, init, is_left_to_right):\n        \'\'\'\n        Build GRU sequence.\n        \'\'\'\n        states = []\n        last = init\n        if is_left_to_right:\n            for i, xs_i in enumerate(xs):\n                h = self.build(xs_i, last, masks[i])\n                states.append(h)\n                last = h\n        else:\n            for i in range(len(xs) - 1, -1, -1):\n                h = self.build(xs[i], last, masks[i])\n                states.insert(0, h)\n                last = h\n        return states\n\n\nclass XGRUCell(RNNCell):\n\n    def __init__(self, hidden_dim, reuse=None):\n        super(XGRUCell, self).__init__(self, _reuse=reuse)\n        self._num_units = hidden_dim\n        self._activation = tf.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n\n        input_dim = inputs.get_shape()[-1]\n        assert input_dim is not None, ""input dimension must be defined""\n        W = tf.get_variable(\n            name=""W"", shape=[input_dim, 3 * self._num_units], dtype=tf.float32)\n        U = tf.get_variable(\n            name=\'U\', shape=[self._num_units, 3 * self._num_units], dtype=tf.float32)\n        b = tf.get_variable(\n            name=\'b\', shape=[1, 3 * self._num_units], dtype=tf.float32)\n\n        xw = tf.split(tf.matmul(inputs, W) + b, 3, 1)\n        hu = tf.split(tf.matmul(state, U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = self._activation(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + state * z\n        return next_h, next_h\n'"
examples/trials/ga_squad/train_model.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nTrain the network combined by RNN and attention.\n\'\'\'\n\nimport tensorflow as tf\n\nfrom attention import DotAttention\nfrom rnn import XGRUCell\nfrom util import dropout\nfrom graph_to_tf import graph_to_network\n\n\nclass GAGConfig:\n    """"""The class for model hyper-parameter configuration.""""""\n    def __init__(self):\n        self.batch_size = 128\n\n        self.dropout = 0.1\n\n        self.char_vcb_size = 1500\n        self.max_char_length = 20\n        self.char_embed_dim = 100\n\n        self.max_query_length = 40\n        self.max_passage_length = 800\n\n        self.att_is_vanilla = True\n        self.att_need_padding = False\n        self.att_is_id = False\n\n        self.ptr_dim = 70\n        self.learning_rate = 0.1\n        self.labelsmoothing = 0.1\n        self.num_heads = 1\n        self.rnn_units = 256\n\n\nclass GAG:\n    """"""The class for the computation graph based QA model.""""""\n    def __init__(self, cfg, embed, graph):\n        self.cfg = cfg\n        self.embed = embed\n        self.graph = graph\n\n        self.query_word = None\n        self.query_mask = None\n        self.query_lengths = None\n        self.passage_word = None\n        self.passage_mask = None\n        self.passage_lengths = None\n        self.answer_begin = None\n        self.answer_end = None\n        self.query_char_ids = None\n        self.query_char_lengths = None\n        self.passage_char_ids = None\n        self.passage_char_lengths = None\n        self.passage_states = None\n        self.query_states = None\n        self.query_init = None\n        self.begin_prob = None\n        self.end_prob = None\n        self.loss = None\n        self.train_op = None\n\n\n    def build_net(self, is_training):\n        """"""Build the whole neural network for the QA model.""""""\n        cfg = self.cfg\n        with tf.device(\'/cpu:0\'):\n            word_embed = tf.get_variable(\n                name=\'word_embed\', initializer=self.embed, dtype=tf.float32, trainable=False)\n            char_embed = tf.get_variable(name=\'char_embed\',\n                                         shape=[cfg.char_vcb_size,\n                                                cfg.char_embed_dim],\n                                         dtype=tf.float32)\n\n        # [query_length, batch_size]\n        self.query_word = tf.placeholder(dtype=tf.int32,\n                                         shape=[None, None],\n                                         name=\'query_word\')\n        self.query_mask = tf.placeholder(dtype=tf.float32,\n                                         shape=[None, None],\n                                         name=\'query_mask\')\n        # [batch_size]\n        self.query_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name=\'query_lengths\')\n\n        # [passage_length, batch_size]\n        self.passage_word = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name=\'passage_word\')\n        self.passage_mask = tf.placeholder(\n            dtype=tf.float32, shape=[None, None], name=\'passage_mask\')\n        # [batch_size]\n        self.passage_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name=\'passage_lengths\')\n\n        if is_training:\n            self.answer_begin = tf.placeholder(\n                dtype=tf.int32, shape=[None], name=\'answer_begin\')\n            self.answer_end = tf.placeholder(\n                dtype=tf.int32, shape=[None], name=\'answer_end\')\n\n        self.query_char_ids = tf.placeholder(dtype=tf.int32,\n                                             shape=[\n                                                 self.cfg.max_char_length, None, None],\n                                             name=\'query_char_ids\')\n        # sequence_length, batch_size\n        self.query_char_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name=\'query_char_lengths\')\n\n        self.passage_char_ids = tf.placeholder(dtype=tf.int32,\n                                               shape=[\n                                                   self.cfg.max_char_length, None, None],\n                                               name=\'passage_char_ids\')\n        # sequence_length, batch_size\n        self.passage_char_lengths = tf.placeholder(dtype=tf.int32,\n                                                   shape=[None, None],\n                                                   name=\'passage_char_lengths\')\n\n        query_char_states = self.build_char_states(char_embed=char_embed,\n                                                   is_training=is_training,\n                                                   reuse=False,\n                                                   char_ids=self.query_char_ids,\n                                                   char_lengths=self.query_char_lengths)\n\n        passage_char_states = self.build_char_states(char_embed=char_embed,\n                                                     is_training=is_training,\n                                                     reuse=True,\n                                                     char_ids=self.passage_char_ids,\n                                                     char_lengths=self.passage_char_lengths)\n\n        with tf.variable_scope(""encoding"") as scope:\n            query_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.query_word), query_char_states], axis=2)\n            scope.reuse_variables()\n            passage_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.passage_word), passage_char_states], axis=2)\n        passage_states = tf.transpose(passage_states, perm=[1, 0, 2])\n        query_states = tf.transpose(query_states, perm=[1, 0, 2])\n        self.passage_states = passage_states\n        self.query_states = query_states\n\n        output, output2 = graph_to_network(passage_states, query_states,\n                                           self.passage_lengths, self.query_lengths,\n                                           self.graph, self.cfg.dropout,\n                                           is_training, num_heads=cfg.num_heads,\n                                           rnn_units=cfg.rnn_units)\n\n        passage_att_mask = self.passage_mask\n        batch_size_x = tf.shape(self.query_lengths)\n        answer_h = tf.zeros(\n            tf.concat([batch_size_x, tf.constant([cfg.ptr_dim], dtype=tf.int32)], axis=0))\n\n        answer_context = tf.reduce_mean(output2, axis=1)\n\n        query_init_w = tf.get_variable(\n            \'query_init_w\', shape=[output2.get_shape().as_list()[-1], cfg.ptr_dim])\n        self.query_init = query_init_w\n        answer_context = tf.matmul(answer_context, query_init_w)\n\n        output = tf.transpose(output, perm=[1, 0, 2])\n\n        with tf.variable_scope(\'answer_ptr_layer\'):\n            ptr_att = DotAttention(\'ptr\',\n                                   hidden_dim=cfg.ptr_dim,\n                                   is_vanilla=self.cfg.att_is_vanilla,\n                                   is_identity_transform=self.cfg.att_is_id,\n                                   need_padding=self.cfg.att_need_padding)\n            answer_pre_compute = ptr_att.get_pre_compute(output)\n            ptr_gru = XGRUCell(hidden_dim=cfg.ptr_dim)\n            begin_prob, begin_logits = ptr_att.get_prob(output, answer_context, passage_att_mask,\n                                                        answer_pre_compute, True)\n            att_state = ptr_att.get_att(output, begin_prob)\n            (_, answer_h) = ptr_gru.call(inputs=att_state, state=answer_h)\n            answer_context = answer_h\n            end_prob, end_logits = ptr_att.get_prob(output, answer_context,\n                                                    passage_att_mask, answer_pre_compute,\n                                                    True)\n\n        self.begin_prob = tf.transpose(begin_prob, perm=[1, 0])\n        self.end_prob = tf.transpose(end_prob, perm=[1, 0])\n        begin_logits = tf.transpose(begin_logits, perm=[1, 0])\n        end_logits = tf.transpose(end_logits, perm=[1, 0])\n\n        if is_training:\n            def label_smoothing(inputs, masks, epsilon=0.1):\n                """"""Modify target for label smoothing.""""""\n                epsilon = cfg.labelsmoothing\n                num_of_channel = tf.shape(inputs)[-1]  # number of channels\n                inputs = tf.cast(inputs, tf.float32)\n                return (((1 - epsilon) * inputs) + (epsilon /\n                                                    tf.cast(num_of_channel, tf.float32))) * masks\n            cost1 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(label_smoothing(\n                    tf.one_hot(self.answer_begin,\n                               depth=tf.shape(self.passage_word)[0]),\n                    tf.transpose(self.passage_mask, perm=[1, 0])), begin_logits))\n            cost2 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(\n                    label_smoothing(tf.one_hot(self.answer_end,\n                                               depth=tf.shape(self.passage_word)[0]),\n                                    tf.transpose(self.passage_mask, perm=[1, 0])), end_logits))\n\n            reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            l2_loss = tf.reduce_sum(reg_ws)\n            loss = cost1 + cost2 + l2_loss\n            self.loss = loss\n\n            optimizer = tf.train.AdamOptimizer(learning_rate=cfg.learning_rate)\n            self.train_op = optimizer.minimize(self.loss)\n\n        return tf.stack([self.begin_prob, self.end_prob])\n\n    def build_char_states(self, char_embed, is_training, reuse, char_ids, char_lengths):\n        """"""Build char embedding network for the QA model.""""""\n        max_char_length = self.cfg.max_char_length\n\n        inputs = dropout(tf.nn.embedding_lookup(char_embed, char_ids),\n                         self.cfg.dropout, is_training)\n        inputs = tf.reshape(\n            inputs, shape=[max_char_length, -1, self.cfg.char_embed_dim])\n        char_lengths = tf.reshape(char_lengths, shape=[-1])\n        with tf.variable_scope(\'char_encoding\', reuse=reuse):\n            cell_fw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            cell_bw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            _, (left_right, right_left) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                sequence_length=char_lengths,\n                inputs=inputs,\n                time_major=True,\n                dtype=tf.float32\n            )\n\n        left_right = tf.reshape(left_right, shape=[-1, self.cfg.char_embed_dim])\n\n        right_left = tf.reshape(right_left, shape=[-1, self.cfg.char_embed_dim])\n\n        states = tf.concat([left_right, right_left], axis=1)\n        out_shape = tf.shape(char_ids)[1:3]\n        out_shape = tf.concat([out_shape, tf.constant(\n            value=[self.cfg.char_embed_dim * 2], dtype=tf.int32)], axis=0)\n        return tf.reshape(states, shape=out_shape)\n'"
examples/trials/ga_squad/trial.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\n\nimport logging\nlogger = logging.getLogger(\'ga_squad\')\n\ntry:\n    import argparse\n    import heapq\n    import json\n    import numpy as np\n    import pickle\n    import graph\n\n    from util import Timer\n\n    import nni\n    import data\n    import evaluate\n    from train_model import *\n\n    os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\nexcept:\n    logger.exception(\'Catch exception in trial.py.\')\n    raise\n\n\ndef get_config():\n    \'\'\'\n    Get config from argument parser.\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'This program is using genetic algorithm to search architecture for SQuAD.\')\n    parser.add_argument(\'--input_file\', type=str,\n                        default=\'./train-v1.1.json\', help=\'input file\')\n    parser.add_argument(\'--dev_file\', type=str,\n                        default=\'./dev-v1.1.json\', help=\'dev file\')\n    parser.add_argument(\'--embedding_file\', type=str,\n                        default=\'./glove.840B.300d.txt\', help=\'dev file\')\n    parser.add_argument(\'--root_path\', default=\'./data/\',\n                        type=str, help=\'Root path of models\')\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=\'batch size\')\n    parser.add_argument(\'--save_path\', type=str,\n                        default=\'./save\', help=\'save path dir\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.0001,\n                        help=\'set half of original learning rate reload data and train.\')\n    parser.add_argument(\'--max_epoch\', type=int, default=30)\n    parser.add_argument(\'--dropout_rate\', type=float,\n                        default=0.1, help=\'dropout_rate\')\n    parser.add_argument(\'--labelsmoothing\', type=float,\n                        default=0.1, help=\'labelsmoothing\')\n    parser.add_argument(\'--num_heads\', type=int, default=1, help=\'num_heads\')\n    parser.add_argument(\'--rnn_units\', type=int, default=256, help=\'rnn_units\')\n\n    args = parser.parse_args()\n    return args\n\n\ndef get_id(word_dict, word):\n    \'\'\'\n    Return word id.\n    \'\'\'\n    return word_dict.get(word, word_dict[\'<unk>\'])\n\n\ndef load_embedding(path):\n    \'\'\'\n    return embedding for a specific file by given file path.\n    \'\'\'\n    EMBEDDING_DIM = 300\n    embedding_dict = {}\n    with open(path, \'r\', encoding=\'utf-8\') as file:\n        pairs = [line.strip(\'\\r\\n\').split() for line in file.readlines()]\n        for pair in pairs:\n            if len(pair) == EMBEDDING_DIM + 1:\n                embedding_dict[pair[0]] = [float(x) for x in pair[1:]]\n    logger.debug(\'embedding_dict size: %d\', len(embedding_dict))\n    return embedding_dict\n\n\nclass MaxQueue:\n    \'\'\'\n    Queue for max value.\n    \'\'\'\n\n    def __init__(self, capacity):\n        assert capacity > 0, \'queue size must be larger than 0\'\n        self._capacity = capacity\n        self._entries = []\n\n    @property\n    def entries(self):\n        return self._entries\n\n    @property\n    def capacity(self):\n        return self._capacity\n\n    @property\n    def size(self):\n        return len(self._entries)\n\n    def clear(self):\n        self._entries = []\n\n    def push(self, item):\n        if self.size < self.capacity:\n            heapq.heappush(self.entries, item)\n        else:\n            heapq.heappushpop(self.entries, item)\n\n\ndef find_best_answer_span(left_prob, right_prob, passage_length, max_answer_length):\n    left = 0\n    right = 0\n    max_prob = left_prob[0] * right_prob[0]\n    for i in range(0, passage_length):\n        left_p = left_prob[i]\n        for j in range(i, min(i + max_answer_length, passage_length)):\n            total_prob = left_p * right_prob[j]\n            if max_prob < total_prob:\n                left, right, max_prob = i, j, total_prob\n    return [(max_prob, left, right)]\n\n\ndef write_prediction(path, position1_result, position2_result):\n    import codecs\n\n    with codecs.open(path, \'w\', encoding=\'utf8\') as file:\n        batch_num = len(position1_result)\n        for i in range(batch_num):\n            position1_batch = position1_result[i]\n            position2_batch = position2_result[i]\n\n            for j in range(position1_batch.shape[0]):\n                file.write(str(position1_batch[j]) +\n                           \'\\t\' + str(position2_batch[j]) + \'\\n\')\n\n\ndef find_kbest_answer_span(k, left_prob, right_prob, passage_length, max_answer_length):\n    if k == 1:\n        return find_best_answer_span(left_prob, right_prob, passage_length, max_answer_length)\n\n    queue = MaxQueue(k)\n    for i in range(0, passage_length):\n        left_p = left_prob[i]\n        for j in range(i, min(i + max_answer_length, passage_length)):\n            total_prob = left_p * right_prob[j]\n            queue.push((total_prob, i, j))\n    return list(sorted(queue.entries, key=lambda x: -x[0]))\n\n\ndef run_epoch(batches, answer_net, is_training):\n    if not is_training:\n        position1_result = []\n        position2_result = []\n        contexts = []\n        ids = []\n\n    loss_sum = 0\n    timer = Timer()\n    count = 0\n    for batch in batches:\n        used = timer.get_elapsed(False)\n        count += 1\n        qps = batch[\'qp_pairs\']\n        question_tokens = [qp[\'question_tokens\'] for qp in qps]\n        passage_tokens = [qp[\'passage_tokens\'] for qp in qps]\n        context = [(qp[\'passage\'], qp[\'passage_tokens\']) for qp in qps]\n        sample_id = [qp[\'id\'] for qp in qps]\n\n        _, query, query_mask, query_lengths = data.get_word_input(\n            data=question_tokens, word_dict=word_vcb, embed=embed, embed_dim=cfg.word_embed_dim)\n        _, passage, passage_mask, passage_lengths = data.get_word_input(\n            data=passage_tokens, word_dict=word_vcb, embed=embed, embed_dim=cfg.word_embed_dim)\n\n        query_char, query_char_lengths = data.get_char_input(\n            data=question_tokens, char_dict=char_vcb, max_char_length=cfg.max_char_length)\n\n        passage_char, passage_char_lengths = data.get_char_input(\n            data=passage_tokens, char_dict=char_vcb, max_char_length=cfg.max_char_length)\n\n        if is_training:\n            answer_begin, answer_end = data.get_answer_begin_end(qps)\n\n        if is_training:\n            feed_dict = {answer_net.query_word: query,\n                         answer_net.query_mask: query_mask,\n                         answer_net.query_lengths: query_lengths,\n                         answer_net.passage_word: passage,\n                         answer_net.passage_mask: passage_mask,\n                         answer_net.passage_lengths: passage_lengths,\n                         answer_net.query_char_ids: query_char,\n                         answer_net.query_char_lengths: query_char_lengths,\n                         answer_net.passage_char_ids: passage_char,\n                         answer_net.passage_char_lengths: passage_char_lengths,\n                         answer_net.answer_begin: answer_begin,\n                         answer_net.answer_end: answer_end}\n            loss, _, = sess.run(\n                [answer_net.loss, answer_net.train_op], feed_dict=feed_dict)\n            if count % 100 == 0:\n                logger.debug(\'%d %g except:%g, loss:%g\' %\n                             (count, used, used / count * len(batches), loss))\n            loss_sum += loss\n        else:\n            feed_dict = {answer_net.query_word: query,\n                         answer_net.query_mask: query_mask,\n                         answer_net.query_lengths: query_lengths,\n                         answer_net.passage_word: passage,\n                         answer_net.passage_mask: passage_mask,\n                         answer_net.passage_lengths: passage_lengths,\n                         answer_net.query_char_ids: query_char,\n                         answer_net.query_char_lengths: query_char_lengths,\n                         answer_net.passage_char_ids: passage_char,\n                         answer_net.passage_char_lengths: passage_char_lengths}\n            position1, position2 = sess.run(\n                [answer_net.begin_prob, answer_net.end_prob], feed_dict=feed_dict)\n            position1_result += position1.tolist()\n            position2_result += position2.tolist()\n            contexts += context\n            ids = np.concatenate((ids, sample_id))\n            if count % 100 == 0:\n                logger.debug(\'%d %g except:%g\' %\n                             (count, used, used / count * len(batches)))\n    loss = loss_sum / len(batches)\n    if is_training:\n        return loss\n    return loss, position1_result, position2_result, ids, contexts\n\n\ndef generate_predict_json(position1_result, position2_result, ids, passage_tokens):\n    \'\'\'\n    Generate json by prediction.\n    \'\'\'\n    predict_len = len(position1_result)\n    logger.debug(\'total prediction num is %s\', str(predict_len))\n\n    answers = {}\n    for i in range(predict_len):\n        sample_id = ids[i]\n        passage, tokens = passage_tokens[i]\n        kbest = find_best_answer_span(\n            position1_result[i], position2_result[i], len(tokens), 23)\n        _, start, end = kbest[0]\n        answer = passage[tokens[start][\'char_begin\']:tokens[end][\'char_end\']]\n        answers[sample_id] = answer\n    logger.debug(\'generate predict done.\')\n    return answers\n\n\ndef generate_data(path, tokenizer, char_vcb, word_vcb, is_training=False):\n    \'\'\'\n    Generate data\n    \'\'\'\n    global root_path\n    qp_pairs = data.load_from_file(path=path, is_training=is_training)\n\n    tokenized_sent = 0\n    # qp_pairs = qp_pairs[:1000]1\n    for qp_pair in qp_pairs:\n        tokenized_sent += 1\n        data.tokenize(qp_pair, tokenizer, is_training)\n        for word in qp_pair[\'question_tokens\']:\n            word_vcb.add(word[\'word\'])\n            for char in word[\'word\']:\n                char_vcb.add(char)\n        for word in qp_pair[\'passage_tokens\']:\n            word_vcb.add(word[\'word\'])\n            for char in word[\'word\']:\n                char_vcb.add(char)\n\n    max_query_length = max(len(x[\'question_tokens\']) for x in qp_pairs)\n    max_passage_length = max(len(x[\'passage_tokens\']) for x in qp_pairs)\n    #min_passage_length = min(len(x[\'passage_tokens\']) for x in qp_pairs)\n    cfg.max_query_length = max_query_length\n    cfg.max_passage_length = max_passage_length\n\n    return qp_pairs\n\n\ndef train_with_graph(graph, qp_pairs, dev_qp_pairs):\n    \'\'\'\n    Train a network from a specific graph.\n    \'\'\'\n    global sess\n    with tf.Graph().as_default():\n        train_model = GAG(cfg, embed, graph)\n        train_model.build_net(is_training=True)\n        tf.get_variable_scope().reuse_variables()\n        dev_model = GAG(cfg, embed, graph)\n        dev_model.build_net(is_training=False)\n        with tf.Session() as sess:\n            logger.debug(\'init variables\')\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            # writer = tf.summary.FileWriter(\'%s/graph/\'%execution_path, sess.graph)\n            logger.debug(\'assign to graph\')\n\n            saver = tf.train.Saver()\n            train_loss = None\n            bestacc = 0\n            patience = 5\n            patience_increase = 2\n            improvement_threshold = 0.995\n\n            for epoch in range(max_epoch):\n                logger.debug(\'begin to train\')\n                train_batches = data.get_batches(qp_pairs, cfg.batch_size)\n                train_loss = run_epoch(train_batches, train_model, True)\n                logger.debug(\'epoch \' + str(epoch) +\n                             \' loss: \' + str(train_loss))\n                dev_batches = list(data.get_batches(\n                    dev_qp_pairs, cfg.batch_size))\n                _, position1, position2, ids, contexts = run_epoch(\n                    dev_batches, dev_model, False)\n\n                answers = generate_predict_json(\n                    position1, position2, ids, contexts)\n                if save_path is not None:\n                    with open(os.path.join(save_path, \'epoch%d.prediction\' % epoch), \'w\') as file:\n                        json.dump(answers, file)\n                else:\n                    answers = json.dumps(answers)\n                    answers = json.loads(answers)\n                iter = epoch + 1\n\n                acc = evaluate.evaluate_with_predictions(\n                    args.dev_file, answers)\n\n                logger.debug(\'Send intermediate acc: %s\', str(acc))\n                nni.report_intermediate_result(acc)\n\n                logger.debug(\'Send intermediate result done.\')\n\n                if acc > bestacc:\n                    if acc * improvement_threshold > bestacc:\n                        patience = max(patience, iter * patience_increase)\n                    bestacc = acc\n\n                    if save_path is not None:\n                        saver.save(os.path.join(sess, save_path + \'epoch%d.model\' % epoch))\n                        with open(os.path.join(save_path, \'epoch%d.score\' % epoch), \'wb\') as file:\n                            pickle.dump(\n                                (position1, position2, ids, contexts), file)\n                logger.debug(\'epoch %d acc %g bestacc %g\' %\n                             (epoch, acc, bestacc))\n                if patience <= iter:\n                    break\n            logger.debug(\'save done.\')\n    return train_loss, bestacc\n\n\nembed = None\nchar_vcb = None\ntokenizer = None\nword_vcb = None\n\n\ndef load_data():\n    global embed, char_vcb, tokenizer, word_vcb\n    logger.debug(\'tokenize data\')\n    tokenizer = data.WhitespaceTokenizer()\n\n    char_set = set()\n    word_set = set()\n    logger.debug(\'generate train data\')\n    qp_pairs = generate_data(input_file, tokenizer,\n                             char_set, word_set, is_training=True)\n    logger.debug(\'generate dev data\')\n    dev_qp_pairs = generate_data(\n        dev_file, tokenizer, char_set, word_set, is_training=False)\n    logger.debug(\'generate data done.\')\n\n    char_vcb = {char: sample_id for sample_id, char in enumerate(char_set)}\n    word_vcb = {word: sample_id for sample_id, word in enumerate(word_set)}\n\n    timer.start()\n    logger.debug(\'read embedding table\')\n\n    cfg.word_embed_dim = 300\n    embed = np.zeros((len(word_vcb), cfg.word_embed_dim), dtype=np.float32)\n\n    embedding = load_embedding(args.embedding_file)\n    for word, sample_id in enumerate(word_vcb):\n        if word in embedding:\n            embed[sample_id] = embedding[word]\n\n    # add UNK into dict\n    unk = np.zeros((1, cfg.word_embed_dim), dtype=np.float32)\n    embed = np.concatenate((unk, embed), axis=0)\n    word_vcb = {key: value + 1 for key, value in word_vcb.items()}\n\n    return qp_pairs, dev_qp_pairs\n\n\nif __name__ == \'__main__\':\n    try:\n        args = get_config()\n\n        root_path = os.path.expanduser(args.root_path)\n        input_file = os.path.expanduser(args.input_file)\n        dev_file = os.path.expanduser(args.dev_file)\n        save_path = None\n        max_epoch = args.max_epoch\n\n        cfg = GAGConfig()\n        cfg.batch_size = args.batch_size\n        cfg.learning_rate = float(args.learning_rate)\n        cfg.dropout = args.dropout_rate\n        cfg.rnn_units = args.rnn_units\n        cfg.labelsmoothing = args.labelsmoothing\n        cfg.num_heads = args.num_heads\n        timer = Timer()\n\n        qp_pairs, dev_qp_pairs = load_data()\n        logger.debug(\'Init finish.\')\n\n        original_params = nni.get_next_parameter()\n        \'\'\'\n        with open(\'data.json\') as f:\n            original_params = json.load(f)\n        \'\'\'\n        try:\n            graph = graph.graph_loads(original_params)\n        except Exception:\n            logger.debug(\'Can\\\'t load graph.\')\n        train_loss, best_acc = train_with_graph(graph, qp_pairs, dev_qp_pairs)\n\n        logger.debug(\'Send best acc: %s\', str(best_acc))\n        nni.report_final_result(best_acc)\n        logger.debug(\'Send final result done\')\n    except:\n        logger.exception(\'Catch exception in trial.py.\')\n        raise\n'"
examples/trials/ga_squad/util.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nUtil Module\n\'\'\'\n\nimport time\n\nimport tensorflow as tf\n\n\ndef shape(tensor):\n    \'\'\'\n    Get shape of variable.\n    Return type is tuple.\n    \'\'\'\n    temp_s = tensor.get_shape()\n    return tuple([temp_s[i].value for i in range(0, len(temp_s))])\n\n\ndef get_variable(name, temp_s):\n    \'\'\'\n    Get variable by name.\n    \'\'\'\n    return tf.Variable(tf.zeros(temp_s), name=name)\n\n\ndef dropout(tensor, drop_prob, is_training):\n    \'\'\'\n    Dropout except test.\n    \'\'\'\n    if not is_training:\n        return tensor\n    return tf.nn.dropout(tensor, 1.0 - drop_prob)\n\n\nclass Timer:\n    \'\'\'\n    Class Timer is for calculate time.\n    \'\'\'\n    def __init__(self):\n        self.__start = time.time()\n\n    def start(self):\n        \'\'\'\n        Start to calculate time.\n        \'\'\'\n        self.__start = time.time()\n\n    def get_elapsed(self, restart=True):\n        \'\'\'\n        Calculate time span.\n        \'\'\'\n        end = time.time()\n        span = end - self.__start\n        if restart:\n            self.__start = end\n        return span\n'"
examples/trials/kaggle-tgs-salt/augmentation.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport cv2\nimport numpy as np\nimport random\nimport torchvision.transforms.functional as F\nfrom torchvision.transforms import RandomResizedCrop, ColorJitter, RandomAffine\nimport PIL\nfrom PIL import Image\nimport collections\n\nimport settings\n\n\nclass RandomHFlipWithMask(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, *imgs):\n        if random.random() < self.p:\n            return map(F.hflip, imgs)\n        else:\n            return imgs\n\nclass RandomVFlipWithMask(object):\n    def __init__(self, p=0.5):\n        self.p = p\n    def __call__(self, *imgs):\n        if random.random() < self.p:\n            return map(F.vflip, imgs)\n        else:\n            return imgs\n\nclass RandomResizedCropWithMask(RandomResizedCrop):\n    def __init__(self, size, scale=(0.08, 1.0), ratio=(3. / 4., 4. / 3.), interpolation=Image.BILINEAR):\n        super(RandomResizedCropWithMask, self).__init__(size, scale, ratio, interpolation)\n    def __call__(self, *imgs):\n        i, j, h, w = self.get_params(imgs[0], self.scale, self.ratio)\n        #print(i,j,h,w)\n        return map(lambda x: F.resized_crop(x, i, j, h, w, self.size, self.interpolation), imgs)\n\nclass RandomAffineWithMask(RandomAffine):\n    def __init__(self, degrees, translate=None, scale=None, shear=None, resample=\'edge\'):\n        super(RandomAffineWithMask, self).__init__(degrees, translate, scale, shear, resample)\n    def __call__(self, *imgs):\n        ret = self.get_params(self.degrees, self.translate, self.scale, self.shear, imgs[0].size)\n        w, h = imgs[0].size\n        imgs = map(lambda x: F.pad(x, w//2, 0, self.resample), imgs)\n        imgs = map(lambda x: F.affine(x, *ret, resample=0), imgs)\n        imgs = map(lambda x: F.center_crop(x, (w, h)), imgs)\n        return imgs\n\nclass RandomRotateWithMask(object):\n    def __init__(self, degrees, pad_mode=\'reflect\', expand=False, center=None):\n        self.pad_mode = pad_mode\n        self.expand = expand\n        self.center = center\n        self.degrees = degrees\n\n    def __call__(self, *imgs):\n        angle = self.get_angle()\n        if angle == int(angle) and angle % 90 == 0:\n            if angle == 0:\n                return imgs\n            else:\n                #print(imgs)\n                return map(lambda x: F.rotate(x, angle, False, False, None), imgs)\n        else:\n            return map(lambda x: self._pad_rotate(x, angle), imgs)\n\n    def get_angle(self):\n        if isinstance(self.degrees, collections.Sequence):\n            index = int(random.random() * len(self.degrees))\n            return self.degrees[index]\n        else:\n            return random.uniform(-self.degrees, self.degrees)\n\n    def _pad_rotate(self, img, angle):\n        w, h = img.size\n        img = F.pad(img, w//2, 0, self.pad_mode)\n        img = F.rotate(img, angle, False, self.expand, self.center)\n        img = F.center_crop(img, (w, h))\n        return img\n\nclass CropWithMask(object):\n    def __init__(self, i, j, h, w):\n        self.i = i\n        self.j = j\n        self.h = h\n        self.w = w\n    def __call__(self, *imgs):\n        return map(lambda x: F.crop(x, self.i, self.j, self.h, self.w), imgs)\n\nclass PadWithMask(object):\n    def __init__(self, padding, padding_mode):\n        self.padding = padding\n        self.padding_mode = padding_mode\n    def __call__(self, *imgs):\n        return map(lambda x: F.pad(x, self.padding, padding_mode=self.padding_mode), imgs)\n\nclass Compose(object):\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *imgs):\n        for t in self.transforms:\n            imgs = t(*imgs)\n        return imgs\n    def __repr__(self):\n        format_string = self.__class__.__name__ + \'(\'\n        for t in self.transforms:\n            format_string += \'\\n\'\n            format_string += \'    {0}\'.format(t)\n        format_string += \'\\n)\'\n        return format_string\n\ndef get_img_mask_augments(train_mode, pad_mode):\n    if pad_mode == \'resize\':\n        img_mask_aug_train = Compose([\n            RandomHFlipWithMask(),\n            RandomAffineWithMask(10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=None)\n        ])\n        img_mask_aug_val = None\n    else:\n        img_mask_aug_train = Compose([\n            PadWithMask((28, 28), padding_mode=pad_mode),\n            RandomHFlipWithMask(),\n            RandomAffineWithMask(10, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=None),\n            RandomResizedCropWithMask(128, scale=(1., 1.), ratio=(1., 1.))\n        ])\n        img_mask_aug_val = PadWithMask((13, 14), padding_mode=pad_mode)\n\n    return img_mask_aug_train, img_mask_aug_val\n\n\ndef test_transform():\n    img_id = \'0b73b427d1.png\'\n    img = Image.open(os.path.join(settings.TRAIN_IMG_DIR, img_id)).convert(\'RGB\')\n    mask = Image.open(os.path.join(settings.TRAIN_MASK_DIR, img_id)).convert(\'L\').point(lambda x: 0 if x < 128 else 1, \'L\')\n\n    img_id = \'0a1ea1af4.jpg\'\n    img = Image.open(os.path.join(r\'D:\\data\\ship\\train_v2\', img_id)).convert(\'RGB\')\n    mask = Image.open(os.path.join(r\'D:\\data\\ship\\train_masks\', img_id)).convert(\'L\').point(lambda x: 0 if x < 128 else 1, \'L\')\n\n    trans = Compose([\n        RandomHFlipWithMask(),\n        RandomVFlipWithMask(),\n        RandomRotateWithMask([0, 90, 180, 270]),\n        #RandomRotateWithMask(15),\n        RandomResizedCropWithMask(768, scale=(0.81, 1))\n    ])\n\n    trans2 = RandomAffineWithMask(45, (0.2,0.2), (0.9, 1.1))\n    trans3, trans4 = get_img_mask_augments(True, \'edge\')\n\n    img, mask = trans4(img, mask)\n\n    img.show()\n    mask.point(lambda x: x*255).show()\n\ndef test_color_trans():\n    img_id = \'00abc623a.jpg\'\n    img = Image.open(os.path.join(settings.TRAIN_IMG_DIR, img_id)).convert(\'RGB\')\n    trans = ColorJitter(0.1, 0.1, 0.1, 0.1)\n\n    img2 = trans(img)\n    img.show()\n    img2.show()\n\n\nclass TTATransform(object):\n    def __init__(self, index):\n        self.index = index\n    def __call__(self, img):\n        trans = {\n            0: lambda x: x,\n            1: lambda x: F.hflip(x),\n            2: lambda x: F.vflip(x),\n            3: lambda x: F.vflip(F.hflip(x)),\n            4: lambda x: F.rotate(x, 90, False, False),\n            5: lambda x: F.hflip(F.rotate(x, 90, False, False)),\n            6: lambda x: F.vflip(F.rotate(x, 90, False, False)),\n            7: lambda x: F.vflip(F.hflip(F.rotate(x, 90, False, False)))\n        }\n        return trans[self.index](img)\n\n# i is tta index, 0: no change, 1: horizon flip, 2: vertical flip, 3: do both\ndef tta_back_mask_np(img, index):\n    print(img.shape)\n    trans = {\n        0: lambda x: x,\n        1: lambda x: np.flip(x, 2),\n        2: lambda x: np.flip(x, 1),\n        3: lambda x: np.flip(np.flip(x, 2), 1),\n        4: lambda x: np.rot90(x, 3, axes=(1,2)),\n        5: lambda x: np.rot90(np.flip(x, 2), 3, axes=(1,2)),\n        6: lambda x: np.rot90(np.flip(x, 1), 3, axes=(1,2)),\n        7: lambda x: np.rot90(np.flip(np.flip(x,2), 1), 3, axes=(1,2))\n    }\n\n    return trans[index](img)\n\ndef test_tta():\n    img_f = os.path.join(settings.TEST_IMG_DIR, \'0c2637aa9.jpg\')\n    img = Image.open(img_f)\n    img = img.convert(\'RGB\')\n\n    tta_index = 7\n    trans1 = TTATransform(tta_index)\n    img = trans1(img)\n    #img.show()\n\n    img_np = np.array(img)\n    img_np = np.expand_dims(img_np, 0)\n    print(img_np.shape)\n    img_np = tta_back_mask_np(img_np, tta_index)\n    img_np = np.reshape(img_np, (768, 768, 3))\n    img_back = F.to_pil_image(img_np)\n    img_back.show()\n\nif __name__ == \'__main__\':\n    test_transform()\n'"
examples/trials/kaggle-tgs-salt/focal_loss.py,12,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss2d(nn.Module):\n\n    def __init__(self, gamma=2, size_average=True):\n        super(FocalLoss2d, self).__init__()\n        self.gamma = gamma\n        self.size_average = size_average\n\n\n    def forward(self, logit, target, class_weight=None, type=\'sigmoid\'):\n        target = target.view(-1, 1).long()\n\n        if type==\'sigmoid\':\n            if class_weight is None:\n                class_weight = [1]*2 #[0.5, 0.5]\n\n            prob   = torch.sigmoid(logit)\n            prob   = prob.view(-1, 1)\n            prob   = torch.cat((1-prob, prob), 1)\n            select = torch.FloatTensor(len(prob), 2).zero_().cuda()\n            select.scatter_(1, target, 1.)\n\n        elif  type==\'softmax\':\n            B,C,H,W = logit.size()\n            if class_weight is None:\n                class_weight =[1]*C #[1/C]*C\n\n            logit   = logit.permute(0, 2, 3, 1).contiguous().view(-1, C)\n            prob    = F.softmax(logit,1)\n            select  = torch.FloatTensor(len(prob), C).zero_().cuda()\n            select.scatter_(1, target, 1.)\n\n        class_weight = torch.FloatTensor(class_weight).cuda().view(-1,1)\n        class_weight = torch.gather(class_weight, 0, target)\n\n        prob       = (prob*select).sum(1).view(-1,1)\n        prob       = torch.clamp(prob,1e-8,1-1e-8)\n        batch_loss = - class_weight *(torch.pow((1-prob), self.gamma))*prob.log()\n\n        if self.size_average:\n            loss = batch_loss.mean()\n        else:\n            loss = batch_loss\n\n        return loss\n\n\nif __name__ == \'__main__\':\n    L = FocalLoss2d()\n    out = torch.randn(2, 3, 3).cuda()\n    target = (torch.sigmoid(out) > 0.5).float()\n    loss = L(out, target)\n    print(loss)\n'"
examples/trials/kaggle-tgs-salt/loader.py,6,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os, cv2, glob\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.utils.data as data\nfrom torchvision import datasets, models, transforms\nfrom utils import read_masks, get_test_meta, get_nfold_split\nimport augmentation as aug\nfrom settings import *\n\nclass ImageDataset(data.Dataset):\n    def __init__(self, train_mode, meta, augment_with_target=None,\n                image_augment=None, image_transform=None, mask_transform=None):\n        self.augment_with_target = augment_with_target\n        self.image_augment = image_augment\n        self.image_transform = image_transform\n        self.mask_transform = mask_transform\n\n        self.train_mode = train_mode\n        self.meta = meta\n\n        self.img_ids = meta[ID_COLUMN].values\n        self.salt_exists = meta[\'salt_exists\'].values\n        self.is_train = meta[\'is_train\'].values\n\n        if self.train_mode:\n            self.mask_filenames = meta[Y_COLUMN].values\n\n    def __getitem__(self, index):\n        base_img_fn = \'{}.png\'.format(self.img_ids[index])\n        if self.is_train[index]: #self.train_mode:\n            img_fn = os.path.join(TRAIN_IMG_DIR, base_img_fn)\n        else:\n            img_fn = os.path.join(TEST_IMG_DIR, base_img_fn)\n        img = self.load_image(img_fn)\n\n        if self.train_mode:\n            base_mask_fn = \'{}.png\'.format(self.img_ids[index])\n            if self.is_train[index]:\n                mask_fn = os.path.join(TRAIN_MASK_DIR, base_mask_fn)\n            else:\n                mask_fn = os.path.join(TEST_DIR, \'masks\', base_mask_fn)\n            mask = self.load_image(mask_fn, True)\n            img, mask = self.aug_image(img, mask)\n            return img, mask, self.salt_exists[index]\n        else:\n            img = self.aug_image(img)\n            return [img]\n\n    def aug_image(self, img, mask=None):\n        if mask is not None:\n            if self.augment_with_target is not None:\n                img, mask = self.augment_with_target(img, mask)\n            if self.image_augment is not None:\n                img = self.image_augment(img)\n            if self.mask_transform is not None:\n                mask = self.mask_transform(mask)\n            if self.image_transform is not None:\n                img = self.image_transform(img)\n            return img, mask\n        else:\n            if self.image_augment is not None:\n                img = self.image_augment(img)\n            if self.image_transform is not None:\n                img = self.image_transform(img)\n            return img\n\n    def load_image(self, img_filepath, grayscale=False):\n        image = Image.open(img_filepath, \'r\')\n        if not grayscale:\n            image = image.convert(\'RGB\')\n        else:\n            image = image.convert(\'L\').point(lambda x: 0 if x < 128 else 1, \'L\')\n        return image\n\n    def __len__(self):\n        return len(self.img_ids)\n\n    def collate_fn(self, batch):\n        imgs = [x[0] for x in batch]\n        inputs = torch.stack(imgs)\n\n        if self.train_mode:\n            masks = [x[1] for x in batch]\n            labels = torch.stack(masks)\n\n            salt_target = [x[2] for x in batch]\n            return inputs, labels, torch.FloatTensor(salt_target)\n        else:\n            return inputs\n\ndef mask_to_tensor(x):\n    x = np.array(x).astype(np.float32)\n    x = np.expand_dims(x, axis=0)\n    x = torch.from_numpy(x)\n    return x\n\nimg_transforms = [\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ]\n\ndef get_tta_transforms(index, pad_mode):\n    tta_transforms = {\n        0: [],\n        1: [transforms.RandomHorizontalFlip(p=2.)],\n        2: [transforms.RandomVerticalFlip(p=2.)],\n        3: [transforms.RandomHorizontalFlip(p=2.), transforms.RandomVerticalFlip(p=2.)]\n    }\n    if pad_mode == \'resize\':\n        return transforms.Compose([transforms.Resize((H, W)), *(tta_transforms[index]), *img_transforms])\n    else:\n        return transforms.Compose([*(tta_transforms[index]), *img_transforms])\n\ndef get_image_transform(pad_mode):\n    if pad_mode == \'resize\':\n        return transforms.Compose([transforms.Resize((H, W)), *img_transforms])\n    else:\n        return transforms.Compose(img_transforms)\n\ndef get_mask_transform(pad_mode):\n    if pad_mode == \'resize\':\n        return transforms.Compose(\n            [\n                transforms.Resize((H, W)),\n                transforms.Lambda(mask_to_tensor),\n            ]\n        )\n    else:\n        return transforms.Compose(\n            [\n                transforms.Lambda(mask_to_tensor),\n            ]\n        )\n\ndef get_img_mask_augments(pad_mode, depths_channel=False):\n    if depths_channel:\n        affine_aug = aug.RandomAffineWithMask(5, translate=(0.1, 0.), scale=(0.9, 1.1), shear=None)\n    else:\n        affine_aug = aug.RandomAffineWithMask(15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=None)\n\n    if pad_mode == \'resize\':\n        img_mask_aug_train = aug.Compose([\n            aug.RandomHFlipWithMask(),\n            affine_aug\n        ])\n        img_mask_aug_val = None\n    else:\n        img_mask_aug_train = aug.Compose([\n            aug.PadWithMask((28, 28), padding_mode=pad_mode),\n            aug.RandomHFlipWithMask(),\n            affine_aug,\n            aug.RandomResizedCropWithMask(H, scale=(1., 1.), ratio=(1., 1.))\n        ])\n        img_mask_aug_val = aug.PadWithMask((13, 13, 14, 14), padding_mode=pad_mode)\n\n    return img_mask_aug_train, img_mask_aug_val\n\ndef get_train_loaders(ifold, batch_size=8, dev_mode=False, pad_mode=\'edge\', meta_version=1, pseudo_label=False, depths=False):\n    train_shuffle = True\n    train_meta, val_meta = get_nfold_split(ifold, nfold=10, meta_version=meta_version)\n\n    if pseudo_label:\n        test_meta = get_test_meta()\n        train_meta = train_meta.append(test_meta, sort=True)\n\n    if dev_mode:\n        train_shuffle = False\n        train_meta = train_meta.iloc[:10]\n        val_meta = val_meta.iloc[:10]\n    #print(val_meta[X_COLUMN].values[:5])\n    #print(val_meta[Y_COLUMN].values[:5])\n    print(train_meta.shape, val_meta.shape)\n    img_mask_aug_train, img_mask_aug_val = get_img_mask_augments(pad_mode, depths)\n\n    train_set = ImageDataset(True, train_meta,\n                            augment_with_target=img_mask_aug_train,\n                            image_augment=transforms.ColorJitter(0.2, 0.2, 0.2, 0.2),\n                            image_transform=get_image_transform(pad_mode),\n                            mask_transform=get_mask_transform(pad_mode))\n\n    train_loader = data.DataLoader(train_set, batch_size=batch_size, shuffle=train_shuffle, num_workers=4, collate_fn=train_set.collate_fn, drop_last=True)\n    train_loader.num = len(train_set)\n\n    val_set = ImageDataset(True, val_meta,\n                            augment_with_target=img_mask_aug_val,\n                            image_augment=None,\n                            image_transform=get_image_transform(pad_mode),\n                            mask_transform=get_mask_transform(pad_mode))\n    val_loader = data.DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=val_set.collate_fn)\n    val_loader.num = len(val_set)\n    val_loader.y_true = read_masks(val_meta[ID_COLUMN].values)\n\n    return train_loader, val_loader\n\ndef get_test_loader(batch_size=16, index=0, dev_mode=False, pad_mode=\'edge\'):\n    test_meta = get_test_meta()\n    if dev_mode:\n        test_meta = test_meta.iloc[:10]\n    test_set = ImageDataset(False, test_meta,\n                            image_augment=None if pad_mode == \'resize\' else transforms.Pad((13,13,14,14), padding_mode=pad_mode),\n                            image_transform=get_tta_transforms(index, pad_mode))\n    test_loader = data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4, collate_fn=test_set.collate_fn, drop_last=False)\n    test_loader.num = len(test_set)\n    test_loader.meta = test_set.meta\n\n    return test_loader\n\ndepth_channel_tensor = None\n\ndef get_depth_tensor(pad_mode):\n    global depth_channel_tensor\n\n    if depth_channel_tensor is not None:\n        return depth_channel_tensor\n\n    depth_tensor = None\n\n    if pad_mode == \'resize\':\n        depth_tensor = np.zeros((H, W))\n        for row, const in enumerate(np.linspace(0, 1, H)):\n            depth_tensor[row, :] = const\n    else:\n        depth_tensor = np.zeros((ORIG_H, ORIG_W))\n        for row, const in enumerate(np.linspace(0, 1, ORIG_H)):\n            depth_tensor[row, :] = const\n        depth_tensor = np.pad(depth_tensor, (14,14), mode=pad_mode) # edge or reflect\n        depth_tensor = depth_tensor[:H, :W]\n\n    depth_channel_tensor = torch.Tensor(depth_tensor)\n    return depth_channel_tensor\n\ndef add_depth_channel(img_tensor, pad_mode):\n    \'\'\'\n    img_tensor: N, C, H, W\n    \'\'\'\n    img_tensor[:, 1] = get_depth_tensor(pad_mode)\n    img_tensor[:, 2] = img_tensor[:, 0] * get_depth_tensor(pad_mode)\n\n\ndef test_train_loader():\n    train_loader, val_loader = get_train_loaders(1, batch_size=4, dev_mode=False, pad_mode=\'edge\', meta_version=2, pseudo_label=True)\n    print(train_loader.num, val_loader.num)\n    for i, data in enumerate(train_loader):\n        imgs, masks, salt_exists = data\n        #pdb.set_trace()\n        print(imgs.size(), masks.size(), salt_exists.size())\n        print(salt_exists)\n        add_depth_channel(imgs, \'resize\')\n        print(masks)\n        break\n        #print(imgs)\n        #print(masks)\n\ndef test_test_loader():\n    test_loader = get_test_loader(4, pad_mode=\'resize\')\n    print(test_loader.num)\n    for i, data in enumerate(test_loader):\n        print(data.size())\n        if i > 5:\n            break\n\nif __name__ == \'__main__\':\n    test_test_loader()\n    #test_train_loader()\n    #small_dict, img_ids = load_small_train_ids()\n    #print(img_ids[:10])\n    #print(get_tta_transforms(3, \'edge\'))\n'"
examples/trials/kaggle-tgs-salt/lovasz_losses.py,8,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom __future__ import print_function, division\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn.functional as F\nimport numpy as np\n\ntry:\n    from itertools import  ifilterfalse\nexcept ImportError: # py3k\n    from itertools import  filterfalse\n\n\ndef lovasz_grad(gt_sorted):\n    """"""\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    """"""\n    p = len(gt_sorted)\n    gts = gt_sorted.sum()\n    intersection = gts - gt_sorted.float().cumsum(0)\n    union = gts + (1 - gt_sorted).float().cumsum(0)\n    jaccard = 1. - intersection / union\n    if p > 1: # cover 1-pixel case\n        jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n    return jaccard\n\n\ndef iou_binary(preds, labels, EMPTY=1., ignore=None, per_image=True):\n    """"""\n    IoU for foreground class\n    binary: 1 foreground, 0 background\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        intersection = ((label == 1) & (pred == 1)).sum()\n        union = ((label == 1) | ((pred == 1) & (label != ignore))).sum()\n        if not union:\n            iou = EMPTY\n        else:\n            iou = float(intersection) / union\n        ious.append(iou)\n    iou = mean(ious)    # mean accross images if per_image\n    return 100 * iou\n\n\ndef iou(preds, labels, C, EMPTY=1., ignore=None, per_image=False):\n    """"""\n    Array of IoU for each (non ignored) class\n    """"""\n    if not per_image:\n        preds, labels = (preds,), (labels,)\n    ious = []\n    for pred, label in zip(preds, labels):\n        iou = []\n        for i in range(C):\n            if i != ignore: # The ignored label is sometimes among predicted classes (ENet - CityScapes)\n                intersection = ((label == i) & (pred == i)).sum()\n                union = ((label == i) | ((pred == i) & (label != ignore))).sum()\n                if not union:\n                    iou.append(EMPTY)\n                else:\n                    iou.append(float(intersection) / union)\n        ious.append(iou)\n    ious = map(mean, zip(*ious)) # mean accross images if per_image\n    return 100 * np.array(ious)\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    """"""\n    if per_image:\n        loss = mean(lovasz_hinge_flat(*flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n                          for log, lab in zip(logits, labels))\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    """"""\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    """"""\n    if len(labels) == 0:\n        # only void pixels, the gradients should be 0\n        return logits.sum() * 0.\n    signs = 2. * labels.float() - 1.\n    errors = (1. - logits * Variable(signs))\n    errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n    perm = perm.data\n    gt_sorted = labels[perm]\n    grad = lovasz_grad(gt_sorted)\n    loss = torch.dot(F.elu(errors_sorted)+1, Variable(grad))\n    #loss = torch.dot(F.relu(errors_sorted), Variable(grad))\n\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to \'ignore\'\n    """"""\n    scores = scores.view(-1)\n    labels = labels.view(-1)\n    if ignore is None:\n        return scores, labels\n    valid = (labels != ignore)\n    vscores = scores[valid]\n    vlabels = labels[valid]\n    return vscores, vlabels\n\n\nclass StableBCELoss(torch.nn.modules.Module):\n    def __init__(self):\n         super(StableBCELoss, self).__init__()\n    def forward(self, input, target):\n         neg_abs = - input.abs()\n         loss = input.clamp(min=0) - input * target + (1 + neg_abs.exp()).log()\n         return loss.mean()\n\n\ndef binary_xloss(logits, labels, ignore=None):\n    """"""\n    Binary Cross entropy loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      ignore: void class id\n    """"""\n    logits, labels = flatten_binary_scores(logits, labels, ignore)\n    loss = StableBCELoss()(logits, Variable(labels.float()))\n    return loss\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, only_present=False, per_image=False, ignore=None):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n    """"""\n    if per_image:\n        loss = mean(lovasz_softmax_flat(*flatten_probas(prob.unsqueeze(0), lab.unsqueeze(0), ignore), only_present=only_present)\n                          for prob, lab in zip(probas, labels))\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore), only_present=only_present)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, only_present=False):\n    """"""\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      only_present: average only on classes present in ground truth\n    """"""\n    C = probas.size(1)\n    losses = []\n    for c in range(C):\n        fg = (labels == c).float() # foreground for class c\n        if only_present and fg.sum() == 0:\n            continue\n        errors = (Variable(fg) - probas[:, c]).abs()\n        errors_sorted, perm = torch.sort(errors, 0, descending=True)\n        perm = perm.data\n        fg_sorted = fg[perm]\n        losses.append(torch.dot(errors_sorted, Variable(lovasz_grad(fg_sorted))))\n    return mean(losses)\n\n\ndef flatten_probas(probas, labels, ignore=None):\n    """"""\n    Flattens predictions in the batch\n    """"""\n    B, C, H, W = probas.size()\n    probas = probas.permute(0, 2, 3, 1).contiguous().view(-1, C)  # B * H * W, C = P, C\n    labels = labels.view(-1)\n    if ignore is None:\n        return probas, labels\n    valid = (labels != ignore)\n    vprobas = probas[valid.nonzero().squeeze()]\n    vlabels = labels[valid]\n    return vprobas, vlabels\n\ndef xloss(logits, labels, ignore=None):\n    """"""\n    Cross entropy loss\n    """"""\n    return F.cross_entropy(logits, Variable(labels), ignore_index=255)\n\n\n# --------------------------- HELPER FUNCTIONS ---------------------------\n\ndef mean(l, ignore_nan=False, empty=0):\n    """"""\n    nanmean compatible with generators.\n    """"""\n    l = iter(l)\n    if ignore_nan:\n        l = ifilterfalse(np.isnan, l)\n    try:\n        n = 1\n        acc = next(l)\n    except StopIteration:\n        if empty == \'raise\':\n            raise ValueError(\'Empty mean\')\n        return empty\n    for n, v in enumerate(l, 2):\n        acc += v\n    if n == 1:\n        return acc\n    return acc / n\n'"
examples/trials/kaggle-tgs-salt/metrics.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport numpy as np\nfrom pycocotools import mask as cocomask\nfrom utils import get_segmentations\n\n\ndef iou(gt, pred):\n    gt[gt > 0] = 1.\n    pred[pred > 0] = 1.\n    intersection = gt * pred\n    union = gt + pred\n    union[union > 0] = 1.\n    intersection = np.sum(intersection)\n    union = np.sum(union)\n    if union == 0:\n        union = 1e-09\n    return intersection / union\n\n\ndef compute_ious(gt, predictions):\n    gt_ = get_segmentations(gt)\n    predictions_ = get_segmentations(predictions)\n\n    if len(gt_) == 0 and len(predictions_) == 0:\n        return np.ones((1, 1))\n    elif len(gt_) != 0 and len(predictions_) == 0:\n        return np.zeros((1, 1))\n    else:\n        iscrowd = [0 for _ in predictions_]\n        ious = cocomask.iou(gt_, predictions_, iscrowd)\n        if not np.array(ious).size:\n            ious = np.zeros((1, 1))\n        return ious\n\n\ndef compute_precision_at(ious, threshold):\n    mx1 = np.max(ious, axis=0)\n    mx2 = np.max(ious, axis=1)\n    tp = np.sum(mx2 >= threshold)\n    fp = np.sum(mx2 < threshold)\n    fn = np.sum(mx1 < threshold)\n    return float(tp) / (tp + fp + fn)\n\n\ndef compute_eval_metric(gt, predictions):\n    thresholds = [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95]\n    ious = compute_ious(gt, predictions)\n    precisions = [compute_precision_at(ious, th) for th in thresholds]\n    return sum(precisions) / len(precisions)\n\n\ndef intersection_over_union(y_true, y_pred):\n    ious = []\n    for y_t, y_p in list(zip(y_true, y_pred)):\n        iou = compute_ious(y_t, y_p)\n        iou_mean = 1.0 * np.sum(iou) / len(iou)\n        ious.append(iou_mean)\n    return np.mean(ious)\n\n\ndef intersection_over_union_thresholds(y_true, y_pred):\n    iouts = []\n    for y_t, y_p in list(zip(y_true, y_pred)):\n        iouts.append(compute_eval_metric(y_t, y_p))\n    return np.mean(iouts)\n'"
examples/trials/kaggle-tgs-salt/models.py,12,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom torchvision import models\nfrom torchvision.models import resnet34, resnet101, resnet50, resnet152\nimport torchvision\n\n\ndef conv3x3(in_, out):\n    return nn.Conv2d(in_, out, 3, padding=1)\n\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_, out):\n        super().__init__()\n        self.conv = conv3x3(in_, out)\n        self.activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.activation(x)\n        return x\n\n\nclass ConvBn2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size=(3,3), stride=(1,1), padding=(1,1)):\n        super(ConvBn2d, self).__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n# Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks\n# https://arxiv.org/abs/1803.02579\n\nclass ChannelAttentionGate(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(ChannelAttentionGate, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return y\n\n\nclass SpatialAttentionGate(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SpatialAttentionGate, self).__init__()\n        self.fc1 = nn.Conv2d(channel, reduction, kernel_size=1, padding=0)\n        self.fc2 = nn.Conv2d(reduction, 1, kernel_size=1, padding=0)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = F.relu(x, inplace=True)\n        x = self.fc2(x)\n        x = torch.sigmoid(x)\n        #print(x.size())\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, middle_channels, out_channels):\n        super(DecoderBlock, self).__init__()\n        self.conv1 = ConvBn2d(in_channels, middle_channels)\n        self.conv2 = ConvBn2d(middle_channels, out_channels)\n        #self.deconv = nn.ConvTranspose2d(middle_channels, out_channels, kernel_size=4, stride=2, padding=1)\n        #self.bn = nn.BatchNorm2d(out_channels)\n        self.spatial_gate = SpatialAttentionGate(out_channels)\n        self.channel_gate = ChannelAttentionGate(out_channels)\n\n    def forward(self, x, e=None):\n        x = F.upsample(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        if e is not None:\n            x = torch.cat([x,e], 1)\n\n        x = F.relu(self.conv1(x), inplace=True)\n        x = F.relu(self.conv2(x), inplace=True)\n\n        g1 = self.spatial_gate(x)\n        g2 = self.channel_gate(x)\n        x = x*g1 + x*g2\n\n        return x\n\nclass EncoderBlock(nn.Module):\n    def __init__(self, block, out_channels):\n        super(EncoderBlock, self).__init__()\n        self.block = block\n        self.out_channels = out_channels\n        self.spatial_gate = SpatialAttentionGate(out_channels)\n        self.channel_gate = ChannelAttentionGate(out_channels)\n\n    def forward(self, x):\n        x = self.block(x)\n        g1 = self.spatial_gate(x)\n        g2 = self.channel_gate(x)\n\n        return x*g1 + x*g2\n\n\ndef create_resnet(layers):\n    if layers == 34:\n        return resnet34(pretrained=True), 512\n    elif layers == 50:\n        return resnet50(pretrained=True), 2048\n    elif layers == 101:\n        return resnet101(pretrained=True), 2048\n    elif layers == 152:\n        return resnet152(pretrained=True), 2048\n    else:\n        raise NotImplementedError(\'only 34, 50, 101, 152 version of Resnet are implemented\')\n\nclass UNetResNetV4(nn.Module):\n    def __init__(self, encoder_depth, num_classes=1, num_filters=32, dropout_2d=0.4,\n                 pretrained=True, is_deconv=True):\n        super(UNetResNetV4, self).__init__()\n        self.name = \'UNetResNetV4_\'+str(encoder_depth)\n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)\n\n        self.encoder1 = EncoderBlock(\n            nn.Sequential(self.resnet.conv1, self.resnet.bn1, self.resnet.relu),\n            num_filters*2\n        )\n        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr//8)\n        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr//4)\n        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr//2)\n        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)\n\n        center_block = nn.Sequential(\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr//2, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.center = EncoderBlock(center_block, bottom_channel_nr//2)\n\n        self.decoder5 = DecoderBlock(bottom_channel_nr + bottom_channel_nr // 2,  num_filters * 16, 64)\n        self.decoder4 = DecoderBlock(64 + bottom_channel_nr // 2,  num_filters * 8,  64)\n        self.decoder3 = DecoderBlock(64 + bottom_channel_nr // 4,  num_filters * 4,  64)\n        self.decoder2 = DecoderBlock(64 + bottom_channel_nr // 8, num_filters * 2,  64)\n        self.decoder1 = DecoderBlock(64, num_filters, 64)\n\n        self.logit = nn.Sequential(\n            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n        )\n\n    def forward(self, x):\n        x = self.encoder1(x) #; print(\'x:\', x.size())\n        e2 = self.encoder2(x) #; print(\'e2:\', e2.size())\n        e3 = self.encoder3(e2) #; print(\'e3:\', e3.size())\n        e4 = self.encoder4(e3) #; print(\'e4:\', e4.size())\n        e5 = self.encoder5(e4) #; print(\'e5:\', e5.size())\n\n        center = self.center(e5) #; print(\'center:\', center.size())\n\n        d5 = self.decoder5(center, e5) #; print(\'d5:\', d5.size())\n        d4 = self.decoder4(d5, e4) #; print(\'d4:\', d4.size())\n        d3 = self.decoder3(d4, e3) #; print(\'d3:\', d3.size())\n        d2 = self.decoder2(d3, e2) #; print(\'d2:\', d2.size())\n        d1 = self.decoder1(d2) #; print(\'d1:\', d1.size())\n\n        f = torch.cat([\n            d1,\n            F.upsample(d2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.upsample(d3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.upsample(d4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.upsample(d5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ], 1)\n\n        f = F.dropout2d(f, p=self.dropout_2d)\n\n        return self.logit(f), None\n\n    def freeze_bn(self):\n        \'\'\'Freeze BatchNorm layers.\'\'\'\n        for layer in self.modules():\n            if isinstance(layer, nn.BatchNorm2d):\n                layer.eval()\n\n    def get_params(self, base_lr):\n        group1 = [self.encoder1, self.encoder2, self.encoder3, self.encoder4, self.encoder5]\n        group2 = [self.decoder1, self.decoder2, self.decoder3, self.decoder4, self.decoder5, self.center, self.logit]\n\n        params1 = []\n        for x in group1:\n            for p in x.parameters():\n                params1.append(p)\n\n        param_group1 = {\'params\': params1, \'lr\': base_lr / 5}\n\n        params2 = []\n        for x in group2:\n            for p in x.parameters():\n                params2.append(p)\n        param_group2 = {\'params\': params2, \'lr\': base_lr}\n\n        return [param_group1, param_group2]\n\nclass DecoderBlockV5(nn.Module):\n    def __init__(self, in_channels_x, in_channels_e, middle_channels, out_channels):\n        super(DecoderBlockV5, self).__init__()\n        self.in_channels = in_channels_x + in_channels_e\n        self.conv1 = ConvBn2d(self.in_channels, middle_channels)\n        self.conv2 = ConvBn2d(middle_channels, out_channels)\n        self.deconv = nn.ConvTranspose2d(in_channels_x, in_channels_x, kernel_size=4, stride=2, padding=1)\n        self.bn = nn.BatchNorm2d(self.in_channels)\n        self.spatial_gate = SpatialAttentionGate(out_channels)\n        self.channel_gate = ChannelAttentionGate(out_channels)\n\n    def forward(self, x, e=None):\n        #x = F.upsample(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        x = self.deconv(x)\n        if e is not None:\n            x = torch.cat([x,e], 1)\n        x = self.bn(x)\n\n        x = F.relu(self.conv1(x), inplace=True)\n        x = F.relu(self.conv2(x), inplace=True)\n\n        g1 = self.spatial_gate(x)\n        g2 = self.channel_gate(x)\n        x = x*g1 + x*g2\n\n        return x\n\n\n\nclass UNetResNetV5(nn.Module):\n    def __init__(self, encoder_depth, num_classes=1, num_filters=32, dropout_2d=0.5):\n        super(UNetResNetV5, self).__init__()\n        self.name = \'UNetResNetV5_\'+str(encoder_depth)\n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)\n\n        self.encoder1 = EncoderBlock(\n            nn.Sequential(self.resnet.conv1, self.resnet.bn1, self.resnet.relu),\n            num_filters*2\n        )\n        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr//8)\n        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr//4)\n        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr//2)\n        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)\n\n        center_block = nn.Sequential(\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr//2, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.center = EncoderBlock(center_block, bottom_channel_nr//2)\n\n        self.decoder5 = DecoderBlockV5(bottom_channel_nr // 2, bottom_channel_nr,  num_filters * 16, 64)\n        self.decoder4 = DecoderBlockV5(64, bottom_channel_nr // 2,  num_filters * 8,  64)\n        self.decoder3 = DecoderBlockV5(64, bottom_channel_nr // 4,  num_filters * 4,  64)\n        self.decoder2 = DecoderBlockV5(64, bottom_channel_nr // 8, num_filters * 2,  64)\n        self.decoder1 = DecoderBlockV5(64, 0, num_filters, 64)\n\n        self.logit = nn.Sequential(\n            nn.Conv2d(320, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n        )\n\n    def forward(self, x):\n        x = self.encoder1(x) #; print(\'x:\', x.size())\n        e2 = self.encoder2(x) #; print(\'e2:\', e2.size())\n        e3 = self.encoder3(e2) #; print(\'e3:\', e3.size())\n        e4 = self.encoder4(e3) #; print(\'e4:\', e4.size())\n        e5 = self.encoder5(e4) #; print(\'e5:\', e5.size())\n\n        center = self.center(e5) #; print(\'center:\', center.size())\n\n        d5 = self.decoder5(center, e5) #; print(\'d5:\', d5.size())\n        d4 = self.decoder4(d5, e4) #; print(\'d4:\', d4.size())\n        d3 = self.decoder3(d4, e3) #; print(\'d3:\', d3.size())\n        d2 = self.decoder2(d3, e2) #; print(\'d2:\', d2.size())\n        d1 = self.decoder1(d2) #; print(\'d1:\', d1.size())\n\n        f = torch.cat([\n            d1,\n            F.interpolate(d2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ], 1)\n\n        f = F.dropout2d(f, p=self.dropout_2d)\n\n        return self.logit(f), None\n\nclass UNetResNetV6(nn.Module):\n    \'\'\'\n    1. Remove first pool from UNetResNetV5, such that resolution is doubled\n    2. Remove scSE from center block\n    3. Increase default dropout\n    \'\'\'\n    def __init__(self, encoder_depth, num_filters=32, dropout_2d=0.5):\n        super(UNetResNetV6, self).__init__()\n        assert encoder_depth == 34, \'UNetResNetV6: only 34 layers is supported!\'\n        self.name = \'UNetResNetV6_\'+str(encoder_depth)\n        self.dropout_2d = dropout_2d\n\n        self.resnet, bottom_channel_nr = create_resnet(encoder_depth)\n\n        self.encoder1 = EncoderBlock(\n            nn.Sequential(self.resnet.conv1, self.resnet.bn1, self.resnet.relu),\n            num_filters*2\n        )\n\n        self.encoder2 = EncoderBlock(self.resnet.layer1, bottom_channel_nr//8)\n        self.encoder3 = EncoderBlock(self.resnet.layer2, bottom_channel_nr//4)\n        self.encoder4 = EncoderBlock(self.resnet.layer3, bottom_channel_nr//2)\n        self.encoder5 = EncoderBlock(self.resnet.layer4, bottom_channel_nr)\n\n        self.center = nn.Sequential(\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d(bottom_channel_nr, bottom_channel_nr//2, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        #self.center = EncoderBlock(center_block, bottom_channel_nr//2)\n\n        self.decoder5 = DecoderBlockV5(bottom_channel_nr // 2, bottom_channel_nr,  num_filters * 16, 64)\n        self.decoder4 = DecoderBlockV5(64, bottom_channel_nr // 2,  num_filters * 8,  64)\n        self.decoder3 = DecoderBlockV5(64, bottom_channel_nr // 4,  num_filters * 4,  64)\n        self.decoder2 = DecoderBlockV5(64, bottom_channel_nr // 8, num_filters * 2,  64)\n        self.decoder1 = DecoderBlockV5(64, 0, num_filters, 64)\n\n        self.logit = nn.Sequential(\n            nn.Conv2d(512, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n        )\n\n        self.logit_image = nn.Sequential(\n            nn.Linear(512, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1)\n        )\n\n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode=\'bilinear\', align_corners=False)\n        x = self.encoder1(x) #; print(\'x:\', x.size())\n        e2 = self.encoder2(x) #; print(\'e2:\', e2.size())\n        e3 = self.encoder3(e2) #; print(\'e3:\', e3.size())\n        e4 = self.encoder4(e3) #; print(\'e4:\', e4.size())\n        e5 = self.encoder5(e4) #; print(\'e5:\', e5.size())\n\n        center = self.center(e5) #; print(\'center:\', center.size())\n\n        d5 = self.decoder5(center, e5) #; print(\'d5:\', d5.size())\n        d4 = self.decoder4(d5, e4) #; print(\'d4:\', d4.size())\n        d3 = self.decoder3(d4, e3) #; print(\'d3:\', d3.size())\n        d2 = self.decoder2(d3, e2) #; print(\'d2:\', d2.size())\n        #d1 = self.decoder1(d2) ; print(\'d1:\', d1.size())\n\n        f = torch.cat([\n            d2,\n            F.interpolate(d3, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d4, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d5, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.interpolate(center, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ], 1)\n\n        f = F.dropout2d(f, p=self.dropout_2d, training=self.training)\n\n        # empty mask classifier\n        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)\n        img_f = F.dropout(img_f, p=0.5, training=self.training)\n        img_logit = self.logit_image(img_f).view(-1)\n\n        return self.logit(f), img_logit\n\n\nclass DecoderBlockV7(nn.Module):\n    def __init__(self, in_channels_x, in_channels_e, middle_channels, out_channels):\n        super(DecoderBlockV7, self).__init__()\n        self.in_channels = in_channels_x + in_channels_e\n        self.conv1 = ConvBn2d(self.in_channels, middle_channels)\n        self.conv2 = ConvBn2d(middle_channels, out_channels)\n        self.deconv = nn.ConvTranspose2d(in_channels_x, in_channels_x, kernel_size=4, stride=2, padding=1)\n        self.bn = nn.BatchNorm2d(self.in_channels)\n        self.spatial_gate = SpatialAttentionGate(out_channels)\n        self.channel_gate = ChannelAttentionGate(out_channels)\n\n    def forward(self, x, e=None, upsample=True):\n        #x = F.upsample(x, scale_factor=2, mode=\'bilinear\', align_corners=True)\n        if upsample:\n            x = self.deconv(x)\n        if e is not None:\n            x = torch.cat([x,e], 1)\n        x = self.bn(x)\n\n        x = F.relu(self.conv1(x), inplace=True)\n        x = F.relu(self.conv2(x), inplace=True)\n\n        g1 = self.spatial_gate(x)\n        g2 = self.channel_gate(x)\n        x = x*g1 + x*g2\n\n        return x\n\nclass UNet7(nn.Module):\n    def __init__(self, encoder_depth, num_classes=1, num_filters=32, dropout_2d=0.5):\n        super(UNet7, self).__init__()\n        nf = num_filters\n        self.name = \'UNet7_\'+str(encoder_depth)+\'_nf\'+str(nf)\n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        self.resnet, nbtm = create_resnet(encoder_depth)\n\n        self.encoder1 = EncoderBlock(\n            nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True),\n            ),\n            64\n        )\n        self.encoder2 = EncoderBlock(\n            nn.Sequential(\n                nn.MaxPool2d(kernel_size=2, stride=2),\n                self.resnet.layer1,\n            ),\n            nbtm//8\n        )\n        self.encoder3 = EncoderBlock(self.resnet.layer2, nbtm//4)\n        self.encoder4 = EncoderBlock(self.resnet.layer3, nbtm//2)\n        self.encoder5 = EncoderBlock(self.resnet.layer4, nbtm)\n\n        center_block = nn.Sequential(\n            ConvBn2d(nbtm, nbtm, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d(nbtm, nbtm//2, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            #nn.MaxPool2d(kernel_size=2, stride=2) # remove\n        )\n        self.center = EncoderBlock(center_block, nbtm//2)\n\n        self.decoder5 = DecoderBlockV7(nbtm // 2, nbtm,  nf * 16, nf*2)\n        self.decoder4 = DecoderBlockV7(nf*2, nbtm // 2,  nf * 8,  nf*2)\n        self.decoder3 = DecoderBlockV7(nf*2, nbtm // 4,  nf * 4,  nf*2)\n        self.decoder2 = DecoderBlockV7(nf*2, nbtm // 8,  nf * 2,  nf*2)\n        self.decoder1 = DecoderBlockV7(nf*2, 64, nf*2, nf*2)\n\n        self.logit = nn.Sequential(\n            nn.Conv2d(nf*10, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n        )\n\n        self.logit_image = nn.Sequential(\n            nn.Linear(nbtm, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x):\n        e1 = self.encoder1(x) #; print(\'e1:\', e1.size())\n        e2 = self.encoder2(e1) #; print(\'e2:\', e2.size())\n        e3 = self.encoder3(e2) #; print(\'e3:\', e3.size())\n        e4 = self.encoder4(e3) #; print(\'e4:\', e4.size())\n        e5 = self.encoder5(e4) #; print(\'e5:\', e5.size())\n\n        center = self.center(e5) #; print(\'center:\', center.size())\n\n        d5 = self.decoder5(center, e5, upsample=False) #; print(\'d5:\', d5.size())\n        d4 = self.decoder4(d5, e4) #; print(\'d4:\', d4.size())\n        d3 = self.decoder3(d4, e3) #; print(\'d3:\', d3.size())\n        d2 = self.decoder2(d3, e2) #; print(\'d2:\', d2.size())\n        d1 = self.decoder1(d2, e1) #; print(\'d1:\', d1.size())\n\n        f = torch.cat([\n            d1,\n            F.interpolate(d2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ], 1)\n\n        f = F.dropout2d(f, p=self.dropout_2d)\n\n        # empty mask classifier\n        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)\n        img_f = F.dropout(img_f, p=0.5, training=self.training)\n        img_logit = self.logit_image(img_f).view(-1)\n\n        return self.logit(f), img_logit\n\n\nclass UNet8(nn.Module):\n    def __init__(self, encoder_depth, num_classes=1, num_filters=32, dropout_2d=0.5):\n        super(UNet8, self).__init__()\n        nf = num_filters\n        self.name = \'UNet8_\'+str(encoder_depth)+\'_nf\'+str(nf)\n        self.num_classes = num_classes\n        self.dropout_2d = dropout_2d\n\n        self.resnet, nbtm = create_resnet(encoder_depth)\n\n        self.encoder1 = EncoderBlock(\n            nn.Sequential(self.resnet.conv1, self.resnet.bn1, self.resnet.relu),\n            64\n        )\n\n        self.encoder2 = EncoderBlock(self.resnet.layer1, nbtm//8)\n        self.encoder3 = EncoderBlock(self.resnet.layer2, nbtm//4)\n        self.encoder4 = EncoderBlock(self.resnet.layer3, nbtm//2)\n        self.encoder5 = EncoderBlock(self.resnet.layer4, nbtm)\n\n        center_block = nn.Sequential(\n            ConvBn2d(nbtm, nbtm, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            ConvBn2d(nbtm, nbtm//2, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            #nn.MaxPool2d(kernel_size=2, stride=2) # remove\n        )\n        self.center = EncoderBlock(center_block, nbtm//2)\n\n        self.decoder5 = DecoderBlockV7(nbtm // 2, nbtm,  nf * 16, nf*2)\n        self.decoder4 = DecoderBlockV7(nf*2, nbtm // 2,  nf * 8,  nf*2)\n        self.decoder3 = DecoderBlockV7(nf*2, nbtm // 4,  nf * 4,  nf*2)\n        self.decoder2 = DecoderBlockV7(nf*2, nbtm // 8,  nf * 2,  nf*2)\n        self.decoder1 = DecoderBlockV7(nf*2+64, 3, nf*2, nf*2)\n\n        self.logit = nn.Sequential(\n            nn.Conv2d(nf*10, 64, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, kernel_size=1, padding=0)\n        )\n\n        self.logit_image = nn.Sequential(\n            nn.Linear(nbtm, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 1),\n        )\n\n    def forward(self, x):\n        e1 = self.encoder1(x) #; print(\'e1:\', e1.size())\n        e2 = self.encoder2(e1) #; print(\'e2:\', e2.size())\n        e3 = self.encoder3(e2) #; print(\'e3:\', e3.size())\n        e4 = self.encoder4(e3) #; print(\'e4:\', e4.size())\n        e5 = self.encoder5(e4) #; print(\'e5:\', e5.size())\n\n        center = self.center(e5) #; print(\'center:\', center.size())\n\n        d5 = self.decoder5(center, e5, upsample=False) #; print(\'d5:\', d5.size())\n        d4 = self.decoder4(d5, e4) #; print(\'d4:\', d4.size())\n        d3 = self.decoder3(d4, e3) #; print(\'d3:\', d3.size())\n        d2 = self.decoder2(d3, e2) #; print(\'d2:\', d2.size())\n        d1 = self.decoder1(torch.cat([d2, e1], 1), x) #; print(\'d1:\', d1.size())\n\n        f = torch.cat([\n            d1,\n            F.interpolate(d2, scale_factor=2, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d3, scale_factor=4, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d4, scale_factor=8, mode=\'bilinear\', align_corners=False),\n            F.interpolate(d5, scale_factor=16, mode=\'bilinear\', align_corners=False),\n        ], 1)\n\n        f = F.dropout2d(f, p=self.dropout_2d)\n\n        # empty mask classifier\n        img_f = F.adaptive_avg_pool2d(e5, 1).view(x.size(0), -1)\n        img_f = F.dropout(img_f, p=0.5, training=self.training)\n        img_logit = self.logit_image(img_f).view(-1)\n\n        return self.logit(f), img_logit\n\n\ndef test():\n    model = UNet8(50, num_filters=32).cuda()\n    inputs = torch.randn(2,3,128,128).cuda()\n    out, _ = model(inputs)\n    #print(model)\n    print(out.size(), _.size()) #, cls_taret.size())\n    #print(out)\n\n\nif __name__ == \'__main__\':\n    test()\n'"
examples/trials/kaggle-tgs-salt/postprocessing.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport numpy as np\nimport pandas as pd\nfrom scipy import ndimage as ndi\nimport cv2\n\nfrom utils import get_crop_pad_sequence, run_length_decoding\nimport settings\n\ndef resize_image(image, target_size):\n    resized_image = cv2.resize(image, target_size)\n    return resized_image\n\ndef crop_image(image, target_size):\n    top_crop, right_crop, bottom_crop, left_crop = get_crop_pad_sequence(image.shape[0] - target_size[0],\n                                                                         image.shape[1] - target_size[1])\n    cropped_image = image[top_crop:image.shape[0] - bottom_crop, left_crop:image.shape[1] - right_crop]\n    return cropped_image\n\ndef binarize(image, threshold):\n    image_binarized = (image > threshold).astype(np.uint8)\n    return image_binarized\n\ndef save_pseudo_label_masks(submission_file):\n    df = pd.read_csv(submission_file, na_filter=False)\n    print(df.head())\n\n    img_dir = os.path.join(settings.TEST_DIR, \'masks\')\n\n    for i, row in enumerate(df.values):\n        decoded_mask = run_length_decoding(row[1], (101,101))\n        filename = os.path.join(img_dir, \'{}.png\'.format(row[0]))\n        rgb_mask = cv2.cvtColor(decoded_mask,cv2.COLOR_GRAY2RGB)\n        print(filename)\n        cv2.imwrite(filename, decoded_mask)\n        if i % 100 == 0:\n            print(i)\n\n\n\nif __name__ == \'__main__\':\n    save_pseudo_label_masks(\'V456_ensemble_1011.csv\')'"
examples/trials/kaggle-tgs-salt/predict.py,7,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport glob\nimport argparse\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn.functional as F\n\nimport settings\nfrom loader import get_test_loader, add_depth_channel\nfrom models import UNetResNetV4, UNetResNetV5, UNetResNetV6, UNet7, UNet8\nfrom postprocessing import crop_image, binarize, resize_image\nfrom metrics import intersection_over_union, intersection_over_union_thresholds\nfrom utils import create_submission\n\ndef do_tta_predict(args, model, ckp_path, tta_num=4):\n    \'\'\'\n    return 18000x128x128 np array\n    \'\'\'\n    model.eval()\n    preds = []\n    meta = None\n\n    # i is tta index, 0: no change, 1: horizon flip, 2: vertical flip, 3: do both\n    for flip_index in range(tta_num):\n        print(\'flip_index:\', flip_index)\n        test_loader = get_test_loader(args.batch_size, index=flip_index, dev_mode=False, pad_mode=args.pad_mode)\n        meta = test_loader.meta\n        outputs = None\n        with torch.no_grad():\n            for i, img in enumerate(test_loader):\n                add_depth_channel(img, args.pad_mode)\n                img = img.cuda()\n                output, _ = model(img)\n                output = torch.sigmoid(output)\n                if outputs is None:\n                    outputs = output.squeeze()\n                else:\n                    outputs = torch.cat([outputs, output.squeeze()], 0)\n\n                print(\'{} / {}\'.format(args.batch_size*(i+1), test_loader.num), end=\'\\r\')\n        outputs = outputs.cpu().numpy()\n        # flip back masks\n        if flip_index == 1:\n            outputs = np.flip(outputs, 2)\n        elif flip_index == 2:\n            outputs = np.flip(outputs, 1)\n        elif flip_index == 3:\n            outputs = np.flip(outputs, 2)\n            outputs = np.flip(outputs, 1)\n        #print(outputs.shape)\n        preds.append(outputs)\n\n    parent_dir = ckp_path+\'_out\'\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n    np_file = os.path.join(parent_dir, \'pred.npy\')\n\n    model_pred_result = np.mean(preds, 0)\n    np.save(np_file, model_pred_result)\n\n    return model_pred_result, meta\n\ndef predict(args, model, checkpoint, out_file):\n    print(\'predicting {}...\'.format(checkpoint))\n    pred, meta = do_tta_predict(args, model, checkpoint, tta_num=2)\n    print(pred.shape)\n    y_pred_test = generate_preds(pred, (settings.ORIG_H, settings.ORIG_W), pad_mode=args.pad_mode)\n\n    submission = create_submission(meta, y_pred_test)\n    submission.to_csv(out_file, index=None, encoding=\'utf-8\')\n\n\ndef ensemble(args, model, checkpoints):\n    preds = []\n    meta = None\n    for checkpoint in checkpoints:\n        model.load_state_dict(torch.load(checkpoint))\n        model = model.cuda()\n        print(\'predicting...\', checkpoint)\n\n        pred, meta = do_tta_predict(args, model, checkpoint, tta_num=2)\n        preds.append(pred)\n\n    y_pred_test = generate_preds(np.mean(preds, 0), (settings.ORIG_H, settings.ORIG_W), args.pad_mode)\n\n    submission = create_submission(meta, y_pred_test)\n    submission.to_csv(args.sub_file, index=None, encoding=\'utf-8\')\n\ndef ensemble_np(args, np_files, save_np=None):\n    preds = []\n    for np_file in np_files:\n        pred = np.load(np_file)\n        print(np_file, pred.shape)\n        preds.append(pred)\n\n    y_pred_test = generate_preds(np.mean(preds, 0), (settings.ORIG_H, settings.ORIG_W), args.pad_mode)\n\n    if save_np is not None:\n        np.save(save_np, np.mean(preds, 0))\n\n    meta = get_test_loader(args.batch_size, index=0, dev_mode=False, pad_mode=args.pad_mode).meta\n\n    submission = create_submission(meta, y_pred_test)\n    submission.to_csv(args.sub_file, index=None, encoding=\'utf-8\')\n\ndef generate_preds(outputs, target_size, pad_mode, threshold=0.5):\n    preds = []\n\n    for output in outputs:\n        #print(output.shape)\n        if pad_mode == \'resize\':\n            cropped = resize_image(output, target_size=target_size)\n        else:\n            cropped = crop_image(output, target_size=target_size)\n        pred = binarize(cropped, threshold)\n        preds.append(pred)\n\n    return preds\n\n\ndef ensemble_predict(args):\n    model = eval(args.model_name)(args.layers, num_filters=args.nf)\n\n    checkpoints = [\n        r\'D:\\data\\salt\\models\\pseudo\\UNetResNetV4_34\\edge\\best_5.pth\',\n        r\'D:\\data\\salt\\models\\pseudo\\UNetResNetV4_34\\edge\\best_6.pth\',\n        r\'D:\\data\\salt\\models\\pseudo\\UNetResNetV4_34\\edge\\best_8.pth\',\n        r\'D:\\data\\salt\\models\\pseudo\\UNetResNetV4_34\\edge\\best_9.pth\'\n    ]\n    print(checkpoints)\n\n    ensemble(args, model, checkpoints)\n\ndef ensemble_np_results(args):\n    np_files1 = glob.glob(r\'D:\\data\\salt\\models\\depths\\UNetResNetV5_50\\edge\\*pth_out\\*.npy\')\n    np_files2 = glob.glob(r\'D:\\data\\salt\\models\\depths\\UNetResNetV4_34\\edge\\*pth_out\\*.npy\')\n    np_files3 = glob.glob(r\'D:\\data\\salt\\models\\depths\\UNetResNetV6_34\\edge\\*pth_out\\*.npy\')\n    np_files6 = glob.glob(r\'D:\\data\\salt\\models\\ensemble\\*.npy\')\n    np_files = np_files1 + np_files2 + np_files3 + np_files6\n    print(np_files)\n    ensemble_np(args, np_files)\n\ndef predict_model(args):\n    model = eval(args.model_name)(args.layers, num_filters=args.nf)\n    model_subdir = args.pad_mode\n    if args.meta_version == 2:\n        model_subdir = args.pad_mode+\'_meta2\'\n    if args.exp_name is None:\n        model_file = os.path.join(settings.MODEL_DIR, model.name,model_subdir, \'best_{}.pth\'.format(args.ifold))\n    else:\n        model_file = os.path.join(settings.MODEL_DIR, args.exp_name, model.name, model_subdir, \'best_{}.pth\'.format(args.ifold))\n\n    if os.path.exists(model_file):\n        print(\'loading {}...\'.format(model_file))\n        model.load_state_dict(torch.load(model_file))\n    else:\n        raise ValueError(\'model file not found: {}\'.format(model_file))\n    model = model.cuda()\n    predict(args, model, model_file, args.sub_file)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Salt segmentation\')\n    parser.add_argument(\'--model_name\', required=True, type=str, help=\'\')\n    parser.add_argument(\'--layers\', default=34, type=int, help=\'model layers\')\n    parser.add_argument(\'--nf\', default=32, type=int, help=\'num_filters param for model\')\n    parser.add_argument(\'--ifold\', required=True, type=int, help=\'kfold indices\')\n    parser.add_argument(\'--batch_size\', default=32, type=int, help=\'batch_size\')\n    parser.add_argument(\'--pad_mode\', required=True, choices=[\'reflect\', \'edge\', \'resize\'], help=\'pad method\')\n    parser.add_argument(\'--exp_name\', default=\'depths\', type=str, help=\'exp name\')\n    parser.add_argument(\'--meta_version\', default=2, type=int, help=\'meta version\')\n    parser.add_argument(\'--sub_file\', default=\'all_ensemble.csv\', type=str, help=\'submission file\')\n\n    args = parser.parse_args()\n\n    predict_model(args)\n    #ensemble_predict(args)\n    #ensemble_np_results(args)\n'"
examples/trials/kaggle-tgs-salt/preprocess.py,1,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport json\nimport torch\nimport torch.nn as nn\nfrom keras.preprocessing.image import load_img\nfrom sklearn.model_selection import StratifiedKFold\nimport settings\nimport utils\n\nDATA_DIR = settings.DATA_DIR\n\ndef prepare_metadata():\n    print(\'creating metadata\')\n    meta = utils.generate_metadata(train_images_dir=settings.TRAIN_DIR,\n                                   test_images_dir=settings.TEST_DIR,\n                                   depths_filepath=settings.DEPTHS_FILE\n                                   )\n    meta.to_csv(settings.META_FILE, index=None)\n\ndef cov_to_class(val):\n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n\ndef generate_stratified_metadata():\n    train_df = pd.read_csv(os.path.join(DATA_DIR, ""train.csv""), index_col=""id"", usecols=[0])\n    depths_df = pd.read_csv(os.path.join(DATA_DIR, ""depths.csv""), index_col=""id"")\n    train_df = train_df.join(depths_df)\n    train_df[""masks""] = [np.array(load_img(os.path.join(DATA_DIR, ""train"", ""masks"", ""{}.png"".format(idx)), grayscale=True)) / 255 for idx in train_df.index]\n    train_df[""coverage""] = train_df.masks.map(np.sum) / pow(settings.ORIG_H, 2)\n    train_df[""coverage_class""] = train_df.coverage.map(cov_to_class)\n    train_df[""salt_exists""] = train_df.coverage_class.map(lambda x: 0 if x == 0 else 1)\n    train_df[""is_train""] = 1\n    train_df[""file_path_image""] = train_df.index.map(lambda x: os.path.join(settings.TRAIN_IMG_DIR, \'{}.png\'.format(x)))\n    train_df[""file_path_mask""] = train_df.index.map(lambda x: os.path.join(settings.TRAIN_MASK_DIR, \'{}.png\'.format(x)))\n\n    train_df.to_csv(os.path.join(settings.DATA_DIR, \'train_meta2.csv\'),\n        columns=[\'file_path_image\',\'file_path_mask\',\'is_train\',\'z\',\'salt_exists\', \'coverage_class\', \'coverage\'])\n    train_splits = {}\n\n    kf = StratifiedKFold(n_splits=10)\n    for i, (train_index, valid_index) in enumerate(kf.split(train_df.index.values.reshape(-1), train_df.coverage_class.values.reshape(-1))):\n        train_splits[str(i)] = {\n            \'train_index\': train_index.tolist(),\n            \'val_index\': valid_index.tolist()\n        }\n    with open(os.path.join(settings.DATA_DIR, \'train_split.json\'), \'w\') as f:\n        json.dump(train_splits, f, indent=4)\n\n    print(\'done\')\n\n\ndef test():\n    meta = pd.read_csv(settings.META_FILE)\n    meta_train = meta[meta[\'is_train\'] == 1]\n    print(type(meta_train))\n\n    cv = utils.KFoldBySortedValue()\n    for train_idx, valid_idx in cv.split(meta_train[settings.DEPTH_COLUMN].values.reshape(-1)):\n        print(len(train_idx), len(valid_idx))\n        print(train_idx[:10])\n        print(valid_idx[:10])\n        #break\n\n    meta_train_split, meta_valid_split = meta_train.iloc[train_idx], meta_train.iloc[valid_idx]\n    print(type(meta_train_split))\n    print(meta_train_split[settings.X_COLUMN].values[:10])\n\nif __name__ == \'__main__\':\n    generate_stratified_metadata()\n'"
examples/trials/kaggle-tgs-salt/settings.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\n\nDATA_DIR = r\'/mnt/chicm/data/salt\'\n\nTRAIN_DIR = os.path.join(DATA_DIR, \'train\')\nTEST_DIR = os.path.join(DATA_DIR, \'test\')\n\nTRAIN_IMG_DIR = os.path.join(TRAIN_DIR, \'images\')\nTRAIN_MASK_DIR =  os.path.join(TRAIN_DIR, \'masks\')\nTEST_IMG_DIR = os.path.join(TEST_DIR, \'images\')\n\nLABEL_FILE = os.path.join(DATA_DIR, \'train.csv\')\nDEPTHS_FILE = os.path.join(DATA_DIR, \'depths.csv\')\nMETA_FILE = os.path.join(DATA_DIR, \'meta.csv\')\n\nMODEL_DIR = os.path.join(DATA_DIR, \'models\')\n\nID_COLUMN = \'id\'\nDEPTH_COLUMN = \'z\'\nX_COLUMN = \'file_path_image\'\nY_COLUMN = \'file_path_mask\'\n\nH = W = 128\nORIG_H = ORIG_W = 101'"
examples/trials/kaggle-tgs-salt/train.py,9,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport argparse\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n\nfrom loader import get_train_loaders, add_depth_channel\nfrom models import UNetResNetV4, UNetResNetV5, UNetResNetV6\nfrom lovasz_losses import lovasz_hinge\nfrom focal_loss import FocalLoss2d\nfrom postprocessing import binarize, crop_image, resize_image\nfrom metrics import intersection_over_union, intersection_over_union_thresholds\nimport settings\n\nMODEL_DIR = settings.MODEL_DIR\nfocal_loss2d = FocalLoss2d()\n\ndef weighted_loss(args, output, target, epoch=0):\n    mask_output, salt_output = output\n    mask_target, salt_target = target\n\n    lovasz_loss = lovasz_hinge(mask_output, mask_target)\n    focal_loss = focal_loss2d(mask_output, mask_target)\n\n    focal_weight = 0.2\n\n    if salt_output is not None and args.train_cls:\n        salt_loss = F.binary_cross_entropy_with_logits(salt_output, salt_target)\n        return salt_loss, focal_loss.item(), lovasz_loss.item(), salt_loss.item(), lovasz_loss.item() + focal_loss.item()*focal_weight\n\n    return lovasz_loss+focal_loss*focal_weight, focal_loss.item(), lovasz_loss.item(), 0., lovasz_loss.item() + focal_loss.item()*focal_weight\n\ndef train(args):\n    print(\'start training...\')\n\n    """"""@nni.variable(nni.choice(\'UNetResNetV4\', \'UNetResNetV5\', \'UNetResNetV6\'), name=model_name)""""""\n    model_name = args.model_name\n\n    model = eval(model_name)(args.layers, num_filters=args.nf)\n    model_subdir = args.pad_mode\n    if args.meta_version == 2:\n        model_subdir = args.pad_mode+\'_meta2\'\n    if args.exp_name is None:\n        model_file = os.path.join(MODEL_DIR, model.name,model_subdir, \'best_{}.pth\'.format(args.ifold))\n    else:\n        model_file = os.path.join(MODEL_DIR, args.exp_name, model.name, model_subdir, \'best_{}.pth\'.format(args.ifold))\n\n    parent_dir = os.path.dirname(model_file)\n    if not os.path.exists(parent_dir):\n        os.makedirs(parent_dir)\n\n    if args.init_ckp is not None:\n        CKP = args.init_ckp\n    else:\n        CKP = model_file\n    if os.path.exists(CKP):\n        print(\'loading {}...\'.format(CKP))\n        model.load_state_dict(torch.load(CKP))\n    model = model.cuda()\n\n    if args.optim == \'Adam\':\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=0.0001)\n    else:\n        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.0001)\n\n    train_loader, val_loader = get_train_loaders(args.ifold, batch_size=args.batch_size, dev_mode=args.dev_mode, \\\n        pad_mode=args.pad_mode, meta_version=args.meta_version, pseudo_label=args.pseudo, depths=args.depths)\n\n    if args.lrs == \'plateau\':\n        lr_scheduler = ReduceLROnPlateau(optimizer, mode=\'max\', factor=args.factor, patience=args.patience, min_lr=args.min_lr)\n    else:\n        lr_scheduler = CosineAnnealingLR(optimizer, args.t_max, eta_min=args.min_lr)\n\n    print(\'epoch |   lr    |   %       |  loss  |  avg   | f loss | lovaz  |  iou   | iout   |  best  | time | save |  salt  |\')\n\n    best_iout, _iou, _f, _l, _salt, best_mix_score = validate(args, model, val_loader, args.start_epoch)\n    print(\'val   |         |           |        |        | {:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f} |      |      | {:.4f} |\'.format(\n        _f, _l, _iou, best_iout, best_iout, _salt))\n    if args.val:\n        return\n\n    model.train()\n\n    if args.lrs == \'plateau\':\n        lr_scheduler.step(best_iout)\n    else:\n        lr_scheduler.step()\n\n    for epoch in range(args.start_epoch, args.epochs):\n        train_loss = 0\n\n        current_lr = get_lrs(optimizer)\n        bg = time.time()\n        for batch_idx, data in enumerate(train_loader):\n            img, target, salt_target = data\n            if args.depths:\n                add_depth_channel(img, args.pad_mode)\n            img, target, salt_target = img.cuda(), target.cuda(), salt_target.cuda()\n            optimizer.zero_grad()\n            output, salt_out = model(img)\n\n            loss, *_ = weighted_loss(args, (output, salt_out), (target, salt_target), epoch=epoch)\n            loss.backward()\n\n            if args.optim == \'Adam\' and args.adamw:\n                wd = 0.0001\n                for group in optimizer.param_groups:\n                    for param in group[\'params\']:\n                        param.data = param.data.add(-wd * group[\'lr\'], param.data)\n\n            optimizer.step()\n\n            train_loss += loss.item()\n            print(\'\\r {:4d} | {:.5f} | {:4d}/{} | {:.4f} | {:.4f} |\'.format(\n                epoch, float(current_lr[0]), args.batch_size*(batch_idx+1), train_loader.num, loss.item(), train_loss/(batch_idx+1)), end=\'\')\n\n        iout, iou, focal_loss, lovaz_loss, salt_loss, mix_score = validate(args, model, val_loader, epoch=epoch)\n        """"""@nni.report_intermediate_result(iout)""""""\n\n        _save_ckp = \'\'\n        if iout > best_iout:\n            best_iout = iout\n            torch.save(model.state_dict(), model_file)\n            _save_ckp = \'*\'\n        if args.store_loss_model and mix_score > best_mix_score:\n            best_mix_score = mix_score\n            torch.save(model.state_dict(), model_file+\'_loss\')\n            _save_ckp += \'.\'\n        print(\' {:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} | {:.4f} |\'.format(\n            focal_loss, lovaz_loss, iou, iout, best_iout, (time.time() - bg) / 60, _save_ckp, salt_loss))\n\n        model.train()\n\n        if args.lrs == \'plateau\':\n            lr_scheduler.step(best_iout)\n        else:\n            lr_scheduler.step()\n\n    del model, train_loader, val_loader, optimizer, lr_scheduler\n    """"""@nni.report_final_result(best_iout)""""""\n\ndef get_lrs(optimizer):\n    lrs = []\n    for pgs in optimizer.state_dict()[\'param_groups\']:\n        lrs.append(pgs[\'lr\'])\n    lrs = [\'{:.6f}\'.format(x) for x in lrs]\n    return lrs\n\ndef validate(args, model, val_loader, epoch=0, threshold=0.5):\n    model.eval()\n    outputs = []\n    focal_loss, lovaz_loss, salt_loss, w_loss = 0, 0, 0, 0\n    with torch.no_grad():\n        for img, target, salt_target in val_loader:\n            if args.depths:\n                add_depth_channel(img, args.pad_mode)\n            img, target, salt_target = img.cuda(), target.cuda(), salt_target.cuda()\n            output, salt_out = model(img)\n\n            _, floss, lovaz, _salt_loss, _w_loss = weighted_loss(args, (output, salt_out), (target, salt_target), epoch=epoch)\n            focal_loss += floss\n            lovaz_loss += lovaz\n            salt_loss += _salt_loss\n            w_loss += _w_loss\n            output = torch.sigmoid(output)\n\n            for o in output.cpu():\n                outputs.append(o.squeeze().numpy())\n\n    n_batches = val_loader.num // args.batch_size if val_loader.num % args.batch_size == 0 else val_loader.num // args.batch_size + 1\n\n    # y_pred, list of np array, each np array\'s shape is 101,101\n    y_pred = generate_preds(args, outputs, (settings.ORIG_H, settings.ORIG_W), threshold)\n\n    iou_score = intersection_over_union(val_loader.y_true, y_pred)\n    iout_score = intersection_over_union_thresholds(val_loader.y_true, y_pred)\n\n    return iout_score, iou_score, focal_loss / n_batches, lovaz_loss / n_batches, salt_loss / n_batches, iout_score*4 - w_loss\n\n\ndef generate_preds(args, outputs, target_size, threshold=0.5):\n    preds = []\n\n    for output in outputs:\n        if args.pad_mode == \'resize\':\n            cropped = resize_image(output, target_size=target_size)\n        else:\n            cropped = crop_image(output, target_size=target_size)\n        pred = binarize(cropped, threshold)\n        preds.append(pred)\n\n    return preds\n\nif __name__ == \'__main__\':\n\n    parser = argparse.ArgumentParser(description=\'TGS Salt segmentation\')\n    parser.add_argument(\'--layers\', default=34, type=int, help=\'model layers\')\n    parser.add_argument(\'--nf\', default=32, type=int, help=\'num_filters param for model\')\n    parser.add_argument(\'--lr\', default=0.001, type=float, help=\'learning rate\')\n    parser.add_argument(\'--min_lr\', default=0.0001, type=float, help=\'min learning rate\')\n    parser.add_argument(\'--ifolds\', default=\'0\', type=str, help=\'kfold indices\')\n    parser.add_argument(\'--batch_size\', default=32, type=int, help=\'batch_size\')\n    parser.add_argument(\'--start_epoch\', default=0, type=int, help=\'start epoch\')\n    parser.add_argument(\'--epochs\', default=200, type=int, help=\'epoch\')\n    parser.add_argument(\'--optim\', default=\'SGD\', choices=[\'SGD\', \'Adam\'], help=\'optimizer\')\n    parser.add_argument(\'--lrs\', default=\'cosine\', choices=[\'cosine\', \'plateau\'], help=\'LR sceduler\')\n    parser.add_argument(\'--patience\', default=6, type=int, help=\'lr scheduler patience\')\n    parser.add_argument(\'--factor\', default=0.5, type=float, help=\'lr scheduler factor\')\n    parser.add_argument(\'--t_max\', default=15, type=int, help=\'lr scheduler patience\')\n    parser.add_argument(\'--pad_mode\', default=\'edge\', choices=[\'reflect\', \'edge\', \'resize\'], help=\'pad method\')\n    parser.add_argument(\'--exp_name\', default=None, type=str, help=\'exp name\')\n    parser.add_argument(\'--model_name\', default=\'UNetResNetV4\', type=str, help=\'\')\n    parser.add_argument(\'--init_ckp\', default=None, type=str, help=\'resume from checkpoint path\')\n    parser.add_argument(\'--val\', action=\'store_true\')\n    parser.add_argument(\'--store_loss_model\', action=\'store_true\')\n    parser.add_argument(\'--train_cls\', action=\'store_true\')\n    parser.add_argument(\'--meta_version\', default=2, type=int, help=\'meta version\')\n    parser.add_argument(\'--pseudo\', action=\'store_true\')\n    parser.add_argument(\'--depths\', action=\'store_true\')\n    parser.add_argument(\'--dev_mode\', action=\'store_true\')\n    parser.add_argument(\'--adamw\', action=\'store_true\')\n\n    args = parser.parse_args()\n\n    \'\'\'@nni.get_next_parameter()\'\'\'\n\n    print(args)\n    ifolds = [int(x) for x in args.ifolds.split(\',\')]\n    print(ifolds)\n\n    for i in ifolds:\n        args.ifold = i\n        train(args)\n'"
examples/trials/kaggle-tgs-salt/utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport json\nimport sys\nimport time\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom pycocotools import mask as cocomask\nfrom sklearn.model_selection import KFold\n\nimport settings\n\ndef create_submission(meta, predictions):\n    output = []\n    for image_id, mask in zip(meta[\'id\'].values, predictions):\n        rle_encoded = \' \'.join(str(rle) for rle in run_length_encoding(mask))\n        output.append([image_id, rle_encoded])\n\n    submission = pd.DataFrame(output, columns=[\'id\', \'rle_mask\']).astype(str)\n    return submission\n\n\ndef encode_rle(predictions):\n    return [run_length_encoding(mask) for mask in predictions]\n\n\ndef read_masks(img_ids):\n    masks = []\n    for img_id in img_ids:\n        base_filename = \'{}.png\'.format(img_id)\n        mask = Image.open(os.path.join(settings.TRAIN_MASK_DIR, base_filename))\n        mask = np.asarray(mask.convert(\'L\').point(lambda x: 0 if x < 128 else 1)).astype(np.uint8)\n        masks.append(mask)\n    return masks\n\n\ndef run_length_encoding(x):\n    bs = np.where(x.T.flatten())[0]\n\n    rle = []\n    prev = -2\n    for b in bs:\n        if (b > prev + 1): rle.extend((b + 1, 0))\n        rle[-1] += 1\n        prev = b\n    return rle\n\n\ndef run_length_decoding(mask_rle, shape):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[1] * shape[0], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 255\n    return img.reshape((shape[1], shape[0])).T\n\ndef get_salt_existence():\n    train_mask = pd.read_csv(settings.LABEL_FILE)\n    salt_exists_dict = {}\n    for row in train_mask.values:\n        salt_exists_dict[row[0]] = 0 if (row[1] is np.nan or len(row[1]) < 1) else 1\n    return salt_exists_dict\n\ndef generate_metadata(train_images_dir, test_images_dir, depths_filepath):\n    depths = pd.read_csv(depths_filepath)\n    salt_exists_dict = get_salt_existence()\n\n    metadata = {}\n    for filename in tqdm(os.listdir(os.path.join(train_images_dir, \'images\'))):\n        image_filepath = os.path.join(train_images_dir, \'images\', filename)\n        mask_filepath = os.path.join(train_images_dir, \'masks\', filename)\n        image_id = filename.split(\'.\')[0]\n        depth = depths[depths[\'id\'] == image_id][\'z\'].values[0]\n\n        metadata.setdefault(\'file_path_image\', []).append(image_filepath)\n        metadata.setdefault(\'file_path_mask\', []).append(mask_filepath)\n        metadata.setdefault(\'is_train\', []).append(1)\n        metadata.setdefault(\'id\', []).append(image_id)\n        metadata.setdefault(\'z\', []).append(depth)\n        metadata.setdefault(\'salt_exists\', []).append(salt_exists_dict[image_id])\n\n    for filename in tqdm(os.listdir(os.path.join(test_images_dir, \'images\'))):\n        image_filepath = os.path.join(test_images_dir, \'images\', filename)\n        image_id = filename.split(\'.\')[0]\n        depth = depths[depths[\'id\'] == image_id][\'z\'].values[0]\n\n        metadata.setdefault(\'file_path_image\', []).append(image_filepath)\n        metadata.setdefault(\'file_path_mask\', []).append(None)\n        metadata.setdefault(\'is_train\', []).append(0)\n        metadata.setdefault(\'id\', []).append(image_id)\n        metadata.setdefault(\'z\', []).append(depth)\n        metadata.setdefault(\'salt_exists\', []).append(0)\n\n    return pd.DataFrame(metadata)\n\ndef rle_from_binary(prediction):\n    prediction = np.asfortranarray(prediction)\n    return cocomask.encode(prediction)\n\n\ndef binary_from_rle(rle):\n    return cocomask.decode(rle)\n\n\ndef get_segmentations(labeled):\n    nr_true = labeled.max()\n    segmentations = []\n    for i in range(1, nr_true + 1):\n        msk = labeled == i\n        segmentation = rle_from_binary(msk.astype(\'uint8\'))\n        segmentation[\'counts\'] = segmentation[\'counts\'].decode(""UTF-8"")\n        segmentations.append(segmentation)\n    return segmentations\n\n\ndef get_crop_pad_sequence(vertical, horizontal):\n    top = int(vertical / 2)\n    bottom = vertical - top\n    right = int(horizontal / 2)\n    left = horizontal - right\n    return (top, right, bottom, left)\n\n\ndef get_nfold_split(ifold, nfold=10, meta_version=1):\n    if meta_version == 2:\n        return get_nfold_split2(ifold, nfold)\n\n    meta = pd.read_csv(settings.META_FILE, na_filter=False)\n    meta_train = meta[meta[\'is_train\'] == 1]\n\n    kf = KFold(n_splits=nfold)\n    for i, (train_index, valid_index) in enumerate(kf.split(meta_train[settings.ID_COLUMN].values.reshape(-1))):\n        if i == ifold:\n            break\n    return meta_train.iloc[train_index], meta_train.iloc[valid_index]\n\ndef get_nfold_split2(ifold, nfold=10):\n    meta_train = pd.read_csv(os.path.join(settings.DATA_DIR, \'train_meta2.csv\'))\n\n    with open(os.path.join(settings.DATA_DIR, \'train_split.json\'), \'r\') as f:\n        train_splits = json.load(f)\n    train_index = train_splits[str(ifold)][\'train_index\']\n    valid_index = train_splits[str(ifold)][\'val_index\']\n\n    return meta_train.iloc[train_index], meta_train.iloc[valid_index]\n\n\ndef get_test_meta():\n    meta = pd.read_csv(settings.META_FILE, na_filter=False)\n    test_meta = meta[meta[\'is_train\'] == 0]\n    print(len(test_meta.values))\n    return test_meta\n\nif __name__ == \'__main__\':\n    get_nfold_split(2)\n'"
examples/trials/mnist-advisor/mnist.py,0,"b'""""""A deep MNIST classifier using convolutional layers.""""""\n\nimport argparse\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport nni\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initializing and building basic network for mnist.\n    \'\'\'\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 conv_size,\n                 hidden_size,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = conv_size\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 conv_size=params[\'conv_size\'],\n                                 hidden_size=params[\'hidden_size\'],\n                                 pool_size=params[\'pool_size\'],\n                                 learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(params[\'batch_num\']):\n            batch = mnist.train.next_batch(params[\'batch_size\'])\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: 1 - params[\'dropout_rate\']}\n                                        )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                nni.report_intermediate_result(test_acc)\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        nni.report_final_result(test_acc)\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\ndef get_params():\n    \'\'\' Get parameters from command line \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/tmp/tensorflow/mnist/input_data\', help=""data directory"")\n    parser.add_argument(""--dropout_rate"", type=float, default=0.5, help=""dropout rate"")\n    parser.add_argument(""--channel_1_num"", type=int, default=32)\n    parser.add_argument(""--channel_2_num"", type=int, default=64)\n    parser.add_argument(""--conv_size"", type=int, default=5)\n    parser.add_argument(""--pool_size"", type=int, default=2)\n    parser.add_argument(""--hidden_size"", type=int, default=1024)\n    parser.add_argument(""--learning_rate"", type=float, default=1e-4)\n    parser.add_argument(""--batch_num"", type=int, default=2700)\n    parser.add_argument(""--batch_size"", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args\n\nif __name__ == \'__main__\':\n    try:\n        # get parameters form tuner\n        tuner_params = nni.get_next_parameter()\n        logger.debug(tuner_params)\n        tuner_params[\'batch_num\'] = tuner_params[\'TRIAL_BUDGET\'] * 100\n        params = vars(get_params())\n        params.update(tuner_params)\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-annotation/mnist.py,0,"b'""""""A deep MNIST classifier using convolutional layers.""""""\n\nimport argparse\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initializing and building basic network for mnist.\n    \'\'\'\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 conv_size,\n                 hidden_size,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        """"""@nni.variable(nni.choice(2, 3, 5, 7),name=self.conv_size)""""""\n        self.conv_size = conv_size\n        """"""@nni.variable(nni.choice(124, 512, 1024), name=self.hidden_size)""""""\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        """"""@nni.variable(nni.loguniform(0.0001, 0.1), name=self.learning_rate)""""""\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            """"""@nni.function_choice(tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1), tf.nn.sigmoid(conv2d(x_image, w_conv1) + b_conv1), tf.nn.tanh(conv2d(x_image, w_conv1) + b_conv1), name=tf.nn.relu)""""""\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            """"""@nni.function_choice(max_pool(h_conv1, self.pool_size), avg_pool(h_conv1, self.pool_size), name=max_pool)""""""\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef avg_pool(x_input, pool_size):\n    return tf.nn.avg_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 conv_size=params[\'conv_size\'],\n                                 hidden_size=params[\'hidden_size\'],\n                                 pool_size=params[\'pool_size\'],\n                                 learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        """"""@nni.variable(nni.choice(16, 32), name=batch_size)""""""\n        batch_size = params[\'batch_size\']\n        for i in range(params[\'batch_num\']):\n            batch = mnist.train.next_batch(batch_size)\n            """"""@nni.variable(nni.choice(0.5, 0.9), name=dropout_rate)""""""\n            dropout_rate = params[\'dropout_rate\']\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: 1 - dropout_rate}\n                                        )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                """"""@nni.report_intermediate_result(test_acc)""""""\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        """"""@nni.report_final_result(test_acc)""""""\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\ndef get_params():\n    \'\'\' Get parameters from command line \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/tmp/tensorflow/mnist/input_data\', help=""data directory"")\n    parser.add_argument(""--dropout_rate"", type=float, default=0.5, help=""dropout rate"")\n    parser.add_argument(""--channel_1_num"", type=int, default=32)\n    parser.add_argument(""--channel_2_num"", type=int, default=64)\n    parser.add_argument(""--conv_size"", type=int, default=5)\n    parser.add_argument(""--pool_size"", type=int, default=2)\n    parser.add_argument(""--hidden_size"", type=int, default=1024)\n    parser.add_argument(""--learning_rate"", type=float, default=1e-4)\n    parser.add_argument(""--batch_num"", type=int, default=2000)\n    parser.add_argument(""--batch_size"", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args\n\nif __name__ == \'__main__\':\n    \'\'\'@nni.get_next_parameter()\'\'\'\n    try:\n        main(vars(get_params()))\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-batch-tune-keras/mnist-keras.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\n\nimport os\nimport keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nfrom keras.datasets import mnist\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.models import Sequential\n\nimport nni\n\nLOG = logging.getLogger(\'mnist_keras\')\nK.set_image_data_format(\'channels_last\')\nTENSORBOARD_DIR = os.environ[\'NNI_OUTPUT_DIR\']\n\nH, W = 28, 28\nNUM_CLASSES = 10\n\ndef create_mnist_model(hyper_params, input_shape=(H, W, 1), num_classes=NUM_CLASSES):\n    \'\'\'\n    Create simple convolutional model\n    \'\'\'\n    layers = [\n        Conv2D(32, kernel_size=(3, 3), activation=\'relu\', input_shape=input_shape),\n        Conv2D(64, (3, 3), activation=\'relu\'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(100, activation=\'relu\'),\n        Dense(num_classes, activation=\'softmax\')\n    ]\n\n    model = Sequential(layers)\n\n    if hyper_params[\'optimizer\'] == \'Adam\':\n        optimizer = keras.optimizers.Adam(lr=hyper_params[\'learning_rate\'])\n    else:\n        optimizer = keras.optimizers.SGD(lr=hyper_params[\'learning_rate\'], momentum=0.9)\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=[\'accuracy\'])\n\n    return model\n\ndef load_mnist_data(args):\n    \'\'\'\n    Load MNIST dataset\n    \'\'\'\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    x_train = (np.expand_dims(x_train, -1).astype(np.float) / 255.)[:args.num_train]\n    x_test = (np.expand_dims(x_test, -1).astype(np.float) / 255.)[:args.num_test]\n    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)[:args.num_train]\n    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)[:args.num_test]\n\n    LOG.debug(\'x_train shape: %s\', (x_train.shape,))\n    LOG.debug(\'x_test shape: %s\', (x_test.shape,))\n\n    return x_train, y_train, x_test, y_test\n\nclass SendMetrics(keras.callbacks.Callback):\n    \'\'\'\n    Keras callback to send metrics to NNI framework\n    \'\'\'\n    def on_epoch_end(self, epoch, logs={}):\n        \'\'\'\n        Run on end of each epoch\n        \'\'\'\n        LOG.debug(logs)\n        # TensorFlow 2.0 API reference claims the key is `val_acc`, but in fact it\'s `val_accuracy`\n        if \'val_acc\' in logs:\n            nni.report_intermediate_result(logs[\'val_acc\'])\n        else:\n            nni.report_intermediate_result(logs[\'val_accuracy\'])\n\ndef train(args, params):\n    \'\'\'\n    Train model\n    \'\'\'\n    x_train, y_train, x_test, y_test = load_mnist_data(args)\n    model = create_mnist_model(params)\n\n    # nni\n    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, verbose=1,\n        validation_data=(x_test, y_test), callbacks=[SendMetrics(), TensorBoard(log_dir=TENSORBOARD_DIR)])\n\n    _, acc = model.evaluate(x_test, y_test, verbose=0)\n    LOG.debug(\'Final result is: %d\', acc)\n    nni.report_final_result(acc)\n\ndef generate_default_params():\n    \'\'\'\n    Generate default hyper parameters\n    \'\'\'\n    return {\n        \'optimizer\': \'Adam\',\n        \'learning_rate\': 0.001\n    }\n\nif __name__ == \'__main__\':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument(""--batch_size"", type=int, default=200, help=""batch size"", required=False)\n    PARSER.add_argument(""--epochs"", type=int, default=10, help=""Train epochs"", required=False)\n    PARSER.add_argument(""--num_train"", type=int, default=60000, help=""Number of train samples to be used, maximum 60000"", required=False)\n    PARSER.add_argument(""--num_test"", type=int, default=10000, help=""Number of test samples to be used, maximum 10000"", required=False)\n\n    ARGS, UNKNOWN = PARSER.parse_known_args()\n\n    try:\n        # get parameters from tuner\n        # RECEIVED_PARAMS = {""optimizer"": ""Adam"", ""learning_rate"": 0.00001}\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = generate_default_params()\n        PARAMS.update(RECEIVED_PARAMS)\n        # train\n        train(ARGS, PARAMS)\n    except Exception as e:\n        LOG.exception(e)\n        raise\n'"
examples/trials/mnist-distributed-pytorch/dist_mnist.py,7,"b'# Copyright 2018 The Kubeflow Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n#\n# NNI (https://github.com/Microsoft/nni) modified this code to show how to\n# integrate distributed pytorch training with NNI SDK\n#\n\nimport os\nimport torch\nimport torch.distributed as dist\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport nni\nimport logging\n\nfrom math import ceil\nfrom random import Random\nfrom torch.autograd import Variable\nfrom torchvision import datasets, transforms\n\nlogger = logging.getLogger(\'nni_pytorch_dist\')\n\nclass Partition(object):\n    """""" Dataset-like object, but only access a subset of it. """"""\n\n    def __init__(self, data, index):\n        self.data = data\n        self.index = index\n\n    def __len__(self):\n        return len(self.index)\n\n    def __getitem__(self, index):\n        data_idx = self.index[index]\n        return self.data[data_idx]\n\n\nclass DataPartitioner(object):\n    """""" Partitions a dataset into different chuncks. """"""\n\n    def __init__(self, data, sizes=[0.7, 0.2, 0.1], seed=1234):\n        self.data = data\n        self.partitions = []\n        rng = Random()\n        rng.seed(seed)\n        data_len = len(data)\n        indexes = [x for x in range(0, data_len)]\n        rng.shuffle(indexes)\n\n        for frac in sizes:\n            part_len = int(frac * data_len)\n            self.partitions.append(indexes[0:part_len])\n            indexes = indexes[part_len:]\n\n    def use(self, partition):\n        return Partition(self.data, self.partitions[partition])\n\n\nclass Net(nn.Module):\n    """""" Network architecture. """"""\n\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef partition_dataset():\n    """""" Partitioning MNIST """"""\n    dataset = datasets.MNIST(\n        \'./data\',\n        train=True,\n        download=True,\n        transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307, ), (0.3081, ))\n        ]))\n    size = dist.get_world_size()\n    bsz = 128 / float(size)\n    partition_sizes = [1.0 / size for _ in range(size)]\n    partition = DataPartitioner(dataset, partition_sizes)\n    partition = partition.use(dist.get_rank())\n    train_set = torch.utils.data.DataLoader(\n        partition, batch_size=int(bsz), shuffle=True)\n    return train_set, bsz\n\n\ndef average_gradients(model):\n    """""" Gradient averaging. """"""\n    size = float(dist.get_world_size())\n    for param in model.parameters():\n        dist.all_reduce(param.grad.data, op=dist.reduce_op.SUM, group=0)\n        param.grad.data /= size\n\n\ndef run(params):\n    """""" Distributed Synchronous SGD Example """"""\n    rank = dist.get_rank()\n    torch.manual_seed(1234)\n    train_set, bsz = partition_dataset()\n    model = Net()\n    model = model\n    optimizer = optim.SGD(model.parameters(), lr=params[\'learning_rate\'], momentum=params[\'momentum\'])\n\n    num_batches = ceil(len(train_set.dataset) / float(bsz))\n    total_loss = 0.0\n    for epoch in range(3):\n        epoch_loss = 0.0\n        for data, target in train_set:\n            data, target = Variable(data), Variable(target)\n            optimizer.zero_grad()\n            output = model(data)\n            loss = F.nll_loss(output, target)\n            epoch_loss += loss.item()\n            loss.backward()\n            average_gradients(model)\n            optimizer.step()\n        #logger.debug(\'Rank: \', rank, \', epoch: \', epoch, \': \', epoch_loss / num_batches)\n        if rank == 0:\n            nni.report_intermediate_result(epoch_loss / num_batches)\n        total_loss += (epoch_loss / num_batches)\n    total_loss /= 3\n    logger.debug(\'Final loss: {}\'.format(total_loss))\n    if rank == 0:\n        nni.report_final_result(total_loss)\n\n\ndef init_processes(fn, params, backend=\'tcp\'):\n    """""" Initialize the distributed environment. """"""\n    dist.init_process_group(backend)\n    fn(params)\n\ndef generate_default_params():\n    \'\'\'\n    Generate default parameters for mnist network.\n    \'\'\'\n    params = {\n        \'learning_rate\': 0.01,\n        \'momentum\': 0.5}\n    return params\n\nif __name__ == ""__main__"":\n    RCV_PARAMS = nni.get_next_parameter()\n    logger.debug(RCV_PARAMS)\n    params = generate_default_params()\n    params.update(RCV_PARAMS)\n    init_processes(run, params)\n\n\n'"
examples/trials/mnist-distributed/dist_mnist.py,0,"b'# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n#\n# NNI (https://github.com/Microsoft/nni) modified this code to show how to\n# integrate distributed tensorflow training with NNI SDK\n#\n""""""Distributed MNIST training and validation, with model replicas.\n\nA simple softmax model with one hidden layer is defined. The parameters\n(weights and biases) are located on one parameter server (ps), while the ops\nare executed on two worker nodes by default. The TF sessions also run on the\nworker node.\nMultiple invocations of this script can be done in parallel, with different\nvalues for --task_index. There should be exactly one invocation with\n--task_index, which will create a master session that carries out variable\ninitialization. The other, non-master, sessions will wait for the master\nsession to finish the initialization before proceeding to the training stage.\n\nThe coordination between the multiple worker invocations occurs due to\nthe definition of the parameters on the same ps devices. The parameter updates\nfrom one worker is visible to all other workers. As such, the workers can\nperform forward computation and gradient calculation in parallel, which\nshould lead to increased training speed for the simple model.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport json\nimport math\nimport os\nimport sys\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nimport nni\n\nflags = tf.app.flags\nflags.DEFINE_string(""data_dir"", ""/tmp/mnist-data"",\n                    ""Directory for storing mnist data"")\nflags.DEFINE_boolean(\n    ""download_only"", False,\n    ""Only perform downloading of data; Do not proceed to ""\n    ""session preparation, model definition or training"")\nflags.DEFINE_integer(\n    ""task_index"", None, ""Worker task index, should be >= 0. task_index=0 is ""\n    ""the master worker task the performs the variable ""\n    ""initialization "")\nflags.DEFINE_integer(\n    ""num_gpus"", 1, ""Total number of gpus for each machine.""\n    ""If you don\'t use GPU, please set it to \'0\'"")\nflags.DEFINE_integer(\n    ""replicas_to_aggregate"", None,\n    ""Number of replicas to aggregate before parameter update""\n    ""is applied (For sync_replicas mode only; default: ""\n    ""num_workers)"")\nflags.DEFINE_integer(""train_steps"", 20000,\n                     ""Number of (global) training steps to perform"")\nflags.DEFINE_boolean(\n    ""sync_replicas"", False,\n    ""Use the sync_replicas (synchronized replicas) mode, ""\n    ""wherein the parameter updates from workers are aggregated ""\n    ""before applied to avoid stale gradients"")\nflags.DEFINE_boolean(\n    ""existing_servers"", False, ""Whether servers already exists. If True, ""\n    ""will use the worker hosts via their GRPC URLs (one client process ""\n    ""per worker host). Otherwise, will create an in-process TensorFlow ""\n    ""server."")\nflags.DEFINE_string(""ps_hosts"", ""localhost:2222"",\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""worker_hosts"", ""localhost:2223,localhost:2224"",\n                    ""Comma-separated list of hostname:port pairs"")\nflags.DEFINE_string(""job_name"", None, ""job name: worker or ps"")\n\nFLAGS = flags.FLAGS\n\nIMAGE_PIXELS = 28\n\n# Example:\n#   cluster = {\'ps\': [\'host1:2222\', \'host2:2222\'],\n#              \'worker\': [\'host3:2222\', \'host4:2222\', \'host5:2222\']}\n#   os.environ[\'TF_CONFIG\'] = json.dumps(\n#       {\'cluster\': cluster,\n#        \'task\': {\'type\': \'worker\', \'index\': 1}})\n\n\ndef generate_default_params():\n    \'\'\'\n    Generate default hyper parameters\n    \'\'\'\n    return {\n        \'learning_rate\': 0.01,\n        \'batch_size\': 100,\n        \'hidden_units\': 100,\n    }\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(unused_argv):\n    # Receive NNI hyper parameter and update it onto default params\n    RECEIVED_PARAMS = nni.get_next_parameter()\n    PARAMS = generate_default_params()\n    PARAMS.update(RECEIVED_PARAMS)\n\n    # Parse environment variable TF_CONFIG to get job_name and task_index\n\n    # If not explicitly specified in the constructor and the TF_CONFIG\n    # environment variable is present, load cluster_spec from TF_CONFIG.\n    tf_config = json.loads(os.environ.get(\'TF_CONFIG\') or \'{}\')\n    task_config = tf_config.get(\'task\', {})\n    task_type = task_config.get(\'type\')\n    task_index = task_config.get(\'index\')\n\n    FLAGS.job_name = task_type\n    FLAGS.task_index = task_index\n\n    mnist = download_mnist_retry(FLAGS.data_dir)\n    if FLAGS.download_only:\n        sys.exit(0)\n\n    if FLAGS.job_name is None or FLAGS.job_name == """":\n        raise ValueError(""Must specify an explicit `job_name`"")\n    if FLAGS.task_index is None or FLAGS.task_index == """":\n        raise ValueError(""Must specify an explicit `task_index`"")\n\n    print(""job name = %s"" % FLAGS.job_name)\n    print(""task index = %d"" % FLAGS.task_index)\n\n    cluster_config = tf_config.get(\'cluster\', {})\n    ps_hosts = cluster_config.get(\'ps\')\n    worker_hosts = cluster_config.get(\'worker\')\n\n    ps_hosts_str = \',\'.join(ps_hosts)\n    worker_hosts_str = \',\'.join(worker_hosts)\n\n    FLAGS.ps_hosts = ps_hosts_str\n    FLAGS.worker_hosts = worker_hosts_str\n\n    # Construct the cluster and start the server\n    ps_spec = FLAGS.ps_hosts.split("","")\n    worker_spec = FLAGS.worker_hosts.split("","")\n\n    # Get the number of workers.\n    num_workers = len(worker_spec)\n\n    cluster = tf.train.ClusterSpec({""ps"": ps_spec, ""worker"": worker_spec})\n\n    if not FLAGS.existing_servers:\n        # Not using existing servers. Create an in-process server.\n        server = tf.train.Server(\n            cluster, job_name=FLAGS.job_name, task_index=FLAGS.task_index)\n        if FLAGS.job_name == ""ps"":\n            server.join()\n\n    is_chief = (FLAGS.task_index == 0)\n    if FLAGS.num_gpus > 0:\n        # Avoid gpu allocation conflict: now allocate task_num -> #gpu\n        # for each worker in the corresponding machine\n        gpu = (FLAGS.task_index % FLAGS.num_gpus)\n        worker_device = ""/job:worker/task:%d/gpu:%d"" % (FLAGS.task_index, gpu)\n    elif FLAGS.num_gpus == 0:\n        # Just allocate the CPU to worker server\n        cpu = 0\n        worker_device = ""/job:worker/task:%d/cpu:%d"" % (FLAGS.task_index, cpu)\n    # The device setter will automatically place Variables ops on separate\n    # parameter servers (ps). The non-Variable ops will be placed on the workers.\n    # The ps use CPU and workers use corresponding GPU\n    with tf.device(\n            tf.train.replica_device_setter(\n                worker_device=worker_device,\n                ps_device=""/job:ps/cpu:0"",\n                cluster=cluster)):\n        global_step = tf.Variable(0, name=""global_step"", trainable=False)\n\n        # Variables of the hidden layer\n        hid_w = tf.Variable(\n            tf.truncated_normal(\n                [IMAGE_PIXELS * IMAGE_PIXELS, PARAMS[\'hidden_units\']],\n                stddev=1.0 / IMAGE_PIXELS),\n            name=""hid_w"")\n        hid_b = tf.Variable(tf.zeros([PARAMS[\'hidden_units\']]), name=""hid_b"")\n\n        # Variables of the softmax layer\n        sm_w = tf.Variable(\n            tf.truncated_normal(\n                [PARAMS[\'hidden_units\'], 10],\n                stddev=1.0 / math.sqrt(PARAMS[\'hidden_units\'])),\n            name=""sm_w"")\n        sm_b = tf.Variable(tf.zeros([10]), name=""sm_b"")\n\n        # Ops: located on the worker specified with FLAGS.task_index\n        x = tf.placeholder(tf.float32, [None, IMAGE_PIXELS * IMAGE_PIXELS])\n        y_ = tf.placeholder(tf.float32, [None, 10])\n\n        hid_lin = tf.nn.xw_plus_b(x, hid_w, hid_b)\n        hid = tf.nn.relu(hid_lin)\n\n        y = tf.nn.softmax(tf.nn.xw_plus_b(hid, sm_w, sm_b))\n        cross_entropy = -tf.reduce_sum(\n            y_ * tf.log(tf.clip_by_value(y, 1e-10, 1.0)))\n\n        opt = tf.train.AdamOptimizer(PARAMS[\'learning_rate\'])\n\n        if FLAGS.sync_replicas:\n            if FLAGS.replicas_to_aggregate is None:\n                replicas_to_aggregate = num_workers\n            else:\n                replicas_to_aggregate = FLAGS.replicas_to_aggregate\n\n            opt = tf.train.SyncReplicasOptimizer(\n                opt,\n                replicas_to_aggregate=replicas_to_aggregate,\n                total_num_replicas=num_workers,\n                name=""mnist_sync_replicas"")\n\n        train_step = opt.minimize(cross_entropy, global_step=global_step)\n\n        if FLAGS.sync_replicas:\n            local_init_op = opt.local_step_init_op\n            if is_chief:\n                local_init_op = opt.chief_init_op\n\n            ready_for_local_init_op = opt.ready_for_local_init_op\n\n            # Initial token and chief queue runners required by the sync_replicas mode\n            chief_queue_runner = opt.get_chief_queue_runner()\n            sync_init_op = opt.get_init_tokens_op()\n\n        init_op = tf.global_variables_initializer()\n        train_dir = tempfile.mkdtemp()\n\n        if FLAGS.sync_replicas:\n            sv = tf.train.Supervisor(\n                is_chief=is_chief,\n                logdir=train_dir,\n                init_op=init_op,\n                local_init_op=local_init_op,\n                ready_for_local_init_op=ready_for_local_init_op,\n                recovery_wait_secs=1,\n                global_step=global_step)\n        else:\n            sv = tf.train.Supervisor(\n                is_chief=is_chief,\n                logdir=train_dir,\n                init_op=init_op,\n                recovery_wait_secs=1,\n                global_step=global_step)\n\n        sess_config = tf.ConfigProto(\n            allow_soft_placement=True,\n            log_device_placement=False,\n            device_filters=[\n                ""/job:ps"", ""/job:worker/task:%d"" % FLAGS.task_index\n            ])\n\n        # The chief worker (task_index==0) session will prepare the session,\n        # while the remaining workers will wait for the preparation to complete.\n        if is_chief:\n            print(""Worker %d: Initializing session..."" % FLAGS.task_index)\n        else:\n            print(""Worker %d: Waiting for session to be initialized..."" %\n                  FLAGS.task_index)\n\n        if FLAGS.existing_servers:\n            server_grpc_url = ""grpc://"" + worker_spec[FLAGS.task_index]\n            print(""Using existing server at: %s"" % server_grpc_url)\n\n            sess = sv.prepare_or_wait_for_session(\n                server_grpc_url, config=sess_config)\n        else:\n            sess = sv.prepare_or_wait_for_session(\n                server.target, config=sess_config)\n\n        print(""Worker %d: Session initialization complete."" % FLAGS.task_index)\n\n        if FLAGS.sync_replicas and is_chief:\n            # Chief worker will start the chief queue runner and call the init op.\n            sess.run(sync_init_op)\n            sv.start_queue_runners(sess, [chief_queue_runner])\n\n        # Perform training\n        time_begin = time.time()\n        print(""Training begins @ %f"" % time_begin)\n\n        local_step = 0\n        while True:\n            # Training feed\n            batch_xs, batch_ys = mnist.train.next_batch(PARAMS[\'batch_size\'])\n            train_feed = {x: batch_xs, y_: batch_ys}\n\n            _, step = sess.run([train_step, global_step], feed_dict=train_feed)\n            local_step += 1\n\n            now = time.time()\n            print(""%f: Worker %d: training step %d done (global step: %d)"" %\n                  (now, FLAGS.task_index, local_step, step))\n\n            if step > 0 and step % 5000 == 0 and is_chief:\n                val_feed = {\n                    x: mnist.validation.images,\n                    y_: mnist.validation.labels\n                }\n                interim_val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n                print(\n                    ""After %d training step(s), validation cross entropy = %g""\n                    % (step, interim_val_xent))\n\n                # Only chief worker can report intermediate metrics\n                nni.report_intermediate_result(interim_val_xent)\n\n            if step >= FLAGS.train_steps:\n                break\n\n        time_end = time.time()\n        print(""Training ends @ %f"" % time_end)\n        training_time = time_end - time_begin\n        print(""Training elapsed time: %f s"" % training_time)\n\n        # Validation feed\n        val_feed = {x: mnist.validation.images, y_: mnist.validation.labels}\n        val_xent = sess.run(cross_entropy, feed_dict=val_feed)\n        print(""After %d training step(s), validation cross entropy = %g"" %\n              (FLAGS.train_steps, val_xent))\n\n        # Only chief worker can report final metrics\n        if is_chief:\n            nni.report_final_result(val_xent)\n\n\nif __name__ == ""__main__"":\n    tf.app.run()\n'"
examples/trials/mnist-keras/mnist-keras.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\n\nimport os\nimport keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nfrom keras.datasets import mnist\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.models import Sequential\n\nimport nni\n\nLOG = logging.getLogger(\'mnist_keras\')\nK.set_image_data_format(\'channels_last\')\nTENSORBOARD_DIR = os.environ[\'NNI_OUTPUT_DIR\']\n\nH, W = 28, 28\nNUM_CLASSES = 10\n\ndef create_mnist_model(hyper_params, input_shape=(H, W, 1), num_classes=NUM_CLASSES):\n    \'\'\'\n    Create simple convolutional model\n    \'\'\'\n    layers = [\n        Conv2D(32, kernel_size=(3, 3), activation=\'relu\', input_shape=input_shape),\n        Conv2D(64, (3, 3), activation=\'relu\'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(100, activation=\'relu\'),\n        Dense(num_classes, activation=\'softmax\')\n    ]\n\n    model = Sequential(layers)\n\n    if hyper_params[\'optimizer\'] == \'Adam\':\n        optimizer = keras.optimizers.Adam(lr=hyper_params[\'learning_rate\'])\n    else:\n        optimizer = keras.optimizers.SGD(lr=hyper_params[\'learning_rate\'], momentum=0.9)\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=[\'accuracy\'])\n\n    return model\n\ndef load_mnist_data(args):\n    \'\'\'\n    Load MNIST dataset\n    \'\'\'\n    mnist_path = os.path.join(os.environ.get(\'NNI_OUTPUT_DIR\'), \'mnist.npz\')\n    (x_train, y_train), (x_test, y_test) = mnist.load_data(path=mnist_path)\n    os.remove(mnist_path)\n\n    x_train = (np.expand_dims(x_train, -1).astype(np.float) / 255.)[:args.num_train]\n    x_test = (np.expand_dims(x_test, -1).astype(np.float) / 255.)[:args.num_test]\n    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)[:args.num_train]\n    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)[:args.num_test]\n\n    LOG.debug(\'x_train shape: %s\', (x_train.shape,))\n    LOG.debug(\'x_test shape: %s\', (x_test.shape,))\n\n    return x_train, y_train, x_test, y_test\n\nclass SendMetrics(keras.callbacks.Callback):\n    \'\'\'\n    Keras callback to send metrics to NNI framework\n    \'\'\'\n    def on_epoch_end(self, epoch, logs={}):\n        \'\'\'\n        Run on end of each epoch\n        \'\'\'\n        LOG.debug(logs)\n        # TensorFlow 2.0 API reference claims the key is `val_acc`, but in fact it\'s `val_accuracy`\n        if \'val_acc\' in logs:\n            nni.report_intermediate_result(logs[\'val_acc\'])\n        else:\n            nni.report_intermediate_result(logs[\'val_accuracy\'])\n\ndef train(args, params):\n    \'\'\'\n    Train model\n    \'\'\'\n    x_train, y_train, x_test, y_test = load_mnist_data(args)\n    model = create_mnist_model(params)\n\n    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, verbose=1,\n        validation_data=(x_test, y_test), callbacks=[SendMetrics(), TensorBoard(log_dir=TENSORBOARD_DIR)])\n\n    _, acc = model.evaluate(x_test, y_test, verbose=0)\n    LOG.debug(\'Final result is: %d\', acc)\n    nni.report_final_result(acc)\n\ndef generate_default_params():\n    \'\'\'\n    Generate default hyper parameters\n    \'\'\'\n    return {\n        \'optimizer\': \'Adam\',\n        \'learning_rate\': 0.001\n    }\n\nif __name__ == \'__main__\':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument(""--batch_size"", type=int, default=200, help=""batch size"", required=False)\n    PARSER.add_argument(""--epochs"", type=int, default=10, help=""Train epochs"", required=False)\n    PARSER.add_argument(""--num_train"", type=int, default=60000, help=""Number of train samples to be used, maximum 60000"", required=False)\n    PARSER.add_argument(""--num_test"", type=int, default=10000, help=""Number of test samples to be used, maximum 10000"", required=False)\n\n    ARGS, UNKNOWN = PARSER.parse_known_args()\n\n    try:\n        # get parameters from tuner\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = generate_default_params()\n        PARAMS.update(RECEIVED_PARAMS)\n        # train\n        train(ARGS, PARAMS)\n    except Exception as e:\n        LOG.exception(e)\n        raise\n'"
examples/trials/mnist-nested-search-space/mnist.py,0,"b'\'\'\'\nmnist.py is an example to show: how to use iterative search space to tune architecture network for mnist.\n\'\'\'\nfrom __future__ import absolute_import, division, print_function\n\nimport logging\nimport math\nimport tempfile\nimport time\nimport argparse\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport nni\n\nlogger = logging.getLogger(\'mnist_nested_search_space\')\nFLAGS = None\n\nclass MnistNetwork(object):\n    def __init__(self, params, feature_size = 784):\n        config = []\n\n        for i in range(4):\n            config.append(params[\'layer\'+str(i)])\n        self.config = config\n        self.feature_size = feature_size\n        self.label_size = 10\n\n\n    def is_expand_dim(self, input):\n        # input is a tensor\n        shape = len(input.get_shape().as_list())\n        if shape < 4:\n            return True\n        return False\n\n\n    def is_flatten(self, input):\n        # input is a tensor\n        shape = len(input.get_shape().as_list())\n        if shape > 2:\n            return True\n        return False\n\n\n    def get_layer(self, layer_config, input, in_height, in_width, id):\n        if layer_config[0] == \'Empty\':\n            return input\n\n        if self.is_expand_dim(input):\n            input = tf.reshape(input, [-1, in_height, in_width, 1])\n        h, w = layer_config[1], layer_config[2]\n\n        if layer_config[0] == \'Conv\':\n            conv_filter = tf.Variable(tf.random_uniform([h, w, 1, 1]), name=\'id_%d_conv_%d_%d\' % (id, h, w))\n            return tf.nn.conv2d(input, filter=conv_filter, strides=[1, 1, 1, 1], padding=\'SAME\')\n        if layer_config[0] == \'Max_pool\':\n            return tf.nn.max_pool(input, ksize=[1, h, w, 1], strides=[1, 1, 1, 1], padding=\'SAME\')\n        if layer_config[0] == \'Avg_pool\':\n            return tf.nn.avg_pool(input, ksize=[1, h, w, 1], strides=[1, 1, 1, 1], padding=\'SAME\')\n\n        print(\'error:\', layer_config)\n        raise Exception(\'%s layer is illegal\'%layer_config[0])\n\n\n    def build_network(self):\n        layer_configs = self.config\n        feature_size = 784\n\n        # define placeholder\n        self.x = tf.placeholder(tf.float32, [None, feature_size], name=""input_x"")\n        self.y = tf.placeholder(tf.int32, [None, self.label_size], name=""input_y"")\n        label_number = 10\n\n        # define network\n        input_layer = self.x\n        in_height = in_width = int(math.sqrt(feature_size))\n        for i, layer_config in enumerate(layer_configs):\n            input_layer = tf.nn.relu(self.get_layer(layer_config, input_layer, in_height, in_width, i))\n\n        output_layer = input_layer\n        if self.is_flatten(output_layer):\n            output_layer = tf.contrib.layers.flatten(output_layer)  # flatten\n        output_layer = tf.layers.dense(output_layer, label_number)\n        child_logit = tf.nn.softmax_cross_entropy_with_logits(logits=output_layer, labels=self.y)\n        child_loss = tf.reduce_mean(child_logit)\n\n        self.train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(child_loss)\n        child_accuracy = tf.equal(tf.argmax(output_layer, 1), tf.argmax(self.y, 1))\n        self.accuracy = tf.reduce_mean(tf.cast(child_accuracy, ""float""))  # add a reduce_mean\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(params)\n    mnist_network.build_network()\n    print(\'build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    #print(\'Saving graph to: %s\' % graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(params[\'batch_num\']):\n            batch = mnist.train.next_batch(params[\'batch_size\'])\n            mnist_network.train_step.run(feed_dict={mnist_network.x: batch[0], mnist_network.y: batch[1]})\n\n            if i % 100 == 0:\n                train_accuracy = mnist_network.accuracy.eval(feed_dict={\n                    mnist_network.x: batch[0], mnist_network.y: batch[1]})\n                print(\'step %d, training accuracy %g\' % (i, train_accuracy))\n\n        test_acc = mnist_network.accuracy.eval(feed_dict={\n            mnist_network.x: mnist.test.images, mnist_network.y: mnist.test.labels})\n\n        nni.report_final_result(test_acc)\n\ndef get_params():\n    \'\'\' Get parameters from command line \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/tmp/tensorflow/mnist/input_data\', help=""data directory"")\n    parser.add_argument(""--batch_num"", type=int, default=1000)\n    parser.add_argument(""--batch_size"", type=int, default=200)\n    args, _ = parser.parse_known_args()\n    return args\n\ndef parse_init_json(data):\n    params = {}\n    for key in data:\n        value = data[key]\n        layer_name = value[""_name""]\n        if layer_name == \'Empty\':\n            # Empty Layer\n            params[key] = [\'Empty\']\n        elif layer_name == \'Conv\':\n            # Conv layer\n            params[key] = [layer_name, value[\'kernel_size\'], value[\'kernel_size\']]\n        else:\n            # Pooling Layer\n            params[key] = [layer_name, value[\'pooling_size\'], value[\'pooling_size\']]\n    return params\n\n\nif __name__ == \'__main__\':\n    try:\n        # get parameters form tuner\n        data = nni.get_next_parameter()\n        logger.debug(data)\n\n        RCV_PARAMS = parse_init_json(data)\n        logger.debug(RCV_PARAMS)\n        params = vars(get_params())\n        params.update(RCV_PARAMS)\n        print(RCV_PARAMS)\n\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-pbt-tuner-pytorch/mnist.py,11,"b'import argparse\nimport logging\n\nimport os\nimport nni\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\n\nlogger = logging.getLogger(\'mnist_pbt_tuner_pytorch_AutoML\')\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 512)\n        self.fc2 = nn.Linear(512, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args[\'log_interval\'] == 0:\n            logger.info(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    accuracy = 100. * correct / len(test_loader.dataset)\n\n    logger.info(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset), accuracy))\n\n    return accuracy\n\n\ndef save_checkpoint(model, checkpoint_path):\n    torch.save(model.state_dict(), checkpoint_path)\n\n\ndef load_checkpoint(checkpoint_path):\n    model_state_dict = torch.load(checkpoint_path)\n    return model_state_dict\n\n\ndef main(args):\n    use_cuda = not args[\'no_cuda\'] and torch.cuda.is_available()\n\n    torch.manual_seed(args[\'seed\'])\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if use_cuda else {}\n\n    data_dir = os.path.join(args[\'data_dir\'], nni.get_trial_id())\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args[\'batch_size\'], shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=1000, shuffle=True, **kwargs)\n\n    model = Net().to(device)\n\n    save_checkpoint_dir = args[\'save_checkpoint_dir\']\n    save_checkpoint_path = os.path.join(save_checkpoint_dir, \'model.pth\')\n    load_checkpoint_path = os.path.join(args[\'load_checkpoint_dir\'], \'model.pth\')\n\n    if os.path.isfile(load_checkpoint_path):\n        model_state_dict = load_checkpoint(load_checkpoint_path)\n        logger.info(""test : "" + load_checkpoint_path)\n        logger.info(type(model_state_dict))\n        model.load_state_dict(model_state_dict)\n\n    optimizer = optim.SGD(model.parameters(), lr=args[\'lr\'],\n                          momentum=args[\'momentum\'])\n\n    #epoch is perturbation interval\n    for epoch in range(1, args[\'epochs\'] + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test_acc = test(args, model, device, test_loader)\n\n        if epoch < args[\'epochs\']:\n            # report intermediate result\n            nni.report_intermediate_result(test_acc)\n            logger.debug(\'test accuracy %g\', test_acc)\n            logger.debug(\'Pipe send intermediate result done.\')\n        else:\n            # report final result\n            nni.report_final_result(test_acc)\n            logger.debug(\'Final result is %g\', test_acc)\n            logger.debug(\'Send final result done.\')\n\n    if not os.path.exists(save_checkpoint_dir):\n        os.makedirs(save_checkpoint_dir)\n    save_checkpoint(model, save_checkpoint_path)\n\n\ndef get_params():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\n    parser.add_argument(""--data_dir"", type=str,\n                        default=\'/tmp/pytorch/mnist/input_data\', help=""data directory"")\n    parser.add_argument(\'--batch_size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                        help=\'SGD momentum (default: 0.5)\')\n    parser.add_argument(\'--epochs\', type=int, default=1, metavar=\'N\',\n                        help=\'number of epochs to train (default: 1)\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--log_interval\', type=int, default=1000, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n\n    parser.add_argument(\'--save_checkpoint_dir\', type=str,\n                        help=\'where to save checkpoint of this trial\')\n    parser.add_argument(\'--load_checkpoint_dir\', type=str,\n                        help=\'where to load the model\')\n\n\n    args, _ = parser.parse_known_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    try:\n        # get parameters form tuner\n        tuner_params = nni.get_next_parameter()\n        logger.debug(tuner_params)\n        params = vars(get_params())\n        params.update(tuner_params)\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise'"
examples/trials/mnist-pytorch/mnist.py,9,"b'""""""\nA deep MNIST classifier using convolutional layers.\n\nThis file is a modification of the official pytorch mnist example:\nhttps://github.com/pytorch/examples/blob/master/mnist/main.py\n""""""\n\nimport os\nimport argparse\nimport logging\nimport nni\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass Net(nn.Module):\n    def __init__(self, hidden_size):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef train(args, model, device, train_loader, optimizer, epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        if (args[\'batch_num\'] is not None) and batch_idx >= args[\'batch_num\']:\n            break\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % args[\'log_interval\'] == 0:\n            logger.info(\'Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\n\ndef test(args, model, device, test_loader):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data)\n            # sum up batch loss\n            test_loss += F.nll_loss(output, target, reduction=\'sum\').item()\n            # get the index of the max log-probability\n            pred = output.argmax(dim=1, keepdim=True)\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n\n    accuracy = 100. * correct / len(test_loader.dataset)\n\n    logger.info(\'\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\'.format(\n        test_loss, correct, len(test_loader.dataset), accuracy))\n\n    return accuracy\n\n\ndef main(args):\n    use_cuda = not args[\'no_cuda\'] and torch.cuda.is_available()\n\n    torch.manual_seed(args[\'seed\'])\n\n    device = torch.device(""cuda"" if use_cuda else ""cpu"")\n\n    kwargs = {\'num_workers\': 1, \'pin_memory\': True} if use_cuda else {}\n\n    data_dir = os.path.join(args[\'data_dir\'], nni.get_trial_id())\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=args[\'batch_size\'], shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(data_dir, train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=1000, shuffle=True, **kwargs)\n\n    hidden_size = args[\'hidden_size\']\n\n    model = Net(hidden_size=hidden_size).to(device)\n    optimizer = optim.SGD(model.parameters(), lr=args[\'lr\'],\n                          momentum=args[\'momentum\'])\n\n    for epoch in range(1, args[\'epochs\'] + 1):\n        train(args, model, device, train_loader, optimizer, epoch)\n        test_acc = test(args, model, device, test_loader)\n\n        # report intermediate result\n        nni.report_intermediate_result(test_acc)\n        logger.debug(\'test accuracy %g\', test_acc)\n        logger.debug(\'Pipe send intermediate result done.\')\n\n    # report final result\n    nni.report_final_result(test_acc)\n    logger.debug(\'Final result is %g\', test_acc)\n    logger.debug(\'Send final result done.\')\n\n\ndef get_params():\n    # Training settings\n    parser = argparse.ArgumentParser(description=\'PyTorch MNIST Example\')\n    parser.add_argument(""--data_dir"", type=str,\n                        default=\'/tmp/pytorch/mnist/input_data\', help=""data directory"")\n    parser.add_argument(\'--batch_size\', type=int, default=64, metavar=\'N\',\n                        help=\'input batch size for training (default: 64)\')\n    parser.add_argument(""--batch_num"", type=int, default=None)\n    parser.add_argument(""--hidden_size"", type=int, default=512, metavar=\'N\',\n                        help=\'hidden layer size (default: 512)\')\n    parser.add_argument(\'--lr\', type=float, default=0.01, metavar=\'LR\',\n                        help=\'learning rate (default: 0.01)\')\n    parser.add_argument(\'--momentum\', type=float, default=0.5, metavar=\'M\',\n                        help=\'SGD momentum (default: 0.5)\')\n    parser.add_argument(\'--epochs\', type=int, default=10, metavar=\'N\',\n                        help=\'number of epochs to train (default: 10)\')\n    parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                        help=\'random seed (default: 1)\')\n    parser.add_argument(\'--no_cuda\', action=\'store_true\', default=False,\n                        help=\'disables CUDA training\')\n    parser.add_argument(\'--log_interval\', type=int, default=1000, metavar=\'N\',\n                        help=\'how many batches to wait before logging training status\')\n\n\n    args, _ = parser.parse_known_args()\n    return args\n\n\nif __name__ == \'__main__\':\n    try:\n        # get parameters form tuner\n        tuner_params = nni.get_next_parameter()\n        logger.debug(tuner_params)\n        params = vars(get_params())\n        params.update(tuner_params)\n        print(params)\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-tfv1/mnist.py,0,"b'""""""A deep MNIST classifier using convolutional layers.""""""\n\nimport argparse\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport nni\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initializing and building basic network for mnist.\n    \'\'\'\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 conv_size,\n                 hidden_size,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = conv_size\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 conv_size=params[\'conv_size\'],\n                                 hidden_size=params[\'hidden_size\'],\n                                 pool_size=params[\'pool_size\'],\n                                 learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(params[\'batch_num\']):\n            batch = mnist.train.next_batch(params[\'batch_size\'])\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: 1 - params[\'dropout_rate\']}\n                                        )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                nni.report_intermediate_result(test_acc)\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        nni.report_final_result(test_acc)\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\ndef get_params():\n    \'\'\' Get parameters from command line \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/tmp/tensorflow/mnist/input_data\', help=""data directory"")\n    parser.add_argument(""--dropout_rate"", type=float, default=0.5, help=""dropout rate"")\n    parser.add_argument(""--channel_1_num"", type=int, default=32)\n    parser.add_argument(""--channel_2_num"", type=int, default=64)\n    parser.add_argument(""--conv_size"", type=int, default=5)\n    parser.add_argument(""--pool_size"", type=int, default=2)\n    parser.add_argument(""--hidden_size"", type=int, default=1024)\n    parser.add_argument(""--learning_rate"", type=float, default=1e-4)\n    parser.add_argument(""--batch_num"", type=int, default=2000)\n    parser.add_argument(""--batch_size"", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args\n\nif __name__ == \'__main__\':\n    try:\n        # get parameters form tuner\n        tuner_params = nni.get_next_parameter()\n        logger.debug(tuner_params)\n        params = vars(get_params())\n        params.update(tuner_params)\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-tfv1/mnist_before.py,0,"b'""""""A deep MNIST classifier using convolutional layers.""""""\nimport argparse\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initializing and building basic network for mnist.\n    \'\'\'\n\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 conv_size,\n                 hidden_size,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = conv_size\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(\n            tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(\n            tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 conv_size=params[\'conv_size\'],\n                                 hidden_size=params[\'hidden_size\'],\n                                 pool_size=params[\'pool_size\'],\n                                 learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        for i in range(params[\'batch_num\']):\n            batch = mnist.train.next_batch(params[\'batch_size\'])\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: 1 - params[\'dropout_rate\']}\n                                         )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\ndef get_params():\n    \'\'\' Get parameters from command line \'\'\'\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--data_dir"", type=str, default=\'/tmp/tensorflow/mnist/input_data\', help=""data directory"")\n    parser.add_argument(""--dropout_rate"", type=float, default=0.5, help=""dropout rate"")\n    parser.add_argument(""--channel_1_num"", type=int, default=32)\n    parser.add_argument(""--channel_2_num"", type=int, default=64)\n    parser.add_argument(""--conv_size"", type=int, default=5)\n    parser.add_argument(""--pool_size"", type=int, default=2)\n    parser.add_argument(""--hidden_size"", type=int, default=1024)\n    parser.add_argument(""--learning_rate"", type=float, default=1e-4)\n    parser.add_argument(""--batch_num"", type=int, default=2000)\n    parser.add_argument(""--batch_size"", type=int, default=32)\n\n    args, _ = parser.parse_known_args()\n    return args\n\nif __name__ == \'__main__\':\n    try:\n        params = vars(get_params())\n        main(params)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/mnist-tfv2/mnist.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nNNI example trial code.\n\n- Experiment type: Hyper-parameter Optimization\n- Trial framework: Tensorflow v2.x (Keras API)\n- Model: LeNet-5\n- Dataset: MNIST\n""""""\n\nimport logging\n\nimport tensorflow as tf\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.layers import (Conv2D, Dense, Dropout, Flatten, MaxPool2D)\nfrom tensorflow.keras.optimizers import Adam\n\nimport nni\n\n_logger = logging.getLogger(\'mnist_example\')\n_logger.setLevel(logging.INFO)\n\n\nclass MnistModel(Model):\n    """"""\n    LeNet-5 Model with customizable hyper-parameters\n    """"""\n    def __init__(self, conv_size, hidden_size, dropout_rate):\n        """"""\n        Initialize hyper-parameters.\n\n        Parameters\n        ----------\n        conv_size : int\n            Kernel size of convolutional layers.\n        hidden_size : int\n            Dimensionality of last hidden layer.\n        dropout_rate : float\n            Dropout rate between two fully connected (dense) layers, to prevent co-adaptation.\n        """"""\n        super().__init__()\n        self.conv1 = Conv2D(filters=32, kernel_size=conv_size, activation=\'relu\')\n        self.pool1 = MaxPool2D(pool_size=2)\n        self.conv2 = Conv2D(filters=64, kernel_size=conv_size, activation=\'relu\')\n        self.pool2 = MaxPool2D(pool_size=2)\n        self.flatten = Flatten()\n        self.fc1 = Dense(units=hidden_size, activation=\'relu\')\n        self.dropout = Dropout(rate=dropout_rate)\n        self.fc2 = Dense(units=10, activation=\'softmax\')\n\n    def call(self, x):\n        """"""Override ``Model.call`` to build LeNet-5 model.""""""\n        x = self.conv1(x)\n        x = self.pool1(x)\n        x = self.conv2(x)\n        x = self.pool2(x)\n        x = self.flatten(x)\n        x = self.fc1(x)\n        x = self.dropout(x)\n        return self.fc2(x)\n\n\nclass ReportIntermediates(Callback):\n    """"""\n    Callback class for reporting intermediate accuracy metrics.\n\n    This callback sends accuracy to NNI framework every 100 steps,\n    so you can view the learning curve on web UI.\n\n    If an assessor is configured in experiment\'s YAML file,\n    it will use these metrics for early stopping.\n    """"""\n    def on_epoch_end(self, epoch, logs=None):\n        """"""Reports intermediate accuracy to NNI framework""""""\n        # TensorFlow 2.0 API reference claims the key is `val_acc`, but in fact it\'s `val_accuracy`\n        if \'val_acc\' in logs:\n            nni.report_intermediate_result(logs[\'val_acc\'])\n        else:\n            nni.report_intermediate_result(logs[\'val_accuracy\'])\n\n\ndef load_dataset():\n    """"""Download and reformat MNIST dataset""""""\n    mnist = tf.keras.datasets.mnist\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n    x_train, x_test = x_train / 255.0, x_test / 255.0\n    x_train = x_train[..., tf.newaxis]\n    x_test = x_test[..., tf.newaxis]\n    return (x_train, y_train), (x_test, y_test)\n\n\ndef main(params):\n    """"""\n    Main program:\n      - Build network\n      - Prepare dataset\n      - Train the model\n      - Report accuracy to tuner\n    """"""\n    model = MnistModel(\n        conv_size=params[\'conv_size\'],\n        hidden_size=params[\'hidden_size\'],\n        dropout_rate=params[\'dropout_rate\']\n    )\n    optimizer = Adam(learning_rate=params[\'learning_rate\'])\n    model.compile(optimizer=optimizer, loss=\'sparse_categorical_crossentropy\', metrics=[\'accuracy\'])\n    _logger.info(\'Model built\')\n\n    (x_train, y_train), (x_test, y_test) = load_dataset()\n    _logger.info(\'Dataset loaded\')\n\n    model.fit(\n        x_train,\n        y_train,\n        batch_size=params[\'batch_size\'],\n        epochs=10,\n        verbose=0,\n        callbacks=[ReportIntermediates()],\n        validation_data=(x_test, y_test)\n    )\n    _logger.info(\'Training completed\')\n\n    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n    nni.report_final_result(accuracy)  # send final accuracy to NNI tuner and web UI\n    _logger.info(\'Final accuracy reported: %s\', accuracy)\n\n\nif __name__ == \'__main__\':\n    params = {\n        \'dropout_rate\': 0.5,\n        \'conv_size\': 5,\n        \'hidden_size\': 1024,\n        \'batch_size\': 32,\n        \'learning_rate\': 1e-4,\n    }\n\n    # fetch hyper-parameters from HPO tuner\n    # comment out following two lines to run the code without NNI framework\n    tuned_params = nni.get_next_parameter()\n    params.update(tuned_params)\n\n    _logger.info(\'Hyper-parameters: %s\', params)\n    main(params)\n'"
examples/tuners/ga_customer_tuner/__init__.py,0,b''
examples/tuners/ga_customer_tuner/customer_tuner.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom graph import *\n\nimport copy\nimport json\nimport logging\nimport random\nimport numpy as np\n\nfrom nni.tuner import Tuner\nfrom nni.utils import extract_scalar_reward\n\nlogger = logging.getLogger(\'ga_customer_tuner\')\n\n\n@unique\nclass OptimizeMode(Enum):\n    Minimize = \'minimize\'\n    Maximize = \'maximize\'\n\n\ndef init_population(population_size=32):\n    population = []\n    graph = Graph(4,\n                  input=[Layer(LayerType.input.value, output=[4, 5], size=\'x\'), Layer(LayerType.input.value, output=[4, 5], size=\'y\')],\n                  output=[Layer(LayerType.output.value, input=[4], size=\'x\'), Layer(LayerType.output.value, input=[5], size=\'y\')],\n                  hide=[Layer(LayerType.attention.value, input=[0, 1], output=[2]), Layer(LayerType.attention.value, input=[1, 0], output=[3])])\n    for _ in range(population_size):\n        g = copy.deepcopy(graph)\n        for _ in range(1):\n            g.mutation()\n        population.append(Individual(g, result=None))\n    return population\n\n\nclass Individual(object):\n    def __init__(self, config=None, info=None, result=None, save_dir=None):\n        self.config = config\n        self.result = result\n        self.info = info\n        self.restore_dir = None\n        self.save_dir = save_dir\n\n    def __str__(self):\n        return ""info: "" + str(self.info) + "", config :"" + str(self.config) + "", result: "" + str(self.result)\n\n    def mutation(self, config=None, info=None, save_dir=None):\n        self.result = None\n        if config is not None:\n            self.config = config\n        self.config.mutation()\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info\n\n\nclass CustomerTuner(Tuner):\n    def __init__(self, optimize_mode, population_size = 32):\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.population = init_population(population_size)\n\n        assert len(self.population) == population_size\n        logger.debug(\'init population done.\')\n        return\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""Returns a set of trial graph config, as a serializable object.\n        parameter_id : int\n        """"""\n        if len(self.population) <= 0:\n            logger.debug(""the len of poplution lower than zero."")\n            raise Exception(\'The population is empty\')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result == None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            temp = json.loads(graph_dumps(indiv.config))\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n            indiv = copy.deepcopy(self.population[0])\n            self.population.pop(1)\n            indiv.mutation()\n            graph = indiv.config\n            temp =  json.loads(graph_dumps(graph))\n        logger.debug(\'generate_parameter return value is:\')\n        logger.debug(temp)\n        return temp\n\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \'\'\'\n        Record an observation of the objective function\n        parameter_id : int\n        parameters : dict of parameters\n        value: final metrics of the trial, including reward\n        \'\'\'\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Minimize:\n            reward = -reward\n\n        logger.debug(\'receive trial result is:\\n\')\n        logger.debug(str(parameters))\n        logger.debug(str(reward))\n\n        indiv = Individual(graph_loads(parameters), result=reward)\n        self.population.append(indiv)\n        return\n\n    def update_search_space(self, data):\n        pass\n\nif __name__ ==\'__main__\':\n    tuner = CustomerTuner(OptimizeMode.Maximize)\n    config = tuner.generate_parameters(0)\n    with open(\'./data.json\', \'w\') as outfile:\n        json.dump(config, outfile)\n    tuner.receive_trial_result(0, config, 0.99)\n'"
examples/tuners/ga_customer_tuner/graph.py,0,"b'# -*- coding: utf-8 -*-\n\nimport copy\nimport json\nimport random\nfrom enum import Enum, unique\n\n@unique\nclass LayerType(Enum):\n    attention = 0\n    self_attention = 1\n    rnn = 2\n    input = 3\n    output = 4\n\nclass Layer(object):\n    def __init__(self, type, input=None, output=None, size=None):\n        self.input = input if input is not None else []\n        self.output = output if output is not None else []\n        self.type = type\n        self.is_delete = False\n        self.size = size\n        if type == LayerType.attention.value:\n            self.input_size = 2\n            self.output_size = 1\n        elif type == LayerType.rnn.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif type == LayerType.self_attention.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif type == LayerType.input.value:\n            self.input_size = 0\n            self.output_size = 1\n        elif type == LayerType.output.value:\n            self.input_size = 1\n            self.output_size = 0\n        else:\n            print(type)\n    def set_size(self, id, size):\n        if self.type == LayerType.attention.value:\n            if self.input[0] == id:\n                self.size = size\n        if self.type == LayerType.rnn.value:\n            self.size = size\n        if self.type == LayerType.self_attention.value:\n            self.size = size\n        if self.type == LayerType.output.value:\n            if self.size != size:\n                return False\n        return True\n\n    def clear_size(self):\n        if self.type == LayerType.attention.value or LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None\n\n    def __str__(self):\n        return \'input:\' + str(self.input) + \' output:\' + str(self.output) + \' type:\' + str(\n            self.type) + \' is_delete:\' + str(self.is_delete) + \' size:\' + str(self.size)\n\ndef graph_dumps(graph):\n    return json.dumps(graph, default=lambda obj: obj.__dict__)\n\ndef graph_loads(js):\n    layers = []\n    for layer in js[\'layers\']:\n        p = Layer(layer[\'type\'],layer[\'input\'],layer[\'output\'],layer[\'size\'])\n        p.is_delete = layer[\'is_delete\']\n        layers.append(p)\n    graph = Graph(js[\'max_layer_num\'],[], [], [])\n    graph.layers = layers\n    return graph\n\nclass Graph(object):\n    def __init__(self, max_layer_num, input, output, hide):\n        self.layers = []\n        self.max_layer_num = max_layer_num\n\n        for layer in input:\n            self.layers.append(layer)\n        for layer in output:\n            self.layers.append(layer)\n        if hide is not None:\n            for layer in hide:\n                self.layers.append(layer)\n        assert self.is_legal()\n\n    def is_topology(self, layers=None):\n        if layers == None:\n            layers = self.layers\n        layers_nodle = []\n        xx = []\n        for i in range(len(layers)):\n            if layers[i].is_delete == False:\n                layers_nodle.append(i)\n        while True:\n            flag_break = True\n            layers_toremove = []\n            for layer1 in layers_nodle:\n                flag_arrive = True\n                for layer2 in layers[layer1].input:\n                    if layer2 in layers_nodle:\n                        flag_arrive = False\n                if flag_arrive == True:\n                    for layer2 in layers[layer1].output:\n                        if layers[layer2].set_size(layer1, layers[layer1].size) == False:  # Size is error\n                            return False\n                    layers_toremove.append(layer1)\n                    xx.append(layer1)\n                    flag_break = False\n            for layer in layers_toremove:\n                layers_nodle.remove(layer)\n            xx.append(\'|\')\n            if flag_break == True:\n                break\n        if len(layers_nodle) > 0:  # There is loop in graph || some layers can\'t to arrive\n            return False\n        return xx\n\n    def layer_num(self, layers=None):\n        if layers == None:\n            layers = self.layers\n        layer_num = 0\n        for layer in layers:\n            if layer.is_delete == False and layer.type != LayerType.input.value and layer.type != LayerType.output.value:\n                layer_num += 1\n        return layer_num\n\n    def is_legal(self, layers=None):\n        if layers == None:\n            layers = self.layers\n\n        for layer in layers:\n            if layer.is_delete == False:\n                if len(layer.input) != layer.input_size:\n                    return False\n                if len(layer.output) < layer.output_size:\n                    return False\n\n        # layer_num <= max_layer_num\n        if self.layer_num(layers) > self.max_layer_num:\n            return False\n\n        if self.is_topology(layers) == False:  # There is loop in graph || some layers can\'t to arrive\n            return False\n\n        return True\n\n    def mutation(self, only_add=False):\n        types = []\n        if self.layer_num() < self.max_layer_num:\n            types.append(0)\n            types.append(1)\n        if self.layer_num() > 0:\n            types.append(2)\n            types.append(3)\n        # 0 : add a layer , delete a edge\n        # 1 : add a layer , change a edge\n        # 2 : delete a layer, delete a edge\n        # 3 : delete a layer, change a edge\n        type = random.choice(types)\n        layer_type = random.choice([LayerType.attention.value, LayerType.self_attention.value, LayerType.rnn.value])\n        layers = copy.deepcopy(self.layers)\n        cnt_try = 0\n        while True:\n            layers_in = []\n            layers_out = []\n            layers_del = []\n            for layer1 in range(len(layers)):\n                layer = layers[layer1]\n                if layer.is_delete == False:\n                    if layer.type != LayerType.output.value:\n                        layers_in.append(layer1)\n                    if layer.type != LayerType.input.value:\n                        layers_out.append(layer1)\n                    if layer.type != LayerType.output.value and layer.type != LayerType.input.value:\n                        layers_del.append(layer1)\n            if type <= 1:\n                new_id = len(layers)\n                out = random.choice(layers_out)\n                input = []\n                output = [out]\n                pos = random.randint(0, len(layers[out].input) - 1)\n                last_in = layers[out].input[pos]\n                layers[out].input[pos] = new_id\n                if type == 0:\n                    layers[last_in].output.remove(out)\n                if type == 1:\n                    layers[last_in].output.remove(out)\n                    layers[last_in].output.append(new_id)\n                    input = [last_in]\n                lay = Layer(type=layer_type, input=input, output=output)\n                while len(input) < lay.input_size:\n                    layer1 = random.choice(layers_in)\n                    input.append(layer1)\n                    layers[layer1].output.append(new_id)\n                lay.input = input\n                layers.append(lay)\n            else:\n                layer1 = random.choice(layers_del)\n                for layer2 in layers[layer1].output:\n                    layers[layer2].input.remove(layer1)\n                    if type == 2:\n                        v2 = random.choice(layers_in)\n                    else:\n                        v2 = random.choice(layers[layer1].input)\n                    layers[layer2].input.append(v2)\n                    layers[v2].output.append(layer2)\n                for layer2 in layers[layer1].input:\n                    layers[layer2].output.remove(layer1)\n                layers[layer1].is_delete = True\n\n            if self.is_legal(layers):\n                self.layers = layers\n                break\n            else:\n                layers = copy.deepcopy(self.layers)\n                cnt_try += 1\n\n    def __str__(self):\n        info = """"\n        for id, layer in enumerate(self.layers):\n            if layer.is_delete == False:\n                info += \'id:%d \' % id + str(layer) + \'\\n\'\n        return info\n\nif __name__ == \'__main__\':\n    graph = Graph(10,\n                  input=[Layer(LayerType.input.value, output=[4, 5], size=\'x\'), Layer(LayerType.input.value, output=[4, 5], size=\'y\')],\n                  output=[Layer(LayerType.output.value, input=[4], size=\'x\'), Layer(LayerType.output.value, input=[5], size=\'y\')],\n                  hide=[Layer(LayerType.attention.value, input=[0, 1], output=[2]), Layer(LayerType.attention.value, input=[1, 0], output=[3])])\n\n    s = graph_dumps(graph)\n    g = graph_loads(json.loads(s))\n    print(g)\n    print(s)\n\n    s = \'\'\'{""count"":2,""array"":[{""input"":%s,""output"":{""output"":0.7}}]}\'\'\'%s\n    print(len(s))\n    print(s)'"
examples/tuners/mnist_keras_customized_advisor/dummy_advisor.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nfrom collections import defaultdict\n\nimport json_tricks\nimport numpy as np\nfrom nni import parameter_expressions as param\nfrom nni.msg_dispatcher_base import MsgDispatcherBase\nfrom nni.protocol import CommandType, send\nfrom nni.utils import MetricType\n\nlogger = logging.getLogger(\'customized_advisor\')\n\n\nclass DummyAdvisor(MsgDispatcherBase):\n    """"""WARNING: Advisor API is subject to change in future releases.\n\n    This advisor creates a new trial when validation accuracy of any one of the trials just dropped.\n    The trial is killed if the validation accuracy doesn\'t improve for at least k last-reported metrics.\n    To demonstrate the high flexibility of writing advisors, we don\'t use tuners or the standard definition of\n    search space. This is just a demo to customize an advisor. It\'s not intended to make any sense.\n    """"""\n    def __init__(self, k=3):\n        super(DummyAdvisor, self).__init__()\n        self.k = k\n        self.random_state = np.random.RandomState()\n\n    def handle_initialize(self, data):\n        logger.info(""Advisor initialized: {}"".format(data))\n        self.handle_update_search_space(data)\n        self.parameters_count = 0\n        self.parameter_best_metric = defaultdict(float)\n        self.parameter_cooldown = defaultdict(int)\n        send(CommandType.Initialized, \'\')\n\n    def _send_new_trial(self):\n        self.parameters_count += 1\n        new_trial = {\n            ""parameter_id"": self.parameters_count,\n            ""parameters"": {\n                ""optimizer"": param.choice(self.searchspace_json[""optimizer""], self.random_state),\n                ""learning_rate"": param.loguniform(self.searchspace_json[""learning_rate""][0],\n                                                  self.searchspace_json[""learning_rate""][1],\n                                                  self.random_state)\n            },\n            ""parameter_source"": ""algorithm""\n        }\n        logger.info(""New trial sent: {}"".format(new_trial))\n        send(CommandType.NewTrialJob, json_tricks.dumps(new_trial))\n\n    def handle_request_trial_jobs(self, data):\n        logger.info(""Request trial jobs: {}"".format(data))\n        for _ in range(data):\n            self._send_new_trial()\n\n    def handle_update_search_space(self, data):\n        logger.info(""Search space update: {}"".format(data))\n        self.searchspace_json = data\n\n    def handle_trial_end(self, data):\n        logger.info(""Trial end: {}"".format(data)) # do nothing\n\n    def handle_report_metric_data(self, data):\n        logger.info(""Metric reported: {}"".format(data))\n        if data[\'type\'] == MetricType.REQUEST_PARAMETER:\n            raise ValueError(""Request parameter not supported"")\n        elif data[""type""] == MetricType.PERIODICAL:\n            parameter_id = data[""parameter_id""]\n            if data[""value""] > self.parameter_best_metric[parameter_id]:\n                self.parameter_best_metric[parameter_id] = data[""value""]\n                self.parameter_cooldown[parameter_id] = 0\n            else:\n                self.parameter_cooldown[parameter_id] += 1\n                logger.info(""Accuracy dropped, cooldown {}, sending a new trial"".format(\n                    self.parameter_cooldown[parameter_id]))\n                self._send_new_trial()\n                if self.parameter_cooldown[parameter_id] >= self.k:\n                    logger.info(""Send kill signal to {}"".format(data))\n                    send(CommandType.KillTrialJob, json_tricks.dumps(data[""trial_job_id""]))\n'"
examples/tuners/mnist_keras_customized_advisor/mnist_keras.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\n\nimport os\nimport keras\nimport numpy as np\nfrom keras import backend as K\nfrom keras.callbacks import TensorBoard\nfrom keras.datasets import mnist\nfrom keras.layers import Conv2D, Dense, Flatten, MaxPooling2D\nfrom keras.models import Sequential\n\nimport nni\n\nLOG = logging.getLogger(\'mnist_keras\')\nK.set_image_data_format(\'channels_last\')\nTENSORBOARD_DIR = os.environ[\'NNI_OUTPUT_DIR\']\n\nH, W = 28, 28\nNUM_CLASSES = 10\n\n\ndef create_mnist_model(hyper_params, input_shape=(H, W, 1), num_classes=NUM_CLASSES):\n    """"""\n    Create simple convolutional model\n    """"""\n    layers = [\n        Conv2D(32, kernel_size=(3, 3), activation=\'relu\', input_shape=input_shape),\n        Conv2D(64, (3, 3), activation=\'relu\'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Flatten(),\n        Dense(100, activation=\'relu\'),\n        Dense(num_classes, activation=\'softmax\')\n    ]\n\n    model = Sequential(layers)\n\n    if hyper_params[\'optimizer\'] == \'Adam\':\n        optimizer = keras.optimizers.Adam(lr=hyper_params[\'learning_rate\'])\n    else:\n        optimizer = keras.optimizers.SGD(lr=hyper_params[\'learning_rate\'], momentum=0.9)\n    model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=[\'accuracy\'])\n\n    return model\n\n\ndef load_mnist_data(args):\n    """"""\n    Load MNIST dataset\n    """"""\n    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n\n    x_train = (np.expand_dims(x_train, -1).astype(np.float) / 255.)[:args.num_train]\n    x_test = (np.expand_dims(x_test, -1).astype(np.float) / 255.)[:args.num_test]\n    y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)[:args.num_train]\n    y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)[:args.num_test]\n\n    LOG.debug(\'x_train shape: %s\', (x_train.shape,))\n    LOG.debug(\'x_test shape: %s\', (x_test.shape,))\n\n    return x_train, y_train, x_test, y_test\n\n\nclass SendMetrics(keras.callbacks.Callback):\n    """"""\n    Keras callback to send metrics to NNI framework\n    """"""\n\n    def on_epoch_end(self, epoch, logs={}):\n        """"""\n        Run on end of each epoch\n        """"""\n        LOG.debug(logs)\n        # Should this be val_acc or val_accuracy? Seems inconsistent behavior of Keras?\n        nni.report_intermediate_result(logs[""val_accuracy""])\n\n\ndef train(args, params):\n    """"""\n    Train model\n    """"""\n    x_train, y_train, x_test, y_test = load_mnist_data(args)\n    model = create_mnist_model(params)\n\n    model.fit(x_train, y_train, batch_size=args.batch_size, epochs=args.epochs, verbose=1,\n              validation_data=(x_test, y_test), callbacks=[SendMetrics(), TensorBoard(log_dir=TENSORBOARD_DIR)])\n\n    _, acc = model.evaluate(x_test, y_test, verbose=0)\n    LOG.debug(\'Final result is: %d\', acc)\n    nni.report_final_result(acc)\n\n\ndef generate_default_params():\n    """"""\n    Generate default hyper parameters\n    """"""\n    return {\n        \'optimizer\': \'Adam\',\n        \'learning_rate\': 0.001\n    }\n\n\nif __name__ == \'__main__\':\n    PARSER = argparse.ArgumentParser()\n    PARSER.add_argument(""--batch_size"", type=int, default=200, help=""batch size"", required=False)\n    PARSER.add_argument(""--epochs"", type=int, default=10, help=""Train epochs"", required=False)\n    PARSER.add_argument(""--num_train"", type=int, default=60000,\n                        help=""Number of train samples to be used, maximum 60000"", required=False)\n    PARSER.add_argument(""--num_test"", type=int, default=10000, help=""Number of test samples to be used, maximum 10000"",\n                        required=False)\n\n    ARGS, UNKNOWN = PARSER.parse_known_args()\n\n    # get parameters from tuner\n    RECEIVED_PARAMS = nni.get_next_parameter()\n    LOG.debug(RECEIVED_PARAMS)\n    PARAMS = generate_default_params()\n    PARAMS.update(RECEIVED_PARAMS)\n    # train\n    train(ARGS, PARAMS)\n'"
examples/tuners/random_nas_tuner/random_nas_tuner.py,0,"b'import numpy as np\n\nfrom nni.tuner import Tuner\n\n\ndef random_archi_generator(nas_ss, random_state):\n    \'\'\'random\n    \'\'\'\n    chosen_arch = {}\n    for key, val in nas_ss.items():\n        assert val[\'_type\'] in [\'layer_choice\', \'input_choice\'], \\\n            ""Random NAS Tuner only receives NAS search space whose _type is \'layer_choice\' or \'input_choice\'""\n        if val[\'_type\'] == \'layer_choice\':\n            choices = val[\'_value\']\n            index = random_state.randint(len(choices))\n            chosen_arch[key] = {\'_value\': choices[index], \'_idx\': index}\n        elif val[\'_type\'] == \'input_choice\':\n            choices = val[\'_value\'][\'candidates\']\n            n_chosen = val[\'_value\'][\'n_chosen\']\n            chosen = []\n            idxs = []\n            for _ in range(n_chosen):\n                index = random_state.randint(len(choices))\n                chosen.append(choices[index])\n                idxs.append(index)\n            chosen_arch[key] = {\'_value\': chosen, \'_idx\': idxs}\n        else:\n            raise ValueError(\'Unknown key %s and value %s\' % (key, val))\n    return chosen_arch\n\n\nclass RandomNASTuner(Tuner):\n    \'\'\'RandomNASTuner\n    \'\'\'\n\n    def __init__(self):\n        self.searchspace_json = None\n        self.random_state = None\n\n    def update_search_space(self, search_space):\n        \'\'\'update\n        \'\'\'\n        self.searchspace_json = search_space\n        self.random_state = np.random.RandomState()\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        \'\'\'generate\n        \'\'\'\n        return random_archi_generator(self.searchspace_json, self.random_state)\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \'\'\'receive\n        \'\'\'\n        pass\n'"
src/sdk/pycli/setup.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport setuptools\n\nsetuptools.setup(\n    name='nnicli',\n    version='999.0.0-developing',\n    packages=setuptools.find_packages(),\n\n    python_requires='>=3.5',\n    install_requires=[\n        'requests'\n    ],\n\n    author='Microsoft NNI Team',\n    author_email='nni@microsoft.com',\n    description='nnicli for Neural Network Intelligence project',\n    license='MIT',\n    url='https://github.com/Microsoft/nni',\n)\n"""
src/sdk/pynni/setup.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport setuptools\n\ndef read(fname):\n    return open(os.path.join(os.path.dirname(__file__), fname), encoding='utf-8').read()\n\nsetuptools.setup(\n    name = 'nni-sdk',\n    version = '999.0.0-developing',\n    packages = setuptools.find_packages(exclude=['tests']),\n\n    python_requires = '>=3.5',\n    install_requires = [\n        'hyperopt==0.1.2',\n        'json_tricks',\n        'numpy',\n        'scipy',\n        'coverage'\n    ],\n    package_data = {'nni': ['**/requirements.txt']},\n\n    test_suite = 'tests',\n\n    author = 'Microsoft NNI Team',\n    author_email = 'nni@microsoft.com',\n    description = 'Python SDK for Neural Network Intelligence project',\n    license = 'MIT',\n    url = 'https://github.com/Microsoft/nni',\n\n    long_description = read('README.md')\n)\n"""
test/config/metrics_test/trial.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport time\nimport json\nimport argparse\nimport nni\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--dict_metrics"", action=\'store_true\')\n    args = parser.parse_args()\n\n    if args.dict_metrics:\n        result_file = \'expected_metrics_dict.json\'\n    else:\n        result_file = \'expected_metrics.json\'\n\n    nni.get_next_parameter()\n    with open(result_file, \'r\') as f:\n        m = json.load(f)\n    for v in m[\'intermediate_result\']:\n        time.sleep(1)\n        print(\'report_intermediate_result:\', v)\n        nni.report_intermediate_result(v)\n    time.sleep(1)\n    print(\'report_final_result:\', m[\'final_result\'])\n    nni.report_final_result(m[\'final_result\'])\n    print(\'done\')\n'"
test/config/multi_phase/multi_phase.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport time\nimport nni\n\nif __name__ == '__main__':\n    for i in range(5):\n        hyper_params = nni.get_next_parameter()\n        print('hyper_params:[{}]'.format(hyper_params))\n        if hyper_params is None:\n            break\n        nni.report_final_result(0.1*i)\n        time.sleep(3)\n"""
test/config/multi_thread/multi_thread_trial.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport nni\nimport time\n\nif __name__ == '__main__':\n    nni.get_next_parameter()\n    time.sleep(3)\n    nni.report_final_result(0.5)\n"""
test/config/multi_thread/multi_thread_tuner.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport time\nfrom nni.tuner import Tuner\n\n\nclass MultiThreadTuner(Tuner):\n    def __init__(self):\n        self.parent_done = False\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        logging.debug('generate_parameters: %s %s', parameter_id, kwargs)\n        if parameter_id == 0:\n            return {'x': 0}\n        else:\n            while not self.parent_done:\n                logging.debug('parameter_id %s sleeping', parameter_id)\n                time.sleep(2)\n            logging.debug('parameter_id %s waked up', parameter_id)\n            return {'x': 1}\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        logging.debug('receive_trial_result: %s %s %s %s', parameter_id, parameters, value, kwargs)\n        if parameter_id == 0:\n            self.parent_done = True\n\n    def update_search_space(self, search_space):\n        pass\n"""
test/config/naive_test/naive_assessor.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport os\n\nfrom nni.assessor import Assessor, AssessResult\n\n_logger = logging.getLogger('NaiveAssessor')\n_logger.info('start')\n\n_pwd = os.path.dirname(__file__)\n_result = open(os.path.join(_pwd, 'assessor_result.txt'), 'w')\n\nclass NaiveAssessor(Assessor):\n    def __init__(self, optimize_mode):\n        self._killed = set()\n        _logger.info('init')\n\n    def assess_trial(self, trial_job_id, trial_history):\n        _logger.info('assess trial %s %s' % (trial_job_id, trial_history))\n\n        id_ = trial_history[0]\n        if id_ in self._killed:\n            return AssessResult.Bad\n\n        s = 0\n        for i, val in enumerate(trial_history):\n            s += val\n            if s % 11 == 1:\n                self._killed.add(id_)\n                _result.write('%d %d\\n' % (id_, i + 1))\n                _result.flush()\n                return AssessResult.Bad\n\n        return AssessResult.Good\n\n    def _on_exit(self):\n        _result.close()\n\n    def _on_error(self):\n        _result.write('ERROR\\n')\n        _result.close()\n"""
test/config/naive_test/naive_trial.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport time\n\nimport nni\n\nparams = nni.get_next_parameter()\nprint('params:', params)\nx = params['x']\n\ntime.sleep(1)\nfor i in range(1, 10):\n    nni.report_intermediate_result(x ** i)\n    time.sleep(0.5)\n\nnni.report_final_result(x ** 10)\n"""
test/config/naive_test/naive_tuner.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\n\nfrom nni.tuner import Tuner\nfrom nni.utils import extract_scalar_reward\n\n_logger = logging.getLogger('NaiveTuner')\n_logger.info('start')\n\n_pwd = os.path.dirname(__file__)\n_result = open(os.path.join(_pwd, 'tuner_result.txt'), 'w')\n\nclass NaiveTuner(Tuner):\n    def __init__(self, optimize_mode):\n        self.cur = 0\n        _logger.info('init')\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        self.cur += 1\n        _logger.info('generate parameters: %s' % self.cur)\n        return { 'x': self.cur }\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        reward = extract_scalar_reward(value)\n        _logger.info('receive trial result: %s, %s, %s' % (parameter_id, parameters, reward))\n        _result.write('%d %d\\n' % (parameters['x'], reward))\n        _result.flush()\n\n    def update_search_space(self, search_space):\n        _logger.info('update_search_space: %s' % search_space)\n        with open(os.path.join(_pwd, 'tuner_search_space.json'), 'w') as file_:\n            json.dump(search_space, file_)\n\n    def _on_exit(self):\n        _result.close()\n\n    def _on_error(self):\n        _result.write('ERROR\\n')\n        _result.close()\n"""
test/config/naive_trial/naive_trial.py,0,"b""import time\nimport nni\n\nif __name__ == '__main__':\n    print('trial start')\n    params = nni.get_next_parameter()\n    print('params:', params)\n    epochs = 2\n\n    for i in range(epochs):\n        nni.report_intermediate_result(0.1 * (i+1))\n        time.sleep(1)\n    nni.report_final_result(0.8)\n    print('trial done')\n"""
test/config/naive_trial/trial.py,0,"b""import random\nimport time\nimport math\nimport nni\n\ncurve_func = {\n    0: lambda x: x,\n    1: lambda x: x * x,\n    2: lambda x: math.pow(x, 0.5),\n    3: lambda x: math.tanh(x)\n}\n\nif __name__ == '__main__':\n    print('trial start')\n\n    params = nni.get_next_parameter()\n    print('params:', params)\n    epochs = 20\n\n    for i in range(epochs):\n        v = curve_func[params['k']](i / epochs)\n        v += v * (random.random() * params['n'])\n        v *= params['d']\n        nni.report_intermediate_result(v)\n\n        if i % 5 == 0:\n            time.sleep(1)\n    nni.report_final_result(v)\n    print('trial done')\n"""
test/config/naive_trial/trial_choices.py,0,"b""import random\nimport nni\n\nif __name__ == '__main__':\n    print('trial start')\n\n    params = nni.get_next_parameter()\n    print('params:', params)\n\n    nni.report_intermediate_result(random.random())\n    nni.report_final_result(random.random())\n\n    print('trial done')\n"""
test/nni_test/nnitest/__init__.py,0,b''
test/nni_test/nnitest/foreground.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport subprocess\nimport argparse\nimport time\nimport shlex\nimport signal\n\ndef test_foreground(args):\n    launch_command = \'nnictl create --config {} --foreground\'.format(args.config)\n    print(\'nnictl foreground launch command: \', launch_command, flush=True)\n\n    proc = subprocess.Popen(shlex.split(launch_command))\n\n    time.sleep(args.timeout)\n    proc.send_signal(signal.SIGINT)\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--config"", type=str, required=True)\n    parser.add_argument(""--timeout"", type=int, default=45)\n    args = parser.parse_args()\n\n    test_foreground(args)\n'"
test/nni_test/nnitest/generate_ts_config.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport sys\nimport os\nimport glob\nimport argparse\nfrom utils import get_yml_content, dump_yml_content\n\nTRAINING_SERVICE_FILE = os.path.join(\'config\', \'training_service.yml\')\n\ndef update_training_service_config(args):\n    config = get_yml_content(TRAINING_SERVICE_FILE)\n    if args.nni_manager_ip is not None:\n        config[args.ts][\'nniManagerIp\'] = args.nni_manager_ip\n    if args.ts == \'paiYarn\':\n        if args.pai_user is not None:\n            config[args.ts][\'paiYarnConfig\'][\'userName\'] = args.pai_user\n        if args.pai_pwd is not None:\n            config[args.ts][\'paiYarnConfig\'][\'passWord\'] = args.pai_pwd\n        if args.pai_host is not None:\n            config[args.ts][\'paiYarnConfig\'][\'host\'] = args.pai_host\n        if args.nni_docker_image is not None:\n            config[args.ts][\'trial\'][\'image\'] = args.nni_docker_image\n        if args.data_dir is not None:\n            config[args.ts][\'trial\'][\'dataDir\'] = args.data_dir\n        if args.output_dir is not None:\n            config[args.ts][\'trial\'][\'outputDir\'] = args.output_dir\n        if args.vc is not None:\n            config[args.ts][\'trial\'][\'virtualCluster\'] = args.vc\n    if args.ts == \'pai\':\n        if args.pai_user is not None:\n            config[args.ts][\'paiConfig\'][\'userName\'] = args.pai_user\n        if args.pai_host is not None:\n            config[args.ts][\'paiConfig\'][\'host\'] = args.pai_host\n        if args.pai_token is not None:\n            config[args.ts][\'paiConfig\'][\'token\'] = args.pai_token\n        if args.nni_docker_image is not None:\n            config[args.ts][\'trial\'][\'image\'] = args.nni_docker_image\n        if args.nni_manager_nfs_mount_path is not None:\n            config[args.ts][\'trial\'][\'nniManagerNFSMountPath\'] = args.nni_manager_nfs_mount_path\n        if args.container_nfs_mount_path is not None:\n            config[args.ts][\'trial\'][\'containerNFSMountPath\'] = args.container_nfs_mount_path\n        if args.pai_storage_plugin is not None:\n            config[args.ts][\'trial\'][\'paiStoragePlugin\'] = args.pai_storage_plugin\n        if args.vc is not None:\n            config[args.ts][\'trial\'][\'virtualCluster\'] = args.vc\n    elif args.ts == \'kubeflow\':\n        if args.nfs_server is not None:\n            config[args.ts][\'kubeflowConfig\'][\'nfs\'][\'server\'] = args.nfs_server\n        if args.nfs_path is not None:\n            config[args.ts][\'kubeflowConfig\'][\'nfs\'][\'path\'] = args.nfs_path\n        if args.keyvault_vaultname is not None:\n            config[args.ts][\'kubeflowConfig\'][\'keyVault\'][\'vaultName\'] = args.keyvault_vaultname\n        if args.keyvault_name is not None:\n            config[args.ts][\'kubeflowConfig\'][\'keyVault\'][\'name\'] = args.keyvault_name\n        if args.azs_account is not None:\n            config[args.ts][\'kubeflowConfig\'][\'azureStorage\'][\'accountName\'] = args.azs_account\n        if args.azs_share is not None:\n            config[args.ts][\'kubeflowConfig\'][\'azureStorage\'][\'azureShare\'] = args.azs_share\n        if args.nni_docker_image is not None:\n            config[args.ts][\'trial\'][\'worker\'][\'image\'] = args.nni_docker_image\n    elif args.ts == \'frameworkcontroller\':\n        if args.nfs_server is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'nfs\'][\'server\'] = args.nfs_server\n        if args.nfs_path is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'nfs\'][\'path\'] = args.nfs_path\n        if args.keyvault_vaultname is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'keyVault\'][\'vaultName\'] = args.keyvault_vaultname\n        if args.keyvault_name is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'keyVault\'][\'name\'] = args.keyvault_name\n        if args.azs_account is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'azureStorage\'][\'accountName\'] = args.azs_account\n        if args.azs_share is not None:\n            config[args.ts][\'frameworkcontrollerConfig\'][\'azureStorage\'][\'azureShare\'] = args.azs_share\n        if args.nni_docker_image is not None:\n            config[args.ts][\'trial\'][\'taskRoles\'][0][\'image\'] = args.nni_docker_image\n    elif args.ts == \'remote\':\n        if args.remote_user is not None:\n            config[args.ts][\'machineList\'][0][\'username\'] = args.remote_user\n        if args.remote_host is not None:\n            config[args.ts][\'machineList\'][0][\'ip\'] = args.remote_host\n        if args.remote_port is not None:\n            config[args.ts][\'machineList\'][0][\'port\'] = args.remote_port\n        if args.remote_pwd is not None:\n            config[args.ts][\'machineList\'][0][\'passwd\'] = args.remote_pwd\n\n    dump_yml_content(TRAINING_SERVICE_FILE, config)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--ts"", type=str, choices=[\'pai\', \'kubeflow\', \'remote\', \'local\', \'frameworkcontroller\'], default=\'pai\')\n    parser.add_argument(""--nni_docker_image"", type=str)\n    parser.add_argument(""--nni_manager_ip"", type=str)\n    # args for PAI\n    parser.add_argument(""--pai_user"", type=str)\n    parser.add_argument(""--pai_pwd"", type=str)\n    parser.add_argument(""--pai_host"", type=str)\n    parser.add_argument(""--data_dir"", type=str)\n    parser.add_argument(""--output_dir"", type=str)\n    parser.add_argument(""--vc"", type=str)\n    parser.add_argument(""--pai_token"", type=str)\n    parser.add_argument(""--pai_storage_plugin"", type=str)\n    parser.add_argument(""--nni_manager_nfs_mount_path"", type=str)\n    parser.add_argument(""--container_nfs_mount_path"", type=str)\n    # args for kubeflow and frameworkController\n    parser.add_argument(""--nfs_server"", type=str)\n    parser.add_argument(""--nfs_path"", type=str)\n    parser.add_argument(""--keyvault_vaultname"", type=str)\n    parser.add_argument(""--keyvault_name"", type=str)\n    parser.add_argument(""--azs_account"", type=str)\n    parser.add_argument(""--azs_share"", type=str)\n    # args for remote\n    parser.add_argument(""--remote_user"", type=str)\n    parser.add_argument(""--remote_pwd"", type=str)\n    parser.add_argument(""--remote_host"", type=str)\n    parser.add_argument(""--remote_port"", type=int)\n    args = parser.parse_args()\n\n    update_training_service_config(args)\n'"
test/nni_test/nnitest/naive_test.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport sys\nimport os.path as osp\nimport argparse\nimport json\nimport subprocess\nimport sys\nimport time\nimport traceback\n\nfrom utils import is_experiment_done, get_experiment_id, get_nni_log_path, read_last_line, remove_files, setup_experiment, detect_port, wait_for_port_available\nfrom utils import GREEN, RED, CLEAR, EXPERIMENT_URL\n\nNNI_SOURCE_DIR = \'..\'\nNAIVE_TEST_CONFIG_DIR = osp.join(NNI_SOURCE_DIR, \'test\', \'config\', \'naive_test\')\n\ndef naive_test(args):\n    \'\'\'run naive integration test\'\'\'\n    to_remove = [\'tuner_search_space.json\', \'tuner_result.txt\', \'assessor_result.txt\']\n    to_remove = list(map(lambda file: osp.join(NAIVE_TEST_CONFIG_DIR, file), to_remove))\n    remove_files(to_remove)\n\n    proc = subprocess.run([\'nnictl\', \'create\', \'--config\', args.config])\n    assert proc.returncode == 0, \'`nnictl create` failed with code %d\' % proc.returncode\n\n    print(\'Spawning trials...\')\n\n    nnimanager_log_path = get_nni_log_path(EXPERIMENT_URL)\n    current_trial = 0\n\n    for _ in range(120):\n        time.sleep(1)\n\n        tuner_status = read_last_line(osp.join(NAIVE_TEST_CONFIG_DIR, \'tuner_result.txt\'))\n        assessor_status = read_last_line(osp.join(NAIVE_TEST_CONFIG_DIR, \'assessor_result.txt\'))\n        experiment_status = is_experiment_done(nnimanager_log_path)\n\n        assert tuner_status != \'ERROR\', \'Tuner exited with error\'\n        assert assessor_status != \'ERROR\', \'Assessor exited with error\'\n\n        if experiment_status:\n            break\n\n        if tuner_status is not None:\n            for line in open(osp.join(NAIVE_TEST_CONFIG_DIR, \'tuner_result.txt\')):\n                if line.strip() == \'ERROR\':\n                    break\n                trial = int(line.split(\' \')[0])\n                if trial > current_trial:\n                    current_trial = trial\n                    print(\'Trial #%d done\' % trial)\n\n    assert experiment_status, \'Failed to finish in 2 min\'\n\n    ss1 = json.load(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'search_space.json\')))\n    ss2 = json.load(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'tuner_search_space.json\')))\n    assert ss1 == ss2, \'Tuner got wrong search space\'\n\n    tuner_result = set(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'tuner_result.txt\')))\n    expected = set(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'expected_tuner_result.txt\')))\n    # Trials may complete before NNI gets assessor\'s result,\n    # so it is possible to have more final result than expected\n    print(\'Tuner result:\', tuner_result)\n    print(\'Expected tuner result:\', expected)\n    assert tuner_result.issuperset(expected), \'Bad tuner result\'\n\n    assessor_result = set(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'assessor_result.txt\')))\n    expected = set(open(osp.join(NAIVE_TEST_CONFIG_DIR, \'expected_assessor_result.txt\')))\n    assert assessor_result == expected, \'Bad assessor result\'\n\n    subprocess.run([\'nnictl\', \'stop\'])\n    wait_for_port_available(8080, 10)\n\ndef stop_experiment_test(args):\n    config_file = args.config\n    \'\'\'Test `nnictl stop` command, including `nnictl stop exp_id` and `nnictl stop all`.\n    Simple `nnictl stop` is not tested here since it is used in all other test code\'\'\'\n    subprocess.run([\'nnictl\', \'create\', \'--config\', config_file, \'--port\', \'8080\'], check=True)\n    subprocess.run([\'nnictl\', \'create\', \'--config\', config_file, \'--port\', \'8888\'], check=True)\n    subprocess.run([\'nnictl\', \'create\', \'--config\', config_file, \'--port\', \'8989\'], check=True)\n    subprocess.run([\'nnictl\', \'create\', \'--config\', config_file, \'--port\', \'8990\'], check=True)\n\n    # test cmd \'nnictl stop id`\n    experiment_id = get_experiment_id(EXPERIMENT_URL)\n    proc = subprocess.run([\'nnictl\', \'stop\', experiment_id])\n    assert proc.returncode == 0, \'`nnictl stop %s` failed with code %d\' % (experiment_id, proc.returncode)\n    wait_for_port_available(8080, 10)\n    assert not detect_port(8080), \'`nnictl stop %s` failed to stop experiments\' % experiment_id\n\n    # test cmd `nnictl stop --port`\n    proc = subprocess.run([\'nnictl\', \'stop\', \'--port\', \'8990\'])\n    assert proc.returncode == 0, \'`nnictl stop %s` failed with code %d\' % (experiment_id, proc.returncode)\n    wait_for_port_available(8990, 10)\n    assert not detect_port(8990), \'`nnictl stop %s` failed to stop experiments\' % experiment_id\n\n    # test cmd `nnictl stop --all`\n    proc = subprocess.run([\'nnictl\', \'stop\', \'--all\'])\n    assert proc.returncode == 0, \'`nnictl stop --all` failed with code %d\' % proc.returncode\n    wait_for_port_available(8888, 10)\n    wait_for_port_available(8989, 10)\n    assert not detect_port(8888) and not detect_port(8989), \'`nnictl stop --all` failed to stop experiments\'\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--config"", type=str, required=True)\n    parser.add_argument(""--preinstall"", action=\'store_true\')\n    args = parser.parse_args()\n    setup_experiment(not args.preinstall)\n    try:\n        naive_test(args)\n        stop_experiment_test(args)\n        # TODO: check the output of rest server\n        print(GREEN + \'PASS\' + CLEAR)\n    except Exception as error:\n        print(RED + \'FAIL\' + CLEAR)\n        print(\'%r\' % error)\n        traceback.print_exc()\n        sys.exit(1)\n'"
test/nni_test/nnitest/remote_docker.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport argparse\nfrom subprocess import check_output, check_call\nimport socket\nimport random\nimport re\n\ndef detect_port(port):\n    '''Detect if the port is used, return True if the port is used'''\n    socket_test = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        socket_test.connect(('127.0.0.1', int(port)))\n        socket_test.close()\n        return True\n    except:\n        return False\n\ndef find_port():\n    '''Find a port which is free'''\n    port = random.randint(10000, 20000)\n    while detect_port(port):\n        port = random.randint(10000, 20000)\n    return port\n\ndef find_wheel_package(dir):\n    '''Find the wheel package uploaded to this machine'''\n    regular = re.compile('^nni-.*\\.whl$')\n    for file_name in os.listdir(dir):\n        if regular.search(file_name):\n            return file_name\n    return None\n\ndef start_container(image, name, nnimanager_os):\n    '''Start docker container, generate a port in /tmp/nnitest/{name}/port file'''\n    port = find_port()\n    source_dir = '/tmp/nnitest/' + name\n    run_cmds = ['docker', 'run', '-d', '-p', str(port) + ':22', '--name', name, '--mount', 'type=bind,source=' + source_dir + ',target=/tmp/nni', image]\n    output = check_output(run_cmds)\n    commit_id = output.decode('utf-8')\n    \n    if nnimanager_os == 'windows':\n        wheel_name = find_wheel_package(os.path.join(source_dir, 'nni-remote/deployment/pypi/dist'))\n    else:\n        wheel_name = find_wheel_package(os.path.join(source_dir, 'dist'))\n        \n    if not wheel_name:\n        print('Error: could not find wheel package in {0}'.format(source_dir))\n        exit(1)\n        \n    def get_dist(wheel_name):\n        '''get the wheel package path'''\n        if nnimanager_os == 'windows':\n            return '/tmp/nni/nni-remote/deployment/pypi/dist/{0}'.format(wheel_name)\n        else:\n            return '/tmp/nni/dist/{0}'.format(wheel_name)\n        \n    pip_cmds = ['docker', 'exec', name, 'python3', '-m', 'pip', 'install', '--upgrade', 'pip', 'setuptools==39.1.0']\n    check_call(pip_cmds)\n    sdk_cmds = ['docker', 'exec', name, 'python3', '-m', 'pip', 'install', get_dist(wheel_name)]\n    check_call(sdk_cmds)\n    with open(source_dir + '/port', 'w') as file:\n        file.write(str(port))\n\ndef stop_container(name):\n    '''Stop docker container'''\n    stop_cmds = ['docker', 'container', 'stop', name]\n    check_call(stop_cmds)\n    rm_cmds = ['docker', 'container', 'rm', name]\n    check_call(rm_cmds)\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--mode', required=True, choices=['start', 'stop'], dest='mode', help='start or stop a container')\n    parser.add_argument('--name', required=True, dest='name', help='the name of container to be used')\n    parser.add_argument('--image', dest='image', help='the image to be used')\n    parser.add_argument('--os', dest='os', default='unix', choices=['unix', 'windows'], help='nniManager os version')\n    args = parser.parse_args()\n    if args.mode == 'start':\n        start_container(args.image, args.name, args.os)\n    else:\n        stop_container(args.name)\n"""
test/nni_test/nnitest/run_tests.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport sys\nimport os\nimport argparse\nimport subprocess\nimport time\nimport datetime\nimport shlex\nimport traceback\nimport json\nimport ruamel.yaml as yaml\n\nfrom utils import get_experiment_status, get_yml_content, dump_yml_content, get_experiment_id, \\\n    parse_max_duration_time, get_trial_stats, deep_update, print_trial_job_log, get_failed_trial_jobs, \\\n    get_experiment_dir, print_experiment_log\nfrom utils import GREEN, RED, CLEAR, STATUS_URL, TRIAL_JOBS_URL, EXPERIMENT_URL, REST_ENDPOINT, wait_for_port_available\nimport validators\n\nit_variables = {}\n\ndef update_training_service_config(config, training_service):\n    it_ts_config = get_yml_content(os.path.join(\'config\', \'training_service.yml\'))\n\n    # hack for kubeflow trial config\n    if training_service == \'kubeflow\':\n        it_ts_config[training_service][\'trial\'][\'worker\'][\'command\'] = config[\'trial\'][\'command\']\n        config[\'trial\'].pop(\'command\')\n        if \'gpuNum\' in config[\'trial\']:\n            config[\'trial\'].pop(\'gpuNum\')\n\n    if training_service == \'frameworkcontroller\':\n        it_ts_config[training_service][\'trial\'][\'taskRoles\'][0][\'command\'] = config[\'trial\'][\'command\']\n        config[\'trial\'].pop(\'command\')\n        if \'gpuNum\' in config[\'trial\']:\n            config[\'trial\'].pop(\'gpuNum\')\n\n    deep_update(config, it_ts_config[\'all\'])\n    deep_update(config, it_ts_config[training_service])\n\ndef prepare_config_file(test_case_config, it_config, args):\n    config_path = args.nni_source_dir + test_case_config[\'configFile\']\n    test_yml_config = get_yml_content(config_path)\n\n    # apply test case specific config\n    if test_case_config.get(\'config\') is not None:\n        deep_update(test_yml_config, test_case_config[\'config\'])\n\n    # hack for windows\n    if sys.platform == \'win32\' and args.ts == \'local\':\n        test_yml_config[\'trial\'][\'command\'] = test_yml_config[\'trial\'][\'command\'].replace(\'python3\', \'python\')\n\n    # apply training service config\n    # user\'s gpuNum, logCollection config is overwritten by the config in training_service.yml\n    # the hack for kubeflow should be applied at last step\n    update_training_service_config(test_yml_config, args.ts)\n\n    # generate temporary config yml file to launch experiment\n    new_config_file = config_path + \'.tmp\'\n    dump_yml_content(new_config_file, test_yml_config)\n    print(yaml.dump(test_yml_config, default_flow_style=False), flush=True)\n\n    return new_config_file\n\ndef run_test_case(test_case_config, it_config, args):\n    new_config_file = prepare_config_file(test_case_config, it_config, args)\n    # set configFile variable\n    it_variables[\'$configFile\'] = new_config_file\n\n    try:\n        launch_test(new_config_file, args.ts, test_case_config)\n        invoke_validator(test_case_config, args.nni_source_dir, args.ts)\n    finally:\n        stop_command = get_command(test_case_config, \'stopCommand\')\n        print(\'Stop command:\', stop_command, flush=True)\n        if stop_command:\n            subprocess.run(shlex.split(stop_command))\n        # remove tmp config file\n        if os.path.exists(new_config_file):\n            os.remove(new_config_file)\n\ndef invoke_validator(test_case_config, nni_source_dir, training_service):\n    validator_config = test_case_config.get(\'validator\')\n    if validator_config is None or validator_config.get(\'class\') is None:\n        return\n\n    validator = validators.__dict__[validator_config.get(\'class\')]()\n    kwargs = validator_config.get(\'kwargs\', {})\n    print(\'kwargs:\', kwargs)\n    experiment_id = get_experiment_id(EXPERIMENT_URL)\n    try:\n        validator(REST_ENDPOINT, get_experiment_dir(EXPERIMENT_URL), nni_source_dir, **kwargs)\n    except:\n        print_experiment_log(experiment_id=experiment_id)\n        print_trial_job_log(training_service, TRIAL_JOBS_URL)\n        raise\n\ndef get_max_values(config_file):\n    experiment_config = get_yml_content(config_file)\n    return parse_max_duration_time(experiment_config[\'maxExecDuration\']), experiment_config[\'maxTrialNum\']\n\ndef get_command(test_case_config, commandKey):\n    command = test_case_config.get(commandKey)\n    if commandKey == \'launchCommand\':\n        assert command is not None\n    if command is None:\n        return None\n\n    # replace variables\n    for k in it_variables:\n        command = command.replace(k, it_variables[k])\n\n    # hack for windows, not limited to local training service\n    if sys.platform == \'win32\':\n        command = command.replace(\'python3\', \'python\')\n\n    return command\n\ndef launch_test(config_file, training_service, test_case_config):\n    launch_command = get_command(test_case_config, \'launchCommand\')\n    print(\'launch command: \', launch_command, flush=True)\n\n    proc = subprocess.run(shlex.split(launch_command))\n\n    assert proc.returncode == 0, \'launch command failed with code %d\' % proc.returncode\n\n    # set experiment ID into variable\n    exp_var_name = test_case_config.get(\'setExperimentIdtoVar\')\n    if exp_var_name is not None:\n        assert exp_var_name.startswith(\'$\')\n        it_variables[exp_var_name] = get_experiment_id(EXPERIMENT_URL)\n    print(\'variables:\', it_variables)\n\n    max_duration, max_trial_num = get_max_values(config_file)\n    print(\'max_duration:\', max_duration, \' max_trial_num:\', max_trial_num)\n\n    if not test_case_config.get(\'experimentStatusCheck\'):\n        return\n\n    bg_time = time.time()\n    print(str(datetime.datetime.now()), \' waiting ...\', flush=True)\n    try:\n        # wait restful server to be ready\n        time.sleep(3)\n        experiment_id = get_experiment_id(EXPERIMENT_URL)\n        while True:\n            waited_time = time.time() - bg_time\n            if  waited_time > max_duration + 10:\n                print(\'waited: {}, max_duration: {}\'.format(waited_time, max_duration))\n                break\n            status = get_experiment_status(STATUS_URL)\n            if status in [\'DONE\', \'ERROR\']:\n                print(\'experiment status:\', status)\n                break\n            num_failed = len(get_failed_trial_jobs(TRIAL_JOBS_URL))\n            if num_failed > 0:\n                print(\'failed jobs: \', num_failed)\n                break\n            time.sleep(1)\n    except:\n        print_experiment_log(experiment_id=experiment_id)\n        raise\n    print(str(datetime.datetime.now()), \' waiting done\', flush=True)\n    if get_experiment_status(STATUS_URL) == \'ERROR\':\n        print_experiment_log(experiment_id=experiment_id)\n\n    trial_stats = get_trial_stats(TRIAL_JOBS_URL)\n    print(json.dumps(trial_stats, indent=4), flush=True)\n    if status != \'DONE\' or trial_stats[\'SUCCEEDED\'] + trial_stats[\'EARLY_STOPPED\'] < max_trial_num:\n        print_experiment_log(experiment_id=experiment_id)\n        print_trial_job_log(training_service, TRIAL_JOBS_URL)\n        raise AssertionError(\'Failed to finish in maxExecDuration\')\n\ndef case_excluded(name, excludes):\n    if name is None:\n        return False\n    if excludes is not None:\n        excludes = excludes.split(\',\')\n        for e in excludes:\n            if name in e or e in name:\n                return True\n    return False\n\ndef case_included(name, cases):\n    assert cases is not None\n    for case in cases.split(\',\'):\n        if case in name:\n            return True\n    return False\n\ndef match_platform(test_case_config):\n    return sys.platform in test_case_config[\'platform\'].split(\' \')\n\ndef run(args):\n    it_config = get_yml_content(args.config)\n\n    for test_case_config in it_config[\'testCases\']:\n        name = test_case_config[\'name\']\n        if case_excluded(name, args.exclude):\n            print(\'{} excluded\'.format(name))\n            continue\n        if args.cases and not case_included(name, args.cases):\n            continue\n\n        # fill test case default config\n        for k in it_config[\'defaultTestCaseConfig\']:\n            if k not in test_case_config:\n                test_case_config[k] = it_config[\'defaultTestCaseConfig\'][k]\n        print(json.dumps(test_case_config, indent=4))\n\n        if not match_platform(test_case_config):\n            print(\'skipped {}, platform {} not match [{}]\'.format(name, sys.platform, test_case_config[\'platform\']))\n            continue\n\n        wait_for_port_available(8080, 30)\n        print(\'{}Testing: {}{}\'.format(GREEN, name, CLEAR))\n        begin_time = time.time()\n\n        run_test_case(test_case_config, it_config, args)\n        print(\'{}Test {}: TEST PASS IN {} SECONDS{}\'.format(GREEN, name, int(time.time()-begin_time), CLEAR), flush=True)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--config"", type=str, required=True)\n    parser.add_argument(""--nni_source_dir"", type=str, default=\'../\')\n    parser.add_argument(""--cases"", type=str, default=None)\n    parser.add_argument(""--exclude"", type=str, default=None)\n    parser.add_argument(""--ts"", type=str, choices=[\'local\', \'remote\', \'pai\', \'kubeflow\', \'frameworkcontroller\'], default=\'local\')\n    args = parser.parse_args()\n\n    run(args)\n'"
test/nni_test/nnitest/utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport contextlib\nimport collections\nimport os\nimport socket\nimport sys\nimport subprocess\nimport requests\nimport time\nimport ruamel.yaml as yaml\nimport shlex\n\nEXPERIMENT_DONE_SIGNAL = \'Experiment done\'\n\nGREEN = \'\\33[32m\'\nRED = \'\\33[31m\'\nCLEAR = \'\\33[0m\'\n\nREST_ENDPOINT = \'http://localhost:8080\'\nAPI_ROOT_URL = REST_ENDPOINT + \'/api/v1/nni\'\nEXPERIMENT_URL = API_ROOT_URL + \'/experiment\'\nSTATUS_URL = API_ROOT_URL + \'/check-status\'\nTRIAL_JOBS_URL = API_ROOT_URL + \'/trial-jobs\'\nMETRICS_URL = API_ROOT_URL + \'/metric-data\'\n\ndef read_last_line(file_name):\n    \'\'\'read last line of a file and return None if file not found\'\'\'\n    try:\n        *_, last_line = open(file_name)\n        return last_line.strip()\n    except (FileNotFoundError, ValueError):\n        return None\n\ndef remove_files(file_list):\n    \'\'\'remove a list of files\'\'\'\n    for file_path in file_list:\n        with contextlib.suppress(FileNotFoundError):\n            os.remove(file_path)\n\ndef get_yml_content(file_path):\n    \'\'\'Load yaml file content\'\'\'\n    with open(file_path, \'r\') as file:\n        return yaml.load(file, Loader=yaml.Loader)\n\ndef dump_yml_content(file_path, content):\n    \'\'\'Dump yaml file content\'\'\'\n    with open(file_path, \'w\') as file:\n        file.write(yaml.dump(content, default_flow_style=False))\n\ndef setup_experiment(installed=True):\n    \'\'\'setup the experiment if nni is not installed\'\'\'\n    if not installed:\n        os.environ[\'PATH\'] = os.environ[\'PATH\'] + \':\' + os.getcwd()\n        sdk_path = os.path.abspath(\'../src/sdk/pynni\')\n        cmd_path = os.path.abspath(\'../tools\')\n        pypath = os.environ.get(\'PYTHONPATH\')\n        if pypath:\n            pypath = \':\'.join([pypath, sdk_path, cmd_path])\n        else:\n            pypath = \':\'.join([sdk_path, cmd_path])\n        os.environ[\'PYTHONPATH\'] = pypath\n\ndef get_experiment_id(experiment_url):\n    experiment_id = requests.get(experiment_url).json()[\'id\']\n    return experiment_id\n\ndef get_experiment_dir(experiment_url=None, experiment_id=None):\n    \'\'\'get experiment root directory\'\'\'\n    assert any([experiment_url, experiment_id])\n    if experiment_id is None:\n        experiment_id = get_experiment_id(experiment_url)\n    return os.path.join(os.path.expanduser(\'~\'), \'nni\', \'experiments\', experiment_id)\n\ndef get_nni_log_dir(experiment_url=None, experiment_id=None):\n    \'\'\'get nni\'s log directory from nni\'s experiment url\'\'\'\n    return os.path.join(get_experiment_dir(experiment_url, experiment_id), \'log\')\n\ndef get_nni_log_path(experiment_url):\n    \'\'\'get nni\'s log path from nni\'s experiment url\'\'\'\n    return os.path.join(get_nni_log_dir(experiment_url), \'nnimanager.log\')\n\ndef is_experiment_done(nnimanager_log_path):\n    \'\'\'check if the experiment is done successfully\'\'\'\n    assert os.path.exists(nnimanager_log_path), \'Experiment starts failed\'\n    \n    with open(nnimanager_log_path, \'r\') as f:\n        log_content = f.read()\n\n    return EXPERIMENT_DONE_SIGNAL in log_content\n\ndef get_experiment_status(status_url):\n    nni_status = requests.get(status_url).json()\n    return nni_status[\'status\']\n\ndef get_trial_stats(trial_jobs_url):\n    trial_jobs = requests.get(trial_jobs_url).json()\n    trial_stats = collections.defaultdict(int)\n    for trial_job in trial_jobs:\n        trial_stats[trial_job[\'status\']] += 1\n    return trial_stats\n\ndef get_trial_jobs(trial_jobs_url, status=None):\n    \'\'\'Return failed trial jobs\'\'\'\n    trial_jobs = requests.get(trial_jobs_url).json()\n    res = []\n    for trial_job in trial_jobs:\n        if status is None or trial_job[\'status\'] == status:\n            res.append(trial_job)\n    return res\n\ndef get_failed_trial_jobs(trial_jobs_url):\n    \'\'\'Return failed trial jobs\'\'\'\n    return get_trial_jobs(trial_jobs_url, \'FAILED\')\n\ndef print_file_content(filepath):\n    with open(filepath, \'r\') as f:\n        content = f.read()\n        print(filepath, flush=True)\n        print(content, flush=True)\n\ndef print_trial_job_log(training_service, trial_jobs_url):\n    trial_jobs = get_trial_jobs(trial_jobs_url)\n    for trial_job in trial_jobs:\n        trial_log_dir = os.path.join(get_experiment_dir(EXPERIMENT_URL), \'trials\', trial_job[\'id\'])\n        log_files = [\'stderr\', \'trial.log\'] if training_service == \'local\' else [\'stdout_log_collection.log\']\n        for log_file in log_files:\n            print_file_content(os.path.join(trial_log_dir, log_file))\n\ndef print_experiment_log(experiment_id):\n    log_dir = get_nni_log_dir(experiment_id=experiment_id)\n    for log_file in [\'dispatcher.log\', \'nnimanager.log\']:\n        filepath = os.path.join(log_dir, log_file)\n        print_file_content(filepath)\n\n    print(\'nnictl log stderr:\')\n    subprocess.run(shlex.split(\'nnictl log stderr {}\'.format(experiment_id)))\n    print(\'nnictl log stdout:\')\n    subprocess.run(shlex.split(\'nnictl log stdout {}\'.format(experiment_id)))\n\ndef parse_max_duration_time(max_exec_duration):\n    unit = max_exec_duration[-1]\n    time = max_exec_duration[:-1]\n    units_dict = {\'s\':1, \'m\':60, \'h\':3600, \'d\':86400}\n    return int(time) * units_dict[unit]\n\ndef deep_update(source, overrides):\n    """"""Update a nested dictionary or similar mapping.\n\n    Modify ``source`` in place.\n    """"""\n    for key, value in overrides.items():\n        if isinstance(value, collections.Mapping) and value:\n            returned = deep_update(source.get(key, {}), value)\n            source[key] = returned\n        else:\n            source[key] = overrides[key]\n    return source\n\ndef detect_port(port):\n    \'\'\'Detect if the port is used\'\'\'\n    socket_test = socket.socket(socket.AF_INET,socket.SOCK_STREAM)\n    try:\n        socket_test.connect((\'127.0.0.1\', int(port)))\n        socket_test.close()\n        return True\n    except:\n        return False\n\n\ndef wait_for_port_available(port, timeout):\n    begin_time = time.time()\n    while True:\n        if not detect_port(port):\n            return\n        if time.time() - begin_time > timeout:\n            msg = \'port {} is not available in {} seconds.\'.format(port, timeout)\n            raise RuntimeError(msg)\n        time.sleep(1)\n'"
test/nni_test/nnitest/validators.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os.path as osp\nfrom os import remove\nimport subprocess\nimport json\nimport requests\nimport nnicli as nc\nfrom utils import METRICS_URL\n\n\nclass ITValidator:\n    def __call__(self, rest_endpoint, experiment_dir, nni_source_dir, **kwargs):\n        pass\n\nclass ExportValidator(ITValidator):\n    def __call__(self, rest_endpoint, experiment_dir, nni_source_dir, **kwargs):\n        exp_id = osp.split(experiment_dir)[-1]\n        proc1 = subprocess.run([""nnictl"", ""experiment"", ""export"", exp_id, ""-t"", ""csv"", ""-f"", ""report.csv""])\n        assert proc1.returncode == 0, \'`nnictl experiment export -t csv` failed with code %d\' % proc1.returncode\n        with open(""report.csv"", \'r\') as f:\n            print(\'Exported CSV file: \\n\')\n            print(\'\'.join(f.readlines()))\n            print(\'\\n\\n\')\n        remove(\'report.csv\')\n\n        proc2 = subprocess.run([""nnictl"", ""experiment"", ""export"", exp_id, ""-t"", ""json"", ""-f"", ""report.json""])\n        assert proc2.returncode == 0, \'`nnictl experiment export -t json` failed with code %d\' % proc2.returncode\n        with open(""report.json"", \'r\') as f:\n            print(\'Exported JSON file: \\n\')\n            print(\'\\n\'.join(f.readlines()))\n            print(\'\\n\\n\')\n        remove(\'report.json\')\n\nclass MetricsValidator(ITValidator):\n    def __call__(self, rest_endpoint, experiment_dir, nni_source_dir, **kwargs):\n        self.check_metrics(nni_source_dir, **kwargs)\n\n    def check_metrics(self, nni_source_dir, **kwargs):\n        expected_result_file = kwargs.get(\'expected_result_file\', \'expected_metrics.json\')\n        with open(osp.join(nni_source_dir, \'test\', \'config\', \'metrics_test\', expected_result_file), \'r\') as f:\n            expected_metrics = json.load(f)\n        print(\'expected metrics:\', expected_metrics)\n        metrics = requests.get(METRICS_URL).json()\n        print(\'RAW METRICS:\', json.dumps(metrics, indent=4))\n        intermediate_result, final_result = self.get_metric_results(metrics)\n\n        assert intermediate_result and final_result\n        for trialjob_id in intermediate_result:\n            trial_final_result = final_result[trialjob_id]\n            trial_intermediate_result = intermediate_result[trialjob_id]\n            print(\'intermediate result:\', trial_intermediate_result)\n            print(\'final result:\', trial_final_result)\n            assert len(trial_final_result) == 1, \'there should be 1 final result\'\n            assert trial_final_result[0] == expected_metrics[\'final_result\']\n            # encode dict/number into json string to compare them in set\n            assert set([json.dumps(x, sort_keys=True) for x in trial_intermediate_result]) \\\n                == set([json.dumps(x, sort_keys=True) for x in expected_metrics[\'intermediate_result\']])\n\n    def get_metric_results(self, metrics):\n        intermediate_result = {}\n        final_result = {}\n        for metric in metrics:\n            # metrics value are encoded by NNI SDK as json string,\n            # here we decode the value by json.loads twice\n            metric_value = json.loads(json.loads(metric[\'data\']))\n            if metric[\'type\'] == \'PERIODICAL\':\n                if metric[\'trialJobId\'] in intermediate_result:\n                    intermediate_result[metric[\'trialJobId\']].append(metric_value)\n                else:\n                    intermediate_result[metric[\'trialJobId\']] = [metric_value]\n            elif metric[\'type\'] == \'FINAL\':\n                if metric[\'trialJobId\'] in final_result:\n                    final_result[metric[\'trialJobId\']].append(metric_value)\n                else:\n                    final_result[metric[\'trialJobId\']] = [metric_value]\n        return intermediate_result, final_result\n\nclass NnicliValidator(ITValidator):\n    def __call__(self, rest_endpoint, experiment_dir, nni_source_dir, **kwargs):\n        print(rest_endpoint)\n        nc.set_endpoint(rest_endpoint)\n        #print(nc.version())\n        print(nc.get_job_statistics())\n        print(nc.get_experiment_status())\n        print(nc.list_trial_jobs())\n'"
tools/nni_annotation/examples/mnist_generated.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""A deep MNIST classifier using convolutional layers.""""""\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport nni\n\nFLAGS = None\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    """"""\n    MnistNetwork is for initlizing and building basic network for mnist.\n    """"""\n\n    def __init__(self, channel_1_num, channel_2_num, conv_size, hidden_size,\n        pool_size, learning_rate, x_dim=784, y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = nni.choice(2, 3, 5, 7, name=\'self.conv_size\')\n        self.hidden_size = nni.choice(124, 512, 1024, name=\'self.hidden_size\')\n        self.pool_size = pool_size\n        self.learning_rate = nni.uniform(0.0001, 0.1, name=\'self.learning_rate\'\n            )\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\n            \'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\n            \'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        """"""\n        Building network for mnist\n        """"""\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\'input dim cannot be sqrt and reshape. input dim: \' +\n                    str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\',\n                    str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable([self.conv_size, self.conv_size, 1,\n                self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = nni.function_choice(lambda : tf.nn.relu(conv2d(\n                x_image, w_conv1) + b_conv1), lambda : tf.nn.sigmoid(conv2d\n                (x_image, w_conv1) + b_conv1), lambda : tf.nn.tanh(conv2d(\n                x_image, w_conv1) + b_conv1), name=\'tf.nn.relu\')\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = nni.function_choice(lambda : max_pool(h_conv1, self.\n                pool_size), lambda : avg_pool(h_conv1, self.pool_size),\n                name=\'max_pool\')\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size, self\n                .channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable([last_dim * last_dim * self.\n                channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, last_dim * last_dim * self.\n            channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(tf.nn.\n                softmax_cross_entropy_with_logits(labels=self.labels,\n                logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(self.learning_rate\n                ).minimize(cross_entropy)\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(\n                self.labels, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.\n                float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\'\n        )\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n        strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef avg_pool(x_input, pool_size):\n    return tf.nn.avg_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n        strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    """"""\n    Main function, build mnist network, run and send result to NNI.\n    """"""\n\ndef main(params):\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n        channel_2_num=params[\'channel_2_num\'], conv_size=params[\'conv_size\'\n        ], hidden_size=params[\'hidden_size\'], pool_size=params[\'pool_size\'],\n        learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        batch_num = nni.choice(50, 250, 500, name=\'batch_num\')\n        for i in range(batch_num):\n            batch = mnist.train.next_batch(batch_num)\n            dropout_rate = nni.choice(1, 5, name=\'dropout_rate\')\n            mnist_network.train_step.run(feed_dict={mnist_network.images:\n                batch[0], mnist_network.labels: batch[1], mnist_network.\n                keep_prob: dropout_rate})\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(feed_dict={\n                    mnist_network.images: mnist.test.images, mnist_network.\n                    labels: mnist.test.labels, mnist_network.keep_prob: 1.0})\n                nni.report_intermediate_result(test_acc)\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n        test_acc = mnist_network.accuracy.eval(feed_dict={mnist_network.\n            images: mnist.test.images, mnist_network.labels: mnist.test.\n            labels, mnist_network.keep_prob: 1.0})\n        nni.report_final_result(test_acc)\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\n\ndef generate_defualt_params():\n    """"""\n    Generate default parameters for mnist network.\n    """"""\n    params = {\'data_dir\': \'/tmp/tensorflow/mnist/input_data\',\n        \'dropout_rate\': 0.5, \'channel_1_num\': 32, \'channel_2_num\': 64,\n        \'conv_size\': 5, \'pool_size\': 2, \'hidden_size\': 1024,\n        \'learning_rate\': 0.0001, \'batch_num\': 200}\n    return params\n\n\nif __name__ == \'__main__\':\n    try:\n        main(generate_defualt_params())\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
tools/nni_annotation/examples/mnist_with_annotation.py,0,"b'#!/usr/bin/python\n\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""A deep MNIST classifier using convolutional layers.""""""\n\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initlizing and building basic network for mnist.\n    \'\'\'\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 conv_size,\n                 hidden_size,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        """"""@nni.variable(nni.choice(2, 3, 5, 7),name=self.conv_size)""""""\n        self.conv_size = conv_size\n        """"""@nni.variable(nni.choice(124, 512, 1024), name=self.hidden_size)""""""\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        """"""@nni.variable(nni.uniform(0.0001, 0.1), name=self.learning_rate)""""""\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            """"""@nni.function_choice(tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1), tf.nn.sigmoid(conv2d(x_image, w_conv1) + b_conv1), tf.nn.tanh(conv2d(x_image, w_conv1) + b_conv1), name=tf.nn.relu)""""""\n            h_conv1 = tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            """"""@nni.function_choice(max_pool(h_conv1, self.pool_size), avg_pool(h_conv1, self.pool_size), name=max_pool)""""""\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef avg_pool(x_input, pool_size):\n    return tf.nn.avg_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 conv_size=params[\'conv_size\'],\n                                 hidden_size=params[\'hidden_size\'],\n                                 pool_size=params[\'pool_size\'],\n                                 learning_rate=params[\'learning_rate\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        """"""@nni.variable(nni.choice(50, 250, 500), name=batch_num)""""""\n        batch_num = params[\'batch_num\']\n        for i in range(batch_num):\n            batch = mnist.train.next_batch(batch_num)\n            """"""@nni.variable(nni.choice(1, 5), name=dropout_rate)""""""\n            dropout_rate = params[\'dropout_rate\']\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: dropout_rate}\n                                        )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                """"""@nni.report_intermediate_result(test_acc)""""""\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        """"""@nni.report_final_result(test_acc)""""""\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\n\ndef generate_defualt_params():\n    \'\'\'\n    Generate default parameters for mnist network.\n    \'\'\'\n    params = {\n        \'data_dir\': \'/tmp/tensorflow/mnist/input_data\',\n        \'dropout_rate\': 0.5,\n        \'channel_1_num\': 32,\n        \'channel_2_num\': 64,\n        \'conv_size\': 5,\n        \'pool_size\': 2,\n        \'hidden_size\': 1024,\n        \'learning_rate\': 1e-4,\n        \'batch_num\': 200}\n    return params\n\n\nif __name__ == \'__main__\':\n    """"""@nni.get_next_parameter()""""""\n    try:\n        main(generate_defualt_params())\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
tools/nni_annotation/examples/mnist_without_annotation.py,0,"b'#!/usr/bin/python\n\n# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""A deep MNIST classifier using convolutional layers.""""""\n\nimport logging\nimport math\nimport tempfile\nimport time\n\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nimport nni\n\nFLAGS = None\n\nlogger = logging.getLogger(\'mnist_AutoML\')\n\n\nclass MnistNetwork(object):\n    \'\'\'\n    MnistNetwork is for initlizing and building basic network for mnist.\n    \'\'\'\n    def __init__(self,\n                 channel_1_num,\n                 channel_2_num,\n                 pool_size,\n                 learning_rate,\n                 x_dim=784,\n                 y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = nni.choice(2, 3, 5, 7, name=\'conv-size\')\n        self.hidden_size = nni.choice(124, 512, 1024)  # example: without name\n        self.pool_size = pool_size\n        self.learning_rate = nni.uniform(0.0001, 0.1, name=\'learning_rate\')\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n        self.images = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.labels = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n\n        self.train_step = None\n        self.accuracy = None\n\n    def build_network(self):\n        \'\'\'\n        Building network for mnist\n        \'\'\'\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                print(\n                    \'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: %s\', str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.images, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            w_conv1 = weight_variable(\n                [self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = nni.function_choice(\n                lambda: tf.nn.relu(conv2d(x_image, w_conv1) + b_conv1),\n                lambda: tf.nn.sigmoid(conv2d(x_image, w_conv1) + b_conv1),\n                lambda: tf.nn.tanh(conv2d(x_image, w_conv1) + b_conv1)\n            )  # example: without name\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n            h_pool1 = nni.function_choice(\n                lambda: max_pool(h_conv1, self.pool_size),\n                lambda: avg_pool(h_conv1, self.pool_size),\n                name=\'h_pool1\')\n\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            w_conv2 = weight_variable([self.conv_size, self.conv_size,\n                                       self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, w_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):  # example: another style\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            w_fc1 = weight_variable(\n                [last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(\n            h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, w_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            w_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, w_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(\n                tf.nn.softmax_cross_entropy_with_logits(labels=self.labels, logits=y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(\n                self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(\n                tf.argmax(y_conv, 1), tf.argmax(self.labels, 1))\n            self.accuracy = tf.reduce_mean(\n                tf.cast(correct_prediction, tf.float32))\n\n\ndef conv2d(x_input, w_matrix):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x_input, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef avg_pool(x_input, pool_size):\n    return tf.nn.avg_pool(x_input, ksize=[1, pool_size, pool_size, 1],\n                          strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef download_mnist_retry(data_dir, max_num_retries=20):\n    """"""Try to download mnist dataset and avoid errors""""""\n    for _ in range(max_num_retries):\n        try:\n            return input_data.read_data_sets(data_dir, one_hot=True)\n        except tf.errors.AlreadyExistsError:\n            time.sleep(1)\n    raise Exception(""Failed to download MNIST."")\n\ndef main(params):\n    \'\'\'\n    Main function, build mnist network, run and send result to NNI.\n    \'\'\'\n    # Import data\n    mnist = download_mnist_retry(params[\'data_dir\'])\n    print(\'Mnist download data done.\')\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork(channel_1_num=params[\'channel_1_num\'],\n                                 channel_2_num=params[\'channel_2_num\'],\n                                 pool_size=params[\'pool_size\'])\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\', graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        batch_num = nni.choice(50, 250, 500, name=\'batch_num\')\n        for i in range(batch_num):\n            batch = mnist.train.next_batch(batch_num)\n            dropout_rate = nni.choice(1, 5, name=\'dropout_rate\')\n            mnist_network.train_step.run(feed_dict={mnist_network.images: batch[0],\n                                                    mnist_network.labels: batch[1],\n                                                    mnist_network.keep_prob: dropout_rate}\n                                        )\n\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(\n                    feed_dict={mnist_network.images: mnist.test.images,\n                               mnist_network.labels: mnist.test.labels,\n                               mnist_network.keep_prob: 1.0})\n\n                nni.report_intermediate_result(test_acc)\n                logger.debug(\'test accuracy %g\', test_acc)\n                logger.debug(\'Pipe send intermediate result done.\')\n\n        test_acc = mnist_network.accuracy.eval(\n            feed_dict={mnist_network.images: mnist.test.images,\n                       mnist_network.labels: mnist.test.labels,\n                       mnist_network.keep_prob: 1.0})\n\n        nni.report_final_result(test_acc)\n        logger.debug(\'Final result is %g\', test_acc)\n        logger.debug(\'Send final result done.\')\n\n\ndef generate_defualt_params():\n    \'\'\'\n    Generate default parameters for mnist network.\n    \'\'\'\n    params = {\n        \'data_dir\': \'/tmp/tensorflow/mnist/input_data\',\n        \'channel_1_num\': 32,\n        \'channel_2_num\': 64,\n        \'pool_size\': 2}\n    return params\n\n\nif __name__ == \'__main__\':\n    try:\n        main(generate_defualt_params())\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
tools/nni_trial_tool/test/test_hdfsClientUtility.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport shutil\nimport random\nimport string\nimport unittest\nimport json\nimport sys\nfrom pyhdfs import HdfsClient\nfrom tools.nni_trial_tool.hdfsClientUtility import copyFileToHdfs, copyDirectoryToHdfs\nsys.path.append("".."")\n\n\nclass HDFSClientUtilityTest(unittest.TestCase):\n    \'\'\'Unit test for hdfsClientUtility.py\'\'\'\n    def setUp(self):\n        self.hdfs_file_path = \'../../.vscode/hdfsInfo.json\'\n        self.hdfs_config = None\n        try:\n            with open(self.hdfs_file_path, \'r\') as file:\n                self.hdfs_config = json.load(file)\n        except Exception as exception:\n            print(exception)\n\n        self.hdfs_client = HdfsClient(hosts=\'{0}:{1}\'.format(self.hdfs_config[\'host\'], \'50070\'), user_name=self.hdfs_config[\'userName\'])\n\n    def get_random_name(self, length):\n        return \'\'.join(random.sample(string.ascii_letters + string.digits, length))\n\n    def test_copy_file_run(self):\n        \'\'\'test copyFileToHdfs\'\'\'\n        file_name = self.get_random_name(8)\n        file_content = \'hello world!\'\n\n        with open(\'./{}\'.format(file_name), \'w\') as file:\n            file.write(file_content)\n\n        result = copyFileToHdfs(\'./{}\'.format(file_name), \'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], file_name), self.hdfs_client)\n        self.assertTrue(result)\n\n        file_list = self.hdfs_client.listdir(\'/{0}\'.format(self.hdfs_config[\'userName\']))\n        self.assertIn(file_name, file_list)\n\n        hdfs_file_name = self.get_random_name(8)\n        self.hdfs_client.copy_to_local(\'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], file_name), \'./{}\'.format(hdfs_file_name))\n        self.assertTrue(os.path.exists(\'./{}\'.format(hdfs_file_name)))\n\n        with open(\'./{}\'.format(hdfs_file_name), \'r\') as file:\n            content = file.readline()\n            self.assertEqual(file_content, content)\n        #clean up\n        os.remove(\'./{}\'.format(file_name))\n        os.remove(\'./{}\'.format(hdfs_file_name))\n        self.hdfs_client.delete(\'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], file_name))\n\n    def test_copy_directory_run(self):\n        \'\'\'test copyDirectoryToHdfs\'\'\'\n        directory_name = self.get_random_name(8)\n        file_name_list = [self.get_random_name(8), self.get_random_name(8)]\n        file_content = \'hello world!\'\n\n        os.makedirs(\'./{}\'.format(directory_name))\n        for file_name in file_name_list:\n            with open(\'./{0}/{1}\'.format(directory_name, file_name), \'w\') as file:\n                file.write(file_content)\n\n        result = copyDirectoryToHdfs(\'./{}\'.format(directory_name),\n                                     \'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], directory_name), self.hdfs_client)\n        self.assertTrue(result)\n\n        directory_list = self.hdfs_client.listdir(\'/{0}\'.format(self.hdfs_config[\'userName\']))\n        self.assertIn(directory_name, directory_list)\n\n        sub_file_list = self.hdfs_client.listdir(\'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], directory_name))\n        for file_name in file_name_list:\n            self.assertIn(file_name, sub_file_list)\n            #clean up\n            self.hdfs_client.delete(\'/{0}/{1}/{2}\'.format(self.hdfs_config[\'userName\'], directory_name, file_name))\n        self.hdfs_client.delete(\'/{0}/{1}\'.format(self.hdfs_config[\'userName\'], directory_name))\n\n        shutil.rmtree(\'./{}\'.format(directory_name))\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
examples/model_compress/models/cifar10/vgg.py,2,"b""import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndefaultcfg = {\n    11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    13: [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512],\n    16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512],\n    19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, depth=16):\n        super(VGG, self).__init__()\n        cfg = defaultcfg[depth]\n        self.cfg = cfg\n        self.feature = self.make_layers(cfg, True)\n        num_classes = 10\n        self.classifier = nn.Sequential(\n            nn.Linear(cfg[-1], 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, num_classes)\n        )\n        self._initialize_weights()\n\n    def make_layers(self, cfg, batch_norm=False):\n        layers = []\n        in_channels = 3\n        for v in cfg:\n            if v == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1, bias=False)\n                if batch_norm:\n                    layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n                else:\n                    layers += [conv2d, nn.ReLU(inplace=True)]\n                in_channels = v\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.feature(x)\n        x = nn.AvgPool2d(2)(x)\n        x = x.view(x.size(0), -1)\n        y = self.classifier(x)\n        return y\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(0.5)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n"""
examples/nas/cdarts/datasets/cifar.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nfrom datasets.data_utils import CIFAR10Policy, Cutout\nfrom datasets.data_utils import SubsetDistributedSampler\n\n\ndef data_transforms_cifar(config, cutout=False):\n    CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n    CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n\n    if config.use_aa:\n        train_transform = transforms.Compose([\n            transforms.RandomCrop(32, padding=4, fill=128),\n            transforms.RandomHorizontalFlip(), CIFAR10Policy(),\n            transforms.ToTensor(),\n            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n        ])\n    else:\n        train_transform = transforms.Compose([\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n        ])\n\n    if cutout:\n        train_transform.transforms.append(Cutout(config.cutout_length))\n\n    valid_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n    ])\n    return train_transform, valid_transform\n\n\ndef get_search_datasets(config):\n    dataset = config.dataset.lower()\n    if dataset == \'cifar10\':\n        dset_cls = dset.CIFAR10\n        n_classes = 10\n    elif dataset == \'cifar100\':\n        dset_cls = dset.CIFAR100\n        n_classes = 100\n    else:\n        raise Exception(""Not support dataset!"")\n\n    train_transform, valid_transform = data_transforms_cifar(config, cutout=False)\n    train_data = dset_cls(root=config.data_dir, train=True, download=True, transform=train_transform)\n    test_data = dset_cls(root=config.data_dir, train=False, download=True, transform=valid_transform)\n\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split_mid = int(np.floor(0.5 * num_train))\n\n    if config.distributed:\n        train_sampler = SubsetDistributedSampler(train_data, indices[:split_mid])\n        valid_sampler = SubsetDistributedSampler(train_data, indices[split_mid:num_train])\n    else:\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split_mid])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split_mid:num_train])\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=train_sampler,\n        pin_memory=False, num_workers=config.workers)\n\n    valid_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=valid_sampler,\n        pin_memory=False, num_workers=config.workers)\n\n    return [train_loader, valid_loader], [train_sampler, valid_sampler]\n\n\ndef get_augment_datasets(config):\n    dataset = config.dataset.lower()\n    if dataset == \'cifar10\':\n        dset_cls = dset.CIFAR10\n    elif dataset == \'cifar100\':\n        dset_cls = dset.CIFAR100\n    else:\n        raise Exception(""Not support dataset!"")\n\n    train_transform, valid_transform = data_transforms_cifar(config, cutout=True)\n    train_data = dset_cls(root=config.data_dir, train=True, download=True, transform=train_transform)\n    test_data = dset_cls(root=config.data_dir, train=False, download=True, transform=valid_transform)\n\n    if config.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n        test_sampler = torch.utils.data.distributed.DistributedSampler(test_data)\n    else:\n        train_sampler = None\n        test_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=train_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_data, batch_size=config.eval_batch_size,\n        sampler=test_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    return [train_loader, test_loader], [train_sampler, test_sampler]\n'"
examples/nas/cdarts/datasets/data_utils.py,18,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport math\nimport random\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom PIL import Image, ImageEnhance, ImageOps\nfrom torch.utils.data import Sampler\n\n\nclass SubsetDistributedSampler(Sampler):\n    """"""\n    Sampler that restricts data loading to a subset of the dataset.\n\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n\n    Dataset is assumed to be of constant size.\n    """"""\n\n    def __init__(self, dataset, indices, num_replicas=None, rank=None, shuffle=True):\n        """"""\n        Initialization.\n\n        Parameters\n        ----------\n        dataset : torch.utils.data.Dataset\n            Dataset used for sampling.\n        num_replicas : int\n            Number of processes participating in distributed training. Default: World size.\n        rank : int\n            Rank of the current process within num_replicas. Default: Current rank.\n        shuffle : bool\n            If true (default), sampler will shuffle the indices.\n        """"""\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.indices = indices\n        self.num_samples = int(math.ceil(len(self.indices) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n        if self.shuffle:\n            # indices = torch.randperm(len(self.dataset), generator=g).tolist()\n            indices = list(self.indices[i] for i in torch.randperm(len(self.indices)))\n        else:\n            # indices = list(range(len(self.dataset)))\n            indices = self.indices\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\nclass data_prefetcher():\n    def __init__(self, loader):\n        self.loader = iter(loader)\n        self.stream = torch.cuda.Stream()\n        self.mean = torch.tensor([0.485 * 255, 0.456 * 255, 0.406 * 255]).cuda().view(1, 3, 1, 1)\n        self.std = torch.tensor([0.229 * 255, 0.224 * 255, 0.225 * 255]).cuda().view(1, 3, 1, 1)\n        self.preload()\n\n    def preload(self):\n        try:\n            self.next_input, self.next_target = next(self.loader)\n        except StopIteration:\n            self.next_input = None\n            self.next_target = None\n            return\n        with torch.cuda.stream(self.stream):\n            self.next_input = self.next_input.cuda(non_blocking=True)\n            self.next_target = self.next_target.cuda(non_blocking=True)\n            self.next_input = self.next_input.float()\n            self.next_input = self.next_input.sub_(self.mean).div_(self.std)\n\n    def next(self):\n        torch.cuda.current_stream().wait_stream(self.stream)\n        input = self.next_input\n        target = self.next_target\n        self.preload()\n        return input, target\n\n\nclass Cutout(object):\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        h, w = img.size(1), img.size(2)\n        mask = np.ones((h, w), np.float32)\n        y = np.random.randint(h)\n        x = np.random.randint(w)\n\n        y1 = np.clip(y - self.length // 2, 0, h)\n        y2 = np.clip(y + self.length // 2, 0, h)\n        x1 = np.clip(x - self.length // 2, 0, w)\n        x2 = np.clip(x + self.length // 2, 0, w)\n\n        mask[y1: y2, x1: x2] = 0.\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n\n        return img\n\n\nclass ImageNetPolicy(object):\n    """""" Randomly choose one of the best 24 Sub-policies on ImageNet.\n        Example:\n        >>> policy = ImageNetPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     ImageNetPolicy(),\n        >>>     transforms.ToTensor()])\n    """"""\n\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.4, ""posterize"", 8, 0.6, ""rotate"", 9, fillcolor),\n            SubPolicy(0.6, ""solarize"", 5, 0.6, ""autocontrast"", 5, fillcolor),\n            SubPolicy(0.8, ""equalize"", 8, 0.6, ""equalize"", 3, fillcolor),\n            SubPolicy(0.6, ""posterize"", 7, 0.6, ""posterize"", 6, fillcolor),\n            SubPolicy(0.4, ""equalize"", 7, 0.2, ""solarize"", 4, fillcolor),\n\n            SubPolicy(0.4, ""equalize"", 4, 0.8, ""rotate"", 8, fillcolor),\n            SubPolicy(0.6, ""solarize"", 3, 0.6, ""equalize"", 7, fillcolor),\n            SubPolicy(0.8, ""posterize"", 5, 1.0, ""equalize"", 2, fillcolor),\n            SubPolicy(0.2, ""rotate"", 3, 0.6, ""solarize"", 8, fillcolor),\n            SubPolicy(0.6, ""equalize"", 8, 0.4, ""posterize"", 6, fillcolor),\n\n            SubPolicy(0.8, ""rotate"", 8, 0.4, ""color"", 0, fillcolor),\n            SubPolicy(0.4, ""rotate"", 9, 0.6, ""equalize"", 2, fillcolor),\n            SubPolicy(0.0, ""equalize"", 7, 0.8, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""invert"", 4, 1.0, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""color"", 4, 1.0, ""contrast"", 8, fillcolor),\n\n            SubPolicy(0.8, ""rotate"", 8, 1.0, ""color"", 2, fillcolor),\n            SubPolicy(0.8, ""color"", 8, 0.8, ""solarize"", 7, fillcolor),\n            SubPolicy(0.4, ""sharpness"", 7, 0.6, ""invert"", 8, fillcolor),\n            SubPolicy(0.6, ""shearX"", 5, 1.0, ""equalize"", 9, fillcolor),\n            SubPolicy(0.4, ""color"", 0, 0.6, ""equalize"", 3, fillcolor),\n\n            SubPolicy(0.4, ""equalize"", 7, 0.2, ""solarize"", 4, fillcolor),\n            SubPolicy(0.6, ""solarize"", 5, 0.6, ""autocontrast"", 5, fillcolor),\n            SubPolicy(0.6, ""invert"", 4, 1.0, ""equalize"", 8, fillcolor),\n            SubPolicy(0.6, ""color"", 4, 1.0, ""contrast"", 8, fillcolor),\n            SubPolicy(0.8, ""equalize"", 8, 0.6, ""equalize"", 3, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return ""AutoAugment ImageNet Policy""\n\n\nclass CIFAR10Policy(object):\n    """""" Randomly choose one of the best 25 Sub-policies on CIFAR10.\n        Example:\n        >>> policy = CIFAR10Policy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     CIFAR10Policy(),\n        >>>     transforms.ToTensor()])\n    """"""\n\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.1, ""invert"", 7, 0.2, ""contrast"", 6, fillcolor),\n            SubPolicy(0.7, ""rotate"", 2, 0.3, ""translateX"", 9, fillcolor),\n            SubPolicy(0.8, ""sharpness"", 1, 0.9, ""sharpness"", 3, fillcolor),\n            SubPolicy(0.5, ""shearY"", 8, 0.7, ""translateY"", 9, fillcolor),\n            SubPolicy(0.5, ""autocontrast"", 8, 0.9, ""equalize"", 2, fillcolor),\n\n            SubPolicy(0.2, ""shearY"", 7, 0.3, ""posterize"", 7, fillcolor),\n            SubPolicy(0.4, ""color"", 3, 0.6, ""brightness"", 7, fillcolor),\n            SubPolicy(0.3, ""sharpness"", 9, 0.7, ""brightness"", 9, fillcolor),\n            SubPolicy(0.6, ""equalize"", 5, 0.5, ""equalize"", 1, fillcolor),\n            SubPolicy(0.6, ""contrast"", 7, 0.6, ""sharpness"", 5, fillcolor),\n\n            SubPolicy(0.7, ""color"", 7, 0.5, ""translateX"", 8, fillcolor),\n            SubPolicy(0.3, ""equalize"", 7, 0.4, ""autocontrast"", 8, fillcolor),\n            SubPolicy(0.4, ""translateY"", 3, 0.2, ""sharpness"", 6, fillcolor),\n            SubPolicy(0.9, ""brightness"", 6, 0.2, ""color"", 8, fillcolor),\n            SubPolicy(0.5, ""solarize"", 2, 0.0, ""invert"", 3, fillcolor),\n\n            SubPolicy(0.2, ""equalize"", 0, 0.6, ""autocontrast"", 0, fillcolor),\n            SubPolicy(0.2, ""equalize"", 8, 0.6, ""equalize"", 4, fillcolor),\n            SubPolicy(0.9, ""color"", 9, 0.6, ""equalize"", 6, fillcolor),\n            SubPolicy(0.8, ""autocontrast"", 4, 0.2, ""solarize"", 8, fillcolor),\n            SubPolicy(0.1, ""brightness"", 3, 0.7, ""color"", 0, fillcolor),\n\n            SubPolicy(0.4, ""solarize"", 5, 0.9, ""autocontrast"", 3, fillcolor),\n            SubPolicy(0.9, ""translateY"", 9, 0.7, ""translateY"", 9, fillcolor),\n            SubPolicy(0.9, ""autocontrast"", 2, 0.8, ""solarize"", 3, fillcolor),\n            SubPolicy(0.8, ""equalize"", 8, 0.1, ""invert"", 3, fillcolor),\n            SubPolicy(0.7, ""translateY"", 9, 0.9, ""autocontrast"", 1, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return ""AutoAugment CIFAR10 Policy""\n\n\nclass SVHNPolicy(object):\n    """""" Randomly choose one of the best 25 Sub-policies on SVHN.\n        Example:\n        >>> policy = SVHNPolicy()\n        >>> transformed = policy(image)\n        Example as a PyTorch Transform:\n        >>> transform=transforms.Compose([\n        >>>     transforms.Resize(256),\n        >>>     SVHNPolicy(),\n        >>>     transforms.ToTensor()])\n    """"""\n\n    def __init__(self, fillcolor=(128, 128, 128)):\n        self.policies = [\n            SubPolicy(0.9, ""shearX"", 4, 0.2, ""invert"", 3, fillcolor),\n            SubPolicy(0.9, ""shearY"", 8, 0.7, ""invert"", 5, fillcolor),\n            SubPolicy(0.6, ""equalize"", 5, 0.6, ""solarize"", 6, fillcolor),\n            SubPolicy(0.9, ""invert"", 3, 0.6, ""equalize"", 3, fillcolor),\n            SubPolicy(0.6, ""equalize"", 1, 0.9, ""rotate"", 3, fillcolor),\n\n            SubPolicy(0.9, ""shearX"", 4, 0.8, ""autocontrast"", 3, fillcolor),\n            SubPolicy(0.9, ""shearY"", 8, 0.4, ""invert"", 5, fillcolor),\n            SubPolicy(0.9, ""shearY"", 5, 0.2, ""solarize"", 6, fillcolor),\n            SubPolicy(0.9, ""invert"", 6, 0.8, ""autocontrast"", 1, fillcolor),\n            SubPolicy(0.6, ""equalize"", 3, 0.9, ""rotate"", 3, fillcolor),\n\n            SubPolicy(0.9, ""shearX"", 4, 0.3, ""solarize"", 3, fillcolor),\n            SubPolicy(0.8, ""shearY"", 8, 0.7, ""invert"", 4, fillcolor),\n            SubPolicy(0.9, ""equalize"", 5, 0.6, ""translateY"", 6, fillcolor),\n            SubPolicy(0.9, ""invert"", 4, 0.6, ""equalize"", 7, fillcolor),\n            SubPolicy(0.3, ""contrast"", 3, 0.8, ""rotate"", 4, fillcolor),\n\n            SubPolicy(0.8, ""invert"", 5, 0.0, ""translateY"", 2, fillcolor),\n            SubPolicy(0.7, ""shearY"", 6, 0.4, ""solarize"", 8, fillcolor),\n            SubPolicy(0.6, ""invert"", 4, 0.8, ""rotate"", 4, fillcolor),\n            SubPolicy(0.3, ""shearY"", 7, 0.9, ""translateX"", 3, fillcolor),\n            SubPolicy(0.1, ""shearX"", 6, 0.6, ""invert"", 5, fillcolor),\n\n            SubPolicy(0.7, ""solarize"", 2, 0.6, ""translateY"", 7, fillcolor),\n            SubPolicy(0.8, ""shearY"", 4, 0.8, ""invert"", 8, fillcolor),\n            SubPolicy(0.7, ""shearX"", 9, 0.8, ""translateY"", 3, fillcolor),\n            SubPolicy(0.8, ""shearY"", 5, 0.7, ""autocontrast"", 3, fillcolor),\n            SubPolicy(0.7, ""shearX"", 2, 0.1, ""invert"", 5, fillcolor)\n        ]\n\n    def __call__(self, img):\n        policy_idx = random.randint(0, len(self.policies) - 1)\n        return self.policies[policy_idx](img)\n\n    def __repr__(self):\n        return ""AutoAugment SVHN Policy""\n\n\nclass SubPolicy(object):\n    def __init__(self, p1, operation1, magnitude_idx1, p2, operation2, magnitude_idx2, fillcolor=(128, 128, 128)):\n        ranges = {\n            ""shearX"": np.linspace(0, 0.3, 10),\n            ""shearY"": np.linspace(0, 0.3, 10),\n            ""translateX"": np.linspace(0, 150 / 331, 10),\n            ""translateY"": np.linspace(0, 150 / 331, 10),\n            ""rotate"": np.linspace(0, 30, 10),\n            ""color"": np.linspace(0.0, 0.9, 10),\n            ""posterize"": np.round(np.linspace(8, 4, 10), 0).astype(np.int),\n            ""solarize"": np.linspace(256, 0, 10),\n            ""contrast"": np.linspace(0.0, 0.9, 10),\n            ""sharpness"": np.linspace(0.0, 0.9, 10),\n            ""brightness"": np.linspace(0.0, 0.9, 10),\n            ""autocontrast"": [0] * 10,\n            ""equalize"": [0] * 10,\n            ""invert"": [0] * 10\n        }\n\n        # from https://stackoverflow.com/questions/5252170/specify-image-filling-color-when-rotating-in-python-with-pil-and-setting-expand\n        def rotate_with_fill(img, magnitude):\n            rot = img.convert(""RGBA"").rotate(magnitude)\n            return Image.composite(rot, Image.new(""RGBA"", rot.size, (128,) * 4), rot).convert(img.mode)\n\n        func = {\n            ""shearX"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, magnitude * random.choice([-1, 1]), 0, 0, 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            ""shearY"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, magnitude * random.choice([-1, 1]), 1, 0),\n                Image.BICUBIC, fillcolor=fillcolor),\n            ""translateX"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, magnitude * img.size[0] * random.choice([-1, 1]), 0, 1, 0),\n                fillcolor=fillcolor),\n            ""translateY"": lambda img, magnitude: img.transform(\n                img.size, Image.AFFINE, (1, 0, 0, 0, 1, magnitude * img.size[1] * random.choice([-1, 1])),\n                fillcolor=fillcolor),\n            ""rotate"": lambda img, magnitude: rotate_with_fill(img, magnitude),\n            ""color"": lambda img, magnitude: ImageEnhance.Color(img).enhance(1 + magnitude * random.choice([-1, 1])),\n            ""posterize"": lambda img, magnitude: ImageOps.posterize(img, magnitude),\n            ""solarize"": lambda img, magnitude: ImageOps.solarize(img, magnitude),\n            ""contrast"": lambda img, magnitude: ImageEnhance.Contrast(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""sharpness"": lambda img, magnitude: ImageEnhance.Sharpness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""brightness"": lambda img, magnitude: ImageEnhance.Brightness(img).enhance(\n                1 + magnitude * random.choice([-1, 1])),\n            ""autocontrast"": lambda img, magnitude: ImageOps.autocontrast(img),\n            ""equalize"": lambda img, magnitude: ImageOps.equalize(img),\n            ""invert"": lambda img, magnitude: ImageOps.invert(img)\n        }\n\n        self.p1 = p1\n        self.operation1 = func[operation1]\n        self.magnitude1 = ranges[operation1][magnitude_idx1]\n        self.p2 = p2\n        self.operation2 = func[operation2]\n        self.magnitude2 = ranges[operation2][magnitude_idx2]\n\n    def __call__(self, img):\n        if random.random() < self.p1:\n            img = self.operation1(img, self.magnitude1)\n        if random.random() < self.p2:\n            img = self.operation2(img, self.magnitude2)\n        return img\n\n\ndef fast_collate(batch):\n    imgs = [img[0] for img in batch]\n    targets = torch.tensor([target[1] for target in batch], dtype=torch.int64)\n    w = imgs[0].size[0]\n    h = imgs[0].size[1]\n    tensor = torch.zeros((len(imgs), 3, h, w), dtype=torch.uint8)\n    for i, img in enumerate(imgs):\n        nump_array = np.asarray(img, dtype=np.uint8)\n        if (nump_array.ndim < 3):\n            nump_array = np.expand_dims(nump_array, axis=-1)\n        nump_array = np.rollaxis(nump_array, 2)\n\n        tensor[i] += torch.from_numpy(nump_array)\n\n    return tensor, targets\n\n\ndef mixup_data(x, y, alpha=1.0, use_cuda=True):\n    \'\'\'Returns mixed inputs, pairs of targets, and lambda\'\'\'\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    batch_size = x.size()[0]\n    if use_cuda:\n        index = torch.randperm(batch_size).cuda()\n    else:\n        index = torch.randperm(batch_size)\n\n    mixed_x = lam * x + (1 - lam) * x[index, :]\n    y_a, y_b = y, y[index]\n    return mixed_x, y_a, y_b, lam\n\n\ndef mixup_criterion(criterion, pred, y_a, y_b, lam):\n    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n'"
examples/nas/cdarts/datasets/imagenet.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nimport numpy as np\nimport torch\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\n\nfrom datasets.data_utils import ImageNetPolicy\nfrom datasets.data_utils import SubsetDistributedSampler\n\n\ndef _imagenet_dataset(config):\n    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    train_dir = os.path.join(config.data_dir, ""train"")\n    test_dir = os.path.join(config.data_dir, ""val"")\n    if hasattr(config, ""use_aa"") and config.use_aa:\n        train_data = dset.ImageFolder(\n            train_dir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                ImageNetPolicy(),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n    else:\n        train_data = dset.ImageFolder(\n            train_dir,\n            transforms.Compose([\n                transforms.RandomResizedCrop(224),\n                transforms.RandomHorizontalFlip(),\n                transforms.ColorJitter(\n                    brightness=0.4,\n                    contrast=0.4,\n                    saturation=0.4,\n                    hue=0.2),\n                transforms.ToTensor(),\n                normalize,\n            ]))\n\n    test_data = dset.ImageFolder(\n        test_dir,\n        transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            normalize,\n        ]))\n\n    return train_data, test_data\n\n\ndef get_search_datasets(config):\n    train_data, test_data = _imagenet_dataset(config)\n    num_train = len(train_data)\n    indices = list(range(num_train))\n    split_mid = int(np.floor(0.5 * num_train))\n\n    if config.distributed:\n        train_sampler = SubsetDistributedSampler(train_data, indices[:split_mid])\n        valid_sampler = SubsetDistributedSampler(train_data, indices[split_mid:num_train])\n    else:\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split_mid])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split_mid:num_train])\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=train_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    valid_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=valid_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    return [train_loader, valid_loader], [train_sampler, valid_sampler]\n\n\ndef get_augment_datasets(config):\n    train_data, test_data = _imagenet_dataset(config)\n    if config.distributed:\n        train_sampler = torch.utils.data.distributed.DistributedSampler(train_data)\n        test_sampler = torch.utils.data.distributed.DistributedSampler(test_data)\n    else:\n        train_sampler = test_sampler = None\n\n    train_loader = torch.utils.data.DataLoader(\n        train_data, batch_size=config.batch_size,\n        sampler=train_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    test_loader = torch.utils.data.DataLoader(\n        test_data, batch_size=config.batch_size,\n        sampler=test_sampler,\n        pin_memory=True, num_workers=config.workers)\n\n    return [train_loader, test_loader], [train_sampler, test_sampler]\n'"
examples/trials/cifar10_pytorch/models/__init__.py,0,b'from .vgg import *\nfrom .densenet import *\nfrom .dpn import *\nfrom .googlenet import *\nfrom .lenet import *\nfrom .mobilenet import *\nfrom .pnasnet import *\nfrom .resnet import *\nfrom .senet import *\nfrom .shufflenet import *\n\n'
examples/trials/cifar10_pytorch/models/densenet.py,4,"b""'''DenseNet in PyTorch.'''\nimport math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, growth_rate):\n        super(Bottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, 4*growth_rate, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(4*growth_rate)\n        self.conv2 = nn.Conv2d(4*growth_rate, growth_rate, kernel_size=3, padding=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv1(F.relu(self.bn1(x)))\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = torch.cat([out,x], 1)\n        return out\n\n\nclass Transition(nn.Module):\n    def __init__(self, in_planes, out_planes):\n        super(Transition, self).__init__()\n        self.bn = nn.BatchNorm2d(in_planes)\n        self.conv = nn.Conv2d(in_planes, out_planes, kernel_size=1, bias=False)\n\n    def forward(self, x):\n        out = self.conv(F.relu(self.bn(x)))\n        out = F.avg_pool2d(out, 2)\n        return out\n\n\nclass DenseNet(nn.Module):\n    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_classes=10):\n        super(DenseNet, self).__init__()\n        self.growth_rate = growth_rate\n\n        num_planes = 2*growth_rate\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, padding=1, bias=False)\n\n        self.dense1 = self._make_dense_layers(block, num_planes, nblocks[0])\n        num_planes += nblocks[0]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans1 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n        num_planes += nblocks[1]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans2 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n        num_planes += nblocks[2]*growth_rate\n        out_planes = int(math.floor(num_planes*reduction))\n        self.trans3 = Transition(num_planes, out_planes)\n        num_planes = out_planes\n\n        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n        num_planes += nblocks[3]*growth_rate\n\n        self.bn = nn.BatchNorm2d(num_planes)\n        self.linear = nn.Linear(num_planes, num_classes)\n\n    def _make_dense_layers(self, block, in_planes, nblock):\n        layers = []\n        for i in range(nblock):\n            layers.append(block(in_planes, self.growth_rate))\n            in_planes += self.growth_rate\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.trans1(self.dense1(out))\n        out = self.trans2(self.dense2(out))\n        out = self.trans3(self.dense3(out))\n        out = self.dense4(out)\n        out = F.avg_pool2d(F.relu(self.bn(out)), 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\ndef DenseNet121():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n\ndef DenseNet169():\n    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n\ndef DenseNet201():\n    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n\ndef DenseNet161():\n    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n\ndef densenet_cifar():\n    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)\n\ndef test():\n    net = densenet_cifar()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/dpn.py,4,"b""'''Dual Path Networks in PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, last_planes, in_planes, out_planes, dense_depth, stride, first_layer):\n        super(Bottleneck, self).__init__()\n        self.out_planes = out_planes\n        self.dense_depth = dense_depth\n\n        self.conv1 = nn.Conv2d(last_planes, in_planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=32, bias=False)\n        self.bn2 = nn.BatchNorm2d(in_planes)\n        self.conv3 = nn.Conv2d(in_planes, out_planes+dense_depth, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes+dense_depth)\n\n        self.shortcut = nn.Sequential()\n        if first_layer:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(last_planes, out_planes+dense_depth, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(out_planes+dense_depth)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        x = self.shortcut(x)\n        d = self.out_planes\n        out = torch.cat([x[:,:d,:,:]+out[:,:d,:,:], x[:,d:,:,:], out[:,d:,:,:]], 1)\n        out = F.relu(out)\n        return out\n\n\nclass DPN(nn.Module):\n    def __init__(self, cfg):\n        super(DPN, self).__init__()\n        in_planes, out_planes = cfg['in_planes'], cfg['out_planes']\n        num_blocks, dense_depth = cfg['num_blocks'], cfg['dense_depth']\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.last_planes = 64\n        self.layer1 = self._make_layer(in_planes[0], out_planes[0], num_blocks[0], dense_depth[0], stride=1)\n        self.layer2 = self._make_layer(in_planes[1], out_planes[1], num_blocks[1], dense_depth[1], stride=2)\n        self.layer3 = self._make_layer(in_planes[2], out_planes[2], num_blocks[2], dense_depth[2], stride=2)\n        self.layer4 = self._make_layer(in_planes[3], out_planes[3], num_blocks[3], dense_depth[3], stride=2)\n        self.linear = nn.Linear(out_planes[3]+(num_blocks[3]+1)*dense_depth[3], 10)\n\n    def _make_layer(self, in_planes, out_planes, num_blocks, dense_depth, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for i,stride in enumerate(strides):\n            layers.append(Bottleneck(self.last_planes, in_planes, out_planes, dense_depth, stride, i==0))\n            self.last_planes = out_planes + (i+2) * dense_depth\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef DPN26():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (2,2,2,2),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\ndef DPN92():\n    cfg = {\n        'in_planes': (96,192,384,768),\n        'out_planes': (256,512,1024,2048),\n        'num_blocks': (3,4,20,3),\n        'dense_depth': (16,32,24,128)\n    }\n    return DPN(cfg)\n\n\ndef test():\n    net = DPN92()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/googlenet.py,4,"b""'''GoogLeNet with PyTorch.'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Inception(nn.Module):\n    def __init__(self, in_planes, n1x1, n3x3red, n3x3, n5x5red, n5x5, pool_planes):\n        super(Inception, self).__init__()\n        # 1x1 conv branch\n        self.b1 = nn.Sequential(\n            nn.Conv2d(in_planes, n1x1, kernel_size=1),\n            nn.BatchNorm2d(n1x1),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 3x3 conv branch\n        self.b2 = nn.Sequential(\n            nn.Conv2d(in_planes, n3x3red, kernel_size=1),\n            nn.BatchNorm2d(n3x3red),\n            nn.ReLU(True),\n            nn.Conv2d(n3x3red, n3x3, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n3x3),\n            nn.ReLU(True),\n        )\n\n        # 1x1 conv -> 5x5 conv branch\n        self.b3 = nn.Sequential(\n            nn.Conv2d(in_planes, n5x5red, kernel_size=1),\n            nn.BatchNorm2d(n5x5red),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5red, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n            nn.Conv2d(n5x5, n5x5, kernel_size=3, padding=1),\n            nn.BatchNorm2d(n5x5),\n            nn.ReLU(True),\n        )\n\n        # 3x3 pool -> 1x1 conv branch\n        self.b4 = nn.Sequential(\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.Conv2d(in_planes, pool_planes, kernel_size=1),\n            nn.BatchNorm2d(pool_planes),\n            nn.ReLU(True),\n        )\n\n    def forward(self, x):\n        y1 = self.b1(x)\n        y2 = self.b2(x)\n        y3 = self.b3(x)\n        y4 = self.b4(x)\n        return torch.cat([y1,y2,y3,y4], 1)\n\n\nclass GoogLeNet(nn.Module):\n    def __init__(self):\n        super(GoogLeNet, self).__init__()\n        self.pre_layers = nn.Sequential(\n            nn.Conv2d(3, 192, kernel_size=3, padding=1),\n            nn.BatchNorm2d(192),\n            nn.ReLU(True),\n        )\n\n        self.a3 = Inception(192,  64,  96, 128, 16, 32, 32)\n        self.b3 = Inception(256, 128, 128, 192, 32, 96, 64)\n\n        self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)\n\n        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n\n        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n\n        self.avgpool = nn.AvgPool2d(8, stride=1)\n        self.linear = nn.Linear(1024, 10)\n\n    def forward(self, x):\n        out = self.pre_layers(x)\n        out = self.a3(out)\n        out = self.b3(out)\n        out = self.maxpool(out)\n        out = self.a4(out)\n        out = self.b4(out)\n        out = self.c4(out)\n        out = self.d4(out)\n        out = self.e4(out)\n        out = self.maxpool(out)\n        out = self.a5(out)\n        out = self.b5(out)\n        out = self.avgpool(out)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = GoogLeNet()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/lenet.py,2,"b""'''LeNet in PyTorch.'''\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1   = nn.Linear(16*5*5, 120)\n        self.fc2   = nn.Linear(120, 84)\n        self.fc3   = nn.Linear(84, 10)\n\n    def forward(self, x):\n        out = F.relu(self.conv1(x))\n        out = F.max_pool2d(out, 2)\n        out = F.relu(self.conv2(out))\n        out = F.max_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = F.relu(self.fc1(out))\n        out = F.relu(self.fc2(out))\n        out = self.fc3(out)\n        return out\n"""
examples/trials/cifar10_pytorch/models/mobilenet.py,3,"b'\'\'\'MobileNet in PyTorch.\n\nSee the paper ""MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications""\nfor more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'Depthwise conv + Pointwise conv\'\'\'\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(Block, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=stride, padding=1, groups=in_planes, bias=False)\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv2 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        return out\n\n\nclass MobileNet(nn.Module):\n    # (128,2) means conv planes=128, conv stride=2, by default conv stride=1\n    cfg = [64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n\n    def __init__(self, num_classes=10):\n        super(MobileNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.linear = nn.Linear(1024, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for x in self.cfg:\n            out_planes = x if isinstance(x, int) else x[0]\n            stride = 1 if isinstance(x, int) else x[1]\n            layers.append(Block(in_planes, out_planes, stride))\n            in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.avg_pool2d(out, 2)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNet()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
examples/trials/cifar10_pytorch/models/mobilenetv2.py,3,"b'\'\'\'MobileNetV2 in PyTorch.\n\nSee the paper ""Inverted Residuals and Linear Bottlenecks:\nMobile Networks for Classification, Detection and Segmentation"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'expand + depthwise + pointwise\'\'\'\n    def __init__(self, in_planes, out_planes, expansion, stride):\n        super(Block, self).__init__()\n        self.stride = stride\n\n        planes = expansion * in_planes\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, groups=planes, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 1 and in_planes != out_planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False),\n                nn.BatchNorm2d(out_planes),\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out = out + self.shortcut(x) if self.stride==1 else out\n        return out\n\n\nclass MobileNetV2(nn.Module):\n    # (expansion, out_planes, num_blocks, stride)\n    cfg = [(1,  16, 1, 1),\n           (6,  24, 2, 1),  # NOTE: change stride 2 -> 1 for CIFAR10\n           (6,  32, 3, 2),\n           (6,  64, 4, 2),\n           (6,  96, 3, 1),\n           (6, 160, 3, 2),\n           (6, 320, 1, 1)]\n\n    def __init__(self, num_classes=10):\n        super(MobileNetV2, self).__init__()\n        # NOTE: change conv1 stride 2 -> 1 for CIFAR10\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(32)\n        self.layers = self._make_layers(in_planes=32)\n        self.conv2 = nn.Conv2d(320, 1280, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(1280)\n        self.linear = nn.Linear(1280, num_classes)\n\n    def _make_layers(self, in_planes):\n        layers = []\n        for expansion, out_planes, num_blocks, stride in self.cfg:\n            strides = [stride] + [1]*(num_blocks-1)\n            for stride in strides:\n                layers.append(Block(in_planes, out_planes, expansion, stride))\n                in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layers(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        # NOTE: change pooling kernel_size 7 -> 4 for CIFAR10\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef test():\n    net = MobileNetV2()\n    x = torch.randn(2,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n'"
examples/trials/cifar10_pytorch/models/pnasnet.py,4,"b""'''PNASNet in PyTorch.\n\nPaper: Progressive Neural Architecture Search\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass SepConv(nn.Module):\n    '''Separable Convolution.'''\n    def __init__(self, in_planes, out_planes, kernel_size, stride):\n        super(SepConv, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, out_planes,\n                               kernel_size, stride,\n                               padding=(kernel_size-1)//2,\n                               bias=False, groups=in_planes)\n        self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        return self.bn1(self.conv1(x))\n\n\nclass CellA(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellA, self).__init__()\n        self.stride = stride\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        if stride==2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        y1 = self.sep_conv1(x)\n        y2 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride==2:\n            y2 = self.bn1(self.conv1(y2))\n        return F.relu(y1+y2)\n\nclass CellB(nn.Module):\n    def __init__(self, in_planes, out_planes, stride=1):\n        super(CellB, self).__init__()\n        self.stride = stride\n        # Left branch\n        self.sep_conv1 = SepConv(in_planes, out_planes, kernel_size=7, stride=stride)\n        self.sep_conv2 = SepConv(in_planes, out_planes, kernel_size=3, stride=stride)\n        # Right branch\n        self.sep_conv3 = SepConv(in_planes, out_planes, kernel_size=5, stride=stride)\n        if stride==2:\n            self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n            self.bn1 = nn.BatchNorm2d(out_planes)\n        # Reduce channels\n        self.conv2 = nn.Conv2d(2*out_planes, out_planes, kernel_size=1, stride=1, padding=0, bias=False)\n        self.bn2 = nn.BatchNorm2d(out_planes)\n\n    def forward(self, x):\n        # Left branch\n        y1 = self.sep_conv1(x)\n        y2 = self.sep_conv2(x)\n        # Right branch\n        y3 = F.max_pool2d(x, kernel_size=3, stride=self.stride, padding=1)\n        if self.stride==2:\n            y3 = self.bn1(self.conv1(y3))\n        y4 = self.sep_conv3(x)\n        # Concat & reduce channels\n        b1 = F.relu(y1+y2)\n        b2 = F.relu(y3+y4)\n        y = torch.cat([b1,b2], 1)\n        return F.relu(self.bn2(self.conv2(y)))\n\nclass PNASNet(nn.Module):\n    def __init__(self, cell_type, num_cells, num_planes):\n        super(PNASNet, self).__init__()\n        self.in_planes = num_planes\n        self.cell_type = cell_type\n\n        self.conv1 = nn.Conv2d(3, num_planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(num_planes)\n\n        self.layer1 = self._make_layer(num_planes, num_cells=6)\n        self.layer2 = self._downsample(num_planes*2)\n        self.layer3 = self._make_layer(num_planes*2, num_cells=6)\n        self.layer4 = self._downsample(num_planes*4)\n        self.layer5 = self._make_layer(num_planes*4, num_cells=6)\n\n        self.linear = nn.Linear(num_planes*4, 10)\n\n    def _make_layer(self, planes, num_cells):\n        layers = []\n        for _ in range(num_cells):\n            layers.append(self.cell_type(self.in_planes, planes, stride=1))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def _downsample(self, planes):\n        layer = self.cell_type(self.in_planes, planes, stride=2)\n        self.in_planes = planes\n        return layer\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = self.layer5(out)\n        out = F.avg_pool2d(out, 8)\n        out = self.linear(out.view(out.size(0), -1))\n        return out\n\n\ndef PNASNetA():\n    return PNASNet(CellA, num_cells=6, num_planes=44)\n\ndef PNASNetB():\n    return PNASNet(CellB, num_cells=6, num_planes=32)\n\n\ndef test():\n    net = PNASNetB()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/preact_resnet.py,3,"b""'''Pre-activation ResNet in PyTorch.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Identity Mappings in Deep Residual Networks. arXiv:1603.05027\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass PreActBlock(nn.Module):\n    '''Pre-activation version of the BasicBlock.'''\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out += shortcut\n        return out\n\n\nclass PreActBottleneck(nn.Module):\n    '''Pre-activation version of the original Bottleneck module.'''\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBottleneck, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n        out = self.conv3(F.relu(self.bn3(out)))\n        out += shortcut\n        return out\n\n\nclass PreActResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(PreActResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef PreActResNet18():\n    return PreActResNet(PreActBlock, [2,2,2,2])\n\ndef PreActResNet34():\n    return PreActResNet(PreActBlock, [3,4,6,3])\n\ndef PreActResNet50():\n    return PreActResNet(PreActBottleneck, [3,4,6,3])\n\ndef PreActResNet101():\n    return PreActResNet(PreActBottleneck, [3,4,23,3])\n\ndef PreActResNet152():\n    return PreActResNet(PreActBottleneck, [3,8,36,3])\n\n\ndef test():\n    net = PreActResNet18()\n    y = net((torch.randn(1,3,32,32)))\n    print(y.size())\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/resnet.py,3,"b""'''ResNet in PyTorch.\n\nFor Pre-activation ResNet, see 'preact_resnet.py'.\n\nReference:\n[1] Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun\n    Deep Residual Learning for Image Recognition. arXiv:1512.03385\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, in_planes, planes, stride=1):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*planes)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(ResNet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512*block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes * block.expansion\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNet18():\n    return ResNet(BasicBlock, [2,2,2,2])\n\ndef ResNet34():\n    return ResNet(BasicBlock, [3,4,6,3])\n\ndef ResNet50():\n    return ResNet(Bottleneck, [3,4,6,3])\n\ndef ResNet101():\n    return ResNet(Bottleneck, [3,4,23,3])\n\ndef ResNet152():\n    return ResNet(Bottleneck, [3,8,36,3])\n\n\ndef test():\n    net = ResNet18()\n    y = net(torch.randn(1,3,32,32))\n    print(y.size())\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/resnext.py,3,"b'\'\'\'ResNeXt in PyTorch.\n\nSee the paper ""Aggregated Residual Transformations for Deep Neural Networks"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Block(nn.Module):\n    \'\'\'Grouped convolution block.\'\'\'\n    expansion = 2\n\n    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n        super(Block, self).__init__()\n        group_width = cardinality * bottleneck_width\n        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(group_width)\n        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n        self.bn2 = nn.BatchNorm2d(group_width)\n        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != self.expansion*group_width:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(self.expansion*group_width)\n            )\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass ResNeXt(nn.Module):\n    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n        super(ResNeXt, self).__init__()\n        self.cardinality = cardinality\n        self.bottleneck_width = bottleneck_width\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(num_blocks[0], 1)\n        self.layer2 = self._make_layer(num_blocks[1], 2)\n        self.layer3 = self._make_layer(num_blocks[2], 2)\n        # self.layer4 = self._make_layer(num_blocks[3], 2)\n        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n\n    def _make_layer(self, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n        # Increase bottleneck_width by 2 after each stage.\n        self.bottleneck_width *= 2\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        # out = self.layer4(out)\n        out = F.avg_pool2d(out, 8)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ResNeXt29_2x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)\n\ndef ResNeXt29_4x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=4, bottleneck_width=64)\n\ndef ResNeXt29_8x64d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=8, bottleneck_width=64)\n\ndef ResNeXt29_32x4d():\n    return ResNeXt(num_blocks=[3,3,3], cardinality=32, bottleneck_width=4)\n\ndef test_resnext():\n    net = ResNeXt29_2x64d()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test_resnext()\n'"
examples/trials/cifar10_pytorch/models/senet.py,3,"b""'''SENet in PyTorch.\n\nSENet is the winner of ImageNet-2017. The paper is not released yet.\n'''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n\n        self.shortcut = nn.Sequential()\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)  # Use nn.Conv2d instead of nn.Linear\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.bn2(self.conv2(out))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w  # New broadcasting feature from v0.2!\n\n        out += self.shortcut(x)\n        out = F.relu(out)\n        return out\n\n\nclass PreActBlock(nn.Module):\n    def __init__(self, in_planes, planes, stride=1):\n        super(PreActBlock, self).__init__()\n        self.bn1 = nn.BatchNorm2d(in_planes)\n        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n\n        if stride != 1 or in_planes != planes:\n            self.shortcut = nn.Sequential(\n                nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)\n            )\n\n        # SE layers\n        self.fc1 = nn.Conv2d(planes, planes//16, kernel_size=1)\n        self.fc2 = nn.Conv2d(planes//16, planes, kernel_size=1)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(x))\n        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n        out = self.conv1(out)\n        out = self.conv2(F.relu(self.bn2(out)))\n\n        # Squeeze\n        w = F.avg_pool2d(out, out.size(2))\n        w = F.relu(self.fc1(w))\n        w = F.sigmoid(self.fc2(w))\n        # Excitation\n        out = out * w\n\n        out += shortcut\n        return out\n\n\nclass SENet(nn.Module):\n    def __init__(self, block, num_blocks, num_classes=10):\n        super(SENet, self).__init__()\n        self.in_planes = 64\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.layer1 = self._make_layer(block,  64, num_blocks[0], stride=1)\n        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n        self.linear = nn.Linear(512, num_classes)\n\n    def _make_layer(self, block, planes, num_blocks, stride):\n        strides = [stride] + [1]*(num_blocks-1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.in_planes, planes, stride))\n            self.in_planes = planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = self.layer4(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef SENet18():\n    return SENet(PreActBlock, [2,2,2,2])\n\n\ndef test():\n    net = SENet18()\n    y = net(torch.randn(1,3,32,32))\n    print(y.size())\n\n# test()\n"""
examples/trials/cifar10_pytorch/models/shufflenet.py,4,"b'\'\'\'ShuffleNet in PyTorch.\n\nSee the paper ""ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"" for more details.\n\'\'\'\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass ShuffleBlock(nn.Module):\n    def __init__(self, groups):\n        super(ShuffleBlock, self).__init__()\n        self.groups = groups\n\n    def forward(self, x):\n        \'\'\'Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]\'\'\'\n        N,C,H,W = x.size()\n        g = self.groups\n        return x.view(N,g,C/g,H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)\n\n\nclass Bottleneck(nn.Module):\n    def __init__(self, in_planes, out_planes, stride, groups):\n        super(Bottleneck, self).__init__()\n        self.stride = stride\n\n        mid_planes = out_planes/4\n        g = 1 if in_planes==24 else groups\n        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n        self.bn1 = nn.BatchNorm2d(mid_planes)\n        self.shuffle1 = ShuffleBlock(groups=g)\n        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n        self.bn2 = nn.BatchNorm2d(mid_planes)\n        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n        self.bn3 = nn.BatchNorm2d(out_planes)\n\n        self.shortcut = nn.Sequential()\n        if stride == 2:\n            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.shuffle1(out)\n        out = F.relu(self.bn2(self.conv2(out)))\n        out = self.bn3(self.conv3(out))\n        res = self.shortcut(x)\n        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n        return out\n\n\nclass ShuffleNet(nn.Module):\n    def __init__(self, cfg):\n        super(ShuffleNet, self).__init__()\n        out_planes = cfg[\'out_planes\']\n        num_blocks = cfg[\'num_blocks\']\n        groups = cfg[\'groups\']\n\n        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(24)\n        self.in_planes = 24\n        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n        self.linear = nn.Linear(out_planes[2], 10)\n\n    def _make_layer(self, out_planes, num_blocks, groups):\n        layers = []\n        for i in range(num_blocks):\n            stride = 2 if i == 0 else 1\n            cat_planes = self.in_planes if i == 0 else 0\n            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n            self.in_planes = out_planes\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = F.relu(self.bn1(self.conv1(x)))\n        out = self.layer1(out)\n        out = self.layer2(out)\n        out = self.layer3(out)\n        out = F.avg_pool2d(out, 4)\n        out = out.view(out.size(0), -1)\n        out = self.linear(out)\n        return out\n\n\ndef ShuffleNetG2():\n    cfg = {\n        \'out_planes\': [200,400,800],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 2\n    }\n    return ShuffleNet(cfg)\n\ndef ShuffleNetG3():\n    cfg = {\n        \'out_planes\': [240,480,960],\n        \'num_blocks\': [4,8,4],\n        \'groups\': 3\n    }\n    return ShuffleNet(cfg)\n\n\ndef test():\n    net = ShuffleNetG2()\n    x = torch.randn(1,3,32,32)\n    y = net(x)\n    print(y)\n\n# test()\n'"
examples/trials/cifar10_pytorch/models/vgg.py,3,"b""'''VGG11/13/16/19 in Pytorch.'''\nimport torch\nimport torch.nn as nn\n\n\ncfg = {\n    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n}\n\n\nclass VGG(nn.Module):\n    def __init__(self, vgg_name):\n        super(VGG, self).__init__()\n        self.features = self._make_layers(cfg[vgg_name])\n        self.classifier = nn.Linear(512, 10)\n\n    def forward(self, x):\n        out = self.features(x)\n        out = out.view(out.size(0), -1)\n        out = self.classifier(out)\n        return out\n\n    def _make_layers(self, cfg):\n        layers = []\n        in_channels = 3\n        for x in cfg:\n            if x == 'M':\n                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n            else:\n                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n                           nn.BatchNorm2d(x),\n                           nn.ReLU(inplace=True)]\n                in_channels = x\n        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n        return nn.Sequential(*layers)\n\n\ndef test():\n    net = VGG('VGG11')\n    x = torch.randn(2,3,32,32)\n    y = net(x)\n    print(y.size())\n\n# test()\n"""
examples/trials/network_morphism/FashionMNIST/FashionMNIST_keras.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\nimport os\n\nimport tensorflow as tf\nimport keras\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.datasets import fashion_mnist\nfrom keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop\nfrom keras.utils import multi_gpu_model, to_categorical\nimport keras.backend.tensorflow_backend as KTF\n\nimport nni\nfrom nni.networkmorphism_tuner.graph import json_to_graph\n\n# set the logger format\nlog_format = ""%(asctime)s %(message)s""\nlogging.basicConfig(\n    filename=""networkmorphism.log"",\n    filemode=""a"",\n    level=logging.INFO,\n    format=log_format,\n    datefmt=""%m/%d %I:%M:%S %p"",\n)\n# set the logger format\nlogger = logging.getLogger(""FashionMNIST-network-morphism-keras"")\n\n\n# restrict gpu usage background\nconfig = tf.ConfigProto()\n# pylint: disable=E1101,W0603\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nKTF.set_session(sess)\n\n\ndef get_args():\n    """""" get args from command line\n    """"""\n    parser = argparse.ArgumentParser(""fashion_mnist"")\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""batch size"")\n    parser.add_argument(""--optimizer"", type=str, default=""SGD"", help=""optimizer"")\n    parser.add_argument(""--epochs"", type=int, default=200, help=""epoch limit"")\n    parser.add_argument(\n        ""--learning_rate"", type=float, default=0.001, help=""learning rate""\n    )\n    parser.add_argument(\n        ""--weight_decay"",\n        type=float,\n        default=1e-5,\n        help=""weight decay of the learning rate"",\n    )\n    return parser.parse_args()\n\n\ntrainloader = None\ntestloader = None\nnet = None\nargs = get_args()\nTENSORBOARD_DIR = os.environ[""NNI_OUTPUT_DIR""]\n\n\ndef build_graph_from_json(ir_model_json):\n    """"""build model from json representation\n    """"""\n    graph = json_to_graph(ir_model_json)\n    logging.debug(graph.operation_history)\n    model = graph.produce_keras_model()\n    return model\n\n\ndef parse_rev_args(receive_msg):\n    """""" parse reveive msgs to global variable\n    """"""\n    global trainloader\n    global testloader\n    global net\n\n    # Loading Data\n    logger.debug(""Preparing data.."")\n\n    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n    y_train = to_categorical(y_train, 10)\n    y_test = to_categorical(y_test, 10)\n    x_train = x_train.reshape(x_train.shape+(1,)).astype(""float32"")\n    x_test = x_test.reshape(x_test.shape+(1,)).astype(""float32"")\n    x_train /= 255.0\n    x_test /= 255.0\n    trainloader = (x_train, y_train)\n    testloader = (x_test, y_test)\n\n    # Model\n    logger.debug(""Building model.."")\n    net = build_graph_from_json(receive_msg)\n\n    # parallel model\n    try:\n        available_devices = os.environ[""CUDA_VISIBLE_DEVICES""]\n        gpus = len(available_devices.split("",""))\n        if gpus > 1:\n            net = multi_gpu_model(net, gpus)\n    except KeyError:\n        logger.debug(""parallel model not support in this config settings"")\n\n    if args.optimizer == ""SGD"":\n        optimizer = SGD(lr=args.learning_rate, momentum=0.9, decay=args.weight_decay)\n    if args.optimizer == ""Adadelta"":\n        optimizer = Adadelta(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adagrad"":\n        optimizer = Adagrad(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adam"":\n        optimizer = Adam(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adamax"":\n        optimizer = Adamax(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""RMSprop"":\n        optimizer = RMSprop(lr=args.learning_rate, decay=args.weight_decay)\n\n    # Compile the model\n    net.compile(\n        loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""accuracy""]\n    )\n    return 0\n\n\nclass SendMetrics(keras.callbacks.Callback):\n    """"""\n    Keras callback to send metrics to NNI framework\n    """"""\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Run on end of each epoch\n        """"""\n        if logs is None:\n            logs = dict()\n        logger.debug(logs)\n        # TensorFlow 2.0 API reference claims the key is `val_acc`, but in fact it\'s `val_accuracy`\n        if \'val_acc\' in logs:\n            nni.report_intermediate_result(logs[\'val_acc\'])\n        else:\n            nni.report_intermediate_result(logs[\'val_accuracy\'])\n\n\n# Training\ndef train_eval():\n    """""" train and eval the model\n    """"""\n\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(""Final result is: %.3f"", acc)\n    nni.report_final_result(acc)\n\n\nif __name__ == ""__main__"":\n    try:\n        # trial get next parameter from network morphism tuner\n        RCV_CONFIG = nni.get_next_parameter()\n        logger.debug(RCV_CONFIG)\n        parse_rev_args(RCV_CONFIG)\n        train_eval()\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/network_morphism/FashionMNIST/FashionMNIST_pytorch.py,6,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\nimport sys\n\nimport nni\nfrom nni.networkmorphism_tuner.graph import json_to_graph\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport utils\n\n\n# set the logger format\nlog_format = ""%(asctime)s %(message)s""\nlogging.basicConfig(\n    filename=""networkmorphism.log"",\n    filemode=""a"",\n    level=logging.INFO,\n    format=log_format,\n    datefmt=""%m/%d %I:%M:%S %p"",\n)\n# pylint: disable=W0603\n# set the logger format\nlogger = logging.getLogger(""FashionMNIST-network-morphism-pytorch"")\n\n\ndef get_args():\n    """""" get args from command line\n    """"""\n    parser = argparse.ArgumentParser(""FashionMNIST"")\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""batch size"")\n    parser.add_argument(""--optimizer"", type=str, default=""SGD"", help=""optimizer"")\n    parser.add_argument(""--epochs"", type=int, default=200, help=""epoch limit"")\n    parser.add_argument(\n        ""--learning_rate"", type=float, default=0.001, help=""learning rate""\n    )\n    parser.add_argument(""--cutout"", action=""store_true"", default=False, help=""use cutout"")\n    parser.add_argument(""--cutout_length"", type=int, default=8, help=""cutout length"")\n    parser.add_argument(\n        ""--model_path"", type=str, default=""./"", help=""Path to save the destination model""\n    )\n    return parser.parse_args()\n\n\ntrainloader = None\ntestloader = None\nnet = None\ncriterion = None\noptimizer = None\ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\nbest_acc = 0.0\nargs = get_args()\n\n\ndef build_graph_from_json(ir_model_json):\n    """"""build model from json representation\n    """"""\n    graph = json_to_graph(ir_model_json)\n    logging.debug(graph.operation_history)\n    model = graph.produce_torch_model()\n    return model\n\n\ndef parse_rev_args(receive_msg):\n    """""" parse reveive msgs to global variable\n    """"""\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    # Loading Data\n    logger.debug(""Preparing data.."")\n\n    raw_train_data = torchvision.datasets.FashionMNIST(\n        root=""./data"", train=True, download=True\n    )\n\n    dataset_mean, dataset_std = (\n        [raw_train_data.train_data.float().mean() / 255],\n        [raw_train_data.train_data.float().std() / 255],\n    )\n\n    transform_train, transform_test = utils.data_transforms_mnist(\n        args, dataset_mean, dataset_std\n    )\n\n    trainset = torchvision.datasets.FashionMNIST(\n        root=""./data"", train=True, download=True, transform=transform_train\n    )\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=args.batch_size, shuffle=True, num_workers=2\n    )\n\n    testset = torchvision.datasets.FashionMNIST(\n        root=""./data"", train=False, download=True, transform=transform_test\n    )\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=args.batch_size, shuffle=False, num_workers=2\n    )\n\n    # Model\n    logger.debug(""Building model.."")\n    net = build_graph_from_json(receive_msg)\n\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n\n    if args.optimizer == ""SGD"":\n        optimizer = optim.SGD(\n            net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4\n        )\n    if args.optimizer == ""Adadelta"":\n        optimizer = optim.Adadelta(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adagrad"":\n        optimizer = optim.Adagrad(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adam"":\n        optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adamax"":\n        optimizer = optim.Adamax(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""RMSprop"":\n        optimizer = optim.RMSprop(net.parameters(), lr=args.learning_rate)\n\n    return 0\n\n\n# Training\ndef train(epoch):\n    """""" train model on each epoch in trainset\n    """"""\n\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    logger.debug(""Epoch: %d"", epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        acc = 100.0 * correct / total\n\n        logger.debug(\n            ""Loss: %.3f | Acc: %.3f%% (%d/%d)"",\n            train_loss / (batch_idx + 1),\n            100.0 * correct / total,\n            correct,\n            total,\n        )\n\n    return acc\n\n\ndef test(epoch):\n    """""" eval model on each epoch in testset\n    """"""\n    global best_acc\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    logger.debug(""Eval on epoch: %d"", epoch)\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            acc = 100.0 * correct / total\n\n            logger.debug(\n                ""Loss: %.3f | Acc: %.3f%% (%d/%d)"",\n                test_loss / (batch_idx + 1),\n                100.0 * correct / total,\n                correct,\n                total,\n            )\n\n    acc = 100.0 * correct / total\n    if acc > best_acc:\n        best_acc = acc\n    return acc, best_acc\n\n\nif __name__ == ""__main__"":\n    try:\n        # trial get next parameter from network morphism tuner\n        RCV_CONFIG = nni.get_next_parameter()\n        logger.debug(RCV_CONFIG)\n\n        parse_rev_args(RCV_CONFIG)\n        train_acc = 0.0\n        best_acc = 0.0\n        early_stop = utils.EarlyStopping(mode=""max"")\n        for ep in range(args.epochs):\n            train_acc = train(ep)\n            test_acc, best_acc = test(ep)\n            nni.report_intermediate_result(test_acc)\n            logger.debug(test_acc)\n            if early_stop.step(test_acc):\n                break\n\n        # trial report best_acc to tuner\n        nni.report_final_result(best_acc)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/network_morphism/FashionMNIST/__init__.py,0,b''
examples/trials/network_morphism/FashionMNIST/utils.py,6,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torchvision.transforms as transforms\n\n\nclass EarlyStopping:\n    """""" EarlyStopping class to keep NN from overfitting\n    """"""\n\n    # pylint: disable=E0202\n    def __init__(self, mode=""min"", min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    def step(self, metrics):\n        """""" EarlyStopping step on each epoch\n        Arguments:\n            metrics {float} -- metric value\n        """"""\n\n        if self.best is None:\n            self.best = metrics\n            return False\n\n        if np.isnan(metrics):\n            return True\n\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            return True\n\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {""min"", ""max""}:\n            raise ValueError(""mode "" + mode + "" is unknown!"")\n        if not percentage:\n            if mode == ""min"":\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == ""max"":\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == ""min"":\n                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n            if mode == ""max"":\n                self.is_better = lambda a, best: a > best + (best * min_delta / 100)\n\n\nclass Cutout:\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        h_img, w_img = img.size(1), img.size(2)\n        mask = np.ones((h_img, w_img), np.float32)\n        y_img = np.random.randint(h_img)\n        x_img = np.random.randint(w_img)\n\n        y1_img = np.clip(y_img - self.length // 2, 0, h_img)\n        y2_img = np.clip(y_img + self.length // 2, 0, h_img)\n        x1_img = np.clip(x_img - self.length // 2, 0, w_img)\n        x2_img = np.clip(x_img + self.length // 2, 0, w_img)\n\n        mask[y1_img:y2_img, x1_img:x2_img] = 0.0\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n        return img\n\n\ndef data_transforms_cifar10(args):\n    """""" data_transforms for cifar10 dataset\n    """"""\n\n    cifar_mean = [0.49139968, 0.48215827, 0.44653124]\n    cifar_std = [0.24703233, 0.24348505, 0.26158768]\n\n    train_transform = transforms.Compose(\n        [\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(cifar_mean, cifar_std),\n        ]\n    )\n    if args.cutout:\n        train_transform.transforms.append(Cutout(args.cutout_length))\n\n    valid_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(cifar_mean, cifar_std)]\n    )\n    return train_transform, valid_transform\n\n\ndef data_transforms_mnist(args, mnist_mean=None, mnist_std=None):\n    """""" data_transforms for mnist dataset\n    """"""\n    if mnist_mean is None:\n        mnist_mean = [0.5]\n\n    if mnist_std is None:\n        mnist_std = [0.5]\n\n    train_transform = transforms.Compose(\n        [\n            transforms.RandomCrop(28, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mnist_mean, mnist_std),\n        ]\n    )\n    if args.cutout:\n        train_transform.transforms.append(Cutout(args.cutout_length))\n\n    valid_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(mnist_mean, mnist_std)]\n    )\n    return train_transform, valid_transform\n\n\ndef get_mean_and_std(dataset):\n    """"""Compute the mean and std value of dataset.""""""\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2\n    )\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(""==> Computing mean and std.."")\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    """"""Init layer parameters.""""""\n    for module in net.modules():\n        if isinstance(module, nn.Conv2d):\n            init.kaiming_normal(module.weight, mode=""fan_out"")\n            if module.bias:\n                init.constant(module.bias, 0)\n        elif isinstance(module, nn.BatchNorm2d):\n            init.constant(module.weight, 1)\n            init.constant(module.bias, 0)\n        elif isinstance(module, nn.Linear):\n            init.normal(module.weight, std=1e-3)\n            if module.bias:\n                init.constant(module.bias, 0)\n'"
examples/trials/network_morphism/cifar10/__init__.py,0,b''
examples/trials/network_morphism/cifar10/cifar10_keras.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\nimport os\n\nimport tensorflow as tf\nimport keras\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.datasets import cifar10\nfrom keras.optimizers import SGD, Adadelta, Adagrad, Adam, Adamax, RMSprop\nfrom keras.utils import multi_gpu_model, to_categorical\nimport keras.backend.tensorflow_backend as KTF\n\nimport nni\nfrom nni.networkmorphism_tuner.graph import json_to_graph\n\n# set the logger format\nlog_format = ""%(asctime)s %(message)s""\nlogging.basicConfig(\n    filename=""networkmorphism.log"",\n    filemode=""a"",\n    level=logging.INFO,\n    format=log_format,\n    datefmt=""%m/%d %I:%M:%S %p"",\n)\n# set the logger format\nlogger = logging.getLogger(""cifar10-network-morphism-keras"")\n\n\n# restrict gpu usage background\nconfig = tf.ConfigProto()\n# pylint: disable=E1101,W0603\nconfig.gpu_options.allow_growth = True\nsess = tf.Session(config=config)\n\nKTF.set_session(sess)\n\n\ndef get_args():\n    """""" get args from command line\n    """"""\n    parser = argparse.ArgumentParser(""cifar10"")\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""batch size"")\n    parser.add_argument(""--optimizer"", type=str, default=""SGD"", help=""optimizer"")\n    parser.add_argument(""--epochs"", type=int, default=200, help=""epoch limit"")\n    parser.add_argument(\n        ""--learning_rate"", type=float, default=0.001, help=""learning rate""\n    )\n    parser.add_argument(\n        ""--weight_decay"",\n        type=float,\n        default=1e-5,\n        help=""weight decay of the learning rate"",\n    )\n    return parser.parse_args()\n\n\ntrainloader = None\ntestloader = None\nnet = None\nargs = get_args()\nTENSORBOARD_DIR = os.environ[""NNI_OUTPUT_DIR""]\n\n\ndef build_graph_from_json(ir_model_json):\n    """"""build model from json representation\n    """"""\n    graph = json_to_graph(ir_model_json)\n    logging.debug(graph.operation_history)\n    model = graph.produce_keras_model()\n    return model\n\n\ndef parse_rev_args(receive_msg):\n    """""" parse reveive msgs to global variable\n    """"""\n    global trainloader\n    global testloader\n    global net\n\n    # Loading Data\n    logger.debug(""Preparing data.."")\n\n    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n    y_train = to_categorical(y_train, 10)\n    y_test = to_categorical(y_test, 10)\n    x_train = x_train.astype(""float32"")\n    x_test = x_test.astype(""float32"")\n    x_train /= 255.0\n    x_test /= 255.0\n    trainloader = (x_train, y_train)\n    testloader = (x_test, y_test)\n\n    # Model\n    logger.debug(""Building model.."")\n    net = build_graph_from_json(receive_msg)\n\n    # parallel model\n    try:\n        available_devices = os.environ[""CUDA_VISIBLE_DEVICES""]\n        gpus = len(available_devices.split("",""))\n        if gpus > 1:\n            net = multi_gpu_model(net, gpus)\n    except KeyError:\n        logger.debug(""parallel model not support in this config settings"")\n\n    if args.optimizer == ""SGD"":\n        optimizer = SGD(lr=args.learning_rate, momentum=0.9, decay=args.weight_decay)\n    if args.optimizer == ""Adadelta"":\n        optimizer = Adadelta(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adagrad"":\n        optimizer = Adagrad(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adam"":\n        optimizer = Adam(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""Adamax"":\n        optimizer = Adamax(lr=args.learning_rate, decay=args.weight_decay)\n    if args.optimizer == ""RMSprop"":\n        optimizer = RMSprop(lr=args.learning_rate, decay=args.weight_decay)\n\n    # Compile the model\n    net.compile(\n        loss=""categorical_crossentropy"", optimizer=optimizer, metrics=[""accuracy""]\n    )\n    return 0\n\n\nclass SendMetrics(keras.callbacks.Callback):\n    """"""\n    Keras callback to send metrics to NNI framework\n    """"""\n\n    def on_epoch_end(self, epoch, logs=None):\n        """"""\n        Run on end of each epoch\n        """"""\n        if logs is None:\n            logs = dict()\n        logger.debug(logs)\n        # TensorFlow 2.0 API reference claims the key is `val_acc`, but in fact it\'s `val_accuracy`\n        if \'val_acc\' in logs:\n            nni.report_intermediate_result(logs[\'val_acc\'])\n        else:\n            nni.report_intermediate_result(logs[\'val_accuracy\'])\n\n\n# Training\ndef train_eval():\n    """""" train and eval the model\n    """"""\n\n    global trainloader\n    global testloader\n    global net\n\n    (x_train, y_train) = trainloader\n    (x_test, y_test) = testloader\n\n    # train procedure\n    net.fit(\n        x=x_train,\n        y=y_train,\n        batch_size=args.batch_size,\n        validation_data=(x_test, y_test),\n        epochs=args.epochs,\n        shuffle=True,\n        callbacks=[\n            SendMetrics(),\n            EarlyStopping(min_delta=0.001, patience=10),\n            TensorBoard(log_dir=TENSORBOARD_DIR),\n        ],\n    )\n\n    # trial report final acc to tuner\n    _, acc = net.evaluate(x_test, y_test)\n    logger.debug(""Final result is: %.3f"", acc)\n    nni.report_final_result(acc)\n\n\nif __name__ == ""__main__"":\n    try:\n        # trial get next parameter from network morphism tuner\n        RCV_CONFIG = nni.get_next_parameter()\n        logger.debug(RCV_CONFIG)\n        parse_rev_args(RCV_CONFIG)\n        train_eval()\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/network_morphism/cifar10/cifar10_pytorch.py,8,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport logging\nimport sys\n\nimport nni\nfrom nni.networkmorphism_tuner.graph import json_to_graph\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport utils\n\n# set the logger format\nlog_format = ""%(asctime)s %(message)s""\nlogging.basicConfig(\n    filename=""networkmorphism.log"",\n    filemode=""a"",\n    level=logging.INFO,\n    format=log_format,\n    datefmt=""%m/%d %I:%M:%S %p"",\n)\n# pylint: disable=W0603\n# set the logger format\nlogger = logging.getLogger(""cifar10-network-morphism-pytorch"")\n\n\ndef get_args():\n    """""" get args from command line\n    """"""\n    parser = argparse.ArgumentParser(""cifar10"")\n    parser.add_argument(""--batch_size"", type=int, default=128, help=""batch size"")\n    parser.add_argument(""--optimizer"", type=str, default=""SGD"", help=""optimizer"")\n    parser.add_argument(""--epochs"", type=int, default=200, help=""epoch limit"")\n    parser.add_argument(\n        ""--learning_rate"", type=float, default=0.001, help=""learning rate""\n    )\n    parser.add_argument(""--cutout"", action=""store_true"", default=False, help=""use cutout"")\n    parser.add_argument(""--cutout_length"", type=int, default=8, help=""cutout length"")\n    parser.add_argument(\n        ""--model_path"", type=str, default=""./"", help=""Path to save the destination model""\n    )\n    return parser.parse_args()\n\n\ntrainloader = None\ntestloader = None\nnet = None\ncriterion = None\noptimizer = None\ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\nbest_acc = 0.0\nargs = get_args()\n\n\ndef build_graph_from_json(ir_model_json):\n    """"""build model from json representation\n    """"""\n    graph = json_to_graph(ir_model_json)\n    logging.debug(graph.operation_history)\n    model = graph.produce_torch_model()\n    return model\n\n\ndef parse_rev_args(receive_msg):\n    """""" parse reveive msgs to global variable\n    """"""\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    # Loading Data\n    logger.debug(""Preparing data.."")\n\n    transform_train, transform_test = utils.data_transforms_cifar10(args)\n\n    trainset = torchvision.datasets.CIFAR10(\n        root=""./data"", train=True, download=True, transform=transform_train\n    )\n    trainloader = torch.utils.data.DataLoader(\n        trainset, batch_size=args.batch_size, shuffle=True, num_workers=2\n    )\n\n    testset = torchvision.datasets.CIFAR10(\n        root=""./data"", train=False, download=True, transform=transform_test\n    )\n    testloader = torch.utils.data.DataLoader(\n        testset, batch_size=args.batch_size, shuffle=False, num_workers=2\n    )\n\n    # Model\n    logger.debug(""Building model.."")\n    net = build_graph_from_json(receive_msg)\n\n    net = net.to(device)\n    criterion = nn.CrossEntropyLoss()\n    if device == ""cuda"" and torch.cuda.device_count() > 1:\n        net = torch.nn.DataParallel(net)\n\n    if args.optimizer == ""SGD"":\n        optimizer = optim.SGD(\n            net.parameters(), lr=args.learning_rate, momentum=0.9, weight_decay=5e-4\n        )\n    if args.optimizer == ""Adadelta"":\n        optimizer = optim.Adadelta(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adagrad"":\n        optimizer = optim.Adagrad(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adam"":\n        optimizer = optim.Adam(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""Adamax"":\n        optimizer = optim.Adamax(net.parameters(), lr=args.learning_rate)\n    if args.optimizer == ""RMSprop"":\n        optimizer = optim.RMSprop(net.parameters(), lr=args.learning_rate)\n\n\n    return 0\n\n\n# Training\ndef train(epoch):\n    """""" train model on each epoch in trainset\n    """"""\n\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    logger.debug(""Epoch: %d"", epoch)\n    net.train()\n    train_loss = 0\n    correct = 0\n    total = 0\n\n    for batch_idx, (inputs, targets) in enumerate(trainloader):\n        inputs, targets = inputs.to(device), targets.to(device)\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        train_loss += loss.item()\n        _, predicted = outputs.max(1)\n        total += targets.size(0)\n        correct += predicted.eq(targets).sum().item()\n\n        acc = 100.0 * correct / total\n\n        logger.debug(\n            ""Loss: %.3f | Acc: %.3f%% (%d/%d)"",\n            train_loss / (batch_idx + 1),\n            100.0 * correct / total,\n            correct,\n            total,\n        )\n\n    return acc\n\n\ndef test(epoch):\n    """""" eval model on each epoch in testset\n    """"""\n    global best_acc\n    global trainloader\n    global testloader\n    global net\n    global criterion\n    global optimizer\n\n    logger.debug(""Eval on epoch: %d"", epoch)\n    net.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for batch_idx, (inputs, targets) in enumerate(testloader):\n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = net(inputs)\n            loss = criterion(outputs, targets)\n\n            test_loss += loss.item()\n            _, predicted = outputs.max(1)\n            total += targets.size(0)\n            correct += predicted.eq(targets).sum().item()\n\n            acc = 100.0 * correct / total\n\n            logger.debug(\n                ""Loss: %.3f | Acc: %.3f%% (%d/%d)"",\n                test_loss / (batch_idx + 1),\n                100.0 * correct / total,\n                correct,\n                total,\n            )\n\n    acc = 100.0 * correct / total\n    if acc > best_acc:\n        best_acc = acc\n    return acc, best_acc\n\n\nif __name__ == ""__main__"":\n    try:\n        # trial get next parameter from network morphism tuner\n        RCV_CONFIG = nni.get_next_parameter()\n        logger.debug(RCV_CONFIG)\n\n        parse_rev_args(RCV_CONFIG)\n        train_acc = 0.0\n        best_acc = 0.0\n        early_stop = utils.EarlyStopping(mode=""max"")\n        for ep in range(args.epochs):\n            train_acc = train(ep)\n            test_acc, best_acc = test(ep)\n            nni.report_intermediate_result(test_acc)\n            logger.debug(test_acc)\n            if early_stop.step(test_acc):\n                break\n\n        # trial report best_acc to tuner\n        nni.report_final_result(best_acc)\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n'"
examples/trials/network_morphism/cifar10/utils.py,6,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\nimport torchvision.transforms as transforms\n\n\nclass EarlyStopping:\n    """""" EarlyStopping class to keep NN from overfitting\n    """"""\n\n    # pylint: disable=E0202\n    def __init__(self, mode=""min"", min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    def step(self, metrics):\n        """""" EarlyStopping step on each epoch\n        Arguments:\n            metrics {float} -- metric value\n        """"""\n\n        if self.best is None:\n            self.best = metrics\n            return False\n\n        if np.isnan(metrics):\n            return True\n\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            return True\n\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {""min"", ""max""}:\n            raise ValueError(""mode "" + mode + "" is unknown!"")\n        if not percentage:\n            if mode == ""min"":\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == ""max"":\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == ""min"":\n                self.is_better = lambda a, best: a < best - (best * min_delta / 100)\n            if mode == ""max"":\n                self.is_better = lambda a, best: a > best + (best * min_delta / 100)\n\n\nclass Cutout:\n    """"""Randomly mask out one or more patches from an image.\n    Args:\n        n_holes (int): Number of patches to cut out of each image.\n        length (int): The length (in pixels) of each square patch.\n    """"""\n\n    def __init__(self, length):\n        self.length = length\n\n    def __call__(self, img):\n        """"""\n        Args:\n            img (Tensor): Tensor image of size (C, H, W).\n        Returns:\n            Tensor: Image with n_holes of dimension length x length cut out of it.\n        """"""\n        h_img, w_img = img.size(1), img.size(2)\n        mask = np.ones((h_img, w_img), np.float32)\n        y_img = np.random.randint(h_img)\n        x_img = np.random.randint(w_img)\n\n        y1_img = np.clip(y_img - self.length // 2, 0, h_img)\n        y2_img = np.clip(y_img + self.length // 2, 0, h_img)\n        x1_img = np.clip(x_img - self.length // 2, 0, w_img)\n        x2_img = np.clip(x_img + self.length // 2, 0, w_img)\n\n        mask[y1_img:y2_img, x1_img:x2_img] = 0.0\n        mask = torch.from_numpy(mask)\n        mask = mask.expand_as(img)\n        img *= mask\n        return img\n\n\ndef data_transforms_cifar10(args):\n    """""" data_transforms for cifar10 dataset\n    """"""\n\n    cifar_mean = [0.49139968, 0.48215827, 0.44653124]\n    cifar_std = [0.24703233, 0.24348505, 0.26158768]\n\n    train_transform = transforms.Compose(\n        [\n            transforms.RandomCrop(32, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(cifar_mean, cifar_std),\n        ]\n    )\n    if args.cutout:\n        train_transform.transforms.append(Cutout(args.cutout_length))\n\n    valid_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(cifar_mean, cifar_std)]\n    )\n    return train_transform, valid_transform\n\n\ndef data_transforms_mnist(args, mnist_mean=None, mnist_std=None):\n    """""" data_transforms for mnist dataset\n    """"""\n    if mnist_mean is None:\n        mnist_mean = [0.5]\n\n    if mnist_std is None:\n        mnist_std = [0.5]\n\n    train_transform = transforms.Compose(\n        [\n            transforms.RandomCrop(28, padding=4),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mnist_mean, mnist_std),\n        ]\n    )\n    if args.cutout:\n        train_transform.transforms.append(Cutout(args.cutout_length))\n\n    valid_transform = transforms.Compose(\n        [transforms.ToTensor(), transforms.Normalize(mnist_mean, mnist_std)]\n    )\n    return train_transform, valid_transform\n\n\ndef get_mean_and_std(dataset):\n    """"""Compute the mean and std value of dataset.""""""\n    dataloader = torch.utils.data.DataLoader(\n        dataset, batch_size=1, shuffle=True, num_workers=2\n    )\n    mean = torch.zeros(3)\n    std = torch.zeros(3)\n    print(""==> Computing mean and std.."")\n    for inputs, _ in dataloader:\n        for i in range(3):\n            mean[i] += inputs[:, i, :, :].mean()\n            std[i] += inputs[:, i, :, :].std()\n    mean.div_(len(dataset))\n    std.div_(len(dataset))\n    return mean, std\n\n\ndef init_params(net):\n    """"""Init layer parameters.""""""\n    for module in net.modules():\n        if isinstance(module, nn.Conv2d):\n            init.kaiming_normal(module.weight, mode=""fan_out"")\n            if module.bias:\n                init.constant(module.bias, 0)\n        elif isinstance(module, nn.BatchNorm2d):\n            init.constant(module.weight, 1)\n            init.constant(module.bias, 0)\n        elif isinstance(module, nn.Linear):\n            init.normal(module.weight, std=1e-3)\n            if module.bias:\n                init.constant(module.bias, 0)\n'"
examples/trials/sklearn/classification/main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport nni\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nimport logging\nimport numpy as np\n\nLOG = logging.getLogger(\'sklearn_classification\')\n\ndef load_data():\n    \'\'\'Load dataset, use 20newsgroups dataset\'\'\'\n    digits = load_digits()\n    X_train, X_test, y_train, y_test = train_test_split(\n        digits.data, digits.target, random_state=99, test_size=0.25)\n\n    ss = StandardScaler()\n    X_train = ss.fit_transform(X_train)\n    X_test = ss.transform(X_test)\n\n    return X_train, X_test, y_train, y_test\n\ndef get_default_parameters():\n    \'\'\'get default parameters\'\'\'\n    params = {\n        \'C\': 1.0,\n        \'kernel\': \'linear\',\n        \'degree\': 3,\n        \'gamma\': 0.01,\n        \'coef0\': 0.01\n    }\n    return params\n\ndef get_model(PARAMS):\n    \'\'\'Get model according to parameters\'\'\'\n    model = SVC()\n    model.C = PARAMS.get(\'C\')\n    model.kernel = PARAMS.get(\'kernel\')\n    model.degree = PARAMS.get(\'degree\')\n    model.gamma = PARAMS.get(\'gamma\')\n    model.coef0 = PARAMS.get(\'coef0\')\n\n    return model\n\ndef run(X_train, X_test, y_train, y_test, model):\n    \'\'\'Train model and predict result\'\'\'\n    model.fit(X_train, y_train)\n    score = model.score(X_test, y_test)\n    LOG.debug(\'score: %s\' % score)\n    nni.report_final_result(score)\n\nif __name__ == \'__main__\':\n    X_train, X_test, y_train, y_test = load_data()\n\n    try:\n        # get parameters from tuner\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = get_default_parameters()\n        PARAMS.update(RECEIVED_PARAMS)\n        LOG.debug(PARAMS)\n        model = get_model(PARAMS)\n        run(X_train, X_test, y_train, y_test, model)\n    except Exception as exception:\n        LOG.exception(exception)\n        raise\n'"
examples/trials/sklearn/regression/main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport nni\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport logging\nimport numpy as np\nfrom sklearn.metrics import r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lars\nfrom sklearn.linear_model import ARDRegression\n\nLOG = logging.getLogger(\'sklearn_regression\')\n\ndef load_data():\n    \'\'\'Load dataset, use boston dataset\'\'\'\n    boston = load_boston()\n    X_train, X_test, y_train, y_test = train_test_split(\n        boston.data, boston.target, random_state=99, test_size=0.25)\n    #normalize data\n    ss_X = StandardScaler()\n    ss_y = StandardScaler()\n\n    X_train = ss_X.fit_transform(X_train)\n    X_test = ss_X.transform(X_test)\n    y_train = ss_y.fit_transform(y_train[:, None])[:, 0]\n    y_test = ss_y.transform(y_test[:, None])[:, 0]\n\n    return X_train, X_test, y_train, y_test\n\ndef get_default_parameters():\n    \'\'\'get default parameters\'\'\'\n    params = {\'model_name\': \'LinearRegression\'}\n    return params\n\ndef get_model(PARAMS):\n    \'\'\'Get model according to parameters\'\'\'\n    model_dict = {\n        \'LinearRegression\': LinearRegression(),\n        \'Ridge\': Ridge(),\n        \'Lars\': Lars(),\n        \'ARDRegression\': ARDRegression()\n\n    }\n    if not model_dict.get(PARAMS[\'model_name\']):\n        LOG.exception(\'Not supported model!\')\n        exit(1)\n\n    model = model_dict[PARAMS[\'model_name\']]\n    model.normalize = bool(PARAMS[\'normalize\'])\n\n    return model\n\ndef run(X_train, X_test, y_train, y_test, model):\n    \'\'\'Train model and predict result\'\'\'\n    model.fit(X_train, y_train)\n    predict_y = model.predict(X_test)\n    score = r2_score(y_test, predict_y)\n    LOG.debug(\'r2 score: %s\' % score)\n    nni.report_final_result(score)\n\nif __name__ == \'__main__\':\n    X_train, X_test, y_train, y_test = load_data()\n\n    try:\n        # get parameters from tuner\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = get_default_parameters()\n        PARAMS.update(RECEIVED_PARAMS)\n        LOG.debug(PARAMS)\n        model = get_model(PARAMS)\n        run(X_train, X_test, y_train, y_test, model)\n    except Exception as exception:\n        LOG.exception(exception)\n        raise\n'"
examples/trials/systems/rocksdb-fillrandom/main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport nni\nimport subprocess\nimport logging\n\nLOG = logging.getLogger(\'rocksdb-fillrandom\')\n\n\ndef run(**parameters):\n    \'\'\'Run rocksdb benchmark and return throughput\'\'\'\n    bench_type = parameters[\'benchmarks\']\n    # recover args\n    args = [""--{}={}"".format(k, v) for k, v in parameters.items()]\n    # subprocess communicate\n    process = subprocess.Popen([\'db_bench\'] + args, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    # split into lines\n    lines = out.decode(""utf8"").splitlines()\n\n    match_lines = []\n    for line in lines:\n        # find the line with matched str\n        if bench_type not in line:\n            continue\n        else:\n            match_lines.append(line)\n            break\n\n    results = {}\n    for line in match_lines:\n        key, _, value = line.partition("":"")\n        key = key.strip()\n        value = value.split(""op"")[1]\n        results[key] = float(value)\n\n    return results[bench_type]\n\n\ndef generate_params(received_params):\n    \'\'\'generate parameters based on received parameters\'\'\'\n    params = {\n        ""benchmarks"": ""fillrandom"",\n        ""threads"": 1,\n        ""key_size"": 20,\n        ""value_size"": 100,\n        ""num"": 13107200,\n        ""db"": ""/tmp/rockdb"",\n        ""disable_wal"": 1,\n        ""max_background_flushes"": 1,\n        ""max_background_compactions"": 4,\n        ""write_buffer_size"": 67108864,\n        ""max_write_buffer_number"": 16,\n        ""min_write_buffer_number_to_merge"": 2,\n        ""level0_file_num_compaction_trigger"": 2,\n        ""max_bytes_for_level_base"": 268435456,\n        ""max_bytes_for_level_multiplier"": 10,\n        ""target_file_size_base"": 33554432,\n        ""target_file_size_multiplier"": 1\n    }\n\n    for k, v in received_params.items():\n        params[k] = int(v)\n\n    return params\n\n\nif __name__ == ""__main__"":\n    try:\n        # get parameters from tuner\n        RECEIVED_PARAMS = nni.get_next_parameter()\n        LOG.debug(RECEIVED_PARAMS)\n        PARAMS = generate_params(RECEIVED_PARAMS)\n        LOG.debug(PARAMS)\n        # run benchmark\n        throughput = run(**PARAMS)\n        # report throughput to nni\n        nni.report_final_result(throughput)\n    except Exception as exception:\n        LOG.exception(exception)\n        raise\n'"
examples/trials/weight_sharing/ga_squad/attention.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport math\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\n\ndef _get_variable(variable_dict, name, shape, initializer=None, dtype=tf.float32):\n    if name not in variable_dict:\n        variable_dict[name] = tf.get_variable(\n            name=name, shape=shape, initializer=initializer, dtype=dtype)\n    return variable_dict[name]\n\n\nclass DotAttention:\n    \'\'\'\n    DotAttention\n    \'\'\'\n\n    def __init__(self, name,\n                 hidden_dim,\n                 is_vanilla=True,\n                 is_identity_transform=False,\n                 need_padding=False):\n        self._name = \'/\'.join([name, \'dot_att\'])\n        self._hidden_dim = hidden_dim\n        self._is_identity_transform = is_identity_transform\n        self._need_padding = need_padding\n        self._is_vanilla = is_vanilla\n        self._var = {}\n\n    @property\n    def is_identity_transform(self):\n        return self._is_identity_transform\n\n    @property\n    def is_vanilla(self):\n        return self._is_vanilla\n\n    @property\n    def need_padding(self):\n        return self._need_padding\n\n    @property\n    def hidden_dim(self):\n        return self._hidden_dim\n\n    @property\n    def name(self):\n        return self._name\n\n    @property\n    def var(self):\n        return self._var\n\n    def _get_var(self, name, shape, initializer=None):\n        with tf.variable_scope(self.name):\n            return _get_variable(self.var, name, shape, initializer)\n\n    def _define_params(self, src_dim, tgt_dim):\n        hidden_dim = self.hidden_dim\n        self._get_var(\'W\', [src_dim, hidden_dim])\n        if not self.is_vanilla:\n            self._get_var(\'V\', [src_dim, hidden_dim])\n            if self.need_padding:\n                self._get_var(\'V_s\', [src_dim, src_dim])\n                self._get_var(\'V_t\', [tgt_dim, tgt_dim])\n            if not self.is_identity_transform:\n                self._get_var(\'T\', [tgt_dim, src_dim])\n        self._get_var(\'U\', [tgt_dim, hidden_dim])\n        self._get_var(\'b\', [1, hidden_dim])\n        self._get_var(\'v\', [hidden_dim, 1])\n\n    def get_pre_compute(self, s):\n        \'\'\'\n        :param s: [src_sequence, batch_size, src_dim]\n        :return: [src_sequence, batch_size. hidden_dim]\n        \'\'\'\n        hidden_dim = self.hidden_dim\n        src_dim = s.get_shape().as_list()[-1]\n        assert src_dim is not None, \'src dim must be defined\'\n        W = self._get_var(\'W\', shape=[src_dim, hidden_dim])\n        b = self._get_var(\'b\', shape=[1, hidden_dim])\n        return tf.tensordot(s, W, [[2], [0]]) + b\n\n    def get_prob(self, src, tgt, mask, pre_compute, return_logits=False):\n        \'\'\'\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param h: [batch_size, tgt_dim] or [tgt_sequence_length, batch_size, tgt_dim]\n        :param mask: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_sizse]\n        :param pre_compute: [src_sequence_length, batch_size, hidden_dim]\n        :return: [src_sequence_length, batch_size]\\\n             or [tgt_sequence_length, src_sequence_length, batch_size]\n        \'\'\'\n        s_shape = src.get_shape().as_list()\n        h_shape = tgt.get_shape().as_list()\n        src_dim = s_shape[-1]\n        tgt_dim = h_shape[-1]\n        assert src_dim is not None, \'src dimension must be defined\'\n        assert tgt_dim is not None, \'tgt dimension must be defined\'\n\n        self._define_params(src_dim, tgt_dim)\n\n        if len(h_shape) == 2:\n            tgt = tf.expand_dims(tgt, 0)\n        if pre_compute is None:\n            pre_compute = self.get_pre_compute(src)\n\n        buf0 = pre_compute\n        buf1 = tf.tensordot(tgt, self.var[\'U\'], axes=[[2], [0]])\n        buf2 = tf.tanh(tf.expand_dims(buf0, 0) + tf.expand_dims(buf1, 1))\n\n        if not self.is_vanilla:\n            xh1 = tgt\n            xh2 = tgt\n            s1 = src\n            if self.need_padding:\n                xh1 = tf.tensordot(xh1, self.var[\'V_t\'], 1)\n                xh2 = tf.tensordot(xh2, self.var[\'S_t\'], 1)\n                s1 = tf.tensordot(s1, self.var[\'V_s\'], 1)\n            if not self.is_identity_transform:\n                xh1 = tf.tensordot(xh1, self.var[\'T\'], 1)\n                xh2 = tf.tensordot(xh2, self.var[\'T\'], 1)\n            buf3 = tf.expand_dims(s1, 0) * tf.expand_dims(xh1, 1)\n            buf3 = tf.tanh(tf.tensordot(buf3, self.var[\'V\'], axes=[[3], [0]]))\n            buf = tf.reshape(tf.tanh(buf2 + buf3), shape=tf.shape(buf3))\n        else:\n            buf = buf2\n        v = self.var[\'v\']\n        e = tf.tensordot(buf, v, [[3], [0]])\n        e = tf.squeeze(e, axis=[3])\n        tmp = tf.reshape(e + (mask - 1) * 10000.0, shape=tf.shape(e))\n        prob = tf.nn.softmax(tmp, 1)\n        if len(h_shape) == 2:\n            prob = tf.squeeze(prob, axis=[0])\n            tmp = tf.squeeze(tmp, axis=[0])\n        if return_logits:\n            return prob, tmp\n        return prob\n\n    def get_att(self, s, prob):\n        \'\'\'\n        :param s: [src_sequence_length, batch_size, src_dim]\n        :param prob: [src_sequence_length, batch_size]\\\n            or [tgt_sequence_length, src_sequence_length, batch_size]\n        :return: [batch_size, src_dim] or [tgt_sequence_length, batch_size, src_dim]\n        \'\'\'\n        buf = s * tf.expand_dims(prob, axis=-1)\n        att = tf.reduce_sum(buf, axis=-3)\n        return att\n'"
examples/trials/weight_sharing/ga_squad/data.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nData processing script for the QA model.\n\'\'\'\n\nimport csv\nimport json\nfrom random import shuffle\n\nimport numpy as np\n\n\nclass WhitespaceTokenizer:\n    \'\'\'\n    Tokenizer for whitespace\n    \'\'\'\n\n    def tokenize(self, text):\n        \'\'\'\n        tokenize function in Tokenizer.\n        \'\'\'\n        start = -1\n        tokens = []\n        for i, character in enumerate(text):\n            if character == \' \' or character == \'\\t\':\n                if start >= 0:\n                    word = text[start:i]\n                    tokens.append({\n                        \'word\': word,\n                        \'original_text\': word,\n                        \'char_begin\': start,\n                        \'char_end\': i})\n                    start = -1\n            else:\n                if start < 0:\n                    start = i\n        if start >= 0:\n            tokens.append({\n                \'word\': text[start:len(text)],\n                \'original_text\': text[start:len(text)],\n                \'char_begin\': start,\n                \'char_end\': len(text)\n            })\n        return tokens\n\n\ndef load_from_file(path, fmt=None, is_training=True):\n    \'\'\'\n    load data from file\n    \'\'\'\n    if fmt is None:\n        fmt = \'squad\'\n    assert fmt in [\'squad\', \'csv\'], \'input format must be squad or csv\'\n    qp_pairs = []\n    if fmt == \'squad\':\n        with open(path) as data_file:\n            data = json.load(data_file)[\'data\']\n            for doc in data:\n                for paragraph in doc[\'paragraphs\']:\n                    passage = paragraph[\'context\']\n                    for qa_pair in paragraph[\'qas\']:\n                        question = qa_pair[\'question\']\n                        qa_id = qa_pair[\'id\']\n                        if not is_training:\n                            qp_pairs.append(\n                                {\'passage\': passage, \'question\': question, \'id\': qa_id})\n                        else:\n                            for answer in qa_pair[\'answers\']:\n                                answer_begin = int(answer[\'answer_start\'])\n                                answer_end = answer_begin + len(answer[\'text\'])\n                                qp_pairs.append({\'passage\': passage,\n                                                 \'question\': question,\n                                                 \'id\': qa_id,\n                                                 \'answer_begin\': answer_begin,\n                                                 \'answer_end\': answer_end})\n    else:\n        with open(path, newline=\'\') as csvfile:\n            reader = csv.reader(csvfile, delimiter=\'\\t\')\n            line_num = 0\n            for row in reader:\n                qp_pairs.append(\n                    {\'passage\': row[1], \'question\': row[0], \'id\': line_num})\n                line_num += 1\n    return qp_pairs\n\n\ndef tokenize(qp_pair, tokenizer=None, is_training=False):\n    \'\'\'\n    tokenize function.\n    \'\'\'\n    question_tokens = tokenizer.tokenize(qp_pair[\'question\'])\n    passage_tokens = tokenizer.tokenize(qp_pair[\'passage\'])\n    if is_training:\n        question_tokens = question_tokens[:300]\n        passage_tokens = passage_tokens[:300]\n    passage_tokens.insert(\n        0, {\'word\': \'<BOS>\', \'original_text\': \'<BOS>\', \'char_begin\': 0, \'char_end\': 0})\n    passage_tokens.append(\n        {\'word\': \'<EOS>\', \'original_text\': \'<EOS>\', \'char_begin\': 0, \'char_end\': 0})\n    qp_pair[\'question_tokens\'] = question_tokens\n    qp_pair[\'passage_tokens\'] = passage_tokens\n\n\ndef collect_vocab(qp_pairs):\n    \'\'\'\n    Build the vocab from corpus.\n    \'\'\'\n    vocab = set()\n    for qp_pair in qp_pairs:\n        for word in qp_pair[\'question_tokens\']:\n            vocab.add(word[\'word\'])\n        for word in qp_pair[\'passage_tokens\']:\n            vocab.add(word[\'word\'])\n    return vocab\n\n\ndef shuffle_step(entries, step):\n    \'\'\'\n    Shuffle the step\n    \'\'\'\n    answer = []\n    for i in range(0, len(entries), step):\n        sub = entries[i:i+step]\n        shuffle(sub)\n        answer += sub\n    return answer\n\n\ndef get_batches(qp_pairs, batch_size, need_sort=True):\n    \'\'\'\n    Get batches data and shuffle.\n    \'\'\'\n    if need_sort:\n        qp_pairs = sorted(qp_pairs, key=lambda qp: (\n            len(qp[\'passage_tokens\']), qp[\'id\']), reverse=True)\n    batches = [{\'qp_pairs\': qp_pairs[i:(i + batch_size)]}\n               for i in range(0, len(qp_pairs), batch_size)]\n    shuffle(batches)\n    return batches\n\n\ndef get_char_input(data, char_dict, max_char_length):\n    \'\'\'\n    Get char input.\n    \'\'\'\n    batch_size = len(data)\n    sequence_length = max(len(d) for d in data)\n    char_id = np.zeros((max_char_length, sequence_length,\n                        batch_size), dtype=np.int32)\n    char_lengths = np.zeros((sequence_length, batch_size), dtype=np.float32)\n    for batch_idx in range(0, min(len(data), batch_size)):\n        batch_data = data[batch_idx]\n        for sample_idx in range(0, min(len(batch_data), sequence_length)):\n            word = batch_data[sample_idx][\'word\']\n            char_lengths[sample_idx, batch_idx] = min(\n                len(word), max_char_length)\n            for i in range(0, min(len(word), max_char_length)):\n                char_id[i, sample_idx, batch_idx] = get_id(char_dict, word[i])\n    return char_id, char_lengths\n\n\ndef get_word_input(data, word_dict, embed, embed_dim):\n    \'\'\'\n    Get word input.\n    \'\'\'\n    batch_size = len(data)\n    max_sequence_length = max(len(d) for d in data)\n    sequence_length = max_sequence_length\n    word_input = np.zeros((max_sequence_length, batch_size,\n                           embed_dim), dtype=np.float32)\n    ids = np.zeros((sequence_length, batch_size), dtype=np.int32)\n    masks = np.zeros((sequence_length, batch_size), dtype=np.float32)\n    lengths = np.zeros([batch_size], dtype=np.int32)\n\n    for batch_idx in range(0, min(len(data), batch_size)):\n        batch_data = data[batch_idx]\n\n        lengths[batch_idx] = len(batch_data)\n\n        for sample_idx in range(0, min(len(batch_data), sequence_length)):\n            word = batch_data[sample_idx][\'word\'].lower()\n            if word in word_dict.keys():\n                word_input[sample_idx, batch_idx] = embed[word_dict[word]]\n                ids[sample_idx, batch_idx] = word_dict[word]\n            masks[sample_idx, batch_idx] = 1\n\n    word_input = np.reshape(word_input, (-1, embed_dim))\n    return word_input, ids, masks, lengths\n\n\ndef get_word_index(tokens, char_index):\n    \'\'\'\n    Given word return word index.\n    \'\'\'\n    for (i, token) in enumerate(tokens):\n        if token[\'char_end\'] == 0:\n            continue\n        if token[\'char_begin\'] <= char_index and char_index <= token[\'char_end\']:\n            return i\n    return 0\n\n\ndef get_answer_begin_end(data):\n    \'\'\'\n    Get answer\'s index of begin and end.\n    \'\'\'\n    begin = []\n    end = []\n    for qa_pair in data:\n        tokens = qa_pair[\'passage_tokens\']\n        char_begin = qa_pair[\'answer_begin\']\n        char_end = qa_pair[\'answer_end\']\n        word_begin = get_word_index(tokens, char_begin)\n        word_end = get_word_index(tokens, char_end)\n        begin.append(word_begin)\n        end.append(word_end)\n    return np.asarray(begin), np.asarray(end)\n\n\ndef get_id(word_dict, word):\n    \'\'\'\n    Given word, return word id.\n    \'\'\'\n    return word_dict.get(word, word_dict[\'<unk>\'])\n\n\ndef get_buckets(min_length, max_length, bucket_count):\n    \'\'\'\n    Get bucket by length.\n    \'\'\'\n    if bucket_count <= 0:\n        return [max_length]\n    unit_length = int((max_length - min_length) // (bucket_count))\n    buckets = [min_length + unit_length *\n               (i + 1) for i in range(0, bucket_count)]\n    buckets[-1] = max_length\n    return buckets\n\n\ndef find_bucket(length, buckets):\n    \'\'\'\n    Find bucket.\n    \'\'\'\n    for bucket in buckets:\n        if length <= bucket:\n            return bucket\n    return buckets[-1]\n'"
examples/trials/weight_sharing/ga_squad/evaluate.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nEvaluation scripts for QA model.\n\'\'\'\n\nfrom __future__ import print_function\nfrom collections import Counter\nimport string\nimport re\nimport argparse\nimport json\nimport sys\n\n\ndef normalize_answer(str_input):\n    """"""Lower text and remove punctuation, articles and extra whitespace.""""""\n    def remove_articles(text):\n        \'\'\'\n        Remove ""a|an|the""\n        \'\'\'\n        return re.sub(r\'\\b(a|an|the)\\b\', \' \', text)\n\n    def white_space_fix(text):\n        \'\'\'\n        Remove unnessary whitespace\n        \'\'\'\n        return \' \'.join(text.split())\n\n    def remove_punc(text):\n        \'\'\'\n        Remove punc\n        \'\'\'\n        exclude = set(string.punctuation)\n        return \'\'.join(ch for ch in text if ch not in exclude)\n\n    def lower(text):\n        \'\'\'\n        Change string to lower form.\n        \'\'\'\n        return text.lower()\n\n    return white_space_fix(remove_articles(remove_punc(lower(str_input))))\n\n\ndef f1_score(prediction, ground_truth):\n    \'\'\'\n    Calculate the f1 score.\n    \'\'\'\n    prediction_tokens = normalize_answer(prediction).split()\n    ground_truth_tokens = normalize_answer(ground_truth).split()\n    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n    num_same = sum(common.values())\n    if num_same == 0:\n        return 0\n    if not prediction_tokens:\n        raise ValueError(""empty prediction tokens"")\n    precision = 1.0 * num_same / len(prediction_tokens)\n\n    if not ground_truth_tokens:\n        raise ValueError(""empty groundtruth tokens"")\n    recall = 1.0 * num_same / len(ground_truth_tokens)\n    f1_result = (2 * precision * recall) / (precision + recall + 1e-10)\n    return f1_result\n\n\ndef exact_match_score(prediction, ground_truth):\n    \'\'\'\n    Calculate the match score with prediction and ground truth.\n    \'\'\'\n    return normalize_answer(prediction) == normalize_answer(ground_truth)\n\n\ndef metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n    \'\'\'\n    Metric max over the ground truths.\n    \'\'\'\n    scores_for_ground_truths = []\n    for ground_truth in ground_truths:\n        score = metric_fn(prediction, ground_truth)\n        scores_for_ground_truths.append(score)\n    return max(scores_for_ground_truths)\n\n\ndef _evaluate(dataset, predictions):\n    \'\'\'\n    Evaluate function.\n    \'\'\'\n    f1_result = exact_match = total = 0\n    count = 0\n    for article in dataset:\n        for paragraph in article[\'paragraphs\']:\n            for qa_pair in paragraph[\'qas\']:\n                total += 1\n                if qa_pair[\'id\'] not in predictions:\n                    count += 1\n                    continue\n                ground_truths = list(\n                    map(lambda x: x[\'text\'], qa_pair[\'answers\']))\n                prediction = predictions[qa_pair[\'id\']]\n                exact_match += metric_max_over_ground_truths(\n                    exact_match_score, prediction, ground_truths)\n                f1_result += metric_max_over_ground_truths(\n                    f1_score, prediction, ground_truths)\n    print(\'total\', total, \'exact_match\',\n          exact_match, \'unanswer_question \', count)\n    exact_match = 100.0 * exact_match / total\n    f1_result = 100.0 * f1_result / total\n    return {\'exact_match\': exact_match, \'f1\': f1_result}\n\n\ndef evaluate(data_file, pred_file):\n    \'\'\'\n    Evaluate.\n    \'\'\'\n    expected_version = \'1.1\'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json[\'version\'] != expected_version:\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    with open(pred_file) as prediction_file:\n        predictions = json.load(prediction_file)\n    # print(json.dumps(evaluate(dataset, predictions)))\n    result = _evaluate(dataset, predictions)\n    # print(\'em:\', result[\'exact_match\'], \'f1:\', result[\'f1\'])\n    return result[\'exact_match\']\n\n\ndef evaluate_with_predictions(data_file, predictions):\n    \'\'\'\n    Evalutate with predictions/\n    \'\'\'\n    expected_version = \'1.1\'\n    with open(data_file) as dataset_file:\n        dataset_json = json.load(dataset_file)\n        if dataset_json[\'version\'] != expected_version:\n            print(\'Evaluation expects v-\' + expected_version +\n                  \', but got dataset with v-\' + dataset_json[\'version\'],\n                  file=sys.stderr)\n        dataset = dataset_json[\'data\']\n    result = _evaluate(dataset, predictions)\n    return result[\'exact_match\']\n\n\nif __name__ == \'__main__\':\n    EXPECT_VERSION = \'1.1\'\n    parser = argparse.ArgumentParser(\n        description=\'Evaluation for SQuAD \' + EXPECT_VERSION)\n    parser.add_argument(\'dataset_file\', help=\'Dataset file\')\n    parser.add_argument(\'prediction_file\', help=\'Prediction File\')\n    args = parser.parse_args()\n    print(evaluate(args.dataset_file, args.prediction_file))\n'"
examples/trials/weight_sharing/ga_squad/graph.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\'\'\'\nGraph is customed-define class, this module contains related class and function about graph.\n\'\'\'\n\n\nimport copy\nimport hashlib\nimport logging\nimport json\nimport random\nfrom collections import deque\nfrom enum import Enum, unique\nfrom typing import Iterable\n\nimport numpy as np\n\n_logger = logging.getLogger(\'ga_squad_graph\')\n\n@unique\nclass LayerType(Enum):\n    \'\'\'\n    Layer type\n    \'\'\'\n    attention = 0\n    self_attention = 1\n    rnn = 2\n    input = 3\n    output = 4\n\nclass Layer(object):\n    \'\'\'\n    Layer class, which contains the information of graph.\n    \'\'\'\n    def __init__(self, graph_type, inputs=None, output=None, size=None, hash_id=None):\n        self.input = inputs if inputs is not None else []\n        self.output = output if output is not None else []\n        self.graph_type = graph_type\n        self.is_delete = False\n        self.size = size\n        self.hash_id = hash_id\n        if graph_type == LayerType.attention.value:\n            self.input_size = 2\n            self.output_size = 1\n        elif graph_type == LayerType.rnn.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.self_attention.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.input.value:\n            self.input_size = 0\n            self.output_size = 1\n            if self.hash_id is None:\n                hasher = hashlib.md5()\n                hasher.update(np.random.bytes(100))\n                self.hash_id = hasher.hexdigest()\n        elif graph_type == LayerType.output.value:\n            self.input_size = 1\n            self.output_size = 0\n        else:\n            raise ValueError(\'Unsupported LayerType: {}\'.format(graph_type))\n\n    def update_hash(self, layers: Iterable):\n        """"""\n        Calculation of `hash_id` of Layer. Which is determined by the properties of itself, and the `hash_id`s of input layers\n        """"""\n        if self.graph_type == LayerType.input.value:\n            return\n        hasher = hashlib.md5()\n        hasher.update(LayerType(self.graph_type).name.encode(\'ascii\'))\n        hasher.update(str(self.size).encode(\'ascii\'))\n        for i in self.input:\n            if layers[i].hash_id is None:\n                raise ValueError(\'Hash id of layer {}: {} not generated!\'.format(i, layers[i]))\n            hasher.update(layers[i].hash_id.encode(\'ascii\'))\n        self.hash_id = hasher.hexdigest()\n\n    def set_size(self, graph_id, size):\n        \'\'\'\n        Set size.\n        \'\'\'\n        if self.graph_type == LayerType.attention.value:\n            if self.input[0] == graph_id:\n                self.size = size\n        if self.graph_type == LayerType.rnn.value:\n            self.size = size\n        if self.graph_type == LayerType.self_attention.value:\n            self.size = size\n        if self.graph_type == LayerType.output.value:\n            if self.size != size:\n                return False\n        return True\n\n    def clear_size(self):\n        \'\'\'\n        Clear size\n        \'\'\'\n        if self.graph_type == LayerType.attention.value or \\\n            LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None\n\n    def __str__(self):\n        return \'input:\' + str(self.input) + \' output:\' + str(self.output) + \' type:\' + str(self.graph_type) + \' is_delete:\' + str(self.is_delete) + \' size:\' + str(self.size)\n\ndef graph_dumps(graph):\n    \'\'\'\n    Dump the graph.\n    \'\'\'\n    return json.dumps(graph, default=lambda obj: obj.__dict__)\n\ndef graph_loads(graph_json):\n    \'\'\'\n    Load graph\n    \'\'\'\n    layers = []\n    for layer in graph_json[\'layers\']:\n        layer_info = Layer(layer[\'graph_type\'], layer[\'input\'], layer[\'output\'], layer[\'size\'], layer[\'hash_id\'])\n        layer_info.is_delete = layer[\'is_delete\']\n        _logger.debug(\'append layer {}\'.format(layer_info))\n        layers.append(layer_info)\n    graph = Graph(graph_json[\'max_layer_num\'], graph_json[\'min_layer_num\'], [], [], [])\n    graph.layers = layers\n    _logger.debug(\'graph {} loaded\'.format(graph))\n    return graph\n\nclass Graph(object):\n    \'\'\'\n    Customed Graph class.\n    \'\'\'\n    def __init__(self, max_layer_num, min_layer_num, inputs, output, hide):\n        self.layers = []\n        self.max_layer_num = max_layer_num\n        self.min_layer_num = min_layer_num\n        assert min_layer_num < max_layer_num\n\n        for layer in inputs:\n            self.layers.append(layer)\n        for layer in output:\n            self.layers.append(layer)\n        if hide is not None:\n            for layer in hide:\n                self.layers.append(layer)\n        assert self.is_legal()\n\n    def is_topology(self, layers=None):\n        \'\'\'\n        valid the topology\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layers_nodle = []\n        result = []\n        for i, layer in enumerate(layers):\n            if layer.is_delete is False:\n                layers_nodle.append(i)\n        while True:\n            flag_break = True\n            layers_toremove = []\n            for layer1 in layers_nodle:\n                flag_arrive = True\n                for layer2 in layers[layer1].input:\n                    if layer2 in layers_nodle:\n                        flag_arrive = False\n                if flag_arrive is True:\n                    for layer2 in layers[layer1].output:\n                        # Size is error\n                        if layers[layer2].set_size(layer1, layers[layer1].size) is False:\n                            return False\n                    layers_toremove.append(layer1)\n                    result.append(layer1)\n                    flag_break = False\n            for layer in layers_toremove:\n                layers_nodle.remove(layer)\n            result.append(\'|\')\n            if flag_break:\n                break\n        # There is loop in graph || some layers can\'t to arrive\n        if layers_nodle:\n            return False\n        return result\n\n    def layer_num(self, layers=None):\n        \'\'\'\n        Reutn number of layer.\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layer_num = 0\n        for layer in layers:\n            if layer.is_delete is False and layer.graph_type != LayerType.input.value\\\n                and layer.graph_type != LayerType.output.value:\n                layer_num += 1\n        return layer_num\n\n    def is_legal(self, layers=None):\n        \'\'\'\n        Judge whether is legal for layers\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n\n        for layer in layers:\n            if layer.is_delete is False:\n                if len(layer.input) != layer.input_size:\n                    return False\n                if len(layer.output) < layer.output_size:\n                    return False\n\n        # layer_num <= max_layer_num\n        if self.layer_num(layers) > self.max_layer_num:\n            return False\n\n        # There is loop in graph || some layers can\'t to arrive\n        if self.is_topology(layers) is False:\n            return False\n\n        return True\n\n    def update_hash(self):\n        """"""\n        update hash id of each layer, in topological order/recursively\n        hash id will be used in weight sharing\n        """"""\n        _logger.debug(\'update hash\')\n        layer_in_cnt = [len(layer.input) for layer in self.layers]\n        topo_queue = deque([i for i, layer in enumerate(self.layers) if not layer.is_delete and layer.graph_type == LayerType.input.value])\n        while topo_queue:\n            layer_i = topo_queue.pop()\n            self.layers[layer_i].update_hash(self.layers)\n            for layer_j in self.layers[layer_i].output:\n                layer_in_cnt[layer_j] -= 1\n                if layer_in_cnt[layer_j] == 0:\n                    topo_queue.appendleft(layer_j)\n\n    def mutation(self, only_add=False):\n        \'\'\'\n        Mutation for a graph\n        \'\'\'\n        types = []\n        if self.layer_num() < self.max_layer_num:\n            types.append(0)\n            types.append(1)\n        if self.layer_num() > self.min_layer_num and only_add is False:\n            types.append(2)\n            types.append(3)\n        # 0 : add a layer , delete a edge\n        # 1 : add a layer , change a edge\n        # 2 : delete a layer, delete a edge\n        # 3 : delete a layer, change a edge\n        graph_type = random.choice(types)\n        layer_type = random.choice([LayerType.attention.value,\\\n            LayerType.self_attention.value, LayerType.rnn.value])\n        layers = copy.deepcopy(self.layers)\n        cnt_try = 0\n        while True:\n            layers_in = []\n            layers_out = []\n            layers_del = []\n            for i, layer in enumerate(layers):\n                if layer.is_delete is False:\n                    if layer.graph_type != LayerType.output.value:\n                        layers_in.append(i)\n                    if layer.graph_type != LayerType.input.value:\n                        layers_out.append(i)\n                    if layer.graph_type != LayerType.output.value\\\n                            and layer.graph_type != LayerType.input.value:\n                        layers_del.append(i)\n            if graph_type <= 1:\n                new_id = len(layers)\n                out = random.choice(layers_out)\n                inputs = []\n                output = [out]\n                pos = random.randint(0, len(layers[out].input) - 1)\n                last_in = layers[out].input[pos]\n                layers[out].input[pos] = new_id\n                if graph_type == 0:\n                    layers[last_in].output.remove(out)\n                if graph_type == 1:\n                    layers[last_in].output.remove(out)\n                    layers[last_in].output.append(new_id)\n                    inputs = [last_in]\n                lay = Layer(graph_type=layer_type, inputs=inputs, output=output)\n                while len(inputs) < lay.input_size:\n                    layer1 = random.choice(layers_in)\n                    inputs.append(layer1)\n                    layers[layer1].output.append(new_id)\n                lay.input = inputs\n                layers.append(lay)\n            else:\n                layer1 = random.choice(layers_del)\n                for layer2 in layers[layer1].output:\n                    layers[layer2].input.remove(layer1)\n                    if graph_type == 2:\n                        random_in = random.choice(layers_in)\n                    else:\n                        random_in = random.choice(layers[layer1].input)\n                    layers[layer2].input.append(random_in)\n                    layers[random_in].output.append(layer2)\n                for layer2 in layers[layer1].input:\n                    layers[layer2].output.remove(layer1)\n                layers[layer1].is_delete = True\n\n            if self.is_legal(layers):\n                self.layers = layers\n                break\n            else:\n                layers = copy.deepcopy(self.layers)\n                cnt_try += 1\n        self.update_hash()\n\n    def __str__(self):\n        info = """"\n        for l_id, layer in enumerate(self.layers):\n            if layer.is_delete is False:\n                info += \'id:%d \' % l_id + str(layer) + \'\\n\'\n        return info\n'"
examples/trials/weight_sharing/ga_squad/graph_to_tf.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport tensorflow as tf\nfrom rnn import XGRUCell\nfrom util import dropout\nfrom graph import LayerType\n\n\ndef normalize(inputs,\n              epsilon=1e-8,\n              scope=""ln""):\n    \'\'\'Applies layer normalization.\n\n    Args:\n      inputs: A tensor with 2 or more dimensions, where the first dimension has\n        `batch_size`.\n      epsilon: A floating number. A very small number for preventing ZeroDivision Error.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A tensor with the same shape and data dtype as `inputs`.\n    \'\'\'\n    with tf.variable_scope(scope):\n        inputs_shape = inputs.get_shape()\n        params_shape = inputs_shape[-1:]\n\n        mean, variance = tf.nn.moments(inputs, [-1], keep_dims=True)\n        beta = tf.Variable(tf.zeros(params_shape))\n        gamma = tf.Variable(tf.ones(params_shape))\n        normalized = (inputs - mean) / ((variance + epsilon) ** (.5))\n        outputs = gamma * normalized + beta\n\n    return outputs\n\n\ndef multihead_attention(queries,\n                        keys,\n                        scope=""multihead_attention"",\n                        num_units=None,\n                        num_heads=4,\n                        dropout_rate=0,\n                        is_training=True,\n                        causality=False):\n    \'\'\'Applies multihead attention.\n\n    Args:\n      queries: A 3d tensor with shape of [N, T_q, C_q].\n      keys: A 3d tensor with shape of [N, T_k, C_k].\n      num_units: A cdscalar. Attention size.\n      dropout_rate: A floating point number.\n      is_training: Boolean. Controller of mechanism for dropout.\n      causality: Boolean. If true, units that reference the future are masked.\n      num_heads: An int. Number of heads.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns\n      A 3d tensor with shape of (N, T_q, C)\n    \'\'\'\n    global look5\n    with tf.variable_scope(scope):\n        # Set the fall back option for num_units\n        if num_units is None:\n            num_units = queries.get_shape().as_list()[-1]\n\n        Q_ = []\n        K_ = []\n        V_ = []\n        for head_i in range(num_heads):\n            Q = tf.layers.dense(queries, num_units / num_heads,\n                                activation=tf.nn.relu, name=\'Query\' + str(head_i))  # (N, T_q, C)\n            K = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu, name=\'Key\' + str(head_i))  # (N, T_k, C)\n            V = tf.layers.dense(keys, num_units / num_heads,\n                                activation=tf.nn.relu, name=\'Value\' + str(head_i))  # (N, T_k, C)\n            Q_.append(Q)\n            K_.append(K)\n            V_.append(V)\n\n        # Split and concat\n        Q_ = tf.concat(Q_, axis=0)  # (h*N, T_q, C/h)\n        K_ = tf.concat(K_, axis=0)  # (h*N, T_k, C/h)\n        V_ = tf.concat(V_, axis=0)  # (h*N, T_k, C/h)\n\n        # Multiplication\n        outputs = tf.matmul(Q_, tf.transpose(K_, [0, 2, 1]))  # (h*N, T_q, T_k)\n\n        # Scale\n        outputs = outputs / (K_.get_shape().as_list()[-1] ** 0.5)\n\n        # Key Masking\n        key_masks = tf.sign(tf.abs(tf.reduce_sum(keys, axis=-1)))  # (N, T_k)\n        key_masks = tf.tile(key_masks, [num_heads, 1])  # (h*N, T_k)\n        key_masks = tf.tile(tf.expand_dims(key_masks, 1),\n                            [1, tf.shape(queries)[1], 1])  # (h*N, T_q, T_k)\n\n        paddings = tf.ones_like(outputs) * (-2 ** 32 + 1)\n        outputs = tf.where(tf.equal(key_masks, 0), paddings,\n                           outputs)  # (h*N, T_q, T_k)\n\n        # Causality = Future blinding\n        if causality:\n            diag_vals = tf.ones_like(outputs[0, :, :])  # (T_q, T_k)\n            tril = tf.contrib.linalg.LinearOperatorTriL(\n                diag_vals).to_dense()  # (T_q, T_k)\n            masks = tf.tile(tf.expand_dims(tril, 0),\n                            [tf.shape(outputs)[0], 1, 1])  # (h*N, T_q, T_k)\n\n            paddings = tf.ones_like(masks) * (-2 ** 32 + 1)\n            outputs = tf.where(tf.equal(masks, 0), paddings,\n                               outputs)  # (h*N, T_q, T_k)\n\n        # Activation\n        look5 = outputs\n        outputs = tf.nn.softmax(outputs)  # (h*N, T_q, T_k)\n\n        # Query Masking\n        query_masks = tf.sign(\n            tf.abs(tf.reduce_sum(queries, axis=-1)))  # (N, T_q)\n        query_masks = tf.tile(query_masks, [num_heads, 1])  # (h*N, T_q)\n        query_masks = tf.tile(tf.expand_dims(\n            query_masks, -1), [1, 1, tf.shape(keys)[1]])  # (h*N, T_q, T_k)\n        outputs *= query_masks  # broadcasting. (N, T_q, C)\n\n        # Dropouts\n        outputs = dropout(outputs, dropout_rate, is_training)\n\n        # Weighted sum\n        outputs = tf.matmul(outputs, V_)  # ( h*N, T_q, C/h)\n\n        # Restore shape\n        outputs = tf.concat(tf.split(outputs, num_heads,\n                                     axis=0), axis=2)  # (N, T_q, C)\n\n        # Residual connection\n        if queries.get_shape().as_list()[-1] == num_units:\n            outputs += queries\n\n        # Normalize\n        outputs = normalize(outputs, scope=scope)  # (N, T_q, C)\n\n    return outputs\n\n\ndef positional_encoding(inputs,\n                        num_units=None,\n                        zero_pad=True,\n                        scale=True,\n                        scope=""positional_encoding"",\n                        reuse=None):\n    \'\'\'\n    Return positinal embedding.\n    \'\'\'\n    Shape = tf.shape(inputs)\n    N = Shape[0]\n    T = Shape[1]\n    num_units = Shape[2]\n    with tf.variable_scope(scope, reuse=reuse):\n        position_ind = tf.tile(tf.expand_dims(tf.range(T), 0), [N, 1])\n\n        # First part of the PE function: sin and cos argument\n        #  Second part, apply the cosine to even columns and sin to odds.\n        X = tf.expand_dims(tf.cast(tf.range(T), tf.float32), axis=1)\n        Y = tf.expand_dims(\n            tf.cast(10000 ** -(2 * tf.range(num_units) / num_units), tf.float32), axis=0)\n        h1 = tf.cast((tf.range(num_units) + 1) % 2, tf.float32)\n        h2 = tf.cast((tf.range(num_units) % 2), tf.float32)\n        position_enc = tf.multiply(X, Y)\n        position_enc = tf.sin(position_enc) * tf.multiply(tf.ones_like(X), h1) + \\\n            tf.cos(position_enc) * tf.multiply(tf.ones_like(X), h2)\n\n        # Convert to a tensor\n        lookup_table = position_enc\n\n        if zero_pad:\n            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n                                      lookup_table[1:, :]), 0)\n        outputs = tf.nn.embedding_lookup(lookup_table, position_ind)\n\n        if scale:\n            outputs = outputs * tf.sqrt(tf.cast(num_units, tf.float32))\n\n        return outputs\n\n\ndef feedforward(inputs,\n                num_units,\n                scope=""multihead_attention""):\n    \'\'\'Point-wise feed forward net.\n\n    Args:\n      inputs: A 3d tensor with shape of [N, T, C].\n      num_units: A list of two integers.\n      scope: Optional scope for `variable_scope`.\n      reuse: Boolean, whether to reuse the weights of a previous layer\n        by the same name.\n\n    Returns:\n      A 3d tensor with the same shape and dtype as inputs\n    \'\'\'\n    with tf.variable_scope(scope):\n        # Inner layer\n        params = {""inputs"": inputs, ""filters"": num_units[0], ""kernel_size"": 1,\n                  ""activation"": tf.nn.relu, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Readout layer\n        params = {""inputs"": outputs, ""filters"": num_units[1], ""kernel_size"": 1,\n                  ""activation"": None, ""use_bias"": True}\n        outputs = tf.layers.conv1d(**params)\n\n        # Residual connection\n        outputs += inputs\n\n        # Normalize\n        outputs = normalize(outputs)\n\n    return outputs\n\n\ndef rnn(input_states, sequence_lengths, dropout_rate, is_training, num_units):\n    layer_cnt = 1\n    states = []\n    xs = tf.transpose(input_states, perm=[1, 0, 2])\n    for i in range(0, layer_cnt):\n        xs = dropout(xs, dropout_rate, is_training)\n        with tf.variable_scope(\'layer_\' + str(i)):\n            cell_fw = XGRUCell(num_units)\n            cell_bw = XGRUCell(num_units)\n            outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                dtype=tf.float32,\n                sequence_length=sequence_lengths,\n                inputs=xs,\n                time_major=True)\n\n        y_lr, y_rl = outputs\n        xs = tf.concat([y_lr, y_rl], 2)\n        states.append(xs)\n\n    return tf.transpose(dropout(tf.concat(states, axis=2),\n                                dropout_rate,\n                                is_training), perm=[1, 0, 2])\n\n\ndef graph_to_network(input1,\n                     input2,\n                     input1_lengths,\n                     input2_lengths,\n                     p_graph,\n                     dropout_rate,\n                     is_training,\n                     num_heads=1,\n                     rnn_units=256):\n    topology = p_graph.is_topology()\n    layers = dict()\n    layers_sequence_lengths = dict()\n    num_units = input1.get_shape().as_list()[-1]\n    layers[0] = input1*tf.sqrt(tf.cast(num_units, tf.float32)) + \\\n        positional_encoding(input1, scale=False, zero_pad=False)\n    layers[1] = input2*tf.sqrt(tf.cast(num_units, tf.float32))\n    layers[0] = dropout(layers[0], dropout_rate, is_training)\n    layers[1] = dropout(layers[1], dropout_rate, is_training)\n    layers_sequence_lengths[0] = input1_lengths\n    layers_sequence_lengths[1] = input2_lengths\n    for _, topo_i in enumerate(topology):\n        if topo_i == \'|\':\n            continue\n\n        # Note: here we use the `hash_id` of layer as scope name,\n        #       so that we can automatically load sharable weights from previous trained models\n        with tf.variable_scope(p_graph.layers[topo_i].hash_id, reuse=tf.AUTO_REUSE):\n            if p_graph.layers[topo_i].graph_type == LayerType.input.value:\n                continue\n            elif p_graph.layers[topo_i].graph_type == LayerType.attention.value:\n                with tf.variable_scope(\'attention\'):\n                    layer = multihead_attention(layers[p_graph.layers[topo_i].input[0]],\n                                                layers[p_graph.layers[topo_i].input[1]],\n                                                scope=""multihead_attention"",\n                                                dropout_rate=dropout_rate,\n                                                is_training=is_training,\n                                                num_heads=num_heads,\n                                                num_units=rnn_units * 2)\n                    layer = feedforward(layer, scope=""feedforward"",\n                                        num_units=[rnn_units * 2 * 4, rnn_units * 2])\n                layers[topo_i] = layer\n                layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                    p_graph.layers[topo_i].input[0]]\n            elif p_graph.layers[topo_i].graph_type == LayerType.self_attention.value:\n                with tf.variable_scope(\'self-attention\'):\n                    layer = multihead_attention(layers[p_graph.layers[topo_i].input[0]],\n                                                layers[p_graph.layers[topo_i].input[0]],\n                                                scope=""multihead_attention"",\n                                                dropout_rate=dropout_rate,\n                                                is_training=is_training,\n                                                num_heads=num_heads,\n                                                num_units=rnn_units * 2)\n                    layer = feedforward(layer, scope=""feedforward"",\n                                        num_units=[rnn_units * 2 * 4, rnn_units * 2])\n                layers[topo_i] = layer\n                layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                    p_graph.layers[topo_i].input[0]]\n            elif p_graph.layers[topo_i].graph_type == LayerType.rnn.value:\n                with tf.variable_scope(\'rnn\'):\n                    layer = rnn(layers[p_graph.layers[topo_i].input[0]],\n                                layers_sequence_lengths[p_graph.layers[topo_i].input[0]],\n                                dropout_rate,\n                                is_training,\n                                rnn_units)\n                layers[topo_i] = layer\n                layers_sequence_lengths[topo_i] = layers_sequence_lengths[\n                    p_graph.layers[topo_i].input[0]]\n            elif p_graph.layers[topo_i].graph_type == LayerType.output.value:\n                layers[topo_i] = layers[p_graph.layers[topo_i].input[0]]\n                if layers[topo_i].get_shape().as_list()[-1] != rnn_units * 1 * 2:\n                    with tf.variable_scope(\'add_dense\'):\n                        layers[topo_i] = tf.layers.dense(\n                            layers[topo_i], units=rnn_units*2)\n    return layers[2], layers[3]\n'"
examples/trials/weight_sharing/ga_squad/rnn.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport tensorflow as tf\nfrom tensorflow.python.ops.rnn_cell_impl import RNNCell\n\n\nclass GRU:\n    \'\'\'\n    GRU class.\n    \'\'\'\n    def __init__(self, name, input_dim, hidden_dim):\n        self.name = \'/\'.join([name, \'gru\'])\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n        self.w_matrix = None\n        self.U = None\n        self.bias = None\n\n    def define_params(self):\n        \'\'\'\n        Define parameters.\n        \'\'\'\n        input_dim = self.input_dim\n        hidden_dim = self.hidden_dim\n        prefix = self.name\n        self.w_matrix = tf.Variable(tf.random_normal([input_dim, 3 * hidden_dim], stddev=0.1),\n                                    name=\'/\'.join([prefix, \'W\']))\n        self.U = tf.Variable(tf.random_normal([hidden_dim, 3 * hidden_dim], stddev=0.1),\n                             name=\'/\'.join([prefix, \'U\']))\n        self.bias = tf.Variable(tf.random_normal([1, 3 * hidden_dim], stddev=0.1),\n                                name=\'/\'.join([prefix, \'b\']))\n        return self\n\n    def build(self, x, h, mask=None):\n        \'\'\'\n        Build the GRU cell.\n        \'\'\'\n        xw = tf.split(tf.matmul(x, self.w_matrix) + self.bias, 3, 1)\n        hu = tf.split(tf.matmul(h, self.U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = tf.tanh(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + h * z\n        if mask is not None:\n            next_h = next_h * mask + h * (1 - mask)\n        return next_h\n\n    def build_sequence(self, xs, masks, init, is_left_to_right):\n        \'\'\'\n        Build GRU sequence.\n        \'\'\'\n        states = []\n        last = init\n        if is_left_to_right:\n            for i, xs_i in enumerate(xs):\n                h = self.build(xs_i, last, masks[i])\n                states.append(h)\n                last = h\n        else:\n            for i in range(len(xs) - 1, -1, -1):\n                h = self.build(xs[i], last, masks[i])\n                states.insert(0, h)\n                last = h\n        return states\n\n\nclass XGRUCell(RNNCell):\n\n    def __init__(self, hidden_dim, reuse=None):\n        super(XGRUCell, self).__init__(self, _reuse=reuse)\n        self._num_units = hidden_dim\n        self._activation = tf.tanh\n\n    @property\n    def state_size(self):\n        return self._num_units\n\n    @property\n    def output_size(self):\n        return self._num_units\n\n    def call(self, inputs, state):\n\n        input_dim = inputs.get_shape()[-1]\n        assert input_dim is not None, ""input dimension must be defined""\n        W = tf.get_variable(\n            name=""W"", shape=[input_dim, 3 * self._num_units], dtype=tf.float32)\n        U = tf.get_variable(\n            name=\'U\', shape=[self._num_units, 3 * self._num_units], dtype=tf.float32)\n        b = tf.get_variable(\n            name=\'b\', shape=[1, 3 * self._num_units], dtype=tf.float32)\n\n        xw = tf.split(tf.matmul(inputs, W) + b, 3, 1)\n        hu = tf.split(tf.matmul(state, U), 3, 1)\n        r = tf.sigmoid(xw[0] + hu[0])\n        z = tf.sigmoid(xw[1] + hu[1])\n        h1 = self._activation(xw[2] + r * hu[2])\n        next_h = h1 * (1 - z) + state * z\n        return next_h, next_h\n'"
examples/trials/weight_sharing/ga_squad/train_model.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nTrain the network combined by RNN and attention.\n\'\'\'\n\nimport tensorflow as tf\n\nfrom attention import DotAttention\nfrom rnn import XGRUCell\nfrom util import dropout\nfrom graph_to_tf import graph_to_network\n\n\nclass GAGConfig:\n    """"""The class for model hyper-parameter configuration.""""""\n    def __init__(self):\n        self.batch_size = 128\n\n        self.dropout = 0.1\n\n        self.char_vcb_size = 1500\n        self.max_char_length = 20\n        self.char_embed_dim = 100\n\n        self.max_query_length = 40\n        self.max_passage_length = 800\n\n        self.att_is_vanilla = True\n        self.att_need_padding = False\n        self.att_is_id = False\n\n        self.ptr_dim = 70\n        self.learning_rate = 0.1\n        self.labelsmoothing = 0.1\n        self.num_heads = 1\n        self.rnn_units = 256\n\n\nclass GAG:\n    """"""The class for the computation graph based QA model.""""""\n    def __init__(self, cfg, embed, p_graph):\n        self.cfg = cfg\n        self.embed = embed\n        self.graph = p_graph\n\n        self.query_word = None\n        self.query_mask = None\n        self.query_lengths = None\n        self.passage_word = None\n        self.passage_mask = None\n        self.passage_lengths = None\n        self.answer_begin = None\n        self.answer_end = None\n        self.query_char_ids = None\n        self.query_char_lengths = None\n        self.passage_char_ids = None\n        self.passage_char_lengths = None\n        self.passage_states = None\n        self.query_states = None\n        self.query_init = None\n        self.begin_prob = None\n        self.end_prob = None\n        self.loss = None\n        self.train_op = None\n\n\n    def build_net(self, is_training):\n        """"""Build the whole neural network for the QA model.""""""\n        cfg = self.cfg\n        word_embed = tf.get_variable(\n            name=\'word_embed\', initializer=self.embed, dtype=tf.float32, trainable=False)\n        char_embed = tf.get_variable(name=\'char_embed\',\n                                        shape=[cfg.char_vcb_size,\n                                            cfg.char_embed_dim],\n                                        dtype=tf.float32)\n\n        # [query_length, batch_size]\n        self.query_word = tf.placeholder(dtype=tf.int32,\n                                         shape=[None, None],\n                                         name=\'query_word\')\n        self.query_mask = tf.placeholder(dtype=tf.float32,\n                                         shape=[None, None],\n                                         name=\'query_mask\')\n        # [batch_size]\n        self.query_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name=\'query_lengths\')\n\n        # [passage_length, batch_size]\n        self.passage_word = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name=\'passage_word\')\n        self.passage_mask = tf.placeholder(\n            dtype=tf.float32, shape=[None, None], name=\'passage_mask\')\n        # [batch_size]\n        self.passage_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None], name=\'passage_lengths\')\n\n        if is_training:\n            self.answer_begin = tf.placeholder(\n                dtype=tf.int32, shape=[None], name=\'answer_begin\')\n            self.answer_end = tf.placeholder(\n                dtype=tf.int32, shape=[None], name=\'answer_end\')\n\n        self.query_char_ids = tf.placeholder(dtype=tf.int32,\n                                             shape=[\n                                                 self.cfg.max_char_length, None, None],\n                                             name=\'query_char_ids\')\n        # sequence_length, batch_size\n        self.query_char_lengths = tf.placeholder(\n            dtype=tf.int32, shape=[None, None], name=\'query_char_lengths\')\n\n        self.passage_char_ids = tf.placeholder(dtype=tf.int32,\n                                               shape=[\n                                                   self.cfg.max_char_length, None, None],\n                                               name=\'passage_char_ids\')\n        # sequence_length, batch_size\n        self.passage_char_lengths = tf.placeholder(dtype=tf.int32,\n                                                   shape=[None, None],\n                                                   name=\'passage_char_lengths\')\n\n        query_char_states = self.build_char_states(char_embed=char_embed,\n                                                   is_training=is_training,\n                                                   reuse=False,\n                                                   char_ids=self.query_char_ids,\n                                                   char_lengths=self.query_char_lengths)\n\n        passage_char_states = self.build_char_states(char_embed=char_embed,\n                                                     is_training=is_training,\n                                                     reuse=True,\n                                                     char_ids=self.passage_char_ids,\n                                                     char_lengths=self.passage_char_lengths)\n\n        with tf.variable_scope(""encoding"") as scope:\n            query_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.query_word), query_char_states], axis=2)\n            scope.reuse_variables()\n            passage_states = tf.concat([tf.nn.embedding_lookup(\n                word_embed, self.passage_word), passage_char_states], axis=2)\n        passage_states = tf.transpose(passage_states, perm=[1, 0, 2])\n        query_states = tf.transpose(query_states, perm=[1, 0, 2])\n        self.passage_states = passage_states\n        self.query_states = query_states\n\n        output, output2 = graph_to_network(passage_states, query_states,\n                                           self.passage_lengths, self.query_lengths,\n                                           self.graph, self.cfg.dropout,\n                                           is_training, num_heads=cfg.num_heads,\n                                           rnn_units=cfg.rnn_units)\n\n        passage_att_mask = self.passage_mask\n        batch_size_x = tf.shape(self.query_lengths)\n        answer_h = tf.zeros(\n            tf.concat([batch_size_x, tf.constant([cfg.ptr_dim], dtype=tf.int32)], axis=0))\n\n        answer_context = tf.reduce_mean(output2, axis=1)\n\n        query_init_w = tf.get_variable(\n            \'query_init_w\', shape=[output2.get_shape().as_list()[-1], cfg.ptr_dim])\n        self.query_init = query_init_w\n        answer_context = tf.matmul(answer_context, query_init_w)\n\n        output = tf.transpose(output, perm=[1, 0, 2])\n\n        with tf.variable_scope(\'answer_ptr_layer\'):\n            ptr_att = DotAttention(\'ptr\',\n                                   hidden_dim=cfg.ptr_dim,\n                                   is_vanilla=self.cfg.att_is_vanilla,\n                                   is_identity_transform=self.cfg.att_is_id,\n                                   need_padding=self.cfg.att_need_padding)\n            answer_pre_compute = ptr_att.get_pre_compute(output)\n            ptr_gru = XGRUCell(hidden_dim=cfg.ptr_dim)\n            begin_prob, begin_logits = ptr_att.get_prob(output, answer_context, passage_att_mask,\n                                                        answer_pre_compute, True)\n            att_state = ptr_att.get_att(output, begin_prob)\n            (_, answer_h) = ptr_gru.call(inputs=att_state, state=answer_h)\n            answer_context = answer_h\n            end_prob, end_logits = ptr_att.get_prob(output, answer_context,\n                                                    passage_att_mask, answer_pre_compute,\n                                                    True)\n\n        self.begin_prob = tf.transpose(begin_prob, perm=[1, 0])\n        self.end_prob = tf.transpose(end_prob, perm=[1, 0])\n        begin_logits = tf.transpose(begin_logits, perm=[1, 0])\n        end_logits = tf.transpose(end_logits, perm=[1, 0])\n\n        if is_training:\n            def label_smoothing(inputs, masks, epsilon=0.1):\n                """"""Modify target for label smoothing.""""""\n                epsilon = cfg.labelsmoothing\n                num_of_channel = tf.shape(inputs)[-1]  # number of channels\n                inputs = tf.cast(inputs, tf.float32)\n                return (((1 - epsilon) * inputs) + (epsilon /\n                                                    tf.cast(num_of_channel, tf.float32))) * masks\n            cost1 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(label_smoothing(\n                    tf.one_hot(self.answer_begin,\n                               depth=tf.shape(self.passage_word)[0]),\n                    tf.transpose(self.passage_mask, perm=[1, 0])), begin_logits))\n            cost2 = tf.reduce_mean(\n                tf.losses.softmax_cross_entropy(\n                    label_smoothing(tf.one_hot(self.answer_end,\n                                               depth=tf.shape(self.passage_word)[0]),\n                                    tf.transpose(self.passage_mask, perm=[1, 0])), end_logits))\n\n            reg_ws = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n            l2_loss = tf.reduce_sum(reg_ws)\n            loss = cost1 + cost2 + l2_loss\n            self.loss = loss\n\n            optimizer = tf.train.AdamOptimizer(learning_rate=cfg.learning_rate)\n            self.train_op = optimizer.minimize(self.loss)\n\n        return tf.stack([self.begin_prob, self.end_prob])\n\n    def build_char_states(self, char_embed, is_training, reuse, char_ids, char_lengths):\n        """"""Build char embedding network for the QA model.""""""\n        max_char_length = self.cfg.max_char_length\n\n        inputs = dropout(tf.nn.embedding_lookup(char_embed, char_ids),\n                         self.cfg.dropout, is_training)\n        inputs = tf.reshape(\n            inputs, shape=[max_char_length, -1, self.cfg.char_embed_dim])\n        char_lengths = tf.reshape(char_lengths, shape=[-1])\n        with tf.variable_scope(\'char_encoding\', reuse=reuse):\n            cell_fw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            cell_bw = XGRUCell(hidden_dim=self.cfg.char_embed_dim)\n            _, (left_right, right_left) = tf.nn.bidirectional_dynamic_rnn(\n                cell_fw=cell_fw,\n                cell_bw=cell_bw,\n                sequence_length=char_lengths,\n                inputs=inputs,\n                time_major=True,\n                dtype=tf.float32\n            )\n\n        left_right = tf.reshape(left_right, shape=[-1, self.cfg.char_embed_dim])\n\n        right_left = tf.reshape(right_left, shape=[-1, self.cfg.char_embed_dim])\n\n        states = tf.concat([left_right, right_left], axis=1)\n        out_shape = tf.shape(char_ids)[1:3]\n        out_shape = tf.concat([out_shape, tf.constant(\n            value=[self.cfg.char_embed_dim * 2], dtype=tf.int32)], axis=0)\n        return tf.reshape(states, shape=out_shape)\n'"
examples/trials/weight_sharing/ga_squad/trial.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport heapq\nimport json\nimport os\nimport pickle\n\nimport logging\nlogger = logging.getLogger(\'ga_squad\')\n\nimport numpy as np\nfrom tensorflow.train import init_from_checkpoint\n\nimport graph\n\nfrom util import Timer\n\nimport nni\nimport data\nimport evaluate\nfrom train_model import *\n\n\nos.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = \'3\'\n\n\ndef get_config():\n    \'\'\'\n    Get config from arument parser.\n    \'\'\'\n    parser = argparse.ArgumentParser(\n        description=\'This program is using genetic algorithm to search architecture for SQuAD.\')\n    parser.add_argument(\'--input_file\', type=str,\n                        default=\'./train-v1.1.json\', help=\'input file\')\n    parser.add_argument(\'--dev_file\', type=str,\n                        default=\'./dev-v1.1.json\', help=\'dev file\')\n    parser.add_argument(\'--embedding_file\', type=str,\n                        default=\'./glove.840B.300d.txt\', help=\'dev file\')\n    parser.add_argument(\'--root_path\', default=\'./data/\',\n                        type=str, help=\'Root path of models\')\n    parser.add_argument(\'--batch_size\', type=int, default=64, help=\'batch size\')\n    parser.add_argument(\'--save_path\', type=str,\n                        default=\'./save\', help=\'save path dir\')\n    parser.add_argument(\'--learning_rate\', type=float, default=0.0001,\n                        help=\'set half of original learning rate reload data and train.\')\n    parser.add_argument(\'--max_epoch\', type=int, default=30)\n    parser.add_argument(\'--dropout_rate\', type=float,\n                        default=0.1, help=\'dropout_rate\')\n    parser.add_argument(\'--labelsmoothing\', type=float,\n                        default=0.1, help=\'labelsmoothing\')\n    parser.add_argument(\'--num_heads\', type=int, default=1, help=\'num_heads\')\n    parser.add_argument(\'--rnn_units\', type=int, default=256, help=\'rnn_units\')\n\n    args = parser.parse_args()\n    return args\n\n\ndef get_id(word_dict, word):\n    \'\'\'\n    Return word id.\n    \'\'\'\n    if word in word_dict.keys():\n        return word_dict[word]\n    return word_dict[\'<unk>\']\n\n\ndef load_embedding(path):\n    \'\'\'\n    return embedding for a specif file by given file path.\n    \'\'\'\n    EMBEDDING_DIM = 300\n    embedding_dict = {}\n    with open(path, \'r\', encoding=\'utf-8\') as file:\n        pairs = [line.strip(\'\\r\\n\').split() for line in file.readlines()]\n        for pair in pairs:\n            if len(pair) == EMBEDDING_DIM + 1:\n                embedding_dict[pair[0]] = [float(x) for x in pair[1:]]\n    logger.debug(\'embedding_dict size: %d\', len(embedding_dict))\n    return embedding_dict\n\n\nclass MaxQueue:\n    \'\'\'\n    Queue for max value.\n    \'\'\'\n\n    def __init__(self, capacity):\n        assert capacity > 0, \'queue size must be larger than 0\'\n        self._capacity = capacity\n        self._entries = []\n\n    @property\n    def entries(self):\n        return self._entries\n\n    @property\n    def capacity(self):\n        return self._capacity\n\n    @property\n    def size(self):\n        return len(self._entries)\n\n    def clear(self):\n        self._entries = []\n\n    def push(self, item):\n        if self.size < self.capacity:\n            heapq.heappush(self.entries, item)\n        else:\n            heapq.heappushpop(self.entries, item)\n\n\ndef find_best_answer_span(left_prob, right_prob, passage_length, max_answer_length):\n    left = 0\n    right = 0\n    max_prob = left_prob[0] * right_prob[0]\n    for i in range(0, passage_length):\n        left_p = left_prob[i]\n        for j in range(i, min(i + max_answer_length, passage_length)):\n            total_prob = left_p * right_prob[j]\n            if max_prob < total_prob:\n                left, right, max_prob = i, j, total_prob\n    return [(max_prob, left, right)]\n\n\ndef write_prediction(path, position1_result, position2_result):\n    import codecs\n\n    with codecs.open(path, \'w\', encoding=\'utf8\') as file:\n        batch_num = len(position1_result)\n        for i in range(batch_num):\n            position1_batch = position1_result[i]\n            position2_batch = position2_result[i]\n\n            for j in range(position1_batch.shape[0]):\n                file.write(str(position1_batch[j]) +\n                           \'\\t\' + str(position2_batch[j]) + \'\\n\')\n\n\ndef find_kbest_answer_span(k, left_prob, right_prob, passage_length, max_answer_length):\n    if k == 1:\n        return find_best_answer_span(left_prob, right_prob, passage_length, max_answer_length)\n\n    queue = MaxQueue(k)\n    for i in range(0, passage_length):\n        left_p = left_prob[i]\n        for j in range(i, min(i + max_answer_length, passage_length)):\n            total_prob = left_p * right_prob[j]\n            queue.push((total_prob, i, j))\n    return list(sorted(queue.entries, key=lambda x: -x[0]))\n\n\ndef run_epoch(batches, answer_net, is_training):\n    if not is_training:\n        position1_result = []\n        position2_result = []\n        contexts = []\n        ids = []\n\n    loss_sum = 0\n    timer = Timer()\n    count = 0\n    for batch in batches:\n        used = timer.get_elapsed(False)\n        count += 1\n        qps = batch[\'qp_pairs\']\n        question_tokens = [qp[\'question_tokens\'] for qp in qps]\n        passage_tokens = [qp[\'passage_tokens\'] for qp in qps]\n        context = [(qp[\'passage\'], qp[\'passage_tokens\']) for qp in qps]\n        sample_id = [qp[\'id\'] for qp in qps]\n\n        _, query, query_mask, query_lengths = data.get_word_input(\n            data=question_tokens, word_dict=word_vcb, embed=embed, embed_dim=cfg.word_embed_dim)\n        _, passage, passage_mask, passage_lengths = data.get_word_input(\n            data=passage_tokens, word_dict=word_vcb, embed=embed, embed_dim=cfg.word_embed_dim)\n\n        query_char, query_char_lengths = data.get_char_input(\n            data=question_tokens, char_dict=char_vcb, max_char_length=cfg.max_char_length)\n\n        passage_char, passage_char_lengths = data.get_char_input(\n            data=passage_tokens, char_dict=char_vcb, max_char_length=cfg.max_char_length)\n\n        if is_training:\n            answer_begin, answer_end = data.get_answer_begin_end(qps)\n\n        if is_training:\n            feed_dict = {answer_net.query_word: query,\n                         answer_net.query_mask: query_mask,\n                         answer_net.query_lengths: query_lengths,\n                         answer_net.passage_word: passage,\n                         answer_net.passage_mask: passage_mask,\n                         answer_net.passage_lengths: passage_lengths,\n                         answer_net.query_char_ids: query_char,\n                         answer_net.query_char_lengths: query_char_lengths,\n                         answer_net.passage_char_ids: passage_char,\n                         answer_net.passage_char_lengths: passage_char_lengths,\n                         answer_net.answer_begin: answer_begin,\n                         answer_net.answer_end: answer_end}\n            loss, _, = sess.run(\n                [answer_net.loss, answer_net.train_op], feed_dict=feed_dict)\n            if count % 100 == 0:\n                logger.debug(\'%d %g except:%g, loss:%g\' %\n                             (count, used, used / count * len(batches), loss))\n            loss_sum += loss\n        else:\n            feed_dict = {answer_net.query_word: query,\n                         answer_net.query_mask: query_mask,\n                         answer_net.query_lengths: query_lengths,\n                         answer_net.passage_word: passage,\n                         answer_net.passage_mask: passage_mask,\n                         answer_net.passage_lengths: passage_lengths,\n                         answer_net.query_char_ids: query_char,\n                         answer_net.query_char_lengths: query_char_lengths,\n                         answer_net.passage_char_ids: passage_char,\n                         answer_net.passage_char_lengths: passage_char_lengths}\n            position1, position2 = sess.run(\n                [answer_net.begin_prob, answer_net.end_prob], feed_dict=feed_dict)\n            position1_result += position1.tolist()\n            position2_result += position2.tolist()\n            contexts += context\n            ids = np.concatenate((ids, sample_id))\n            if count % 100 == 0:\n                logger.debug(\'%d %g except:%g\' %\n                             (count, used, used / count * len(batches)))\n    loss = loss_sum / len(batches)\n    if is_training:\n        return loss\n    return loss, position1_result, position2_result, ids, contexts\n\n\ndef generate_predict_json(position1_result, position2_result, ids, passage_tokens):\n    \'\'\'\n    Generate json by prediction.\n    \'\'\'\n    predict_len = len(position1_result)\n    logger.debug(\'total prediction num is %s\', str(predict_len))\n\n    answers = {}\n    for i in range(predict_len):\n        sample_id = ids[i]\n        passage, tokens = passage_tokens[i]\n        kbest = find_best_answer_span(\n            position1_result[i], position2_result[i], len(tokens), 23)\n        _, start, end = kbest[0]\n        answer = passage[tokens[start][\'char_begin\']:tokens[end][\'char_end\']]\n        answers[sample_id] = answer\n    logger.debug(\'generate predict done.\')\n    return answers\n\n\ndef generate_data(path, tokenizer, char_vcb, word_vcb, is_training=False):\n    \'\'\'\n    Generate data\n    \'\'\'\n    global root_path\n    qp_pairs = data.load_from_file(path=path, is_training=is_training)\n\n    tokenized_sent = 0\n    # qp_pairs = qp_pairs[:1000]1\n    for qp_pair in qp_pairs:\n        tokenized_sent += 1\n        data.tokenize(qp_pair, tokenizer, is_training)\n        for word in qp_pair[\'question_tokens\']:\n            word_vcb.add(word[\'word\'])\n            for char in word[\'word\']:\n                char_vcb.add(char)\n        for word in qp_pair[\'passage_tokens\']:\n            word_vcb.add(word[\'word\'])\n            for char in word[\'word\']:\n                char_vcb.add(char)\n\n    max_query_length = max(len(x[\'question_tokens\']) for x in qp_pairs)\n    max_passage_length = max(len(x[\'passage_tokens\']) for x in qp_pairs)\n    #min_passage_length = min(len(x[\'passage_tokens\']) for x in qp_pairs)\n    cfg.max_query_length = max_query_length\n    cfg.max_passage_length = max_passage_length\n\n    return qp_pairs\n\n\ndef train_with_graph(p_graph, qp_pairs, dev_qp_pairs):\n    \'\'\'\n    Train a network from a specific graph.\n    \'\'\'\n    global sess\n    with tf.Graph().as_default():\n        train_model = GAG(cfg, embed, p_graph)\n        train_model.build_net(is_training=True)\n        tf.get_variable_scope().reuse_variables()\n        dev_model = GAG(cfg, embed, p_graph)\n        dev_model.build_net(is_training=False)\n        with tf.Session() as sess:\n            if restore_path is not None:\n                restore_mapping = dict(zip(restore_shared, restore_shared))\n                logger.debug(\'init shared variables from {}, restore_scopes: {}\'.format(restore_path, restore_shared))\n                init_from_checkpoint(restore_path, restore_mapping)\n            logger.debug(\'init variables\')\n            logger.debug(sess.run(tf.report_uninitialized_variables()))\n            init = tf.global_variables_initializer()\n            sess.run(init)\n            # writer = tf.summary.FileWriter(\'%s/graph/\'%execution_path, sess.graph)\n            logger.debug(\'assign to graph\')\n\n            saver = tf.train.Saver()\n            train_loss = None\n            bestacc = 0\n            patience = 5\n            patience_increase = 2\n            improvement_threshold = 0.995\n\n            for epoch in range(max_epoch):\n                logger.debug(\'begin to train\')\n                train_batches = data.get_batches(qp_pairs, cfg.batch_size)\n                train_loss = run_epoch(train_batches, train_model, True)\n                logger.debug(\'epoch \' + str(epoch) +\n                             \' loss: \' + str(train_loss))\n                dev_batches = list(data.get_batches(\n                    dev_qp_pairs, cfg.batch_size))\n                _, position1, position2, ids, contexts = run_epoch(\n                    dev_batches, dev_model, False)\n\n                answers = generate_predict_json(\n                    position1, position2, ids, contexts)\n                if save_path is not None:\n                    logger.info(\'save prediction file to {}\'.format(save_path))\n                    with open(os.path.join(save_path, \'epoch%d.prediction\' % epoch), \'w\') as file:\n                        json.dump(answers, file)\n                else:\n                    answers = json.dumps(answers)\n                    answers = json.loads(answers)\n                iter = epoch + 1\n\n                acc = evaluate.evaluate_with_predictions(\n                    args.dev_file, answers)\n\n                logger.debug(\'Send intermediate acc: %s\', str(acc))\n                nni.report_intermediate_result(acc)\n\n                logger.debug(\'Send intermediate result done.\')\n\n                if acc > bestacc:\n                    if acc * improvement_threshold > bestacc:\n                        patience = max(patience, iter * patience_increase)\n                    bestacc = acc\n\n                    if save_path is not None:\n                        logger.info(\'save model & prediction to {}\'.format(save_path))\n                        saver.save(sess, os.path.join(save_path, \'epoch%d.model\' % epoch))\n                        with open(os.path.join(save_path, \'epoch%d.score\' % epoch), \'wb\') as file:\n                            pickle.dump(\n                                (position1, position2, ids, contexts), file)\n                logger.debug(\'epoch %d acc %g bestacc %g\' %\n                             (epoch, acc, bestacc))\n                if patience <= iter:\n                    break\n            logger.debug(\'save done.\')\n    return train_loss, bestacc\n\n\nembed = None\nchar_vcb = None\ntokenizer = None\nword_vcb = None\n\n\ndef load_data():\n    global embed, char_vcb, tokenizer, word_vcb\n    logger.debug(\'tokenize data\')\n    tokenizer = data.WhitespaceTokenizer()\n\n    char_set = set()\n    word_set = set()\n    logger.debug(\'generate train data\')\n    qp_pairs = generate_data(input_file, tokenizer,\n                             char_set, word_set, is_training=True)\n    logger.debug(\'generate dev data\')\n    dev_qp_pairs = generate_data(\n        dev_file, tokenizer, char_set, word_set, is_training=False)\n    logger.debug(\'generate data done.\')\n\n    char_vcb = {char: sample_id for sample_id, char in enumerate(char_set)}\n    word_vcb = {word: sample_id for sample_id, word in enumerate(word_set)}\n\n    timer.start()\n    logger.debug(\'read embedding table\')\n\n    cfg.word_embed_dim = 300\n    embed = np.zeros((len(word_vcb), cfg.word_embed_dim), dtype=np.float32)\n\n    embedding = load_embedding(args.embedding_file)\n    for word, sample_id in enumerate(word_vcb):\n        if word in embedding:\n            embed[sample_id] = embedding[word]\n\n    # add UNK into dict\n    unk = np.zeros((1, cfg.word_embed_dim), dtype=np.float32)\n    embed = np.concatenate((unk, embed), axis=0)\n    word_vcb = {key: value + 1 for key, value in word_vcb.items()}\n\n    return qp_pairs, dev_qp_pairs\n\n\nif __name__ == \'__main__\':\n    try:\n        args = get_config()\n\n        root_path = os.path.expanduser(args.root_path)\n        input_file = os.path.expanduser(args.input_file)\n        dev_file = os.path.expanduser(args.dev_file)\n        max_epoch = args.max_epoch\n\n        cfg = GAGConfig()\n        cfg.batch_size = args.batch_size\n        cfg.learning_rate = float(args.learning_rate)\n        cfg.dropout = args.dropout_rate\n        cfg.rnn_units = args.rnn_units\n        cfg.labelsmoothing = args.labelsmoothing\n        cfg.num_heads = args.num_heads\n        timer = Timer()\n\n        qp_pairs, dev_qp_pairs = load_data()\n        logger.debug(\'Init finish.\')\n\n        original_params = nni.get_next_parameter()\n        \'\'\'\n        with open(\'data.json\') as f:\n            original_params = json.load(f)\n        \'\'\'\n        p_graph = graph.graph_loads(original_params[\'graph\'])\n        save_path = original_params[\'save_dir\']\n        os.makedirs(save_path)\n        restore_path = original_params[\'restore_dir\']\n        restore_shared = [hash_id + \'/\' for hash_id in original_params[\'shared_id\']] if original_params[\'shared_id\'] is not None else [] + [\'word_embed\', \'char_embed\', \'char_encoding/\']\n        train_loss, best_acc = train_with_graph(p_graph, qp_pairs, dev_qp_pairs)\n\n        logger.debug(\'Send best acc: %s\', str(best_acc))\n        nni.report_final_result(best_acc)\n        logger.debug(\'Send final result done\')\n    except:\n        logger.exception(\'Catch exception in trial.py.\')\n        raise\n'"
examples/trials/weight_sharing/ga_squad/util.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\'\'\'\nUtil Module\n\'\'\'\n\nimport time\n\nimport tensorflow as tf\n\n\ndef shape(tensor):\n    \'\'\'\n    Get shape of variable.\n    Return type is tuple.\n    \'\'\'\n    temp_s = tensor.get_shape()\n    return tuple([temp_s[i].value for i in range(0, len(temp_s))])\n\n\ndef get_variable(name, temp_s):\n    \'\'\'\n    Get variable by name.\n    \'\'\'\n    return tf.Variable(tf.zeros(temp_s), name=name)\n\n\ndef dropout(tensor, drop_prob, is_training):\n    \'\'\'\n    Dropout except test.\n    \'\'\'\n    if not is_training:\n        return tensor\n    return tf.nn.dropout(tensor, 1.0 - drop_prob)\n\n\nclass Timer:\n    \'\'\'\n    Class Timer is for calculate time.\n    \'\'\'\n    def __init__(self):\n        self.__start = time.time()\n\n    def start(self):\n        \'\'\'\n        Start to calculate time.\n        \'\'\'\n        self.__start = time.time()\n\n    def get_elapsed(self, restart=True):\n        \'\'\'\n        Calculate time span.\n        \'\'\'\n        end = time.time()\n        span = end - self.__start\n        if restart:\n            self.__start = end\n        return span\n'"
examples/tuners/weight_sharing/ga_customer_tuner/__init__.py,0,b''
examples/tuners/weight_sharing/ga_customer_tuner/customer_tuner.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport copy\nimport json\nimport logging\nimport random\nimport os\n\nfrom threading import Event, Lock, current_thread\n\nfrom nni.tuner import Tuner\nfrom nni.utils import extract_scalar_reward\n\nfrom graph import Graph, Layer, LayerType, Enum, graph_dumps, graph_loads, unique\n\nlogger = logging.getLogger(\'ga_customer_tuner\')\n\n\n@unique\nclass OptimizeMode(Enum):\n    Minimize = \'minimize\'\n    Maximize = \'maximize\'\n\n\n\n\nclass Individual(object):\n    """"""\n    Basic Unit for evolution algorithm\n    """"""\n    def __init__(self, graph_cfg: Graph = None, info=None, result=None, indiv_id=None):\n        self.config = graph_cfg\n        self.result = result\n        self.info = info\n        self.indiv_id = indiv_id\n        self.parent_id = None\n        self.shared_ids = {layer.hash_id for layer in self.config.layers if layer.is_delete is False}\n\n    def __str__(self):\n        return ""info: "" + str(self.info) + "", config :"" + str(self.config) + "", result: "" + str(self.result)\n\n    def mutation(self, indiv_id: int, graph_cfg: Graph = None, info=None):\n        self.result = None\n        if graph_cfg is not None:\n            self.config = graph_cfg\n        self.config.mutation()\n        self.info = info\n        self.parent_id = self.indiv_id\n        self.indiv_id = indiv_id\n        self.shared_ids.intersection_update({layer.hash_id for layer in self.config.layers if layer.is_delete is False})\n\n\nclass CustomerTuner(Tuner):\n    """"""\n    NAS Tuner using Evolution Algorithm, with weight sharing enabled\n    """"""\n    def __init__(self, optimize_mode, save_dir_root, population_size=32, graph_max_layer=6, graph_min_layer=3):\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.indiv_counter = 0\n        self.events = []\n        self.thread_lock = Lock()\n        self.save_dir_root = save_dir_root\n        self.population = self.init_population(population_size, graph_max_layer, graph_min_layer)\n        assert len(self.population) == population_size\n        logger.debug(\'init population done.\')\n        return\n\n    def generate_new_id(self):\n        """"""\n        generate new id and event hook for new Individual\n        """"""\n        self.events.append(Event())\n        indiv_id = self.indiv_counter\n        self.indiv_counter += 1\n        return indiv_id\n\n    def save_dir(self, indiv_id):\n        if indiv_id is None:\n            return None\n        else:\n            return os.path.join(self.save_dir_root, str(indiv_id))\n\n    def init_population(self, population_size, graph_max_layer, graph_min_layer):\n        """"""\n        initialize populations for evolution tuner\n        """"""\n        population = []\n        graph = Graph(max_layer_num=graph_max_layer, min_layer_num=graph_min_layer,\n                      inputs=[Layer(LayerType.input.value, output=[4, 5], size=\'x\'), Layer(LayerType.input.value, output=[4, 5], size=\'y\')],\n                      output=[Layer(LayerType.output.value, inputs=[4], size=\'x\'), Layer(LayerType.output.value, inputs=[5], size=\'y\')],\n                      hide=[Layer(LayerType.attention.value, inputs=[0, 1], output=[2]),\n                            Layer(LayerType.attention.value, inputs=[1, 0], output=[3])])\n        for _ in range(population_size):\n            graph_tmp = copy.deepcopy(graph)\n            graph_tmp.mutation()\n            population.append(Individual(indiv_id=self.generate_new_id(), graph_cfg=graph_tmp, result=None))\n        return population\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""Returns a set of trial graph config, as a serializable object.\n        An example configuration:\n        ```json\n        {\n            ""shared_id"": [\n                ""4a11b2ef9cb7211590dfe81039b27670"",\n                ""370af04de24985e5ea5b3d72b12644c9"",\n                ""11f646e9f650f5f3fedc12b6349ec60f"",\n                ""0604e5350b9c734dd2d770ee877cfb26"",\n                ""6dbeb8b022083396acb721267335f228"",\n                ""ba55380d6c84f5caeb87155d1c5fa654""\n            ],\n            ""graph"": {\n                ""layers"": [\n                    ...\n                    {\n                        ""hash_id"": ""ba55380d6c84f5caeb87155d1c5fa654"",\n                        ""is_delete"": false,\n                        ""size"": ""x"",\n                        ""graph_type"": 0,\n                        ""output"": [\n                            6\n                        ],\n                        ""output_size"": 1,\n                        ""input"": [\n                            7,\n                            1\n                        ],\n                        ""input_size"": 2\n                    },\n                    ...\n                ]\n            },\n            ""restore_dir"": ""/mnt/nfs/nni/ga_squad/87"",\n            ""save_dir"": ""/mnt/nfs/nni/ga_squad/95""\n        }\n        ```\n        `restore_dir` means the path in which to load the previous trained model weights. if null, init from stratch.\n        `save_dir` means the path to save trained model for current trial.\n        `graph` is the configuration of model network.\n                Note: each configuration of layers has a `hash_id` property,\n                which tells tuner & trial code whether to share trained weights or not.\n        `shared_id` is the hash_id of layers that should be shared with previously trained model.\n        """"""\n        logger.debug(\'acquiring lock for param {}\'.format(parameter_id))\n        self.thread_lock.acquire()\n        logger.debug(\'lock for current thread acquired\')\n        if not self.population:\n            logger.debug(""the len of poplution lower than zero."")\n            raise Exception(\'The population is empty\')\n        pos = -1\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            graph_param = json.loads(graph_dumps(indiv.config))\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n            indiv = copy.deepcopy(self.population[0])\n            self.population.pop(1)\n            indiv.mutation(indiv_id = self.generate_new_id())\n            graph_param = json.loads(graph_dumps(indiv.config))\n        param_json = {\n            \'graph\': graph_param,\n            \'restore_dir\': self.save_dir(indiv.parent_id),\n            \'save_dir\': self.save_dir(indiv.indiv_id),\n            \'shared_id\': list(indiv.shared_ids) if indiv.parent_id is not None else None,\n        }\n        logger.debug(\'generate_parameter return value is:\')\n        logger.debug(param_json)\n        logger.debug(\'releasing lock\')\n        self.thread_lock.release()\n        if indiv.parent_id is not None:\n            logger.debug(""new trial {} pending on parent experiment {}"".format(indiv.indiv_id, indiv.parent_id))\n            self.events[indiv.parent_id].wait()\n        logger.debug(""trial {} ready"".format(indiv.indiv_id))\n        return param_json\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        \'\'\'\n        Record an observation of the objective function\n        parameter_id : int\n        parameters : dict of parameters\n        value: final metrics of the trial, including reward\n        \'\'\'\n        logger.debug(\'acquiring lock for param {}\'.format(parameter_id))\n        self.thread_lock.acquire()\n        logger.debug(\'lock for current acquired\')\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Minimize:\n            reward = -reward\n\n        logger.debug(\'receive trial result is:\\n\')\n        logger.debug(str(parameters))\n        logger.debug(str(reward))\n\n        indiv = Individual(indiv_id=int(os.path.split(parameters[\'save_dir\'])[1]),\n                           graph_cfg=graph_loads(parameters[\'graph\']), result=reward)\n        self.population.append(indiv)\n        logger.debug(\'releasing lock\')\n        self.thread_lock.release()\n        self.events[indiv.indiv_id].set()\n\n    def update_search_space(self, data):\n        pass\n'"
examples/tuners/weight_sharing/ga_customer_tuner/graph.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge,\n# to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""),\n# to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\n# and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\'\'\'\nGraph is customed-define class, this module contains related class and function about graph.\n\'\'\'\n\n\nimport copy\nimport hashlib\nimport logging\nimport json\nimport random\nfrom collections import deque\nfrom enum import Enum, unique\nfrom typing import Iterable\n\nimport numpy as np\n\n_logger = logging.getLogger(\'ga_squad_graph\')\n\n@unique\nclass LayerType(Enum):\n    \'\'\'\n    Layer type\n    \'\'\'\n    attention = 0\n    self_attention = 1\n    rnn = 2\n    input = 3\n    output = 4\n\nclass Layer(object):\n    \'\'\'\n    Layer class, which contains the information of graph.\n    \'\'\'\n    def __init__(self, graph_type, inputs=None, output=None, size=None, hash_id=None):\n        self.input = inputs if inputs is not None else []\n        self.output = output if output is not None else []\n        self.graph_type = graph_type\n        self.is_delete = False\n        self.size = size\n        self.hash_id = hash_id\n        if graph_type == LayerType.attention.value:\n            self.input_size = 2\n            self.output_size = 1\n        elif graph_type == LayerType.rnn.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.self_attention.value:\n            self.input_size = 1\n            self.output_size = 1\n        elif graph_type == LayerType.input.value:\n            self.input_size = 0\n            self.output_size = 1\n            if self.hash_id is None:\n                hasher = hashlib.md5()\n                hasher.update(np.random.bytes(100))\n                self.hash_id = hasher.hexdigest()\n        elif graph_type == LayerType.output.value:\n            self.input_size = 1\n            self.output_size = 0\n        else:\n            raise ValueError(\'Unsupported LayerType: {}\'.format(graph_type))\n\n    def update_hash(self, layers: Iterable):\n        """"""\n        Calculation of `hash_id` of Layer. Which is determined by the properties of itself, and the `hash_id`s of input layers\n        """"""\n        if self.graph_type == LayerType.input.value:\n            return\n        hasher = hashlib.md5()\n        hasher.update(LayerType(self.graph_type).name.encode(\'ascii\'))\n        hasher.update(str(self.size).encode(\'ascii\'))\n        for i in self.input:\n            if layers[i].hash_id is None:\n                raise ValueError(\'Hash id of layer {}: {} not generated!\'.format(i, layers[i]))\n            hasher.update(layers[i].hash_id.encode(\'ascii\'))\n        self.hash_id = hasher.hexdigest()\n\n    def set_size(self, graph_id, size):\n        \'\'\'\n        Set size.\n        \'\'\'\n        if self.graph_type == LayerType.attention.value:\n            if self.input[0] == graph_id:\n                self.size = size\n        if self.graph_type == LayerType.rnn.value:\n            self.size = size\n        if self.graph_type == LayerType.self_attention.value:\n            self.size = size\n        if self.graph_type == LayerType.output.value:\n            if self.size != size:\n                return False\n        return True\n\n    def clear_size(self):\n        \'\'\'\n        Clear size\n        \'\'\'\n        if self.graph_type == LayerType.attention.value or \\\n            LayerType.rnn.value or LayerType.self_attention.value:\n            self.size = None\n\n    def __str__(self):\n        return \'input:\' + str(self.input) + \' output:\' + str(self.output) + \' type:\' + str(self.graph_type) + \' is_delete:\' + str(self.is_delete) + \' size:\' + str(self.size)\n\ndef graph_dumps(graph):\n    \'\'\'\n    Dump the graph.\n    \'\'\'\n    return json.dumps(graph, default=lambda obj: obj.__dict__)\n\ndef graph_loads(graph_json):\n    \'\'\'\n    Load graph\n    \'\'\'\n    layers = []\n    for layer in graph_json[\'layers\']:\n        layer_info = Layer(layer[\'graph_type\'], layer[\'input\'], layer[\'output\'], layer[\'size\'], layer[\'hash_id\'])\n        layer_info.is_delete = layer[\'is_delete\']\n        _logger.debug(\'append layer {}\'.format(layer_info))\n        layers.append(layer_info)\n    graph = Graph(graph_json[\'max_layer_num\'], graph_json[\'min_layer_num\'], [], [], [])\n    graph.layers = layers\n    _logger.debug(\'graph {} loaded\'.format(graph))\n    return graph\n\nclass Graph(object):\n    \'\'\'\n    Customed Graph class.\n    \'\'\'\n    def __init__(self, max_layer_num, min_layer_num, inputs, output, hide):\n        self.layers = []\n        self.max_layer_num = max_layer_num\n        self.min_layer_num = min_layer_num\n        assert min_layer_num < max_layer_num\n\n        for layer in inputs:\n            self.layers.append(layer)\n        for layer in output:\n            self.layers.append(layer)\n        if hide is not None:\n            for layer in hide:\n                self.layers.append(layer)\n        assert self.is_legal()\n\n    def is_topology(self, layers=None):\n        \'\'\'\n        valid the topology\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layers_nodle = []\n        result = []\n        for i, layer in enumerate(layers):\n            if layer.is_delete is False:\n                layers_nodle.append(i)\n        while True:\n            flag_break = True\n            layers_toremove = []\n            for layer1 in layers_nodle:\n                flag_arrive = True\n                for layer2 in layers[layer1].input:\n                    if layer2 in layers_nodle:\n                        flag_arrive = False\n                if flag_arrive is True:\n                    for layer2 in layers[layer1].output:\n                        # Size is error\n                        if layers[layer2].set_size(layer1, layers[layer1].size) is False:\n                            return False\n                    layers_toremove.append(layer1)\n                    result.append(layer1)\n                    flag_break = False\n            for layer in layers_toremove:\n                layers_nodle.remove(layer)\n            result.append(\'|\')\n            if flag_break:\n                break\n        # There is loop in graph || some layers can\'t to arrive\n        if layers_nodle:\n            return False\n        return result\n\n    def layer_num(self, layers=None):\n        \'\'\'\n        Reutn number of layer.\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n        layer_num = 0\n        for layer in layers:\n            if layer.is_delete is False and layer.graph_type != LayerType.input.value\\\n                and layer.graph_type != LayerType.output.value:\n                layer_num += 1\n        return layer_num\n\n    def is_legal(self, layers=None):\n        \'\'\'\n        Judge whether is legal for layers\n        \'\'\'\n        if layers is None:\n            layers = self.layers\n\n        for layer in layers:\n            if layer.is_delete is False:\n                if len(layer.input) != layer.input_size:\n                    return False\n                if len(layer.output) < layer.output_size:\n                    return False\n\n        # layer_num <= max_layer_num\n        if self.layer_num(layers) > self.max_layer_num:\n            return False\n\n        # There is loop in graph || some layers can\'t to arrive\n        if self.is_topology(layers) is False:\n            return False\n\n        return True\n\n    def update_hash(self):\n        """"""\n        update hash id of each layer, in topological order/recursively\n        hash id will be used in weight sharing\n        """"""\n        _logger.debug(\'update hash\')\n        layer_in_cnt = [len(layer.input) for layer in self.layers]\n        topo_queue = deque([i for i, layer in enumerate(self.layers) if not layer.is_delete and layer.graph_type == LayerType.input.value])\n        while topo_queue:\n            layer_i = topo_queue.pop()\n            self.layers[layer_i].update_hash(self.layers)\n            for layer_j in self.layers[layer_i].output:\n                layer_in_cnt[layer_j] -= 1\n                if layer_in_cnt[layer_j] == 0:\n                    topo_queue.appendleft(layer_j)\n\n    def mutation(self, only_add=False):\n        \'\'\'\n        Mutation for a graph\n        \'\'\'\n        types = []\n        if self.layer_num() < self.max_layer_num:\n            types.append(0)\n            types.append(1)\n        if self.layer_num() > self.min_layer_num and only_add is False:\n            types.append(2)\n            types.append(3)\n        # 0 : add a layer , delete a edge\n        # 1 : add a layer , change a edge\n        # 2 : delete a layer, delete a edge\n        # 3 : delete a layer, change a edge\n        graph_type = random.choice(types)\n        layer_type = random.choice([LayerType.attention.value,\\\n            LayerType.self_attention.value, LayerType.rnn.value])\n        layers = copy.deepcopy(self.layers)\n        cnt_try = 0\n        while True:\n            layers_in = []\n            layers_out = []\n            layers_del = []\n            for i, layer in enumerate(layers):\n                if layer.is_delete is False:\n                    if layer.graph_type != LayerType.output.value:\n                        layers_in.append(i)\n                    if layer.graph_type != LayerType.input.value:\n                        layers_out.append(i)\n                    if layer.graph_type != LayerType.output.value\\\n                            and layer.graph_type != LayerType.input.value:\n                        layers_del.append(i)\n            if graph_type <= 1:\n                new_id = len(layers)\n                out = random.choice(layers_out)\n                inputs = []\n                output = [out]\n                pos = random.randint(0, len(layers[out].input) - 1)\n                last_in = layers[out].input[pos]\n                layers[out].input[pos] = new_id\n                if graph_type == 0:\n                    layers[last_in].output.remove(out)\n                if graph_type == 1:\n                    layers[last_in].output.remove(out)\n                    layers[last_in].output.append(new_id)\n                    inputs = [last_in]\n                lay = Layer(graph_type=layer_type, inputs=inputs, output=output)\n                while len(inputs) < lay.input_size:\n                    layer1 = random.choice(layers_in)\n                    inputs.append(layer1)\n                    layers[layer1].output.append(new_id)\n                lay.input = inputs\n                layers.append(lay)\n            else:\n                layer1 = random.choice(layers_del)\n                for layer2 in layers[layer1].output:\n                    layers[layer2].input.remove(layer1)\n                    if graph_type == 2:\n                        random_in = random.choice(layers_in)\n                    else:\n                        random_in = random.choice(layers[layer1].input)\n                    layers[layer2].input.append(random_in)\n                    layers[random_in].output.append(layer2)\n                for layer2 in layers[layer1].input:\n                    layers[layer2].output.remove(layer1)\n                layers[layer1].is_delete = True\n\n            if self.is_legal(layers):\n                self.layers = layers\n                break\n            else:\n                layers = copy.deepcopy(self.layers)\n                cnt_try += 1\n        self.update_hash()\n\n    def __str__(self):\n        info = """"\n        for l_id, layer in enumerate(self.layers):\n            if layer.is_delete is False:\n                info += \'id:%d \' % l_id + str(layer) + \'\\n\'\n        return info\n'"
src/nni_manager/core/test/assessor.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n_in_file = open(3, 'rb')\n_out_file = open(4, 'wb')\n\n\ndef send(command, data):\n    command = command.encode('utf8')\n    data = data.encode('utf8')\n    msg = b'%b%14d%b' % (command, len(data), data)\n    _out_file.write(msg)\n    _out_file.flush()\n\n\ndef receive():\n    header = _in_file.read(16)\n    l = int(header[2:])\n    command = header[:2].decode('utf8')\n    data = _in_file.read(l).decode('utf8')\n    return command, data\n\n\nprint(receive())\n\nsend('KI', '')\n\nprint(receive())\n\nsend('KI', 'hello')\n\nsend('KI', '\xe4\xb8\x96\xe7\x95\x8c')\n"""
src/nni_manager/core/test/dummy_assessor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom nni.assessor import Assessor, AssessResult\n\nclass DummyAssessor(Assessor):\n    def assess_trial(self, trial_job_id, trial_history):\n        return AssessResult.Good\n'"
src/nni_manager/core/test/dummy_tuner.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom nni.tuner import Tuner\n\nclass DummyTuner(Tuner):\n    def generate_parameters(self, parameter_id):\n        return 'unit-test-parm'\n\n    def generate_multiple_parameters(self, parameter_id_list):\n        return ['unit-test-param1', 'unit-test-param2']\n\n    def receive_trial_result(self, parameter_id, parameters, value):\n        pass\n\n    def receive_customized_trial_result(self, parameter_id, parameters, value):\n        pass\n\n    def update_search_space(self, search_space):\n        pass\n"""
src/nni_manager/training_service/test/mockedTrial.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport time\n\nMETRICS_FILENAME = '.nni/metrics'\nMAGIC = 'ME'\n\ndef sdk_send_data(data):\n    out_dir = os.getenv('NNI_SYS_DIR')\n    if not os.path.isdir(out_dir):\n        raise Exception('Can not find NNI_SYS_DIR: {}'.format(out_dir))\n\n    filename = os.path.join(out_dir, METRICS_FILENAME)\n    wrapped_data = data + '\\n'\n    datalen = len(wrapped_data)\n    if datalen < 2:\n        return\n    with open(filename, 'a') as f:\n        f.write('ME{:06d}{}'.format(datalen, wrapped_data))\n\ndef user_code():\n\n    epochs = 20\n\n    val_acc = 0\n    batch_size = 32\n    for epoch in range(epochs):\n        #Training\n        time.sleep(1)\n        val_acc += 0.5\n        metrics = 'epoch: {}, val accuracy: {:.2f}, batch size: {}'.format(epoch, val_acc, batch_size)\n        sdk_send_data(metrics)\n\nif __name__ == '__main__':\n    print('>>>start...')\n    user_code()\n    print('>>>end...')\n"""
src/sdk/pycli/nnicli/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .nni_client import *\n'
src/sdk/pycli/nnicli/nni_client.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n"""""" A python wrapper for nni rest api\n\nExample:\n\nimport nnicli as nc\n\nnc.start_nni(\'../../../../examples/trials/mnist/config.yml\')\n\nnc.set_endpoint(\'http://localhost:8080\')\n\nprint(nc.version())\nprint(nc.get_experiment_status())\n\nprint(nc.get_job_statistics())\nprint(nc.list_trial_jobs())\n\nnc.stop_nni()\n\n""""""\n\nimport sys\nimport os\nimport subprocess\nimport requests\n\n__all__ = [\n    \'start_nni\',\n    \'stop_nni\',\n    \'set_endpoint\',\n    \'version\',\n    \'get_experiment_status\',\n    \'get_experiment_profile\',\n    \'get_trial_job\',\n    \'list_trial_jobs\',\n    \'get_job_statistics\',\n    \'get_job_metrics\',\n    \'export_data\'\n]\n\nEXPERIMENT_PATH = \'experiment\'\nVERSION_PATH = \'version\'\nSTATUS_PATH = \'check-status\'\nJOB_STATISTICS_PATH = \'job-statistics\'\nTRIAL_JOBS_PATH = \'trial-jobs\'\nMETRICS_PATH = \'metric-data\'\nEXPORT_DATA_PATH = \'export-data\'\n\nAPI_ROOT_PATH = \'api/v1/nni\'\n\n_api_endpoint = None\n\ndef set_endpoint(endpoint):\n    """"""set endpoint of nni rest server for nnicli, for example:\n    http://localhost:8080\n    """"""\n    global _api_endpoint\n    _api_endpoint = endpoint\n\ndef _check_endpoint():\n    if _api_endpoint is None:\n        raise AssertionError(""Please call set_endpoint to specify nni endpoint"")\n\ndef _nni_rest_get(api_path, response_type=\'json\'):\n    _check_endpoint()\n    uri = \'{}/{}/{}\'.format(_api_endpoint, API_ROOT_PATH, api_path)\n    res = requests.get(uri)\n    if _http_succeed(res.status_code):\n        if response_type == \'json\':\n            return res.json()\n        elif response_type == \'text\':\n            return res.text\n        else:\n            raise AssertionError(\'Incorrect response_type\')\n    else:\n        return None\n\ndef _http_succeed(status_code):\n    return status_code // 100 == 2\n\ndef _create_process(cmd):\n    if sys.platform == \'win32\':\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE, creationflags=subprocess.CREATE_NEW_PROCESS_GROUP)\n    else:\n        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)\n\n    while process.poll() is None:\n        output = process.stdout.readline()\n        if output:\n            print(output.decode(\'utf-8\').strip())\n    return process.returncode\n\ndef start_nni(config_file):\n    """"""start nni experiment with specified configuration file""""""\n    cmd = \'nnictl create --config {}\'.format(config_file).split(\' \')\n    if _create_process(cmd) != 0:\n        raise RuntimeError(\'Failed to start nni.\')\n\ndef stop_nni():\n    """"""stop nni experiment""""""\n    cmd = \'nnictl stop\'.split(\' \')\n    if _create_process(cmd) != 0:\n        raise RuntimeError(\'Failed to stop nni.\')\n\ndef version():\n    """"""return version of nni""""""\n    return _nni_rest_get(VERSION_PATH, \'text\')\n\ndef get_experiment_status():\n    """"""return experiment status as a dict""""""\n    return _nni_rest_get(STATUS_PATH)\n\ndef get_experiment_profile():\n    """"""return experiment profile as a dict""""""\n    return _nni_rest_get(EXPERIMENT_PATH)\n\ndef get_trial_job(trial_job_id):\n    """"""return trial job information as a dict""""""\n    assert trial_job_id is not None\n    return _nni_rest_get(os.path.join(TRIAL_JOBS_PATH, trial_job_id))\n\ndef list_trial_jobs():\n    """"""return information for all trial jobs as a list""""""\n    return _nni_rest_get(TRIAL_JOBS_PATH)\n\ndef get_job_statistics():\n    """"""return trial job statistics information as a dict""""""\n    return _nni_rest_get(JOB_STATISTICS_PATH)\n\ndef get_job_metrics(trial_job_id=None):\n    """"""return trial job metrics""""""\n    api_path = METRICS_PATH if trial_job_id is None else os.path.join(METRICS_PATH, trial_job_id)\n    return _nni_rest_get(api_path)\n\ndef export_data():\n    """"""return exported information for all trial jobs""""""\n    return _nni_rest_get(EXPORT_DATA_PATH)\n'"
src/sdk/pynni/nni/__init__.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n__version__ = '999.0.0-developing'\n\nfrom .env_vars import dispatcher_env_vars\n\nif dispatcher_env_vars.SDK_PROCESS != 'dispatcher':\n    from .trial import *\n    from .smartparam import *\n    from .nas_utils import training_update\n\nclass NoMoreTrialError(Exception):\n    def __init__(self, ErrorInfo):\n        super().__init__(self)\n        self.errorinfo = ErrorInfo\n\n    def __str__(self):\n        return self.errorinfo\n"""
src/sdk/pynni/nni/__main__.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport argparse\nimport logging\nimport json\nimport importlib\nimport base64\n\nfrom .common import enable_multi_thread, enable_multi_phase\nfrom .constants import ModuleName, ClassName, ClassArgs, AdvisorModuleName, AdvisorClassName\nfrom .msg_dispatcher import MsgDispatcher\n\nlogger = logging.getLogger('nni.main')\nlogger.debug('START')\n\nif os.environ.get('COVERAGE_PROCESS_START'):\n    import coverage\n    coverage.process_startup()\n\ndef augment_classargs(input_class_args, classname):\n    if classname in ClassArgs:\n        for key, value in ClassArgs[classname].items():\n            if key not in input_class_args:\n                input_class_args[key] = value\n    return input_class_args\n\n\ndef create_builtin_class_instance(class_name, class_args, builtin_module_dict, builtin_class_dict):\n    if class_name not in builtin_module_dict or \\\n        importlib.util.find_spec(builtin_module_dict[class_name]) is None:\n        raise RuntimeError('Builtin module is not found: {}'.format(class_name))\n    class_module = importlib.import_module(builtin_module_dict[class_name])\n    class_constructor = getattr(class_module, builtin_class_dict[class_name])\n\n    if class_args is None:\n        class_args = {}\n    class_args = augment_classargs(class_args, class_name)\n    instance = class_constructor(**class_args)\n\n    return instance\n\n\ndef create_customized_class_instance(class_params):\n    code_dir = class_params.get('codeDir')\n    class_filename = class_params.get('classFileName')\n    class_name = class_params.get('className')\n    class_args = class_params.get('classArgs')\n\n    if not os.path.isfile(os.path.join(code_dir, class_filename)):\n        raise ValueError('Class file not found: {}'.format(\n            os.path.join(code_dir, class_filename)))\n    sys.path.append(code_dir)\n    module_name = os.path.splitext(class_filename)[0]\n    class_module = importlib.import_module(module_name)\n    class_constructor = getattr(class_module, class_name)\n\n    if class_args is None:\n        class_args = {}\n    instance = class_constructor(**class_args)\n\n    return instance\n\n\ndef main():\n    parser = argparse.ArgumentParser(description='Dispatcher command line parser')\n    parser.add_argument('--exp_params', type=str, required=True)\n    args, _ = parser.parse_known_args()\n\n    exp_params_decode = base64.b64decode(args.exp_params).decode('utf-8')\n    logger.debug('decoded exp_params: [%s]', exp_params_decode)\n    exp_params = json.loads(exp_params_decode)\n    logger.debug('exp_params json obj: [%s]', json.dumps(exp_params, indent=4))\n\n    if exp_params.get('multiThread'):\n        enable_multi_thread()\n    if exp_params.get('multiPhase'):\n        enable_multi_phase()\n\n    if exp_params.get('advisor') is not None:\n        # advisor is enabled and starts to run\n        _run_advisor(exp_params)\n    else:\n        # tuner (and assessor) is enabled and starts to run\n        assert exp_params.get('tuner') is not None\n        tuner = _create_tuner(exp_params)\n        if exp_params.get('assessor') is not None:\n            assessor = _create_assessor(exp_params)\n        else:\n            assessor = None\n        dispatcher = MsgDispatcher(tuner, assessor)\n\n        try:\n            dispatcher.run()\n            tuner._on_exit()\n            if assessor is not None:\n                assessor._on_exit()\n        except Exception as exception:\n            logger.exception(exception)\n            tuner._on_error()\n            if assessor is not None:\n                assessor._on_error()\n            raise\n\n\ndef _run_advisor(exp_params):\n    if exp_params.get('advisor').get('builtinAdvisorName') in AdvisorModuleName:\n        dispatcher = create_builtin_class_instance(\n            exp_params.get('advisor').get('builtinAdvisorName'),\n            exp_params.get('advisor').get('classArgs'),\n            AdvisorModuleName, AdvisorClassName)\n    else:\n        dispatcher = create_customized_class_instance(exp_params.get('advisor'))\n    if dispatcher is None:\n        raise AssertionError('Failed to create Advisor instance')\n    try:\n        dispatcher.run()\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n\n\ndef _create_tuner(exp_params):\n    if exp_params.get('tuner').get('builtinTunerName') in ModuleName:\n        tuner = create_builtin_class_instance(\n            exp_params.get('tuner').get('builtinTunerName'),\n            exp_params.get('tuner').get('classArgs'),\n            ModuleName, ClassName)\n    else:\n        tuner = create_customized_class_instance(exp_params.get('tuner'))\n    if tuner is None:\n        raise AssertionError('Failed to create Tuner instance')\n    return tuner\n\n\ndef _create_assessor(exp_params):\n    if exp_params.get('assessor').get('builtinAssessorName') in ModuleName:\n        assessor = create_builtin_class_instance(\n            exp_params.get('assessor').get('builtinAssessorName'),\n            exp_params.get('assessor').get('classArgs'),\n            ModuleName, ClassName)\n    else:\n        assessor = create_customized_class_instance(exp_params.get('assessor'))\n    if assessor is None:\n        raise AssertionError('Failed to create Assessor instance')\n    return assessor\n\n\nif __name__ == '__main__':\n    try:\n        main()\n    except Exception as exception:\n        logger.exception(exception)\n        raise\n"""
src/sdk/pynni/nni/_graph_utils.py,12,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\nimport logging\nimport queue\nimport re\nfrom collections import defaultdict\nimport torch\nfrom torch.utils.tensorboard._pytorch_graph import NodePy, NodePyIO, NodePyOP, GraphPy\n\nCLASSTYPE_KIND = \'ClassType\'\nGETATTR_KIND = \'prim::GetAttr\'\n\n_logger = logging.getLogger(__name__)\n\ndef build_module_graph(model, dummy_input):\n    return TorchModuleGraph(model, dummy_input)\n\ndef build_graph(model, dummy_input, verbose=False):\n    g = TorchProtoGraph(model, dummy_input, verbose)\n    return g.graph_def, g.stepstats\n\ndef parse_traced_name(module_name):\n    prefix = \'TracedModule[\'\n    suffix = \']\'\n    if module_name.startswith(prefix) and module_name.endswith(suffix):\n        module_name = module_name[len(prefix):-len(suffix)]\n    return module_name\n\nclass TorchGraph:\n    """"""\n    This class is to extract pytorch model topology graph by tracing\n    """"""\n    def __init__(self, model, dummy_input):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        """"""\n        assert torch.__version__ >= \'1.3.1\'\n\n        self.bound_model = model\n        self._trace(model, dummy_input)\n\n    def _trace(self, model, dummy_input):\n        with torch.onnx.set_training(model, False):\n            self.trace = torch.jit.trace(model, dummy_input)\n            torch._C._jit_pass_inline(self.trace.graph)\n\nclass TorchProtoGraph(TorchGraph):\n    """"""\n    Generates model graph for pytorch models in protobuf, this implementation is borrowed from pytorch v1.4.0,\n    and fixed following issues:\n    https://github.com/pytorch/pytorch/issues/33691\n    https://github.com/pytorch/pytorch/issues/33670\n\n    """"""\n    def __init__(self, model, dummy_input, verbose=False):\n        super().__init__(model, dummy_input)\n\n        from tensorboard.compat.proto.config_pb2 import RunMetadata\n        from tensorboard.compat.proto.graph_pb2 import GraphDef\n        from tensorboard.compat.proto.step_stats_pb2 import StepStats, DeviceStepStats\n        from tensorboard.compat.proto.versions_pb2 import VersionDef\n\n        list_of_nodes = self.parse(self.trace.graph, self.trace, dummy_input)\n        if verbose:\n            print(self.trace.graph)\n        self.stepstats = RunMetadata(step_stats=StepStats(dev_stats=[DeviceStepStats(device=""/device:CPU:0"")]))\n        self.graph_def = GraphDef(node=list_of_nodes, versions=VersionDef(producer=22))\n\n    def parse(self, graph, trace, args=None, omit_useless_nodes=True):\n        """"""This method parses an optimized PyTorch model graph and produces\n        a list of nodes and node stats for eventual conversion to TensorBoard\n        protobuf format.\n\n        Args:\n        graph (PyTorch module): The model graph to be parsed.\n        trace (PyTorch JIT TracedModule): The model trace to be parsed.\n        args (tuple): input tensor[s] for the model.\n        omit_useless_nodes (boolean): Whether to remove nodes from the graph.\n        """"""\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != CLASSTYPE_KIND:\n                nodes_py.append(NodePyIO(node, \'input\'))\n\n        attr_to_scope = dict()\n        node_to_name = lambda d: str(d).split("":"")[0].strip()\n        for node in graph.nodes():\n            if node.kind() == GETATTR_KIND:\n                attr_name = node.s(\'name\')\n                node_name = node_to_name(node)\n                parent = node.input().node()\n                if parent.kind() == GETATTR_KIND:  # If the parent node is not the top-level ""self"" node\n                    parent_scope = attr_to_scope[node_to_name(parent)]\n                    attr_scope = parent_scope.split(\'/\')[-1]\n                    attr_to_scope[node_name] = \'{}/{}.{}\'.format(parent_scope, attr_scope, attr_name)\n                else:\n                    attr_to_scope[node_name] = \'__module.{}\'.format(attr_name)\n                # We don\'t need classtype nodes; scope will provide this information\n                if node.output().type().kind() != CLASSTYPE_KIND:\n                    node_py = NodePyOP(node)\n                    node_py.scopeName = attr_to_scope[node_name]\n                    nodes_py.append(node_py)\n            else:\n                nodes_py.append(NodePyOP(node))\n\n        for i, node in enumerate(graph.outputs()):  # Create sink nodes for output ops\n            node_py = NodePyIO(node, \'output\')\n            node_py.debugName = ""output.{}"".format(i + 1)\n            node_py.inputs = [node.debugName()]\n            nodes_py.append(node_py)\n\n        alias_to_name = dict()\n        base_name = parse_traced_name(trace._name)\n        for name, module in trace.named_modules(prefix=\'__module\'):\n            mod_name = parse_traced_name(module._name)\n            attr_name = name.split(\'.\')[-1]\n            alias_to_name[name] = \'{}[{}]\'.format(mod_name, attr_name)\n\n        for node in nodes_py.nodes_op:\n            module_aliases = node.scopeName.split(\'/\')[-1].split(\'.\')\n            module_name = \'\'\n            for i, alias in enumerate(module_aliases):\n                if i == 0:\n                    module_name = alias\n                    node.scopeName = base_name\n                else:\n                    module_name += \'.\' + alias\n                    node.scopeName += \'/\' + (alias_to_name[module_name] if module_name in alias_to_name else alias)\n\n        nodes_py.populate_namespace_from_OP_to_IO()\n        return nodes_py.to_proto()\n\nclass NodePyGroup(NodePy):\n    """"""\n    This class is used to represent a graph node which consists of multiple jit traced nodes. In a pytorch trace graph,\n    there are multiple nodes are traced for one torch.nn.Module object, we group them together to form a single node to\n    represent the torch.nn.Module object. We also group some functional call trace nodes together to form a new node.\n    """"""\n    def __init__(self, name, node_type, op_type, node_cpps, inputs=None, outputs=None):\n        """"""\n        Parameters:\n        -----------\n        name: str\n            node name, such as `conv1`, `backbone.classifier`\n        node_type: str\n            `module` or `func`\n        op_type: str\n            operation type, such as `Conv2d`, `aten::view`\n        node_cpps: list of torch._C.Node\n            jit trace nodes which are included in this new node\n        inputs: list of str\n            All the inputs of this node, each element is debugName of one input\n        outputs: list of str\n            All the outputs of this node, each element is debugName of one output\n        """"""\n        super(NodePyGroup, self).__init__(name, [])\n        self.node_cpps = node_cpps\n        self.name = name\n        self.op_type = op_type\n        self.type = node_type\n        self.nodes = []\n        self.auxiliary = None\n        self.add_nodes(node_cpps)\n        self.inputs = inputs\n        self.outputs = outputs\n\n    def add_nodes(self, node_cpps):\n        for node_cpp in node_cpps:\n            nodepy = NodePyOP(node_cpp)\n            nodepy.name = str(node_cpp).split(\':\')[0].strip().replace(\'%\', \'\')\n            self.nodes.append(nodepy)\n\n    def sub_node_names(self):\n        return [x.name for x in self.nodes]\n\n    def __repr__(self):\n        return \'name: {}, type: {}, op_type: {}, sub_nodes: {}, inputs: {}, outputs: {}, aux: {}\'.format(\n            self.name, self.type, self.op_type, self.sub_node_names(), self.inputs, self.outputs, self.auxiliary\n        )\n\n\nclass TorchModuleGraph(TorchGraph):\n    """"""\n    Generates model graph, each node is created from single or multiple jit trace nodes.\n    """"""\n    def __init__(self, model, dummy_input):\n        super().__init__(model, dummy_input)\n        self.global_count = 0\n        self.name_to_node, self.input_to_node, self.output_to_node = self._build_graph()\n\n    def _expand_non_prim_node(self, node, nodes, input_to_node, output_to_node):\n        """"""\n        For trace graph nodes, some nodes are not in modules, these nodes are usually generated by\n        the functions directly called in module ```forward```. For such nodes, some of them are\n        trivial op which are label by ```prim::```, some of them are not such ops which is call\n        non-prim ops. This function is to merge neighbor prim ops to a non-prim op, to construct\n        a node.\n\n        Parameters\n        ----------\n        node : trace graph node\n            The non-prim node to expand\n        nodes : list of trace graph node\n            All the trace graph nodes within the same scope as the non-prim node\n        input_to_node : dict\n            key: input name, value: a node that uses this input\n        output_to_node : dict\n            key: output name, value: a node that generates this output\n\n        Returns\n        -------\n        node\n            the expanded non-prim node\n        """"""\n        # TODO: scope name could be empty\n        node_name = \'.\'.join([self._get_module_name(node.scopeName()), node.kind(), str(self.global_count)])\n        _logger.debug(""expand non-prim node, node name: %s"", node_name)\n        self.global_count += 1\n        op_type = node.kind()\n\n        node_group = [node]\n        inputs = list()\n        outputs = list()\n        node_queue = queue.Queue()\n        node_queue.put(node)\n        while not node_queue.empty():\n            curr_node = node_queue.get()\n            for _input in curr_node.inputs():\n                input_name = _input.debugName()\n                if input_name in output_to_node and output_to_node[input_name] in nodes:\n                        predecessor_node = output_to_node[input_name]\n                        if predecessor_node.kind().startswith(\'prim::\'):\n                            node_group.append(predecessor_node)\n                            node_queue.put(predecessor_node)\n                        else:\n                            inputs.append(input_name)\n                else:\n                    inputs.append(input_name)\n        for output in node.outputs():\n            outputs.append(output.debugName())\n        nodepy = NodePyGroup(node_name, \'func\', op_type, node_group, inputs=inputs, outputs=outputs)\n        return nodepy\n\n    def _build_module_node_group(self, module_name, op_type, node_cpps, input_to_node, output_to_node):\n        graph = self.trace.graph\n        inputs, outputs = [], []\n        for n in node_cpps:\n            for i in n.inputs():\n                name = i.debugName()\n                if not name in output_to_node and i in graph.inputs():\n                    inputs.append(name)\n                elif output_to_node[name] not in node_cpps:\n                    inputs.append(name)\n            for o in n.outputs():\n                name = o.debugName()\n                if not name in input_to_node and o in graph.outputs():\n                    outputs.append(name)\n                elif input_to_node[name] not in node_cpps:\n                    outputs.append(name)\n\n        return NodePyGroup(module_name, \'module\', op_type, node_cpps, inputs, outputs)\n\n\n    def _extract_shape_info(self, node):\n        """"""\n        Extract the shape information of ```aten::view``` node\n\n        Parameters\n        ----------\n        node : trace graph node\n            It should be ```aten::view``` node\n\n        Returns\n        -------\n        dict\n            Include shape of input tensor and shape of output tensor\n        """"""\n        t_input = None\n        for _input in node.inputs():\n            t_input = _input\n            break\n        t_output = node.output()\n        assert isinstance(t_input.type(), torch._C.TensorType)\n        assert isinstance(t_output.type(), torch._C.TensorType)\n        in_shape = t_input.type().sizes()\n        out_shape = t_output.type().sizes()\n        return {\'in_shape\': in_shape, \'out_shape\': out_shape}\n\n    def _extract_leaf_modules(self):\n        """"""\n        Extract leaf modules from the given graph. Leaf module means it does not have submodules.\n        To extract leaf modules because only leaf module can be replaced. And shape inference can\n        be done in leaf module level. Other shape inference is done in lower level i.e.,\n        operation level.\n\n        Returns\n        -------\n        list\n            a list of scope name of all the leaf modules\n        """"""\n        def is_parent(name1, name2):\n            """"""\n            check if name1 is parent node of name2, for example:\n            name1: aa.bb,  name2: aa.bb.cc,  return True\n            name1: aa.b,  name2: aa.bb, return False\n            """"""\n            parts1, parts2 = name1.split(\'.\'), name2.split(\'.\')\n            if len(parts1) >= len(parts2):\n                return False\n            for i in range(len(parts1)):\n                if parts2[i] != parts1[i]:\n                    return False\n            return True\n        module_names = sorted([x[0] for x in self.trace.named_modules() if x[0]])\n        leaf_nodes = []\n        for i, name in enumerate(module_names):\n            if i + 1 >= len(module_names) or not is_parent(name, module_names[i + 1]):\n                leaf_nodes.append(name)\n        return leaf_nodes\n\n    def _get_module_name(self, scope_name):\n        """"""\n        Retrieve module name from scope name.\n        Parameters:\n        -----------\n        scope_name: str\n            scope_name of a graph node, for example:\n            for pytorch 1.3.1: MyModel/BackboneModel[backbone]/Conv2d[conv2]\n            for pytorch 1.4.0: __module.backbone/__module.backbone.conv2\n\n        Returns:\n        -------\n        str\n            module name, such as backbone.conv2\n        """"""\n        if torch.__version__ >= \'1.4.0\':\n            return scope_name.split(\'/\')[-1].replace(\'__module.\', \'\')\n        else:\n            return \'.\'.join(re.findall(r\'\\[(.*?)\\]\', scope_name))\n\n    def _build_index(self, nodes_op):\n        name_to_node = dict()\n        input_to_node = defaultdict(list)\n        output_to_node = dict()\n        for node in nodes_op:\n            name_to_node[node.name] = node\n            for _input in node.inputs:\n                input_to_node[_input].append(node)\n            for output in node.outputs:\n                assert not output in output_to_node, \\\n                    ""One output cannot be generated by multiple nodes""\n                output_to_node[output] = node\n        return name_to_node, input_to_node, output_to_node\n\n    def _build_graph(self):\n        """"""\n        Build graph using our defined format from jit trace.\n        There are basically three steps: first, construct necessary information (data structures),\n        second, extract all the modules to convert to node, Third, extract all functions to convert\n        to node.\n\n        Returns\n        -------\n        dict\n            use name to index nodes, key: node name, value: node\n        dict\n            use input (its name) to index nodes,\n            key: input, value: list of nodes that take this input\n        dict\n            use output (its name) to index nodes,\n            key: output, value: node that generates this output\n        """"""\n        omit_useless_nodes = True\n        graph = self.trace.graph\n        _logger.debug(graph)\n        # build output mapping, from output debugName to its node\n        output_to_node = {x.debugName(): n for n in graph.nodes() for x in n.outputs()}\n        # build input mapping, from input debugName to its node\n        input_to_node = {x.debugName(): n for n in graph.nodes() for x in n.inputs()}\n        # build module mapping, from module name to all nodes (as list) under this module scope\n        module_to_nodes = defaultdict(list)\n        # the mapping of function (non-module in forward) to nodes, key is scope name\n        func_to_nodes = defaultdict(list)\n\n        nodes_py = GraphPy()\n        for node in graph.inputs():\n            if omit_useless_nodes:\n                if not node.uses():  # number of user of the node (= number of outputs/ fanout)\n                    continue\n\n            if node.type().kind() != \'ClassType\':\n                nodes_py.append(NodePyIO(node, \'input\'))\n\n        self.leaf_modules = self._extract_leaf_modules()\n        module_to_type = {name: parse_traced_name(module._name) for name, module in self.trace.named_modules()}\n\n        # associate module name with their trace graph nodes\n        for node in graph.nodes():\n            module_name = self._get_module_name(node.scopeName())\n            if module_name in self.leaf_modules:\n                module_to_nodes[module_name].append(node)\n            else:\n                func_to_nodes[node.scopeName()].append(node)\n\n        # build node group for module\n        for module_name, node_cpps in module_to_nodes.items():\n            node_group = self._build_module_node_group(\n                module_name, module_to_type[module_name], node_cpps, input_to_node, output_to_node\n            )\n            _logger.debug(\'node_group: %s\', node_group)\n            nodes_py.nodes_op.append(node_group)\n\n        # each scope_name may have multiple funcs, we split them and create node for each of them\n        # build node group for torch.nn.functional\n        for _, nodes in func_to_nodes.items():\n            # extract non prim:: nodes\n            non_prim_nodes = list()\n            for node in nodes:\n                if not node.kind().startswith(\'prim::\'):\n                    non_prim_nodes.append(node)\n            # for each non prim node, expand it\n            for node in non_prim_nodes:\n                node_group = self._expand_non_prim_node(node, nodes, input_to_node, output_to_node)\n                nodes_py.nodes_op.append(node_group)\n                # get shape infor for view (aten::view) func\n                if node_group.op_type in [\'aten::view\', \'aten::flatten\']:\n                    node_group.auxiliary = self._extract_shape_info(node)\n        for node in graph.outputs():  # Create sink nodes for output ops\n            node_py = NodePyIO(node, \'output\')\n            nodes_py.append(node_py)\n\n        self.nodes_py = nodes_py\n        # build index\n        return self._build_index(self.nodes_py.nodes_op)\n\n    def find_predecessors(self, module_name):\n        """"""\n        Find predecessor node of the given node\n\n        Parameters\n        ----------\n        module_name : str\n            The name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node\'s predecessor\n        """"""\n        predecessors = []\n        for _input in self.name_to_node[module_name].inputs:\n            if not _input in self.output_to_node:\n                _logger.debug(""cannot find node with %s as its output"", _input)\n            else:\n                node_py = self.output_to_node[_input]\n                predecessors.append(node_py.name)\n        return predecessors\n\n    def find_successors(self, module_name):\n        """"""\n        Find successor nodes of the given node\n\n        Parameters\n        ----------\n        module_name : str\n            The name of the node\n\n        Returns\n        -------\n        list\n            a list of nodes who are the given node\'s successor\n        """"""\n        successors = []\n        for output in self.name_to_node[module_name].outputs:\n            assert output in self.input_to_node, ""No node with input {}"".format(output)\n            nodes_py = self.input_to_node[output]\n            for node_py in nodes_py:\n                successors.append(node_py.name)\n        return successors\n'"
src/sdk/pynni/nni/assessor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nAssessor analyzes trial\'s intermediate results (e.g., periodically evaluated accuracy on test dataset)\nto tell whether this trial can be early stopped or not.\n\nSee :class:`Assessor`\' specification and ``docs/en_US/assessors.rst`` for details.\n""""""\n\nfrom enum import Enum\nimport logging\n\nfrom .recoverable import Recoverable\n\n__all__ = [\'AssessResult\', \'Assessor\']\n\n_logger = logging.getLogger(__name__)\n\n\nclass AssessResult(Enum):\n    """"""\n    Enum class for :meth:`Assessor.assess_trial` return value.\n    """"""\n\n    Good = True\n    """"""The trial works well.""""""\n\n    Bad = False\n    """"""The trial works poorly and should be early stopped.""""""\n\n\nclass Assessor(Recoverable):\n    """"""\n    Assessor analyzes trial\'s intermediate results (e.g., periodically evaluated accuracy on test dataset)\n    to tell whether this trial can be early stopped or not.\n\n    This is the abstract base class for all assessors.\n    Early stopping algorithms should inherit this class and override :meth:`assess_trial` method,\n    which receives intermediate results from trials and give an assessing result.\n\n    If :meth:`assess_trial` returns :obj:`AssessResult.Bad` for a trial,\n    it hints NNI framework that the trial is likely to result in a poor final accuracy,\n    and therefore should be killed to save resource.\n\n    If an accessor want\'s to be notified when a trial ends, it can also override :meth:`trial_end`.\n\n    To write a new assessor, you can reference :class:`~nni.medianstop_assessor.MedianstopAssessor`\'s code as an example.\n\n    See Also\n    --------\n    Builtin assessors:\n    :class:`~nni.medianstop_assessor.MedianstopAssessor`\n    :class:`~nni.curvefitting_assessor.CurvefittingAssessor`\n    """"""\n\n    def assess_trial(self, trial_job_id, trial_history):\n        """"""\n        Abstract method for determining whether a trial should be killed. Must override.\n\n        The NNI framework has little guarantee on ``trial_history``.\n        This method is not guaranteed to be invoked for each time ``trial_history`` get updated.\n        It is also possible that a trial\'s history keeps updating after receiving a bad result.\n        And if the trial failed and retried, ``trial_history`` may be inconsistent with its previous value.\n\n        The only guarantee is that ``trial_history`` is always growing.\n        It will not be empty and will always be longer than previous value.\n\n        This is an example of how :meth:`assess_trial` get invoked sequentially:\n\n        ::\n\n            trial_job_id | trial_history   | return value\n            ------------ | --------------- | ------------\n            Trial_A      | [1.0, 2.0]      | Good\n            Trial_B      | [1.5, 1.3]      | Bad\n            Trial_B      | [1.5, 1.3, 1.9] | Good\n            Trial_A      | [0.9, 1.8, 2.3] | Good\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        trial_history : list\n            Intermediate results of this trial. The element type is decided by trial code.\n\n        Returns\n        -------\n        AssessResult\n            :obj:`AssessResult.Good` or :obj:`AssessResult.Bad`.\n        """"""\n        raise NotImplementedError(\'Assessor: assess_trial not implemented\')\n\n    def trial_end(self, trial_job_id, success):\n        """"""\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        trial_job_id : str\n            Unique identifier of the trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        """"""\n\n    def load_checkpoint(self):\n        """"""\n        Internal API under revising, not recommended for end users.\n        """"""\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info(\'Load checkpoint ignored by assessor, checkpoint path: %s\', checkpoin_path)\n\n    def save_checkpoint(self):\n        """"""\n        Internal API under revising, not recommended for end users.\n        """"""\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info(\'Save checkpoint ignored by assessor, checkpoint path: %s\', checkpoin_path)\n\n    def _on_exit(self):\n        pass\n\n    def _on_error(self):\n        pass\n'"
src/sdk/pynni/nni/common.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom datetime import datetime\nfrom io import TextIOBase\nimport logging\nimport sys\nimport time\n\nlog_level_map = {\n    \'fatal\': logging.FATAL,\n    \'error\': logging.ERROR,\n    \'warning\': logging.WARNING,\n    \'info\': logging.INFO,\n    \'debug\': logging.DEBUG\n}\n\n_time_format = \'%m/%d/%Y, %I:%M:%S %p\'\n\nclass _LoggerFileWrapper(TextIOBase):\n    def __init__(self, logger_file):\n        self.file = logger_file\n\n    def write(self, s):\n        if s != \'\\n\':\n            cur_time = datetime.now().strftime(_time_format)\n            self.file.write(\'[{}] PRINT \'.format(cur_time) + s + \'\\n\')\n            self.file.flush()\n        return len(s)\n\ndef init_logger(logger_file_path, log_level_name=\'info\'):\n    """"""Initialize root logger.\n    This will redirect anything from logging.getLogger() as well as stdout to specified file.\n    logger_file_path: path of logger file (path-like object).\n    """"""\n    log_level = log_level_map.get(log_level_name, logging.INFO)\n    logger_file = open(logger_file_path, \'w\')\n    fmt = \'[%(asctime)s] %(levelname)s (%(name)s/%(threadName)s) %(message)s\'\n    logging.Formatter.converter = time.localtime\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(logger_file)\n    handler.setFormatter(formatter)\n\n    root_logger = logging.getLogger()\n    root_logger.addHandler(handler)\n    root_logger.setLevel(log_level)\n\n    # these modules are too verbose\n    logging.getLogger(\'matplotlib\').setLevel(log_level)\n\n    sys.stdout = _LoggerFileWrapper(logger_file)\n\ndef init_standalone_logger():\n    """"""\n    Initialize root logger for standalone mode.\n    This will set NNI\'s log level to INFO and print its log to stdout.\n    """"""\n    fmt = \'[%(asctime)s] %(levelname)s (%(name)s) %(message)s\'\n    formatter = logging.Formatter(fmt, _time_format)\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(formatter)\n    nni_logger = logging.getLogger(\'nni\')\n    nni_logger.addHandler(handler)\n    nni_logger.setLevel(logging.INFO)\n    nni_logger.propagate = False\n\n    # Following line does not affect NNI loggers, but without this user\'s logger won\'t be able to\n    # print log even it\'s level is set to INFO, so we do it for user\'s convenience.\n    # If this causes any issue in future, remove it and use `logging.info` instead of\n    # `logging.getLogger(\'xxx\')` in all examples.\n    logging.basicConfig()\n\n\n_multi_thread = False\n_multi_phase = False\n\ndef enable_multi_thread():\n    global _multi_thread\n    _multi_thread = True\n\ndef multi_thread_enabled():\n    return _multi_thread\n\ndef enable_multi_phase():\n    global _multi_phase\n    _multi_phase = True\n\ndef multi_phase_enabled():\n    return _multi_phase\n'"
src/sdk/pynni/nni/constants.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\nModuleName = {\n    'TPE': 'nni.hyperopt_tuner.hyperopt_tuner',\n    'Random': 'nni.hyperopt_tuner.hyperopt_tuner',\n    'Anneal': 'nni.hyperopt_tuner.hyperopt_tuner',\n    'Evolution': 'nni.evolution_tuner.evolution_tuner',\n    'SMAC': 'nni.smac_tuner.smac_tuner',\n    'BatchTuner': 'nni.batch_tuner.batch_tuner',\n    'Medianstop': 'nni.medianstop_assessor.medianstop_assessor',\n    'GridSearch': 'nni.gridsearch_tuner.gridsearch_tuner',\n    'NetworkMorphism': 'nni.networkmorphism_tuner.networkmorphism_tuner',\n    'Curvefitting': 'nni.curvefitting_assessor.curvefitting_assessor',\n    'MetisTuner': 'nni.metis_tuner.metis_tuner',\n    'GPTuner': 'nni.gp_tuner.gp_tuner',\n    'PPOTuner': 'nni.ppo_tuner.ppo_tuner',\n    'PBTTuner': 'nni.pbt_tuner.pbt_tuner'\n}\n\nClassName = {\n    'TPE': 'HyperoptTuner',\n    'Random': 'HyperoptTuner',\n    'Anneal': 'HyperoptTuner',\n    'Evolution': 'EvolutionTuner',\n    'SMAC': 'SMACTuner',\n    'BatchTuner': 'BatchTuner',\n    'GridSearch': 'GridSearchTuner',\n    'NetworkMorphism':'NetworkMorphismTuner',\n    'MetisTuner':'MetisTuner',\n    'GPTuner':'GPTuner',\n    'PPOTuner': 'PPOTuner',\n    'PBTTuner': 'PBTTuner',\n\n    'Medianstop': 'MedianstopAssessor',\n    'Curvefitting': 'CurvefittingAssessor'\n}\n\nClassArgs = {\n    'TPE': {\n        'algorithm_name': 'tpe'\n    },\n    'Random': {\n        'algorithm_name': 'random_search'\n    },\n    'Anneal': {\n        'algorithm_name': 'anneal'\n    }\n}\n\nAdvisorModuleName = {\n    'Hyperband': 'nni.hyperband_advisor.hyperband_advisor',\n    'BOHB': 'nni.bohb_advisor.bohb_advisor'\n}\n\nAdvisorClassName = {\n    'Hyperband': 'Hyperband',\n    'BOHB': 'BOHB'\n}\n"""
src/sdk/pynni/nni/env_vars.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nfrom collections import namedtuple\n\n\n_trial_env_var_names = [\n    'NNI_PLATFORM',\n    'NNI_EXP_ID',\n    'NNI_TRIAL_JOB_ID',\n    'NNI_SYS_DIR',\n    'NNI_OUTPUT_DIR',\n    'NNI_TRIAL_SEQ_ID',\n    'MULTI_PHASE'\n]\n\n_dispatcher_env_var_names = [\n    'SDK_PROCESS',\n    'NNI_MODE',\n    'NNI_CHECKPOINT_DIRECTORY',\n    'NNI_LOG_DIRECTORY',\n    'NNI_LOG_LEVEL',\n    'NNI_INCLUDE_INTERMEDIATE_RESULTS'\n]\n\ndef _load_env_vars(env_var_names):\n    env_var_dict = {k: os.environ.get(k) for k in env_var_names}\n    return namedtuple('EnvVars', env_var_names)(**env_var_dict)\n\ntrial_env_vars = _load_env_vars(_trial_env_var_names)\n\ndispatcher_env_vars = _load_env_vars(_dispatcher_env_var_names)\n"""
src/sdk/pynni/nni/msg_dispatcher.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nfrom collections import defaultdict\nimport json_tricks\n\nfrom nni import NoMoreTrialError\nfrom .protocol import CommandType, send\nfrom .msg_dispatcher_base import MsgDispatcherBase\nfrom .assessor import AssessResult\nfrom .common import multi_thread_enabled, multi_phase_enabled\nfrom .env_vars import dispatcher_env_vars\nfrom .utils import MetricType, to_json\n\n_logger = logging.getLogger(__name__)\n\n# Assessor global variables\n_trial_history = defaultdict(dict)\n\'\'\'key: trial job ID; value: intermediate results, mapping from sequence number to data\'\'\'\n\n_ended_trials = set()\n\'\'\'trial_job_id of all ended trials.\nWe need this because NNI manager may send metrics after reporting a trial ended.\nTODO: move this logic to NNI manager\n\'\'\'\n\n\ndef _sort_history(history):\n    ret = []\n    for i, _ in enumerate(history):\n        if i in history:\n            ret.append(history[i])\n        else:\n            break\n    return ret\n\n\n# Tuner global variables\n_next_parameter_id = 0\n_trial_params = {}\n\'\'\'key: trial job ID; value: parameters\'\'\'\n_customized_parameter_ids = set()\n\n\ndef _create_parameter_id():\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1\n\n\ndef _pack_parameter(parameter_id, params, customized=False, trial_job_id=None, parameter_index=None):\n    _trial_params[parameter_id] = params\n    ret = {\n        \'parameter_id\': parameter_id,\n        \'parameter_source\': \'customized\' if customized else \'algorithm\',\n        \'parameters\': params\n    }\n    if trial_job_id is not None:\n        ret[\'trial_job_id\'] = trial_job_id\n    if parameter_index is not None:\n        ret[\'parameter_index\'] = parameter_index\n    else:\n        ret[\'parameter_index\'] = 0\n    return to_json(ret)\n\n\nclass MsgDispatcher(MsgDispatcherBase):\n    def __init__(self, tuner, assessor=None):\n        super(MsgDispatcher, self).__init__()\n        self.tuner = tuner\n        self.assessor = assessor\n        if assessor is None:\n            _logger.debug(\'Assessor is not configured\')\n\n    def load_checkpoint(self):\n        self.tuner.load_checkpoint()\n        if self.assessor is not None:\n            self.assessor.load_checkpoint()\n\n    def save_checkpoint(self):\n        self.tuner.save_checkpoint()\n        if self.assessor is not None:\n            self.assessor.save_checkpoint()\n\n    def handle_initialize(self, data):\n        """"""Data is search space\n        """"""\n        self.tuner.update_search_space(data)\n        send(CommandType.Initialized, \'\')\n\n    def send_trial_callback(self, id_, params):\n        """"""For tuner to issue trial config when the config is generated\n        """"""\n        send(CommandType.NewTrialJob, _pack_parameter(id_, params))\n\n    def handle_request_trial_jobs(self, data):\n        # data: number or trial jobs\n        ids = [_create_parameter_id() for _ in range(data)]\n        _logger.debug(""requesting for generating params of %s"", ids)\n        params_list = self.tuner.generate_multiple_parameters(ids, st_callback=self.send_trial_callback)\n\n        for i, _ in enumerate(params_list):\n            send(CommandType.NewTrialJob, _pack_parameter(ids[i], params_list[i]))\n        # when parameters is None.\n        if len(params_list) < len(ids):\n            send(CommandType.NoMoreTrialJobs, _pack_parameter(ids[0], \'\'))\n\n    def handle_update_search_space(self, data):\n        self.tuner.update_search_space(data)\n\n    def handle_import_data(self, data):\n        """"""Import additional data for tuning\n        data: a list of dictionaries, each of which has at least two keys, \'parameter\' and \'value\'\n        """"""\n        for entry in data:\n            entry[\'value\'] = json_tricks.loads(entry[\'value\'])\n        self.tuner.import_data(data)\n\n    def handle_add_customized_trial(self, data):\n        # data: parameters\n        id_ = _create_parameter_id()\n        _customized_parameter_ids.add(id_)\n\n    def handle_report_metric_data(self, data):\n        """"""\n        data: a dict received from nni_manager, which contains:\n              - \'parameter_id\': id of the trial\n              - \'value\': metric value reported by nni.report_final_result()\n              - \'type\': report type, support {\'FINAL\', \'PERIODICAL\'}\n        """"""\n        # metrics value is dumped as json string in trial, so we need to decode it here\n        if \'value\' in data:\n            data[\'value\'] = json_tricks.loads(data[\'value\'])\n        if data[\'type\'] == MetricType.FINAL:\n            self._handle_final_metric_data(data)\n        elif data[\'type\'] == MetricType.PERIODICAL:\n            if self.assessor is not None:\n                self._handle_intermediate_metric_data(data)\n        elif data[\'type\'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data[\'trial_job_id\'] is not None\n            assert data[\'parameter_index\'] is not None\n            param_id = _create_parameter_id()\n            try:\n                param = self.tuner.generate_parameters(param_id, trial_job_id=data[\'trial_job_id\'])\n            except NoMoreTrialError:\n                param = None\n            send(CommandType.SendTrialJobParameter, _pack_parameter(param_id, param, trial_job_id=data[\'trial_job_id\'],\n                                                                    parameter_index=data[\'parameter_index\']))\n        else:\n            raise ValueError(\'Data type not supported: {}\'.format(data[\'type\']))\n\n    def handle_trial_end(self, data):\n        """"""\n        data: it has three keys: trial_job_id, event, hyper_params\n             - trial_job_id: the id generated by training service\n             - event: the job\'s state\n             - hyper_params: the hyperparameters generated and returned by tuner\n        """"""\n        trial_job_id = data[\'trial_job_id\']\n        _ended_trials.add(trial_job_id)\n        if trial_job_id in _trial_history:\n            _trial_history.pop(trial_job_id)\n            if self.assessor is not None:\n                self.assessor.trial_end(trial_job_id, data[\'event\'] == \'SUCCEEDED\')\n        if self.tuner is not None:\n            self.tuner.trial_end(json_tricks.loads(data[\'hyper_params\'])[\'parameter_id\'], data[\'event\'] == \'SUCCEEDED\')\n\n    def _handle_final_metric_data(self, data):\n        """"""Call tuner to process final results\n        """"""\n        id_ = data[\'parameter_id\']\n        value = data[\'value\']\n        if id_ is None or id_ in _customized_parameter_ids:\n            if not hasattr(self.tuner, \'_accept_customized\'):\n                self.tuner._accept_customized = False\n            if not self.tuner._accept_customized:\n                _logger.info(\'Customized trial job %s ignored by tuner\', id_)\n                return\n            customized = True\n        else:\n            customized = False\n            self.tuner.receive_trial_result(id_, _trial_params[id_], value, customized=customized,\n                                            trial_job_id=data.get(\'trial_job_id\'))\n\n    def _handle_intermediate_metric_data(self, data):\n        """"""Call assessor to process intermediate results\n        """"""\n        if data[\'type\'] != MetricType.PERIODICAL:\n            return\n        if self.assessor is None:\n            return\n\n        trial_job_id = data[\'trial_job_id\']\n        if trial_job_id in _ended_trials:\n            return\n\n        history = _trial_history[trial_job_id]\n        history[data[\'sequence\']] = data[\'value\']\n        ordered_history = _sort_history(history)\n        if len(ordered_history) < data[\'sequence\']:  # no user-visible update since last time\n            return\n\n        try:\n            result = self.assessor.assess_trial(trial_job_id, ordered_history)\n        except Exception as e:\n            _logger.error(\'Assessor error\')\n            _logger.exception(e)\n\n        if isinstance(result, bool):\n            result = AssessResult.Good if result else AssessResult.Bad\n        elif not isinstance(result, AssessResult):\n            msg = \'Result of Assessor.assess_trial must be an object of AssessResult, not %s\'\n            raise RuntimeError(msg % type(result))\n\n        if result is AssessResult.Bad:\n            _logger.debug(\'BAD, kill %s\', trial_job_id)\n            send(CommandType.KillTrialJob, json_tricks.dumps(trial_job_id))\n            # notify tuner\n            _logger.debug(\'env var: NNI_INCLUDE_INTERMEDIATE_RESULTS: [%s]\',\n                          dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS)\n            if dispatcher_env_vars.NNI_INCLUDE_INTERMEDIATE_RESULTS == \'true\':\n                self._earlystop_notify_tuner(data)\n        else:\n            _logger.debug(\'GOOD\')\n\n    def _earlystop_notify_tuner(self, data):\n        """"""Send last intermediate result as final result to tuner in case the\n        trial is early stopped.\n        """"""\n        _logger.debug(\'Early stop notify tuner data: [%s]\', data)\n        data[\'type\'] = MetricType.FINAL\n        if multi_thread_enabled():\n            self._handle_final_metric_data(data)\n        else:\n            data[\'value\'] = to_json(data[\'value\'])\n            self.enqueue_command(CommandType.ReportMetricData, data)\n'"
src/sdk/pynni/nni/msg_dispatcher_base.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport threading\nimport logging\nfrom multiprocessing.dummy import Pool as ThreadPool\nfrom queue import Queue, Empty\nimport json_tricks\n\nfrom .common import multi_thread_enabled\nfrom .env_vars import dispatcher_env_vars\nfrom .utils import init_dispatcher_logger\nfrom .recoverable import Recoverable\nfrom .protocol import CommandType, receive\n\ninit_dispatcher_logger()\n\n_logger = logging.getLogger(__name__)\n\nQUEUE_LEN_WARNING_MARK = 20\n_worker_fast_exit_on_terminate = True\n\n\nclass MsgDispatcherBase(Recoverable):\n    """"""This is where tuners and assessors are not defined yet.\n    Inherits this class to make your own advisor.\n    """"""\n\n    def __init__(self):\n        if multi_thread_enabled():\n            self.pool = ThreadPool()\n            self.thread_results = []\n        else:\n            self.stopping = False\n            self.default_command_queue = Queue()\n            self.assessor_command_queue = Queue()\n            self.default_worker = threading.Thread(target=self.command_queue_worker, args=(self.default_command_queue,))\n            self.assessor_worker = threading.Thread(target=self.command_queue_worker,\n                                                    args=(self.assessor_command_queue,))\n            self.default_worker.start()\n            self.assessor_worker.start()\n            self.worker_exceptions = []\n\n    def run(self):\n        """"""Run the tuner.\n        This function will never return unless raise.\n        """"""\n        _logger.info(\'Start dispatcher\')\n        if dispatcher_env_vars.NNI_MODE == \'resume\':\n            self.load_checkpoint()\n\n        while True:\n            command, data = receive()\n            if data:\n                data = json_tricks.loads(data)\n\n            if command is None or command is CommandType.Terminate:\n                break\n            if multi_thread_enabled():\n                result = self.pool.map_async(self.process_command_thread, [(command, data)])\n                self.thread_results.append(result)\n                if any([thread_result.ready() and not thread_result.successful() for thread_result in\n                        self.thread_results]):\n                    _logger.debug(\'Caught thread exception\')\n                    break\n            else:\n                self.enqueue_command(command, data)\n                if self.worker_exceptions:\n                    break\n\n        _logger.info(\'Dispatcher exiting...\')\n        self.stopping = True\n        if multi_thread_enabled():\n            self.pool.close()\n            self.pool.join()\n        else:\n            self.default_worker.join()\n            self.assessor_worker.join()\n\n        _logger.info(\'Terminated by NNI manager\')\n\n    def command_queue_worker(self, command_queue):\n        """"""Process commands in command queues.\n        """"""\n        while True:\n            try:\n                # set timeout to ensure self.stopping is checked periodically\n                command, data = command_queue.get(timeout=3)\n                try:\n                    self.process_command(command, data)\n                except Exception as e:\n                    _logger.exception(e)\n                    self.worker_exceptions.append(e)\n                    break\n            except Empty:\n                pass\n            if self.stopping and (_worker_fast_exit_on_terminate or command_queue.empty()):\n                break\n\n    def enqueue_command(self, command, data):\n        """"""Enqueue command into command queues\n        """"""\n        if command == CommandType.TrialEnd or (\n                command == CommandType.ReportMetricData and data[\'type\'] == \'PERIODICAL\'):\n            self.assessor_command_queue.put((command, data))\n        else:\n            self.default_command_queue.put((command, data))\n\n        qsize = self.default_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning(\'default queue length: %d\', qsize)\n\n        qsize = self.assessor_command_queue.qsize()\n        if qsize >= QUEUE_LEN_WARNING_MARK:\n            _logger.warning(\'assessor queue length: %d\', qsize)\n\n    def process_command_thread(self, request):\n        """"""Worker thread to process a command.\n        """"""\n        command, data = request\n        if multi_thread_enabled():\n            try:\n                self.process_command(command, data)\n            except Exception as e:\n                _logger.exception(str(e))\n                raise\n        else:\n            pass\n\n    def process_command(self, command, data):\n        _logger.debug(\'process_command: command: [%s], data: [%s]\', command, data)\n\n        command_handlers = {\n            # Tuner commands:\n            CommandType.Initialize: self.handle_initialize,\n            CommandType.RequestTrialJobs: self.handle_request_trial_jobs,\n            CommandType.UpdateSearchSpace: self.handle_update_search_space,\n            CommandType.ImportData: self.handle_import_data,\n            CommandType.AddCustomizedTrialJob: self.handle_add_customized_trial,\n\n            # Tuner/Assessor commands:\n            CommandType.ReportMetricData: self.handle_report_metric_data,\n\n            CommandType.TrialEnd: self.handle_trial_end,\n            CommandType.Ping: self.handle_ping,\n        }\n        if command not in command_handlers:\n            raise AssertionError(\'Unsupported command: {}\'.format(command))\n        command_handlers[command](data)\n\n    def handle_ping(self, data):\n        pass\n\n    def handle_initialize(self, data):\n        """"""Initialize search space and tuner, if any\n        This method is meant to be called only once for each experiment, after calling this method,\n        dispatcher should `send(CommandType.Initialized, \'\')`, to set the status of the experiment to be ""INITIALIZED"".\n        Parameters\n        ----------\n        data: dict\n            search space\n        """"""\n        raise NotImplementedError(\'handle_initialize not implemented\')\n\n    def handle_request_trial_jobs(self, data):\n        """"""The message dispatcher is demanded to generate ``data`` trial jobs.\n        These trial jobs should be sent via ``send(CommandType.NewTrialJob, json_tricks.dumps(parameter))``,\n        where ``parameter`` will be received by NNI Manager and eventually accessible to trial jobs as ""next parameter"".\n        Semantically, message dispatcher should do this ``send`` exactly ``data`` times.\n\n        The JSON sent by this method should follow the format of\n\n        ::\n\n            {\n                ""parameter_id"": 42\n                ""parameters"": {\n                    // this will be received by trial\n                },\n                ""parameter_source"": ""algorithm"" // optional\n            }\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        """"""\n        raise NotImplementedError(\'handle_request_trial_jobs not implemented\')\n\n    def handle_update_search_space(self, data):\n        """"""This method will be called when search space is updated.\n        It\'s recommended to call this method in `handle_initialize` to initialize search space.\n        *No need to* notify NNI Manager when this update is done.\n        Parameters\n        ----------\n        data: dict\n            search space\n        """"""\n        raise NotImplementedError(\'handle_update_search_space not implemented\')\n\n    def handle_import_data(self, data):\n        """"""Import previous data when experiment is resumed.\n        Parameters\n        ----------\n        data: list\n            a list of dictionaries, each of which has at least two keys, \'parameter\' and \'value\'\n        """"""\n        raise NotImplementedError(\'handle_import_data not implemented\')\n\n    def handle_add_customized_trial(self, data):\n        """"""Experimental API. Not recommended for usage.\n        """"""\n        raise NotImplementedError(\'handle_add_customized_trial not implemented\')\n\n    def handle_report_metric_data(self, data):\n        """"""Called when metric data is reported or new parameters are requested (for multiphase).\n        When new parameters are requested, this method should send a new parameter.\n\n        Parameters\n        ----------\n        data: dict\n            a dict which contains \'parameter_id\', \'value\', \'trial_job_id\', \'type\', \'sequence\'.\n            type: can be `MetricType.REQUEST_PARAMETER`, `MetricType.FINAL` or `MetricType.PERIODICAL`.\n            `REQUEST_PARAMETER` is used to request new parameters for multiphase trial job. In this case,\n            the dict will contain additional keys: `trial_job_id`, `parameter_index`. Refer to `msg_dispatcher.py`\n            as an example.\n\n        Raises\n        ------\n        ValueError\n            Data type is not supported\n        """"""\n        raise NotImplementedError(\'handle_report_metric_data not implemented\')\n\n    def handle_trial_end(self, data):\n        """"""Called when the state of one of the trials is changed\n\n        Parameters\n        ----------\n        data: dict\n            a dict with keys: trial_job_id, event, hyper_params.\n            trial_job_id: the id generated by training service.\n            event: the job\xe2\x80\x99s state.\n            hyper_params: the string that is sent by message dispatcher during the creation of trials.\n\n        """"""\n        raise NotImplementedError(\'handle_trial_end not implemented\')\n'"
src/sdk/pynni/nni/nas_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport functools\nimport logging\n\nfrom . import trial\n\n\n_logger = logging.getLogger(__name__)\n_MUTABLE_LAYER_SPACE_PREFIX = ""_mutable_layer""\n_namespace = {}\n_tf_variables = {}\n_arch_logits_list = []\n_optimizer = None\n_train_op = None\n\n\ndef classic_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size):\n    \'\'\'Execute the chosen function and inputs directly.\n    In this mode, the trial code is only running the chosen subgraph (i.e., the chosen ops and inputs),\n    without touching the full model graph.\'\'\'\n    if trial.get_current_parameter() is None:\n        trial.get_next_parameter()\n\n    chosen_layer, chosen_inputs = _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id,\n                                                                   list(optional_inputs.keys()))\n    real_chosen_inputs = [optional_inputs[input_name] for input_name in chosen_inputs]\n    layer_out = funcs[chosen_layer]([fixed_inputs, real_chosen_inputs], **funcs_args[chosen_layer])\n\n    return layer_out\n\n\ndef enas_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    \'\'\'For enas mode, we build the full model graph in trial but only run a subgraph\xe3\x80\x82\n    This is implemented by masking inputs and branching ops.\n    Specifically, based on the received subgraph (through nni.get_next_parameter),\n    it can be known which inputs should be masked and which op should be executed.\'\'\'\n    name_prefix = ""{}_{}"".format(mutable_id, mutable_layer_id)\n    # store namespace\n    _namespace[mutable_id] = True\n    _namespace[name_prefix] = dict()\n    _namespace[name_prefix][\'funcs\'] = list(funcs)\n    _namespace[name_prefix][\'optional_inputs\'] = list(optional_inputs)\n    # create tensorflow variables as 1/0 signals used to form subgraph\n    name_for_optional_inputs = name_prefix + \'_optional_inputs\'\n    name_for_funcs = name_prefix + \'_funcs\'\n    _tf_variables[name_prefix] = dict()\n    _tf_variables[name_prefix][\'optional_inputs\'] = tf.get_variable(\n        name_for_optional_inputs,\n        [len(optional_inputs)],\n        dtype=tf.bool,\n        trainable=False\n    )\n    _tf_variables[name_prefix][\'funcs\'] = tf.get_variable(\n        name_for_funcs, [], dtype=tf.int64, trainable=False)\n\n    # get real values using their variable names\n    real_optional_inputs_value = [optional_inputs[name]\n                                  for name in _namespace[name_prefix][\'optional_inputs\']]\n    real_func_value = [funcs[name]\n                       for name in _namespace[name_prefix][\'funcs\']]\n    real_funcs_args = [funcs_args[name]\n                       for name in _namespace[name_prefix][\'funcs\']]\n    # build tensorflow graph of geting chosen inputs by masking\n    real_chosen_inputs = tf.boolean_mask(\n        real_optional_inputs_value, _tf_variables[name_prefix][\'optional_inputs\'])\n    # build tensorflow graph of different branches by using tf.case\n    branches = dict()\n    func_output = None\n    for func_id in range(len(funcs)):\n        func_output = real_func_value[func_id]([fixed_inputs, real_chosen_inputs], **real_funcs_args[func_id])\n        branches[tf.equal(_tf_variables[name_prefix][\'funcs\'], func_id)] = lambda: func_output\n    layer_out = tf.case(branches, exclusive=True, default=lambda: func_output)\n\n    return layer_out\n\n\ndef oneshot_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    \'\'\'Similar to enas mode, oneshot mode also builds the full model graph.\n    The difference is that oneshot mode does not receive subgraph.\n    Instead, it uses dropout to randomly dropout inputs and ops.\'\'\'\n    # NNI requires to get_next_parameter before report a result. But the parameter will not be used in this mode\n    if trial.get_current_parameter() is None:\n        trial.get_next_parameter()\n    optional_inputs = list(optional_inputs.values())\n    inputs_num = len(optional_inputs)\n    # Calculate dropout rate according to the formular r^(1/k), where r is a hyper-parameter and k is the number of inputs\n    if inputs_num > 0:\n        rate = 0.01 ** (1 / inputs_num)\n        noise_shape = [inputs_num] + [1] * len(optional_inputs[0].get_shape())\n        optional_inputs = tf.nn.dropout(\n            optional_inputs, rate=rate, noise_shape=noise_shape)\n        optional_inputs = [optional_inputs[idx] for idx in range(inputs_num)]\n    layer_outs = [func([fixed_inputs, optional_inputs], **funcs_args[func_name])\n                  for func_name, func in funcs.items()]\n    output_num = len(layer_outs)\n    rate = 0.01 ** (1 / output_num)\n    noise_shape = [output_num] + [1] * len(layer_outs[0].get_shape())\n    layer_outs = tf.nn.dropout(layer_outs, rate=rate, noise_shape=noise_shape)\n    layer_out = tf.reduce_sum(layer_outs, axis=0)\n\n    return layer_out\n\n\ndef darts_mode(\n        mutable_id,\n        mutable_layer_id,\n        funcs,\n        funcs_args,\n        fixed_inputs,\n        optional_inputs,\n        optional_input_size,\n        tf):\n    optional_inputs = list(optional_inputs.values())\n    layer_outs = [func([fixed_inputs, optional_inputs], **funcs_args[func_name])\n                  for func_name, func in funcs.items()]\n    # Create architecture weights for every func(op)\n    var_name = ""{}_{}_arch_weights"".format(mutable_id, mutable_layer_id)\n    arch_logits = tf.get_variable(var_name, shape=[len(funcs)], trainable=False)\n    _arch_logits_list.append(arch_logits)\n    arch_weights = tf.nn.softmax(arch_logits)\n    layer_out = tf.add_n([arch_weights[idx] * out for idx, out in enumerate(layer_outs)])\n\n    return layer_out\n\n\ndef reload_tensorflow_variables(tf, session):\n    \'\'\'In Enas mode, this function reload every signal varaible created in `enas_mode` function so\n    the whole tensorflow graph will be changed into certain subgraph recerived from Tuner.\n    ---------------\n    session: the tensorflow session created by users\n    tf: tensorflow module\n    \'\'\'\n    subgraph_from_tuner = trial.get_next_parameter()\n    mutable_layers = set()\n    for subgraph_key in subgraph_from_tuner:\n        if ""/"" in subgraph_key:\n            # has to remove the last, could be layer_choice or whatever\n            mutable_id, mutable_layer_id = _decompose_general_key(subgraph_key[:subgraph_key.rfind(""/"")])\n            if mutable_id is not None:\n                mutable_layers.add((mutable_id, mutable_layer_id))\n    mutable_layers = sorted(list(mutable_layers))\n    for mutable_id, mutable_layer_id in mutable_layers:\n        if mutable_id not in _namespace:\n            _logger.warning(""%s not found in name space"", mutable_id)\n            continue\n        name_prefix = ""{}_{}"".format(mutable_id, mutable_layer_id)\n        # get optional inputs names\n        optional_inputs = _namespace[name_prefix][\'optional_inputs\']\n        # extract layer information from the subgraph sampled by tuner\n        chosen_layer, chosen_inputs = _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id, optional_inputs)\n        chosen_layer = _namespace[name_prefix][\'funcs\'].index(chosen_layer)\n        chosen_inputs = [1 if inp in chosen_inputs else 0 for inp in optional_inputs]\n        # load these information into pre-defined tensorflow variables\n        _tf_variables[name_prefix][\'funcs\'].load(chosen_layer, session)\n        _tf_variables[name_prefix][\'optional_inputs\'].load(\n            chosen_inputs, session)\n\n\ndef _construct_general_key(mutable_id, mutable_layer_id):\n    # Mutable layer key in a general (search space) format\n    # that is, prefix/mutable_id/mutable_layer_id\n    return _MUTABLE_LAYER_SPACE_PREFIX + ""/"" + mutable_id + ""/"" + mutable_layer_id\n\n\ndef _decompose_general_key(key):\n    # inverse operation of above\n    if not key.startswith(_MUTABLE_LAYER_SPACE_PREFIX):\n        return None, None\n    else:\n        _, mutable_id, mutable_layer_id = key.split(""/"", maxsplit=2)\n        return mutable_id, mutable_layer_id\n\n\ndef darts_training(tf, session, loss, feed_dict):\n    global _optimizer, _train_op\n    if _optimizer is None:\n        _optimizer = tf.MomentumOptimizer(learning_rate=0.025)\n        # TODO: Calculate loss\n        grads_and_vars = _optimizer.compute_gradients(loss, _arch_logits_list)\n        _train_op = _optimizer.apply_gradients(grads_and_vars)\n    session.run(_train_op)\n\n\ndef training_update(nas_mode, tf=None, session=None, loss=None, feed_dict=None):\n    if nas_mode == \'darts_mode\':\n        darts_training(tf, session, loss, feed_dict)\n    elif nas_mode == \'enas_mode\':\n        reload_tensorflow_variables(tf, session)\n\n\ndef _get_layer_and_inputs_from_tuner(mutable_id, mutable_layer_id, optional_inputs):\n    # optional_inputs should be name(key)s of the optional inputs\n    try:\n        mutable_block = trial.get_current_parameter(mutable_id)\n\n        # There is a NAS tuner\n        chosen_layer = mutable_block[mutable_layer_id][""chosen_layer""]\n        chosen_inputs = mutable_block[mutable_layer_id][""chosen_inputs""]\n    except KeyError:\n        # Try to find converted NAS parameters\n        params = trial.get_current_parameter()\n        expected_prefix = _construct_general_key(mutable_id, mutable_layer_id)\n        chosen_layer = params[expected_prefix + ""/layer_choice""]\n\n        # find how many to choose\n        optional_input_size = int(params[expected_prefix + ""/optional_input_size""])  # convert uniform to randint\n\n        # find who to choose, can duplicate\n        optional_input_state = params[expected_prefix + ""/optional_input_chosen_state""]\n        chosen_inputs = []\n        # make sure dict -> list produce stable result by sorting\n        optional_inputs_keys = sorted(optional_inputs)\n        for _ in range(optional_input_size):\n            chosen_inputs.append(optional_inputs_keys[optional_input_state % len(optional_inputs)])\n            optional_input_state //= len(optional_inputs)\n\n    _logger.info(""%s_%s: layer: %s, optional inputs: %s"", mutable_id, mutable_layer_id, chosen_layer, chosen_inputs)\n    return chosen_layer, chosen_inputs\n\n\ndef convert_nas_search_space(search_space):\n    """"""\n    Args:\n        param search_space: raw search space\n        return: the new search space, mutable_layers will be converted into choice\n    """"""\n    if not isinstance(search_space, dict):\n        return search_space\n    ret = dict()\n    for k, v in search_space.items():\n        if ""_type"" not in v:\n            # this should not happen\n            _logger.warning(""There is no _type in one of your search space values with key \'%s\'""\n                            "". Please check your search space"", k)\n            ret[k] = v\n        elif v[""_type""] != ""mutable_layer"":\n            ret[k] = v\n        else:\n            _logger.info(""Converting mutable_layer search space with key \'%s\'"", k)\n            # v[""_value""] looks like {\'mutable_layer_1\': {\'layer_choice\': ...} ...}\n            values = v[""_value""]\n            for layer_name, layer_data in values.items():\n                # there should be at most layer_choice, optional_inputs, optional_input_size in layer_data\n\n                # add ""_mutable_layer"" as prefix so that they can be recovered later\n                layer_key = _construct_general_key(k, layer_name)\n\n                if layer_data.get(""layer_choice""):  # filter out empty choice and no choice\n                    layer_choice = layer_data[""layer_choice""]\n                else:\n                    raise ValueError(""No layer choice found in %s"" % layer_key)\n\n                if layer_data.get(""optional_input_size""):\n                    input_size = layer_data[""optional_input_size""]\n                    if isinstance(input_size, int):\n                        input_size = [input_size, input_size]\n                    if input_size[0] > input_size[1] or input_size[0] < 0:\n                        _logger.error(""Might not be able to handle optional_input_size < 0, please double check"")\n                    input_size[1] += 1\n                else:\n                    _logger.info(""Optional input choices are set to empty by default in %s"", layer_key)\n                    input_size = [0, 1]\n\n                if layer_data.get(""optional_inputs""):\n                    total_state_size = len(layer_data[""optional_inputs""]) ** (input_size[1] - 1)\n                else:\n                    _logger.info(""Optional inputs not found in %s"", layer_key)\n                    total_state_size = 1\n\n                converted = {\n                    layer_key + ""/layer_choice"": {\n                        ""_type"": ""choice"", ""_value"": layer_choice\n                    },\n                    layer_key + ""/optional_input_size"": {\n                        ""_type"": ""randint"", ""_value"": input_size\n                    },\n                    layer_key + ""/optional_input_chosen_state"": {\n                        ""_type"": ""randint"", ""_value"": [0, total_state_size]\n                    }\n                }\n                _logger.info(converted)\n                ret.update(converted)\n\n    return ret\n\n\ndef rewrite_nas_space(func):\n    @functools.wraps(func)\n    def wrap(self, search_space):\n        search_space = convert_nas_search_space(search_space)\n        return func(self, search_space)\n    return wrap\n'"
src/sdk/pynni/nni/parameter_expressions.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n'''\nparameter_expression.py\n'''\n\nimport numpy as np\n\n\ndef choice(options, random_state):\n    '''\n    options: 1-D array-like or int\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.choice(options)\n\n\ndef randint(lower, upper, random_state):\n    '''\n    Generate a random integer from `lower` (inclusive) to `upper` (exclusive).\n    lower: an int that represent an lower bound\n    upper: an int that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.randint(lower, upper)\n\n\ndef uniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert high >= low, 'Upper bound must be larger than lower bound'\n    return random_state.uniform(low, high)\n\n\ndef quniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.clip(np.round(uniform(low, high, random_state) / q) * q, low, high)\n\n\ndef loguniform(low, high, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    random_state: an object of numpy.random.RandomState\n    '''\n    assert low > 0, 'Lower bound must be positive'\n    return np.exp(uniform(np.log(low), np.log(high), random_state))\n\n\ndef qloguniform(low, high, q, random_state):\n    '''\n    low: an float that represent an lower bound\n    high: an float that represent an upper bound\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.clip(np.round(loguniform(low, high, random_state) / q) * q, low, high)\n\n\ndef normal(mu, sigma, random_state):\n    '''\n    The probability density function of the normal distribution,\n    first derived by De Moivre and 200 years later by both Gauss and Laplace independently.\n    mu: float or array_like of floats\n        Mean (\xe2\x80\x9ccentre\xe2\x80\x9d) of the distribution.\n    sigma: float or array_like of floats\n           Standard deviation (spread or \xe2\x80\x9cwidth\xe2\x80\x9d) of the distribution.\n    random_state: an object of numpy.random.RandomState\n    '''\n    return random_state.normal(mu, sigma)\n\n\ndef qnormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(normal(mu, sigma, random_state) / q) * q\n\n\ndef lognormal(mu, sigma, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.exp(normal(mu, sigma, random_state))\n\n\ndef qlognormal(mu, sigma, q, random_state):\n    '''\n    mu: float or array_like of floats\n    sigma: float or array_like of floats\n    q: sample step\n    random_state: an object of numpy.random.RandomState\n    '''\n    return np.round(lognormal(mu, sigma, random_state) / q) * q\n"""
src/sdk/pynni/nni/protocol.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport threading\nfrom enum import Enum\n\n\nclass CommandType(Enum):\n    # in\n    Initialize = b\'IN\'\n    RequestTrialJobs = b\'GE\'\n    ReportMetricData = b\'ME\'\n    UpdateSearchSpace = b\'SS\'\n    ImportData = b\'FD\'\n    AddCustomizedTrialJob = b\'AD\'\n    TrialEnd = b\'EN\'\n    Terminate = b\'TE\'\n    Ping = b\'PI\'\n\n    # out\n    Initialized = b\'ID\'\n    NewTrialJob = b\'TR\'\n    SendTrialJobParameter = b\'SP\'\n    NoMoreTrialJobs = b\'NO\'\n    KillTrialJob = b\'KI\'\n\n_lock = threading.Lock()\ntry:\n    _in_file = open(3, \'rb\')\n    _out_file = open(4, \'wb\')\nexcept OSError:\n    _msg = \'IPC pipeline not exists, maybe you are importing tuner/assessor from trial code?\'\n    logging.getLogger(__name__).warning(_msg)\n\n\ndef send(command, data):\n    """"""Send command to Training Service.\n    command: CommandType object.\n    data: string payload.\n    """"""\n    global _lock\n    try:\n        _lock.acquire()\n        data = data.encode(\'utf8\')\n        msg = b\'%b%014d%b\' % (command.value, len(data), data)\n        logging.getLogger(__name__).debug(\'Sending command, data: [%s]\', msg)\n        _out_file.write(msg)\n        _out_file.flush()\n    finally:\n        _lock.release()\n\n\ndef receive():\n    """"""Receive a command from Training Service.\n    Returns a tuple of command (CommandType) and payload (str)\n    """"""\n    header = _in_file.read(16)\n    logging.getLogger(__name__).debug(\'Received command, header: [%s]\', header)\n    if header is None or len(header) < 16:\n        # Pipe EOF encountered\n        logging.getLogger(__name__).debug(\'Pipe EOF encountered\')\n        return None, None\n    length = int(header[2:])\n    data = _in_file.read(length)\n    command = CommandType(header[:2])\n    data = data.decode(\'utf8\')\n    logging.getLogger(__name__).debug(\'Received command, data: [%s]\', data)\n    return command, data\n'"
src/sdk/pynni/nni/recoverable.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nclass Recoverable:\n\n    def load_checkpoint(self):\n        pass\n\n    def save_checkpoint(self):\n        pass\n\n    def get_checkpoint_path(self):\n        ckp_path = os.getenv('NNI_CHECKPOINT_DIRECTORY')\n        if ckp_path is not None and os.path.isdir(ckp_path):\n            return ckp_path\n        return None\n"""
src/sdk/pynni/nni/smartparam.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nfrom .env_vars import trial_env_vars\nfrom . import trial\nfrom . import parameter_expressions as param_exp\nfrom .nas_utils import classic_mode, enas_mode, oneshot_mode, darts_mode\n\n\n__all__ = [\n    \'choice\',\n    \'randint\',\n    \'uniform\',\n    \'quniform\',\n    \'loguniform\',\n    \'qloguniform\',\n    \'normal\',\n    \'qnormal\',\n    \'lognormal\',\n    \'qlognormal\',\n    \'function_choice\',\n    \'mutable_layer\'\n]\n\n\nif trial_env_vars.NNI_PLATFORM is None:\n    def choice(*options, name=None):\n        return param_exp.choice(options, np.random.RandomState())\n\n    def randint(lower, upper, name=None):\n        return param_exp.randint(lower, upper, np.random.RandomState())\n\n    def uniform(low, high, name=None):\n        return param_exp.uniform(low, high, np.random.RandomState())\n\n    def quniform(low, high, q, name=None):\n        assert high > low, \'Upper bound must be larger than lower bound\'\n        return param_exp.quniform(low, high, q, np.random.RandomState())\n\n    def loguniform(low, high, name=None):\n        assert low > 0, \'Lower bound must be positive\'\n        return param_exp.loguniform(low, high, np.random.RandomState())\n\n    def qloguniform(low, high, q, name=None):\n        return param_exp.qloguniform(low, high, q, np.random.RandomState())\n\n    def normal(mu, sigma, name=None):\n        return param_exp.normal(mu, sigma, np.random.RandomState())\n\n    def qnormal(mu, sigma, q, name=None):\n        return param_exp.qnormal(mu, sigma, q, np.random.RandomState())\n\n    def lognormal(mu, sigma, name=None):\n        return param_exp.lognormal(mu, sigma, np.random.RandomState())\n\n    def qlognormal(mu, sigma, q, name=None):\n        return param_exp.qlognormal(mu, sigma, q, np.random.RandomState())\n\n    def function_choice(*funcs, name=None):\n        return param_exp.choice(funcs, np.random.RandomState())()\n\n    def mutable_layer():\n        raise RuntimeError(\'Cannot call nni.mutable_layer in this mode\')\n\nelse:\n\n    def choice(options, name=None, key=None):\n        return options[_get_param(key)]\n\n    def randint(lower, upper, name=None, key=None):\n        return _get_param(key)\n\n    def uniform(low, high, name=None, key=None):\n        return _get_param(key)\n\n    def quniform(low, high, q, name=None, key=None):\n        return _get_param(key)\n\n    def loguniform(low, high, name=None, key=None):\n        return _get_param(key)\n\n    def qloguniform(low, high, q, name=None, key=None):\n        return _get_param(key)\n\n    def normal(mu, sigma, name=None, key=None):\n        return _get_param(key)\n\n    def qnormal(mu, sigma, q, name=None, key=None):\n        return _get_param(key)\n\n    def lognormal(mu, sigma, name=None, key=None):\n        return _get_param(key)\n\n    def qlognormal(mu, sigma, q, name=None, key=None):\n        return _get_param(key)\n\n    def function_choice(funcs, name=None, key=None):\n        return funcs[_get_param(key)]()\n\n    def mutable_layer(\n            mutable_id,\n            mutable_layer_id,\n            funcs,\n            funcs_args,\n            fixed_inputs,\n            optional_inputs,\n            optional_input_size,\n            mode=\'classic_mode\',\n            tf=None):\n        \'\'\'execute the chosen function and inputs.\n        Below is an example of chosen function and inputs:\n        {\n            ""mutable_id"": {\n                ""mutable_layer_id"": {\n                    ""chosen_layer"": ""pool"",\n                    ""chosen_inputs"": [""out1"", ""out3""]\n                }\n            }\n        }\n        Parameters:\n        ---------------\n        mutable_id: the name of this mutable_layer block (which could have multiple mutable layers)\n        mutable_layer_id: the name of a mutable layer in this block\n        funcs: dict of function calls\n        funcs_args:\n        fixed_inputs:\n        optional_inputs: dict of optional inputs\n        optional_input_size: number of candidate inputs to be chosen\n        tf: tensorflow module\n        \'\'\'\n        args = (mutable_id, mutable_layer_id, funcs, funcs_args, fixed_inputs, optional_inputs, optional_input_size)\n        if mode == \'classic_mode\':\n            return classic_mode(*args)\n        assert tf is not None, \'Internal Error: Tensorflow should not be None in modes other than classic_mode\'\n        if mode == \'enas_mode\':\n            return enas_mode(*args, tf)\n        if mode == \'oneshot_mode\':\n            return oneshot_mode(*args, tf)\n        if mode == \'darts_mode\':\n            return darts_mode(*args, tf)\n        raise RuntimeError(\'Unrecognized mode: %s\' % mode)\n\n    def _get_param(key):\n        if trial.get_current_parameter() is None:\n            trial.get_next_parameter()\n        return trial.get_current_parameter(key)\n'"
src/sdk/pynni/nni/trial.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .utils import to_json\nfrom .env_vars import trial_env_vars\nfrom . import platform\n\n\n__all__ = [\n    \'get_next_parameter\',\n    \'get_current_parameter\',\n    \'report_intermediate_result\',\n    \'report_final_result\',\n    \'get_experiment_id\',\n    \'get_trial_id\',\n    \'get_sequence_id\'\n]\n\n\n_params = None\n_experiment_id = platform.get_experiment_id()\n_trial_id = platform.get_trial_id()\n_sequence_id = platform.get_sequence_id()\n\n\ndef get_next_parameter():\n    """"""\n    Get the hyper paremeters generated by tuner. For a multiphase experiment, it returns a new group of hyper\n    parameters at each call of get_next_parameter. For a non-multiphase (multiPhase is not configured or set to False)\n    experiment, it returns hyper parameters only on the first call for each trial job, it returns None since second call.\n    This API should be called only once in each trial job of an experiment which is not specified as multiphase.\n\n    Returns\n    -------\n    dict\n        A dict object contains the hyper parameters generated by tuner, the keys of the dict are defined in\n        search space. Returns None if no more hyper parameters can be generated by tuner.\n    """"""\n    global _params\n    _params = platform.get_next_parameter()\n    if _params is None:\n        return None\n    return _params[\'parameters\']\n\ndef get_current_parameter(tag=None):\n    """"""\n    Get current hyper parameters generated by tuner. It returns the same group of hyper parameters as the last\n    call of get_next_parameter returns.\n\n    Parameters\n    ----------\n    tag: str\n        hyper parameter key\n    """"""\n    global _params\n    if _params is None:\n        return None\n    if tag is None:\n        return _params[\'parameters\']\n    return _params[\'parameters\'][tag]\n\ndef get_experiment_id():\n    """"""\n    Get experiment ID.\n\n    Returns\n    -------\n    str\n        Identifier of current experiment\n    """"""\n    return _experiment_id\n\ndef get_trial_id():\n    """"""\n    Get trial job ID which is string identifier of a trial job, for example \'MoXrp\'. In one experiment, each trial\n    job has an unique string ID.\n\n    Returns\n    -------\n    str\n        Identifier of current trial job which is calling this API.\n    """"""\n    return _trial_id\n\ndef get_sequence_id():\n    """"""\n    Get trial job sequence nubmer. A sequence number is an integer value assigned to each trial job base on the\n    order they are submitted, incremental starting from 0. In one experiment, both trial job ID and sequence number\n    are unique for each trial job, they are of different data types.\n\n    Returns\n    -------\n    int\n        Sequence number of current trial job which is calling this API.\n    """"""\n    return _sequence_id\n\n_intermediate_seq = 0\n\ndef report_intermediate_result(metric):\n    """"""\n    Reports intermediate result to NNI.\n\n    Parameters\n    ----------\n    metric:\n        serializable object.\n    """"""\n    global _intermediate_seq\n    assert _params or trial_env_vars.NNI_PLATFORM is None, \\\n        \'nni.get_next_parameter() needs to be called before report_intermediate_result\'\n    metric = to_json({\n        \'parameter_id\': _params[\'parameter_id\'] if _params else None,\n        \'trial_job_id\': trial_env_vars.NNI_TRIAL_JOB_ID,\n        \'type\': \'PERIODICAL\',\n        \'sequence\': _intermediate_seq,\n        \'value\': to_json(metric)\n    })\n    _intermediate_seq += 1\n    platform.send_metric(metric)\n\ndef report_final_result(metric):\n    """"""\n    Reports final result to NNI.\n\n    Parameters\n    ----------\n    metric: serializable object\n        Usually (for built-in tuners to work), it should be a number, or\n        a dict with key ""default"" (a number), and any other extra keys.\n    """"""\n    assert _params or trial_env_vars.NNI_PLATFORM is None, \\\n        \'nni.get_next_parameter() needs to be called before report_final_result\'\n    metric = to_json({\n        \'parameter_id\': _params[\'parameter_id\'] if _params else None,\n        \'trial_job_id\': trial_env_vars.NNI_TRIAL_JOB_ID,\n        \'type\': \'FINAL\',\n        \'sequence\': 0,\n        \'value\': to_json(metric)\n    })\n    platform.send_metric(metric)\n'"
src/sdk/pynni/nni/tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nTuner is an AutoML algorithm, which generates a new configuration for the next try.\nA new trial will run with this configuration.\n\nSee :class:`Tuner`\' specification and ``docs/en_US/tuners.rst`` for details.\n""""""\n\nimport logging\n\nimport nni\n\nfrom .recoverable import Recoverable\n\n__all__ = [\'Tuner\']\n\n_logger = logging.getLogger(__name__)\n\n\nclass Tuner(Recoverable):\n    """"""\n    Tuner is an AutoML algorithm, which generates a new configuration for the next try.\n    A new trial will run with this configuration.\n\n    This is the abstract base class for all tuners.\n    Tuning algorithms should inherit this class and override :meth:`update_search_space`, :meth:`receive_trial_result`,\n    as well as :meth:`generate_parameters` or :meth:`generate_multiple_parameters`.\n\n    After initializing, NNI will first call :meth:`update_search_space` to tell tuner the feasible region,\n    and then call :meth:`generate_parameters` one or more times to request for hyper-parameter configurations.\n\n    The framework will train several models with given configuration.\n    When one of them is finished, the final accuracy will be reported to :meth:`receive_trial_result`.\n    And then another configuration will be reqeusted and trained, util the whole experiment finish.\n\n    If a tuner want\'s to know when a trial ends, it can also override :meth:`trial_end`.\n\n    Tuners use *parameter ID* to track trials.\n    In tuner context, there is a one-to-one mapping between parameter ID and trial.\n    When the framework ask tuner to generate hyper-parameters for a new trial,\n    an ID has already been assigned and can be recorded in :meth:`generate_parameters`.\n    Later when the trial ends, the ID will be reported to :meth:`trial_end`,\n    and :meth:`receive_trial_result` if it has a final result.\n    Parameter IDs are unique integers.\n\n    The type/format of search space and hyper-parameters are not limited,\n    as long as they are JSON-serializable and in sync with trial code.\n    For HPO tuners, however, there is a widely shared common interface,\n    which supports ``choice``, ``randint``, ``uniform``, and so on.\n    See ``docs/en_US/Tutorial/SearchSpaceSpec.md`` for details of this interface.\n\n    [WIP] For advanced tuners which take advantage of trials\' intermediate results,\n    an ``Advisor`` interface is under development.\n\n    See Also\n    --------\n    Builtin tuners:\n    :class:`~nni.hyperopt_tuner.hyperopt_tuner.HyperoptTuner`\n    :class:`~nni.evolution_tuner.evolution_tuner.EvolutionTuner`\n    :class:`~nni.smac_tuner.SMACTuner`\n    :class:`~nni.gridsearch_tuner.GridSearchTuner`\n    :class:`~nni.networkmorphism_tuner.networkmorphism_tuner.NetworkMorphismTuner`\n    :class:`~nni.metis_tuner.mets_tuner.MetisTuner`\n    :class:`~nni.ppo_tuner.PPOTuner`\n    :class:`~nni.gp_tuner.gp_tuner.GPTuner`\n    """"""\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Abstract method which provides a set of hyper-parameters.\n\n        This method will get called when the framework is about to launch a new trial,\n        if user does not override :meth:`generate_multiple_parameters`.\n\n        The return value of this method will be received by trials via :func:`nni.get_next_parameter`.\n        It should fit in the search space, though the framework will not verify this.\n\n        User code must override either this method or :meth:`generate_multiple_parameters`.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        any\n            The hyper-parameters, a dict in most cases, but could be any JSON-serializable type when needed.\n\n        Raises\n        ------\n        nni.NoMoreTrialError\n            If the search space is fully explored, tuner can raise this exception.\n        """"""\n        # FIXME: some tuners raise NoMoreTrialError when they are waiting for more trial results\n        # we need to design a new exception for this purpose\n        raise NotImplementedError(\'Tuner: generate_parameters not implemented\')\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        """"""\n        Callback method which provides multiple sets of hyper-parameters.\n\n        This method will get called when the framework is about to launch one or more new trials.\n\n        If user does not override this method, it will invoke :meth:`generate_parameters` on each parameter ID.\n\n        See :meth:`generate_parameters` for details.\n\n        User code must override either this method or :meth:`generate_parameters`.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n\n        Returns\n        -------\n        list\n            List of hyper-parameters. An empty list indicates there are no more trials.\n        """"""\n        result = []\n        for parameter_id in parameter_id_list:\n            try:\n                _logger.debug(""generating param for %s"", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                return result\n            result.append(res)\n        return result\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Abstract method invoked when a trial reports its final result. Must override.\n\n        This method only listens to results of algorithm-generated hyper-parameters.\n        Currently customized trials added from web UI will not report result to this method.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        """"""\n        raise NotImplementedError(\'Tuner: receive_trial_result not implemented\')\n\n    def _accept_customized_trials(self, accept=True):\n        # FIXME: because Tuner is designed as interface, this API should not be here\n\n        # Enable or disable receiving results of user-added hyper-parameters.\n        # By default `receive_trial_result()` will only receive results of algorithm-generated hyper-parameters.\n        # If tuners want to receive those of customized parameters as well, they can call this function in `__init__()`.\n\n        # pylint: disable=attribute-defined-outside-init\n        self._accept_customized = accept\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        """"""\n        Abstract method invoked when a trial is completed or terminated. Do nothing by default.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        """"""\n\n    def update_search_space(self, search_space):\n        """"""\n        Abstract method for updating the search space. Must override.\n\n        Tuners are advised to support updating search space at run-time.\n        If a tuner can only set search space once before generating first hyper-parameters,\n        it should explicitly document this behaviour.\n\n        Parameters\n        ----------\n        search_space\n            JSON object defined by experiment owner.\n        """"""\n        raise NotImplementedError(\'Tuner: update_search_space not implemented\')\n\n    def load_checkpoint(self):\n        """"""\n        Internal API under revising, not recommended for end users.\n        """"""\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info(\'Load checkpoint ignored by tuner, checkpoint path: %s\', checkpoin_path)\n\n    def save_checkpoint(self):\n        """"""\n        Internal API under revising, not recommended for end users.\n        """"""\n        checkpoin_path = self.get_checkpoint_path()\n        _logger.info(\'Save checkpoint ignored by tuner, checkpoint path: %s\', checkpoin_path)\n\n    def import_data(self, data):\n        """"""\n        Internal API under revising, not recommended for end users.\n        """"""\n        # Import additional data for tuning\n        # data: a list of dictionarys, each of which has at least two keys, \'parameter\' and \'value\'\n        pass\n\n    def _on_exit(self):\n        pass\n\n    def _on_error(self):\n        pass\n'"
src/sdk/pynni/nni/utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport copy\nimport functools\nfrom enum import Enum, unique\nimport json_tricks\n\nfrom . import parameter_expressions\nfrom .common import init_logger\nfrom .env_vars import dispatcher_env_vars\n\n\nto_json = functools.partial(json_tricks.dumps, allow_nan=True)\n\n@unique\nclass OptimizeMode(Enum):\n    """"""Optimize Mode class\n\n    if OptimizeMode is \'minimize\', it means the tuner need to minimize the reward\n    that received from Trial.\n\n    if OptimizeMode is \'maximize\', it means the tuner need to maximize the reward\n    that received from Trial.\n    """"""\n    Minimize = \'minimize\'\n    Maximize = \'maximize\'\n\n\nclass NodeType:\n    """"""Node Type class\n    """"""\n    ROOT = \'root\'\n    TYPE = \'_type\'\n    VALUE = \'_value\'\n    INDEX = \'_index\'\n    NAME = \'_name\'\n\n\nclass MetricType:\n    """"""The types of metric data\n    """"""\n    FINAL = \'FINAL\'\n    PERIODICAL = \'PERIODICAL\'\n    REQUEST_PARAMETER = \'REQUEST_PARAMETER\'\n\n\ndef split_index(params):\n    """"""\n    Delete index infromation from params\n    """"""\n    if isinstance(params, dict):\n        if NodeType.INDEX in params.keys():\n            return split_index(params[NodeType.VALUE])\n        result = {}\n        for key in params:\n            result[key] = split_index(params[key])\n        return result\n    else:\n        return params\n\n\ndef extract_scalar_reward(value, scalar_key=\'default\'):\n    """"""\n    Extract scalar reward from trial result.\n\n    Parameters\n    ----------\n    value : int, float, dict\n        the reported final metric data\n    scalar_key : str\n        the key name that indicates the numeric number\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named ""default"" whose value is float/int.\n    """"""\n    if isinstance(value, (float, int)):\n        reward = value\n    elif isinstance(value, dict) and scalar_key in value and isinstance(value[scalar_key], (float, int)):\n        reward = value[scalar_key]\n    else:\n        raise RuntimeError(\'Incorrect final result: the final result should be float/int, \' \\\n            \'or a dict which has a key named ""default"" whose value is float/int.\')\n    return reward\n\n\ndef extract_scalar_history(trial_history, scalar_key=\'default\'):\n    """"""\n    Extract scalar value from a list of intermediate results.\n\n    Parameters\n    ----------\n    trial_history : list\n        accumulated intermediate results of a trial\n    scalar_key : str\n        the key name that indicates the numeric number\n\n    Raises\n    ------\n    RuntimeError\n        Incorrect final result: the final result should be float/int,\n        or a dict which has a key named ""default"" whose value is float/int.\n    """"""\n    return [extract_scalar_reward(ele, scalar_key) for ele in trial_history]\n\n\ndef convert_dict2tuple(value):\n    """"""\n    convert dict type to tuple to solve unhashable problem.\n    """"""\n    if isinstance(value, dict):\n        for _keys in value:\n            value[_keys] = convert_dict2tuple(value[_keys])\n        return tuple(sorted(value.items()))\n    return value\n\n\ndef init_dispatcher_logger():\n    """"""\n    Initialize dispatcher logging configuration\n    """"""\n    logger_file_path = \'dispatcher.log\'\n    if dispatcher_env_vars.NNI_LOG_DIRECTORY is not None:\n        logger_file_path = os.path.join(dispatcher_env_vars.NNI_LOG_DIRECTORY, logger_file_path)\n    init_logger(logger_file_path, dispatcher_env_vars.NNI_LOG_LEVEL)\n\n\ndef json2space(x, oldy=None, name=NodeType.ROOT):\n    """"""\n    Change search space from json format to hyperopt format\n\n    """"""\n    y = list()\n    if isinstance(x, dict):\n        if NodeType.TYPE in x.keys():\n            _type = x[NodeType.TYPE]\n            name = name + \'-\' + _type\n            if _type == \'choice\':\n                if oldy is not None:\n                    _index = oldy[NodeType.INDEX]\n                    y += json2space(x[NodeType.VALUE][_index],\n                                    oldy[NodeType.VALUE], name=name+\'[%d]\' % _index)\n                else:\n                    y += json2space(x[NodeType.VALUE], None, name=name)\n            y.append(name)\n        else:\n            for key in x.keys():\n                y += json2space(x[key], oldy[key] if oldy else None, name+""[%s]"" % str(key))\n    elif isinstance(x, list):\n        for i, x_i in enumerate(x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\'\\\'_name\\\' key is not found in this nested search space.\')\n            y += json2space(x_i, oldy[i] if oldy else None, name + ""[%d]"" % i)\n    return y\n\n\ndef json2parameter(x, is_rand, random_state, oldy=None, Rand=False, name=NodeType.ROOT):\n    """"""\n    Json to pramaters.\n\n    """"""\n    if isinstance(x, dict):\n        if NodeType.TYPE in x.keys():\n            _type = x[NodeType.TYPE]\n            _value = x[NodeType.VALUE]\n            name = name + \'-\' + _type\n            Rand |= is_rand[name]\n            if Rand is True:\n                if _type == \'choice\':\n                    _index = random_state.randint(len(_value))\n                    y = {\n                        NodeType.INDEX: _index,\n                        NodeType.VALUE: json2parameter(\n                            x[NodeType.VALUE][_index],\n                            is_rand,\n                            random_state,\n                            None,\n                            Rand,\n                            name=name+""[%d]"" % _index\n                        )\n                    }\n                else:\n                    y = getattr(parameter_expressions, _type)(*(_value + [random_state]))\n            else:\n                y = copy.deepcopy(oldy)\n        else:\n            y = dict()\n            for key in x.keys():\n                y[key] = json2parameter(\n                    x[key],\n                    is_rand,\n                    random_state,\n                    oldy[key] if oldy else None,\n                    Rand,\n                    name + ""[%s]"" % str(key)\n                )\n    elif isinstance(x, list):\n        y = list()\n        for i, x_i in enumerate(x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\'\\\'_name\\\' key is not found in this nested search space.\')\n            y.append(json2parameter(\n                x_i,\n                is_rand,\n                random_state,\n                oldy[i] if oldy else None,\n                Rand,\n                name + ""[%d]"" % i\n            ))\n    else:\n        y = copy.deepcopy(x)\n    return y\n'"
src/sdk/pynni/tests/__init__.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nos.environ['NNI_PLATFORM'] = 'unittest'\nos.environ['NNI_TRIAL_JOB_ID'] = 'test_trial_job_id'\n"""
src/sdk/pynni/tests/test_assessor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport nni.protocol\nfrom nni.protocol import CommandType, send, receive\nfrom nni.assessor import Assessor, AssessResult\nfrom nni.msg_dispatcher import MsgDispatcher\n\nfrom io import BytesIO\nimport json\nfrom unittest import TestCase, main\n\n_trials = []\n_end_trials = []\n\n\nclass NaiveAssessor(Assessor):\n    def assess_trial(self, trial_job_id, trial_history):\n        _trials.append(trial_job_id)\n        if sum(trial_history) % 2 == 0:\n            return AssessResult.Good\n        else:\n            return AssessResult.Bad\n\n    def trial_end(self, trial_job_id, success):\n        _end_trials.append((trial_job_id, success))\n\n\n_in_buf = BytesIO()\n_out_buf = BytesIO()\n\n\ndef _reverse_io():\n    _in_buf.seek(0)\n    _out_buf.seek(0)\n    nni.protocol._out_file = _in_buf\n    nni.protocol._in_file = _out_buf\n\n\ndef _restore_io():\n    _in_buf.seek(0)\n    _out_buf.seek(0)\n    nni.protocol._in_file = _in_buf\n    nni.protocol._out_file = _out_buf\n\n\nclass AssessorTestCase(TestCase):\n    def test_assessor(self):\n        _reverse_io()\n        send(CommandType.ReportMetricData, \'{""trial_job_id"":""A"",""type"":""PERIODICAL"",""sequence"":0,""value"":""2""}\')\n        send(CommandType.ReportMetricData, \'{""trial_job_id"":""B"",""type"":""PERIODICAL"",""sequence"":0,""value"":""2""}\')\n        send(CommandType.ReportMetricData, \'{""trial_job_id"":""A"",""type"":""PERIODICAL"",""sequence"":1,""value"":""3""}\')\n        send(CommandType.TrialEnd, \'{""trial_job_id"":""A"",""event"":""SYS_CANCELED""}\')\n        send(CommandType.TrialEnd, \'{""trial_job_id"":""B"",""event"":""SUCCEEDED""}\')\n        send(CommandType.NewTrialJob, \'null\')\n        _restore_io()\n\n        assessor = NaiveAssessor()\n        dispatcher = MsgDispatcher(None, assessor)\n        nni.msg_dispatcher_base._worker_fast_exit_on_terminate = False\n\n        dispatcher.run()\n        e = dispatcher.worker_exceptions[0]\n        self.assertIs(type(e), AssertionError)\n        self.assertEqual(e.args[0], \'Unsupported command: CommandType.NewTrialJob\')\n\n        self.assertEqual(_trials, [\'A\', \'B\', \'A\'])\n        self.assertEqual(_end_trials, [(\'A\', False), (\'B\', True)])\n\n        _reverse_io()\n        command, data = receive()\n        self.assertIs(command, CommandType.KillTrialJob)\n        self.assertEqual(data, \'""A""\')\n        self.assertEqual(len(_out_buf.read()), 0)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_builtin_tuners.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport glob\nimport json\nimport logging\nimport os\nimport random\nimport shutil\nimport sys\nfrom collections import deque\nfrom unittest import TestCase, main\n\nfrom nni.batch_tuner.batch_tuner import BatchTuner\nfrom nni.evolution_tuner.evolution_tuner import EvolutionTuner\nfrom nni.gp_tuner.gp_tuner import GPTuner\nfrom nni.gridsearch_tuner.gridsearch_tuner import GridSearchTuner\nfrom nni.hyperopt_tuner.hyperopt_tuner import HyperoptTuner\nfrom nni.metis_tuner.metis_tuner import MetisTuner\nfrom nni.msg_dispatcher import _pack_parameter, MsgDispatcher\nfrom nni.pbt_tuner.pbt_tuner import PBTTuner\n\ntry:\n    from nni.smac_tuner.smac_tuner import SMACTuner\nexcept ImportError:\n    assert sys.platform == ""win32""\nfrom nni.tuner import Tuner\n\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(\'test_tuner\')\n\n\nclass BuiltinTunersTestCase(TestCase):\n    """"""\n    Targeted at testing functions of built-in tuners, including\n        - [ ] load_checkpoint\n        - [ ] save_checkpoint\n        - [X] update_search_space\n        - [X] generate_multiple_parameters\n        - [X] import_data\n        - [ ] trial_end\n        - [x] receive_trial_result\n    """"""\n\n    def setUp(self):\n        self.test_round = 3\n        self.params_each_round = 50\n        self.exhaustive = False\n\n    def send_trial_callback(self, param_queue):\n        def receive(*args):\n            param_queue.append(tuple(args))\n        return receive\n\n    def search_space_test_one(self, tuner_factory, search_space):\n        tuner = tuner_factory()\n        self.assertIsInstance(tuner, Tuner)\n        tuner.update_search_space(search_space)\n\n        for i in range(self.test_round):\n            queue = deque()\n            parameters = tuner.generate_multiple_parameters(list(range(i * self.params_each_round,\n                                                                       (i + 1) * self.params_each_round)),\n                                                            st_callback=self.send_trial_callback(queue))\n            logger.debug(parameters)\n            self.check_range(parameters, search_space)\n            for k in range(min(len(parameters), self.params_each_round)):\n                tuner.receive_trial_result(self.params_each_round * i + k, parameters[k], random.uniform(-100, 100))\n            while queue:\n                id_, params = queue.popleft()\n                self.check_range([params], search_space)\n                tuner.receive_trial_result(id_, params, random.uniform(-100, 100))\n            if not parameters and not self.exhaustive:\n                raise ValueError(""No parameters generated"")\n\n    def check_range(self, generated_params, search_space):\n        EPS = 1E-6\n        for param in generated_params:\n            if self._testMethodName == ""test_batch"":\n                param = {list(search_space.keys())[0]: param}\n            for k, v in param.items():\n                if k == ""load_checkpoint_dir"" or k == ""save_checkpoint_dir"":\n                    self.assertIsInstance(v, str)\n                    continue\n                if k.startswith(""_mutable_layer""):\n                    _, block, layer, choice = k.split(""/"")\n                    cand = search_space[block][""_value""][layer].get(choice)\n                    # cand could be None, e.g., optional_inputs_chosen_state\n                    if choice == ""layer_choice"":\n                        self.assertIn(v, cand)\n                    if choice == ""optional_input_size"":\n                        if isinstance(cand, int):\n                            self.assertEqual(v, cand)\n                        else:\n                            self.assertGreaterEqual(v, cand[0])\n                            self.assertLessEqual(v, cand[1])\n                    if choice == ""optional_inputs"":\n                        pass  # ignore for now\n                    continue\n                item = search_space[k]\n                if item[""_type""] == ""choice"":\n                    self.assertIn(v, item[""_value""])\n                if item[""_type""] == ""randint"":\n                    self.assertIsInstance(v, int)\n                if item[""_type""] == ""uniform"":\n                    self.assertIsInstance(v, float)\n                if item[""_type""] in (""randint"", ""uniform"", ""quniform"", ""loguniform"", ""qloguniform""):\n                    self.assertGreaterEqual(v, item[""_value""][0])\n                    self.assertLessEqual(v, item[""_value""][1])\n                if item[""_type""].startswith(""q""):\n                    multiple = v / item[""_value""][2]\n                    print(k, v, multiple, item)\n                    if item[""_value""][0] + EPS < v < item[""_value""][1] - EPS:\n                        self.assertAlmostEqual(int(round(multiple)), multiple)\n                if item[""_type""] in (""qlognormal"", ""lognormal""):\n                    self.assertGreaterEqual(v, 0)\n                if item[""_type""] == ""mutable_layer"":\n                    for layer_name in item[""_value""].keys():\n                        self.assertIn(v[layer_name][""chosen_layer""], item[""layer_choice""])\n\n    def search_space_test_all(self, tuner_factory, supported_types=None, ignore_types=None, fail_types=None):\n        # Three types: 1. supported; 2. ignore; 3. fail.\n        # NOTE(yuge): ignore types\n        # Supported types are listed in the table. They are meant to be supported and should be correct.\n        # Other than those, all the rest are ""unsupported"", which are expected to produce ridiculous results\n        # or throw some exceptions. However, there are certain types I can\'t check. For example, generate\n        # ""normal"" using GP Tuner returns successfully and results are fine if we check the range (-inf to +inf),\n        # but they make no sense: it\'s not a normal distribution. So they are ignored in tests for now.\n        with open(os.path.join(os.path.dirname(__file__), ""assets/search_space.json""), ""r"") as fp:\n            search_space_all = json.load(fp)\n        if supported_types is None:\n            supported_types = [""choice"", ""randint"", ""uniform"", ""quniform"", ""loguniform"", ""qloguniform"",\n                               ""normal"", ""qnormal"", ""lognormal"", ""qlognormal""]\n        if fail_types is None:\n            fail_types = []\n        if ignore_types is None:\n            ignore_types = []\n        full_supported_search_space = dict()\n        for single in search_space_all:\n            space = search_space_all[single]\n            if any(single.startswith(t) for t in ignore_types):\n                continue\n            expected_fail = not any(single.startswith(t) for t in supported_types) or \\\n                any(single.startswith(t) for t in fail_types) or \\\n                ""fail"" in single  # name contains fail (fail on all)\n            single_search_space = {single: space}\n            if not expected_fail:\n                # supports this key\n                self.search_space_test_one(tuner_factory, single_search_space)\n                full_supported_search_space.update(single_search_space)\n            else:\n                # unsupported key\n                with self.assertRaises(Exception, msg=""Testing {}"".format(single)) as cm:\n                    self.search_space_test_one(tuner_factory, single_search_space)\n                logger.info(""%s %s %s"", tuner_factory, single, cm.exception)\n        if not any(t in self._testMethodName for t in [""batch"", ""grid_search""]):\n            # grid search fails for too many combinations\n            logger.info(""Full supported search space: %s"", full_supported_search_space)\n            self.search_space_test_one(tuner_factory, full_supported_search_space)\n\n    def import_data_test_for_pbt(self):\n        """"""\n        test1: import data with complete epoch\n        test2: import data with incomplete epoch\n        """"""\n        search_space = {\n            ""choice_str"": {\n                ""_type"": ""choice"",\n                ""_value"": [""cat"", ""dog"", ""elephant"", ""cow"", ""sheep"", ""panda""]\n            }\n        }\n        all_checkpoint_dir = os.path.expanduser(""~/nni/checkpoint/test/"")\n        population_size = 4\n        # ===import data at the beginning===\n        tuner = PBTTuner(\n            all_checkpoint_dir=all_checkpoint_dir,\n            population_size=population_size\n        )\n        self.assertIsInstance(tuner, Tuner)\n        tuner.update_search_space(search_space)\n        save_dirs = [os.path.join(all_checkpoint_dir, str(i), str(0)) for i in range(population_size)]\n        # create save checkpoint directory\n        for save_dir in save_dirs:\n            os.makedirs(save_dir, exist_ok=True)\n        # for simplicity, omit ""load_checkpoint_dir""\n        data = [{""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[0]}, ""value"": 1.1},\n                {""parameter"": {""choice_str"": ""dog"", ""save_checkpoint_dir"": save_dirs[1]}, ""value"": {""default"": 1.2, ""tmp"": 2}},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[2]}, ""value"": 11},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[3]}, ""value"": 7}]\n        epoch = tuner.import_data(data)\n        self.assertEqual(epoch, 1)\n        logger.info(""Imported data successfully at the beginning"")\n        shutil.rmtree(all_checkpoint_dir)\n        # ===import another data at the beginning, test the case when there is an incompleted epoch===\n        tuner = PBTTuner(\n            all_checkpoint_dir=all_checkpoint_dir,\n            population_size=population_size\n        )\n        self.assertIsInstance(tuner, Tuner)\n        tuner.update_search_space(search_space)\n        for i in range(population_size - 1):\n            save_dirs.append(os.path.join(all_checkpoint_dir, str(i), str(1)))\n        for save_dir in save_dirs:\n            os.makedirs(save_dir, exist_ok=True)\n        data = [{""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[0]}, ""value"": 1.1},\n                {""parameter"": {""choice_str"": ""dog"", ""save_checkpoint_dir"": save_dirs[1]}, ""value"": {""default"": 1.2, ""tmp"": 2}},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[2]}, ""value"": 11},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[3]}, ""value"": 7},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[4]}, ""value"": 1.1},\n                {""parameter"": {""choice_str"": ""dog"", ""save_checkpoint_dir"": save_dirs[5]}, ""value"": {""default"": 1.2, ""tmp"": 2}},\n                {""parameter"": {""choice_str"": ""cat"", ""save_checkpoint_dir"": save_dirs[6]}, ""value"": 11}]\n        epoch = tuner.import_data(data)\n        self.assertEqual(epoch, 1)\n        logger.info(""Imported data successfully at the beginning with incomplete epoch"")\n        shutil.rmtree(all_checkpoint_dir)\n\n    def import_data_test(self, tuner_factory, stype=""choice_str""):\n        """"""\n        import data at the beginning with number value and dict value\n        import data in the middle also with number value and dict value, and duplicate data record\n        generate parameters after data import\n\n        Parameters\n        ----------\n        tuner_factory : lambda\n            a lambda for instantiate a tuner\n        stype : str\n            the value type of hp choice, support ""choice_str"" and ""choice_num""\n        """"""\n        if stype == ""choice_str"":\n            search_space = {\n                ""choice_str"": {\n                    ""_type"": ""choice"",\n                    ""_value"": [""cat"", ""dog"", ""elephant"", ""cow"", ""sheep"", ""panda""]\n                }\n            }\n        elif stype == ""choice_num"":\n            search_space = {\n                ""choice_num"": {\n                    ""_type"": ""choice"",\n                    ""_value"": [10, 20, 30, 40, 50, 60]\n                }\n            }\n        else:\n            raise RuntimeError(""Unexpected stype"")\n        tuner = tuner_factory()\n        self.assertIsInstance(tuner, Tuner)\n        tuner.update_search_space(search_space)\n        # import data at the beginning\n        if stype == ""choice_str"":\n            data = [{""parameter"": {""choice_str"": ""cat""}, ""value"": 1.1},\n                    {""parameter"": {""choice_str"": ""dog""}, ""value"": {""default"": 1.2, ""tmp"": 2}}]\n        else:\n            data = [{""parameter"": {""choice_num"": 20}, ""value"": 1.1},\n                    {""parameter"": {""choice_num"": 60}, ""value"": {""default"": 1.2, ""tmp"": 2}}]\n        tuner.import_data(data)\n        logger.info(""Imported data successfully at the beginning"")\n        # generate parameters\n        parameters = tuner.generate_multiple_parameters(list(range(3)))\n        for i in range(3):\n            tuner.receive_trial_result(i, parameters[i], random.uniform(-100, 100))\n        # import data in the middle\n        if stype == ""choice_str"":\n            data = [{""parameter"": {""choice_str"": ""cat""}, ""value"": 1.1},\n                    {""parameter"": {""choice_str"": ""dog""}, ""value"": {""default"": 1.2, ""tmp"": 2}},\n                    {""parameter"": {""choice_str"": ""cow""}, ""value"": 1.3}]\n        else:\n            data = [{""parameter"": {""choice_num"": 20}, ""value"": 1.1},\n                    {""parameter"": {""choice_num"": 60}, ""value"": {""default"": 1.2, ""tmp"": 2}},\n                    {""parameter"": {""choice_num"": 50}, ""value"": 1.3}]\n        tuner.import_data(data)\n        logger.info(""Imported data successfully in the middle"")\n        # generate parameters again\n        parameters = tuner.generate_multiple_parameters([3])\n        tuner.receive_trial_result(3, parameters[0], random.uniform(-100, 100))\n\n    def test_grid_search(self):\n        self.exhaustive = True\n        tuner_fn = lambda: GridSearchTuner()\n        self.search_space_test_all(tuner_fn,\n                                   supported_types=[""choice"", ""randint"", ""quniform""])\n        self.import_data_test(tuner_fn)\n\n    def test_tpe(self):\n        tuner_fn = lambda: HyperoptTuner(""tpe"")\n        self.search_space_test_all(tuner_fn,\n                                   ignore_types=[""uniform_equal"", ""qloguniform_equal"", ""loguniform_equal"", ""quniform_clip_2""])\n        # NOTE: types are ignored because `tpe.py line 465, in adaptive_parzen_normal assert prior_sigma > 0`\n        self.import_data_test(tuner_fn)\n\n    def test_random_search(self):\n        tuner_fn = lambda: HyperoptTuner(""random_search"")\n        self.search_space_test_all(tuner_fn)\n        self.import_data_test(tuner_fn)\n\n    def test_anneal(self):\n        tuner_fn = lambda: HyperoptTuner(""anneal"")\n        self.search_space_test_all(tuner_fn)\n        self.import_data_test(tuner_fn)\n\n    def test_smac(self):\n        if sys.platform == ""win32"":\n            return  # smac doesn\'t work on windows\n        tuner_fn = lambda: SMACTuner()\n        self.search_space_test_all(tuner_fn,\n                                   supported_types=[""choice"", ""randint"", ""uniform"", ""quniform"", ""loguniform""])\n        self.import_data_test(tuner_fn)\n\n    def test_batch(self):\n        self.exhaustive = True\n        tuner_fn = lambda: BatchTuner()\n        self.search_space_test_all(tuner_fn,\n                                   supported_types=[""choice""])\n        self.import_data_test(tuner_fn)\n\n    def test_evolution(self):\n        # Needs enough population size, otherwise it will throw a runtime error\n        tuner_fn = lambda: EvolutionTuner(population_size=100)\n        self.search_space_test_all(tuner_fn)\n        self.import_data_test(tuner_fn)\n\n    def test_gp(self):\n        self.test_round = 1  # NOTE: GP tuner got hanged for multiple testing round\n        tuner_fn = lambda: GPTuner()\n        self.search_space_test_all(tuner_fn,\n                                   supported_types=[""choice"", ""randint"", ""uniform"", ""quniform"", ""loguniform"",\n                                                    ""qloguniform""],\n                                   ignore_types=[""normal"", ""lognormal"", ""qnormal"", ""qlognormal""],\n                                   fail_types=[""choice_str"", ""choice_mixed""])\n        self.import_data_test(tuner_fn, ""choice_num"")\n\n    def test_metis(self):\n        self.test_round = 1  # NOTE: Metis tuner got hanged for multiple testing round\n        tuner_fn = lambda: MetisTuner()\n        self.search_space_test_all(tuner_fn,\n                                   supported_types=[""choice"", ""randint"", ""uniform"", ""quniform""],\n                                   fail_types=[""choice_str"", ""choice_mixed""])\n        self.import_data_test(tuner_fn, ""choice_num"")\n\n    def test_networkmorphism(self):\n        pass\n\n    def test_ppo(self):\n        pass\n\n    def test_pbt(self):\n        self.search_space_test_all(lambda: PBTTuner(\n            all_checkpoint_dir=os.path.expanduser(""~/nni/checkpoint/test/""),\n            population_size=12\n        ))\n        self.search_space_test_all(lambda: PBTTuner(\n            all_checkpoint_dir=os.path.expanduser(""~/nni/checkpoint/test/""),\n            population_size=100\n        ))\n        self.import_data_test_for_pbt()\n\n    def tearDown(self):\n        file_list = glob.glob(""smac3*"") + [""param_config_space.pcs"", ""scenario.txt"", ""model_path""]\n        for file in file_list:\n            if os.path.exists(file):\n                if os.path.isdir(file):\n                    shutil.rmtree(file)\n                else:\n                    os.remove(file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_compressor.py,45,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom unittest import TestCase, main\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport torch.nn.functional as F\nimport schema\nimport nni.compression.torch as torch_compressor\nimport math\n\nif tf.__version__ >= \'2.0\':\n    import nni.compression.tensorflow as tf_compressor\n\n\ndef get_tf_model():\n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Conv2D(filters=5, kernel_size=7, input_shape=[28, 28, 1], activation=\'relu\', padding=""SAME""),\n        tf.keras.layers.MaxPooling2D(pool_size=2),\n        tf.keras.layers.Conv2D(filters=10, kernel_size=3, activation=\'relu\', padding=""SAME""),\n        tf.keras.layers.MaxPooling2D(pool_size=2),\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(units=128, activation=\'relu\'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(units=10, activation=\'softmax\'),\n    ])\n    model.compile(loss=""sparse_categorical_crossentropy"",\n                  optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n                  metrics=[""accuracy""])\n    return model\n\n\nclass TorchModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = torch.nn.Conv2d(1, 5, 5, 1)\n        self.bn1 = torch.nn.BatchNorm2d(5)\n        self.conv2 = torch.nn.Conv2d(5, 10, 5, 1)\n        self.bn2 = torch.nn.BatchNorm2d(10)\n        self.fc1 = torch.nn.Linear(4 * 4 * 10, 100)\n        self.fc2 = torch.nn.Linear(100, 10)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4 * 4 * 10)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n\n\ndef tf2(func):\n    def test_tf2_func(*args):\n        if tf.__version__ >= \'2.0\':\n            func(*args)\n\n    return test_tf2_func\n\n\n# for fpgm filter pruner test\nw = np.array([[[[i + 1] * 3] * 3] * 5 for i in range(10)])\n\n\nclass CompressorTestCase(TestCase):\n    def test_torch_quantizer_modules_detection(self):\n        # test if modules can be detected\n        model = TorchModel()\n        config_list = [{\n            \'quant_types\': [\'weight\'],\n            \'quant_bits\': 8,\n            \'op_types\': [\'Conv2d\', \'Linear\']\n        }, {\n            \'quant_types\': [\'output\'],\n            \'quant_bits\': 8,\n            \'quant_start_step\': 0,\n            \'op_types\': [\'ReLU\']\n        }]\n\n        model.relu = torch.nn.ReLU()\n        quantizer = torch_compressor.QAT_Quantizer(model, config_list)\n        quantizer.compress()\n        modules_to_compress = quantizer.get_modules_to_compress()\n        modules_to_compress_name = [t[0].name for t in modules_to_compress]\n        assert ""conv1"" in modules_to_compress_name\n        assert ""conv2"" in modules_to_compress_name\n        assert ""fc1"" in modules_to_compress_name\n        assert ""fc2"" in modules_to_compress_name\n        assert ""relu"" in modules_to_compress_name\n        assert len(modules_to_compress_name) == 5\n\n    def test_torch_level_pruner(self):\n        model = TorchModel()\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n        configure_list = [{\'sparsity\': 0.8, \'op_types\': [\'default\']}]\n        torch_compressor.LevelPruner(model, configure_list, optimizer).compress()\n\n    @tf2\n    def test_tf_level_pruner(self):\n        configure_list = [{\'sparsity\': 0.8, \'op_types\': [\'default\']}]\n        tf_compressor.LevelPruner(get_tf_model(), configure_list).compress()\n\n    def test_torch_naive_quantizer(self):\n        model = TorchModel()\n        configure_list = [{\n            \'quant_types\': [\'weight\'],\n            \'quant_bits\': {\n                \'weight\': 8,\n            },\n            \'op_types\': [\'Conv2d\', \'Linear\']\n        }]\n        torch_compressor.NaiveQuantizer(model, configure_list).compress()\n\n    @tf2\n    def test_tf_naive_quantizer(self):\n        tf_compressor.NaiveQuantizer(get_tf_model(), [{\'op_types\': [\'default\']}]).compress()\n\n    def test_torch_fpgm_pruner(self):\n        """"""\n        With filters(kernels) weights defined as above (w), it is obvious that w[4] and w[5] is the Geometric Median\n        which minimize the total geometric distance by defination of Geometric Median in this paper:\n        Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration,\n        https://arxiv.org/pdf/1811.00250.pdf\n\n        So if sparsity is 0.2, the expected masks should mask out w[4] and w[5], this can be verified through:\n        `all(torch.sum(masks, (1, 2, 3)).numpy() == np.array([45., 45., 45., 45., 0., 0., 45., 45., 45., 45.]))`\n\n        If sparsity is 0.6, the expected masks should mask out w[2] - w[7], this can be verified through:\n        `all(torch.sum(masks, (1, 2, 3)).numpy() == np.array([45., 45., 0., 0., 0., 0., 0., 0., 45., 45.]))`\n        """"""\n\n        model = TorchModel()\n        config_list = [{\'sparsity\': 0.6, \'op_types\': [\'Conv2d\']}, {\'sparsity\': 0.2, \'op_types\': [\'Conv2d\']}]\n        pruner = torch_compressor.FPGMPruner(model, config_list, torch.optim.SGD(model.parameters(), lr=0.01))\n\n        model.conv2.module.weight.data = torch.tensor(w).float()\n        masks = pruner.calc_mask(model.conv2)\n        assert all(torch.sum(masks[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([45., 45., 45., 45., 0., 0., 45., 45., 45., 45.]))\n\n        model.conv2.module.weight.data = torch.tensor(w).float()\n        model.conv2.if_calculated = False\n        model.conv2.config = config_list[0]\n        masks = pruner.calc_mask(model.conv2)\n        assert all(torch.sum(masks[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([45., 45., 0., 0., 0., 0., 0., 0., 45., 45.]))\n\n    @tf2\n    def test_tf_fpgm_pruner(self):\n        model = get_tf_model()\n        config_list = [{\'sparsity\': 0.2, \'op_types\': [\'Conv2D\']}]\n\n        pruner = tf_compressor.FPGMPruner(model, config_list)\n        weights = model.layers[2].weights\n        weights[0] = np.array(w).astype(np.float32).transpose([2, 3, 0, 1]).transpose([0, 1, 3, 2])\n        model.layers[2].set_weights([weights[0], weights[1].numpy()])\n\n        layer = tf_compressor.compressor.LayerInfo(model.layers[2])\n        masks = pruner.calc_mask(layer, config_list[0]).numpy()\n        masks = masks.reshape((-1, masks.shape[-1])).transpose([1, 0])\n\n        assert all(masks.sum((1)) == np.array([45., 45., 45., 45., 0., 0., 45., 45., 45., 45.]))\n        \n    def test_torch_l1filter_pruner(self):\n        """"""\n        Filters with the minimum sum of the weights\' L1 norm are pruned in this paper:\n        PRUNING FILTERS FOR EFFICIENT CONVNETS,\n        https://arxiv.org/abs/1608.08710\n\n        So if sparsity is 0.2, the expected masks should mask out filter 0, this can be verified through:\n        `all(torch.sum(mask1, (1, 2, 3)).numpy() == np.array([0., 27., 27., 27., 27.]))`\n\n        If sparsity is 0.6, the expected masks should mask out filter 0,1,2, this can be verified through:\n        `all(torch.sum(mask2, (1, 2, 3)).numpy() == np.array([0., 0., 0., 27., 27.]))`\n        """"""\n        w = np.array([np.zeros((3, 3, 3)), np.ones((3, 3, 3)), np.ones((3, 3, 3)) * 2,\n                      np.ones((3, 3, 3)) * 3, np.ones((3, 3, 3)) * 4])\n        model = TorchModel()\n        config_list = [{\'sparsity\': 0.2, \'op_types\': [\'Conv2d\'], \'op_names\': [\'conv1\']},\n                       {\'sparsity\': 0.6, \'op_types\': [\'Conv2d\'], \'op_names\': [\'conv2\']}]\n        pruner = torch_compressor.L1FilterPruner(model, config_list)\n\n        model.conv1.module.weight.data = torch.tensor(w).float()\n        model.conv2.module.weight.data = torch.tensor(w).float()\n        mask1 = pruner.calc_mask(model.conv1)\n        mask2 = pruner.calc_mask(model.conv2)\n        assert all(torch.sum(mask1[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([0., 27., 27., 27., 27.]))\n        assert all(torch.sum(mask2[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([0., 0., 0., 27., 27.]))\n\n    def test_torch_slim_pruner(self):\n        """"""\n        Scale factors with minimum l1 norm in the BN layers are pruned in this paper:\n        Learning Efficient Convolutional Networks through Network Slimming,\n        https://arxiv.org/pdf/1708.06519.pdf\n\n        So if sparsity is 0.2, the expected masks should mask out channel 0, this can be verified through:\n        `all(mask1.numpy() == np.array([0., 1., 1., 1., 1.]))`\n        `all(mask2.numpy() == np.array([0., 1., 1., 1., 1.]))`\n\n        If sparsity is 0.6, the expected masks should mask out channel 0,1,2, this can be verified through:\n        `all(mask1.numpy() == np.array([0., 0., 0., 1., 1.]))`\n        `all(mask2.numpy() == np.array([0., 0., 0., 1., 1.]))`\n        """"""\n        w = np.array([0, 1, 2, 3, 4])\n        model = TorchModel()\n        config_list = [{\'sparsity\': 0.2, \'op_types\': [\'BatchNorm2d\']}]\n        model.bn1.weight.data = torch.tensor(w).float()\n        model.bn2.weight.data = torch.tensor(-w).float()\n        pruner = torch_compressor.SlimPruner(model, config_list)\n\n        mask1 = pruner.calc_mask(model.bn1)\n        mask2 = pruner.calc_mask(model.bn2)\n        assert all(mask1[\'weight_mask\'].numpy() == np.array([0., 1., 1., 1., 1.]))\n        assert all(mask2[\'weight_mask\'].numpy() == np.array([0., 1., 1., 1., 1.]))\n        assert all(mask1[\'bias_mask\'].numpy() == np.array([0., 1., 1., 1., 1.]))\n        assert all(mask2[\'bias_mask\'].numpy() == np.array([0., 1., 1., 1., 1.]))\n\n        model = TorchModel()\n        config_list = [{\'sparsity\': 0.6, \'op_types\': [\'BatchNorm2d\']}]\n        model.bn1.weight.data = torch.tensor(w).float()\n        model.bn2.weight.data = torch.tensor(w).float()\n        pruner = torch_compressor.SlimPruner(model, config_list)\n\n        mask1 = pruner.calc_mask(model.bn1)\n        mask2 = pruner.calc_mask(model.bn2)\n        assert all(mask1[\'weight_mask\'].numpy() == np.array([0., 0., 0., 1., 1.]))\n        assert all(mask2[\'weight_mask\'].numpy() == np.array([0., 0., 0., 1., 1.]))\n        assert all(mask1[\'bias_mask\'].numpy() == np.array([0., 0., 0., 1., 1.]))\n        assert all(mask2[\'bias_mask\'].numpy() == np.array([0., 0., 0., 1., 1.]))\n\n    def test_torch_taylorFOweight_pruner(self):\n        """"""\n        Filters with the minimum importance approxiamtion based on the first order \n        taylor expansion on the weights (w*grad)**2 are pruned in this paper:\n        Importance Estimation for Neural Network Pruning,\n        http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf\n\n        So if sparsity of conv1 is 0.2, the expected masks should mask out filter 0, this can be verified through:\n        `all(torch.sum(mask1[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([0., 25., 25., 25., 25.]))`\n\n        If sparsity of conv2 is 0.6, the expected masks should mask out filter 4,5,6,7,8,9 this can be verified through:\n        `all(torch.sum(mask2[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([125., 125., 125., 125., 0., 0., 0., 0., 0., 0., ]))`\n        """"""\n\n        w1 = np.array([np.zeros((1, 5, 5)), np.ones((1, 5, 5)), np.ones((1, 5, 5)) * 2,\n                      np.ones((1, 5, 5)) * 3, np.ones((1, 5, 5)) * 4])\n        w2 = np.array([[[[i + 1] * 5] * 5] * 5 for i in range(10)[::-1]])\n\n        grad1 = np.array([np.ones((1, 5, 5)) * -1, np.ones((1, 5, 5)) * 1, np.ones((1, 5, 5)) * -1,\n                      np.ones((1, 5, 5)) * 1, np.ones((1, 5, 5)) * -1])\n\n        grad2 = np.array([[[[(-1)**i] * 5] * 5] * 5 for i in range(10)])\n\n        config_list = [{\'sparsity\': 0.2, \'op_types\': [\'Conv2d\'], \'op_names\': [\'conv1\']},\n                       {\'sparsity\': 0.6, \'op_types\': [\'Conv2d\'], \'op_names\': [\'conv2\']}]\n\n        model = TorchModel()\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n        pruner = torch_compressor.TaylorFOWeightFilterPruner(model, config_list, optimizer, statistics_batch_num=1)\n        \n        x = torch.rand((1, 1, 28, 28), requires_grad=True)\n        model.conv1.module.weight.data = torch.tensor(w1).float()\n        model.conv2.module.weight.data = torch.tensor(w2).float()\n\n        y = model(x)\n        y.backward(torch.ones_like(y))\n\n        model.conv1.module.weight.grad.data = torch.tensor(grad1).float()\n        model.conv2.module.weight.grad.data = torch.tensor(grad2).float()\n        optimizer.step()\n\n        mask1 = pruner.calc_mask(model.conv1)\n        mask2 = pruner.calc_mask(model.conv2)\n        assert all(torch.sum(mask1[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([0., 25., 25., 25., 25.]))\n        assert all(torch.sum(mask2[\'weight_mask\'], (1, 2, 3)).numpy() == np.array([125., 125., 125., 125., 0., 0., 0., 0., 0., 0., ]))\n\n    def test_torch_QAT_quantizer(self):\n        model = TorchModel()\n        config_list = [{\n            \'quant_types\': [\'weight\'],\n            \'quant_bits\': 8,\n            \'op_types\': [\'Conv2d\', \'Linear\']\n        }, {\n            \'quant_types\': [\'output\'],\n            \'quant_bits\': 8,\n            \'quant_start_step\': 0,\n            \'op_types\': [\'ReLU\']\n        }]\n        model.relu = torch.nn.ReLU()\n        quantizer = torch_compressor.QAT_Quantizer(model, config_list)\n        quantizer.compress()\n        # test quantize\n        # range not including 0\n        eps = 1e-7\n        weight = torch.tensor([[1, 2], [3, 5]]).float()\n        quantize_weight = quantizer.quantize_weight(weight, model.conv2)\n        assert math.isclose(model.conv2.module.scale, 5 / 255, abs_tol=eps)\n        assert model.conv2.module.zero_point == 0\n        # range including 0\n        weight = torch.tensor([[-1, 2], [3, 5]]).float()\n        quantize_weight = quantizer.quantize_weight(weight, model.conv2)\n        assert math.isclose(model.conv2.module.scale, 6 / 255, abs_tol=eps)\n        assert model.conv2.module.zero_point in (42, 43)\n\n        # test ema\n        x = torch.tensor([[-0.2, 0], [0.1, 0.2]])\n        out = model.relu(x)\n        assert math.isclose(model.relu.module.tracked_min_biased, 0, abs_tol=eps)\n        assert math.isclose(model.relu.module.tracked_max_biased, 0.002, abs_tol=eps)\n\n        quantizer.step_with_optimizer()\n        x = torch.tensor([[0.2, 0.4], [0.6, 0.8]])\n        out = model.relu(x)\n        assert math.isclose(model.relu.module.tracked_min_biased, 0.002, abs_tol=eps)\n        assert math.isclose(model.relu.module.tracked_max_biased, 0.00998, abs_tol=eps)\n\n    def test_torch_pruner_validation(self):\n        # test bad configuraiton\n        pruner_classes = [torch_compressor.__dict__[x] for x in \\\n            [\'LevelPruner\', \'SlimPruner\', \'FPGMPruner\', \'L1FilterPruner\', \'L2FilterPruner\', \'AGP_Pruner\', \\\n            \'ActivationMeanRankFilterPruner\', \'ActivationAPoZRankFilterPruner\']]\n\n        bad_configs = [\n            [\n                {\'sparsity\': \'0.2\'},\n                {\'sparsity\': 0.6 }\n            ],\n            [\n                {\'sparsity\': 0.2},\n                {\'sparsity\': 1.6 }\n            ],\n            [\n                {\'sparsity\': 0.2, \'op_types\': \'default\'},\n                {\'sparsity\': 0.6 }\n            ],\n            [\n                {\'sparsity\': 0.2 },\n                {\'sparsity\': 0.6, \'op_names\': \'abc\' }\n            ]\n        ]\n        model = TorchModel()\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        for pruner_class in pruner_classes:\n            for config_list in bad_configs:\n                try:\n                    pruner_class(model, config_list, optimizer)\n                    print(config_list)\n                    assert False, \'Validation error should be raised for bad configuration\'\n                except schema.SchemaError:\n                    pass\n                except:\n                    print(\'FAILED:\', pruner_class, config_list)\n                    raise\n\n    def test_torch_quantizer_validation(self):\n        # test bad configuraiton\n        quantizer_classes = [torch_compressor.__dict__[x] for x in \\\n            [\'NaiveQuantizer\', \'QAT_Quantizer\', \'DoReFaQuantizer\', \'BNNQuantizer\']]\n\n        bad_configs = [\n            [\n                {\'bad_key\': \'abc\'}\n            ],\n            [\n                {\'quant_types\': \'abc\'}\n            ],\n            [\n                {\'quant_bits\': 34}\n            ],\n            [\n                {\'op_types\': \'default\'}\n            ],\n            [\n                {\'quant_bits\': {\'abc\': 123}}\n            ]\n        ]\n        model = TorchModel()\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        for quantizer_class in quantizer_classes:\n            for config_list in bad_configs:\n                try:\n                    quantizer_class(model, config_list, optimizer)\n                    print(config_list)\n                    assert False, \'Validation error should be raised for bad configuration\'\n                except schema.SchemaError:\n                    pass\n                except:\n                    print(\'FAILED:\', quantizer_class, config_list)\n                    raise\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_curvefitting_assessor.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\nimport unittest\n\nfrom nni.curvefitting_assessor import CurvefittingAssessor\nfrom nni.curvefitting_assessor.model_factory import CurveModel\nfrom nni.assessor import AssessResult\n\nclass TestCurveFittingAssessor(unittest.TestCase):\n    def test_init(self):\n        new_assessor = CurvefittingAssessor(20)\n        self.assertEqual(new_assessor.start_step, 6)\n        self.assertEqual(new_assessor.target_pos, 20)\n\n    def test_insufficient_point(self):\n        new_assessor = CurvefittingAssessor(20)\n        ret = new_assessor.assess_trial(1, [1])\n        self.assertEqual(ret, AssessResult.Good)\n\n    def test_not_converged(self):\n        new_assessor = CurvefittingAssessor(20)\n        with self.assertRaises(TypeError):\n            ret = new_assessor.assess_trial([1, 199, 0, 199, 1, 209, 2])\n        ret = new_assessor.assess_trial(1, [1, 199, 0, 199, 1, 209, 2])\n        self.assertEqual(ret, AssessResult.Good)\n        models = CurveModel(21)\n        self.assertEqual(models.predict([1, 199, 0, 199, 1, 209, 2]), None)\n\n    def test_curve_model(self):\n        test_model = CurveModel(21)\n        test_model.effective_model = ['vap', 'pow3', 'linear', 'logx_linear', 'dr_hill_zero_background', 'log_power', 'pow4', 'mmf', 'exp4', 'ilog2', 'weibull', 'janoschek']\n        test_model.effective_model_num = 12\n        test_model.point_num = 9\n        test_model.target_pos = 20\n        test_model.trial_history = ([1, 1, 1, 1, 1, 1, 1, 1, 1])\n        test_model.weight_samples = np.ones((test_model.effective_model_num), dtype=np.float) / test_model.effective_model_num\n        self.assertAlmostEqual(test_model.predict_y('vap', 9), 0.5591906328335763)\n        self.assertAlmostEqual(test_model.predict_y('logx_linear', 15), 1.0704360293379522)\n        self.assertAlmostEqual(test_model.f_comb(9, test_model.weight_samples), 1.1543379521172443)\n        self.assertAlmostEqual(test_model.f_comb(15, test_model.weight_samples), 1.6949395581692737)\n\nif __name__ == '__main__':\n    unittest.main()\n"""
src/sdk/pynni/tests/test_evolution_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\ntest_evolution_tuner.py\n""""""\n\nimport numpy as np\n\nfrom unittest import TestCase, main\n\nfrom nni.utils import json2space, json2parameter\n\n\nclass EvolutionTunerTestCase(TestCase):\n    def test_json2space(self):\n        """"""test for json2space\n        """"""\n        json_search_space = {\n            ""optimizer"": {\n                ""_type"": ""choice"",\n                ""_value"": [""Adam"", ""SGD""]\n            },\n            ""learning_rate"": {\n                ""_type"": ""choice"",\n                ""_value"": [0.0001, 0.001, 0.002, 0.005, 0.01]\n            }\n        }\n        search_space_instance = json2space(json_search_space)\n        self.assertIn(\'root[optimizer]-choice\', search_space_instance)\n        self.assertIn(\'root[learning_rate]-choice\', search_space_instance)\n\n    def test_json2parameter(self):\n        """"""test for json2parameter\n        """"""\n        json_search_space = {\n            ""optimizer"":{\n                ""_type"":""choice"",""_value"":[""Adam"", ""SGD""]\n            },\n            ""learning_rate"":{\n                ""_type"":""choice"",\n                ""_value"":[0.0001, 0.001, 0.002, 0.005, 0.01]\n            }\n        }\n        space = json2space(json_search_space)\n        random_state = np.random.RandomState()\n        is_rand = dict()\n        for item in space:\n            is_rand[item] = True\n        search_space_instance = json2parameter(json_search_space, is_rand, random_state)\n        self.assertIn(search_space_instance[""optimizer""][""_index""], range(2))\n        self.assertIn(search_space_instance[""optimizer""][""_value""], [""Adam"", ""SGD""])\n        self.assertIn(search_space_instance[""learning_rate""][""_index""], range(5))\n        self.assertIn(search_space_instance[""learning_rate""][""_value""], [0.0001, 0.001, 0.002, 0.005, 0.01])\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_graph_utils.py,14,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport sys\nimport os\nimport math\nimport uuid\nimport shutil\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tensorboard.compat.proto.graph_pb2 import GraphDef\nfrom google.protobuf import text_format\nimport unittest\nfrom unittest import TestCase, main\n\nfrom nni._graph_utils import build_module_graph, build_graph\n\nclass BackboneModel1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 1, 1, 1)\n    def forward(self, x):\n        return self.conv1(x)\n\nclass BackboneModel2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(x.size(0), -1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass BigModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone1 = BackboneModel1()\n        self.backbone2 = BackboneModel2()\n        self.fc3 = nn.Linear(10, 2) \n    def forward(self, x):\n        x = self.backbone1(x)\n        x = self.backbone2(x)\n        x = self.fc3(x)\n        return x\n\nclass GraphUtilsTestCase(TestCase):\n    def test_build_module_graph(self):\n        big_model = BigModel()\n        g = build_module_graph(big_model, torch.randn(2, 1, 28, 28))\n        print(g.name_to_node.keys())\n        leaf_modules = set([\n            \'backbone1.conv1\', \'backbone2.bn1\', \'backbone2.bn2\', \'backbone2.conv1\',\n            \'backbone2.conv2\', \'backbone2.fc1\', \'backbone2.fc2\', \'fc3\'\n        ])\n\n        assert set(g.leaf_modules) == leaf_modules\n        assert not leaf_modules - set(g.name_to_node.keys())\n        assert g.find_successors(\'backbone2.conv1\') == [\'backbone2.bn1\']\n        assert g.find_successors(\'backbone2.conv2\') == [\'backbone2.bn2\']\n        assert g.find_predecessors(\'backbone2.bn1\') == [\'backbone2.conv1\']\n        assert g.find_predecessors(\'backbone2.bn2\') == [\'backbone2.conv2\']\n\n    def _test_graph(self, model, dummy_input, expected_file):\n        actual_proto, _ = build_graph(model, dummy_input)\n\n        assert os.path.exists(expected_file), expected_file\n        with open(expected_file, ""r"") as f:\n            expected_str = f.read()\n\n        expected_proto = GraphDef()\n        text_format.Parse(expected_str, expected_proto)\n\n        self.assertEqual(len(expected_proto.node), len(actual_proto.node))\n        for i in range(len(expected_proto.node)):\n            expected_node = expected_proto.node[i]\n            actual_node = actual_proto.node[i]\n            self.assertEqual(expected_node.name, actual_node.name)\n            self.assertEqual(expected_node.op, actual_node.op)\n            self.assertEqual(expected_node.input, actual_node.input)\n            self.assertEqual(expected_node.device, actual_node.device)\n            self.assertEqual(\n                sorted(expected_node.attr.keys()), sorted(actual_node.attr.keys()))\n\n    @unittest.skipIf(torch.__version__ < ""1.4.0"", ""not supported"")\n    def test_graph_module1(self):\n        dummy_input = (torch.zeros(1, 3),)\n\n        class myLinear(torch.nn.Module):\n            def __init__(self):\n                super(myLinear, self).__init__()\n                self.l = torch.nn.Linear(3, 5)\n\n            def forward(self, x):\n                return self.l(x)\n\n        self._test_graph(\n            myLinear(),\n            dummy_input,\n            os.path.join(os.path.dirname(__file__), ""expect"", ""test_graph_module1.expect"")\n        )\n\n    @unittest.skipIf(torch.__version__ < ""1.4.0"", ""not supported"")\n    def test_graph_module2(self):\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.weight = nn.Linear(5, 3)\n                self.bias = nn.Linear(5, 3)\n                self.module = nn.Linear(6, 1)\n\n            def forward(self, x):\n                tensors = [self.weight(x), self.bias(x)]\n                self.module(torch.cat(tensors, dim=1))\n                return x\n\n        self._test_graph(\n            MyModule(),\n            torch.randn(4, 5),\n            os.path.join(os.path.dirname(__file__), ""expect"", ""test_graph_module2.expect"")\n        )\n\n    @unittest.skipIf(torch.__version__ < ""1.4.0"", ""not supported"")\n    def test_graph_module3(self):\n        class MyModule(nn.Module):\n            def __init__(self):\n                super().__init__()\n                self.module = nn.ModuleList([\n                    nn.Linear(5, 3),\n                    nn.Linear(3, 1)\n                ])\n\n            def forward(self, x):\n                x = self.module[0](x)\n                x = self.module[1](x)\n                return x\n\n        self._test_graph(\n            MyModule(),\n            torch.randn(4, 5),\n            os.path.join(os.path.dirname(__file__), ""expect"", ""test_graph_module3.expect"")\n        )\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_hyperopt_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\ntest_hyperopt_tuner.py\n""""""\n\nfrom unittest import TestCase, main\n\nimport hyperopt as hp\n\nfrom nni.hyperopt_tuner.hyperopt_tuner import json2space, json2parameter, json2vals, HyperoptTuner\n\n\nclass HyperoptTunerTestCase(TestCase):\n    def test_json2space(self):\n        """"""test for json2space\n        """"""\n        json_search_space = {\n            ""optimizer"": {\n                ""_type"": ""choice"",\n                ""_value"": [""Adam"", ""SGD""]\n            },\n            ""learning_rate"": {\n                ""_type"": ""choice"",\n                ""_value"": [0.0001, 0.001, 0.002, 0.005, 0.01]\n            }\n        }\n        search_space_instance = json2space(json_search_space)\n        self.assertIsInstance(search_space_instance[""optimizer""],\n                              hp.pyll.base.Apply)\n        self.assertIsInstance(search_space_instance[""learning_rate""],\n                              hp.pyll.base.Apply)\n\n    def test_json2parameter(self):\n        """"""test for json2parameter\n        """"""\n        json_search_space = {\n            ""optimizer"": {\n                ""_type"": ""choice"",\n                ""_value"": [""Adam"", ""SGD""]\n            },\n            ""learning_rate"": {\n                ""_type"": ""choice"",\n                ""_value"": [0.0001, 0.001, 0.002, 0.005, 0.01]\n            }\n        }\n        parameter = {\n            \'root[learning_rate]-choice\': 2,\n            \'root[optimizer]-choice\': 0\n        }\n        search_space_instance = json2parameter(json_search_space, parameter)\n        self.assertEqual(search_space_instance[""optimizer""][""_index""], 0)\n        self.assertEqual(search_space_instance[""optimizer""][""_value""], ""Adam"")\n        self.assertEqual(search_space_instance[""learning_rate""][""_index""], 2)\n        self.assertEqual(search_space_instance[""learning_rate""][""_value""], 0.002)\n\n    def test_json2vals(self):\n        """"""test for json2vals\n        """"""\n        json_search_space = {\n            ""optimizer"": {\n                ""_type"": ""choice"",\n                ""_value"": [""Adam"", ""SGD""]\n            },\n            ""learning_rate"": {\n                ""_type"": ""choice"",\n                ""_value"": [0.0001, 0.001, 0.002, 0.005, 0.01]\n            }\n        }\n        out_y = dict()\n        vals = {\n            \'optimizer\': {\n                \'_index\': 0,\n                \'_value\': \'Adam\'\n            },\n            \'learning_rate\': {\n                \'_index\': 1,\n                \'_value\': 0.001\n            }\n        }\n        json2vals(json_search_space, vals, out_y)\n        self.assertEqual(out_y[""root[optimizer]-choice""], 0)\n        self.assertEqual(out_y[""root[learning_rate]-choice""], 1)\n\n    def test_tuner_generate(self):\n        for algorithm in [""tpe"", ""random_search"", ""anneal""]:\n            tuner = HyperoptTuner(algorithm)\n            choice_list = [""a"", ""b"", 1, 2]\n            tuner.update_search_space({\n                ""a"": {\n                    ""_type"": ""randint"",\n                    ""_value"": [1, 3]\n                },\n                ""b"": {\n                    ""_type"": ""choice"",\n                    ""_value"": choice_list\n                }\n            })\n            for k in range(30):\n                # sample multiple times\n                param = tuner.generate_parameters(k)\n                print(param)\n                self.assertIsInstance(param[""a""], int)\n                self.assertGreaterEqual(param[""a""], 1)\n                self.assertLessEqual(param[""a""], 2)\n                self.assertIn(param[""b""], choice_list)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_model_speedup.py,9,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision.models.vgg import vgg16\nfrom torchvision.models.resnet import resnet18\nfrom unittest import TestCase, main\n\nfrom nni.compression.torch import L1FilterPruner, apply_compression_results, ModelSpeedup\n\ntorch.manual_seed(0)\n\nclass BackboneModel1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 1, 1, 1)\n    def forward(self, x):\n        return self.conv1(x)\n\nclass BackboneModel2(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.bn1 = nn.BatchNorm2d(self.conv1.out_channels)\n        self.bn2 = nn.BatchNorm2d(self.conv2.out_channels)\n        self.fc1 = nn.Linear(4 * 4 * 50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(x.size(0), -1)\n        \n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\nclass BigModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.backbone1 = BackboneModel1()\n        self.backbone2 = BackboneModel2()\n        self.fc3 =  nn.Sequential(\n            nn.Linear(10, 10),\n            nn.BatchNorm1d(10),\n            nn.ReLU(inplace=True),\n            nn.Linear(10, 2)\n        )\n    def forward(self, x):\n        x = self.backbone1(x)\n        x = self.backbone2(x)\n        x = self.fc3(x)\n        return x\n\ndummy_input = torch.randn(2, 1, 28, 28)\nSPARSITY = 0.5\nMODEL_FILE, MASK_FILE = './11_model.pth', './l1_mask.pth'\n\ndef prune_model_l1(model):\n    config_list = [{\n        'sparsity': SPARSITY,\n        'op_types': ['Conv2d']\n    }]\n    pruner = L1FilterPruner(model, config_list)\n    pruner.compress()\n    pruner.export_model(model_path=MODEL_FILE, mask_path=MASK_FILE)\n\nclass SpeedupTestCase(TestCase):\n    def test_speedup_vgg16(self):\n        prune_model_l1(vgg16())\n        model = vgg16()\n        model.train()\n        ms = ModelSpeedup(model, torch.randn(2, 3, 32, 32), MASK_FILE)\n        ms.speedup_model()\n\n        orig_model = vgg16()\n        assert model.training\n        assert model.features[2].out_channels == int(orig_model.features[2].out_channels * SPARSITY)\n        assert model.classifier[0].in_features == int(orig_model.classifier[0].in_features * SPARSITY)\n\n    #def test_speedup_resnet(self):\n        #TODO support resnet\n        #model = resnet18()\n\n    def test_speedup_bigmodel(self):\n        prune_model_l1(BigModel())\n        model = BigModel()\n        apply_compression_results(model, MASK_FILE, 'cpu')\n        model.eval()\n        mask_out = model(dummy_input)\n\n        model.train()\n        ms = ModelSpeedup(model, dummy_input, MASK_FILE)\n        ms.speedup_model()\n        assert model.training\n\n        model.eval()\n        speedup_out = model(dummy_input)\n        if not torch.allclose(mask_out, speedup_out, atol=1e-07):\n            print('input:', dummy_input.size(), torch.abs(dummy_input).sum((2,3)))\n            print('mask_out:', mask_out)\n            print('speedup_out:', speedup_out)\n            raise RuntimeError('model speedup inference result is incorrect!')\n\n        orig_model = BigModel()\n\n        assert model.backbone2.conv1.out_channels == int(orig_model.backbone2.conv1.out_channels * SPARSITY)\n        assert model.backbone2.conv2.in_channels == int(orig_model.backbone2.conv2.in_channels * SPARSITY)\n        assert model.backbone2.conv2.out_channels == int(orig_model.backbone2.conv2.out_channels * SPARSITY)\n        assert model.backbone2.fc1.in_features == int(orig_model.backbone2.fc1.in_features * SPARSITY)\n\n    def tearDown(self):\n        os.remove(MODEL_FILE)\n        os.remove(MASK_FILE)\n\nif __name__ == '__main__':\n    main()\n"""
src/sdk/pynni/tests/test_msg_dispatcher.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nfrom io import BytesIO\nfrom unittest import TestCase, main\n\nimport nni.protocol\nfrom nni.msg_dispatcher import MsgDispatcher\nfrom nni.protocol import CommandType, send, receive\nfrom nni.tuner import Tuner\nfrom nni.utils import extract_scalar_reward\n\n\nclass NaiveTuner(Tuner):\n    def __init__(self):\n        self.param = 0\n        self.trial_results = []\n        self.search_space = None\n        self._accept_customized_trials()\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        # report Tuner\'s internal states to generated parameters,\n        # so we don\'t need to pause the main loop\n        self.param += 2\n        return {\n            \'param\': self.param,\n            \'trial_results\': self.trial_results,\n            \'search_space\': self.search_space\n        }\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        reward = extract_scalar_reward(value)\n        self.trial_results.append((parameter_id, parameters[\'param\'], reward, kwargs.get(""customized"")))\n\n    def update_search_space(self, search_space):\n        self.search_space = search_space\n\n\n_in_buf = BytesIO()\n_out_buf = BytesIO()\n\n\ndef _reverse_io():\n    _in_buf.seek(0)\n    _out_buf.seek(0)\n    nni.protocol._out_file = _in_buf\n    nni.protocol._in_file = _out_buf\n\n\ndef _restore_io():\n    _in_buf.seek(0)\n    _out_buf.seek(0)\n    nni.protocol._in_file = _in_buf\n    nni.protocol._out_file = _out_buf\n\n\nclass MsgDispatcherTestCase(TestCase):\n    def test_msg_dispatcher(self):\n        _reverse_io()  # now we are sending to Tuner\'s incoming stream\n        send(CommandType.RequestTrialJobs, \'2\')\n        send(CommandType.ReportMetricData, \'{""parameter_id"":0,""type"":""PERIODICAL"",""value"":""10""}\')\n        send(CommandType.ReportMetricData, \'{""parameter_id"":1,""type"":""FINAL"",""value"":""11""}\')\n        send(CommandType.UpdateSearchSpace, \'{""name"":""SS0""}\')\n        send(CommandType.RequestTrialJobs, \'1\')\n        send(CommandType.KillTrialJob, \'null\')\n        _restore_io()\n\n        tuner = NaiveTuner()\n        dispatcher = MsgDispatcher(tuner)\n        nni.msg_dispatcher_base._worker_fast_exit_on_terminate = False\n\n        dispatcher.run()\n        e = dispatcher.worker_exceptions[0]\n        self.assertIs(type(e), AssertionError)\n        self.assertEqual(e.args[0], \'Unsupported command: CommandType.KillTrialJob\')\n\n        _reverse_io()  # now we are receiving from Tuner\'s outgoing stream\n        self._assert_params(0, 2, [], None)\n        self._assert_params(1, 4, [], None)\n\n        self._assert_params(2, 6, [[1, 4, 11, False]], {\'name\': \'SS0\'})\n\n        self.assertEqual(len(_out_buf.read()), 0)  # no more commands\n\n    def _assert_params(self, parameter_id, param, trial_results, search_space):\n        command, data = receive()\n        self.assertIs(command, CommandType.NewTrialJob)\n        data = json.loads(data)\n        self.assertEqual(data[\'parameter_id\'], parameter_id)\n        self.assertEqual(data[\'parameter_source\'], \'algorithm\')\n        self.assertEqual(data[\'parameters\'][\'param\'], param)\n        self.assertEqual(data[\'parameters\'][\'trial_results\'], trial_results)\n        self.assertEqual(data[\'parameters\'][\'search_space\'], search_space)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_nas.py,14,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\nimport importlib\nimport os\nimport sys\nfrom collections import OrderedDict\nfrom unittest import TestCase, main\n\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch.classic_nas import get_and_apply_next_architecture\nfrom nni.nas.pytorch.darts import DartsMutator\nfrom nni.nas.pytorch.enas import EnasMutator\nfrom nni.nas.pytorch.fixed import apply_fixed_architecture\nfrom nni.nas.pytorch.mutables import LayerChoice\nfrom nni.nas.pytorch.random import RandomMutator\nfrom nni.nas.pytorch.utils import _reset_global_mutable_counting\n\n\nclass NasTestCase(TestCase):\n\n    def setUp(self):\n        self.default_input_size = [3, 32, 32]\n        self.model_path = os.path.join(os.path.dirname(__file__), ""models"")\n        sys.path.append(self.model_path)\n        self.model_module = importlib.import_module(""pytorch_models"")\n        self.default_cls = [self.model_module.NaiveSearchSpace, self.model_module.SpaceWithMutableScope]\n        self.cuda_test = [0]\n        if torch.cuda.is_available():\n            self.cuda_test.append(1)\n        if torch.cuda.device_count() > 1:\n            self.cuda_test.append(torch.cuda.device_count())\n\n    def tearDown(self):\n        sys.path.remove(self.model_path)\n\n    def iterative_sample_and_forward(self, model, mutator=None, input_size=None, n_iters=20, test_backward=True,\n                                     use_cuda=False):\n        if input_size is None:\n            input_size = self.default_input_size\n        # support pytorch only\n        input_size = [8 if use_cuda else 2] + input_size  # at least 2 samples to enable batch norm\n        for _ in range(n_iters):\n            for param in model.parameters():\n                param.grad = None\n            if mutator is not None:\n                mutator.reset()\n            x = torch.randn(input_size)\n            if use_cuda:\n                x = x.cuda()\n            y = torch.sum(model(x))\n            if test_backward:\n                y.backward()\n\n    def default_mutator_test_pipeline(self, mutator_cls):\n        for model_cls in self.default_cls:\n            for cuda_test in self.cuda_test:\n                _reset_global_mutable_counting()\n                model = model_cls(self)\n                mutator = mutator_cls(model)\n                if cuda_test:\n                    model.cuda()\n                    mutator.cuda()\n                    if cuda_test > 1:\n                        model = nn.DataParallel(model)\n                self.iterative_sample_and_forward(model, mutator, use_cuda=cuda_test)\n                _reset_global_mutable_counting()\n                model_fixed = model_cls(self)\n                if cuda_test:\n                    model_fixed.cuda()\n                    if cuda_test > 1:\n                        model_fixed = nn.DataParallel(model_fixed)\n                with torch.no_grad():\n                    arc = mutator.export()\n                apply_fixed_architecture(model_fixed, arc)\n                self.iterative_sample_and_forward(model_fixed, n_iters=1, use_cuda=cuda_test)\n\n    def test_random_mutator(self):\n        self.default_mutator_test_pipeline(RandomMutator)\n\n    def test_enas_mutator(self):\n        self.default_mutator_test_pipeline(EnasMutator)\n\n    def test_darts_mutator(self):\n        # DARTS doesn\'t support DataParallel. To be fixed.\n        self.cuda_test = [t for t in self.cuda_test if t <= 1]\n        self.default_mutator_test_pipeline(DartsMutator)\n\n    def test_apply_twice(self):\n        model = self.model_module.NaiveSearchSpace(self)\n        with self.assertRaises(RuntimeError):\n            for _ in range(2):\n                RandomMutator(model)\n\n    def test_nested_space(self):\n        model = self.model_module.NestedSpace(self)\n        with self.assertRaises(RuntimeError):\n            RandomMutator(model)\n\n    def test_classic_nas(self):\n        for model_cls in self.default_cls:\n            model = model_cls(self)\n            get_and_apply_next_architecture(model)\n            self.iterative_sample_and_forward(model)\n\n    def test_proxylessnas(self):\n        model = self.model_module.LayerChoiceOnlySearchSpace(self)\n        get_and_apply_next_architecture(model)\n        self.iterative_sample_and_forward(model)\n\n    def test_layer_choice(self):\n        for i in range(2):\n            for j in range(2):\n                if j == 0:\n                    # test number\n                    layer_choice = LayerChoice([nn.Conv2d(3, 3, 3), nn.Conv2d(3, 5, 3), nn.Conv2d(3, 6, 3)])\n                else:\n                    # test ordered dict\n                    layer_choice = LayerChoice(OrderedDict([\n                        (""conv1"", nn.Conv2d(3, 3, 3)),\n                        (""conv2"", nn.Conv2d(3, 5, 3)),\n                        (""conv3"", nn.Conv2d(3, 6, 3))\n                    ]))\n                if i == 0:\n                    # test modify\n                    self.assertEqual(len(layer_choice.choices), 3)\n                    layer_choice[1] = nn.Conv2d(3, 4, 3)\n                    self.assertEqual(layer_choice[1].out_channels, 4)\n                    self.assertEqual(len(layer_choice[0:2]), 2)\n                    if j > 0:\n                        layer_choice[""conv3""] = nn.Conv2d(3, 7, 3)\n                        self.assertEqual(layer_choice[-1].out_channels, 7)\n                if i == 1:\n                    # test delete\n                    del layer_choice[1]\n                    self.assertEqual(len(layer_choice), 2)\n                    self.assertEqual(len(list(layer_choice)), 2)\n                    self.assertEqual(layer_choice.names, [""conv1"", ""conv3""] if j > 0 else [""0"", ""2""])\n                    if j > 0:\n                        del layer_choice[""conv1""]\n                        self.assertEqual(len(layer_choice), 1)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_networkmorphism_tuner.py,6,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nfrom unittest import TestCase, main\nfrom copy import deepcopy\nimport torch\n\nfrom nni.networkmorphism_tuner.graph import graph_to_json, json_to_graph\nfrom nni.networkmorphism_tuner.graph_transformer import (\n    to_deeper_graph,\n    to_skip_connection_graph,\n    to_wider_graph,\n)\nfrom nni.networkmorphism_tuner.layers import layer_description_extractor\nfrom nni.networkmorphism_tuner.networkmorphism_tuner import NetworkMorphismTuner\nfrom nni.networkmorphism_tuner.nn import CnnGenerator\n\n\nclass NetworkMorphismTestCase(TestCase):\n    """"""  unittest for NetworkMorphismTuner\n    """"""\n\n    def test_graph_json_transform(self):\n        """""" unittest for graph_json_transform function\n        """"""\n\n        graph_init = CnnGenerator(10, (32, 32, 3)).generate()\n        graph_init = to_wider_graph(deepcopy(graph_init))\n        graph_init = to_deeper_graph(deepcopy(graph_init))\n        graph_init = to_skip_connection_graph(deepcopy(graph_init))\n        json_out = graph_to_json(graph_init, ""temp.json"")\n\n        graph_recover = json_to_graph(json_out)\n\n        # compare all data in graph\n        self.assertEqual(graph_init.input_shape, graph_recover.input_shape)\n        self.assertEqual(graph_init.weighted, graph_recover.weighted)\n        self.assertEqual(\n            graph_init.layer_id_to_input_node_ids,\n            graph_recover.layer_id_to_input_node_ids,\n        )\n        self.assertEqual(graph_init.adj_list, graph_recover.adj_list)\n        self.assertEqual(\n            graph_init.reverse_adj_list,\n            graph_recover.reverse_adj_list)\n        self.assertEqual(\n            len(graph_init.operation_history), len(\n                graph_recover.operation_history)\n        )\n        self.assertEqual(graph_init.n_dim, graph_recover.n_dim)\n        self.assertEqual(graph_init.conv, graph_recover.conv)\n        self.assertEqual(graph_init.batch_norm, graph_recover.batch_norm)\n        self.assertEqual(graph_init.vis, graph_recover.vis)\n\n        node_list_init = [node.shape for node in graph_init.node_list]\n        node_list_recover = [node.shape for node in graph_recover.node_list]\n        self.assertEqual(node_list_init, node_list_recover)\n        self.assertEqual(len(graph_init.node_to_id),\n                         len(graph_recover.node_to_id))\n        layer_list_init = [\n            layer_description_extractor(item, graph_init.node_to_id)\n            for item in graph_init.layer_list\n        ]\n        layer_list_recover = [\n            layer_description_extractor(item, graph_recover.node_to_id)\n            for item in graph_recover.layer_list\n        ]\n        self.assertEqual(layer_list_init, layer_list_recover)\n\n        node_to_id_init = [graph_init.node_to_id[node]\n                           for node in graph_init.node_list]\n        node_to_id_recover = [\n            graph_recover.node_to_id[node] for node in graph_recover.node_list\n        ]\n        self.assertEqual(node_to_id_init, node_to_id_recover)\n\n        layer_to_id_init = [\n            graph_init.layer_to_id[layer] for layer in graph_init.layer_list\n        ]\n        layer_to_id_recover = [\n            graph_recover.layer_to_id[layer] for layer in graph_recover.layer_list\n        ]\n        self.assertEqual(layer_to_id_init, layer_to_id_recover)\n\n    def test_to_wider_graph(self):\n        """""" unittest for to_wider_graph function\n        """"""\n\n        graph_init = CnnGenerator(10, (32, 32, 3)).generate()\n        json_out = graph_to_json(graph_init, ""temp.json"")\n        graph_recover = json_to_graph(json_out)\n        wider_graph = to_wider_graph(deepcopy(graph_recover))\n        model = wider_graph.produce_torch_model()\n        out = model(torch.ones(1, 3, 32, 32))\n        self.assertEqual(out.shape, torch.Size([1, 10]))\n\n    def test_to_deeper_graph(self):\n        """""" unittest for to_deeper_graph function\n        """"""\n\n        graph_init = CnnGenerator(10, (32, 32, 3)).generate()\n        json_out = graph_to_json(graph_init, ""temp.json"")\n        graph_recover = json_to_graph(json_out)\n        deeper_graph = to_deeper_graph(deepcopy(graph_recover))\n        model = deeper_graph.produce_torch_model()\n        out = model(torch.ones(1, 3, 32, 32))\n        self.assertEqual(out.shape, torch.Size([1, 10]))\n\n    def test_to_skip_connection_graph(self):\n        """""" unittest for to_skip_connection_graph function\n        """"""\n\n        graph_init = CnnGenerator(10, (32, 32, 3)).generate()\n        json_out = graph_to_json(graph_init, ""temp.json"")\n        graph_recover = json_to_graph(json_out)\n        skip_connection_graph = to_skip_connection_graph(deepcopy(graph_recover))\n        model = skip_connection_graph.produce_torch_model()\n        out = model(torch.ones(1, 3, 32, 32))\n        self.assertEqual(out.shape, torch.Size([1, 10]))\n\n    def test_generate_parameters(self):\n        """""" unittest for generate_parameters function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        model_json = tuner.generate_parameters(0)\n        model_json = json.loads(model_json)\n        self.assertEqual(model_json[""input_shape""], [32, 32, 3])\n        self.assertEqual(tuner.total_data[0][1:], (-1, 0))\n\n    def test_receive_trial_result(self):\n        """""" unittest for receive_trial_result function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        model_json = tuner.generate_parameters(0)\n        tuner.receive_trial_result(0, {}, 0.7)\n        (json_out, father_id, model_id) = tuner.total_data[0]\n\n        self.assertEqual(father_id, -1)\n        self.assertEqual(model_json, json_out)\n\n        ret = {""model_id"": 0, ""metric_value"": 0.7}\n        self.assertEqual(tuner.bo.search_tree.adj_list[model_id], [])\n        self.assertEqual(tuner.history[-1], ret)\n\n    def test_update_search_space(self):\n        """""" unittest for update_search_space function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        self.assertEqual(tuner.search_space, dict())\n        tuner.update_search_space(""Test"")\n        self.assertEqual(tuner.search_space, ""Test"")\n\n    def test_init_search(self):\n        """""" unittest for init_search function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        self.assertEqual(tuner.history, [])\n        tuner.init_search()\n        self.assertEqual(tuner.model_count, 1)\n        self.assertEqual(len(tuner.training_queue), 1)\n        self.assertEqual(len(tuner.descriptors), 1)\n\n    def test_add_model(self):\n        """""" unittest for add_model function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        tuner.add_model(0.8, 0)\n        ret = {""model_id"": 0, ""metric_value"": 0.8}\n        self.assertEqual(tuner.history[-1], ret)\n\n    def test_get_best_model_id(self):\n        """""" unittest for get_best_model_id function\n        """"""\n\n        tuner = NetworkMorphismTuner()\n        tuner.add_model(0.8, 0)\n        tuner.add_model(0.9, 1)\n        self.assertEqual(tuner.get_best_model_id(), 1)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/sdk/pynni/tests/test_protocol.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport nni.protocol\nfrom nni.protocol import CommandType, send, receive\n\nfrom io import BytesIO\nfrom unittest import TestCase, main\n\n\ndef _prepare_send():\n    nni.protocol._out_file = BytesIO()\n    return nni.protocol._out_file\n\ndef _prepare_receive(data):\n    nni.protocol._in_file = BytesIO(data)\n\n\nclass ProtocolTestCase(TestCase):\n    def test_send_en(self):\n        out_file = _prepare_send()\n        send(CommandType.NewTrialJob, 'CONTENT')\n        self.assertEqual(out_file.getvalue(), b'TR00000000000007CONTENT')\n\n    def test_send_zh(self):\n        out_file = _prepare_send()\n        send(CommandType.NewTrialJob, '\xe4\xbd\xa0\xe5\xa5\xbd')\n        self.assertEqual(out_file.getvalue(), 'TR00000000000006\xe4\xbd\xa0\xe5\xa5\xbd'.encode('utf8'))\n\n    def test_receive_en(self):\n        _prepare_receive(b'IN00000000000005hello')\n        command, data = receive()\n        self.assertIs(command, CommandType.Initialize)\n        self.assertEqual(data, 'hello')\n\n    def test_receive_zh(self):\n        _prepare_receive('IN00000000000006\xe4\xb8\x96\xe7\x95\x8c'.encode('utf8'))\n        command, data = receive()\n        self.assertIs(command, CommandType.Initialize)\n        self.assertEqual(data, '\xe4\xb8\x96\xe7\x95\x8c')\n\n\nif __name__ == '__main__':\n    main()\n"""
src/sdk/pynni/tests/test_pruners.py,7,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport math\nfrom unittest import TestCase, main\nfrom nni.compression.torch import LevelPruner, SlimPruner, FPGMPruner, L1FilterPruner, \\\n    L2FilterPruner, AGP_Pruner, ActivationMeanRankFilterPruner, ActivationAPoZRankFilterPruner\n\ndef validate_sparsity(wrapper, sparsity, bias=False):\n    masks = [wrapper.weight_mask]\n    if bias and wrapper.bias_mask is not None:\n        masks.append(wrapper.bias_mask)\n    for m in masks:\n        actual_sparsity = (m == 0).sum().item() / m.numel()\n        msg = 'actual sparsity: {:.2f}, target sparsity: {:.2f}'.format(actual_sparsity, sparsity)\n        assert math.isclose(actual_sparsity, sparsity, abs_tol=0.1), msg\n\nprune_config = {\n    'level': {\n        'pruner_class': LevelPruner,\n        'config_list': [{\n            'sparsity': 0.5,\n            'op_types': ['default'],\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, False),\n            lambda model: validate_sparsity(model.fc, 0.5, False)\n        ]\n    },\n    'agp': {\n        'pruner_class': AGP_Pruner,\n        'config_list': [{\n            'initial_sparsity': 0.,\n            'final_sparsity': 0.8,\n            'start_epoch': 0,\n            'end_epoch': 10,\n            'frequency': 1,\n            'op_types': ['default']\n        }],\n        'validators': []\n    },\n    'slim': {\n        'pruner_class': SlimPruner,\n        'config_list': [{\n            'sparsity': 0.7,\n            'op_types': ['BatchNorm2d']\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.bn1, 0.7, model.bias)\n        ]\n    },\n    'fpgm': {\n        'pruner_class': FPGMPruner,\n        'config_list':[{\n            'sparsity': 0.5,\n            'op_types': ['Conv2d']\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, model.bias)\n        ]\n    },\n    'l1': {\n        'pruner_class': L1FilterPruner,\n        'config_list': [{\n            'sparsity': 0.5,\n            'op_types': ['Conv2d'],\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, model.bias)\n        ]\n    },\n    'l2': {\n        'pruner_class': L2FilterPruner,\n        'config_list': [{\n            'sparsity': 0.5,\n            'op_types': ['Conv2d'],\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, model.bias)\n        ]\n    },\n    'mean_activation': {\n        'pruner_class': ActivationMeanRankFilterPruner,\n        'config_list': [{\n            'sparsity': 0.5,\n            'op_types': ['Conv2d'],\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, model.bias)\n        ]\n    },\n    'apoz': {\n        'pruner_class': ActivationAPoZRankFilterPruner,\n        'config_list': [{\n            'sparsity': 0.5,\n            'op_types': ['Conv2d'],\n        }],\n        'validators': [\n            lambda model: validate_sparsity(model.conv1, 0.5, model.bias)\n        ]\n    }\n}\n\nclass Model(nn.Module):\n    def __init__(self, bias=True):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 8, kernel_size=3, padding=1, bias=bias)\n        self.bn1 = nn.BatchNorm2d(8)\n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(8, 2, bias=bias)\n        self.bias = bias\n    def forward(self, x):\n        return self.fc(self.pool(self.bn1(self.conv1(x))).view(x.size(0), -1))\n\ndef pruners_test(pruner_names=['level', 'agp', 'slim', 'fpgm', 'l1', 'l2', 'mean_activation', 'apoz'], bias=True):\n    for pruner_name in pruner_names:\n        print('testing {}...'.format(pruner_name))\n        model = Model(bias=bias)\n        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n        config_list = prune_config[pruner_name]['config_list']\n\n        x = torch.randn(2, 1, 28, 28)\n        y = torch.tensor([0, 1]).long()\n        out = model(x)\n        loss = F.cross_entropy(out, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pruner = prune_config[pruner_name]['pruner_class'](model, config_list, optimizer)\n        pruner.compress()\n\n        x = torch.randn(2, 1, 28, 28)\n        y = torch.tensor([0, 1]).long()\n        out = model(x)\n        loss = F.cross_entropy(out, y)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        pruner.export_model('./model_tmp.pth', './mask_tmp.pth', './onnx_tmp.pth', input_shape=(2,1,28,28))\n\n        for v in prune_config[pruner_name]['validators']:\n            v(model)\n\n    os.remove('./model_tmp.pth')\n    os.remove('./mask_tmp.pth')\n    os.remove('./onnx_tmp.pth')\n\nclass PrunerTestCase(TestCase):\n    def test_pruners(self):\n        pruners_test(bias=True)\n\n    def test_pruners_no_bias(self):\n        pruners_test(bias=False)\n\nif __name__ == '__main__':\n    main()\n"""
src/sdk/pynni/tests/test_smartparam.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\n\nos.environ[\'NNI_PLATFORM\'] = \'unittest\'\n\nimport nni\nimport nni.platform.test as test_platform\nimport nni.trial\n\nfrom unittest import TestCase, main\n\n\n\nclass SmartParamTestCase(TestCase):\n    def setUp(self):\n        params = {\n            \'test_smartparam/choice1/choice\': \'a\',\n            \'test_smartparam/choice2/choice\': \'3*2+1\',\n            \'test_smartparam/choice3/choice\': \'[1, 2]\',\n            \'test_smartparam/choice4/choice\': \'{""a"", 2}\',\n            \'test_smartparam/func/function_choice\': \'bar\',\n            \'test_smartparam/lambda_func/function_choice\': ""lambda: 2*3"",\n            \'mutable_block_66\':{\n                \'mutable_layer_0\':{\n                    \'chosen_layer\': \'conv2D(size=5)\',\n                    \'chosen_inputs\': [\'y\']\n                }\n            }\n        }\n        nni.trial._params = { \'parameter_id\': \'test_trial\', \'parameters\': params }\n\n\n    def test_specified_name(self):\n        val = nni.choice({\'a\': \'a\', \'3*2+1\': 3*2+1, \'[1, 2]\': [1, 2], \'{""a"", 2}\': {""a"", 2}}, name = \'choice1\', key=\'test_smartparam/choice1/choice\')\n        self.assertEqual(val, \'a\')\n        val = nni.choice({\'a\': \'a\', \'3*2+1\': 3*2+1, \'[1, 2]\': [1, 2], \'{""a"", 2}\': {""a"", 2}}, name = \'choice2\', key=\'test_smartparam/choice2/choice\')\n        self.assertEqual(val, 7)\n        val = nni.choice({\'a\': \'a\', \'3*2+1\': 3*2+1, \'[1, 2]\': [1, 2], \'{""a"", 2}\': {""a"", 2}}, name = \'choice3\', key=\'test_smartparam/choice3/choice\')\n        self.assertEqual(val, [1, 2])\n        val = nni.choice({\'a\': \'a\', \'3*2+1\': 3*2+1, \'[1, 2]\': [1, 2], \'{""a"", 2}\': {""a"", 2}}, name = \'choice4\', key=\'test_smartparam/choice4/choice\')\n        self.assertEqual(val, {""a"", 2})\n\n    def test_func(self):\n        val = nni.function_choice({\'foo\': foo, \'bar\': bar}, name=\'func\', key=\'test_smartparam/func/function_choice\')\n        self.assertEqual(val, \'bar\')\n\n    def test_lambda_func(self):\n        val = nni.function_choice({""lambda: 2*3"": lambda: 2*3, ""lambda: 3*4"": lambda: 3*4}, name = \'lambda_func\', key=\'test_smartparam/lambda_func/function_choice\')\n        self.assertEqual(val, 6)\n\n    def test_mutable_layer(self):\n        layer_out = nni.mutable_layer(\'mutable_block_66\',\n                \'mutable_layer_0\', {\'conv2D(size=3)\': conv2D, \'conv2D(size=5)\': conv2D}, {\'conv2D(size=3)\':\n                {\'size\':3}, \'conv2D(size=5)\': {\'size\':5}}, [100], {\'x\':1,\'y\':2}, 1, \'classic_mode\')\n        self.assertEqual(layer_out, [100, 2, 5])\n        \n\n\ndef foo():\n    return \'foo\'\n\ndef bar():\n    return \'bar\'\n\ndef conv2D(inputs, size=3):\n    return inputs[0] + inputs[1] + [size]\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/tests/test_trial.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport nni\nimport nni.platform.test as test_platform\nimport nni.trial\n\nimport numpy as np\nfrom unittest import TestCase, main\n\n\nclass TrialTestCase(TestCase):\n    def setUp(self):\n        self._trial_params = { 'msg': 'hi', 'x': 123, 'dict': { 'key': 'value', 'y': None } }\n        test_platform._params = { 'parameter_id': 'test_param', 'parameters': self._trial_params }\n\n    def test_get_next_parameter(self):\n        self.assertEqual(nni.get_next_parameter(), self._trial_params)\n\n    def test_get_current_parameter(self):\n        nni.get_next_parameter()\n        self.assertEqual(nni.get_current_parameter('x'), 123)\n\n    def test_get_experiment_id(self):\n        self.assertEqual(nni.get_experiment_id(), 'fakeidex')\n\n    def test_get_trial_id(self):\n        self.assertEqual(nni.get_trial_id(), 'fakeidtr')\n    \n    def test_get_sequence_id(self):\n        self.assertEqual(nni.get_sequence_id(), 0)\n\n    def test_report_intermediate_result(self):\n        nni.report_intermediate_result(123)\n        self.assertEqual(test_platform.get_last_metric(), {\n            'parameter_id': 'test_param',\n            'trial_job_id': 'test_trial_job_id',\n            'type': 'PERIODICAL',\n            'sequence': 0,\n            'value': 123\n        })\n\n    def test_report_final_result_simple(self):\n        self._test_report_final_result(123, 123)\n\n    def test_report_final_result_object(self):\n        obj = ['obj1', {'key1': 'v1', 'k2': None}, 233, 0.456]\n        self._test_report_final_result(obj, obj)\n\n    def test_report_final_result_numpy(self):\n        self._test_report_final_result(np.float32(0.25), 0.25)\n\n    def test_report_final_result_nparray(self):\n        arr = np.array([[1, 2, 3], [4, 5, 6]])\n        nni.report_final_result(arr)\n        out = test_platform.get_last_metric()\n        self.assertEqual(len(arr), 2)\n        self.assertEqual(len(arr[0]), 3)\n        self.assertEqual(len(arr[1]), 3)\n        self.assertEqual(arr[0][0], 1)\n        self.assertEqual(arr[0][1], 2)\n        self.assertEqual(arr[0][2], 3)\n        self.assertEqual(arr[1][0], 4)\n        self.assertEqual(arr[1][1], 5)\n        self.assertEqual(arr[1][2], 6)\n\n    def _test_report_final_result(self, in_, out):\n        nni.report_final_result(in_)\n        self.assertEqual(test_platform.get_last_metric(), {\n            'parameter_id': 'test_param',\n            'trial_job_id': 'test_trial_job_id',\n            'type': 'FINAL',\n            'sequence': 0,\n            'value': out\n        })\n\n\nif __name__ == '__main__':\n    main()\n"""
src/sdk/pynni/tests/test_utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom unittest import TestCase, main\n\nimport nni\nfrom nni.utils import split_index\n\n\nclass UtilsTestCase(TestCase):\n    def test_split_index_normal(self):\n        """"""test for normal search space\n        """"""\n        normal__params_with_index = {\n            ""dropout_rate"": {\n                ""_index"" : 1,\n                ""_value"" : 0.9\n            },\n            ""hidden_size"": {\n                ""_index"" : 1,\n                ""_value"" : 512\n            }\n        }\n        normal__params= {\n            ""dropout_rate"": 0.9,\n            ""hidden_size"": 512\n        }\n\n        params = split_index(normal__params_with_index)\n        self.assertEqual(params, normal__params)\n\n    def test_split_index_nested(self):\n        """"""test for nested search space\n        """"""\n        nested_params_with_index = {\n            ""layer0"": {\n                ""_name"": ""Avg_pool"",\n                ""pooling_size"":{\n                    ""_index"" : 1,\n                    ""_value"" : 2\n                } \n            },\n            ""layer1"": {\n                ""_name"": ""Empty""\n            },\n            ""layer2"": {\n                ""_name"": ""Max_pool"",\n                ""pooling_size"": {\n                    ""_index"" : 2,\n                    ""_value"" : 3\n                } \n            },\n            ""layer3"": {\n                ""_name"": ""Conv"",\n                ""kernel_size"":  {\n                    ""_index"" : 3,\n                    ""_value"" : 5\n                },\n                ""output_filters"":  {\n                    ""_index"" : 3,\n                    ""_value"" : 64\n                }\n            }\n        }\n        nested_params =  {\n            ""layer0"": {\n                ""_name"": ""Avg_pool"",\n                ""pooling_size"": 2\n            },\n            ""layer1"": {\n                ""_name"": ""Empty""\n            },\n            ""layer2"": {\n                ""_name"": ""Max_pool"",\n                ""pooling_size"": 3\n            },\n            ""layer3"": {\n                ""_name"": ""Conv"",\n                ""kernel_size"": 5,\n                ""output_filters"": 64\n            }\n        }\n        params = split_index(nested_params_with_index)\n        self.assertEqual(params, nested_params)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/nni_annotation/testcase/annotated/handwrite.py,0,"b""import nni\ndef max_pool(k):\n    pass\n\nh_conv1 = 1\nnni.choice({'foo': foo, 'bar': bar})(1)\nconv_size = nni.choice({2: 2, 3: 3, 5: 5, 7: 7}, name='conv_size')\nabc = nni.choice({'2': '2', 3: 3, '(5 * 6)': 5 * 6, 7: 7}, name='abc')\nh_pool1 = nni.function_choice({'max_pool': lambda : max_pool(h_conv1),\n    'h_conv1': lambda : h_conv1,\n    'avg_pool': lambda : avg_pool(h_conv2, h_conv3)}\n)\nh_pool1 = nni.function_choice({'max_pool(h_conv1)': lambda : max_pool(\n    h_conv1), 'avg_pool(h_conv2, h_conv3)': lambda : avg_pool(h_conv2,\n    h_conv3)}, name='max_pool')\nh_pool2 = nni.function_choice({'max_poo(h_conv1)': lambda : max_poo(h_conv1\n    ), '(2 * 3 + 4)': lambda : 2 * 3 + 4, '(lambda x: 1 + x)': lambda : lambda\n    x: 1 + x}, name='max_poo')\ntmp = nni.qlognormal(1.2, 3, 4.5)\ntest_acc = 1\nnni.report_intermediate_result(test_acc)\ntest_acc = 2\nnni.report_final_result(test_acc)\n"""
tools/nni_annotation/testcase/annotated/mnist.py,0,"b'""""""A deep MNIST classifier using convolutional layers.\n""""""\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\nimport nni\nimport logging\nimport math\nimport tempfile\nimport tensorflow as tf\nfrom tensorflow.examples.tutorials.mnist import input_data\nlogger = logging.getLogger(\'mnist\')\nFLAGS = None\n\n\nclass MnistNetwork(object):\n\n    def __init__(self, channel_1_num=32, channel_2_num=64, conv_size=5,\n        hidden_size=1024, pool_size=2, learning_rate=0.0001, x_dim=784,\n        y_dim=10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        self.conv_size = nni.choice({2: 2, 3: 3, 5: 5, 7: 7}, name=\n            \'self.conv_size\')\n        self.hidden_size = nni.choice({124: 124, 512: 512, 1024: 1024},\n            name=\'self.hidden_size\')\n        self.pool_size = pool_size\n        self.learning_rate = nni.randint(2, 3, 5, name=\'self.learning_rate\')\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n    def build_network(self):\n        self.x = tf.placeholder(tf.float32, [None, self.x_dim], name=\'input_x\')\n        self.y = tf.placeholder(tf.float32, [None, self.y_dim], name=\'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name=\'keep_prob\')\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                logger.debug(\n                    \'input dim cannot be sqrt and reshape. input dim: \' +\n                    str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.x, [-1, input_dim, input_dim, 1])\n        with tf.name_scope(\'conv1\'):\n            W_conv1 = weight_variable([self.conv_size, self.conv_size, 1,\n                self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            h_conv1 = nni.function_choice({\n                \'tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\': lambda :\n                tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1),\n                \'tf.nn.sigmoid(conv2d(x_image, W_conv1) + b_conv1)\': lambda :\n                tf.nn.sigmoid(conv2d(x_image, W_conv1) + b_conv1),\n                \'tf.nn.tanh(conv2d(x_image, W_conv1) + b_conv1)\': lambda :\n                tf.nn.tanh(conv2d(x_image, W_conv1) + b_conv1)}, name=\n                \'tf.nn.relu\')\n        with tf.name_scope(\'pool1\'):\n            h_pool1 = nni.function_choice({\n                \'max_pool(h_conv1, self.pool_size)\': lambda : max_pool(\n                h_conv1, self.pool_size),\n                \'avg_pool(h_conv1, self.pool_size)\': lambda : avg_pool(\n                h_conv1, self.pool_size)}, name=\'max_pool\')\n        with tf.name_scope(\'conv2\'):\n            W_conv2 = weight_variable([self.conv_size, self.conv_size, self\n                .channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n        with tf.name_scope(\'pool2\'):\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            W_fc1 = weight_variable([last_dim * last_dim * self.\n                channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n        h_pool2_flat = tf.reshape(h_pool2, [-1, last_dim * last_dim * self.\n            channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n        with tf.name_scope(\'fc2\'):\n            W_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(tf.nn.\n                softmax_cross_entropy_with_logits(labels=self.y, logits=y_conv)\n                )\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(self.learning_rate\n                ).minimize(cross_entropy)\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(\n                self.y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.\n                float32))\n        return\n\n\ndef conv2d(x, W):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\n\ndef max_pool(x, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x, ksize=[1, pool_size, pool_size, 1], strides=[1,\n        pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef avg_pool(x, pool_size):\n    return tf.nn.avg_pool(x, ksize=[1, pool_size, pool_size, 1], strides=[1,\n        pool_size, pool_size, 1], padding=\'SAME\')\n\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\n\ndef main():\n    data_dir = \'/tmp/tensorflow/mnist/input_data\'\n    mnist = input_data.read_data_sets(data_dir, one_hot=True)\n    logger.debug(\'Mnist download data done.\')\n    mnist_network = MnistNetwork()\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\' % graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        batch_num = 200\n        for i in range(batch_num):\n            batch_size = nni.choice({50: 50, 250: 250, 500: 500}, name=\n                \'batch_size\')\n            batch = mnist.train.next_batch(batch_size)\n            dropout_rate = nni.choice({1: 1, 5: 5}, name=\'dropout_rate\')\n            mnist_network.train_step.run(feed_dict={mnist_network.x: batch[\n                0], mnist_network.y: batch[1], mnist_network.keep_prob:\n                dropout_rate})\n            if i % 100 == 0:\n                test_acc = mnist_network.accuracy.eval(feed_dict={\n                    mnist_network.x: mnist.test.images, mnist_network.y:\n                    mnist.test.labels, mnist_network.keep_prob: 1.0})\n                nni.report_intermediate_result(test_acc)\n        test_acc = mnist_network.accuracy.eval(feed_dict={mnist_network.x:\n            mnist.test.images, mnist_network.y: mnist.test.labels,\n            mnist_network.keep_prob: 1.0})\n        nni.report_final_result(test_acc)\n\n\ndef generate_default_params():\n    params = {\'data_dir\': \'/tmp/tensorflow/mnist/input_data\',\n        \'dropout_rate\': 0.5, \'channel_1_num\': 32, \'channel_2_num\': 64,\n        \'conv_size\': 5, \'pool_size\': 2, \'hidden_size\': 1024, \'batch_size\':\n        50, \'batch_num\': 200, \'learning_rate\': 0.0001}\n    return params\n\n\nif __name__ == \'__main__\':\n    nni.get_next_parameter()\n    try:\n        params = generate_default_params()\n        logger.debug(\'params\')\n        logger.debug(\'params update\')\n        main()\n    except:\n        logger.exception(\'Got some exception in while loop in mnist.py\')\n        raise\n'"
tools/nni_annotation/testcase/annotated/nas.py,0,"b""import nni\nimport time\n\n\ndef add_one(inputs):\n    return inputs + 1\n\n\ndef add_two(inputs):\n    return inputs + 2\n\n\ndef add_three(inputs):\n    return inputs + 3\n\n\ndef add_four(inputs):\n    return inputs + 4\n\n\ndef main():\n    images = 5\n    layer_1_out = nni.mutable_layer('mutable_block_39', 'mutable_layer_0',\n        {'add_one()': add_one, 'add_two()': add_two, 'add_three()':\n        add_three, 'add_four()': add_four}, {'add_one()': {}, 'add_two()':\n        {}, 'add_three()': {}, 'add_four()': {}}, [], {'images': images}, 1,\n        'classic_mode')\n    layer_2_out = nni.mutable_layer('mutable_block_39', 'mutable_layer_1',\n        {'add_one()': add_one, 'add_two()': add_two, 'add_three()':\n        add_three, 'add_four()': add_four}, {'add_one()': {}, 'add_two()':\n        {}, 'add_three()': {}, 'add_four()': {}}, [], {'layer_1_out':\n        layer_1_out}, 1, 'classic_mode')\n    layer_3_out = nni.mutable_layer('mutable_block_39', 'mutable_layer_2',\n        {'add_one()': add_one, 'add_two()': add_two, 'add_three()':\n        add_three, 'add_four()': add_four}, {'add_one()': {}, 'add_two()':\n        {}, 'add_three()': {}, 'add_four()': {}}, [], {'layer_1_out':\n        layer_1_out, 'layer_2_out': layer_2_out}, 1, 'classic_mode')\n    nni.report_intermediate_result(layer_1_out)\n    time.sleep(2)\n    nni.report_intermediate_result(layer_2_out)\n    time.sleep(2)\n    nni.report_intermediate_result(layer_3_out)\n    time.sleep(2)\n    layer_3_out = layer_3_out + 10\n    nni.report_final_result(layer_3_out)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/nni_annotation/testcase/usercode/mnist.py,0,"b'# -*- encoding:utf8 -*-\n\n""""""A deep MNIST classifier using convolutional layers.\n""""""\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport logging\nimport math\nimport tempfile\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\n\nlogger = logging.getLogger(\'mnist\')\n\nFLAGS = None\n\nclass MnistNetwork(object):\n    def __init__(self,\n                 channel_1_num = 32,\n                 channel_2_num = 64,\n                 conv_size = 5,\n                 hidden_size = 1024,\n                 pool_size = 2,\n                 learning_rate = 0.0001,\n                 x_dim = 784,\n                 y_dim = 10):\n        self.channel_1_num = channel_1_num\n        self.channel_2_num = channel_2_num\n        \'\'\'@nni.variable(nni.choice(2,3,5,7),name=self.conv_size)\'\'\'\n        self.conv_size = conv_size\n        \'\'\'@nni.variable(nni.choice(124,512,1024),name=self.hidden_size)\'\'\'\n        self.hidden_size = hidden_size\n        self.pool_size = pool_size\n        \'\'\'@nni.variable(nni.randint(2,3,5),name=self.learning_rate)\'\'\'\n        self.learning_rate = learning_rate\n        self.x_dim = x_dim\n        self.y_dim = y_dim\n\n    def build_network(self):\n        self.x = tf.placeholder(tf.float32, [None, self.x_dim], name = \'input_x\')\n        self.y = tf.placeholder(tf.float32, [None, self.y_dim], name = \'input_y\')\n        self.keep_prob = tf.placeholder(tf.float32, name = \'keep_prob\')\n\n        # Reshape to use within a convolutional neural net.\n        # Last dimension is for ""features"" - there is only one here, since images are\n        # grayscale -- it would be 3 for an RGB image, 4 for RGBA, etc.\n        with tf.name_scope(\'reshape\'):\n            try:\n                input_dim = int(math.sqrt(self.x_dim))\n            except:\n                #print(\'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                logger.debug(\'input dim cannot be sqrt and reshape. input dim: \' + str(self.x_dim))\n                raise\n            x_image = tf.reshape(self.x, [-1, input_dim, input_dim, 1])\n\n        # First convolutional layer - maps one grayscale image to 32 feature maps.\n        with tf.name_scope(\'conv1\'):\n            W_conv1 = weight_variable([self.conv_size, self.conv_size, 1, self.channel_1_num])\n            b_conv1 = bias_variable([self.channel_1_num])\n            """"""@nni.function_choice(tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1),tf.nn.sigmoid(conv2d(x_image, W_conv1) + b_conv1),tf.nn.tanh(conv2d(x_image, W_conv1) + b_conv1),name=tf.nn.relu)""""""\n            h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n\n        # Pooling layer - downsamples by 2X.\n        with tf.name_scope(\'pool1\'):\n            """"""@nni.function_choice(max_pool(h_conv1, self.pool_size),avg_pool(h_conv1, self.pool_size),name=max_pool)""""""\n            h_pool1 = max_pool(h_conv1, self.pool_size)\n\n        # Second convolutional layer -- maps 32 feature maps to 64.\n        with tf.name_scope(\'conv2\'):\n            W_conv2 = weight_variable([self.conv_size, self.conv_size, self.channel_1_num, self.channel_2_num])\n            b_conv2 = bias_variable([self.channel_2_num])\n            h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n\n        # Second pooling layer.\n        with tf.name_scope(\'pool2\'):\n            #""""""@nni.dynamic(input={cnn_block:1, concat:2},function_choice={""cnn_block"":(x,nni.choice([3,4])),""cnn_block"":(x),""concat"":(x,y)},limit={""cnn_block.input"":[concat,input],""concat.input"":[this.depth-1,this.depth-3,this.depth-5],""graph.width"":[1]})""""""\n            h_pool2 = max_pool(h_conv2, self.pool_size)\n\n        # Fully connected layer 1 -- after 2 round of downsampling, our 28x28 image\n        # is down to 7x7x64 feature maps -- maps this to 1024 features.\n        last_dim = int(input_dim / (self.pool_size * self.pool_size))\n        with tf.name_scope(\'fc1\'):\n            W_fc1 = weight_variable([last_dim * last_dim * self.channel_2_num, self.hidden_size])\n            b_fc1 = bias_variable([self.hidden_size])\n\n        h_pool2_flat = tf.reshape(h_pool2, [-1, last_dim * last_dim * self.channel_2_num])\n        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n\n        # Dropout - controls the complexity of the model, prevents co-adaptation of features.\n        with tf.name_scope(\'dropout\'):\n            h_fc1_drop = tf.nn.dropout(h_fc1, self.keep_prob)\n\n        # Map the 1024 features to 10 classes, one for each digit\n        with tf.name_scope(\'fc2\'):\n            W_fc2 = weight_variable([self.hidden_size, self.y_dim])\n            b_fc2 = bias_variable([self.y_dim])\n            y_conv = tf.matmul(h_fc1_drop, W_fc2) + b_fc2\n\n        with tf.name_scope(\'loss\'):\n            cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels = self.y, logits = y_conv))\n        with tf.name_scope(\'adam_optimizer\'):\n            self.train_step = tf.train.AdamOptimizer(self.learning_rate).minimize(cross_entropy)\n\n        with tf.name_scope(\'accuracy\'):\n            correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(self.y, 1))\n            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        return\n\ndef conv2d(x, W):\n    """"""conv2d returns a 2d convolution layer with full stride.""""""\n    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding=\'SAME\')\n\ndef max_pool(x, pool_size):\n    """"""max_pool downsamples a feature map by 2X.""""""\n    return tf.nn.max_pool(x, ksize=[1, pool_size, pool_size, 1],\n                        strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\ndef avg_pool(x,pool_size):\n    return tf.nn.avg_pool(x, ksize=[1, pool_size, pool_size, 1],\n                        strides=[1, pool_size, pool_size, 1], padding=\'SAME\')\n\ndef weight_variable(shape):\n    """"""weight_variable generates a weight variable of a given shape.""""""\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    return tf.Variable(initial)\n\ndef bias_variable(shape):\n    """"""bias_variable generates a bias variable of a given shape.""""""\n    initial = tf.constant(0.1, shape=shape)\n    return tf.Variable(initial)\n\ndef main():\n    # Import data\n    data_dir= \'/tmp/tensorflow/mnist/input_data\'\n    mnist = input_data.read_data_sets(data_dir, one_hot=True)\n    logger.debug(\'Mnist download data done.\')\n\n    # Create the model\n    # Build the graph for the deep net\n    mnist_network = MnistNetwork()\n    mnist_network.build_network()\n    logger.debug(\'Mnist build network done.\')\n\n    # Write log\n    graph_location = tempfile.mkdtemp()\n    logger.debug(\'Saving graph to: %s\' % graph_location)\n    # print(\'Saving graph to: %s\' % graph_location)\n    train_writer = tf.summary.FileWriter(graph_location)\n    train_writer.add_graph(tf.get_default_graph())\n\n    test_acc = 0.0\n    with tf.Session() as sess:\n        sess.run(tf.global_variables_initializer())\n        batch_num=200\n        for i in range(batch_num):\n            \'\'\'@nni.variable(nni.choice(50,250,500),name=batch_size)\'\'\'\n            batch_size=50\n            batch = mnist.train.next_batch(batch_size)\n            \'\'\'@nni.variable(nni.choice(1,5),name=dropout_rate)\'\'\'\n            dropout_rate=0.5\n            mnist_network.train_step.run(feed_dict={mnist_network.x: batch[0], mnist_network.y: batch[1], mnist_network.keep_prob: dropout_rate})\n\n            if i % 100 == 0:\n                #train_accuracy = mnist_network.accuracy.eval(feed_dict={\n                #    mnist_network.x: batch[0], mnist_network.y: batch[1], mnist_network.keep_prob: params[\'dropout_rate\']})\n                #print(\'step %d, training accuracy %g\' % (i, train_accuracy))\n\n                test_acc = mnist_network.accuracy.eval(feed_dict={\n                    mnist_network.x: mnist.test.images, mnist_network.y: mnist.test.labels, mnist_network.keep_prob: 1.0})\n                \'\'\'@nni.report_intermediate_result(test_acc)\'\'\'\n\n        test_acc = mnist_network.accuracy.eval(feed_dict={\n            mnist_network.x: mnist.test.images, mnist_network.y: mnist.test.labels, mnist_network.keep_prob: 1.0})\n        \'\'\'@nni.report_final_result(test_acc)\'\'\'\n\n\ndef generate_default_params():\n    params = {\'data_dir\': \'/tmp/tensorflow/mnist/input_data\',\n              \'dropout_rate\': 0.5,\n              \'channel_1_num\': 32,\n              \'channel_2_num\': 64,\n              \'conv_size\': 5,\n              \'pool_size\': 2,\n              \'hidden_size\': 1024,\n              \'batch_size\': 50,\n              \'batch_num\': 200,\n              \'learning_rate\': 1e-4}\n    return params\n\nif __name__ == \'__main__\':\n    # run command: python mnist.py --init_file_path ./init.json\n\n    #FLAGS, unparsed = parse_command()\n    #original_params = parse_init_json(FLAGS.init_file_path, {})\n\n    #pipe_interface.set_params_to_env()\n    \'\'\'@nni.get_next_parameter()\'\'\'\n    try:\n        params = generate_default_params()\n        logger.debug(\'params\')\n        logger.debug(\'params update\')\n        main()\n    except:\n        logger.exception(\'Got some exception in while loop in mnist.py\')\n        raise\n'"
tools/nni_annotation/testcase/usercode/nas.py,0,"b'import time\n\ndef add_one(inputs):\n    return inputs + 1\n\ndef add_two(inputs):\n    return inputs + 2\n\ndef add_three(inputs):\n    return inputs + 3\n\ndef add_four(inputs):\n    return inputs + 4\n\n\ndef main():\n\n    images = 5\n\n    """"""@nni.mutable_layers(\n    {\n        layer_choice: [add_one(), add_two(), add_three(), add_four()],\n        optional_inputs: [images],\n        optional_input_size: 1,\n        layer_output: layer_1_out\n    },\n    {\n        layer_choice: [add_one(), add_two(), add_three(), add_four()],\n        optional_inputs: [layer_1_out],\n        optional_input_size: 1,\n        layer_output: layer_2_out\n    },\n    {\n        layer_choice: [add_one(), add_two(), add_three(), add_four()],\n        optional_inputs: [layer_1_out, layer_2_out],\n        optional_input_size: 1,\n        layer_output: layer_3_out\n    }\n    )""""""\n\n    """"""@nni.report_intermediate_result(layer_1_out)""""""\n    time.sleep(2)\n    """"""@nni.report_intermediate_result(layer_2_out)""""""\n    time.sleep(2)\n    """"""@nni.report_intermediate_result(layer_3_out)""""""\n    time.sleep(2)\n\n    layer_3_out = layer_3_out + 10\n\n    """"""@nni.report_final_result(layer_3_out)""""""\n\nif __name__ == \'__main__\':\n    main()\n'"
src/sdk/pynni/nni/batch_tuner/__init__.py,0,b''
src/sdk/pynni/nni/batch_tuner/batch_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nbatch_tuner.py including:\n    class BatchTuner\n""""""\n\nimport logging\n\nimport nni\nfrom nni.tuner import Tuner\n\nTYPE = \'_type\'\nCHOICE = \'choice\'\nVALUE = \'_value\'\n\nLOGGER = logging.getLogger(\'batch_tuner_AutoML\')\n\nclass BatchTuner(Tuner):\n    """"""\n    BatchTuner is tuner will running all the configure that user want to run batchly.\n\n    Examples\n    --------\n    The search space only be accepted like:\n\n        ::\n\n            {\'combine_params\':\n                { \'_type\': \'choice\',\n                            \'_value\': \'[{...}, {...}, {...}]\',\n                }\n            }\n\n    """"""\n\n    def __init__(self):\n        self._count = -1\n        self._values = []\n\n    def is_valid(self, search_space):\n        """"""\n        Check the search space is valid: only contains \'choice\' type\n\n        Parameters\n        ----------\n        search_space : dict\n\n        Returns\n        -------\n        None or list\n            If valid, return candidate values; else return None.\n        """"""\n        if not len(search_space) == 1:\n            raise RuntimeError(\'BatchTuner only supprt one combined-paramreters key.\')\n\n        for param in search_space:\n            param_type = search_space[param][TYPE]\n            if not param_type == CHOICE:\n                raise RuntimeError(\'BatchTuner only supprt \\\n                                    one combined-paramreters type is choice.\')\n\n            if isinstance(search_space[param][VALUE], list):\n                return search_space[param][VALUE]\n\n            raise RuntimeError(\'The combined-paramreters \\\n                                value in BatchTuner is not a list.\')\n        return None\n\n    def update_search_space(self, search_space):\n        """"""Update the search space\n\n        Parameters\n        ----------\n        search_space : dict\n        """"""\n        self._values = self.is_valid(search_space)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""Returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A candidate parameter group.\n        """"""\n        self._count += 1\n        if self._count > len(self._values) - 1:\n            raise nni.NoMoreTrialError(\'no more parameters now.\')\n        return self._values[self._count]\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        pass\n\n    def import_data(self, data):\n        """"""Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, \'parameter\' and \'value\'\n        """"""\n        if not self._values:\n            LOGGER.info(""Search space has not been initialized, skip this data import"")\n            return\n\n        self._values = self._values[(self._count+1):]\n        self._count = -1\n\n        _completed_num = 0\n        for trial_info in data:\n            LOGGER .info(""Importing data, current processing \\\n                            progress %s / %s"", _completed_num, len(data))\n            # simply validate data format\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                LOGGER.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            _completed_num += 1\n            if _params in self._values:\n                self._values.remove(_params)\n        LOGGER .info(""Successfully import data to batch tuner, \\\n                        total data: %d, imported data: %d."", len(data), _completed_num)\n'"
src/sdk/pynni/nni/bohb_advisor/__init__.py,0,b''
src/sdk/pynni/nni/bohb_advisor/bohb_advisor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\'\'\'\nbohb_advisor.py\n\'\'\'\n\nimport sys\nimport math\nimport logging\nimport json_tricks\n\nimport ConfigSpace as CS\nimport ConfigSpace.hyperparameters as CSH\n\nfrom nni.protocol import CommandType, send\nfrom nni.msg_dispatcher_base import MsgDispatcherBase\nfrom nni.utils import OptimizeMode, MetricType, extract_scalar_reward\nfrom nni.common import multi_phase_enabled\n\nfrom .config_generator import CG_BOHB\n\nlogger = logging.getLogger(\'BOHB_Advisor\')\n\n_next_parameter_id = 0\n_KEY = \'TRIAL_BUDGET\'\n_epsilon = 1e-6\n\n\ndef create_parameter_id():\n    """"""Create an id\n\n    Returns\n    -------\n    int\n        parameter id\n    """"""\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1\n\n\ndef create_bracket_parameter_id(brackets_id, brackets_curr_decay, increased_id=-1):\n    """"""Create a full id for a specific bracket\'s hyperparameter configuration\n\n    Parameters\n    ----------\n    brackets_id: int\n        brackets id\n    brackets_curr_decay: int\n        brackets curr decay\n    increased_id: int\n        increased id\n    Returns\n    -------\n    int\n        params id\n    """"""\n    if increased_id == -1:\n        increased_id = str(create_parameter_id())\n    params_id = \'_\'.join([str(brackets_id),\n                          str(brackets_curr_decay),\n                          increased_id])\n    return params_id\n\n\nclass Bracket:\n    """"""\n    A bracket in BOHB, all the information of a bracket is managed by\n    an instance of this class.\n\n    Parameters\n    ----------\n    s: int\n        The current Successive Halving iteration index.\n    s_max: int\n        total number of Successive Halving iterations\n    eta: float\n        In each iteration, a complete run of sequential halving is executed. In it,\n\t\tafter evaluating each configuration on the same subset size, only a fraction of\n\t\t1/eta of them \'advances\' to the next round.\n\tmax_budget : float\n\t\tThe largest budget to consider. Needs to be larger than min_budget!\n\t\tThe budgets will be geometrically distributed\n        :math:`a^2 + b^2 = c^2 \\\\sim \\\\eta^k` for :math:`k\\\\in [0, 1, ... , num\\\\_subsets - 1]`.\n    optimize_mode: str\n        optimize mode, \'maximize\' or \'minimize\'\n    """"""\n    def __init__(self, s, s_max, eta, max_budget, optimize_mode):\n        self.s = s\n        self.s_max = s_max\n        self.eta = eta\n        self.max_budget = max_budget\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        self.n = math.ceil((s_max + 1) * eta**s / (s + 1) - _epsilon)\n        self.r = max_budget / eta**s\n        self.i = 0\n        self.hyper_configs = []         # [ {id: params}, {}, ... ]\n        self.configs_perf = []          # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []    # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.no_more_trial = False\n\n    def is_completed(self):\n        """"""check whether this bracket has sent out all the hyperparameter configurations""""""\n        return self.no_more_trial\n\n    def get_n_r(self):\n        """"""return the values of n and r for the next round""""""\n        return math.floor(self.n / self.eta**self.i + _epsilon), math.floor(self.r * self.eta**self.i +_epsilon)\n\n    def increase_i(self):\n        """"""i means the ith round. Increase i by 1""""""\n        self.i += 1\n\n    def set_config_perf(self, i, parameter_id, seq, value):\n        """"""update trial\'s latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        """"""\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]\n\n    def inform_trial_end(self, i):\n        """"""If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n\n        Returns\n        -------\n        new trial or None:\n            If we have generated new trials after this trial end, we will return a new trial parameters.\n            Otherwise, we will return None.\n        """"""\n        global _KEY\n        self.num_finished_configs[i] += 1\n        logger.debug(\'bracket id: %d, round: %d %d, finished: %d, all: %d\',\n                     self.s, self.i, i, self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            # finish this bracket\n            if self.i > self.s:\n                self.no_more_trial = True\n                return None\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(\n                ), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(\n                    this_round_perf.items(), key=lambda kv: kv[1][1])\n            logger.debug(\n                \'bracket %s next round %s, sorted hyper configs: %s\', self.s, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            logger.debug(\'bracket %s next round %s, next_n=%d, next_r=%d\',\n                         self.s, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split(\'_\')[-1]\n                new_id = create_bracket_parameter_id(\n                    self.s, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None\n\n    def get_hyperparameter_configurations(self, num, r, config_generator):\n        """"""generate num hyperparameter configurations from search space using Bayesian optimization\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        """"""\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.s, self.i)\n            params = config_generator.get_config(r)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]\n\n    def _record_hyper_configs(self, hyper_configs):\n        """"""after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        """"""\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()\n\n\nclass BOHB(MsgDispatcherBase):\n    """"""\n    BOHB performs robust and efficient hyperparameter optimization\n    at scale by combining the speed of Hyperband searches with the\n    guidance and guarantees of convergence of Bayesian Optimization.\n    Instead of sampling new configurations at random, BOHB uses\n    kernel density estimators to select promising candidates.\n\n    Parameters\n    ----------\n    optimize_mode: str\n        optimize mode, \'maximize\' or \'minimize\'\n    min_budget: float\n        The smallest budget to consider. Needs to be positive!\n    max_budget: float\n        The largest budget to consider. Needs to be larger than min_budget!\n        The budgets will be geometrically distributed\n        :math:`a^2 + b^2 = c^2 \\\\sim \\\\eta^k` for :math:`k\\\\in [0, 1, ... , num\\\\_subsets - 1]`.\n    eta: int\n        In each iteration, a complete run of sequential halving is executed. In it,\n        after evaluating each configuration on the same subset size, only a fraction of\n        1/eta of them \'advances\' to the next round.\n        Must be greater or equal to 2.\n    min_points_in_model: int\n        number of observations to start building a KDE. Default \'None\' means\n        dim+1, the bare minimum.\n    top_n_percent: int\n        percentage ( between 1 and 99, default 15) of the observations that are considered good.\n    num_samples: int\n        number of samples to optimize EI (default 64)\n    random_fraction: float\n    fraction of purely random configurations that are sampled from the\n        prior without the model.\n    bandwidth_factor: float\n        to encourage diversity, the points proposed to optimize EI, are sampled\n        from a \'widened\' KDE where the bandwidth is multiplied by this factor (default: 3)\n    min_bandwidth: float\n        to keep diversity, even when all (good) samples have the same value for one of the parameters,\n        a minimum bandwidth (Default: 1e-3) is used instead of zero.\n    """"""\n\n    def __init__(self,\n                 optimize_mode=\'maximize\',\n                 min_budget=1,\n                 max_budget=3,\n                 eta=3,\n                 min_points_in_model=None,\n                 top_n_percent=15,\n                 num_samples=64,\n                 random_fraction=1/3,\n                 bandwidth_factor=3,\n                 min_bandwidth=1e-3):\n        super(BOHB, self).__init__()\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.min_budget = min_budget\n        self.max_budget = max_budget\n        self.eta = eta\n        self.min_points_in_model = min_points_in_model\n        self.top_n_percent = top_n_percent\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n        self.bandwidth_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        # all the configs waiting for run\n        self.generated_hyper_configs = []\n        # all the completed configs\n        self.completed_hyper_configs = []\n\n        self.s_max = math.floor(\n            math.log(self.max_budget / self.min_budget, self.eta) + _epsilon)\n        # current bracket(s) number\n        self.curr_s = self.s_max\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n        self.brackets = dict()\n        self.search_space = None\n        # [key, value] = [parameter_id, parameter]\n        self.parameters = dict()\n\n        # config generator\n        self.cg = None\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()\n        # record the unsatisfied parameter request from trial jobs\n        self.unsatisfied_jobs = []\n\n    def handle_initialize(self, data):\n        """"""Initialize Tuner, including creating Bayesian optimization-based parametric models\n        and search space formations\n\n        Parameters\n        ----------\n        data: search space\n            search space of this experiment\n\n        Raises\n        ------\n        ValueError\n            Error: Search space is None\n        """"""\n        logger.info(\'start to handle_initialize\')\n        # convert search space jason to ConfigSpace\n        self.handle_update_search_space(data)\n\n        # generate BOHB config_generator using Bayesian optimization\n        if self.search_space:\n            self.cg = CG_BOHB(configspace=self.search_space,\n                              min_points_in_model=self.min_points_in_model,\n                              top_n_percent=self.top_n_percent,\n                              num_samples=self.num_samples,\n                              random_fraction=self.random_fraction,\n                              bandwidth_factor=self.bandwidth_factor,\n                              min_bandwidth=self.min_bandwidth)\n        else:\n            raise ValueError(\'Error: Search space is None\')\n        # generate first brackets\n        self.generate_new_bracket()\n        send(CommandType.Initialized, \'\')\n\n    def generate_new_bracket(self):\n        """"""generate a new bracket""""""\n        logger.debug(\n            \'start to create a new SuccessiveHalving iteration, self.curr_s=%d\', self.curr_s)\n        if self.curr_s < 0:\n            logger.info(""s < 0, Finish this round of Hyperband in BOHB. Generate new round"")\n            self.curr_s = self.s_max\n        self.brackets[self.curr_s] = Bracket(\n            s=self.curr_s, s_max=self.s_max, eta=self.eta,\n            max_budget=self.max_budget, optimize_mode=self.optimize_mode\n        )\n        next_n, next_r = self.brackets[self.curr_s].get_n_r()\n        logger.debug(\n            \'new SuccessiveHalving iteration, next_n=%d, next_r=%d\', next_n, next_r)\n        # rewrite with TPE\n        generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(\n            next_n, next_r, self.cg)\n        self.generated_hyper_configs = generated_hyper_configs.copy()\n\n    def handle_request_trial_jobs(self, data):\n        """"""recerive the number of request and generate trials\n\n        Parameters\n        ----------\n        data: int\n            number of trial jobs that nni manager ask to generate\n        """"""\n        # Receive new request\n        self.credit += data\n\n        for _ in range(self.credit):\n            self._request_one_trial_job()\n\n    def _get_one_trial_job(self):\n        """"""get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return ""NewTrialJob"" with a dict:\n        {\n            \'parameter_id\': id of new hyperparameter\n            \'parameter_source\': \'algorithm\'\n            \'parameters\': value of new hyperparameter\n        }\n        b. If BOHB don\'t have parameter waiting, will return ""NoMoreTrialJobs"" with\n        {\n            \'parameter_id\': \'-1_0_0\',\n            \'parameter_source\': \'algorithm\',\n            \'parameters\': \'\'\n        }\n        """"""\n        if not self.generated_hyper_configs:\n            ret = {\n                \'parameter_id\': \'-1_0_0\',\n                \'parameter_source\': \'algorithm\',\n                \'parameters\': \'\'\n            }\n            send(CommandType.NoMoreTrialJobs, json_tricks.dumps(ret))\n            return None\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            \'parameter_id\': params[0],\n            \'parameter_source\': \'algorithm\',\n            \'parameters\': params[1]\n        }\n        self.parameters[params[0]] = params[1]\n        return ret\n\n    def _request_one_trial_job(self):\n        """"""get one trial job, i.e., one hyperparameter configuration.\n\n        If this function is called, Command will be sent by BOHB:\n        a. If there is a parameter need to run, will return ""NewTrialJob"" with a dict:\n        {\n            \'parameter_id\': id of new hyperparameter\n            \'parameter_source\': \'algorithm\'\n            \'parameters\': value of new hyperparameter\n        }\n        b. If BOHB don\'t have parameter waiting, will return ""NoMoreTrialJobs"" with\n        {\n            \'parameter_id\': \'-1_0_0\',\n            \'parameter_source\': \'algorithm\',\n            \'parameters\': \'\'\n        }\n        """"""\n        ret = self._get_one_trial_job()\n        if ret is not None:\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n            self.credit -= 1\n\n    def handle_update_search_space(self, data):\n        """"""change json format to ConfigSpace format dict<dict> -> configspace\n\n        Parameters\n        ----------\n        data: JSON object\n            search space of this experiment\n        """"""\n        search_space = data\n        cs = CS.ConfigurationSpace()\n        for var in search_space:\n            _type = str(search_space[var][""_type""])\n            if _type == \'choice\':\n                cs.add_hyperparameter(CSH.CategoricalHyperparameter(\n                    var, choices=search_space[var][""_value""]))\n            elif _type == \'randint\':\n                cs.add_hyperparameter(CSH.UniformIntegerHyperparameter(\n                    var, lower=search_space[var][""_value""][0], upper=search_space[var][""_value""][1] - 1))\n            elif _type == \'uniform\':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][""_value""][0], upper=search_space[var][""_value""][1]))\n            elif _type == \'quniform\':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][""_value""][0], upper=search_space[var][""_value""][1],\n                    q=search_space[var][""_value""][2]))\n            elif _type == \'loguniform\':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][""_value""][0], upper=search_space[var][""_value""][1],\n                    log=True))\n            elif _type == \'qloguniform\':\n                cs.add_hyperparameter(CSH.UniformFloatHyperparameter(\n                    var, lower=search_space[var][""_value""][0], upper=search_space[var][""_value""][1],\n                    q=search_space[var][""_value""][2], log=True))\n            elif _type == \'normal\':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][""_value""][1], sigma=search_space[var][""_value""][2]))\n            elif _type == \'qnormal\':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][""_value""][1], sigma=search_space[var][""_value""][2],\n                    q=search_space[var][""_value""][3]))\n            elif _type == \'lognormal\':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][""_value""][1], sigma=search_space[var][""_value""][2],\n                    log=True))\n            elif _type == \'qlognormal\':\n                cs.add_hyperparameter(CSH.NormalFloatHyperparameter(\n                    var, mu=search_space[var][""_value""][1], sigma=search_space[var][""_value""][2],\n                    q=search_space[var][""_value""][3], log=True))\n            else:\n                raise ValueError(\n                    \'unrecognized type in search_space, type is {}\'.format(_type))\n\n        self.search_space = cs\n\n    def handle_trial_end(self, data):\n        """"""receive the information of trial end and generate next configuaration.\n\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job\'s state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        """"""\n        logger.debug(\'Tuner handle trial end, result is %s\', data)\n        hyper_params = json_tricks.loads(data[\'hyper_params\'])\n        self._handle_trial_end(hyper_params[\'parameter_id\'])\n        if data[\'trial_job_id\'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data[\'trial_job_id\']]\n\n    def _send_new_trial(self):\n        while self.unsatisfied_jobs:\n            ret = self._get_one_trial_job()\n            if ret is None:\n                break\n            one_unsatisfied = self.unsatisfied_jobs.pop(0)\n            ret[\'trial_job_id\'] = one_unsatisfied[\'trial_job_id\']\n            ret[\'parameter_index\'] = one_unsatisfied[\'parameter_index\']\n            # update parameter_id in self.job_id_para_id_map\n            self.job_id_para_id_map[ret[\'trial_job_id\']] = ret[\'parameter_id\']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        for _ in range(self.credit):\n            self._request_one_trial_job()\n\n    def _handle_trial_end(self, parameter_id):\n        s, i, _ = parameter_id.split(\'_\')\n        hyper_configs = self.brackets[int(s)].inform_trial_end(int(i))\n\n        if hyper_configs is not None:\n            logger.debug(\n                \'bracket %s next round %s, hyper_configs: %s\', s, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n        # Finish this bracket and generate a new bracket\n        elif self.brackets[int(s)].no_more_trial:\n            self.curr_s -= 1\n            self.generate_new_bracket()\n        self._send_new_trial()\n\n    def handle_report_metric_data(self, data):\n        """"""reveice the metric data and update Bayesian optimization with final result\n\n        Parameters\n        ----------\n        data:\n            it is an object which has keys \'parameter_id\', \'value\', \'trial_job_id\', \'type\', \'sequence\'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        """"""\n        logger.debug(\'handle report metric data = %s\', data)\n        if \'value\' in data:\n            data[\'value\'] = json_tricks.loads(data[\'value\'])\n        if data[\'type\'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data[\'trial_job_id\'] is not None\n            assert data[\'parameter_index\'] is not None\n            assert data[\'trial_job_id\'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data[\'trial_job_id\']])\n            ret = self._get_one_trial_job()\n            if ret is None:\n                self.unsatisfied_jobs.append({\'trial_job_id\': data[\'trial_job_id\'], \'parameter_index\': data[\'parameter_index\']})\n            else:\n                ret[\'trial_job_id\'] = data[\'trial_job_id\']\n                ret[\'parameter_index\'] = data[\'parameter_index\']\n                # update parameter_id in self.job_id_para_id_map\n                self.job_id_para_id_map[data[\'trial_job_id\']] = ret[\'parameter_id\']\n                send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            assert \'value\' in data\n            value = extract_scalar_reward(data[\'value\'])\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -value\n            else:\n                reward = value\n            assert \'parameter_id\' in data\n            s, i, _ = data[\'parameter_id\'].split(\'_\')\n            logger.debug(\'bracket id = %s, metrics value = %s, type = %s\', s, value, data[\'type\'])\n            s = int(s)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data[\'trial_job_id\'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data[\'trial_job_id\']] == data[\'parameter_id\']\n            else:\n                self.job_id_para_id_map[data[\'trial_job_id\']] = data[\'parameter_id\']\n\n            assert \'type\' in data\n            if data[\'type\'] == MetricType.FINAL:\n                # and PERIODICAL metric are independent, thus, not comparable.\n                assert \'sequence\' in data\n                self.brackets[s].set_config_perf(\n                    int(i), data[\'parameter_id\'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n\n                _parameters = self.parameters[data[\'parameter_id\']]\n                _parameters.pop(_KEY)\n                # update BO with loss, max_s budget, hyperparameters\n                self.cg.new_result(loss=reward, budget=data[\'sequence\'], parameters=_parameters, update_model=True)\n            elif data[\'type\'] == MetricType.PERIODICAL:\n                self.brackets[s].set_config_perf(\n                    int(i), data[\'parameter_id\'], data[\'sequence\'], value)\n            else:\n                raise ValueError(\n                    \'Data type not supported: {}\'.format(data[\'type\']))\n\n    def handle_add_customized_trial(self, data):\n        pass\n\n    def handle_import_data(self, data):\n        """"""Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, \'parameter\' and \'value\'\n\n        Raises\n        ------\n        AssertionError\n            data doesn\'t have required key \'parameter\' and \'value\'\n        """"""\n        for entry in data:\n            entry[\'value\'] = json_tricks.loads(entry[\'value\'])\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            budget_exist_flag = False\n            barely_params = dict()\n            for keys in _params:\n                if keys == _KEY:\n                    _budget = _params[keys]\n                    budget_exist_flag = True\n                else:\n                    barely_params[keys] = _params[keys]\n            if not budget_exist_flag:\n                _budget = self.max_budget\n                logger.info(""Set \\""TRIAL_BUDGET\\"" value to %s (max budget)"", self.max_budget)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                reward = -_value\n            else:\n                reward = _value\n            self.cg.new_result(loss=reward, budget=_budget, parameters=barely_params, update_model=True)\n        logger.info(""Successfully import tuning data to BOHB advisor."")\n'"
src/sdk/pynni/nni/bohb_advisor/config_generator.py,0,"b'# BSD 3-Clause License\n# Copyright (c) 2017-2018, ML4AAD\n# All rights reserved.\n\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n\n# * Redistributions of source code must retain the above copyright notice, this\n#   list of conditions and the following disclaimer.\n\n# * Redistributions in binary form must reproduce the above copyright notice,\n#   this list of conditions and the following disclaimer in the documentation\n#   and/or other materials provided with the distribution.\n\n# * Neither the name of the copyright holder nor the names of its\n#   contributors may be used to endorse or promote products derived from\n#   this software without specific prior written permission.\n\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS ""AS IS""\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nimport logging\n\nimport ConfigSpace\nimport ConfigSpace.hyperparameters\nimport ConfigSpace.util\nimport numpy as np\nimport scipy.stats as sps\nimport statsmodels.api as sm\n\nlogger = logging.getLogger(\'BOHB_Advisor\')\n\nclass CG_BOHB:\n    def __init__(self, configspace, min_points_in_model=None,\n                 top_n_percent=15, num_samples=64, random_fraction=1/3,\n                 bandwidth_factor=3, min_bandwidth=1e-3):\n        """"""Fits for each given budget a kernel density estimator on the best N percent of the\n        evaluated configurations on this budget.\n\n\n        Parameters:\n        -----------\n        configspace: ConfigSpace\n            Configuration space object\n        top_n_percent: int\n            Determines the percentile of configurations that will be used as training data\n            for the kernel density estimator, e.g if set to 10 the 10% best configurations will be considered\n            for training.\n        min_points_in_model: int\n            minimum number of datapoints needed to fit a model\n        num_samples: int\n            number of samples drawn to optimize EI via sampling\n        random_fraction: float\n            fraction of random configurations returned\n        bandwidth_factor: float\n            widens the bandwidth for contiuous parameters for proposed points to optimize EI\n        min_bandwidth: float\n            to keep diversity, even when all (good) samples have the same value for one of the parameters,\n            a minimum bandwidth (Default: 1e-3) is used instead of zero.\n        """"""\n        self.top_n_percent = top_n_percent\n        self.configspace = configspace\n        self.bw_factor = bandwidth_factor\n        self.min_bandwidth = min_bandwidth\n\n        self.min_points_in_model = min_points_in_model\n        if min_points_in_model is None:\n            self.min_points_in_model = len(self.configspace.get_hyperparameters())+1\n\n        if self.min_points_in_model < len(self.configspace.get_hyperparameters())+1:\n            logger.warning(\'Invalid min_points_in_model value. Setting it to %i\', len(self.configspace.get_hyperparameters()) + 1)\n            self.min_points_in_model = len(self.configspace.get_hyperparameters()) + 1\n\n        self.num_samples = num_samples\n        self.random_fraction = random_fraction\n\n        hps = self.configspace.get_hyperparameters()\n\n        self.kde_vartypes = """"\n        self.vartypes = []\n\n        for h in hps:\n            if hasattr(h, \'choices\'):\n                self.kde_vartypes += \'u\'\n                self.vartypes += [len(h.choices)]\n            else:\n                self.kde_vartypes += \'c\'\n                self.vartypes += [0]\n\n        self.vartypes = np.array(self.vartypes, dtype=int)\n\n        # store precomputed probs for the categorical parameters\n        self.cat_probs = []\n\n        self.configs = dict()\n        self.losses = dict()\n        self.good_config_rankings = dict()\n        self.kde_models = dict()\n\n    def largest_budget_with_model(self):\n        if not self.kde_models:\n            return -float(\'inf\')\n        return max(self.kde_models.keys())\n\n    def sample_from_largest_budget(self, info_dict):\n        """"""We opted for a single multidimensional KDE compared to the\n        hierarchy of one-dimensional KDEs used in TPE. The dimensional is\n        seperated by budget. This function sample a configuration from\n        largest budget. Firstly we sample ""num_samples"" configurations,\n        then prefer one with the largest l(x)/g(x).\n\n        Parameters:\n        -----------\n        info_dict: dict\n            record the information of this configuration\n\n        Returns\n        -------\n        dict:\n            new configuration named sample\n        dict:\n            info_dict, record the information of this configuration\n        """"""\n        best = np.inf\n        best_vector = None\n\n        budget = max(self.kde_models.keys())\n\n        l = self.kde_models[budget][\'good\'].pdf\n        g = self.kde_models[budget][\'bad\'].pdf\n\n        minimize_me = lambda x: max(1e-32, g(x))/max(l(x), 1e-32)\n\n        kde_good = self.kde_models[budget][\'good\']\n        kde_bad = self.kde_models[budget][\'bad\']\n\n        for i in range(self.num_samples):\n            idx = np.random.randint(0, len(kde_good.data))\n            datum = kde_good.data[idx]\n            vector = []\n\n            for m, bw, t in zip(datum, kde_good.bw, self.vartypes):\n\n                bw = max(bw, self.min_bandwidth)\n                if t == 0:\n                    bw = self.bw_factor*bw\n                    vector.append(sps.truncnorm.rvs(-m/bw, (1-m)/bw, loc=m, scale=bw))\n                else:\n                    if np.random.rand() < (1-bw):\n                        vector.append(int(m))\n                    else:\n                        vector.append(np.random.randint(t))\n            val = minimize_me(vector)\n\n            if not np.isfinite(val):\n                logger.warning(\'sampled vector: %s has EI value %s\', vector, val)\n                logger.warning(""data in the KDEs:\\n%s\\n%s"", kde_good.data, kde_bad.data)\n                logger.warning(""bandwidth of the KDEs:\\n%s\\n%s"", kde_good.bw, kde_bad.bw)\n                logger.warning(""l(x) = %s"", l(vector))\n                logger.warning(""g(x) = %s"", g(vector))\n\n                # right now, this happens because a KDE does not contain all values for a categorical parameter\n                # this cannot be fixed with the statsmodels KDE, so for now, we are just going to evaluate this one\n                # if the good_kde has a finite value, i.e. there is no config with that value in the bad kde,\n                # so it shouldn\'t be terrible.\n                if np.isfinite(l(vector)):\n                    best_vector = vector\n                    break\n\n            if val < best:\n                best = val\n                best_vector = vector\n\n        if best_vector is None:\n            logger.debug(""Sampling based optimization with %i samples failed -> using random configuration"", self.num_samples)\n            sample = self.configspace.sample_configuration().get_dictionary()\n            info_dict[\'model_based_pick\'] = False\n\n        else:\n            logger.debug(\'best_vector: %s, %s, %s, %s\', best_vector, best, l(best_vector), g(best_vector))\n            for i, _ in enumerate(best_vector):\n                hp = self.configspace.get_hyperparameter(self.configspace.get_hyperparameter_by_idx(i))\n                if isinstance(hp, ConfigSpace.hyperparameters.CategoricalHyperparameter):\n                    best_vector[i] = int(np.rint(best_vector[i]))\n            sample = ConfigSpace.Configuration(self.configspace, vector=best_vector).get_dictionary()\n\n            sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n                configuration_space=self.configspace,\n                configuration=sample)\n            info_dict[\'model_based_pick\'] = True\n\n        return sample, info_dict\n\n    def get_config(self, budget):\n        """"""Function to sample a new configuration\n        This function is called inside BOHB to query a new configuration\n\n        Parameters:\n        -----------\n        budget: float\n            the budget for which this configuration is scheduled\n\n        Returns\n        -------\n        config\n            return a valid configuration with parameters and budget\n        """"""\n        logger.debug(\'start sampling a new configuration.\')\n        sample = None\n        info_dict = {}\n\n        # If no model is available, sample from prior\n        # also mix in a fraction of random configs\n        if not self.kde_models.keys() or np.random.rand() < self.random_fraction:\n            sample = self.configspace.sample_configuration()\n            info_dict[\'model_based_pick\'] = False\n\n        if sample is None:\n            sample, info_dict = self.sample_from_largest_budget(info_dict)\n\n        sample = ConfigSpace.util.deactivate_inactive_hyperparameters(\n            configuration_space=self.configspace,\n            configuration=sample.get_dictionary()\n        ).get_dictionary()\n\n        logger.debug(\'done sampling a new configuration.\')\n        sample[\'TRIAL_BUDGET\'] = budget\n        return sample\n\n    def impute_conditional_data(self, array):\n        return_array = np.empty_like(array)\n        for i in range(array.shape[0]):\n            datum = np.copy(array[i])\n            nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            while np.any(nan_indices):\n                nan_idx = nan_indices[0]\n                valid_indices = np.argwhere(np.isfinite(array[:, nan_idx])).flatten()\n                if valid_indices:\n                    # pick one of them at random and overwrite all NaN values\n                    row_idx = np.random.choice(valid_indices)\n                    datum[nan_indices] = array[row_idx, nan_indices]\n                else:\n                    # no good point in the data has this value activated, so fill it with a valid but random value\n                    t = self.vartypes[nan_idx]\n                    if t == 0:\n                        datum[nan_idx] = np.random.rand()\n                    else:\n                        datum[nan_idx] = np.random.randint(t)\n                nan_indices = np.argwhere(np.isnan(datum)).flatten()\n            return_array[i, :] = datum\n        return return_array\n\n    def new_result(self, loss, budget, parameters, update_model=True):\n        """"""\n        Function to register finished runs. Every time a run has finished, this function should be called\n        to register it with the loss.\n\n        Parameters:\n        -----------\n        loss: float\n            the loss of the parameters\n        budget: float\n            the budget of the parameters\n        parameters: dict\n            the parameters of this trial\n        update_model: bool\n            whether use this parameter to update BP model\n\n        Returns\n        -------\n        None\n        """"""\n        if loss is None:\n            # One could skip crashed results, but we decided\n            # assign a +inf loss and count them as bad configurations\n            loss = np.inf\n\n        if budget not in self.configs.keys():\n            self.configs[budget] = []\n            self.losses[budget] = []\n\n        # skip model building if we already have a bigger model\n        if max(list(self.kde_models.keys()) + [-np.inf]) > budget:\n            return\n\n        # We want to get a numerical representation of the configuration in the original space\n        conf = ConfigSpace.Configuration(self.configspace, parameters)\n        self.configs[budget].append(conf.get_array())\n        self.losses[budget].append(loss)\n\n        # skip model building:\n        # a) if not enough points are available\n        if len(self.configs[budget]) <= self.min_points_in_model - 1:\n            logger.debug(""Only %i run(s) for budget %f available, need more than %s \\\n            -> can\'t build model!"", len(self.configs[budget]), budget, self.min_points_in_model+1)\n            return\n        # b) during warnm starting when we feed previous results in and only update once\n        if not update_model:\n            return\n\n        train_configs = np.array(self.configs[budget])\n        train_losses = np.array(self.losses[budget])\n\n        n_good = max(self.min_points_in_model, (self.top_n_percent * train_configs.shape[0])//100)\n        n_bad = max(self.min_points_in_model, ((100-self.top_n_percent)*train_configs.shape[0])//100)\n\n        # Refit KDE for the current budget\n        idx = np.argsort(train_losses)\n\n        train_data_good = self.impute_conditional_data(train_configs[idx[:n_good]])\n        train_data_bad = self.impute_conditional_data(train_configs[idx[n_good:n_good+n_bad]])\n\n        if train_data_good.shape[0] <= train_data_good.shape[1]:\n            return\n        if train_data_bad.shape[0] <= train_data_bad.shape[1]:\n            return\n\n        #more expensive crossvalidation method\n        #bw_estimation = \'cv_ls\'\n        # quick rule of thumb\n        bw_estimation = \'normal_reference\'\n\n        bad_kde = sm.nonparametric.KDEMultivariate(data=train_data_bad, var_type=self.kde_vartypes, bw=bw_estimation)\n        good_kde = sm.nonparametric.KDEMultivariate(data=train_data_good, var_type=self.kde_vartypes, bw=bw_estimation)\n\n        bad_kde.bw = np.clip(bad_kde.bw, self.min_bandwidth, None)\n        good_kde.bw = np.clip(good_kde.bw, self.min_bandwidth, None)\n\n        self.kde_models[budget] = {\n            \'good\': good_kde,\n            \'bad\' : bad_kde\n        }\n\n        # update probs for the categorical parameters for later sampling\n        logger.debug(\'done building a new model for budget %f based on %i/%i split\\nBest loss for this budget:%f\\n\',\n                     budget, n_good, n_bad, np.min(train_losses))\n'"
src/sdk/pynni/nni/compression/__init__.py,0,b''
src/sdk/pynni/nni/curvefitting_assessor/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .curvefitting_assessor import CurvefittingAssessor\n'
src/sdk/pynni/nni/curvefitting_assessor/curvefitting_assessor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport datetime\nfrom nni.assessor import Assessor, AssessResult\nfrom nni.utils import extract_scalar_history\nfrom .model_factory import CurveModel\n\nlogger = logging.getLogger(\'curvefitting_Assessor\')\n\n\nclass CurvefittingAssessor(Assessor):\n    """"""CurvefittingAssessor uses learning curve fitting algorithm to predict the learning curve performance in the future.\n    It stops a pending trial X at step S if the trial\'s forecast result at target step is convergence and lower than the\n    best performance in the history.\n\n    Parameters\n    ----------\n    epoch_num : int\n        The total number of epoch\n    start_step : int\n        only after receiving start_step number of reported intermediate results\n    threshold : float\n        The threshold that we decide to early stop the worse performance curve.\n    """"""\n\n    def __init__(self, epoch_num=20, start_step=6, threshold=0.95, gap=1):\n        if start_step <= 0:\n            logger.warning(\'It\\\'s recommended to set start_step to a positive number\')\n        # Record the target position we predict\n        self.target_pos = epoch_num\n        # Start forecasting when historical data reaches start step\n        self.start_step = start_step\n        # Record the compared threshold\n        self.threshold = threshold\n        # Record the number of gap\n        self.gap = gap\n        # Record the number of intermediate result in the lastest judgment\n        self.last_judgment_num = dict()\n        # Record the best performance\n        self.set_best_performance = False\n        self.completed_best_performance = None\n        self.trial_history = []\n        logger.info(\'Successfully initials the curvefitting assessor\')\n\n    def trial_end(self, trial_job_id, success):\n        """"""update the best performance of completed trial job\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        """"""\n        if success:\n            if self.set_best_performance:\n                self.completed_best_performance = max(self.completed_best_performance, self.trial_history[-1])\n            else:\n                self.set_best_performance = True\n                self.completed_best_performance = self.trial_history[-1]\n            logger.info(\'Updated complted best performance, trial job id: %s\', trial_job_id)\n        else:\n            logger.info(\'No need to update, trial job id: %s\', trial_job_id)\n\n    def assess_trial(self, trial_job_id, trial_history):\n        """"""assess whether a trial should be early stop by curve fitting algorithm\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in curvefitting_assessor\n        """"""\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self.trial_history = scalar_trial_history\n        if not self.set_best_performance:\n            return AssessResult.Good\n        curr_step = len(scalar_trial_history)\n        if curr_step < self.start_step:\n            return AssessResult.Good\n\n        if trial_job_id in self.last_judgment_num.keys() and curr_step - self.last_judgment_num[trial_job_id] < self.gap:\n            return AssessResult.Good\n        self.last_judgment_num[trial_job_id] = curr_step\n\n        try:\n            start_time = datetime.datetime.now()\n            # Predict the final result\n            curvemodel = CurveModel(self.target_pos)\n            predict_y = curvemodel.predict(scalar_trial_history)\n            log_message = ""Prediction done. Trial job id = {}, Predict value = {}"".format(trial_job_id, predict_y)\n            if predict_y is None:\n                logger.info(\'%s, wait for more information to predict precisely\', log_message)\n                return AssessResult.Good\n            else:\n                logger.info(log_message)\n            standard_performance = self.completed_best_performance * self.threshold\n\n            end_time = datetime.datetime.now()\n            if (end_time - start_time).seconds > 60:\n                logger.warning(\n                    \'Curve Fitting Assessor Runtime Exceeds 60s, Trial Id = %s Trial History = %s\',\n                    trial_job_id, self.trial_history\n                )\n\n            if predict_y > standard_performance:\n                return AssessResult.Good\n            return AssessResult.Bad\n\n        except Exception as exception:\n            logger.exception(\'unrecognize exception in curvefitting_assessor %s\', exception)\n'"
src/sdk/pynni/nni/curvefitting_assessor/curvefunctions.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nA family of functions used by CurvefittingAssessor\n""""""\n\nimport numpy as np\n\nall_models = {}\nmodel_para = {}\nmodel_para_num = {}\n\ncurve_combination_models = [\'vap\', \'pow3\', \'linear\', \'logx_linear\', \'dr_hill_zero_background\', \'log_power\', \'pow4\', \'mmf\',\n                            \'exp4\', \'ilog2\', \'weibull\', \'janoschek\']\n\n\ndef vap(x, a, b, c):\n    """"""Vapor pressure model\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        np.exp(a+b/x+c*np.log(x))\n    """"""\n    return np.exp(a+b/x+c*np.log(x))\n\n\nall_models[\'vap\'] = vap\nmodel_para[\'vap\'] = [-0.622028, -0.470050, 0.042322]\nmodel_para_num[\'vap\'] = 3\n\n\ndef pow3(x, c, a, alpha):\n    """"""pow3\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n    alpha : float\n\n    Returns\n    -------\n    float\n        c - a * x**(-alpha)\n    """"""\n    return c - a * x**(-alpha)\n\n\nall_models[\'pow3\'] = pow3\nmodel_para[\'pow3\'] = [0.84, 0.52, 0.01]\nmodel_para_num[\'pow3\'] = 3\n\n\ndef linear(x, a, b):\n    """"""linear\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n\n    Returns\n    -------\n    float\n        a*x + b\n    """"""\n    return a*x + b\n\n\nall_models[\'linear\'] = linear\nmodel_para[\'linear\'] = [1., 0]\nmodel_para_num[\'linear\'] = 2\n\n\ndef logx_linear(x, a, b):\n    """"""logx linear\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n\n    Returns\n    -------\n    float\n        a * np.log(x) + b\n    """"""\n    x = np.log(x)\n    return a*x + b\n\n\nall_models[\'logx_linear\'] = logx_linear\nmodel_para[\'logx_linear\'] = [0.378106, 0.046506]\nmodel_para_num[\'logx_linear\'] = 2\n\n\ndef dr_hill_zero_background(x, theta, eta, kappa):\n    """"""dr hill zero background\n\n    Parameters\n    ----------\n    x : int\n    theta : float\n    eta : float\n    kappa : float\n\n    Returns\n    -------\n    float\n        (theta* x**eta) / (kappa**eta + x**eta)\n    """"""\n    return (theta * x**eta) / (kappa**eta + x**eta)\n\n\nall_models[\'dr_hill_zero_background\'] = dr_hill_zero_background\nmodel_para[\'dr_hill_zero_background\'] = [0.772320, 0.586449, 2.460843]\nmodel_para_num[\'dr_hill_zero_background\'] = 3\n\n\ndef log_power(x, a, b, c):\n    """"""""logistic power\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        a/(1.+(x/np.exp(b))**c)\n    """"""\n    return a/(1.+(x/np.exp(b))**c)\n\n\nall_models[\'log_power\'] = log_power\nmodel_para[\'log_power\'] = [0.77, 2.98, -0.51]\nmodel_para_num[\'log_power\'] = 3\n\n\ndef pow4(x, alpha, a, b, c):\n    """"""pow4\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    a : float\n    b : float\n    c : float\n\n    Returns\n    -------\n    float\n        c - (a*x+b)**-alpha\n    """"""\n    return c - (a*x+b)**-alpha\n\n\nall_models[\'pow4\'] = pow4\nmodel_para[\'pow4\'] = [0.1, 200, 0., 0.8]\nmodel_para_num[\'pow4\'] = 4\n\n\ndef mmf(x, alpha, beta, kappa, delta):\n    """"""Morgan-Mercer-Flodin\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    beta : float\n    kappa : float\n    delta : float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n    """"""\n    return alpha - (alpha - beta) / (1. + (kappa * x)**delta)\n\n\nall_models[\'mmf\'] = mmf\nmodel_para[\'mmf\'] = [0.7, 0.1, 0.01, 5]\nmodel_para_num[\'mmf\'] = 4\n\n\ndef exp4(x, c, a, b, alpha):\n    """"""exp4\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n    b : float\n    alpha : float\n\n    Returns\n    -------\n    float\n        c - np.exp(-a*(x**alpha)+b)\n    """"""\n    return c - np.exp(-a*(x**alpha)+b)\n\n\nall_models[\'exp4\'] = exp4\nmodel_para[\'exp4\'] = [0.7, 0.8, -0.8, 0.3]\nmodel_para_num[\'exp4\'] = 4\n\n\ndef ilog2(x, c, a):\n    """"""ilog2\n\n    Parameters\n    ----------\n    x : int\n    c : float\n    a : float\n\n    Returns\n    -------\n    float\n        c - a / np.log(x)\n    """"""\n    return c - a / np.log(x)\n\n\nall_models[\'ilog2\'] = ilog2\nmodel_para[\'ilog2\'] = [0.78, 0.43]\nmodel_para_num[\'ilog2\'] = 2\n\n\ndef weibull(x, alpha, beta, kappa, delta):\n    """"""Weibull model\n    http://www.pisces-conservation.com/growthhelp/index.html?morgan_mercer_floden.htm\n\n    Parameters\n    ----------\n    x : int\n    alpha : float\n    beta : float\n    kappa : float\n    delta : float\n\n    Returns\n    -------\n    float\n        alpha - (alpha - beta) * np.exp(-(kappa * x)**delta)\n    """"""\n    return alpha - (alpha - beta) * np.exp(-(kappa * x)**delta)\n\n\nall_models[\'weibull\'] = weibull\nmodel_para[\'weibull\'] = [0.7, 0.1, 0.01, 1]\nmodel_para_num[\'weibull\'] = 4\n\n\ndef janoschek(x, a, beta, k, delta):\n    """"""http://www.pisces-conservation.com/growthhelp/janoschek.htm\n\n    Parameters\n    ----------\n    x : int\n    a : float\n    beta : float\n    k : float\n    delta : float\n\n    Returns\n    -------\n    float\n        a - (a - beta) * np.exp(-k*x**delta)\n    """"""\n    return a - (a - beta) * np.exp(-k*x**delta)\n\n\nall_models[\'janoschek\'] = janoschek\nmodel_para[\'janoschek\'] = [0.73, 0.07, 0.355, 0.46]\nmodel_para_num[\'janoschek\'] = 4\n'"
src/sdk/pynni/nni/curvefitting_assessor/model_factory.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport numpy as np\nfrom scipy import optimize\nfrom .curvefunctions import *  # pylint: disable=wildcard-import,unused-wildcard-import\n\n# Number of curve functions we prepared, more details can be found in ""curvefunctions.py""\nNUM_OF_FUNCTIONS = 12\n# Number of simulation time when we do MCMC sampling\nNUM_OF_SIMULATION_TIME = 20\n# Number of samples we select when we do MCMC sampling\nNUM_OF_INSTANCE = 10\n# The step size of each noise when we do MCMC sampling\nSTEP_SIZE = 0.0005\n# Number of least fitting function, if effective function is lower than this number, we will ask for more information\nLEAST_FITTED_FUNCTION = 4\n\nlogger = logging.getLogger(\'curvefitting_Assessor\')\n\nclass CurveModel:\n    """"""Build a Curve Model to predict the performance\n\n    Algorithm: https://github.com/Microsoft/nni/blob/master/src/sdk/pynni/nni/curvefitting_assessor/README.md\n\n    Parameters\n    ----------\n    target_pos : int\n        The point we need to predict\n    """"""\n    def __init__(self, target_pos):\n        self.target_pos = target_pos\n        self.trial_history = []\n        self.point_num = 0\n        self.effective_model = []\n        self.effective_model_num = 0\n        self.weight_samples = []\n\n    def fit_theta(self):\n        """"""use least squares to fit all default curves parameter seperately\n\n        Returns\n        -------\n        None\n        """"""\n        x = range(1, self.point_num + 1)\n        y = self.trial_history\n        for i in range(NUM_OF_FUNCTIONS):\n            model = curve_combination_models[i]\n            try:\n                # The maximum number of iterations to fit is 100*(N+1), where N is the number of elements in `x0`.\n                if model_para_num[model] == 2:\n                    a, b = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                elif model_para_num[model] == 3:\n                    a, b, c = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                elif model_para_num[model] == 4:\n                    a, b, c, d = optimize.curve_fit(all_models[model], x, y)[0]\n                    model_para[model][0] = a\n                    model_para[model][1] = b\n                    model_para[model][2] = c\n                    model_para[model][3] = d\n            except (RuntimeError, FloatingPointError, OverflowError, ZeroDivisionError):\n                # Ignore exceptions caused by numerical calculations\n                pass\n            except Exception as exception:\n                logger.critical(""Exceptions in fit_theta: %s"", exception)\n\n    def filter_curve(self):\n        """"""filter the poor performing curve\n\n        Returns\n        -------\n        None\n        """"""\n        avg = np.sum(self.trial_history) / self.point_num\n        standard = avg * avg * self.point_num\n        predict_data = []\n        tmp_model = []\n        for i in range(NUM_OF_FUNCTIONS):\n            var = 0\n            model = curve_combination_models[i]\n            for j in range(1, self.point_num + 1):\n                y = self.predict_y(model, j)\n                var += (y - self.trial_history[j - 1]) * (y - self.trial_history[j - 1])\n            if var < standard:\n                predict_data.append(y)\n                tmp_model.append(curve_combination_models[i])\n        median = np.median(predict_data)\n        std = np.std(predict_data)\n        for model in tmp_model:\n            y = self.predict_y(model, self.target_pos)\n            epsilon = self.point_num / 10 * std\n            if y < median + epsilon and y > median - epsilon:\n                self.effective_model.append(model)\n        self.effective_model_num = len(self.effective_model)\n        logger.info(\'List of effective model: %s\', self.effective_model)\n\n    def predict_y(self, model, pos):\n        """"""return the predict y of \'model\' when epoch = pos\n\n        Parameters\n        ----------\n        model : string\n            name of the curve function model\n        pos : int\n            the epoch number of the position you want to predict\n\n        Returns\n        -------\n        int\n            The expected matrix at pos\n        """"""\n        if model_para_num[model] == 2:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1])\n        elif model_para_num[model] == 3:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2])\n        elif model_para_num[model] == 4:\n            y = all_models[model](pos, model_para[model][0], model_para[model][1], model_para[model][2], model_para[model][3])\n        return y\n\n    def f_comb(self, pos, sample):\n        """"""return the value of the f_comb when epoch = pos\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        int\n            The expected matrix at pos with all the active function\'s prediction\n        """"""\n        ret = 0\n        for i in range(self.effective_model_num):\n            model = self.effective_model[i]\n            y = self.predict_y(model, pos)\n            ret += sample[i] * y\n        return ret\n\n    def normalize_weight(self, samples):\n        """"""normalize weight\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it\'s a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        list\n            samples after normalize weight\n        """"""\n        for i in range(NUM_OF_INSTANCE):\n            total = 0\n            for j in range(self.effective_model_num):\n                total += samples[i][j]\n            for j in range(self.effective_model_num):\n                samples[i][j] /= total\n        return samples\n\n    def sigma_sq(self, sample):\n        """"""returns the value of sigma square, given the weight\'s sample\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of sigma square, given the weight\'s sample\n        """"""\n        ret = 0\n        for i in range(1, self.point_num + 1):\n            temp = self.trial_history[i - 1] - self.f_comb(i, sample)\n            ret += temp * temp\n        return 1.0 * ret / self.point_num\n\n    def normal_distribution(self, pos, sample):\n        """"""returns the value of normal distribution, given the weight\'s sample and target position\n\n        Parameters\n        ----------\n        pos : int\n            the epoch number of the position you want to predict\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            the value of normal distribution\n        """"""\n        curr_sigma_sq = self.sigma_sq(sample)\n        delta = self.trial_history[pos - 1] - self.f_comb(pos, sample)\n        return np.exp(np.square(delta) / (-2.0 * curr_sigma_sq)) / np.sqrt(2 * np.pi * np.sqrt(curr_sigma_sq))\n\n    def likelihood(self, samples):\n        """"""likelihood\n\n        Parameters\n        ----------\n        sample : list\n            sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n\n        Returns\n        -------\n        float\n            likelihood\n        """"""\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(1, self.point_num + 1):\n                ret[i] *= self.normal_distribution(j, samples[i])\n        return ret\n\n    def prior(self, samples):\n        """"""priori distribution\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it\'s a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            priori distribution\n        """"""\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            for j in range(self.effective_model_num):\n                if not samples[i][j] > 0:\n                    ret[i] = 0\n            if self.f_comb(1, samples[i]) >= self.f_comb(self.target_pos, samples[i]):\n                ret[i] = 0\n        return ret\n\n    def target_distribution(self, samples):\n        """"""posterior probability\n\n        Parameters\n        ----------\n        samples : list\n            a collection of sample, it\'s a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n\n        Returns\n        -------\n        float\n            posterior probability\n        """"""\n        curr_likelihood = self.likelihood(samples)\n        curr_prior = self.prior(samples)\n        ret = np.ones(NUM_OF_INSTANCE)\n        for i in range(NUM_OF_INSTANCE):\n            ret[i] = curr_likelihood[i] * curr_prior[i]\n        return ret\n\n    def mcmc_sampling(self):\n        """"""Adjust the weight of each function using mcmc sampling.\n        The initial value of each weight is evenly distribute.\n        Brief introduction:\n        (1)Definition of sample:\n            Sample is a (1 * NUM_OF_FUNCTIONS) matrix, representing{w1, w2, ... wk}\n        (2)Definition of samples:\n            Samples is a collection of sample, it\'s a (NUM_OF_INSTANCE * NUM_OF_FUNCTIONS) matrix,\n            representing{{w11, w12, ..., w1k}, {w21, w22, ... w2k}, ...{wk1, wk2,..., wkk}}\n        (3)Definition of model:\n            Model is the function we chose right now. Such as: \'wap\', \'weibull\'.\n        (4)Definition of pos:\n            Pos is the position we want to predict, corresponds to the value of epoch.\n\n        Returns\n        -------\n        None\n        """"""\n        init_weight = np.ones((self.effective_model_num), dtype=np.float) / self.effective_model_num\n        self.weight_samples = np.broadcast_to(init_weight, (NUM_OF_INSTANCE, self.effective_model_num))\n        for _ in range(NUM_OF_SIMULATION_TIME):\n            # sample new value from Q(i, j)\n            new_values = np.random.randn(NUM_OF_INSTANCE, self.effective_model_num) * STEP_SIZE + self.weight_samples\n            new_values = self.normalize_weight(new_values)\n            # compute alpha(i, j) = min{1, P(j)Q(j, i)/P(i)Q(i, j)}\n            alpha = np.minimum(1, self.target_distribution(new_values) / self.target_distribution(self.weight_samples))\n            # sample u\n            u = np.random.rand(NUM_OF_INSTANCE)\n            # new value\n            change_value_flag = (u < alpha).astype(np.int)\n            for j in range(NUM_OF_INSTANCE):\n                new_values[j] = self.weight_samples[j] * (1 - change_value_flag[j]) + new_values[j] * change_value_flag[j]\n            self.weight_samples = new_values\n\n    def predict(self, trial_history):\n        """"""predict the value of target position\n\n        Parameters\n        ----------\n        trial_history : list\n            The history performance matrix of each trial.\n\n        Returns\n        -------\n        float\n            expected final result performance of this hyperparameter config\n        """"""\n        self.trial_history = trial_history\n        self.point_num = len(trial_history)\n        self.fit_theta()\n        self.filter_curve()\n        if self.effective_model_num < LEAST_FITTED_FUNCTION:\n            # different curve\'s predictions are too scattered, requires more information\n            return None\n        self.mcmc_sampling()\n        ret = 0\n        for i in range(NUM_OF_INSTANCE):\n            ret += self.f_comb(self.target_pos, self.weight_samples[i])\n        return ret / NUM_OF_INSTANCE\n'"
src/sdk/pynni/nni/evolution_tuner/__init__.py,0,b''
src/sdk/pynni/nni/evolution_tuner/evolution_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nevolution_tuner.py\n""""""\n\nimport copy\nimport random\n\nimport numpy as np\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward, split_index, json2parameter, json2space\n\n\nclass Individual:\n    """"""\n    Indicidual class to store the indv info.\n\n    Attributes\n    ----------\n    config : str\n        Search space.\n    info : str\n        The str to save information of individual.\n    result : float\n        The final metric of a individual.\n    store_dir : str\n    save_dir : str\n    """"""\n\n    def __init__(self, config=None, info=None, result=None, save_dir=None):\n        """"""\n        Parameters\n        ----------\n        config : str\n            A config to represent a group of parameters.\n        info : str\n        result : float\n        save_dir : str\n        """"""\n        self.config = config\n        self.result = result\n        self.info = info\n        self.restore_dir = None\n        self.save_dir = save_dir\n\n    def __str__(self):\n        return ""info: "" + str(self.info) + \\\n            "", config :"" + str(self.config) + "", result: "" + str(self.result)\n\n    def mutation(self, config=None, info=None, save_dir=None):\n        """"""\n        Mutation by reset state information.\n\n        Parameters\n        ----------\n        config : str\n        info : str\n        save_dir : str\n        """"""\n        self.result = None\n        self.config = config\n        self.restore_dir = self.save_dir\n        self.save_dir = save_dir\n        self.info = info\n\n\nclass EvolutionTuner(Tuner):\n    """"""\n    EvolutionTuner is tuner using navie evolution algorithm.\n    """"""\n\n    def __init__(self, optimize_mode=""maximize"", population_size=32):\n        """"""\n        Parameters\n        ----------\n        optimize_mode : str, default \'maximize\'\n        population_size : int\n            initial population size. The larger population size,\n        the better evolution performance.\n        """"""\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.population_size = population_size\n\n        self.trial_result = []\n        self.searchspace_json = None\n        self.total_data = {}\n        self.random_state = None\n        self.population = None\n        self.space = None\n\n\n    def update_search_space(self, search_space):\n        """"""\n        Update search space.\n\n        Search_space contains the information that user pre-defined.\n\n        Parameters\n        ----------\n        search_space : dict\n        """"""\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for _ in range(self.population_size):\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            self.population.append(Individual(config=config))\n\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        This function will returns a dict of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        dict\n            A group of candaidte parameters that evolution tuner generated.\n        """"""\n        if not self.population:\n            raise RuntimeError(\'The population is empty\')\n\n        pos = -1\n\n        for i in range(len(self.population)):\n            if self.population[i].result is None:\n                pos = i\n                break\n\n        if pos != -1:\n            indiv = copy.deepcopy(self.population[pos])\n            self.population.pop(pos)\n            total_config = indiv.config\n        else:\n            random.shuffle(self.population)\n            if self.population[0].result < self.population[1].result:\n                self.population[0] = self.population[1]\n\n            # mutation\n            space = json2space(self.searchspace_json,\n                               self.population[0].config)\n            is_rand = dict()\n            mutation_pos = space[random.randint(0, len(space)-1)]\n\n            for i in range(len(self.space)):\n                is_rand[self.space[i]] = (self.space[i] == mutation_pos)\n            config = json2parameter(\n                self.searchspace_json, is_rand, self.random_state, self.population[0].config)\n            self.population.pop(1)\n            # remove ""_index"" from config and save params-id\n\n            total_config = config\n\n        self.total_data[parameter_id] = total_config\n        config = split_index(total_config)\n\n        return config\n\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Record the result from a trial\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have ""default"" key.\n            value is final metrics of the trial.\n        """"""\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\'Received parameter_id not in total_data.\')\n        # restore the paramsters contains ""_index""\n        params = self.total_data[parameter_id]\n\n        if self.optimize_mode == OptimizeMode.Minimize:\n            reward = -reward\n\n        indiv = Individual(config=params, result=reward)\n        self.population.append(indiv)\n\n    def import_data(self, data):\n        pass\n'"
src/sdk/pynni/nni/feature_engineering/__init__.py,0,b'\n'
src/sdk/pynni/nni/feature_engineering/feature_selector.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\nimport logging\n\n_logger = logging.getLogger(__name__)\n\n\nclass FeatureSelector():\n\n    def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None\n\n\n    def fit(self, X, y, **kwargs):\n        """"""\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y: array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        """"""\n        self.X = X\n        self.y = y\n\n\n    def get_selected_features(self):\n        """"""\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        """"""\n        return self.selected_features_\n'"
src/sdk/pynni/nni/gp_tuner/__init__.py,0,b''
src/sdk/pynni/nni/gp_tuner/gp_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nGPTuner is a Bayesian Optimization method where Gaussian Process is used for modeling loss functions.\n\nSee :class:`GPTuner` for details.\n""""""\n\nimport warnings\nimport logging\nimport numpy as np\n\nfrom sklearn.gaussian_process.kernels import Matern\nfrom sklearn.gaussian_process import GaussianProcessRegressor\n\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward\n\nfrom .target_space import TargetSpace\nfrom .util import UtilityFunction, acq_max\n\nlogger = logging.getLogger(""GP_Tuner_AutoML"")\n\n\nclass GPTuner(Tuner):\n    """"""\n    GPTuner is a Bayesian Optimization method where Gaussian Process is used for modeling loss functions.\n\n    Parameters\n    ----------\n    optimize_mode : str\n        optimize mode, \'maximize\' or \'minimize\', by default \'maximize\'\n    utility : str\n        utility function (also called \'acquisition funcition\') to use, which can be \'ei\', \'ucb\' or \'poi\'. By default \'ei\'.\n    kappa : float\n        value used by utility function \'ucb\'. The bigger kappa is, the more the tuner will be exploratory. By default 5.\n    xi : float\n        used by utility function \'ei\' and \'poi\'. The bigger xi is, the more the tuner will be exploratory. By default 0.\n    nu : float\n        used to specify Matern kernel. The smaller nu, the less smooth the approximated function is. By default 2.5.\n    alpha : float\n        Used to specify Gaussian Process Regressor. Larger values correspond to increased noise level in the observations.\n        By default 1e-6.\n    cold_start_num : int\n        Number of random exploration to perform before Gaussian Process. By default 10.\n    selection_num_warm_up : int\n        Number of random points to evaluate for getting the point which maximizes the acquisition function. By default 100000\n    selection_num_starting_points : int\n        Number of times to run L-BFGS-B from a random starting point after the warmup. By default 250.\n    """"""\n\n    def __init__(self, optimize_mode=""maximize"", utility=\'ei\', kappa=5, xi=0, nu=2.5, alpha=1e-6, cold_start_num=10,\n                 selection_num_warm_up=100000, selection_num_starting_points=250):\n        self._optimize_mode = OptimizeMode(optimize_mode)\n\n        # utility function related\n        self._utility = utility\n        self._kappa = kappa\n        self._xi = xi\n\n        # target space\n        self._space = None\n\n        self._random_state = np.random.RandomState()\n\n        # nu, alpha are GPR related params\n        self._gp = GaussianProcessRegressor(\n            kernel=Matern(nu=nu),\n            alpha=alpha,\n            normalize_y=True,\n            n_restarts_optimizer=25,\n            random_state=self._random_state\n        )\n        # num of random evaluations before GPR\n        self._cold_start_num = cold_start_num\n\n        # params for acq_max\n        self._selection_num_warm_up = selection_num_warm_up\n        self._selection_num_starting_points = selection_num_starting_points\n\n        # num of imported data\n        self._supplement_data_num = 0\n\n    def update_search_space(self, search_space):\n        """"""\n        Update the self.bounds and self.types by the search_space.json file.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        """"""\n        self._space = TargetSpace(search_space, self._random_state)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Method which provides one set of hyper-parameters.\n        If the number of trial result is lower than cold_start_number, GPTuner will first randomly generate some parameters.\n        Otherwise, choose the parameters by the Gussian Process Model.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        """"""\n        if self._space.len() < self._cold_start_num:\n            results = self._space.random_sample()\n        else:\n            # Sklearn\'s GP throws a large number of warnings at times, but\n            # we don\'t really need to see them here.\n            with warnings.catch_warnings():\n                warnings.simplefilter(""ignore"")\n                self._gp.fit(self._space.params, self._space.target)\n\n            util = UtilityFunction(\n                kind=self._utility, kappa=self._kappa, xi=self._xi)\n\n            results = acq_max(\n                f_acq=util.utility,\n                gp=self._gp,\n                y_max=self._space.target.max(),\n                bounds=self._space.bounds,\n                space=self._space,\n                num_warmup=self._selection_num_warm_up,\n                num_starting_points=self._selection_num_starting_points\n            )\n\n        results = self._space.array_to_params(results)\n        logger.info(""Generate paramageters:\\n %s"", results)\n        return results\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Method invoked when a trial reports its final result.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        """"""\n        value = extract_scalar_reward(value)\n        if self._optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        logger.info(""Received trial result."")\n        logger.info(""value :%s"", value)\n        logger.info(""parameter : %s"", parameters)\n        self._space.register(parameters, value)\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning.\n\n        Override of the abstract method in :class:`~nni.tuner.Tuner`.\n        """"""\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(\n                ""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                logger.info(\n                    ""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            self._supplement_data_num += 1\n            _parameter_id = \'_\'.join(\n                [""ImportData"", str(self._supplement_data_num)])\n            self.receive_trial_result(\n                parameter_id=_parameter_id, parameters=_params, value=_value)\n        logger.info(""Successfully import data to GP tuner."")\n'"
src/sdk/pynni/nni/gp_tuner/target_space.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nTool class to hold the param-space coordinates (X) and target values (Y).\n""""""\n\nimport numpy as np\nimport nni.parameter_expressions as parameter_expressions\n\n\ndef _hashable(params):\n    """"""\n    Transform list params to tuple format. Ensure that an point is hashable by a python dict.\n\n    Parameters\n    ----------\n    params : numpy array\n        array format of parameters\n\n    Returns\n    -------\n    tuple\n        tuple format of parameters\n    """"""\n    return tuple(map(float, params))\n\n\nclass TargetSpace():\n    """"""\n    Holds the param-space coordinates (X) and target values (Y)\n\n    Parameters\n    ----------\n    pbounds : dict\n        Dictionary with parameters names and legal values.\n\n    random_state : int, RandomState, or None\n        optionally specify a seed for a random number generator, by default None.\n    """"""\n\n    def __init__(self, pbounds, random_state=None):\n        self._random_state = random_state\n\n        # Get the name of the parameters\n        self._keys = sorted(pbounds)\n\n        # Create an array with parameters bounds\n        self._bounds = np.array(\n            [item[1] for item in sorted(pbounds.items(), key=lambda x: x[0])]\n        )\n\n        # check values type\n        for _bound in self._bounds:\n            if _bound[\'_type\'] == \'choice\':\n                try:\n                    [float(val) for val in _bound[\'_value\']]\n                except ValueError:\n                    raise ValueError(""GP Tuner supports only numerical values"")\n\n        # preallocated memory for X and Y points\n        self._params = np.empty(shape=(0, self.dim))\n        self._target = np.empty(shape=(0))\n\n        # keep track of unique points we have seen so far\n        self._cache = {}\n\n    def __contains__(self, params):\n        """"""\n        check if a parameter is already registered\n\n        Parameters\n        ----------\n        params : numpy array\n\n        Returns\n        -------\n        bool\n            True if the parameter is already registered, else false\n        """"""\n        return _hashable(params) in self._cache\n\n    def len(self):\n        """"""\n        length of registered params and targets\n\n        Returns\n        -------\n        int\n        """"""\n        assert len(self._params) == len(self._target)\n        return len(self._target)\n\n    @property\n    def params(self):\n        """"""\n        registered parameters\n\n        Returns\n        -------\n        numpy array\n        """"""\n        return self._params\n\n    @property\n    def target(self):\n        """"""\n        registered target values\n\n        Returns\n        -------\n        numpy array\n        """"""\n        return self._target\n\n    @property\n    def dim(self):\n        """"""\n        dimension of parameters\n\n        Returns\n        -------\n        int\n        """"""\n        return len(self._keys)\n\n    @property\n    def keys(self):\n        """"""\n        keys of parameters\n\n        Returns\n        -------\n        numpy array\n        """"""\n        return self._keys\n\n    @property\n    def bounds(self):\n        """"""\n        bounds of parameters\n\n        Returns\n        -------\n        numpy array\n        """"""\n        return self._bounds\n\n    def params_to_array(self, params):\n        """"""\n        dict to array\n\n        Parameters\n        ----------\n        params : dict\n            dict format of parameters\n\n        Returns\n        -------\n        numpy array\n            array format of parameters\n        """"""\n        try:\n            assert set(params) == set(self.keys)\n        except AssertionError:\n            raise ValueError(\n                ""Parameters\' keys ({}) do "".format(sorted(params)) +\n                ""not match the expected set of keys ({})."".format(self.keys)\n            )\n        return np.asarray([params[key] for key in self.keys])\n\n    def array_to_params(self, x):\n        """"""\n        array to dict\n\n        maintain int type if the paramters is defined as int in search_space.json\n        Parameters\n        ----------\n        x : numpy array\n            array format of parameters\n\n        Returns\n        -------\n        dict\n            dict format of parameters\n        """"""\n        try:\n            assert len(x) == len(self.keys)\n        except AssertionError:\n            raise ValueError(\n                ""Size of array ({}) is different than the "".format(len(x)) +\n                ""expected number of parameters ({})."".format(self.dim)\n            )\n\n        params = {}\n        for i, _bound in enumerate(self._bounds):\n            if _bound[\'_type\'] == \'choice\' and all(isinstance(val, int) for val in _bound[\'_value\']):\n                params.update({self.keys[i]: int(x[i])})\n            elif _bound[\'_type\'] in [\'randint\']:\n                params.update({self.keys[i]: int(x[i])})\n            else:\n                params.update({self.keys[i]:  x[i]})\n\n        return params\n\n    def register(self, params, target):\n        """"""\n        Append a point and its target value to the known data.\n\n        Parameters\n        ----------\n        params : dict\n            parameters\n\n        target : float\n            target function value\n        """"""\n\n        x = self.params_to_array(params)\n        if x in self:\n            print(\'Data point {} is not unique\'.format(x))\n\n        # Insert data into unique dictionary\n        self._cache[_hashable(x.ravel())] = target\n\n        self._params = np.concatenate([self._params, x.reshape(1, -1)])\n        self._target = np.concatenate([self._target, [target]])\n\n    def random_sample(self):\n        """"""\n        Creates a random point within the bounds of the space.\n\n        Returns\n        -------\n        numpy array\n            one groupe of parameter\n        """"""\n        params = np.empty(self.dim)\n        for col, _bound in enumerate(self._bounds):\n            if _bound[\'_type\'] == \'choice\':\n                params[col] = parameter_expressions.choice(\n                    _bound[\'_value\'], self._random_state)\n            elif _bound[\'_type\'] == \'randint\':\n                params[col] = self._random_state.randint(\n                    _bound[\'_value\'][0], _bound[\'_value\'][1], size=1)\n            elif _bound[\'_type\'] == \'uniform\':\n                params[col] = parameter_expressions.uniform(\n                    _bound[\'_value\'][0], _bound[\'_value\'][1], self._random_state)\n            elif _bound[\'_type\'] == \'quniform\':\n                params[col] = parameter_expressions.quniform(\n                    _bound[\'_value\'][0], _bound[\'_value\'][1], _bound[\'_value\'][2], self._random_state)\n            elif _bound[\'_type\'] == \'loguniform\':\n                params[col] = parameter_expressions.loguniform(\n                    _bound[\'_value\'][0], _bound[\'_value\'][1], self._random_state)\n            elif _bound[\'_type\'] == \'qloguniform\':\n                params[col] = parameter_expressions.qloguniform(\n                    _bound[\'_value\'][0], _bound[\'_value\'][1], _bound[\'_value\'][2], self._random_state)\n\n        return params\n\n    def max(self):\n        """"""\n        Get maximum target value found and its corresponding parameters.\n\n        Returns\n        -------\n        dict\n            target value and parameters, empty dict if nothing registered\n        """"""\n        try:\n            res = {\n                \'target\': self.target.max(),\n                \'params\': dict(\n                    zip(self.keys, self.params[self.target.argmax()])\n                )\n            }\n        except ValueError:\n            res = {}\n        return res\n\n    def res(self):\n        """"""\n        Get all target values found and corresponding parameters.\n\n        Returns\n        -------\n        list\n            a list of target values and their corresponding parameters\n        """"""\n        params = [dict(zip(self.keys, p)) for p in self.params]\n\n        return [\n            {""target"": target, ""params"": param}\n            for target, param in zip(self.target, params)\n        ]\n'"
src/sdk/pynni/nni/gp_tuner/util.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nutility functions and classes for GPTuner\n""""""\n\nimport warnings\nimport numpy as np\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\n\ndef _match_val_type(vals, bounds):\n    """"""\n    Update values in the array, to match their corresponding type, make sure the value is legal.\n\n    Parameters\n    ----------\n    vals : numpy array\n        values of parameters\n    bounds : numpy array\n        list of dictionary which stores parameters names and legal values.\n\n    Returns\n    -------\n    vals_new : list\n        The closest legal value to the original value\n    """"""\n    vals_new = []\n\n    for i, bound in enumerate(bounds):\n        _type = bound[\'_type\']\n        if _type == ""choice"":\n            # Find the closest integer in the array, vals_bounds\n            # pylint: disable=cell-var-from-loop\n            vals_new.append(min(bound[\'_value\'], key=lambda x: abs(x - vals[i])))\n        elif _type in [\'quniform\', \'randint\']:\n            vals_new.append(np.around(vals[i]))\n        else:\n            vals_new.append(vals[i])\n\n    return vals_new\n\n\ndef acq_max(f_acq, gp, y_max, bounds, space, num_warmup, num_starting_points):\n    """"""\n    A function to find the maximum of the acquisition function\n\n    It uses a combination of random sampling (cheap) and the \'L-BFGS-B\'\n    optimization method. First by sampling ``num_warmup`` points at random,\n    and then running L-BFGS-B from ``num_starting_points`` random starting points.\n\n    Parameters\n    ----------\n    f_acq : UtilityFunction.utility\n        The acquisition function object that return its point-wise value.\n\n    gp : GaussianProcessRegressor\n        A gaussian process fitted to the relevant data.\n\n    y_max : float\n        The current maximum known value of the target function.\n\n    bounds : numpy array\n        The variables bounds to limit the search of the acq max.\n\n    num_warmup : int\n        number of times to randomly sample the aquisition function\n\n    num_starting_points : int\n        number of times to run scipy.minimize\n\n    Returns\n    -------\n    numpy array\n        The parameter which achieves max of the acquisition function.\n    """"""\n\n    # Warm up with random points\n    x_tries = [space.random_sample()\n               for _ in range(int(num_warmup))]\n    ys = f_acq(x_tries, gp=gp, y_max=y_max)\n    x_max = x_tries[ys.argmax()]\n    max_acq = ys.max()\n\n\n    # Explore the parameter space more throughly\n    x_seeds = [space.random_sample() for _ in range(int(num_starting_points))]\n\n    bounds_minmax = np.array(\n        [[bound[\'_value\'][0], bound[\'_value\'][-1]] for bound in bounds])\n\n    for x_try in x_seeds:\n        # Find the minimum of minus the acquisition function\n        res = minimize(lambda x: -f_acq(x.reshape(1, -1), gp=gp, y_max=y_max),\n                       x_try.reshape(1, -1),\n                       bounds=bounds_minmax,\n                       method=""L-BFGS-B"")\n\n        # See if success\n        if not res.success:\n            continue\n\n        # Store it if better than previous minimum(maximum).\n        if max_acq is None or -res.fun[0] >= max_acq:\n            x_max = _match_val_type(res.x, bounds)\n            max_acq = -res.fun[0]\n\n    # Clip output to make sure it lies within the bounds. Due to floating\n    # point technicalities this is not always the case.\n    return np.clip(x_max, bounds_minmax[:, 0], bounds_minmax[:, 1])\n\n\nclass UtilityFunction():\n    """"""\n    A class to compute different acquisition function values.\n\n    Parameters\n    ----------\n    kind : string\n        specification of utility function to use\n    kappa : float\n        parameter usedd for \'ucb\' acquisition function\n    xi : float\n        parameter usedd for \'ei\' and \'poi\' acquisition function\n    """"""\n\n    def __init__(self, kind, kappa, xi):\n        self._kappa = kappa\n        self._xi = xi\n\n        if kind not in [\'ucb\', \'ei\', \'poi\']:\n            err = ""The utility function "" \\\n                ""{} has not been implemented, "" \\\n                ""please choose one of ucb, ei, or poi."".format(kind)\n            raise NotImplementedError(err)\n        self._kind = kind\n\n    def utility(self, x, gp, y_max):\n        """"""\n        return utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n\n        Returns\n        -------\n        function\n            return corresponding function, return None if parameter is illegal\n        """"""\n        if self._kind == \'ucb\':\n            return self._ucb(x, gp, self._kappa)\n        if self._kind == \'ei\':\n            return self._ei(x, gp, y_max, self._xi)\n        if self._kind == \'poi\':\n            return self._poi(x, gp, y_max, self._xi)\n        return None\n\n    @staticmethod\n    def _ucb(x, gp, kappa):\n        """"""\n        Upper Confidence Bound (UCB) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        kappa : float\n\n        Returns\n        -------\n        float\n        """"""\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            mean, std = gp.predict(x, return_std=True)\n\n        return mean + kappa * std\n\n    @staticmethod\n    def _ei(x, gp, y_max, xi):\n        """"""\n        Expected Improvement (EI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        """"""\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return (mean - y_max - xi) * norm.cdf(z) + std * norm.pdf(z)\n\n    @staticmethod\n    def _poi(x, gp, y_max, xi):\n        """"""\n        Possibility Of Improvement (POI) utility function\n\n        Parameters\n        ----------\n        x : numpy array\n            parameters\n        gp : GaussianProcessRegressor\n        y_max : float\n            maximum target value observed so far\n        xi : float\n\n        Returns\n        -------\n        float\n        """"""\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n            mean, std = gp.predict(x, return_std=True)\n\n        z = (mean - y_max - xi)/std\n        return norm.cdf(z)\n'"
src/sdk/pynni/nni/gridsearch_tuner/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .gridsearch_tuner import GridSearchTuner\n'
src/sdk/pynni/nni/gridsearch_tuner/gridsearch_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\ngridsearch_tuner.py including:\n    class GridSearchTuner\n""""""\n\nimport copy\nimport logging\nimport numpy as np\n\nimport nni\nfrom nni.tuner import Tuner\nfrom nni.utils import convert_dict2tuple\n\nTYPE = \'_type\'\nCHOICE = \'choice\'\nVALUE = \'_value\'\n\nlogger = logging.getLogger(\'grid_search_AutoML\')\n\nclass GridSearchTuner(Tuner):\n    """"""\n    GridSearchTuner will search all the possible configures that the user define in the searchSpace.\n    The only acceptable types of search space are ``choice``, ``quniform``, ``randint``\n\n    Type ``choice`` will select one of the options. Note that it can also be nested.\n\n    Type ``quniform`` will receive three values [``low``, ``high``, ``q``],\n    where [``low``, ``high``] specifies a range and ``q`` specifies the interval.\n    It will be sampled in a way that the first sampled value is ``low``,\n    and each of the following values is \'interval\' larger than the value in front of it.\n\n    Type ``randint`` gives all possible intergers in range[``low``, ``high``). Note that ``high`` is not included.\n    """"""\n\n    def __init__(self):\n        self.count = -1\n        self.expanded_search_space = []\n        self.supplement_data = dict()\n\n    def _json2parameter(self, ss_spec):\n        """"""\n        Generate all possible configs for hyperparameters from hyperparameter space.\n\n        Parameters\n        ----------\n        ss_spec : dict or list\n            Hyperparameter space or the ``_value`` of a hyperparameter\n\n        Returns\n        -------\n        list or dict\n            All the candidate choices of hyperparameters. for a hyperparameter, chosen_params\n            is a list. for multiple hyperparameters (e.g., search space), chosen_params is a dict.\n        """"""\n        if isinstance(ss_spec, dict):\n            if \'_type\' in ss_spec.keys():\n                _type = ss_spec[\'_type\']\n                _value = ss_spec[\'_value\']\n                chosen_params = list()\n                if _type == \'choice\':\n                    for value in _value:\n                        choice = self._json2parameter(value)\n                        if isinstance(choice, list):\n                            chosen_params.extend(choice)\n                        else:\n                            chosen_params.append(choice)\n                elif _type == \'quniform\':\n                    chosen_params = self._parse_quniform(_value)\n                elif _type == \'randint\':\n                    chosen_params = self._parse_randint(_value)\n                else:\n                    raise RuntimeError(""Not supported type: %s"" % _type)\n            else:\n                chosen_params = dict()\n                for key in ss_spec.keys():\n                    chosen_params[key] = self._json2parameter(ss_spec[key])\n                return self._expand_parameters(chosen_params)\n        elif isinstance(ss_spec, list):\n            chosen_params = list()\n            for subspec in ss_spec[1:]:\n                choice = self._json2parameter(subspec)\n                if isinstance(choice, list):\n                    chosen_params.extend(choice)\n                else:\n                    chosen_params.append(choice)\n            chosen_params = list(map(lambda v: {ss_spec[0]: v}, chosen_params))\n        else:\n            chosen_params = copy.deepcopy(ss_spec)\n        return chosen_params\n\n    def _parse_quniform(self, param_value):\n        """"""\n        Parse type of quniform parameter and return a list\n        """"""\n        low, high, q = param_value[0], param_value[1], param_value[2]\n        return np.clip(np.arange(np.round(low/q), np.round(high/q)+1) * q, low, high)\n\n    def _parse_randint(self, param_value):\n        """"""\n        Parse type of randint parameter and return a list\n        """"""\n        if param_value[0] >= param_value[1]:\n            raise ValueError(""Randint should contain at least 1 candidate, but [%s, %s) contains none."",\n                             param_value[0], param_value[1])\n        return np.arange(param_value[0], param_value[1]).tolist()\n\n    def _expand_parameters(self, para):\n        """"""\n        Enumerate all possible combinations of all parameters\n\n        Parameters\n        ----------\n        para : dict\n            {key1: [v11, v12, ...], key2: [v21, v22, ...], ...}\n\n        Returns\n        -------\n        dict\n            {{key1: v11, key2: v21, ...}, {key1: v11, key2: v22, ...}, ...}\n        """"""\n        if len(para) == 1:\n            for key, values in para.items():\n                return list(map(lambda v: {key: v}, values))\n\n        key = list(para)[0]\n        values = para.pop(key)\n        rest_para = self._expand_parameters(para)\n        ret_para = list()\n        for val in values:\n            for config in rest_para:\n                config[key] = val\n                ret_para.append(copy.deepcopy(config))\n        return ret_para\n\n    def update_search_space(self, search_space):\n        """"""\n        Check if the search space is valid and expand it: support only ``choice``, ``quniform``, ``randint``.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        """"""\n        self.expanded_search_space = self._json2parameter(search_space)\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Generate parameters for one trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id for the generated hyperparameter\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One configuration from the expanded search space.\n\n        Raises\n        ------\n        NoMoreTrialError\n            If all the configurations has been sent, raise :class:`~nni.NoMoreTrialError`.\n        """"""\n        self.count += 1\n        while self.count <= len(self.expanded_search_space) - 1:\n            _params_tuple = convert_dict2tuple(self.expanded_search_space[self.count])\n            if _params_tuple in self.supplement_data:\n                self.count += 1\n            else:\n                return self.expanded_search_space[self.count]\n        raise nni.NoMoreTrialError(\'no more parameters now.\')\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Receive a trial\'s final performance result reported through :func:`~nni.report_final_result` by the trial.\n        GridSearchTuner does not need trial\'s results.\n        """"""\n        pass\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        """"""\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            _params_tuple = convert_dict2tuple(_params)\n            self.supplement_data[_params_tuple] = True\n        logger.info(""Successfully import data to grid search tuner."")\n'"
src/sdk/pynni/nni/hyperband_advisor/__init__.py,0,b''
src/sdk/pynni/nni/hyperband_advisor/hyperband_advisor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nhyperband_advisor.py\n""""""\n\nimport copy\nimport logging\nimport math\nimport sys\n\nimport json_tricks\nimport numpy as np\nfrom nni.common import multi_phase_enabled\nfrom nni.msg_dispatcher_base import MsgDispatcherBase\nfrom nni.protocol import CommandType, send\nfrom nni.utils import NodeType, OptimizeMode, MetricType, extract_scalar_reward\nfrom nni import parameter_expressions\n\n_logger = logging.getLogger(__name__)\n\n_next_parameter_id = 0\n_KEY = \'TRIAL_BUDGET\'\n_epsilon = 1e-6\n\n\ndef create_parameter_id():\n    """"""Create an id\n\n    Returns\n    -------\n    int\n        parameter id\n    """"""\n    global _next_parameter_id\n    _next_parameter_id += 1\n    return _next_parameter_id - 1\n\n\ndef create_bracket_parameter_id(brackets_id, brackets_curr_decay, increased_id=-1):\n    """"""Create a full id for a specific bracket\'s hyperparameter configuration\n\n    Parameters\n    ----------\n    brackets_id: int\n        brackets id\n    brackets_curr_decay:\n        brackets curr decay\n    increased_id: int\n        increased id\n\n    Returns\n    -------\n    int\n        params id\n    """"""\n    if increased_id == -1:\n        increased_id = str(create_parameter_id())\n    params_id = \'_\'.join([str(brackets_id),\n                          str(brackets_curr_decay),\n                          increased_id])\n    return params_id\n\n\ndef json2parameter(ss_spec, random_state):\n    """"""Randomly generate values for hyperparameters from hyperparameter space i.e., x.\n\n    Parameters\n    ----------\n    ss_spec:\n        hyperparameter space\n    random_state:\n        random operator to generate random values\n\n    Returns\n    -------\n    Parameter:\n        Parameters in this experiment\n    """"""\n    if isinstance(ss_spec, dict):\n        if NodeType.TYPE in ss_spec.keys():\n            _type = ss_spec[NodeType.TYPE]\n            _value = ss_spec[NodeType.VALUE]\n            if _type == \'choice\':\n                _index = random_state.randint(len(_value))\n                chosen_params = json2parameter(ss_spec[NodeType.VALUE][_index], random_state)\n            else:\n                chosen_params = getattr(parameter_expressions, _type)(*(_value + [random_state]))\n        else:\n            chosen_params = dict()\n            for key in ss_spec.keys():\n                chosen_params[key] = json2parameter(ss_spec[key], random_state)\n    elif isinstance(ss_spec, list):\n        chosen_params = list()\n        for _, subspec in enumerate(ss_spec):\n            chosen_params.append(json2parameter(subspec, random_state))\n    else:\n        chosen_params = copy.deepcopy(ss_spec)\n    return chosen_params\n\n\nclass Bracket():\n    """"""A bracket in Hyperband, all the information of a bracket is managed by an instance of this class\n\n    Parameters\n    ----------\n    s: int\n        The current SH iteration index.\n    s_max: int\n        total number of SH iterations\n    eta: float\n        In each iteration, a complete run of sequential halving is executed. In it,\n\t\tafter evaluating each configuration on the same subset size, only a fraction of\n\t\t1/eta of them \'advances\' to the next round.\n    R:\n        the budget associated with each stage\n    optimize_mode: str\n        optimize mode, \'maximize\' or \'minimize\'\n    """"""\n\n    def __init__(self, s, s_max, eta, R, optimize_mode):\n        self.bracket_id = s\n        self.s_max = s_max\n        self.eta = eta\n        self.n = math.ceil((s_max + 1) * (eta ** s) / (s + 1) - _epsilon)\n        self.r = R / eta ** s\n        self.i = 0\n        self.hyper_configs = []  # [ {id: params}, {}, ... ]\n        self.configs_perf = []  # [ {id: [seq, acc]}, {}, ... ]\n        self.num_configs_to_run = []  # [ n, n, n, ... ]\n        self.num_finished_configs = []  # [ n, n, n, ... ]\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.no_more_trial = False\n\n    def is_completed(self):\n        """"""check whether this bracket has sent out all the hyperparameter configurations""""""\n        return self.no_more_trial\n\n    def get_n_r(self):\n        """"""return the values of n and r for the next round""""""\n        return math.floor(self.n / self.eta ** self.i + _epsilon), math.floor(self.r * self.eta ** self.i + _epsilon)\n\n    def increase_i(self):\n        """"""i means the ith round. Increase i by 1""""""\n        self.i += 1\n        if self.i > self.bracket_id:\n            self.no_more_trial = True\n\n    def set_config_perf(self, i, parameter_id, seq, value):\n        """"""update trial\'s latest result with its sequence number, e.g., epoch number or batch number\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        parameter_id: int\n            the id of the trial/parameter\n        seq: int\n            sequence number, e.g., epoch number or batch number\n        value: int\n            latest result with sequence number seq\n\n        Returns\n        -------\n        None\n        """"""\n        if parameter_id in self.configs_perf[i]:\n            if self.configs_perf[i][parameter_id][0] < seq:\n                self.configs_perf[i][parameter_id] = [seq, value]\n        else:\n            self.configs_perf[i][parameter_id] = [seq, value]\n\n    def inform_trial_end(self, i):\n        """"""If the trial is finished and the corresponding round (i.e., i) has all its trials finished,\n        it will choose the top k trials for the next round (i.e., i+1)\n\n        Parameters\n        ----------\n        i: int\n            the ith round\n        """"""\n        global _KEY\n        self.num_finished_configs[i] += 1\n        _logger.debug(\'bracket id: %d, round: %d %d, finished: %d, all: %d\', self.bracket_id, self.i, i,\n                      self.num_finished_configs[i], self.num_configs_to_run[i])\n        if self.num_finished_configs[i] >= self.num_configs_to_run[i] \\\n                and self.no_more_trial is False:\n            # choose candidate configs from finished configs to run in the next round\n            assert self.i == i + 1\n            this_round_perf = self.configs_perf[i]\n            if self.optimize_mode is OptimizeMode.Maximize:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1], reverse=True)  # reverse\n            else:\n                sorted_perf = sorted(this_round_perf.items(), key=lambda kv: kv[1][1])\n            _logger.debug(\'bracket %s next round %s, sorted hyper configs: %s\', self.bracket_id, self.i, sorted_perf)\n            next_n, next_r = self.get_n_r()\n            _logger.debug(\'bracket %s next round %s, next_n=%d, next_r=%d\', self.bracket_id, self.i, next_n, next_r)\n            hyper_configs = dict()\n            for k in range(next_n):\n                params_id = sorted_perf[k][0]\n                params = self.hyper_configs[i][params_id]\n                params[_KEY] = next_r  # modify r\n                # generate new id\n                increased_id = params_id.split(\'_\')[-1]\n                new_id = create_bracket_parameter_id(self.bracket_id, self.i, increased_id)\n                hyper_configs[new_id] = params\n            self._record_hyper_configs(hyper_configs)\n            return [[key, value] for key, value in hyper_configs.items()]\n        return None\n\n    def get_hyperparameter_configurations(self, num, r, searchspace_json, random_state):\n        """"""Randomly generate num hyperparameter configurations from search space\n\n        Parameters\n        ----------\n        num: int\n            the number of hyperparameter configurations\n\n        Returns\n        -------\n        list\n            a list of hyperparameter configurations. Format: [[key1, value1], [key2, value2], ...]\n        """"""\n        global _KEY\n        assert self.i == 0\n        hyperparameter_configs = dict()\n        for _ in range(num):\n            params_id = create_bracket_parameter_id(self.bracket_id, self.i)\n            params = json2parameter(searchspace_json, random_state)\n            params[_KEY] = r\n            hyperparameter_configs[params_id] = params\n        self._record_hyper_configs(hyperparameter_configs)\n        return [[key, value] for key, value in hyperparameter_configs.items()]\n\n    def _record_hyper_configs(self, hyper_configs):\n        """"""after generating one round of hyperconfigs, this function records the generated hyperconfigs,\n        creates a dict to record the performance when those hyperconifgs are running, set the number of finished configs\n        in this round to be 0, and increase the round number.\n\n        Parameters\n        ----------\n        hyper_configs: list\n            the generated hyperconfigs\n        """"""\n        self.hyper_configs.append(hyper_configs)\n        self.configs_perf.append(dict())\n        self.num_finished_configs.append(0)\n        self.num_configs_to_run.append(len(hyper_configs))\n        self.increase_i()\n\n\nclass Hyperband(MsgDispatcherBase):\n    """"""Hyperband inherit from MsgDispatcherBase rather than Tuner, because it integrates both tuner\'s functions and assessor\'s functions.\n    This is an implementation that could fully leverage available resources, i.e., high parallelism.\n    A single execution of Hyperband takes a finite budget of (s_max + 1)B.\n\n    Parameters\n    ----------\n    R: int\n        the maximum amount of resource that can be allocated to a single configuration\n    eta: int\n        the variable that controls the proportion of configurations discarded in each round of SuccessiveHalving\n    optimize_mode: str\n        optimize mode, \'maximize\' or \'minimize\'\n    """"""\n\n    def __init__(self, R=60, eta=3, optimize_mode=\'maximize\'):\n        """"""B = (s_max + 1)R""""""\n        super(Hyperband, self).__init__()\n        self.R = R\n        self.eta = eta\n        self.brackets = dict()  # dict of Bracket\n        self.generated_hyper_configs = []  # all the configs waiting for run\n        self.completed_hyper_configs = []  # all the completed configs\n        self.s_max = math.floor(math.log(self.R, self.eta) + _epsilon)\n        self.curr_s = self.s_max\n\n        self.searchspace_json = None\n        self.random_state = None\n        self.optimize_mode = OptimizeMode(optimize_mode)\n\n        # This is for the case that nnimanager requests trial config, but tuner cannot provide immediately.\n        # In this case, tuner increases self.credit to issue a trial config sometime later.\n        self.credit = 0\n\n        # record the latest parameter_id of the trial job trial_job_id.\n        # if there is no running parameter_id, self.job_id_para_id_map[trial_job_id] == None\n        # new trial job is added to this dict and finished trial job is removed from it.\n        self.job_id_para_id_map = dict()\n\n    def handle_initialize(self, data):\n        """"""callback for initializing the advisor\n        Parameters\n        ----------\n        data: dict\n            search space\n        """"""\n        self.handle_update_search_space(data)\n        send(CommandType.Initialized, \'\')\n\n    def handle_request_trial_jobs(self, data):\n        """"""\n        Parameters\n        ----------\n        data: int\n            number of trial jobs\n        """"""\n        for _ in range(data):\n            ret = self._get_one_trial_job()\n            send(CommandType.NewTrialJob, json_tricks.dumps(ret))\n\n    def _get_one_trial_job(self):\n        """"""get one trial job, i.e., one hyperparameter configuration.""""""\n        if not self.generated_hyper_configs:\n            if self.curr_s < 0:\n                self.curr_s = self.s_max\n            _logger.debug(\'create a new bracket, self.curr_s=%d\', self.curr_s)\n            self.brackets[self.curr_s] = Bracket(self.curr_s, self.s_max, self.eta, self.R, self.optimize_mode)\n            next_n, next_r = self.brackets[self.curr_s].get_n_r()\n            _logger.debug(\'new bracket, next_n=%d, next_r=%d\', next_n, next_r)\n            assert self.searchspace_json is not None and self.random_state is not None\n            generated_hyper_configs = self.brackets[self.curr_s].get_hyperparameter_configurations(next_n, next_r,\n                                                                                                   self.searchspace_json,\n                                                                                                   self.random_state)\n            self.generated_hyper_configs = generated_hyper_configs.copy()\n            self.curr_s -= 1\n\n        assert self.generated_hyper_configs\n        params = self.generated_hyper_configs.pop(0)\n        ret = {\n            \'parameter_id\': params[0],\n            \'parameter_source\': \'algorithm\',\n            \'parameters\': params[1]\n        }\n        return ret\n\n    def handle_update_search_space(self, data):\n        """"""data: JSON object, which is search space\n        """"""\n        self.searchspace_json = data\n        self.random_state = np.random.RandomState()\n\n    def _handle_trial_end(self, parameter_id):\n        """"""\n        Parameters\n        ----------\n        parameter_id: parameter id of the finished config\n        """"""\n        bracket_id, i, _ = parameter_id.split(\'_\')\n        hyper_configs = self.brackets[int(bracket_id)].inform_trial_end(int(i))\n        if hyper_configs is not None:\n            _logger.debug(\'bracket %s next round %s, hyper_configs: %s\', bracket_id, i, hyper_configs)\n            self.generated_hyper_configs = self.generated_hyper_configs + hyper_configs\n\n    def handle_trial_end(self, data):\n        """"""\n        Parameters\n        ----------\n        data: dict()\n            it has three keys: trial_job_id, event, hyper_params\n            trial_job_id: the id generated by training service\n            event: the job\'s state\n            hyper_params: the hyperparameters (a string) generated and returned by tuner\n        """"""\n        hyper_params = json_tricks.loads(data[\'hyper_params\'])\n        self._handle_trial_end(hyper_params[\'parameter_id\'])\n        if data[\'trial_job_id\'] in self.job_id_para_id_map:\n            del self.job_id_para_id_map[data[\'trial_job_id\']]\n\n    def handle_report_metric_data(self, data):\n        """"""\n        Parameters\n        ----------\n        data:\n            it is an object which has keys \'parameter_id\', \'value\', \'trial_job_id\', \'type\', \'sequence\'.\n\n        Raises\n        ------\n        ValueError\n            Data type not supported\n        """"""\n        if \'value\' in data:\n            data[\'value\'] = json_tricks.loads(data[\'value\'])\n        if data[\'type\'] == MetricType.REQUEST_PARAMETER:\n            assert multi_phase_enabled()\n            assert data[\'trial_job_id\'] is not None\n            assert data[\'parameter_index\'] is not None\n            assert data[\'trial_job_id\'] in self.job_id_para_id_map\n            self._handle_trial_end(self.job_id_para_id_map[data[\'trial_job_id\']])\n            ret = self._get_one_trial_job()\n            if data[\'trial_job_id\'] is not None:\n                ret[\'trial_job_id\'] = data[\'trial_job_id\']\n            if data[\'parameter_index\'] is not None:\n                ret[\'parameter_index\'] = data[\'parameter_index\']\n            self.job_id_para_id_map[data[\'trial_job_id\']] = ret[\'parameter_id\']\n            send(CommandType.SendTrialJobParameter, json_tricks.dumps(ret))\n        else:\n            value = extract_scalar_reward(data[\'value\'])\n            bracket_id, i, _ = data[\'parameter_id\'].split(\'_\')\n            bracket_id = int(bracket_id)\n\n            # add <trial_job_id, parameter_id> to self.job_id_para_id_map here,\n            # because when the first parameter_id is created, trial_job_id is not known yet.\n            if data[\'trial_job_id\'] in self.job_id_para_id_map:\n                assert self.job_id_para_id_map[data[\'trial_job_id\']] == data[\'parameter_id\']\n            else:\n                self.job_id_para_id_map[data[\'trial_job_id\']] = data[\'parameter_id\']\n\n            if data[\'type\'] == MetricType.FINAL:\n                # sys.maxsize indicates this value is from FINAL metric data, because data[\'sequence\'] from FINAL metric\n                # and PERIODICAL metric are independent, thus, not comparable.\n                self.brackets[bracket_id].set_config_perf(int(i), data[\'parameter_id\'], sys.maxsize, value)\n                self.completed_hyper_configs.append(data)\n            elif data[\'type\'] == MetricType.PERIODICAL:\n                self.brackets[bracket_id].set_config_perf(int(i), data[\'parameter_id\'], data[\'sequence\'], value)\n            else:\n                raise ValueError(\'Data type not supported: {}\'.format(data[\'type\']))\n\n    def handle_add_customized_trial(self, data):\n        pass\n\n    def handle_import_data(self, data):\n        pass\n'"
src/sdk/pynni/nni/hyperopt_tuner/__init__.py,0,b''
src/sdk/pynni/nni/hyperopt_tuner/hyperopt_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nhyperopt_tuner.py\n""""""\n\nimport copy\nimport logging\n\nimport hyperopt as hp\nimport numpy as np\nfrom nni.tuner import Tuner\nfrom nni.utils import NodeType, OptimizeMode, extract_scalar_reward, split_index\n\nlogger = logging.getLogger(\'hyperopt_AutoML\')\n\n\ndef json2space(in_x, name=NodeType.ROOT):\n    """"""\n    Change json to search space in hyperopt.\n\n    Parameters\n    ----------\n    in_x : dict/list/str/int/float\n        The part of json.\n    name : str\n        name could be NodeType.ROOT, NodeType.TYPE, NodeType.VALUE or NodeType.INDEX, NodeType.NAME.\n    """"""\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + \'-\' + _type\n            _value = json2space(in_x[NodeType.VALUE], name=name)\n            if _type == \'choice\':\n                out_y = hp.hp.choice(name, _value)\n            elif _type == \'randint\':\n                out_y = hp.hp.randint(name, _value[1] - _value[0])\n            else:\n                if _type in [\'loguniform\', \'qloguniform\']:\n                    _value[:2] = np.log(_value[:2])\n                out_y = getattr(hp.hp, _type)(name, *_value)\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2space(in_x[key], name + \'[%s]\' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\n                        \'\\\'_name\\\' key is not found in this nested search space.\'\n                    )\n            out_y.append(json2space(x_i, name + \'[%d]\' % i))\n    return out_y\n\n\ndef json2parameter(in_x, parameter, name=NodeType.ROOT):\n    """"""\n    Change json to parameters.\n    """"""\n    out_y = copy.deepcopy(in_x)\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + \'-\' + _type\n            if _type == \'choice\':\n                _index = parameter[name]\n                out_y = {\n                    NodeType.INDEX:\n                    _index,\n                    NodeType.VALUE:\n                    json2parameter(in_x[NodeType.VALUE][_index],\n                                   parameter,\n                                   name=name + \'[%d]\' % _index)\n                }\n            else:\n                if _type in [\'quniform\', \'qloguniform\']:\n                    out_y = np.clip(parameter[name], in_x[NodeType.VALUE][0], in_x[NodeType.VALUE][1])\n                elif _type == \'randint\':\n                    out_y = parameter[name] + in_x[NodeType.VALUE][0]\n                else:\n                    out_y = parameter[name]\n        else:\n            out_y = dict()\n            for key in in_x.keys():\n                out_y[key] = json2parameter(in_x[key], parameter,\n                                            name + \'[%s]\' % str(key))\n    elif isinstance(in_x, list):\n        out_y = list()\n        for i, x_i in enumerate(in_x):\n            if isinstance(x_i, dict):\n                if NodeType.NAME not in x_i.keys():\n                    raise RuntimeError(\n                        \'\\\'_name\\\' key is not found in this nested search space.\'\n                    )\n            out_y.append(json2parameter(x_i, parameter, name + \'[%d]\' % i))\n    return out_y\n\n\ndef json2vals(in_x, vals, out_y, name=NodeType.ROOT):\n    if isinstance(in_x, dict):\n        if NodeType.TYPE in in_x.keys():\n            _type = in_x[NodeType.TYPE]\n            name = name + \'-\' + _type\n\n            try:\n                out_y[name] = vals[NodeType.INDEX]\n            # TODO - catch exact Exception\n            except Exception:\n                out_y[name] = vals\n\n            if _type == \'choice\':\n                _index = vals[NodeType.INDEX]\n                json2vals(in_x[NodeType.VALUE][_index],\n                          vals[NodeType.VALUE],\n                          out_y,\n                          name=name + \'[%d]\' % _index)\n            if _type == \'randint\':\n                out_y[name] -= in_x[NodeType.VALUE][0]\n        else:\n            for key in in_x.keys():\n                json2vals(in_x[key], vals[key], out_y,\n                          name + \'[%s]\' % str(key))\n    elif isinstance(in_x, list):\n        for i, temp in enumerate(in_x):\n            # nested json\n            if isinstance(temp, dict):\n                if NodeType.NAME not in temp.keys():\n                    raise RuntimeError(\n                        \'\\\'_name\\\' key is not found in this nested search space.\'\n                    )\n                else:\n                    json2vals(temp, vals[i], out_y, name + \'[%d]\' % i)\n            else:\n                json2vals(temp, vals[i], out_y, name + \'[%d]\' % i)\n\n\ndef _add_index(in_x, parameter):\n    """"""\n    change parameters in NNI format to parameters in hyperopt format(This function also support nested dict.).\n    For example, receive parameters like:\n        {\'dropout_rate\': 0.8, \'conv_size\': 3, \'hidden_size\': 512}\n    Will change to format in hyperopt, like:\n        {\'dropout_rate\': 0.8, \'conv_size\': {\'_index\': 1, \'_value\': 3}, \'hidden_size\': {\'_index\': 1, \'_value\': 512}}\n    """"""\n    if NodeType.TYPE not in in_x: # if at the top level\n        out_y = dict()\n        for key, value in parameter.items():\n            out_y[key] = _add_index(in_x[key], value)\n        return out_y\n    elif isinstance(in_x, dict):\n        value_type = in_x[NodeType.TYPE]\n        value_format = in_x[NodeType.VALUE]\n        if value_type == ""choice"":\n            choice_name = parameter[0] if isinstance(parameter,\n                                                     list) else parameter\n            for pos, item in enumerate(\n                    value_format):  # here value_format is a list\n                if isinstance(\n                        item,\n                        list):  # this format is [""choice_key"", format_dict]\n                    choice_key = item[0]\n                    choice_value_format = item[1]\n                    if choice_key == choice_name:\n                        return {\n                            NodeType.INDEX: pos,\n                            NodeType.VALUE: [\n                                choice_name,\n                                _add_index(choice_value_format, parameter[1])\n                            ]\n                        }\n                elif choice_name == item:\n                    return {NodeType.INDEX: pos, NodeType.VALUE: item}\n        else:\n            return parameter\n    return None  # note: this is not written by original author, feel free to modify if you think it\'s incorrect\n\n\nclass HyperoptTuner(Tuner):\n    """"""\n    HyperoptTuner is a tuner which using hyperopt algorithm.\n    """"""\n\n    def __init__(self, algorithm_name, optimize_mode=\'minimize\',\n                 parallel_optimize=False, constant_liar_type=\'min\'):\n        """"""\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes ""tpe"", ""random_search"" and anneal"".\n        optimize_mode : str\n        parallel_optimize : bool\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        constant_liar_type : str\n            constant_liar_type including ""min"", ""max"" and ""mean""\n            More detail could reference: docs/en_US/Tuner/HyperoptTuner.md\n        """"""\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.rval = None\n        self.supplement_data_num = 0\n\n        self.parallel = parallel_optimize\n        if self.parallel:\n            self.CL_rval = None\n            self.constant_liar_type = constant_liar_type\n            self.running_data = []\n            self.optimal_y = None\n\n    def _choose_tuner(self, algorithm_name):\n        """"""\n        Parameters\n        ----------\n        algorithm_name : str\n            algorithm_name includes ""tpe"", ""random_search"" and anneal""\n        """"""\n        if algorithm_name == \'tpe\':\n            return hp.tpe.suggest\n        if algorithm_name == \'random_search\':\n            return hp.rand.suggest\n        if algorithm_name == \'anneal\':\n            return hp.anneal.suggest\n        raise RuntimeError(\'Not support tuner algorithm in hyperopt.\')\n\n    def update_search_space(self, search_space):\n        """"""\n        Update search space definition in tuner by search_space in parameters.\n\n        Will called when first setup experiemnt or update search space in WebUI.\n\n        Parameters\n        ----------\n        search_space : dict\n        """"""\n        self.json = search_space\n\n        search_space_instance = json2space(self.json)\n        rstate = np.random.RandomState()\n        trials = hp.Trials()\n        domain = hp.Domain(None,\n                           search_space_instance,\n                           pass_expr_memo_ctrl=None)\n        algorithm = self._choose_tuner(self.algorithm_name)\n        self.rval = hp.FMinIter(algorithm,\n                                domain,\n                                trials,\n                                max_evals=-1,\n                                rstate=rstate,\n                                verbose=0)\n        self.rval.catch_eval_exceptions = False\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Returns a set of trial (hyper-)parameters, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        params : dict\n        """"""\n        total_params = self.get_suggestion(random_search=False)\n        # avoid generating same parameter with concurrent trials because hyperopt doesn\'t support parallel mode\n        if total_params in self.total_data.values():\n            # but it can cause duplicate parameter rarely\n            total_params = self.get_suggestion(random_search=True)\n        self.total_data[parameter_id] = total_params\n\n        if self.parallel:\n            self.running_data.append(parameter_id)\n\n        params = split_index(total_params)\n        return params\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Record an observation of the objective function\n\n        Parameters\n        ----------\n        parameter_id : int\n        parameters : dict\n        value : dict/float\n            if value is dict, it should have ""default"" key.\n            value is final metrics of the trial.\n        """"""\n        reward = extract_scalar_reward(value)\n        # restore the paramsters contains \'_index\'\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\'Received parameter_id not in total_data.\')\n        params = self.total_data[parameter_id]\n\n        # code for parallel\n        if self.parallel:\n            constant_liar = kwargs.get(\'constant_liar\', False)\n\n            if constant_liar:\n                rval = self.CL_rval\n            else:\n                rval = self.rval\n                # ignore duplicated reported final result (due to aware of intermedate result)\n                if parameter_id not in self.running_data:\n                    logger.info(""Received duplicated final result with parameter id: %s"", parameter_id)\n                    return\n                self.running_data.remove(parameter_id)\n\n                # update the reward of optimal_y\n                if self.optimal_y is None:\n                    if self.constant_liar_type == \'mean\':\n                        self.optimal_y = [reward, 1]\n                    else:\n                        self.optimal_y = reward\n                else:\n                    if self.constant_liar_type == \'mean\':\n                        _sum = self.optimal_y[0] + reward\n                        _number = self.optimal_y[1] + 1\n                        self.optimal_y = [_sum, _number]\n                    elif self.constant_liar_type == \'min\':\n                        self.optimal_y = min(self.optimal_y, reward)\n                    elif self.constant_liar_type == \'max\':\n                        self.optimal_y = max(self.optimal_y, reward)\n                logger.debug(""Update optimal_y with reward, optimal_y = %s"", self.optimal_y)\n        else:\n            rval = self.rval\n\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        domain = rval.domain\n        trials = rval.trials\n\n        new_id = len(trials)\n\n        rval_specs = [None]\n        rval_results = [domain.new_result()]\n        rval_miscs = [dict(tid=new_id, cmd=domain.cmd, workdir=domain.workdir)]\n\n        vals = params\n        idxs = dict()\n\n        out_y = dict()\n        json2vals(self.json, vals, out_y)\n        vals = out_y\n        for key in domain.params:\n            if key in [NodeType.VALUE, NodeType.INDEX]:\n                continue\n            if key not in vals or vals[key] is None or vals[key] == []:\n                idxs[key] = vals[key] = []\n            else:\n                idxs[key] = [new_id]\n                vals[key] = [vals[key]]\n\n        self.miscs_update_idxs_vals(rval_miscs,\n                                    idxs,\n                                    vals,\n                                    idxs_map={new_id: new_id},\n                                    assert_all_vals_used=False)\n\n        trial = trials.new_trial_docs([new_id], rval_specs, rval_results,\n                                      rval_miscs)[0]\n        trial[\'result\'] = {\'loss\': reward, \'status\': \'ok\'}\n        trial[\'state\'] = hp.JOB_STATE_DONE\n        trials.insert_trial_docs([trial])\n        trials.refresh()\n\n    def miscs_update_idxs_vals(self,\n                               miscs,\n                               idxs,\n                               vals,\n                               assert_all_vals_used=True,\n                               idxs_map=None):\n        """"""\n        Unpack the idxs-vals format into the list of dictionaries that is\n        `misc`.\n\n        Parameters\n        ----------\n        idxs_map : dict\n            idxs_map is a dictionary of id->id mappings so that the misc[\'idxs\'] can\n        contain different numbers than the idxs argument.\n        """"""\n        if idxs_map is None:\n            idxs_map = {}\n\n        assert set(idxs.keys()) == set(vals.keys())\n\n        misc_by_id = {m[\'tid\']: m for m in miscs}\n        for m in miscs:\n            m[\'idxs\'] = {key: [] for key in idxs}\n            m[\'vals\'] = {key: [] for key in idxs}\n\n        for key in idxs:\n            assert len(idxs[key]) == len(vals[key])\n            for tid, val in zip(idxs[key], vals[key]):\n                tid = idxs_map.get(tid, tid)\n                if assert_all_vals_used or tid in misc_by_id:\n                    misc_by_id[tid][\'idxs\'][key] = [tid]\n                    misc_by_id[tid][\'vals\'][key] = [val]\n\n    def get_suggestion(self, random_search=False):\n        """"""\n        get suggestion from hyperopt\n\n        Parameters\n        ----------\n        random_search : bool\n            flag to indicate random search or not (default: {False})\n\n        Returns\n        ----------\n        total_params : dict\n            parameter suggestion\n        """"""\n        if self.parallel and len(self.total_data) > 20 and self.running_data and self.optimal_y is not None:\n            self.CL_rval = copy.deepcopy(self.rval)\n            if self.constant_liar_type == \'mean\':\n                _constant_liar_y = self.optimal_y[0] / self.optimal_y[1]\n            else:\n                _constant_liar_y = self.optimal_y\n            for _parameter_id in self.running_data:\n                self.receive_trial_result(parameter_id=_parameter_id, parameters=None, value=_constant_liar_y, constant_liar=True)\n            rval = self.CL_rval\n\n            random_state = np.random.randint(2**31 - 1)\n        else:\n            rval = self.rval\n            random_state = rval.rstate.randint(2**31 - 1)\n\n        trials = rval.trials\n        algorithm = rval.algo\n        new_ids = rval.trials.new_trial_ids(1)\n        rval.trials.refresh()\n\n        if random_search:\n            new_trials = hp.rand.suggest(new_ids, rval.domain, trials,\n                                         random_state)\n        else:\n            new_trials = algorithm(new_ids, rval.domain, trials, random_state)\n        rval.trials.refresh()\n        vals = new_trials[0][\'misc\'][\'vals\']\n        parameter = dict()\n        for key in vals:\n            try:\n                parameter[key] = vals[key][0].item()\n            except (KeyError, IndexError):\n                parameter[key] = None\n\n        # remove \'_index\' from json2parameter and save params-id\n        total_params = json2parameter(self.json, parameter)\n        return total_params\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data:\n            a list of dictionarys, each of which has at least two keys, \'parameter\' and \'value\'\n        """"""\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            if self.algorithm_name == \'random_search\':\n                return\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = \'_\'.join(\n                [""ImportData"", str(self.supplement_data_num)])\n            self.total_data[_parameter_id] = _add_index(in_x=self.json,\n                                                        parameter=_params)\n            self.receive_trial_result(parameter_id=_parameter_id,\n                                      parameters=_params,\n                                      value=_value)\n        logger.info(""Successfully import data to TPE/Anneal tuner."")\n'"
src/sdk/pynni/nni/medianstop_assessor/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .medianstop_assessor import MedianstopAssessor\n'
src/sdk/pynni/nni/medianstop_assessor/medianstop_assessor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nfrom nni.assessor import Assessor, AssessResult\nfrom nni.utils import extract_scalar_history\n\nlogger = logging.getLogger(\'medianstop_Assessor\')\n\nclass MedianstopAssessor(Assessor):\n    """"""MedianstopAssessor is The median stopping rule stops a pending trial X at step S\n    if the trial\xe2\x80\x99s best objective value by step S is strictly worse than the median value\n    of the running averages of all completed trials\xe2\x80\x99 objectives reported up to step S\n\n    Parameters\n    ----------\n    optimize_mode : str\n        optimize mode, \'maximize\' or \'minimize\'\n    start_step : int\n        only after receiving start_step number of reported intermediate results\n    """"""\n    def __init__(self, optimize_mode=\'maximize\', start_step=0):\n        self._start_step = start_step\n        self._running_history = dict()\n        self._completed_avg_history = dict()\n        if optimize_mode == \'maximize\':\n            self._high_better = True\n        elif optimize_mode == \'minimize\':\n            self._high_better = False\n        else:\n            self._high_better = True\n            logger.warning(\'unrecognized optimize_mode %s\', optimize_mode)\n\n    def _update_data(self, trial_job_id, trial_history):\n        """"""update data\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n        """"""\n        if trial_job_id not in self._running_history:\n            self._running_history[trial_job_id] = []\n        self._running_history[trial_job_id].extend(trial_history[len(self._running_history[trial_job_id]):])\n\n    def trial_end(self, trial_job_id, success):\n        """"""trial_end\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        success : bool\n            True if succssfully finish the experiment, False otherwise\n        """"""\n        if trial_job_id in self._running_history:\n            if success:\n                cnt = 0\n                history_sum = 0\n                self._completed_avg_history[trial_job_id] = []\n                for each in self._running_history[trial_job_id]:\n                    cnt += 1\n                    history_sum += each\n                    self._completed_avg_history[trial_job_id].append(history_sum / cnt)\n            self._running_history.pop(trial_job_id)\n        else:\n            logger.warning(\'trial_end: trial_job_id does not exist in running_history\')\n\n    def assess_trial(self, trial_job_id, trial_history):\n        """"""assess_trial\n\n        Parameters\n        ----------\n        trial_job_id : int\n            trial job id\n        trial_history : list\n            The history performance matrix of each trial\n\n        Returns\n        -------\n        bool\n            AssessResult.Good or AssessResult.Bad\n\n        Raises\n        ------\n        Exception\n            unrecognize exception in medianstop_assessor\n        """"""\n        curr_step = len(trial_history)\n        if curr_step < self._start_step:\n            return AssessResult.Good\n\n        scalar_trial_history = extract_scalar_history(trial_history)\n        self._update_data(trial_job_id, scalar_trial_history)\n        if self._high_better:\n            best_history = max(scalar_trial_history)\n        else:\n            best_history = min(scalar_trial_history)\n\n        avg_array = []\n        for id_ in self._completed_avg_history:\n            if len(self._completed_avg_history[id_]) >= curr_step:\n                avg_array.append(self._completed_avg_history[id_][curr_step - 1])\n        if avg_array:\n            avg_array.sort()\n            if self._high_better:\n                median = avg_array[(len(avg_array)-1) // 2]\n                return AssessResult.Bad if best_history < median else AssessResult.Good\n            else:\n                median = avg_array[len(avg_array) // 2]\n                return AssessResult.Bad if best_history > median else AssessResult.Good\n        else:\n            return AssessResult.Good\n'"
src/sdk/pynni/nni/medianstop_assessor/test.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport argparse\nimport logging\nimport random\n\nfrom .medianstop_assessor import MedianstopAssessor\nfrom nni.assessor import AssessResult\n\nlogger = logging.getLogger('nni.contrib.medianstop_assessor')\nlogger.debug('START')\n\n\ndef test():\n    '''\n    tests.\n    '''\n    parser = argparse.ArgumentParser(description='parse command line parameters.')\n    parser.add_argument('--start_from', type=int, default=10, dest='start_step',\n                        help='Assessing each trial from the step start_step.')\n    parser.add_argument('--optimize_mode', type=str, default='maximize',\n                        help='Select optimize mode for Tuner: minimize or maximize.')\n    FLAGS, _ = parser.parse_known_args()\n\n    lcs = [[1,1,1,1,1,1,1,1,1,1],\n           [2,2,2,2,2,2,2,2,2,2],\n           [3,3,3,3,3,3,3,3,3,3],\n           [4,4,4,4,4,4,4,4,4,4]]\n    #lcs = [[1,1,1,1,1,1,1,1,1,1],\n    #       [1,1,1,1,1,1,1,1,1,1],\n    #       [1,1,1,1,1,1,1,1,1,1]]\n\n    assessor = MedianstopAssessor(FLAGS.optimize_mode, FLAGS.start_step)\n    for i in range(len(lcs)):\n        #lc = []\n        to_complete = True\n        for k in range(len(lcs[0])):\n            #d = random.randint(i*100+0, i*100+100)\n            #lc.append(d)\n            ret = assessor.assess_trial(i, lcs[i][:k+1])\n            print('result: %d', ret)\n            if ret == AssessResult.Bad:\n                assessor.trial_end(i, False)\n                to_complete = False\n                break\n        if to_complete:\n            assessor.trial_end(i, True)\n\ntry:\n    test()\nexcept Exception as exception:\n    logger.exception(exception)\n    raise\n"""
src/sdk/pynni/nni/metis_tuner/__init__.py,0,b''
src/sdk/pynni/nni/metis_tuner/lib_acquisition_function.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nlib_acquisition_function.py\n""""""\n\nimport sys\nimport numpy\n\nfrom scipy.stats import norm\nfrom scipy.optimize import minimize\n\nimport nni.metis_tuner.lib_data as lib_data\n\n\ndef next_hyperparameter_expected_improvement(fun_prediction,\n                                             fun_prediction_args,\n                                             x_bounds, x_types,\n                                             samples_y_aggregation,\n                                             minimize_starting_points,\n                                             minimize_constraints_fun=None):\n    """"""\n    ""Expected Improvement"" acquisition function\n    """"""\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_expected_improvement,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=""L-BFGS-B"",\n                       args=(fun_prediction,\n                             fun_prediction_args,\n                             x_bounds,\n                             x_types,\n                             samples_y_aggregation,\n                             minimize_constraints_fun))\n\n        if (best_acquisition_value is None) or \\\n                (res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or \\\n                    (minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {\'hyperparameter\': best_x, \'expected_mu\': mu,\n                   \'expected_sigma\': sigma, \'acquisition_func\': ""ei""}\n\n    return outputs\n\n\ndef _expected_improvement(x, fun_prediction, fun_prediction_args,\n                          x_bounds, x_types, samples_y_aggregation,\n                          minimize_constraints_fun):\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    expected_improvement = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, sigma = fun_prediction(x, *fun_prediction_args)\n\n        loss_optimum = min(samples_y_aggregation)\n        scaling_factor = -1\n\n        # In case sigma equals zero\n        with numpy.errstate(divide=""ignore""):\n            Z = scaling_factor * (mu - loss_optimum) / sigma\n            expected_improvement = scaling_factor * (mu - loss_optimum) * \\\n                norm.cdf(Z) + sigma * norm.pdf(Z)\n            expected_improvement = 0.0 if sigma == 0.0 else expected_improvement\n\n        # We want expected_improvement to be as large as possible\n        # (i.e., as small as possible for minimize(...))\n        expected_improvement = -1 * expected_improvement\n    return expected_improvement\n\n\ndef next_hyperparameter_lowest_confidence(fun_prediction,\n                                          fun_prediction_args,\n                                          x_bounds, x_types,\n                                          minimize_starting_points,\n                                          minimize_constraints_fun=None):\n    """"""\n    ""Lowest Confidence"" acquisition function\n    """"""\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_lowest_confidence,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=""L-BFGS-B"",\n                       args=(fun_prediction,\n                             fun_prediction_args,\n                             x_bounds,\n                             x_types,\n                             minimize_constraints_fun))\n\n        if (best_acquisition_value) is None or (\n                res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or (\n                    minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {\'hyperparameter\': best_x, \'expected_mu\': mu,\n                   \'expected_sigma\': sigma, \'acquisition_func\': ""lc""}\n    return outputs\n\n\ndef _lowest_confidence(x, fun_prediction, fun_prediction_args,\n                       x_bounds, x_types, minimize_constraints_fun):\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    ci = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, sigma = fun_prediction(x, *fun_prediction_args)\n        ci = (sigma * 1.96 * 2) / mu\n        # We want ci to be as large as possible\n        # (i.e., as small as possible for minimize(...),\n        # because this would mean lowest confidence\n        ci = -1 * ci\n\n    return ci\n\n\ndef next_hyperparameter_lowest_mu(fun_prediction,\n                                  fun_prediction_args,\n                                  x_bounds, x_types,\n                                  minimize_starting_points,\n                                  minimize_constraints_fun=None):\n    """"""\n    ""Lowest Mu"" acquisition function\n    """"""\n    best_x = None\n    best_acquisition_value = None\n    x_bounds_minmax = [[i[0], i[-1]] for i in x_bounds]\n    x_bounds_minmax = numpy.array(x_bounds_minmax)\n\n    for starting_point in numpy.array(minimize_starting_points):\n        res = minimize(fun=_lowest_mu,\n                       x0=starting_point.reshape(1, -1),\n                       bounds=x_bounds_minmax,\n                       method=""L-BFGS-B"",\n                       args=(fun_prediction, fun_prediction_args,\n                             x_bounds, x_types, minimize_constraints_fun))\n\n        if (best_acquisition_value is None) or (\n                res.fun < best_acquisition_value):\n            res.x = numpy.ndarray.tolist(res.x)\n            res.x = lib_data.match_val_type(res.x, x_bounds, x_types)\n            if (minimize_constraints_fun is None) or (\n                    minimize_constraints_fun(res.x) is True):\n                best_acquisition_value = res.fun\n                best_x = res.x\n\n    outputs = None\n    if best_x is not None:\n        mu, sigma = fun_prediction(best_x, *fun_prediction_args)\n        outputs = {\'hyperparameter\': best_x, \'expected_mu\': mu,\n                   \'expected_sigma\': sigma, \'acquisition_func\': ""lm""}\n    return outputs\n\n\ndef _lowest_mu(x, fun_prediction, fun_prediction_args,\n               x_bounds, x_types, minimize_constraints_fun):\n    """"""\n    Calculate the lowest mu\n    """"""\n    # This is only for step-wise optimization\n    x = lib_data.match_val_type(x, x_bounds, x_types)\n\n    mu = sys.maxsize\n    if (minimize_constraints_fun is None) or (\n            minimize_constraints_fun(x) is True):\n        mu, _ = fun_prediction(x, *fun_prediction_args)\n    return mu\n'"
src/sdk/pynni/nni/metis_tuner/lib_constraint_summation.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nlib_constraint_summation.py\n""""""\n\nimport math\nimport random\n\nfrom operator import itemgetter\n\n\ndef check_feasibility(x_bounds, lowerbound, upperbound):\n    \'\'\'\n    This can have false positives.\n    For examples, parameters can only be 0 or 5, and the summation constraint is between 6 and 7.\n    \'\'\'\n    # x_bounds should be sorted, so even for ""discrete_int"" type,\n    # the smallest and the largest number should the first and the last element\n    x_bounds_lowerbound = sum([x_bound[0] for x_bound in x_bounds])\n    x_bounds_upperbound = sum([x_bound[-1] for x_bound in x_bounds])\n\n    # return ((x_bounds_lowerbound <= lowerbound) and (x_bounds_upperbound >= lowerbound)) or \\\n    #        ((x_bounds_lowerbound <= upperbound) and (x_bounds_upperbound >= upperbound))\n    return (x_bounds_lowerbound <= lowerbound <= x_bounds_upperbound) or \\\n           (x_bounds_lowerbound <= upperbound <= x_bounds_upperbound)\n\n\ndef rand(x_bounds, x_types, lowerbound, upperbound, max_retries=100):\n    \'\'\'\n    Key idea is that we try to move towards upperbound, by randomly choose one\n    value for each parameter. However, for the last parameter,\n    we need to make sure that its value can help us get above lowerbound\n    \'\'\'\n    outputs = None\n\n    if check_feasibility(x_bounds, lowerbound, upperbound) is True:\n        # Order parameters by their range size. We want the smallest range first,\n        # because the corresponding parameter has less numbers to choose from\n        x_idx_sorted = []\n        for i, _ in enumerate(x_bounds):\n            if x_types[i] == ""discrete_int"":\n                x_idx_sorted.append([i, len(x_bounds[i])])\n            elif (x_types[i] == ""range_int"") or (x_types[i] == ""range_continuous""):\n                x_idx_sorted.append(\n                    [i, math.floor(x_bounds[i][1] - x_bounds[i][0])])\n        x_idx_sorted = sorted(x_idx_sorted, key=itemgetter(1))\n\n        for _ in range(max_retries):\n            budget_allocated = 0\n            outputs = [None] * len(x_bounds)\n\n            for i, _ in enumerate(x_idx_sorted):\n                x_idx = x_idx_sorted[i][0]\n                # The amount of unallocated space that we have\n                budget_max = upperbound - budget_allocated\n                # NOT the Last x that we need to assign a random number\n                if i < (len(x_idx_sorted) - 1):\n                    if x_bounds[x_idx][0] <= budget_max:\n                        if x_types[x_idx] == ""discrete_int"":\n                            # Note the valid integer\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                if j <= budget_max:\n                                    temp.append(j)\n                            # Randomly pick a number from the integer array\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(\n                                    0, len(temp) - 1)]\n\n                        elif (x_types[x_idx] == ""range_int"") or \\\n                                (x_types[x_idx] == ""range_continuous""):\n                            outputs[x_idx] = random.randint(\n                                x_bounds[x_idx][0], min(x_bounds[x_idx][-1], budget_max))\n\n                else:\n                    # The last x that we need to assign a random number\n                    randint_lowerbound = lowerbound - budget_allocated\n                    randint_lowerbound = 0 if randint_lowerbound < 0 else randint_lowerbound\n\n                    # This check:\n                    # is our smallest possible value going to overflow the available budget space,\n                    # and is our largest possible value going to underflow the\n                    # lower bound\n                    if (x_bounds[x_idx][0] <= budget_max) and \\\n                            (x_bounds[x_idx][-1] >= randint_lowerbound):\n                        if x_types[x_idx] == ""discrete_int"":\n                            temp = []\n                            for j in x_bounds[x_idx]:\n                                # if (j <= budget_max) and (j >=\n                                # randint_lowerbound):\n                                if randint_lowerbound <= j <= budget_max:\n                                    temp.append(j)\n                            if temp:\n                                outputs[x_idx] = temp[random.randint(\n                                    0, len(temp) - 1)]\n                        elif (x_types[x_idx] == ""range_int"") or \\\n                                (x_types[x_idx] == ""range_continuous""):\n                            outputs[x_idx] = random.randint(\n                                randint_lowerbound, min(\n                                    x_bounds[x_idx][1], budget_max))\n                if outputs[x_idx] is None:\n                    break\n                budget_allocated += outputs[x_idx]\n            if None not in outputs:\n                break\n    return outputs\n'"
src/sdk/pynni/nni/metis_tuner/lib_data.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport math\nimport random\n\n\ndef match_val_type(vals, vals_bounds, vals_types):\n    \'\'\'\n    Update values in the array, to match their corresponding type\n    \'\'\'\n    vals_new = []\n\n    for i, _ in enumerate(vals_types):\n        if vals_types[i] == ""discrete_int"":\n            # Find the closest integer in the array, vals_bounds\n            # pylint: disable=cell-var-from-loop\n            vals_new.append(min(vals_bounds[i], key=lambda x: abs(x - vals[i])))\n        elif vals_types[i] == ""range_int"":\n            # Round down to the nearest integer\n            vals_new.append(math.floor(vals[i]))\n        elif vals_types[i] == ""range_continuous"":\n            # Don\'t do any processing for continous numbers\n            vals_new.append(vals[i])\n        else:\n            return None\n\n    return vals_new\n\n\ndef rand(x_bounds, x_types):\n    \'\'\'\n    Random generate variable value within their bounds\n    \'\'\'\n    outputs = []\n\n    for i, _ in enumerate(x_bounds):\n        if x_types[i] == ""discrete_int"":\n            temp = x_bounds[i][random.randint(0, len(x_bounds[i]) - 1)]\n            outputs.append(temp)\n        elif x_types[i] == ""range_int"":\n            temp = random.randint(x_bounds[i][0], x_bounds[i][1] - 1)\n            outputs.append(temp)\n        elif x_types[i] == ""range_continuous"":\n            temp = random.uniform(x_bounds[i][0], x_bounds[i][1])\n            outputs.append(temp)\n        else:\n            return None\n\n    return outputs\n'"
src/sdk/pynni/nni/metis_tuner/metis_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nmetis_tuner.py\n""""""\n\nimport copy\nimport logging\nimport random\nimport statistics\nimport warnings\nfrom multiprocessing.dummy import Pool as ThreadPool\nimport numpy as np\n\nimport nni.metis_tuner.lib_constraint_summation as lib_constraint_summation\nimport nni.metis_tuner.lib_data as lib_data\nimport nni.metis_tuner.Regression_GMM.CreateModel as gmm_create_model\nimport nni.metis_tuner.Regression_GMM.Selection as gmm_selection\nimport nni.metis_tuner.Regression_GP.CreateModel as gp_create_model\nimport nni.metis_tuner.Regression_GP.OutlierDetection as gp_outlier_detection\nimport nni.metis_tuner.Regression_GP.Prediction as gp_prediction\nimport nni.metis_tuner.Regression_GP.Selection as gp_selection\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward\n\nlogger = logging.getLogger(""Metis_Tuner_AutoML"")\n\nNONE_TYPE = \'\'\nCONSTRAINT_LOWERBOUND = None\nCONSTRAINT_UPPERBOUND = None\nCONSTRAINT_PARAMS_IDX = []\n\n\nclass MetisTuner(Tuner):\n    """"""\n    Metis Tuner\n\n    More algorithm information you could reference here:\n    https://www.microsoft.com/en-us/research/publication/metis-robustly-tuning-tail-latencies-cloud-systems/\n\n    Attributes\n    ----------\n        optimize_mode : str\n            optimize_mode is a string that including two mode ""maximize"" and ""minimize""\n\n        no_resampling : bool\n            True or False.\n            Should Metis consider re-sampling as part of the search strategy?\n            If you are confident that the training dataset is noise-free,\n            then you do not need re-sampling.\n\n        no_candidates : bool\n            True or False.\n            Should Metis suggest parameters for the next benchmark?\n            If you do not plan to do more benchmarks,\n            Metis can skip this step.\n\n        selection_num_starting_points : int\n            How many times Metis should try to find the global optimal in the search space?\n            The higher the number, the longer it takes to output the solution.\n\n        cold_start_num : int\n            Metis need some trial result to get cold start.\n            when the number of trial result is less than\n            cold_start_num, Metis will randomly sample hyper-parameter for trial.\n\n        exploration_probability: float\n            The probability of Metis to select parameter from exploration instead of exploitation.\n    """"""\n\n    def __init__(\n            self,\n            optimize_mode=""maximize"",\n            no_resampling=True,\n            no_candidates=False,\n            selection_num_starting_points=600,\n            cold_start_num=10,\n            exploration_probability=0.9):\n        """"""\n        Parameters\n        ----------\n        optimize_mode : str\n            optimize_mode is a string that including two mode ""maximize"" and ""minimize""\n\n        no_resampling : bool\n            True or False.\n            Should Metis consider re-sampling as part of the search strategy?\n            If you are confident that the training dataset is noise-free,\n            then you do not need re-sampling.\n\n        no_candidates : bool\n            True or False.\n            Should Metis suggest parameters for the next benchmark?\n            If you do not plan to do more benchmarks,\n            Metis can skip this step.\n\n        selection_num_starting_points : int\n            How many times Metis should try to find the global optimal in the search space?\n            The higher the number, the longer it takes to output the solution.\n\n        cold_start_num : int\n            Metis need some trial result to get cold start.\n            when the number of trial result is less than\n            cold_start_num, Metis will randomly sample hyper-parameter for trial.\n\n        exploration_probability : float\n            The probability of Metis to select parameter from exploration instead of exploitation.\n\n        x_bounds : list\n            The constration of parameters.\n\n        x_types : list\n            The type of parameters.\n        """"""\n\n        self.samples_x = []\n        self.samples_y = []\n        self.samples_y_aggregation = []\n        self.total_data = []\n        self.space = None\n        self.no_resampling = no_resampling\n        self.no_candidates = no_candidates\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.key_order = []\n        self.cold_start_num = cold_start_num\n        self.selection_num_starting_points = selection_num_starting_points\n        self.exploration_probability = exploration_probability\n        self.minimize_constraints_fun = None\n        self.minimize_starting_points = None\n        self.supplement_data_num = 0\n        self.x_bounds = []\n        self.x_types = []\n\n\n    def update_search_space(self, search_space):\n        """"""\n        Update the self.x_bounds and self.x_types by the search_space.json\n\n        Parameters\n        ----------\n        search_space : dict\n        """"""\n        self.x_bounds = [[] for i in range(len(search_space))]\n        self.x_types = [NONE_TYPE for i in range(len(search_space))]\n\n        for key in search_space:\n            self.key_order.append(key)\n\n        key_type = {}\n        if isinstance(search_space, dict):\n            for key in search_space:\n                key_type = search_space[key][\'_type\']\n                key_range = search_space[key][\'_value\']\n                idx = self.key_order.index(key)\n                if key_type == \'quniform\':\n                    if key_range[2] == 1 and key_range[0].is_integer(\n                    ) and key_range[1].is_integer():\n                        self.x_bounds[idx] = [key_range[0], key_range[1] + 1]\n                        self.x_types[idx] = \'range_int\'\n                    else:\n                        low, high, q = key_range\n                        bounds = np.clip(\n                            np.arange(\n                                np.round(\n                                    low / q),\n                                np.round(\n                                    high / q) + 1) * q,\n                            low,\n                            high)\n                        self.x_bounds[idx] = bounds\n                        self.x_types[idx] = \'discrete_int\'\n                elif key_type == \'randint\':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = \'range_int\'\n                elif key_type == \'uniform\':\n                    self.x_bounds[idx] = [key_range[0], key_range[1]]\n                    self.x_types[idx] = \'range_continuous\'\n                elif key_type == \'choice\':\n                    self.x_bounds[idx] = key_range\n\n                    for key_value in key_range:\n                        if not isinstance(key_value, (int, float)):\n                            raise RuntimeError(\n                                ""Metis Tuner only support numerical choice."")\n\n                    self.x_types[idx] = \'discrete_int\'\n                else:\n                    logger.info(\n                        ""Metis Tuner doesn\'t support this kind of variable: %s"",\n                        str(key_type))\n                    raise RuntimeError(\n                        ""Metis Tuner doesn\'t support this kind of variable: %s"" %\n                        str(key_type))\n        else:\n            logger.info(""The format of search space is not a dict."")\n            raise RuntimeError(""The format of search space is not a dict."")\n\n        self.minimize_starting_points = _rand_init(\n            self.x_bounds, self.x_types, self.selection_num_starting_points)\n\n\n    def _pack_output(self, init_parameter):\n        """"""\n        Pack the output\n\n        Parameters\n        ----------\n        init_parameter : dict\n\n        Returns\n        -------\n        output : dict\n        """"""\n        output = {}\n        for i, param in enumerate(init_parameter):\n            output[self.key_order[i]] = param\n\n        return output\n\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Generate next parameter for trial\n\n        If the number of trial result is lower than cold start number,\n        metis will first random generate some parameters.\n        Otherwise, metis will choose the parameters by\n        the Gussian Process Model and the Gussian Mixture Model.\n\n        Parameters\n        ----------\n        parameter_id : int\n\n        Returns\n        -------\n        result : dict\n        """"""\n        if len(self.samples_x) < self.cold_start_num:\n            init_parameter = _rand_init(self.x_bounds, self.x_types, 1)[0]\n            results = self._pack_output(init_parameter)\n        else:\n            self.minimize_starting_points = _rand_init(\n                self.x_bounds, self.x_types, self.selection_num_starting_points)\n            results = self._selection(\n                self.samples_x,\n                self.samples_y_aggregation,\n                self.samples_y,\n                self.x_bounds,\n                self.x_types,\n                threshold_samplessize_resampling=(\n                    None if self.no_resampling is True else 50),\n                no_candidates=self.no_candidates,\n                minimize_starting_points=self.minimize_starting_points,\n                minimize_constraints_fun=self.minimize_constraints_fun)\n\n        logger.info(""Generate paramageters: \\n%s"", str(results))\n        return results\n\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Tuner receive result from trial.\n\n        Parameters\n        ----------\n        parameter_id : int\n            The id of parameters, generated by nni manager.\n        parameters : dict\n            A group of parameters that trial has tried.\n        value : dict/float\n            if value is dict, it should have ""default"" key.\n        """"""\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Maximize:\n            value = -value\n\n        logger.info(""Received trial result."")\n        logger.info(""value is : %s"", str(value))\n        logger.info(""parameter is : %s"", str(parameters))\n\n        # parse parameter to sample_x\n        sample_x = [0 for i in range(len(self.key_order))]\n        for key in parameters:\n            idx = self.key_order.index(key)\n            sample_x[idx] = parameters[key]\n\n        # parse value to sample_y\n        temp_y = []\n        if sample_x in self.samples_x:\n            idx = self.samples_x.index(sample_x)\n            temp_y = self.samples_y[idx]\n            temp_y.append(value)\n            self.samples_y[idx] = temp_y\n\n            # calculate y aggregation\n            median = get_median(temp_y)\n            self.samples_y_aggregation[idx] = [median]\n        else:\n            self.samples_x.append(sample_x)\n            self.samples_y.append([value])\n\n            # calculate y aggregation\n            self.samples_y_aggregation.append([value])\n\n\n    def _selection(\n            self,\n            samples_x,\n            samples_y_aggregation,\n            samples_y,\n            x_bounds,\n            x_types,\n            max_resampling_per_x=3,\n            threshold_samplessize_exploitation=12,\n            threshold_samplessize_resampling=50,\n            no_candidates=False,\n            minimize_starting_points=None,\n            minimize_constraints_fun=None):\n\n        with warnings.catch_warnings():\n            warnings.simplefilter(""ignore"")\n\n        next_candidate = None\n        candidates = []\n        samples_size_all = sum([len(i) for i in samples_y])\n        samples_size_unique = len(samples_y)\n\n        # ===== STEP 1: Compute the current optimum =====\n        gp_model = gp_create_model.create_model(\n            samples_x, samples_y_aggregation)\n        lm_current = gp_selection.selection(\n            ""lm"",\n            samples_y_aggregation,\n            x_bounds,\n            x_types,\n            gp_model[\'model\'],\n            minimize_starting_points,\n            minimize_constraints_fun=minimize_constraints_fun)\n        if not lm_current:\n            return None\n        logger.info({\n            \'hyperparameter\': lm_current[\'hyperparameter\'],\n            \'expected_mu\': lm_current[\'expected_mu\'],\n            \'expected_sigma\': lm_current[\'expected_sigma\'],\n            \'reason\': ""exploitation_gp""\n        })\n\n        if no_candidates is False:\n            # ===== STEP 2: Get recommended configurations for exploration ====\n            results_exploration = gp_selection.selection(\n                ""lc"",\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                gp_model[\'model\'],\n                minimize_starting_points,\n                minimize_constraints_fun=minimize_constraints_fun)\n\n            if results_exploration is not None:\n                if _num_past_samples(results_exploration[\'hyperparameter\'], samples_x, samples_y) == 0:\n                    temp_candidate = {\n                        \'hyperparameter\': results_exploration[\'hyperparameter\'],\n                        \'expected_mu\': results_exploration[\'expected_mu\'],\n                        \'expected_sigma\': results_exploration[\'expected_sigma\'],\n                        \'reason\': ""exploration""\n                    }\n                    candidates.append(temp_candidate)\n\n                    logger.info(""DEBUG: 1 exploration candidate selected\\n"")\n                    logger.info(temp_candidate)\n            else:\n                logger.info(""DEBUG: No suitable exploration candidates were"")\n\n            # ===== STEP 3: Get recommended configurations for exploitation ===\n            if samples_size_all >= threshold_samplessize_exploitation:\n                logger.info(""Getting candidates for exploitation...\\n"")\n                try:\n                    gmm = gmm_create_model.create_model(\n                        samples_x, samples_y_aggregation)\n\n                    if (""discrete_int"" in x_types) or (""range_int"" in x_types):\n                        results_exploitation = gmm_selection.selection(\n                            x_bounds,\n                            x_types,\n                            gmm[\'clusteringmodel_good\'],\n                            gmm[\'clusteringmodel_bad\'],\n                            minimize_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n                    else:\n                        # If all parameters are of ""range_continuous"",\n                        # let\'s use GMM to generate random starting points\n                        results_exploitation = gmm_selection.selection_r(\n                            x_bounds,\n                            x_types,\n                            gmm[\'clusteringmodel_good\'],\n                            gmm[\'clusteringmodel_bad\'],\n                            num_starting_points=self.selection_num_starting_points,\n                            minimize_constraints_fun=minimize_constraints_fun)\n\n                    if results_exploitation is not None:\n                        if _num_past_samples(results_exploitation[\'hyperparameter\'], samples_x, samples_y) == 0:\n                            temp_expected_mu, temp_expected_sigma = \\\n                                    gp_prediction.predict(results_exploitation[\'hyperparameter\'], gp_model[\'model\'])\n                            temp_candidate = {\n                                \'hyperparameter\': results_exploitation[\'hyperparameter\'],\n                                \'expected_mu\': temp_expected_mu,\n                                \'expected_sigma\': temp_expected_sigma,\n                                \'reason\': ""exploitation_gmm""\n                            }\n                            candidates.append(temp_candidate)\n\n                            logger.info(\n                                ""DEBUG: 1 exploitation_gmm candidate selected\\n"")\n                            logger.info(temp_candidate)\n                    else:\n                        logger.info(\n                            ""DEBUG: No suitable exploitation_gmm candidates were found\\n"")\n\n                except ValueError as exception:\n                    # The exception: ValueError: Fitting the mixture model failed\n                    # because some components have ill-defined empirical covariance\n                    # (for instance caused by singleton or collapsed samples).\n                    # Try to decrease the number of components, or increase\n                    # reg_covar.\n                    logger.info(\n                        ""DEBUG: No suitable exploitation_gmm \\\n                        candidates were found due to exception."")\n                    logger.info(exception)\n\n            # ===== STEP 4: Get a list of outliers =====\n            if (threshold_samplessize_resampling is not None) and \\\n                    (samples_size_unique >= threshold_samplessize_resampling):\n                logger.info(""Getting candidates for re-sampling...\\n"")\n                results_outliers = gp_outlier_detection.outlierDetection_threaded(\n                    samples_x, samples_y_aggregation)\n\n                if results_outliers is not None:\n                    for results_outlier in results_outliers:  # pylint: disable=not-an-iterable\n                        if _num_past_samples(samples_x[results_outlier[\'samples_idx\']], samples_x, samples_y) < max_resampling_per_x:\n                            temp_candidate = {\'hyperparameter\': samples_x[results_outlier[\'samples_idx\']],\\\n                                               \'expected_mu\': results_outlier[\'expected_mu\'],\\\n                                               \'expected_sigma\': results_outlier[\'expected_sigma\'],\\\n                                               \'reason\': ""resampling""}\n                            candidates.append(temp_candidate)\n                    logger.info(""DEBUG: %d re-sampling candidates selected\\n"")\n                    logger.info(temp_candidate)\n                else:\n                    logger.info(\n                        ""DEBUG: No suitable resampling candidates were found\\n"")\n\n            if candidates:\n                # ===== STEP 5: Compute the information gain of each candidate\n                logger.info(\n                    ""Evaluating information gain of %d candidates...\\n"")\n                next_improvement = 0\n\n                threads_inputs = [[\n                    candidate, samples_x, samples_y, x_bounds, x_types,\n                    minimize_constraints_fun, minimize_starting_points\n                ] for candidate in candidates]\n                threads_pool = ThreadPool(4)\n                # Evaluate what would happen if we actually sample each\n                # candidate\n                threads_results = threads_pool.map(\n                    _calculate_lowest_mu_threaded, threads_inputs)\n                threads_pool.close()\n                threads_pool.join()\n\n                for threads_result in threads_results:\n                    if threads_result[\'expected_lowest_mu\'] < lm_current[\'expected_mu\']:\n                        # Information gain\n                        temp_improvement = threads_result[\'expected_lowest_mu\'] - \\\n                            lm_current[\'expected_mu\']\n\n                        if next_improvement > temp_improvement:\n                            next_improvement = temp_improvement\n                            next_candidate = threads_result[\'candidate\']\n            else:\n                # ===== STEP 6: If we have no candidates, randomly pick one ===\n                logger.info(\n                    ""DEBUG: No candidates from exploration, exploitation,\\\n                                 and resampling. We will random a candidate for next_candidate\\n""\n                )\n\n                next_candidate = _rand_with_constraints(\n                    x_bounds,\n                    x_types) if minimize_starting_points is None else minimize_starting_points[0]\n                next_candidate = lib_data.match_val_type(\n                    next_candidate, x_bounds, x_types)\n                expected_mu, expected_sigma = gp_prediction.predict(\n                    next_candidate, gp_model[\'model\'])\n                next_candidate = {\n                    \'hyperparameter\': next_candidate,\n                    \'reason\': ""random"",\n                    \'expected_mu\': expected_mu,\n                    \'expected_sigma\': expected_sigma}\n\n        # STEP 7: If current optimal hyperparameter occurs in the history\n        # or exploration probability is less than the threshold, take next\n        # config as exploration step\n        outputs = self._pack_output(lm_current[\'hyperparameter\'])\n        ap = random.uniform(0, 1)\n        if outputs in self.total_data or ap <= self.exploration_probability:\n            if next_candidate is not None:\n                outputs = self._pack_output(next_candidate[\'hyperparameter\'])\n            else:\n                random_parameter = _rand_init(x_bounds, x_types, 1)[0]\n                outputs = self._pack_output(random_parameter)\n        self.total_data.append(outputs)\n        return outputs\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning\n\n        Parameters\n        ----------\n        data : a list of dict\n               each of which has at least two keys: \'parameter\' and \'value\'.\n        """"""\n        _completed_num = 0\n        for trial_info in data:\n            logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            self.supplement_data_num += 1\n            _parameter_id = \'_\'.join(\n                [""ImportData"", str(self.supplement_data_num)])\n            self.total_data.append(_params)\n            self.receive_trial_result(\n                parameter_id=_parameter_id,\n                parameters=_params,\n                value=_value)\n        logger.info(""Successfully import data to metis tuner."")\n\n\ndef _rand_with_constraints(x_bounds, x_types):\n    outputs = None\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n\n    x_val_withconstraints = lib_constraint_summation.rand(\n        x_bounds_withconstraints,\n        x_types_withconstraints,\n        CONSTRAINT_LOWERBOUND,\n        CONSTRAINT_UPPERBOUND)\n    if not x_val_withconstraints:\n        outputs = [None] * len(x_bounds)\n\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n\n        for i, output in enumerate(outputs):\n            if not output:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs\n\n\ndef _calculate_lowest_mu_threaded(inputs):\n    [candidate, samples_x, samples_y, x_bounds, x_types,\n     minimize_constraints_fun, minimize_starting_points] = inputs\n\n    outputs = {""candidate"": candidate, ""expected_lowest_mu"": None}\n\n    for expected_mu in [\n            candidate[\'expected_mu\'] +\n            1.96 *\n            candidate[\'expected_sigma\'],\n            candidate[\'expected_mu\'] -\n            1.96 *\n            candidate[\'expected_sigma\']]:\n        temp_samples_x = copy.deepcopy(samples_x)\n        temp_samples_y = copy.deepcopy(samples_y)\n\n        try:\n            idx = temp_samples_x.index(candidate[\'hyperparameter\'])\n            # This handles the case of re-sampling a potential outlier\n            temp_samples_y[idx].append(expected_mu)\n        except ValueError:\n            temp_samples_x.append(candidate[\'hyperparameter\'])\n            temp_samples_y.append([expected_mu])\n\n        # Aggregates multiple observation of the sample sampling points\n        temp_y_aggregation = [statistics.median(\n            temp_sample_y) for temp_sample_y in temp_samples_y]\n        temp_gp = gp_create_model.create_model(\n            temp_samples_x, temp_y_aggregation)\n        temp_results = gp_selection.selection(\n            ""lm"",\n            temp_y_aggregation,\n            x_bounds,\n            x_types,\n            temp_gp[\'model\'],\n            minimize_starting_points,\n            minimize_constraints_fun=minimize_constraints_fun)\n\n        if outputs[""expected_lowest_mu""] is None \\\n            or outputs[""expected_lowest_mu""] > temp_results[\'expected_mu\']:\n            outputs[""expected_lowest_mu""] = temp_results[\'expected_mu\']\n\n    return outputs\n\n\ndef _num_past_samples(x, samples_x, samples_y):\n    try:\n        idx = samples_x.index(x)\n        return len(samples_y[idx])\n    except ValueError:\n        logger.info(""x not in sample_x"")\n        return 0\n\n\ndef _rand_init(x_bounds, x_types, selection_num_starting_points):\n    \'\'\'\n    Random sample some init seed within bounds.\n    \'\'\'\n    return [lib_data.rand(x_bounds, x_types) for i\n            in range(0, selection_num_starting_points)]\n\n\ndef get_median(temp_list):\n    """"""\n    Return median\n    """"""\n    num = len(temp_list)\n    temp_list.sort()\n    print(temp_list)\n    if num % 2 == 0:\n        median = (temp_list[int(num / 2)] + temp_list[int(num / 2) - 1]) / 2\n    else:\n        median = temp_list[int(num / 2)]\n    return median\n'"
src/sdk/pynni/nni/nas/__init__.py,0,b''
src/sdk/pynni/nni/networkmorphism_tuner/__init__.py,0,b''
src/sdk/pynni/nni/networkmorphism_tuner/bayesian.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport math\nimport random\nfrom copy import deepcopy\nfrom functools import total_ordering\nfrom queue import PriorityQueue\n\nimport numpy as np\nfrom scipy.linalg import LinAlgError, cho_solve, cholesky, solve_triangular\nfrom scipy.optimize import linear_sum_assignment\nfrom sklearn.metrics.pairwise import rbf_kernel\n\nfrom nni.utils import OptimizeMode\nfrom nni.networkmorphism_tuner.graph_transformer import transform\nfrom nni.networkmorphism_tuner.utils import Constant\nfrom nni.networkmorphism_tuner.layers import is_layer\n\n\ndef layer_distance(a, b):\n    """"""The distance between two layers.""""""\n    # pylint: disable=unidiomatic-typecheck\n    if not isinstance(a, type(b)):\n        return 1.0\n    if is_layer(a, ""Conv""):\n        att_diff = [\n            (a.filters, b.filters),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    if is_layer(a, ""Pooling""):\n        att_diff = [\n            (a.padding, b.padding),\n            (a.kernel_size, b.kernel_size),\n            (a.stride, b.stride),\n        ]\n        return attribute_difference(att_diff)\n    return 0.0\n\n\ndef attribute_difference(att_diff):\n    \'\'\' The attribute distance.\n    \'\'\'\n\n    ret = 0\n    for a_value, b_value in att_diff:\n        if max(a_value, b_value) == 0:\n            ret += 0\n        else:\n            ret += abs(a_value - b_value) * 1.0 / max(a_value, b_value)\n    return ret * 1.0 / len(att_diff)\n\n\ndef layers_distance(list_a, list_b):\n    """"""The distance between the layers of two neural networks.""""""\n    len_a = len(list_a)\n    len_b = len(list_b)\n    f = np.zeros((len_a + 1, len_b + 1))\n    f[-1][-1] = 0\n    for i in range(-1, len_a):\n        f[i][-1] = i + 1\n    for j in range(-1, len_b):\n        f[-1][j] = j + 1\n    for i in range(len_a):\n        for j in range(len_b):\n            f[i][j] = min(\n                f[i][j - 1] + 1,\n                f[i - 1][j] + 1,\n                f[i - 1][j - 1] + layer_distance(list_a[i], list_b[j]),\n            )\n    return f[len_a - 1][len_b - 1]\n\n\ndef skip_connection_distance(a, b):\n    """"""The distance between two skip-connections.""""""\n    if a[2] != b[2]:\n        return 1.0\n    len_a = abs(a[1] - a[0])\n    len_b = abs(b[1] - b[0])\n    return (abs(a[0] - b[0]) + abs(len_a - len_b)) / \\\n        (max(a[0], b[0]) + max(len_a, len_b))\n\n\ndef skip_connections_distance(list_a, list_b):\n    """"""The distance between the skip-connections of two neural networks.""""""\n    distance_matrix = np.zeros((len(list_a), len(list_b)))\n    for i, a in enumerate(list_a):\n        for j, b in enumerate(list_b):\n            distance_matrix[i][j] = skip_connection_distance(a, b)\n    return distance_matrix[linear_sum_assignment(distance_matrix)].sum() + abs(\n        len(list_a) - len(list_b)\n    )\n\n\ndef edit_distance(x, y):\n    """"""The distance between two neural networks.\n    Args:\n        x: An instance of NetworkDescriptor.\n        y: An instance of NetworkDescriptor\n    Returns:\n        The edit-distance between x and y.\n    """"""\n\n    ret = layers_distance(x.layers, y.layers)\n    ret += Constant.KERNEL_LAMBDA * skip_connections_distance(\n        x.skip_connections, y.skip_connections\n    )\n    return ret\n\n\nclass IncrementalGaussianProcess:\n    """"""Gaussian process regressor.\n    Attributes:\n        alpha: A hyperparameter.\n    """"""\n\n    def __init__(self):\n        self.alpha = 1e-10\n        self._distance_matrix = None\n        self._x = None\n        self._y = None\n        self._first_fitted = False\n        self._l_matrix = None\n        self._alpha_vector = None\n\n    @property\n    def kernel_matrix(self):\n        \'\'\' Kernel matric.\n        \'\'\'\n        return self._distance_matrix\n\n    def fit(self, train_x, train_y):\n        """""" Fit the regressor with more data.\n        Args:\n            train_x: A list of NetworkDescriptor.\n            train_y: A list of metric values.\n        """"""\n        if self.first_fitted:\n            self.incremental_fit(train_x, train_y)\n        else:\n            self.first_fit(train_x, train_y)\n\n    def incremental_fit(self, train_x, train_y):\n        """""" Incrementally fit the regressor. """"""\n        if not self._first_fitted:\n            raise ValueError(\n                ""The first_fit function needs to be called first."")\n\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        # Incrementally compute K\n        up_right_k = edit_distance_matrix(self._x, train_x)\n        down_left_k = np.transpose(up_right_k)\n        down_right_k = edit_distance_matrix(train_x)\n        up_k = np.concatenate((self._distance_matrix, up_right_k), axis=1)\n        down_k = np.concatenate((down_left_k, down_right_k), axis=1)\n        temp_distance_matrix = np.concatenate((up_k, down_k), axis=0)\n        k_matrix = bourgain_embedding_matrix(temp_distance_matrix)\n        diagonal = np.diag_indices_from(k_matrix)\n        diagonal = (diagonal[0][-len(train_x):], diagonal[1][-len(train_x):])\n        k_matrix[diagonal] += self.alpha\n\n        try:\n            self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n        except LinAlgError:\n            return self\n\n        self._x = np.concatenate((self._x, train_x), axis=0)\n        self._y = np.concatenate((self._y, train_y), axis=0)\n        self._distance_matrix = temp_distance_matrix\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        return self\n\n    @property\n    def first_fitted(self):\n        \'\'\' if it is firsr fitted\n        \'\'\'\n        return self._first_fitted\n\n    def first_fit(self, train_x, train_y):\n        """""" Fit the regressor for the first time. """"""\n        train_x, train_y = np.array(train_x), np.array(train_y)\n\n        self._x = np.copy(train_x)\n        self._y = np.copy(train_y)\n\n        self._distance_matrix = edit_distance_matrix(self._x)\n        k_matrix = bourgain_embedding_matrix(self._distance_matrix)\n        k_matrix[np.diag_indices_from(k_matrix)] += self.alpha\n\n        self._l_matrix = cholesky(k_matrix, lower=True)  # Line 2\n\n        self._alpha_vector = cho_solve(\n            (self._l_matrix, True), self._y)  # Line 3\n\n        self._first_fitted = True\n        return self\n\n    def predict(self, train_x):\n        """"""Predict the result.\n        Args:\n            train_x: A list of NetworkDescriptor.\n        Returns:\n            y_mean: The predicted mean.\n            y_std: The predicted standard deviation.\n        """"""\n        k_trans = np.exp(-np.power(edit_distance_matrix(train_x, self._x), 2))\n        y_mean = k_trans.dot(self._alpha_vector)  # Line 4 (y_mean = f_star)\n\n        # compute inverse K_inv of K based on its Cholesky\n        # decomposition L and its inverse L_inv\n        l_inv = solve_triangular(\n            self._l_matrix.T, np.eye(\n                self._l_matrix.shape[0]))\n        k_inv = l_inv.dot(l_inv.T)\n        # Compute variance of predictive distribution\n        y_var = np.ones(len(train_x), dtype=np.float)\n        y_var -= np.einsum(""ij,ij->i"", np.dot(k_trans, k_inv), k_trans)\n\n        # Check if any of the variances is negative because of\n        # numerical issues. If yes: set the variance to 0.\n        y_var_negative = y_var < 0\n        if np.any(y_var_negative):\n            y_var[y_var_negative] = 0.0\n        return y_mean, np.sqrt(y_var)\n\n\ndef edit_distance_matrix(train_x, train_y=None):\n    """"""Calculate the edit distance.\n    Args:\n        train_x: A list of neural architectures.\n        train_y: A list of neural architectures.\n    Returns:\n        An edit-distance matrix.\n    """"""\n    if train_y is None:\n        ret = np.zeros((train_x.shape[0], train_x.shape[0]))\n        for x_index, x in enumerate(train_x):\n            for y_index, y in enumerate(train_x):\n                if x_index == y_index:\n                    ret[x_index][y_index] = 0\n                elif x_index < y_index:\n                    ret[x_index][y_index] = edit_distance(x, y)\n                else:\n                    ret[x_index][y_index] = ret[y_index][x_index]\n        return ret\n    ret = np.zeros((train_x.shape[0], train_y.shape[0]))\n    for x_index, x in enumerate(train_x):\n        for y_index, y in enumerate(train_y):\n            ret[x_index][y_index] = edit_distance(x, y)\n    return ret\n\n\ndef vector_distance(a, b):\n    """"""The Euclidean distance between two vectors.""""""\n    a = np.array(a)\n    b = np.array(b)\n    return np.linalg.norm(a - b)\n\n\ndef bourgain_embedding_matrix(distance_matrix):\n    """"""Use Bourgain algorithm to embed the neural architectures based on their edit-distance.\n    Args:\n        distance_matrix: A matrix of edit-distances.\n    Returns:\n        A matrix of distances after embedding.\n    """"""\n    distance_matrix = np.array(distance_matrix)\n    n = len(distance_matrix)\n    if n == 1:\n        return distance_matrix\n    np.random.seed(123)\n    distort_elements = []\n    r = range(n)\n    k = int(math.ceil(math.log(n) / math.log(2) - 1))\n    t = int(math.ceil(math.log(n)))\n    counter = 0\n    for i in range(0, k + 1):\n        for t in range(t):\n            s = np.random.choice(r, 2 ** i)\n            for j in r:\n                d = min([distance_matrix[j][s] for s in s])\n                counter += len(s)\n                if i == 0 and t == 0:\n                    distort_elements.append([d])\n                else:\n                    distort_elements[j].append(d)\n    return rbf_kernel(distort_elements, distort_elements)\n\n\nclass BayesianOptimizer:\n    """""" A Bayesian optimizer for neural architectures.\n    Attributes:\n        searcher: The Searcher which is calling the Bayesian optimizer.\n        t_min: The minimum temperature for simulated annealing.\n        metric: An instance of the Metric subclasses.\n        gpr: A GaussianProcessRegressor for bayesian optimization.\n        beta: The beta in acquisition function. (refer to our paper)\n        search_tree: The network morphism search tree.\n    """"""\n\n    def __init__(self, searcher, t_min, optimizemode, beta=None):\n        self.searcher = searcher\n        self.t_min = t_min\n        self.optimizemode = optimizemode\n        self.gpr = IncrementalGaussianProcess()\n        self.beta = beta if beta is not None else Constant.BETA\n        self.search_tree = SearchTree()\n\n    def fit(self, x_queue, y_queue):\n        """""" Fit the optimizer with new architectures and performances.\n        Args:\n            x_queue: A list of NetworkDescriptor.\n            y_queue: A list of metric values.\n        """"""\n        self.gpr.fit(x_queue, y_queue)\n\n    def generate(self, descriptors):\n        """"""Generate new architecture.\n        Args:\n            descriptors: All the searched neural architectures.\n        Returns:\n            graph: An instance of Graph. A morphed neural network with weights.\n            father_id: The father node ID in the search tree.\n        """"""\n        model_ids = self.search_tree.adj_list.keys()\n\n        target_graph = None\n        father_id = None\n        descriptors = deepcopy(descriptors)\n        elem_class = Elem\n        if self.optimizemode is OptimizeMode.Maximize:\n            elem_class = ReverseElem\n\n        # Initialize the priority queue.\n        pq = PriorityQueue()\n        temp_list = []\n        for model_id in model_ids:\n            metric_value = self.searcher.get_metric_value_by_id(model_id)\n            temp_list.append((metric_value, model_id))\n        temp_list = sorted(temp_list)\n        for metric_value, model_id in temp_list:\n            graph = self.searcher.load_model_by_id(model_id)\n            graph.clear_operation_history()\n            graph.clear_weights()\n            pq.put(elem_class(metric_value, model_id, graph))\n\n        t = 1.0\n        t_min = self.t_min\n        alpha = 0.9\n        opt_acq = self._get_init_opt_acq_value()\n        while not pq.empty() and t > t_min:\n            elem = pq.get()\n            if self.optimizemode is OptimizeMode.Maximize:\n                temp_exp = min((elem.metric_value - opt_acq) / t, 1.0)\n            else:\n                temp_exp = min((opt_acq - elem.metric_value) / t, 1.0)\n            ap = math.exp(temp_exp)\n            if ap >= random.uniform(0, 1):\n                for temp_graph in transform(elem.graph):\n                    if contain(descriptors, temp_graph.extract_descriptor()):\n                        continue\n\n                    temp_acq_value = self.acq(temp_graph)\n                    pq.put(\n                        elem_class(\n                            temp_acq_value,\n                            elem.father_id,\n                            temp_graph))\n                    descriptors.append(temp_graph.extract_descriptor())\n                    if self._accept_new_acq_value(opt_acq, temp_acq_value):\n                        opt_acq = temp_acq_value\n                        father_id = elem.father_id\n                        target_graph = deepcopy(temp_graph)\n            t *= alpha\n\n        # Did not found a not duplicated architecture\n        if father_id is None:\n            return None, None\n        nm_graph = self.searcher.load_model_by_id(father_id)\n        for args in target_graph.operation_history:\n            getattr(nm_graph, args[0])(*list(args[1:]))\n        return nm_graph, father_id\n\n    def acq(self, graph):\n        \'\'\' estimate the value of generated graph\n        \'\'\'\n        mean, std = self.gpr.predict(np.array([graph.extract_descriptor()]))\n        if self.optimizemode is OptimizeMode.Maximize:\n            return mean + self.beta * std\n        return mean - self.beta * std\n\n    def _get_init_opt_acq_value(self):\n        if self.optimizemode is OptimizeMode.Maximize:\n            return -np.inf\n        return np.inf\n\n    def _accept_new_acq_value(self, opt_acq, temp_acq_value):\n        if temp_acq_value > opt_acq and self.optimizemode is OptimizeMode.Maximize:\n            return True\n        if temp_acq_value < opt_acq and not self.optimizemode is OptimizeMode.Maximize:\n            return True\n        return False\n\n    def add_child(self, father_id, model_id):\n        \'\'\' add child to the search tree\n        Arguments:\n            father_id {int} -- father id\n            model_id {int} -- model id\n        \'\'\'\n\n        self.search_tree.add_child(father_id, model_id)\n\n\n@total_ordering\nclass Elem:\n    """"""Elements to be sorted according to metric value.""""""\n\n    def __init__(self, metric_value, father_id, graph):\n        self.father_id = father_id\n        self.graph = graph\n        self.metric_value = metric_value\n\n    def __eq__(self, other):\n        return self.metric_value == other.metric_value\n\n    def __lt__(self, other):\n        return self.metric_value < other.metric_value\n\n\nclass ReverseElem(Elem):\n    """"""Elements to be reversely sorted according to metric value.""""""\n\n    def __lt__(self, other):\n        return self.metric_value > other.metric_value\n\n\ndef contain(descriptors, target_descriptor):\n    """"""Check if the target descriptor is in the descriptors.""""""\n    for descriptor in descriptors:\n        if edit_distance(descriptor, target_descriptor) < 1e-5:\n            return True\n    return False\n\n\nclass SearchTree:\n    """"""The network morphism search tree.""""""\n\n    def __init__(self):\n        self.root = None\n        self.adj_list = {}\n\n    def add_child(self, u, v):\n        \'\'\' add child to search tree itself.\n        Arguments:\n            u {int} -- father id\n            v {int} --  child id\n        \'\'\'\n\n        if u == -1:\n            self.root = v\n            self.adj_list[v] = []\n            return\n        if v not in self.adj_list[u]:\n            self.adj_list[u].append(v)\n        if v not in self.adj_list:\n            self.adj_list[v] = []\n\n    def get_dict(self, u=None):\n        """""" A recursive function to return the content of the tree in a dict.""""""\n        if u is None:\n            return self.get_dict(self.root)\n        children = []\n        for v in self.adj_list[u]:\n            children.append(self.get_dict(v))\n        ret = {""name"": u, ""children"": children}\n        return ret\n'"
src/sdk/pynni/nni/networkmorphism_tuner/graph.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nfrom collections.abc import Iterable\nfrom copy import deepcopy, copy\nfrom queue import Queue\n\nimport numpy as np\nimport torch\n\nfrom nni.networkmorphism_tuner.layer_transformer import (\n    add_noise,\n    wider_bn,\n    wider_next_conv,\n    wider_next_dense,\n    wider_pre_conv,\n    wider_pre_dense,\n    init_dense_weight,\n    init_conv_weight,\n    init_bn_weight,\n)\nfrom nni.networkmorphism_tuner.layers import (\n    StubAdd,\n    StubConcatenate,\n    StubReLU,\n    get_batch_norm_class,\n    get_conv_class,\n    is_layer,\n    layer_width,\n    set_keras_weight_to_stub,\n    set_stub_weight_to_keras,\n    set_stub_weight_to_torch,\n    set_torch_weight_to_stub,\n    to_real_keras_layer,\n    layer_description_extractor,\n    layer_description_builder,\n)\nfrom nni.networkmorphism_tuner.utils import Constant\n\n\nclass NetworkDescriptor:\n    """"""A class describing the neural architecture for neural network kernel.\n    It only record the width of convolutional and dense layers, and the skip-connection types and positions.\n    """"""\n\n    CONCAT_CONNECT = ""concat""\n    ADD_CONNECT = ""add""\n\n    def __init__(self):\n        self.skip_connections = []\n        self.layers = []\n\n    @property\n    def n_layers(self):\n        return len(self.layers)\n\n    def add_skip_connection(self, u, v, connection_type):\n        """""" Add a skip-connection to the descriptor.\n        Args:\n            u: Number of convolutional layers before the starting point.\n            v: Number of convolutional layers before the ending point.\n            connection_type: Must be either CONCAT_CONNECT or ADD_CONNECT.\n        """"""\n        if connection_type not in [self.CONCAT_CONNECT, self.ADD_CONNECT]:\n            raise ValueError(\n                ""connection_type should be NetworkDescriptor.CONCAT_CONNECT ""\n                ""or NetworkDescriptor.ADD_CONNECT.""\n            )\n        self.skip_connections.append((u, v, connection_type))\n\n    def to_json(self):\n        \'\'\' NetworkDescriptor to json representation\n        \'\'\'\n\n        skip_list = []\n        for u, v, connection_type in self.skip_connections:\n            skip_list.append({""from"": u, ""to"": v, ""type"": connection_type})\n        return {""node_list"": self.layers, ""skip_list"": skip_list}\n\n    def add_layer(self, layer):\n        \'\'\' add one layer\n        \'\'\'\n\n        self.layers.append(layer)\n\n\nclass Node:\n    """"""A class for intermediate output tensor (node) in the Graph.\n    Attributes:\n        shape: A tuple describing the shape of the tensor.\n    """"""\n\n    def __init__(self, shape):\n        self.shape = shape\n\n\nclass Graph:\n    """"""A class representing the neural architecture graph of a model.\n    Graph extracts the neural architecture graph from a model.\n    Each node in the graph is a intermediate tensor between layers.\n    Each layer is an edge in the graph.\n    Notably, multiple edges may refer to the same layer.\n    (e.g. Add layer is adding two tensor into one tensor. So it is related to two edges.)\n    Attributes:\n        weighted: A boolean of whether the weights and biases in the neural network\n            should be included in the graph.\n        input_shape: A tuple of integers, which does not include the batch axis.\n        node_list: A list of integers. The indices of the list are the identifiers.\n        layer_list: A list of stub layers. The indices of the list are the identifiers.\n        node_to_id: A dict instance mapping from node integers to their identifiers.\n        layer_to_id: A dict instance mapping from stub layers to their identifiers.\n        layer_id_to_input_node_ids: A dict instance mapping from layer identifiers\n            to their input nodes identifiers.\n        layer_id_to_output_node_ids: A dict instance mapping from layer identifiers\n            to their output nodes identifiers.\n        adj_list: A two dimensional list. The adjacency list of the graph. The first dimension is\n            identified by tensor identifiers. In each edge list, the elements are two-element tuples\n            of (tensor identifier, layer identifier).\n        reverse_adj_list: A reverse adjacent list in the same format as adj_list.\n        operation_history: A list saving all the network morphism operations.\n        vis: A dictionary of temporary storage for whether an local operation has been done\n            during the network morphism.\n    """"""\n\n    def __init__(self, input_shape, weighted=True):\n        """"""Initializer for Graph.\n        """"""\n        self.input_shape = input_shape\n        self.weighted = weighted\n        self.node_list = []\n        self.layer_list = []\n        # node id start with 0\n        self.node_to_id = {}\n        self.layer_to_id = {}\n        self.layer_id_to_input_node_ids = {}\n        self.layer_id_to_output_node_ids = {}\n        self.adj_list = {}\n        self.reverse_adj_list = {}\n        self.operation_history = []\n        self.n_dim = len(input_shape) - 1\n        self.conv = get_conv_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)\n\n        self.vis = None\n        self._add_node(Node(input_shape))\n\n    def add_layer(self, layer, input_node_id):\n        """"""Add a layer to the Graph.\n        Args:\n            layer: An instance of the subclasses of StubLayer in layers.py.\n            input_node_id: An integer. The ID of the input node of the layer.\n        Returns:\n            output_node_id: An integer. The ID of the output node of the layer.\n        """"""\n        if isinstance(input_node_id, Iterable):\n            layer.input = list(map(lambda x: self.node_list[x], input_node_id))\n            output_node_id = self._add_node(Node(layer.output_shape))\n            for node_id in input_node_id:\n                self._add_edge(layer, node_id, output_node_id)\n\n        else:\n            layer.input = self.node_list[input_node_id]\n            output_node_id = self._add_node(Node(layer.output_shape))\n            self._add_edge(layer, input_node_id, output_node_id)\n\n        layer.output = self.node_list[output_node_id]\n        return output_node_id\n\n    def clear_operation_history(self):\n        self.operation_history = []\n\n    @property\n    def n_nodes(self):\n        """"""Return the number of nodes in the model.""""""\n        return len(self.node_list)\n\n    @property\n    def n_layers(self):\n        """"""Return the number of layers in the model.""""""\n        return len(self.layer_list)\n\n    def _add_node(self, node):\n        """"""Add a new node to node_list and give the node an ID.\n        Args:\n            node: An instance of Node.\n        Returns:\n            node_id: An integer.\n        """"""\n        node_id = len(self.node_list)\n        self.node_to_id[node] = node_id\n        self.node_list.append(node)\n        self.adj_list[node_id] = []\n        self.reverse_adj_list[node_id] = []\n        return node_id\n\n    def _add_edge(self, layer, input_id, output_id):\n        """"""Add a new layer to the graph. The nodes should be created in advance.""""""\n\n        if layer in self.layer_to_id:\n            layer_id = self.layer_to_id[layer]\n            if input_id not in self.layer_id_to_input_node_ids[layer_id]:\n                self.layer_id_to_input_node_ids[layer_id].append(input_id)\n            if output_id not in self.layer_id_to_output_node_ids[layer_id]:\n                self.layer_id_to_output_node_ids[layer_id].append(output_id)\n        else:\n            layer_id = len(self.layer_list)\n            self.layer_list.append(layer)\n            self.layer_to_id[layer] = layer_id\n            self.layer_id_to_input_node_ids[layer_id] = [input_id]\n            self.layer_id_to_output_node_ids[layer_id] = [output_id]\n\n        self.adj_list[input_id].append((output_id, layer_id))\n        self.reverse_adj_list[output_id].append((input_id, layer_id))\n\n    def _redirect_edge(self, u_id, v_id, new_v_id):\n        """"""Redirect the layer to a new node.\n        Change the edge originally from `u_id` to `v_id` into an edge from `u_id` to `new_v_id`\n        while keeping all other property of the edge the same.\n        """"""\n        layer_id = None\n        for index, edge_tuple in enumerate(self.adj_list[u_id]):\n            if edge_tuple[0] == v_id:\n                layer_id = edge_tuple[1]\n                self.adj_list[u_id][index] = (new_v_id, layer_id)\n                self.layer_list[layer_id].output = self.node_list[new_v_id]\n                break\n\n        for index, edge_tuple in enumerate(self.reverse_adj_list[v_id]):\n            if edge_tuple[0] == u_id:\n                layer_id = edge_tuple[1]\n                self.reverse_adj_list[v_id].remove(edge_tuple)\n                break\n        self.reverse_adj_list[new_v_id].append((u_id, layer_id))\n        for index, value in enumerate(\n                self.layer_id_to_output_node_ids[layer_id]):\n            if value == v_id:\n                self.layer_id_to_output_node_ids[layer_id][index] = new_v_id\n                break\n\n    def _replace_layer(self, layer_id, new_layer):\n        """"""Replace the layer with a new layer.""""""\n        old_layer = self.layer_list[layer_id]\n        new_layer.input = old_layer.input\n        new_layer.output = old_layer.output\n        new_layer.output.shape = new_layer.output_shape\n        self.layer_list[layer_id] = new_layer\n        self.layer_to_id[new_layer] = layer_id\n        self.layer_to_id.pop(old_layer)\n\n    @property\n    def topological_order(self):\n        """"""Return the topological order of the node IDs from the input node to the output node.""""""\n        q = Queue()\n        in_degree = {}\n        for i in range(self.n_nodes):\n            in_degree[i] = 0\n        for u in range(self.n_nodes):\n            for v, _ in self.adj_list[u]:\n                in_degree[v] += 1\n        for i in range(self.n_nodes):\n            if in_degree[i] == 0:\n                q.put(i)\n\n        order_list = []\n        while not q.empty():\n            u = q.get()\n            order_list.append(u)\n            for v, _ in self.adj_list[u]:\n                in_degree[v] -= 1\n                if in_degree[v] == 0:\n                    q.put(v)\n        return order_list\n\n    def _get_pooling_layers(self, start_node_id, end_node_id):\n        """"""Given two node IDs, return all the pooling layers between them.""""""\n        layer_list = []\n        node_list = [start_node_id]\n        assert self._depth_first_search(end_node_id, layer_list, node_list)\n        ret = []\n        for layer_id in layer_list:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, ""Pooling""):\n                ret.append(layer)\n            elif is_layer(layer, ""Conv"") and layer.stride != 1:\n                ret.append(layer)\n        return ret\n\n    def _depth_first_search(self, target_id, layer_id_list, node_list):\n        """"""Search for all the layers and nodes down the path.\n        A recursive function to search all the layers and nodes between the node in the node_list\n            and the node with target_id.""""""\n        assert len(node_list) <= self.n_nodes\n        u = node_list[-1]\n        if u == target_id:\n            return True\n\n        for v, layer_id in self.adj_list[u]:\n            layer_id_list.append(layer_id)\n            node_list.append(v)\n            if self._depth_first_search(target_id, layer_id_list, node_list):\n                return True\n            layer_id_list.pop()\n            node_list.pop()\n\n        return False\n\n    def _search(self, u, start_dim, total_dim, n_add):\n        """"""Search the graph for all the layers to be widened caused by an operation.\n        It is an recursive function with duplication check to avoid deadlock.\n        It searches from a starting node u until the corresponding layers has been widened.\n        Args:\n            u: The starting node ID.\n            start_dim: The position to insert the additional dimensions.\n            total_dim: The total number of dimensions the layer has before widening.\n            n_add: The number of dimensions to add.\n        """"""\n        if (u, start_dim, total_dim, n_add) in self.vis:\n            return\n        self.vis[(u, start_dim, total_dim, n_add)] = True\n        for v, layer_id in self.adj_list[u]:\n            layer = self.layer_list[layer_id]\n\n            if is_layer(layer, ""Conv""):\n                new_layer = wider_next_conv(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, ""Dense""):\n                new_layer = wider_next_dense(\n                    layer, start_dim, total_dim, n_add, self.weighted\n                )\n                self._replace_layer(layer_id, new_layer)\n\n            elif is_layer(layer, ""BatchNormalization""):\n                new_layer = wider_bn(\n                    layer, start_dim, total_dim, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n                self._search(v, start_dim, total_dim, n_add)\n\n            elif is_layer(layer, ""Concatenate""):\n                if self.layer_id_to_input_node_ids[layer_id][1] == u:\n                    # u is on the right of the concat\n                    # next_start_dim += next_total_dim - total_dim\n                    left_dim = self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][0]\n                    )\n                    next_start_dim = start_dim + left_dim\n                    next_total_dim = total_dim + left_dim\n                else:\n                    next_start_dim = start_dim\n                    next_total_dim = total_dim + self._upper_layer_width(\n                        self.layer_id_to_input_node_ids[layer_id][1]\n                    )\n                self._search(v, next_start_dim, next_total_dim, n_add)\n\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, ""Conv""):\n                new_layer = wider_pre_conv(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, ""Dense""):\n                new_layer = wider_pre_dense(layer, n_add, self.weighted)\n                self._replace_layer(layer_id, new_layer)\n            elif is_layer(layer, ""Concatenate""):\n                continue\n            else:\n                self._search(v, start_dim, total_dim, n_add)\n\n    def _upper_layer_width(self, u):\n        for v, layer_id in self.reverse_adj_list[u]:\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, ""Conv"") or is_layer(layer, ""Dense""):\n                return layer_width(layer)\n            elif is_layer(layer, ""Concatenate""):\n                a = self.layer_id_to_input_node_ids[layer_id][0]\n                b = self.layer_id_to_input_node_ids[layer_id][1]\n                return self._upper_layer_width(a) + self._upper_layer_width(b)\n            else:\n                return self._upper_layer_width(v)\n        return self.node_list[0].shape[-1]\n\n    def to_deeper_model(self, target_id, new_layer):\n        """"""Insert a relu-conv-bn block after the target block.\n        Args:\n            target_id: A convolutional layer ID. The new block should be inserted after the block.\n            new_layer: An instance of StubLayer subclasses.\n        """"""\n        self.operation_history.append(\n            (""to_deeper_model"", target_id, new_layer))\n        input_id = self.layer_id_to_input_node_ids[target_id][0]\n        output_id = self.layer_id_to_output_node_ids[target_id][0]\n        if self.weighted:\n            if is_layer(new_layer, ""Dense""):\n                init_dense_weight(new_layer)\n            elif is_layer(new_layer, ""Conv""):\n                init_conv_weight(new_layer)\n            elif is_layer(new_layer, ""BatchNormalization""):\n                init_bn_weight(new_layer)\n\n        self._insert_new_layers([new_layer], input_id, output_id)\n\n    def to_wider_model(self, pre_layer_id, n_add):\n        """"""Widen the last dimension of the output of the pre_layer.\n        Args:\n            pre_layer_id: The ID of a convolutional layer or dense layer.\n            n_add: The number of dimensions to add.\n        """"""\n        self.operation_history.append((""to_wider_model"", pre_layer_id, n_add))\n        pre_layer = self.layer_list[pre_layer_id]\n        output_id = self.layer_id_to_output_node_ids[pre_layer_id][0]\n        dim = layer_width(pre_layer)\n        self.vis = {}\n        self._search(output_id, dim, dim, n_add)\n        # Update the tensor shapes.\n        for u in self.topological_order:\n            for v, layer_id in self.adj_list[u]:\n                self.node_list[v].shape = self.layer_list[layer_id].output_shape\n\n    def _insert_new_layers(self, new_layers, start_node_id, end_node_id):\n        """"""Insert the new_layers after the node with start_node_id.""""""\n        new_node_id = self._add_node(deepcopy(self.node_list[end_node_id]))\n        temp_output_id = new_node_id\n        for layer in new_layers[:-1]:\n            temp_output_id = self.add_layer(layer, temp_output_id)\n\n        self._add_edge(new_layers[-1], temp_output_id, end_node_id)\n        new_layers[-1].input = self.node_list[temp_output_id]\n        new_layers[-1].output = self.node_list[end_node_id]\n        self._redirect_edge(start_node_id, end_node_id, new_node_id)\n\n    def _block_end_node(self, layer_id, block_size):\n        ret = self.layer_id_to_output_node_ids[layer_id][0]\n        for _ in range(block_size - 2):\n            ret = self.adj_list[ret][0][0]\n        return ret\n\n    def _dense_block_end_node(self, layer_id):\n        return self.layer_id_to_input_node_ids[layer_id][0]\n\n    def _conv_block_end_node(self, layer_id):\n        """"""Get the input node ID of the last layer in the block by layer ID.\n            Return the input node ID of the last layer in the convolutional block.\n        Args:\n            layer_id: the convolutional layer ID.\n        """"""\n        return self._block_end_node(layer_id, Constant.CONV_BLOCK_DISTANCE)\n\n    def to_add_skip_model(self, start_id, end_id):\n        """"""Add a weighted add skip-connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        """"""\n        self.operation_history.append((""to_add_skip_model"", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        # Add the conv layer\n        new_conv_layer = get_conv_class(\n            self.n_dim)(\n                filters_start,\n                filters_end,\n                1)\n        skip_output_id = self.add_layer(new_conv_layer, skip_output_id)\n\n        # Add the add layer.\n        add_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        add_layer = StubAdd()\n\n        self._redirect_edge(pre_end_node_id, end_node_id, add_input_node_id)\n        self._add_edge(add_layer, add_input_node_id, end_node_id)\n        self._add_edge(add_layer, skip_output_id, end_node_id)\n        add_layer.input = [\n            self.node_list[add_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        add_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = add_layer.output_shape\n\n        # Set weights to the additional conv layer.\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_start) + filter_shape)\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )\n\n    def to_concat_skip_model(self, start_id, end_id):\n        """"""Add a weighted add concatenate connection from after start node to end node.\n        Args:\n            start_id: The convolutional layer ID, after which to start the skip-connection.\n            end_id: The convolutional layer ID, after which to end the skip-connection.\n        """"""\n        self.operation_history.append(\n            (""to_concat_skip_model"", start_id, end_id))\n        filters_end = self.layer_list[end_id].output.shape[-1]\n        filters_start = self.layer_list[start_id].output.shape[-1]\n        start_node_id = self.layer_id_to_output_node_ids[start_id][0]\n\n        pre_end_node_id = self.layer_id_to_input_node_ids[end_id][0]\n        end_node_id = self.layer_id_to_output_node_ids[end_id][0]\n\n        skip_output_id = self._insert_pooling_layer_chain(\n            start_node_id, end_node_id)\n\n        concat_input_node_id = self._add_node(\n            deepcopy(self.node_list[end_node_id]))\n        self._redirect_edge(pre_end_node_id, end_node_id, concat_input_node_id)\n\n        concat_layer = StubConcatenate()\n        concat_layer.input = [\n            self.node_list[concat_input_node_id],\n            self.node_list[skip_output_id],\n        ]\n        concat_output_node_id = self._add_node(Node(concat_layer.output_shape))\n        self._add_edge(\n            concat_layer,\n            concat_input_node_id,\n            concat_output_node_id)\n        self._add_edge(concat_layer, skip_output_id, concat_output_node_id)\n        concat_layer.output = self.node_list[concat_output_node_id]\n        self.node_list[concat_output_node_id].shape = concat_layer.output_shape\n\n        # Add the concatenate layer.\n        new_conv_layer = get_conv_class(self.n_dim)(\n            filters_start + filters_end, filters_end, 1\n        )\n        self._add_edge(new_conv_layer, concat_output_node_id, end_node_id)\n        new_conv_layer.input = self.node_list[concat_output_node_id]\n        new_conv_layer.output = self.node_list[end_node_id]\n        self.node_list[end_node_id].shape = new_conv_layer.output_shape\n\n        if self.weighted:\n            filter_shape = (1,) * self.n_dim\n            weights = np.zeros((filters_end, filters_end) + filter_shape)\n            for i in range(filters_end):\n                filter_weight = np.zeros((filters_end,) + filter_shape)\n                center_index = (i,) + (0,) * self.n_dim\n                filter_weight[center_index] = 1\n                weights[i, ...] = filter_weight\n            weights = np.concatenate(\n                (weights, np.zeros((filters_end, filters_start) + filter_shape)), axis=1\n            )\n            bias = np.zeros(filters_end)\n            new_conv_layer.set_weights(\n                (add_noise(weights, np.array([0, 1])), add_noise(\n                    bias, np.array([0, 1])))\n            )\n\n    def _insert_pooling_layer_chain(self, start_node_id, end_node_id):\n        skip_output_id = start_node_id\n        for layer in self._get_pooling_layers(start_node_id, end_node_id):\n            new_layer = deepcopy(layer)\n            if is_layer(new_layer, ""Conv""):\n                filters = self.node_list[start_node_id].shape[-1]\n                new_layer = get_conv_class(self.n_dim)(\n                    filters, filters, 1, layer.stride)\n                if self.weighted:\n                    init_conv_weight(new_layer)\n            else:\n                new_layer = deepcopy(layer)\n            skip_output_id = self.add_layer(new_layer, skip_output_id)\n        skip_output_id = self.add_layer(StubReLU(), skip_output_id)\n        return skip_output_id\n\n    def extract_descriptor(self):\n        """"""Extract the the description of the Graph as an instance of NetworkDescriptor.""""""\n        main_chain = self.get_main_chain()\n        index_in_main_chain = {}\n        for index, u in enumerate(main_chain):\n            index_in_main_chain[u] = index\n\n        ret = NetworkDescriptor()\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    continue\n                layer = self.layer_list[layer_id]\n                copied_layer = copy(layer)\n                copied_layer.weights = None\n                ret.add_layer(deepcopy(copied_layer))\n\n        for u in index_in_main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v not in index_in_main_chain:\n                    temp_u = u\n                    temp_v = v\n                    temp_layer_id = layer_id\n                    skip_type = None\n                    while not (\n                            temp_v in index_in_main_chain and temp_u in index_in_main_chain):\n                        if is_layer(\n                                self.layer_list[temp_layer_id], ""Concatenate""):\n                            skip_type = NetworkDescriptor.CONCAT_CONNECT\n                        if is_layer(self.layer_list[temp_layer_id], ""Add""):\n                            skip_type = NetworkDescriptor.ADD_CONNECT\n                        temp_u = temp_v\n                        temp_v, temp_layer_id = self.adj_list[temp_v][0]\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[temp_u], skip_type\n                    )\n\n                elif index_in_main_chain[v] - index_in_main_chain[u] != 1:\n                    skip_type = None\n                    if is_layer(self.layer_list[layer_id], ""Concatenate""):\n                        skip_type = NetworkDescriptor.CONCAT_CONNECT\n                    if is_layer(self.layer_list[layer_id], ""Add""):\n                        skip_type = NetworkDescriptor.ADD_CONNECT\n                    ret.add_skip_connection(\n                        index_in_main_chain[u], index_in_main_chain[v], skip_type\n                    )\n\n        return ret\n\n    def clear_weights(self):\n        \'\'\' clear weights of the graph\n        \'\'\'\n        self.weighted = False\n        for layer in self.layer_list:\n            layer.weights = None\n\n    def produce_torch_model(self):\n        """"""Build a new Torch model based on the current graph.""""""\n        return TorchModel(self)\n\n    def produce_keras_model(self):\n        """"""Build a new keras model based on the current graph.""""""\n        return KerasModel(self).model\n\n    def produce_onnx_model(self):\n        """"""Build a new ONNX model based on the current graph.""""""\n        return ONNXModel(self)\n\n    def parsing_onnx_model(self, onnx_model):\n        \'\'\'to do in the future to use the onnx model\n        \'\'\'\n        return ONNXModel(onnx_model)\n\n    def produce_json_model(self):\n        """"""Build a new Json model based on the current graph.""""""\n        return JSONModel(self).data\n\n    @classmethod\n    def parsing_json_model(cls, json_model):\n        \'\'\'build a graph from json\n        \'\'\'\n        return json_to_graph(json_model)\n\n    def _layer_ids_in_order(self, layer_ids):\n        node_id_to_order_index = {}\n        for index, node_id in enumerate(self.topological_order):\n            node_id_to_order_index[node_id] = index\n        return sorted(\n            layer_ids,\n            key=lambda layer_id: node_id_to_order_index[\n                self.layer_id_to_output_node_ids[layer_id][0]\n            ],\n        )\n\n    def _layer_ids_by_type(self, type_str):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], type_str),\n                range(self.n_layers),\n            )\n        )\n\n    def get_main_chain_layers(self):\n        """"""Return a list of layer IDs in the main chain.""""""\n        main_chain = self.get_main_chain()\n        ret = []\n        for u in main_chain:\n            for v, layer_id in self.adj_list[u]:\n                if v in main_chain and u in main_chain:\n                    ret.append(layer_id)\n        return ret\n\n    def _conv_layer_ids_in_order(self):\n        return list(\n            filter(\n                lambda layer_id: is_layer(self.layer_list[layer_id], ""Conv""),\n                self.get_main_chain_layers(),\n            )\n        )\n\n    def _dense_layer_ids_in_order(self):\n        return self._layer_ids_in_order(self._layer_ids_by_type(""Dense""))\n\n    def deep_layer_ids(self):\n        ret = []\n        for layer_id in self.get_main_chain_layers():\n            layer = self.layer_list[layer_id]\n            if is_layer(layer, ""GlobalAveragePooling""):\n                break\n            if is_layer(layer, ""Add"") or is_layer(layer, ""Concatenate""):\n                continue\n            ret.append(layer_id)\n        return ret\n\n    def wide_layer_ids(self):\n        return (\n            self._conv_layer_ids_in_order(\n            )[:-1] + self._dense_layer_ids_in_order()[:-1]\n        )\n\n    def skip_connection_layer_ids(self):\n        return self.deep_layer_ids()[:-1]\n\n    def size(self):\n        return sum(list(map(lambda x: x.size(), self.layer_list)))\n\n    def get_main_chain(self):\n        """"""Returns the main chain node ID list.""""""\n        pre_node = {}\n        distance = {}\n        for i in range(self.n_nodes):\n            distance[i] = 0\n            pre_node[i] = i\n        for i in range(self.n_nodes - 1):\n            for u in range(self.n_nodes):\n                for v, _ in self.adj_list[u]:\n                    if distance[u] + 1 > distance[v]:\n                        distance[v] = distance[u] + 1\n                        pre_node[v] = u\n        temp_id = 0\n        for i in range(self.n_nodes):\n            if distance[i] > distance[temp_id]:\n                temp_id = i\n        ret = []\n        for i in range(self.n_nodes + 5):\n            ret.append(temp_id)\n            if pre_node[temp_id] == temp_id:\n                break\n            temp_id = pre_node[temp_id]\n        assert temp_id == pre_node[temp_id]\n        ret.reverse()\n        return ret\n\n\nclass TorchModel(torch.nn.Module):\n    """"""A neural network class using pytorch constructed from an instance of Graph.""""""\n\n    def __init__(self, graph):\n        super(TorchModel, self).__init__()\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(layer.to_real_layer())\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_torch(self.graph.layer_list[index], layer)\n        for index, layer in enumerate(self.layers):\n            self.add_module(str(index), layer)\n\n    def forward(self, input_tensor):\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                torch_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = torch_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n        return node_list[output_id]\n\n    def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_torch_weight_to_stub(layer, self.graph.layer_list[index])\n\n\nclass KerasModel:\n    def __init__(self, graph):\n        import keras\n\n        self.graph = graph\n        self.layers = []\n        for layer in graph.layer_list:\n            self.layers.append(to_real_keras_layer(layer))\n\n        # Construct the keras graph.\n        # Input\n        topo_node_list = self.graph.topological_order\n        output_id = topo_node_list[-1]\n        input_id = topo_node_list[0]\n        input_tensor = keras.layers.Input(\n            shape=graph.node_list[input_id].shape)\n\n        node_list = deepcopy(self.graph.node_list)\n        node_list[input_id] = input_tensor\n\n        # Output\n        for v in topo_node_list:\n            for u, layer_id in self.graph.reverse_adj_list[v]:\n                layer = self.graph.layer_list[layer_id]\n                keras_layer = self.layers[layer_id]\n\n                if isinstance(layer, (StubAdd, StubConcatenate)):\n                    edge_input_tensor = list(\n                        map(\n                            lambda x: node_list[x],\n                            self.graph.layer_id_to_input_node_ids[layer_id],\n                        )\n                    )\n                else:\n                    edge_input_tensor = node_list[u]\n\n                temp_tensor = keras_layer(edge_input_tensor)\n                node_list[v] = temp_tensor\n\n        output_tensor = node_list[output_id]\n        output_tensor = keras.layers.Activation(""softmax"", name=""activation_add"")(\n            output_tensor\n        )\n        self.model = keras.models.Model(\n            inputs=input_tensor, outputs=output_tensor)\n\n        if graph.weighted:\n            for index, layer in enumerate(self.layers):\n                set_stub_weight_to_keras(self.graph.layer_list[index], layer)\n\n    def set_weight_to_graph(self):\n        self.graph.weighted = True\n        for index, layer in enumerate(self.layers):\n            set_keras_weight_to_stub(layer, self.graph.layer_list[index])\n\n\nclass ONNXModel:\n    # to do in the future using onnx ir\n    def __init__(self, graph):\n        pass\n\n\nclass JSONModel:\n    def __init__(self, graph):\n        data = dict()\n        node_list = list()\n        layer_list = list()\n        operation_history = list()\n\n        data[""input_shape""] = graph.input_shape\n        vis = graph.vis\n        data[""vis""] = list(vis.keys()) if vis is not None else None\n        data[""weighted""] = graph.weighted\n\n        for item in graph.operation_history:\n            if item[0] == ""to_deeper_model"":\n                operation_history.append(\n                    [\n                        item[0],\n                        item[1],\n                        layer_description_extractor(item[2], graph.node_to_id),\n                    ]\n                )\n            else:\n                operation_history.append(item)\n        data[""operation_history""] = operation_history\n        data[""layer_id_to_input_node_ids""] = graph.layer_id_to_input_node_ids\n        data[""layer_id_to_output_node_ids""] = graph.layer_id_to_output_node_ids\n        data[""adj_list""] = graph.adj_list\n        data[""reverse_adj_list""] = graph.reverse_adj_list\n\n        for node in graph.node_list:\n            node_id = graph.node_to_id[node]\n            node_information = node.shape\n            node_list.append((node_id, node_information))\n\n        for layer_id, item in enumerate(graph.layer_list):\n            layer = graph.layer_list[layer_id]\n            layer_information = layer_description_extractor(\n                layer, graph.node_to_id)\n            layer_list.append((layer_id, layer_information))\n\n        data[""node_list""] = node_list\n        data[""layer_list""] = layer_list\n\n        self.data = data\n\n\ndef graph_to_onnx(graph, onnx_model_path):\n    import onnx\n    # to do in the future using onnx ir\n    onnx_out = graph.produce_onnx_model()\n    onnx.save(onnx_out, onnx_model_path)\n    return onnx_out\n\n\ndef onnx_to_graph(onnx_model, input_shape):\n    # to do in the future using onnx ir\n    graph = Graph(input_shape, False)\n    graph.parsing_onnx_model(onnx_model)\n    return graph\n\n\ndef graph_to_json(graph, json_model_path):\n    json_out = graph.produce_json_model()\n    with open(json_model_path, ""w"") as fout:\n        json.dump(json_out, fout)\n    json_out = json.dumps(json_out)\n    return json_out\n\n\ndef json_to_graph(json_model: str):\n    json_model = json.loads(json_model)\n    # restore graph data from json data\n    input_shape = tuple(json_model[""input_shape""])\n    node_list = list()\n    node_to_id = dict()\n    id_to_node = dict()\n    layer_list = list()\n    layer_to_id = dict()\n    operation_history = list()\n    graph = Graph(input_shape, False)\n\n    graph.input_shape = input_shape\n    vis = json_model[""vis""]\n    graph.vis = {\n        tuple(item): True for item in vis} if vis is not None else None\n    graph.weighted = json_model[""weighted""]\n    layer_id_to_input_node_ids = json_model[""layer_id_to_input_node_ids""]\n    graph.layer_id_to_input_node_ids = {\n        int(k): v for k, v in layer_id_to_input_node_ids.items()\n    }\n    layer_id_to_output_node_ids = json_model[""layer_id_to_output_node_ids""]\n    graph.layer_id_to_output_node_ids = {\n        int(k): v for k, v in layer_id_to_output_node_ids.items()\n    }\n    adj_list = {}\n    for k, v in json_model[""adj_list""].items():\n        adj_list[int(k)] = [tuple(i) for i in v]\n    graph.adj_list = adj_list\n    reverse_adj_list = {}\n    for k, v in json_model[""reverse_adj_list""].items():\n        reverse_adj_list[int(k)] = [tuple(i) for i in v]\n    graph.reverse_adj_list = reverse_adj_list\n\n    for item in json_model[""node_list""]:\n        new_node = Node(tuple(item[1]))\n        node_id = item[0]\n        node_list.append(new_node)\n        node_to_id[new_node] = node_id\n        id_to_node[node_id] = new_node\n\n    for item in json_model[""operation_history""]:\n        if item[0] == ""to_deeper_model"":\n            operation_history.append(\n                (item[0], item[1], layer_description_builder(item[2], id_to_node))\n            )\n        else:\n            operation_history.append(item)\n    graph.operation_history = operation_history\n\n    for item in json_model[""layer_list""]:\n        new_layer = layer_description_builder(item[1], id_to_node)\n        layer_id = int(item[0])\n        layer_list.append(new_layer)\n        layer_to_id[new_layer] = layer_id\n\n    graph.node_list = node_list\n    graph.node_to_id = node_to_id\n    graph.layer_list = layer_list\n    graph.layer_to_id = layer_to_id\n\n    return graph\n'"
src/sdk/pynni/nni/networkmorphism_tuner/graph_transformer.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom copy import deepcopy\n\nfrom random import randrange, sample\n\nfrom nni.networkmorphism_tuner.graph import NetworkDescriptor\nfrom nni.networkmorphism_tuner.layers import (\n    StubDense,\n    StubReLU,\n    get_batch_norm_class,\n    get_conv_class,\n    get_dropout_class,\n    get_pooling_class,\n    is_layer,\n)\nfrom nni.networkmorphism_tuner.utils import Constant\n\n\ndef to_wider_graph(graph):\n    \'\'\' wider graph\n    \'\'\'\n    weighted_layer_ids = graph.wide_layer_ids()\n    weighted_layer_ids = list(\n        filter(\n            lambda x: graph.layer_list[x].output.shape[-1], weighted_layer_ids)\n    )\n    wider_layers = sample(weighted_layer_ids, 1)\n\n    for layer_id in wider_layers:\n        layer = graph.layer_list[layer_id]\n        if is_layer(layer, ""Conv""):\n            n_add = layer.filters\n        else:\n            n_add = layer.units\n\n        graph.to_wider_model(layer_id, n_add)\n    return graph\n\n\ndef to_skip_connection_graph(graph):\n    \'\'\' skip connection graph\n    \'\'\'\n    # The last conv layer cannot be widen since wider operator cannot be done\n    # over the two sides of flatten.\n    weighted_layer_ids = graph.skip_connection_layer_ids()\n    valid_connection = []\n    for skip_type in sorted(\n            [NetworkDescriptor.ADD_CONNECT, NetworkDescriptor.CONCAT_CONNECT]):\n        for index_a in range(len(weighted_layer_ids)):\n            for index_b in range(len(weighted_layer_ids))[index_a + 1:]:\n                valid_connection.append((index_a, index_b, skip_type))\n\n    if not valid_connection:\n        return graph\n    for index_a, index_b, skip_type in sample(valid_connection, 1):\n        a_id = weighted_layer_ids[index_a]\n        b_id = weighted_layer_ids[index_b]\n        if skip_type == NetworkDescriptor.ADD_CONNECT:\n            graph.to_add_skip_model(a_id, b_id)\n        else:\n            graph.to_concat_skip_model(a_id, b_id)\n    return graph\n\n\ndef create_new_layer(layer, n_dim):\n    \'\'\' create  new layer for the graph\n    \'\'\'\n\n    input_shape = layer.output.shape\n    dense_deeper_classes = [StubDense, get_dropout_class(n_dim), StubReLU]\n    conv_deeper_classes = [\n        get_conv_class(n_dim),\n        get_batch_norm_class(n_dim),\n        StubReLU]\n    if is_layer(layer, ""ReLU""):\n        conv_deeper_classes = [\n            get_conv_class(n_dim),\n            get_batch_norm_class(n_dim)]\n        dense_deeper_classes = [StubDense, get_dropout_class(n_dim)]\n    elif is_layer(layer, ""Dropout""):\n        dense_deeper_classes = [StubDense, StubReLU]\n    elif is_layer(layer, ""BatchNormalization""):\n        conv_deeper_classes = [get_conv_class(n_dim), StubReLU]\n\n    layer_class = None\n    if len(input_shape) == 1:\n        # It is in the dense layer part.\n        layer_class = sample(dense_deeper_classes, 1)[0]\n    else:\n        # It is in the conv layer part.\n        layer_class = sample(conv_deeper_classes, 1)[0]\n\n    if layer_class == StubDense:\n        new_layer = StubDense(input_shape[0], input_shape[0])\n\n    elif layer_class == get_dropout_class(n_dim):\n        new_layer = layer_class(Constant.DENSE_DROPOUT_RATE)\n\n    elif layer_class == get_conv_class(n_dim):\n        new_layer = layer_class(\n            input_shape[-1], input_shape[-1], sample((1, 3, 5), 1)[0], stride=1\n        )\n\n    elif layer_class == get_batch_norm_class(n_dim):\n        new_layer = layer_class(input_shape[-1])\n\n    elif layer_class == get_pooling_class(n_dim):\n        new_layer = layer_class(sample((1, 3, 5), 1)[0])\n\n    else:\n        new_layer = layer_class()\n\n    return new_layer\n\n\ndef to_deeper_graph(graph):\n    \'\'\' deeper graph\n    \'\'\'\n\n    weighted_layer_ids = graph.deep_layer_ids()\n    if len(weighted_layer_ids) >= Constant.MAX_LAYERS:\n        return None\n\n    deeper_layer_ids = sample(weighted_layer_ids, 1)\n\n    for layer_id in deeper_layer_ids:\n        layer = graph.layer_list[layer_id]\n        new_layer = create_new_layer(layer, graph.n_dim)\n        graph.to_deeper_model(layer_id, new_layer)\n    return graph\n\n\ndef legal_graph(graph):\n    \'\'\'judge if a graph is legal or not.\n    \'\'\'\n\n    descriptor = graph.extract_descriptor()\n    skips = descriptor.skip_connections\n    if len(skips) != len(set(skips)):\n        return False\n    return True\n\n\ndef transform(graph):\n    \'\'\'core transform function for graph.\n    \'\'\'\n\n    graphs = []\n    for _ in range(Constant.N_NEIGHBOURS * 2):\n        random_num = randrange(3)\n        temp_graph = None\n        if random_num == 0:\n            temp_graph = to_deeper_graph(deepcopy(graph))\n        elif random_num == 1:\n            temp_graph = to_wider_graph(deepcopy(graph))\n        elif random_num == 2:\n            temp_graph = to_skip_connection_graph(deepcopy(graph))\n\n        if temp_graph is not None and temp_graph.size() <= Constant.MAX_MODEL_SIZE:\n            graphs.append(temp_graph)\n\n        if len(graphs) >= Constant.N_NEIGHBOURS:\n            break\n\n    return graphs\n'"
src/sdk/pynni/nni/networkmorphism_tuner/layer_transformer.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport numpy as np\n\nfrom nni.networkmorphism_tuner.layers import (\n    StubDense,\n    StubReLU,\n    get_batch_norm_class,\n    get_conv_class,\n    get_n_dim,\n)\n\nNOISE_RATIO = 1e-4\n\n\ndef deeper_conv_block(conv_layer, kernel_size, weighted=True):\n    '''deeper conv layer.\n    '''\n    n_dim = get_n_dim(conv_layer)\n    filter_shape = (kernel_size,) * 2\n    n_filters = conv_layer.filters\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n    new_conv_layer = get_conv_class(n_dim)(\n        conv_layer.filters, n_filters, kernel_size=kernel_size\n    )\n    bn = get_batch_norm_class(n_dim)(n_filters)\n\n    if weighted:\n        new_conv_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])),\n             add_noise(bias, np.array([0, 1])))\n        )\n        new_weights = [\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n            add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        ]\n        bn.set_weights(new_weights)\n\n    return [StubReLU(), new_conv_layer, bn]\n\n\ndef dense_to_deeper_block(dense_layer, weighted=True):\n    '''deeper dense layer.\n    '''\n    units = dense_layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    new_dense_layer = StubDense(units, units)\n    if weighted:\n        new_dense_layer.set_weights(\n            (add_noise(weight, np.array([0, 1])),\n             add_noise(bias, np.array([0, 1])))\n        )\n    return [StubReLU(), new_dense_layer]\n\n\ndef wider_pre_dense(layer, n_add, weighted=True):\n    '''wider previous dense layer.\n    '''\n    if not weighted:\n        return StubDense(layer.input_units, layer.units + n_add)\n\n    n_units2 = layer.units\n\n    teacher_w, teacher_b = layer.get_weights()\n    rand = np.random.randint(n_units2, size=n_add)\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n\n    # target layer update (i)\n    for i in range(n_add):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, :]\n        new_weight = new_weight[np.newaxis, :]\n        student_w = np.concatenate(\n            (student_w, add_noise(new_weight, student_w)), axis=0)\n        student_b = np.append(\n            student_b, add_noise(\n                teacher_b[teacher_index], student_b))\n\n    new_pre_layer = StubDense(layer.input_units, n_units2 + n_add)\n    new_pre_layer.set_weights((student_w, student_b))\n\n    return new_pre_layer\n\n\ndef wider_pre_conv(layer, n_add_filters, weighted=True):\n    '''wider previous conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(\n            layer.input_channel,\n            layer.filters + n_add_filters,\n            kernel_size=layer.kernel_size,\n        )\n\n    n_pre_filters = layer.filters\n    rand = np.random.randint(n_pre_filters, size=n_add_filters)\n    teacher_w, teacher_b = layer.get_weights()\n\n    student_w = teacher_w.copy()\n    student_b = teacher_b.copy()\n    # target layer update (i)\n    for i, _ in enumerate(rand):\n        teacher_index = rand[i]\n        new_weight = teacher_w[teacher_index, ...]\n        new_weight = new_weight[np.newaxis, ...]\n        student_w = np.concatenate((student_w, new_weight), axis=0)\n        student_b = np.append(student_b, teacher_b[teacher_index])\n    new_pre_layer = get_conv_class(n_dim)(\n        layer.input_channel, n_pre_filters + n_add_filters, layer.kernel_size\n    )\n    new_pre_layer.set_weights(\n        (add_noise(student_w, teacher_w), add_noise(student_b, teacher_b))\n    )\n    return new_pre_layer\n\n\ndef wider_next_conv(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next conv layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_conv_class(n_dim)(layer.input_channel + n_add,\n                                     layer.filters,\n                                     kernel_size=layer.kernel_size,\n                                     stride=layer.stride)\n    n_filters = layer.filters\n    teacher_w, teacher_b = layer.get_weights()\n\n    new_weight_shape = list(teacher_w.shape)\n    new_weight_shape[1] = n_add\n    new_weight = np.zeros(tuple(new_weight_shape))\n\n    student_w = np.concatenate((teacher_w[:, :start_dim, ...].copy(),\n                                add_noise(new_weight, teacher_w),\n                                teacher_w[:, start_dim:total_dim, ...].copy()), axis=1)\n    new_layer = get_conv_class(n_dim)(layer.input_channel + n_add,\n                                      n_filters,\n                                      kernel_size=layer.kernel_size,\n                                      stride=layer.stride)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer\n\n\ndef wider_bn(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider batch norm layer.\n    '''\n    n_dim = get_n_dim(layer)\n    if not weighted:\n        return get_batch_norm_class(n_dim)(layer.num_features + n_add)\n\n    weights = layer.get_weights()\n\n    new_weights = [\n        add_noise(np.ones(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_add, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_add, dtype=np.float32), np.array([0, 1])),\n    ]\n\n    student_w = tuple()\n    for weight, new_weight in zip(weights, new_weights):\n        temp_w = weight.copy()\n        temp_w = np.concatenate(\n            (temp_w[:start_dim], new_weight, temp_w[start_dim:total_dim])\n        )\n        student_w += (temp_w,)\n    new_layer = get_batch_norm_class(n_dim)(layer.num_features + n_add)\n    new_layer.set_weights(student_w)\n    return new_layer\n\n\ndef wider_next_dense(layer, start_dim, total_dim, n_add, weighted=True):\n    '''wider next dense layer.\n    '''\n    if not weighted:\n        return StubDense(layer.input_units + n_add, layer.units)\n    teacher_w, teacher_b = layer.get_weights()\n    student_w = teacher_w.copy()\n    n_units_each_channel = int(teacher_w.shape[1] / total_dim)\n\n    new_weight = np.zeros((teacher_w.shape[0], n_add * n_units_each_channel))\n    student_w = np.concatenate(\n        (\n            student_w[:, : start_dim * n_units_each_channel],\n            add_noise(new_weight, student_w),\n            student_w[\n                :, start_dim * n_units_each_channel: total_dim * n_units_each_channel\n            ],\n        ),\n        axis=1,\n    )\n\n    new_layer = StubDense(layer.input_units + n_add, layer.units)\n    new_layer.set_weights((student_w, teacher_b))\n    return new_layer\n\n\ndef add_noise(weights, other_weights):\n    '''add noise to the layer.\n    '''\n    w_range = np.ptp(other_weights.flatten())\n    noise_range = NOISE_RATIO * w_range\n    noise = np.random.uniform(-noise_range / 2.0,\n                              noise_range / 2.0, weights.shape)\n    return np.add(noise, weights)\n\n\ndef init_dense_weight(layer):\n    '''initilize dense layer weight.\n    '''\n    units = layer.units\n    weight = np.eye(units)\n    bias = np.zeros(units)\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])),\n         add_noise(bias, np.array([0, 1])))\n    )\n\n\ndef init_conv_weight(layer):\n    '''initilize conv layer weight.\n    '''\n    n_filters = layer.filters\n    filter_shape = (layer.kernel_size,) * get_n_dim(layer)\n    weight = np.zeros((n_filters, n_filters) + filter_shape)\n\n    center = tuple(map(lambda x: int((x - 1) / 2), filter_shape))\n    for i in range(n_filters):\n        filter_weight = np.zeros((n_filters,) + filter_shape)\n        index = (i,) + center\n        filter_weight[index] = 1\n        weight[i, ...] = filter_weight\n    bias = np.zeros(n_filters)\n\n    layer.set_weights(\n        (add_noise(weight, np.array([0, 1])),\n         add_noise(bias, np.array([0, 1])))\n    )\n\n\ndef init_bn_weight(layer):\n    '''initilize batch norm layer weight.\n    '''\n    n_filters = layer.num_features\n    new_weights = [\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.zeros(n_filters, dtype=np.float32), np.array([0, 1])),\n        add_noise(np.ones(n_filters, dtype=np.float32), np.array([0, 1])),\n    ]\n    layer.set_weights(new_weights)\n"""
src/sdk/pynni/nni/networkmorphism_tuner/layers.py,23,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom abc import abstractmethod\nfrom collections.abc import Iterable\n\nimport torch\nfrom torch import nn\nfrom torch.nn import functional\nfrom nni.networkmorphism_tuner.utils import Constant\n\n\nclass AvgPool(nn.Module):\n    """"""\n    AvgPool Module.\n    """"""\n\n    def __init__(self):\n        super().__init__()\n\n    @abstractmethod\n    def forward(self, input_tensor):\n        pass\n\n\nclass GlobalAvgPool1d(AvgPool):\n    """"""\n    GlobalAvgPool1d Module.\n    """"""\n\n    def forward(self, input_tensor):\n        return functional.avg_pool1d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )\n\n\nclass GlobalAvgPool2d(AvgPool):\n    """"""\n    GlobalAvgPool2d Module.\n    """"""\n\n    def forward(self, input_tensor):\n        return functional.avg_pool2d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )\n\n\nclass GlobalAvgPool3d(AvgPool):\n    """"""\n    GlobalAvgPool3d Module.\n    """"""\n\n    def forward(self, input_tensor):\n        return functional.avg_pool3d(input_tensor, input_tensor.size()[2:]).view(\n            input_tensor.size()[:2]\n        )\n\n\nclass StubLayer:\n    """"""\n    StubLayer Module. Base Module.\n    """"""\n\n    def __init__(self, input_node=None, output_node=None):\n        self.input = input_node\n        self.output = output_node\n        self.weights = None\n\n    def build(self, shape):\n        """"""\n        build shape.\n        """"""\n\n    def set_weights(self, weights):\n        """"""\n        set weights.\n        """"""\n        self.weights = weights\n\n    def import_weights(self, torch_layer):\n        """"""\n        import weights.\n        """"""\n\n    def import_weights_keras(self, keras_layer):\n        """"""\n        import weights from keras layer.\n        """"""\n\n    def export_weights(self, torch_layer):\n        """"""\n        export weights.\n        """"""\n\n    def export_weights_keras(self, keras_layer):\n        """"""\n        export weights to keras layer.\n        """"""\n\n    def get_weights(self):\n        """"""\n        get weights.\n        """"""\n        return self.weights\n\n    def size(self):\n        """"""\n        size().\n        """"""\n        return 0\n\n    @property\n    def output_shape(self):\n        """"""\n        output shape.\n        """"""\n        return self.input.shape\n\n    def to_real_layer(self):\n        """"""\n        to real layer.\n        """"""\n\n    def __str__(self):\n        """"""\n        str() function to print.\n        """"""\n        return type(self).__name__[4:]\n\n\nclass StubWeightBiasLayer(StubLayer):\n    """"""\n    StubWeightBiasLayer Module to set the bias.\n    """"""\n\n    def import_weights(self, torch_layer):\n        self.set_weights(\n            (torch_layer.weight.data.cpu().numpy(),\n             torch_layer.bias.data.cpu().numpy())\n        )\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(keras_layer.get_weights())\n\n    def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights(self.weights)\n\n\nclass StubBatchNormalization(StubWeightBiasLayer):\n    """"""\n    StubBatchNormalization Module. Batch Norm.\n    """"""\n\n    def __init__(self, num_features, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.num_features = num_features\n\n    def import_weights(self, torch_layer):\n        self.set_weights(\n            (\n                torch_layer.weight.data.cpu().numpy(),\n                torch_layer.bias.data.cpu().numpy(),\n                torch_layer.running_mean.cpu().numpy(),\n                torch_layer.running_var.cpu().numpy(),\n            )\n        )\n\n    def export_weights(self, torch_layer):\n        torch_layer.weight.data = torch.Tensor(self.weights[0])\n        torch_layer.bias.data = torch.Tensor(self.weights[1])\n        torch_layer.running_mean = torch.Tensor(self.weights[2])\n        torch_layer.running_var = torch.Tensor(self.weights[3])\n\n    def size(self):\n        return self.num_features * 4\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n\nclass StubBatchNormalization1d(StubBatchNormalization):\n    """"""\n    StubBatchNormalization1d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm1d(self.num_features)\n\n\nclass StubBatchNormalization2d(StubBatchNormalization):\n    """"""\n    StubBatchNormalization2d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm2d(self.num_features)\n\n\nclass StubBatchNormalization3d(StubBatchNormalization):\n    """"""\n    StubBatchNormalization3d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.BatchNorm3d(self.num_features)\n\n\nclass StubDense(StubWeightBiasLayer):\n    """"""\n    StubDense Module. Linear.\n    """"""\n\n    def __init__(self, input_units, units, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_units = input_units\n        self.units = units\n\n    @property\n    def output_shape(self):\n        return (self.units,)\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))\n\n    def size(self):\n        return self.input_units * self.units + self.units\n\n    def to_real_layer(self):\n        return torch.nn.Linear(self.input_units, self.units)\n\n\nclass StubConv(StubWeightBiasLayer):\n    """"""\n    StubConv Module. Conv.\n    """"""\n\n    def __init__(self, input_channel, filters, kernel_size,\n                 stride=1, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.input_channel = input_channel\n        self.filters = filters\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = int(self.kernel_size / 2)\n\n    @property\n    def output_shape(self):\n        ret = list(self.input.shape[:-1])\n        for index, dim in enumerate(ret):\n            ret[index] = (\n                int((dim + 2 * self.padding - self.kernel_size) / self.stride) + 1\n            )\n        ret = ret + [self.filters]\n        return tuple(ret)\n\n    def import_weights_keras(self, keras_layer):\n        self.set_weights(\n            (keras_layer.get_weights()[0].T,\n             keras_layer.get_weights()[1]))\n\n    def export_weights_keras(self, keras_layer):\n        keras_layer.set_weights((self.weights[0].T, self.weights[1]))\n\n    def size(self):\n        return (self.input_channel * self.kernel_size *\n                self.kernel_size + 1) * self.filters\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n    def __str__(self):\n        return (\n            super().__str__()\n            + ""(""\n            + "", "".join(\n                str(item)\n                for item in [\n                    self.input_channel,\n                    self.filters,\n                    self.kernel_size,\n                    self.stride,\n                ]\n            )\n            + "")""\n        )\n\n\nclass StubConv1d(StubConv):\n    """"""\n    StubConv1d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Conv1d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n\nclass StubConv2d(StubConv):\n    """"""\n    StubConv2d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Conv2d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n\nclass StubConv3d(StubConv):\n    """"""\n    StubConv3d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Conv3d(\n            self.input_channel,\n            self.filters,\n            self.kernel_size,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n\nclass StubAggregateLayer(StubLayer):\n    """"""\n    StubAggregateLayer Module.\n    """"""\n\n    def __init__(self, input_nodes=None, output_node=None):\n        if input_nodes is None:\n            input_nodes = []\n        super().__init__(input_nodes, output_node)\n\n\nclass StubConcatenate(StubAggregateLayer):\n    """"""StubConcatenate Module.\n    """"""\n    @property\n    def output_shape(self):\n        ret = 0\n        for current_input in self.input:\n            ret += current_input.shape[-1]\n        ret = self.input[0].shape[:-1] + (ret,)\n        return ret\n\n    def to_real_layer(self):\n        return TorchConcatenate()\n\n\nclass StubAdd(StubAggregateLayer):\n    """"""\n    StubAdd Module.\n    """"""\n    @property\n    def output_shape(self):\n        return self.input[0].shape\n\n    def to_real_layer(self):\n        return TorchAdd()\n\n\nclass StubFlatten(StubLayer):\n    """"""\n    StubFlatten Module.\n    """"""\n    @property\n    def output_shape(self):\n        ret = 1\n        for dim in self.input.shape:\n            ret *= dim\n        return (ret,)\n\n    def to_real_layer(self):\n        return TorchFlatten()\n\n\nclass StubReLU(StubLayer):\n    """"""\n    StubReLU Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.ReLU()\n\n\nclass StubSoftmax(StubLayer):\n    """"""\n    StubSoftmax Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.LogSoftmax(dim=1)\n\n\nclass StubDropout(StubLayer):\n    """"""\n    StubDropout Module.\n    """"""\n\n    def __init__(self, rate, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n        self.rate = rate\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n\nclass StubDropout1d(StubDropout):\n    """"""\n    StubDropout1d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Dropout(self.rate)\n\n\nclass StubDropout2d(StubDropout):\n    """"""\n    StubDropout2d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Dropout2d(self.rate)\n\n\nclass StubDropout3d(StubDropout):\n    """"""\n    StubDropout3d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.Dropout3d(self.rate)\n\n\nclass StubInput(StubLayer):\n    """"""\n    StubInput Module.\n    """"""\n\n    def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n\n\nclass StubPooling(StubLayer):\n    """"""\n    StubPooling Module.\n    """"""\n\n    def __init__(self,\n                 kernel_size=None,\n                 stride=None,\n                 padding=0,\n                 input_node=None,\n                 output_node=None):\n        super().__init__(input_node, output_node)\n        self.kernel_size = (\n            kernel_size if kernel_size is not None else Constant.POOLING_KERNEL_SIZE\n        )\n        self.stride = stride if stride is not None else self.kernel_size\n        self.padding = padding\n\n    @property\n    def output_shape(self):\n        ret = tuple()\n        for dim in self.input.shape[:-1]:\n            ret = ret + (max(int(dim / self.kernel_size), 1),)\n        ret = ret + (self.input.shape[-1],)\n        return ret\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n\nclass StubPooling1d(StubPooling):\n    """"""\n    StubPooling1d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool1d(self.kernel_size, stride=self.stride)\n\n\nclass StubPooling2d(StubPooling):\n    """"""\n    StubPooling2d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool2d(self.kernel_size, stride=self.stride)\n\n\nclass StubPooling3d(StubPooling):\n    """"""\n    StubPooling3d Module.\n    """"""\n\n    def to_real_layer(self):\n        return torch.nn.MaxPool3d(self.kernel_size, stride=self.stride)\n\n\nclass StubGlobalPooling(StubLayer):\n    """"""\n    StubGlobalPooling Module.\n    """"""\n\n    def __init__(self, input_node=None, output_node=None):\n        super().__init__(input_node, output_node)\n\n    @property\n    def output_shape(self):\n        return (self.input.shape[-1],)\n\n    @abstractmethod\n    def to_real_layer(self):\n        pass\n\n\nclass StubGlobalPooling1d(StubGlobalPooling):\n    """"""\n    StubGlobalPooling1d Module.\n    """"""\n\n    def to_real_layer(self):\n        return GlobalAvgPool1d()\n\n\nclass StubGlobalPooling2d(StubGlobalPooling):\n    """"""\n    StubGlobalPooling2d Module.\n    """"""\n\n    def to_real_layer(self):\n        return GlobalAvgPool2d()\n\n\nclass StubGlobalPooling3d(StubGlobalPooling):\n    """"""\n    StubGlobalPooling3d Module.\n    """"""\n\n    def to_real_layer(self):\n        return GlobalAvgPool3d()\n\n\nclass TorchConcatenate(nn.Module):\n    """"""\n    TorchConcatenate Module.\n    """"""\n\n    def forward(self, input_list):\n        return torch.cat(input_list, dim=1)\n\n\nclass TorchAdd(nn.Module):\n    """"""\n    TorchAdd Module.\n    """"""\n\n    def forward(self, input_list):\n        return input_list[0] + input_list[1]\n\n\nclass TorchFlatten(nn.Module):\n    """"""\n    TorchFlatten Module.\n    """"""\n\n    def forward(self, input_tensor):\n        return input_tensor.view(input_tensor.size(0), -1)\n\n\ndef keras_dropout(layer, rate):\n    """"""\n    Keras dropout layer.\n    """"""\n\n    from keras import layers\n\n    input_dim = len(layer.input.shape)\n    if input_dim == 2:\n        return layers.SpatialDropout1D(rate)\n    elif input_dim == 3:\n        return layers.SpatialDropout2D(rate)\n    elif input_dim == 4:\n        return layers.SpatialDropout3D(rate)\n    else:\n        return layers.Dropout(rate)\n\n\ndef to_real_keras_layer(layer):\n    """"""\n    Real keras layer.\n    """"""\n    from keras import layers\n\n    if is_layer(layer, ""Dense""):\n        return layers.Dense(layer.units, input_shape=(layer.input_units,))\n    if is_layer(layer, ""Conv""):\n        return layers.Conv2D(\n            layer.filters,\n            layer.kernel_size,\n            input_shape=layer.input.shape,\n            padding=""same"",\n        )  # padding\n    if is_layer(layer, ""Pooling""):\n        return layers.MaxPool2D(2)\n    if is_layer(layer, ""BatchNormalization""):\n        return layers.BatchNormalization(input_shape=layer.input.shape)\n    if is_layer(layer, ""Concatenate""):\n        return layers.Concatenate()\n    if is_layer(layer, ""Add""):\n        return layers.Add()\n    if is_layer(layer, ""Dropout""):\n        return keras_dropout(layer, layer.rate)\n    if is_layer(layer, ""ReLU""):\n        return layers.Activation(""relu"")\n    if is_layer(layer, ""Softmax""):\n        return layers.Activation(""softmax"")\n    if is_layer(layer, ""Flatten""):\n        return layers.Flatten()\n    if is_layer(layer, ""GlobalAveragePooling""):\n        return layers.GlobalAveragePooling2D()\n    return None  # note: this is not written by original author, feel free to modify if you think it\'s incorrect\n\n\ndef is_layer(layer, layer_type):\n    """"""\n    Judge the layer type.\n\n    Returns\n    -------\n    bool\n        boolean -- True or False\n    """"""\n\n    if layer_type == ""Input"":\n        return isinstance(layer, StubInput)\n    elif layer_type == ""Conv"":\n        return isinstance(layer, StubConv)\n    elif layer_type == ""Dense"":\n        return isinstance(layer, (StubDense,))\n    elif layer_type == ""BatchNormalization"":\n        return isinstance(layer, (StubBatchNormalization,))\n    elif layer_type == ""Concatenate"":\n        return isinstance(layer, (StubConcatenate,))\n    elif layer_type == ""Add"":\n        return isinstance(layer, (StubAdd,))\n    elif layer_type == ""Pooling"":\n        return isinstance(layer, StubPooling)\n    elif layer_type == ""Dropout"":\n        return isinstance(layer, (StubDropout,))\n    elif layer_type == ""Softmax"":\n        return isinstance(layer, (StubSoftmax,))\n    elif layer_type == ""ReLU"":\n        return isinstance(layer, (StubReLU,))\n    elif layer_type == ""Flatten"":\n        return isinstance(layer, (StubFlatten,))\n    elif layer_type == ""GlobalAveragePooling"":\n        return isinstance(layer, StubGlobalPooling)\n    return None  # note: this is not written by original author, feel free to modify if you think it\'s incorrect\n\n\ndef layer_description_extractor(layer, node_to_id):\n    """"""\n    Get layer description.\n    """"""\n\n    layer_input = layer.input\n    layer_output = layer.output\n    if layer_input is not None:\n        if isinstance(layer_input, Iterable):\n            layer_input = list(map(lambda x: node_to_id[x], layer_input))\n        else:\n            layer_input = node_to_id[layer_input]\n\n    if layer_output is not None:\n        layer_output = node_to_id[layer_output]\n\n    if isinstance(layer, StubConv):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_channel,\n            layer.filters,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    elif isinstance(layer, (StubDense,)):\n        return [\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.input_units,\n            layer.units,\n        ]\n    elif isinstance(layer, (StubBatchNormalization,)):\n        return (type(layer).__name__, layer_input,\n                layer_output, layer.num_features)\n    elif isinstance(layer, (StubDropout,)):\n        return (type(layer).__name__, layer_input, layer_output, layer.rate)\n    elif isinstance(layer, StubPooling):\n        return (\n            type(layer).__name__,\n            layer_input,\n            layer_output,\n            layer.kernel_size,\n            layer.stride,\n            layer.padding,\n        )\n    else:\n        return (type(layer).__name__, layer_input, layer_output)\n\n\ndef layer_description_builder(layer_information, id_to_node):\n    """"""build layer from description.\n    """"""\n    layer_type = layer_information[0]\n\n    layer_input_ids = layer_information[1]\n    if isinstance(layer_input_ids, Iterable):\n        layer_input = list(map(lambda x: id_to_node[x], layer_input_ids))\n    else:\n        layer_input = id_to_node[layer_input_ids]\n    layer_output = id_to_node[layer_information[2]]\n    if layer_type.startswith(""StubConv""):\n        input_channel = layer_information[3]\n        filters = layer_information[4]\n        kernel_size = layer_information[5]\n        stride = layer_information[6]\n        return globals()[layer_type](\n            input_channel, filters, kernel_size, stride, layer_input, layer_output\n        )\n    elif layer_type.startswith(""StubDense""):\n        input_units = layer_information[3]\n        units = layer_information[4]\n        return globals()[layer_type](input_units, units, layer_input, layer_output)\n    elif layer_type.startswith(""StubBatchNormalization""):\n        num_features = layer_information[3]\n        return globals()[layer_type](num_features, layer_input, layer_output)\n    elif layer_type.startswith(""StubDropout""):\n        rate = layer_information[3]\n        return globals()[layer_type](rate, layer_input, layer_output)\n    elif layer_type.startswith(""StubPooling""):\n        kernel_size = layer_information[3]\n        stride = layer_information[4]\n        padding = layer_information[5]\n        return globals()[layer_type](kernel_size, stride, padding, layer_input, layer_output)\n    else:\n        return globals()[layer_type](layer_input, layer_output)\n\n\ndef layer_width(layer):\n    """"""\n    Get layer width.\n    """"""\n\n    if is_layer(layer, ""Dense""):\n        return layer.units\n    if is_layer(layer, ""Conv""):\n        return layer.filters\n    raise TypeError(""The layer should be either Dense or Conv layer."")\n\n\ndef set_torch_weight_to_stub(torch_layer, stub_layer):\n    stub_layer.import_weights(torch_layer)\n\n\ndef set_keras_weight_to_stub(keras_layer, stub_layer):\n    stub_layer.import_weights_keras(keras_layer)\n\n\ndef set_stub_weight_to_torch(stub_layer, torch_layer):\n    stub_layer.export_weights(torch_layer)\n\n\ndef set_stub_weight_to_keras(stub_layer, keras_layer):\n    stub_layer.export_weights_keras(keras_layer)\n\n\ndef get_conv_class(n_dim):\n    conv_class_list = [StubConv1d, StubConv2d, StubConv3d]\n    return conv_class_list[n_dim - 1]\n\n\ndef get_dropout_class(n_dim):\n    dropout_class_list = [StubDropout1d, StubDropout2d, StubDropout3d]\n    return dropout_class_list[n_dim - 1]\n\n\ndef get_global_avg_pooling_class(n_dim):\n    global_avg_pooling_class_list = [\n        StubGlobalPooling1d,\n        StubGlobalPooling2d,\n        StubGlobalPooling3d,\n    ]\n    return global_avg_pooling_class_list[n_dim - 1]\n\n\ndef get_pooling_class(n_dim):\n    pooling_class_list = [StubPooling1d, StubPooling2d, StubPooling3d]\n    return pooling_class_list[n_dim - 1]\n\n\ndef get_batch_norm_class(n_dim):\n    batch_norm_class_list = [\n        StubBatchNormalization1d,\n        StubBatchNormalization2d,\n        StubBatchNormalization3d,\n    ]\n    return batch_norm_class_list[n_dim - 1]\n\n\ndef get_n_dim(layer):\n    if isinstance(layer, (\n            StubConv1d,\n            StubDropout1d,\n            StubGlobalPooling1d,\n            StubPooling1d,\n            StubBatchNormalization1d,\n    )):\n        return 1\n    if isinstance(layer, (\n            StubConv2d,\n            StubDropout2d,\n            StubGlobalPooling2d,\n            StubPooling2d,\n            StubBatchNormalization2d,\n    )):\n        return 2\n    if isinstance(layer, (\n            StubConv3d,\n            StubDropout3d,\n            StubGlobalPooling3d,\n            StubPooling3d,\n            StubBatchNormalization3d,\n    )):\n        return 3\n    return -1\n'"
src/sdk/pynni/nni/networkmorphism_tuner/networkmorphism_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nnetworkmorphsim_tuner.py\n""""""\n\nimport logging\nimport os\n\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward\nfrom nni.networkmorphism_tuner.bayesian import BayesianOptimizer\nfrom nni.networkmorphism_tuner.nn import CnnGenerator, MlpGenerator\nfrom nni.networkmorphism_tuner.utils import Constant\n\nfrom nni.networkmorphism_tuner.graph import graph_to_json, json_to_graph\n\nlogger = logging.getLogger(""NetworkMorphism_AutoML"")\n\n\nclass NetworkMorphismTuner(Tuner):\n    """"""\n    NetworkMorphismTuner is a tuner which using network morphism techniques.\n\n    Attributes\n    ----------\n    n_classes : int\n        The class number or output node number (default: ``10``)\n    input_shape : tuple\n        A tuple including: (input_width, input_width, input_channel)\n    t_min : float\n        The minimum temperature for simulated annealing. (default: ``Constant.T_MIN``)\n    beta : float\n        The beta in acquisition function. (default: ``Constant.BETA``)\n    algorithm_name : str\n        algorithm name used in the network morphism (default: ``""Bayesian""``)\n    optimize_mode : str\n        optimize mode ""minimize"" or ""maximize"" (default: ``""minimize""``)\n    verbose : bool\n        verbose to print the log (default: ``True``)\n    bo : BayesianOptimizer\n        The optimizer used in networkmorphsim tuner.\n    max_model_size : int\n        max model size to the graph (default: ``Constant.MAX_MODEL_SIZE``)\n    default_model_len : int\n        default model length (default: ``Constant.MODEL_LEN``)\n    default_model_width : int\n        default model width (default: ``Constant.MODEL_WIDTH``)\n    search_space : dict\n    """"""\n\n    def __init__(\n            self,\n            task=""cv"",\n            input_width=32,\n            input_channel=3,\n            n_output_node=10,\n            algorithm_name=""Bayesian"",\n            optimize_mode=""maximize"",\n            path=""model_path"",\n            verbose=True,\n            beta=Constant.BETA,\n            t_min=Constant.T_MIN,\n            max_model_size=Constant.MAX_MODEL_SIZE,\n            default_model_len=Constant.MODEL_LEN,\n            default_model_width=Constant.MODEL_WIDTH,\n    ):\n        """"""\n        initilizer of the NetworkMorphismTuner.\n        """"""\n\n        if not os.path.exists(path):\n            os.makedirs(path)\n        self.path = os.path.join(os.getcwd(), path)\n        if task == ""cv"":\n            self.generators = [CnnGenerator]\n        elif task == ""common"":\n            self.generators = [MlpGenerator]\n        else:\n            raise NotImplementedError(\n                \'{} task not supported in List [""cv"",""common""]\')\n\n        self.n_classes = n_output_node\n        self.input_shape = (input_width, input_width, input_channel)\n\n        self.t_min = t_min\n        self.beta = beta\n        self.algorithm_name = algorithm_name\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.json = None\n        self.total_data = {}\n        self.verbose = verbose\n        self.model_count = 0\n\n        self.bo = BayesianOptimizer(\n            self, self.t_min, self.optimize_mode, self.beta)\n        self.training_queue = []\n        self.descriptors = []\n        self.history = []\n\n        self.max_model_size = max_model_size\n        self.default_model_len = default_model_len\n        self.default_model_width = default_model_width\n\n        self.search_space = dict()\n\n\n    def update_search_space(self, search_space):\n        """"""\n        Update search space definition in tuner by search_space in neural architecture.\n        """"""\n        self.search_space = search_space\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Returns a set of trial neural architecture, as a serializable object.\n\n        Parameters\n        ----------\n        parameter_id : int\n        """"""\n        if not self.history:\n            self.init_search()\n\n        new_father_id = None\n        generated_graph = None\n        if not self.training_queue:\n            new_father_id, generated_graph = self.generate()\n            new_model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append(\n                (generated_graph, new_father_id, new_model_id))\n            self.descriptors.append(generated_graph.extract_descriptor())\n\n        graph, father_id, model_id = self.training_queue.pop(0)\n\n        # from graph to json\n        json_model_path = os.path.join(self.path, str(model_id) + "".json"")\n        json_out = graph_to_json(graph, json_model_path)\n        self.total_data[parameter_id] = (json_out, father_id, model_id)\n\n        return json_out\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Record an observation of the objective function.\n\n        Parameters\n        ----------\n        parameter_id : int\n            the id of a group of paramters that generated by nni manager.\n        parameters : dict\n            A group of parameters.\n        value : dict/float\n            if value is dict, it should have ""default"" key.\n        """"""\n        reward = extract_scalar_reward(value)\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(""Received parameter_id not in total_data."")\n\n        (_, father_id, model_id) = self.total_data[parameter_id]\n\n        graph = self.bo.searcher.load_model_by_id(model_id)\n\n        # to use the value and graph\n        self.add_model(reward, model_id)\n        self.update(father_id, graph, reward, model_id)\n\n\n    def init_search(self):\n        """"""\n        Call the generators to generate the initial architectures for the search.\n        """"""\n        if self.verbose:\n            logger.info(""Initializing search."")\n        for generator in self.generators:\n            graph = generator(self.n_classes, self.input_shape).generate(\n                self.default_model_len, self.default_model_width\n            )\n            model_id = self.model_count\n            self.model_count += 1\n            self.training_queue.append((graph, -1, model_id))\n            self.descriptors.append(graph.extract_descriptor())\n\n        if self.verbose:\n            logger.info(""Initialization finished."")\n\n\n    def generate(self):\n        """"""\n        Generate the next neural architecture.\n\n        Returns\n        -------\n        other_info : any object\n            Anything to be saved in the training queue together with the architecture.\n        generated_graph : Graph\n            An instance of Graph.\n        """"""\n        generated_graph, new_father_id = self.bo.generate(self.descriptors)\n        if new_father_id is None:\n            new_father_id = 0\n            generated_graph = self.generators[0](\n                self.n_classes, self.input_shape\n            ).generate(self.default_model_len, self.default_model_width)\n\n        return new_father_id, generated_graph\n\n    def update(self, other_info, graph, metric_value, model_id):\n        """"""\n        Update the controller with evaluation result of a neural architecture.\n\n        Parameters\n        ----------\n        other_info: any object\n            In our case it is the father ID in the search tree.\n        graph: Graph\n            An instance of Graph. The trained neural architecture.\n        metric_value: float\n            The final evaluated metric value.\n        model_id: int\n        """"""\n        father_id = other_info\n        self.bo.fit([graph.extract_descriptor()], [metric_value])\n        self.bo.add_child(father_id, model_id)\n\n    def add_model(self, metric_value, model_id):\n        """"""\n        Add model to the history, x_queue and y_queue\n\n        Parameters\n        ----------\n        metric_value : float\n        graph : dict\n        model_id : int\n\n        Returns\n        -------\n        model : dict\n        """"""\n        if self.verbose:\n            logger.info(""Saving model."")\n\n        # Update best_model text file\n        ret = {""model_id"": model_id, ""metric_value"": metric_value}\n        self.history.append(ret)\n        if model_id == self.get_best_model_id():\n            file = open(os.path.join(self.path, ""best_model.txt""), ""w"")\n            file.write(""best model: "" + str(model_id))\n            file.close()\n        return ret\n\n\n    def get_best_model_id(self):\n        """"""\n        Get the best model_id from history using the metric value\n        """"""\n\n        if self.optimize_mode is OptimizeMode.Maximize:\n            return max(self.history, key=lambda x: x[""metric_value""])[\n                ""model_id""]\n        return min(self.history, key=lambda x: x[""metric_value""])[""model_id""]\n\n\n    def load_model_by_id(self, model_id):\n        """"""\n        Get the model by model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        """"""\n\n        with open(os.path.join(self.path, str(model_id) + "".json"")) as fin:\n            json_str = fin.read().replace(""\\n"", """")\n\n        load_model = json_to_graph(json_str)\n        return load_model\n\n    def load_best_model(self):\n        """"""\n        Get the best model by model id\n\n        Returns\n        -------\n        load_model : Graph\n            the model graph representation\n        """"""\n        return self.load_model_by_id(self.get_best_model_id())\n\n    def get_metric_value_by_id(self, model_id):\n        """"""\n        Get the model metric valud by its model_id\n\n        Parameters\n        ----------\n        model_id : int\n            model index\n\n        Returns\n        -------\n        float\n             the model metric\n        """"""\n        for item in self.history:\n            if item[""model_id""] == model_id:\n                return item[""metric_value""]\n        return None\n\n    def import_data(self, data):\n        pass\n'"
src/sdk/pynni/nni/networkmorphism_tuner/nn.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom abc import abstractmethod\n\nfrom nni.networkmorphism_tuner.graph import Graph\nfrom nni.networkmorphism_tuner.layers import (StubDense, StubDropout1d,\n                                              StubReLU, get_batch_norm_class,\n                                              get_conv_class,\n                                              get_dropout_class,\n                                              get_global_avg_pooling_class,\n                                              get_pooling_class)\nfrom nni.networkmorphism_tuner.utils import Constant\n\n\nclass NetworkGenerator:\n    """"""The base class for generating a network.\n    It can be used to generate a CNN or Multi-Layer Perceptron.\n    Attributes:\n        n_output_node: Number of output nodes in the network.\n        input_shape: A tuple to represent the input shape.\n    """"""\n\n    def __init__(self, n_output_node, input_shape):\n        self.n_output_node = n_output_node\n        self.input_shape = input_shape\n\n    @abstractmethod\n    def generate(self, model_len, model_width):\n        pass\n\n\nclass CnnGenerator(NetworkGenerator):\n    """"""A class to generate CNN.\n    Attributes:\n          n_dim: `len(self.input_shape) - 1`\n          conv: A class that represents `(n_dim-1)` dimensional convolution.\n          dropout: A class that represents `(n_dim-1)` dimensional dropout.\n          global_avg_pooling: A class that represents `(n_dim-1)` dimensional Global Average Pooling.\n          pooling: A class that represents `(n_dim-1)` dimensional pooling.\n          batch_norm: A class that represents `(n_dim-1)` dimensional batch normalization.\n    """"""\n\n    def __init__(self, n_output_node, input_shape):\n        super(CnnGenerator, self).__init__(n_output_node, input_shape)\n        self.n_dim = len(self.input_shape) - 1\n        if len(self.input_shape) > 4:\n            raise ValueError(""The input dimension is too high."")\n        if len(self.input_shape) < 2:\n            raise ValueError(""The input dimension is too low."")\n        self.conv = get_conv_class(self.n_dim)\n        self.dropout = get_dropout_class(self.n_dim)\n        self.global_avg_pooling = get_global_avg_pooling_class(self.n_dim)\n        self.pooling = get_pooling_class(self.n_dim)\n        self.batch_norm = get_batch_norm_class(self.n_dim)\n\n    def generate(self, model_len=None, model_width=None):\n        """"""Generates a CNN.\n        Args:\n            model_len: An integer. Number of convolutional layers.\n            model_width: An integer. Number of filters for the convolutional layers.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        """"""\n\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        pooling_len = int(model_len / 4)\n        graph = Graph(self.input_shape, False)\n        temp_input_channel = self.input_shape[-1]\n        output_node_id = 0\n        stride = 1\n        for i in range(model_len):\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            output_node_id = graph.add_layer(\n                self.batch_norm(\n                    graph.node_list[output_node_id].shape[-1]), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                self.conv(\n                    temp_input_channel,\n                    model_width,\n                    kernel_size=3,\n                    stride=stride),\n                output_node_id,\n            )\n            temp_input_channel = model_width\n            if pooling_len == 0 or (\n                    (i + 1) % pooling_len == 0 and i != model_len - 1):\n                output_node_id = graph.add_layer(\n                    self.pooling(), output_node_id)\n\n        output_node_id = graph.add_layer(\n            self.global_avg_pooling(), output_node_id)\n        output_node_id = graph.add_layer(\n            self.dropout(Constant.CONV_DROPOUT_RATE), output_node_id\n        )\n        output_node_id = graph.add_layer(\n            StubDense(graph.node_list[output_node_id].shape[0], model_width),\n            output_node_id,\n        )\n        output_node_id = graph.add_layer(StubReLU(), output_node_id)\n        graph.add_layer(\n            StubDense(\n                model_width,\n                self.n_output_node),\n            output_node_id)\n        return graph\n\n\nclass MlpGenerator(NetworkGenerator):\n    """"""A class to generate Multi-Layer Perceptron.\n    """"""\n\n    def __init__(self, n_output_node, input_shape):\n        """"""Initialize the instance.\n        Args:\n            n_output_node: An integer. Number of output nodes in the network.\n            input_shape: A tuple. Input shape of the network. If it is 1D, ensure the value is appended by a comma\n                in the tuple.\n        """"""\n        super(MlpGenerator, self).__init__(n_output_node, input_shape)\n        if len(self.input_shape) > 1:\n            raise ValueError(""The input dimension is too high."")\n\n    def generate(self, model_len=None, model_width=None):\n        """"""Generates a Multi-Layer Perceptron.\n        Args:\n            model_len: An integer. Number of hidden layers.\n            model_width: An integer or a list of integers of length `model_len`. If it is a list, it represents the\n                number of nodes in each hidden layer. If it is an integer, all hidden layers have nodes equal to this\n                value.\n        Returns:\n            An instance of the class Graph. Represents the neural architecture graph of the generated model.\n        """"""\n        if model_len is None:\n            model_len = Constant.MODEL_LEN\n        if model_width is None:\n            model_width = Constant.MODEL_WIDTH\n        if isinstance(model_width, list) and not len(model_width) == model_len:\n            raise ValueError(\n                ""The length of \'model_width\' does not match \'model_len\'"")\n        elif isinstance(model_width, int):\n            model_width = [model_width] * model_len\n\n        graph = Graph(self.input_shape, False)\n        output_node_id = 0\n        n_nodes_prev_layer = self.input_shape[0]\n        for width in model_width:\n            output_node_id = graph.add_layer(\n                StubDense(n_nodes_prev_layer, width), output_node_id\n            )\n            output_node_id = graph.add_layer(\n                StubDropout1d(Constant.MLP_DROPOUT_RATE), output_node_id\n            )\n            output_node_id = graph.add_layer(StubReLU(), output_node_id)\n            n_nodes_prev_layer = width\n\n        graph.add_layer(\n            StubDense(\n                n_nodes_prev_layer,\n                self.n_output_node),\n            output_node_id)\n        return graph\n'"
src/sdk/pynni/nni/networkmorphism_tuner/utils.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n\nclass Constant:\n    '''Constant for the Tuner.\n    '''\n    MAX_LAYERS = 100\n    N_NEIGHBOURS = 8\n    MAX_MODEL_SIZE = 1 << 24\n    KERNEL_LAMBDA = 1.0\n    BETA = 2.576\n    MLP_MODEL_LEN = 3\n    MLP_MODEL_WIDTH = 5\n    MODEL_LEN = 3\n    MODEL_WIDTH = 64\n    POOLING_KERNEL_SIZE = 2\n    DENSE_DROPOUT_RATE = 0.5\n    CONV_DROPOUT_RATE = 0.25\n    MLP_DROPOUT_RATE = 0.25\n    CONV_BLOCK_DISTANCE = 2\n    BATCH_SIZE = 128\n    T_MIN = 0.0001\n"""
src/sdk/pynni/nni/pbt_tuner/__init__.py,0,b''
src/sdk/pynni/nni/pbt_tuner/pbt_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport copy\nimport logging\nimport os\nimport random\nimport numpy as np\n\nimport nni\nimport nni.parameter_expressions\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward, split_index, json2parameter, json2space\n\n\nlogger = logging.getLogger(\'pbt_tuner_AutoML\')\n\n\ndef perturbation(hyperparameter_type, value, resample_probablity, uv, ub, lv, lb, random_state):\n    """"""\n    Perturbation for hyperparameters\n\n    Parameters\n    ----------\n    hyperparameter_type : str\n        type of hyperparameter\n    value : list\n        parameters for sampling hyperparameter\n    resample_probability : float\n        probability for resampling\n    uv : float/int\n        upper value after perturbation\n    ub : float/int\n        upper bound\n    lv : float/int\n        lower value after perturbation\n    lb : float/int\n        lower bound\n    random_state : RandomState\n        random state\n    """"""\n    if random.random() < resample_probablity:\n        if hyperparameter_type == ""choice"":\n            return value.index(nni.parameter_expressions.choice(value, random_state))\n        else:\n            return getattr(nni.parameter_expressions, hyperparameter_type)(*(value + [random_state]))\n    else:\n        if random.random() > 0.5:\n            return min(uv, ub)\n        else:\n            return max(lv, lb)\n\n\ndef exploit_and_explore(bot_trial_info, top_trial_info, factor, resample_probability, epoch, search_space):\n    """"""\n    Replace checkpoint of bot_trial with top, and perturb hyperparameters\n\n    Parameters\n    ----------\n    bot_trial_info : TrialInfo\n        bottom model whose parameters should be replaced\n    top_trial_info : TrialInfo\n        better model\n    factor : float\n        factor for perturbation\n    resample_probability : float\n        probability for resampling\n    epoch : int\n        step of PBTTuner\n    search_space : dict\n        search_space to keep perturbed hyperparameters in range\n    """"""\n    bot_checkpoint_dir = bot_trial_info.checkpoint_dir\n    top_hyper_parameters = top_trial_info.hyper_parameters\n    hyper_parameters = copy.deepcopy(top_hyper_parameters)\n    random_state = np.random.RandomState()\n    hyper_parameters[\'load_checkpoint_dir\'] = hyper_parameters[\'save_checkpoint_dir\']\n    hyper_parameters[\'save_checkpoint_dir\'] = os.path.join(bot_checkpoint_dir, str(epoch))\n    for key in hyper_parameters.keys():\n        hyper_parameter = hyper_parameters[key]\n        if key == \'load_checkpoint_dir\' or key == \'save_checkpoint_dir\':\n            continue\n        elif search_space[key][""_type""] == ""choice"":\n            choices = search_space[key][""_value""]\n            ub, uv = len(choices) - 1, choices.index(hyper_parameter) + 1\n            lb, lv = 0, choices.index(hyper_parameter) - 1\n        elif search_space[key][""_type""] == ""randint"":\n            lb, ub = search_space[key][""_value""][:2]\n            ub -= 1\n            uv = hyper_parameter + 1\n            lv = hyper_parameter - 1\n        elif search_space[key][""_type""] == ""uniform"":\n            lb, ub = search_space[key][""_value""][:2]\n            perturb = (ub - lb) * factor\n            uv = hyper_parameter + perturb\n            lv = hyper_parameter - perturb\n        elif search_space[key][""_type""] == ""quniform"":\n            lb, ub, q = search_space[key][""_value""][:3]\n            multi = round(hyper_parameter / q)\n            uv = (multi + 1) * q\n            lv = (multi - 1) * q\n        elif search_space[key][""_type""] == ""loguniform"":\n            lb, ub = search_space[key][""_value""][:2]\n            perturb = (np.log(ub) - np.log(lb)) * factor\n            uv = np.exp(min(np.log(hyper_parameter) + perturb, np.log(ub)))\n            lv = np.exp(max(np.log(hyper_parameter) - perturb, np.log(lb)))\n        elif search_space[key][""_type""] == ""qloguniform"":\n            lb, ub, q = search_space[key][""_value""][:3]\n            multi = round(hyper_parameter / q)\n            uv = (multi + 1) * q\n            lv = (multi - 1) * q\n        elif search_space[key][""_type""] == ""normal"":\n            sigma = search_space[key][""_value""][1]\n            perturb = sigma * factor\n            uv = ub = hyper_parameter + perturb\n            lv = lb = hyper_parameter - perturb\n        elif search_space[key][""_type""] == ""qnormal"":\n            q = search_space[key][""_value""][2]\n            uv = ub = hyper_parameter + q\n            lv = lb = hyper_parameter - q\n        elif search_space[key][""_type""] == ""lognormal"":\n            sigma = search_space[key][""_value""][1]\n            perturb = sigma * factor\n            uv = ub = np.exp(np.log(hyper_parameter) + perturb)\n            lv = lb = np.exp(np.log(hyper_parameter) - perturb)\n        elif search_space[key][""_type""] == ""qlognormal"":\n            q = search_space[key][""_value""][2]\n            uv = ub = hyper_parameter + q\n            lv, lb = hyper_parameter - q, 1E-10\n        else:\n            logger.warning(""Illegal type to perturb: %s"", search_space[key][""_type""])\n            continue\n\n        if search_space[key][""_type""] == ""choice"":\n            idx = perturbation(search_space[key][""_type""], search_space[key][""_value""],\n                               resample_probability, uv, ub, lv, lb, random_state)\n            hyper_parameters[key] = choices[idx]\n        else:\n            hyper_parameters[key] = perturbation(search_space[key][""_type""], search_space[key][""_value""],\n                                                 resample_probability, uv, ub, lv, lb, random_state)\n    bot_trial_info.hyper_parameters = hyper_parameters\n    bot_trial_info.clean_id()\n\n\nclass TrialInfo:\n    """"""\n    Information of each trial, refresh for each epoch\n\n    """"""\n\n    def __init__(self, checkpoint_dir=None, hyper_parameters=None, parameter_id=None, score=None):\n        self.checkpoint_dir = checkpoint_dir\n        self.hyper_parameters = hyper_parameters\n        self.parameter_id = parameter_id\n        self.score = score\n\n    def clean_id(self):\n        self.parameter_id = None\n\n\nclass PBTTuner(Tuner):\n    def __init__(self, optimize_mode=""maximize"", all_checkpoint_dir=None, population_size=10, factor=0.2,\n                 resample_probability=0.25, fraction=0.2):\n        """"""\n        Initialization\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        all_checkpoint_dir : str\n            directory to store training model checkpoint\n        population_size : int\n            number of trials for each epoch\n        factor : float\n            factor for perturbation\n        resample_probability : float\n            probability for resampling\n        fraction : float\n            fraction for selecting bottom and top trials\n        """"""\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        if all_checkpoint_dir is None:\n            all_checkpoint_dir = os.getenv(\'NNI_CHECKPOINT_DIRECTORY\')\n            logger.info(""Checkpoint dir is set to %s by default."", all_checkpoint_dir)\n        self.all_checkpoint_dir = all_checkpoint_dir\n        self.population_size = population_size\n        self.factor = factor\n        self.resample_probability = resample_probability\n        self.fraction = fraction\n        # defined in trial code\n        #self.perturbation_interval = perturbation_interval\n\n        self.population = None\n        self.pos = -1\n        self.param_ids = []\n        self.running = {}\n        self.finished = []\n        self.credit = 0\n        self.finished_trials = 0\n        self.epoch = 0\n\n        self.searchspace_json = None\n        self.space = None\n\n        self.send_trial_callback = None\n\n        logger.info(\'PBT tuner initialization\')\n\n    def update_search_space(self, search_space):\n        """"""\n        Get search space\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space\n        """"""\n        logger.info(\'Update search space %s\', search_space)\n        self.searchspace_json = search_space\n        self.space = json2space(self.searchspace_json)\n\n        self.random_state = np.random.RandomState()\n        self.population = []\n        is_rand = dict()\n\n        for item in self.space:\n            is_rand[item] = True\n\n        for i in range(self.population_size):\n            hyper_parameters = json2parameter(\n                self.searchspace_json, is_rand, self.random_state)\n            hyper_parameters = split_index(hyper_parameters)\n            checkpoint_dir = os.path.join(self.all_checkpoint_dir, str(i))\n            hyper_parameters[\'load_checkpoint_dir\'] = os.path.join(checkpoint_dir, str(self.epoch))\n            hyper_parameters[\'save_checkpoint_dir\'] = os.path.join(checkpoint_dir, str(self.epoch))\n            self.population.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=hyper_parameters))\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        """"""\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Used for send_trial_callback.\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        """"""\n        result = []\n        self.send_trial_callback = kwargs[\'st_callback\']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(""generating param for %s"", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        """"""\n        if self.pos == self.population_size - 1:\n            logger.debug(\'Credit added by one in parameters request\')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError(\'No more parameters now.\')\n        self.pos += 1\n        trial_info = self.population[self.pos]\n        trial_info.parameter_id = parameter_id\n        self.running[parameter_id] = trial_info\n        logger.info(\'Generate parameter : %s\', trial_info.hyper_parameters)\n        return trial_info.hyper_parameters\n\n    def _proceed_next_epoch(self):\n        """"""\n        """"""\n        logger.info(\'Proceeding to next epoch\')\n        self.epoch += 1\n        self.population = []\n        self.pos = -1\n        self.running = {}\n        #exploit and explore\n        reverse = True if self.optimize_mode == OptimizeMode.Maximize else False\n        self.finished = sorted(self.finished, key=lambda x: x.score, reverse=reverse)\n        cutoff = int(np.ceil(self.fraction * len(self.finished)))\n        tops = self.finished[:cutoff]\n        bottoms = self.finished[self.finished_trials - cutoff:]\n        for bottom in bottoms:\n            top = np.random.choice(tops)\n            exploit_and_explore(bottom, top, self.factor, self.resample_probability, self.epoch, self.searchspace_json)\n        for trial in self.finished:\n            if trial not in bottoms:\n                trial.clean_id()\n                trial.hyper_parameters[\'load_checkpoint_dir\'] = trial.hyper_parameters[\'save_checkpoint_dir\']\n                trial.hyper_parameters[\'save_checkpoint_dir\'] = os.path.join(trial.checkpoint_dir, str(self.epoch))\n        self.finished_trials = 0\n        for _ in range(self.population_size):\n            trial_info = self.finished.pop()\n            self.population.append(trial_info)\n        while self.credit > 0 and self.pos + 1 < len(self.population):\n            self.credit -= 1\n            self.pos += 1\n            parameter_id = self.param_ids.pop()\n            trial_info = self.population[self.pos]\n            trial_info.parameter_id = parameter_id\n            self.running[parameter_id] = trial_info\n            self.send_trial_callback(parameter_id, trial_info.hyper_parameters)\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Receive trial\'s result. if the number of finished trials equals ``self.population_size``, start the next epoch to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        """"""\n        logger.info(\'Get one trial result, id = %d, value = %s\', parameter_id, value)\n        value = extract_scalar_reward(value)\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        """"""\n        Deal with trial failure\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Unstable parameters which should be ignored by normal users.\n        """"""\n        if success:\n            return\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = float(\'inf\')\n        else:\n            value = float(\'-inf\')\n        trial_info = self.running.pop(parameter_id, None)\n        trial_info.score = value\n        self.finished.append(trial_info)\n        self.finished_trials += 1\n        if self.finished_trials == self.population_size:\n            self._proceed_next_epoch()\n\n    def import_data(self, data):\n        """"""\n        Parameters\n        ----------\n        data : json obj\n            imported data records\n\n        Returns\n        -------\n        int\n            the start epoch number after data imported, only used for unittest\n        """"""\n        if self.running:\n            logger.warning(""Do not support importing data in the middle of experiment"")\n            return\n        # the following is for experiment resume\n        _completed_num = 0\n        epoch_data_dict = {}\n        for trial_info in data:\n            logger.info(""Process data record %s / %s"", _completed_num, len(data))\n            _completed_num += 1\n            # simply validate data format\n            _params = trial_info[""parameter""]\n            _value = trial_info[\'value\']\n            # assign fake value for failed trials\n            if not _value:\n                logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                _value = float(\'inf\') if self.optimize_mode == OptimizeMode.Minimize else float(\'-inf\')\n            _value = extract_scalar_reward(_value)\n            if \'save_checkpoint_dir\' not in _params:\n                logger.warning(""Invalid data record: save_checkpoint_dir is missing, abandon data import."")\n                return\n            epoch_num = int(os.path.basename(_params[\'save_checkpoint_dir\']))\n            if epoch_num not in epoch_data_dict:\n                epoch_data_dict[epoch_num] = []\n            epoch_data_dict[epoch_num].append((_params, _value))\n        if not epoch_data_dict:\n            logger.warning(""No valid epochs, abandon data import."")\n            return\n        # figure out start epoch for resume\n        max_epoch_num = max(epoch_data_dict, key=int)\n        if len(epoch_data_dict[max_epoch_num]) < self.population_size:\n            max_epoch_num -= 1\n        # If there is no a single complete round, no data to import, start from scratch\n        if max_epoch_num < 0:\n            logger.warning(""No completed epoch, abandon data import."")\n            return\n        assert len(epoch_data_dict[max_epoch_num]) == self.population_size\n        # check existence of trial save checkpoint dir\n        for params, _ in epoch_data_dict[max_epoch_num]:\n            if not os.path.isdir(params[\'save_checkpoint_dir\']):\n                logger.warning(""save_checkpoint_dir %s does not exist, data will not be resumed"", params[\'save_checkpoint_dir\'])\n                return\n        # resume data\n        self.epoch = max_epoch_num\n        self.finished_trials = self.population_size\n        for params, value in epoch_data_dict[max_epoch_num]:\n            checkpoint_dir = os.path.dirname(params[\'save_checkpoint_dir\'])\n            self.finished.append(TrialInfo(checkpoint_dir=checkpoint_dir, hyper_parameters=params, score=value))\n        self._proceed_next_epoch()\n        logger.info(""Successfully import data to PBT tuner, total data: %d, imported data: %d."", len(data), self.population_size)\n        logger.info(""Start from epoch %d ..."", self.epoch)\n        return self.epoch # return for test\n'"
src/sdk/pynni/nni/platform/__init__.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom ..env_vars import trial_env_vars, dispatcher_env_vars\n\nassert dispatcher_env_vars.SDK_PROCESS != 'dispatcher'\n\nif trial_env_vars.NNI_PLATFORM is None:\n    from .standalone import *\nelif trial_env_vars.NNI_PLATFORM == 'unittest':\n    from .test import *\nelif trial_env_vars.NNI_PLATFORM in ('local', 'remote', 'pai', 'kubeflow', 'frameworkcontroller', 'paiYarn', 'dlts'):\n    from .local import *\nelse:\n    raise RuntimeError('Unknown platform %s' % trial_env_vars.NNI_PLATFORM)\n"""
src/sdk/pynni/nni/platform/local.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport json\nimport time\nimport subprocess\n\nfrom ..common import init_logger\nfrom ..env_vars import trial_env_vars\nfrom ..utils import to_json\n\n_sysdir = trial_env_vars.NNI_SYS_DIR\nif not os.path.exists(os.path.join(_sysdir, \'.nni\')):\n    os.makedirs(os.path.join(_sysdir, \'.nni\'))\n_metric_file = open(os.path.join(_sysdir, \'.nni\', \'metrics\'), \'wb\')\n\n_outputdir = trial_env_vars.NNI_OUTPUT_DIR\nif not os.path.exists(_outputdir):\n    os.makedirs(_outputdir)\n\n_nni_platform = trial_env_vars.NNI_PLATFORM\nif _nni_platform == \'local\':\n    _log_file_path = os.path.join(_outputdir, \'trial.log\')\n    init_logger(_log_file_path)\n\n_multiphase = trial_env_vars.MULTI_PHASE\n\n_param_index = 0\n\ndef request_next_parameter():\n    metric = to_json({\n        \'trial_job_id\': trial_env_vars.NNI_TRIAL_JOB_ID,\n        \'type\': \'REQUEST_PARAMETER\',\n        \'sequence\': 0,\n        \'parameter_index\': _param_index\n    })\n    send_metric(metric)\n\ndef get_next_parameter():\n    global _param_index\n    params_file_name = \'\'\n    if _multiphase in (\'true\', \'True\'):\n        params_file_name = (\'parameter_{}.cfg\'.format(_param_index), \'parameter.cfg\')[_param_index == 0]\n    else:\n        if _param_index > 0:\n            return None\n        elif _param_index == 0:\n            params_file_name = \'parameter.cfg\'\n        else:\n            raise AssertionError(\'_param_index value ({}) should >=0\'.format(_param_index))\n\n    params_filepath = os.path.join(_sysdir, params_file_name)\n    if not os.path.isfile(params_filepath):\n        request_next_parameter()\n    while not (os.path.isfile(params_filepath) and os.path.getsize(params_filepath) > 0):\n        time.sleep(3)\n    params_file = open(params_filepath, \'r\')\n    params = json.load(params_file)\n    _param_index += 1\n    return params\n\ndef send_metric(string):\n    if _nni_platform != \'local\':\n        assert len(string) < 1000000, \'Metric too long\'\n        print(""NNISDK_MEb\'%s\'"" % (string), flush=True)\n    else:\n        data = (string + \'\\n\').encode(\'utf8\')\n        assert len(data) < 1000000, \'Metric too long\'\n        _metric_file.write(b\'ME%06d%b\' % (len(data), data))\n        _metric_file.flush()\n        if sys.platform == ""win32"":\n            file = open(_metric_file.name)\n            file.close()\n        else:\n            subprocess.run([\'touch\', _metric_file.name], check=True)\n\ndef get_experiment_id():\n    return trial_env_vars.NNI_EXP_ID\n\ndef get_trial_id():\n    return trial_env_vars.NNI_TRIAL_JOB_ID\n\ndef get_sequence_id():\n    return int(trial_env_vars.NNI_TRIAL_SEQ_ID)\n'"
src/sdk/pynni/nni/platform/standalone.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport json_tricks\n\nfrom ..common import init_standalone_logger\n\n__all__ = [\n    'get_next_parameter',\n    'get_experiment_id',\n    'get_trial_id',\n    'get_sequence_id',\n    'send_metric',\n]\n\ninit_standalone_logger()\n_logger = logging.getLogger('nni')\n\n\ndef get_next_parameter():\n    _logger.warning('Requesting parameter without NNI framework, returning empty dict')\n    return {\n        'parameter_id': None,\n        'parameters': {}\n    }\n\ndef get_experiment_id():\n    return 'STANDALONE'\n\ndef get_trial_id():\n    return 'STANDALONE'\n\ndef get_sequence_id():\n    return 0\n\ndef send_metric(string):\n    metric = json_tricks.loads(string)\n    if metric['type'] == 'FINAL':\n        _logger.info('Final result: %s', metric['value'])\n    elif metric['type'] == 'PERIODICAL':\n        _logger.info('Intermediate result: %s  (Index %s)', metric['value'], metric['sequence'])\n    else:\n        _logger.error('Unexpected metric: %s', string)\n"""
src/sdk/pynni/nni/platform/test.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n# pylint: skip-file\n\nimport copy\nimport json_tricks\n\n\n_params = None\n_last_metric = None\n\n\ndef get_next_parameter():\n    return _params\n\ndef get_experiment_id():\n    return 'fakeidex'\n\ndef get_trial_id():\n    return 'fakeidtr'\n\ndef get_sequence_id():\n    return 0\n\ndef send_metric(string):\n    global _last_metric\n    _last_metric = string\n\n\ndef init_params(params):\n    global _params\n    _params = copy.deepcopy(params)\n\ndef get_last_metric():\n    metrics = json_tricks.loads(_last_metric)\n    metrics['value'] = json_tricks.loads(metrics['value'])\n\n    return metrics\n"""
src/sdk/pynni/nni/ppo_tuner/__init__.py,0,b'from .ppo_tuner import PPOTuner\n'
src/sdk/pynni/nni/ppo_tuner/distri.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nfunctions for sampling from hidden state\n""""""\n\nimport tensorflow as tf\n\nfrom .util import fc\n\n\nclass Pd:\n    """"""\n    A particular probability distribution\n    """"""\n    def flatparam(self):\n        raise NotImplementedError\n    def mode(self):\n        raise NotImplementedError\n    def neglogp(self, x):\n        # Usually it\'s easier to define the negative logprob\n        raise NotImplementedError\n    def kl(self, other):\n        raise NotImplementedError\n    def entropy(self):\n        raise NotImplementedError\n    def sample(self):\n        raise NotImplementedError\n    def logp(self, x):\n        return - self.neglogp(x)\n    def get_shape(self):\n        return self.flatparam().shape\n    @property\n    def shape(self):\n        return self.get_shape()\n    def __getitem__(self, idx):\n        return self.__class__(self.flatparam()[idx])\n\nclass PdType:\n    """"""\n    Parametrized family of probability distributions\n    """"""\n    def pdclass(self):\n        raise NotImplementedError\n    def pdfromflat(self, flat, mask, nsteps, size, is_act_model):\n        return self.pdclass()(flat, mask, nsteps, size, is_act_model)\n    def pdfromlatent(self, latent_vector, init_scale, init_bias):\n        raise NotImplementedError\n    def param_shape(self):\n        raise NotImplementedError\n    def sample_shape(self):\n        raise NotImplementedError\n    def sample_dtype(self):\n        raise NotImplementedError\n\n    def param_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=tf.float32, shape=prepend_shape+self.param_shape(), name=name)\n    def sample_placeholder(self, prepend_shape, name=None):\n        return tf.placeholder(dtype=self.sample_dtype(), shape=prepend_shape+self.sample_shape(), name=name)\n\nclass CategoricalPd(Pd):\n    """"""\n    Categorical probability distribution\n    """"""\n    def __init__(self, logits, mask_npinf, nsteps, size, is_act_model):\n        self.logits = logits\n        self.mask_npinf = mask_npinf\n        self.nsteps = nsteps\n        self.size = size\n        self.is_act_model = is_act_model\n    def flatparam(self):\n        return self.logits\n    def mode(self):\n        return tf.argmax(self.logits, axis=-1)\n\n    @property\n    def mean(self):\n        return tf.nn.softmax(self.logits)\n    def neglogp(self, x):\n        """"""\n        return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n        Note: we can\'t use sparse_softmax_cross_entropy_with_logits because\n              the implementation does not allow second-order derivatives...\n        """"""\n        if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n            # one-hot encoding\n            x_shape_list = x.shape.as_list()\n            logits_shape_list = self.logits.get_shape().as_list()[:-1]\n            for xs, ls in zip(x_shape_list, logits_shape_list):\n                if xs is not None and ls is not None:\n                    assert xs == ls, \'shape mismatch: {} in x vs {} in logits\'.format(xs, ls)\n\n            x = tf.one_hot(x, self.logits.get_shape().as_list()[-1])\n        else:\n            # already encoded\n            assert x.shape.as_list() == self.logits.shape.as_list()\n\n        return tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=self.logits,\n            labels=x)\n\n    def kl(self, other):\n        """"""kl""""""\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        a1 = other.logits - tf.reduce_max(other.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        ea1 = tf.exp(a1)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        z1 = tf.reduce_sum(ea1, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (a0 - tf.log(z0) - a1 + tf.log(z1)), axis=-1)\n\n    def entropy(self):\n        """"""compute entropy""""""\n        a0 = self.logits - tf.reduce_max(self.logits, axis=-1, keepdims=True)\n        ea0 = tf.exp(a0)\n        z0 = tf.reduce_sum(ea0, axis=-1, keepdims=True)\n        p0 = ea0 / z0\n        return tf.reduce_sum(p0 * (tf.log(z0) - a0), axis=-1)\n\n    def sample(self):\n        """"""sample from logits""""""\n        if not self.is_act_model:\n            re_res = tf.reshape(self.logits, [-1, self.nsteps, self.size])\n            masked_res = tf.math.add(re_res, self.mask_npinf)\n            re_masked_res = tf.reshape(masked_res, [-1, self.size])\n\n            u = tf.random_uniform(tf.shape(re_masked_res), dtype=self.logits.dtype)\n            return tf.argmax(re_masked_res - tf.log(-1*tf.log(u)), axis=-1)\n        else:\n            u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)\n            return tf.argmax(self.logits - tf.log(-1*tf.log(u)), axis=-1)\n\n    @classmethod\n    def fromflat(cls, flat):\n        return cls(flat) # pylint: disable=no-value-for-parameter\n\nclass CategoricalPdType(PdType):\n    """"""\n    To create CategoricalPd\n    """"""\n    def __init__(self, ncat, nsteps, np_mask, is_act_model):\n        self.ncat = ncat\n        self.nsteps = nsteps\n        self.np_mask = np_mask\n        self.is_act_model = is_act_model\n    def pdclass(self):\n        return CategoricalPd\n\n    def pdfromlatent(self, latent_vector, init_scale=1.0, init_bias=0.0):\n        """"""add fc and create CategoricalPd""""""\n        pdparam, mask, mask_npinf = _matching_fc(latent_vector, \'pi\', self.ncat, self.nsteps,\n                                                 init_scale=init_scale, init_bias=init_bias,\n                                                 np_mask=self.np_mask, is_act_model=self.is_act_model)\n        return self.pdfromflat(pdparam, mask_npinf, self.nsteps, self.ncat, self.is_act_model), pdparam, mask, mask_npinf\n\n    def param_shape(self):\n        return [self.ncat]\n    def sample_shape(self):\n        return []\n    def sample_dtype(self):\n        return tf.int32\n\ndef _matching_fc(tensor, name, size, nsteps, init_scale, init_bias, np_mask, is_act_model):\n    """"""\n    Add fc op, and add mask op when not in action mode\n    """"""\n    if tensor.shape[-1] == size:\n        assert False\n        return tensor\n    else:\n        mask = tf.get_variable(""act_mask"", dtype=tf.float32, initializer=np_mask[0], trainable=False)\n        mask_npinf = tf.get_variable(""act_mask_npinf"", dtype=tf.float32, initializer=np_mask[1], trainable=False)\n        res = fc(tensor, name, size, init_scale=init_scale, init_bias=init_bias)\n        if not is_act_model:\n            re_res = tf.reshape(res, [-1, nsteps, size])\n            masked_res = tf.math.multiply(re_res, mask)\n            re_masked_res = tf.reshape(masked_res, [-1, size])\n            return re_masked_res, mask, mask_npinf\n        else:\n            return res, mask, mask_npinf\n'"
src/sdk/pynni/nni/ppo_tuner/model.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nthe main model of policy/value network\n""""""\n\nimport tensorflow as tf\n\nfrom .util import initialize, get_session\n\nclass Model:\n    """"""\n    We use this object to :\n        __init__:\n            - Creates the step_model\n            - Creates the train_model\n\n        train():\n            - Make the training part (feedforward and retropropagation of gradients)\n\n        save/load():\n            - Save load the model\n    """"""\n    def __init__(self, *, policy, nbatch_act, nbatch_train,\n                 nsteps, ent_coef, vf_coef, max_grad_norm, microbatch_size=None, np_mask=None):\n        self.sess = sess = get_session()\n\n        with tf.variable_scope(\'ppo2_model\', reuse=tf.AUTO_REUSE):\n            # CREATE OUR TWO MODELS\n            # act_model that is used for sampling\n            act_model = policy(nbatch_act, 1, sess, np_mask=np_mask, is_act_model=True)\n\n            # Train model for training\n            if microbatch_size is None:\n                train_model = policy(nbatch_train, nsteps, sess, np_mask=np_mask, is_act_model=False)\n            else:\n                train_model = policy(microbatch_size, nsteps, sess, np_mask=np_mask, is_act_model=False)\n\n        # CREATE THE PLACEHOLDERS\n        self.A = A = train_model.pdtype.sample_placeholder([None])\n        self.ADV = ADV = tf.placeholder(tf.float32, [None])\n        self.R = R = tf.placeholder(tf.float32, [None])\n        # Keep track of old actor\n        self.OLDNEGLOGPAC = OLDNEGLOGPAC = tf.placeholder(tf.float32, [None])\n        # Keep track of old critic\n        self.OLDVPRED = OLDVPRED = tf.placeholder(tf.float32, [None])\n        self.LR = LR = tf.placeholder(tf.float32, [])\n        # Cliprange\n        self.CLIPRANGE = CLIPRANGE = tf.placeholder(tf.float32, [])\n\n        neglogpac = train_model.pd.neglogp(A)\n\n        # Calculate the entropy\n        # Entropy is used to improve exploration by limiting the premature convergence to suboptimal policy.\n        entropy = tf.reduce_mean(train_model.pd.entropy())\n\n        # CALCULATE THE LOSS\n        # Total loss = Policy gradient loss - entropy * entropy coefficient + Value coefficient * value loss\n\n        # Clip the value to reduce variability during Critic training\n        # Get the predicted value\n        vpred = train_model.vf\n        vpredclipped = OLDVPRED + tf.clip_by_value(train_model.vf - OLDVPRED, - CLIPRANGE, CLIPRANGE)\n        # Unclipped value\n        vf_losses1 = tf.square(vpred - R)\n        # Clipped value\n        vf_losses2 = tf.square(vpredclipped - R)\n\n        vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n\n        # Calculate ratio (pi current policy / pi old policy)\n        ratio = tf.exp(OLDNEGLOGPAC - neglogpac)\n\n        # Defining Loss = - J is equivalent to max J\n        pg_losses = -ADV * ratio\n\n        pg_losses2 = -ADV * tf.clip_by_value(ratio, 1.0 - CLIPRANGE, 1.0 + CLIPRANGE)\n\n        # Final PG loss\n        pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n        approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - OLDNEGLOGPAC))\n        clipfrac = tf.reduce_mean(tf.to_float(tf.greater(tf.abs(ratio - 1.0), CLIPRANGE)))\n\n        # Total loss\n        loss = pg_loss - entropy * ent_coef + vf_loss * vf_coef\n\n        # UPDATE THE PARAMETERS USING LOSS\n        # 1. Get the model parameters\n        params = tf.trainable_variables(\'ppo2_model\')\n        # 2. Build our trainer\n        self.trainer = tf.train.AdamOptimizer(learning_rate=LR, epsilon=1e-5)\n        # 3. Calculate the gradients\n        grads_and_var = self.trainer.compute_gradients(loss, params)\n        grads, var = zip(*grads_and_var)\n\n        if max_grad_norm is not None:\n            # Clip the gradients (normalize)\n            grads, _grad_norm = tf.clip_by_global_norm(grads, max_grad_norm)\n        grads_and_var = list(zip(grads, var))\n        # zip aggregate each gradient with parameters associated\n        # For instance zip(ABCD, xyza) => Ax, By, Cz, Da\n\n        self.grads = grads\n        self.var = var\n        self._train_op = self.trainer.apply_gradients(grads_and_var)\n        self.loss_names = [\'policy_loss\', \'value_loss\', \'policy_entropy\', \'approxkl\', \'clipfrac\']\n        self.stats_list = [pg_loss, vf_loss, entropy, approxkl, clipfrac]\n\n\n        self.train_model = train_model\n        self.act_model = act_model\n        self.step = act_model.step\n        self.value = act_model.value\n        self.initial_state = act_model.initial_state\n\n        initialize()\n\n    def train(self, lr, cliprange, obs, returns, masks, actions, values, neglogpacs, states=None):\n        """"""\n        Train the model.\n        Here we calculate advantage A(s,a) = R + yV(s\') - V(s)\n\n        Returns\n        -------\n        obj\n            = R + yV(s\')\n        """"""\n        advs = returns - values\n\n        # Normalize the advantages\n        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n\n        td_map = {\n            self.train_model.X : obs,\n            self.A : actions,\n            self.ADV : advs,\n            self.R : returns,\n            self.LR : lr,\n            self.CLIPRANGE : cliprange,\n            self.OLDNEGLOGPAC : neglogpacs,\n            self.OLDVPRED : values\n        }\n        if states is not None:\n            td_map[self.train_model.S] = states\n            td_map[self.train_model.M] = masks\n\n        return self.sess.run(\n            self.stats_list + [self._train_op],\n            td_map\n        )[:-1]\n'"
src/sdk/pynni/nni/ppo_tuner/policy.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nbuild policy/value network from model\n""""""\n\nimport tensorflow as tf\n\nfrom .distri import CategoricalPdType\nfrom .util import lstm_model, fc, observation_placeholder, adjust_shape\n\n\nclass PolicyWithValue:\n    """"""\n    Encapsulates fields and methods for RL policy and value function estimation with shared parameters\n    """"""\n\n    def __init__(self, env, observations, latent, estimate_q=False, vf_latent=None, sess=None, np_mask=None, is_act_model=False, **tensors):\n        """"""\n        Parameters\n        ----------\n        env : obj\n            RL environment\n        observations : tensorflow placeholder\n            Tensorflow placeholder in which the observations will be fed\n        latent : tensor\n            Latent state from which policy distribution parameters should be inferred\n        vf_latent : tensor\n            Latent state from which value function should be inferred (if None, then latent is used)\n        sess : tensorflow session\n            Tensorflow session to run calculations in (if None, default session is used)\n        **tensors\n            Tensorflow tensors for additional attributes such as state or mask\n        """"""\n\n        self.X = observations\n        self.state = tf.constant([])\n        self.initial_state = None\n        self.__dict__.update(tensors)\n\n        vf_latent = vf_latent if vf_latent is not None else latent\n\n        vf_latent = tf.layers.flatten(vf_latent)\n        latent = tf.layers.flatten(latent)\n\n        # Based on the action space, will select what probability distribution type\n        self.np_mask = np_mask\n        self.pdtype = CategoricalPdType(env.action_space.n, env.nsteps, np_mask, is_act_model)\n\n        self.act_latent = latent\n        self.nh = env.action_space.n\n\n        self.pd, self.pi, self.mask, self.mask_npinf = self.pdtype.pdfromlatent(latent, init_scale=0.01)\n\n        # Take an action\n        self.action = self.pd.sample()\n\n        # Calculate the neg log of our probability\n        self.neglogp = self.pd.neglogp(self.action)\n        self.sess = sess or tf.get_default_session()\n\n        assert estimate_q is False\n        self.vf = fc(vf_latent, \'vf\', 1)\n        self.vf = self.vf[:, 0]\n\n        if is_act_model:\n            self._build_model_for_step()\n\n    def _evaluate(self, variables, observation, **extra_feed):\n        sess = self.sess\n        feed_dict = {self.X: adjust_shape(self.X, observation)}\n        for inpt_name, data in extra_feed.items():\n            if inpt_name in self.__dict__.keys():\n                inpt = self.__dict__[inpt_name]\n                if isinstance(inpt, tf.Tensor) and inpt._op.type == \'Placeholder\':\n                    feed_dict[inpt] = adjust_shape(inpt, data)\n\n        return sess.run(variables, feed_dict)\n\n    def _build_model_for_step(self):\n        # multiply with weight and apply mask on self.act_latent to generate\n        self.act_step = step = tf.placeholder(shape=(), dtype=tf.int64, name=\'act_step\')\n        with tf.variable_scope(\'pi\', reuse=tf.AUTO_REUSE):\n            from .util import ortho_init\n            nin = self.act_latent.get_shape()[1].value\n            w = tf.get_variable(""w"", [nin, self.nh], initializer=ortho_init(0.01))\n            b = tf.get_variable(""b"", [self.nh], initializer=tf.constant_initializer(0.0))\n            logits = tf.matmul(self.act_latent, w)+b\n            piece = tf.slice(self.mask, [step, 0], [1, self.nh])\n            re_piece = tf.reshape(piece, [-1])\n            masked_logits = tf.math.multiply(logits, re_piece)\n\n            npinf_piece = tf.slice(self.mask_npinf, [step, 0], [1, self.nh])\n            re_npinf_piece = tf.reshape(npinf_piece, [-1])\n\n        def sample(logits, mask_npinf):\n            new_logits = tf.math.add(logits, mask_npinf)\n            u = tf.random_uniform(tf.shape(new_logits), dtype=logits.dtype)\n            return tf.argmax(new_logits - tf.log(-1*tf.log(u)), axis=-1)\n\n        def neglogp(logits, x):\n            # return tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=x)\n            # Note: we can\'t use sparse_softmax_cross_entropy_with_logits because\n            #       the implementation does not allow second-order derivatives...\n            if x.dtype in {tf.uint8, tf.int32, tf.int64}:\n                # one-hot encoding\n                x_shape_list = x.shape.as_list()\n                logits_shape_list = logits.get_shape().as_list()[:-1]\n                for xs, ls in zip(x_shape_list, logits_shape_list):\n                    if xs is not None and ls is not None:\n                        assert xs == ls, \'shape mismatch: {} in x vs {} in logits\'.format(xs, ls)\n\n                x = tf.one_hot(x, logits.get_shape().as_list()[-1])\n            else:\n                # already encoded\n                assert x.shape.as_list() == logits.shape.as_list()\n\n            return tf.nn.softmax_cross_entropy_with_logits_v2(\n                logits=logits,\n                labels=x)\n\n        self.act_action = sample(masked_logits, re_npinf_piece)\n        self.act_neglogp = neglogp(masked_logits, self.act_action)\n\n\n    def step(self, step, observation, **extra_feed):\n        """"""\n        Compute next action(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        (action, value estimate, next state, negative log likelihood of the action under current policy parameters) tuple\n        """"""\n        extra_feed[\'act_step\'] = step\n        a, v, state, neglogp = self._evaluate([self.act_action, self.vf, self.state, self.act_neglogp], observation, **extra_feed)\n        if state.size == 0:\n            state = None\n        return a, v, state, neglogp\n\n    def value(self, ob, *args, **kwargs):\n        """"""\n        Compute value estimate(s) given the observation(s)\n\n        Parameters\n        ----------\n        observation : np array\n            Observation data (either single or a batch)\n        **extra_feed\n            Additional data such as state or mask (names of the arguments should match the ones in constructor, see __init__)\n\n        Returns\n        -------\n        Value estimate\n        """"""\n        return self._evaluate(self.vf, ob, *args, **kwargs)\n\n\ndef build_lstm_policy(model_config, value_network=None, estimate_q=False, **policy_kwargs):\n    """"""\n    Build lstm policy and value network, they share the same lstm network.\n    the parameters all use their default values.\n\n    Parameter\n    ---------\n    model_config : obj\n        Configurations of the model\n    value_network : obj\n        The network for value function\n    estimate_q : bool\n        Whether to estimate ``q``\n    **policy_kwargs\n        The kwargs for policy network, i.e., lstm model\n\n    Returns\n    -------\n    func\n        The policy network\n    """"""\n    policy_network = lstm_model(**policy_kwargs)\n\n    def policy_fn(nbatch=None, nsteps=None, sess=None, observ_placeholder=None, np_mask=None, is_act_model=False):\n        ob_space = model_config.observation_space\n\n        X = observ_placeholder if observ_placeholder is not None else observation_placeholder(ob_space, batch_size=nbatch)\n\n        extra_tensors = {}\n\n        # encode_observation is not necessary anymore as we use embedding_lookup\n        encoded_x = X\n\n        with tf.variable_scope(\'pi\', reuse=tf.AUTO_REUSE):\n            policy_latent = policy_network(encoded_x, 1, model_config.observation_space.n)\n            if isinstance(policy_latent, tuple):\n                policy_latent, recurrent_tensors = policy_latent\n\n                if recurrent_tensors is not None:\n                    # recurrent architecture, need a few more steps\n                    nenv = nbatch // nsteps\n                    assert nenv > 0, \'Bad input for recurrent policy: batch size {} smaller than nsteps {}\'.format(nbatch, nsteps)\n                    policy_latent, recurrent_tensors = policy_network(encoded_x, nenv, model_config.observation_space.n)\n                    extra_tensors.update(recurrent_tensors)\n\n        _v_net = value_network\n\n        assert _v_net is None or _v_net == \'shared\'\n        vf_latent = policy_latent\n\n        policy = PolicyWithValue(\n            env=model_config,\n            observations=X,\n            latent=policy_latent,\n            vf_latent=vf_latent,\n            sess=sess,\n            estimate_q=estimate_q,\n            np_mask=np_mask,\n            is_act_model=is_act_model,\n            **extra_tensors\n        )\n        return policy\n\n    return policy_fn\n'"
src/sdk/pynni/nni/ppo_tuner/ppo_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nppo_tuner.py including:\n    class PPOTuner\n""""""\n\nimport copy\nimport logging\nimport numpy as np\nfrom gym import spaces\n\nimport nni\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward\n\nfrom .model import Model\nfrom .util import set_global_seeds\nfrom .policy import build_lstm_policy\n\n\nlogger = logging.getLogger(\'ppo_tuner_AutoML\')\n\ndef _constfn(val):\n    """"""\n    Wrap as function\n    """"""\n    def f(_):\n        return val\n    return f\n\n\nclass ModelConfig:\n    """"""\n    Configurations of the PPO model\n    """"""\n    def __init__(self):\n        self.observation_space = None\n        self.action_space = None\n        self.num_envs = 0\n        self.nsteps = 0\n\n        self.ent_coef = 0.0\n        self.lr = 3e-4\n        self.vf_coef = 0.5\n        self.max_grad_norm = 0.5\n        self.gamma = 0.99\n        self.lam = 0.95\n        self.cliprange = 0.2\n        self.embedding_size = None  # the embedding is for each action\n\n        self.noptepochs = 4         # number of training epochs per update\n        self.total_timesteps = 5000 # number of timesteps (i.e. number of actions taken in the environment)\n        self.nminibatches = 4       # number of training minibatches per update. For recurrent policies,\n                                    # should be smaller or equal than number of environments run in parallel.\n\nclass TrialsInfo:\n    """"""\n    Informations of each trial from one model inference\n    """"""\n    def __init__(self, obs, actions, values, neglogpacs, dones, last_value, inf_batch_size):\n        self.iter = 0\n        self.obs = obs\n        self.actions = actions\n        self.values = values\n        self.neglogpacs = neglogpacs\n        self.dones = dones\n        self.last_value = last_value\n\n        self.rewards = None\n        self.returns = None\n\n        self.inf_batch_size = inf_batch_size\n        #self.states = None\n\n    def get_next(self):\n        """"""\n        Get actions of the next trial\n        """"""\n        if self.iter >= self.inf_batch_size:\n            return None, None\n        actions = []\n        for step in self.actions:\n            actions.append(step[self.iter])\n        self.iter += 1\n        return self.iter - 1, actions\n\n    def update_rewards(self, rewards, returns):\n        """"""\n        After the trial is finished, reward and return of this trial is updated\n        """"""\n        self.rewards = rewards\n        self.returns = returns\n\n    def convert_shape(self):\n        """"""\n        Convert shape\n        """"""\n        def sf01(arr):\n            """"""\n            swap and then flatten axes 0 and 1\n            """"""\n            s = arr.shape\n            return arr.swapaxes(0, 1).reshape(s[0] * s[1], *s[2:])\n        self.obs = sf01(self.obs)\n        self.returns = sf01(self.returns)\n        self.dones = sf01(self.dones)\n        self.actions = sf01(self.actions)\n        self.values = sf01(self.values)\n        self.neglogpacs = sf01(self.neglogpacs)\n\n\nclass PPOModel:\n    """"""\n    PPO Model\n    """"""\n    def __init__(self, model_config, mask):\n        self.model_config = model_config\n        self.states = None    # initial state of lstm in policy/value network\n        self.nupdates = None  # the number of func train is invoked, used to tune lr and cliprange\n        self.cur_update = 1   # record the current update\n        self.np_mask = mask   # record the mask of each action within one trial\n\n        set_global_seeds(None)\n        assert isinstance(self.model_config.lr, float)\n        self.lr = _constfn(self.model_config.lr)\n        assert isinstance(self.model_config.cliprange, float)\n        self.cliprange = _constfn(self.model_config.cliprange)\n\n        # build lstm policy network, value share the same network\n        policy = build_lstm_policy(model_config)\n\n        # Get the nb of env\n        nenvs = model_config.num_envs\n\n        # Calculate the batch_size\n        self.nbatch = nbatch = nenvs * model_config.nsteps # num of record per update\n        nbatch_train = nbatch // model_config.nminibatches # get batch size\n        # self.nupdates is used to tune lr and cliprange\n        self.nupdates = self.model_config.total_timesteps // self.nbatch\n\n        # Instantiate the model object (that creates act_model and train_model)\n        self.model = Model(policy=policy, nbatch_act=nenvs, nbatch_train=nbatch_train,\n                           nsteps=model_config.nsteps, ent_coef=model_config.ent_coef, vf_coef=model_config.vf_coef,\n                           max_grad_norm=model_config.max_grad_norm, np_mask=self.np_mask)\n\n        self.states = self.model.initial_state\n\n        logger.info(\'=== finished PPOModel initialization\')\n\n    def inference(self, num):\n        """"""\n        Generate actions along with related info from policy network.\n        observation is the action of the last step.\n\n        Parameters\n        ----------\n        num: int\n            The number of trials to generate\n\n        Returns\n        -------\n        mb_obs : list\n            Observation of the ``num`` configurations\n        mb_actions : list\n            Actions of the ``num`` configurations\n        mb_values : list\n            Values from the value function of the ``num`` configurations\n        mb_neglogpacs : list\n            ``neglogp`` of the ``num`` configurations\n        mb_dones : list\n            To show whether the play is done, always ``True``\n        last_values : tensorflow tensor\n            The last values of the ``num`` configurations, got with session run\n        """"""\n        # Here, we init the lists that will contain the mb of experiences\n        mb_obs, mb_actions, mb_values, mb_dones, mb_neglogpacs = [], [], [], [], []\n        # initial observation\n        # use the (n+1)th embedding to represent the first step action\n        first_step_ob = self.model_config.action_space.n\n        obs = [first_step_ob for _ in range(num)]\n        dones = [True for _ in range(num)]\n        states = self.states\n        # For n in range number of steps\n        for cur_step in range(self.model_config.nsteps):\n            # Given observations, get action value and neglopacs\n            # We already have self.obs because Runner superclass run self.obs[:] = env.reset() on init\n            actions, values, states, neglogpacs = self.model.step(cur_step, obs, S=states, M=dones)\n            mb_obs.append(obs.copy())\n            mb_actions.append(actions)\n            mb_values.append(values)\n            mb_neglogpacs.append(neglogpacs)\n            mb_dones.append(dones)\n\n            # Take actions in env and look the results\n            # Infos contains a ton of useful informations\n            obs[:] = actions\n            if cur_step == self.model_config.nsteps - 1:\n                dones = [True for _ in range(num)]\n            else:\n                dones = [False for _ in range(num)]\n\n        #batch of steps to batch of rollouts\n        np_obs = np.asarray(obs)\n        mb_obs = np.asarray(mb_obs, dtype=np_obs.dtype)\n        mb_actions = np.asarray(mb_actions)\n        mb_values = np.asarray(mb_values, dtype=np.float32)\n        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n        last_values = self.model.value(np_obs, S=states, M=dones)\n\n        return mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values\n\n    def compute_rewards(self, trials_info, trials_result):\n        """"""\n        Compute the rewards of the trials in trials_info based on trials_result,\n        and update the rewards in trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Info of the generated trials\n        trials_result : list\n            Final results (e.g., acc) of the generated trials\n        """"""\n        mb_rewards = np.asarray([trials_result for _ in trials_info.actions], dtype=np.float32)\n        # discount/bootstrap off value fn\n        mb_returns = np.zeros_like(mb_rewards)\n        mb_advs = np.zeros_like(mb_rewards)\n        lastgaelam = 0\n        last_dones = np.asarray([True for _ in trials_result], dtype=np.bool) # ugly\n        for t in reversed(range(self.model_config.nsteps)):\n            if t == self.model_config.nsteps - 1:\n                nextnonterminal = 1.0 - last_dones\n                nextvalues = trials_info.last_value\n            else:\n                nextnonterminal = 1.0 - trials_info.dones[t+1]\n                nextvalues = trials_info.values[t+1]\n            delta = mb_rewards[t] + self.model_config.gamma * nextvalues * nextnonterminal - trials_info.values[t]\n            lastgaelam = delta + self.model_config.gamma * self.model_config.lam * nextnonterminal * lastgaelam\n            mb_advs[t] = lastgaelam # pylint: disable=unsupported-assignment-operation\n        mb_returns = mb_advs + trials_info.values\n\n        trials_info.update_rewards(mb_rewards, mb_returns)\n        trials_info.convert_shape()\n\n    def train(self, trials_info, nenvs):\n        """"""\n        Train the policy/value network using trials_info\n\n        Parameters\n        ----------\n        trials_info : TrialsInfo\n            Complete info of the generated trials from the previous inference\n        nenvs : int\n            The batch size of the (previous) inference\n        """"""\n        # keep frac decay for future optimization\n        if self.cur_update <= self.nupdates:\n            frac = 1.0 - (self.cur_update - 1.0) / self.nupdates\n        else:\n            logger.warning(\'current update (self.cur_update) %d has exceeded total updates (self.nupdates) %d\',\n                           self.cur_update, self.nupdates)\n            frac = 1.0 - (self.nupdates - 1.0) / self.nupdates\n        lrnow = self.lr(frac)\n        cliprangenow = self.cliprange(frac)\n        self.cur_update += 1\n\n        states = self.states\n\n        assert states is not None # recurrent version\n        assert nenvs % self.model_config.nminibatches == 0\n        envsperbatch = nenvs // self.model_config.nminibatches\n        envinds = np.arange(nenvs)\n        flatinds = np.arange(nenvs * self.model_config.nsteps).reshape(nenvs, self.model_config.nsteps)\n        for _ in range(self.model_config.noptepochs):\n            np.random.shuffle(envinds)\n            for start in range(0, nenvs, envsperbatch):\n                end = start + envsperbatch\n                mbenvinds = envinds[start:end]\n                mbflatinds = flatinds[mbenvinds].ravel()\n                slices = (arr[mbflatinds] for arr in (trials_info.obs, trials_info.returns, trials_info.dones,\n                                                      trials_info.actions, trials_info.values, trials_info.neglogpacs))\n                mbstates = states[mbenvinds]\n                self.model.train(lrnow, cliprangenow, *slices, mbstates)\n\n\nclass PPOTuner(Tuner):\n    """"""\n    PPOTuner, the implementation inherits the main logic of the implementation\n    [ppo2 from openai](https://github.com/openai/baselines/tree/master/baselines/ppo2), and is adapted for NAS scenario.\n    It uses ``lstm`` for its policy network and value network, policy and value share the same network.\n    """"""\n\n    def __init__(self, optimize_mode, trials_per_update=20, epochs_per_update=4, minibatch_size=4,\n                 ent_coef=0.0, lr=3e-4, vf_coef=0.5, max_grad_norm=0.5, gamma=0.99, lam=0.95, cliprange=0.2):\n        """"""\n        Initialization, PPO model is not initialized here as search space is not received yet.\n\n        Parameters\n        ----------\n        optimize_mode : str\n            maximize or minimize\n        trials_per_update : int\n            Number of trials to have for each model update\n        epochs_per_update : int\n            Number of epochs to run for each model update\n        minibatch_size : int\n            Minibatch size (number of trials) for the update\n        ent_coef : float\n            Policy entropy coefficient in the optimization objective\n        lr : float\n            Learning rate of the model (lstm network), constant\n        vf_coef : float\n            Value function loss coefficient in the optimization objective\n        max_grad_norm : float\n            Gradient norm clipping coefficient\n        gamma : float\n            Discounting factor\n        lam : float\n            Advantage estimation discounting factor (lambda in the paper)\n        cliprange : float\n            Cliprange in the PPO algorithm, constant\n        """"""\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.model_config = ModelConfig()\n        self.model = None\n        self.search_space = None\n        self.running_trials = {}                  # key: parameter_id, value: actions/states/etc.\n        self.inf_batch_size = trials_per_update   # number of trials to generate in one inference\n        self.first_inf = True                     # indicate whether it is the first time to inference new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)] # results of finished trials\n\n        self.credit = 0 # record the unsatisfied trial requests\n        self.param_ids = []\n        self.finished_trials = 0\n        self.chosen_arch_template = {}\n\n        self.actions_spaces = None\n        self.actions_to_config = None\n        self.full_act_space = None\n        self.trials_info = None\n\n        self.all_trials = {} # used to dedup the same trial, key: config, value: final result\n\n        self.model_config.num_envs = self.inf_batch_size\n        self.model_config.noptepochs = epochs_per_update\n        self.model_config.nminibatches = minibatch_size\n\n        self.send_trial_callback = None\n        logger.info(\'Finished PPOTuner initialization\')\n\n    def _process_nas_space(self, search_space):\n        actions_spaces = []\n        actions_to_config = []\n        for key, val in search_space.items():\n            if val[\'_type\'] == \'layer_choice\':\n                actions_to_config.append((key, \'layer_choice\'))\n                actions_spaces.append(val[\'_value\'])\n                self.chosen_arch_template[key] = None\n            elif val[\'_type\'] == \'input_choice\':\n                candidates = val[\'_value\'][\'candidates\']\n                n_chosen = val[\'_value\'][\'n_chosen\']\n                if n_chosen not in [0, 1, [0, 1]]:\n                    raise ValueError(\'Optional_input_size can only be 0, 1, or [0, 1], but the pecified one is %s\'\n                                     % (n_chosen))\n                if isinstance(n_chosen, list):\n                    actions_to_config.append((key, \'input_choice\'))\n                    # FIXME: risk, candidates might also have None\n                    actions_spaces.append([\'None\', *candidates])\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 1:\n                    actions_to_config.append((key, \'input_choice\'))\n                    actions_spaces.append(candidates)\n                    self.chosen_arch_template[key] = None\n                elif n_chosen == 0:\n                    self.chosen_arch_template[key] = []\n            else:\n                raise ValueError(\'Unsupported search space type: %s\' % (val[\'_type\']))\n\n        # calculate observation space\n        dedup = {}\n        for step in actions_spaces:\n            for action in step:\n                dedup[action] = 1\n        full_act_space = [act for act, _ in dedup.items()]\n        assert len(full_act_space) == len(dedup)\n        observation_space = len(full_act_space)\n        nsteps = len(actions_spaces)\n\n        return actions_spaces, actions_to_config, full_act_space, observation_space, nsteps\n\n    def _generate_action_mask(self):\n        """"""\n        Different step could have different action space. to deal with this case, we merge all the\n        possible actions into one action space, and use mask to indicate available actions for each step\n        """"""\n        two_masks = []\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [0 for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 1\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        mask = []\n        for acts in self.actions_spaces:\n            one_mask = [-np.inf for _ in range(len(self.full_act_space))]\n            for act in acts:\n                idx = self.full_act_space.index(act)\n                one_mask[idx] = 0\n            mask.append(one_mask)\n        two_masks.append(mask)\n\n        return np.asarray(two_masks, dtype=np.float32)\n\n    def update_search_space(self, search_space):\n        """"""\n        Get search space, currently the space only includes that for NAS\n\n        Parameters\n        ----------\n        search_space : dict\n            Search space for NAS\n            the format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        """"""\n        logger.info(\'update search space %s\', search_space)\n        assert self.search_space is None\n        self.search_space = search_space\n\n        assert self.model_config.observation_space is None\n        assert self.model_config.action_space is None\n\n        self.actions_spaces, self.actions_to_config, self.full_act_space, obs_space, nsteps = self._process_nas_space(search_space)\n\n        self.model_config.observation_space = spaces.Discrete(obs_space)\n        self.model_config.action_space = spaces.Discrete(obs_space)\n        self.model_config.nsteps = nsteps\n\n        # generate mask in numpy\n        mask = self._generate_action_mask()\n\n        assert self.model is None\n        self.model = PPOModel(self.model_config, mask)\n\n    def _actions_to_config(self, actions):\n        """"""\n        Given actions, to generate the corresponding trial configuration\n        """"""\n        chosen_arch = copy.deepcopy(self.chosen_arch_template)\n        for cnt, act in enumerate(actions):\n            act_name = self.full_act_space[act]\n            (_key, _type) = self.actions_to_config[cnt]\n            if _type == \'input_choice\':\n                if act_name == \'None\':\n                    chosen_arch[_key] = {\'_value\': [], \'_idx\': []}\n                else:\n                    candidates = self.search_space[_key][\'_value\'][\'candidates\']\n                    idx = candidates.index(act_name)\n                    chosen_arch[_key] = {\'_value\': [act_name], \'_idx\': [idx]}\n            elif _type == \'layer_choice\':\n                idx = self.search_space[_key][\'_value\'].index(act_name)\n                chosen_arch[_key] = {\'_value\': act_name, \'_idx\': idx}\n            else:\n                raise ValueError(\'unrecognized key: {0}\'.format(_type))\n        return chosen_arch\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        """"""\n        Returns multiple sets of trial (hyper-)parameters, as iterable of serializable objects.\n\n        Parameters\n        ----------\n        parameter_id_list : list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            A list of newly generated configurations\n        """"""\n        result = []\n        self.send_trial_callback = kwargs[\'st_callback\']\n        for parameter_id in parameter_id_list:\n            had_exception = False\n            try:\n                logger.debug(""generating param for %s"", parameter_id)\n                res = self.generate_parameters(parameter_id, **kwargs)\n            except nni.NoMoreTrialError:\n                had_exception = True\n            if not had_exception:\n                result.append(res)\n        return result\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Generate parameters, if no trial configration for now, self.credit plus 1 to send the config later\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters.\n            This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n\n        """"""\n        if self.first_inf:\n            self.trials_result = [None for _ in range(self.inf_batch_size)]\n            mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n            self.trials_info = TrialsInfo(mb_obs, mb_actions, mb_values, mb_neglogpacs,\n                                          mb_dones, last_values, self.inf_batch_size)\n            self.first_inf = False\n\n        trial_info_idx, actions = self.trials_info.get_next()\n        if trial_info_idx is None:\n            logger.debug(\'Credit added by one in parameters request\')\n            self.credit += 1\n            self.param_ids.append(parameter_id)\n            raise nni.NoMoreTrialError(\'no more parameters now.\')\n\n        self.running_trials[parameter_id] = trial_info_idx\n        new_config = self._actions_to_config(actions)\n        return new_config\n\n    def _next_round_inference(self):\n        """"""\n        Run a inference to generate next batch of configurations\n        """"""\n        logger.debug(\'Start next round inference...\')\n        self.finished_trials = 0\n        self.model.compute_rewards(self.trials_info, self.trials_result)\n        self.model.train(self.trials_info, self.inf_batch_size)\n        self.running_trials = {}\n        # generate new trials\n        self.trials_result = [None for _ in range(self.inf_batch_size)]\n        mb_obs, mb_actions, mb_values, mb_neglogpacs, mb_dones, last_values = self.model.inference(self.inf_batch_size)\n        self.trials_info = TrialsInfo(mb_obs, mb_actions,\n                                      mb_values, mb_neglogpacs,\n                                      mb_dones, last_values,\n                                      self.inf_batch_size)\n        logger.debug(\'Next round inference complete.\')\n        # check credit and submit new trials\n        for _ in range(self.credit):\n            trial_info_idx, actions = self.trials_info.get_next()\n            if trial_info_idx is None:\n                logger.warning(\'No enough trial config, trials_per_update is suggested to be larger than trialConcurrency\')\n                break\n            assert self.param_ids\n            param_id = self.param_ids.pop()\n            self.running_trials[param_id] = trial_info_idx\n            new_config = self._actions_to_config(actions)\n            self.send_trial_callback(param_id, new_config)\n            self.credit -= 1\n            logger.debug(\'Send new trial (%d, %s) for reducing credit\', param_id, new_config)\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Receive trial\'s result. if the number of finished trials equals self.inf_batch_size, start the next update to\n        train the model.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n        """"""\n        trial_info_idx = self.running_trials.pop(parameter_id, None)\n        assert trial_info_idx is not None\n\n        value = extract_scalar_reward(value)\n        if self.optimize_mode == OptimizeMode.Minimize:\n            value = -value\n\n        self.trials_result[trial_info_idx] = value\n        self.finished_trials += 1\n\n        logger.debug(\'receive_trial_result, parameter_id %d, trial_info_idx %d, finished_trials %d, inf_batch_size %d\',\n                     parameter_id, trial_info_idx, self.finished_trials, self.inf_batch_size)\n        if self.finished_trials == self.inf_batch_size:\n            logger.debug(\'Start next round inference in receive_trial_result\')\n            self._next_round_inference()\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        """"""\n        To deal with trial failure. If a trial fails, it is popped out from ``self.running_trials``,\n        and the final result of this trial is assigned with the average of the finished trials.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for hyper-parameters used by this trial.\n        success : bool\n            True if the trial successfully completed; False if failed or terminated.\n        **kwargs\n            Not used\n        """"""\n        if not success:\n            if parameter_id not in self.running_trials:\n                logger.warning(\'The trial is failed, but self.running_trial does not have this trial\')\n                return\n            trial_info_idx = self.running_trials.pop(parameter_id, None)\n            assert trial_info_idx is not None\n            # use mean of finished trials as the result of this failed trial\n            values = [val for val in self.trials_result if val is not None]\n            logger.warning(\'In trial_end, values: %s\', values)\n            self.trials_result[trial_info_idx] = (sum(values) / len(values)) if values else 0\n            self.finished_trials += 1\n            if self.finished_trials == self.inf_batch_size:\n                logger.debug(\'Start next round inference in trial_end\')\n                self._next_round_inference()\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning, not supported yet.\n\n        Parameters\n        ----------\n        data : list\n            A list of dictionarys, each of which has at least two keys, ``parameter`` and ``value``\n        """"""\n        logger.warning(\'PPOTuner cannot leverage imported data.\')\n'"
src/sdk/pynni/nni/ppo_tuner/util.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nutil functions\n""""""\n\nimport os\nimport random\nimport multiprocessing\nimport numpy as np\nimport tensorflow as tf\nfrom gym.spaces import Discrete, Box, MultiDiscrete\n\ndef set_global_seeds(i):\n    """"""set global seeds""""""\n    rank = 0\n    myseed = i  + 1000 * rank if i is not None else None\n    tf.set_random_seed(myseed)\n    np.random.seed(myseed)\n    random.seed(myseed)\n\ndef batch_to_seq(h, nbatch, nsteps, flat=False):\n    """"""convert from batch to sequence""""""\n    if flat:\n        h = tf.reshape(h, [nbatch, nsteps])\n    else:\n        h = tf.reshape(h, [nbatch, nsteps, -1])\n    return [tf.squeeze(v, [1]) for v in tf.split(axis=1, num_or_size_splits=nsteps, value=h)]\n\ndef seq_to_batch(h, flat=False):\n    """"""convert from sequence to batch""""""\n    shape = h[0].get_shape().as_list()\n    if not flat:\n        assert len(shape) > 1\n        nh = h[0].get_shape()[-1].value\n        return tf.reshape(tf.concat(axis=1, values=h), [-1, nh])\n    else:\n        return tf.reshape(tf.stack(values=h, axis=1), [-1])\n\ndef lstm(xs, ms, s, scope, nh, init_scale=1.0):\n    """"""lstm cell""""""\n    _, nin = [v.value for v in xs[0].get_shape()] # the first is nbatch\n    with tf.variable_scope(scope):\n        wx = tf.get_variable(""wx"", [nin, nh*4], initializer=ortho_init(init_scale))\n        wh = tf.get_variable(""wh"", [nh, nh*4], initializer=ortho_init(init_scale))\n        b = tf.get_variable(""b"", [nh*4], initializer=tf.constant_initializer(0.0))\n\n    c, h = tf.split(axis=1, num_or_size_splits=2, value=s)\n    for idx, (x, m) in enumerate(zip(xs, ms)):\n        c = c*(1-m)\n        h = h*(1-m)\n        z = tf.matmul(x, wx) + tf.matmul(h, wh) + b\n        i, f, o, u = tf.split(axis=1, num_or_size_splits=4, value=z)\n        i = tf.nn.sigmoid(i)\n        f = tf.nn.sigmoid(f)\n        o = tf.nn.sigmoid(o)\n        u = tf.tanh(u)\n        c = f*c + i*u\n        h = o*tf.tanh(c)\n        xs[idx] = h\n    s = tf.concat(axis=1, values=[c, h])\n    return xs, s\n\ndef lstm_model(nlstm=128, layer_norm=False):\n    """"""\n    Builds LSTM (Long-Short Term Memory) network to be used in a policy.\n    Note that the resulting function returns not only the output of the LSTM\n    (i.e. hidden state of lstm for each step in the sequence), but also a dictionary\n    with auxiliary tensors to be set as policy attributes.\n\n    Specifically,\n        S is a placeholder to feed current state (LSTM state has to be managed outside policy)\n        M is a placeholder for the mask (used to mask out observations after the end of the episode, but can be used for other purposes too)\n        initial_state is a numpy array containing initial lstm state (usually zeros)\n        state is the output LSTM state (to be fed into S at the next call)\n\n\n    An example of usage of lstm-based policy can be found here: common/tests/test_doc_examples.py/test_lstm_example\n\n    Parameters\n    ----------\n    nlstm : int\n        LSTM hidden state size\n    layer_norm : bool\n        if True, layer-normalized version of LSTM is used\n\n    Returns\n    -------\n    function that builds LSTM with a given input tensor / placeholder\n    """"""\n\n    def network_fn(X, nenv=1, obs_size=-1):\n        with tf.variable_scope(""emb"", reuse=tf.AUTO_REUSE):\n            w_emb = tf.get_variable(""w_emb"", [obs_size+1, 32])\n            X = tf.nn.embedding_lookup(w_emb, X)\n\n        nbatch = X.shape[0]\n        nsteps = nbatch // nenv\n\n        h = tf.layers.flatten(X)\n\n        M = tf.placeholder(tf.float32, [nbatch]) #mask (done t-1)\n        S = tf.placeholder(tf.float32, [nenv, 2*nlstm]) #states\n\n        xs = batch_to_seq(h, nenv, nsteps)\n        ms = batch_to_seq(M, nenv, nsteps)\n\n        assert not layer_norm\n        h5, snew = lstm(xs, ms, S, scope=\'lstm\', nh=nlstm)\n\n        h = seq_to_batch(h5)\n        initial_state = np.zeros(S.shape.as_list(), dtype=float)\n\n        return h, {\'S\':S, \'M\':M, \'state\':snew, \'initial_state\':initial_state}\n\n    return network_fn\n\ndef ortho_init(scale=1.0):\n    """"""init approach""""""\n    def _ortho_init(shape, dtype, partition_info=None):\n        #lasagne ortho init for tf\n        shape = tuple(shape)\n        if len(shape) == 2:\n            flat_shape = shape\n        elif len(shape) == 4: # assumes NHWC\n            flat_shape = (np.prod(shape[:-1]), shape[-1])\n        else:\n            raise NotImplementedError\n        a = np.random.normal(0.0, 1.0, flat_shape)\n        u, _, v = np.linalg.svd(a, full_matrices=False)\n        q = u if u.shape == flat_shape else v # pick the one with the correct shape\n        q = q.reshape(shape)\n        return (scale * q[:shape[0], :shape[1]]).astype(np.float32)\n    return _ortho_init\n\ndef fc(x, scope, nh, *, init_scale=1.0, init_bias=0.0):\n    """"""fully connected op""""""\n    with tf.variable_scope(scope):\n        nin = x.get_shape()[1].value\n        w = tf.get_variable(""w"", [nin, nh], initializer=ortho_init(init_scale))\n        b = tf.get_variable(""b"", [nh], initializer=tf.constant_initializer(init_bias))\n        return tf.matmul(x, w)+b\n\ndef _check_shape(placeholder_shape, data_shape):\n    """"""\n    check if two shapes are compatible (i.e. differ only by dimensions of size 1, or by the batch dimension)\n    """"""\n\n    return True\n\n# ================================================================\n# Shape adjustment for feeding into tf placeholders\n# ================================================================\ndef adjust_shape(placeholder, data):\n    """"""\n    adjust shape of the data to the shape of the placeholder if possible.\n    If shape is incompatible, AssertionError is thrown\n\n    Parameters\n    ----------\n    placeholder\n        tensorflow input placeholder\n    data\n        input data to be (potentially) reshaped to be fed into placeholder\n\n    Returns\n    -------\n    reshaped data\n    """"""\n    if not isinstance(data, np.ndarray) and not isinstance(data, list):\n        return data\n    if isinstance(data, list):\n        data = np.array(data)\n\n    placeholder_shape = [x or -1 for x in placeholder.shape.as_list()]\n\n    assert _check_shape(placeholder_shape, data.shape), \\\n        \'Shape of data {} is not compatible with shape of the placeholder {}\'.format(data.shape, placeholder_shape)\n\n    return np.reshape(data, placeholder_shape)\n\n# ================================================================\n# Global session\n# ================================================================\n\ndef get_session(config=None):\n    """"""Get default session or create one with a given config""""""\n    sess = tf.get_default_session()\n    if sess is None:\n        sess = make_session(config=config, make_default=True)\n    return sess\n\ndef make_session(config=None, num_cpu=None, make_default=False, graph=None):\n    """"""Returns a session that will use <num_cpu> CPU\'s only""""""\n    if num_cpu is None:\n        num_cpu = int(os.getenv(\'RCALL_NUM_CPU\', multiprocessing.cpu_count()))\n    if config is None:\n        config = tf.ConfigProto(\n            allow_soft_placement=True,\n            inter_op_parallelism_threads=num_cpu,\n            intra_op_parallelism_threads=num_cpu)\n        config.gpu_options.allow_growth = True\n\n    if make_default:\n        return tf.InteractiveSession(config=config, graph=graph)\n    else:\n        return tf.Session(config=config, graph=graph)\n\nALREADY_INITIALIZED = set()\n\ndef initialize():\n    """"""Initialize all the uninitialized variables in the global scope.""""""\n    new_variables = set(tf.global_variables()) - ALREADY_INITIALIZED\n    get_session().run(tf.variables_initializer(new_variables))\n\n    ALREADY_INITIALIZED.update(new_variables)\n\ndef observation_placeholder(ob_space, batch_size=None, name=\'Ob\'):\n    """"""\n    Create placeholder to feed observations into of the size appropriate to the observation space\n\n    Parameters\n    ----------\n    ob_space : gym.Space\n        observation space\n    batch_size : int\n        size of the batch to be fed into input. Can be left None in most cases.\n    name : str\n        name of the placeholder\n\n    Returns\n    -------\n    tensorflow placeholder tensor\n    """"""\n\n    assert isinstance(ob_space, (Discrete, Box, MultiDiscrete)), \\\n        \'Can only deal with Discrete and Box observation spaces for now\'\n\n    dtype = ob_space.dtype\n    if dtype == np.int8:\n        dtype = np.uint8\n\n    return tf.placeholder(shape=(batch_size,) + ob_space.shape, dtype=dtype, name=name)\n\ndef explained_variance(ypred, y):\n    """"""\n    Computes fraction of variance that ypred explains about y.\n    Returns 1 - Var[y-ypred] / Var[y]\n\n    interpretation:\n        ev=0  =>  might as well have predicted zero\n        ev=1  =>  perfect prediction\n        ev<0  =>  worse than just predicting zero\n\n    """"""\n    assert y.ndim == 1 and ypred.ndim == 1\n    vary = np.var(y)\n    return np.nan if vary == 0 else 1 - np.var(y-ypred)/vary\n'"
src/sdk/pynni/nni/smac_tuner/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .smac_tuner import SMACTuner\n'
src/sdk/pynni/nni/smac_tuner/convert_ss_to_scenario.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\n\nimport numpy as np\n\n\ndef get_json_content(file_path):\n    """"""\n    Load json file content\n\n    Parameters\n    ----------\n    file_path:\n        path to the file\n\n    Raises\n    ------\n    TypeError\n        Error with the file path\n    """"""\n    try:\n        with open(file_path, \'r\') as file:\n            return json.load(file)\n    except TypeError as err:\n        print(\'Error: \', err)\n        return None\n\n\ndef generate_pcs(nni_search_space_content):\n    """"""\n    Generate the Parameter Configuration Space (PCS) which defines the\n    legal ranges of the parameters to be optimized and their default values.\n    Generally, the format is:\n    # parameter_name categorical {value_1, ..., value_N} [default value]\n    # parameter_name ordinal {value_1, ..., value_N} [default value]\n    # parameter_name integer [min_value, max_value] [default value]\n    # parameter_name integer [min_value, max_value] [default value] log\n    # parameter_name real [min_value, max_value] [default value]\n    # parameter_name real [min_value, max_value] [default value] log\n    Reference: https://automl.github.io/SMAC3/stable/options.html\n\n    Parameters\n    ----------\n    nni_search_space_content: search_space\n        The search space in this experiment in nni\n\n    Returns\n    -------\n    Parameter Configuration Space (PCS)\n        the legal ranges of the parameters to be optimized and their default values\n\n    Raises\n    ------\n    RuntimeError\n        unsupported type or value error or incorrect search space\n    """"""\n    categorical_dict = {}\n    search_space = nni_search_space_content\n\n    def dump_categorical(fd, key, categories):\n        choice_len = len(categories)\n        if key in categorical_dict:\n            raise RuntimeError(\n                \'%s has already existed, please make sure search space has no duplicate key.\' % key)\n        categorical_dict[key] = search_space[key][\'_value\']\n        fd.write(\'%s categorical {%s} [0]\\n\' % (key, \',\'.join(map(str, range(choice_len)))))\n\n    with open(\'param_config_space.pcs\', \'w\') as pcs_fd:\n        if isinstance(search_space, dict):\n            for key in search_space.keys():\n                if isinstance(search_space[key], dict):\n                    try:\n                        if search_space[key][\'_type\'] == \'choice\':\n                            dump_categorical(pcs_fd, key, search_space[key][\'_value\'])\n                        elif search_space[key][\'_type\'] == \'randint\':\n                            lower, upper = search_space[key][\'_value\']\n                            if lower + 1 == upper:\n                                dump_categorical(pcs_fd, key, [lower])\n                            else:\n                                pcs_fd.write(\'%s integer [%d, %d] [%d]\\n\' % (key, lower, upper - 1, lower))\n                        elif search_space[key][\'_type\'] == \'uniform\':\n                            low, high = search_space[key][\'_value\']\n                            if low == high:\n                                dump_categorical(pcs_fd, key, [low])\n                            else:\n                                pcs_fd.write(\'%s real [%s, %s] [%s]\\n\' % (key, low, high, low))\n                        elif search_space[key][\'_type\'] == \'loguniform\':\n                            # use np.round here to ensure that the rounded default value is in the range,\n                            # which will be rounded in configure_space package\n                            low, high = list(np.round(np.log(search_space[key][\'_value\']), 10))\n                            if low == high:\n                                dump_categorical(pcs_fd, key, [search_space[key][\'_value\'][0]])\n                            else:\n                                pcs_fd.write(\'%s real [%s, %s] [%s]\\n\' % (key, low, high, low))\n                        elif search_space[key][\'_type\'] == \'quniform\':\n                            low, high, q = search_space[key][\'_value\'][0:3]\n                            vals = np.clip(np.arange(np.round(low / q), np.round(high / q) + 1) * q, low, high).tolist()\n                            pcs_fd.write(\'%s ordinal {%s} [%s]\\n\' % (\n                                key,\n                                json.dumps(vals)[1:-1],\n                                json.dumps(vals[0])))\n                        else:\n                            raise RuntimeError(\'unsupported _type %s\' % search_space[key][\'_type\'])\n                    except:\n                        raise RuntimeError(\'_type or _value error.\')\n        else:\n            raise RuntimeError(\'incorrect search space.\')\n        return categorical_dict\n    return None\n\n\ndef generate_scenario(ss_content):\n    """"""\n    Generate the scenario. The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and\n    can be constructed either by providing an actual scenario-object, or by specifing the options in a scenario file.\n    Reference: https://automl.github.io/SMAC3/stable/options.html\n    The format of the scenario file is one option per line:\n    OPTION1 = VALUE1\n    OPTION2 = VALUE2\n    ...\n    Parameters\n    ----------\n    abort_on_first_run_crash: bool\n        If true, SMAC will abort if the first run of the target algorithm crashes. Default: True,\n        because trials reported to nni tuner would always in success state\n    algo: function\n        Specifies the target algorithm call that SMAC will optimize. Interpreted as a bash-command.\n        Not required by tuner, but required by nni\'s training service for running trials\n    always_race_default:\n        Race new incumbents always against default configuration\n    cost_for_crash:\n        Defines the cost-value for crashed runs on scenarios with quality as run-obj. Default: 2147483647.0.\n        Trials reported to nni tuner would always in success state\n    cutoff_time:\n        Maximum runtime, after which the target algorithm is cancelled. `Required if *run_obj* is runtime`\n    deterministic: bool\n        If true, the optimization process will be repeatable.\n    execdir:\n        Specifies the path to the execution-directory. Default: .\n        Trials are executed by nni\'s training service\n    feature_file:\n        Specifies the file with the instance-features.\n        No features specified or feature file is not supported\n    initial_incumbent:\n        DEFAULT is the default from the PCS. Default: DEFAULT. Must be from: [\xe2\x80\x98DEFAULT\xe2\x80\x99, \xe2\x80\x98RANDOM\xe2\x80\x99].\n    input_psmac_dirs:\n        For parallel SMAC, multiple output-directories are used.\n        Parallelism is supported by nni\n    instance_file:\n        Specifies the file with the training-instances. Not supported\n    intensification_percentage:\n        The fraction of time to be used on intensification (versus choice of next Configurations). Default: 0.5.\n        Not supported, trials are controlled by nni\'s training service and kill be assessor\n    maxR: int\n        Maximum number of calls per configuration. Default: 2000.\n    memory_limit:\n        Maximum available memory the target algorithm can occupy before being cancelled.\n    minR: int\n        Minimum number of calls per configuration. Default: 1.\n    output_dir:\n        Specifies the output-directory for all emerging files, such as logging and results.\n        Default: smac3-output_2018-01-22_15:05:56_807070.\n    overall_obj:\n    \tPARX, where X is an integer defining the penalty imposed on timeouts (i.e. runtimes that exceed the cutoff-time).\n        Timeout is not supported\n    paramfile:\n        Specifies the path to the PCS-file.\n    run_obj:\n        Defines what metric to optimize. When optimizing runtime, cutoff_time is required as well.\n        Must be from: [\xe2\x80\x98runtime\xe2\x80\x99, \xe2\x80\x98quality\xe2\x80\x99].\n    runcount_limit: int\n        Maximum number of algorithm-calls during optimization. Default: inf.\n        Use default because this is controlled by nni\n    shared_model:\n        Whether to run SMAC in parallel mode. Parallelism is supported by nni\n    test_instance_file:\n        Specifies the file with the test-instances. Instance is not supported\n    tuner-timeout:\n        Maximum amount of CPU-time used for optimization. Not supported\n    wallclock_limit: int\n        Maximum amount of wallclock-time used for optimization. Default: inf.\n        Use default because this is controlled by nni\n\n    Returns\n    -------\n    Scenario:\n        The scenario-object (smac.scenario.scenario.Scenario) is used to configure SMAC and can be constructed\n        either by providing an actual scenario-object, or by specifing the options in a scenario file\n    """"""\n    with open(\'scenario.txt\', \'w\') as sce_fd:\n        sce_fd.write(\'deterministic = 0\\n\')\n        # sce_fd.write(\'output_dir = \\n\')\n        sce_fd.write(\'paramfile = param_config_space.pcs\\n\')\n        sce_fd.write(\'run_obj = quality\\n\')\n\n    return generate_pcs(ss_content)\n\n\nif __name__ == \'__main__\':\n    generate_scenario(\'search_space.json\')\n'"
src/sdk/pynni/nni/smac_tuner/smac_tuner.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nsmac_tuner.py\n""""""\n\nimport logging\nimport sys\n\nimport numpy as np\n\nfrom smac.facade.epils_facade import EPILS\nfrom smac.facade.roar_facade import ROAR\nfrom smac.facade.smac_facade import SMAC\nfrom smac.scenario.scenario import Scenario\nfrom smac.utils.io.cmd_reader import CMDReader\n\nfrom ConfigSpaceNNI import Configuration\n\nimport nni\nfrom nni.tuner import Tuner\nfrom nni.utils import OptimizeMode, extract_scalar_reward\n\nfrom .convert_ss_to_scenario import generate_scenario\n\nlogger = logging.getLogger(\'smac_AutoML\')\n\nclass SMACTuner(Tuner):\n    """"""\n    This is a wrapper of [SMAC](https://github.com/automl/SMAC3) following NNI tuner interface.\n    It only supports ``SMAC`` mode, and does not support the multiple instances of SMAC3 (i.e.,\n    the same configuration is run multiple times).\n    """"""\n    def __init__(self, optimize_mode=""maximize"", config_dedup=False):\n        """"""\n        Parameters\n        ----------\n        optimize_mode : str\n            Optimize mode, \'maximize\' or \'minimize\', by default \'maximize\'\n        config_dedup : bool\n            If True, the tuner will not generate a configuration that has been already generated.\n            If False, a configuration may be generated twice, but it is rare for relatively large search space.\n        """"""\n        self.logger = logger\n        self.optimize_mode = OptimizeMode(optimize_mode)\n        self.total_data = {}\n        self.optimizer = None\n        self.smbo_solver = None\n        self.first_one = True\n        self.update_ss_done = False\n        self.loguniform_key = set()\n        self.categorical_dict = {}\n        self.cs = None\n        self.dedup = config_dedup\n\n    def _main_cli(self):\n        """"""\n        Main function of SMAC for CLI interface. Some initializations of the wrapped SMAC are done\n        in this function.\n\n        Returns\n        -------\n        obj\n            The object of the SMAC optimizer\n        """"""\n        self.logger.info(""SMAC call: %s"", "" "".join(sys.argv))\n\n        cmd_reader = CMDReader()\n        args, _ = cmd_reader.read_cmd()\n\n        root_logger = logging.getLogger()\n        root_logger.setLevel(args.verbose_level)\n        logger_handler = logging.StreamHandler(stream=sys.stdout)\n        if root_logger.level >= logging.INFO:\n            formatter = logging.Formatter(""%(levelname)s:\\t%(message)s"")\n        else:\n            formatter = logging.Formatter(\n                ""%(asctime)s:%(levelname)s:%(name)s:%(message)s"",\n                ""%Y-%m-%d %H:%M:%S"")\n        logger_handler.setFormatter(formatter)\n        root_logger.addHandler(logger_handler)\n        # remove default handler\n        root_logger.removeHandler(root_logger.handlers[0])\n\n        # Create defaults\n        rh = None\n        initial_configs = None\n        stats = None\n        incumbent = None\n\n        # Create scenario-object\n        scen = Scenario(args.scenario_file, [])\n        self.cs = scen.cs\n\n        if args.mode == ""SMAC"":\n            optimizer = SMAC(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                stats=stats,\n                restore_incumbent=incumbent,\n                run_id=args.seed)\n        elif args.mode == ""ROAR"":\n            optimizer = ROAR(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        elif args.mode == ""EPILS"":\n            optimizer = EPILS(\n                scenario=scen,\n                rng=np.random.RandomState(args.seed),\n                runhistory=rh,\n                initial_configurations=initial_configs,\n                run_id=args.seed)\n        else:\n            optimizer = None\n\n        return optimizer\n\n    def update_search_space(self, search_space):\n        """"""\n        Convert search_space to the format that ``SMAC3`` could recognize, thus, not all the search space types\n        are supported. In this function, we also do the initialization of `SMAC3`, i.e., calling ``self._main_cli``.\n\n        NOTE: updating search space during experiment running is not supported.\n\n        Parameters\n        ----------\n        search_space : dict\n            The format could be referred to search space spec (https://nni.readthedocs.io/en/latest/Tutorial/SearchSpaceSpec.html).\n        """"""\n        self.logger.info(\'update search space in SMAC.\')\n        if not self.update_ss_done:\n            self.categorical_dict = generate_scenario(search_space)\n            if self.categorical_dict is None:\n                raise RuntimeError(\'categorical dict is not correctly returned after parsing search space.\')\n            # TODO: this is ugly, we put all the initialization work in this method, because initialization relies\n            #         on search space, also because update_search_space is called at the beginning.\n            self.optimizer = self._main_cli()\n            self.smbo_solver = self.optimizer.solver\n            self.loguniform_key = {key for key in search_space.keys() if search_space[key][\'_type\'] == \'loguniform\'}\n            self.update_ss_done = True\n        else:\n            self.logger.warning(\'update search space is not supported.\')\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Receive a trial\'s final performance result reported through :func:``nni.report_final_result`` by the trial.\n        GridSearchTuner does not need trial\'s results.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier of used hyper-parameters, same with :meth:`generate_parameters`.\n        parameters : dict\n            Hyper-parameters generated by :meth:`generate_parameters`.\n        value : dict\n            Result from trial (the return value of :func:`nni.report_final_result`).\n\n        Raises\n        ------\n        RuntimeError\n            Received parameter id not in ``self.total_data``\n        """"""\n        reward = extract_scalar_reward(value)\n        if self.optimize_mode is OptimizeMode.Maximize:\n            reward = -reward\n\n        if parameter_id not in self.total_data:\n            raise RuntimeError(\'Received parameter_id not in total_data.\')\n        if self.first_one:\n            self.smbo_solver.nni_smac_receive_first_run(self.total_data[parameter_id], reward)\n            self.first_one = False\n        else:\n            self.smbo_solver.nni_smac_receive_runs(self.total_data[parameter_id], reward)\n\n    def param_postprocess(self, challenger_dict):\n        """"""\n        Postprocessing for a set of hyperparameters includes:\n            1. Convert the values of type ``loguniform`` back to their initial range.\n            2. Convert ``categorical``: categorical values in search space are changed to list of numbers before,\n               those original values will be changed back in this function.\n\n        Parameters\n        ----------\n        challenger_dict : dict\n            challenger dict\n\n        Returns\n        -------\n        dict\n            dict which stores copy of challengers\n        """"""\n        converted_dict = {}\n        for key, value in challenger_dict.items():\n            # convert to loguniform\n            if key in self.loguniform_key:\n                converted_dict[key] = np.exp(challenger_dict[key])\n            # convert categorical back to original value\n            elif key in self.categorical_dict:\n                idx = challenger_dict[key]\n                converted_dict[key] = self.categorical_dict[key][idx]\n            else:\n                converted_dict[key] = value\n        return converted_dict\n\n    def generate_parameters(self, parameter_id, **kwargs):\n        """"""\n        Generate one instance of hyperparameters (i.e., one configuration).\n        Get one from SMAC3\'s ``challengers``.\n\n        Parameters\n        ----------\n        parameter_id : int\n            Unique identifier for requested hyper-parameters. This will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        dict\n            One newly generated configuration\n        """"""\n        if self.first_one:\n            init_challenger = self.smbo_solver.nni_smac_start()\n            self.total_data[parameter_id] = init_challenger\n            return self.param_postprocess(init_challenger.get_dictionary())\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            challengers_empty = True\n            for challenger in challengers:\n                challengers_empty = False\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id] = challenger\n                return self.param_postprocess(challenger.get_dictionary())\n            assert challengers_empty is False, \'The case that challengers is empty is not handled.\'\n            self.logger.info(\'In generate_parameters: No more new parameters.\')\n            raise nni.NoMoreTrialError(\'No more new parameters.\')\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        """"""\n        Generate mutiple instances of hyperparameters. If it is a first request,\n        retrieve the instances from initial challengers. While if it is not, request\n        new challengers and retrieve instances from the requested challengers.\n\n        Parameters\n        ----------\n        parameter_id_list: list of int\n            Unique identifiers for each set of requested hyper-parameters.\n            These will later be used in :meth:`receive_trial_result`.\n        **kwargs\n            Not used\n\n        Returns\n        -------\n        list\n            a list of newly generated configurations\n        """"""\n        if self.first_one:\n            params = []\n            for one_id in parameter_id_list:\n                init_challenger = self.smbo_solver.nni_smac_start()\n                self.total_data[one_id] = init_challenger\n                params.append(self.param_postprocess(init_challenger.get_dictionary()))\n        else:\n            challengers = self.smbo_solver.nni_smac_request_challengers()\n            cnt = 0\n            params = []\n            for challenger in challengers:\n                if cnt >= len(parameter_id_list):\n                    break\n                if self.dedup:\n                    match = [v for k, v in self.total_data.items() \\\n                             if v.get_dictionary() == challenger.get_dictionary()]\n                    if match:\n                        continue\n                self.total_data[parameter_id_list[cnt]] = challenger\n                params.append(self.param_postprocess(challenger.get_dictionary()))\n                cnt += 1\n            if self.dedup and not params:\n                self.logger.info(\'In generate_multiple_parameters: No more new parameters.\')\n        return params\n\n    def import_data(self, data):\n        """"""\n        Import additional data for tuning.\n\n        Parameters\n        ----------\n        data : list of dict\n            Each of which has at least two keys, ``parameter`` and ``value``.\n        """"""\n        _completed_num = 0\n        for trial_info in data:\n            self.logger.info(""Importing data, current processing progress %s / %s"", _completed_num, len(data))\n            # simply validate data format\n            assert ""parameter"" in trial_info\n            _params = trial_info[""parameter""]\n            assert ""value"" in trial_info\n            _value = trial_info[\'value\']\n            if not _value:\n                self.logger.info(""Useless trial data, value is %s, skip this trial data."", _value)\n                continue\n            _value = extract_scalar_reward(_value)\n            # convert the keys in loguniform and categorical types\n            valid_entry = True\n            for key, value in _params.items():\n                if key in self.loguniform_key:\n                    _params[key] = np.log(value)\n                elif key in self.categorical_dict:\n                    if value in self.categorical_dict[key]:\n                        _params[key] = self.categorical_dict[key].index(value)\n                    else:\n                        self.logger.info(""The value %s of key %s is not in search space."", str(value), key)\n                        valid_entry = False\n                        break\n            if not valid_entry:\n                continue\n            # start import this data entry\n            _completed_num += 1\n            config = Configuration(self.cs, values=_params)\n            if self.optimize_mode is OptimizeMode.Maximize:\n                _value = -_value\n            if self.first_one:\n                self.smbo_solver.nni_smac_receive_first_run(config, _value)\n                self.first_one = False\n            else:\n                self.smbo_solver.nni_smac_receive_runs(config, _value)\n        self.logger.info(""Successfully import data to smac tuner, total data: %d, imported data: %d."", len(data), _completed_num)\n'"
tools/nni_annotation/testcase/annotated/dir/simple.py,0,"b'import nni\n\n\ndef max_pool(k):\n    pass\n\n\nh_conv1 = 1\nconv_size = nni.choice({2: 2, 3: 3, 5: 5, 7: 7}, name=\'conv_size\')\nabc = nni.choice({\'2\': \'2\', 3: 3, \'(5 * 6)\': 5 * 6, ""{(1): 2, \'3\': 4}"": {(1\n    ): 2, \'3\': 4}, \'[1, 2, 3]\': [1, 2, 3]}, name=\'abc\')\nh_pool1 = nni.function_choice({\'max_pool(h_conv1)\': lambda : max_pool(\n    h_conv1), \'avg_pool(h_conv2, h_conv3)\': lambda : avg_pool(h_conv2,\n    h_conv3)}, name=\'max_pool\')\nh_pool2 = nni.function_choice({\'max_poo(h_conv1)\': lambda : max_poo(h_conv1\n    ), \'(2 * 3 + 4)\': lambda : 2 * 3 + 4, \'(lambda x: 1 + x)\': lambda : lambda\n    x: 1 + x}, name=\'max_poo\')\ntest_acc = 1\nnni.report_intermediate_result(test_acc)\ntest_acc = 2\nnni.report_final_result(test_acc)\n'"
tools/nni_annotation/testcase/annotated/non_annotation/bar.py,0,"b'import nni\n\ndef bar():\n    """"""I\'m doc string""""""\n    return nni.report_final_result(0)\n'"
tools/nni_annotation/testcase/annotated/non_annotation/foo.py,0,"b""print('hello')\n"""
tools/nni_annotation/testcase/usercode/dir/simple.py,0,"b'def max_pool(k):\n    pass\nh_conv1=1\n""""""@nni.variable(nni.choice(2,3,5,7),name=conv_size)""""""\nconv_size = 5\n""""""@nni.variable(nni.choice(\'2\',3,5*6,{1:2, \'3\':4},[1,2,3]),name=abc)""""""\nabc = 5\n""""""@nni.function_choice(max_pool(h_conv1), avg_pool(h_conv2,h_conv3), name=max_pool)""""""\nh_pool1 = max_pool(h_conv1)\n""""""@nni.function_choice(max_poo(h_conv1), 2 * 3 + 4, lambda x: 1+x, name=max_poo)""""""\nh_pool2 = max_poo(h_conv1)\ntest_acc=1\n\'\'\'@nni.report_intermediate_result(test_acc)\'\'\'\ntest_acc=2\n\'\'\'@nni.report_final_result(test_acc)\'\'\'\n'"
tools/nni_annotation/testcase/usercode/non_annotation/bar.py,0,"b'import nni\n\ndef bar():\n    """"""I\'m doc string""""""\n    return nni.report_final_result(0)\n'"
tools/nni_annotation/testcase/usercode/non_annotation/foo.py,0,"b""print('hello')\n"""
src/sdk/pynni/nni/compression/tensorflow/__init__.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .compressor import LayerInfo, Compressor, Pruner, Quantizer\nfrom .builtin_pruners import *\nfrom .builtin_quantizers import *\n'"
src/sdk/pynni/nni/compression/tensorflow/builtin_pruners.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport numpy as np\nimport tensorflow as tf\nfrom .compressor import Pruner\n\n__all__ = [\'LevelPruner\', \'AGP_Pruner\', \'FPGMPruner\']\n\n_logger = logging.getLogger(__name__)\n\n\nclass LevelPruner(Pruner):\n    def __init__(self, model, config_list):\n        """"""\n        config_list: supported keys:\n            - sparsity\n        """"""\n        super().__init__(model, config_list)\n        self.mask_list = {}\n        self.if_init_list = {}\n\n    def calc_mask(self, layer, config):\n        weight = layer.weight\n        op_name = layer.name\n        if self.if_init_list.get(op_name, True):\n            threshold = tf.contrib.distributions.percentile(tf.abs(weight), config[\'sparsity\'] * 100)\n            mask = tf.cast(tf.math.greater(tf.abs(weight), threshold), weight.dtype)\n            self.mask_list.update({op_name: mask})\n            self.if_init_list.update({op_name: False})\n        else:\n            mask = self.mask_list[op_name]\n        return mask\n\n\nclass AGP_Pruner(Pruner):\n    """"""An automated gradual pruning algorithm that prunes the smallest magnitude\n    weights to achieve a preset level of network sparsity.\n    Michael Zhu and Suyog Gupta, ""To prune, or not to prune: exploring the\n    efficacy of pruning for model compression"", 2017 NIPS Workshop on Machine\n    Learning of Phones and other Consumer Devices,\n    https://arxiv.org/pdf/1710.01878.pdf\n    """"""\n\n    def __init__(self, model, config_list):\n        """"""\n        config_list: supported keys:\n            - initial_sparsity\n            - final_sparsity: you should make sure initial_sparsity <= final_sparsity\n            - start_epoch: start epoch numer begin update mask\n            - end_epoch: end epoch number stop update mask\n            - frequency: if you want update every 2 epoch, you can set it 2\n        """"""\n        super().__init__(model, config_list)\n        self.mask_list = {}\n        self.if_init_list = {}\n        self.now_epoch = tf.Variable(0)\n        self.assign_handler = []\n\n    def calc_mask(self, layer, config):\n        weight = layer.weight\n        op_name = layer.name\n        start_epoch = config.get(\'start_epoch\', 0)\n        freq = config.get(\'frequency\', 1)\n        if self.now_epoch >= start_epoch and self.if_init_list.get(op_name, True) and (\n                self.now_epoch - start_epoch) % freq == 0:\n            target_sparsity = self.compute_target_sparsity(config)\n            threshold = tf.contrib.distributions.percentile(weight, target_sparsity * 100)\n            # stop gradient in case gradient change the mask\n            mask = tf.stop_gradient(tf.cast(tf.math.greater(weight, threshold), weight.dtype))\n            self.assign_handler.append(tf.assign(weight, weight * mask))\n            self.mask_list.update({op_name: tf.constant(mask)})\n            self.if_init_list.update({op_name: False})\n        else:\n            mask = self.mask_list[op_name]\n        return mask\n\n    def compute_target_sparsity(self, config):\n        end_epoch = config.get(\'end_epoch\', 1)\n        start_epoch = config.get(\'start_epoch\', 0)\n        freq = config.get(\'frequency\', 1)\n        final_sparsity = config.get(\'final_sparsity\', 0)\n        initial_sparsity = config.get(\'initial_sparsity\', 0)\n\n        if end_epoch <= start_epoch or initial_sparsity >= final_sparsity:\n            _logger.warning(\'your end epoch <= start epoch or initial_sparsity >= final_sparsity\')\n            return final_sparsity\n\n        now_epoch = tf.minimum(self.now_epoch, tf.constant(end_epoch))\n        span = int(((end_epoch - start_epoch - 1) // freq) * freq)\n        assert span > 0\n        base = tf.cast(now_epoch - start_epoch, tf.float32) / span\n        target_sparsity = (final_sparsity +\n                           (initial_sparsity - final_sparsity) *\n                           (tf.pow(1.0 - base, 3)))\n        return target_sparsity\n\n    def update_epoch(self, epoch, sess):\n        sess.run(self.assign_handler)\n        sess.run(tf.assign(self.now_epoch, int(epoch)))\n        for k in self.if_init_list:\n            self.if_init_list[k] = True\n\nclass FPGMPruner(Pruner):\n    """"""\n    A filter pruner via geometric median.\n    ""Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration"",\n    https://arxiv.org/pdf/1811.00250.pdf\n    """"""\n\n    def __init__(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            the model user wants to compress\n        config_list: list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        """"""\n        super().__init__(model, config_list)\n        self.mask_dict = {}\n        self.assign_handler = []\n        self.epoch_pruned_layers = set()\n\n    def calc_mask(self, layer, config):\n        """"""\n        Supports Conv1D, Conv2D\n        filter dimensions for Conv1D:\n        LEN: filter length\n        IN: number of input channel\n        OUT: number of output channel\n\n        filter dimensions for Conv2D:\n        H: filter height\n        W: filter width\n        IN: number of input channel\n        OUT: number of output channel\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            calculate mask for `layer`\'s weight\n        config : dict\n            the configuration for generating the mask\n        """"""\n\n        weight = layer.weight\n        op_type = layer.type\n        op_name = layer.name\n        assert 0 <= config.get(\'sparsity\') < 1\n        assert op_type in [\'Conv1D\', \'Conv2D\']\n        assert op_type in config[\'op_types\']\n\n        if layer.name in self.epoch_pruned_layers:\n            assert layer.name in self.mask_dict\n            return self.mask_dict.get(layer.name)\n\n        try:\n            w = tf.stop_gradient(tf.transpose(tf.reshape(weight, (-1, weight.shape[-1])), [1, 0]))\n            masks = np.ones(w.shape)\n            num_filters = w.shape[0]\n            num_prune = int(num_filters * config.get(\'sparsity\'))\n            if num_filters < 2 or num_prune < 1:\n                return masks\n            min_gm_idx = self._get_min_gm_kernel_idx(w, num_prune)\n\n            for idx in min_gm_idx:\n                masks[idx] = 0.\n        finally:\n            masks = tf.reshape(tf.transpose(masks, [1, 0]), weight.shape)\n            masks = tf.Variable(masks)\n            self.mask_dict.update({op_name: masks})\n            self.epoch_pruned_layers.add(layer.name)\n\n        return masks\n\n    def _get_min_gm_kernel_idx(self, weight, n):\n        dist_list = []\n        for out_i in range(weight.shape[0]):\n            dist_sum = self._get_distance_sum(weight, out_i)\n            dist_list.append((dist_sum, out_i))\n        min_gm_kernels = sorted(dist_list, key=lambda x: x[0])[:n]\n        return [x[1] for x in min_gm_kernels]\n\n    def _get_distance_sum(self, weight, out_idx):\n        anchor_w = tf.tile(tf.expand_dims(weight[out_idx], 0), [weight.shape[0], 1])\n        x = weight - anchor_w\n        x = tf.math.reduce_sum((x*x), -1)\n        x = tf.math.sqrt(x)\n        return tf.math.reduce_sum(x)\n\n    def update_epoch(self, epoch):\n        self.epoch_pruned_layers = set()\n'"
src/sdk/pynni/nni/compression/tensorflow/builtin_quantizers.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport tensorflow as tf\nfrom .compressor import Quantizer\n\n__all__ = [\'NaiveQuantizer\', \'QAT_Quantizer\', \'DoReFaQuantizer\']\n\n_logger = logging.getLogger(__name__)\n\n\nclass NaiveQuantizer(Quantizer):\n    """"""quantize weight to 8 bits\n    """"""\n    def __init__(self, model, config_list):\n        super().__init__(model, config_list)\n        self.layer_scale = {}\n\n    def quantize_weight(self, weight, config, op_name, **kwargs):\n        new_scale = tf.reduce_max(tf.abs(weight)) / 127\n        scale = tf.maximum(self.layer_scale.get(op_name, tf.constant(0.0)), new_scale)\n        self.layer_scale[op_name] = scale\n        orig_type = weight.dtype\n        return tf.cast(tf.cast(weight / scale, tf.int8), orig_type) * scale\n\n\nclass QAT_Quantizer(Quantizer):\n    """"""Quantizer using the DoReFa scheme, as defined in:\n    Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\n    http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf\n    """"""\n    def __init__(self, model, config_list):\n        """"""\n        config_list: supported keys:\n            - q_bits\n        """"""\n        super().__init__(model, config_list)\n\n    def quantize_weight(self, weight, config, **kwargs):\n        a = tf.stop_gradient(tf.reduce_min(weight))\n        b = tf.stop_gradient(tf.reduce_max(weight))\n        n = tf.cast(2 ** config[\'q_bits\'], tf.float32)\n        scale = b-a/(n-1)\n\n        # use gradient_override_map to change round to idetity for gradient\n        with tf.get_default_graph().gradient_override_map({\'Round\': \'Identity\'}):\n            qw = tf.round((weight-a)/scale)*scale +a\n\n        return qw\n\n\nclass DoReFaQuantizer(Quantizer):\n    """"""Quantizer using the DoReFa scheme, as defined in:\n    Zhou et al., DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\n    (https://arxiv.org/abs/1606.06160)\n    """"""\n    def __init__(self, model, config_list):\n        """"""\n        config_list: supported keys:\n            - q_bits\n        """"""\n        super().__init__(model, config_list)\n\n    def quantize_weight(self, weight, config, **kwargs):\n        a = tf.math.tanh(weight)\n        b = a/(2*tf.reduce_max(tf.abs(weight))) + 0.5\n\n        scale = pow(2, config[\'q_bits\'] - 1)\n        # use gradient_override_map to change round to idetity for gradient\n        with tf.get_default_graph().gradient_override_map({\'Round\': \'Identity\'}):\n            qw = tf.round(b*scale)/scale\n        r_qw = 2 * qw - 1\n        return r_qw\n'"
src/sdk/pynni/nni/compression/tensorflow/compressor.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport tensorflow as tf\nfrom . import default_layers\ntf.config.experimental_run_functions_eagerly(True)\n\n_logger = logging.getLogger(__name__)\n\n\nclass LayerInfo:\n    def __init__(self, keras_layer):\n        self.keras_layer = keras_layer\n        self.name = keras_layer.name\n        self.type = default_layers.get_op_type(type(keras_layer))\n        self.weight_index = default_layers.get_weight_index(self.type)\n        if self.weight_index is not None:\n            self.weight = keras_layer.weights[self.weight_index]\n        self._call = None\n\nclass Compressor:\n    """"""\n    Abstract base TensorFlow compressor\n    """"""\n\n    def __init__(self, model, config_list):\n        """"""\n        Record necessary info in class members\n\n        Parameters\n        ----------\n        model : keras model\n            the model user wants to compress\n        config_list : list\n            the configurations that users specify for compression\n        """"""\n        self.bound_model = model\n        self.config_list = config_list\n        self.modules_to_compress = []\n\n    def detect_modules_to_compress(self):\n        """"""\n        detect all modules should be compressed, and save the result in `self.modules_to_compress`.\n\n        The model will be instrumented and user should never edit it after calling this method.\n        """"""\n        if self.modules_to_compress is None:\n            self.modules_to_compress = []\n            for keras_layer in self.bound_model.layers:\n                layer = LayerInfo(keras_layer)\n                config = self.select_config(layer)\n                if config is not None:\n                    self.modules_to_compress.append((layer, config))\n        return self.modules_to_compress\n\n    def compress(self):\n        """"""\n        Compress the model with algorithm implemented by subclass.\n\n        The model will be instrumented and user should never edit it after calling this method.\n        `self.modules_to_compress` records all the to-be-compressed layers\n        """"""\n        modules_to_compress = self.detect_modules_to_compress()\n        for layer, config in modules_to_compress:\n            self._instrument_layer(layer, config)\n        return self.bound_model\n\n    def get_modules_to_compress(self):\n        """"""\n        To obtain all the to-be-compressed layers.\n\n        Returns\n        -------\n        self.modules_to_compress : list\n            a list of the layers, each of which is a tuple (`layer`, `config`),\n            `layer` is `LayerInfo`, `config` is a `dict`\n        """"""\n        return self.modules_to_compress\n\n    def select_config(self, layer):\n        """"""\n        Find the configuration for `layer` by parsing `self.config_list`\n\n        Parameters\n        ----------\n        layer: LayerInfo\n            one layer\n\n        Returns\n        -------\n        ret : config or None\n            the retrieved configuration for this layer, if None, this layer should\n            not be compressed\n        """"""\n        ret = None\n        if layer.type is None:\n            return None\n        for config in self.config_list:\n            config = config.copy()\n            config[\'op_types\'] = self._expand_config_op_types(config)\n            if layer.type not in config[\'op_types\']:\n                continue\n            if config.get(\'op_names\') and layer.name not in config[\'op_names\']:\n                continue\n            ret = config\n        if ret is None or ret.get(\'exclude\'):\n            return None\n        return ret\n\n    def update_epoch(self, epoch):\n        """"""\n        If user want to update model every epoch, user can override this method.\n        This method should be called at the beginning of each epoch\n\n        Parameters\n        ----------\n        epoch : num\n            the current epoch number\n        """"""\n\n    def step(self):\n        """"""\n        If user want to update mask every step, user can override this method\n        """"""\n\n\n    def _instrument_layer(self, layer, config):\n        """"""\n        This method is implemented in the subclasses, i.e., `Pruner` and `Quantizer`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the compression operation\n        config : dict\n            the configuration for compressing this layer\n        """"""\n        raise NotImplementedError()\n\n    def _expand_config_op_types(self, config):\n        if config is None:\n            return []\n        op_types = []\n\n        for op_type in config.get(\'op_types\', []):\n            if op_type == \'default\':\n                op_types.extend(default_layers.default_layers)\n            else:\n                op_types.append(op_type)\n        return op_types\n\n\nclass Pruner(Compressor):\n    """"""\n    Abstract base TensorFlow pruner\n    """"""\n\n    def calc_mask(self, layer, config):\n        """"""\n        Pruners should overload this method to provide mask for weight tensors.\n        The mask must have the same shape and type comparing to the weight.\n        It will be applied with `mul()` operation on the weight.\n        This method is effectively hooked to `forward()` method of the model.\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            calculate mask for `layer`\'s weight\n        config : dict\n            the configuration for generating the mask\n        """"""\n        raise NotImplementedError(""Pruners must overload calc_mask()"")\n\n    def _instrument_layer(self, layer, config):\n        """"""\n        Create a wrapper forward function to replace the original one.\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for generating the mask\n        """"""\n        layer._call = layer.keras_layer.call\n\n        def new_call(*inputs):\n            weights = [x.numpy() for x in layer.keras_layer.weights]\n            mask = self.calc_mask(layer, config)\n            weights[layer.weight_index] = weights[layer.weight_index] * mask\n            layer.keras_layer.set_weights(weights)\n            ret = layer._call(*inputs)\n            return ret\n\n        layer.keras_layer.call = new_call\n\nclass Quantizer(Compressor):\n    """"""\n    Abstract base TensorFlow quantizer\n    """"""\n\n    def quantize_weight(self, weight, config, op, op_type, op_name):\n        raise NotImplementedError(""Quantizer must overload quantize_weight()"")\n'"
src/sdk/pynni/nni/compression/tensorflow/default_layers.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom tensorflow import keras\n\nsupported_layers = {\n    keras.layers.Conv1D: ('Conv1D', 0),\n    keras.layers.Conv2D: ('Conv2D', 0),\n    keras.layers.Conv2DTranspose: ('Conv2DTranspose', 0),\n    keras.layers.Conv3D: ('Conv3D', 0),\n    keras.layers.Conv3DTranspose: ('Conv3DTranspose', 0),\n    keras.layers.ConvLSTM2D: ('ConvLSTM2D', 0),\n    keras.layers.Dense: ('Dense', 0),\n    keras.layers.Embedding: ('Embedding', 0),\n    keras.layers.GRU: ('GRU', 0),\n    keras.layers.LSTM: ('LSTM', 0),\n}\n\ndefault_layers = [x[0] for x in supported_layers.values()]\n\ndef get_op_type(layer_type):\n    if layer_type in supported_layers:\n        return supported_layers[layer_type][0]\n    else:\n        return None\n\ndef get_weight_index(op_type):\n    for k in supported_layers:\n        if supported_layers[k][0] == op_type:\n            return supported_layers[k][1]\n    return None\n"""
src/sdk/pynni/nni/compression/torch/__init__.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .pruning import *\nfrom .quantization import *\nfrom .compressor import Compressor, Pruner, Quantizer\nfrom .speedup import ModelSpeedup\n'"
src/sdk/pynni/nni/compression/torch/compressor.py,17,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport types\nimport logging\nimport torch\nfrom . import default_layers\n\n_logger = logging.getLogger(__name__)\n\n\nclass LayerInfo:\n    def __init__(self, name, module):\n        self.module = module\n        self.name = name\n        self.type = type(module).__name__\n\ndef _setattr(model, name, module):\n    name_list = name.split(""."")\n    for name in name_list[:-1]:\n        model = getattr(model, name)\n    setattr(model, name_list[-1], module)\n\n\nclass Compressor:\n    """"""\n    Abstract base PyTorch compressor\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Record necessary info in class members\n\n        Parameters\n        ----------\n        model : pytorch model\n            the model user wants to compress\n        config_list : list\n            the configurations that users specify for compression\n        optimizer: pytorch optimizer\n            optimizer used to train the model\n        """"""\n        assert isinstance(model, torch.nn.Module)\n        self.validate_config(model, config_list)\n\n        self.bound_model = model\n        self.config_list = config_list\n        self.optimizer = optimizer\n\n        self.modules_to_compress = None\n        self.modules_wrapper = []\n        self.is_wrapped = False\n\n        self._fwd_hook_handles = {}\n        self._fwd_hook_id = 0\n\n        for layer, config in self._detect_modules_to_compress():\n            wrapper = self._wrap_modules(layer, config)\n            self.modules_wrapper.append(wrapper)\n        if not self.modules_wrapper:\n            _logger.warning(\'Nothing is configured to compress, please check your model and config_list\')\n\n        self._wrap_model()\n\n    def validate_config(self, model, config_list):\n        """"""\n        subclass can optionally implement this method to check if config_list if valid\n        """"""\n        pass\n\n    def _detect_modules_to_compress(self):\n        """"""\n        detect all modules should be compressed, and save the result in `self.modules_to_compress`.\n        The model will be instrumented and user should never edit it after calling this method.\n        """"""\n        if self.modules_to_compress is None:\n            self.modules_to_compress = []\n            for name, module in self.bound_model.named_modules():\n                if module == self.bound_model:\n                    continue\n                layer = LayerInfo(name, module)\n                config = self.select_config(layer)\n                if config is not None:\n                    self.modules_to_compress.append((layer, config))\n        return self.modules_to_compress\n\n    def _wrap_model(self):\n        """"""\n        wrap all modules that needed to be compressed\n\n        """"""\n        for wrapper in reversed(self.get_modules_wrapper()):\n            _setattr(self.bound_model, wrapper.name, wrapper)\n        self.is_wrapped = True\n\n    def _unwrap_model(self):\n        """"""\n        unwrap all modules that needed to be compressed\n\n        """"""\n        for wrapper in self.get_modules_wrapper():\n            _setattr(self.bound_model, wrapper.name, wrapper.module)\n        self.is_wrapped = False\n\n    def compress(self):\n        """"""\n        Compress the model with algorithm implemented by subclass.\n\n        The model will be instrumented and user should never edit it after calling this method.\n        `self.modules_to_compress` records all the to-be-compressed layers\n\n        Returns\n        -------\n        torch.nn.Module\n            model with specified modules compressed.\n        """"""\n        return self.bound_model\n\n    def set_wrappers_attribute(self, name, value):\n        """"""\n        To register attributes used in wrapped module\'s forward method.\n        If the type of the value is Torch.tensor, then this value is registered as a buffer in wrapper,\n        which will be saved by model.state_dict. Otherwise, this value is just a regular variable in wrapper.\n\n        Parameters\n        ----------\n        name : str\n            name of the variable\n        value: any\n            value of the variable\n        """"""\n        for wrapper in self.get_modules_wrapper():\n            if isinstance(value, torch.Tensor):\n                wrapper.register_buffer(name, value.clone())\n            else:\n                setattr(wrapper, name, value)\n\n    def get_modules_to_compress(self):\n        """"""\n        To obtain all the to-be-compressed modules.\n\n        Returns\n        -------\n        list\n            a list of the layers, each of which is a tuple (`layer`, `config`),\n            `layer` is `LayerInfo`, `config` is a `dict`\n        """"""\n        return self.modules_to_compress\n\n    def get_modules_wrapper(self):\n        """"""\n        To obtain all the wrapped modules.\n\n        Returns\n        -------\n        list\n            a list of the wrapped modules\n        """"""\n        return self.modules_wrapper\n\n    def select_config(self, layer):\n        """"""\n        Find the configuration for `layer` by parsing `self.config_list`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            one layer\n\n        Returns\n        -------\n        config or None\n            the retrieved configuration for this layer, if None, this layer should\n            not be compressed\n        """"""\n        ret = None\n        for config in self.config_list:\n            config = config.copy()\n            # expand config if key `default` is in config[\'op_types\']\n            if \'op_types\' in config and \'default\' in config[\'op_types\']:\n                expanded_op_types = []\n                for op_type in config[\'op_types\']:\n                    if op_type == \'default\':\n                        expanded_op_types.extend(default_layers.weighted_modules)\n                    else:\n                        expanded_op_types.append(op_type)\n                config[\'op_types\'] = expanded_op_types\n\n            # check if condition is satisified\n            if \'op_types\' in config and layer.type not in config[\'op_types\']:\n                continue\n            if \'op_names\' in config and layer.name not in config[\'op_names\']:\n                continue\n\n            ret = config\n        if ret is None or \'exclude\' in ret:\n            return None\n        return ret\n\n    def update_epoch(self, epoch):\n        """"""\n        If user want to update model every epoch, user can override this method.\n        This method should be called at the beginning of each epoch\n\n        Parameters\n        ----------\n        epoch : num\n            the current epoch number\n        """"""\n        pass\n\n    def _wrap_modules(self, layer, config):\n        """"""\n        This method is implemented in the subclasses, i.e., `Pruner` and `Quantizer`\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the compression operation\n        config : dict\n            the configuration for compressing this layer\n        """"""\n        raise NotImplementedError()\n\n\n    def add_activation_collector(self, collector):\n        self._fwd_hook_id += 1\n        self._fwd_hook_handles[self._fwd_hook_id] = []\n        for wrapper in self.get_modules_wrapper():\n            handle = wrapper.register_forward_hook(collector)\n            self._fwd_hook_handles[self._fwd_hook_id].append(handle)\n        return self._fwd_hook_id\n\n    def remove_activation_collector(self, fwd_hook_id):\n        if fwd_hook_id not in self._fwd_hook_handles:\n            raise ValueError(""%s is not a valid collector id"" % str(fwd_hook_id))\n        for handle in self._fwd_hook_handles[fwd_hook_id]:\n            handle.remove()\n        del self._fwd_hook_handles[fwd_hook_id]\n\n    def patch_optimizer(self, *tasks):\n        def patch_step(old_step):\n            def new_step(_, *args, **kwargs):\n                # call origin optimizer step method\n                output = old_step(*args, **kwargs)\n                # calculate mask\n                for task in tasks:\n                    task()\n                return output\n            return new_step\n        if self.optimizer is not None:\n            self.optimizer.step = types.MethodType(patch_step(self.optimizer.step), self.optimizer)\n\nclass PrunerModuleWrapper(torch.nn.Module):\n    def __init__(self, module, module_name, module_type, config, pruner):\n        """"""\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        pruner \xef\xbc\x9a Pruner\n            the pruner used to calculate mask\n        """"""\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.pruner = pruner\n\n        # register buffer for mask\n        self.register_buffer(""weight_mask"", torch.ones(self.module.weight.shape))\n        if hasattr(self.module, \'bias\') and self.module.bias is not None:\n            self.register_buffer(""bias_mask"", torch.ones(self.module.bias.shape))\n        else:\n            self.register_buffer(""bias_mask"", None)\n\n    def forward(self, *inputs):\n        # apply mask to weight, bias\n        self.module.weight.data = self.module.weight.data.mul_(self.weight_mask)\n        if hasattr(self.module, \'bias\') and self.module.bias is not None:\n            self.module.bias.data = self.module.bias.data.mul_(self.bias_mask)\n        return self.module(*inputs)\n\nclass Pruner(Compressor):\n    """"""\n    Prune to an exact pruning level specification\n\n    Attributes\n    ----------\n    mask_dict : dict\n        Dictionary for saving masks, `key` should be layer name and\n        `value` should be a tensor which has the same shape with layer\'s weight\n\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        if optimizer is not None:\n            self.patch_optimizer(self.update_mask)\n\n    def compress(self):\n        self.update_mask()\n        return self.bound_model\n\n    def update_mask(self):\n        for wrapper_idx, wrapper in enumerate(self.get_modules_wrapper()):\n            masks = self.calc_mask(wrapper, wrapper_idx=wrapper_idx)\n            if masks is not None:\n                for k in masks:\n                    assert hasattr(wrapper, k), ""there is no attribute \'%s\' in wrapper"" % k\n                    setattr(wrapper, k, masks[k])\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Pruners should overload this method to provide mask for weight tensors.\n        The mask must have the same shape and type comparing to the weight.\n        It will be applied with `mul()` operation on the weight.\n        This method is effectively hooked to `forward()` method of the model.\n\n        Parameters\n        ----------\n        wrapper : Module\n            calculate mask for `wrapper.module`\'s weight\n        """"""\n        raise NotImplementedError(""Pruners must overload calc_mask()"")\n\n    def _wrap_modules(self, layer, config):\n        """"""\n        Create a wrapper module to replace the original one.\n\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for generating the mask\n        """"""\n        _logger.info(""compressing module %s."", layer.name)\n        wrapper = PrunerModuleWrapper(layer.module, layer.name, layer.type, config, self)\n        assert hasattr(layer.module, \'weight\'), ""module %s does not have \'weight\' attribute"" % layer.name\n        # move newly registered buffers to the same device of weight\n        wrapper.to(layer.module.weight.device)\n        return wrapper\n\n    def export_model(self, model_path, mask_path=None, onnx_path=None, input_shape=None, device=None):\n        """"""\n        Export pruned model weights, masks and onnx model(optional)\n\n        Parameters\n        ----------\n        model_path : str\n            path to save pruned model state_dict\n        mask_path : str\n            (optional) path to save mask dict\n        onnx_path : str\n            (optional) path to save onnx model\n        input_shape : list or tuple\n            input shape to onnx model\n        device : torch.device\n            device of the model, used to place the dummy input tensor for exporting onnx file.\n            the tensor is placed on cpu if ```device``` is None\n        """"""\n        assert model_path is not None, \'model_path must be specified\'\n        mask_dict = {}\n        self._unwrap_model() # used for generating correct state_dict name without wrapper state\n\n        for wrapper in self.get_modules_wrapper():\n            weight_mask = wrapper.weight_mask\n            bias_mask = wrapper.bias_mask\n            if weight_mask is not None:\n                mask_sum = weight_mask.sum().item()\n                mask_num = weight_mask.numel()\n                _logger.info(\'Layer: %s  Sparsity: %.2f\', wrapper.name, 1 - mask_sum / mask_num)\n                wrapper.module.weight.data = wrapper.module.weight.data.mul(weight_mask)\n            if bias_mask is not None:\n                wrapper.module.bias.data = wrapper.module.bias.data.mul(bias_mask)\n            # save mask to dict\n            mask_dict[wrapper.name] = {""weight"": weight_mask, ""bias"": bias_mask}\n\n        torch.save(self.bound_model.state_dict(), model_path)\n        _logger.info(\'Model state_dict saved to %s\', model_path)\n        if mask_path is not None:\n            torch.save(mask_dict, mask_path)\n            _logger.info(\'Mask dict saved to %s\', mask_path)\n        if onnx_path is not None:\n            assert input_shape is not None, \'input_shape must be specified to export onnx model\'\n            # input info needed\n            if device is None:\n                device = torch.device(\'cpu\')\n            input_data = torch.Tensor(*input_shape)\n            torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)\n            _logger.info(\'Model in onnx with input shape %s saved to %s\', input_data.shape, onnx_path)\n\n        self._wrap_model()\n\n    def load_model_state_dict(self, model_state):\n        """"""\n        Load the state dict saved from unwrapped model.\n\n        Parameters:\n        -----------\n        model_state : dict\n            state dict saved from unwrapped model\n        """"""\n        if self.is_wrapped:\n            self._unwrap_model()\n            self.bound_model.load_state_dict(model_state)\n            self._wrap_model()\n        else:\n            self.bound_model.load_state_dict(model_state)\n\nclass QuantizerModuleWrapper(torch.nn.Module):\n    def __init__(self, module, module_name, module_type, config, quantizer):\n        """"""\n        Wrap an module to enable data parallel, forward method customization and buffer registeration.\n\n        Parameters\n        ----------\n        module : pytorch module\n            the module user wants to compress\n        config : dict\n            the configurations that users specify for compression\n        module_name : str\n            the name of the module to compress, wrapper module shares same name\n        module_type : str\n            the type of the module to compress\n        quantizer \xef\xbc\x9aquantizer\n            the quantizer used to calculate mask\n        """"""\n        super().__init__()\n        # origin layer information\n        self.module = module\n        self.name = module_name\n        self.type = module_type\n        # config and pruner\n        self.config = config\n        self.quantizer = quantizer\n\n        # register buffer and parameter\n        # old_weight is used to store origin weight and weight is used to store quantized weight\n        # the reason why weight is buffer instead of parameter is because in pytorch parameter is used as leaf\n        # if weight is leaf , then old_weight can not be updated.\n        if \'weight\' in config[\'quant_types\']:\n            if not _check_weight(self.module):\n                _logger.warning(\'Module %s does not have parameter ""weight""\', self.name)\n            else:\n                self.module.register_parameter(\'old_weight\', torch.nn.Parameter(self.module.weight))\n                delattr(self.module, \'weight\')\n                self.module.register_buffer(\'weight\', self.module.old_weight)\n\n    def forward(self, *inputs):\n        if \'input\' in self.config[\'quant_types\']:\n            inputs = self.quantizer.quant_grad.apply(\n                inputs,\n                QuantType.QUANT_INPUT,\n                self)\n\n        if \'weight\' in self.config[\'quant_types\'] and _check_weight(self.module):\n            new_weight = self.quantizer.quant_grad.apply(\n                self.module.old_weight,\n                QuantType.QUANT_WEIGHT,\n                self)\n            self.module.weight = new_weight\n            result = self.module(*inputs)\n        else:\n            result = self.module(*inputs)\n\n        if \'output\' in self.config[\'quant_types\']:\n            result = self.quantizer.quant_grad.apply(\n                result,\n                QuantType.QUANT_OUTPUT,\n                self)\n        return result\n\nclass Quantizer(Compressor):\n    """"""\n    Base quantizer for pytorch quantizer\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = QuantGrad\n        if self.optimizer is not None:\n            self.patch_optimizer(self.step_with_optimizer)\n            for wrapper in self.get_modules_wrapper():\n                if \'weight\' in wrapper.config[\'quant_types\']:\n                    # old_weight is registered to keep track of weight before quantization\n                    # and it is trainable, therefore, it should be added to optimizer.\n                    self.optimizer.add_param_group({""params"": wrapper.module.old_weight})\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        """"""\n        quantize should overload this method to quantize weight.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        weight : Tensor\n            weight that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        """"""\n        raise NotImplementedError(\'Quantizer must overload quantize_weight()\')\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        """"""\n        quantize should overload this method to quantize output.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        output : Tensor\n            output that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        """"""\n        raise NotImplementedError(\'Quantizer must overload quantize_output()\')\n\n    def quantize_input(self, *inputs, wrapper, **kwargs):\n        """"""\n        quantize should overload this method to quantize input.\n        This method is effectively hooked to :meth:`forward` of the model.\n        Parameters\n        ----------\n        inputs : Tensor\n            inputs that needs to be quantized\n        wrapper : QuantizerModuleWrapper\n            the wrapper for origin module\n        """"""\n        raise NotImplementedError(\'Quantizer must overload quantize_input()\')\n\n\n    def _wrap_modules(self, layer, config):\n        """"""\n        Create a wrapper forward function to replace the original one.\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to instrument the mask\n        config : dict\n            the configuration for quantization\n        """"""\n        assert \'quant_types\' in config, \'must provide quant_types in config\'\n        assert isinstance(config[\'quant_types\'], list), \'quant_types must be list type\'\n        assert \'quant_bits\' in config, \'must provide quant_bits in config\'\n        assert isinstance(config[\'quant_bits\'], int) or isinstance(config[\'quant_bits\'], dict), \'quant_bits must be dict type or int type\'\n\n        if isinstance(config[\'quant_bits\'], dict):\n            for quant_type in config[\'quant_types\']:\n                assert quant_type in config[\'quant_bits\'], \'bits length for %s must be specified in quant_bits dict\' % quant_type\n\n        return QuantizerModuleWrapper(layer.module, layer.name, layer.type, config, self)\n\n    def step_with_optimizer(self):\n        pass\n\nclass QuantType:\n    """"""\n    Enum class for quantization type.\n    """"""\n    QUANT_INPUT = 0\n    QUANT_WEIGHT = 1\n    QUANT_OUTPUT = 2\n\n\nclass QuantGrad(torch.autograd.Function):\n    """"""\n    Base class for overriding backward function of quantization operation.\n    """"""\n    @staticmethod\n    def quant_backward(tensor, grad_output, quant_type):\n        """"""\n        This method should be overrided by subclass to provide customized backward function,\n        default implementation is Straight-Through Estimator\n        Parameters\n        ----------\n        tensor : Tensor\n            input of quantization operation\n        grad_output : Tensor\n            gradient of the output of quantization operation\n        quant_type : QuantType\n            the type of quantization, it can be `QuantType.QUANT_INPUT`, `QuantType.QUANT_WEIGHT`, `QuantType.QUANT_OUTPUT`,\n            you can define different behavior for different types.\n        Returns\n        -------\n        tensor\n            gradient of the input of quantization operation\n        """"""\n        return grad_output\n\n    @staticmethod\n    def forward(ctx, tensor, quant_type, wrapper, **kwargs):\n        ctx.save_for_backward(tensor, torch.Tensor([quant_type]))\n        if quant_type == QuantType.QUANT_INPUT:\n            return wrapper.quantizer.quantize_input(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_WEIGHT:\n            return wrapper.quantizer.quantize_weight(tensor, wrapper, **kwargs)\n        elif quant_type == QuantType.QUANT_OUTPUT:\n            return wrapper.quantizer.quantize_output(tensor, wrapper, **kwargs)\n        else:\n            raise ValueError(""unrecognized QuantType."")\n\n    @classmethod\n    def backward(cls, ctx, grad_output):\n        tensor, quant_type = ctx.saved_variables\n        output = cls.quant_backward(tensor, grad_output, quant_type)\n        return output, None, None, None\n\ndef _check_weight(module):\n    try:\n        return isinstance(module.weight.data, torch.Tensor)\n    except AttributeError:\n        return False\n'"
src/sdk/pynni/nni/compression/torch/default_layers.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nweighted_modules = [\n    'Conv1d', 'Conv2d', 'Conv3d', 'ConvTranspose1d', 'ConvTranspose2d', 'ConvTranspose3d',\n    'Linear', 'Bilinear',\n    'PReLU',\n    'Embedding', 'EmbeddingBag',\n]\n"""
src/sdk/pynni/nni/feature_engineering/gbdt_selector/__init__.py,0,b'from .gbdt_selector import GBDTSelector'
src/sdk/pynni/nni/feature_engineering/gbdt_selector/gbdt_selector.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n""""""\ngbdt_selector.py including:\n    class GBDTSelector\n""""""\n\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nni.feature_engineering.feature_selector import FeatureSelector\n\n# pylint: disable=E0401\nimport lightgbm as lgb\n\n\nclass GBDTSelector(FeatureSelector):\n\n    def __init__(self, **kwargs):\n        self.selected_features_ = None\n        self.X = None\n        self.y = None\n        self.feature_importance = None\n        self.lgb_params = None\n        self.eval_ratio = None\n        self.early_stopping_rounds = None\n        self.importance_type = None\n        self.num_boost_round = None\n        self.model = None\n\n\n    def fit(self, X, y, **kwargs):\n        """"""\n        Fit the training data to FeatureSelector\n\n        Paramters\n        ---------\n        X : array-like numpy matrix\n            The training input samples, which shape is [n_samples, n_features].\n        y : array-like numpy matrix\n            The target values (class labels in classification, real numbers in\n            regression). Which shape is [n_samples].\n        lgb_params : dict\n            Parameters of lightgbm\n        eval_ratio : float\n            The ratio of data size. It\'s used for split the eval data and train data from self.X.\n        early_stopping_rounds : int\n            The early stopping setting in lightgbm.\n        importance_type : str\n            Supporting type is \'gain\' or \'split\'.\n        num_boost_round : int\n            num_boost_round in lightgbm.\n        """"""\n        assert kwargs[\'lgb_params\']\n        assert kwargs[\'eval_ratio\']\n        assert kwargs[\'early_stopping_rounds\']\n        assert kwargs[\'importance_type\']\n        assert kwargs[\'num_boost_round\']\n\n        self.X = X\n        self.y = y\n        self.lgb_params = kwargs[\'lgb_params\']\n        self.eval_ratio = kwargs[\'eval_ratio\']\n        self.early_stopping_rounds = kwargs[\'early_stopping_rounds\']\n        self.importance_type = kwargs[\'importance_type\']\n        self.num_boost_round = kwargs[\'num_boost_round\']\n\n        X_train, X_test, y_train, y_test = train_test_split(self.X,\n                                                            self.y,\n                                                            test_size=self.eval_ratio,\n                                                            random_state=random.seed(41))\n        lgb_train = lgb.Dataset(X_train, y_train)\n        lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n        self.model = lgb.train(self.lgb_params,\n                               lgb_train,\n                               num_boost_round=self.num_boost_round,\n                               valid_sets=lgb_eval,\n                               early_stopping_rounds=self.early_stopping_rounds)\n\n        self.feature_importance = self.model.feature_importance(self.importance_type)\n\n\n    def get_selected_features(self, topk):\n        """"""\n        Fit the training data to FeatureSelector\n\n        Returns\n        -------\n        list :\n                Return the index of imprtant feature.\n        """"""\n        assert topk > 0\n\n        self.selected_features_ = self.feature_importance.argsort()[-topk:][::-1]\n\n        return self.selected_features_\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/__init__.py,0,b'from .gradient_selector import FeatureGradientSelector'
src/sdk/pynni/nni/feature_engineering/gradient_selector/constants.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n\nimport numpy as np\n\n\nclass StorageLevel:\n    DISK = \'disk\'\n    SPARSE = \'sparse\'\n    DENSE = \'dense\'\n\n\nclass DataFormat:\n    SVM = \'svm\'\n    NUMPY = \'numpy\'\n    ALL_FORMATS = [SVM, NUMPY]\n\n\nclass Preprocess:\n    """"""\n    center the data to mean 0 and create unit variance\n    center the data to mean 0\n    """"""\n    ZSCORE = \'zscore\'\n    CENTER = \'center\'\n\n\nclass Device:\n    CUDA = \'cuda\'\n    CPU = \'cpu\'\n\n\nclass Checkpoint:\n    MODEL = \'model_state_dict\'\n    OPT = \'optimizer_state_dict\'\n    RNG = \'torch_rng_state\'\n\n\nclass NanError(ValueError):\n    pass\n\n\nclass Initialization:\n    ZERO = \'zero\'\n    ON = \'on\'\n    OFF = \'off\'\n    ON_HIGH = \'onhigh\'\n    OFF_HIGH = \'offhigh\'\n    SKLEARN = \'sklearn\'\n    RANDOM = \'random\'\n    VALUE_DICT = {ZERO: 0,\n                  ON: 1,\n                  OFF: -1,\n                  ON_HIGH: 5,\n                  OFF_HIGH: -1,\n                  SKLEARN: None,\n                  RANDOM: None}\n\n\nclass Coefficients:\n    """"""""\n    coefficients for sublinear estimator were computed running the sublinear\n    paper\'s authors\' code\n    """"""\n    SLE = {1: np.array([0.60355337]),\n           2: np.array([1.52705001, -0.34841729]),\n           3: np.array([2.90254224, -1.87216745, 0.]),\n           4: np.array([4.63445685, -5.19936195, 0., 1.50391676]),\n           5: np.array([6.92948049, -14.12216211, 9.4475009, 0., -1.21093546]),\n           6: np.array([9.54431082, -28.09414643, 31.84703652, -11.18763791, -1.14175281, 0.]),\n           7: np.array([12.54505041, -49.64891525, 79.78828031, -46.72250909, 0., 0., 5.02973646]),\n           8: np.array([16.03550163, -84.286182, 196.86078756, -215.36747071, 92.63961263, 0., 0., -4.86280869]),\n           9: np.array([19.86409184, -130.76801006, 390.95349861, -570.09210416, 354.77764899, 0., -73.84234865, 0., 10.09148767]),\n           10: np.array([2.41117752e+01, -1.94946061e+02, 7.34214614e+02, -1.42851995e+03, 1.41567410e+03, \\\n                         -5.81738134e+02, 0., 0., 3.11664751e+01, 1.05018365e+00]),\n           11: np.array([28.75280839, -279.22576729, 1280.46325445, -3104.47148101, 3990.6092248, -2300.29413333, \\\n                         0., 427.35289033, 0., 0., -42.17587475]),\n           12: np.array([33.85141912, -391.4229382, 2184.97827882, -6716.28280208, 11879.75233977, -11739.97267239, \\\n                         5384.94542245, 0., -674.23291712, 0., 0., 39.37456439])}\n\n\nEPSILON = 1e-8\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/fginitialize.py,25,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n\nimport os\nimport pickle\nimport sys\nimport time\n\nimport numpy as np\nimport scipy.sparse\nfrom sklearn.datasets import load_svmlight_file\n\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\n# pylint: disable=E0611\nfrom torch.utils.data.dataloader import _DataLoaderIter, _utils\n\nfrom . import constants\nfrom . import syssettings\n\ntorch.set_default_tensor_type(syssettings.torch.tensortype)\nsparsetensor = syssettings.torch.sparse.tensortype\n\nBYTESPERREAL = 8.\nBYTESPERGB = 1024. ** 3\n\n\nclass PrepareData(Dataset):\n\n    def __init__(self,\n                 path_data=None,\n                 data_format=constants.DataFormat.NUMPY,\n                 D=None, N=None,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess=None,\n                 n_to_estimate=None,\n                 MAXMEMGB=syssettings.MAXMEMGB,\n                 set_params=True,\n                 path_mappings=None,\n                 X=None,\n                 y=None,\n                 verbose=0,\n                 n_classes=None,\n                 device=constants.Device.CPU):\n        """"""\n        Dataset class with helpful features and functions for being included in a dataloader\n        and managing memory usage.\n        can read following formats:\n            svm:        svm light format (sklearn.datasets.load_svmlight_file)\n            numpy:      Pass X and y as numpy or sparse arrays\n\n        assumes\n            1. if classification, y is in {-1, 1} or continuous and 0 indexed\n            2. y can fit into memory\n            3. consecutive calls to __getitem__() have consecutive idx values\n\n        notes:\n            1. this implementation is not careful wrt/ precise memory reqts. for\n            example, being able to store one dense row in memory is necessary,\n            but not sufficient.\n            2. for y with 4.2 billion elements, 31.3 GB of memory is  necessary\n            @ 8 bytes/scalar. Use partial fit to avoid loading the entire dataset\n            at once\n            3. disk_size always refer to size of complete data file, even after\n            a split().\n\n\n        Parameters\n        ----------\n        path_data : str\n            Path to load data from\n        data_format : str\n            File ending for path data.\n            ""numpy"" is the default when passing in X and y\n        D : int\n            Number of features.\n        N : int\n            Number of rows.\n        classification : bool\n            If True, problem is classification, else regression.\n        ordinal: bool\n            If True, problem is ordinal classification. Requires classification to be True.\n        balanced : bool\n            If true, each class is weighted equally in optimization, otherwise\n            weighted is done via support of each class. Requires classification to be True.\n        prerocess : str\n            \'zscore\' which refers to centering and normalizing data to unit variance or\n            \'center\' which only centers the data to 0 mean\n        n_to_estimate : int\n            Number of rows of data to estimate\n        MAXMEMGB : float\n            Maximum allowable size for a minibatch\n        set_params : bool\n            Whether or not to determine the statistics of the dataset\n        path_mappings : str\n            Used when streaming from disk\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        device : str\n            \'cpu\' to run on CPU and \'cuda\' to run on GPU. Runs much faster on GPU\n        n_classes : int\n            number of classes\n        """"""\n\n        self.path_data = path_data\n        if self.path_data:\n            self.disk_size = os.path.getsize(path_data)\n        else:\n            assert X is not None, \'X must be specified if no path data\'\n            self.disk_size = X.nbytes if not scipy.sparse.issparse(\n                X) else X.data.nbytes\n        assert data_format in constants.DataFormat.ALL_FORMATS, \'Format must in {0}.\'.format(\n            "", "".join(constants.DataFormat.ALL_FORMATS))\n        self.format = data_format\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.MAXMEMGB = MAXMEMGB\n        self.preprocess = preprocess\n        self.set_params = set_params\n        self.verbose = verbose\n        self.n_classes = n_classes\n        self.device = device\n\n        self.path_data_stats = None\n\n        if D is None:\n            assert self.disk_size / BYTESPERGB <= self.MAXMEMGB, \\\n                \'Cannot load data into memory. Supply D.\'\n\n            if self.format == constants.DataFormat.SVM:\n                self.X, self.y = load_svmlight_file(path_data)\n            elif self.format == constants.DataFormat.NUMPY:\n                assert X is not None, \'X must be specified in numpy mode\'\n                assert y is not None, \'y must be specified in numpy mode\'\n                self.X = X\n                self.y = y\n                if self.n_classes is None:\n                    self.n_classes = np.unique(y).shape[0]\n                elif self.classification:\n                    assert self.n_classes >= np.unique(y).shape[0], \\\n                        \'n_classes given must be greater than or equal to the number of classes in y\'\n            else:\n                raise NotImplementedError\n            self.y = torch.as_tensor(self.y, dtype=torch.get_default_dtype())\n\n            self.N, self.D = self.X.shape\n\n            # assumes X was returned as a sparse array\n            self.storage_level = (constants.StorageLevel.SPARSE\n                                  if scipy.sparse.issparse(self.X)\n                                  else constants.StorageLevel.DENSE)\n\n        else:\n            assert N is not None, \'Supply N.\'\n            self.N, self.D = N, D\n\n            # assume sparse matrix cannot fit into memory\n            self.storage_level = constants.StorageLevel.DISK\n\n        self.dense_size_gb = self.get_dense_size()\n\n        # check dense size\n        self.set_dense_X()\n\n        self.max_rows = int(self.MAXMEMGB * BYTESPERGB / BYTESPERREAL / self.D)\n        assert self.max_rows, \\\n            \'Cannot fit one dense row into %d GB memory.\' % self.MAXMEMGB\n        self.max_rows = self.max_batch_size()\n        sys.stdout.flush()\n\n        if n_to_estimate is None:\n            self.n_to_estimate = self.max_batch_size()\n        else:\n            assert n_to_estimate <= self.N, \'n_to_estimate must be <= N.\'\n            self.n_to_estimate = n_to_estimate\n\n        # initialize disk loader\n        if self.storage_level == constants.StorageLevel.DISK and self.set_params:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError(\n                    \'Please use partial fit to train on datasets that do not fit in memory\')\n            else:\n                raise NotImplementedError\n\n        # TODO: use a passed-in RNG here\n        self.ix_statistics = np.random.permutation(self.N)[:self.n_to_estimate]\n        self.n_features = self.D\n        if self.set_params:\n            if self.verbose:\n                print(\'Finding data statistics...\', end=\'\')\n                sys.stdout.flush()\n            Xmn, sv1, Xsd, ymn, ysd = self.compute_data_stats()\n            self.set_data_stats(Xmn, sv1, Xsd, ymn, ysd)\n            if self.verbose:\n                print()\n            self.set_return_raw(False)\n        else:\n            self.set_return_raw(True)\n\n        self.set_return_np(False)\n\n        # this needs to occur after setting preprocessing params\n        if (self.storage_level == constants.StorageLevel.DISK and\n                self.format == constants.DataFormat.SVM and self.set_params):\n            self.loader.batchsize = 1\n\n    def get_dense_size(self):\n        return self.N * self.D * BYTESPERREAL / BYTESPERGB\n\n    def set_dense_X(self):\n        if self.storage_level != constants.StorageLevel.DISK:\n            if self.dense_size_gb <= self.MAXMEMGB:\n                if self.storage_level == constants.StorageLevel.SPARSE:\n                    self.X = self.X.toarray()\n                self.X = torch.as_tensor(\n                    self.X, dtype=torch.get_default_dtype())\n                self.storage_level = constants.StorageLevel.DENSE\n\n    def set_return_np(self, boolean):\n\n        self.return_np = boolean\n\n    def set_return_raw(self, boolean):\n\n        self.return_raw = boolean\n\n    def save_data_stats(self, path_data_stats):\n        """"""\n        Dumps dataset statistics to pickle file.\n        """"""\n\n        data_stats = {\n            \'Xmn\': self.Xmn,\n            \'sv1\': self.sv1,\n            \'Xsd\': self.Xsd,\n            \'ymn\': self.ymn,\n            \'ysd\': self.ysd,\n            \'ix_statistics\': self.ix_statistics,\n        }\n        pickle.dump(data_stats, open(path_data_stats, \'wb\'))\n\n    def load_data_stats(self, path_data_stats):\n\n        stats = pickle.load(open(path_data_stats, \'rb\'))\n        self.path_data_stats = path_data_stats\n\n        self.set_data_stats(np.asarray(stats[\'Xmn\']), stats[\'sv1\'],\n                            stats[\'Xsd\'], stats[\'ymn\'], stats[\'ysd\'])\n\n        if self.storage_level == constants.StorageLevel.DISK and hasattr(\n                self, \'path_mappings\'):\n            if \'ix_statistics\' in stats:\n                self.ix_statistics = stats[\'ix_statistics\']\n            else:\n                self.ix_statistics = range(self.N)\n\n        self.set_return_raw(False)\n\n    def reset(self):\n        """"""\n        Resets the dataloader. Only implemented for disk StorageLevel.\n        """"""\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            pass\n        elif self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                self.loader.reset()\n            else:\n                raise NotImplementedError\n\n    def todense(self):\n\n        assert hasattr(self, \'Xmn\'), \'Set preprocess params first.\'\n        assert len(self) <= self.max_batch_size(\n        ), \'N must be <= max_batch_size().\'\n\n        with torch.no_grad():\n            dense, _ = self.split(range(len(self)))\n            Braw = self.return_raw\n            Bnp = self.return_np\n            self.set_return_raw(True)\n            self.set_return_np(True)\n            dense.X, dense.y = [], []\n\n            def f_Xy(X, y):\n                dense.X.append(X)\n                dense.y.append(y)\n            self.apply(f_Xy=f_Xy)\n            dense.X = dense.X[-1]\n            dense.y = dense.y[-1]\n            self.set_return_raw(Braw)\n            self.set_return_np(Bnp)\n            dense.storage_level = constants.StorageLevel.DENSE\n\n            return dense\n\n    def split(self, ix):\n\n        assert hasattr(self, \'Xmn\'), \'Run set_preprocess_params() first.\'\n\n        first = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n        second = type(self)(\n            self.path_data,\n            self.format,\n            self.D,\n            N=self.N - len(ix),\n            classification=self.classification,\n            preprocess=self.preprocess,\n            n_to_estimate=None,\n            MAXMEMGB=self.MAXMEMGB,\n            set_params=False)\n\n        first.storage_level = self.storage_level\n        second.storage_level = self.storage_level\n\n        # copy preprocess params\n        if not self.classification:\n            first.ymn = self.ymn\n            second.ymn = self.ymn\n            first.ysd = self.ysd\n            second.ysd = self.ysd\n\n        first.Xmn = self.Xmn\n        second.Xmn = self.Xmn\n        first.sv1 = self.sv1\n        second.sv1 = self.sv1\n\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                first.Xsd = self.Xsd\n                second.Xsd = self.Xsd\n            else:\n                raise NotImplementedError\n\n        # initialize data structures\n        if self.storage_level == constants.StorageLevel.DISK:\n            if self.format == constants.DataFormat.SVM:\n                raise NotImplementedError\n            raise NotImplementedError\n        elif self.storage_level in [constants.StorageLevel.SPARSE,\n                                    constants.StorageLevel.DENSE]:\n            first.X, first.y = self.X[ix], self.y[ix]\n            ixsec = list(set(range(self.N)).difference(set(ix)))\n            second.X, second.y = self.X[ixsec], self.y[ixsec]\n\n        return first, second\n\n    @staticmethod\n    def sparse_std(X, X_mean):\n        """"""\n        Calculate the column wise standard deviations of a sparse matrix.\n        """"""\n        X_copy = X.copy()\n        X_copy.data **= 2  # square non zero elements\n        E_x_squared = np.array(X_copy.mean(axis=0)).ravel()\n        Xsd = np.sqrt(E_x_squared - X_mean**2)\n        return Xsd\n\n    def compute_data_stats(self):\n        """"""\n        1. computes/estimates feature means\n        2. if preprocess == \'zscore\', computes/estimates feature standard devs\n        3. if not classification, computes/estimates target mean/standard dev\n        4. estimates largest singular value of data matrix\n        """"""\n        t = time.time()\n        X, y = self.X[self.ix_statistics], self.y[self.ix_statistics]\n        preprocess = self.preprocess\n        classification = self.classification\n\n        Xmn = (X.mean(dim=0)\n               if not scipy.sparse.issparse(X)\n               else np.array(X.mean(axis=0)).ravel())\n\n        if preprocess == constants.Preprocess.ZSCORE:\n            Xsd = (X.std(dim=0)\n                   if not scipy.sparse.issparse(X)\n                   else PrepareData.sparse_std(X, Xmn))\n            Xsd[Xsd == 0] = 1.\n        else:\n            Xsd = 1.\n\n        if preprocess is not None and preprocess:\n            if preprocess == constants.Preprocess.ZSCORE:\n                Xc = (X - Xmn) / Xsd\n            else:\n                Xc = X - Xmn\n        else:\n            Xc = X - Xmn\n\n        sv1 = scipy.sparse.linalg.svds(Xc / (\n            torch.sqrt(torch.prod(torch.as_tensor(y.size(), dtype=torch.get_default_dtype())))\n            if not scipy.sparse.issparse(X) else y.numpy().size),\n                                       k=1,\n                                       which=\'LM\',\n                                       return_singular_vectors=False)\n        # avoid runaway sv1\n        sv1 = np.array([min(np.finfo(np.float32).max,\n                            sv1[0])])\n\n        if not classification:\n            ymn = y.mean()\n            ysd = y.std()\n        else:\n            # TODO: set these, for each class?\n            ymn = 0.\n            ysd = 1.\n        if self.verbose:\n            print("" computing data statistics took: "", time.time() - t)\n\n        return Xmn, sv1, Xsd, ymn, ysd\n\n\n    def set_data_stats(self, Xmn, sv1, Xsd=1., ymn=0., ysd=1.):\n        """"""\n        Saves dataset stats to self to be used for preprocessing.\n        """"""\n\n        self.Xmn = torch.as_tensor(\n            Xmn, dtype=torch.get_default_dtype()).to(self.device)\n        self.sv1 = torch.as_tensor(\n            sv1, dtype=torch.get_default_dtype()).to(self.device)\n        self.Xsd = torch.as_tensor(\n            Xsd, dtype=torch.get_default_dtype()).to(self.device)\n        self.ymn = torch.as_tensor(\n            ymn, dtype=torch.get_default_dtype()).to(self.device)\n        self.ysd = torch.as_tensor(\n            ysd, dtype=torch.get_default_dtype()).to(self.device)\n\n\n    def apply_preprocess(self, X, y):\n        """"""\n        Faster on gpu device, while dataloading takes up a large portion of the time.\n        """"""\n\n        with torch.no_grad():\n            if not self.classification:\n                y = (y.reshape((-1, 1)) - self.ymn) / self.ysd\n            else:\n                y = y.reshape((-1, 1))\n            X = (X - self.Xmn) / self.sv1\n\n            if self.preprocess == constants.Preprocess.ZSCORE:\n                X /= self.Xsd\n\n            return X, y\n\n\n    def max_batch_size(self):\n        """"""\n        Return the maximum batchsize for the dataset.\n        """"""\n\n        return int(np.min([self.max_rows, self.N]))\n\n\n    def apply(self, ix_rows=None, ix_cols=None, f_Xy=None):\n\n        if f_Xy is None:\n            return\n\n        if ix_rows is None:\n            ix_rows = range(self.N)\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        f_Xy((self.X[ix_rows, ix_cols]\n              if not self.storage_level == constants.StorageLevel.SPARSE\n              else self.X[ix_rows, ix_cols].toarray()), self.y[ix_rows])\n\n\n    def get_dense_data(self, ix_cols=None, ix_rows=None):\n\n        if ix_cols is None:\n            ix_cols = range(self.n_features)\n\n        X = [np.zeros((0, len(ix_cols)))]\n        y = [np.zeros((0, 1))]\n        Bnp = self.return_np\n\n        def f_Xy(Xb, yb, n):\n            X[-1] = np.concatenate((X[-1], Xb), axis=0)\n            y[-1] = np.concatenate((y[-1], yb), axis=0)\n        self.apply(f_Xy=f_Xy, ix_rows=ix_rows, ix_cols=ix_cols)\n        self.set_return_np(Bnp)\n\n        return X[-1], y[-1]\n\n\n    def __len__(self):\n\n        return self.N\n\n\n    def getXy(self, idx):\n\n        if self.storage_level == constants.StorageLevel.DENSE:\n            X, y = self.X[idx], self.y[idx]\n        elif self.storage_level == constants.StorageLevel.SPARSE:\n            # assume subset can fit into memory even if whole matrix cant\n            X, y = self.X[idx].toarray(), self.y[idx]\n        else:\n            raise NotImplementedError\n\n        return X, y\n\n\n    def __getitem__(self, idx):\n\n        with torch.no_grad():\n            X, y = self.getXy(idx)\n            X = X.toarray() if scipy.sparse.issparse(X) else X\n\n            X = torch.as_tensor(\n                X, dtype=torch.get_default_dtype()).to(self.device)\n            y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype()).to(self.device)\n\n            if not self.return_raw:\n                X, y = self.apply_preprocess(X, y)\n\n            if self.classification and (\n                    self.n_classes is None or self.n_classes == 2):\n                y[y == 0] = -1\n\n            if self.return_np:\n                if constants.Device.CPU not in self.device:\n                    X = X.cpu()\n                    y = y.cpu()\n                X = X.numpy()\n                y = y.numpy()\n                return X, y\n\n            return X, y\n\n\nclass ChunkDataLoader(DataLoader):\n    """"""\n    DataLoader class used to more quickly load a batch of indices at once.\n    """"""\n\n    def __iter__(self):\n        return _ChunkDataLoaderIter(self)\n\n\nclass _ChunkDataLoaderIter(_DataLoaderIter):\n    """"""\n    DataLoaderIter class used to more quickly load a batch of indices at once.\n    """"""\n\n    def __next__(self):\n        # only chunk that is edited from base\n        if self.num_workers == 0:  # same-process loading\n            indices = next(self.sample_iter)  # may raise StopIteration\n            if len(indices) > 1:\n                batch = self.dataset[np.array(indices)]\n            else:\n                batch = self.collate_fn([self.dataset[i] for i in indices])\n\n            if self.pin_memory:\n                batch = _utils.pin_memory.pin_memory_batch(batch)\n            return batch\n\n        # check if the next sample has already been generated\n        if self.rcvd_idx in self.reorder_dict:\n            batch = self.reorder_dict.pop(self.rcvd_idx)\n            return self._process_next_batch(batch)\n\n        if self.batches_outstanding == 0:\n            self._shutdown_workers()\n            raise StopIteration\n\n        while True:\n            assert (not self.shutdown and self.batches_outstanding > 0)\n            idx, batch = self._get_batch()\n            self.batches_outstanding -= 1\n            if idx != self.rcvd_idx:\n                # store out-of-order samples\n                self.reorder_dict[idx] = batch\n                continue\n            return self._process_next_batch(batch)\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/fgtrain.py,7,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n\nimport time\n\nimport numpy as np\nimport torch\nfrom sklearn.feature_selection import SelectKBest, \\\n    f_classif, mutual_info_classif, f_regression, mutual_info_regression\n\nfrom . import constants\nfrom . import syssettings\nfrom .learnability import Solver\nfrom .utils import EMA\n\ntorch.set_default_tensor_type(syssettings.torch.tensortype)\n\n\ndef get_optim_f_stop(maxiter, maxtime, dftol_stop, freltol_stop,\n                     minibatch=True):\n    """"""\n    Check stopping conditions.\n    """"""\n\n    discount_factor = 1. / 3\n\n    total_t = [0.]\n    df_store = [np.nan]\n    it_store = [0]\n    relchange_store = [np.nan]\n    f_ma = EMA(discount_factor=discount_factor)\n    df_ma = EMA(discount_factor=discount_factor)\n\n    def f_stop(f0, v0, it, t):\n\n        flag_stop = False\n\n        total_t[-1] += t\n        g = f0.x.grad.clone().cpu().detach()\n        df = g.abs().max().numpy().squeeze()\n        v = v0.clone().cpu().detach()\n        f = v.numpy().squeeze()\n\n        if it >= maxiter:\n            flag_stop = True\n\n        elif total_t[-1] >= maxtime:\n            flag_stop = True\n\n        f_ma.update(f)\n        df_ma.update(df)\n        rel_change = f_ma.relchange()\n\n        if ((not minibatch) and (df < dftol_stop)) \\\n           or (minibatch and (df_ma() < dftol_stop)):\n            flag_stop = True\n\n        if rel_change < freltol_stop:\n            flag_stop = True\n\n        if not minibatch:\n            df_store[-1] = df\n        else:\n            df_store[-1] = df_ma()\n        relchange_store[-1] = rel_change\n        it_store[-1] = it\n\n        return flag_stop\n\n    return f_stop, {\'t\': total_t, \'it\': it_store, \'df\': df_store,\n                    \'relchange\': relchange_store}\n\n\ndef get_init(data_train, init_type=\'on\', rng=np.random.RandomState(0), prev_score=None):\n    """"""\n    Initialize the \'x\' variable with different settings\n    """"""\n\n    D = data_train.n_features\n    value_off = constants.Initialization.VALUE_DICT[\n        constants.Initialization.OFF]\n    value_on = constants.Initialization.VALUE_DICT[\n        constants.Initialization.ON]\n\n    if prev_score is not None:\n        x0 = prev_score\n    elif not isinstance(init_type, str):\n        x0 = value_off * np.ones(D)\n        x0[init_type] = value_on\n    elif init_type.startswith(constants.Initialization.RANDOM):\n        d = int(init_type.replace(constants.Initialization.RANDOM, \'\'))\n        x0 = value_off * np.ones(D)\n        x0[rng.permutation(D)[:d]] = value_on\n    elif init_type == constants.Initialization.SKLEARN:\n        B = data_train.return_raw\n        X, y = data_train.get_dense_data()\n        data_train.set_return_raw(B)\n        ix = train_sk_dense(init_type, X, y, data_train.classification)\n        x0 = value_off * np.ones(D)\n        x0[ix] = value_on\n    elif init_type in constants.Initialization.VALUE_DICT:\n        x0 = constants.Initialization.VALUE_DICT[init_type] * np.ones(D)\n    else:\n        raise NotImplementedError(\n            \'init_type {0} not supported yet\'.format(init_type))\n    # pylint: disable=E1102\n    return torch.tensor(x0.reshape((-1, 1)),\n                        dtype=torch.get_default_dtype())\n\n\ndef get_checkpoint(S, stop_conds, rng=None, get_state=True):\n    """"""\n    Save the necessary information into a dictionary\n    """"""\n\n    m = {}\n    m[\'ninitfeats\'] = S.ninitfeats\n    m[\'x0\'] = S.x0\n    x = S.x.clone().cpu().detach()\n    m[\'feats\'] = np.where(x.numpy() >= 0)[0]\n    m.update({k: v[0] for k, v in stop_conds.items()})\n    if get_state:\n        m.update({constants.Checkpoint.MODEL: S.state_dict(),\n                  constants.Checkpoint.OPT: S.opt_train.state_dict(),\n                  constants.Checkpoint.RNG: torch.get_rng_state(),\n                  })\n    if rng:\n        m.update({\'rng_state\': rng.get_state()})\n\n    return m\n\n\ndef _train(data_train, Nminibatch, order, C, rng, lr_train, debug, maxiter,\n           maxtime, init, dftol_stop, freltol_stop, dn_log, accum_steps,\n           path_save, shuffle, device=constants.Device.CPU,\n           verbose=1,\n           prev_checkpoint=None,\n           groups=None,\n           soft_groups=None):\n    """"""\n    Main training loop.\n    """"""\n\n    t_init = time.time()\n\n    x0 = get_init(data_train, init, rng)\n    if isinstance(init, str) and init == constants.Initialization.ZERO:\n        ninitfeats = -1\n    else:\n        ninitfeats = np.where(x0.detach().numpy() > 0)[0].size\n\n    S = Solver(data_train, order,\n               Nminibatch=Nminibatch, x0=x0, C=C,\n               ftransform=lambda x: torch.sigmoid(2 * x),\n               get_train_opt=lambda p: torch.optim.Adam(p, lr_train),\n               rng=rng,\n               accum_steps=accum_steps,\n               shuffle=shuffle,\n               groups=groups,\n               soft_groups=soft_groups,\n               device=device,\n               verbose=verbose)\n    S = S.to(device)\n\n    S.ninitfeats = ninitfeats\n    S.x0 = x0\n\n    if prev_checkpoint:\n        S.load_state_dict(prev_checkpoint[constants.Checkpoint.MODEL])\n        S.opt_train.load_state_dict(prev_checkpoint[constants.Checkpoint.OPT])\n        torch.set_rng_state(prev_checkpoint[constants.Checkpoint.RNG])\n\n    minibatch = S.Ntrain != S.Nminibatch\n\n    f_stop, stop_conds = get_optim_f_stop(maxiter, maxtime, dftol_stop,\n                                          freltol_stop, minibatch=minibatch)\n\n    if debug:\n        pass\n    else:\n        f_callback = None\n    stop_conds[\'t\'][-1] = time.time() - t_init\n\n    S.train(f_stop=f_stop, f_callback=f_callback)\n\n    return get_checkpoint(S, stop_conds, rng), S\n\n\ndef train_sk_dense(ty, X, y, classification):\n    if classification:\n        if ty.startswith(\'skf\'):\n            d = int(ty.replace(\'skf\', \'\'))\n            f_sk = f_classif\n        elif ty.startswith(\'skmi\'):\n            d = int(ty.replace(\'skmi\', \'\'))\n            f_sk = mutual_info_classif\n    else:\n        if ty.startswith(\'skf\'):\n            d = int(ty.replace(\'skf\', \'\'))\n            f_sk = f_regression\n        elif ty.startswith(\'skmi\'):\n            d = int(ty.replace(\'skmi\', \'\'))\n            f_sk = mutual_info_regression\n    t = time.time()\n    clf = SelectKBest(f_sk, k=d)\n    clf.fit_transform(X, y.squeeze())\n    ix = np.argsort(-clf.scores_)\n    ix = ix[np.where(np.invert(np.isnan(clf.scores_[ix])))[0]][:d]\n    t = time.time() - t\n    return {\'feats\': ix, \'t\': t}\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/gradient_selector.py,8,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\nimport time\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.base import BaseEstimator\nfrom sklearn.feature_selection.base import SelectorMixin\nfrom sklearn.utils.validation import check_is_fitted\n\nimport torch\n\nfrom nni.feature_engineering.feature_selector import FeatureSelector\nfrom . import constants\nfrom .fginitialize import PrepareData\nfrom .fgtrain import _train\n\n\nclass FeatureGradientSelector(FeatureSelector, BaseEstimator, SelectorMixin):\n    def __init__(self,\n                 order=4,\n                 penalty=1,\n                 n_features=None,\n                 max_features=None,\n                 learning_rate=1e-1,\n                 init=\'zero\',\n                 n_epochs=1,\n                 shuffle=True,\n                 batch_size=1000,\n                 target_batch_size=1000,\n                 max_time=np.inf,\n                 classification=True,\n                 ordinal=False,\n                 balanced=True,\n                 preprocess=\'zscore\',\n                 soft_grouping=False,\n                 verbose=0,\n                 device=\'cpu\'):\n        """"""\n            FeatureGradientSelector is a class that selects features for a machine\n            learning model using a gradient based search.\n\n            Parameters\n            ----------\n            order : int\n                What order of interactions to include. Higher orders\n                may be more accurate but increase the run time. 12 is the maximum allowed order.\n            penatly : int\n                Constant that multiplies the regularization term.\n            n_features: int\n                If None, will automatically choose number of features based on search.\n                Otherwise, number of top features to select.\n            max_features : int\n                If not None, will use the \'elbow method\' to determine the number of features\n                with max_features as the upper limit.\n            learning_rate : float\n            init : str\n                How to initialize the vector of scores. \'zero\' is the default.\n                Options: {\'zero\', \'on\', \'off\', \'onhigh\', \'offhigh\', \'sklearn\'}\n            n_epochs : int\n                number of epochs to run\n            shuffle : bool\n                Shuffle ""rows"" prior to an epoch.\n            batch_size : int\n                Nnumber of ""rows"" to process at a time\n            target_batch_size : int\n                Number of ""rows"" to accumulate gradients over.\n                Useful when many rows will not fit into memory but are needed for accurate estimation.\n            classification : bool\n                If True, problem is classification, else regression.\n            ordinal : bool\n                If True, problem is ordinal classification. Requires classification to be True.\n            balanced : bool\n                If true, each class is weighted equally in optimization, otherwise\n                weighted is done via support of each class. Requires classification to be True.\n            prerocess : str\n                \'zscore\' which refers to centering and normalizing data to unit variance or\n                \'center\' which only centers the data to 0 mean\n            soft_grouping : bool\n                if True, groups represent features that come from the same source.\n                Used to encourage sparsity of groups and features within groups.\n            verbose : int\n                Controls the verbosity when fitting. Set to 0 for no printing\n                1 or higher for printing every verbose number of gradient steps.\n            device : str\n                \'cpu\' to run on CPU and \'cuda\' to run on GPU. Runs much faster on GPU\n        """"""\n        assert order <= 12 and order >= 1, \'order must be an integer between 1 and 12, inclusive\'\n        assert n_features is None or max_features is None, \\\n            \'only specify one of n_features and max_features at a time\'\n\n        self.order = order\n        self.penalty = penalty\n        self.n_features = n_features\n        self.max_features = max_features\n        self.learning_rate = learning_rate\n        self.init = init\n        self.n_epochs = n_epochs\n        self.shuffle = shuffle\n        self.batch_size = batch_size\n        self.target_batch_size = target_batch_size\n        self.max_time = max_time\n        self.dftol_stop = -1\n        self.freltol_stop = -1\n        self.classification = classification\n        self.ordinal = ordinal\n        self.balanced = balanced\n        self.preprocess = preprocess\n        self.soft_grouping = soft_grouping\n        self.verbose = verbose\n        self.device = device\n\n        self.model_ = None\n        self.scores_ = None\n        self._prev_checkpoint = None\n        self._data_train = None\n\n    def partial_fit(self, X, y,\n                    n_classes=None,\n                    groups=None):\n        """"""\n        Select Features via a gradient based search on (X, y) on the given samples.\n        Can be called repeatedly with different X and y to handle streaming datasets.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y :  array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        n_classes : int\n            Number of classes\n            Classes across all calls to partial_fit.\n            Can be obtained by via `np.unique(y_all).shape[0]`, where y_all is the\n            target vector of the entire dataset.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n            Note that y doesn\'t need to contain all labels in `classes`.\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n            This argument is expected for the first call to partial_fit,\n            otherwise will assume all classes are present in the batch of y given.\n            It will be ignored in the subsequent calls.\n        """"""\n        try:\n            self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n        except constants.NanError:\n            if hasattr(self, \'_prev_checkpoint\'):\n                # if it\'s already done some batches successfully just ignore it\n                print(\'failed fitting this batch, loss was nan\')\n            else:\n                # if this is the first batch, reset and try with doubles\n                if self.verbose:\n                    print(\'Loss was nan, trying with Doubles\')\n                self._reset()\n                torch.set_default_tensor_type(torch.DoubleTensor)\n                self._partial_fit(X, y, n_classes=n_classes, groups=groups)\n\n        return self\n\n    def _partial_fit(self, X, y, n_classes=None, groups=None):\n        """"""\n        Private function for partial_fit to enable trying floats before doubles.\n        """"""\n        # pass in X and y in chunks\n        if hasattr(self, \'_data_train\'):\n            # just overwrite the X and y from the new chunk but make them tensors\n            # keep dataset stats from previous\n            self._data_train.X = X.values if isinstance(X, pd.DataFrame) else X\n            self._data_train.N, self._data_train.D = self._data_train.X.shape\n            self._data_train.dense_size_gb = self._data_train.get_dense_size()\n            self._data_train.set_dense_X()\n\n            self._data_train.y = y.values if isinstance(y, pd.Series) else y\n            self._data_train.y = torch.as_tensor(\n                y, dtype=torch.get_default_dtype())\n        else:\n            data_train = self._prepare_data(X, y, n_classes=n_classes)\n            self._data_train = data_train\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            self._data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and do other stuff?\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(self._data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           prev_checkpoint=self._prev_checkpoint if hasattr(\n                               self, \'_prev_checkpoint\') else None,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._prev_checkpoint = m\n        self._process_results(m, solver, X, groups=groups)\n        return self\n\n    def fit(self, X, y,\n            groups=None):\n        """"""\n        Select Features via a gradient based search on (X, y).\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        """"""\n        try:\n            self._fit(X, y, groups=groups)\n        except constants.NanError:\n            if self.verbose:\n                print(\'Loss was nan, trying with Doubles\')\n            torch.set_default_tensor_type(torch.DoubleTensor)\n            self._fit(X, y, groups=groups)\n        return self\n\n    def get_selected_features(self):\n        return self.selected_features_\n\n    def _prepare_data(self, X, y, n_classes=None):\n        """"""\n        Returns a PrepareData object.\n        """"""\n        return PrepareData(X=X.values if isinstance(X, pd.DataFrame) else X,\n                           y=y.values if isinstance(y, pd.Series) else y,\n                           data_format=constants.DataFormat.NUMPY,\n                           classification=int(self.classification),\n                           ordinal=self.ordinal,\n                           balanced=self.balanced,\n                           preprocess=self.preprocess,\n                           verbose=self.verbose,\n                           device=self.device,\n                           n_classes=n_classes)\n\n    def _fit(self, X, y, groups=None):\n        """"""\n        Private function for fit to enable trying floats before doubles.\n        """"""\n        data_train = self._prepare_data(X, y)\n\n        batch_size, _, accum_steps, max_iter = self._set_batch_size(\n            data_train)\n\n        rng = None  # not used\n        debug = 0  # {0,1} print messages and log to tensorboard\n        dn_logs = None  # tensorboard logs; only specify if debug=1\n        path_save = None  # intermediate models saves; only specify if debug=1\n        m, solver = _train(data_train,\n                           batch_size,\n                           self.order,\n                           self.penalty,\n                           rng,\n                           self.learning_rate,\n                           debug,\n                           max_iter,\n                           self.max_time,\n                           self.init,\n                           self.dftol_stop,\n                           self.freltol_stop,\n                           dn_logs,\n                           accum_steps,\n                           path_save,\n                           self.shuffle,\n                           device=self.device,\n                           verbose=self.verbose,\n                           groups=groups if not self.soft_grouping else None,\n                           soft_groups=groups if self.soft_grouping else None)\n\n        self._process_results(m, solver, X, groups=groups)\n        return self\n\n    def _process_torch_scores(self, scores):\n        """"""\n        Convert scores into flat numpy arrays.\n        """"""\n        if constants.Device.CUDA in scores.device.type:\n            scores = scores.cpu()\n        return scores.numpy().ravel()\n\n    def _set_batch_size(self, data_train):\n        """"""\n        Ensures that batch_size is less than the number of rows.\n        """"""\n        batch_size = min(self.batch_size, data_train.N)\n        target_batch_size = min(max(\n            self.batch_size, self.target_batch_size), data_train.N)\n        accum_steps = max(int(np.ceil(target_batch_size / self.batch_size)), 1)\n        max_iter = self.n_epochs * (data_train.N // batch_size)\n        return batch_size, target_batch_size, accum_steps, max_iter\n\n    def _process_results(self, m, solver, X, groups=None):\n        """"""\n        Process the results of a run into something suitable for transform().\n        """"""\n        self.scores_ = self._process_torch_scores(\n            torch.sigmoid(m[constants.Checkpoint.MODEL][\'x\'] * 2))\n        if self.max_features:\n            self.max_features = min([self.max_features, self.scores_.shape[0]])\n            n_features = self._recommend_number_features(solver)\n            self.set_n_features(n_features, groups=groups)\n        elif self.n_features:\n            self.set_n_features(self.n_features, groups=groups)\n        else:\n            self.selected_features_ = m[\'feats\']\n\n        # subtract elapsed time from max_time\n        self.max_time -= m[\'t\']\n\n        self.model_ = m\n\n        return self\n\n    def transform(self, X):\n        """"""\n        Returns selected features from X.\n\n        Paramters\n        ---------\n        X: array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        """"""\n\n        self._get_support_mask()\n        if self.selected_features_.shape[0] == 0:\n            raise ValueError(\n                \'No Features selected, consider lowering the penalty or specifying n_features\')\n        return (X.iloc[:, self.selected_features_]\n                if isinstance(X, pd.DataFrame)\n                else X[:, self.selected_features_])\n\n    def get_support(self, indices=False):\n        """"""\n        Get a mask, or integer index, of the features selected.\n\n        Parameters\n        ----------\n        indices : bool\n            Default False\n            If True, the return value will be an array of integers, rather than a boolean mask.\n\n        Returns\n        -------\n        list :\n            returns support: An index that selects the retained features from a feature vector.\n            If indices is False, this is a boolean array of shape [# input features],\n            in which an element is True iff its corresponding feature is selected for retention.\n            If indices is True, this is an integer array of shape [# output features] whose values\n            are indices into the input feature vector.\n        """"""\n        self._get_support_mask()\n        if indices:\n            return self.selected_features_\n\n        mask = np.zeros_like(self.scores_, dtype=bool)\n        # pylint: disable=E1137\n        mask[self.selected_features_] = True\n        return mask\n\n    def inverse_transform(self, X):\n        """"""\n        Returns transformed X to the original number of column.\n        This operation is lossy and all columns not in the transformed data\n        will be returned as columns of 0s.\n        """"""\n        self._get_support_mask()\n        X_new = np.zeros((X.shape[0], self.scores_.shape[0]))\n        X_new[self.selected_features_] = X\n        return X_new\n\n    def get_params(self, deep=True):\n        """"""\n        Get parameters for this estimator.\n        """"""\n        params = self.__dict__\n        params = {key: val for (key, val) in params.items()\n                  if not key.endswith(\'_\')}\n        return params\n\n    def set_params(self, **params):\n        """"""\n        Set the parameters of this estimator.\n        """"""\n        for param in params:\n            if hasattr(self, param):\n                setattr(self, param, params[param])\n        return self\n\n    def fit_transform(self, X, y):\n        """"""\n        Select features and then return X with the selected features.\n\n        Parameters\n        ----------\n        X : array-like\n            Shape = [n_samples, n_features]\n            The training input samples.\n        y : array-like\n            Shape = [n_samples]\n            The target values (class labels in classification, real numbers in\n            regression).\n        """"""\n        self.fit(X, y)\n        return self.transform(X)\n\n    def _get_support_mask(self):\n        """"""\n        Check if it is fitted.\n        """"""\n        check_is_fitted(self, \'scores_\')\n\n    def _generate_scores(self, solver, xsub, ysub, step_size, feature_order):\n        """"""\n        Generate forward passes to determine the number of features when max_features is set.\n        """"""\n        scores = []\n        for i in np.arange(1, self.max_features + 1, step_size):\n            # optimization possible since xsub is growing?\n            i = int(np.ceil(i))\n            # pylint: disable=E1102\n            score = solver.f_train(torch.tensor(np.ones(i),\n                                                dtype=torch.get_default_dtype()\n                                                ).unsqueeze(1).to(self.device),\n                                   xsub[:, feature_order[:i]],\n                                   ysub)\n            if constants.Device.CUDA in score.device.type:\n                score = score.cpu()\n            # score.numpy()[0][0]\n            scores.append(score)\n        return scores\n\n    def set_n_features(self, n, groups=None):\n        """"""\n        Set the number of features to return after fitting.\n        """"""\n        self._get_support_mask()\n        self.n_features = n\n        return self._set_top_features(groups=groups)\n\n    def _set_top_features(self, groups=None):\n        """"""\n        Set the selected features after a run.\n\n        With groups, ensures that if any member of a group is selected, all members are selected\n        """"""\n        self._get_support_mask()\n        assert self.n_features <= self.scores_.shape[0], \\\n            \'n_features must be less than or equal to the number of columns in X\'\n        # pylint: disable=E1130\n        self.selected_features_ = np.argpartition(\n            self.scores_, -self.n_features)[-self.n_features:]\n        if groups is not None and not self.soft_grouping:\n            selected_feature_set = set(self.selected_features_.tolist())\n            for _ in np.unique(groups):\n                group_members = np.where(groups == groups)[0].tolist()\n                if selected_feature_set.intersection(group_members):\n                    selected_feature_set.update(group_members)\n            self.selected_features_ = np.array(list(selected_feature_set))\n        self.selected_features_ = np.sort(self.selected_features_)\n        return self\n\n    def set_top_percentile(self, percentile, groups=None):\n        """"""\n        Set the percentile of features to return after fitting.\n        """"""\n        self._get_support_mask()\n        assert percentile <= 1 and percentile >= 0, \\\n            \'percentile must between 0 and 1 inclusive\'\n        self.n_features = int(self.scores_.shape[0] * percentile)\n        return self._set_top_features(groups=groups)\n\n    def _recommend_number_features(self, solver, max_time=None):\n        """"""\n        Get the recommended number of features by doing forward passes when max_features is set.\n        """"""\n        max_time = max_time if max_time else self.max_time\n        if max_time < 0:\n            max_time = 60  # allow 1 minute extra if we already spent max_time\n        MAX_FORWARD_PASS = 200\n        MAX_FULL_BATCHES = 3  # the forward passes can take longer than the fitting\n        # if we allow a full epoch of data to be included. By only doing 3 full batches at most\n        # we get enough accuracy without increasing the time too much. This\n        # constant may not be optimal\n        accum_steps = solver.accum_steps\n        step_size = max(self.max_features / MAX_FORWARD_PASS, 1)\n        # pylint: disable=E1130\n        feature_order = np.argsort(-self.scores_)  # note the negative\n        t = time.time()\n\n        dataloader_iterator = iter(solver.ds_train)\n        full_scores = []\n        # keep_going = True\n        with torch.no_grad():\n            # might want to only consider a batch valid if there are at least\n            # two classes\n            for _ in range(accum_steps * MAX_FULL_BATCHES):\n                scores = []\n                try:\n                    xsub, ysub = next(dataloader_iterator)\n                except StopIteration:\n                    # done with epoch, don\'t do more than one epoch\n                    break\n                except Exception as e:\n                    print(e)\n                    break\n                if max_time and time.time() - t > max_time:\n                    if self.verbose:\n                        print(\n                            ""Stoppinn forward passes because they reached max_time: "",\n                            max_time)\n                    if not full_scores:\n                        # no forward passes worked, return half of max_features\n                        return self.max_features // 2\n                    break\n                if solver.multiclass:\n                    for target_class in range(solver.n_classes):\n                        ysub_binary = solver.transform_y_into_binary(\n                            ysub, target_class)\n                        scaling_value = solver._get_scaling_value(\n                            ysub, target_class)\n                        if not solver._skip_y_forward(ysub_binary):\n                            scores = self._generate_scores(\n                                solver, xsub, ysub_binary, step_size, feature_order)\n                            # one row will represent one class that is present in the data\n                            # all classes are weighted equally\n                            full_scores.append(\n                                [score * scaling_value for score in scores])\n                else:\n                    if not solver._skip_y_forward(ysub):\n                        scores = self._generate_scores(\n                            solver, xsub, ysub, step_size, feature_order)\n                        full_scores.append(scores)\n        best_index = FeatureGradientSelector._find_best_index_elbow(\n            full_scores)\n        if self.verbose:\n            print(""Forward passes took: "", time.time() - t)\n        # account for step size and off by one (n_features is 1 indexed, not 0\n        # )\n        return int(\n            np.ceil(\n                np.arange(\n                    1,\n                    self.max_features +\n                    1,\n                    step_size))[best_index])\n\n    @staticmethod\n    def _find_best_index_elbow(full_scores):\n        """"""\n        Finds the point on the curve that maximizes distance from the line determined by the endpoints.\n        """"""\n        scores = pd.DataFrame(full_scores).mean(0).values.tolist()\n        first_point = np.array([0, scores[0]])\n        last_point = np.array([len(scores) - 1, scores[-1]])\n        elbow_metric = []\n        for i in range(len(scores)):\n            elbow_metric.append(\n                FeatureGradientSelector._distance_to_line(\n                    first_point, last_point, np.array([i, scores[i]])))\n        return np.argmax(elbow_metric)\n\n    @staticmethod\n    def _distance_to_line(start_point, end_point, new_point):\n        """"""\n        Calculates the shortest distance from new_point to the line determined by start_point and end_point.\n        """"""\n        # for calculating elbow method\n        return np.cross(new_point - start_point,\n                        end_point - start_point) / np.linalg.norm(\n                            end_point - start_point)\n\n    def _reset(self):\n        """"""\n        Reset the estimator by deleting all private and fit parameters.\n        """"""\n        params = self.__dict__\n        for key, _ in params.items():\n            if key.endswith(\'_\') or key.startswith(\'_\'):\n                delattr(self, key)\n        return self\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/learnability.py,46,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\nimport time\n\nimport numpy as np\nimport scipy.special\nimport torch\nimport torch.nn as nn\n\nfrom . import constants\nfrom . import syssettings\nfrom .fginitialize import ChunkDataLoader\n\ntorch.set_default_tensor_type(syssettings.torch.tensortype)\nsparsetensor = syssettings.torch.sparse.tensortype\n\n\ndef def_train_opt(p):\n    """"""\n    Return the default optimizer.\n    """"""\n    return torch.optim.Adam(p, 1e-1, amsgrad=False)\n\n\ndef revcumsum(U):\n    """"""\n    Reverse cumulative sum for faster performance.\n    """"""\n    return U.flip(dims=[0]).cumsum(dim=0).flip(dims=[0])\n\n\ndef triudr(X, r):\n\n    Zr = torch.zeros_like(X, requires_grad=False)\n    U = X * r\n    Zr[:-1] = X[:-1] * revcumsum(U)[1:]\n\n    return Zr\n\n\ndef triudl(X, l):\n\n    Zl = torch.zeros_like(X, requires_grad=False)\n    U = X * l\n    Zl[1:] = X[1:] * (U.cumsum(dim=0)[:-1])\n\n    return Zl\n\n\nclass ramp(torch.autograd.Function):\n    """"""\n    Ensures input is between 0 and 1\n    """"""\n\n    @staticmethod\n    def forward(ctx, input_data):\n        ctx.save_for_backward(input_data)\n        return input_data.clamp(min=0, max=1)\n\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        input_data, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input_data < 0] = 1e-2\n        grad_input[input_data > 1] = -1e-2\n        return grad_input\n\n\nclass safesqrt(torch.autograd.Function):\n    """"""\n    Square root without dividing by 0.\n    """"""\n    @staticmethod\n    def forward(ctx, input_data):\n        o = input_data.sqrt()\n        ctx.save_for_backward(input_data, o)\n        return o\n\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        _, o = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input *= 0.5 / (o + constants.EPSILON)\n        return grad_input\n\n\nclass LearnabilityMB(nn.Module):\n    """"""\n    Calculates the learnability of a set of features.\n    mini-batch version w/ ""left"" and ""right"" multiplies\n    """"""\n\n\n    def __init__(self, Nminibatch, D, coeff, groups=None, binary=False,\n                 device=constants.Device.CPU):\n        super(LearnabilityMB, self).__init__()\n\n        a = coeff / scipy.special.binom(Nminibatch, np.arange(coeff.size) + 2)\n        self.order = a.size\n        # pylint: disable=E1102\n        self.a = torch.tensor(a, dtype=torch.get_default_dtype(), requires_grad=False)\n        self.binary = binary\n\n        self.a = self.a.to(device)\n\n\n    def ret_val(self, z):\n        """"""\n        Get the return value based on z.\n        """"""\n\n        if not self.binary:\n            return 1 - z\n\n        else:\n            return 0.5 * (1 - safesqrt.apply(ramp.apply(z)))\n\n\n    def forward(self, s, X, y):\n\n        l = y.clone()\n        r = y.clone()\n        z = 0\n\n        for i in range(self.order):\n            if i % 2 == 0:\n                Z = triudr(X, r)\n                r = torch.mm(Z, s)\n            else:\n                Z = triudl(X, l)\n                l = torch.mm(Z, s)\n            if self.a[i] != 0:\n                # same the computation if a[i] is 0\n                p = torch.mm(l.t(), r)\n                z += self.a[i] * p\n        return self.ret_val(z)\n\n\nclass Solver(nn.Module):\n    """"""\n    Class that performs the main optimization.\n    Keeps track of the current x and iterates through data to learn x given the penalty and order.\n    """"""\n\n    def __init__(self,\n                 PreparedData,\n                 order,\n                 Nminibatch=None,\n                 groups=None,\n                 soft_groups=None,\n                 x0=None,\n                 C=1,\n                 ftransform=torch.sigmoid,\n                 get_train_opt=def_train_opt,\n                 accum_steps=1,\n                 rng=np.random.RandomState(0),\n                 max_norm_clip=1.,\n                 shuffle=True,\n                 device=constants.Device.CPU,\n                 verbose=1):\n        """"""\n\n        Parameters\n        ----------\n        PreparedData : Dataset of PrepareData class\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        Nminibatch : int\n            Number of rows in a mini batch\n        groups : array-like\n            Optional, shape = [n_features]\n            Groups of columns that must be selected as a unit\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        soft_groups : array-like\n            optional, shape = [n_features]\n            Groups of columns come from the same source\n            Used to encourage sparsity of number of sources selected\n            e.g. [0, 0, 1, 2] specifies the first two columns are part of a group.\n        x0 : torch.tensor\n            Optional, initialization of x.\n        C : float\n            Penalty parameter.\n        get_train_opt : function\n            Function that returns a pytorch optimizer, Adam is the default\n        accum_steps : int\n            Number of steps\n        rng : random state\n        max_norm_clip : float\n            Maximum allowable size of the gradient\n        shuffle : bool\n            Whether or not to shuffle data within the dataloader\n        order : int\n            What order of interactions to include. Higher orders\n            may be more accurate but increase the run time. 12 is the maximum allowed order.\n        penalty : int\n            Constant that multiplies the regularization term.\n        ftransform : function\n            Function to transform the x. sigmoid is the default.\n        device : str\n            \'cpu\' to run on CPU and \'cuda\' to run on GPU. Runs much faster on GPU\n        verbose : int\n            Controls the verbosity when fitting. Set to 0 for no printing\n            1 or higher for printing every verbose number of gradient steps.\n        """"""\n        super(Solver, self).__init__()\n\n        self.Ntrain, self.D = PreparedData.N, PreparedData.n_features\n        if groups is not None:\n            # pylint: disable=E1102\n            groups = torch.tensor(groups, dtype=torch.long)\n            self.groups = groups\n        else:\n            self.groups = None\n        if soft_groups is not None:\n            # pylint: disable=E1102\n            soft_groups = torch.tensor(soft_groups, dtype=torch.long)\n            self.soft_D = torch.unique(soft_groups).size()[0]\n        else:\n            self.soft_D = None\n        self.soft_groups = soft_groups\n\n        if Nminibatch is None:\n            Nminibatch = self.Ntrain\n        else:\n            if Nminibatch > self.Ntrain:\n                print(\'Minibatch larger than sample size.\'\n                      + (\' Reducing from %d to %d.\'\n                         % (Nminibatch, self.Ntrain)))\n                Nminibatch = self.Ntrain\n        if Nminibatch > PreparedData.max_rows:\n            print(\'Minibatch larger than mem-allowed.\'\n                  + (\' Reducing from %d to %d.\' % (Nminibatch,\n                                                   PreparedData.max_rows)))\n            Nminibatch = int(np.min([Nminibatch, PreparedData.max_rows]))\n        self.Nminibatch = Nminibatch\n        self.accum_steps = accum_steps\n\n        if x0 is None:\n            x0 = torch.zeros(self.D, 1, dtype=torch.get_default_dtype())\n        self.ftransform = ftransform\n        self.x = nn.Parameter(x0)\n        self.max_norm = max_norm_clip\n\n        self.device = device\n        self.verbose = verbose\n\n        self.multiclass = PreparedData.classification and PreparedData.n_classes and PreparedData.n_classes > 2\n        if self.multiclass:\n            self.n_classes = PreparedData.n_classes\n        else:\n            self.n_classes = None\n        # whether to treat all classes equally\n        self.balanced = PreparedData.balanced\n        self.ordinal = PreparedData.ordinal\n\n        if (hasattr(PreparedData, \'mappings\')\n                or PreparedData.storage_level == \'disk\'):\n            num_workers = PreparedData.num_workers\n        elif PreparedData.storage_level == constants.StorageLevel.DENSE:\n            num_workers = 0\n        else:\n            num_workers = 0\n\n        if constants.Device.CUDA in device:\n            pin_memory = False\n        else:\n            pin_memory = False\n\n        self.ds_train = ChunkDataLoader(\n            PreparedData,\n            batch_size=self.Nminibatch,\n            shuffle=shuffle,\n            drop_last=True,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            timeout=60)\n        self.f_train = LearnabilityMB(self.Nminibatch, self.D,\n                                      constants.Coefficients.SLE[order],\n                                      self.groups,\n                                      binary=PreparedData.classification,\n                                      device=self.device)\n        self.opt_train = get_train_opt(torch.nn.ParameterList([self.x]))\n        self.it = 0\n        self.iters_per_epoch = int(np.ceil(len(self.ds_train.dataset)\n                                           / self.ds_train.batch_size))\n        self.f_train = self.f_train.to(device)\n        # pylint: disable=E1102\n        self.w = torch.tensor(\n            C / (C + 1),\n            dtype=torch.get_default_dtype(), requires_grad=False)\n        self.w = self.w.to(device)\n\n\n    def penalty(self, s):\n        """"""\n        Calculate L1 Penalty.\n        """"""\n        to_return = torch.sum(s) / self.D\n        if self.soft_groups is not None:\n            # if soft_groups, there is an additional penalty for using more\n            # groups\n            s_grouped = torch.zeros(self.soft_D, 1,\n                                    dtype=torch.get_default_dtype(),\n                                    device=self.device)\n            for group in torch.unique(self.soft_groups):\n                # groups should be indexed 0 to n_group - 1\n                # TODO: consider other functions here\n                s_grouped[group] = s[self.soft_groups == group].max()\n            # each component of the penalty contributes .5\n            # TODO: could make this a user given parameter\n            to_return = (to_return + torch.sum(s_grouped) / self.soft_D) * .5\n        return to_return\n\n\n    def forward_and_backward(self, s, xsub, ysub, retain_graph=False):\n        """"""\n        Completes the forward operation and computes gradients for learnability and penalty.\n        """"""\n        f_train = self.f_train(s, xsub, ysub)\n        pen = self.penalty(s)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g1, = torch.autograd.grad([f_train], [self.x], grad_outputs,\n                                  retain_graph=True)\n        # pylint: disable=E1102\n        grad_outputs = torch.tensor([[1]], dtype=torch.get_default_dtype(),\n                                    device=self.device)\n        g2, = torch.autograd.grad([pen], [self.x], grad_outputs,\n                                  retain_graph=retain_graph)\n        return f_train, pen, g1, g2\n\n\n    def combine_gradient(self, g1, g2):\n        """"""\n        Combine gradients from learnability and penalty\n\n        Parameters\n        ----------\n        g1 : array-like\n            gradient from learnability\n        g2 : array-like\n            gradient from penalty\n        """"""\n        to_return = ((1 - self.w) * g1 + self.w * g2) / self.accum_steps\n        if self.groups is not None:\n            # each column will get a gradient\n            # but we can only up or down groups, so the gradient for the group\n            # should be the average of the gradients of the columns\n            to_return_grouped = torch.zeros_like(self.x)\n            for group in torch.unique(self.groups):\n                to_return_grouped[self.groups ==\n                                  group] = to_return[self.groups == group].mean()\n            to_return = to_return_grouped\n        return to_return\n\n\n    def combine_loss(self, f_train, pen):\n        """"""\n        Combine the learnability and L1 penalty.\n        """"""\n        return ((1 - self.w) * f_train.detach() + self.w * pen.detach()) \\\n            / self.accum_steps\n\n\n    def transform_y_into_binary(self, ysub, target_class):\n        """"""\n        Transforms multiclass classification problems into a binary classification problem.\n        """"""\n        with torch.no_grad():\n            ysub_binary = torch.zeros_like(ysub)\n            if self.ordinal:\n                # turn ordinal problems into n-1 classifications of is this\n                # example less than rank k\n                if target_class == 0:\n                    return None\n\n                ysub_binary[ysub >= target_class] = 1\n                ysub_binary[ysub < target_class] = -1\n            else:\n                # turn multiclass problems into n binary classifications\n                ysub_binary[ysub == target_class] = 1\n                ysub_binary[ysub != target_class] = -1\n        return ysub_binary\n\n\n    def _get_scaling_value(self, ysub, target_class):\n        """"""\n        Returns the weight given to a class for multiclass classification.\n        """"""\n        if self.balanced:\n            if self.ordinal:\n                return 1 / (torch.unique(ysub).size()[0] - 1)\n\n            return 1 / torch.unique(ysub).size()[0]\n        else:\n            if self.ordinal:\n                this_class_proportion = torch.mean(ysub >= target_class)\n                normalizing_constant = 0\n                for i in range(1, self.n_classes):\n                    normalizing_constant += torch.mean(ysub >= i)\n                return this_class_proportion / normalizing_constant\n            else:\n                return torch.mean(ysub == target_class)\n\n\n    def _skip_y_forward(self, y):\n        """"""\n        Returns boolean of whether to skip the currrent y if there is nothing to be learned from it.\n        """"""\n        if y is None:\n            return True\n        elif torch.unique(y).size()[0] < 2:\n            return True\n        else:\n            return False\n\n\n    def train(self, f_callback=None, f_stop=None):\n        """"""\n        Trains the estimator to determine which features to include.\n\n        Parameters\n        ----------\n        f_callback : function\n            Function that performs a callback\n        f_stop: function\n            Function that tells you when to stop\n        """"""\n\n        t = time.time()\n        h = torch.zeros([1, 1], dtype=torch.get_default_dtype())\n        h = h.to(self.device)\n        # h_complete is so when we divide by the number of classes\n        # we only do that for that minibatch if accumulating\n        h_complete = h.clone()\n        flag_stop = False\n        dataloader_iterator = iter(self.ds_train)\n        self.x.grad = torch.zeros_like(self.x)\n        while not flag_stop:\n            try:\n                xsub, ysub = next(dataloader_iterator)\n            except StopIteration:\n                dataloader_iterator = iter(self.ds_train)\n                xsub, ysub = next(dataloader_iterator)\n            try:\n                s = self.ftransform(self.x)\n                s = s.to(self.device)\n                if self.multiclass:\n                    # accumulate gradients over each class, classes range from\n                    # 0 to n_classes - 1\n                    #num_classes_batch = torch.unique(ysub).size()[0]\n                    for target_class in range(self.n_classes):\n                        ysub_binary = self.transform_y_into_binary(\n                            ysub, target_class)\n                        if self._skip_y_forward(ysub_binary):\n                            continue\n                        # should should skip if target class is not included\n                        # but that changes what we divide by\n                        scaling_value = self._get_scaling_value(\n                            ysub, target_class)\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub_binary, retain_graph=True)\n                        self.x.grad += self.combine_gradient(\n                            g1, g2) * scaling_value\n                        h += self.combine_loss(f_train,\n                                               pen) * scaling_value\n                else:\n                    if not self._skip_y_forward(ysub):\n                        f_train, pen, g1, g2 = self.forward_and_backward(\n                            s, xsub, ysub)\n                        self.x.grad += self.combine_gradient(g1, g2)\n                        h += self.combine_loss(f_train, pen)\n                    else:\n                        continue\n                h_complete += h\n                self.it += 1\n                if torch.isnan(h):\n                    raise constants.NanError(\n                        \'Loss is nan, something may be misconfigured\')\n                if self.it % self.accum_steps == 0:\n                    torch.nn.utils.clip_grad_norm_(\n                        torch.nn.ParameterList([self.x]),\n                        max_norm=self.max_norm)\n                    self.opt_train.step()\n\n                    t = time.time() - t\n                    if f_stop is not None:\n                        flag_stop = f_stop(self, h, self.it, t)\n\n                    if f_callback is not None:\n                        f_callback(self, h, self.it, t)\n                    elif self.verbose and (self.it // self.accum_steps) % self.verbose == 0:\n                        epoch = int(self.it / self.iters_per_epoch)\n                        print(\n                            \'[Minibatch: %6d/ Epoch: %3d/ t: %3.3f s] Loss: %0.3f\' %\n                            (self.it, epoch, t, h_complete / self.accum_steps))\n\n                    if flag_stop:\n                        break\n\n                    self.opt_train.zero_grad()\n                    h = 0\n                    h_complete = 0\n                    t = time.time()\n            except KeyboardInterrupt:\n                flag_stop = True\n                break\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/syssettings.py,2,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n\nimport torch\n\n# pytorch\ntorch.tensortype = torch.FloatTensor\ntorch.sparse.tensortype = torch.sparse.FloatTensor\n\n# mem\nMAXMEMGB = 10\n'"
src/sdk/pynni/nni/feature_engineering/gradient_selector/utils.py,0,"b'# Copyright (c) Microsoft Corporation. All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and\n# associated documentation files (the ""Software""), to deal in the Software without restriction,\n# including without limitation the rights to use, copy, modify, merge, publish, distribute,\n# sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all copies or\n# substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT\n# NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT\n# OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n# ==================================================================================================\n\n\nimport numpy as np\n\nclass EMA():\n    """"""\n    maintains an exponential moving average\n    """"""\n\n    def __init__(self, f=np.nan, discount_factor=0.1, valid_after=None,\n                 n_iters_relchange=3):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.gamma = discount_factor\n        self.rel_change = [np.nan]\n        if valid_after is None:\n            self.valid_after = int(1/discount_factor)\n        else:\n            self.valid_after = valid_after\n        self.n_iters_relchange = n_iters_relchange\n        self.initialized = False\n\n    def reset(self, f):\n\n        self.f_ma = [f]\n        self.fs = [f]\n        self.rel_change = [np.nan]\n        self.initialized = True\n\n    def relchange(self):\n\n        if self.num_updates() > np.max([self.valid_after,\n                                        self.n_iters_relchange]):\n            return np.max(self.rel_change[-self.n_iters_relchange:])\n        else:\n            return np.nan\n\n    def update(self, f_new):\n\n        if not self.initialized:\n            self.reset(f_new)\n        else:\n            self.fs.append(f_new)\n            self.f_ma.append(self.f_ma[-1]*(1-self.gamma) + self.gamma*f_new)\n            if self.num_updates() > self.valid_after:\n                self.rel_change.append(np.abs((self.f_ma[-1]-self.f_ma[-2])\n                                              / self.f_ma[-2]))\n\n    def num_updates(self):\n\n        return len(self.f_ma)\n\n    def __call__(self):\n\n        if self.num_updates() > self.valid_after:\n            return self.f_ma[-1]\n        else:\n            return np.nan\n'"
src/sdk/pynni/nni/metis_tuner/Regression_GMM/CreateModel.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nfrom operator import itemgetter\n\nimport sklearn.mixture as mm\n\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n\n\ndef create_model(samples_x, samples_y_aggregation, percentage_goodbatch=0.34):\n    \'\'\'\n    Create the Gaussian Mixture Model\n    \'\'\'\n    samples = [samples_x[i] + [samples_y_aggregation[i]]\n               for i in range(0, len(samples_x))]\n\n    # Sorts so that we can get the top samples\n    samples = sorted(samples, key=itemgetter(-1))\n    samples_goodbatch_size = int(len(samples) * percentage_goodbatch)\n    samples_goodbatch = samples[0:samples_goodbatch_size]\n    samples_badbatch = samples[samples_goodbatch_size:]\n\n    samples_x_goodbatch = [sample_goodbatch[0:-1]\n                           for sample_goodbatch in samples_goodbatch]\n    #samples_y_goodbatch = [sample_goodbatch[-1] for sample_goodbatch in samples_goodbatch]\n    samples_x_badbatch = [sample_badbatch[0:-1]\n                          for sample_badbatch in samples_badbatch]\n\n    # === Trains GMM clustering models === #\n    #sys.stderr.write(""[%s] Train GMM\'s GMM model\\n"" % (os.path.basename(__file__)))\n    bgmm_goodbatch = mm.BayesianGaussianMixture(\n        n_components=max(1, samples_goodbatch_size - 1))\n    bad_n_components = max(1, len(samples_x) - samples_goodbatch_size - 1)\n    bgmm_badbatch = mm.BayesianGaussianMixture(n_components=bad_n_components)\n    bgmm_goodbatch.fit(samples_x_goodbatch)\n    bgmm_badbatch.fit(samples_x_badbatch)\n\n    model = {}\n    model[\'clusteringmodel_good\'] = bgmm_goodbatch\n    model[\'clusteringmodel_bad\'] = bgmm_badbatch\n    return model\n'"
src/sdk/pynni/nni/metis_tuner/Regression_GMM/Selection.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport random\nimport sys\n\nimport nni.metis_tuner.lib_acquisition_function as lib_acquisition_function\nimport nni.metis_tuner.lib_constraint_summation as lib_constraint_summation\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\n\nCONSTRAINT_LOWERBOUND = None\nCONSTRAINT_UPPERBOUND = None\nCONSTRAINT_PARAMS_IDX = []\n\n\ndef _ratio_scores(parameters_value, clusteringmodel_gmm_good,\n                  clusteringmodel_gmm_bad):\n    '''\n    The ratio is smaller the better\n    '''\n    ratio = clusteringmodel_gmm_good.score(\n        [parameters_value]) / clusteringmodel_gmm_bad.score([parameters_value])\n    sigma = 0\n    return ratio, sigma\n\n\ndef selection_r(x_bounds,\n                x_types,\n                clusteringmodel_gmm_good,\n                clusteringmodel_gmm_bad,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    '''\n    Select using different types.\n    '''\n    minimize_starting_points = clusteringmodel_gmm_good.sample(n_samples=num_starting_points)\n\n    outputs = selection(x_bounds, x_types,\n                        clusteringmodel_gmm_good,\n                        clusteringmodel_gmm_bad,\n                        minimize_starting_points[0],\n                        minimize_constraints_fun)\n\n    return outputs\n\n\ndef selection(x_bounds,\n              x_types,\n              clusteringmodel_gmm_good,\n              clusteringmodel_gmm_bad,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    '''\n    Select the lowest mu value\n    '''\n    results = lib_acquisition_function.next_hyperparameter_lowest_mu(\n        _ratio_scores, [clusteringmodel_gmm_good, clusteringmodel_gmm_bad],\n        x_bounds, x_types, minimize_starting_points,\n        minimize_constraints_fun=minimize_constraints_fun)\n\n    return results\n\n\ndef _rand_with_constraints(x_bounds, x_types):\n    '''\n    Random generate the variable with constraints\n    '''\n    outputs = None\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_val_withconstraints = lib_constraint_summation.rand(x_bounds_withconstraints,\n                                                          x_types_withconstraints,\n                                                          CONSTRAINT_LOWERBOUND,\n                                                          CONSTRAINT_UPPERBOUND)\n    if x_val_withconstraints is not None:\n        outputs = [None] * len(x_bounds)\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n        for i, _ in enumerate(outputs):\n            if outputs[i] is None:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs\n\n\ndef _minimize_constraints_fun_summation(x):\n    '''\n    Minimize constraints fun summation\n    '''\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND\n"""
src/sdk/pynni/nni/metis_tuner/Regression_GMM/__init__.py,0,b''
src/sdk/pynni/nni/metis_tuner/Regression_GP/CreateModel.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\nimport numpy\n\nimport sklearn.gaussian_process as gp\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\n\ndef create_model(samples_x, samples_y_aggregation,\n                 n_restarts_optimizer=250, is_white_kernel=False):\n    '''\n    Trains GP regression model\n    '''\n    kernel = gp.kernels.ConstantKernel(constant_value=1,\n                                       constant_value_bounds=(1e-12, 1e12)) * \\\n                                                gp.kernels.Matern(nu=1.5)\n    if is_white_kernel is True:\n        kernel += gp.kernels.WhiteKernel(noise_level=1, noise_level_bounds=(1e-12, 1e12))\n    regressor = gp.GaussianProcessRegressor(kernel=kernel,\n                                            n_restarts_optimizer=n_restarts_optimizer,\n                                            normalize_y=True,\n                                            alpha=1e-10)\n    regressor.fit(numpy.array(samples_x), numpy.array(samples_y_aggregation))\n\n    model = {}\n    model['model'] = regressor\n    model['kernel_prior'] = str(kernel)\n    model['kernel_posterior'] = str(regressor.kernel_)\n    model['model_loglikelihood'] = regressor.log_marginal_likelihood(regressor.kernel_.theta)\n\n    return model\n"""
src/sdk/pynni/nni/metis_tuner/Regression_GP/OutlierDetection.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\n""""""\nOutlierDectection.py\n""""""\n\nimport os\nimport sys\nfrom multiprocessing.dummy import Pool as ThreadPool\n\nimport nni.metis_tuner.Regression_GP.CreateModel as gp_create_model\nimport nni.metis_tuner.Regression_GP.Prediction as gp_prediction\n\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n\n\ndef _outlierDetection_threaded(inputs):\n    """"""\n    Detect the outlier\n    """"""\n    [samples_idx, samples_x, samples_y_aggregation] = inputs\n    sys.stderr.write(""[%s] DEBUG: Evaluating %dth of %d samples\\n""\n                     % (os.path.basename(__file__), samples_idx + 1, len(samples_x)))\n    outlier = None\n\n    # Create a diagnostic regression model which removes the sample that we\n    # want to evaluate\n    diagnostic_regressor_gp = gp_create_model.create_model(\n        samples_x[0:samples_idx] + samples_x[samples_idx + 1:],\n        samples_y_aggregation[0:samples_idx] + samples_y_aggregation[samples_idx + 1:])\n    mu, sigma = gp_prediction.predict(\n        samples_x[samples_idx], diagnostic_regressor_gp[\'model\'])\n\n    # 2.33 is the z-score for 98% confidence level\n    if abs(samples_y_aggregation[samples_idx] - mu) > (2.33 * sigma):\n        outlier = {""samples_idx"": samples_idx,\n                   ""expected_mu"": mu,\n                   ""expected_sigma"": sigma,\n                   ""difference"": abs(samples_y_aggregation[samples_idx] - mu) - (2.33 * sigma)}\n    return outlier\n\n\ndef outlierDetection_threaded(samples_x, samples_y_aggregation):\n    """"""\n    Use Multi-thread to detect the outlier\n    """"""\n    outliers = []\n\n    threads_inputs = [[samples_idx, samples_x, samples_y_aggregation]\n                      for samples_idx in range(0, len(samples_x))]\n    threads_pool = ThreadPool(min(4, len(threads_inputs)))\n    threads_results = threads_pool.map(\n        _outlierDetection_threaded, threads_inputs)\n    threads_pool.close()\n    threads_pool.join()\n\n    for threads_result in threads_results:\n        if threads_result is not None:\n            outliers.append(threads_result)\n        else:\n            print(""Error: threads_result is None."")\n\n    outliers = outliers if outliers else None\n    return outliers\n\n\ndef outlierDetection(samples_x, samples_y_aggregation):\n    outliers = []\n    for samples_idx, _ in enumerate(samples_x):\n        #sys.stderr.write(""[%s] DEBUG: Evaluating %d of %d samples\\n""\n        #  \\ % (os.path.basename(__file__), samples_idx + 1, len(samples_x)))\n        diagnostic_regressor_gp = gp_create_model.create_model(\\\n                                        samples_x[0:samples_idx] + samples_x[samples_idx + 1:],\\\n                                        samples_y_aggregation[0:samples_idx] + samples_y_aggregation[samples_idx + 1:])\n        mu, sigma = gp_prediction.predict(samples_x[samples_idx],\n                                          diagnostic_regressor_gp[\'model\'])\n        # 2.33 is the z-score for 98% confidence level\n        if abs(samples_y_aggregation[samples_idx] - mu) > (2.33 * sigma):\n            outliers.append({""samples_idx"": samples_idx,\n                             ""expected_mu"": mu,\n                             ""expected_sigma"": sigma,\n                             ""difference"": \\\n                                abs(samples_y_aggregation[samples_idx] - mu) - (2.33 * sigma)})\n\n    outliers = outliers if outliers else None\n    return outliers\n'"
src/sdk/pynni/nni/metis_tuner/Regression_GP/Prediction.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport sys\n\nimport numpy\n\nsys.path.insert(1, os.path.join(sys.path[0], '..'))\n\n\ndef predict(parameters_value, regressor_gp):\n    '''\n    Predict by Gaussian Process Model\n    '''\n    parameters_value = numpy.array(parameters_value).reshape(-1, len(parameters_value))\n    mu, sigma = regressor_gp.predict(parameters_value, return_std=True)\n\n    return mu[0], sigma[0]\n"""
src/sdk/pynni/nni/metis_tuner/Regression_GP/Selection.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport os\nimport random\nimport sys\n\nimport nni.metis_tuner.lib_acquisition_function as lib_acquisition_function\nimport nni.metis_tuner.lib_constraint_summation as lib_constraint_summation\nimport nni.metis_tuner.lib_data as lib_data\nimport nni.metis_tuner.Regression_GP.Prediction as gp_prediction\n\nsys.path.insert(1, os.path.join(sys.path[0], \'..\'))\n\nCONSTRAINT_LOWERBOUND = None\nCONSTRAINT_UPPERBOUND = None\nCONSTRAINT_PARAMS_IDX = []\n\n\ndef selection_r(acquisition_function,\n                samples_y_aggregation,\n                x_bounds,\n                x_types,\n                regressor_gp,\n                num_starting_points=100,\n                minimize_constraints_fun=None):\n    \'\'\'\n    Selecte R value\n    \'\'\'\n    minimize_starting_points = [lib_data.rand(x_bounds, x_types) \\\n                                    for i in range(0, num_starting_points)]\n    outputs = selection(acquisition_function, samples_y_aggregation,\n                        x_bounds, x_types, regressor_gp,\n                        minimize_starting_points,\n                        minimize_constraints_fun=minimize_constraints_fun)\n\n    return outputs\n\ndef selection(acquisition_function,\n              samples_y_aggregation,\n              x_bounds, x_types,\n              regressor_gp,\n              minimize_starting_points,\n              minimize_constraints_fun=None):\n    \'\'\'\n    selection\n    \'\'\'\n    outputs = None\n\n    sys.stderr.write(""[%s] Exercise \\""%s\\"" acquisition function\\n"" \\\n                        % (os.path.basename(__file__), acquisition_function))\n\n    if acquisition_function == ""ei"":\n        outputs = lib_acquisition_function.next_hyperparameter_expected_improvement(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types, \\\n                        samples_y_aggregation, minimize_starting_points, \\\n                        minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == ""lc"":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_confidence(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    elif acquisition_function == ""lm"":\n        outputs = lib_acquisition_function.next_hyperparameter_lowest_mu(\\\n                        gp_prediction.predict, [regressor_gp], x_bounds, x_types,\\\n                        minimize_starting_points, minimize_constraints_fun=minimize_constraints_fun)\n    return outputs\n\ndef _rand_with_constraints(x_bounds, x_types):\n    \'\'\'\n    Random generate with constraints\n    \'\'\'\n    outputs = None\n\n    x_bounds_withconstraints = [x_bounds[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_types_withconstraints = [x_types[i] for i in CONSTRAINT_PARAMS_IDX]\n    x_val_withconstraints = lib_constraint_summation.rand(x_bounds_withconstraints,\n                                                          x_types_withconstraints,\n                                                          CONSTRAINT_LOWERBOUND,\n                                                          CONSTRAINT_UPPERBOUND)\n    if x_val_withconstraints is not None:\n        outputs = [None] * len(x_bounds)\n\n        for i, _ in enumerate(CONSTRAINT_PARAMS_IDX):\n            outputs[CONSTRAINT_PARAMS_IDX[i]] = x_val_withconstraints[i]\n\n        for i, _ in enumerate(outputs):\n            if outputs[i] is None:\n                outputs[i] = random.randint(x_bounds[i][0], x_bounds[i][1])\n    return outputs\n\n\ndef _minimize_constraints_fun_summation(x):\n    \'\'\'\n    Minimize the constraints fun summation\n    \'\'\'\n    summation = sum([x[i] for i in CONSTRAINT_PARAMS_IDX])\n    return CONSTRAINT_UPPERBOUND >= summation >= CONSTRAINT_LOWERBOUND\n'"
src/sdk/pynni/nni/metis_tuner/Regression_GP/__init__.py,0,b''
src/sdk/pynni/nni/nas/pytorch/__init__.py,0,b''
src/sdk/pynni/nni/nas/pytorch/base_mutator.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport torch.nn as nn\nfrom nni.nas.pytorch.mutables import Mutable, MutableScope, InputChoice\nfrom nni.nas.pytorch.utils import StructuredMutableTreeNode\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseMutator(nn.Module):\n    """"""\n    A mutator is responsible for mutating a graph by obtaining the search space from the network and implementing\n    callbacks that are called in ``forward`` in mutables.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to apply mutator on.\n    """"""\n\n    def __init__(self, model):\n        super().__init__()\n        self.__dict__[""model""] = model\n        self._structured_mutables = self._parse_search_space(self.model)\n\n    def _parse_search_space(self, module, root=None, prefix="""", memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError(""Cannot have nested search space. Error at {} in {}""\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError(""\'{}\' required by \'{}\' not found in keys that appeared before, and is not NO_KEY.""\n                                               .format(k, module.key))\n            for name, submodule in module._modules.items():\n                if submodule is None:\n                    continue\n                submodule_prefix = prefix + (""."" if prefix else """") + name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo,\n                                         nested_detection=nested_detection)\n        return root\n\n    @property\n    def mutables(self):\n        """"""\n        A generator of all modules inheriting :class:`~nni.nas.pytorch.mutables.Mutable`.\n        Modules are yielded in the order that they are defined in ``__init__``.\n        For mutables with their keys appearing multiple times, only the first one will appear.\n        """"""\n        return self._structured_mutables\n\n    @property\n    def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)\n\n    def forward(self, *inputs):\n        """"""\n        Warnings\n        --------\n        Don\'t call forward of a mutator.\n        """"""\n        raise RuntimeError(""Forward is undefined for mutators."")\n\n    def __setattr__(self, name, value):\n        if name == ""model"":\n            raise AttributeError(""Attribute `model` can be set at most once, and you shouldn\'t use `self.model = model` to ""\n                                 ""include you network, as it will include all parameters in model into the mutator."")\n        return super().__setattr__(name, value)\n\n    def enter_mutable_scope(self, mutable_scope):\n        """"""\n        Callback when forward of a MutableScope is entered.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is entered.\n        """"""\n        pass\n\n    def exit_mutable_scope(self, mutable_scope):\n        """"""\n        Callback when forward of a MutableScope is exited.\n\n        Parameters\n        ----------\n        mutable_scope : MutableScope\n            The mutable scope that is exited.\n        """"""\n        pass\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        """"""\n        Callbacks of forward in LayerChoice.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Module whose forward is called.\n        args : list of torch.Tensor\n            The arguments of its forward function.\n        kwargs : dict\n            The keyword arguments of its forward function.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        """"""\n        raise NotImplementedError\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        """"""\n        Callbacks of forward in InputChoice.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Mutable that is called.\n        tensor_list : list of torch.Tensor\n            The arguments mutable is called with.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output tensor and mask.\n        """"""\n        raise NotImplementedError\n\n    def export(self):\n        """"""\n        Export the data of all decisions. This should output the decisions of all the mutables, so that the whole\n        network can be fully determined with these decisions for further training from scratch.\n\n        Returns\n        -------\n        dict\n            Mappings from mutable keys to decisions.\n        """"""\n        raise NotImplementedError\n'"
src/sdk/pynni/nni/nas/pytorch/base_trainer.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom abc import ABC, abstractmethod\n\n\nclass BaseTrainer(ABC):\n\n    @abstractmethod\n    def train(self):\n        """"""\n        Override the method to train.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def validate(self):\n        """"""\n        Override the method to validate.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def export(self, file):\n        """"""\n        Override the method to export to file.\n\n        Parameters\n        ----------\n        file : str\n            File path to export to.\n        """"""\n        raise NotImplementedError\n\n    @abstractmethod\n    def checkpoint(self):\n        """"""\n        Override to dump a checkpoint.\n        """"""\n        raise NotImplementedError\n'"
src/sdk/pynni/nni/nas/pytorch/callbacks.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport os\n\nimport torch\nimport torch.nn as nn\n\n_logger = logging.getLogger(__name__)\n\n\nclass Callback:\n    """"""\n    Callback provides an easy way to react to events like begin/end of epochs.\n    """"""\n\n    def __init__(self):\n        self.model = None\n        self.mutator = None\n        self.trainer = None\n\n    def build(self, model, mutator, trainer):\n        """"""\n        Callback needs to be built with model, mutator, trainer, to get updates from them.\n\n        Parameters\n        ----------\n        model : nn.Module\n            Model to be trained.\n        mutator : nn.Module\n            Mutator that mutates the model.\n        trainer : BaseTrainer\n            Trainer that is to call the callback.\n        """"""\n        self.model = model\n        self.mutator = mutator\n        self.trainer = trainer\n\n    def on_epoch_begin(self, epoch):\n        """"""\n        Implement this to do something at the begin of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        """"""\n        pass\n\n    def on_epoch_end(self, epoch):\n        """"""\n        Implement this to do something at the end of epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number, starting from 0.\n        """"""\n        pass\n\n    def on_batch_begin(self, epoch):\n        pass\n\n    def on_batch_end(self, epoch):\n        pass\n\n\nclass LRSchedulerCallback(Callback):\n    """"""\n    Calls scheduler on every epoch ends.\n\n    Parameters\n    ----------\n    scheduler : LRScheduler\n        Scheduler to be called.\n    """"""\n    def __init__(self, scheduler, mode=""epoch""):\n        super().__init__()\n        assert mode == ""epoch""\n        self.scheduler = scheduler\n        self.mode = mode\n\n    def on_epoch_end(self, epoch):\n        """"""\n        Call ``self.scheduler.step()`` on epoch end.\n        """"""\n        self.scheduler.step()\n\n\nclass ArchitectureCheckpoint(Callback):\n    """"""\n    Calls ``trainer.export()`` on every epoch ends.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Location to save checkpoints.\n    """"""\n    def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def on_epoch_end(self, epoch):\n        """"""\n        Dump to ``/checkpoint_dir/epoch_{number}.json`` on epoch end.\n        """"""\n        dest_path = os.path.join(self.checkpoint_dir, ""epoch_{}.json"".format(epoch))\n        _logger.info(""Saving architecture to %s"", dest_path)\n        self.trainer.export(dest_path)\n\n\nclass ModelCheckpoint(Callback):\n    """"""\n    Calls ``trainer.export()`` on every epoch ends.\n\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Location to save checkpoints.\n    """"""\n    def __init__(self, checkpoint_dir):\n        super().__init__()\n        self.checkpoint_dir = checkpoint_dir\n        os.makedirs(self.checkpoint_dir, exist_ok=True)\n\n    def on_epoch_end(self, epoch):\n        """"""\n        Dump to ``/checkpoint_dir/epoch_{number}.pth.tar`` on every epoch end.\n        ``DataParallel`` object will have their inside modules exported.\n        """"""\n        if isinstance(self.model, nn.DataParallel):\n            state_dict = self.model.module.state_dict()\n        else:\n            state_dict = self.model.state_dict()\n        dest_path = os.path.join(self.checkpoint_dir, ""epoch_{}.pth.tar"".format(epoch))\n        _logger.info(""Saving model to %s"", dest_path)\n        torch.save(state_dict, dest_path)\n'"
src/sdk/pynni/nni/nas/pytorch/fixed.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\n\nfrom .mutables import InputChoice, LayerChoice, MutableScope\nfrom .mutator import Mutator\nfrom .utils import to_list\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass FixedArchitecture(Mutator):\n    """"""\n    Fixed architecture mutator that always selects a certain graph.\n\n    Parameters\n    ----------\n    model : nn.Module\n        A mutable network.\n    fixed_arc : dict\n        Preloaded architecture object.\n    strict : bool\n        Force everything that appears in ``fixed_arc`` to be used at least once.\n    """"""\n\n    def __init__(self, model, fixed_arc, strict=True):\n        super().__init__(model)\n        self._fixed_arc = fixed_arc\n\n        mutable_keys = set([mutable.key for mutable in self.mutables if not isinstance(mutable, MutableScope)])\n        fixed_arc_keys = set(self._fixed_arc.keys())\n        if fixed_arc_keys - mutable_keys:\n            raise RuntimeError(""Unexpected keys found in fixed architecture: {}."".format(fixed_arc_keys - mutable_keys))\n        if mutable_keys - fixed_arc_keys:\n            raise RuntimeError(""Missing keys in fixed architecture: {}."".format(mutable_keys - fixed_arc_keys))\n        self._fixed_arc = self._from_human_readable_architecture(self._fixed_arc)\n\n    def _from_human_readable_architecture(self, human_arc):\n        # convert from an exported architecture\n        result_arc = {k: to_list(v) for k, v in human_arc.items()}  # there could be tensors, numpy arrays, etc.\n        # First, convert non-list to list, because there could be {""op1"": 0} or {""op1"": ""conv""},\n        # which means {""op1"": [0, ]} ir {""op1"": [""conv"", ]}\n        result_arc = {k: v if isinstance(v, list) else [v] for k, v in result_arc.items()}\n        # Second, infer which ones are multi-hot arrays and which ones are in human-readable format.\n        # This is non-trivial, since if an array in [0, 1], we cannot know for sure it means [false, true] or [true, true].\n        # Here, we assume an multihot array has to be a boolean array or a float array and matches the length.\n        for mutable in self.mutables:\n            if mutable.key not in result_arc:\n                continue  # skip silently\n            choice_arr = result_arc[mutable.key]\n            if all(isinstance(v, bool) for v in choice_arr) or all(isinstance(v, float) for v in choice_arr):\n                if (isinstance(mutable, LayerChoice) and len(mutable) == len(choice_arr)) or \\\n                        (isinstance(mutable, InputChoice) and mutable.n_candidates == len(choice_arr)):\n                    # multihot, do nothing\n                    continue\n            if isinstance(mutable, LayerChoice):\n                choice_arr = [mutable.names.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(len(mutable))]\n            elif isinstance(mutable, InputChoice):\n                choice_arr = [mutable.choose_from.index(val) if isinstance(val, str) else val for val in choice_arr]\n                choice_arr = [i in choice_arr for i in range(mutable.n_candidates)]\n            result_arc[mutable.key] = choice_arr\n        return result_arc\n\n    def sample_search(self):\n        """"""\n        Always returns the fixed architecture.\n        """"""\n        return self._fixed_arc\n\n    def sample_final(self):\n        """"""\n        Always returns the fixed architecture.\n        """"""\n        return self._fixed_arc\n\n    def replace_layer_choice(self, module=None, prefix=""""):\n        """"""\n        Replace layer choices with selected candidates. It\'s done with best effort.\n        In case of weighted choices or multiple choices. if some of the choices on weighted with zero, delete them.\n        If single choice, replace the module with a normal module.\n\n        Parameters\n        ----------\n        module : nn.Module\n            Module to be processed.\n        prefix : str\n            Module name under global namespace.\n        """"""\n        if module is None:\n            module = self.model\n        for name, mutable in module.named_children():\n            global_name = (prefix + ""."" if prefix else """") + name\n            if isinstance(mutable, LayerChoice):\n                chosen = self._fixed_arc[mutable.key]\n                if sum(chosen) == 1 and max(chosen) == 1 and not mutable.return_mask:\n                    # sum is one, max is one, there has to be an only one\n                    # this is compatible with both integer arrays, boolean arrays and float arrays\n                    _logger.info(""Replacing %s with candidate number %d."", global_name, chosen.index(1))\n                    setattr(module, name, mutable[chosen.index(1)])\n                else:\n                    if mutable.return_mask:\n                        _logger.info(""`return_mask` flag of %s is true. As it relies on the behavior of LayerChoice, "" \\\n                                     ""LayerChoice will not be replaced."")\n                    # remove unused parameters\n                    for ch, n in zip(chosen, mutable.names):\n                        if ch == 0 and not isinstance(ch, float):\n                            setattr(mutable, n, None)\n            else:\n                self.replace_layer_choice(mutable, global_name)\n\n\ndef apply_fixed_architecture(model, fixed_arc):\n    """"""\n    Load architecture from `fixed_arc` and apply to model.\n\n    Parameters\n    ----------\n    model : torch.nn.Module\n        Model with mutables.\n    fixed_arc : str or dict\n        Path to the JSON that stores the architecture, or dict that stores the exported architecture.\n\n    Returns\n    -------\n    FixedArchitecture\n        Mutator that is responsible for fixes the graph.\n    """"""\n\n    if isinstance(fixed_arc, str):\n        with open(fixed_arc) as f:\n            fixed_arc = json.load(f)\n    architecture = FixedArchitecture(model, fixed_arc)\n    architecture.reset()\n\n    # for the convenience of parameters counting\n    architecture.replace_layer_choice()\n    return architecture\n'"
src/sdk/pynni/nni/nas/pytorch/mutables.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport warnings\nfrom collections import OrderedDict\n\nimport torch.nn as nn\n\nfrom nni.nas.pytorch.utils import global_mutable_counting\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mutable(nn.Module):\n    """"""\n    Mutable is designed to function as a normal layer, with all necessary operators\' weights.\n    States and weights of architectures should be included in mutator, instead of the layer itself.\n\n    Mutable has a key, which marks the identity of the mutable. This key can be used by users to share\n    decisions among different mutables. In mutator\'s implementation, mutators should use the key to\n    distinguish different mutables. Mutables that share the same key should be ""similar"" to each other.\n\n    Currently the default scope for keys is global. By default, the keys uses a global counter from 1 to\n    produce unique ids.\n\n    Parameters\n    ----------\n    key : str\n        The key of mutable.\n\n    Notes\n    -----\n    The counter is program level, but mutables are model level. In case multiple models are defined, and\n    you want to have `counter` starting from 1 in the second model, it\'s recommended to assign keys manually\n    instead of using automatic keys.\n    """"""\n\n    def __init__(self, key=None):\n        super().__init__()\n        if key is not None:\n            if not isinstance(key, str):\n                key = str(key)\n                logger.warning(""Warning: key \\""%s\\"" is not string, converted to string."", key)\n            self._key = key\n        else:\n            self._key = self.__class__.__name__ + str(global_mutable_counting())\n        self.init_hook = self.forward_hook = None\n\n    def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(""Deep copy doesn\'t work for mutables."")\n\n    def __call__(self, *args, **kwargs):\n        self._check_built()\n        return super().__call__(*args, **kwargs)\n\n    def set_mutator(self, mutator):\n        if ""mutator"" in self.__dict__:\n            raise RuntimeError(""`set_mutator` is called more than once. Did you parse the search space multiple times? ""\n                               ""Or did you apply multiple fixed architectures?"")\n        self.__dict__[""mutator""] = mutator\n\n    @property\n    def key(self):\n        """"""\n        Read-only property of key.\n        """"""\n        return self._key\n\n    @property\n    def name(self):\n        """"""\n        After the search space is parsed, it will be the module name of the mutable.\n        """"""\n        return self._name if hasattr(self, ""_name"") else ""_key""\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    def _check_built(self):\n        if not hasattr(self, ""mutator""):\n            raise ValueError(\n                ""Mutator not set for {}. You might have forgotten to initialize and apply your mutator. ""\n                ""Or did you initialize a mutable on the fly in forward pass? Move to `__init__` ""\n                ""so that trainer can locate all your mutables. See NNI docs for more details."".format(self))\n\n\nclass MutableScope(Mutable):\n    """"""\n    Mutable scope marks a subgraph/submodule to help mutators make better decisions.\n\n    If not annotated with mutable scope, search space will be flattened as a list. However, some mutators might\n    need to leverage the concept of a ""cell"". So if a module is defined as a mutable scope, everything in it will\n    look like ""sub-search-space"" in the scope. Scopes can be nested.\n\n    There are two ways mutators can use mutable scope. One is to traverse the search space as a tree during initialization\n    and reset. The other is to implement `enter_mutable_scope` and `exit_mutable_scope`. They are called before and after\n    the forward method of the class inheriting mutable scope.\n\n    Mutable scopes are also mutables that are listed in the mutator.mutables (search space), but they are not supposed\n    to appear in the dict of choices.\n\n    Parameters\n    ----------\n    key : str\n        Key of mutable scope.\n    """"""\n    def __init__(self, key):\n        super().__init__(key=key)\n\n    def __call__(self, *args, **kwargs):\n        try:\n            self._check_built()\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)\n\n\nclass LayerChoice(Mutable):\n    """"""\n    Layer choice selects one of the ``op_candidates``, then apply it on inputs and return results.\n    In rare cases, it can also select zero or many.\n\n    Layer choice does not allow itself to be nested.\n\n    Parameters\n    ----------\n    op_candidates : list of nn.Module or OrderedDict\n        A module list to be selected from.\n    reduction : str\n        ``mean``, ``concat``, ``sum`` or ``none``. Policy if multiples are selected.\n        If ``none``, a list is returned. ``mean`` returns the average. ``sum`` returns the sum.\n        ``concat`` concatenate the list at dimension 1.\n    return_mask : bool\n        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n    key : str\n        Key of the input choice.\n\n    Attributes\n    ----------\n    length : int\n        Deprecated. Number of ops to choose from. ``len(layer_choice)`` is recommended.\n    names : list of str\n        Names of candidates.\n    choices : list of Module\n        Deprecated. A list of all candidate modules in the layer choice module.\n        ``list(layer_choice)`` is recommended, which will serve the same purpose.\n\n    Notes\n    -----\n    ``op_candidates`` can be a list of modules or a ordered dict of named modules, for example,\n\n    .. code-block:: python\n\n        self.op_choice = LayerChoice(OrderedDict([\n            (""conv3x3"", nn.Conv2d(3, 16, 128)),\n            (""conv5x5"", nn.Conv2d(5, 16, 128)),\n            (""conv7x7"", nn.Conv2d(7, 16, 128))\n        ]))\n\n    Elements in layer choice can be modified or deleted. Use ``del self.op_choice[""conv5x5""]`` or\n    ``self.op_choice[1] = nn.Conv3d(...)``. Adding more choices is not supported yet.\n    """"""\n\n    def __init__(self, op_candidates, reduction=""sum"", return_mask=False, key=None):\n        super().__init__(key=key)\n        self.names = []\n        if isinstance(op_candidates, OrderedDict):\n            for name, module in op_candidates.items():\n                assert name not in [""length"", ""reduction"", ""return_mask"", ""_key"", ""key"", ""names""], \\\n                    ""Please don\'t use a reserved name \'{}\' for your module."".format(name)\n                self.add_module(name, module)\n                self.names.append(name)\n        elif isinstance(op_candidates, list):\n            for i, module in enumerate(op_candidates):\n                self.add_module(str(i), module)\n                self.names.append(str(i))\n        else:\n            raise TypeError(""Unsupported op_candidates type: {}"".format(type(op_candidates)))\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def __getitem__(self, idx):\n        if isinstance(idx, str):\n            return self._modules[idx]\n        return list(self)[idx]\n\n    def __setitem__(self, idx, module):\n        key = idx if isinstance(idx, str) else self.names[idx]\n        return setattr(self, key, module)\n\n    def __delitem__(self, idx):\n        if isinstance(idx, slice):\n            for key in self.names[idx]:\n                delattr(self, key)\n        else:\n            if isinstance(idx, str):\n                key, idx = idx, self.names.index(idx)\n            else:\n                key = self.names[idx]\n            delattr(self, key)\n        del self.names[idx]\n\n    @property\n    def length(self):\n        warnings.warn(""layer_choice.length is deprecated. Use `len(layer_choice)` instead."", DeprecationWarning)\n        return len(self)\n\n    def __len__(self):\n        return len(self.names)\n\n    def __iter__(self):\n        return map(lambda name: self._modules[name], self.names)\n\n    @property\n    def choices(self):\n        warnings.warn(""layer_choice.choices is deprecated. Use `list(layer_choice)` instead."", DeprecationWarning)\n        return list(self)\n\n    def forward(self, *args, **kwargs):\n        """"""\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        """"""\n        out, mask = self.mutator.on_forward_layer_choice(self, *args, **kwargs)\n        if self.return_mask:\n            return out, mask\n        return out\n\n\nclass InputChoice(Mutable):\n    """"""\n    Input choice selects ``n_chosen`` inputs from ``choose_from`` (contains ``n_candidates`` keys). For beginners,\n    use ``n_candidates`` instead of ``choose_from`` is a safe option. To get the most power out of it, you might want to\n    know about ``choose_from``.\n\n    The keys in ``choose_from`` can be keys that appear in past mutables, or ``NO_KEY`` if there are no suitable ones.\n    The keys are designed to be the keys of the sources. To help mutators make better decisions,\n    mutators might be interested in how the tensors to choose from come into place. For example, the tensor is the\n    output of some operator, some node, some cell, or some module. If this operator happens to be a mutable (e.g.,\n    ``LayerChoice`` or ``InputChoice``), it has a key naturally that can be used as a source key. If it\'s a\n    module/submodule, it needs to be annotated with a key: that\'s where a :class:`MutableScope` is needed.\n\n    In the example below, ``input_choice`` is a 4-choose-any. The first 3 is semantically output of cell1, output of cell2,\n    output of cell3 with respectively. Notice that an extra max pooling is followed by cell1, indicating x1 is not\n    ""actually"" the direct output of cell1.\n\n    .. code-block:: python\n\n        class Cell(MutableScope):\n            pass\n\n        class Net(nn.Module):\n            def __init__(self):\n                self.cell1 = Cell(""cell1"")\n                self.cell2 = Cell(""cell2"")\n                self.op = LayerChoice([conv3x3(), conv5x5()], key=""op"")\n                self.input_choice = InputChoice(choose_from=[""cell1"", ""cell2"", ""op"", InputChoice.NO_KEY])\n\n            def forward(self, x):\n                x1 = max_pooling(self.cell1(x))\n                x2 = self.cell2(x)\n                x3 = self.op(x)\n                x4 = torch.zeros_like(x)\n                return self.input_choice([x1, x2, x3, x4])\n\n    Parameters\n    ----------\n    n_candidates : int\n        Number of inputs to choose from.\n    choose_from : list of str\n        List of source keys to choose from. At least of one of ``choose_from`` and ``n_candidates`` must be fulfilled.\n        If ``n_candidates`` has a value but ``choose_from`` is None, it will be automatically treated as ``n_candidates``\n        number of empty string.\n    n_chosen : int\n        Recommended inputs to choose. If None, mutator is instructed to select any.\n    reduction : str\n        ``mean``, ``concat``, ``sum`` or ``none``. See :class:`LayerChoice`.\n    return_mask : bool\n        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n    key : str\n        Key of the input choice.\n    """"""\n\n    NO_KEY = """"\n\n    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None,\n                 reduction=""sum"", return_mask=False, key=None):\n        super().__init__(key=key)\n        # precondition check\n        assert n_candidates is not None or choose_from is not None, ""At least one of `n_candidates` and `choose_from`"" \\\n                                                                    ""must be not None.""\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), ""Number of candidates must be equal to the length of `choose_from`.""\n        assert n_candidates > 0, ""Number of candidates must be greater than 0.""\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, ""Expected selected number must be None or no more "" \\\n                                                                  ""than number of candidates.""\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def forward(self, optional_inputs):\n        """"""\n        Forward method of LayerChoice.\n\n        Parameters\n        ----------\n        optional_inputs : list or dict\n            Recommended to be a dict. As a dict, inputs will be converted to a list that follows the order of\n            ``choose_from`` in initialization. As a list, inputs must follow the semantic order that is the same as\n            ``choose_from``.\n\n        Returns\n        -------\n        tuple of tensors\n            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n        """"""\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n            ""Optional input list must be a list, not a {}."".format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n            ""Length of the input list must be equal to number of candidates.""\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out\n'"
src/sdk/pynni/nni/nas/pytorch/mutator.py,15,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\n\nfrom .base_mutator import BaseMutator\nfrom .mutables import LayerChoice, InputChoice\nfrom .utils import to_list\n\nlogger = logging.getLogger(__name__)\n\n\nclass Mutator(BaseMutator):\n\n    def __init__(self, model):\n        super().__init__(model)\n        self._cache = dict()\n        self._connect_all = False\n\n    def sample_search(self):\n        """"""\n        Override to implement this method to iterate over mutables and make decisions.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        """"""\n        raise NotImplementedError\n\n    def sample_final(self):\n        """"""\n        Override to implement this method to iterate over mutables and make decisions that is final\n        for export and retraining.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        """"""\n        raise NotImplementedError\n\n    def reset(self):\n        """"""\n        Reset the mutator by call the `sample_search` to resample (for search). Stores the result in a local\n        variable so that `on_forward_layer_choice` and `on_forward_input_choice` can use the decision directly.\n        """"""\n        self._cache = self.sample_search()\n\n    def export(self):\n        """"""\n        Resample (for final) and return results.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions.\n        """"""\n        sampled = self.sample_final()\n        result = dict()\n        for mutable in self.mutables:\n            if not isinstance(mutable, (LayerChoice, InputChoice)):\n                # not supported as built-in\n                continue\n            result[mutable.key] = self._convert_mutable_decision_to_human_readable(mutable, sampled.pop(mutable.key))\n        if sampled:\n            raise ValueError(""Unexpected keys returned from \'sample_final()\': %s"", list(sampled.keys()))\n        return result\n\n    def status(self):\n        """"""\n        Return current selection status of mutator.\n\n        Returns\n        -------\n        dict\n            A mapping from key of mutables to decisions. All weights (boolean type and float type)\n            are converted into real number values. Numpy arrays and tensors are converted into list.\n        """"""\n        data = dict()\n        for k, v in self._cache.items():\n            if torch.is_tensor(v):\n                v = v.detach().cpu().numpy()\n            if isinstance(v, np.ndarray):\n                v = v.astype(np.float32).tolist()\n            data[k] = v\n        return data\n\n    def graph(self, inputs):\n        """"""\n        Return model supernet graph.\n\n        Parameters\n        ----------\n        inputs: tuple of tensor\n            Inputs that will be feeded into the network.\n\n        Returns\n        -------\n        dict\n            Containing ``node``, in Tensorboard GraphDef format.\n            Additional key ``mutable`` is a map from key to list of modules.\n        """"""\n        if not torch.__version__.startswith(""1.4""):\n            logger.warning(""Graph is only tested with PyTorch 1.4. Other versions might not work."")\n        from nni._graph_utils import build_graph\n        from google.protobuf import json_format\n        # protobuf should be installed as long as tensorboard is installed\n        try:\n            self._connect_all = True\n            graph_def, _ = build_graph(self.model, inputs, verbose=False)\n            result = json_format.MessageToDict(graph_def)\n        finally:\n            self._connect_all = False\n\n        # `mutable` is to map the keys to a list of corresponding modules.\n        # A key can be linked to multiple modules, use `dedup=False` to find them all.\n        result[""mutable""] = defaultdict(list)\n        for mutable in self.mutables.traverse(deduplicate=False):\n            # A module will be represent in the format of\n            # [{""type"": ""Net"", ""name"": """"}, {""type"": ""Cell"", ""name"": ""cell1""}, {""type"": ""Conv2d"": ""name"": ""conv""}]\n            # which will be concatenated into Net/Cell[cell1]/Conv2d[conv] in frontend.\n            # This format is aligned with the scope name jit gives.\n            modules = mutable.name.split(""."")\n            path = [\n                {""type"": self.model.__class__.__name__, ""name"": """"}\n            ]\n            m = self.model\n            for module in modules:\n                m = getattr(m, module)\n                path.append({\n                    ""type"": m.__class__.__name__,\n                    ""name"": module\n                })\n            result[""mutable""][mutable.key].append(path)\n        return result\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        """"""\n        On default, this method retrieves the decision obtained previously, and select certain operations.\n        Only operations with non-zero weight will be executed. The results will be added to a list.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            Layer choice module.\n        args : list of torch.Tensor\n            Inputs\n        kwargs : dict\n            Inputs\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        """"""\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction,\n                                                      [op(*args, **kwargs) for op in mutable]), \\\n                torch.ones(len(mutable))\n\n        def _map_fn(op, args, kwargs):\n            return op(*args, **kwargs)\n\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n            ""Invalid mask, expected {} to be of length {}."".format(mask, len(mutable))\n        out, mask = self._select_with_mask(_map_fn, [(choice, args, kwargs) for choice in mutable], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        """"""\n        On default, this method retrieves the decision obtained previously, and select certain tensors.\n        Then it will reduce the list of all tensor outputs with the policy specified in `mutable.reduction`.\n\n        Parameters\n        ----------\n        mutable : InputChoice\n            Input choice module.\n        tensor_list : list of torch.Tensor\n            Tensor list to apply the decision on.\n\n        Returns\n        -------\n        tuple of torch.Tensor and torch.Tensor\n            Output and mask.\n        """"""\n        if self._connect_all:\n            return self._all_connect_tensor_reduction(mutable.reduction, tensor_list), \\\n                torch.ones(mutable.n_candidates)\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n            ""Invalid mask, expected {} to be of length {}."".format(mask, mutable.n_candidates)\n        out, mask = self._select_with_mask(lambda x: x, [(t,) for t in tensor_list], mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def _select_with_mask(self, map_fn, candidates, mask):\n        """"""\n        Select masked tensors and return a list of tensors.\n\n        Parameters\n        ----------\n        map_fn : function\n            Convert candidates to target candidates. Can be simply identity.\n        candidates : list of torch.Tensor\n            Tensor list to apply the decision on.\n        mask : list-like object\n            Can be a list, an numpy array or a tensor (recommended). Needs to\n            have the same length as ``candidates``.\n\n        Returns\n        -------\n        tuple of list of torch.Tensor and torch.Tensor\n            Output and mask.\n        """"""\n        if (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], bool)) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype == np.bool) or \\\n                ""BoolTensor"" in mask.type():\n            out = [map_fn(*cand) for cand, m in zip(candidates, mask) if m]\n        elif (isinstance(mask, list) and len(mask) >= 1 and isinstance(mask[0], (float, int))) or \\\n                (isinstance(mask, np.ndarray) and mask.dtype in (np.float32, np.float64, np.int32, np.int64)) or \\\n                ""FloatTensor"" in mask.type():\n            out = [map_fn(*cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError(""Unrecognized mask \'%s\'"" % mask)\n        if not torch.is_tensor(mask):\n            mask = torch.tensor(mask)  # pylint: disable=not-callable\n        return out, mask\n\n    def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == ""none"":\n            return tensor_list\n        if not tensor_list:\n            return None  # empty. return None for now\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == ""sum"":\n            return sum(tensor_list)\n        if reduction_type == ""mean"":\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == ""concat"":\n            return torch.cat(tensor_list, dim=1)\n        raise ValueError(""Unrecognized reduction policy: \\""{}\\"""".format(reduction_type))\n\n    def _all_connect_tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == ""none"":\n            return tensor_list\n        if reduction_type == ""concat"":\n            return torch.cat(tensor_list, dim=1)\n        return torch.stack(tensor_list).sum(0)\n\n    def _get_decision(self, mutable):\n        """"""\n        By default, this method checks whether `mutable.key` is already in the decision cache,\n        and returns the result without double-check.\n\n        Parameters\n        ----------\n        mutable : Mutable\n\n        Returns\n        -------\n        object\n        """"""\n        if mutable.key not in self._cache:\n            raise ValueError(""\\""{}\\"" not found in decision cache."".format(mutable.key))\n        result = self._cache[mutable.key]\n        logger.debug(""Decision %s: %s"", mutable.key, result)\n        return result\n\n    def _convert_mutable_decision_to_human_readable(self, mutable, sampled):\n        # Assert the existence of mutable.key in returned architecture.\n        # Also check if there is anything extra.\n        multihot_list = to_list(sampled)\n        converted = None\n        # If it\'s a boolean array, we can do optimization.\n        if all([t == 0 or t == 1 for t in multihot_list]):\n            if isinstance(mutable, LayerChoice):\n                assert len(multihot_list) == len(mutable), \\\n                    ""Results returned from \'sample_final()\' (%s: %s) either too short or too long."" \\\n                        % (mutable.key, multihot_list)\n                # check if all modules have different names and they indeed have names\n                if len(set(mutable.names)) == len(mutable) and not all(d.isdigit() for d in mutable.names):\n                    converted = [name for i, name in enumerate(mutable.names) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n            if isinstance(mutable, InputChoice):\n                assert len(multihot_list) == mutable.n_candidates, \\\n                    ""Results returned from \'sample_final()\' (%s: %s) either too short or too long."" \\\n                        % (mutable.key, multihot_list)\n                # check if all input candidates have different names\n                if len(set(mutable.choose_from)) == mutable.n_candidates:\n                    converted = [name for i, name in enumerate(mutable.choose_from) if multihot_list[i]]\n                else:\n                    converted = [i for i in range(len(multihot_list)) if multihot_list[i]]\n        if converted is not None:\n            # if only one element, then remove the bracket\n            if len(converted) == 1:\n                converted = converted[0]\n        else:\n            # do nothing\n            converted = multihot_list\n        return converted\n'"
src/sdk/pynni/nni/nas/pytorch/trainer.py,9,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\nimport time\nfrom abc import abstractmethod\n\nimport torch\n\nfrom .base_trainer import BaseTrainer\n\n_logger = logging.getLogger(__name__)\n\n\nclass TorchTensorEncoder(json.JSONEncoder):\n    def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            olist = o.tolist()\n            if ""bool"" not in o.type().lower() and all(map(lambda d: d == 0 or d == 1, olist)):\n                _logger.warning(""Every element in %s is either 0 or 1. ""\n                                ""You might consider convert it into bool."", olist)\n            return olist\n        return super().default(o)\n\n\nclass Trainer(BaseTrainer):\n    """"""\n    A trainer with some helper functions implemented. To implement a new trainer,\n    users need to implement :meth:`train_one_epoch`, :meth:`validate_one_epoch` and :meth:`checkpoint`.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model with mutables.\n    mutator : BaseMutator\n        A mutator object that has been initialized with the model.\n    loss : callable\n        Called with logits and targets. Returns a loss tensor.\n        See `PyTorch loss functions`_ for examples.\n    metrics : callable\n        Called with logits and targets. Returns a dict that maps metrics keys to metrics data. For example,\n\n        .. code-block:: python\n\n            def metrics_fn(output, target):\n                return {""acc1"": accuracy(output, target, topk=1), ""acc5"": accuracy(output, target, topk=5)}\n\n    optimizer : Optimizer\n        Optimizer that optimizes the model.\n    num_epochs : int\n        Number of epochs of training.\n    dataset_train : torch.utils.data.Dataset\n        Dataset of training. If not otherwise specified, ``dataset_train`` and ``dataset_valid`` should be standard\n        PyTorch Dataset. See `torch.utils.data`_ for examples.\n    dataset_valid : torch.utils.data.Dataset\n        Dataset of validation/testing.\n    batch_size : int\n        Batch size.\n    workers : int\n        Number of workers used in data preprocessing.\n    device : torch.device\n        Device object. Either ``torch.device(""cuda"")`` or ``torch.device(""cpu"")``. When ``None``, trainer will\n        automatic detects GPU and selects GPU first.\n    log_frequency : int\n        Number of mini-batches to log metrics.\n    callbacks : list of Callback\n        Callbacks to plug into the trainer. See Callbacks.\n\n\n    .. _`PyTorch loss functions`: https://pytorch.org/docs/stable/nn.html#loss-functions\n    .. _`torch.utils.data`: https://pytorch.org/docs/stable/data.html\n    """"""\n    def __init__(self, model, mutator, loss, metrics, optimizer, num_epochs,\n                 dataset_train, dataset_valid, batch_size, workers, device, log_frequency, callbacks):\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"") if device is None else device\n        self.model = model\n        self.mutator = mutator\n        self.loss = loss\n\n        self.metrics = metrics\n        self.optimizer = optimizer\n\n        self.model.to(self.device)\n        self.mutator.to(self.device)\n        self.loss.to(self.device)\n\n        self.num_epochs = num_epochs\n        self.dataset_train = dataset_train\n        self.dataset_valid = dataset_valid\n        self.batch_size = batch_size\n        self.workers = workers\n        self.log_frequency = log_frequency\n        self.log_dir = os.path.join(""logs"", str(time.time()))\n        os.makedirs(self.log_dir, exist_ok=True)\n        self.status_writer = open(os.path.join(self.log_dir, ""log""), ""w"")\n        self.callbacks = callbacks if callbacks is not None else []\n        for callback in self.callbacks:\n            callback.build(self.model, self.mutator, self)\n\n    @abstractmethod\n    def train_one_epoch(self, epoch):\n        """"""\n        Train one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        """"""\n        pass\n\n    @abstractmethod\n    def validate_one_epoch(self, epoch):\n        """"""\n        Validate one epoch.\n\n        Parameters\n        ----------\n        epoch : int\n            Epoch number starting from 0.\n        """"""\n        pass\n\n    def train(self, validate=True):\n        """"""\n        Train ``num_epochs``.\n        Trigger callbacks at the start and the end of each epoch.\n\n        Parameters\n        ----------\n        validate : bool\n            If ``true``, will do validation every epoch.\n        """"""\n        for epoch in range(self.num_epochs):\n            for callback in self.callbacks:\n                callback.on_epoch_begin(epoch)\n\n            # training\n            _logger.info(""Epoch %d Training"", epoch + 1)\n            self.train_one_epoch(epoch)\n\n            if validate:\n                # validation\n                _logger.info(""Epoch %d Validating"", epoch + 1)\n                self.validate_one_epoch(epoch)\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)\n\n    def validate(self):\n        """"""\n        Do one validation.\n        """"""\n        self.validate_one_epoch(-1)\n\n    def export(self, file):\n        """"""\n        Call ``mutator.export()`` and dump the architecture to ``file``.\n\n        Parameters\n        ----------\n        file : str\n            A file path. Expected to be a JSON.\n        """"""\n        mutator_export = self.mutator.export()\n        with open(file, ""w"") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def checkpoint(self):\n        """"""\n        Return trainer checkpoint.\n        """"""\n        raise NotImplementedError(""Not implemented yet"")\n\n    def enable_visualization(self):\n        """"""\n        Enable visualization. Write graph and training log to folder ``logs/<timestamp>``.\n        """"""\n        sample = None\n        for x, _ in self.train_loader:\n            sample = x.to(self.device)[:2]\n            break\n        if sample is None:\n            _logger.warning(""Sample is %s."", sample)\n        _logger.info(""Creating graph json, writing to %s. Visualization enabled."", self.log_dir)\n        with open(os.path.join(self.log_dir, ""graph.json""), ""w"") as f:\n            json.dump(self.mutator.graph(sample), f)\n        self.visualization_enabled = True\n\n    def _write_graph_status(self):\n        if hasattr(self, ""visualization_enabled"") and self.visualization_enabled:\n            print(json.dumps(self.mutator.status()), file=self.status_writer, flush=True)\n'"
src/sdk/pynni/nni/nas/pytorch/utils.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nfrom collections import OrderedDict\n\nimport numpy as np\nimport torch\n\n_counter = 0\n\n_logger = logging.getLogger(__name__)\n\n\ndef global_mutable_counting():\n    """"""\n    A program level counter starting from 1.\n    """"""\n    global _counter\n    _counter += 1\n    return _counter\n\n\ndef _reset_global_mutable_counting():\n    """"""\n    Reset the global mutable counting to count from 1. Useful when defining multiple models with default keys.\n    """"""\n    global _counter\n    _counter = 0\n\n\ndef to_device(obj, device):\n    """"""\n    Move a tensor, tuple, list, or dict onto device.\n    """"""\n    if torch.is_tensor(obj):\n        return obj.to(device)\n    if isinstance(obj, tuple):\n        return tuple(to_device(t, device) for t in obj)\n    if isinstance(obj, list):\n        return [to_device(t, device) for t in obj]\n    if isinstance(obj, dict):\n        return {k: to_device(v, device) for k, v in obj.items()}\n    if isinstance(obj, (int, float, str)):\n        return obj\n    raise ValueError(""\'%s\' has unsupported type \'%s\'"" % (obj, type(obj)))\n\n\ndef to_list(arr):\n    if torch.is_tensor(arr):\n        return arr.cpu().numpy().tolist()\n    if isinstance(arr, np.ndarray):\n        return arr.tolist()\n    if isinstance(arr, (list, tuple)):\n        return list(arr)\n    return arr\n\n\nclass AverageMeterGroup:\n    """"""\n    Average meter group for multiple average meters.\n    """"""\n\n    def __init__(self):\n        self.meters = OrderedDict()\n\n    def update(self, data):\n        """"""\n        Update the meter group with a dict of metrics.\n        Non-exist average meters will be automatically created.\n        """"""\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k, "":4f"")\n            self.meters[k].update(v)\n\n    def __getattr__(self, item):\n        return self.meters[item]\n\n    def __getitem__(self, item):\n        return self.meters[item]\n\n    def __str__(self):\n        return ""  "".join(str(v) for v in self.meters.values())\n\n    def summary(self):\n        """"""\n        Return a summary string of group data.\n        """"""\n        return ""  "".join(v.summary() for v in self.meters.values())\n\n\nclass AverageMeter:\n    """"""\n    Computes and stores the average and current value.\n\n    Parameters\n    ----------\n    name : str\n        Name to display.\n    fmt : str\n        Format string to print the values.\n    """"""\n\n    def __init__(self, name, fmt=\':f\'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        """"""\n        Reset the meter.\n        """"""\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        """"""\n        Update with value and weight.\n\n        Parameters\n        ----------\n        val : float or int\n            The new value to be accounted in.\n        n : int\n            The weight of the new value.\n        """"""\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = \'{name} {val\' + self.fmt + \'} ({avg\' + self.fmt + \'})\'\n        return fmtstr.format(**self.__dict__)\n\n    def summary(self):\n        fmtstr = \'{name}: {avg\' + self.fmt + \'}\'\n        return fmtstr.format(**self.__dict__)\n\n\nclass StructuredMutableTreeNode:\n    """"""\n    A structured representation of a search space.\n    A search space comes with a root (with `None` stored in its `mutable`), and a bunch of children in its `children`.\n    This tree can be seen as a ""flattened"" version of the module tree. Since nested mutable entity is not supported yet,\n    the following must be true: each subtree corresponds to a ``MutableScope`` and each leaf corresponds to a\n    ``Mutable`` (other than ``MutableScope``).\n\n    Parameters\n    ----------\n    mutable : nni.nas.pytorch.mutables.Mutable\n        The mutable that current node is linked with.\n    """"""\n\n    def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []\n\n    def add_child(self, mutable):\n        """"""\n        Add a tree node to the children list of current node.\n        """"""\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]\n\n    def type(self):\n        """"""\n        Return the ``type`` of mutable content.\n        """"""\n        return type(self.mutable)\n\n    def __iter__(self):\n        return self.traverse()\n\n    def traverse(self, order=""pre"", deduplicate=True, memo=None):\n        """"""\n        Return a generator that generates a list of mutables in this tree.\n\n        Parameters\n        ----------\n        order : str\n            pre or post. If pre, current mutable is yield before children. Otherwise after.\n        deduplicate : bool\n            If true, mutables with the same key will not appear after the first appearance.\n        memo : dict\n            An auxiliary dict that memorize keys seen before, so that deduplication is possible.\n\n        Returns\n        -------\n        generator of Mutable\n        """"""\n        if memo is None:\n            memo = set()\n        assert order in [""pre"", ""post""]\n        if order == ""pre"":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == ""post"":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n'"
src/sdk/pynni/nni/nas/tensorflow/__init__.py,0,b''
src/sdk/pynni/nni/nas/tensorflow/base_mutator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom tensorflow.keras import Model\n\nfrom .mutables import Mutable, MutableScope, InputChoice\nfrom .utils import StructuredMutableTreeNode\n\n\nclass BaseMutator(Model):\n    def __init__(self, model):\n        super().__init__()\n        self.__dict__[\'model\'] = model\n        self._structured_mutables = self._parse_search_space(self.model)\n\n    def _parse_search_space(self, module, root=None, prefix=\'\', memo=None, nested_detection=None):\n        if memo is None:\n            memo = set()\n        if root is None:\n            root = StructuredMutableTreeNode(None)\n        if module not in memo:\n            memo.add(module)\n            if isinstance(module, Mutable):\n                if nested_detection is not None:\n                    raise RuntimeError(\'Cannot have nested search space. Error at {} in {}\'\n                                       .format(module, nested_detection))\n                module.name = prefix\n                module.set_mutator(self)\n                root = root.add_child(module)\n                if not isinstance(module, MutableScope):\n                    nested_detection = module\n                if isinstance(module, InputChoice):\n                    for k in module.choose_from:\n                        if k != InputChoice.NO_KEY and k not in [m.key for m in memo if isinstance(m, Mutable)]:\n                            raise RuntimeError(\'""{}"" required by ""{}"" not found in keys that appeared before, and is not NO_KEY.\'\n                                               .format(k, module.key))\n            for submodule in module.layers:\n                if not isinstance(submodule, Model):\n                    continue\n                submodule_prefix = prefix + (\'.\' if prefix else \'\') + submodule.name\n                self._parse_search_space(submodule, root, submodule_prefix, memo=memo, nested_detection=nested_detection)\n        return root\n\n    @property\n    def mutables(self):\n        return self._structured_mutables\n\n    def undedup_mutables(self):\n        return self._structured_mutables.traverse(deduplicate=False)\n\n    def call(self, *inputs):\n        raise RuntimeError(\'Call is undefined for mutators.\')\n\n    def __setattr__(self, name, value):\n        if name == \'model\':\n            raise AttributeError(""Attribute `model` can be set at most once, and you shouldn\'t use `self.model = model` to ""\n                                 ""include your network, as it will include all parameters in model into the mutator."")\n        return super().__setattr__(name, value)\n\n    def enter_mutable_scope(self, mutable_scope):\n        pass\n\n    def exit_mutable_scope(self, mutable_scope):\n        pass\n\n    def on_forward_layer_choice(self, mutable, *inputs):\n        raise NotImplementedError\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        raise NotImplementedError\n\n    def export(self):\n        raise NotImplementedError\n'"
src/sdk/pynni/nni/nas/tensorflow/mutables.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nfrom tensorflow.keras import Model\n\nfrom .utils import global_mutable_counting\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass Mutable(Model):\n    def __init__(self, key=None):\n        super().__init__()\n        if key is None:\n            self._key = \'{}_{}\'.format(type(self).__name__, global_mutable_counting())\n        elif isinstance(key, str):\n            self._key = key\n        else:\n            self._key = str(key)\n            _logger.warning(\'Key ""%s"" is not string, converted to string.\', key)\n        self.init_hook = None\n        self.forward_hook = None\n\n    def __deepcopy__(self, memodict=None):\n        raise NotImplementedError(""Deep copy doesn\'t work for mutables."")\n\n    def __call__(self, *args, **kwargs):\n        self._check_built()\n        return super().__call__(*args, **kwargs)\n\n    def set_mutator(self, mutator):\n        if \'mutator\' in self.__dict__:\n            raise RuntimeError(\'`set_mutator is called more than once. \'\n                               \'Did you parse the search space multiple times? \'\n                               \'Or did you apply multiple fixed architectures?\')\n        self.__dict__[\'mutator\'] = mutator\n\n    def call(self, *inputs):\n        raise NotImplementedError(\'Method `call` of Mutable must be overridden\')\n\n    @property\n    def key(self):\n        return self._key\n\n    @property\n    def name(self):\n        return self._name if hasattr(self, \'_name\') else self._key\n\n    @name.setter\n    def name(self, name):\n        self._name = name\n\n    def _check_built(self):\n        if not hasattr(self, \'mutator\'):\n            raise ValueError(\n                ""Mutator not set for {}. You might have forgotten to initialize and apply your mutator. ""\n                ""Or did you initialize a mutable on the fly in forward pass? Move to `__init__` ""\n                ""so that trainer can locate all your mutables. See NNI docs for more details."".format(self))\n\n    def __repr__(self):\n        return \'{} ({})\'.format(self.name, self.key)\n\n\nclass MutableScope(Mutable):\n    def __call__(self, *args, **kwargs):\n        try:\n            self._check_built()\n            self.mutator.enter_mutable_scope(self)\n            return super().__call__(*args, **kwargs)\n        finally:\n            self.mutator.exit_mutable_scope(self)\n\n\nclass LayerChoice(Mutable):\n    def __init__(self, op_candidates, reduction=\'sum\', return_mask=False, key=None):\n        super().__init__(key=key)\n        self.length = len(op_candidates)\n        self.choices = op_candidates\n        self.reduction = reduction\n        self.return_mask = return_mask\n        self._built = False\n\n    def call(self, *inputs):\n        if not self._built:\n            for op in self.choices:\n                if len(inputs) > 1:  # FIXME: not tested\n                    op.build([inp.shape for inp in inputs])\n                elif len(inputs) == 1:\n                    op.build(inputs[0].shape)\n            self._built = True\n        out, mask = self.mutator.on_forward_layer_choice(self, *inputs)\n        if self.return_mask:\n            return out, mask\n        return out\n\n    def __len__(self):\n        return len(self.choices)\n\n\nclass InputChoice(Mutable):\n    NO_KEY = \'\'\n\n    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None, reduction=\'sum\', return_mask=False, key=None):\n        super().__init__(key=key)\n        assert n_candidates is not None or choose_from is not None, \\\n                \'At least one of `n_candidates` and `choose_from` must be not None.\'\n        if choose_from is not None and n_candidates is None:\n            n_candidates = len(choose_from)\n        elif choose_from is None and n_candidates is not None:\n            choose_from = [self.NO_KEY] * n_candidates\n        assert n_candidates == len(choose_from), \'Number of candidates must be equal to the length of `choose_from`.\'\n        assert n_candidates > 0, \'Number of candidates must be greater than 0.\'\n        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \\\n                \'Expected selected number must be None or no more than number of candidates.\'\n\n        self.n_candidates = n_candidates\n        self.choose_from = choose_from.copy()\n        self.n_chosen = n_chosen\n        self.reduction = reduction\n        self.return_mask = return_mask\n\n    def call(self, optional_inputs):\n        optional_input_list = optional_inputs\n        if isinstance(optional_inputs, dict):\n            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n        assert isinstance(optional_input_list, list), \\\n                \'Optional input list must be a list, not a {}.\'.format(type(optional_input_list))\n        assert len(optional_inputs) == self.n_candidates, \\\n                \'Length of the input list must be equal to number of candidates.\'\n        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n        if self.return_mask:\n            return out, mask\n        return out\n'"
src/sdk/pynni/nni/nas/tensorflow/mutator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport tensorflow as tf\n\nfrom .base_mutator import BaseMutator\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass Mutator(BaseMutator):\n    def __init__(self, model):\n        super().__init__(model)\n        self._cache = {}\n\n    def sample_search(self):\n        raise NotImplementedError(\'Method `sample_search` must be overridden\')\n\n    def sample_final(self):\n        raise NotImplementedError(\'Method `sample_final` must be overriden for exporting\')\n\n    def reset(self):\n        self._cache = self.sample_search()\n\n    def export(self):\n        return self.sample_final()\n\n    # TODO: status\n    # TODO: graph\n\n    def on_forward_layer_choice(self, mutable, *inputs):\n        mask = self._get_decision(mutable)\n        assert len(mask) == len(mutable), \\\n                \'Invalid mask, expected {} to be of length {}.\'.format(mask, len(mutable))\n        out = self._select_with_mask(lambda choice: choice(*inputs), mutable.choices, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def on_forward_input_choice(self, mutable, tensor_list):\n        mask = self._get_decision(mutable)\n        assert len(mask) == mutable.n_candidates, \\\n                \'Invalid mask, expected {} to be of length {}.\'.format(mask, mutable.n_candidates)\n        out = self._select_with_mask(lambda tensor: tensor, tensor_list, mask)\n        return self._tensor_reduction(mutable.reduction, out), mask\n\n    def _select_with_mask(self, map_fn, candidates, mask):\n        if mask.dtype.is_bool:\n            out = [map_fn(cand) for cand, m in zip(candidates, mask) if m]\n        elif mask.dtype.is_floating:\n            out = [map_fn(cand) * m for cand, m in zip(candidates, mask) if m]\n        else:\n            raise ValueError(\'Unrecognized mask, dtype is {}\'.format(mask.dtype.name))\n        return out\n\n    def _tensor_reduction(self, reduction_type, tensor_list):\n        if reduction_type == \'none\':\n            return tensor_list\n        if not tensor_list:\n            return None\n        if len(tensor_list) == 1:\n            return tensor_list[0]\n        if reduction_type == \'sum\':\n            return sum(tensor_list)\n        if reduction_type == \'mean\':\n            return sum(tensor_list) / len(tensor_list)\n        if reduction_type == \'concat\':\n            return tf.concat(tensor_list, axis=0)\n        raise ValueError(\'Unrecognized reduction policy: ""{}\'.format(reduction_type))\n\n    def _get_decision(self, mutable):\n        if mutable.key not in self._cache:\n            raise ValueError(\'""{}"" not found in decision cache.\'.format(mutable.key))\n        result = self._cache[mutable.key]\n        _logger.debug(\'Decision %s: %s\', mutable.key, result)\n        return result\n'"
src/sdk/pynni/nni/nas/tensorflow/utils.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\n\n_counter = 0\n\ndef global_mutable_counting():\n    global _counter\n    _counter += 1\n    return _counter\n\n\nclass AverageMeter:\n    def __init__(self, name):\n        self.name = name\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val):\n        self.val = val\n        self.sum += val\n        self.count += 1\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        return \'{name} {val:4f} ({avg:4f})\'.format(**self.__dict__)\n\n    def summary(self):\n        return \'{name}: {avg:4f}\'.format(**self.__dict__)\n\n\nclass AverageMeterGroup:\n    def __init__(self):\n        self.meters = {}\n\n    def update(self, data):\n        for k, v in data.items():\n            if k not in self.meters:\n                self.meters[k] = AverageMeter(k)\n            self.meters[k].update(v)\n\n    def __str__(self):\n        return \'  \'.join(str(v) for v in self.meters.values())\n\n    def summary(self):\n        return \'  \'.join(v.summary() for v in self.meters.values())\n\n\nclass StructuredMutableTreeNode:\n    def __init__(self, mutable):\n        self.mutable = mutable\n        self.children = []\n\n    def add_child(self, mutable):\n        self.children.append(StructuredMutableTreeNode(mutable))\n        return self.children[-1]\n\n    def type(self):\n        return type(self.mutable)\n\n    def __iter__(self):\n        return self.traverse()\n\n    def traverse(self, order=""pre"", deduplicate=True, memo=None):\n        if memo is None:\n            memo = set()\n        assert order in [""pre"", ""post""]\n        if order == ""pre"":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n        for child in self.children:\n            for m in child.traverse(order=order, deduplicate=deduplicate, memo=memo):\n                yield m\n        if order == ""post"":\n            if self.mutable is not None:\n                if not deduplicate or self.mutable.key not in memo:\n                    memo.add(self.mutable.key)\n                    yield self.mutable\n\n\ndef fill_zero_grads(grads, weights):\n    ret = []\n    for grad, weight in zip(grads, weights):\n        if grad is not None:\n            ret.append(grad)\n        else:\n            ret.append(tf.zeros_like(weight))\n    return ret\n'"
src/sdk/pynni/tests/models/pytorch_models/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .layer_choice_only import LayerChoiceOnlySearchSpace\nfrom .mutable_scope import SpaceWithMutableScope\nfrom .naive import NaiveSearchSpace\nfrom .nested import NestedSpace\n'
src/sdk/pynni/tests/models/pytorch_models/layer_choice_only.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutables import LayerChoice\n\n\nclass LayerChoiceOnlySearchSpace(nn.Module):\n    def __init__(self, test_case):\n        super().__init__()\n        self.test_case = test_case\n        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(3, 6, 5, padding=2)])\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d(6, 16, 5, padding=2)],\n                                 return_mask=True)\n        self.conv3 = nn.Conv2d(16, 16, 1)\n        self.bn = nn.BatchNorm2d(16)\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(16, 10)\n\n    def forward(self, x):\n        bs = x.size(0)\n\n        x = self.pool(F.relu(self.conv1(x)))\n        x0, mask = self.conv2(x)\n        self.test_case.assertEqual(mask.size(), torch.Size([2]))\n        x1 = F.relu(self.conv3(x0))\n\n        x = self.pool(self.bn(x1))\n        self.test_case.assertEqual(mask.size(), torch.Size([2]))\n\n        x = self.gap(x).view(bs, -1)\n        x = self.fc(x)\n        return x\n'"
src/sdk/pynni/tests/models/pytorch_models/mutable_scope.py,6,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice, MutableScope\n\n\nclass Cell(MutableScope):\n    def __init__(self, cell_name, prev_labels, channels):\n        super().__init__(cell_name)\n        self.input_choice = InputChoice(choose_from=prev_labels, n_chosen=1, return_mask=True,\n                                        key=cell_name + ""_input"")\n        self.op_choice = LayerChoice([\n            nn.Conv2d(channels, channels, 3, padding=1),\n            nn.Conv2d(channels, channels, 5, padding=2),\n            nn.MaxPool2d(3, stride=1, padding=1),\n            nn.AvgPool2d(3, stride=1, padding=1),\n            nn.Identity()\n        ], key=cell_name + ""_op"")\n\n    def forward(self, prev_layers):\n        chosen_input, chosen_mask = self.input_choice(prev_layers)\n        cell_out = self.op_choice(chosen_input)\n        return cell_out, chosen_mask\n\n\nclass Node(MutableScope):\n    def __init__(self, node_name, prev_node_names, channels):\n        super().__init__(node_name)\n        self.cell_x = Cell(node_name + ""_x"", prev_node_names, channels)\n        self.cell_y = Cell(node_name + ""_y"", prev_node_names, channels)\n\n    def forward(self, prev_layers):\n        out_x, mask_x = self.cell_x(prev_layers)\n        out_y, mask_y = self.cell_y(prev_layers)\n        return out_x + out_y, mask_x | mask_y\n\n\nclass Layer(nn.Module):\n    def __init__(self, num_nodes, channels):\n        super().__init__()\n        self.num_nodes = num_nodes\n        self.nodes = nn.ModuleList()\n        node_labels = [InputChoice.NO_KEY, InputChoice.NO_KEY]\n        for i in range(num_nodes):\n            node_labels.append(""node_{}"".format(i))\n            self.nodes.append(Node(node_labels[-1], node_labels[:-1], channels))\n        self.final_conv_w = nn.Parameter(torch.zeros(channels, self.num_nodes + 2, channels, 1, 1),\n                                         requires_grad=True)\n        self.bn = nn.BatchNorm2d(channels, affine=False)\n\n    def forward(self, pprev, prev):\n        prev_nodes_out = [pprev, prev]\n        nodes_used_mask = torch.zeros(self.num_nodes + 2, dtype=torch.bool, device=prev.device)\n        for i in range(self.num_nodes):\n            node_out, mask = self.nodes[i](prev_nodes_out)\n            nodes_used_mask[:mask.size(0)] |= mask.to(prev.device)\n            # NOTE: which device should we put mask on?\n            prev_nodes_out.append(node_out)\n\n        unused_nodes = torch.cat([out for used, out in zip(nodes_used_mask, prev_nodes_out) if not used], 1)\n        unused_nodes = F.relu(unused_nodes)\n        conv_weight = self.final_conv_w[:, ~nodes_used_mask, :, :, :]\n        conv_weight = conv_weight.view(conv_weight.size(0), -1, 1, 1)\n        out = F.conv2d(unused_nodes, conv_weight)\n        return prev, self.bn(out)\n\n\nclass SpaceWithMutableScope(nn.Module):\n    def __init__(self, test_case, num_layers=4, num_nodes=5, channels=16, in_channels=3, num_classes=10):\n        super().__init__()\n        self.test_case = test_case\n        self.num_layers = num_layers\n\n        self.stem = nn.Sequential(\n            nn.Conv2d(in_channels, channels, 3, 1, 1, bias=False),\n            nn.BatchNorm2d(channels)\n        )\n\n        self.layers = nn.ModuleList()\n        for _ in range(self.num_layers + 2):\n            self.layers.append(Layer(num_nodes, channels))\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Linear(channels, num_classes)\n\n    def forward(self, x):\n        prev = cur = self.stem(x)\n        for layer in self.layers:\n            prev, cur = layer(prev, cur)\n\n        cur = self.gap(F.relu(cur)).view(x.size(0), -1)\n        return self.dense(cur)\n'"
src/sdk/pynni/tests/models/pytorch_models/naive.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\n\n\nclass NaiveSearchSpace(nn.Module):\n    def __init__(self, test_case):\n        super().__init__()\n        self.test_case = test_case\n        self.conv1 = LayerChoice([nn.Conv2d(3, 6, 3, padding=1), nn.Conv2d(3, 6, 5, padding=2)])\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = LayerChoice([nn.Conv2d(6, 16, 3, padding=1), nn.Conv2d(6, 16, 5, padding=2)],\n                                 return_mask=True)\n        self.conv3 = nn.Conv2d(16, 16, 1)\n\n        self.skipconnect = InputChoice(n_candidates=1)\n        self.skipconnect2 = InputChoice(n_candidates=2, return_mask=True)\n        self.bn = nn.BatchNorm2d(16)\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(16, 10)\n\n    def forward(self, x):\n        bs = x.size(0)\n\n        x = self.pool(F.relu(self.conv1(x)))\n        x0, mask = self.conv2(x)\n        self.test_case.assertEqual(mask.size(), torch.Size([2]))\n        x1 = F.relu(self.conv3(x0))\n\n        _, mask = self.skipconnect2([x0, x1])\n        x0 = self.skipconnect([x0])\n        if x0 is not None:\n            x1 += x0\n        x = self.pool(self.bn(x1))\n        self.test_case.assertEqual(mask.size(), torch.Size([2]))\n\n        x = self.gap(x).view(bs, -1)\n        x = self.fc(x)\n        return x\n'"
src/sdk/pynni/tests/models/pytorch_models/nested.py,3,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\n\n\nclass MutableOp(nn.Module):\n    def __init__(self, kernel_size):\n        super().__init__()\n        self.conv = nn.Conv2d(3, 120, kernel_size, padding=kernel_size // 2)\n        self.nested_mutable = InputChoice(n_candidates=10)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass NestedSpace(nn.Module):\n    # this doesn't pass tests\n    def __init__(self, test_case):\n        super().__init__()\n        self.test_case = test_case\n        self.conv1 = LayerChoice([MutableOp(3), MutableOp(5)])\n        self.gap = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Linear(120, 10)\n\n    def forward(self, x):\n        bs = x.size(0)\n        x = F.relu(self.conv1(x))\n        x = self.gap(x).view(bs, -1)\n        x = self.fc(x)\n        return x\n"""
src/sdk/pynni/nni/compression/torch/pruning/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .pruners import *\nfrom .weight_rank_filter_pruners import *\nfrom .activation_rank_filter_pruners import *\nfrom .apply_compression import apply_compression_results\nfrom .gradient_rank_filter_pruners import *\n'
src/sdk/pynni/nni/compression/torch/pruning/activation_rank_filter_pruners.py,20,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom schema import And, Optional\nfrom ..utils.config_validation import CompressorSchema\nfrom ..compressor import Pruner\n\n__all__ = [\'ActivationAPoZRankFilterPruner\', \'ActivationMeanRankFilterPruner\']\n\nlogger = logging.getLogger(\'torch activation rank filter pruners\')\n\nclass ActivationRankFilterPruner(Pruner):\n    """"""\n    A structured pruning base class that prunes the filters with the smallest\n    importance criterion in convolution layers (using activation values)\n    to achieve a preset level of network sparsity.\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None, activation=\'relu\', statistics_batch_num=1):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        activation : str\n            Activation function\n        statistics_batch_num : int\n            Num of batches for activation statistics\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(""if_calculated"", False)\n        self.statistics_batch_num = statistics_batch_num\n        self.hook_id = self._add_activation_collector()\n\n        assert activation in [\'relu\', \'relu6\']\n        if activation == \'relu\':\n            self.activation = torch.nn.functional.relu\n        elif activation == \'relu6\':\n            self.activation = torch.nn.functional.relu6\n        else:\n            self.activation = None\n\n    def _add_activation_collector(self):\n        def collector(collected_activation):\n            def hook(module_, input_, output):\n                collected_activation.append(self.activation(output.detach().cpu()))\n            return hook\n        self.collected_activation = {}\n        self._fwd_hook_id += 1\n        self._fwd_hook_handles[self._fwd_hook_id] = []\n\n        for wrapper_idx, wrapper in enumerate(self.get_modules_wrapper()):\n            self.collected_activation[wrapper_idx] = []\n            handle = wrapper.register_forward_hook(collector(self.collected_activation[wrapper_idx]))\n            self._fwd_hook_handles[self._fwd_hook_id].append(handle)\n        return self._fwd_hook_id\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n       """"""\n        schema = CompressorSchema([{\n            \'sparsity\': And(float, lambda n: 0 < n < 1),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def get_mask(self, base_mask, activations, num_prune):\n        raise NotImplementedError(\'{} get_mask is not implemented\'.format(self.__class__.__name__))\n\n    def calc_mask(self, wrapper, wrapper_idx, **kwargs):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest importance criterion which is calculated from the activation are masked.\n\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        weight = wrapper.module.weight.data\n        op_type = wrapper.type\n        config = wrapper.config\n        assert 0 <= config.get(\'sparsity\') < 1, ""sparsity must in the range [0, 1)""\n        assert op_type in [\'Conv2d\'], ""only support Conv2d""\n        assert op_type in config.get(\'op_types\')\n\n        if wrapper.if_calculated:\n            return None\n\n        mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        if hasattr(wrapper.module, \'bias\') and wrapper.module.bias is not None:\n            mask_bias = torch.ones(wrapper.module.bias.size()).type_as(wrapper.module.bias).detach()\n        else:\n            mask_bias = None\n        mask = {\'weight_mask\': mask_weight, \'bias_mask\': mask_bias}\n\n        try:\n            filters = weight.size(0)\n            num_prune = int(filters * config.get(\'sparsity\'))\n            acts = self.collected_activation[wrapper_idx]\n            if filters < 2 or num_prune < 1 or len(acts) < self.statistics_batch_num:\n                return mask\n            mask = self.get_mask(mask, acts, num_prune)\n        finally:\n            if len(acts) >= self.statistics_batch_num:\n                wrapper.if_calculated = True\n                if self.hook_id in self._fwd_hook_handles:\n                    self.remove_activation_collector(self.hook_id)\n\n        return mask\n\n\nclass ActivationAPoZRankFilterPruner(ActivationRankFilterPruner):\n    """"""\n    A structured pruning algorithm that prunes the filters with the\n    smallest APoZ(average percentage of zeros) of output activations.\n    Hengyuan Hu, Rui Peng, Yu-Wing Tai and Chi-Keung Tang,\n    ""Network Trimming: A Data-Driven Neuron Pruning Approach towards Efficient Deep Architectures"", ICLR 2016.\n    https://arxiv.org/abs/1607.03250\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None, activation=\'relu\', statistics_batch_num=1):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        activation : str\n            Activation function\n        statistics_batch_num : int\n            Num of batches for activation statistics\n        """"""\n        super().__init__(model, config_list, optimizer, activation, statistics_batch_num)\n\n    def get_mask(self, base_mask, activations, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the largest APoZ(average percentage of zeros) of output activations are masked.\n\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight, all item in the basic mask is 1.\n        activations : list\n            Layer\'s output activations\n        num_prune : int\n            Num of filters to prune\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        apoz = self._calc_apoz(activations)\n        prune_indices = torch.argsort(apoz, descending=True)[:num_prune]\n        for idx in prune_indices:\n            base_mask[\'weight_mask\'][idx] = 0.\n            if base_mask[\'bias_mask\'] is not None:\n                base_mask[\'bias_mask\'][idx] = 0.\n        return base_mask\n\n    def _calc_apoz(self, activations):\n        """"""\n        Calculate APoZ(average percentage of zeros) of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer\'s output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter\'s APoZ(average percentage of zeros) of the activations\n        """"""\n        activations = torch.cat(activations, 0)\n        _eq_zero = torch.eq(activations, torch.zeros_like(activations))\n        _apoz = torch.sum(_eq_zero, dim=(0, 2, 3)) / torch.numel(_eq_zero[:, 0, :, :])\n        return _apoz\n\n\nclass ActivationMeanRankFilterPruner(ActivationRankFilterPruner):\n    """"""\n    A structured pruning algorithm that prunes the filters with the\n    smallest mean value of output activations.\n    Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila and Jan Kautz,\n    ""Pruning Convolutional Neural Networks for Resource Efficient Inference"", ICLR 2017.\n    https://arxiv.org/abs/1611.06440\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None, activation=\'relu\', statistics_batch_num=1):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        activation : str\n            Activation function\n        statistics_batch_num : int\n            Num of batches for activation statistics\n        """"""\n        super().__init__(model, config_list, optimizer, activation, statistics_batch_num)\n\n    def get_mask(self, base_mask, activations, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest APoZ(average percentage of zeros) of output activations are masked.\n\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight, all item in the basic mask is 1.\n        activations : list\n            Layer\'s output activations\n        num_prune : int\n            Num of filters to prune\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        mean_activation = self._cal_mean_activation(activations)\n        prune_indices = torch.argsort(mean_activation)[:num_prune]\n        for idx in prune_indices:\n            base_mask[\'weight_mask\'][idx] = 0.\n            if base_mask[\'bias_mask\'] is not None:\n                base_mask[\'bias_mask\'][idx] = 0.\n        return base_mask\n\n    def _cal_mean_activation(self, activations):\n        """"""\n        Calculate mean value of activations.\n\n        Parameters\n        ----------\n        activations : list\n            Layer\'s output activations\n\n        Returns\n        -------\n        torch.Tensor\n            Filter\'s mean value of the output activations\n        """"""\n        activations = torch.cat(activations, 0)\n        mean_activation = torch.mean(activations, dim=(0, 2, 3))\n        return mean_activation\n'"
src/sdk/pynni/nni/compression/torch/pruning/apply_compression.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\n\nlogger = logging.getLogger(\'torch apply compression\')\n\ndef apply_compression_results(model, masks_file, map_location=None):\n    """"""\n    Apply the masks from ```masks_file``` to the model\n    Note: this API is for inference, because it simply multiplies weights with\n    corresponding masks when this API is called.\n\n    Parameters\n    ----------\n    model : torch.nn.module\n        The model to be compressed\n    masks_file : str\n        The path of the mask file\n    map_location : str\n        the device on which masks are placed, same to map_location in ```torch.load```\n    """"""\n    masks = torch.load(masks_file, map_location)\n    for name, module in model.named_modules():\n        if name in masks:\n            module.weight.data = module.weight.data.mul_(masks[name][\'weight\'])\n            if hasattr(module, \'bias\') and module.bias is not None and \'bias\' in masks[name]:\n                module.bias.data = module.bias.data.mul_(masks[name][\'bias\'])'"
src/sdk/pynni/nni/compression/torch/pruning/gradient_rank_filter_pruners.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom ..compressor import Pruner\n\n__all__ = [\'TaylorFOWeightFilterPruner\']\n\nlogger = logging.getLogger(\'torch gradient rank filter pruners\')\n\nclass GradientRankFilterPruner(Pruner):\n    """"""\n    A structured pruning base class that prunes the filters with the smallest\n    importance criterion in convolution layers (using gradient values)\n    to achieve a preset level of network sparsity.\n    """"""\n\n    def __init__(self, model, config_list, optimizer, statistics_batch_num=1):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        statistics_batch_num : int\n            Num of batches for calculating contribution\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(""if_calculated"", False)\n        self.set_wrappers_attribute(""contribution"", None)\n        self.statistics_batch_num = statistics_batch_num\n        self.iterations = 0\n        self.old_step = self.optimizer.step\n        self.patch_optimizer(self.calc_contributions)\n\n    def calc_contributions(self):\n        raise NotImplementedError(\'{} calc_contributions is not implemented\'.format(self.__class__.__name__))\n\n    def get_mask(self, base_mask, contribution, num_prune):\n        raise NotImplementedError(\'{} get_mask is not implemented\'.format(self.__class__.__name__))\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest importance criterion which is calculated from the activation are masked.\n\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        weight = wrapper.module.weight.data\n        op_type = wrapper.type\n        config = wrapper.config\n        assert 0 <= config.get(\'sparsity\') < 1, ""sparsity must in the range [0, 1)""\n        assert op_type in config.get(\'op_types\')\n\n        if wrapper.if_calculated:\n            return None\n\n        mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        if hasattr(wrapper.module, \'bias\') and wrapper.module.bias is not None:\n            mask_bias = torch.ones(wrapper.module.bias.size()).type_as(wrapper.module.bias).detach()\n        else:\n            mask_bias = None\n        mask = {\'weight_mask\': mask_weight, \'bias_mask\': mask_bias}\n        try:\n            filters = weight.size(0)\n            num_prune = int(filters * config.get(\'sparsity\'))\n            if filters < 2 or num_prune < 1 or self.iterations < self.statistics_batch_num:\n                return mask\n\n            mask = self.get_mask(mask, wrapper.contribution, num_prune)\n        finally:\n            if self.iterations >= self.statistics_batch_num:\n                wrapper.if_calculated = True\n\n        return mask\n\n\nclass TaylorFOWeightFilterPruner(GradientRankFilterPruner):\n    """"""\n    A structured pruning algorithm that prunes the filters with the smallest\n    importance approximations based on the first order taylor expansion on the weight.\n    Molchanov, Pavlo and Mallya, Arun and Tyree, Stephen and Frosio, Iuri and Kautz, Jan,\n    ""Importance Estimation for Neural Network Pruning"", CVPR 2019.\n    http://jankautz.com/publications/Importance4NNPruning_CVPR19.pdf\n    """"""\n\n    def __init__(self, model, config_list, optimizer, statistics_batch_num=1):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        statistics_batch_num : int\n            Num of batches for activation statistics\n        """"""\n        super().__init__(model, config_list, optimizer, statistics_batch_num)\n\n    def get_mask(self, base_mask, contribution, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest importance approximations are masked.\n\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight, all item in the basic mask is 1.\n        contribution : torch.Tensor\n            Layer\'s importance approximations\n        num_prune : int\n            Num of filters to prune\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        prune_indices = torch.argsort(contribution)[:num_prune]\n        for idx in prune_indices:\n            base_mask[\'weight_mask\'][idx] = 0.\n            if base_mask[\'bias_mask\'] is not None:\n                base_mask[\'bias_mask\'][idx] = 0.\n        return base_mask\n\n    def calc_contributions(self):\n        """"""\n        Calculate the estimated importance of filters as a sum of individual contribution\n        based on the first order taylor expansion.\n        """"""\n\n        if self.iterations >= self.statistics_batch_num:\n            return\n        for wrapper in self.get_modules_wrapper():\n            filters = wrapper.module.weight.size(0)\n            contribution = (wrapper.module.weight*wrapper.module.weight.grad).data.pow(2).view(filters, -1).sum(dim=1)\n            if wrapper.contribution is None:\n                wrapper.contribution = contribution\n            else:\n                wrapper.contribution += contribution\n\n        self.iterations += 1\n'"
src/sdk/pynni/nni/compression/torch/pruning/pruners.py,23,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport copy\nimport logging\nimport torch\nfrom schema import And, Optional\nfrom ..utils.config_validation import CompressorSchema\nfrom ..compressor import Pruner\n\n__all__ = [\'LevelPruner\', \'AGP_Pruner\', \'SlimPruner\', \'LotteryTicketPruner\']\n\nlogger = logging.getLogger(\'torch pruner\')\n\n\nclass LevelPruner(Pruner):\n    """"""\n    Prune to an exact pruning level specification\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(""if_calculated"", False)\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        """"""\n        schema = CompressorSchema([{\n            \'sparsity\': And(float, lambda n: 0 < n < 1),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Calculate the mask of given layer\n        Parameters\n        ----------\n        wrapper : Module\n            the module to instrument the compression operation\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        config = wrapper.config\n        weight = wrapper.module.weight.data\n\n        if not wrapper.if_calculated:\n            w_abs = weight.abs()\n            k = int(weight.numel() * config[\'sparsity\'])\n            if k == 0:\n                return torch.ones(weight.shape).type_as(weight)\n            threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n            mask_weight = torch.gt(w_abs, threshold).type_as(weight)\n            mask = {\'weight_mask\': mask_weight}\n            wrapper.if_calculated = True\n            return mask\n        else:\n            return None\n\n\nclass AGP_Pruner(Pruner):\n    """"""\n    An automated gradual pruning algorithm that prunes the smallest magnitude\n    weights to achieve a preset level of network sparsity.\n    Michael Zhu and Suyog Gupta, ""To prune, or not to prune: exploring the\n    efficacy of pruning for model compression"", 2017 NIPS Workshop on Machine\n    Learning of Phones and other Consumer Devices,\n    https://arxiv.org/pdf/1710.01878.pdf\n    """"""\n\n    def __init__(self, model, config_list, optimizer):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        assert isinstance(optimizer, torch.optim.Optimizer), ""AGP pruner is an iterative pruner, please pass optimizer of the model to it""\n\n        self.now_epoch = 0\n        self.set_wrappers_attribute(""if_calculated"", False)\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            List on pruning configs\n        """"""\n        schema = CompressorSchema([{\n            \'initial_sparsity\': And(float, lambda n: 0 <= n <= 1),\n            \'final_sparsity\': And(float, lambda n: 0 <= n <= 1),\n            \'start_epoch\': And(int, lambda n: n >= 0),\n            \'end_epoch\': And(int, lambda n: n >= 0),\n            \'frequency\': And(int, lambda n: n > 0),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Calculate the mask of given layer.\n        Scale factors with the smallest absolute value in the BN layer are masked.\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        config = wrapper.config\n        weight = wrapper.module.weight.data\n        start_epoch = config.get(\'start_epoch\', 0)\n        freq = config.get(\'frequency\', 1)\n\n        if wrapper.if_calculated:\n            return None\n        if not (self.now_epoch >= start_epoch and (self.now_epoch - start_epoch) % freq == 0):\n            return None\n\n        mask = {\'weight_mask\': wrapper.weight_mask}\n        target_sparsity = self.compute_target_sparsity(config)\n        k = int(weight.numel() * target_sparsity)\n        if k == 0 or target_sparsity >= 1 or target_sparsity <= 0:\n            return mask\n        # if we want to generate new mask, we should update weigth first\n        w_abs = weight.abs() * mask[\'weight_mask\']\n        threshold = torch.topk(w_abs.view(-1), k, largest=False)[0].max()\n        new_mask = {\'weight_mask\': torch.gt(w_abs, threshold).type_as(weight)}\n        wrapper.if_calculated = True\n\n        return new_mask\n\n    def compute_target_sparsity(self, config):\n        """"""\n        Calculate the sparsity for pruning\n        Parameters\n        ----------\n        config : dict\n            Layer\'s pruning config\n        Returns\n        -------\n        float\n            Target sparsity to be pruned\n        """"""\n\n        end_epoch = config.get(\'end_epoch\', 1)\n        start_epoch = config.get(\'start_epoch\', 0)\n        freq = config.get(\'frequency\', 1)\n        final_sparsity = config.get(\'final_sparsity\', 0)\n        initial_sparsity = config.get(\'initial_sparsity\', 0)\n        if end_epoch <= start_epoch or initial_sparsity >= final_sparsity:\n            logger.warning(\'your end epoch <= start epoch or initial_sparsity >= final_sparsity\')\n            return final_sparsity\n\n        if end_epoch <= self.now_epoch:\n            return final_sparsity\n\n        span = ((end_epoch - start_epoch - 1) // freq) * freq\n        assert span > 0\n        target_sparsity = (final_sparsity +\n                           (initial_sparsity - final_sparsity) *\n                           (1.0 - ((self.now_epoch - start_epoch) / span)) ** 3)\n        return target_sparsity\n\n    def update_epoch(self, epoch):\n        """"""\n        Update epoch\n        Parameters\n        ----------\n        epoch : int\n            current training epoch\n        """"""\n\n        if epoch > 0:\n            self.now_epoch = epoch\n            for wrapper in self.get_modules_wrapper():\n                wrapper.if_calculated = False\n\nclass SlimPruner(Pruner):\n    """"""\n    A structured pruning algorithm that prunes channels by pruning the weights of BN layers.\n    Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan and Changshui Zhang\n    ""Learning Efficient Convolutional Networks through Network Slimming"", 2017 ICCV\n    https://arxiv.org/pdf/1708.06519.pdf\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        weight_list = []\n        if len(config_list) > 1:\n            logger.warning(\'Slim pruner only supports 1 configuration\')\n        config = config_list[0]\n        for (layer, config) in self.get_modules_to_compress():\n            assert layer.type == \'BatchNorm2d\', \'SlimPruner only supports 2d batch normalization layer pruning\'\n            weight_list.append(layer.module.weight.data.abs().clone())\n        all_bn_weights = torch.cat(weight_list)\n        k = int(all_bn_weights.shape[0] * config[\'sparsity\'])\n        self.global_threshold = torch.topk(all_bn_weights.view(-1), k, largest=False)[0].max()\n        self.set_wrappers_attribute(""if_calculated"", False)\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        """"""\n        schema = CompressorSchema([{\n            \'sparsity\': And(float, lambda n: 0 < n < 1),\n            \'op_types\': [\'BatchNorm2d\'],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Calculate the mask of given layer.\n        Scale factors with the smallest absolute value in the BN layer are masked.\n        Parameters\n        ----------\n        wrapper : Module\n            the layer to instrument the compression operation\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        config = wrapper.config\n        weight = wrapper.module.weight.data\n        op_type = wrapper.type\n\n        assert op_type == \'BatchNorm2d\', \'SlimPruner only supports 2d batch normalization layer pruning\'\n        if wrapper.if_calculated:\n            return None\n        base_mask = torch.ones(weight.size()).type_as(weight).detach()\n        mask = {\'weight_mask\': base_mask.detach(), \'bias_mask\': base_mask.clone().detach()}\n        filters = weight.size(0)\n        num_prune = int(filters * config.get(\'sparsity\'))\n        if filters >= 2 and num_prune >= 1:\n            w_abs = weight.abs()\n            mask_weight = torch.gt(w_abs, self.global_threshold).type_as(weight)\n            mask_bias = mask_weight.clone()\n            mask = {\'weight_mask\': mask_weight.detach(), \'bias_mask\': mask_bias.detach()}\n        wrapper.if_calculated = True\n        return mask\n\nclass LotteryTicketPruner(Pruner):\n    """"""\n    This is a Pytorch implementation of the paper ""The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"",\n    following NNI model compression interface.\n\n    1. Randomly initialize a neural network f(x;theta_0) (where theta_0 follows D_{theta}).\n    2. Train the network for j iterations, arriving at parameters theta_j.\n    3. Prune p% of the parameters in theta_j, creating a mask m.\n    4. Reset the remaining parameters to their values in theta_0, creating the winning ticket f(x;m*theta_0).\n    5. Repeat step 2, 3, and 4.\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None, lr_scheduler=None, reset_weights=True):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            The model to be pruned\n        config_list : list\n            Supported keys:\n                - prune_iterations : The number of rounds for the iterative pruning.\n                - sparsity : The final sparsity when the compression is done.\n        optimizer : pytorch optimizer\n            The optimizer for the model\n        lr_scheduler : pytorch lr scheduler\n            The lr scheduler for the model if used\n        reset_weights : bool\n            Whether reset weights and optimizer at the beginning of each round.\n        """"""\n        # save init weights and optimizer\n        self.reset_weights = reset_weights\n        if self.reset_weights:\n            self._model = model\n            self._optimizer = optimizer\n            self._model_state = copy.deepcopy(model.state_dict())\n            self._optimizer_state = copy.deepcopy(optimizer.state_dict())\n            self._lr_scheduler = lr_scheduler\n            if lr_scheduler is not None:\n                self._scheduler_state = copy.deepcopy(lr_scheduler.state_dict())\n\n        super().__init__(model, config_list, optimizer)\n        self.curr_prune_iteration = None\n        self.prune_iterations = config_list[0][\'prune_iterations\']\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            Supported keys:\n                - prune_iterations : The number of rounds for the iterative pruning.\n                - sparsity : The final sparsity when the compression is done.\n        """"""\n        schema = CompressorSchema([{\n            \'sparsity\': And(float, lambda n: 0 < n < 1),\n            \'prune_iterations\': And(int, lambda n: n > 0),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n        assert len(set([x[\'prune_iterations\'] for x in config_list])) == 1, \'The values of prune_iterations must be equal in your config\'\n\n    def _calc_sparsity(self, sparsity):\n        keep_ratio_once = (1 - sparsity) ** (1 / self.prune_iterations)\n        curr_keep_ratio = keep_ratio_once ** self.curr_prune_iteration\n        return max(1 - curr_keep_ratio, 0)\n\n    def _calc_mask(self, weight, sparsity, curr_w_mask):\n        if self.curr_prune_iteration == 0:\n            mask = torch.ones(weight.shape).type_as(weight)\n        else:\n            curr_sparsity = self._calc_sparsity(sparsity)\n            w_abs = weight.abs() * curr_w_mask\n            k = int(w_abs.numel() * curr_sparsity)\n            threshold = torch.topk(w_abs.view(-1), k, largest=False).values.max()\n            mask = torch.gt(w_abs, threshold).type_as(weight)\n        return {\'weight_mask\': mask}\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Generate mask for the given ``weight``.\n\n        Parameters\n        ----------\n        wrapper : Module\n            The layer to be pruned\n\n        Returns\n        -------\n        tensor\n            The mask for this weight, it is ```None``` because this pruner\n            calculates and assigns masks in ```prune_iteration_start```,\n            no need to do anything in this function.\n        """"""\n        return None\n\n    def get_prune_iterations(self):\n        """"""\n        Return the range for iterations.\n        In the first prune iteration, masks are all one, thus, add one more iteration\n\n        Returns\n        -------\n        list\n            A list for pruning iterations\n        """"""\n        return range(self.prune_iterations + 1)\n\n    def prune_iteration_start(self):\n        """"""\n        Control the pruning procedure on updated epoch number.\n        Should be called at the beginning of the epoch.\n        """"""\n        if self.curr_prune_iteration is None:\n            self.curr_prune_iteration = 0\n        else:\n            self.curr_prune_iteration += 1\n        assert self.curr_prune_iteration < self.prune_iterations + 1, \'Exceed the configured prune_iterations\'\n\n        modules_wrapper = self.get_modules_wrapper()\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            module_wrapper = None\n            for wrapper in modules_wrapper:\n                if wrapper.name == layer.name:\n                    module_wrapper = wrapper\n                    break\n            assert module_wrapper is not None\n\n            sparsity = config.get(\'sparsity\')\n            mask = self._calc_mask(layer.module.weight.data, sparsity, module_wrapper.weight_mask)\n            # TODO: directly use weight_mask is not good\n            module_wrapper.weight_mask = mask[\'weight_mask\']\n            # there is no mask for bias\n\n        # reinit weights back to original after new masks are generated\n        if self.reset_weights:\n            # should use this member function to reset model weights\n            self.load_model_state_dict(self._model_state)\n            self._optimizer.load_state_dict(self._optimizer_state)\n            if self._lr_scheduler is not None:\n                self._lr_scheduler.load_state_dict(self._scheduler_state)\n'"
src/sdk/pynni/nni/compression/torch/pruning/weight_rank_filter_pruners.py,23,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom schema import And, Optional\nfrom ..utils.config_validation import CompressorSchema\nfrom ..compressor import Pruner\n\n__all__ = [\'L1FilterPruner\', \'L2FilterPruner\', \'FPGMPruner\']\n\nlogger = logging.getLogger(\'torch weight rank filter pruners\')\n\nclass WeightRankFilterPruner(Pruner):\n    """"""\n    A structured pruning base class that prunes the filters with the smallest\n    importance criterion in convolution layers to achieve a preset level of network sparsity.\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n        self.set_wrappers_attribute(""if_calculated"", False)\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n       """"""\n        schema = CompressorSchema([{\n            \'sparsity\': And(float, lambda n: 0 < n < 1),\n            Optional(\'op_types\'): [\'Conv2d\'],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def get_mask(self, base_mask, weight, num_prune):\n        raise NotImplementedError(\'{} get_mask is not implemented\'.format(self.__class__.__name__))\n\n    def calc_mask(self, wrapper, **kwargs):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest importance criterion of the kernel weights are masked.\n        Parameters\n        ----------\n        wrapper : Module\n            the module to instrument the compression operation\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        weight = wrapper.module.weight.data\n        op_type = wrapper.type\n        config = wrapper.config\n        assert 0 <= config.get(\'sparsity\') < 1, ""sparsity must in the range [0, 1)""\n        assert op_type in [\'Conv1d\', \'Conv2d\'], ""only support Conv1d and Conv2d""\n        assert op_type in config.get(\'op_types\')\n\n        if wrapper.if_calculated:\n            return None\n        mask_weight = torch.ones(weight.size()).type_as(weight).detach()\n        if hasattr(wrapper.module, \'bias\') and wrapper.module.bias is not None:\n            mask_bias = torch.ones(wrapper.module.bias.size()).type_as(wrapper.module.bias).detach()\n        else:\n            mask_bias = None\n        mask = {\'weight_mask\': mask_weight, \'bias_mask\': mask_bias}\n        try:\n            filters = weight.size(0)\n            num_prune = int(filters * config.get(\'sparsity\'))\n            if filters < 2 or num_prune < 1:\n                return mask\n            mask = self.get_mask(mask, weight, num_prune)\n        finally:\n            wrapper.if_calculated = True\n        return mask\n\n\nclass L1FilterPruner(WeightRankFilterPruner):\n    """"""\n    A structured pruning algorithm that prunes the filters of smallest magnitude\n    weights sum in the convolution layers to achieve a preset level of network sparsity.\n    Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet and Hans Peter Graf,\n    ""PRUNING FILTERS FOR EFFICIENT CONVNETS"", 2017 ICLR\n    https://arxiv.org/abs/1608.08710\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n\n    def get_mask(self, base_mask, weight, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest sum of its absolute kernel weights are masked.\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight or bias, all item in the basic mask is 1.\n        weight : torch.Tensor\n            Layer\'s weight\n        num_prune : int\n            Num of filters to prune\n\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n\n        filters = weight.shape[0]\n        w_abs = weight.abs()\n        w_abs_structured = w_abs.view(filters, -1).sum(dim=1)\n        threshold = torch.topk(w_abs_structured.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_abs_structured, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_abs_structured, threshold).type_as(weight).detach() if base_mask[\'bias_mask\'] is not None else None\n\n        return {\'weight_mask\': mask_weight.detach(), \'bias_mask\': mask_bias}\n\n\nclass L2FilterPruner(WeightRankFilterPruner):\n    """"""\n    A structured pruning algorithm that prunes the filters with the\n    smallest L2 norm of the weights.\n    """"""\n\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n\n        super().__init__(model, config_list, optimizer)\n\n    def get_mask(self, base_mask, weight, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest L2 norm of the absolute kernel weights are masked.\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight or bias, all item in the basic mask is 1.\n        weight : torch.Tensor\n            Layer\'s weight\n        num_prune : int\n            Num of filters to prune\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        filters = weight.shape[0]\n        w = weight.view(filters, -1)\n        w_l2_norm = torch.sqrt((w ** 2).sum(dim=1))\n        threshold = torch.topk(w_l2_norm.view(-1), num_prune, largest=False)[0].max()\n        mask_weight = torch.gt(w_l2_norm, threshold)[:, None, None, None].expand_as(weight).type_as(weight)\n        mask_bias = torch.gt(w_l2_norm, threshold).type_as(weight).detach() if base_mask[\'bias_mask\'] is not None else None\n\n        return {\'weight_mask\': mask_weight.detach(), \'bias_mask\': mask_bias}\n\n\nclass FPGMPruner(WeightRankFilterPruner):\n    """"""\n    A filter pruner via geometric median.\n    ""Filter Pruning via Geometric Median for Deep Convolutional Neural Networks Acceleration"",\n    https://arxiv.org/pdf/1811.00250.pdf\n    """"""\n\n    def __init__(self, model, config_list, optimizer):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            the model user wants to compress\n        config_list: list\n            support key for each list item:\n                - sparsity: percentage of convolutional filters to be pruned.\n        optimizer: torch.optim.Optimizer\n            Optimizer used to train model\n        """"""\n        super().__init__(model, config_list, optimizer)\n        assert isinstance(optimizer, torch.optim.Optimizer), ""FPGM pruner is an iterative pruner, please pass optimizer of the model to it""\n\n    def get_mask(self, base_mask, weight, num_prune):\n        """"""\n        Calculate the mask of given layer.\n        Filters with the smallest sum of its absolute kernel weights are masked.\n        Parameters\n        ----------\n        base_mask : dict\n            The basic mask with the same shape of weight and bias, all item in the basic mask is 1.\n        weight : torch.Tensor\n            Layer\'s weight\n        num_prune : int\n            Num of filters to prune\n        Returns\n        -------\n        dict\n            dictionary for storing masks\n        """"""\n        min_gm_idx = self._get_min_gm_kernel_idx(weight, num_prune)\n        for idx in min_gm_idx:\n            base_mask[\'weight_mask\'][idx] = 0.\n            if base_mask[\'bias_mask\'] is not None:\n                base_mask[\'bias_mask\'][idx] = 0.\n        return base_mask\n\n    def _get_min_gm_kernel_idx(self, weight, n):\n        assert len(weight.size()) in [3, 4]\n\n        dist_list = []\n        for out_i in range(weight.size(0)):\n            dist_sum = self._get_distance_sum(weight, out_i)\n            dist_list.append((dist_sum, out_i))\n        min_gm_kernels = sorted(dist_list, key=lambda x: x[0])[:n]\n        return [x[1] for x in min_gm_kernels]\n\n    def _get_distance_sum(self, weight, out_idx):\n        """"""\n        Calculate the total distance between a specified filter (by out_idex and in_idx) and\n        all other filters.\n        Optimized verision of following naive implementation:\n        def _get_distance_sum(self, weight, in_idx, out_idx):\n            w = weight.view(-1, weight.size(-2), weight.size(-1))\n            dist_sum = 0.\n            for k in w:\n                dist_sum += torch.dist(k, weight[in_idx, out_idx], p=2)\n            return dist_sum\n        Parameters\n        ----------\n        weight: Tensor\n            convolutional filter weight\n        out_idx: int\n            output channel index of specified filter, this method calculates the total distance\n            between this specified filter and all other filters.\n        Returns\n        -------\n        float32\n            The total distance\n        """"""\n        logger.debug(\'weight size: %s\', weight.size())\n        assert len(weight.size()) in [3, 4], \'unsupported weight shape\'\n\n        w = weight.view(weight.size(0), -1)\n        anchor_w = w[out_idx].unsqueeze(0).expand(w.size(0), w.size(1))\n        x = w - anchor_w\n        x = (x * x).sum(-1)\n        x = torch.sqrt(x)\n        return x.sum()\n\n    def update_epoch(self, epoch):\n        for wrapper in self.get_modules_wrapper():\n            wrapper.if_calculated = False\n'"
src/sdk/pynni/nni/compression/torch/quantization/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .quantizers import *\n'
src/sdk/pynni/nni/compression/torch/quantization/quantizers.py,20,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom schema import Schema, And, Or, Optional\nfrom ..utils.config_validation import CompressorSchema\nfrom ..compressor import Quantizer, QuantGrad, QuantType\n\n__all__ = [\'NaiveQuantizer\', \'QAT_Quantizer\', \'DoReFaQuantizer\', \'BNNQuantizer\']\n\nlogger = logging.getLogger(__name__)\n\n\nclass NaiveQuantizer(Quantizer):\n    """"""quantize weight to 8 bits\n    """"""\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.layer_scale = {}\n\n    def validate_config(self, model, config_list):\n        schema = CompressorSchema([{\n            Optional(\'quant_types\'): [\'weight\'],\n            Optional(\'quant_bits\'): Or(8, {\'weight\': 8}),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        new_scale = weight.abs().max() / 127\n        scale = max(self.layer_scale.get(wrapper.name, 0), new_scale)\n        self.layer_scale[wrapper.name] = scale\n        orig_type = weight.type()  # TODO: user layer\n        return weight.div(scale).type(torch.int8).type(orig_type).mul(scale)\n\n\ndef update_ema(biased_ema, value, decay, step):\n    """"""\n    calculate biased stat and unbiased stat in each step using exponential moving average method\n\n    Parameters\n    ----------\n    biased_ema : float\n        previous stat value\n    value : float\n        current stat value\n    decay : float\n        the weight of previous stat value, larger means smoother curve\n    step : int\n        current step\n\n    Returns\n    -------\n    float, float\n    """"""\n    biased_ema = biased_ema * decay + (1 - decay) * value\n    unbiased_ema = biased_ema / (1 - decay ** step)  # Bias correction\n    return biased_ema, unbiased_ema\n\ndef update_quantization_param(bits, rmin, rmax):\n    """"""\n    calculate the `zero_point` and `scale`.\n\n    Parameters\n    ----------\n    bits : int\n        quantization bits length\n    rmin : float\n        min value of real value\n    rmax : float\n        max value of real value\n\n    Returns\n    -------\n    float, float\n    """"""\n    # extend the [min, max] interval to ensure that it contains 0.\n    # Otherwise, we would not meet the requirement that 0 be an exactly\n    # representable value.\n    rmin = min(rmin, 0)\n    rmax = max(rmax, 0)\n\n    # the min and max quantized values, as floating-point values\n    qmin = 0\n    qmax = (1 << bits) - 1\n    # First determine the scale.\n    scale = (rmax - rmin) / (qmax - qmin)\n\n    # Zero-point computation.\n    initial_zero_point = qmin - rmin / scale\n\n    # Now we need to nudge the zero point to be an integer\n    nudged_zero_point = 0\n    if initial_zero_point < qmin:\n        nudged_zero_point = qmin\n    elif initial_zero_point > qmax:\n        nudged_zero_point = qmax\n    else:\n        nudged_zero_point = torch.round(initial_zero_point)\n\n    return scale, nudged_zero_point\n\n\ndef get_bits_length(config, quant_type):\n    if isinstance(config[""quant_bits""], int):\n        return config[""quant_bits""]\n    else:\n        return config[""quant_bits""].get(quant_type)\n\n\nclass QAT_Quantizer(Quantizer):\n    """"""Quantizer defined in:\n    Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference\n    http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf\n    """"""\n    def __init__(self, model, config_list, optimizer=None):\n        """"""\n        Parameters\n        ----------\n        layer : LayerInfo\n            the layer to quantize\n        config_list : list of dict\n            list of configurations for quantization\n            supported keys for dict:\n                - quant_types : list of string\n                    type of quantization you want to apply, currently support \'weight\', \'input\', \'output\'\n                - quant_bits : int or dict of {str : int}\n                    bits length of quantization, key is the quantization type, value is the length, eg. {\'weight\', 8},\n                    when the type is int, all quantization types share same bits length\n                - quant_start_step : int\n                    disable quantization until model are run by certain number of steps, this allows the network to enter a more stable\n                    state where activation quantization ranges do not exclude a signi\xef\xac\x81cant fraction of values, default value is 0\n                - op_types : list of string\n                    types of nn.module you want to apply quantization, eg. \'Conv2d\'\n        """"""\n        super().__init__(model, config_list, optimizer)\n        self.steps = 1\n        modules_to_compress = self.get_modules_to_compress()\n        for layer, config in modules_to_compress:\n            layer.module.register_buffer(""zero_point"", None)\n            layer.module.register_buffer(""scale"", None)\n            if ""output"" in config.get(""quant_types"", []):\n                layer.module.register_buffer(\'ema_decay\', torch.Tensor([0.99]))\n                layer.module.register_buffer(\'tracked_min_biased\', torch.zeros(1))\n                layer.module.register_buffer(\'tracked_min\', torch.zeros(1))\n                layer.module.register_buffer(\'tracked_max_biased\', torch.zeros(1))\n                layer.module.register_buffer(\'tracked_max\', torch.zeros(1))\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        """"""\n        schema = CompressorSchema([{\n            Optional(\'quant_types\'): Schema([lambda x: x in [\'weight\', \'output\']]),\n            Optional(\'quant_bits\'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional(\'weight\'): And(int, lambda n: 0 < n < 32),\n                Optional(\'output\'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional(\'quant_start_step\'): And(int, lambda n: n >= 0),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def _quantize(self, bits, op, real_val):\n        """"""\n        quantize real value.\n\n        Parameters\n        ----------\n        bits : int\n            quantization bits length\n        op : torch.nn.module\n            target module\n        real_val : float\n            real value to be quantized\n\n        Returns\n        -------\n        float\n        """"""\n        transformed_val = op.zero_point + real_val / op.scale\n        qmin = 0\n        qmax = (1 << bits) - 1\n        clamped_val = torch.clamp(transformed_val, qmin, qmax)\n        quantized_val = torch.round(clamped_val)\n        return quantized_val\n\n    def _dequantize(self, op, quantized_val):\n        """"""\n        dequantize quantized value.\n        Because we simulate quantization in training process, all the computations still happen as float point computations, which means we\n        first quantize tensors then dequantize them. For more details, please refer to the paper.\n\n        Parameters\n        ----------\n        op : torch.nn.Module\n            target module\n        quantized_val : float\n            quantized_val value to be dequantized\n\n        Returns\n        -------\n        float\n        """"""\n        real_val = op.scale * (quantized_val - op.zero_point)\n        return real_val\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        weight_bits = get_bits_length(config, \'weight\')\n        quant_start_step = config.get(\'quant_start_step\', 0)\n        assert weight_bits >= 1, ""quant bits length should be at least 1""\n\n        if quant_start_step > self.steps:\n            return weight\n        rmin, rmax = torch.min(weight), torch.max(weight)\n        module.scale, module.zero_point = update_quantization_param(weight_bits, rmin, rmax)\n        out = self._quantize(weight_bits, module, weight)\n        out = self._dequantize(module, out)\n        return out\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        config = wrapper.config\n        module = wrapper.module\n        output_bits = get_bits_length(config, \'output\')\n        quant_start_step = config.get(\'quant_start_step\', 0)\n        assert output_bits >= 1, ""quant bits length should be at least 1""\n\n        if quant_start_step > self.steps:\n            return output\n\n        current_min, current_max = torch.min(output), torch.max(output)\n        module.tracked_min_biased, module.tracked_min = update_ema(module.tracked_min_biased, current_min, module.ema_decay, self.steps)\n        module.tracked_max_biased, module.tracked_max = update_ema(module.tracked_max_biased, current_max, module.ema_decay, self.steps)\n        module.scale, module.zero_point = update_quantization_param(output_bits, module.tracked_min, module.tracked_max)\n        out = self._quantize(output_bits, module, output)\n        out = self._dequantize(module, out)\n        return out\n\n    def fold_bn(self, config, **kwargs):\n        # TODO simulate folded weight\n        pass\n\n    def step_with_optimizer(self):\n        """"""\n        override `compressor` `step` method, quantization only happens after certain number of steps\n        """"""\n        self.steps += 1\n\n\nclass DoReFaQuantizer(Quantizer):\n    """"""Quantizer using the DoReFa scheme, as defined in:\n    Zhou et al., DoReFa-Net: Training Low Bitwidth Convolutional Neural Networks with Low Bitwidth Gradients\n    (https://arxiv.org/abs/1606.06160)\n    """"""\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        """"""\n        schema = CompressorSchema([{\n            Optional(\'quant_types\'): Schema([lambda x: x in [\'weight\']]),\n            Optional(\'quant_bits\'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional(\'weight\'): And(int, lambda n: 0 < n < 32)\n            })),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        weight_bits = get_bits_length(wrapper.config, \'weight\')\n        out = weight.tanh()\n        out = out / (2 * out.abs().max()) + 0.5\n        out = self.quantize(out, weight_bits)\n        out = 2 * out -1\n        return out\n\n    def quantize(self, input_ri, q_bits):\n        scale = pow(2, q_bits)-1\n        output = torch.round(input_ri*scale)/scale\n        return output\n\n\nclass ClipGrad(QuantGrad):\n    @staticmethod\n    def quant_backward(tensor, grad_output, quant_type):\n        if quant_type == QuantType.QUANT_OUTPUT:\n            grad_output[torch.abs(tensor) > 1] = 0\n        return grad_output\n\n\nclass BNNQuantizer(Quantizer):\n    """"""Binarized Neural Networks, as defined in:\n    Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1\n    (https://arxiv.org/abs/1602.02830)\n    """"""\n    def __init__(self, model, config_list, optimizer=None):\n        super().__init__(model, config_list, optimizer)\n        self.quant_grad = ClipGrad\n\n    def validate_config(self, model, config_list):\n        """"""\n        Parameters\n        ----------\n        model : torch.nn.module\n            Model to be pruned\n        config_list : list of dict\n            List of configurations\n        """"""\n        schema = CompressorSchema([{\n            Optional(\'quant_types\'): Schema([lambda x: x in [\'weight\', \'output\']]),\n            Optional(\'quant_bits\'): Or(And(int, lambda n: 0 < n < 32), Schema({\n                Optional(\'weight\'): And(int, lambda n: 0 < n < 32),\n                Optional(\'output\'): And(int, lambda n: 0 < n < 32),\n            })),\n            Optional(\'op_types\'): [str],\n            Optional(\'op_names\'): [str]\n        }], model, logger)\n\n        schema.validate(config_list)\n\n    def quantize_weight(self, weight, wrapper, **kwargs):\n        out = torch.sign(weight)\n        # remove zeros\n        out[out == 0] = 1\n        return out\n\n    def quantize_output(self, output, wrapper, **kwargs):\n        out = torch.sign(output)\n        # remove zeros\n        out[out == 0] = 1\n        return out\n'"
src/sdk/pynni/nni/compression/torch/speedup/__init__.py,0,b'from .compressor import ModelSpeedup'
src/sdk/pynni/nni/compression/torch/speedup/compress_modules.py,17,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom .infer_shape import ModuleMasks\n\n_logger = logging.getLogger(__name__)\n\nreplace_module = {\n    \'BatchNorm2d\': lambda module, mask: replace_batchnorm2d(module, mask),\n    \'Conv2d\': lambda module, mask: replace_conv2d(module, mask),\n    \'MaxPool2d\': lambda module, mask: no_replace(module, mask),\n    \'AvgPool2d\': lambda module, mask: no_replace(module, mask),\n    \'AdaptiveAvgPool2d\': lambda module, mask: no_replace(module, mask),\n    \'ReLU\': lambda module, mask: no_replace(module, mask),\n    \'Linear\': lambda module, mask: replace_linear(module, mask)\n}\n\ndef no_replace(module, mask):\n    """"""\n    No need to replace\n    """"""\n    _logger.debug(""no need to replace"")\n    return module\n\ndef replace_linear(linear, mask):\n    """"""\n    Parameters\n    ----------\n    linear : torch.nn.Linear\n        The linear module to be replace\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.Linear\n        The new linear module\n    """"""\n    assert isinstance(mask, ModuleMasks)\n    assert mask.input_mask is not None\n    assert mask.output_mask is None\n    assert not mask.param_masks\n    index = mask.input_mask.mask_index[-1]\n    in_features = index.size()[0]\n    _logger.debug(""replace linear with new in_features: %d"", in_features)\n    new_linear = torch.nn.Linear(in_features=in_features,\n                                 out_features=linear.out_features,\n                                 bias=linear.bias is not None)\n    new_linear.to(linear.weight.device)\n    new_linear.weight.data = torch.index_select(linear.weight.data, -1, index.to(linear.weight.device))\n    if linear.bias is not None:\n        new_linear.bias.data.copy_(linear.bias.data)\n    return new_linear\n\ndef replace_batchnorm2d(norm, mask):\n    """"""\n    Parameters\n    ----------\n    norm : torch.nn.BatchNorm2d\n        The batchnorm module to be replace\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.BatchNorm2d\n        The new batchnorm module\n    """"""\n    assert isinstance(mask, ModuleMasks)\n    assert \'weight\' in mask.param_masks and \'bias\' in mask.param_masks\n    index = mask.param_masks[\'weight\'].mask_index[0]\n    num_features = index.size()[0]\n    _logger.debug(""replace batchnorm2d with num_features: %d"", num_features)\n    new_norm = torch.nn.BatchNorm2d(num_features=num_features,\n                                    eps=norm.eps,\n                                    momentum=norm.momentum,\n                                    affine=norm.affine,\n                                    track_running_stats=norm.track_running_stats)\n    # assign weights\n    new_norm.weight.data = torch.index_select(norm.weight.data, 0, index)\n    new_norm.bias.data = torch.index_select(norm.bias.data, 0, index)\n    if norm.track_running_stats:\n        new_norm.running_mean.data = torch.index_select(norm.running_mean.data, 0, index)\n        new_norm.running_var.data = torch.index_select(norm.running_var.data, 0, index)\n    return new_norm\n\ndef replace_conv2d(conv, mask):\n    """"""\n    Parameters\n    ----------\n    conv : torch.nn.Conv2d\n        The conv2d module to be replaced\n    mask : ModuleMasks\n        The masks of this module\n\n    Returns\n    -------\n    torch.nn.Conv2d\n        The new conv2d module\n    """"""\n    assert isinstance(mask, ModuleMasks)\n    if mask.input_mask is None:\n        in_channels = conv.in_channels\n    else:\n        in_channels_index = mask.input_mask.mask_index[1]\n        in_channels = in_channels_index.size()[0]\n    if mask.output_mask is None:\n        out_channels = conv.out_channels\n    else:\n        out_channels_index = mask.output_mask.mask_index[1]\n        out_channels = out_channels_index.size()[0]\n    _logger.debug(""replace conv2d with in_channels: %d, out_channels: %d"", in_channels, out_channels)\n    new_conv = torch.nn.Conv2d(in_channels=in_channels,\n                               out_channels=out_channels,\n                               kernel_size=conv.kernel_size,\n                               stride=conv.stride,\n                               padding=conv.padding,\n                               dilation=conv.dilation,\n                               groups=1, # currently only support groups is 1\n                               bias=conv.bias is not None,\n                               padding_mode=conv.padding_mode)\n    new_conv.to(conv.weight.device)\n    tmp_weight_data = tmp_bias_data = None\n    if mask.output_mask is not None:\n        tmp_weight_data = torch.index_select(conv.weight.data, 0, out_channels_index)\n        if conv.bias is not None:\n            tmp_bias_data = torch.index_select(conv.bias.data, 0, out_channels_index)\n    # NOTE: does not support group\n    if mask.input_mask is not None:\n        tmp_weight_data = torch.index_select(conv.weight.data if tmp_weight_data is None else tmp_weight_data,\n                                             1, in_channels_index)\n    assert tmp_weight_data is not None, ""Conv2d weight should be updated based on masks""\n    new_conv.weight.data.copy_(tmp_weight_data)\n    if conv.bias is not None:\n        new_conv.bias.data.copy_(conv.bias.data if tmp_bias_data is None else tmp_bias_data)\n    return new_conv\n'"
src/sdk/pynni/nni/compression/torch/speedup/compressor.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nimport torch\nfrom nni._graph_utils import build_module_graph\nfrom .compress_modules import replace_module\nfrom .infer_shape import ModuleMasks, infer_from_mask, infer_from_inshape, infer_from_outshape\n\n_logger = logging.getLogger(__name__)\n\n\ndef get_module_by_name(model, module_name):\n    """"""\n    Get a module specified by its module name\n\n    Parameters\n    ----------\n    model : pytorch model\n        the pytorch model from which to get its module\n    module_name : str\n        the name of the required module\n\n    Returns\n    -------\n    module, module\n        the parent module of the required module, the required module\n    """"""\n    name_list = module_name.split(""."")\n    for name in name_list[:-1]:\n        model = getattr(model, name)\n    leaf_module = getattr(model, name_list[-1])\n    return model, leaf_module\n\nclass ModelSpeedup:\n    """"""\n    This class is to speedup the model with provided weight mask\n    """"""\n\n    def __init__(self, model, dummy_input, masks_file, map_location=None):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            The model user wants to speed up\n        dummy_input : pytorch tensor\n            The dummy input for ```jit.trace```, users should put it on right device before pass in\n        masks_file : str\n            The path of user provided mask file\n        map_location : str\n            the device on which masks are placed, same to map_location in ```torch.load```\n        """"""\n        self.bound_model = model\n        self.masks = torch.load(masks_file, map_location)\n        self.inferred_masks = dict() # key: module_name, value: ModuleMasks\n        self.torch_graph = build_module_graph(model, dummy_input)\n\n    def infer_module_mask(self, module_name, mask=None, in_shape=None, out_shape=None):\n        """"""\n        Infer input shape / output shape based on the module\'s weight mask / input shape / output shape.\n\n        For a module:\n            Infer its input and output shape from its weight mask\n            Infer its output shape from its input shape\n            Infer its input shape from its output shape\n\n        If its input shape is changed, continue infering its predecessors\n        If its output shape is changed, continue infering its successors\n\n        Parameters\n        ----------\n        module_name : str\n            The name of the node\n        mask : tensor of mask or ModuleMasks\n            Mask of the weights in this node (i.e., module)\n        in_shape : ModuleMasks\n            Input shape of this node\n        out_shape : ModuleMasks\n            Output shape of this node\n        """"""\n        input_cmask = output_cmask = None\n        if module_name in self.inferred_masks:\n            module_masks = self.inferred_masks[module_name]\n        else:\n            module_masks = ModuleMasks(module_name)\n            self.inferred_masks[module_name] = module_masks\n\n        m_type = self.torch_graph.name_to_node[module_name].op_type\n        _logger.debug(""infer mask of module %s with op_type %s"", module_name, m_type)\n        if mask is not None:\n            _logger.debug(""mask is not None"")\n            if not m_type in infer_from_mask:\n                raise RuntimeError(\n                    ""Has not supported infering input/output shape from mask for module/function: `{}`, {}""\n                    .format(m_type, module_name))\n            input_cmask, output_cmask = infer_from_mask[m_type](module_masks, mask)\n        if in_shape is not None:\n            _logger.debug(""in_shape is not None"")\n            if not m_type in infer_from_inshape:\n                raise RuntimeError(\n                    ""Has not supported infering output shape from input shape for module/function: `{}`, {}""\n                    .format(m_type, module_name))\n            if m_type in [\'aten::view\', \'aten::flatten\']:\n                output_cmask = infer_from_inshape[m_type](module_masks,\n                                                          in_shape,\n                                                          self.torch_graph.name_to_node[module_name].auxiliary)\n            else:\n                output_cmask = infer_from_inshape[m_type](module_masks, in_shape)\n        if out_shape is not None:\n            _logger.debug(""out_shape is not None"")\n            if not m_type in infer_from_outshape:\n                raise RuntimeError(\n                    ""Has not supported infering input shape from output shape for module/function: `{}`, {}""\n                    .format(m_type, module_name))\n            input_cmask = infer_from_outshape[m_type](module_masks, out_shape)\n\n        if input_cmask:\n            predecessors = self.torch_graph.find_predecessors(module_name)\n            for _module_name in predecessors:\n                self.infer_module_mask(_module_name, out_shape=input_cmask)\n        if output_cmask:\n            successors = self.torch_graph.find_successors(module_name)\n            for _module_name in successors:\n                self.infer_module_mask(_module_name, in_shape=output_cmask)\n\n    def infer_modules_masks(self):\n        """"""\n        Do shape inference of involved modules, including the shape of weights, inputs, output\n        """"""\n        for module_name, mask in self.masks.items():\n            self.infer_module_mask(module_name, mask=mask)\n\n    def replace_compressed_modules(self):\n        """"""\n        Replace all the modules that have changed (weights/inputs/output) shape.\n        The new module is created using the same arguments of the to-be-replaced module,\n        and correctly inherits its weights.\n\n        NOTE: ```func``` type cannot be replaced as it is not a module, thus, one limitation\n        is that ```func``` should be not required to be replaced.\n        """"""\n        for module_name in self.inferred_masks:\n            g_node = self.torch_graph.name_to_node[module_name]\n            _logger.debug(""replace %s, in %s type, with op_type %s"",\n                          module_name, g_node.type, g_node.op_type)\n            if g_node.type == \'module\':\n                super_module, leaf_module = get_module_by_name(self.bound_model, module_name)\n                m_type = g_node.op_type\n                if not m_type in replace_module:\n                    raise RuntimeError(""Has not supported replacing the module: `{}`"".format(m_type))\n                _logger.info(""replace module (name: %s, op_type: %s)"", module_name, m_type)\n                compressed_module = replace_module[m_type](leaf_module, self.inferred_masks[module_name])\n                setattr(super_module, module_name.split(\'.\')[-1], compressed_module)\n            elif g_node.type == \'func\':\n                _logger.info(""Warning: cannot replace (name: %s, op_type: %s) which is func type"",\n                             module_name, g_node.op_type)\n            else:\n                raise RuntimeError(""Unsupported node type: {}"".format(g_node.type))\n\n    def speedup_model(self):\n        """"""\n        There are basically two steps:\n        first, do mask/shape inference,\n        second, replace modules\n        """"""\n        training = self.bound_model.training\n        _logger.info(""start to speed up the model"")\n        _logger.info(""infer module masks..."")\n        self.infer_modules_masks()\n        _logger.info(""replace compressed modules..."")\n        self.replace_compressed_modules()\n        self.bound_model.train(training)\n        _logger.info(""speedup done"")\n'"
src/sdk/pynni/nni/compression/torch/speedup/infer_shape.py,12,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n""""""\nFor each operation or module, there are two functions.\nOne is given output shape, infer its input shape and initialization parameters (e.g., weight\'s shape)\nThe other is given input shape, infer its output shape and initialization parameters (e.g., weight\'s shape)\n""""""\n\nimport torch\n\nclass CoarseMask:\n    """"""\n    Coarse grained mask for a given tensor, here tensor could be weights,\n    input tensor, or output tensor\n    """"""\n    def __init__(self, num_dim):\n        """"""\n        Parameters\n        ----------\n        num_dim : int\n            The number of dimensions of the tensor that will be masked\n        """"""\n        self.mask_index = [None for _ in range(num_dim)]\n\n    def add_index_mask(self, dim, index):\n        """"""\n        Add mask for the specified dimension\n\n        Parameters\n        ----------\n        dim : int\n            The dimension to add mask\n        index : tensor\n            The mask for this dimension, its a 1 dimension tensor which specifies\n            the index of the elements that are not pruned\n        """"""\n        self.mask_index[dim] = index\n\n    @staticmethod\n    def merge_index(index_a, index_b):\n        """"""\n        Parameters\n        ----------\n        index_a : tensor\n            One index (1-dimension) tensor\n        index_b : tensor\n            The other index (1-dimension) tensor\n\n        Returns\n        -------\n        tensor\n            The merged index (1-dimension) tensor\n        """"""\n        s = set()\n        for num in index_a:\n            s.add(num)\n        for num in index_b:\n            s.add(num)\n        return torch.tensor(sorted(s)) # pylint: disable=not-callable\n\n    def merge(self, cmask):\n        """"""\n        Merge another CoarseMask\n\n        Parameters\n        ----------\n        cmask : CoarseMask\n            Another CoarseMask to merge\n\n        Returns\n        -------\n        list\n            The member variable ```mask_index```\n        """"""\n        assert isinstance(cmask, CoarseMask)\n        assert len(self.mask_index) == len(cmask.mask_index), \\\n            ""Only masks with the same number of dimensions can be merged""\n        for i, index in enumerate(self.mask_index):\n            if index is None:\n                self.mask_index[i] = cmask.mask_index[i]\n            elif cmask.mask_index[i] is not None:\n                self.mask_index[i] = CoarseMask.merge_index(self.mask_index[i],\n                                                            cmask.mask_index[i])\n        return self.mask_index\n\n    def __repr__(self):\n        return \'mask_index: {}\'.format(self.mask_index)\n\nclass ModuleMasks:\n    """"""\n    The masks of a module, including the masks for weights, inputs, output\n    """"""\n    def __init__(self, module_name):\n        """"""\n        Parameters\n        ----------\n        module_name : str\n            The name of the module or function\n        """"""\n        self.module_name = module_name\n        self.param_masks = dict()\n        self.input_mask = None\n        self.output_mask = None\n\n    def set_param_masks(self, name, mask):\n        """"""\n        Parameters\n        ----------\n        name : str\n            The name of the weight\n        mask : CoarseMask\n            The mask for this weight\n        """"""\n        self.param_masks[name] = mask\n\n    def set_input_mask(self, mask):\n        """"""\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for input\n        """"""\n        self.input_mask = mask\n\n    def set_output_mask(self, mask):\n        """"""\n        Parameters\n        ----------\n        mask : CoarseMask\n            The mask for output\n        """"""\n        self.output_mask = mask\n\n    def __repr__(self):\n        return \'input_mask: {}, output_mask: {}, param_masks: {}\'.format(\n            self.input_mask, self.output_mask, self.param_masks\n        )\n\n""""""\nInfer input and output shape of a module/function from its weight mask\n""""""\ninfer_from_mask = {\n    \'BatchNorm2d\': lambda module_masks, mask: batchnorm2d_mask(module_masks, mask),\n    \'Conv2d\': lambda module_masks, mask: conv2d_mask(module_masks, mask)\n}\n\n""""""\nInfer output and weight shape of a module/function from its input shape\n""""""\ninfer_from_inshape = {\n    \'ReLU\': lambda module_masks, mask: relu_inshape(module_masks, mask),\n    \'aten::relu\': lambda module_masks, mask: relu_inshape(module_masks, mask),\n    \'Conv2d\': lambda module_masks, mask: conv2d_inshape(module_masks, mask),\n    \'MaxPool2d\': lambda module_masks, mask: maxpool2d_inshape(module_masks, mask),\n    \'aten::max_pool2d\': lambda module_masks, mask: maxpool2d_inshape(module_masks, mask),\n    \'aten::avg_pool2d\': lambda module_masks, mask: maxpool2d_inshape(module_masks, mask),\n    \'AvgPool2d\': lambda module_masks, mask: maxpool2d_inshape(module_masks, mask),\n    \'AdaptiveAvgPool2d\': lambda module_masks, mask: maxpool2d_inshape(module_masks, mask),\n    \'aten::size\': lambda module_masks, mask: size_inshape(module_masks, mask),\n    \'aten::view\': lambda module_masks, mask, shape: view_inshape(module_masks, mask, shape),\n    \'aten::flatten\': lambda module_masks, mask, shape: view_inshape(module_masks, mask, shape), # support only start_dim=1\n    \'Linear\': lambda module_masks, mask: linear_inshape(module_masks, mask),\n    \'BatchNorm2d\': lambda module_masks, mask: batchnorm2d_inshape(module_masks, mask)\n}\n\n""""""\nInfer input and weight shape of a module/function from its output shape\n""""""\ninfer_from_outshape = {\n    \'Conv2d\': lambda module_masks, mask: conv2d_outshape(module_masks, mask)\n}\n\ndef batchnorm2d_inshape(module_masks, mask):\n    """"""\n    We assume only the second dimension has coarse grained mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the batchnorm2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    """"""\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    weight_cmask = CoarseMask(num_dim=1)\n    weight_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    module_masks.set_param_masks(\'weight\', weight_cmask)\n    module_masks.set_param_masks(\'bias\', weight_cmask)\n    return mask\n\ndef linear_inshape(module_masks, mask):\n    """"""\n    Coarse grained input mask does not change the shape of weights and output tensor\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the linear\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor, ```None``` means shape of output tensor is not changed\n    """"""\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[0] is None\n    assert module_masks.input_mask is None\n    module_masks.set_input_mask(mask)\n    return None\n\ndef view_inshape(module_masks, mask, shape):\n    """"""\n    This is a limited support\n\n    TODO: consider replace tensor.view with nn.Flatten, because tensor.view is not\n    included in module, thus, cannot be replaced by our framework.\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the ```view``` op\n    mask : CoarseMask\n        The mask of its input tensor\n    shape : dict\n        Original shape of its input and output tensors\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    """"""\n    # NOTE: the case constrained by the following four asserts\n    assert shape[\'in_shape\'][0] == shape[\'out_shape\'][0]\n    assert len(shape[\'in_shape\']) == 4\n    assert len(shape[\'out_shape\']) == 2\n    assert shape[\'out_shape\'][1] == shape[\'in_shape\'][1]*shape[\'in_shape\'][2]*shape[\'in_shape\'][3]\n\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    assert module_masks.input_mask is None\n    module_masks.set_input_mask(mask)\n    output_cmask = CoarseMask(num_dim=2)\n    index = []\n    step_size = shape[\'in_shape\'][2] * shape[\'in_shape\'][3]\n    for loc in mask.mask_index[1]:\n        index.extend([loc * step_size + i for i in range(step_size)])\n    output_cmask.add_index_mask(dim=1, index=torch.tensor(index)) # pylint: disable=not-callable\n    module_masks.set_output_mask(output_cmask)\n    return output_cmask\n\n\ndef size_inshape(module_masks, mask):\n    """"""\n    No need to do anything for this ```size``` op\n    """"""\n    return None\n\ndef maxpool2d_inshape(module_masks, mask):\n    """"""\n    Assume only the second dimension is masked\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the maxpool2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    """"""\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n    assert module_masks.input_mask is None\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    return mask\n\ndef relu_inshape(module_masks, mask):\n    """"""\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the relu\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    """"""\n    assert isinstance(mask, CoarseMask)\n    # TODO: double check this assert, is it possible that a module is passed twice\n    assert module_masks.input_mask is None, ""A relu op can only be processed once""\n    module_masks.set_input_mask(mask)\n    module_masks.set_output_mask(mask)\n    return mask\n\ndef batchnorm2d_mask(module_masks, mask):\n    """"""\n    Infer input and output shape from weight mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the batchnorm2d\n    mask : dict\n        The mask of its weights, from the user provided mask file\n\n    Returns\n    -------\n    CoarseMask, CoarseMask\n        The mask of its input tensor, the mask of its output tensor\n    """"""\n    assert \'weight\' in mask and \'bias\' in mask\n    sum_mask = mask[\'weight\'] + mask[\'bias\']\n    nonzero_index = torch.nonzero(sum_mask, as_tuple=True)[0]\n    # infer shape of parameters\n    param_cmask = CoarseMask(num_dim=1)\n    param_cmask.add_index_mask(dim=0, index=nonzero_index)\n    module_masks.set_param_masks(\'weight\', param_cmask)\n    module_masks.set_param_masks(\'bias\', param_cmask)\n    # infer shape of input tensor\n    input_cmask = CoarseMask(num_dim=4)\n    input_cmask.add_index_mask(dim=1,\n                               index=torch.nonzero(mask[\'weight\'], as_tuple=True)[0])\n    module_masks.set_input_mask(input_cmask)\n    # infer shape of output tensor\n    output_cmask = CoarseMask(num_dim=4)\n    output_cmask.add_index_mask(dim=1, index=nonzero_index)\n    module_masks.set_output_mask(output_cmask)\n    return input_cmask, output_cmask\n\ndef conv2d_mask(module_masks, mask):\n    """"""\n    Infer input and output shape from weight mask\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : dict\n        The mask of its weights, from the user provided mask file\n\n    Returns\n    -------\n    CoarseMask, CoarseMask\n        The mask of its input tensor, the mask of its output tensor\n    """"""\n    def convert_to_coarse_mask(mask):\n        """"""\n        Parameters\n        ----------\n        mask : dict\n            Weight mask from user provided mask file\n\n        Returns\n        -------\n        LongTensor, CoarseMask, CoarseMask\n            Index of the masked dimension, weight mask, bias mask\n        """"""\n        assert \'weight\' in mask\n        assert isinstance(mask[\'weight\'], torch.Tensor)\n        weight_mask = mask[\'weight\']\n        shape = weight_mask.size()\n        ones = torch.ones(shape[1:]).to(weight_mask.device)\n        zeros = torch.zeros(shape[1:]).to(weight_mask.device)\n        index = []\n        for i in range(shape[0]):\n            if torch.all(torch.eq(weight_mask[i], ones)):\n                index.append(i)\n            elif torch.all(torch.eq(weight_mask[i], zeros)):\n                continue\n            else:\n                index = None\n                break\n        if index is None:\n            return None, None, None\n        else:\n            index = torch.LongTensor(index).to(weight_mask.device)\n            weight_cmask = CoarseMask(num_dim=4)\n            weight_cmask.add_index_mask(dim=0, index=index)\n            bias_cmask = None\n            if \'bias\' in mask and mask[\'bias\'] is not None:\n                bias_index = torch.nonzero(mask[\'bias\'], as_tuple=True)[0]\n                assert torch.all(torch.eq(index, bias_index)), \\\n                    ""bias mask should be consistent with weight mask""\n                bias_cmask = CoarseMask(num_dim=1)\n                bias_cmask.add_index_mask(dim=0, index=bias_index)\n            return index, weight_cmask, bias_cmask\n    index, weight_cmask, bias_cmask = convert_to_coarse_mask(mask)\n    if index is None:\n        # TODO: fine grained mask speedup\n        return None, None\n    # deal with coarse grain mask\n    if \'weight\' in module_masks.param_masks:\n        module_masks.param_masks[\'weight\'].merge(weight_cmask)\n        module_masks.param_masks[\'bias\'].merge(bias_cmask)\n    else:\n        module_masks.set_param_masks(\'weight\', weight_cmask)\n        module_masks.set_param_masks(\'bias\', bias_cmask)\n    output_cmask = CoarseMask(num_dim=4)\n    output_cmask.add_index_mask(dim=1, index=index)\n    if module_masks.output_mask is None:\n        module_masks.set_output_mask(output_cmask)\n    else:\n        module_masks.output_mask.merge(output_cmask)\n    return None, module_masks.output_mask\n\ndef conv2d_inshape(module_masks, mask):\n    """"""\n    Shape change of input tensor does not affect the shape of its output tensor\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : CoarseMask\n        The mask of its input tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its output tensor\n    """"""\n    assert isinstance(mask, CoarseMask)\n    assert module_masks.input_mask is None\n    module_masks.set_input_mask(mask)\n    return None\n\ndef conv2d_outshape(module_masks, mask):\n    """"""\n    Assume only the second dimension is masked\n\n    Parameters\n    ----------\n    module_masks : ModuleMasks\n        The ModuleMasks instance of the conv2d\n    mask : CoarseMask\n        The mask of its output tensor\n\n    Returns\n    -------\n    CoarseMask\n        The mask of its input tensor\n    """"""\n    assert isinstance(mask, CoarseMask)\n    assert mask.mask_index[1] is not None\n    assert mask.mask_index[0] is None\n    assert mask.mask_index[2] is None\n    assert mask.mask_index[3] is None\n\n    if module_masks.output_mask is not None:\n        assert isinstance(module_masks.output_mask, CoarseMask)\n        # set shape of output\n        mask = module_masks.output_mask.merge(mask)\n    else:\n        module_masks.output_mask = mask\n    # infer shape of parameters\n    weight_cmask = CoarseMask(num_dim=4)\n    weight_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    bias_cmask = CoarseMask(num_dim=1)\n    bias_cmask.add_index_mask(dim=0, index=mask.mask_index[1])\n    module_masks.set_param_masks(\'weight\', weight_cmask)\n    module_masks.set_param_masks(\'bias\', bias_cmask)\n    # input shape is not changed\n    return None\n    '"
src/sdk/pynni/nni/compression/torch/utils/__init__.py,0,b''
src/sdk/pynni/nni/compression/torch/utils/config_validation.py,0,"b""# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom schema import Schema, And, SchemaError\n\ndef validate_op_names(model, op_names, logger):\n    found_names = set(map(lambda x: x[0], model.named_modules()))\n\n    not_found_op_names = list(set(op_names) - found_names)\n    if not_found_op_names:\n        logger.warning('op_names %s not found in model', not_found_op_names)\n\n    return True\n\ndef validate_op_types(model, op_types, logger):\n    found_types = set(['default']) | set(map(lambda x: type(x[1]).__name__, model.named_modules()))\n\n    not_found_op_types = list(set(op_types) - found_types)\n    if not_found_op_types:\n        logger.warning('op_types %s not found in model', not_found_op_types)\n\n    return True\n\ndef validate_op_types_op_names(data):\n    if not ('op_types' in data or 'op_names' in data):\n        raise SchemaError('Either op_types or op_names must be specified.')\n    return True\n\nclass CompressorSchema:\n    def __init__(self, data_schema, model, logger):\n        assert isinstance(data_schema, list) and len(data_schema) <= 1\n        self.data_schema = data_schema\n        self.compressor_schema = Schema(self._modify_schema(data_schema, model, logger))\n\n    def _modify_schema(self, data_schema, model, logger):\n        if not data_schema:\n            return data_schema\n\n        for k in data_schema[0]:\n            old_schema = data_schema[0][k]\n            if k == 'op_types' or (isinstance(k, Schema) and k._schema == 'op_types'):\n                new_schema = And(old_schema, lambda n: validate_op_types(model, n, logger))\n                data_schema[0][k] = new_schema\n            if k == 'op_names' or (isinstance(k, Schema) and k._schema == 'op_names'):\n                new_schema = And(old_schema, lambda n: validate_op_names(model, n, logger))\n                data_schema[0][k] = new_schema\n\n        data_schema[0] = And(data_schema[0], lambda d: validate_op_types_op_names(d))\n\n        return data_schema\n\n    def validate(self, data):\n        self.compressor_schema.validate(data)\n"""
src/sdk/pynni/nni/nas/pytorch/cdarts/__init__.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .mutator import RegularizedDartsMutator, RegularizedMutatorParallel, DartsDiscreteMutator\nfrom .trainer import CdartsTrainer'"
src/sdk/pynni/nni/nas/pytorch/cdarts/mutator.py,7,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\n\nfrom apex.parallel import DistributedDataParallel  # pylint: disable=import-error\nfrom nni.nas.pytorch.darts import DartsMutator  # pylint: disable=wrong-import-order\nfrom nni.nas.pytorch.mutables import LayerChoice  # pylint: disable=wrong-import-order\nfrom nni.nas.pytorch.mutator import Mutator  # pylint: disable=wrong-import-order\n\n\nclass RegularizedDartsMutator(DartsMutator):\n    """"""\n    This is :class:`~nni.nas.pytorch.darts.DartsMutator` basically, with two differences.\n\n    1. Choices can be cut (bypassed). This is done by ``cut_choices``. Cutted choices will not be used in\n    forward pass and thus consumes no memory.\n\n    2. Regularization on choices, to prevent the mutator from overfitting on some choices.\n    """"""\n\n    def reset(self):\n        """"""\n        Warnings\n        --------\n        Renamed :func:`~reset_with_loss` to return regularization loss on reset.\n        """"""\n        raise ValueError(""You should probably call `reset_with_loss`."")\n\n    def cut_choices(self, cut_num=2):\n        """"""\n        Cut the choices with the smallest weights.\n        ``cut_num`` should be the accumulative number of cutting, e.g., if first time cutting\n        is 2, the second time should be 4 to cut another two.\n\n        Parameters\n        ----------\n        cut_num : int\n            Number of choices to cut, so far.\n\n        Warnings\n        --------\n        Though the parameters are set to :math:`-\\infty` to be bypassed, they will still receive gradient of 0,\n        which introduced ``nan`` problem when calling ``optimizer.step()``. To solve this issue, a simple way is to\n        reset nan to :math:`-\\infty` each time after the parameters are updated.\n        """"""\n        # `cut_choices` is implemented but not used in current implementation of CdartsTrainer\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                _, idx = torch.topk(-self.choices[mutable.key], cut_num)\n                with torch.no_grad():\n                    for i in idx:\n                        self.choices[mutable.key][i] = -float(""inf"")\n\n    def reset_with_loss(self):\n        """"""\n        Resample and return loss. If loss is 0, to avoid device issue, it will return ``None``.\n\n        Currently loss penalty are proportional to the L1-norm of parameters corresponding\n        to modules if their type name contains certain substrings. These substrings include: ``poolwithoutbn``,\n        ``identity``, ``dilconv``.\n        """"""\n        self._cache, reg_loss = self.sample_search()\n        return reg_loss\n\n    def sample_search(self):\n        result = super().sample_search()\n        loss = []\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                def need_reg(choice):\n                    return any(t in str(type(choice)).lower() for t in [""poolwithoutbn"", ""identity"", ""dilconv""])\n\n                for i, choice in enumerate(mutable.choices):\n                    if need_reg(choice):\n                        norm = torch.abs(self.choices[mutable.key][i])\n                        if norm < 1E10:\n                            loss.append(norm)\n        if not loss:\n            return result, None\n        return result, sum(loss)\n\n    def export(self, logger=None):\n        """"""\n        Export an architecture with logger. Genotype will be printed with logger.\n\n        Returns\n        -------\n        dict\n            A mapping from mutable keys to decisions.\n        """"""\n        result = self.sample_final()\n        if hasattr(self.model, ""plot_genotype"") and logger is not None:\n            genotypes = self.model.plot_genotype(result, logger)\n        return result, genotypes\n\n\nclass RegularizedMutatorParallel(DistributedDataParallel):\n    """"""\n    Parallelize :class:`~RegularizedDartsMutator`.\n\n    This makes :func:`~RegularizedDartsMutator.reset_with_loss` method parallelized,\n    also allowing :func:`~RegularizedDartsMutator.cut_choices` and :func:`~RegularizedDartsMutator.export`\n    to be easily accessible.\n    """"""\n    def reset_with_loss(self):\n        """"""\n        Parallelized :func:`~RegularizedDartsMutator.reset_with_loss`.\n        """"""\n        result = self.module.reset_with_loss()\n        self.callback_queued = False\n        return result\n\n    def cut_choices(self, *args, **kwargs):\n        """"""\n        Parallelized :func:`~RegularizedDartsMutator.cut_choices`.\n        """"""\n        self.module.cut_choices(*args, **kwargs)\n\n    def export(self, logger):\n        """"""\n        Parallelized :func:`~RegularizedDartsMutator.export`.\n        """"""\n        return self.module.export(logger)\n\n\nclass DartsDiscreteMutator(Mutator):\n    """"""\n    A mutator that applies the final sampling result of a parent mutator on another model to train.\n\n    Parameters\n    ----------\n    model : nn.Module\n        The model to apply the mutator.\n    parent_mutator : Mutator\n        The mutator that provides ``sample_final`` method, that will be called to get the architecture.\n    """"""\n    def __init__(self, model, parent_mutator):\n        super().__init__(model)\n        self.__dict__[""parent_mutator""] = parent_mutator  # avoid parameters to be included\n\n    def sample_search(self):\n        return self.parent_mutator.sample_final()\n'"
src/sdk/pynni/nni/nas/pytorch/cdarts/trainer.py,14,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport apex  # pylint: disable=import-error\nfrom apex.parallel import DistributedDataParallel  # pylint: disable=import-error\nfrom nni.nas.pytorch.cdarts import RegularizedDartsMutator, RegularizedMutatorParallel, DartsDiscreteMutator  # pylint: disable=wrong-import-order\nfrom nni.nas.pytorch.utils import AverageMeterGroup  # pylint: disable=wrong-import-order\n\nfrom .utils import CyclicIterator, TorchTensorEncoder, accuracy, reduce_metrics\n\nPHASE_SMALL = ""small""\nPHASE_LARGE = ""large""\n\n\nclass InteractiveKLLoss(nn.Module):\n    def __init__(self, temperature):\n        super().__init__()\n        self.temperature = temperature\n        # self.kl_loss = nn.KLDivLoss(reduction = \'batchmean\')\n        self.kl_loss = nn.KLDivLoss()\n\n    def forward(self, student, teacher):\n        return self.kl_loss(F.log_softmax(student / self.temperature, dim=1),\n                            F.softmax(teacher / self.temperature, dim=1))\n\n\nclass CdartsTrainer(object):\n    """"""\n    CDARTS trainer.\n\n    Parameters\n    ----------\n    model_small : nn.Module\n        PyTorch model to be trained. This is the search network of CDARTS.\n    model_large : nn.Module\n        PyTorch model to be trained. This is the evaluation network of CDARTS.\n    criterion : callable\n        Receives logits and ground truth label, return a loss tensor, e.g., ``nn.CrossEntropyLoss()``.\n    loaders : list of torch.utils.data.DataLoader\n        List of train data and valid data loaders, for training weights and architecture weights respectively.\n    samplers : list of torch.utils.data.Sampler\n        List of train data and valid data samplers. This can be PyTorch standard samplers if not distributed.\n        In distributed mode, sampler needs to have ``set_epoch`` method. Refer to data utils in CDARTS example for details.\n    logger : logging.Logger\n        The logger for logging. Will use nni logger by default (if logger is ``None``).\n    regular_coeff : float\n        The coefficient of regular loss.\n    regular_ratio : float\n        The ratio of regular loss.\n    warmup_epochs : int\n        The epochs to warmup the search network\n    fix_head : bool\n        ``True`` if fixing the paramters of auxiliary heads, else unfix the paramters of auxiliary heads.\n    epochs : int\n        Number of epochs planned for training.\n    steps_per_epoch : int\n        Steps of one epoch.\n    loss_alpha : float\n        The loss coefficient.\n    loss_T : float\n        The loss coefficient.\n    distributed : bool\n        ``True`` if using distributed training, else non-distributed training.\n    log_frequency : int\n        Step count per logging.\n    grad_clip : float\n        Gradient clipping for weights.\n    interactive_type : string\n        ``kl`` or ``smoothl1``.\n    output_path : string\n        Log storage path.\n    w_lr : float\n        Learning rate of the search network parameters.\n    w_momentum : float\n        Momentum of the search and the evaluation network.\n    w_weight_decay : float\n        The weight decay the search and the evaluation network parameters.\n    alpha_lr : float\n        Learning rate of the architecture parameters.\n    alpha_weight_decay : float\n        The weight decay the architecture parameters.\n    nasnet_lr : float\n        Learning rate of the evaluation network parameters.\n    local_rank : int\n        The number of thread.\n    share_module : bool\n        ``True`` if sharing the stem and auxiliary heads, else not sharing these modules.\n    """"""\n    def __init__(self, model_small, model_large, criterion, loaders, samplers, logger=None,\n                 regular_coeff=5, regular_ratio=0.2, warmup_epochs=2, fix_head=True,\n                 epochs=32, steps_per_epoch=None, loss_alpha=2, loss_T=2, distributed=True,\n                 log_frequency=10, grad_clip=5.0, interactive_type=\'kl\', output_path=\'./outputs\',\n                 w_lr=0.2, w_momentum=0.9, w_weight_decay=3e-4, alpha_lr=0.2, alpha_weight_decay=1e-4,\n                 nasnet_lr=0.2, local_rank=0, share_module=True):\n        if logger is None:\n            logger = logging.getLogger(__name__)\n        train_loader, valid_loader = loaders\n        train_sampler, valid_sampler = samplers\n        self.train_loader = CyclicIterator(train_loader, train_sampler, distributed)\n        self.valid_loader = CyclicIterator(valid_loader, valid_sampler, distributed)\n\n        self.regular_coeff = regular_coeff\n        self.regular_ratio = regular_ratio\n        self.warmup_epochs = warmup_epochs\n        self.fix_head = fix_head\n        self.epochs = epochs\n        self.steps_per_epoch = steps_per_epoch\n        if self.steps_per_epoch is None:\n            self.steps_per_epoch = min(len(self.train_loader), len(self.valid_loader))\n        self.loss_alpha = loss_alpha\n        self.grad_clip = grad_clip\n        if interactive_type == ""kl"":\n            self.interactive_loss = InteractiveKLLoss(loss_T)\n        elif interactive_type == ""smoothl1"":\n            self.interactive_loss = nn.SmoothL1Loss()\n        self.loss_T = loss_T\n        self.distributed = distributed\n        self.log_frequency = log_frequency\n        self.main_proc = not distributed or local_rank == 0\n\n        self.logger = logger\n        self.checkpoint_dir = output_path\n        if self.main_proc:\n            os.makedirs(self.checkpoint_dir, exist_ok=True)\n        if distributed:\n            torch.distributed.barrier()\n\n        self.model_small = model_small\n        self.model_large = model_large\n        if self.fix_head:\n            for param in self.model_small.aux_head.parameters():\n                param.requires_grad = False\n            for param in self.model_large.aux_head.parameters():\n                param.requires_grad = False\n\n        self.mutator_small = RegularizedDartsMutator(self.model_small).cuda()\n        self.mutator_large = DartsDiscreteMutator(self.model_large, self.mutator_small).cuda()\n        self.criterion = criterion\n\n        self.optimizer_small = torch.optim.SGD(self.model_small.parameters(), w_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_large = torch.optim.SGD(self.model_large.parameters(), nasnet_lr,\n                                               momentum=w_momentum, weight_decay=w_weight_decay)\n        self.optimizer_alpha = torch.optim.Adam(self.mutator_small.parameters(), alpha_lr,\n                                                betas=(0.5, 0.999), weight_decay=alpha_weight_decay)\n\n        if distributed:\n            apex.parallel.convert_syncbn_model(self.model_small)\n            apex.parallel.convert_syncbn_model(self.model_large)\n            self.model_small = DistributedDataParallel(self.model_small, delay_allreduce=True)\n            self.model_large = DistributedDataParallel(self.model_large, delay_allreduce=True)\n            self.mutator_small = RegularizedMutatorParallel(self.mutator_small, delay_allreduce=True)\n            if share_module:\n                self.model_small.callback_queued = True\n                self.model_large.callback_queued = True\n            # mutator large never gets optimized, so do not need parallelized\n\n    def _warmup(self, phase, epoch):\n        assert phase in [PHASE_SMALL, PHASE_LARGE]\n        if phase == PHASE_SMALL:\n            model, optimizer = self.model_small, self.optimizer_small\n        elif phase == PHASE_LARGE:\n            model, optimizer = self.model_large, self.optimizer_large\n        model.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            x, y = next(self.train_loader)\n            x, y = x.cuda(), y.cuda()\n\n            optimizer.zero_grad()\n            logits_main, _ = model(x)\n            loss = self.criterion(logits_main, y)\n            loss.backward()\n\n            self._clip_grad_norm(model)\n            optimizer.step()\n            prec1, prec5 = accuracy(logits_main, y, topk=(1, 5))\n            metrics = {""prec1"": prec1, ""prec5"": prec5, ""loss"": loss}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(""Epoch [%d/%d] Step [%d/%d] (%s)  %s"", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, phase, meters)\n\n    def _clip_grad_norm(self, model):\n        if isinstance(model, DistributedDataParallel):\n            nn.utils.clip_grad_norm_(model.module.parameters(), self.grad_clip)\n        else:\n            nn.utils.clip_grad_norm_(model.parameters(), self.grad_clip)\n\n    def _reset_nan(self, parameters):\n        with torch.no_grad():\n            for param in parameters:\n                for i, p in enumerate(param):\n                    if p != p:  # equivalent to `isnan(p)`\n                        param[i] = float(""-inf"")\n\n    def _joint_train(self, epoch):\n        self.model_large.train()\n        self.model_small.train()\n        meters = AverageMeterGroup()\n        for step in range(self.steps_per_epoch):\n            trn_x, trn_y = next(self.train_loader)\n            val_x, val_y = next(self.valid_loader)\n            trn_x, trn_y = trn_x.cuda(), trn_y.cuda()\n            val_x, val_y = val_x.cuda(), val_y.cuda()\n\n            # step 1. optimize architecture\n            self.optimizer_alpha.zero_grad()\n            self.optimizer_large.zero_grad()\n            reg_decay = max(self.regular_coeff * (1 - float(epoch - self.warmup_epochs) / (\n                (self.epochs - self.warmup_epochs) * self.regular_ratio)), 0)\n            loss_regular = self.mutator_small.reset_with_loss()\n            if loss_regular:\n                loss_regular *= reg_decay\n            logits_search, emsemble_logits_search = self.model_small(val_x)\n            logits_main, emsemble_logits_main = self.model_large(val_x)\n            loss_cls = (self.criterion(logits_search, val_y) + self.criterion(logits_main, val_y)) / self.loss_alpha\n            loss_interactive = self.interactive_loss(emsemble_logits_search, emsemble_logits_main) * (self.loss_T ** 2) * self.loss_alpha\n            loss = loss_cls + loss_interactive + loss_regular\n            loss.backward()\n            self._clip_grad_norm(self.model_large)\n            self.optimizer_large.step()\n            self.optimizer_alpha.step()\n            # NOTE: need to call here `self._reset_nan(self.mutator_small.parameters())` if `cut_choices`\n\n            # step 2. optimize op weights\n            self.optimizer_small.zero_grad()\n            with torch.no_grad():\n                # resample architecture since parameters have been changed\n                self.mutator_small.reset_with_loss()\n            logits_search_train, _ = self.model_small(trn_x)\n            loss_weight = self.criterion(logits_search_train, trn_y)\n            loss_weight.backward()\n            self._clip_grad_norm(self.model_small)\n            self.optimizer_small.step()\n\n            metrics = {""loss_cls"": loss_cls, ""loss_interactive"": loss_interactive,\n                       ""loss_regular"": loss_regular, ""loss_weight"": loss_weight}\n            metrics = reduce_metrics(metrics, self.distributed)\n            meters.update(metrics)\n\n            if self.main_proc and (step % self.log_frequency == 0 or step + 1 == self.steps_per_epoch):\n                self.logger.info(""Epoch [%d/%d] Step [%d/%d] (joint)  %s"", epoch + 1, self.epochs,\n                                 step + 1, self.steps_per_epoch, meters)\n\n    def train(self):\n        for epoch in range(self.epochs):\n            if epoch < self.warmup_epochs:\n                with torch.no_grad():  # otherwise grads will be retained on the architecture params\n                    self.mutator_small.reset_with_loss()\n                self._warmup(PHASE_SMALL, epoch)\n            else:\n                with torch.no_grad():\n                    self.mutator_large.reset()\n                self._warmup(PHASE_LARGE, epoch)\n                self._joint_train(epoch)\n\n            self.export(os.path.join(self.checkpoint_dir, ""epoch_{:02d}.json"".format(epoch)),\n                        os.path.join(self.checkpoint_dir, ""epoch_{:02d}.genotypes"".format(epoch)))\n\n    def export(self, file, genotype_file):\n        if self.main_proc:\n            mutator_export, genotypes = self.mutator_small.export(self.logger)\n            with open(file, ""w"") as f:\n                json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n            with open(genotype_file, ""w"") as f:\n                f.write(str(genotypes))\n'"
src/sdk/pynni/nni/nas/pytorch/cdarts/utils.py,2,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport os\n\nimport torch\nimport torch.distributed as dist\n\n\nclass CyclicIterator:\n    def __init__(self, loader, sampler, distributed):\n        self.loader = loader\n        self.sampler = sampler\n        self.epoch = 0\n        self.distributed = distributed\n        self._next_epoch()\n\n    def _next_epoch(self):\n        if self.distributed:\n            self.sampler.set_epoch(self.epoch)\n        self.iterator = iter(self.loader)\n        self.epoch += 1\n\n    def __len__(self):\n        return len(self.loader)\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            return next(self.iterator)\n        except StopIteration:\n            self._next_epoch()\n            return next(self.iterator)\n\n\nclass TorchTensorEncoder(json.JSONEncoder):\n    def default(self, o):  # pylint: disable=method-hidden\n        if isinstance(o, torch.Tensor):\n            return o.tolist()\n        return super().default(o)\n\n\ndef accuracy(output, target, topk=(1,)):\n    """""" Computes the precision@k for the specified values of k """"""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    # one-hot case\n    if target.ndimension() > 1:\n        target = target.max(1)[1]\n\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(1.0 / batch_size))\n    return res\n\n\ndef reduce_tensor(tensor):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.ReduceOp.SUM)\n    rt /= float(os.environ[""WORLD_SIZE""])\n    return rt\n\n\ndef reduce_metrics(metrics, distributed=False):\n    if distributed:\n        return {k: reduce_tensor(v).item() for k, v in metrics.items()}\n    return {k: v.item() for k, v in metrics.items()}\n'"
src/sdk/pynni/nni/nas/pytorch/classic_nas/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .mutator import get_and_apply_next_architecture\n'
src/sdk/pynni/nni/nas/pytorch/classic_nas/mutator.py,5,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\nimport sys\n\nimport torch\n\nimport nni\nfrom nni.env_vars import trial_env_vars\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice, MutableScope\nfrom nni.nas.pytorch.mutator import Mutator\n\nlogger = logging.getLogger(__name__)\n\nNNI_GEN_SEARCH_SPACE = ""NNI_GEN_SEARCH_SPACE""\nLAYER_CHOICE = ""layer_choice""\nINPUT_CHOICE = ""input_choice""\n\n\ndef get_and_apply_next_architecture(model):\n    """"""\n    Wrapper of :class:`~nni.nas.pytorch.classic_nas.mutator.ClassicMutator` to make it more meaningful,\n    similar to ``get_next_parameter`` for HPO.\n\n    Tt will generate search space based on ``model``.\n    If env ``NNI_GEN_SEARCH_SPACE`` exists, this is in dry run mode for\n    generating search space for the experiment.\n    If not, there are still two mode, one is nni experiment mode where users\n    use ``nnictl`` to start an experiment. The other is standalone mode\n    where users directly run the trial command, this mode chooses the first\n    one(s) for each LayerChoice and InputChoice.\n\n    Parameters\n    ----------\n    model : nn.Module\n        User\'s model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    """"""\n    ClassicMutator(model)\n\n\nclass ClassicMutator(Mutator):\n    """"""\n    This mutator is to apply the architecture chosen from tuner.\n    It implements the forward function of LayerChoice and InputChoice,\n    to only activate the chosen ones.\n\n    Parameters\n    ----------\n    model : nn.Module\n        User\'s model with search space (e.g., LayerChoice, InputChoice) embedded in it.\n    """"""\n\n    def __init__(self, model):\n        super(ClassicMutator, self).__init__(model)\n        self._chosen_arch = {}\n        self._search_space = self._generate_search_space()\n        if NNI_GEN_SEARCH_SPACE in os.environ:\n            # dry run for only generating search space\n            self._dump_search_space(os.environ[NNI_GEN_SEARCH_SPACE])\n            sys.exit(0)\n\n        if trial_env_vars.NNI_PLATFORM is None:\n            logger.warning(""This is in standalone mode, the chosen are the first one(s)."")\n            self._chosen_arch = self._standalone_generate_chosen()\n        else:\n            # get chosen arch from tuner\n            self._chosen_arch = nni.get_next_parameter()\n            if self._chosen_arch is None:\n                if trial_env_vars.NNI_PLATFORM == ""unittest"":\n                    # happens if NNI_PLATFORM is intentionally set, e.g., in UT\n                    logger.warning(""`NNI_PLATFORM` is set but `param` is None. Falling back to standalone mode."")\n                    self._chosen_arch = self._standalone_generate_chosen()\n                else:\n                    raise RuntimeError(""Chosen architecture is None. This may be a platform error."")\n        self.reset()\n\n    def _sample_layer_choice(self, mutable, idx, value, search_space_item):\n        """"""\n        Convert layer choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        """"""\n        # doesn\'t support multihot for layer choice yet\n        onehot_list = [False] * len(mutable)\n        assert 0 <= idx < len(mutable) and search_space_item[idx] == value, \\\n            ""Index \'{}\' in search space \'{}\' is not \'{}\'"".format(idx, search_space_item, value)\n        onehot_list[idx] = True\n        return torch.tensor(onehot_list, dtype=torch.bool)  # pylint: disable=not-callable\n\n    def _sample_input_choice(self, mutable, idx, value, search_space_item):\n        """"""\n        Convert input choice to tensor representation.\n\n        Parameters\n        ----------\n        mutable : Mutable\n        idx : int\n            Number `idx` of list will be selected.\n        value : str\n            The verbose representation of the selected value.\n        search_space_item : list\n            The list for corresponding search space.\n        """"""\n        candidate_repr = search_space_item[""candidates""]\n        multihot_list = [False] * mutable.n_candidates\n        for i, v in zip(idx, value):\n            assert 0 <= i < mutable.n_candidates and candidate_repr[i] == v, \\\n                ""Index \'{}\' in search space \'{}\' is not \'{}\'"".format(i, candidate_repr, v)\n            assert not multihot_list[i], ""\'{}\' is selected twice in \'{}\', which is not allowed."".format(i, idx)\n            multihot_list[i] = True\n        return torch.tensor(multihot_list, dtype=torch.bool)  # pylint: disable=not-callable\n\n    def sample_search(self):\n        """"""\n        See :meth:`sample_final`.\n        """"""\n        return self.sample_final()\n\n    def sample_final(self):\n        """"""\n        Convert the chosen arch and apply it on model.\n        """"""\n        assert set(self._chosen_arch.keys()) == set(self._search_space.keys()), \\\n            ""Unmatched keys, expected keys \'{}\' from search space, found \'{}\'."".format(self._search_space.keys(),\n                                                                                       self._chosen_arch.keys())\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, (LayerChoice, InputChoice)):\n                assert mutable.key in self._chosen_arch, \\\n                    ""Expected \'{}\' in chosen arch, but not found."".format(mutable.key)\n                data = self._chosen_arch[mutable.key]\n                assert isinstance(data, dict) and ""_value"" in data and ""_idx"" in data, \\\n                    ""\'{}\' is not a valid choice."".format(data)\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = self._sample_layer_choice(mutable, data[""_idx""], data[""_value""],\n                                                                self._search_space[mutable.key][""_value""])\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = self._sample_input_choice(mutable, data[""_idx""], data[""_value""],\n                                                                self._search_space[mutable.key][""_value""])\n            elif isinstance(mutable, MutableScope):\n                logger.info(""Mutable scope \'%s\' is skipped during parsing choices."", mutable.key)\n            else:\n                raise TypeError(""Unsupported mutable type: \'%s\'."" % type(mutable))\n        return result\n\n    def _standalone_generate_chosen(self):\n        """"""\n        Generate the chosen architecture for standalone mode,\n        i.e., choose the first one(s) for LayerChoice and InputChoice.\n        ::\n            { key_name: {""_value"": ""conv1"",\n                         ""_idx"": 0} }\n            { key_name: {""_value"": [""in1""],\n                         ""_idx"": [0]} }\n        Returns\n        -------\n        dict\n            the chosen architecture\n        """"""\n        chosen_arch = {}\n        for key, val in self._search_space.items():\n            if val[""_type""] == LAYER_CHOICE:\n                choices = val[""_value""]\n                chosen_arch[key] = {""_value"": choices[0], ""_idx"": 0}\n            elif val[""_type""] == INPUT_CHOICE:\n                choices = val[""_value""][""candidates""]\n                n_chosen = val[""_value""][""n_chosen""]\n                if n_chosen is None:\n                    n_chosen = len(choices)\n                chosen_arch[key] = {""_value"": choices[:n_chosen], ""_idx"": list(range(n_chosen))}\n            else:\n                raise ValueError(""Unknown key \'%s\' and value \'%s\'."" % (key, val))\n        return chosen_arch\n\n    def _generate_search_space(self):\n        """"""\n        Generate search space from mutables.\n        Here is the search space format:\n        ::\n            { key_name: {""_type"": ""layer_choice"",\n                         ""_value"": [""conv1"", ""conv2""]} }\n            { key_name: {""_type"": ""input_choice"",\n                         ""_value"": {""candidates"": [""in1"", ""in2""],\n                                    ""n_chosen"": 1}} }\n        Returns\n        -------\n        dict\n            the generated search space\n        """"""\n        search_space = {}\n        for mutable in self.mutables:\n            # for now we only generate flattened search space\n            if isinstance(mutable, LayerChoice):\n                key = mutable.key\n                val = mutable.names\n                search_space[key] = {""_type"": LAYER_CHOICE, ""_value"": val}\n            elif isinstance(mutable, InputChoice):\n                key = mutable.key\n                search_space[key] = {""_type"": INPUT_CHOICE,\n                                     ""_value"": {""candidates"": mutable.choose_from,\n                                                ""n_chosen"": mutable.n_chosen}}\n            elif isinstance(mutable, MutableScope):\n                logger.info(""Mutable scope \'%s\' is skipped during generating search space."", mutable.key)\n            else:\n                raise TypeError(""Unsupported mutable type: \'%s\'."" % type(mutable))\n        return search_space\n\n    def _dump_search_space(self, file_path):\n        with open(file_path, ""w"") as ss_file:\n            json.dump(self._search_space, ss_file, sort_keys=True, indent=2)\n'"
src/sdk/pynni/nni/nas/pytorch/darts/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .mutator import DartsMutator\nfrom .trainer import DartsTrainer'
src/sdk/pynni/nni/nas/pytorch/darts/mutator.py,12,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutator import Mutator\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\n\n_logger = logging.getLogger(__name__)\n\n\nclass DartsMutator(Mutator):\n    """"""\n    Connects the model in a DARTS (differentiable) way.\n\n    An extra connection is automatically inserted for each LayerChoice, when this connection is selected, there is no\n    op on this LayerChoice (namely a ``ZeroOp``), in which case, every element in the exported choice list is ``false``\n    (not chosen).\n\n    All input choice will be fully connected in the search phase. On exporting, the input choice will choose inputs based\n    on keys in ``choose_from``. If the keys were to be keys of LayerChoices, the top logit of the corresponding LayerChoice\n    will join the competition of input choice to compete against other logits. Otherwise, the logit will be assumed 0.\n\n    It\'s possible to cut branches by setting parameter ``choices`` in a particular position to ``-inf``. After softmax, the\n    value would be 0. Framework will ignore 0 values and not connect. Note that the gradient on the ``-inf`` location will\n    be 0. Since manipulations with ``-inf`` will be ``nan``, you need to handle the gradient update phase carefully.\n\n    Attributes\n    ----------\n    choices: ParameterDict\n        dict that maps keys of LayerChoices to weighted-connection float tensors.\n    """"""\n    def __init__(self, model):\n        super().__init__(model)\n        self.choices = nn.ParameterDict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(mutable.length + 1))\n\n    def device(self):\n        for v in self.choices.values():\n            return v.device\n\n    def sample_search(self):\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                result[mutable.key] = F.softmax(self.choices[mutable.key], dim=-1)[:-1]\n            elif isinstance(mutable, InputChoice):\n                result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())\n        return result\n\n    def sample_final(self):\n        result = dict()\n        edges_max = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                max_val, index = torch.max(F.softmax(self.choices[mutable.key], dim=-1)[:-1], 0)\n                edges_max[mutable.key] = max_val\n                result[mutable.key] = F.one_hot(index, num_classes=len(mutable)).view(-1).bool()\n        for mutable in self.mutables:\n            if isinstance(mutable, InputChoice):\n                if mutable.n_chosen is not None:\n                    weights = []\n                    for src_key in mutable.choose_from:\n                        if src_key not in edges_max:\n                            _logger.warning(""InputChoice.NO_KEY in \'%s\' is weighted 0 when selecting inputs."", mutable.key)\n                        weights.append(edges_max.get(src_key, 0.))\n                    weights = torch.tensor(weights)  # pylint: disable=not-callable\n                    _, topk_edge_indices = torch.topk(weights, mutable.n_chosen)\n                    selected_multihot = []\n                    for i, src_key in enumerate(mutable.choose_from):\n                        if i not in topk_edge_indices and src_key in result:\n                            # If an edge is never selected, there is no need to calculate any op on this edge.\n                            # This is to eliminate redundant calculation.\n                            result[src_key] = torch.zeros_like(result[src_key])\n                        selected_multihot.append(i in topk_edge_indices)\n                    result[mutable.key] = torch.tensor(selected_multihot, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n                else:\n                    result[mutable.key] = torch.ones(mutable.n_candidates, dtype=torch.bool, device=self.device())  # pylint: disable=not-callable\n        return result\n'"
src/sdk/pynni/nni/nas/pytorch/darts/trainer.py,20,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport copy\nimport logging\n\nimport torch\nimport torch.nn as nn\nfrom nni.nas.pytorch.trainer import Trainer\nfrom nni.nas.pytorch.utils import AverageMeterGroup\n\nfrom .mutator import DartsMutator\n\nlogger = logging.getLogger(__name__)\n\n\nclass DartsTrainer(Trainer):\n    """"""\n    DARTS trainer.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to be trained.\n    loss : callable\n        Receives logits and ground truth label, return a loss tensor.\n    metrics : callable\n        Receives logits and ground truth label, return a dict of metrics.\n    optimizer : Optimizer\n        The optimizer used for optimizing the model.\n    num_epochs : int\n        Number of epochs planned for training.\n    dataset_train : Dataset\n        Dataset for training. Will be split for training weights and architecture weights.\n    dataset_valid : Dataset\n        Dataset for testing.\n    mutator : DartsMutator\n        Use in case of customizing your own DartsMutator. By default will instantiate a DartsMutator.\n    batch_size : int\n        Batch size.\n    workers : int\n        Workers for data loading.\n    device : torch.device\n        ``torch.device(""cpu"")`` or ``torch.device(""cuda"")``.\n    log_frequency : int\n        Step count per logging.\n    callbacks : list of Callback\n        list of callbacks to trigger at events.\n    arc_learning_rate : float\n        Learning rate of architecture parameters.\n    unrolled : float\n        ``True`` if using second order optimization, else first order optimization.\n    """"""\n    def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None, arc_learning_rate=3.0E-4, unrolled=False):\n        super().__init__(model, mutator if mutator is not None else DartsMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.ctrl_optim = torch.optim.Adam(self.mutator.parameters(), arc_learning_rate, betas=(0.5, 0.999),\n                                           weight_decay=1.0E-3)\n        self.unrolled = unrolled\n\n        n_train = len(self.dataset_train)\n        split = n_train // 2\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=batch_size,\n                                                       num_workers=workers)\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for step, ((trn_X, trn_y), (val_X, val_y)) in enumerate(zip(self.train_loader, self.valid_loader)):\n            trn_X, trn_y = trn_X.to(self.device), trn_y.to(self.device)\n            val_X, val_y = val_X.to(self.device), val_y.to(self.device)\n\n            # phase 1. architecture step\n            self.ctrl_optim.zero_grad()\n            if self.unrolled:\n                self._unrolled_backward(trn_X, trn_y, val_X, val_y)\n            else:\n                self._backward(val_X, val_y)\n            self.ctrl_optim.step()\n\n            # phase 2: child network step\n            self.optimizer.zero_grad()\n            logits, loss = self._logits_and_loss(trn_X, trn_y)\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)  # gradient clipping\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, trn_y)\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(""Epoch [%s/%s] Step [%s/%s]  %s"", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)\n\n    def validate_one_epoch(self, epoch):\n        self.model.eval()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            self.mutator.reset()\n            for step, (X, y) in enumerate(self.test_loader):\n                X, y = X.to(self.device), y.to(self.device)\n                logits = self.model(X)\n                metrics = self.metrics(logits, y)\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(""Epoch [%s/%s] Step [%s/%s]  %s"", epoch + 1,\n                                self.num_epochs, step + 1, len(self.test_loader), meters)\n\n    def _logits_and_loss(self, X, y):\n        self.mutator.reset()\n        logits = self.model(X)\n        loss = self.loss(logits, y)\n        self._write_graph_status()\n        return logits, loss\n\n    def _backward(self, val_X, val_y):\n        """"""\n        Simple backward with gradient descent\n        """"""\n        _, loss = self._logits_and_loss(val_X, val_y)\n        loss.backward()\n\n    def _unrolled_backward(self, trn_X, trn_y, val_X, val_y):\n        """"""\n        Compute unrolled loss and backward its gradients\n        """"""\n        backup_params = copy.deepcopy(tuple(self.model.parameters()))\n\n        # do virtual step on training data\n        lr = self.optimizer.param_groups[0][""lr""]\n        momentum = self.optimizer.param_groups[0][""momentum""]\n        weight_decay = self.optimizer.param_groups[0][""weight_decay""]\n        self._compute_virtual_model(trn_X, trn_y, lr, momentum, weight_decay)\n\n        # calculate unrolled loss on validation data\n        # keep gradients for model here for compute hessian\n        _, loss = self._logits_and_loss(val_X, val_y)\n        w_model, w_ctrl = tuple(self.model.parameters()), tuple(self.mutator.parameters())\n        w_grads = torch.autograd.grad(loss, w_model + w_ctrl)\n        d_model, d_ctrl = w_grads[:len(w_model)], w_grads[len(w_model):]\n\n        # compute hessian and final gradients\n        hessian = self._compute_hessian(backup_params, d_model, trn_X, trn_y)\n        with torch.no_grad():\n            for param, d, h in zip(w_ctrl, d_ctrl, hessian):\n                # gradient = dalpha - lr * hessian\n                param.grad = d - lr * h\n\n        # restore weights\n        self._restore_weights(backup_params)\n\n    def _compute_virtual_model(self, X, y, lr, momentum, weight_decay):\n        """"""\n        Compute unrolled weights w`\n        """"""\n        # don\'t need zero_grad, using autograd to calculate gradients\n        _, loss = self._logits_and_loss(X, y)\n        gradients = torch.autograd.grad(loss, self.model.parameters())\n        with torch.no_grad():\n            for w, g in zip(self.model.parameters(), gradients):\n                m = self.optimizer.state[w].get(""momentum_buffer"", 0.)\n                w = w - lr * (momentum * m + g + weight_decay * w)\n\n    def _restore_weights(self, backup_params):\n        with torch.no_grad():\n            for param, backup in zip(self.model.parameters(), backup_params):\n                param.copy_(backup)\n\n    def _compute_hessian(self, backup_params, dw, trn_X, trn_y):\n        """"""\n            dw = dw` { L_val(w`, alpha) }\n            w+ = w + eps * dw\n            w- = w - eps * dw\n            hessian = (dalpha { L_trn(w+, alpha) } - dalpha { L_trn(w-, alpha) }) / (2*eps)\n            eps = 0.01 / ||dw||\n        """"""\n        self._restore_weights(backup_params)\n        norm = torch.cat([w.view(-1) for w in dw]).norm()\n        eps = 0.01 / norm\n        if norm < 1E-8:\n            logger.warning(""In computing hessian, norm is smaller than 1E-8, cause eps to be %.6f."", norm.item())\n\n        dalphas = []\n        for e in [eps, -2. * eps]:\n            # w+ = w + eps*dw`, w- = w - eps*dw`\n            with torch.no_grad():\n                for p, d in zip(self.model.parameters(), dw):\n                    p += e * d\n\n            _, loss = self._logits_and_loss(trn_X, trn_y)\n            dalphas.append(torch.autograd.grad(loss, self.mutator.parameters()))\n\n        dalpha_pos, dalpha_neg = dalphas  # dalpha { L_trn(w+) }, # dalpha { L_trn(w-) }\n        hessian = [(p - n) / 2. * eps for p, n in zip(dalpha_pos, dalpha_neg)]\n        return hessian\n'"
src/sdk/pynni/nni/nas/pytorch/enas/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .mutator import EnasMutator\nfrom .trainer import EnasTrainer\n'
src/sdk/pynni/nni/nas/pytorch/enas/mutator.py,23,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutator import Mutator\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice, MutableScope\n\n\nclass StackedLSTMCell(nn.Module):\n    def __init__(self, layers, size, bias):\n        super().__init__()\n        self.lstm_num_layers = layers\n        self.lstm_modules = nn.ModuleList([nn.LSTMCell(size, size, bias=bias)\n                                           for _ in range(self.lstm_num_layers)])\n\n    def forward(self, inputs, hidden):\n        prev_c, prev_h = hidden\n        next_c, next_h = [], []\n        for i, m in enumerate(self.lstm_modules):\n            curr_c, curr_h = m(inputs, (prev_c[i], prev_h[i]))\n            next_c.append(curr_c)\n            next_h.append(curr_h)\n            # current implementation only supports batch size equals 1,\n            # but the algorithm does not necessarily have this limitation\n            inputs = curr_h[-1].view(1, -1)\n        return next_c, next_h\n\n\nclass EnasMutator(Mutator):\n    """"""\n    A mutator that mutates the graph with RL.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model.\n    lstm_size : int\n        Controller LSTM hidden units.\n    lstm_num_layers : int\n        Number of layers for stacked LSTM.\n    tanh_constant : float\n        Logits will be equal to ``tanh_constant * tanh(logits)``. Don\'t use ``tanh`` if this value is ``None``.\n    cell_exit_extra_step : bool\n        If true, RL controller will perform an extra step at the exit of each MutableScope, dump the hidden state\n        and mark it as the hidden state of this MutableScope. This is to align with the original implementation of paper.\n    skip_target : float\n        Target probability that skipconnect will appear.\n    temperature : float\n        Temperature constant that divides the logits.\n    branch_bias : float\n        Manual bias applied to make some operations more likely to be chosen.\n        Currently this is implemented with a hardcoded match rule that aligns with original repo.\n        If a mutable has a ``reduce`` in its key, all its op choices\n        that contains `conv` in their typename will receive a bias of ``+self.branch_bias`` initially; while others\n        receive a bias of ``-self.branch_bias``.\n    entropy_reduction : str\n        Can be one of ``sum`` and ``mean``. How the entropy of multi-input-choice is reduced.\n    """"""\n\n    def __init__(self, model, lstm_size=64, lstm_num_layers=1, tanh_constant=1.5, cell_exit_extra_step=False,\n                 skip_target=0.4, temperature=None, branch_bias=0.25, entropy_reduction=""sum""):\n        super().__init__(model)\n        self.lstm_size = lstm_size\n        self.lstm_num_layers = lstm_num_layers\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n        self.skip_target = skip_target\n        self.branch_bias = branch_bias\n\n        self.lstm = StackedLSTMCell(self.lstm_num_layers, self.lstm_size, False)\n        self.attn_anchor = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.attn_query = nn.Linear(self.lstm_size, self.lstm_size, bias=False)\n        self.v_attn = nn.Linear(self.lstm_size, 1, bias=False)\n        self.g_emb = nn.Parameter(torch.randn(1, self.lstm_size) * 0.1)\n        self.skip_targets = nn.Parameter(torch.tensor([1.0 - self.skip_target, self.skip_target]), requires_grad=False)  # pylint: disable=not-callable\n        assert entropy_reduction in [""sum"", ""mean""], ""Entropy reduction must be one of sum and mean.""\n        self.entropy_reduction = torch.sum if entropy_reduction == ""sum"" else torch.mean\n        self.cross_entropy_loss = nn.CrossEntropyLoss(reduction=""none"")\n        self.bias_dict = nn.ParameterDict()\n\n        self.max_layer_choice = 0\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                    ""ENAS mutator requires all layer choice have the same number of candidates.""\n                # We are judging by keys and module types to add biases to layer choices. Needs refactor.\n                if ""reduce"" in mutable.key:\n                    def is_conv(choice):\n                        return ""conv"" in str(type(choice)).lower()\n                    bias = torch.tensor([self.branch_bias if is_conv(choice) else -self.branch_bias  # pylint: disable=not-callable\n                                         for choice in mutable])\n                    self.bias_dict[mutable.key] = nn.Parameter(bias, requires_grad=False)\n\n        self.embedding = nn.Embedding(self.max_layer_choice + 1, self.lstm_size)\n        self.soft = nn.Linear(self.lstm_size, self.max_layer_choice, bias=False)\n\n    def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        return self._choices\n\n    def sample_final(self):\n        return self.sample_search()\n\n    def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            if self.cell_exit_extra_step:\n                self._lstm_next_step()\n            self._mark_anchor(mutable.key)\n\n    def _initialize(self):\n        self._choices = dict()\n        self._anchors_hid = dict()\n        self._inputs = self.g_emb.data\n        self._c = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self._h = [torch.zeros((1, self.lstm_size),\n                               dtype=self._inputs.dtype,\n                               device=self._inputs.device) for _ in range(self.lstm_num_layers)]\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n    def _lstm_next_step(self):\n        self._c, self._h = self.lstm(self._inputs, (self._c, self._h))\n\n    def _mark_anchor(self, key):\n        self._anchors_hid[key] = self._h[-1]\n\n    def _sample_layer_choice(self, mutable):\n        self._lstm_next_step()\n        logit = self.soft(self._h[-1])\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * torch.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        branch_id = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n        log_prob = self.cross_entropy_loss(logit, branch_id)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = self.embedding(branch_id)\n        return F.one_hot(branch_id, num_classes=self.max_layer_choice).bool().view(-1)\n\n    def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._lstm_next_step()\n                self._mark_anchor(label)  # empty loop, fill not found\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = torch.cat(query, 0)\n        query = torch.tanh(query + self.attn_query(self._h[-1]))\n        query = self.v_attn(query)\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * torch.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = torch.cat([-query, query], 1)  # pylint: disable=invalid-unary-operand-type\n\n            skip = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip_prob = torch.sigmoid(logit)\n            kl = torch.sum(skip_prob * torch.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(logit, skip)\n            self._inputs = (torch.matmul(skip.float(), torch.cat(anchors, 0)) / (1. + torch.sum(skip))).unsqueeze(0)\n        else:\n            assert mutable.n_chosen == 1, ""Input choice must select exactly one or any in ENAS.""\n            logit = query.view(1, -1)\n            index = torch.multinomial(F.softmax(logit, dim=-1), 1).view(-1)\n            skip = F.one_hot(index, num_classes=mutable.n_candidates).view(-1)\n            log_prob = self.cross_entropy_loss(logit, index)\n            self._inputs = anchors[index.item()]\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = (log_prob * torch.exp(-log_prob)).detach()  # pylint: disable=invalid-unary-operand-type\n        self.sample_entropy += self.entropy_reduction(entropy)\n        return skip.bool()\n'"
src/sdk/pynni/nni/nas/pytorch/enas/trainer.py,14,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\nfrom itertools import cycle\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom nni.nas.pytorch.trainer import Trainer\nfrom nni.nas.pytorch.utils import AverageMeterGroup, to_device\nfrom .mutator import EnasMutator\n\nlogger = logging.getLogger(__name__)\n\n\nclass EnasTrainer(Trainer):\n    """"""\n    ENAS trainer.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model to be trained.\n    loss : callable\n        Receives logits and ground truth label, return a loss tensor.\n    metrics : callable\n        Receives logits and ground truth label, return a dict of metrics.\n    reward_function : callable\n        Receives logits and ground truth label, return a tensor, which will be feeded to RL controller as reward.\n    optimizer : Optimizer\n        The optimizer used for optimizing the model.\n    num_epochs : int\n        Number of epochs planned for training.\n    dataset_train : Dataset\n        Dataset for training. Will be split for training weights and architecture weights.\n    dataset_valid : Dataset\n        Dataset for testing.\n    mutator : EnasMutator\n        Use when customizing your own mutator or a mutator with customized parameters.\n    batch_size : int\n        Batch size.\n    workers : int\n        Workers for data loading.\n    device : torch.device\n        ``torch.device(""cpu"")`` or ``torch.device(""cuda"")``.\n    log_frequency : int\n        Step count per logging.\n    callbacks : list of Callback\n        list of callbacks to trigger at events.\n    entropy_weight : float\n        Weight of sample entropy loss.\n    skip_weight : float\n        Weight of skip penalty loss.\n    baseline_decay : float\n        Decay factor of baseline. New baseline will be equal to ``baseline_decay * baseline_old + reward * (1 - baseline_decay)``.\n    child_steps : int\n        How many mini-batches for model training per epoch.\n    mutator_lr : float\n        Learning rate for RL controller.\n    mutator_steps_aggregate : int\n        Number of steps that will be aggregated into one mini-batch for RL controller.\n    mutator_steps : int\n        Number of mini-batches for each epoch of RL controller learning.\n    aux_weight : float\n        Weight of auxiliary head loss. ``aux_weight * aux_loss`` will be added to total loss.\n    test_arc_per_epoch : int\n        How many architectures are chosen for direct test after each epoch.\n    """"""\n    def __init__(self, model, loss, metrics, reward_function,\n                 optimizer, num_epochs, dataset_train, dataset_valid,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None,\n                 entropy_weight=0.0001, skip_weight=0.8, baseline_decay=0.999, child_steps=500,\n                 mutator_lr=0.00035, mutator_steps_aggregate=20, mutator_steps=50, aux_weight=0.4,\n                 test_arc_per_epoch=1):\n        super().__init__(model, mutator if mutator is not None else EnasMutator(model),\n                         loss, metrics, optimizer, num_epochs, dataset_train, dataset_valid,\n                         batch_size, workers, device, log_frequency, callbacks)\n        self.reward_function = reward_function\n        self.mutator_optim = optim.Adam(self.mutator.parameters(), lr=mutator_lr)\n        self.batch_size = batch_size\n        self.workers = workers\n\n        self.entropy_weight = entropy_weight\n        self.skip_weight = skip_weight\n        self.baseline_decay = baseline_decay\n        self.baseline = 0.\n        self.mutator_steps_aggregate = mutator_steps_aggregate\n        self.mutator_steps = mutator_steps\n        self.child_steps = child_steps\n        self.aux_weight = aux_weight\n        self.test_arc_per_epoch = test_arc_per_epoch\n\n        self.init_dataloader()\n\n    def init_dataloader(self):\n        n_train = len(self.dataset_train)\n        split = n_train // 10\n        indices = list(range(n_train))\n        train_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[:-split])\n        valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(indices[-split:])\n        self.train_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=train_sampler,\n                                                        num_workers=self.workers)\n        self.valid_loader = torch.utils.data.DataLoader(self.dataset_train,\n                                                        batch_size=self.batch_size,\n                                                        sampler=valid_sampler,\n                                                        num_workers=self.workers)\n        self.test_loader = torch.utils.data.DataLoader(self.dataset_valid,\n                                                       batch_size=self.batch_size,\n                                                       num_workers=self.workers)\n        self.train_loader = cycle(self.train_loader)\n        self.valid_loader = cycle(self.valid_loader)\n\n    def train_one_epoch(self, epoch):\n        # Sample model and train\n        self.model.train()\n        self.mutator.eval()\n        meters = AverageMeterGroup()\n        for step in range(1, self.child_steps + 1):\n            x, y = next(self.train_loader)\n            x, y = to_device(x, self.device), to_device(y, self.device)\n            self.optimizer.zero_grad()\n\n            with torch.no_grad():\n                self.mutator.reset()\n            self._write_graph_status()\n            logits = self.model(x)\n\n            if isinstance(logits, tuple):\n                logits, aux_logits = logits\n                aux_loss = self.loss(aux_logits, y)\n            else:\n                aux_loss = 0.\n            metrics = self.metrics(logits, y)\n            loss = self.loss(logits, y)\n            loss = loss + self.aux_weight * aux_loss\n            loss.backward()\n            nn.utils.clip_grad_norm_(self.model.parameters(), 5.)\n            self.optimizer.step()\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(""Model Epoch [%d/%d] Step [%d/%d]  %s"", epoch + 1,\n                            self.num_epochs, step, self.child_steps, meters)\n\n        # Train sampler (mutator)\n        self.model.eval()\n        self.mutator.train()\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, self.mutator_steps + 1):\n            self.mutator_optim.zero_grad()\n            for step in range(1, self.mutator_steps_aggregate + 1):\n                x, y = next(self.valid_loader)\n                x, y = to_device(x, self.device), to_device(y, self.device)\n\n                self.mutator.reset()\n                with torch.no_grad():\n                    logits = self.model(x)\n                self._write_graph_status()\n                metrics = self.metrics(logits, y)\n                reward = self.reward_function(logits, y)\n                if self.entropy_weight:\n                    reward += self.entropy_weight * self.mutator.sample_entropy.item()\n                self.baseline = self.baseline * self.baseline_decay + reward * (1 - self.baseline_decay)\n                loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                if self.skip_weight:\n                    loss += self.skip_weight * self.mutator.sample_skip_penalty\n                metrics[""reward""] = reward\n                metrics[""loss""] = loss.item()\n                metrics[""ent""] = self.mutator.sample_entropy.item()\n                metrics[""log_prob""] = self.mutator.sample_log_prob.item()\n                metrics[""baseline""] = self.baseline\n                metrics[""skip""] = self.mutator.sample_skip_penalty\n\n                loss /= self.mutator_steps_aggregate\n                loss.backward()\n                meters.update(metrics)\n\n                cur_step = step + (mutator_step - 1) * self.mutator_steps_aggregate\n                if self.log_frequency is not None and cur_step % self.log_frequency == 0:\n                    logger.info(""RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s"", epoch + 1, self.num_epochs,\n                                mutator_step, self.mutator_steps, step, self.mutator_steps_aggregate,\n                                meters)\n\n            nn.utils.clip_grad_norm_(self.mutator.parameters(), 5.)\n            self.mutator_optim.step()\n\n    def validate_one_epoch(self, epoch):\n        with torch.no_grad():\n            for arc_id in range(self.test_arc_per_epoch):\n                meters = AverageMeterGroup()\n                for x, y in self.test_loader:\n                    x, y = to_device(x, self.device), to_device(y, self.device)\n                    self.mutator.reset()\n                    logits = self.model(x)\n                    if isinstance(logits, tuple):\n                        logits, _ = logits\n                    metrics = self.metrics(logits, y)\n                    loss = self.loss(logits, y)\n                    metrics[""loss""] = loss.item()\n                    meters.update(metrics)\n\n                logger.info(""Test Epoch [%d/%d] Arc [%d/%d] Summary  %s"",\n                            epoch + 1, self.num_epochs, arc_id + 1, self.test_arc_per_epoch,\n                            meters.summary())\n'"
src/sdk/pynni/nni/nas/pytorch/pdarts/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .trainer import PdartsTrainer\n'
src/sdk/pynni/nni/nas/pytorch/pdarts/mutator.py,4,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport copy\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nfrom nni.nas.pytorch.darts import DartsMutator\nfrom nni.nas.pytorch.mutables import LayerChoice\n\n\nclass PdartsMutator(DartsMutator):\n    """"""\n    It works with PdartsTrainer to calculate ops weights,\n    and drop weights in different PDARTS epochs.\n    """"""\n\n    def __init__(self, model, pdarts_epoch_index, pdarts_num_to_drop, switches={}):\n        self.pdarts_epoch_index = pdarts_epoch_index\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        if switches is None:\n            self.switches = {}\n        else:\n            self.switches = switches\n\n        super(PdartsMutator, self).__init__(model)\n\n        # this loop go through mutables with different keys,\n        # it\'s mainly to update length of choices.\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n\n                switches = self.switches.get(mutable.key, [True for j in range(len(mutable))])\n                choices = self.choices[mutable.key]\n\n                operations_count = np.sum(switches)\n                # +1 and -1 are caused by zero operation in darts network\n                # the zero operation is not in choices list in network, but its weight are in,\n                # so it needs one more weights and switch for zero.\n                self.choices[mutable.key] = nn.Parameter(1.0E-3 * torch.randn(operations_count + 1))\n                self.switches[mutable.key] = switches\n\n        # update LayerChoice instances in model,\n        # it\'s physically remove dropped choices operations.\n        for module in self.model.modules():\n            if isinstance(module, LayerChoice):\n                switches = self.switches.get(module.key)\n                choices = self.choices[module.key]\n                if len(module) > len(choices):\n                    # from last to first, so that it won\'t effect previous indexes after removed one.\n                    for index in range(len(switches)-1, -1, -1):\n                        if switches[index] == False:\n                            del module[index]\n                assert len(module) <= len(choices), ""Failed to remove dropped choices.""\n\n    def export(self):\n        # Cannot rely on super().export() because P-DARTS has deleted some of the choices and has misaligned length.\n        results = super().sample_final()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                # As some operations are dropped physically,\n                # so it needs to fill back false to track dropped operations.\n                trained_result = results[mutable.key]\n                trained_index = 0\n                switches = self.switches[mutable.key]\n                result = torch.Tensor(switches).bool()\n                for index in range(len(result)):\n                    if result[index]:\n                        result[index] = trained_result[trained_index]\n                        trained_index += 1\n                results[mutable.key] = result\n        return results\n\n    def drop_paths(self):\n        """"""\n        This method is called when a PDARTS epoch is finished.\n        It prepares switches for next epoch.\n        candidate operations with False switch will be doppped in next epoch.\n        """"""\n        all_switches = copy.deepcopy(self.switches)\n        for key in all_switches:\n            switches = all_switches[key]\n            idxs = []\n            for j in range(len(switches)):\n                if switches[j]:\n                    idxs.append(j)\n            sorted_weights = self.choices[key].data.cpu().numpy()[:-1]\n            drop = np.argsort(sorted_weights)[:self.pdarts_num_to_drop[self.pdarts_epoch_index]]\n            for idx in drop:\n                switches[idxs[idx]] = False\n        return all_switches\n'"
src/sdk/pynni/nni/nas/pytorch/pdarts/trainer.py,3,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\n\nfrom nni.nas.pytorch.callbacks import LRSchedulerCallback\nfrom nni.nas.pytorch.darts import DartsTrainer\nfrom nni.nas.pytorch.trainer import BaseTrainer, TorchTensorEncoder\n\nfrom .mutator import PdartsMutator\n\nlogger = logging.getLogger(__name__)\n\n\nclass PdartsTrainer(BaseTrainer):\n    """"""\n    This trainer implements the PDARTS algorithm.\n    PDARTS bases on DARTS algorithm, and provides a network growth approach to find deeper and better network.\n    This class relies on pdarts_num_layers and pdarts_num_to_drop parameters to control how network grows.\n    pdarts_num_layers means how many layers more than first epoch.\n    pdarts_num_to_drop means how many candidate operations should be dropped in each epoch.\n        So that the grew network can in similar size.\n    """"""\n\n    def __init__(self, model_creator, init_layers, metrics,\n                 num_epochs, dataset_train, dataset_valid,\n                 pdarts_num_layers=[0, 6, 12], pdarts_num_to_drop=[3, 2, 1],\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None, callbacks=None, unrolled=False):\n        super(PdartsTrainer, self).__init__()\n        self.model_creator = model_creator\n        self.init_layers = init_layers\n        self.pdarts_num_layers = pdarts_num_layers\n        self.pdarts_num_to_drop = pdarts_num_to_drop\n        self.pdarts_epoch = len(pdarts_num_to_drop)\n        self.darts_parameters = {\n            ""metrics"": metrics,\n            ""num_epochs"": num_epochs,\n            ""dataset_train"": dataset_train,\n            ""dataset_valid"": dataset_valid,\n            ""batch_size"": batch_size,\n            ""workers"": workers,\n            ""device"": device,\n            ""log_frequency"": log_frequency,\n            ""unrolled"": unrolled\n        }\n        self.callbacks = callbacks if callbacks is not None else []\n\n    def train(self):\n\n        switches = None\n        for epoch in range(self.pdarts_epoch):\n\n            layers = self.init_layers+self.pdarts_num_layers[epoch]\n            model, criterion, optim, lr_scheduler = self.model_creator(layers)\n            self.mutator = PdartsMutator(model, epoch, self.pdarts_num_to_drop, switches)\n\n            for callback in self.callbacks:\n                callback.build(model, self.mutator, self)\n                callback.on_epoch_begin(epoch)\n\n            darts_callbacks = []\n            if lr_scheduler is not None:\n                darts_callbacks.append(LRSchedulerCallback(lr_scheduler))\n\n            self.trainer = DartsTrainer(model, mutator=self.mutator, loss=criterion, optimizer=optim,\n                                        callbacks=darts_callbacks, **self.darts_parameters)\n            logger.info(""start pdarts training epoch %s..."", epoch)\n\n            self.trainer.train()\n\n            switches = self.mutator.drop_paths()\n\n            for callback in self.callbacks:\n                callback.on_epoch_end(epoch)\n\n    def validate(self):\n        self.trainer.validate()\n\n    def export(self, file):\n        mutator_export = self.mutator.export()\n        with open(file, ""w"") as f:\n            json.dump(mutator_export, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def checkpoint(self):\n        raise NotImplementedError(""Not implemented yet"")\n'"
src/sdk/pynni/nni/nas/pytorch/proxylessnas/__init__.py,0,b'from .mutator import ProxylessNasMutator\nfrom .trainer import ProxylessNasTrainer\n'
src/sdk/pynni/nni/nas/pytorch/proxylessnas/mutator.py,22,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport math\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nimport numpy as np\n\nfrom nni.nas.pytorch.base_mutator import BaseMutator\nfrom nni.nas.pytorch.mutables import LayerChoice\nfrom .utils import detach_variable\n\nclass ArchGradientFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, binary_gates, run_func, backward_func):\n        ctx.run_func = run_func\n        ctx.backward_func = backward_func\n\n        detached_x = detach_variable(x)\n        with torch.enable_grad():\n            output = run_func(detached_x)\n        ctx.save_for_backward(detached_x, output)\n        return output.data\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        detached_x, output = ctx.saved_tensors\n\n        grad_x = torch.autograd.grad(output, detached_x, grad_output, only_inputs=True)\n        # compute gradients w.r.t. binary_gates\n        binary_grads = ctx.backward_func(detached_x.data, output.data, grad_output.data)\n\n        return grad_x[0], binary_grads, None, None\n\nclass MixedOp(nn.Module):\n    """"""\n    This class is to instantiate and manage info of one LayerChoice.\n    It includes architecture weights, binary weights, and member functions\n    operating the weights.\n\n    forward_mode:\n        forward/backward mode for LayerChoice: None, two, full, and full_v2.\n        For training architecture weights, we use full_v2 by default, and for training\n        model weights, we use None.\n    """"""\n    forward_mode = None\n    def __init__(self, mutable):\n        """"""\n        Parameters\n        ----------\n        mutable : LayerChoice\n            A LayerChoice in user model\n        """"""\n        super(MixedOp, self).__init__()\n        self.ap_path_alpha = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_wb = nn.Parameter(torch.Tensor(len(mutable)))\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False\n        self.active_index = [0]\n        self.inactive_index = None\n        self.log_prob = None\n        self.current_prob_over_ops = None\n        self.n_choices = len(mutable)\n\n    def get_ap_path_alpha(self):\n        return self.ap_path_alpha\n\n    def to_requires_grad(self):\n        self.ap_path_alpha.requires_grad = True\n        self.ap_path_wb.requires_grad = True\n\n    def to_disable_grad(self):\n        self.ap_path_alpha.requires_grad = False\n        self.ap_path_wb.requires_grad = False\n\n    def forward(self, mutable, x):\n        """"""\n        Define forward of LayerChoice. For \'full_v2\', backward is also defined.\n        The \'two\' mode is explained in section 3.2.1 in the paper.\n        The \'full_v2\' mode is explained in Appendix D in the paper.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer\'s mutable\n        x : tensor\n            inputs of this layer, only support one input\n\n        Returns\n        -------\n        output: tensor\n            output of this layer\n        """"""\n        if MixedOp.forward_mode == \'full\' or MixedOp.forward_mode == \'two\':\n            output = 0\n            for _i in self.active_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi\n            for _i in self.inactive_index:\n                oi = self.candidate_ops[_i](x)\n                output = output + self.ap_path_wb[_i] * oi.detach()\n        elif MixedOp.forward_mode == \'full_v2\':\n            def run_function(key, candidate_ops, active_id):\n                def forward(_x):\n                    return candidate_ops[active_id](_x)\n                return forward\n\n            def backward_function(key, candidate_ops, active_id, binary_gates):\n                def backward(_x, _output, grad_output):\n                    binary_grads = torch.zeros_like(binary_gates.data)\n                    with torch.no_grad():\n                        for k in range(len(candidate_ops)):\n                            if k != active_id:\n                                out_k = candidate_ops[k](_x.data)\n                            else:\n                                out_k = _output.data\n                            grad_k = torch.sum(out_k * grad_output)\n                            binary_grads[k] = grad_k\n                    return binary_grads\n                return backward\n            output = ArchGradientFunction.apply(\n                x, self.ap_path_wb, run_function(mutable.key, list(mutable), self.active_index[0]),\n                backward_function(mutable.key, list(mutable), self.active_index[0], self.ap_path_wb))\n        else:\n            output = self.active_op(mutable)(x)\n        return output\n\n    @property\n    def probs_over_ops(self):\n        """"""\n        Apply softmax on alpha to generate probability distribution\n\n        Returns\n        -------\n        pytorch tensor\n            probability distribution\n        """"""\n        probs = F.softmax(self.ap_path_alpha, dim=0)  # softmax to probability\n        return probs\n\n    @property\n    def chosen_index(self):\n        """"""\n        choose the op with max prob\n\n        Returns\n        -------\n        int\n            index of the chosen one\n        numpy.float32\n            prob of the chosen one\n        """"""\n        probs = self.probs_over_ops.data.cpu().numpy()\n        index = int(np.argmax(probs))\n        return index, probs[index]\n\n    def active_op(self, mutable):\n        """"""\n        assume only one path is active\n\n        Returns\n        -------\n        PyTorch module\n            the chosen operation\n        """"""\n        return mutable[self.active_index[0]]\n\n    @property\n    def active_op_index(self):\n        """"""\n        return active op\'s index, the active op is sampled\n\n        Returns\n        -------\n        int\n            index of the active op\n        """"""\n        return self.active_index[0]\n\n    def set_chosen_op_active(self):\n        """"""\n        set chosen index, active and inactive indexes\n        """"""\n        chosen_idx, _ = self.chosen_index\n        self.active_index = [chosen_idx]\n        self.inactive_index = [_i for _i in range(0, chosen_idx)] + \\\n                              [_i for _i in range(chosen_idx + 1, self.n_choices)]\n\n    def binarize(self, mutable):\n        """"""\n        Sample based on alpha, and set binary weights accordingly.\n        ap_path_wb is set in this function, which is called binarize.\n\n        Parameters\n        ----------\n        mutable : LayerChoice\n            this layer\'s mutable\n        """"""\n        self.log_prob = None\n        # reset binary gates\n        self.ap_path_wb.data.zero_()\n        probs = self.probs_over_ops\n        if MixedOp.forward_mode == \'two\':\n            # sample two ops according to probs\n            sample_op = torch.multinomial(probs.data, 2, replacement=False)\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in sample_op\n            ]), dim=0)\n            self.current_prob_over_ops = torch.zeros_like(probs)\n            for i, idx in enumerate(sample_op):\n                self.current_prob_over_ops[idx] = probs_slice[i]\n            # choose one to be active and the other to be inactive according to probs_slice\n            c = torch.multinomial(probs_slice.data, 1)[0] # 0 or 1\n            active_op = sample_op[c].item()\n            inactive_op = sample_op[1-c].item()\n            self.active_index = [active_op]\n            self.inactive_index = [inactive_op]\n            # set binary gate\n            self.ap_path_wb.data[active_op] = 1.0\n        else:\n            sample = torch.multinomial(probs, 1)[0].item()\n            self.active_index = [sample]\n            self.inactive_index = [_i for _i in range(0, sample)] + \\\n                                [_i for _i in range(sample + 1, len(mutable))]\n            self.log_prob = torch.log(probs[sample])\n            self.current_prob_over_ops = probs\n            self.ap_path_wb.data[sample] = 1.0\n        # avoid over-regularization\n        for choice in mutable:\n            for _, param in choice.named_parameters():\n                param.grad = None\n\n    @staticmethod\n    def delta_ij(i, j):\n        if i == j:\n            return 1\n        else:\n            return 0\n\n    def set_arch_param_grad(self, mutable):\n        """"""\n        Calculate alpha gradient for this LayerChoice.\n        It is calculated using gradient of binary gate, probs of ops.\n        """"""\n        binary_grads = self.ap_path_wb.grad.data\n        if self.active_op(mutable).is_zero_layer():\n            self.ap_path_alpha.grad = None\n            return\n        if self.ap_path_alpha.grad is None:\n            self.ap_path_alpha.grad = torch.zeros_like(self.ap_path_alpha.data)\n        if MixedOp.forward_mode == \'two\':\n            involved_idx = self.active_index + self.inactive_index\n            probs_slice = F.softmax(torch.stack([\n                self.ap_path_alpha[idx] for idx in involved_idx\n            ]), dim=0).data\n            for i in range(2):\n                for j in range(2):\n                    origin_i = involved_idx[i]\n                    origin_j = involved_idx[j]\n                    self.ap_path_alpha.grad.data[origin_i] += \\\n                        binary_grads[origin_j] * probs_slice[j] * (MixedOp.delta_ij(i, j) - probs_slice[i])\n            for _i, idx in enumerate(self.active_index):\n                self.active_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n            for _i, idx in enumerate(self.inactive_index):\n                self.inactive_index[_i] = (idx, self.ap_path_alpha.data[idx].item())\n        else:\n            probs = self.probs_over_ops.data\n            for i in range(self.n_choices):\n                for j in range(self.n_choices):\n                    self.ap_path_alpha.grad.data[i] += binary_grads[j] * probs[j] * (MixedOp.delta_ij(i, j) - probs[i])\n        return\n\n    def rescale_updated_arch_param(self):\n        """"""\n        rescale architecture weights for the \'two\' mode.\n        """"""\n        if not isinstance(self.active_index[0], tuple):\n            assert self.active_op.is_zero_layer()\n            return\n        involved_idx = [idx for idx, _ in (self.active_index + self.inactive_index)]\n        old_alphas = [alpha for _, alpha in (self.active_index + self.inactive_index)]\n        new_alphas = [self.ap_path_alpha.data[idx] for idx in involved_idx]\n\n        offset = math.log(\n            sum([math.exp(alpha) for alpha in new_alphas]) / sum([math.exp(alpha) for alpha in old_alphas])\n        )\n\n        for idx in involved_idx:\n            self.ap_path_alpha.data[idx] -= offset\n\n\nclass ProxylessNasMutator(BaseMutator):\n    """"""\n    This mutator initializes and operates all the LayerChoices of the input model.\n    It is for the corresponding trainer to control the training process of LayerChoices,\n    coordinating with whole training process.\n    """"""\n    def __init__(self, model):\n        """"""\n        Init a MixedOp instance for each mutable i.e., LayerChoice.\n        And register the instantiated MixedOp in corresponding LayerChoice.\n        If does not register it in LayerChoice, DataParallel does not work then,\n        because architecture weights are not included in the DataParallel model.\n        When MixedOPs are registered, we use ```requires_grad``` to control\n        whether calculate gradients of architecture weights.\n\n        Parameters\n        ----------\n        model : pytorch model\n            The model that users want to tune, it includes search space defined with nni nas apis\n        """"""\n        super(ProxylessNasMutator, self).__init__(model)\n        self._unused_modules = None\n        self.mutable_list = []\n        for mutable in self.undedup_mutables:\n            self.mutable_list.append(mutable)\n            mutable.registered_module = MixedOp(mutable)\n\n    def on_forward_layer_choice(self, mutable, *args, **kwargs):\n        """"""\n        Callback of layer choice forward. This function defines the forward\n        logic of the input mutable. So mutable is only interface, its real\n        implementation is defined in mutator.\n\n        Parameters\n        ----------\n        mutable: LayerChoice\n            forward logic of this input mutable\n        args: list of torch.Tensor\n            inputs of this mutable\n        kwargs: dict\n            inputs of this mutable\n\n        Returns\n        -------\n        torch.Tensor\n            output of this mutable, i.e., LayerChoice\n        int\n            index of the chosen op\n        """"""\n        # FIXME: return mask, to be consistent with other algorithms\n        idx = mutable.registered_module.active_op_index\n        return mutable.registered_module(mutable, *args, **kwargs), idx\n\n    def reset_binary_gates(self):\n        """"""\n        For each LayerChoice, binarize binary weights\n        based on alpha to only activate one op.\n        It traverses all the mutables in the model to do this.\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.binarize(mutable)\n\n    def set_chosen_op_active(self):\n        """"""\n        For each LayerChoice, set the op with highest alpha as the chosen op.\n        Usually used for validation.\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_chosen_op_active()\n\n    def num_arch_params(self):\n        """"""\n        The number of mutables, i.e., LayerChoice\n\n        Returns\n        -------\n        int\n            the number of LayerChoice in user model\n        """"""\n        return len(self.mutable_list)\n\n    def set_arch_param_grad(self):\n        """"""\n        For each LayerChoice, calculate gradients for architecture weights, i.e., alpha\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.set_arch_param_grad(mutable)\n\n    def get_architecture_parameters(self):\n        """"""\n        Get all the architecture parameters.\n\n        yield\n        -----\n        PyTorch Parameter\n            Return ap_path_alpha of the traversed mutable\n        """"""\n        for mutable in self.undedup_mutables:\n            yield mutable.registered_module.get_ap_path_alpha()\n\n    def change_forward_mode(self, mode):\n        """"""\n        Update forward mode of MixedOps, as training architecture weights and\n        model weights use different forward modes.\n        """"""\n        MixedOp.forward_mode = mode\n\n    def get_forward_mode(self):\n        """"""\n        Get forward mode of MixedOp\n\n        Returns\n        -------\n        string\n            the current forward mode of MixedOp\n        """"""\n        return MixedOp.forward_mode\n\n    def rescale_updated_arch_param(self):\n        """"""\n        Rescale architecture weights in \'two\' mode.\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.rescale_updated_arch_param()\n\n    def unused_modules_off(self):\n        """"""\n        Remove unused modules for each mutables.\n        The removed modules are kept in ```self._unused_modules``` for resume later.\n        """"""\n        self._unused_modules = []\n        for mutable in self.undedup_mutables:\n            mixed_op = mutable.registered_module\n            unused = {}\n            if self.get_forward_mode() in [\'full\', \'two\', \'full_v2\']:\n                involved_index = mixed_op.active_index + mixed_op.inactive_index\n            else:\n                involved_index = mixed_op.active_index\n            for i in range(mixed_op.n_choices):\n                if i not in involved_index:\n                    unused[i] = mutable[i]\n                    mutable[i] = None\n            self._unused_modules.append(unused)\n\n    def unused_modules_back(self):\n        """"""\n        Resume the removed modules back.\n        """"""\n        if self._unused_modules is None:\n            return\n        for m, unused in zip(self.mutable_list, self._unused_modules):\n            for i in unused:\n                m[i] = unused[i]\n        self._unused_modules = None\n\n    def arch_requires_grad(self):\n        """"""\n        Make architecture weights require gradient\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_requires_grad()\n\n    def arch_disable_grad(self):\n        """"""\n        Disable gradient of architecture weights, i.e., does not\n        calcuate gradient for them.\n        """"""\n        for mutable in self.undedup_mutables:\n            mutable.registered_module.to_disable_grad()\n\n    def sample_final(self):\n        """"""\n        Generate the final chosen architecture.\n\n        Returns\n        -------\n        dict\n            the choice of each mutable, i.e., LayerChoice\n        """"""\n        result = dict()\n        for mutable in self.undedup_mutables:\n            assert isinstance(mutable, LayerChoice)\n            index, _ = mutable.registered_module.chosen_index\n            # pylint: disable=not-callable\n            result[mutable.key] = F.one_hot(torch.tensor(index), num_classes=len(mutable)).view(-1).bool()\n        return result\n'"
src/sdk/pynni/nni/nas/pytorch/proxylessnas/trainer.py,8,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport math\nimport time\nimport json\nimport logging\n\nimport torch\nfrom torch import nn as nn\n\nfrom nni.nas.pytorch.base_trainer import BaseTrainer\nfrom nni.nas.pytorch.trainer import TorchTensorEncoder\nfrom nni.nas.pytorch.utils import AverageMeter\nfrom .mutator import ProxylessNasMutator\nfrom .utils import cross_entropy_with_label_smoothing, accuracy\n\nlogger = logging.getLogger(__name__)\n\nclass ProxylessNasTrainer(BaseTrainer):\n    def __init__(self, model, model_optim, device,\n                 train_loader, valid_loader, label_smoothing=0.1,\n                 n_epochs=120, init_lr=0.025, binary_mode=\'full_v2\',\n                 arch_init_type=\'normal\', arch_init_ratio=1e-3,\n                 arch_optim_lr=1e-3, arch_weight_decay=0,\n                 grad_update_arch_param_every=5, grad_update_steps=1,\n                 warmup=True, warmup_epochs=25,\n                 arch_valid_frequency=1,\n                 load_ckpt=False, ckpt_path=None, arch_path=None):\n        """"""\n        Parameters\n        ----------\n        model : pytorch model\n            the user model, which has mutables\n        model_optim : pytorch optimizer\n            the user defined optimizer\n        device : pytorch device\n            the devices to train/search the model\n        train_loader : pytorch data loader\n            data loader for the training set\n        valid_loader : pytorch data loader\n            data loader for the validation set\n        label_smoothing : float\n            for label smoothing\n        n_epochs : int\n            number of epochs to train/search\n        init_lr : float\n            init learning rate for training the model\n        binary_mode : str\n            the forward/backward mode for the binary weights in mutator\n        arch_init_type : str\n            the way to init architecture parameters\n        arch_init_ratio : float\n            the ratio to init architecture parameters\n        arch_optim_lr : float\n            learning rate of the architecture parameters optimizer\n        arch_weight_decay : float\n            weight decay of the architecture parameters optimizer\n        grad_update_arch_param_every : int\n            update architecture weights every this number of minibatches\n        grad_update_steps : int\n            during each update of architecture weights, the number of steps to train\n        warmup : bool\n            whether to do warmup\n        warmup_epochs : int\n            the number of epochs to do during warmup\n        arch_valid_frequency : int\n            frequency of printing validation result\n        load_ckpt : bool\n            whether load checkpoint\n        ckpt_path : str\n            checkpoint path, if load_ckpt is True, ckpt_path cannot be None\n        arch_path : str\n            the path to store chosen architecture\n        """"""\n        self.model = model\n        self.model_optim = model_optim\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n        self.device = device\n        self.n_epochs = n_epochs\n        self.init_lr = init_lr\n        self.warmup = warmup\n        self.warmup_epochs = warmup_epochs\n        self.arch_valid_frequency = arch_valid_frequency\n        self.label_smoothing = label_smoothing\n\n        self.train_batch_size = train_loader.batch_sampler.batch_size\n        self.valid_batch_size = valid_loader.batch_sampler.batch_size\n        # update architecture parameters every this number of minibatches\n        self.grad_update_arch_param_every = grad_update_arch_param_every\n        # the number of steps per architecture parameter update\n        self.grad_update_steps = grad_update_steps\n        self.binary_mode = binary_mode\n\n        self.load_ckpt = load_ckpt\n        self.ckpt_path = ckpt_path\n        self.arch_path = arch_path\n\n        # init mutator\n        self.mutator = ProxylessNasMutator(model)\n\n        # DataParallel should be put behind the init of mutator\n        self.model = torch.nn.DataParallel(self.model)\n        self.model.to(self.device)\n\n        # iter of valid dataset for training architecture weights\n        self._valid_iter = None\n        # init architecture weights\n        self._init_arch_params(arch_init_type, arch_init_ratio)\n        # build architecture optimizer\n        self.arch_optimizer = torch.optim.Adam(self.mutator.get_architecture_parameters(),\n                                               arch_optim_lr,\n                                               weight_decay=arch_weight_decay,\n                                               betas=(0, 0.999),\n                                               eps=1e-8)\n\n        self.criterion = nn.CrossEntropyLoss()\n        self.warmup_curr_epoch = 0\n        self.train_curr_epoch = 0\n\n    def _init_arch_params(self, init_type=\'normal\', init_ratio=1e-3):\n        """"""\n        Initialize architecture weights\n        """"""\n        for param in self.mutator.get_architecture_parameters():\n            if init_type == \'normal\':\n                param.data.normal_(0, init_ratio)\n            elif init_type == \'uniform\':\n                param.data.uniform_(-init_ratio, init_ratio)\n            else:\n                raise NotImplementedError\n\n    def _validate(self):\n        """"""\n        Do validation. During validation, LayerChoices use the chosen active op.\n\n        Returns\n        -------\n        float, float, float\n            average loss, average top1 accuracy, average top5 accuracy\n        """"""\n        self.valid_loader.batch_sampler.batch_size = self.valid_batch_size\n        self.valid_loader.batch_sampler.drop_last = False\n\n        self.mutator.set_chosen_op_active()\n        # remove unused modules to save memory\n        self.mutator.unused_modules_off()\n        # test on validation set under train mode\n        self.model.train()\n        batch_time = AverageMeter(\'batch_time\')\n        losses = AverageMeter(\'losses\')\n        top1 = AverageMeter(\'top1\')\n        top5 = AverageMeter(\'top5\')\n        end = time.time()\n        with torch.no_grad():\n            for i, (images, labels) in enumerate(self.valid_loader):\n                images, labels = images.to(self.device), labels.to(self.device)\n                output = self.model(images)\n                loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == len(self.valid_loader):\n                    test_log = \'Valid\' + \': [{0}/{1}]\\t\'\\\n                                        \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\'\\\n                                        \'Loss {loss.val:.4f} ({loss.avg:.4f})\\t\'\\\n                                        \'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\'.\\\n                        format(i, len(self.valid_loader) - 1, batch_time=batch_time, loss=losses, top1=top1)\n                    # return top5:\n                    test_log += \'\\tTop-5 acc {top5.val:.3f} ({top5.avg:.3f})\'.format(top5=top5)\n                    logger.info(test_log)\n        self.mutator.unused_modules_back()\n        return losses.avg, top1.avg, top5.avg\n\n    def _warm_up(self):\n        """"""\n        Warm up the model, during warm up, architecture weights are not trained.\n        """"""\n        lr_max = 0.05\n        data_loader = self.train_loader\n        nBatch = len(data_loader)\n        T_total = self.warmup_epochs * nBatch # total num of batches\n\n        for epoch in range(self.warmup_curr_epoch, self.warmup_epochs):\n            logger.info(\'\\n--------Warmup epoch: %d--------\\n\', epoch + 1)\n            batch_time = AverageMeter(\'batch_time\')\n            data_time = AverageMeter(\'data_time\')\n            losses = AverageMeter(\'losses\')\n            top1 = AverageMeter(\'top1\')\n            top5 = AverageMeter(\'top5\')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            logger.info(\'warm_up epoch: %d\', epoch)\n            for i, (images, labels) in enumerate(data_loader):\n                data_time.update(time.time() - end)\n                # lr\n                T_cur = epoch * nBatch + i\n                warmup_lr = 0.5 * lr_max * (1 + math.cos(math.pi * T_cur / T_total))\n                for param_group in self.model_optim.param_groups:\n                    param_group[\'lr\'] = warmup_lr\n                images, labels = images.to(self.device), labels.to(self.device)\n                # compute output\n                self.mutator.reset_binary_gates() # random sample binary gates\n                self.mutator.unused_modules_off() # remove unused module for speedup\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                # measure accuracy and record loss\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                # compute gradient and do SGD step\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                # unused modules back\n                self.mutator.unused_modules_back()\n                # measure elapsed time\n                batch_time.update(time.time() - end)\n                end = time.time()\n\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = \'Warmup Train [{0}][{1}/{2}]\\t\' \\\n                                \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' \\\n                                \'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t\' \\\n                                \'Loss {losses.val:.4f} ({losses.avg:.4f})\\t\' \\\n                                \'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t\' \\\n                                \'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}\'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=warmup_lr)\n                    logger.info(batch_log)\n            val_loss, val_top1, val_top5 = self._validate()\n            val_log = \'Warmup Valid [{0}/{1}]\\tloss {2:.3f}\\ttop-1 acc {3:.3f}\\ttop-5 acc {4:.3f}\\t\' \\\n                      \'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}M\'. \\\n                format(epoch + 1, self.warmup_epochs, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n            logger.info(val_log)\n            self.save_checkpoint()\n            self.warmup_curr_epoch += 1\n\n    def _get_update_schedule(self, nBatch):\n        """"""\n        Generate schedule for training architecture weights. Key means after which minibatch\n        to update architecture weights, value means how many steps for the update.\n\n        Parameters\n        ----------\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        dict\n            the schedule for updating architecture weights\n        """"""\n        schedule = {}\n        for i in range(nBatch):\n            if (i + 1) % self.grad_update_arch_param_every == 0:\n                schedule[i] = self.grad_update_steps\n        return schedule\n\n    def _calc_learning_rate(self, epoch, batch=0, nBatch=None):\n        """"""\n        Update learning rate.\n        """"""\n        T_total = self.n_epochs * nBatch\n        T_cur = epoch * nBatch + batch\n        lr = 0.5 * self.init_lr * (1 + math.cos(math.pi * T_cur / T_total))\n        return lr\n\n    def _adjust_learning_rate(self, optimizer, epoch, batch=0, nBatch=None):\n        """"""\n        Adjust learning of a given optimizer and return the new learning rate\n\n        Parameters\n        ----------\n        optimizer : pytorch optimizer\n            the used optimizer\n        epoch : int\n            the current epoch number\n        batch : int\n            the current minibatch\n        nBatch : int\n            the total number of minibatches in one epoch\n\n        Returns\n        -------\n        float\n            the adjusted learning rate\n        """"""\n        new_lr = self._calc_learning_rate(epoch, batch, nBatch)\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = new_lr\n        return new_lr\n\n    def _train(self):\n        """"""\n        Train the model, it trains model weights and architecute weights.\n        Architecture weights are trained according to the schedule.\n        Before updating architecture weights, ```requires_grad``` is enabled.\n        Then, it is disabled after the updating, in order not to update\n        architecture weights when training model weights.\n        """"""\n        nBatch = len(self.train_loader)\n        arch_param_num = self.mutator.num_arch_params()\n        binary_gates_num = self.mutator.num_arch_params()\n        logger.info(\'#arch_params: %d\\t#binary_gates: %d\', arch_param_num, binary_gates_num)\n\n        update_schedule = self._get_update_schedule(nBatch)\n\n        for epoch in range(self.train_curr_epoch, self.n_epochs):\n            logger.info(\'\\n--------Train epoch: %d--------\\n\', epoch + 1)\n            batch_time = AverageMeter(\'batch_time\')\n            data_time = AverageMeter(\'data_time\')\n            losses = AverageMeter(\'losses\')\n            top1 = AverageMeter(\'top1\')\n            top5 = AverageMeter(\'top5\')\n            # switch to train mode\n            self.model.train()\n\n            end = time.time()\n            for i, (images, labels) in enumerate(self.train_loader):\n                data_time.update(time.time() - end)\n                lr = self._adjust_learning_rate(self.model_optim, epoch, batch=i, nBatch=nBatch)\n                # train weight parameters\n                images, labels = images.to(self.device), labels.to(self.device)\n                self.mutator.reset_binary_gates()\n                self.mutator.unused_modules_off()\n                output = self.model(images)\n                if self.label_smoothing > 0:\n                    loss = cross_entropy_with_label_smoothing(output, labels, self.label_smoothing)\n                else:\n                    loss = self.criterion(output, labels)\n                acc1, acc5 = accuracy(output, labels, topk=(1, 5))\n                losses.update(loss, images.size(0))\n                top1.update(acc1[0], images.size(0))\n                top5.update(acc5[0], images.size(0))\n                self.model.zero_grad()\n                loss.backward()\n                self.model_optim.step()\n                self.mutator.unused_modules_back()\n                if epoch > 0:\n                    for _ in range(update_schedule.get(i, 0)):\n                        start_time = time.time()\n                        # GradientArchSearchConfig\n                        self.mutator.arch_requires_grad()\n                        arch_loss, exp_value = self._gradient_step()\n                        self.mutator.arch_disable_grad()\n                        used_time = time.time() - start_time\n                        log_str = \'Architecture [%d-%d]\\t Time %.4f\\t Loss %.4f\\t null %s\' % \\\n                                    (epoch + 1, i, used_time, arch_loss, exp_value)\n                        logger.info(log_str)\n                batch_time.update(time.time() - end)\n                end = time.time()\n                # training log\n                if i % 10 == 0 or i + 1 == nBatch:\n                    batch_log = \'Train [{0}][{1}/{2}]\\t\' \\\n                                \'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t\' \\\n                                \'Data Time {data_time.val:.3f} ({data_time.avg:.3f})\\t\' \\\n                                \'Loss {losses.val:.4f} ({losses.avg:.4f})\\t\' \\\n                                \'Top-1 acc {top1.val:.3f} ({top1.avg:.3f})\\t\' \\\n                                \'Top-5 acc {top5.val:.3f} ({top5.avg:.3f})\\tlr {lr:.5f}\'. \\\n                        format(epoch + 1, i, nBatch - 1, batch_time=batch_time, data_time=data_time,\n                               losses=losses, top1=top1, top5=top5, lr=lr)\n                    logger.info(batch_log)\n            # validate\n            if (epoch + 1) % self.arch_valid_frequency == 0:\n                val_loss, val_top1, val_top5 = self._validate()\n                val_log = \'Valid [{0}]\\tloss {1:.3f}\\ttop-1 acc {2:.3f} \\ttop-5 acc {3:.3f}\\t\' \\\n                          \'Train top-1 {top1.avg:.3f}\\ttop-5 {top5.avg:.3f}\'. \\\n                    format(epoch + 1, val_loss, val_top1, val_top5, top1=top1, top5=top5)\n                logger.info(val_log)\n            self.save_checkpoint()\n            self.train_curr_epoch += 1\n\n    def _valid_next_batch(self):\n        """"""\n        Get next one minibatch from validation set\n\n        Returns\n        -------\n        (tensor, tensor)\n            the tuple of images and labels\n        """"""\n        if self._valid_iter is None:\n            self._valid_iter = iter(self.valid_loader)\n        try:\n            data = next(self._valid_iter)\n        except StopIteration:\n            self._valid_iter = iter(self.valid_loader)\n            data = next(self._valid_iter)\n        return data\n\n    def _gradient_step(self):\n        """"""\n        This gradient step is for updating architecture weights.\n        Mutator is intensively used in this function to operate on\n        architecture weights.\n\n        Returns\n        -------\n        float, None\n            loss of the model, None\n        """"""\n        # use the same batch size as train batch size for architecture weights\n        self.valid_loader.batch_sampler.batch_size = self.train_batch_size\n        self.valid_loader.batch_sampler.drop_last = True\n        self.model.train()\n        self.mutator.change_forward_mode(self.binary_mode)\n        time1 = time.time()  # time\n        # sample a batch of data from validation set\n        images, labels = self._valid_next_batch()\n        images, labels = images.to(self.device), labels.to(self.device)\n        time2 = time.time()  # time\n        self.mutator.reset_binary_gates()\n        self.mutator.unused_modules_off()\n        output = self.model(images)\n        time3 = time.time()\n        ce_loss = self.criterion(output, labels)\n        expected_value = None\n        loss = ce_loss\n        self.model.zero_grad()\n        loss.backward()\n        self.mutator.set_arch_param_grad()\n        self.arch_optimizer.step()\n        if self.mutator.get_forward_mode() == \'two\':\n            self.mutator.rescale_updated_arch_param()\n        self.mutator.unused_modules_back()\n        self.mutator.change_forward_mode(None)\n        time4 = time.time()\n        logger.info(\'(%.4f, %.4f, %.4f)\', time2 - time1, time3 - time2, time4 - time3)\n        return loss.data.item(), expected_value.item() if expected_value is not None else None\n\n    def save_checkpoint(self):\n        """"""\n        Save checkpoint of the whole model. Saving model weights and architecture weights in\n        ```ckpt_path```, and saving currently chosen architecture in ```arch_path```.\n        """"""\n        if self.ckpt_path:\n            state = {\n                \'warmup_curr_epoch\': self.warmup_curr_epoch,\n                \'train_curr_epoch\': self.train_curr_epoch,\n                \'model\': self.model.state_dict(),\n                \'optim\': self.model_optim.state_dict(),\n                \'arch_optim\': self.arch_optimizer.state_dict()\n            }\n            torch.save(state, self.ckpt_path)\n        if self.arch_path:\n            self.export(self.arch_path)\n\n    def load_checkpoint(self):\n        """"""\n        Load the checkpoint from ```ckpt_path```.\n        """"""\n        assert self.ckpt_path is not None, ""If load_ckpt is not None, ckpt_path should not be None""\n        ckpt = torch.load(self.ckpt_path)\n        self.warmup_curr_epoch = ckpt[\'warmup_curr_epoch\']\n        self.train_curr_epoch = ckpt[\'train_curr_epoch\']\n        self.model.load_state_dict(ckpt[\'model\'])\n        self.model_optim.load_state_dict(ckpt[\'optim\'])\n        self.arch_optimizer.load_state_dict(ckpt[\'arch_optim\'])\n\n    def train(self):\n        """"""\n        Train the whole model.\n        """"""\n        if self.load_ckpt:\n            self.load_checkpoint()\n        if self.warmup:\n            self._warm_up()\n        self._train()\n\n    def export(self, file_name):\n        """"""\n        Export the chosen architecture into a file\n\n        Parameters\n        ----------\n        file_name : str\n            the file that stores exported chosen architecture\n        """"""\n        exported_arch = self.mutator.sample_final()\n        with open(file_name, \'w\') as f:\n            json.dump(exported_arch, f, indent=2, sort_keys=True, cls=TorchTensorEncoder)\n\n    def validate(self):\n        raise NotImplementedError\n\n    def checkpoint(self):\n        raise NotImplementedError\n'"
src/sdk/pynni/nni/nas/pytorch/proxylessnas/utils.py,4,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport torch\nimport torch.nn as nn\n\ndef detach_variable(inputs):\n    """"""\n    Detach variables\n\n    Parameters\n    ----------\n    inputs : pytorch tensors\n        pytorch tensors\n    """"""\n    if isinstance(inputs, tuple):\n        return tuple([detach_variable(x) for x in inputs])\n    else:\n        x = inputs.detach()\n        x.requires_grad = inputs.requires_grad\n        return x\n\ndef cross_entropy_with_label_smoothing(pred, target, label_smoothing=0.1):\n    """"""\n    Parameters\n    ----------\n    pred : pytorch tensor\n        predicted value\n    target : pytorch tensor\n        label\n    label_smoothing : float\n        the degree of label smoothing\n\n    Returns\n    -------\n    pytorch tensor\n        cross entropy\n    """"""\n    logsoftmax = nn.LogSoftmax()\n    n_classes = pred.size(1)\n    # convert to one-hot\n    target = torch.unsqueeze(target, 1)\n    soft_target = torch.zeros_like(pred)\n    soft_target.scatter_(1, target, 1)\n    # label smoothing\n    soft_target = soft_target * (1 - label_smoothing) + label_smoothing / n_classes\n    return torch.mean(torch.sum(- soft_target * logsoftmax(pred), 1))\n\ndef accuracy(output, target, topk=(1,)):\n    """"""\n    Computes the precision@k for the specified values of k\n\n    Parameters\n    ----------\n    output : pytorch tensor\n        output, e.g., predicted value\n    target : pytorch tensor\n        label\n    topk : tuple\n        specify top1 and top5\n\n    Returns\n    -------\n    list\n        accuracy of top1 and top5\n    """"""\n    maxk = max(topk)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n'"
src/sdk/pynni/nni/nas/pytorch/random/__init__.py,0,b'from .mutator import RandomMutator'
src/sdk/pynni/nni/nas/pytorch/random/mutator.py,7,"b'import torch\nimport torch.nn.functional as F\n\nfrom nni.nas.pytorch.mutator import Mutator\nfrom nni.nas.pytorch.mutables import LayerChoice, InputChoice\n\n\nclass RandomMutator(Mutator):\n    """"""\n    Random mutator that samples a random candidate in the search space each time ``reset()``.\n    It uses random function in PyTorch, so users can set seed in PyTorch to ensure deterministic behavior.\n    """"""\n\n    def sample_search(self):\n        """"""\n        Sample a random candidate.\n        """"""\n        result = dict()\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                gen_index = torch.randint(high=len(mutable), size=(1, ))\n                result[mutable.key] = F.one_hot(gen_index, num_classes=len(mutable)).view(-1).bool()\n            elif isinstance(mutable, InputChoice):\n                if mutable.n_chosen is None:\n                    result[mutable.key] = torch.randint(high=2, size=(mutable.n_candidates,)).view(-1).bool()\n                else:\n                    perm = torch.randperm(mutable.n_candidates)\n                    mask = [i in perm[:mutable.n_chosen] for i in range(mutable.n_candidates)]\n                    result[mutable.key] = torch.tensor(mask, dtype=torch.bool)  # pylint: disable=not-callable\n        return result\n\n    def sample_final(self):\n        """"""\n        Same as :meth:`sample_search`.\n        """"""\n        return self.sample_search()\n'"
src/sdk/pynni/nni/nas/pytorch/spos/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .evolution import SPOSEvolution\nfrom .mutator import SPOSSupernetTrainingMutator\nfrom .trainer import SPOSSupernetTrainer\n'
src/sdk/pynni/nni/nas/pytorch/spos/evolution.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport json\nimport logging\nimport os\nimport re\nfrom collections import deque\n\nimport numpy as np\nfrom nni.tuner import Tuner\nfrom nni.nas.pytorch.classic_nas.mutator import LAYER_CHOICE, INPUT_CHOICE\n\n\n_logger = logging.getLogger(__name__)\n\n\nclass SPOSEvolution(Tuner):\n    """"""\n    SPOS evolution tuner.\n\n    Parameters\n    ----------\n    max_epochs : int\n        Maximum number of epochs to run.\n    num_select : int\n        Number of survival candidates of each epoch.\n    num_population : int\n        Number of candidates at the start of each epoch. If candidates generated by\n        crossover and mutation are not enough, the rest will be filled with random\n        candidates.\n    m_prob : float\n        The probability of mutation.\n    num_crossover : int\n        Number of candidates generated by crossover in each epoch.\n    num_mutation : int\n        Number of candidates generated by mutation in each epoch.\n    """"""\n\n    def __init__(self, max_epochs=20, num_select=10, num_population=50, m_prob=0.1,\n                 num_crossover=25, num_mutation=25):\n        assert num_population >= num_select\n        self.max_epochs = max_epochs\n        self.num_select = num_select\n        self.num_population = num_population\n        self.m_prob = m_prob\n        self.num_crossover = num_crossover\n        self.num_mutation = num_mutation\n        self.epoch = 0\n        self.candidates = []\n        self.search_space = None\n        self.random_state = np.random.RandomState(0)\n\n        # async status\n        self._to_evaluate_queue = deque()\n        self._sending_parameter_queue = deque()\n        self._pending_result_ids = set()\n        self._reward_dict = dict()\n        self._id2candidate = dict()\n        self._st_callback = None\n\n    def update_search_space(self, search_space):\n        """"""\n        Handle the initialization/update event of search space.\n        """"""\n        self._search_space = search_space\n        self._next_round()\n\n    def _next_round(self):\n        _logger.info(""Epoch %d, generating..."", self.epoch)\n        if self.epoch == 0:\n            self._get_random_population()\n            self.export_results(self.candidates)\n        else:\n            best_candidates = self._select_top_candidates()\n            self.export_results(best_candidates)\n            if self.epoch >= self.max_epochs:\n                return\n            self.candidates = self._get_mutation(best_candidates) + self._get_crossover(best_candidates)\n            self._get_random_population()\n        self.epoch += 1\n\n    def _random_candidate(self):\n        chosen_arch = dict()\n        for key, val in self._search_space.items():\n            if val[""_type""] == LAYER_CHOICE:\n                choices = val[""_value""]\n                index = self.random_state.randint(len(choices))\n                chosen_arch[key] = {""_value"": choices[index], ""_idx"": index}\n            elif val[""_type""] == INPUT_CHOICE:\n                raise NotImplementedError(""Input choice is not implemented yet."")\n        return chosen_arch\n\n    def _add_to_evaluate_queue(self, cand):\n        _logger.info(""Generate candidate %s, adding to eval queue."", self._get_architecture_repr(cand))\n        self._reward_dict[self._hashcode(cand)] = 0.\n        self._to_evaluate_queue.append(cand)\n\n    def _get_random_population(self):\n        while len(self.candidates) < self.num_population:\n            cand = self._random_candidate()\n            if self._is_legal(cand):\n                _logger.info(""Random candidate generated."")\n                self._add_to_evaluate_queue(cand)\n                self.candidates.append(cand)\n\n    def _get_crossover(self, best):\n        result = []\n        for _ in range(10 * self.num_crossover):\n            cand_p1 = best[self.random_state.randint(len(best))]\n            cand_p2 = best[self.random_state.randint(len(best))]\n            assert cand_p1.keys() == cand_p2.keys()\n            cand = {k: cand_p1[k] if self.random_state.randint(2) == 0 else cand_p2[k]\n                    for k in cand_p1.keys()}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_crossover:\n                break\n        _logger.info(""Found %d architectures with crossover."", len(result))\n        return result\n\n    def _get_mutation(self, best):\n        result = []\n        for _ in range(10 * self.num_mutation):\n            cand = best[self.random_state.randint(len(best))].copy()\n            mutation_sample = np.random.random_sample(len(cand))\n            for s, k in zip(mutation_sample, cand):\n                if s < self.m_prob:\n                    choices = self._search_space[k][""_value""]\n                    index = self.random_state.randint(len(choices))\n                    cand[k] = {""_value"": choices[index], ""_idx"": index}\n            if self._is_legal(cand):\n                result.append(cand)\n                self._add_to_evaluate_queue(cand)\n            if len(result) >= self.num_mutation:\n                break\n        _logger.info(""Found %d architectures with mutation."", len(result))\n        return result\n\n    def _get_architecture_repr(self, cand):\n        return re.sub(r""\\"".*?\\"": \\{\\""_idx\\"": (\\d+), \\""_value\\"": \\"".*?\\""\\}"", r""\\1"",\n                      self._hashcode(cand))\n\n    def _is_legal(self, cand):\n        if self._hashcode(cand) in self._reward_dict:\n            return False\n        return True\n\n    def _select_top_candidates(self):\n        reward_query = lambda cand: self._reward_dict[self._hashcode(cand)]\n        _logger.info(""All candidate rewards: %s"", list(map(reward_query, self.candidates)))\n        result = sorted(self.candidates, key=reward_query, reverse=True)[:self.num_select]\n        _logger.info(""Best candidate rewards: %s"", list(map(reward_query, result)))\n        return result\n\n    @staticmethod\n    def _hashcode(d):\n        return json.dumps(d, sort_keys=True)\n\n    def _bind_and_send_parameters(self):\n        """"""\n        There are two types of resources: parameter ids and candidates. This function is called at\n        necessary times to bind these resources to send new trials with st_callback.\n        """"""\n        result = []\n        while self._sending_parameter_queue and self._to_evaluate_queue:\n            parameter_id = self._sending_parameter_queue.popleft()\n            parameters = self._to_evaluate_queue.popleft()\n            self._id2candidate[parameter_id] = parameters\n            result.append(parameters)\n            self._pending_result_ids.add(parameter_id)\n            self._st_callback(parameter_id, parameters)\n            _logger.info(""Send parameter [%d] %s."", parameter_id, self._get_architecture_repr(parameters))\n        return result\n\n    def generate_multiple_parameters(self, parameter_id_list, **kwargs):\n        """"""\n        Callback function necessary to implement a tuner. This will put more parameter ids into the\n        parameter id queue.\n        """"""\n        if ""st_callback"" in kwargs and self._st_callback is None:\n            self._st_callback = kwargs[""st_callback""]\n        for parameter_id in parameter_id_list:\n            self._sending_parameter_queue.append(parameter_id)\n        self._bind_and_send_parameters()\n        return []  # always not use this. might induce problem of over-sending\n\n    def receive_trial_result(self, parameter_id, parameters, value, **kwargs):\n        """"""\n        Callback function. Receive a trial result.\n        """"""\n        _logger.info(""Candidate %d, reported reward %f"", parameter_id, value)\n        self._reward_dict[self._hashcode(self._id2candidate[parameter_id])] = value\n\n    def trial_end(self, parameter_id, success, **kwargs):\n        """"""\n        Callback function when a trial is ended and resource is released.\n        """"""\n        self._pending_result_ids.remove(parameter_id)\n        if not self._pending_result_ids and not self._to_evaluate_queue:\n            # a new epoch now\n            self._next_round()\n            assert self._st_callback is not None\n            self._bind_and_send_parameters()\n\n    def export_results(self, result):\n        """"""\n        Export a number of candidates to `checkpoints` dir.\n\n        Parameters\n        ----------\n        result : dict\n            Chosen architectures to be exported.\n        """"""\n        os.makedirs(""checkpoints"", exist_ok=True)\n        for i, cand in enumerate(result):\n            converted = dict()\n            for cand_key, cand_val in cand.items():\n                onehot = [k == cand_val[""_idx""] for k in range(len(self._search_space[cand_key][""_value""]))]\n                converted[cand_key] = onehot\n            with open(os.path.join(""checkpoints"", ""%03d_%03d.json"" % (self.epoch, i)), ""w"") as fp:\n                json.dump(converted, fp)\n'"
src/sdk/pynni/nni/nas/pytorch/spos/mutator.py,1,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport numpy as np\nfrom nni.nas.pytorch.random import RandomMutator\n\n_logger = logging.getLogger(__name__)\n\n\nclass SPOSSupernetTrainingMutator(RandomMutator):\n    """"""\n    A random mutator with flops limit.\n\n    Parameters\n    ----------\n    model : nn.Module\n        PyTorch model.\n    flops_func : callable\n        Callable that takes a candidate from `sample_search` and returns its candidate. When `flops_func`\n        is None, functions related to flops will be deactivated.\n    flops_lb : number\n        Lower bound of flops.\n    flops_ub : number\n        Upper bound of flops.\n    flops_bin_num : number\n        Number of bins divided for the interval of flops to ensure the uniformity. Bigger number will be more\n        uniform, but the sampling will be slower.\n    flops_sample_timeout : int\n        Maximum number of attempts to sample before giving up and use a random candidate.\n    """"""\n    def __init__(self, model, flops_func=None, flops_lb=None, flops_ub=None,\n                 flops_bin_num=7, flops_sample_timeout=500):\n\n        super().__init__(model)\n        self._flops_func = flops_func\n        if self._flops_func is not None:\n            self._flops_bin_num = flops_bin_num\n            self._flops_bins = [flops_lb + (flops_ub - flops_lb) / flops_bin_num * i for i in range(flops_bin_num + 1)]\n            self._flops_sample_timeout = flops_sample_timeout\n\n    def sample_search(self):\n        """"""\n        Sample a candidate for training. When `flops_func` is not None, candidates will be sampled uniformly\n        relative to flops.\n\n        Returns\n        -------\n        dict\n        """"""\n        if self._flops_func is not None:\n            for times in range(self._flops_sample_timeout):\n                idx = np.random.randint(self._flops_bin_num)\n                cand = super().sample_search()\n                if self._flops_bins[idx] <= self._flops_func(cand) <= self._flops_bins[idx + 1]:\n                    _logger.debug(""Sampled candidate flops %f in %d times."", cand, times)\n                    return cand\n            _logger.warning(""Failed to sample a flops-valid candidate within %d tries."", self._flops_sample_timeout)\n        return super().sample_search()\n\n    def sample_final(self):\n        """"""\n        Implement only to suffice the interface of Mutator.\n        """"""\n        return self.sample_search()\n'"
src/sdk/pynni/nni/nas/pytorch/spos/trainer.py,6,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport torch\nfrom nni.nas.pytorch.trainer import Trainer\nfrom nni.nas.pytorch.utils import AverageMeterGroup\n\nfrom .mutator import SPOSSupernetTrainingMutator\n\nlogger = logging.getLogger(__name__)\n\n\nclass SPOSSupernetTrainer(Trainer):\n    """"""\n    This trainer trains a supernet that can be used for evolution search.\n\n    Parameters\n    ----------\n    model : nn.Module\n        Model with mutables.\n    mutator : Mutator\n        A mutator object that has been initialized with the model.\n    loss : callable\n        Called with logits and targets. Returns a loss tensor.\n    metrics : callable\n        Returns a dict that maps metrics keys to metrics data.\n    optimizer : Optimizer\n        Optimizer that optimizes the model.\n    num_epochs : int\n        Number of epochs of training.\n    train_loader : iterable\n        Data loader of training. Raise ``StopIteration`` when one epoch is exhausted.\n    dataset_valid : iterable\n        Data loader of validation. Raise ``StopIteration`` when one epoch is exhausted.\n    batch_size : int\n        Batch size.\n    workers: int\n        Number of threads for data preprocessing. Not used for this trainer. Maybe removed in future.\n    device : torch.device\n        Device object. Either ``torch.device(""cuda"")`` or ``torch.device(""cpu"")``. When ``None``, trainer will\n        automatic detects GPU and selects GPU first.\n    log_frequency : int\n        Number of mini-batches to log metrics.\n    callbacks : list of Callback\n        Callbacks to plug into the trainer. See Callbacks.\n    """"""\n\n    def __init__(self, model, loss, metrics,\n                 optimizer, num_epochs, train_loader, valid_loader,\n                 mutator=None, batch_size=64, workers=4, device=None, log_frequency=None,\n                 callbacks=None):\n        assert torch.cuda.is_available()\n        super().__init__(model, mutator if mutator is not None else SPOSSupernetTrainingMutator(model),\n                         loss, metrics, optimizer, num_epochs, None, None,\n                         batch_size, workers, device, log_frequency, callbacks)\n\n        self.train_loader = train_loader\n        self.valid_loader = valid_loader\n\n    def train_one_epoch(self, epoch):\n        self.model.train()\n        meters = AverageMeterGroup()\n        for step, (x, y) in enumerate(self.train_loader):\n            self.optimizer.zero_grad()\n            self.mutator.reset()\n            logits = self.model(x)\n            loss = self.loss(logits, y)\n            loss.backward()\n            self.optimizer.step()\n\n            metrics = self.metrics(logits, y)\n            metrics[""loss""] = loss.item()\n            meters.update(metrics)\n            if self.log_frequency is not None and step % self.log_frequency == 0:\n                logger.info(""Epoch [%s/%s] Step [%s/%s]  %s"", epoch + 1,\n                            self.num_epochs, step + 1, len(self.train_loader), meters)\n\n    def validate_one_epoch(self, epoch):\n        self.model.eval()\n        meters = AverageMeterGroup()\n        with torch.no_grad():\n            for step, (x, y) in enumerate(self.valid_loader):\n                self.mutator.reset()\n                logits = self.model(x)\n                loss = self.loss(logits, y)\n                metrics = self.metrics(logits, y)\n                metrics[""loss""] = loss.item()\n                meters.update(metrics)\n                if self.log_frequency is not None and step % self.log_frequency == 0:\n                    logger.info(""Epoch [%s/%s] Validation Step [%s/%s]  %s"", epoch + 1,\n                                self.num_epochs, step + 1, len(self.valid_loader), meters)\n'"
src/sdk/pynni/nni/nas/tensorflow/enas/__init__.py,0,b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nfrom .mutator import EnasMutator\nfrom .trainer import EnasTrainer\n'
src/sdk/pynni/nni/nas/tensorflow/enas/mutator.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Embedding, LSTMCell, RNN\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy, Reduction\n\nfrom nni.nas.tensorflow.mutator import Mutator\nfrom nni.nas.tensorflow.mutables import LayerChoice, InputChoice, MutableScope\n\n\nclass EnasMutator(Mutator):\n    def __init__(self, model,\n                 lstm_size=64,\n                 lstm_num_layers=1,\n                 tanh_constant=1.5,\n                 cell_exit_extra_step=False,\n                 skip_target=0.4,\n                 temperature=None,\n                 branch_bias=0.25,\n                 entropy_reduction=\'sum\'):\n        super().__init__(model)\n        self.tanh_constant = tanh_constant\n        self.temperature = temperature\n        self.cell_exit_extra_step = cell_exit_extra_step\n\n        cells = [LSTMCell(units=lstm_size, use_bias=False) for _ in range(lstm_num_layers)]\n        self.lstm = RNN(cells, stateful=True)\n        self.g_emb = tf.random.normal((1, 1, lstm_size)) * 0.1\n        self.skip_targets = tf.constant([1.0 - skip_target, skip_target])\n\n        self.max_layer_choice = 0\n        self.bias_dict = {}\n        for mutable in self.mutables:\n            if isinstance(mutable, LayerChoice):\n                if self.max_layer_choice == 0:\n                    self.max_layer_choice = len(mutable)\n                assert self.max_layer_choice == len(mutable), \\\n                        ""ENAS mutator requires all layer choice have the same number of candidates.""\n                if \'reduce\' in mutable.key:\n                    bias = []\n                    for choice in mutable.choices:\n                        if \'conv\' in str(type(choice)).lower():\n                            bias.append(branch_bias)\n                        else:\n                            bias.append(-branch_bias)\n                    self.bias_dict[mutable.key] = tf.constant(bias)\n\n        # exposed for trainer\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n        # internal nn layers\n        self.embedding = Embedding(self.max_layer_choice + 1, lstm_size)\n        self.soft = Dense(self.max_layer_choice, use_bias=False)\n        self.attn_anchor = Dense(lstm_size, use_bias=False)\n        self.attn_query = Dense(lstm_size, use_bias=False)\n        self.v_attn = Dense(1, use_bias=False)\n        assert entropy_reduction in [\'sum\', \'mean\'], \'Entropy reduction must be one of sum and mean.\'\n        self.entropy_reduction = tf.reduce_sum if entropy_reduction == \'sum\' else tf.reduce_mean\n        self.cross_entropy_loss = SparseCategoricalCrossentropy(from_logits=True, reduction=Reduction.NONE)\n\n        self._first_sample = True\n\n    def sample_search(self):\n        self._initialize()\n        self._sample(self.mutables)\n        self._first_sample = False\n        return self._choices\n\n    def sample_final(self):\n        return self.sample_search()\n\n    def _sample(self, tree):\n        mutable = tree.mutable\n        if isinstance(mutable, LayerChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_layer_choice(mutable)\n        elif isinstance(mutable, InputChoice) and mutable.key not in self._choices:\n            self._choices[mutable.key] = self._sample_input_choice(mutable)\n        for child in tree.children:\n            self._sample(child)\n        if self.cell_exit_extra_step and isinstance(mutable, MutableScope) and mutable.key not in self._anchors_hid:\n            self._anchors_hid[mutable.key] = self.lstm(self._inputs, 1)\n\n    def _initialize(self):\n        self._choices = {}\n        self._anchors_hid = {}\n        self._inputs = self.g_emb\n        # seems the `input_shape` parameter of RNN does not work\n        # workaround it by omitting `reset_states` for first run\n        if not self._first_sample:\n            self.lstm.reset_states()\n        self.sample_log_prob = 0\n        self.sample_entropy = 0\n        self.sample_skip_penalty = 0\n\n    def _sample_layer_choice(self, mutable):\n        logit = self.soft(self.lstm(self._inputs))\n        if self.temperature is not None:\n            logit /= self.temperature\n        if self.tanh_constant is not None:\n            logit = self.tanh_constant * tf.tanh(logit)\n        if mutable.key in self.bias_dict:\n            logit += self.bias_dict[mutable.key]\n        softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n        branch_id = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [1])\n        log_prob = self.cross_entropy_loss(branch_id, logit)\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.math.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        self._inputs = tf.reshape(self.embedding(branch_id), [1, 1, -1])\n        mask = tf.one_hot(branch_id, self.max_layer_choice)\n        return tf.cast(tf.reshape(mask, [-1]), tf.bool)\n\n    def _sample_input_choice(self, mutable):\n        query, anchors = [], []\n        for label in mutable.choose_from:\n            if label not in self._anchors_hid:\n                self._anchors_hid[label] = self.lstm(self._inputs)\n            query.append(self.attn_anchor(self._anchors_hid[label]))\n            anchors.append(self._anchors_hid[label])\n        query = tf.concat(query, axis=0)\n        query = tf.tanh(query + self.attn_query(anchors[-1]))\n        query = self.v_attn(query)\n\n        if self.temperature is not None:\n            query /= self.temperature\n        if self.tanh_constant is not None:\n            query = self.tanh_constant * tf.tanh(query)\n\n        if mutable.n_chosen is None:\n            logit = tf.concat([-query, query], axis=1)\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            skip = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip_prob = tf.math.sigmoid(logit)\n            kl = tf.reduce_sum(skip_prob * tf.math.log(skip_prob / self.skip_targets))\n            self.sample_skip_penalty += kl\n            log_prob = self.cross_entropy_loss(skip, logit)\n\n            skip = tf.cast(skip, tf.float32)\n            inputs = tf.tensordot(skip, tf.concat(anchors, 0), 1) / (1. + tf.reduce_sum(skip))\n            self._inputs = tf.reshape(inputs, [1, 1, -1])\n\n        else:\n            assert mutable.n_chosen == 1, ""Input choice must select exactly one or any in ENAS.""\n            logit = tf.reshape(query, [1, -1])\n            softmax_logit = tf.math.log(tf.nn.softmax(logit, axis=-1))\n            index = tf.reshape(tf.random.categorical(softmax_logit, num_samples=1), [-1])\n            skip = tf.reshape(tf.one_hot(index, mutable.n_candidates), [-1])\n            # when the size is 1, tf does not accept tensor here, complaining the shape is wrong\n            # but using a numpy array seems fine\n            log_prob = self.cross_entropy_loss(logit, query.numpy())\n            self._inputs = tf.reshape(anchors[index.numpy()[0]], [1, 1, -1])\n\n        self.sample_log_prob += self.entropy_reduction(log_prob)\n        entropy = log_prob * tf.exp(-log_prob)\n        self.sample_entropy += self.entropy_reduction(entropy)\n        assert len(skip) == mutable.n_candidates, (skip, mutable.n_candidates, mutable.n_chosen)\n        return tf.cast(skip, tf.bool)\n'"
src/sdk/pynni/nni/nas/tensorflow/enas/trainer.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT license.\n\nimport logging\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n\nfrom nni.nas.tensorflow.utils import AverageMeterGroup, fill_zero_grads\n\nfrom .mutator import EnasMutator\n\nlogger = logging.getLogger(__name__)\n\n\nlog_frequency = 100\nentropy_weight = 0.0001\nskip_weight = 0.8\nbaseline_decay = 0.999\nchild_steps = 500\nmutator_lr = 0.00035\nmutator_steps = 50\nmutator_steps_aggregate = 20\naux_weight = 0.4\ntest_arc_per_epoch = 1\n\n\nclass EnasTrainer:\n    def __init__(self, model, loss, metrics, reward_function, optimizer, batch_size, num_epochs,\n                 dataset_train, dataset_valid):\n        self.model = model\n        self.loss = loss\n        self.metrics = metrics\n        self.reward_function = reward_function\n        self.optimizer = optimizer\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n\n        x, y = dataset_train\n        split = int(len(x) * 0.9)\n        self.train_set = tf.data.Dataset.from_tensor_slices((x[:split], y[:split]))\n        self.valid_set = tf.data.Dataset.from_tensor_slices((x[split:], y[split:]))\n        self.test_set = tf.data.Dataset.from_tensor_slices(dataset_valid)\n\n        self.mutator = EnasMutator(model)\n        self.mutator_optim = Adam(learning_rate=mutator_lr)\n\n        self.baseline = 0.\n\n\n    def train(self, validate=True):\n        for epoch in range(self.num_epochs):\n            logger.info(""Epoch %d Training"", epoch + 1)\n            self.train_one_epoch(epoch)\n            logger.info(""Epoch %d Validating"", epoch + 1)\n            self.validate_one_epoch(epoch)\n\n    def validate(self):\n        self.validate_one_epoch(-1)\n\n\n    def train_one_epoch(self, epoch):\n        train_loader, valid_loader = self._create_train_loader()\n\n        # Sample model and train\n        meters = AverageMeterGroup()\n\n        for step in range(1, child_steps + 1):\n            x, y = next(train_loader)\n            self.mutator.reset()\n\n            with tf.GradientTape() as tape:\n                logits = self.model(x, training=True)\n                if isinstance(logits, tuple):\n                    logits, aux_logits = logits\n                    aux_loss = self.loss(aux_logits, y)\n                else:\n                    aux_loss = 0.\n                metrics = self.metrics(y, logits)\n                loss = self.loss(y, logits) + aux_weight * aux_loss\n\n            grads = tape.gradient(loss, self.model.trainable_weights)\n            grads = fill_zero_grads(grads, self.model.trainable_weights)\n            grads, _ = tf.clip_by_global_norm(grads, 5.0)\n            self.optimizer.apply_gradients(zip(grads, self.model.trainable_weights))\n\n            metrics[\'loss\'] = tf.reduce_mean(loss).numpy()\n            meters.update(metrics)\n\n            if log_frequency and step % log_frequency == 0:\n                logger.info(""Model Epoch [%d/%d] Step [%d/%d]  %s"", epoch + 1,\n                            self.num_epochs, step, child_steps, meters)\n\n        # Train sampler (mutator)\n        meters = AverageMeterGroup()\n        for mutator_step in range(1, mutator_steps + 1):\n            grads_list = []\n            for step in range(1, mutator_steps_aggregate + 1):\n                with tf.GradientTape() as tape:\n                    x, y = next(valid_loader)\n                    self.mutator.reset()\n\n                    logits = self.model(x, training=False)\n                    metrics = self.metrics(y, logits)\n                    reward = self.reward_function(y, logits) + entropy_weight * self.mutator.sample_entropy\n                    self.baseline = self.baseline * baseline_decay + reward * (1 - baseline_decay)\n                    loss = self.mutator.sample_log_prob * (reward - self.baseline)\n                    loss += skip_weight * self.mutator.sample_skip_penalty\n\n                    meters.update({\n                        \'reward\': reward,\n                        \'loss\': tf.reduce_mean(loss).numpy(),\n                        \'ent\': self.mutator.sample_entropy.numpy(),\n                        \'log_prob\': self.mutator.sample_log_prob.numpy(),\n                        \'baseline\': self.baseline,\n                        \'skip\': self.mutator.sample_skip_penalty,\n                    })\n\n                    cur_step = step + (mutator_step - 1) * mutator_steps_aggregate\n                    if log_frequency and cur_step % log_frequency == 0:\n                        logger.info(""RL Epoch [%d/%d] Step [%d/%d] [%d/%d]  %s"", epoch + 1, self.num_epochs,\n                                    mutator_step, mutator_steps, step, mutator_steps_aggregate,\n                                    meters)\n\n                grads = tape.gradient(loss, self.mutator.trainable_weights)\n                grads = fill_zero_grads(grads, self.mutator.trainable_weights)\n                grads_list.append(grads)\n            total_grads = [tf.math.add_n(weight_grads) for weight_grads in zip(*grads_list)]\n            total_grads, _ = tf.clip_by_global_norm(total_grads, 5.0)\n            self.mutator_optim.apply_gradients(zip(total_grads, self.mutator.trainable_weights))\n\n    def validate_one_epoch(self, epoch):\n        test_loader = self._create_validate_loader()\n\n        for arc_id in range(test_arc_per_epoch):\n            meters = AverageMeterGroup()\n            for x, y in test_loader:\n                self.mutator.reset()\n                logits = self.model(x)\n                if isinstance(logits, tuple):\n                    logits, _ = logits\n                metrics = self.metrics(logits, y)\n                loss = self.loss(y, logits)\n                metrics[\'loss\'] = tf.reduce_mean(loss).numpy()\n                meters.update(metrics)\n\n            logger.info(""Test Epoch [%d/%d] Arc [%d/%d] Summary  %s"",\n                        epoch + 1, self.num_epochs, arc_id + 1, test_arc_per_epoch,\n                        meters.summary())\n\n\n    def _create_train_loader(self):\n        train_set = self.train_set.shuffle(1000000).repeat().batch(self.batch_size)\n        test_set = self.test_set.shuffle(1000000).repeat().batch(self.batch_size)\n        return iter(train_set), iter(test_set)\n\n    def _create_validate_loader(self):\n        return iter(self.test_set.shuffle(1000000).repeat().batch(self.batch_size))\n'"
