file_path,api_count,code
paictl.py,0,"b'#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\n\nimport time\nimport os\nimport sys\nimport argparse\nimport logging\nimport logging.config\n\nfrom deployment.paiLibrary.common import file_handler\n\n\nfrom deployment.checkCmd import CheckCmd\nfrom deployment.configCmd import ConfigCmd\nfrom deployment.layoutCmd import LayoutCmd\nfrom deployment.clusterCmd import ClusterCmd\nfrom deployment.serviceCmd import ServiceCmd\nfrom deployment.machineCmd import MachineCmd\nfrom deployment.utilityCmd import UtilityCmd\n\nlogger = logging.getLogger(__name__)\n\n\ndef setup_logging():\n    """"""\n    Setup logging configuration.\n    """"""\n    configuration_path = ""deployment/sysconf/logging.yaml""\n    logging_configuration = file_handler.load_yaml_config(configuration_path)\n    logging.config.dictConfig(logging_configuration)\n\n\ndef main(args):\n    parser = argparse.ArgumentParser()\n    sub_parser = parser.add_subparsers(help=""paictl operations"")\n\n    # create the parser for ""layout"" command\n    layout_parser = sub_parser.add_parser(""layout"", help=""layout tools"")\n    layout_cmd = LayoutCmd()\n    layout_cmd.register(layout_parser)\n\n    # create the parser for ""check"" command\n    check_parser = sub_parser.add_parser(""check"", help=""check PAI status"")\n    check_cmd = CheckCmd()\n    check_cmd.register(check_parser)\n\n    # create the parser for ""config"" command\n    config_parser = sub_parser.add_parser(""config"")\n    config_cmd = ConfigCmd()\n    config_cmd.register(config_parser)\n\n    # create the parser for ""cluster"" command\n    cluster_parser = sub_parser.add_parser(""cluster"")\n    cluster_cmd = ClusterCmd()\n    cluster_cmd.register(cluster_parser)\n\n    # create the parser for ""service"" command\n    service_parser = sub_parser.add_parser(""service"")\n    service_config = ServiceCmd()\n    service_config.register(service_parser)\n\n    # create the parser for ""machine"" command\n    machine_parser = sub_parser.add_parser(""machine"")\n    machine_cmd = MachineCmd()\n    machine_cmd.register(machine_parser)\n\n    # create the parser for ""utility"" command\n    utility_parser = sub_parser.add_parser(""utility"")\n    utility_cmd = UtilityCmd()\n    utility_cmd.register(utility_parser)\n\n    # execute the command: call the ""handler"" with parsed arguments\n    parserd = parser.parse_args(args)\n    parserd.handler(parserd)\n\n\nif __name__ == ""__main__"":\n    script_dir = os.path.dirname(os.path.realpath(__file__))\n    os.chdir(script_dir)\n\n    setup_logging()\n    main(sys.argv[1:])\n\n'"
build/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
build/pai_build.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom core import build_center\nfrom core import build_utility\nfrom model import config_model\n\n\nimport os\nimport sys\nimport argparse\nimport datetime\nimport logging\nimport logging.config\n\nlogger = logging.getLogger(__name__)\n\ndef load_build_config(config_dir):\n    buildConfig = config_model.ConfigModel(config_dir)\n    configModel = buildConfig.build_config_parse()\n    return configModel\n\ndef build_service(args,config_model):\n    pai_build = build_center.BuildCenter(config_model, args.service, \'k8s\')\n    pai_build.build_center()\n\ndef push_image(args,config_model):\n    pai_push = build_center.BuildCenter(config_model, args.imagelist, \'k8s\')\n    pai_push.push_center()\n\ndef main():\n\n    # Define execution path to root folder\n    scriptFolder=os.path.dirname(os.path.realpath(__file__))\n    os.chdir(os.path.dirname(scriptFolder))\n\n    starttime = datetime.datetime.now()\n    parser = argparse.ArgumentParser(description=""pai build client"")\n    logger.info(""Pai build starts at {0}"".format(starttime))\n\n    subparsers = parser.add_subparsers(help=\'build service cli\')\n\n    # Build commands\n    build_parser = subparsers.add_parser(\'build\',help=\'build service cli\')\n    build_parser.add_argument(\n        \'-c\', \'--config\',\n        type=bytes,\n        required=True,\n        help=\'The path of your configuration directory.\'\n    )\n    build_parser.add_argument(\n        \'-s\', \'--service\',\n        type=bytes,\n        nargs=\'+\',\n        help=""The service list you want to build""\n    )\n    build_parser.set_defaults(func = build_service)\n\n    # Push commands\n    push_parser = subparsers.add_parser(\'push\',help=\'push image cli\')\n    push_parser.add_argument(\n        \'-c\', \'--config\',\n        type=bytes,\n        required=True,\n        help=\'The path of your configuration directory.\'\n    )\n    push_parser.add_argument(\n        \'-i\', \'--imagelist\',\n        type=bytes,\n        nargs=\'+\',\n        help=""The image list you want to push""\n    )\n    push_parser.set_defaults(func = push_image)\n\n    args = parser.parse_args()\n    config_model = load_build_config(args.config)\n    args.func(args, config_model)\n\n    endtime = datetime.datetime.now()\n    logger.info(""Pai build ends at {0}"".format(endtime))\n    logger.info(""Pai build costs {0}"".format(endtime - starttime))\n\nif __name__ == ""__main__"":\n    build_utility.setup_logger_config(logger)\n    main()\n'"
deployment/__init__.py,0,b''
deployment/checkCmd.py,0,"b'from clusterObjectModel.cluster_object_model import cluster_object_model\nimport os\nimport logging\nimport logging.config\nimport yaml\nimport re\nfrom k8sPaiLibrary.maintainlib import common as pai_common\n\nlogger = logging.getLogger(__name__)\n\n\ndef validate_config(config_path):\n    layout_file = os.path.join(config_path, ""layout.yaml"")\n    old_cluster_config_file = os.path.join(config_path, ""cluster-configuration.yaml"")\n    if(os.path.exists(old_cluster_config_file) and not os.path.exists(layout_file)):\n        logger.error(""[Error] - Please upgrade config files!"")\n        exit(-1)\n\n    # kubernetes config\n    my_cluster_object_model = cluster_object_model(config_path)\n    kubernetes_config = my_cluster_object_model.kubernetes_config()\n    logger.info(""[OK] vaildate kubernetes config."")\n\n    # pai service config\n    service_config = my_cluster_object_model.service_config()\n    logger.info(""[OK] vaildate PAI services config."")\n\n\ndef validata_node_os(config_path):\n    layout_file = os.path.join(config_path, ""layout.yaml"")\n    hosts = yaml.load(open(layout_file), yaml.SafeLoader)[""machine-list""]\n    print hosts\n\n    host_not_match = []\n    regex = r""Description:\\s+Ubuntu\\s+16.04(.\\d)?\\s+LTS""\n    for host in hosts:\n        logger.info(""Check os for %s ..."", host[""hostip""])\n        result_stdout, result_stderr = pai_common.ssh_shell_paramiko_with_result(\n            host,\n            ""sudo lsb_release -d"")\n        logger.info(result_stdout)\n        # should match  Ubuntu 16.04.5 LTS\n        if re.match(regex, result_stdout) is None:\n            host_not_match.append(host[""hostip""])\n\n    if len(host_not_match) > 0:\n        logger.error(""[Error - unsupported OS] - OpenPAI only supports ubuntu 16.04, the following node(s) are not ubuntu 16.04: %s"", host_not_match)\n        exit(-1)\n\n\nclass CheckCmd():\n    def register(self, check_parser):\n        #check_parser.add_argument(""--pre"", dest=""pre"", type=bool, default=False, help=""Precheck. Check the prerequisites, and valid the configuration."")\n        check_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""path of cluster configuration file"")\n        check_parser.set_defaults(handler=self.check)\n\n    def check(self, args):\n        # validate config\n\n        logger.info(""Begin to check config files. "")\n        config_path = os.path.expanduser(args.config_path)\n        logger.info(""Config files directory: %s"", config_path)\n\n        validate_config(config_path)\n        validata_node_os(config_path)\n'"
deployment/clusterCmd.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport time\nimport readline\nimport logging\nimport logging.config\n\nfrom k8sPaiLibrary.maintainlib import add as k8s_add\nfrom k8sPaiLibrary.maintainlib import remove as k8s_remove\nfrom k8sPaiLibrary.maintainlib import etcdfix as k8s_etcd_fix\nfrom k8sPaiLibrary.maintainlib import kubectl_conf_check\nfrom k8sPaiLibrary.maintainlib import kubectl_install\nfrom k8sPaiLibrary.maintainlib import update as k8s_update\nfrom k8sPaiLibrary.maintainlib import k8s_util\n\nfrom clusterObjectModel.cluster_object_model import cluster_object_model\n\nlogger = logging.getLogger(__name__)\n\n\nclass ClusterCmd():\n    def register(self, parser):\n        cluster_parser = parser.add_subparsers(help=""cluster operations"")\n\n        # ./paictl.py cluster k8s-bootup ...\n        bootup_parser = cluster_parser.add_parser(""k8s-bootup"")\n        bootup_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""path of cluster configuration file"")\n        bootup_parser.set_defaults(handler=self.k8s_bootup)\n\n        # ./paictl.py cluster k8s-clean ...\n        clean_parser = cluster_parser.add_parser(""k8s-clean"")\n        clean_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""path of cluster configuration file"")\n        clean_parser.add_argument(""-f"", ""--force"", dest=""force"", required=False, action=""store_true"", help=""clean all the data forcefully"")\n        clean_parser.set_defaults(handler=self.k8s_clean)\n\n        # ./paictl.py cluster k8s-set-env ...\n        env_parser = cluster_parser.add_parser(""k8s-set-env"")\n        env_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", help=""path of cluster configuration file"")\n        env_parser.set_defaults(handler=self.k8s_set_environment)\n\n    def prompt_deprecated(self):\n        logger.warning(""Kubernetes deployment in paictl will be deprecated in the future!"")\n        logger.warning(""We highly recommend deploying kubernetes with kubespray."")\n        logger.warning(\n            ""If you wanna deploy k8s with kubespray, please refer to https://github.com/microsoft/pai/tree/master/contrib/kubespray"")\n        count_input_deprecated = 0\n\n        while True:\n            user_input_deprecated = raw_input(""Do you want to continue this operation? (Y/N) "")\n            if user_input_deprecated == ""N"":\n                sys.exit(0)\n            elif user_input_deprecated == ""Y"":\n                break\n            else:\n                print("" Please type Y or N."")\n            count_input_deprecated = count_input_deprecated + 1\n            if count_input_deprecated == 3:\n                logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                sys.exit(0)\n\n    def k8s_bootup(self, args):\n        self.prompt_deprecated()\n        cluster_object_model_instance = cluster_object_model(args.config_path)\n        com = cluster_object_model_instance.kubernetes_config()\n        logger.info(""Begin to initialize PAI k8s cluster."")\n        k8s_util.maintain_cluster_k8s(com, option_name=""deploy"", clean=True)\n        logger.info(""Finish initializing PAI k8s cluster."")\n\n    def k8s_clean(self, args):\n        self.prompt_deprecated()\n        # just use \'k8s-clean\' for testing temporarily.\n        cluster_object_model_instance = cluster_object_model(args.config_path)\n        com = cluster_object_model_instance.kubernetes_config()\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""----------     Dangerous Operation!!!    ---------------"")\n        logger.warning(""------     Your k8s Cluster will be destroyed    -------"")\n        logger.warning(""------     PAI service on k8s will be stopped    -------"")\n        logger.warning(""--------------------------------------------------------"")\n        if args.force:\n            logger.warning(""--------------------------------------------------------"")\n            logger.warning(""----------    ETCD data will be cleaned.    ------------"")\n            logger.warning(""-----    If you wanna keep pai\'s user data.    ---------"")\n            logger.warning(""-----         Please backup etcd data.         ---------"")\n            logger.warning(""-----      And restore it after k8s-bootup     ---------"")\n            logger.warning(""---     And restore it before deploy pai service    ----"")\n            logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""----    Please ensure you wanna do this operator, ------"")\n        logger.warning(""-------        after knowing all risk above.     -------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n\n        count_input = 0\n\n        while True:\n            user_input = raw_input(""Do you want to continue this operation? (Y/N) "")\n            if user_input == ""N"":\n                return\n            elif user_input == ""Y"":\n                break\n            else:\n                print("" Please type Y or N."")\n            count_input = count_input + 1\n            if count_input == 3:\n                logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                return\n\n        logger.info(""Begin to clean up whole cluster."")\n        k8s_util.maintain_cluster_k8s(com, option_name=""clean"", force=args.force, clean=True)\n        logger.info(""Clean up job finished"")\n\n    def k8s_set_environment(self, args):\n        self.prompt_deprecated()\n        if args.config_path != None:\n            args.config_path = os.path.expanduser(args.config_path)\n            cluster_object_model_instance = cluster_object_model(args.config_path)\n            com = cluster_object_model_instance.kubernetes_config()\n        else:\n            com = None\n        kubectl_install_worker = kubectl_install.kubectl_install(com)\n        kubectl_install_worker.run()\n'"
deployment/configCmd.py,0,"b'import os\nimport argparse\nfrom deployment.confStorage.download import download_configuration\nfrom deployment.confStorage.synchronization import synchronization\nfrom deployment.confStorage.external_version_control.external_config import uploading_external_config\nfrom deployment.confStorage.get_cluster_id import get_cluster_id\nfrom utility import pai_version\n\nimport logging\nimport logging.config\nimport importlib\nfrom paiLibrary.common import file_handler\nfrom paiLibrary.common import template_handler\nfrom k8sPaiLibrary.maintainlib import common as pai_common\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef generate_configuration(quick_start_config_file, configuration_directory, force):\n    """"""Automatically generate the following configuration files from a quick-start file:\n        * Machine-level configurations: laylout.yaml\n        * Kubernetes-level configurations I: kubernetes-configuration.yaml\n        * Kubernetes-level configurations II: k8s-role-definition.yaml\n        * Service-level configurations: service-configuration.yaml\n    """"""\n    quick_start_config_raw = file_handler.load_yaml_config(quick_start_config_file)\n\n    #\n    # Prepare machine list\n    machine_list = []\n    for m in quick_start_config_raw[""machines""]:\n        machine = {""hostip"": m, ""machine-type"": ""GENERIC""}\n        # TODO on premise, using ip as ""nodename""\n        machine[""nodename""] = m\n        machine[""docker-data""] = ""/var/lib/docker""\n        machine[""username""] = quick_start_config_raw[""ssh-username""]\n        if ""ssh-password"" in quick_start_config_raw:\n            machine[""password""] = quick_start_config_raw[""ssh-password""]\n        else:\n            machine[""ssh-keyfile-path""] = quick_start_config_raw[""ssh-keyfile-path""]\n            machine[""ssh-secret-name""] = quick_start_config_raw[""ssh-secret-name""]\n        machine[""ssh-port""] = 22 if ""ssh-port"" not in quick_start_config_raw else quick_start_config_raw[""ssh-port""]\n\n        machine_list.append(machine)\n\n    # workers\n    worker_noders = machine_list[1:] if len(machine_list) > 1 else machine_list\n    for machine in worker_noders:\n        # k8s attributes\n        machine[""k8s-role""] = ""worker""\n        # PAI attributes\n        machine[""pai-worker""] = ""true""\n\n    # master\n    master_node = machine_list[0]\n    # k8s attributes\n    master_node[""k8s-role""] = ""master""\n    master_node[""etcdid""] = ""etcdid1""\n    master_node[""dashboard""] = ""true""\n    # PAI attributes\n    master_node[""pai-master""] = ""true""\n    master_node[""zkid""] = ""1""\n\n    #\n    # Prepare config of cluster IP range.\n    service_cluster_ip_range = \\\n        ""10.254.0.0/16"" if ""service-cluster-ip-range"" not in quick_start_config_raw \\\n        else quick_start_config_raw[""service-cluster-ip-range""]\n    #\n    # Auto-complete missing configuration items: Part 1 -- DNS.\n    if ""dns"" in quick_start_config_raw:\n        dns = quick_start_config_raw[""dns""]\n    else:\n        result_stdout, result_stderr = pai_common.ssh_shell_paramiko_with_result(\n            master_node,\n            ""cat /etc/resolv.conf | grep nameserver | cut -d \' \' -f 2 | head -n 1"")\n        dns = result_stdout.strip()\n    #\n    # Auto-complete missing configuration items: Part 2 -- hostnames.\n    for host_config in machine_list:\n        result_stdout, result_stderr = pai_common.ssh_shell_paramiko_with_result(\n            host_config,\n            ""hostname"")\n        host_config[""hostname""] = result_stdout.strip()\n\n    #\n    # kubernetes info\n    api_servers_url = ""http://{0}:{1}"".format(master_node[""hostip""], 8080)\n    # TODO we some k8s template still using the \'dashboard_host\'\n    dashboard_host = master_node[""hostip""]\n    dashboard_url = ""http://{0}:{1}"".format(master_node[""hostip""], 9090)\n\n    #\n    # Generate configuration files.\n    target_file_names = [\n        ""layout.yaml"",\n        ""kubernetes-configuration.yaml"",\n        ""k8s-role-definition.yaml"",\n        ""services-configuration.yaml""\n    ]\n    for x in target_file_names:\n        target_file_path = os.path.join(configuration_directory, x)\n        if file_handler.file_exist_or_not(target_file_path) and force is False:\n            logger.warning(""File %s exists. Skip."" % (target_file_path))\n            pass\n        else:\n            file_handler.create_folder_if_not_exist(configuration_directory)\n            file_handler.write_generated_file(\n                target_file_path,\n                template_handler.generate_from_template_dict(\n                    file_handler.read_template(""./deployment/quick-start/%s.template"" % (x)),\n                    {""env"":\n                        {\n                            ""machines"": machine_list,\n                            ""dns"": dns,\n                            ""load-balance-ip"": master_node[""hostip""],\n                            ""service-cluster-ip-range"": service_cluster_ip_range,\n                            ""api-servers-url"": api_servers_url,\n                            ""dashboard-host"": dashboard_host,\n                            ""dashboard-url"": dashboard_url,\n                            ""pai-version"": pai_version.paictl_version()\n                        }\n                     }))\n\n\nclass ConfigCmd():\n\n    def register(self, parser):\n        conf_parser = parser.add_subparsers(help=""configuration operations"")\n\n        # ./paictl.py config generate ...\n        generate_parser = conf_parser.add_parser(""generate"", description=""Generate configuration files based on a quick-start yaml file."", formatter_class=argparse.RawDescriptionHelpFormatter)\n        generate_parser.add_argument(""-i"", ""--input"", dest=""quick_start_config_file"", required=True, help=""the path of the quick-start configuration file (yaml format) as the input"")\n        generate_parser.add_argument(""-o"", ""--output"", dest=""configuration_directory"", required=True, help=""the path of the directory the configurations will be generated to"")\n        generate_parser.add_argument(""-f"", ""--force"", dest=""force"", action=""store_true"", default=False,  help=""overwrite existing files"")\n        generate_parser.set_defaults(handler=self.generate_configuration)\n\n        # ./paictl.py config push ...\n        push_parser = conf_parser.add_parser(""push"", description=""Push configuration to kubernetes cluster as configmap."", formatter_class=argparse.RawDescriptionHelpFormatter)\n        mutually_update_option = push_parser.add_mutually_exclusive_group()\n        mutually_update_option.add_argument(""-p"", ""--cluster-conf-path"", dest=""cluster_conf_path"", default=None, help=""the path of directory which stores the cluster configuration."")\n        mutually_update_option.add_argument(""-e"", ""--external-storage-conf-path"", dest=""external_storage_conf_path"",  default=None, help=""the path of external storage configuration."")\n        push_parser.add_argument(""-m"", ""--push-mode"", dest=""push_mode"", default=""all"", choices=[\'all\', \'service\'], help=""the mode to push configuration. service mode won\'t push the k8s related configuration."" )\n        push_parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG file. Default value: ~/.kube/config"")\n        push_parser.set_defaults(handler=self.push_configuration)\n\n        # ./paictl.py config pull ...\n        pull_parser = conf_parser.add_parser(""pull"", description=""Get the configuration stored in the k8s cluster."", formatter_class=argparse.RawDescriptionHelpFormatter)\n        pull_parser.add_argument(""-o"", ""--config-output-path"", dest=""config_output_path"", required=True, help=""the path of the directory to store the configuration downloaded from k8s."")\n        pull_parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG file. Default value: ~/.kube/config"")\n        pull_parser.set_defaults(handler=self.pull_configuration)\n\n        # ./paictl.py config get-id ...\n        get_id_parser = conf_parser.add_parser(""get-id"", description=""Get the cluster-id stored in the k8s cluster."", formatter_class=argparse.RawDescriptionHelpFormatter)\n        get_id_parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG file. Default value: ~/.kube/config"")\n        get_id_parser.set_defaults(handler=self.get_cluster_id)\n\n        # ./paictl.py config external-config-update ...\n        external_config_update_parser = conf_parser.add_parser(""external-config-update"", description=""Update configuration of external storage where you could configure the place to sync the latest cluster configuration"", formatter_class=argparse.RawDescriptionHelpFormatter)\n        external_config_update_parser.add_argument(""-e"", ""--extneral-storage-conf-path"", dest=""external_storage_conf_path"", required=True, help=""the path of external storage configuration."")\n        external_config_update_parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG gile. Default value: ~/.kube/config"")\n        external_config_update_parser.set_defaults(handler=self.update_external_config)\n\n    def generate_configuration(self, args):\n        generate_configuration(\n            args.quick_start_config_file,\n            args.configuration_directory,\n            args.force)\n\n    def push_configuration(self, args):\n        if args.cluster_conf_path != None:\n            args.cluster_conf_path = os.path.expanduser(args.cluster_conf_path)\n        if args.external_storage_conf_path != None:\n            args.external_storage_conf_path = os.path.expanduser(args.external_storage_conf_path)\n        if args.kube_config_path != None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n        push_list = [\n            ""k8s-role-definition.yaml"",\n            ""kubernetes-configuration.yaml"",\n            ""layout.yaml"",\n            ""services-configuration.yaml""\n        ]\n        if args.push_mode == ""service"":\n            push_list = [\n                ""layout.yaml"",\n                ""services-configuration.yaml""\n            ]\n        sync_handler = synchronization(\n            pai_cluster_configuration_path=args.cluster_conf_path,\n            local_conf_path=args.external_storage_conf_path,\n            kube_config_path=args.kube_config_path,\n            config_push_list = push_list\n        )\n        sync_handler.sync_data_from_source()\n\n    def pull_configuration(self, args):\n        if args.config_output_path != None:\n            args.config_output_path = os.path.expanduser(args.config_output_path)\n        if args.kube_config_path != None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n        get_handler = download_configuration(\n            config_output_path=args.config_output_path,\n            kube_config_path=args.kube_config_path\n        )\n        get_handler.run()\n\n    def get_cluster_id(self, args):\n        if args.kube_config_path != None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n        get_id_handler = get_cluster_id(kube_config_path=args.kube_config_path)\n        get_id_handler.run()\n\n    def update_external_config(self, args):\n        if args.kube_config_path != None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n        if args.external_storage_conf_path != None:\n            args.external_storage_conf_path = os.path.expanduser(args.external_storage_conf_path)\n        external_conf_update = uploading_external_config(\n            external_storage_conf_path=args.external_storage_conf_path,\n            kube_config_path=args.kube_config_path\n        )\n        external_conf_update.update_latest_external_configuration()\n'"
deployment/layoutCmd.py,0,"b'import os\nimport yaml\nimport logging\nimport logging.config\n\nfrom kubernetes import client, config\n\nlogger = logging.getLogger(__name__)\n\n\ndef generate_layout(output_file):\n    # init client\n    config.load_kube_config()\n    v1 = client.CoreV1Api()\n\n    # api server url\n    api_servers_url = v1.api_client.configuration.host\n    # generate dashboard-url\n    services = v1.list_service_for_all_namespaces(field_selector=""metadata.name=kubernetes-dashboard"", pretty=False, timeout_seconds=56, watch=False)\n    dashboard_service = services.items[0]\n    dashboard_url = ""http://{0}:80"".format(dashboard_service.spec.cluster_ip)\n\n    # query k8s nodes\n    nodes = v1.list_node(pretty=False, timeout_seconds=56, watch=False)\n    addressesList = map(lambda node: node.status.addresses, nodes.items)\n    machineList = []\n    for addresses in addressesList:\n        machine = dict()\n        machine[\'machine-type\'] = \'GENERIC\'\n        for address in addresses:\n            if address.type == \'InternalIP\':\n                machine[\'hostip\'] = address.address\n            if address.type == \'Hostname\':\n                machine[\'hostname\'] = address.address\n                # TODO nodename == hostname on aks\n                machine[\'nodename\'] = address.address\n        machineList.append(machine)\n    machineList.sort(key=lambda k: k[\'hostname\'])\n\n    # assgin pai-master\n    master = machineList[0]\n    master[\'pai-master\'] = \'true\'\n    master[\'zkid\'] = 1\n\n    # assign pai-workers\n    workers = machineList[1:] if len(machineList) > 1 else machineList\n    for worker in workers:\n        worker[\'pai-worker\'] = \'true\'\n\n    # the default sku\n    machineSku = yaml.load(""""""\nGENERIC:\n    mem: 1\n    gpu:\n        type: generic\n        count: 1\n    cpu:\n        vcore: 1\n    os: ubuntu16.04\n    """""", yaml.SafeLoader)\n\n    layout = {\n        ""kubernetes"": {\n            ""api-servers-url"": api_servers_url,\n            ""dashboard-url"": dashboard_url\n        },\n        ""machine-sku"": machineSku,\n        ""machine-list"": machineList\n    }\n    # print(yaml.dump(layout, default_flow_style=False))\n    with open(output_file, \'w\') as outfile:\n        yaml.dump(layout, outfile, default_flow_style=False)\n\n\nclass LayoutCmd():\n    def register(self, layout_parser):\n        layout_parser.add_argument(""-o"", ""--output"", dest=""output"", default=""/cluster-configuration"", help=""Output directory of layout.yaml"")\n        layout_parser.add_argument(""-f"", ""--force"", dest=""force"", action=""store_true"", default=False, help=""Force to overwrite"")\n        layout_parser.set_defaults(handler=self.generate_layout)\n\n    def generate_layout(self, args):\n        layoutFile = os.path.join(args.output, ""layout.yaml"")\n        logger.info(""Generating:"" + layoutFile)\n        if(os.path.exists(layoutFile)):\n            if(args.force):\n                logger.info(""Will overwrite existing file:"" + layoutFile)\n            else:\n                logger.info(""File existing, please passing \'-f\' to overwrite:"" + layoutFile)\n        generate_layout(layoutFile)\n'"
deployment/machineCmd.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport time\nimport readline\nimport logging\nimport logging.config\n\nfrom k8sPaiLibrary.maintainlib import add as k8s_add\nfrom k8sPaiLibrary.maintainlib import remove as k8s_remove\nfrom k8sPaiLibrary.maintainlib import etcdfix as k8s_etcd_fix\nfrom k8sPaiLibrary.maintainlib import kubectl_conf_check\nfrom k8sPaiLibrary.maintainlib import kubectl_install\nfrom k8sPaiLibrary.maintainlib import update as k8s_update\n\n\nfrom paiLibrary.common import file_handler\nfrom clusterObjectModel.cluster_object_model import cluster_object_model\n\nlogger = logging.getLogger(__name__)\n\n# True : continue\n# False: exit\ndef kubectl_env_checking(cluster_object_mode):\n\n    kubectl_conf_ck_worker = kubectl_conf_check.kubectl_conf_check(cluster_object_mode)\n    if kubectl_conf_ck_worker.check() == False:\n        count_input = 0\n\n        while True:\n            user_input = raw_input(""Do you want to re-install kubectl by paictl? (Y/N) "")\n\n            if user_input == ""N"":\n                count_quit = 0\n                while True:\n                    quit_or_not = raw_input(""Do you want to quit by this operation? (Y/N) "")\n                    if quit_or_not == ""Y"":\n                        return False\n                    elif quit_or_not == ""N"":\n                        return True\n                    else:\n                        print("" Please type Y or N."")\n                    count_quit = count_quit + 1\n                    if count_quit == 3:\n                        logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                        return False\n\n            elif user_input == ""Y"":\n                kubectl_install_worker = kubectl_install.kubectl_install(cluster_object_mode)\n                kubectl_install_worker.run()\n                return True\n\n            else:\n                print("" Please type Y or N."")\n\n            count_input = count_input + 1\n            if count_input == 3:\n                logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                return False\n    return True\n\n\nclass MachineCmd():\n    def register(self, parser):\n        machine_parser = parser.add_subparsers(help=""machine operations"")\n\n        # ./paictl.py machine add ...\n        add_parser = machine_parser.add_parser(""add"")\n        add_parser.set_defaults(handler=self.machine_add)\n\n        # ./paictl.py machine remove ...\n        remove_parser = machine_parser.add_parser(""remove"")\n        remove_parser.set_defaults(handler=self.machine_remove)\n\n        # ./paictl.py machine etcd-fix ...\n        etcd_parser = machine_parser.add_parser(""etcd-fix"")\n        etcd_parser.set_defaults(handler=self.etcd_fix)\n\n        # ./paictl.py machine update ...\n        #update_parser = machine_parser.add_parser(""update"")\n        #update_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", default=None, help=""the path of directory which stores the cluster configuration."")\n        #update_parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG file. Default value: ~/.kube/config"")\n        #update_parser.set_defaults(handler=self.machine_update)\n\n        def add_arguments(parser):\n            parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""The path of your configuration directory."")\n            parser.add_argument(""-l"", ""--node-list"", dest=""node_list"", required=True, help=""The node-list to be operator"")\n\n        add_arguments(add_parser)\n        add_arguments(remove_parser)\n        add_arguments(etcd_parser)\n\n    def process_args(self, args):\n        cluster_object_model_instance = cluster_object_model(args.config_path)\n        com = cluster_object_model_instance.kubernetes_config()\n        node_list = file_handler.load_yaml_config(args.node_list)\n\n        if not kubectl_env_checking(com):\n            raise RuntimeError(""failed to do kubectl checking"")\n\n        for host in node_list[""machine-list""]:\n            if ""nodename"" not in host:\n                host[""nodename""] = host[""hostip""]\n\n        return com, node_list\n\n    def machine_add(self, args):\n        cluster_object_model_k8s, node_list = self.process_args(args)\n\n        for host in node_list[""machine-list""]:\n            add_worker = k8s_add.add(cluster_object_model_k8s, host, True)\n            add_worker.run()\n\n            if host[""k8s-role""] == ""master"":\n                logger.info(""Master Node is added, sleep 60s to wait it ready."")\n                time.sleep(60)\n\n    def machine_remove(self, args):\n        cluster_object_model_k8s, node_list = self.process_args(args)\n\n        for host in node_list[""machine-list""]:\n            add_worker = k8s_remove.remove(cluster_object_model_k8s, host, True)\n            add_worker.run()\n\n            if host[""k8s-role""] == ""master"":\n                logger.info(""master node is removed, sleep 60s for etcd cluster\'s updating"")\n                time.sleep(60)\n\n    def machine_update(self, args):\n        if args.kube_config_path != None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n\n        update_worker = k8s_update.update(kube_config_path=args.kube_config_path)\n        update_worker.run()\n        logger.info(""Congratulations! Machine update is finished."")\n\n    def etcd_fix(self, args):\n        cluster_object_model_k8s, node_list = self.process_args(args)\n\n        if len(node_list[""machine-list""]) > 1:\n            logger.error(""etcd-fix can\'t fix more than one machine everytime. Please fix them one by one!"")\n            sys.exit(1)\n\n        for host in node_list[""machine-list""]:\n            etcd_fix_worker = k8s_etcd_fix.etcdfix(cluster_object_model_k8s, host, True)\n            etcd_fix_worker.run()\n\n        logger.info(""Etcd has been fixed."")\n'"
deployment/serviceCmd.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport readline\nimport logging\nimport logging.config\n\nfrom paiLibrary.paiService import service_management_start\nfrom paiLibrary.paiService import service_management_stop\nfrom paiLibrary.paiService import service_management_delete\nfrom paiLibrary.paiService import service_management_refresh\n\nlogger = logging.getLogger(__name__)\n\n\nclass ServiceCmd():\n    def register(self, parser):\n        service_parser = parser.add_subparsers(help=""service operations"")\n\n        # ./paictl.py service start ...\n        start_parser = service_parser.add_parser(""start"")\n        start_parser.set_defaults(handler=self.service_start)\n\n        # ./paictl.py service stop ...\n        stop_parser = service_parser.add_parser(""stop"")\n        stop_parser.set_defaults(handler=self.service_stop)\n\n        # ./paictl.py service delete ...\n        delete_parser = service_parser.add_parser(""delete"")\n        delete_parser.set_defaults(handler=self.service_delete)\n\n        # ./paictl.py service refresh\n        refresh_parser = service_parser.add_parser(""refresh"")\n        refresh_parser.set_defaults(handler=self.service_refresh)\n\n        def add_arguments(parser):\n            parser.add_argument(""-c"", ""--kube-config-path"", dest=""kube_config_path"", default=""~/.kube/config"", help=""The path to KUBE_CONFIG file. Default value: ~/.kube/config"")\n            parser.add_argument(""-n"", ""--service-list"", nargs=\'+\', dest=""service_list"", default=None, help=""Service list to manage"")\n\n        add_arguments(start_parser)\n        add_arguments(stop_parser)\n        add_arguments(delete_parser)\n        add_arguments(refresh_parser)\n\n    def process_args(self, args):\n        if args.kube_config_path is not None:\n            args.kube_config_path = os.path.expanduser(args.kube_config_path)\n        return args.service_list\n\n    def service_start(self, args):\n        service_list = self.process_args(args)\n\n        service_management_starter = service_management_start.serivce_management_start(args.kube_config_path, service_list)\n        service_management_starter.run()\n\n    def service_stop(self, args):\n        service_list = self.process_args(args)\n\n        service_management_stopper = service_management_stop.service_management_stop(args.kube_config_path, service_list)\n        service_management_stopper.run()\n\n    def service_delete(self, args):\n        service_list = self.process_args(args)\n\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""----------     Dangerous Operation!!!    ---------------"")\n        logger.warning(""------     The target service will be stopped    -------"")\n        logger.warning(""------    And the persistent data on the disk    -------"")\n        logger.warning(""-------             will be deleted             --------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------     It\'s an irreversible operation      -------"")\n        logger.warning(""--------           After this operation,         -------"")\n        logger.warning(""------ the deleted service data is unrecoverable -------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""----    Please ensure you wanna do this operator, ------"")\n        logger.warning(""-------        after knowing all risk above.     -------"")\n        logger.warning(""--------------------------------------------------------"")\n        logger.warning(""--------------------------------------------------------"")\n\n        count_input = 0\n        while True:\n            user_input = raw_input(""Do you want to continue this operation? (Y/N) "")\n            if user_input == ""N"":\n                return\n            elif user_input == ""Y"":\n                break\n            else:\n                print("" Please type Y or N."")\n            count_input = count_input + 1\n            if count_input == 3:\n                logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                return\n\n        service_management_deleter = service_management_delete.service_management_delete(args.kube_config_path, service_list)\n        service_management_deleter.run()\n\n    def service_refresh(self, args):\n        service_list = self.process_args(args)\n\n        service_management_refresher = service_management_refresh.service_management_refresh(args.kube_config_path, service_list)\n        service_management_refresher.run()\n'"
deployment/utilityCmd.py,0,"b'import os\nimport sys\nimport logging\nimport logging.config\n\nfrom deployment.utility.ssh import OpenPaiSSH\nfrom deployment.utility.sftp_copy import OpenPaiSftpCopy\nfrom clusterObjectModel.cluster_object_model import cluster_object_model\n\nlogger = logging.getLogger(__name__)\n\n\nclass UtilityCmd():\n    def register(self, parser):\n        utility_parser = parser.add_subparsers(help=""utility for maintaining in a easy way."")\n\n        ssh_parser = utility_parser.add_parser(""ssh"")\n        ssh_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""path of cluster configuration file"")\n        ssh_parser.add_argument(""-f"", ""--filter"", dest=""filter"", nargs=\'+\', help=""Rule to filter machine. Format: key1=value1 key2=value2 ..."")\n        ssh_parser.add_argument(""-c"", ""--command"", dest=""command"", required=True, help=""The command to be executed remotely."")\n        ssh_parser.set_defaults(handler=self.cluster_ssh)\n\n        sftp_cp_parser = utility_parser.add_parser(""sftp-copy"")\n        sftp_cp_parser.add_argument(""-f"", ""--filter"", dest=""filter"", nargs=\'+\', help=""Rule to filter machine. Format: key1=value1 key2=value2 ..."")\n        sftp_cp_parser.add_argument(""-n"", ""--file-name"", dest=""file_name"", required=True, help=""File Name."")\n        sftp_cp_parser.add_argument(""-s"", ""--source"", dest=""source"", required=True, help=""The source path of the file."")\n        sftp_cp_parser.add_argument(""-d"", ""--destination"", dest=""dest"", required=True, help=""The target path of the file in the remote machines."")\n        sftp_cp_parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True, help=""path of cluster configuration file"")\n        sftp_cp_parser.set_defaults(handler=self.cluster_sftp_copy)\n\n    def get_machine_list(self, config_path):\n        objectModelFactoryHandler = cluster_object_model(configuration_path=config_path)\n        return objectModelFactoryHandler.kubernetes_config()[""layout""][""machine-list""]\n\n    def rule_check(self, rule_list):\n        if rule_list == None:\n            return\n        for rule in rule_list:\n            kv = rule.split(""="")\n            if len(kv) != 2:\n                logger.error(""Please check the filter rule {0}. It\'s invalid."".format(rule))\n                sys.exit(1)\n\n    def cluster_ssh(self, args):\n        if args.config_path != None:\n            args.config_path = os.path.expanduser(args.config_path)\n            machine_list = self.get_machine_list(args.config_path)\n        else:\n            machine_list = {}\n        self.rule_check(args.filter)\n        ssh_handler = OpenPaiSSH(args.command, machine_list, args.filter)\n        ssh_handler.run()\n\n    def cluster_sftp_copy(self, args):\n        if args.config_path != None:\n            args.config_path = os.path.expanduser(args.config_path)\n            machine_list = self.get_machine_list(args.config_path)\n        else:\n            machine_list = {}\n        if args.source != None:\n            args.source = os.path.expanduser(args.source)\n        if args.dest != None and os.path.isabs(args.dest) is not True:\n            logger.error(""The path of destination should an absolute path."")\n            sys.exit(1)\n        self.rule_check(args.filter)\n        sftp_copy_handler = OpenPaiSftpCopy(args.file_name, args.source, args.dest, machine_list, args.filter)\n        sftp_copy_handler.run()\n'"
build/core/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
build/core/build_center.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\n\nfrom model import dependency_graph\nfrom . import build_utility\nfrom . import build_handler\n\nimport os\nimport sys\nimport logging\nimport logging.config\n\nclass BuildCenter:\n\n    def __init__(self, build_config, process_list, type):\n\n        self.logger = logging.getLogger(__name__)\n        build_utility.setup_logger_config(self.logger)\n\n        self.build_config = build_config\n        self.task_type = type\n\n        self.process_list = [service.lower() for service in process_list] if process_list is not None else None\n\n        # Initialize docker_cli instance\n        self.docker_cli = build_utility.DockerClient(\n            docker_registry = self.build_config[\'dockerRegistryInfo\'][\'dockerRegistryDomain\'],\n            docker_namespace = self.build_config[\'dockerRegistryInfo\'][\'dockerNameSpace\'],\n            docker_username = self.build_config[\'dockerRegistryInfo\'][\'dockerUserName\'],\n            docker_password = self.build_config[\'dockerRegistryInfo\'][\'dockerPassword\']\n        )\n\n        # Initialize graph instance\n        self.graph = dependency_graph.ServiceGraph()\n\n        # Initialize dirs\n        self.codeDir = ""src""\n        self.dependencyDir = ""dependency/""\n\n\n    def construct_graph(self):\n        self.logger.info(""Starts to construct service graph"")\n\n        g = os.walk(self.codeDir)\n        for path, dir_list, file_list in g:\n            if path == self.codeDir:\n                for service in dir_list:\n                    self.graph.add_service(os.path.join(path, service), service)\n            service_name = path.split(os.sep)\n            service_name = service_name[-2] if len(service_name) > 1 else None\n            for file_name in file_list:\n                if file_name.endswith("".dockerfile""):\n                    passed = False\n                    if self.task_type == \'all\':\n                        passed = True\n                    elif file_name.endswith("".common.dockerfile"") or file_name.endswith(\n                            "".{0}.dockerfile"".format(self.task_type)):\n                        passed = True\n                    if passed:\n                        self.graph.add_image_to_service(str(os.path.splitext(file_name)[0]), service_name)\n        self.logger.info(""Construct service graph successfully"")\n\n    def resolve_dependency(self):\n        self.logger.info(""Starts to resolve components dependency"")\n\n        g = os.walk(self.codeDir)\n        for path, dir_list, file_list in g:\n            service_name = path.split(os.sep)\n            service_name = service_name[-2] if len(service_name) > 1 else None\n            for file_name in file_list:\n                if file_name.endswith("".dockerfile""):\n                    passed = False\n                    if self.task_type == \'all\':\n                        passed = True\n                    elif file_name.endswith("".common.dockerfile"") or file_name.endswith(\n                            "".{0}.dockerfile"".format(self.task_type)):\n                        passed = True\n                    if passed:\n                        with open(os.path.join(path, file_name), \'r\') as fin:\n                            for line in fin:\n                                if line.strip().startswith(""FROM""):\n                                    image = line.split()[1]\n                                    self.graph.add_dependency(self.graph.image_to_service.get(image), service_name)\n                elif file_name == ""component.dep"":\n                    with open(os.path.join(path, file_name), ""r"") as fin:\n                        for line in fin:\n                            self.graph.add_dependency(line.strip(), service_name)\n        # Show dependency graph\n        self.graph.dump()\n\n        self.logger.info(""Resolves dependency successfully"")\n\n    def build_center(self):\n\n        # Find services and map dockfile to services\n        self.construct_graph()\n\n        # Check all process_list items are valid or not\n        if self.process_list is not None:\n            for item in self.process_list:\n                if item not in self.graph.services:\n                    self.logger.error(""service {0} is invalid"".format(item))\n                    sys.exit(1)\n\n        # Resolve dependency\n        self.resolve_dependency()\n\n        # Build topology sequence\n        services = self.graph.topology()\n        self.logger.info(""topological sequence:{0}"".format(services))\n\n        # Start build each component according to topological sequence\n        try:\n            build_worker = build_handler.BuildHandler(self.docker_cli)\n            self.process_list = self.graph.extract_sub_graph(self.process_list) if self.process_list else services\n            for item in services:\n                if item in self.process_list:\n                    for inedge in self.graph.services[item].inedges:\n                        build_worker.copy_dependency_folder(os.path.join(self.codeDir,inedge),\n                        os.path.join(self.graph.services[item].path,self.dependencyDir+inedge))\n                    build_worker.build_single_component(self.graph.services[item])\n            self.logger.info(""Build all components succeed"")\n\n        except Exception as e:\n            self.logger.error(str(e))\n            self.logger.error(""Build all components failed"")\n            sys.exit(1)\n\n        finally:\n            # Clean generated folder\n            self.logger.info(""Begin to clean all temp folder"")\n            for item in services:\n                build_worker.clean_temp_folder(self.graph.services[item].path)\n            self.logger.info(""Clean all temp folder succeed"")\n\n\n    def push_center(self):\n\n        # Find services and map dockfile to services\n        self.construct_graph()\n\n        if self.process_list is not None:\n            for image in self.process_list:\n                if image not in self.graph.image_to_service:\n                    self.logger.error(""{0} not in image list"".format(image))\n                    sys.exit(1)\n\n                self.logger.info(""Starts to push image: {0}"".format(image))\n                self.docker_cli.docker_image_tag(image,self.build_config[\'dockerRegistryInfo\'][\'dockerTag\'])\n                self.docker_cli.docker_image_push(image,self.build_config[\'dockerRegistryInfo\'][\'dockerTag\'])\n                self.logger.info(""Push image:{0} successfully"".format(image))\n        else:\n            # by default push all images\n            for image in self.graph.image_to_service:\n                self.logger.info(""Starts to push image: {0}"".format(image))\n                self.docker_cli.docker_image_tag(image,self.build_config[\'dockerRegistryInfo\'][\'dockerTag\'])\n                self.docker_cli.docker_image_push(image,self.build_config[\'dockerRegistryInfo\'][\'dockerTag\'])\n                self.logger.info(""Push image:{0} successfully"".format(image))\n'"
build/core/build_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom . import build_utility\n\nimport sys\nimport logging\nimport logging.config\nimport os\nimport shutil\n\n\nclass BuildHandler:\n\n    def __init__(self, docker_cli):\n\n        self.logger = logging.getLogger(__name__)\n        build_utility.setup_logger_config(self.logger)\n\n        self.docker_cli = docker_cli\n\n        self.build_pre = \'build/build-pre.sh\'\n        self.build_post = \'build/build-post.sh\'\n        self.generatedDir = \'generated\'\n        self.dependencyDir = \'dependency\'\n\n\n    def build_single_component(self, service):\n\n        self.logger.info(""Starts to build {0}"".format(service.service_name))\n\n        pre_build = os.path.join(service.path, self.build_pre)\n        if os.path.exists(pre_build):\n            chmod_command = \'chmod u+x {0}\'.format(pre_build)\n            build_utility.execute_shell(chmod_command)\n            build_utility.execute_shell(pre_build)\n\n        for dockerfile_prefix in service.docker_files:\n            image_name = os.path.splitext(dockerfile_prefix)[0]\n            dockerfile = os.path.join(service.path, \'build/\' + dockerfile_prefix + \'.dockerfile\')\n            self.docker_cli.docker_image_build(image_name, dockerfile, service.path)\n\n        post_build = os.path.join(service.path, self.build_post)\n        if os.path.exists(post_build):\n            chmod_command = \'chmod u+x {0}\'.format(post_build)\n            build_utility.execute_shell(chmod_command)\n            build_utility.execute_shell(post_build)\n\n        self.logger.info(""Build {0} successfully"".format(service.service_name))\n\n\n    def copy_dependency_folder(self, source, destination):\n\n        if not os.path.exists(source):\n            self.logger.error(""{0} folder path does not exist"".format(source))\n            sys.exit(1)\n        else:\n            if os.path.isdir(destination):\n               shutil.rmtree(destination)\n            shutil.copytree(source,destination)\n\n    def clean_temp_folder(self, service_path):\n        temp_generated_dir = os.path.join(service_path, self.generatedDir)\n        temp_dependency_dir = os.path.join(service_path, self.dependencyDir)\n\n        if os.path.isdir(temp_generated_dir):\n            shutil.rmtree(temp_generated_dir)\n\n        if os.path.isdir(temp_dependency_dir):\n            shutil.rmtree(temp_dependency_dir)\n\n'"
build/core/build_utility.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport logging.config\n\n\nimport sys\nimport os\nimport subprocess\nimport yaml\n\n\nclass DockerClient:\n\n    def __init__(self, docker_registry, docker_namespace, docker_username, docker_password):\n\n        docker_registry = """" if docker_registry == ""public"" else docker_registry\n\n        self.docker_registry = docker_registry\n        self.docker_namespace = docker_namespace\n        self.docker_username = docker_username\n        self.docker_password = docker_password\n\n        self.docker_login()\n\n\n    def resolve_image_name(self, image_name):\n        prefix = """" if self.docker_registry == """" else self.docker_registry + ""/""\n        return ""{0}{1}/{2}"".format(prefix, self.docker_namespace, image_name)\n\n\n\n    def docker_login(self):\n        shell_cmd = ""docker login -u {0} -p {1} {2}"".format(self.docker_username, self.docker_password, self.docker_registry)\n        execute_shell(shell_cmd)\n\n\n    def docker_image_build(self, image_name, dockerfile_path, build_path):\n        cmd = ""docker build -t {0} -f {1} {2}"".format(image_name, dockerfile_path, build_path)\n        execute_shell(cmd)\n\n\n    def docker_image_tag(self, origin_image_name, image_tag):\n        origin_tag = origin_image_name\n        target_tag = ""{0}:{1}"".format(self.resolve_image_name(origin_image_name), image_tag)\n        cmd = ""docker tag {0} {1}"".format(origin_tag, target_tag)\n        execute_shell(cmd)\n\n\n\n    def docker_image_push(self, image_name, image_tag):\n        target_tag = ""{0}:{1}"".format(self.resolve_image_name(image_name), image_tag)\n        cmd = ""docker push {0}"".format(target_tag)\n        execute_shell(cmd)\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\ndef execute_shell(shell_cmd):\n    try:\n        logger.info(""Begin to execute the command: {0}"".format(shell_cmd))\n        subprocess.check_call( shell_cmd, shell=True )\n        logger.info(""Executing command successfully: {0}"".format(shell_cmd))\n    except subprocess.CalledProcessError:\n        logger.error(""Executing command failed: {0}"".format(shell_cmd))\n        sys.exit(1)\n\n\n\ndef execute_shell_with_output(shell_cmd):\n    try:\n        logger.info(""Begin to execute the command: {0}"".format(shell_cmd))\n        res = subprocess.check_output( shell_cmd, shell=True )\n        logger.info(""Executes command successfully: {0}"".format(shell_cmd))\n    except subprocess.CalledProcessError:\n        logger.error(""Executes command failed: {0}"".format(shell_cmd))\n        sys.exit(1)\n\n    return res\n\ndef load_yaml_config(config_path):\n\n    if not os.path.exists(config_path):\n        logger.error(""Invalid config path: {0}"".format(config_path))\n        sys.exit(1)\n\n    with open(config_path, ""r"") as f:\n        cluster_data = yaml.load(f)\n\n    return cluster_data\n'"
build/model/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
build/model/config_model.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\nfrom core import build_utility\n\nimport os\n\nclass ConfigModel:\n\n    def __init__(self, configuration_dir):\n        self.buildConfigDict = dict()\n        self.buildConfiguration = os.path.join(configuration_dir,""services-configuration.yaml"")\n\n    def build_config_parse(self):\n\n        # load config yaml file\n        buildConfigContent = build_utility.load_yaml_config(self.buildConfiguration)\n\n        # section : dockerRegistryInfo:\n        # forward compatibility with the old cluster configuration\n\n        if ""docker-registry-info"" in buildConfigContent[""cluster""]:\n            self.buildConfigDict[""dockerRegistryInfo""] = buildConfigContent[""cluster""][""docker-registry-info""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerNameSpace""] = \\\n                buildConfigContent[""cluster""][""docker-registry-info""][""docker-namespace""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerRegistryDomain""] = \\\n                buildConfigContent[""cluster""][""docker-registry-info""][""docker-registry-domain""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerUserName""] = buildConfigContent[""cluster""][""docker-registry-info""][""docker-username""] \\\n                if  ""docker-username"" in buildConfigContent[""cluster""][""docker-registry-info""] else None\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerPassword""] = buildConfigContent[""cluster""][""docker-registry-info""][""docker-password""] \\\n                if  ""docker-password"" in buildConfigContent[""cluster""][""docker-registry-info""] else None\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerTag""] = \\\n                buildConfigContent[""cluster""][""docker-registry-info""][""docker-tag""]\n            self.buildConfigDict[""dockerRegistryInfo""][""secretName""] = \\\n                buildConfigContent[""cluster""][""docker-registry-info""][""secret-name""]\n\n        else:\n            self.buildConfigDict[""dockerRegistryInfo""] = buildConfigContent[""cluster""][""docker-registry""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerNameSpace""] = \\\n                buildConfigContent[""cluster""][""docker-registry""][""namespace""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerRegistryDomain""] = \\\n                buildConfigContent[""cluster""][""docker-registry""][""domain""]\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerUserName""] = buildConfigContent[""cluster""][""docker-registry""][""username""] \\\n                if  ""username"" in buildConfigContent[""cluster""][""docker-registry""] else None\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerPassword""] = buildConfigContent[""cluster""][""docker-registry""][""password""] \\\n                if  ""password"" in buildConfigContent[""cluster""][""docker-registry""] else None\n            self.buildConfigDict[""dockerRegistryInfo""][""dockerTag""] = \\\n                buildConfigContent[""cluster""][""docker-registry""][""tag""]\n            self.buildConfigDict[""dockerRegistryInfo""][""secretName""] = \\\n                buildConfigContent[""cluster""][""docker-registry""][""secret-name""]\n\n        return self.buildConfigDict'"
build/model/dependency_graph.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nfrom core import build_utility\n\nimport os\nimport sys\nimport datetime\nimport logging\nimport logging.config\n\nclass ServiceNode(object):\n\n    def __init__(self, path, service_name):\n        self.path = path\n        self.service_name = service_name\n        self.docker_files = list()\n        self.inedges = list()\n        self.outedges = list()\n\n        self.logger = logging.getLogger(__name__)\n        build_utility.setup_logger_config(self.logger)\n\n\n    def dump(self):\n        self.logger.info(""Path:{} Name:{} Inedge:{} Outedge:{}"".format(\n            self.path,\n            self.service_name,\n            self.inedges,\n            self.outedges\n        ))\n\n\nclass ServiceGraph(object):\n\n    logger = logging.getLogger(__name__)\n    build_utility.setup_logger_config(logger)\n\n    def __init__(self):\n        self.services = dict()\n        self.image_to_service = dict()\n\n    def add_service(self, path, service_name):\n        if not service_name in self.services:\n            self.services[service_name] = ServiceNode(path, service_name)\n\n\n    def add_image_to_service(self, image_docker_file_prefix, service_name):\n        image_name = os.path.splitext(image_docker_file_prefix)[0]\n        if image_name in self.image_to_service:\n            self.logger.error(""Same image name:{0} detected! Please check!"".format(image_name))\n            self.logger.error(""Duplication image belongs to service:{0} and service:{1}"".format(service_name, self.image_to_service[image_name]))\n            sys.exit(1)\n        self.image_to_service[image_name] = service_name\n        self.services[service_name].docker_files.append(image_docker_file_prefix)\n\n\n    def add_dependency(self, prev_service, succ_service):\n        if not prev_service:\n            return\n        if prev_service in self.services and succ_service in self.services:\n            self.services[prev_service].outedges.append(succ_service)\n            self.services[succ_service].inedges.append(prev_service)\n        else:\n            self.logger.error(""Invalid dependency found: {0} in {1}"".format(prev_service,succ_service))\n            sys.exit(1)\n\n    def topology(self):\n        prev_count = dict()\n        ret = list()\n        search_queue = list()\n        for name, node in self.services.items():\n            prev_count[name] = len(node.inedges)\n            if prev_count[name] == 0:\n                search_queue.append(name)\n\n        while search_queue:\n            current_node = search_queue[0]\n            search_queue.pop(0)\n            ret.append(current_node)\n            for succ_service in self.services[current_node].outedges:\n                prev_count[succ_service] -= 1\n                if prev_count[succ_service] == 0:\n                    search_queue.append(succ_service)\n\n        # Add dependency loop check\n        if not len(self.services) == len(ret):\n            self.logger.error(""Dependency loop detected! Please check!"")\n            sys.exit(1)\n\n        return ret\n\n\n    def dump(self):\n        for _, service in self.services.items():\n            service.dump()\n\n\n    def extract_sub_graph(self, dest_nodes):\n        if not dest_nodes:\n            return None\n        search_queue = dest_nodes\n        ret = search_queue[:]\n        while search_queue:\n            current_node = search_queue[0]\n            search_queue.pop(0)\n            for prev_service in self.services[current_node].inedges:\n                if not prev_service in ret:\n                    ret.append(prev_service)\n                    search_queue.append(prev_service)\n        return ret\n'"
contrib/kubespray/namespace_secret_backup.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport logging.config\nimport yaml\nimport os\nimport argparse\nimport sys\nimport time\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\n\ndef get_namespaced_secret(namespace):\n    config.load_kube_config()\n    try:\n        api_instance = client.CoreV1Api()\n        api_response = api_instance.list_namespaced_secret(namespace)\n        return api_response.items\n    except ApiException as e:\n        if e.status == 404:\n            return []\n        sys.exit(1)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-n\', \'--namespace\', dest=""namespace"", required=True,\n                        help=""The secret in which namespace should be backup"")\n    parser.add_argument(\'-o\', \'--output\', dest=""output"", required=True, help=""the output file to store "")\n\n    args = parser.parse_args()\n    namespaces = args.namespace\n    output = args.output\n    data = get_namespaced_secret(namespaces)\n    output_data = []\n    for item in data:\n        if \'default-token-\' in item.metadata.name:\n            continue\n        output_data.append({\n            \'name\': item.metadata.name,\n            \'data\': item.data\n        })\n    with open(output, \'w\') as yaml_file:\n        yaml.dump(output_data, yaml_file, default_flow_style=False)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
contrib/kubespray/namespace_secret_recover.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport logging.config\nimport yaml\nimport os\nimport argparse\nimport sys\nimport time\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\n\ndef create_namespace_if_not_exist(namespace):\n    config.load_kube_config()\n    try:\n        api_instance = client.CoreV1Api()\n        api_instance.read_namespace(namespace)\n    except ApiException as e:\n        if e.status == 404:\n            api_instance = client.CoreV1Api()\n            meta_data = client.V1ObjectMeta()\n            meta_data.name = namespace\n            body = client.V1Namespace(\n                metadata=meta_data\n            )\n            api_instance.create_namespace(body)\n            return True\n        logger.error(""Failed to create namespace [{0}]"".format(namespace))\n        sys.exit(1)\n    return False\n\n\ndef create_secret_in_namespace_if_not_exist(namespace, payload):\n    config.load_kube_config()\n    try:\n        api_instance = client.CoreV1Api()\n        api_instance.read_namespaced_secret(payload[\'name\'], namespace)\n    except ApiException as e:\n        if e.status == 404:\n            try:\n                api_instance = client.CoreV1Api()\n                meta_data = client.V1ObjectMeta()\n                meta_data.name = payload[\'name\']\n                body = client.V1Secret(\n                    metadata=meta_data,\n                    data=payload[\'data\']\n                )\n                api_instance.create_namespaced_secret(namespace, body)\n            except ApiException as create_e:\n                logger.error(""Exception when calling CoreV1Api->create_namespaced_secret: %s\\n"" % create_e)\n                sys.exit(1)\n        else:\n            logger.error(""Exception when calling CoreV1Api->read_namespaced_secret: %s\\n"" % e)\n            sys.exit(1)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-n\', \'--namespace\', dest=""namespace"", required=True,\n                        help=""The secret in which namespace should be backup"")\n    parser.add_argument(\'-i\', \'--input\', dest=""input"", required=True, help=""the input file to restore data "")\n    parser.add_argument(\'-d\', \'--delete-backup\', dest=""delete"", default=False, action=\'store_true\')\n\n    args = parser.parse_args()\n    namespaces = args.namespace\n    input = args.input\n    delete_backup = args.delete\n\n    create_namespace_if_not_exist(namespaces)\n\n    with open(input, ""r"") as f:\n        secret_data = yaml.load(f, yaml.SafeLoader)\n\n    for item in secret_data:\n        create_secret_in_namespace_if_not_exist(\n            namespaces,\n            item\n        )\n\n    if delete_backup:\n        try:\n            os.unlink(input)\n        except OSError as e:\n            logger.exception(e)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
contrib/notebook-extension/setup.py,0,"b'""""""this is the setup (install) script for OpenPAI notebook extension\n""""""\nimport os\nimport sys\nfrom argparse import ArgumentParser\nfrom subprocess import check_output\n\n\ndef run(cmds: list, comment: str = None):\n    if comment:\n        print(comment, flush=True)\n    check_output(cmds, shell=True)\n\n\nif __name__ == \'__main__\':\n    parser = ArgumentParser()\n    parser.add_argument(\'--user\', action=\'store_true\', default=False, help=\'pip install in user mode\')\n    parser.add_argument(\'--ignore-sdk\', \'-i\', action=\'store_true\', default=False,\n                        help=\'dont install python sdk, make sure you have a workable version instead\')\n    args = parser.parse_args()\n\n    pip_cmd = [sys.executable, \'-m\', \'pip\', \'install\']\n    if args.user:\n        pip_cmd += [\'--user\']\n    jupyter_cmd = [sys.executable, \'-m\', \'jupyter\']\n\n    run(\n        pip_cmd + [\'jupyter\', \'jupyter_contrib_nbextensions\'],\n        \'==== install requirements ====\'\n    )\n\n    run(\n        jupyter_cmd + [\'contrib\', \'nbextension\', \'install\', \'--user\'],\n        \'==== install nbextension ====\'\n    )\n\n    if not args.ignore_sdk:\n        run(\n            pip_cmd + [\'--upgrade\', os.path.join(\'..\', \'python-sdk\')],\n            \'==== install sdk ====\'\n        )\n\n    run(\n        jupyter_cmd + [\'nbextension\', \'install\', \'openpai_submitter\'],\n        \'==== install openpai_submitter ====\'\n    )\n    run(\n        jupyter_cmd + [\'nbextension\', \'enable\', \'openpai_submitter/main\'],\n        \'==== enable openpai_submitter ====\'\n    )\n'"
contrib/profiler/profiler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation the\n# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software,\n# and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above\n# copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n""""""\nThe profiler is used to profile the using information of the hardware while a deep learing model is running\n""""""\nimport pynvml as nv\nimport numpy as np\nimport pandas as pd\nimport glob\nimport csv\nimport os\nimport time\nimport argparse\nimport matplotlib as mpl\n\nmpl.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom utils import Sample\nfrom utils import Adviser\nfrom utils import print_process\nfrom utils import GPU_INFO_OFFSET\nfrom utils import INFO_NUM_PER_GPU\nfrom utils import GPU_MEM_OFFSET\nfrom utils import SAMPLE_INFO\n\n\n# To get the CPU running time of system from being booted\ndef get_system_cpu_ticks():\n    with open(\'/proc/stat\', \'r\') as f:\n        for line in f.readlines():\n            if line.startswith(\'cpu \'):\n                items = line.split()\n                if len(items) < 8:\n                    return -1\n\n                total_clock_ticks = 0\n                for item in items[1:8]:\n                    total_clock_ticks += int(item)\n                return total_clock_ticks\n    return -1\n\n\n# To get the CPU running time of container from being booted\ndef get_container_cpu_ticks(file_name):\n    user_time = 0\n    system_time = 0\n    with open(file_name, \'r\') as f:\n        for line in f:\n            items = line.split()\n            if len(items) != 2:\n                return -1\n            if items[0] == \'user\':\n                user_time = int(items[1])\n            elif items[1] == \'system\':\n                system_time = int(items[1])\n        return user_time + system_time\n\n\ndef get_cpu_ticks(file_name):\n    sys_ticks = get_system_cpu_ticks()\n    container_ticks = get_container_cpu_ticks(file_name)\n    return [sys_ticks, container_ticks]\n\n\ndef get_gpu_utilization(gpu_idx):\n    try:\n        handle = nv.nvmlDeviceGetHandleByIndex(gpu_idx)\n        util = nv.nvmlDeviceGetUtilizationRates(handle)\n    except nv.NVMLError as err:\n        util = err\n    return util\n\n\ndef get_gpu_memory(gpu_idx):\n    try:\n        handle = nv.nvmlDeviceGetHandleByIndex(gpu_idx)\n        mem = nv.nvmlDeviceGetMemoryInfo(handle)\n    except nv.NVMLError as err:\n        mem = err\n    return mem\n\n\ndef get_memory_percent(file_name):\n    total_memory_path = \'/proc/meminfo\'\n\n    memory_docker_used = 0.0\n    total_memory = 1.0\n    with open(file_name, \'r\') as f:\n        for line in f:\n            memory_docker_used = int(line)\n\n    with open(total_memory_path, \'r\') as f:\n        for line in f:\n            if line.startswith(\'MemTotal\'):\n                lines = line.split()\n                total_memory = int(lines[1]) * 1024\n                break\n    return [memory_docker_used, total_memory]\n\n\ndef get_disk_bytes(file_name):\n    read_bytes, write_bytes = 0, 0\n    with open(file_name, \'r\') as f:\n        for line in f:\n            items = line.split()\n            if len(items) != 3 and len(items) != 2:\n                return -1\n            if items[1] == \'Read\':\n                read_bytes += int(items[2])\n            elif items[1] == \'Write\':\n                write_bytes += int(items[2])\n    return [read_bytes, write_bytes]\n\n\ndef get_network_bytes(file_name):\n    receive_bytes, transmit_bytes = 0, 0\n    with open(file_name, \'r\') as f:\n        for line in f:\n            if len(line.split()) != 17:\n                continue\n            else:\n                items = line.split()\n                receive_bytes += int(items[1])\n                transmit_bytes += int(items[9])\n    return [receive_bytes, transmit_bytes]\n\n\nByte_GiByte = 1024 * 1024 * 1024\nByte_MiByte = 1024 * 1024\nByte_KiByte = 1024\n\n\n# get the sample data according to the system file\ndef get_sample_data(cpu_file, mem_file, blk_file, net_file, gpu_id, period):\n    [mem_used, mem_total] = get_memory_percent(mem_file)\n\n    # 1st info about I/O, network and CPU\n    # read_bytes1 = get_disk_read_bytes(blk_file)\n    # write_bytes1 = get_disk_write_bytes(blk_file)\n    [read_bytes1, write_bytes1] = get_disk_bytes(blk_file)\n    [network_receive1, network_transmit1] = get_network_bytes(net_file)\n    [sys_ticks1, container_ticks1] = get_cpu_ticks(cpu_file)\n    time.sleep(period)\n    # 2nd info about I/O, network and CPU, calculate how many bytes used in this period\n    # read_bytes2 = get_disk_read_bytes(blk_file)\n    # write_bytes2 = get_disk_write_bytes(blk_file)\n    [read_bytes2, write_bytes2] = get_disk_bytes(blk_file)\n    [network_receive2, network_transmit2] = get_network_bytes(net_file)\n    [sys_ticks2, container_ticks2] = get_cpu_ticks(cpu_file)\n\n    online_cpus = os.sysconf(os.sysconf_names[\'SC_NPROCESSORS_ONLN\'])\n    cpu_usage = (container_ticks2 - container_ticks1) * 1.0 / (sys_ticks2 - sys_ticks1) * online_cpus * 100\n\n    # get the usage of the GPU to analyze\n    gpu_usage = list()\n    gpu_mem = list()\n    gpu_mem_used = list()\n    gpu_mem_total = list()\n    for gid in gpu_id:\n        gpu_usage.append(get_gpu_utilization(gid).gpu)\n        gpu_mem.append(get_gpu_utilization(gid).memory)\n        gpu_mem_used.append(get_gpu_memory(gid).used / Byte_GiByte)\n        gpu_mem_total.append(get_gpu_memory(gid).total / Byte_GiByte)\n    sample_data = Sample(cpu_usage, mem_used / Byte_GiByte, mem_total / Byte_GiByte,\n                         (read_bytes2 - read_bytes1) / period / Byte_KiByte,\n                         (write_bytes2 - write_bytes1) / period / Byte_KiByte,\n                         (network_receive2 - network_receive1) / period / Byte_KiByte,\n                         (network_transmit2 - network_transmit1) / period / Byte_KiByte,\n                         gpu_usage, gpu_mem, gpu_mem_used, gpu_mem_total)\n    return sample_data\n\n\n# draw the graphs and save them to the files\ndef draw_graph(sample_datas, output_dir, period, gpu_id):\n    if not os.path.exists(output_dir + \'/img\'):\n        os.mkdir(output_dir + \'/img\')\n    sample_datas = np.array(sample_datas)\n    gpu_nums = len(gpu_id)\n\n    # draw the GPU memory usage\n    gpu_mem, legends, times = list(), list(), list()\n    for i in range(int(gpu_nums)):\n        gpu_mem.append(100 * sample_datas[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + i * INFO_NUM_PER_GPU] /\n                       sample_datas[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + i * INFO_NUM_PER_GPU + 1]\n                       )\n        legends.append(\'gpu_mem_used_\' + str(gpu_id[i]))\n    for i in range(sample_datas.shape[0]):\n        times.append(i * period)\n    plt.figure()\n    plt.title(\'GPU Memory Utilization\')\n    plt.xlabel(\'Time(s)\')\n    plt.ylabel(\'GPU memory utilization(%)\')\n    plt.plot(times, np.array(gpu_mem).T)\n    plt.legend(legends)\n    plt.grid(True)\n    plt.savefig(output_dir + \'/img/GPU_MEM_Utilization.png\')\n\n    # draw the GPU usage\n    gpu_usage, legends, times = list(), list(), list()\n    for i in range(int(gpu_nums)):\n        gpu_usage.append(sample_datas[:, GPU_INFO_OFFSET + i * INFO_NUM_PER_GPU])\n        legends.append(\'gpu_used_\' + str(gpu_id[i]))\n    for i in range(sample_datas.shape[0]):\n        times.append(i * period)\n    plt.figure()\n    plt.title(\'GPU Utilization\')\n    plt.xlabel(\'Time(s)\')\n    plt.ylabel(\'GPU utilization(%)\')\n    plt.plot(times, np.array(gpu_usage).T)\n    plt.legend(legends)\n    plt.grid(True)\n    plt.savefig(output_dir + \'/img/GPU_UTI_Utilization.png\')\n\n    # draw the CPU and GPU usage\n    times = list()\n    length = sample_datas.shape[0]\n    gpu_usage = sample_datas[int(0.6 * length):int(0.6 * length) + 1000, GPU_INFO_OFFSET]\n    cpu_usage = sample_datas[int(0.6 * length):int(0.6 * length) + 1000, SAMPLE_INFO.cpu_usage.value]\n    for i in range(gpu_usage.shape[0]):\n        times.append(i * period)\n    fig = plt.figure()\n    a1 = fig.add_subplot(111)\n    a1.set_title(\'CPU & GPU Utilization\')\n    a1.plot(times, cpu_usage, label=\'cpu\')\n    plt.legend(loc=\'best\')\n    a1.set_ylim([0, np.max(cpu_usage) if np.max(cpu_usage) > 100 else 100])\n    a1.set_ylabel(\'CPU utilization(%)\')\n    a1.set_xlabel(\'Time(s)\')\n    a2 = a1.twinx()\n    a2.plot(times, gpu_usage, \'orange\', label=\'gpu\')\n    plt.legend(loc=\'best\')\n    a2.set_ylim([0, 100])\n    a2.set_ylabel(\'GPU utilization(%)\')\n    plt.grid(True)\n    plt.savefig(output_dir + \'/img/CPU_GPU_Utilization.png\')\n\n    # draw the IO usage\n    times = list()\n    # index 3 and 4 are the column of the I/O rate\n    io_rate = [sample_datas[:, SAMPLE_INFO.io_read.value], sample_datas[:, SAMPLE_INFO.io_write.value]]\n    legends = [\'Disk read\', \'Disk write\']\n    for i in range(sample_datas.shape[0]):\n        times.append(i * period)\n    plt.figure()\n    plt.title(\'Disk Utilization\')\n    plt.xlabel(\'Time(s)\')\n    plt.ylabel(\'Disk Utilization(KBps)\')\n    plt.plot(times, np.array(io_rate).T)\n    plt.legend(legends)\n    plt.grid(True)\n    plt.savefig(output_dir + \'/img/Disk_Utilization.png\')\n\n    # draw the network usage\n    times = list()\n    # index 5 and 6 are the column of the network rate\n    network_rate = [sample_datas[:, SAMPLE_INFO.network_inbound.value],\n                    sample_datas[:, SAMPLE_INFO.network_outbound.value]]\n    legends = [\'Network Inbound\', \'Network Outbound\']\n    for i in range(sample_datas.shape[0]):\n        times.append(i * period)\n    plt.figure()\n    plt.title(\'Network Usage\')\n    plt.xlabel(\'Time(s)\')\n    plt.ylabel(\'Network Utilization(KBps)\')\n    plt.plot(times, np.array(network_rate).T)\n    plt.legend(legends)\n    plt.grid(True)\n    plt.savefig(output_dir + \'/img/Network_Utilization.png\')\n\n\ndef analyze_value(sample_datas, period, gpu_id):\n    sample_datas = np.array(sample_datas)\n    gpu_nums = len(gpu_id)\n\n    # analyze the CPU usage\n    # index 0 is the CPU usage\n    cpu_usage = np.sort(sample_datas[:, SAMPLE_INFO.cpu_usage.value])\n    print(\'For the CPU, here is the analyze result:\')\n    print(\'The max value of the CPU Utilization is\', str(np.max(cpu_usage)) + \'%\')\n    print(\'The min value of the CPU Utilization is\', str(np.min(cpu_usage)) + \'%\')\n    print(\'The average value of the CPU Utilization is\', str(np.average(cpu_usage)) + \'%\')\n    print(\'The standard deviation of the CPU Utilization is\', str(np.std(cpu_usage)) + \'%\')\n    print(\'Less than 50% value is more than\', str(cpu_usage[int(0.5 * cpu_usage.shape[0])]) + \'%\')\n    print(\'Less than 20% value is more than\', str(cpu_usage[int(0.8 * cpu_usage.shape[0])]) + \'%\')\n    print(\'===================================================================\')\n\n    # analyze the Disk\n    # index 3 and 4 are the Disk read and write\n    disk_read = np.sort(sample_datas[:, SAMPLE_INFO.io_read.value])\n    disk_write = np.sort(sample_datas[:, SAMPLE_INFO.io_write.value])\n    print(\'For the Disk, here is the analyze result:\')\n    print(\'The max value of the Disk read is\', str(np.max(disk_read)) + \'KBps\')\n    min_read = 0\n    for i in range(disk_read.shape[0]):\n        min_read = disk_read[i]\n        if min_read > 0:\n            break\n    print(\'The min value of the Disk read (without zero) is\', str(min_read) + \'KBps\')\n    print(\'The max value of the Disk write is\', str(np.max(disk_write)) + \'KBps\')\n    min_write = 0\n    for i in range(disk_write.shape[0]):\n        min_write = disk_write[i]\n        if min_write > 0:\n            break\n    print(\'The min value of the Disk write (without zero) is\', str(min_write) + \'KBps\')\n    print(\'The total read volume of the Disk is\', str(np.sum(disk_read) * period) + \'KB\')\n    print(\'The total write volume of the Disk is\', str(np.sum(disk_write) * period) + \'KB\')\n    print(\'===================================================================\')\n\n    # analyze the Network\n    # index 5 and 6 are the Network inbound and outbound\n    network_inbound = np.sort(sample_datas[:, SAMPLE_INFO.network_inbound.value])\n    network_outbound = np.sort(sample_datas[:, SAMPLE_INFO.network_outbound.value])\n    print(\'For the Network, here is the analyze result:\')\n    print(\'The max value of the Network Inbound is\', str(np.max(network_inbound)) + \'KBps\')\n    min_inbound = 0\n    for i in range(network_inbound.shape[0]):\n        min_inbound = network_inbound[i]\n        if min_inbound > 0:\n            break\n    print(\'The min value of the Network Inbound (without zero) is\', str(min_inbound) + \'KBps\')\n    print(\'The max value of the Network Outbound is\', str(np.max(network_outbound)) + \'KBps\')\n    min_outbound = 0\n    for i in range(network_outbound.shape[0]):\n        min_outbound = network_outbound[i]\n        if min_outbound > 0:\n            break\n    print(\'The min value of the Network Outbound (without zero) is\', str(min_outbound) + \'KBps\')\n    print(\'The total Inbound volume of the Network is\', str(np.sum(network_inbound) * period) + \'KB\')\n    print(\'The total Outbound volume of the Network is\', str(np.sum(network_outbound) * period) + \'KB\')\n    print(\'===================================================================\')\n\n    print(\'For the GPU, here is the analyze result:\')\n    print(\'The total number of the GPU cards is\', gpu_nums)\n    print(\'===================================================================\')\n\n    # analyze the GPU memory\n    print(\'For the GPU memory:\')\n    for i in range(gpu_nums):\n        total_gpu_mem = np.max(sample_datas[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + 1 + INFO_NUM_PER_GPU * i])\n        max_gpu_men = np.max(sample_datas[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + INFO_NUM_PER_GPU * i])\n        print(\'The memory Utilization of Card Index \', gpu_id[i], \'is %.4f%%\' % (max_gpu_men / total_gpu_mem * 100))\n    print(\'===================================================================\')\n\n    # analyze the GPU usage\n    gpu_usage = np.sort(sample_datas[:, GPU_INFO_OFFSET])\n    print(\'For the GPU utilization, we choose the master card to calculate the result.\')\n    print(\'The max value of the GPU Utilization is\', str(np.max(gpu_usage)) + \'%\')\n    print(\'The min value of the GPU Utilization is\', str(np.min(gpu_usage)) + \'%\')\n    print(\'The average value of the GPU Utilization is\', str(np.average(gpu_usage)) + \'%\')\n    print(\'The standard deviation of the GPU Utilization is\', str(np.std(gpu_usage)) + \'%\')\n    print(\'Less than 50% value is more than\', str(gpu_usage[int(0.5 * gpu_usage.shape[0])]) + \'%\')\n    print(\'Less than 20% value is more than\', str(gpu_usage[int(0.8 * gpu_usage.shape[0])]) + \'%\')\n\n\ndef start_sample(container_id, period, analyze_period, output_dir, gpu_id, container_pid, duration_time):\n    start_time = time.time()\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    with open(output_dir + \'/log_result.csv\', \'w\') as result_file:\n        realtime_log = csv.writer(result_file)\n\n        str_write_realtime = [\'timestamp\', \'cpu_usage(%)\', \'mem_used(GiByte)\', \'mem_total(GiByte)\', \'IO_read(KiByte/s)\',\n                              \'IO_write(KiByte/s)\', \'network_receive(KiByte/s)\', \'network_transmit(KiByte/s)\']\n        for i in range(len(gpu_id)):\n            str_write_realtime.append(\'gpu_usage_\' + str(gpu_id[i]))\n            str_write_realtime.append(\'gpu_mem_usage_\' + str(gpu_id[i]))\n            str_write_realtime.append(\'gpu_mem_used_\' + str(gpu_id[i]))\n            str_write_realtime.append(\'gpu_mem_total_\' + str(gpu_id[i]))\n        realtime_log.writerow(str_write_realtime)\n\n        sample_list = list()\n        # container_cpu_file = \'\'\n        # container_mem_file = \'\'\n        # container_blk_file = \'\'\n        # container_net_file = \'\'\n\n        if int(container_pid) == -1:\n            container_cpu_file = \'/sys/fs/cgroup/cpuacct/cpuacct.stat\'\n            container_mem_file = \'/sys/fs/cgroup/memory/memory.usage_in_bytes\'\n            container_blk_file = \'/sys/fs/cgroup/blkio/blkio.throttle.io_service_bytes\'\n            container_net_file = \'/proc/net/dev\'\n        else:\n            container_cpu_file = glob.glob(\'/sys/fs/cgroup/cpuacct/docker/\' + str(container_id) + \'*/cpuacct.stat\')[0]\n            container_mem_file = glob.glob(\n                \'/sys/fs/cgroup/memory/docker/\' + str(container_id) + \'*/memory.usage_in_bytes\')[0]\n            container_blk_file = glob.glob(\n                \'/sys/fs/cgroup/blkio/docker/\' + str(container_id) + \'*/blkio.throttle.io_service_bytes\')[0]\n            container_net_file = \'/proc/\' + str(container_pid) + \'/net/dev\'\n\n        adviser = Adviser()\n        sample_datas = list()\n        stop_flag = False\n        last_time = time.time()\n        while not (os.path.exists(""./stop.flag"") or stop_flag):\n            sample_data = get_sample_data(container_cpu_file, container_mem_file, container_blk_file,\n                                          container_net_file, gpu_id, period)\n\n            str_write_realtime = sample_data.get_array()\n            str_write_realtime.insert(0, time.time() - start_time)\n            sample_list.append(str_write_realtime)\n            sample_datas.append(str_write_realtime)\n            realtime_log.writerow(str_write_realtime)\n\n            # if len(sample_list) > analyze_period / period:\n            if time.time() - last_time >= analyze_period:\n                last_time = time.time()\n                adviser.detect_pattern(sample_list)\n                sample_list = list()\n                if duration_time != -1:\n                    print_process((time.time() - start_time) / (duration_time * 60))\n                    stop_flag = True if time.time() - start_time > duration_time * 60 else False\n        print_process(1)\n        adviser.get_advise()\n    sample_datas = pd.read_csv(output_dir + \'/log_result.csv\').values\n    analyze_value(sample_datas, period, gpu_id)\n    draw_graph(sample_datas, output_dir, period, gpu_id)\n\n\n# prepare for the args\nparser = argparse.ArgumentParser(description=\'The profiler to collect the hardware information\')\nparser.add_argument(\'--container_id\', \'-i\', help=\'The SHA of the docker container\', required=True)\nparser.add_argument(\'--container_pid\', \'-p\', help=\'The pid of the docker container\', required=True)\nparser.add_argument(\'--sample_period\', help=\'The period of the CPU usage collecting\', required=True, type=float)\nparser.add_argument(\'--analyze_period\', help=\'The period of the CPU usage analyzing\', required=True, type=float)\nparser.add_argument(\'--output_dir\', \'-o\', help=\'The output directory to store the files\', required=True)\nparser.add_argument(\'--duration_time\', \'-t\', help=\'How long the profiler will execute\', required=True, type=int)\nargs = parser.parse_args()\n\n# Setting the max duration if the job will cost too much time\nMAX_TIME_DURATION = 60\nif __name__ == \'__main__\':\n    # get the GPU INDEX\n    nv.nvmlInit()\n    GPU_INDEX = list(range(nv.nvmlDeviceGetCount()))\n    duration = MAX_TIME_DURATION if args.duration_time == -1 else args.duration_time\n    if os.path.exists(""./stop.flag""):\n        os.remove(""./stop.flag"")\n    start_sample(args.container_id, args.sample_period, args.analyze_period, args.output_dir, GPU_INDEX,\n                 args.container_pid, duration)\n'"
contrib/profiler/utils.py,0,"b""import numpy as np\nfrom enum import Enum\n\n# The info nums before GPU_info\nGPU_INFO_OFFSET = 8\n# The info nums of each GPU card\nINFO_NUM_PER_GPU = 4\n# The info nums before GPU_mem in each GPU info\nGPU_MEM_OFFSET = 2\n\n\nclass SAMPLE_INFO(Enum):\n    timestamp = 0\n    cpu_usage = 1\n    mem_used = 2\n    men_total = 3\n    io_read = 4\n    io_write = 5\n    network_inbound = 6\n    network_outbound = 7\n\n\nclass Adviser:\n    def __init__(self):\n        self._phenomena = ['There is mal-distribution of the GPU memory between the multiple GPUs.',\n                           'Both the GPU and GPU memory have free resource.',\n                           'GPU memory is full, but the GPU utilization has free resource.',\n                           'IO, CPU and GPU raise alternately.'\n                           ]\n        self._times = [0] * len(self._phenomena)\n        self._total = 0\n        self._no_need_optimize = True\n\n    def add_total(self):\n        self._total += 1\n\n    def add_times(self, index):\n        if index >= len(self._times):\n            return\n        self._times[index] += 1\n\n    def detect_pattern(self, sample_list):\n        # sample_list is a 2-D array with m rows and 7 + (num_GPU * 4) cols\n        # The number of rows is decided by the sampling time.\n        # The number of cols is decided by the number of GPU that used.\n        sample_list = np.array(sample_list, dtype=np.float)\n        used_gpu_num = int((sample_list.shape[1] - GPU_INFO_OFFSET) / INFO_NUM_PER_GPU)\n        cpu_usage = sample_list[:, SAMPLE_INFO.cpu_usage.value]\n        mem_usage = sample_list[:, SAMPLE_INFO.mem_used.value] / sample_list[:, SAMPLE_INFO.men_total.value]\n        gpu_usage = list()\n        gpu_mem_usage = list()\n        for i in range(int(used_gpu_num)):\n            gpu_usage.append(sample_list[:, GPU_INFO_OFFSET + i * INFO_NUM_PER_GPU])\n            gpu_mem_usage.append(\n                sample_list[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + i * INFO_NUM_PER_GPU]\n                / sample_list[:, GPU_INFO_OFFSET + GPU_MEM_OFFSET + 1 + i * INFO_NUM_PER_GPU]\n            )\n        for index in range(0, len(self._phenomena)):\n            if index == 0:\n                if used_gpu_num >= 2:\n                    # multiple GPUs, analyze whether each GPU has the same memory usage\n                    gpu_mem_usage_avg = list()\n                    for i in range(used_gpu_num):\n                        gpu_mem_usage_avg.append(np.average(gpu_mem_usage[i]))\n                    gpu_mem_usage_avg.sort()\n                    if gpu_mem_usage_avg[-1] < 0.01:\n                        continue\n                    if abs(gpu_mem_usage_avg[-1] - gpu_mem_usage_avg[0]) > 0.15:\n                        self.add_times(index=0)\n                    elif abs(gpu_mem_usage_avg[-1] - gpu_mem_usage_avg[0]) / gpu_mem_usage_avg[-1] > 0.15:\n                        self.add_times(index=0)\n            elif index == 1:\n                if np.average(gpu_usage[0]) < 85 and np.average(gpu_mem_usage[0]) < 0.80:\n                    self.add_times(index=1)\n            elif index == 2:\n                if np.average(gpu_usage[0]) < 85 and np.average(gpu_mem_usage[0]) >= 0.80:\n                    self.add_times(index=2)\n            elif index == 3:\n                if np.average(gpu_usage[0]) < 85:\n                    slide_windows = SlideWindows(10)\n                    cpu_slide = list()\n                    for i in range(cpu_usage.shape[0]):\n                        cpu_slide.append(slide_windows.get_data(cpu_usage[i]))\n                    cpu_slide_copy = cpu_slide.copy()\n                    cpu_slide_copy.sort()\n                    cpu_std_max = cpu_slide_copy[int(len(cpu_slide_copy) * 0.8)]\n                    cpu_std_min = cpu_slide_copy[int(len(cpu_slide_copy) * 0.2)]\n\n                    gpu_usage_up_down = [0]\n                    for i in range(1, len(gpu_usage[0])):\n                        if gpu_usage[0][i] > gpu_usage[0][i - 1]:\n                            gpu_usage_up_down.append(1)\n                        elif gpu_usage[0][i] < gpu_usage[0][i - 1]:\n                            gpu_usage_up_down.append(-1)\n                        else:\n                            gpu_usage_up_down.append(0)\n                    gpu_usage_up_down[0] = gpu_usage_up_down[1]\n\n                    gpu_up_interval = list()\n                    gpu_down_interval = list()\n                    up_flag = True\n                    down_flag = True\n                    for i in range(len(gpu_usage_up_down)):\n                        if gpu_usage_up_down[i] == 1 and up_flag:\n                            up_flag = False\n                        elif i >= 1 and gpu_usage_up_down[i] == -1 and not up_flag:\n                            up_flag = True\n\n                        if gpu_usage_up_down[i] == -1 and down_flag:\n                            down_flag = False\n                        elif i >= 1 and gpu_usage_up_down[i] == 1 and not down_flag:\n                            down_flag = True\n\n                        if not up_flag:\n                            gpu_up_interval.append(i)\n                        elif not down_flag:\n                            gpu_down_interval.append(i)\n                    up_cpu_min, down_cpu_min = 0, 0\n                    up_cpu_max, down_cpu_max = 0, 0\n                    for i in range(1, len(cpu_slide) - 1):\n                        if cpu_slide[i] < cpu_slide[i - 1] and \\\n                                cpu_slide[i] < cpu_slide[i + 1] and cpu_slide[i] < cpu_std_min:\n                            if i in gpu_up_interval:\n                                up_cpu_min += 1\n                            elif i in gpu_down_interval:\n                                down_cpu_min += 1\n                        if cpu_slide[i] > cpu_slide[i - 1] and \\\n                                cpu_slide[i] > cpu_slide[i + 1] and cpu_slide[i] > cpu_std_max:\n                            if i in gpu_up_interval:\n                                up_cpu_max += 1\n                            elif i in gpu_down_interval:\n                                down_cpu_max += 1\n                    if up_cpu_min + down_cpu_min != 0 and up_cpu_max + down_cpu_max != 0:\n                        if float(down_cpu_min / (up_cpu_min + down_cpu_min)) > 0.6 or float(\n                                up_cpu_max / (up_cpu_max + down_cpu_max)) > 0.6:\n                            self.add_times(index=3)\n        self.add_total()\n\n    def get_advise(self):\n        print('===================================================================')\n        print('We have finished the analyzing %d times and the result is as follow.' % self._total)\n        print('According to the voting, the weight of each advice is as follow:')\n        for i in range(len(self._phenomena)):\n            weight = self._times[i] / self._total * 100\n            if weight >= 80:\n                frequency = 'MOST'\n            elif weight >= 30:\n                frequency = 'SEVERAL'\n            elif weight > 0:\n                frequency = 'A LITTLE'\n            else:\n                frequency = 'NO'\n            print('There is ', frequency, ' sample showing that \\'', self._phenomena[i], '\\'')\n        print('===================================================================')\n\n\nclass Sample:\n    def __init__(self, cpu_usage, mem_used, mem_total, read_bytes, write_bytes, network_receive, network_transmit,\n                 gpu_usage, gpu_mem, gpu_mem_used, gpu_mem_total):\n        self._cpu_usage = cpu_usage\n        self._mem_used = mem_used\n        self._mem_total = mem_total\n        self._read_bytes = read_bytes\n        self._write_bytes = write_bytes\n        self._network_receive = network_receive\n        self._network_transmit = network_transmit\n        self._gpu_usage = gpu_usage\n        self._gpu_mem = gpu_mem\n        self._gpu_mem_used = gpu_mem_used\n        self._gpu_men_total = gpu_mem_total\n\n    def get_cpu_usage(self):\n        return self._cpu_usage\n\n    def get_mem_used(self):\n        return self._mem_used\n\n    def get_mem_total(self):\n        return self._mem_total\n\n    def get_read_bytes(self):\n        return self._read_bytes\n\n    def get_write_bytes(self):\n        return self._write_bytes\n\n    def get_network_receive(self):\n        return self._network_receive\n\n    def get_network_transmit(self):\n        return self._network_transmit\n\n    def get_gpu_usage(self):\n        return self._gpu_usage\n\n    def get_gpu_mem(self):\n        return self._gpu_mem\n\n    def get_gpu_mem_used(self):\n        return self._gpu_mem_used\n\n    def get_gpu_mem_total(self):\n        return self._gpu_men_total\n\n    def get_array(self):\n        result = [self._cpu_usage, self._mem_used, self._mem_total, self._read_bytes, self._write_bytes,\n                  self._network_receive, self._network_transmit]\n        for i in range(len(self._gpu_usage)):\n            result.append(self._gpu_usage[i])\n            result.append(self._gpu_mem[i])\n            result.append(self._gpu_mem_used[i])\n            result.append(self._gpu_men_total[i])\n        return result\n\n\nclass SlideWindows(object):\n    def __init__(self, size):\n        self._size = size\n        self._index = 0\n        self._data = list()\n        self._avg = 0.0\n\n    def get_data(self, new_data):\n        if len(self._data) < self._size:\n            self._data.append(new_data)\n            self._avg = (self._avg * (len(self._data) - 1) + new_data) / len(self._data)\n            self._index = (self._index + 1) % self._size\n        else:\n            self._avg = (self._avg * self._size - self._data[self._index] + new_data) / self._size\n            self._data[self._index] = new_data\n            self._index = (self._index + 1) % self._size\n        return self._avg\n\n\ndef print_process(process):\n    if process > 1:\n        process = 1\n    result = ['='] * 100\n    for i in range(int(process * 100)):\n        result[i] = '-'\n    result[int(process * 100) - 1] = '>'\n    print('[' + ''.join(result) + ']' + '%.2f' % float(process * 100) + '% has been finished')\n"""
contrib/python-sdk/setup.py,0,"b""from setuptools import setup\n\nsetup(name='openpaisdk',\n      version='0.4.00',\n      description='A simple SDK for OpenPAI',\n      url='https://github.com/microsoft/pai/contrib/python-sdk',\n      packages=['openpaisdk'],\n      install_requires=[\n          'requests', 'hdfs', 'PyYAML', 'requests-toolbelt', 'html2text', 'tabulate'\n      ],\n      entry_points={\n          'console_scripts': ['opai=openpaisdk.command_line:main'],\n      },\n      zip_safe=False\n      )\n"""
contrib/remote-dev-tool/remote-dev-tool.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport re\nimport os\nimport sys\nimport time\nimport random\nimport string\nimport configparser\nimport subprocess\nimport logging\nimport getpass\nimport socket\nimport requests\n\nUSERNAME = """"\nTOKEN = """"\nSERVER_IP = """"\nUSER_DIR = """"\nSSH_INFO = """"\nLOGGER = """"\nSSH_INFO = {}\nIS_SHELL = """"\nJOB_NAME = """"\n\n\ndef read_env():\n    global USERNAME, TOKEN, SERVER_IP\n    LOGGER.debug(""read env"")\n    cf = configparser.ConfigParser()\n    cf.read("".env"")\n    USERNAME = cf.get(""PAI_ENV"", ""username"")\n    TOKEN = cf.get(""PAI_ENV"", ""token"")\n    SERVER_IP = cf.get(""PAI_ENV"", ""serverip"")\n\n\ndef check_platform():\n    LOGGER.debug(""check platform"")\n    global IS_SHELL, USER_DIR\n    if sys.platform.find(""win"") != -1:\n        IS_SHELL = False\n        USER_DIR = os.environ.get(\'UserProfile\')\n    elif sys.platform.find(""linux"") != -1:\n        IS_SHELL = True\n        USER_DIR = os.environ[""HOME""]\n    else:\n        LOGGER.debug(""unsupported platform"")\n        exit(-1)\n\n\ndef generate_conf():\n    LOGGER.debug(""generate conf files"")\n    pai_dir = os.path.join(USER_DIR, "".openpai/"")\n    if not os.path.exists(pai_dir):\n        os.makedirs(pai_dir)\n    conf_files = [\'clusters\', \'exports\']\n    for conf_file in conf_files:\n        f1 = open(""conf/{}.template"".format(conf_file), \'r+\')\n        f2 = open(""{}{}.yaml"".format(pai_dir, conf_file), \'w+\')\n        for line in f1.readlines():\n            line = re.sub(""\\$username"", USERNAME, line)\n            line = re.sub(""\\$token"", TOKEN, line)\n            line = re.sub(""\\$serverip"", SERVER_IP, line)\n            f2.write(line)\n        f1.close()\n        f2.close()\n\n\ndef init():\n    global LOGGER\n    logging.basicConfig(level=logging.INFO, format=\'%(asctime)s-%(name)s-%(levelname)s-%(message)s\')\n    LOGGER = logging.getLogger(__name__)\n    script_folder = os.path.dirname(os.path.realpath(__file__))\n    os.chdir(script_folder)\n    read_env()\n    check_platform()\n    generate_conf()\n\n\ndef read_input(message):\n    while True:\n        answer = input(""{} (y/n)?"".format(message))\n        if answer in {""Y"", ""y"", ""yes"", ""Yes"", ""YES""}:\n            return True\n        elif answer in {""N"", ""n"", ""no"", ""No"", ""NO""}:\n            return False\n\n\ndef run_subprocess(command):\n    LOGGER.debug(""run subprocess {}, shell={}"".format(command, IS_SHELL))\n    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=IS_SHELL)\n    p_output, p_error = p.communicate()\n    LOGGER.debug(p_output)\n    if p.returncode != 0:\n        LOGGER.debug(p_error)\n    return p_output.decode(""utf-8"", ""ignore"")\n\n\ndef get_args():\n    parser = argparse.ArgumentParser(description=""Remote Development Tool"")\n    parser.add_argument(""-g"", ""--getssh"", metavar=""job_name"", help=""get ssh info"", dest=""job_name"")\n    parser.add_argument(""-c"", ""--config"", metavar=\'job_path\', help=""submit local job"", dest=""job_path"")\n    parser.add_argument(""-s"", ""--share"", metavar=""share_path"", help=""share local folder"", dest=""share_path"")\n    parser.add_argument(""-v"", ""--verbose"", action=""store_true"", help=""verbose mode"", dest=\'need_verbose\')\n    args = parser.parse_args()\n    if len(sys.argv) < 2 or (len(sys.argv) == 2 and args.need_verbose is True):\n        parser.print_help()\n        parser.exit(-1)\n    if args.job_name is not None and args.job_path is not None:\n        LOGGER.info(""cannot use --submit and --getssh at the same time"")\n        parser.print_help()\n        parser.exit(-1)\n    if args.job_name is None and args.job_path is None:\n        LOGGER.info(""cannot use without both --submit and --getssh"")\n        parser.print_help()\n        parser.exit(-1)\n\n    return args\n\n\ndef get_ssh_info(job_name):\n    global LOGGER, SSH_INFO, USER_DIR, JOB_NAME\n    LOGGER.debug(""get SSH info"")\n    JOB_NAME = job_name\n    pai_dir = os.path.join(USER_DIR, "".openpai/"")\n\n    while True:\n        LOGGER.info(""wait for the ssh to start"")\n        output = run_subprocess(""opai job status -a remote_dev_bed {} ssh"".format(job_name))\n        ssh_port = re.findall(r""sshPort: \'(.+?)\'"", output)\n        ssh_ip = re.findall(r""sshIp: (.+?)\\n"", output)\n        ssh_link = re.findall(r""privateKeyDirectDownloadLink: (.+?)\\n"", output)\n        if len(ssh_ip) != 0 and len(ssh_port) != 0 and len(ssh_link) != 0:\n            break\n        time.sleep(10)\n    ssh_ip = re.sub(""\\r"", """", ssh_ip[0])\n    ssh_port = ssh_port[0]\n    ssh_link = re.sub(""\\r"", """", ssh_link[0])\n    LOGGER.debug(""download SSH key"")\n    req = requests.get(ssh_link)\n    ssh_key = os.path.abspath(""{}{}.key"".format(pai_dir, job_name))\n    with open(ssh_key, ""w+"") as f:\n        f.write(req.text)\n    f.close()\n\n    if IS_SHELL is True:\n        run_subprocess(""chmod 600 {}"".format(ssh_key))\n    ssh_cmd = ""ssh -i {} -p {} -o StrictHostKeyChecking=no root@{}"".format(ssh_key, ssh_port, ssh_ip)\n    LOGGER.info(""SSH IP: {}"".format(ssh_ip))\n    LOGGER.info(""SSH Port: {}"".format(ssh_port))\n    LOGGER.info(""SSH Key: {}"".format(ssh_key))\n    LOGGER.info(""SSH CMD: {}"".format(ssh_cmd))\n    SSH_INFO[\'ip\'] = ssh_ip\n    SSH_INFO[\'port\'] = ssh_port\n    SSH_INFO[\'key\'] = ssh_key\n    SSH_INFO[\'cmd\'] = ssh_cmd\n    configure_vscode()\n\n\ndef share_local_path(local_path):\n    global SSH_INFO\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    s.connect((""www.microsoft.com"", 80))\n    host_ip = s.getsockname()[0]\n    s.close()\n    if read_input(""Is your host ip {}"".format(host_ip)) is False:\n        host_ip = input(""Please input your host IP: "")\n    if IS_SHELL is False:\n        ms_username = getpass.getuser() + ""@microsoft.com""\n        if read_input(""Is your user name {}"".format(ms_username)) is False:\n            ms_username = input(""Please input your username: "")\n        ms_password = getpass.getpass(""Please input your password: "")\n        run_subprocess(r\'net share dev=""{}"" /GRANT:{},FULL\'.format(local_path, ms_username))\n        SSH_INFO[\'mount\'] = ""mount -t cifs //{}/dev /mnt/local -o vers=3.0,username={},password={}"".format(\n            host_ip,\n            ms_username,\n            ms_password)\n    else:\n        run_subprocess(r""docker stop remote_dev_nfs &> /dev/null || true"")\n        run_subprocess(r""docker rm remote_dev_nfs &> /dev/null || true"")\n        run_subprocess(r""docker run -itd --privileged --cap-add SYS_ADMIN --cap-add SYS_MODULE \\\n                -v /lib/modules:/lib/modules:ro \\\n                -v {}:/workspace \\\n                -v ~/.openpai/exports.yaml:/etc/exports:ro\\\n                -p 2049:2049 --name remote_dev_nfs \\\n                erichough/nfs-server &> /dev/null"".format(local_path))\n        SSH_INFO[\'mount\'] = ""mount -t nfs4 {}:/ /mnt/local"".format(host_ip)\n\n\ndef submit_job(job_path):\n    global JOB_NAME\n    LOGGER.debug(""submit job"")\n    job_name = """".join(random.sample(string.ascii_letters + string.digits, 10))\n    job_name = ""remote_dev_{}"".format(job_name)\n    JOB_NAME = job_name\n    run_subprocess(\n        ""opai job submit -a remote_dev_bed --update name={} {}"".format(job_name, job_path))\n\n    while True:\n        LOGGER.info(""wait for the job to run"")\n        output = run_subprocess(""opai job status -a remote_dev_bed {}"".format(job_name))\n        if output.find(""RUNNING"") != -1:\n            break\n        time.sleep(10)\n    LOGGER.info(""job name: {}"".format(job_name))\n    LOGGER.info(""job started"")\n    time.sleep(10)\n    get_ssh_info(job_name)\n\n\ndef configure_vscode():\n    LOGGER.debug(""configure vscode"")\n    vscode_dir = os.path.join(USER_DIR, "".ssh"", ""config"")\n    with open(vscode_dir, \'a+\') as f:\n        f.write(""\\nHost {}\\n"".format(JOB_NAME))\n        f.write(""    Hostname {}\\n"".format(SSH_INFO[""ip""]))\n        f.write(""    Port {}\\n"".format(SSH_INFO[\'port\']))\n        f.write(""    User root\\n"")\n        f.write(""    IdentityFile {}"".format(SSH_INFO[\'key\']))\n        f.close()\n\n\ndef start_ssh(share_path):\n    if share_path is not None:\n        run_subprocess(r\'{} ""apt update && apt install -y nfs-common cifs-utils""\'.format(SSH_INFO[\'cmd\']))\n        run_subprocess(r\'{} ""mkdir -p /mnt/local""\'.format(SSH_INFO[\'cmd\']))\n        run_subprocess(r\'{} ""{}""\'.format(SSH_INFO[\'cmd\'], SSH_INFO[\'mount\']))\n    subprocess.run(SSH_INFO[\'cmd\'], shell=IS_SHELL)\n    run_subprocess(""net share dev /delete || cd ."")\n\n\ndef main():\n    init()\n    args = get_args()\n    if args.need_verbose is True:\n        LOGGER.setLevel(logging.DEBUG)\n    if args.job_name is not None:\n        if args.job_name.find(""~"") != -1:\n            split_str = args.job_name.split(""~"");\n            if len(split_str) == 2:\n                get_ssh_info(split_str[1])\n            else:\n                LOGGER.error(""Wrong job name"")\n        else:\n            get_ssh_info(args.job_name)\n    if args.job_path is not None:\n        submit_job(args.job_path)\n    if args.share_path is not None:\n        share_local_path(args.share_path)\n    time.sleep(5)\n    start_ssh(args.share_path)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
contrib/storage_plugin/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
contrib/storage_plugin/storagectl.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport argparse\nimport datetime\nimport logging\nimport logging.config\nimport json\nimport base64\nimport subprocess\nimport multiprocessing\nimport random,string\n\nfrom kubernetes import client, config, watch\nfrom kubernetes.client.rest import ApiException\n\nfrom utils.storage_util import *\n\nimport binascii\n\nlogger = logging.getLogger(__name__)\n\n# Save server config to k8s secret\ndef save_secret(secret_name, name, content_dict):\n    secret_dict = dict()\n    secret_dict[name] = base64.b64encode(json.dumps(content_dict))\n    patch_secret(secret_name, secret_dict, ""pai-storage"")\n\ndef show_secret(args):\n    secret_data = get_secret(args.secret_name, ""pai-storage"")\n    if secret_data is None:\n        logger.error(""No secret found."")\n    else:\n        for key, value in secret_data.iteritems():\n            if args.name is None or key in args.name:\n                print(key)\n                print(base64.b64decode(value))\n\ndef delete_secret(args):\n    delete_secret_content(args.secret_name, args.name, ""pai-storage"")\n\n\ndef server_set(args):\n    content_dict = dict()\n    content_dict[""spn""] = args.name\n    content_dict[""type""] = args.server_type\n    if args.server_type == ""nfs"":\n        content_dict[""address""] = args.address\n        content_dict[""rootPath""] = args.root_path\n    elif args.server_type == ""samba"":\n        content_dict[""address""] = args.address\n        content_dict[""rootPath""] = args.root_path\n        content_dict[""userName""] = args.user_name\n        content_dict[""password""] = args.password\n        content_dict[""domain""] = args.domain\n    elif args.server_type == ""azurefile"":\n        content_dict[""dataStore""] = args.data_store\n        content_dict[""fileShare""] = args.file_share\n        content_dict[""accountName""] = args.account_name\n        content_dict[""key""] = args.key\n        if args.proxy is not None:\n            content_dict[""proxy""] = args.proxy\n    elif args.server_type == ""azureblob"":\n        content_dict[""dataStore""] = args.data_store\n        content_dict[""containerName""] = args.container_name\n        content_dict[""accountName""] = args.account_name\n        content_dict[""key""] = args.key\n    elif args.server_type == ""hdfs"":\n        content_dict[""namenode""] = args.namenode\n        content_dict[""port""] = args.port\n    else:\n        logger.error(""Unknow storage type"")\n        sys.exit(1)\n    save_secret(""storage-server"", args.name, content_dict)\n\n\ndef config_set(args):\n    try:\n        content_dict = dict()\n        content_dict[""name""] = args.name\n        content_dict[""servers""] = args.servers\n        content_dict[""default""] = args.default\n        if args.mount_info is not None:\n            mount_infos = []\n            for info_data in args.mount_info:\n                # Verify mount point, mountPoint should starts with ""/"" and path should not\n                if not info_data[0].startswith(""/""):\n                    raise NameError(""MOUNT_POINT should be absolute path and starts with \\\'/\\\'"")\n                elif info_data[2].startswith(""/""):\n                    raise NameError(""PATH should be relative path and not starts with \\\'/\\\'"")\n                else:\n                    info = {""mountPoint"" : info_data[0], ""server"" : info_data[1], ""path"" : info_data[2]}\n                    mount_infos.append(info)\n            content_dict[""mountInfos""] = mount_infos\n    except NameError as e:\n        logger.error(e)\n    else:\n        save_secret(""storage-config"", args.name, content_dict)\n\ndef get_group_extension(group_name):\n    group_hex = binascii.hexlify(group_name)\n    secret_data = get_secret(group_hex, ""pai-group"")\n    if secret_data is None:\n        logger.error(""No group found."")\n        return None\n    else:\n        extension = json.loads(base64.b64decode(secret_data[""extension""]))\n        return extension\n\ndef groupsc_add(args):\n    extension = get_group_extension(args.group_name)\n    if extension is not None:\n        if ""storageConfigs"" not in extension[""acls""]:\n            extension[""acls""][""storageConfigs""] = []\n        storageConfigs = extension[""acls""][""storageConfigs""]\n        if args.config_name not in storageConfigs:\n            storageConfigs.append(args.config_name)\n            secret_dict = dict()\n            secret_dict[""extension""] = base64.b64encode(json.dumps(extension))\n            patch_secret(binascii.hexlify(args.group_name), secret_dict, ""pai-group"")\n            logger.info(""Successfully added storage config to group!"")\n\ndef groupsc_delete(args):\n    extension = get_group_extension(args.group_name)\n    if extension is not None:\n        storageConfigs = extension[""acls""][""storageConfigs""]\n        if args.config_name in storageConfigs:\n            storageConfigs.remove(args.config_name)\n            secret_dict = dict()\n            secret_dict[""extension""] = base64.b64encode(json.dumps(extension))\n            patch_secret(binascii.hexlify(args.group_name), secret_dict, ""pai-group"")\n            logger.info(""Successfully deleted storage config from group!"")\n\ndef groupsc_list(args):\n    extension = get_group_extension(args.group_name)\n    if extension is not None:\n        print(extension[""acls""][""storageConfigs""])\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s - %(name)s - %(levelname)s - %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\ndef main():\n    scriptFolder=os.path.dirname(os.path.realpath(__file__))\n    os.chdir(scriptFolder)\n\n    parser = argparse.ArgumentParser(description=""pai storage management tool"")\n    subparsers = parser.add_subparsers(help=\'Storage management cli\')\n\n    # ./storagectl.py server set|list|delete \n    server_parser = subparsers.add_parser(""server"", description=""Commands to manage servers."", formatter_class=argparse.RawDescriptionHelpFormatter)\n    server_subparsers = server_parser.add_subparsers(help=""Add/modify, list or delete server"")\n    # ./storgectl.py server set ...\n    server_set_parser = server_subparsers.add_parser(""set"")\n    server_set_parser.add_argument(""name"")\n    server_set_subparsers = server_set_parser.add_subparsers(help=""Add/modify storage types, currently support nfs, samba, azurefile and azureblob"")\n    # ./storagectl.py server set NAME nfs ADDRESS ROOTPATH\n    server_set_nfs_parser = server_set_subparsers.add_parser(""nfs"")\n    server_set_nfs_parser.add_argument(""address"", metavar=""address"", help=""Nfs remote address"")\n    server_set_nfs_parser.add_argument(""root_path"", metavar=""rootpath"", help=""Nfs remote root path"")\n    server_set_nfs_parser.set_defaults(func=server_set, server_type=""nfs"")\n    # ./storagectl.py server set NAME samba ADDRESS ROOTPATH USERNAME PASSWORD DOMAIN\n    server_set_samba_parser = server_set_subparsers.add_parser(""samba"")\n    server_set_samba_parser.add_argument(""address"", metavar=""address"", help=""Samba remote address"")\n    server_set_samba_parser.add_argument(""root_path"", metavar=""rootpath"", help=""Samba remote root path"")\n    server_set_samba_parser.add_argument(""user_name"", metavar=""username"", help=""Samba PAI username"")\n    server_set_samba_parser.add_argument(""password"", metavar=""password"", help=""Samba PAI password"")\n    server_set_samba_parser.add_argument(""domain"", metavar=""domain"", help=""Samba PAI domain"")\n    server_set_samba_parser.set_defaults(func=server_set, server_type=""samba"")\n    # ./storagectl.py server set NAME azurefile DATASTORE FILESHARE ACCOUNTNAME KEY [-p PROXY_ADDRESS PROXY_PASSWORD]\n    server_set_azurefile_parser = server_set_subparsers.add_parser(""azurefile"")\n    server_set_azurefile_parser.add_argument(""data_store"", metavar=""datastore"", help=""Azurefile data store"")\n    server_set_azurefile_parser.add_argument(""file_share"", metavar=""fileshare"", help=""Azurefile file share"")\n    server_set_azurefile_parser.add_argument(""account_name"", metavar=""accountname"", help=""Azurefile account name"")\n    server_set_azurefile_parser.add_argument(""key"", metavar=""key"", help=""Azurefile share key"")\n    server_set_azurefile_parser.add_argument(""-p"", ""--proxy"", dest=""proxy"", nargs=2, help=""Proxy to mount azure file: PROXY_INFO PROXY_PASSWORD"")\n    server_set_azurefile_parser.set_defaults(func=server_set, server_type=""azurefile"")\n    # ./storagectl.py server set NAME azureblob DATASTORE CONTAINERNAME ACCOUNTNAME KEY\n    server_set_azureblob_parser = server_set_subparsers.add_parser(""azureblob"")\n    server_set_azureblob_parser.add_argument(""data_store"", metavar=""datastore"", help=""Azureblob data store"")\n    server_set_azureblob_parser.add_argument(""container_name"", metavar=""containername"", help=""Azureblob container name"")\n    server_set_azureblob_parser.add_argument(""account_name"", metavar=""accountname"", help=""Azureblob account name"")\n    server_set_azureblob_parser.add_argument(""key"", metavar=""key"", help=""Azureblob share key"")\n    server_set_azureblob_parser.set_defaults(func=server_set, server_type=""azureblob"")\n    # ./storagectl.py server set NAME hdfs NAMENODE PORT\n    server_set_hdfs_parser = server_set_subparsers.add_parser(""hdfs"")\n    server_set_hdfs_parser.add_argument(""namenode"", metavar=""namenode"", help=""HDFS name node"")\n    server_set_hdfs_parser.add_argument(""port"", metavar=""port"", help=""HDFS name node port"")\n    server_set_hdfs_parser.set_defaults(func=server_set, server_type=""hdfs"")\n    # ./storagectl.py server list [-n SERVER_NAME_1, SERVER_NAME_2 ...]\n    server_list_parser = server_subparsers.add_parser(""list"")\n    server_list_parser.add_argument(""-n"", ""--name"", dest=""name"", nargs=""+"", help=""filter result by names"")\n    server_list_parser.set_defaults(func=show_secret, secret_name=""storage-server"")\n    # ./storagectl.py user delete SERVER_NAME\n    server_del_parser = server_subparsers.add_parser(""delete"")\n    server_del_parser.add_argument(""name"")\n    server_del_parser.set_defaults(func=delete_secret, secret_name=""storage-server"")\n\n    # ./storagectl.py config ...\n    config_parser = subparsers.add_parser(""config"", description=""Manage config"", formatter_class=argparse.RawDescriptionHelpFormatter)\n    config_subparsers = config_parser.add_subparsers(help=""Manage config"")\n    # ./storagectl.py config set CONFIG_NAME GROUP_NAME [-s SERVER_NAME_1 SERVER_NAME_2 ...] [-m MOUNT_POINT SERVER PATH]... [-d]\n    config_set_parser = config_subparsers.add_parser(""set"")\n    config_set_parser.add_argument(""name"", help=""Config name"")\n    config_set_parser.add_argument(""-s"", ""--server"", dest=""servers"", nargs=""+"", help=""-s SERVER_NAME_1 SERVER_NAME_2 ..."")\n    config_set_parser.add_argument(""-m"", ""--mountinfo"", dest=""mount_info"", nargs=3, action=""append"", help=""-m MOUNT_POINT SERVER SUB_PATH"")\n    config_set_parser.add_argument(""-d"", ""--default"", action=""store_true"", help=""Mount by default"")\n    config_set_parser.set_defaults(func=config_set)\n    # ./storagectl.py config list [-n CONFIG_NAME_1, CONFIG_NAME_2 ...] [-g GROUP_NAME_1, GROUP_NAME_2 ...]\n    config_list_parser = config_subparsers.add_parser(""list"")\n    config_list_parser.add_argument(""-n"", ""--name"", dest=""name"", nargs=""+"", help=""filter result by names"")\n    config_list_parser.add_argument(""-g"", ""--group"", dest=""group"", nargs=""+"", help=""filter result by groups"")\n    config_list_parser.set_defaults(func=show_secret, secret_name=""storage-config"")\n    # ./storagectl.py config delete CONFIG_NAME\n    config_del_parser = config_subparsers.add_parser(""delete"")\n    config_del_parser.add_argument(""name"")\n    config_del_parser.set_defaults(func=delete_secret, secret_name=""storage-config"")\n\n    # ./storagectl.py groupsc add|delete|list\n    groupsc_parser = subparsers.add_parser(""groupsc"", description=""Manage group storage config"", formatter_class=argparse.RawDescriptionHelpFormatter)\n    groupsc_subparsers = groupsc_parser.add_subparsers(help=""Manage group storage config"")\n    # ./storagectl.py groupsc add GROUP_NAME STORAGE_CONFIG_NAME\n    groupsc_add_parser = groupsc_subparsers.add_parser(""add"")\n    groupsc_add_parser.add_argument(""group_name"")\n    groupsc_add_parser.add_argument(""config_name"")\n    groupsc_add_parser.set_defaults(func=groupsc_add)\n    # ./storagectl.py groupsc delete GROUP_NAME STORAGE_CONFIG_NAME\n    groupsc_delete_parser = groupsc_subparsers.add_parser(""delete"")\n    groupsc_delete_parser.add_argument(""group_name"")\n    groupsc_delete_parser.add_argument(""config_name"")\n    groupsc_delete_parser.set_defaults(func=groupsc_delete)\n    # ./storagectl.py groupsc list GROUP_NAME\n    groupsc_list_parser = groupsc_subparsers.add_parser(""list"")\n    groupsc_list_parser.add_argument(""group_name"")\n    groupsc_list_parser.set_defaults(func=groupsc_list)\n\n    args = parser.parse_args()\n    args.func(args)\n\n\nif __name__ == ""__main__"":\n    setup_logger_config(logger)\n    main()\n'"
deployment/clusterObjectModel/__init__.py,0,b''
deployment/clusterObjectModel/cluster_object_model.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport importlib\nimport time\nimport logging\nimport logging.config\n\n\nfrom . import forward_compatibility\nfrom ..paiLibrary.common import file_handler\nfrom ..paiLibrary.common import directory_handler\nfrom ..paiLibrary.common import linux_shell\nfrom .mainParser import kubernetes as pai_com_kubernetes\nfrom .mainParser import machine as pai_com_machine\nfrom .mainParser import layout as pai_com_layout\n\n\npackage_directory_com = os.path.dirname(os.path.abspath(__file__))\n\n\nclass cluster_object_model:\n\n    def __init__(self, configuration_path):\n        self.logger = logging.getLogger(__name__)\n        self.configuration_path = configuration_path\n\n    def get_service_model_list(self):\n        sub_model_list = []\n\n        sub_dir_list = directory_handler.get_subdirectory_list(""{0}/../../src/"".format(package_directory_com))\n        for sub_dir_name in sub_dir_list:\n            parser_path = ""{0}/../../src/{1}/config/{2}.py"".format(package_directory_com, sub_dir_name, sub_dir_name.replace(""-"", ""_""))\n            if file_handler.file_exist_or_not(parser_path):\n                sub_model_list.append(sub_dir_name)\n        return sub_model_list\n\n    def get_service_parser(self, service_name, cluster_type):\n\n        sys.path.insert(0, \'{0}/../../src/{1}/config\'.format(package_directory_com, service_name))\n        default_path = ""{0}/../../src/{1}/config/{1}.yaml"".format(package_directory_com, service_name)\n\n        # Prepare Service Configuration\n        layout = self.layout\n        service_type = ""common""\n        default_service_cfg = {}\n        if file_handler.file_exist_or_not(default_path):\n            default_service_cfg = file_handler.load_yaml_config(default_path)\n        if default_service_cfg is not None and ""service_type"" in default_service_cfg:\n            service_type = default_service_cfg[""service_type""]\n\n        overwrite_service_cfg = {}\n        if self.overwrite_service_configuration is not None and service_name in self.overwrite_service_configuration:\n            overwrite_service_cfg = self.overwrite_service_configuration[service_name]\n        if ""service_type"" in overwrite_service_cfg:\n            service_type = overwrite_service_cfg[""service_type""]\n\n        if service_type != ""common"" and service_type != cluster_type:\n            return None\n\n        # Init parser instance\n        parser_module = importlib.import_module(service_name.replace(""-"", ""_""))\n        parser_class_name = service_name.replace(""-"", "" "").title().replace("" "", """")\n        service_parser_class = getattr(parser_module, parser_class_name)\n        parser_instance = service_parser_class(layout, overwrite_service_cfg, default_service_cfg)\n\n        sys.path.remove(\'{0}/../../src/{1}/config\'.format(package_directory_com, service_name))\n\n        return parser_instance\n\n    def load_config(self, parser_dict):\n        # Pre Validation\n        self.logger.info(""Begin to do pre-validation for each service parser."")\n        for key in parser_dict.iterkeys():\n            value = parser_dict[key]\n            self.logger.info(""Begin to do pre-validation of {0}"".format(key))\n            ok, msg = value.validation_pre()\n            if ok is False:\n                self.logger.error(msg)\n                sys.exit(1)\n            self.logger.info(""Pre-validation of {0} is passed"".format(key))\n        self.logger.info(""Pre-validation is successful!"")\n\n        cluster_object_model = dict()\n\n        # Generate object model\n        self.logger.info(""Begin to do generate cluster object model."")\n        for key in parser_dict.iterkeys():\n            value = parser_dict[key]\n            self.logger.info(""Begin to do generate object model of {0}."".format(key))\n            cluster_object_model[key] = value.run()\n            self.logger.info(""Object model of {0} is generated."".format(key))\n        self.logger.info(""Cluster Object Model is generated."")\n\n        # Post Validation\n        self.logger.info(""Begin to do post-validation."")\n        for key in parser_dict.iterkeys():\n            value = parser_dict[key]\n            self.logger.info(""Begin to do post-validation of {0}."".format(key))\n            ok, msg = value.validation_post(cluster_object_model)\n            if ok is False:\n                self.logger.error(msg)\n                sys.exit(1)\n            self.logger.info(""Post-validation of {0} is passed."".format(key))\n        self.logger.info(""Post-validation is successful!"")\n\n        return cluster_object_model\n\n    def service_config(self):\n        self.layout = file_handler.load_yaml_config(""{0}/layout.yaml"".format(self.configuration_path))\n        overwrite_service_configuration = file_handler.load_yaml_config(""{0}/services-configuration.yaml"".format(self.configuration_path))\n        self.overwrite_service_configuration, updated = forward_compatibility.service_configuration_convert(overwrite_service_configuration)\n        cluster_type = ""k8s""\n        \n        parser_dict = dict()\n        parser_dict[""layout""] = pai_com_layout.Layout(self.layout)\n\n        service_model_list = self.get_service_model_list()\n        for service_name in service_model_list:\n            parser = self.get_service_parser(service_name, cluster_type)\n            if parser is None:\n                continue\n            parser_dict[service_name] = parser\n\n        return self.load_config(parser_dict)\n\n    def kubernetes_config(self):\n        self.layout = file_handler.load_yaml_config(""{0}/layout.yaml"".format(self.configuration_path))\n\n        parser_dict = dict()\n\n        # init main parser\n        kubernetes_configuration = file_handler.load_yaml_config(""{0}/kubernetes-configuration.yaml"".format(self.configuration_path))\n        kubernetes_parser = pai_com_kubernetes.Kubernetes(self.layout, kubernetes_configuration)\n        parser_dict[""kubernetes""] = kubernetes_parser\n        parser_dict[""layout""] = pai_com_layout.Layout(self.layout)\n\n        return self.load_config(parser_dict)\n'"
deployment/clusterObjectModel/forward_compatibility.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport json\n\ndef transform(old_model, old_key, new_model, new_key):\n    old_key_list = old_key.split(\'.\')\n    new_key_list = new_key.split(\'.\')\n    old_dict = old_model\n    new_dict = new_model\n    if old_dict == None:\n        return\n    for key in old_key_list:\n        if key not in old_dict:\n            return\n        old_dict = old_dict[key]\n    for key in new_key_list[:-1]:\n        if key not in new_dict:\n            new_dict[key] = {}\n        new_dict = new_dict[key]\n    new_dict[new_key_list[-1]] = old_dict\n\ndef service_configuration_convert(service_configuration):\n\n    if ""hadoop"" not in service_configuration and ""rest-server"" in service_configuration:\n        return service_configuration_add_cluster_type(service_configuration)\n\n    new_configuration = {}\n\n    transform(service_configuration, ""drivers"", new_configuration, ""drivers"")\n    transform(service_configuration, ""webportal"", new_configuration, ""webportal"")\n    transform(service_configuration, ""pylon"", new_configuration, ""pylon"")\n\n    transform(service_configuration, ""cluster.cluster-id"", new_configuration, ""cluster.common.cluster-id"")\n    transform(service_configuration, ""cluster.data-path"", new_configuration, ""cluster.common.data-path"")\n    transform(service_configuration, ""cluster.docker-registry-info.docker-namespace"",\n              new_configuration, ""cluster.docker-registry.namespace"")\n    transform(service_configuration, ""cluster.docker-registry-info.docker-registry-domain"",\n              new_configuration, ""cluster.docker-registry.domain"")\n    transform(service_configuration, ""cluster.docker-registry-info.docker-username"",\n              new_configuration, ""cluster.docker-registry.username"")\n    transform(service_configuration, ""cluster.docker-registry-info.docker-password"",\n              new_configuration, ""cluster.docker-registry.password"")\n    transform(service_configuration, ""cluster.docker-registry-info.docker-tag"",\n              new_configuration, ""cluster.docker-registry.tag"")\n    transform(service_configuration, ""cluster.docker-registry-info.secret-name"",\n              new_configuration, ""cluster.docker-registry.secret-name"")\n\n    transform(service_configuration, ""restserver"", new_configuration, ""rest-server"")\n    transform(service_configuration, ""frameworklauncher"", new_configuration, ""yarn-frameworklauncher"")\n\n    transform(service_configuration, ""hadoop.virtualClusters"",\n              new_configuration, ""hadoop-resource-manager.virtualClusters"")\n    transform(service_configuration, ""prometheus.yarn_exporter_port"",\n              new_configuration, ""hadoop-resource-manager.yarn_exporter_port"")\n\n    transform(service_configuration, ""prometheus.prometheus-port"",\n              new_configuration, ""prometheus.port"")\n    transform(service_configuration, ""prometheus.scrape_interval"",\n              new_configuration, ""prometheus.scrape_interval"")\n\n    transform(service_configuration, ""prometheus.alerting.alert_receiver"",\n              new_configuration, ""alert-manager.receiver"")\n    transform(service_configuration, ""prometheus.alerting.alert_manager_port"",\n              new_configuration, ""alert-manager.port"")\n    transform(service_configuration, ""prometheus.alerting.smtp_url"",\n              new_configuration, ""alert-manager.smtp_url"")\n    transform(service_configuration, ""prometheus.alerting.smtp_from"",\n              new_configuration, ""alert-manager.smtp_from"")\n    transform(service_configuration, ""prometheus.alerting.smtp_auth_username"",\n              new_configuration, ""alert-manager.smtp_auth_username"")\n    transform(service_configuration, ""prometheus.alerting.smtp_auth_password"",\n              new_configuration, ""alert-manager.smtp_auth_password"")\n\n    transform(service_configuration, ""grafana.grafana-port"",\n              new_configuration, ""grafana.port"")\n\n    transform(service_configuration, ""prometheus.node-exporter-port"",\n              new_configuration, ""node-exporter.port"")\n\n    return service_configuration_add_cluster_type(new_configuration, True)\n\n\ndef service_configuration_add_cluster_type(service_configuration, converted = False):\n    if ""cluster"" not in service_configuration:\n        service_configuration[""cluster""] = {""common"": {""cluster-type"": ""k8s""}}\n        return service_configuration, True\n    else:\n        if ""common"" not in service_configuration[""cluster""]:\n            service_configuration[""cluster""][""common""] = {""cluster-type"": ""k8s""}\n            return service_configuration, True\n        else:\n            if ""cluster-type"" not in service_configuration[""cluster""][""common""]:\n                service_configuration[""cluster""][""common""][""cluster-type""] = ""k8s""\n                return service_configuration, True\n            else:\n                return service_configuration, converted\n'"
deployment/clusterObjectModel/service_config_update.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport time\nimport logging\nimport logging.config\nimport forward_compatibility\nfrom ..paiLibrary.common import file_handler\nfrom ..paiLibrary.common import directory_handler\nfrom ..paiLibrary.common import linux_shell\n\nclass ServiceConfigUpdate:\n\n    def __init__(self, config_path):\n        self.logger = logging.getLogger(__name__)\n        self.config_path = config_path\n\n\n    def run(self):\n        service_configuration = file_handler.load_yaml_config(""{0}/services-configuration.yaml"".format(self.config_path))\n        self.overwrite_service_configuration, updated = forward_compatibility.service_configuration_convert(service_configuration)\n\n        if updated is True:\n            self.logger.warning(""======================================================================="")\n            self.logger.warning(""============  Your service configuration is out of date. =============="")\n            self.logger.warning(""======================================================================="")\n            self.logger.warning(""============    Following Operation Will Be Performed    =============="")\n            self.logger.warning(""==== service-configuration.yaml -> service-configuraiton.yaml.old ====="")\n            self.logger.warning(""= a new service-configuraiton.yaml with latest format will be created ="")\n            self.logger.warning(""======================================================================="")\n\n            linux_shell.execute_shell(\n                ""mv {0}/services-configuration.yaml {0}/services-configuration.yaml.old"".format(self.config_path),\n                ""failed to mv the old services-configuration.yaml""\n            )\n            file_handler.dump_yaml_data(""{0}/services-configuration.yaml"".format(self.config_path), self.overwrite_service_configuration)\n\n            self.logger.warning(""======================================================================="")\n            self.logger.warning(""===============  Process will continue after 15s.    =================="")\n            self.logger.warning(""======================================================================="")\n            time.sleep(15)\n'"
deployment/confStorage/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
deployment/confStorage/conf_storage_util.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport errno\nimport logging\nimport logging.config\nimport yaml\n\nfrom pprint import pprint\n\nimport kubernetes.client\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes import client, config, watch\n\nfrom ..paiLibrary.common import kubernetes_handler\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_subdirectory_list(path):\n\n    return next(os.walk(path))[1]\n\n\ndef create_path(path):\n\n    if not os.path.exists(""{0}"".format(path)):\n        try:\n            os.makedirs(path)\n\n        except OSError as exc:\n            if exc.errno == errno.EEXIST and os.path.isdir(path):\n                logger.warning(""Failed to create path {0}, due to that the path exists."".format(path))\n            else:\n                sys.exit(1)\n\n\ndef read_file_from_path(file_path):\n    with open(file_path, ""r"") as fin:\n        file_data = fin.read().decode(\'utf-8\')\n    return file_data\n\n\ndef load_yaml_config(config_path):\n\n    with open(config_path, ""r"") as f:\n        cluster_data = yaml.load(f, yaml.SafeLoader)\n\n    return cluster_data\n\n\ndef write_generated_file(generated_file, file_path):\n\n    with open(file_path, ""w+"") as fout:\n        fout.write(generated_file.encode(\'utf-8\'))\n\n\ndef get_cluster_id(PAI_KUBE_CONFIG_DEFAULT_LOCATION):\n\n    resp = kubernetes_handler.get_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, ""pai-cluster-id"")\n    if resp is None:\n        return None\n\n    # return a string\n    return resp[""data""][""cluster-id""]\n\n\ndef update_cluster_id(PAI_KUBE_CONFIG_DEFAULT_LOCATION, cluster_id):\n\n    data_dict = dict()\n    data_dict[""cluster-id""] = cluster_id\n    kubernetes_handler.update_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, ""pai-cluster-id"", data_dict)\n\n\ndef get_conf_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION):\n\n    resp = kubernetes_handler.get_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, ""pai-configuration"")\n    if resp is None:\n        return None\n\n    # return a dict\n    return resp[""data""]\n\n\ndef update_conf_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, conf_data_dict):\n\n    kubernetes_handler.update_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, ""pai-configuration"", conf_data_dict)\n'"
deployment/confStorage/download.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport readline\nimport logging\nimport logging.config\n\nfrom . import conf_storage_util\nfrom ..utility import pai_version\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\nclass download_configuration:\n\n    def __init__(self, config_output_path, kube_config_path):\n\n        self.logger = logging.getLogger(__name__)\n\n        if kube_config_path != None:\n            self.KUBE_CONFIG_DEFAULT_LOCATION = kube_config_path\n        if config_output_path != None:\n            self.config_path = config_output_path\n        else:\n            self.config_path = "".""\n\n    def check_cluster_id(self):\n\n        cluster_id = conf_storage_util.get_cluster_id(self.KUBE_CONFIG_DEFAULT_LOCATION)\n\n        if cluster_id is None:\n            self.logger.error(""No cluster_id found in your cluster, which should be done the first time you upload your configuration."")\n            self.logger.error(""Please execute the command following!"")\n            self.logger.error(""paictl.py config push [-c /path/to/kubeconfig ] [-p /path/to/cluster/configuration | -e /path/to/external/storage/conf/path]"")\n            self.logger.error(""More detailed information, please refer to the following link."")\n            self.logger.error(""https://github.com/Microsoft/pai/blob/master/docs/paictl/paictl-manual.md"")\n            sys.exit(1)\n\n        user_input = raw_input(""Please input the cluster-id which you wanna operate: "")\n        if user_input != cluster_id:\n            self.logger.error(""Ops, maybe you find the wrong cluster. Please check your input and the target cluster."")\n            sys.exit(1)\n\n        self.logger.info(""Congratulations: Cluster-id checking passed."")\n\n    def download_cluster_configuration(self, local_path):\n\n        # cluster_id = conf_storage_util.get_cluster_id(self.KUBE_CONFIG_DEFAULT_LOCATION)\n        configuration_dict = conf_storage_util.get_conf_configmap(self.KUBE_CONFIG_DEFAULT_LOCATION)\n\n        if configuration_dict is None:\n            self.logger.error(""The configuration doesn\'t exists on your cluster. Please upload it first."")\n            sys.exit(1)\n\n        conf_storage_util.create_path(""{0}"".format(local_path))\n        for key in configuration_dict:\n            conf_storage_util.write_generated_file(configuration_dict[key], ""{0}/{1}"".format(local_path, key))\n\n    def run(self):\n\n        self.check_cluster_id()\n        pai_version.check_cluster_version()\n        self.download_cluster_configuration(self.config_path)\n'"
deployment/confStorage/environment.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nimport logging\nimport subprocess\nimport logging.config\n\nimport kubernetes.client\nfrom kubernetes import client, config, watch\nfrom kubernetes.client.rest import ApiException\n\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass environment_check:\n\n    def __init__(self):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.KUBE_CONFIG_DEFAULT_LOCATION = os.path.expanduser(""~/.kube/config"")\n        if os.environ.get(\'KUBECONFIG\', None) != None:\n            self.KUBE_CONFIG_DEFAULT_LOCATION = os.environ.get(\'KUBECONFIG\', None)\n\n\n\n    def execute_shell_return(self, shell_cmd, error_msg):\n\n        try:\n            subprocess.check_call(shell_cmd, shell=True)\n\n        except subprocess.CalledProcessError:\n            self.logger.error(error_msg)\n            return False\n\n        return True\n\n\n    def check_conf_exits(self):\n\n        if not os.path.isfile(self.KUBE_CONFIG_DEFAULT_LOCATION):\n            self.logger.error(\n                ""CHECKING FAILED: The path {0} doesn\'t exist."".format(self.KUBE_CONFIG_DEFAULT_LOCATION)\n            )\n            sys.exit(1)\n\n        self.logger.info(\n            ""CHECKING SUCCESSFULLY: Kubeconfig is found.""\n        )\n\n\n\n    def check_kubectl(self):\n\n        api_versions_cmd = ""kubectl api-versions""\n        error_msg = ""Failed to execute the command [ kubectl api-versions ]""\n        if self.execute_shell_return(api_versions_cmd, error_msg) == False:\n            self.logger.error(\n                ""CHECKING FAILED: There is something wrong with kubectl. Please check.""\n            )\n            sys.exit(1)\n        self.logger.info(\n            ""CHECKING SUCCESSFULLY: Kubectl is found. And execute it successfully.""\n        )\n\n\n\n    def check_python_kubernetes(self):\n\n        #configuration = kubernetes.client.Configuration()\n        core_api_instance = client.CoreApi()\n        try_count = 0\n\n        while True:\n\n            try:\n                self.logger.info(""Try to access to the target kubernetes cluster"")\n                config.load_kube_config(config_file=self.KUBE_CONFIG_DEFAULT_LOCATION)\n                api_response = core_api_instance.get_api_versions()\n                self.logger.info(str(api_response))\n                break\n\n            except ApiException as e:\n                self.logger.error(""Failed connect to k8s with python client."")\n                try_count = try_count + 1\n\n            if try_count == 3:\n                self.logger.error(""All 3 tries of connecting k8s with python client fails."")\n                sys.exit(1)\n\n            time.sleep(5)\n\n        self.logger.info(\n            ""CHECKING SUCCESSFULLY: Successfully access kubernetes through python client. ""\n        )\n\n\n\n    def run(self):\n\n        self.check_conf_exits()\n        self.check_kubectl()\n        self.check_python_kubernetes()\n'"
deployment/confStorage/get_cluster_id.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport logging\nimport logging.config\n\nfrom . import conf_storage_util\n\n\nclass get_cluster_id:\n\n    def __init__(self, kube_config_path):\n        self.logger = logging.getLogger(__name__)\n        self.kube_config_path = kube_config_path\n\n\n    def run(self):\n\n        cluster_id = conf_storage_util.get_cluster_id(self.kube_config_path)\n\n        if cluster_id is None:\n            self.logger.info(""Cluster-id hasn\'t been set. You could set it when push the cluster configuration into cluster."")\n        else:\n            self.logger.info(""Cluster-id is: {0}"".format(cluster_id))'"
deployment/confStorage/synchronization.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport logging.config\n\nfrom external_version_control.external_config import getting_external_config\nfrom external_version_control.storage_factory import get_external_storage\nfrom .upload import upload_configuration\nfrom ..clusterObjectModel.service_config_update import ServiceConfigUpdate\n\n\nclass synchronization:\n\n    def __init__(self, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        # Configuration for local conf\n        self.local_conf_path = None\n        # Configuration for configmap [Access to k8s through exist kube_config.]\n        self.kube_config_path = None\n        # Cluster Configuration of pai.\n        self.pai_cluster_configuration_path = None\n\n        # External storage configuration data\n        self.external_storage_configuration = None\n\n        # The config list which should be pushed into cluster.\n        self.config_push_list = None\n\n        if ""local_conf_path"" in kwargs and kwargs[""local_conf_path""] != None:\n            self.local_conf_path = kwargs[""local_conf_path""]\n\n        if ""kube_config_path"" in kwargs and kwargs[""kube_config_path""] != None:\n            self.kube_config_path = kwargs[""kube_config_path""]\n\n        if ""pai_cluster_configuration_path"" in kwargs and kwargs[""pai_cluster_configuration_path""] != None:\n            self.pai_cluster_configuration_path = kwargs[""pai_cluster_configuration_path""]\n\n        if ""config_push_list"" in kwargs and kwargs[""config_push_list""] != None:\n            self.config_push_list = kwargs[""config_push_list""]\n        else:\n            self.config_push_list = [\n                ""k8s-role-definition.yaml"",\n                ""kubernetes-configuration.yaml"",\n                ""layout.yaml"",\n                ""services-configuration.yaml""\n            ]\n\n\n\n    def get_external_storage_conf(self):\n        external_config = getting_external_config(\n            external_storage_conf_path = self.local_conf_path,\n            local_cluster_configuration = self.pai_cluster_configuration_path,\n            kube_config_path = self.kube_config_path\n        )\n        return external_config.get_latest_external_configuration()\n\n\n\n    def sync_data_from_source(self):\n\n        self.external_storage_configuration = self.get_external_storage_conf()\n        with get_external_storage(self.external_storage_configuration) as configuration_path:\n            self.logger.info(""The temporary cluster configuration path is : {0}"".format(configuration_path))\n\n            config_format_check = ServiceConfigUpdate(configuration_path)\n            config_format_check.run()\n\n            conf_uploader = upload_configuration(configuration_path, self.kube_config_path, self.config_push_list)\n            conf_uploader.run()\n            self.logger.info(""Cluster Configuration synchronization from external storage is successful."")\n\n'"
deployment/confStorage/upload.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport readline\nimport logging\nimport logging.config\nimport time\n\nfrom . import conf_storage_util\nfrom ..paiLibrary.common import file_handler, directory_handler\nfrom ..utility import pai_version\n\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\nclass upload_configuration:\n\n    def __init__(self, config_path, kube_confg_path, upload_list = None):\n\n        self.logger = logging.getLogger(__name__)\n        self.KUBE_CONFIG_DEFAULT_LOCATION = os.path.expanduser(""~/.kube/config"")\n        if kube_confg_path != None:\n            self.KUBE_CONFIG_DEFAULT_LOCATION = kube_confg_path\n        if upload_list != None:\n            self.upload_list = upload_list\n        else:\n            self.upload_list = [\n                ""k8s-role-definition.yaml"",\n                ""kubernetes-configuration.yaml"",\n                ""layout.yaml"",\n                ""services-configuration.yaml""\n            ]\n\n        self.config_path = config_path\n\n    def check_cluster_id(self):\n        service_config = conf_storage_util.load_yaml_config(""{0}/services-configuration.yaml"".format(self.config_path))\n        # Default cluster-id in our default configuration is pai.\n        cluster_id_in_config = \'pai\'\n        if \'cluster\' in service_config and \'common\' in service_config[\'cluster\'] and \'cluster-id\' in service_config[\'cluster\'][\'common\']:\n            cluster_id_in_config = service_config[\'cluster\'][\'common\'][\'cluster-id\']\n\n        cluster_id = conf_storage_util.get_cluster_id(self.KUBE_CONFIG_DEFAULT_LOCATION)\n\n        if cluster_id is None:\n            self.logger.warning(""No cluster-id found in your cluster."")\n            self.logger.warning(""cluster-id [ {0} ] in configuration will be updated into your cluster."".format(cluster_id_in_config))\n            if cluster_id_in_config == \'pai\':\n                self.logger.warning(""cluster_id [ pai ] is the default ID in configuration."")\n                self.logger.warning(""Because no cluster-id found in your configuration, it [pai] will be used."")\n            conf_storage_util.update_cluster_id(self.KUBE_CONFIG_DEFAULT_LOCATION, cluster_id_in_config)\n            self.logger.warning(""Waiting 5s to update cluster-id."")\n            time.sleep(5)\n            cluster_id = cluster_id_in_config\n\n        user_input = raw_input(""Please input the cluster-id which you wanna operate: "")\n        if user_input != cluster_id:\n            self.logger.error(""Ops, maybe you find the wrong cluster. Please check your input and the target cluster."")\n            sys.exit(1)\n\n        if cluster_id != cluster_id_in_config:\n            self.logger.warning(""Cluster ID\'s update is detected from your service-configuration!"")\n            self.logger.warning(""The old one will be overwrote!"")\n            self.logger.warning(""The new one is [ {0} ]!"".format(cluster_id_in_config))\n            conf_storage_util.update_cluster_id(self.KUBE_CONFIG_DEFAULT_LOCATION, cluster_id_in_config)\n            self.logger.warning(""Waiting 5s to update cluster-id."")\n            time.sleep(5)\n\n        self.logger.info(""Congratulations: Cluster-id checking passed."")\n        return True\n\n    def upload_latest_configuration(self):\n\n        conf_dict = dict()\n        for config_name in self.upload_list:\n            conf_dict[config_name] = conf_storage_util.read_file_from_path(""{0}/{1}"".format(self.config_path, config_name))\n        if file_handler.file_exist_or_not(""{0}/services-configuration.yaml.old"".format(self.config_path)) == True:\n            conf_dict[""services-configuration.yaml.old""] = conf_storage_util.read_file_from_path(\n                ""{0}/services-configuration.yaml.old"".format(self.config_path))\n        conf_storage_util.update_conf_configmap(self.KUBE_CONFIG_DEFAULT_LOCATION, conf_dict)\n\n    def run(self):\n\n        self.check_cluster_id()\n        pai_version.check_cluster_version()\n        self.upload_latest_configuration()\n'"
deployment/k8sPaiLibrary/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n'"
deployment/paiLibrary/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
deployment/test/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
deployment/test/test_maintainlib_common.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport unittest\nimport filecmp\nimport os\nimport yaml\nimport tarfile\nimport shutil\nimport sys\nimport logging\nimport logging.config\n\nfrom k8sPaiLibrary.maintainlib import common\n\n\nclass TestMaintainlibCommon(unittest.TestCase):\n\n    """"""\n    Test the common\'s api\n    """"""\n\n    def setUp(self):\n\n        try:\n\n            os.chdir(os.path.abspath(""test""))\n\n        except:\n\n            pass\n\n        configuration_path = ""test_logging.yaml""\n\n        if os.path.exists(configuration_path):\n            with open(configuration_path, \'rt\') as f:\n                logging_configuration = yaml.safe_load(f.read())\n\n            logging.config.dictConfig(logging_configuration)\n\n            logging.getLogger()\n\n\n\n    def tearDown(self):\n\n        try:\n\n            os.chdir(os.path.abspath(""..""))\n\n        except:\n\n            pass\n\n\n\n    def test_yaml_lod(self):\n        pass\n\n\n\n    def test_template2generated(self):\n\n        cluster_data = {\n            \'clusterinfo\' : {\n                \'testkey2\' : \'testkey2\'\n            },\n            \'testkey3\': \'testkey3\'\n        }\n\n        host_data = {\n            \'testkey1\': \'testkey1\'\n        }\n\n        template_data = common.read_template(""data/data_maintainlib_common/test.yaml"")\n        generated_data = common.generate_from_template(template_data, cluster_data, host_data)\n        common.write_generated_file(generated_data, ""data/data_maintainlib_common/output.yaml"")\n\n        self.assertTrue(\n            filecmp.cmp(\n                ""data/data_maintainlib_common/test.yaml"",\n                ""data/data_maintainlib_common/output.yaml""\n            )\n        )\n\n        os.remove(""data/data_maintainlib_common/output.yaml"")\n\n\n\n    def test_template2generated_dict(self):\n\n        cluster_data = {\n            \'clusterinfo\' : {\n                \'testkey2\' : \'testkey2\'\n            },\n            \'testkey3\': \'testkey3\'\n        }\n\n        host_data = {\n            \'testkey1\': \'testkey1\'\n        }\n\n        dict_map = {\n            ""hostcofig"": host_data,\n            ""clusterconfig"": cluster_data[\'clusterinfo\'],\n            ""cluster"": cluster_data\n        }\n\n        template_data = common.read_template(""data/data_maintainlib_common/test.yaml"")\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n        common.write_generated_file(generated_data, ""data/data_maintainlib_common/output.yaml"")\n\n        self.assertTrue(\n            filecmp.cmp(\n                ""data/data_maintainlib_common/test.yaml"",\n                ""data/data_maintainlib_common/output.yaml""\n            )\n        )\n\n        os.remove(""data/data_maintainlib_common/output.yaml"")\n\n\n\n    def test_package_common_1(self):\n\n        maintain_config = common.load_yaml_file(""test-maintain.yaml"")\n        cluster_object_model = common.load_yaml_file(""test-generated-cluster-object-model.yaml"")\n        node_config = cluster_object_model[\'layout\'][\'machine-list\'][\'worker-01\']\n\n        common.maintain_package_wrapper(cluster_object_model, maintain_config, node_config, ""unittest-common-1"")\n        self.assertTrue(os.path.exists(""parcel-center/1.2.3.2/unittest-common-1.tar""))\n\n        package = tarfile.open(""parcel-center/1.2.3.2/unittest-common-1.tar"", ""r:"")\n        package.extractall()\n        self.assertTrue(os.path.exists(""unittest-common-1/""))\n\n        target_file_list = [""testfile1.sh"", ""testfile2.sh""]\n        package_file_list = os.listdir(""unittest-common-1/"")\n        self.assertListEqual(sorted(target_file_list), sorted(package_file_list))\n        shutil.rmtree(""unittest-common-1/"")\n\n        common.maintain_package_cleaner(node_config)\n        self.assertFalse(os.path.exists(""parcel-center/1.2.3.2""))\n        self.assertTrue(os.path.exists(""parcel-center""))\n\n        shutil.rmtree(""parcel-center/"")\n\n\n\n    def test_package_common_2(self):\n\n        maintain_config = common.load_yaml_file(""test-maintain.yaml"")\n        cluster_object_model = common.load_yaml_file(""test-generated-cluster-object-model.yaml"")\n        node_config = cluster_object_model[\'layout\'][\'machine-list\'][\'worker-01\']\n\n        common.maintain_package_wrapper(cluster_object_model, maintain_config, node_config, ""unittest-common-2"")\n        self.assertTrue(os.path.exists(""parcel-center/1.2.3.2/unittest-common-2.tar""))\n\n        package = tarfile.open(""parcel-center/1.2.3.2/unittest-common-2.tar"", ""r:"")\n        package.extractall()\n        self.assertTrue(os.path.exists(""unittest-common-2/""))\n\n        target_file_list = [""testfile2.sh""]\n        package_file_list = os.listdir(""unittest-common-2/"")\n        self.assertListEqual(sorted(target_file_list), sorted(package_file_list))\n        shutil.rmtree(""unittest-common-2/"")\n\n        common.maintain_package_cleaner(node_config)\n        self.assertFalse(os.path.exists(""parcel-center/1.2.3.2""))\n        self.assertTrue(os.path.exists(""parcel-center""))\n\n        shutil.rmtree(""parcel-center/"")\n\n\n\n    def test_package_common_3(self):\n\n        maintain_config = common.load_yaml_file(""test-maintain.yaml"")\n        cluster_object_model = common.load_yaml_file(""test-generated-cluster-object-model.yaml"")\n        node_config = cluster_object_model[\'layout\'][\'machine-list\'][\'worker-01\']\n\n\n        common.maintain_package_wrapper(cluster_object_model, maintain_config, node_config, ""unittest-common-3"")\n        self.assertTrue(os.path.exists(""parcel-center/1.2.3.2/unittest-common-3.tar""))\n\n\n        package = tarfile.open(""parcel-center/1.2.3.2/unittest-common-3.tar"", ""r:"")\n        package.extractall()\n        self.assertTrue(os.path.exists(""unittest-common-3/""))\n\n\n        target_file_list = [""testfile1.sh""]\n        package_file_list = os.listdir(""unittest-common-3/"")\n        self.assertListEqual(sorted(target_file_list), sorted(package_file_list))\n        shutil.rmtree(""unittest-common-3/"")\n\n        common.maintain_package_cleaner(node_config)\n        self.assertFalse(os.path.exists(""parcel-center/1.2.3.2""))\n        self.assertTrue(os.path.exists(""parcel-center""))\n\n        shutil.rmtree(""parcel-center/"")\n\n\n\n    def test_ipv4_address_validation_correct(self):\n\n        addr1 = ""128.0.0.x""\n        self.assertFalse(common.ipv4_address_validation(addr1))\n\n        addr2 = ""256.0.0.0""\n        self.assertFalse(common.ipv4_address_validation(addr2))\n\n        addr3 = ""128.0.0.1""\n        self.assertTrue(common.ipv4_address_validation(addr3))\n\n        addr4 = ""127.0.0.1""\n        self.assertTrue(common.ipv4_address_validation(addr4))\n\n        addr5 = ""mydefaultip""\n        self.assertFalse(common.ipv4_address_validation(addr5))\n\n        addr6 = ""localhost""\n        self.assertFalse(common.ipv4_address_validation(addr6))\n\n        addr7 = ""0.-1.0.0""\n        self.assertFalse(common.ipv4_address_validation(addr7))\n\n        addr8 = ""2001:0db8:85a3:0000:0000:8a2e:0370:7334""\n        self.assertFalse(common.ipv4_address_validation(addr8))\n\n\n\n    def test_port_validation(self):\n\n        port1 = 22\n        self.assertTrue(common.port_validation(port1))\n\n        port2 = ""232""\n        self.assertTrue(common.port_validation(port2))\n\n        port3 = ""12xxx""\n        self.assertFalse(common.port_validation(port3))\n\n        port4 = ""65536""\n        self.assertFalse(common.port_validation(port4))\n\n        port5 = ""-22""\n        self.assertFalse(common.port_validation(port5))\n\n        port6 = 0\n        self.assertTrue(common.port_validation(port6))\n\n\n\n\n\nif __name__ == \'__main__\':\n\n\n    unittest.main()\n'"
deployment/test/test_maintainlib_etcdfix.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport unittest\nimport filecmp\nimport os\nimport yaml\nimport tarfile\nimport shutil\nimport sys\nimport logging\nimport logging.config\n\nfrom k8sPaiLibrary.maintainlib import etcdfix\nfrom k8sPaiLibrary.maintainlib import common\n\n\nclass TestMaintainlibEtcdFix(unittest.TestCase):\n\n    """"""\n        Test the EtcdFix\'s api\n    """"""\n\n    def setUp(self):\n\n        try:\n\n            os.chdir(os.path.abspath(""test""))\n\n        except:\n\n            pass\n\n        configuration_path = ""test_logging.yaml""\n\n        if os.path.exists(configuration_path):\n            with open(configuration_path, \'rt\') as f:\n                logging_configuration = yaml.safe_load(f.read())\n\n            logging.config.dictConfig(logging_configuration)\n\n            logging.getLogger()\n\n\n\n    def tearDown(self):\n\n        try:\n\n            os.chdir(os.path.abspath(""..""))\n\n        except:\n\n            pass\n\n\n\n    def test_etcdfix_conf_validation_node_config_validation(self):\n\n        node_list = common.load_yaml_file(""data/data_maintainlib_etcdfix/test_node_list_config.yaml"")\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-ok.yaml"")\n\n        node_config = node_list[\'machinelist\'][\'ok-machine-node\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertTrue(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'miss-node-name\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'miss-host-ip\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'wrong-host-ip\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'wrong-ssh-port\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'miss-user-name\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'miss-password\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n        node_config = node_list[\'machinelist\'][\'miss-etcd-id\']\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.node_conf_validation())\n\n\n\n    def test_etcdfix_conf_validation_cluster_config_validation(self):\n\n        node_list = common.load_yaml_file(""data/data_maintainlib_etcdfix/test_node_list_config.yaml"")\n        node_config = node_list[\'machinelist\'][\'ok-machine-node\']\n\n\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-ok.yaml"")\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertTrue(validation.cluster_conf_validation())\n\n\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-miss-master.yaml"")\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.cluster_conf_validation())\n\n\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-miss-node-config.yaml"")\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.cluster_conf_validation())\n\n\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-wrong-node-config.yaml"")\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.cluster_conf_validation())\n\n\n        cluster_config = common.load_yaml_file(""data/data_maintainlib_etcdfix/generated-cluster-object-model-inconsistent-node-config.yaml"")\n        validation = etcdfix.etcdfix_conf_validation(cluster_config, node_config)\n        self.assertFalse(validation.cluster_conf_validation())\n'"
deployment/tools/__init__.py,0,b''
deployment/tools/configMigration.py,0,"b'#!/usr/bin/env python\n\nimport shutil\nimport yaml\nimport os\nimport sys\nsys.path.extend("".."")\nfrom deployment.clusterObjectModel import forward_compatibility\nfrom deployment.utility import pai_version\n\nprint(""This script is used for migrating config to v0.11!"")\nprint(""Usage: configMigration.py from_directory to_directory"")\n\ninput_dir = os.path.expanduser(sys.argv[1])\noutput_dir = os.path.expanduser(sys.argv[2])\nif not os.path.exists(output_dir):\n    os.makedirs(output_dir)\nprint(""Input directory: {}"".format(input_dir))\nprint(""Output directory: {}"".format(output_dir))\n\n# generate layout.yaml\nold_cluster_config_file = os.path.join(input_dir, ""cluster-configuration.yaml"")\nif os.path.isfile(old_cluster_config_file):\n    # Configuration before v0.9\n    print(""Migrating layout.yaml..."")\n    old_cluster_config = yaml.load(open(old_cluster_config_file), yaml.SafeLoader)\n    default_properties = old_cluster_config[""default-machine-properties""]\n    layout = {""machine-list"": []}\n    for node in old_cluster_config[""machine-list""]:\n        new_node_config = {}\n        new_node_config.update(default_properties)\n        new_node_config.update(node)\n        new_node_config[""nodename""] = node[""hostip""]\n        layout[""machine-list""].append(new_node_config)\n    layout[""machine-sku""] = old_cluster_config[""machine-sku""]\n    # read api-server ip from kubernetes-configuration.yaml\n    kubernetes_config = yaml.load(open(os.path.join(input_dir, ""kubernetes-configuration.yaml"")), yaml.SafeLoader)\n    api_servers_schema = kubernetes_config[""kubernetes""][""api-servers-http-schema""] if kubernetes_config[""kubernetes""].get(""api-servers-http-schema"") is not None else ""http""\n    api_servers_port = kubernetes_config[""kubernetes""][""api-servers-port""] if kubernetes_config[""kubernetes""].get(""api-servers-port"") is not None else ""8080""\n    # got dashboard ip\n    dashboard_ip = """"\n    for node in old_cluster_config[""machine-list""]:\n        if ""dashboard"" in node and (node[""dashboard""] == ""true""):\n            dashboard_ip = node[""hostip""]\n            break\n    # assign api-servers-url and dashboard-url\n    layout[""kubernetes""] = {\n        ""api-servers-url"": api_servers_schema + ""://""+kubernetes_config[""kubernetes""][""load-balance-ip""]+"":"" + api_servers_port,\n        ""dashboard-url"": ""http://""+dashboard_ip+"":9090""}\n\n    # write out the file\n    layout_file = os.path.join(output_dir, ""layout.yaml"")\n    if os.path.exists(layout_file):\n        print(""!!! Error, file already existing: {}"".format(layout_file))\n        sys.exit(-1)\n\n    with open(layout_file, \'w\') as outfile:\n        yaml.dump(layout, outfile, default_flow_style=False)\nelse:\n    # Configuration after v0.10\n    shutil.copy2(os.path.join(input_dir, ""layout.yaml""), output_dir)\n\n\n# copy kubernetes-configuration.yaml, k8s-role-definition.yaml\nshutil.copy2(os.path.join(input_dir, ""kubernetes-configuration.yaml""), output_dir)\nshutil.copy2(os.path.join(input_dir, ""k8s-role-definition.yaml""), output_dir)\n\n# convert service-configuration.yaml\nold_service_configuration = yaml.load(open(os.path.join(input_dir, ""services-configuration.yaml"")), yaml.SafeLoader)\nservice_configuration, updated = forward_compatibility.service_configuration_convert(old_service_configuration)\n# upgrade image tag version to v0.10.1\nservice_configuration[""cluster""][""docker-registry""][""tag""] = pai_version.paictl_version()\nwith open(os.path.join(output_dir, ""services-configuration.yaml""), \'w\') as outfile:\n    yaml.dump(service_configuration, outfile, default_flow_style=False)\n'"
deployment/tools/pluginIdMigration.py,0,"b'#!/usr/bin/env python\n\nimport shutil\nimport yaml\nimport os\nimport sys\n\ntry:\n    from slugify import slugify\nexcept:\n    os.system(""pip install python-slugify"")\n    from slugify import slugify\n\n\nprint(""This script is used for migrating config from v0.10 to v0.11!"")\nprint(""Usage: pluginIdMigration.py from_directory to_directory"")\n\ninput_dir = os.path.expanduser(sys.argv[1])\noutput_dir = os.path.expanduser(sys.argv[2])\nprint ""Input directory:"", input_dir\nprint ""Output directory:"", output_dir\n\n# Migrate services-configuration.yaml\nprint(""Migrating services-configuration.yaml..."")\nold_services_config_file = os.path.join(input_dir, ""services-configuration.yaml"")\nservices_config = yaml.load(open(old_services_config_file), yaml.SafeLoader)\nif ""webportal"" in services_config and ""plugins"" in services_config[""webportal""]:\n    plugins = services_config[""webportal""][""plugins""]\n    for plugin in plugins:\n        if ""id"" not in plugin:\n            plugin[""id""] = slugify(plugin[""title""])\nelse:\n    print ""No webportal plugins to convert""\n\n\n# write out the file\nservice_config_file = os.path.join(output_dir, ""services-configuration.yaml"")\n\nwith open(service_config_file, \'w\') as outfile:\n    yaml.dump(services_config, outfile, default_flow_style=False)\n\n\n# copy layout.yaml, kubernetes-configuration.yaml, k8s-role-definition.yaml\n\nshutil.copy2(os.path.join(input_dir, ""layout.yaml""), output_dir)\nshutil.copy2(os.path.join(input_dir, ""kubernetes-configuration.yaml""), output_dir)\nshutil.copy2(os.path.join(input_dir, ""k8s-role-definition.yaml""), output_dir)\n'"
deployment/utility/__init__.py,0,b''
deployment/utility/pai_version.py,0,"b'import os\nimport subprocess\nimport logging\nfrom ..paiLibrary.common import linux_shell\n\nlogger = logging.getLogger(__name__)\n\n\ndef paictl_version():\n    dirname = os.path.dirname(__file__)\n    version_file = os.path.join(dirname, \'../../version/PAI.VERSION\')\n    version = open(version_file, \'r\').readline()\n    return version.strip() # remove \'\\n\' from readline() https://docs.python.org/3/tutorial/inputoutput.html\n\n\ndef cluster_version():\n    version = """"\n    try:\n        version = subprocess.check_output(""kubectl get configmap pai-version -o jsonpath=\'{.data.PAI\\.VERSION}\'"", shell=True)\n    except subprocess.CalledProcessError:\n        logger.warning(""Can\'t fetch cluster version!"")\n    return version.strip() # same as paictl_version\n\n\ndef check_cluster_version():\n    c_version = cluster_version()\n    p_version = paictl_version()\n    logger.info(""Cluster version: %s, paictl version: %s"", c_version, p_version)\n    if p_version != c_version:\n        # TODO, now we only print a warning info\n        logger.warn(""!!! Paictl version is different from the cluster version: %s != %s"", c_version, p_version)\n        return False\n    return True\n'"
deployment/utility/sftp_copy.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom ..clusterObjectModel import cluster_object_model\nfrom ..k8sPaiLibrary.maintainlib import common\n\nimport sys\nimport readline\nimport logging\nimport logging.config\n\n\nclass OpenPaiSftpCopy:\n\n    def __init__(self, filename, source, dest, machine_list, filter):\n        self.filename = filename\n        self.source = source\n        self.dest = dest\n        self.origin_machine_list = machine_list\n        self.filter_rule = filter\n        self.machine_list = {}\n\n        self.logger = logging.getLogger(__name__)\n\n\n    def construct_machine_list(self):\n        rule_list = []\n        self.logger.info(""============================================="")\n        self.logger.info(""================ Filter Rule ================"")\n        self.logger.info(""============================================="")\n        if self.filter_rule != None:\n            for rule in self.filter_rule:\n                kv = rule.split(""="")\n                rule_list.append({""key"":kv[0], ""value"":kv[1]})\n                self.logger.info(""key = {0}, value = {1}"".format(kv[0], kv[1]))\n        else:\n            self.logger.info(""No filter rule."")\n        self.logger.info(""\\n"")\n        self.logger.info(""\\n"")\n\n        self.logger.info(""============================================="")\n        self.logger.info(""======= Machine List After filtered ========="")\n        self.logger.info(""============================================="")\n        for hostname in self.origin_machine_list:\n            host = self.origin_machine_list[hostname]\n            for rule in rule_list:\n                if rule[""key""] not in host:\n                    break\n                if host[rule[""key""]] != rule[""value""]:\n                    break\n            else:\n                self.machine_list[hostname] = host\n                self.logger.info(""Machine Host Name: {0},   Machine Ip Address: {1}"".format(hostname, host[""hostip""]))\n        self.logger.info(""\\n"")\n        self.logger.info(""\\n"")\n\n        count_input = 0\n        while True:\n            user_input = raw_input(""Do you want to continue this operation? (Y/N) "")\n            if user_input == ""N"":\n                sys.exit(1)\n            elif user_input == ""Y"":\n                break\n            else:\n                print("" Please type Y or N."")\n            count_input = count_input + 1\n            if count_input == 3:\n                self.logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                sys.exit(1)\n\n    def run(self):\n\n        self.construct_machine_list()\n\n        for hostname in self.machine_list:\n            host = self.machine_list[hostname]\n            if common.sftp_paramiko(self.source, self.dest, self.filename, host) == False:\n                self.logger.error(""[ Failed ]: Task on the machine [ hostname: {0},  ip-address: {1} ]"".format(hostname, host[""hostip""]))\n            else:\n                self.logger.info(""[ Successful ]: Task on the machine [ hostname: {0},  ip-address: {1} ]"".format(hostname, host[""hostip""]))\n'"
deployment/utility/ssh.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom ..clusterObjectModel import cluster_object_model\nfrom ..k8sPaiLibrary.maintainlib import common\nimport sys\nimport readline\nimport logging\nimport logging.config\n\n\nclass OpenPaiSSH:\n\n    def __init__(self, command, machine_list, filter):\n        self.cmd = command\n        self.origin_machine_list = machine_list\n        self.filter_rule = filter\n        self.machine_list = {}\n\n        self.logger = logging.getLogger(__name__)\n\n\n    def construct_machine_list(self):\n        rule_list = []\n        self.logger.info(""============================================="")\n        self.logger.info(""================ Filter Rule ================"")\n        self.logger.info(""============================================="")\n        if self.filter_rule != None:\n            for rule in self.filter_rule:\n                kv = rule.split(""="")\n                rule_list.append({""key"":kv[0], ""value"":kv[1]})\n                self.logger.info(""key = {0}, value = {1}"".format(kv[0], kv[1]))\n        else:\n            self.logger.info(""No filter rule."")\n        self.logger.info(""\\n"")\n        self.logger.info(""\\n"")\n\n        self.logger.info(""============================================="")\n        self.logger.info(""======= Machine List After filtered ========="")\n        self.logger.info(""============================================="")\n        for hostname in self.origin_machine_list:\n            host = self.origin_machine_list[hostname]\n            for rule in rule_list:\n                if rule[""key""] not in host:\n                    break\n                if host[rule[""key""]] != rule[""value""]:\n                    break\n            else:\n                self.machine_list[hostname] = host\n                self.logger.info(""Machine Host Name: {0},   Machine Ip Address: {1}"".format(hostname, host[""hostip""]))\n        self.logger.info(""\\n"")\n        self.logger.info(""\\n"")\n\n        count_input = 0\n        while True:\n            user_input = raw_input(""Do you want to continue this operation? (Y/N) "")\n            if user_input == ""N"":\n                sys.exit(1)\n            elif user_input == ""Y"":\n                break\n            else:\n                print("" Please type Y or N."")\n            count_input = count_input + 1\n            if count_input == 3:\n                self.logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                sys.exit(1)\n\n\n    def run(self):\n\n        self.construct_machine_list()\n\n        for hostname in self.machine_list:\n            host = self.machine_list[hostname]\n            if common.ssh_shell_with_password_input_paramiko(host, self.cmd) == False:\n                self.logger.error(""[ Failed ]: Task on the machine [ hostname: {0},  ip-address: {1} ]"".format(hostname, host[""hostip""]))\n            else:\n                self.logger.info(""[ Successful ]: Task on the machine [ hostname: {0},  ip-address: {1} ]"".format(hostname, host[""hostip""]))\n\n\n\n'"
src/cleaner/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/cleaner/cleaner_main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport time\nimport argparse\nimport os\nfrom datetime import timedelta\nfrom cleaner.scripts.clean_docker import DockerCleaner\nfrom cleaner.worker import Worker\nfrom cleaner.utils.logger import LoggerMixin\nfrom cleaner.utils import common\n\n\nclass Cleaner(LoggerMixin):\n\n    def __init__(self, liveness):\n        self.workers = {}\n        self.liveness = liveness\n\n    def add_worker(self, key, worker):\n        if key not in self.workers:\n            self.workers[key] = worker\n        else:\n            self.logger.warn(""worker with key %s already exists."", key)\n\n    def start(self):\n        for k, w in self.workers.items():\n            w.start()\n            self.logger.info(""worker %s started."", k)\n\n    def terminate(self):\n        for k, w in self.workers.items():\n            try:\n                # terminate the worker and all its subprocesses\n                common.kill_process_tree(w.pid, 5, self.logger)\n            except Exception as e:\n                self.logger.error(""errors occur when terminating worker %s."", k)\n                self.logger.exception(e)\n\n    def update_liveness(self):\n        if self.liveness:\n            file_name = os.path.join(""/tmp"", self.liveness)\n            with open(file_name, ""a""):\n                os.utime(file_name, None)\n\n    def sync(self):\n        try:\n            while True:\n                stopped_workers = [(k, w) for k, w in self.workers.items() if not w.is_alive()]\n                if len(stopped_workers) > 0:\n                    for k, w in stopped_workers:\n                        self.logger.error(""worker %s exit with code %s"", k, w.exitcode)\n                        self.workers.pop(k)\n                if len(self.workers) == 0:\n                    self.logger.info(""all workers are stopped and exit cleaner."")\n                    break\n                self.update_liveness()\n                time.sleep(2)\n        except Exception as e:\n            self.logger.exception(""cleaner interrupted and will exit."")\n            self.terminate()\n            time.sleep(1)\n\n\ndef get_worker(threshold):\n    worker = Worker(clean_docker.check_and_clean, threshold, timeout=timedelta(minutes=10), cool_down_time=60)\n    return worker;\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-t"", ""--threshold"", help=""the disk usage precent to start cleaner"")\n    parser.add_argument(""-i"", ""--interval"", help=""the base interval to check disk usage"")\n    args = parser.parse_args()\n\n    common.setup_logging()\n\n    cleaner = DockerCleaner(args.threshold, args.interval, timedelta(minutes=10))\n    cleaner.run()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/cleaner/worker.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom cleaner.utils.logger import LoggerMixin\nfrom cleaner.utils.timer import CountdownTimer, Timeout\nimport multiprocessing\nimport time\nfrom datetime import timedelta\n\n\nclass Worker(LoggerMixin, multiprocessing.Process):\n\n    def __init__(self, method, arg, timeout=timedelta(hours=1), long_run=True, cool_down_time=2):\n        super(Worker, self).__init__()\n        self.method = method\n        self.timeout = timeout\n        self.long_run = long_run\n        self.cool_down_time = cool_down_time\n        self.arg = arg\n\n    def _exec(self):\n        exc = None\n        method_name = self.method.__name__\n        try:\n            self.logger.info(""start to execute method %s."", method_name)\n            with CountdownTimer(duration=self.timeout):\n                self.method(self.arg)\n        except Timeout as e:\n            self.logger.error(""command %s timeout."", method_name)\n            exc = e\n        except Exception as e:\n            self.logger.error(""unexpected error to run method %s."", method_name)\n            exc = e\n\n        if exc is not None:\n            self.logger.exception(exc)\n\n    def run(self):\n        if self.method is None:\n            self.logger.error(""cannot start worker with empty method."")\n            return\n\n        if self.long_run and self.cool_down_time <= 0:\n            self.cool_down_time = 1\n            self.logger.warn(""input cool down time should be positive, will use value %d."", self.cool_down_time)\n\n        if self.long_run:\n            while True:\n                # allow a delay before the cleaning\n                time.sleep(self.cool_down_time)\n                self._exec()\n        else:\n            self._exec()\n'"
src/tools/node_maintain.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\n\nimport argparse\nimport time\nimport re\nimport logging\nimport copy\nimport sys\n\nfrom utility import log\nlog.setup_logging()\n\nfrom operator_wrapper import AlertOperator, KubernetesOperator, YarnOperator, Resource, RestserverOperator\n\nlogger = logging.getLogger(__name__)\n\ndef get_unready_nodes(decommissioned_nodes, current_status):\n    unready_nodes = {}\n    for node, state in current_status.items():\n        # should decommission but not\n        if state not in {""DECOMMISSIONED""} and node in decommissioned_nodes:\n            unready_nodes[node] = state\n        # should recommission but not\n        if state in {""DECOMMISSIONED"", ""DECOMMISSIONING""} and node not in decommissioned_nodes:\n            unready_nodes[node] = state\n    return unready_nodes\n\n\ndef validate_string_is_ip(validated_str):\n    ip_pattern = re.compile(r""^(1\\d{2}|2[0-4]\\d|25[0-5]|[1-9]\\d|[1-9])(\\.(1\\d{2}|2[0-4]\\d|25[0-5]|[1-9]\\d|\\d)){3}$"")\n    found = ip_pattern.match(validated_str) is not None\n    return found\n\n\ndef get_gpu_alert(args):\n    alert_operator = AlertOperator(args.prometheus_ip, args.prometheus_port)\n    alerting_nodes = alert_operator.get_gpu_alert_nodes()\n    logger.info(""Successfully aggregate gpu alerts."")\n    if len(alerting_nodes) > 0:\n        output_info = \'\\n\'.join([node_name+\': \'+alert_type for node_name, alert_type in alerting_nodes.items()])\n    else:\n        output_info = ""No gpu alerting nodes""\n    print(output_info)\n\n\ndef get_decommission_nodes(args):\n    k8s_operator = KubernetesOperator(args.api_server_ip)\n    existing_nodes = k8s_operator.get_nodes()\n    logger.info(""Successfully aggregate blacklist info."")\n    if len(existing_nodes) > 0:\n        output_info = \',\'.join(existing_nodes)\n    else:\n        output_info = ""No blacklist nodes""\n    print(output_info)\n    return existing_nodes\n\n\ndef add_decommission_nodes(args):\n    k8s_operator = KubernetesOperator(args.api_server_ip)\n    existing_nodes = k8s_operator.get_nodes()\n    nodes = args.nodes\n    inter_list = existing_nodes & nodes\n    if len(inter_list) > 0:\n        logger.warning(""Try to add existing blacklist nodes: {}"".format(\',\'.join(inter_list)))\n    full_list = existing_nodes | nodes\n    k8s_operator.set_nodes(full_list)\n    logger.info(""Add node: {} to blacklist"".format(\',\'.join(args.nodes)))\n    return full_list\n\n\ndef remove_decommission_nodes(args):\n    k8s_operator = KubernetesOperator(args.api_server_ip)\n    existing_nodes = k8s_operator.get_nodes()\n    nodes = args.nodes\n    supplement_list = nodes - existing_nodes\n    if len(supplement_list) > 0:\n        logger.warning(""Try to remove non-existing blacklist nodes: {}"".format(\',\'.join(supplement_list)))\n    full_list = existing_nodes - nodes\n    k8s_operator.set_nodes(full_list)\n    logger.info(""Remove node: {} from blacklist"".format(\',\'.join(args.nodes)))\n    return full_list\n\n\ndef update_decommission_nodes(args):\n    k8s_operator = KubernetesOperator(args.api_server_ip)\n    nodes = args.nodes\n    k8s_operator.set_nodes(nodes)\n    logger.info(""Update blacklist nodes: {}"".format(\',\'.join(args.nodes)))\n    return nodes\n\n\ndef refresh_yarn_nodes(args):\n    k8s_operator = KubernetesOperator(args.api_server_ip)\n    yarn_operator = YarnOperator(args.resource_manager_ip)\n    while True:\n        yarn_operator.decommission_nodes()\n        node_info = yarn_operator.get_nodes_info()\n        current_status = {k: v[""state""] for k, v in node_info.items()}\n        decommissioned_nodes = k8s_operator.get_nodes()\n        unready_nodes = get_unready_nodes(decommissioned_nodes, current_status)\n        if len(unready_nodes) == 0:\n            break\n        unready_info = \',\'.join([node_name+"" in ""+status for node_name, status in unready_nodes.items()])\n        logger.info(""Unready nodes: {}. Waiting..."".format(unready_info))\n        time.sleep(30)\n    logger.info(""Successfully refresh nodes."")\n\n\ndef convert_nodes(nodes_str):\n    if isinstance(nodes_str, str):\n        nodes = set(nodes_str.split(\',\'))\n        for node in nodes:\n            if not validate_string_is_ip(node):\n                raise argparse.ArgumentTypeError(""Value has to be a comma-delimited ip list, but found {}"".format(node))\n        return nodes\n    return set()\n\n\ndef validate_vc_name(vc_name_str):\n    if re.match(r""^[A-Za-z0-9_]+$"", vc_name_str) is None:\n        raise argparse.ArgumentTypeError(""invalid vc name: {}. Only alphanumeric and _ allowed"".format(vc_name_str))\n    return vc_name_str\n\n\ndef is_dedicated_vc(queue_name, queue_attr):\n    # print(json.dumps(queue_attr, indent=2))\n    if queue_name == """" or queue_name == ""*"" or queue_attr[""defaultNodeLabelExpression""] != queue_name:\n        return False\n    if queue_name not in queue_attr[""capacities""] or queue_attr[""capacities""][queue_name][""maxCapacity""] != 100:\n        return False\n    return True\n\n\ndef get_resource_by_label(nodes_info):\n    labels_dict = {}\n    default_resource = Resource(**{""cpus"": 0, ""memory"": 0, ""gpus"": 0})\n    for node, info in nodes_info.items():\n        if info[""nodeLabel""] not in labels_dict:\n            labels_dict[info[""nodeLabel""]] = {\n                ""resource"": default_resource\n            }\n        labels_dict[info[""nodeLabel""]][""resource""] += info[""resource""]\n    return labels_dict\n\n\ndef get_dedicate_vc(args):\n    yarn_operator = YarnOperator(args.resource_manager_ip)\n    queues_info = yarn_operator.get_queues_info()\n    nodes_info = yarn_operator.get_nodes_info()\n    dedicate_queues = {queue_name: {""resource"": Resource(**{""cpus"": 0, ""memory"": 0, ""gpus"": 0}), ""nodes"": []} for queue_name, queue_info in queues_info.items() if\n                       is_dedicated_vc(queue_name, queue_info)}\n    if len(dedicate_queues) == 0:\n        logger.info(""No dedicated vc found"")\n        return\n\n    labeled_resources = get_resource_by_label(nodes_info)\n    for partition in labeled_resources:\n        if partition in dedicate_queues:\n            dedicate_queues[partition][""resource""] = labeled_resources[partition][""resource""]\n\n    for node in nodes_info:\n        if nodes_info[node][""nodeLabel""] in dedicate_queues:\n            dedicate_queues[nodes_info[node][""nodeLabel""]][""nodes""].append(node)\n    for queue_name, queue_attr in dedicate_queues.items():\n        print(queue_name + "":"")\n        print(""\\tNodes: "" + "","".join(queue_attr[""nodes""]))\n        print(""\\tResource: <CPUs:{}, Memory:{}MB, GPUs:{}>"".format(queue_attr[""resource""].cpus, queue_attr[""resource""].memory, queue_attr[""resource""].gpus))\n\n\ndef convert_percentage_to_gpus(queues_info, partition_resource):\n    new_queues_info = copy.deepcopy(queues_info)\n    for queue, info in new_queues_info.items():\n        p = info[""capacity""] / float(100)\n        info[""gpus""] = partition_resource.gpus * p\n    return new_queues_info\n\n\ndef convert_gpus_to_percentage(queues_info, partition_resource):\n    new_queues_info = copy.deepcopy(queues_info)\n    if partition_resource.gpus > 0:\n        for queue, info in new_queues_info.items():\n            gpus = info[""gpus""]\n            info[""capacity""] = float(gpus) / partition_resource.gpus * 100\n    return new_queues_info\n\n\ndef normalize_percentage(queues_info):\n    new_queues_info = copy.deepcopy(queues_info)\n    sum_percentage = 0\n    for queue, info in new_queues_info.items():\n        sum_percentage += info[""capacity""]\n\n    if sum_percentage != 100:\n        logger.warning(""Renormalize percentage to 100%, current: {}%"".format(sum_percentage))\n    new_queues_info[""default""][""capacity""] -= sum_percentage - 100\n\n    for queue, info in new_queues_info.items():\n        if queue != ""default"":\n            info[""maxCapacity""] = info[""capacity""]\n\n    return new_queues_info\n\n\ndef add_dedicate_vc(args):\n    yarn_operator = YarnOperator(args.resource_manager_ip)\n    restserver_operator = RestserverOperator(args.restserver_ip)\n    vc_name = args.vc_name\n    nodes = args.nodes\n\n    logger.info(""Adding cluster label..."")\n    existing_labels = yarn_operator.get_cluster_labels()\n    if vc_name in existing_labels:\n        logger.warning(""Label already exists: {}"".format(vc_name))\n    else:\n        yarn_operator.add_cluster_label(vc_name)\n\n    logger.info(""Adding dedicated vc..."")\n    queues_info = yarn_operator.get_queues_info()\n    if vc_name in queues_info:\n        logger.warning(""Virtual cluster already exists: {}. Adding node to it"".format(vc_name))\n    else:\n        restserver_operator.add_vc(vc_name)\n        yarn_operator.add_dedicated_queue(vc_name)\n\n    nodes_info = yarn_operator.get_nodes_info()\n    if len(nodes) > 0:\n        logger.info(""Labeling node..."")\n\n        if queues_info[""default""][""maxCapacity""] == 100 or queues_info[""default""][""maxCapacity""] > \\\n                queues_info[""default""][""capacity""]:\n            queues_info[""default""][""maxCapacity""] = 100.0\n\n        added_resource = Resource(**{""cpus"": 0, ""memory"": 0, ""gpus"": 0})\n        for node, info in nodes_info.items():\n            if node in nodes and info[""nodeLabel""] == """":\n                added_resource += info[""resource""]\n\n        default_partition_resource = get_resource_by_label(nodes_info)[""""][""resource""]\n        default_vc_percentage = queues_info[""default""][""capacity""] / 100.0\n        default_vc_resource = default_partition_resource * default_vc_percentage\n\n        if default_vc_resource.cpus < added_resource.cpus \\\n            or default_vc_resource.gpus < added_resource.gpus \\\n                or default_vc_resource.memory < added_resource.memory:\n            logger.error(""Default vc resource isn\'t enough for the dedicated vc, please free some resource"")\n            sys.exit(1)\n\n        new_default_partition_resource = default_partition_resource - added_resource\n        new_default_vc_resource = default_vc_resource - added_resource\n\n        queues_info_with_gpus = convert_percentage_to_gpus(queues_info, default_partition_resource)\n        queues_info_with_gpus[""default""][""gpus""] = new_default_vc_resource.gpus\n        new_queues_percentage = convert_gpus_to_percentage(queues_info_with_gpus, new_default_partition_resource)\n        new_queues_percentage = normalize_percentage(new_queues_percentage)\n        updated_dict = {}\n        for queue, info in new_queues_percentage.items():\n            updated_dict[queue] = {\n                ""capacity"": info[""capacity""],\n                ""maximum-capacity"": info[""maxCapacity""]\n            }\n            if queue != ""default"":\n                updated_dict[queue][""disable_preemption""] = True\n\n        yarn_operator.label_nodes(nodes, vc_name)\n        yarn_operator.update_queue_capacity(updated_dict)\n\n\ndef remove_dedicate_vc(args):\n    yarn_operator = YarnOperator(args.resource_manager_ip)\n    restserver_operator = RestserverOperator(args.restserver_ip)\n    vc_name = args.vc_name\n    nodes = args.nodes\n    remove_queue_flag = nodes is None\n\n    logger.info(""Unlabeling node..."")\n    nodes_info = yarn_operator.get_nodes_info()\n    queues_info = yarn_operator.get_queues_info()\n    if nodes is None:\n        nodes = set(nodes_info.keys())\n    t_nodes = [node for node in nodes if nodes_info[node][""nodeLabel""] == vc_name]\n    if len(t_nodes) > 0:\n\n        if queues_info[""default""][""maxCapacity""] == 100 or queues_info[""default""][""maxCapacity""] > \\\n                queues_info[""default""][""capacity""]:\n            queues_info[""default""][""maxCapacity""] = 100.0\n\n        removed_resource = Resource(**{""cpus"": 0, ""memory"": 0, ""gpus"": 0})\n        for node, info in nodes_info.items():\n            if node in nodes and info[""nodeLabel""] == vc_name:\n                removed_resource += info[""resource""]\n\n        default_partition_resource = get_resource_by_label(nodes_info)[""""][""resource""]\n        default_vc_percentage = queues_info[""default""][""capacity""] / 100.0\n        default_vc_resource = default_partition_resource * default_vc_percentage\n\n        new_default_partition_resource = default_partition_resource + removed_resource\n        new_default_vc_resource = default_vc_resource + removed_resource\n\n        queues_info_with_gpus = convert_percentage_to_gpus(queues_info, default_partition_resource)\n        queues_info_with_gpus[""default""][""gpus""] = new_default_vc_resource.gpus\n        new_queues_percentage = convert_gpus_to_percentage(queues_info_with_gpus, new_default_partition_resource)\n        new_queues_percentage = normalize_percentage(new_queues_percentage)\n        updated_dict = {}\n        for queue, info in new_queues_percentage.items():\n            updated_dict[queue] = {\n                ""capacity"": info[""capacity""],\n                ""maximum-capacity"": info[""maxCapacity""]\n            }\n\n        yarn_operator.label_nodes(t_nodes, """")\n        yarn_operator.update_queue_capacity(updated_dict)\n\n    if remove_queue_flag:\n        logger.info(""Removing dedicated vc..."")\n        if vc_name not in queues_info:\n            logger.warning(""Virtual cluster not found: {}."".format(vc_name))\n        else:\n            yarn_operator.remove_dedicated_queue(vc_name)\n            restserver_operator.delete_vc(vc_name)\n\n        logger.info(""Removing cluster label..."")\n        if vc_name not in yarn_operator.get_cluster_labels():\n            logger.warning(""Cluster label not found: {}"".format(vc_name))\n        else:\n            yarn_operator.remove_cluster_label(vc_name)\n\ndef setup_user(args):\n    username = args.username\n    password = args.password\n    RestserverOperator.setup_user(username, password)\n    logger.info(""Setup user done"")\n\n\ndef setup_parser():\n    top_parser = argparse.ArgumentParser()\n    sub_parser = top_parser.add_subparsers(dest=""subcommands"")\n\n    # a parent parser to avoid repeatedly add arguments for all subcommands\n    parent_parser = argparse.ArgumentParser(add_help=False)\n    parent_parser.add_argument(""-m"", ""--master"", dest=""master_ip"",\n                               help=""master node ip"", required=True)\n    parent_parser.add_argument(""--resource-manager-ip"",\n                               help=""specify yarn resource manager ip separately, by default it\'s master node ip"")\n    parent_parser.add_argument(""--api-server-ip"",\n                               help=""specify kubernetes api-server ip separately, by default it\'s master node ip"")\n    parent_parser.add_argument(""--prometheus-ip"",\n                               help=""specify prometheus ip separately, by default it\'s master node ip"")\n    parent_parser.add_argument(""--restserver-ip"",\n                               help=""specify restserver ip separately, by default it\'s master node ip"")\n    parent_parser.add_argument(""--prometheus-port"", default=9091,\n                               help=""specify prometheus port, by default it\'s 9091"")\n\n    # setup restserver user\n    user_parser = sub_parser.add_parser(""user"", help=""query prometheus alerts"")\n    user_subparsers = user_parser.add_subparsers(dest=""action"")\n\n    parser_set = user_subparsers.add_parser(""set"", parents=[parent_parser], help=""print current gpu alerts"")\n    parser_set.add_argument(""-u"", ""--username"", required=True)\n    parser_set.add_argument(""-p"", ""--password"", required=True)\n    parser_set.set_defaults(func=setup_user)\n\n    # prometheus operator parser\n    prometheus_parser = sub_parser.add_parser(""badgpus"", help=""query prometheus alerts"")\n    prometheus_subparsers = prometheus_parser.add_subparsers(dest=""action"")\n\n    parser_get = prometheus_subparsers.add_parser(""get"", parents=[parent_parser], help=""print current gpu alerts"")\n    parser_get.set_defaults(func=get_gpu_alert)\n\n    # blacklist parser\n    blacklist_parser = sub_parser.add_parser(""blacklist"", help=""blacklist operation"")\n    blacklist_subparsers = blacklist_parser.add_subparsers(dest=""action"")\n\n    parser_get = blacklist_subparsers.add_parser(""get"", parents=[parent_parser], help=""get blacklist nodes"")\n    parser_get.set_defaults(func=get_decommission_nodes)\n\n    parser_add = blacklist_subparsers.add_parser(""add"", parents=[parent_parser], help=""add nodes to blacklist"")\n    parser_add.add_argument(""-n"", ""--nodes"", type=convert_nodes, help=""support comma-delimited node list"", required=True)\n    parser_add.set_defaults(func=add_decommission_nodes)\n\n    parser_remove = blacklist_subparsers.add_parser(""remove"", parents=[parent_parser], help=""remove nodes from blacklist"")\n    parser_remove.add_argument(""-n"", ""--nodes"", type=convert_nodes, help=""support comma-delimited node list"", required=True)\n    parser_remove.set_defaults(func=remove_decommission_nodes)\n\n    parser_update = blacklist_subparsers.add_parser(""update"", parents=[parent_parser], help=""update blacklist"")\n    parser_update.add_argument(""-n"", ""--nodes"", type=convert_nodes, help=""support comma-delimited node list"")\n    parser_update.set_defaults(func=update_decommission_nodes)\n\n    parser_refresh = blacklist_subparsers.add_parser(""enforce"", parents=[parent_parser],\n                                                    help=""enforce yarn to gracefully decommission nodes in blacklist"")\n    parser_refresh.set_defaults(func=refresh_yarn_nodes)\n\n    # dedicated vc parser\n    dedicated_vc_parser = sub_parser.add_parser(""dedicated-vc"", help=""operate dedicated vc"")\n    dedicated_vc_subparsers = dedicated_vc_parser.add_subparsers(dest=""action"")\n\n    parser_get = dedicated_vc_subparsers.add_parser(""get"", parents=[parent_parser], help=""get dedicate vc info"")\n    parser_get.set_defaults(func=get_dedicate_vc)\n\n    parser_add = dedicated_vc_subparsers.add_parser(""add"", parents=[parent_parser], help=""add dedicate vc"")\n    parser_add.add_argument(""-n"", ""--nodes"", type=convert_nodes, help=""support comma-delimited node list"", default={})\n    parser_add.add_argument(""-v"", ""--vc-name"", type=validate_vc_name, required=True)\n    parser_add.set_defaults(func=add_dedicate_vc)\n\n    parser_remove = dedicated_vc_subparsers.add_parser(""remove"", parents=[parent_parser], help=""remove dedicate vc"")\n    parser_remove.add_argument(""-v"", ""--vc-name"", type=validate_vc_name, required=True)\n    parser_remove.add_argument(""-n"", ""--nodes"", type=convert_nodes, help=""support comma-delimited node list"")\n    parser_remove.set_defaults(func=remove_dedicate_vc)\n\n    return top_parser\n\n\ndef main():\n    parser = setup_parser()\n    args = parser.parse_args()\n    args.resource_manager_ip = args.resource_manager_ip or args.master_ip\n    args.api_server_ip = args.api_server_ip or args.master_ip\n    args.prometheus_ip = args.prometheus_ip or args.master_ip\n    args.restserver_ip = args.restserver_ip or args.master_ip\n    try:\n        args.func(args)\n    except Exception as e:\n        from subprocess import CalledProcessError\n        if isinstance(e, CalledProcessError):\n            logger.error(e.output)\n        else:\n            logger.exception(e)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/tools/reports.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND # NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport urllib.parse\nimport argparse\nimport logging\nimport datetime\nimport json\nimport collections\nimport re\nimport sys\nimport math\n\nimport sqlite3\nimport requests\n\nimport flask\nfrom flask import Flask\nfrom flask import request\nfrom flask import Response\n\nlogger = logging.getLogger(__name__)\n\n\ndef walk_json_field_safe(obj, *fields):\n    """""" for example a=[{""a"": {""b"": 2}}]\n    walk_json_field_safe(a, 0, ""a"", ""b"") will get 2\n    walk_json_field_safe(a, 0, ""not_exist"") will get None\n    """"""\n    try:\n        for f in fields:\n            obj = obj[f]\n        return obj\n    except:\n        return None\n\n\ndef request_with_error_handling(url):\n    try:\n        response = requests.get(url, allow_redirects=True, timeout=15)\n        response.raise_for_status()\n        return response.json()\n    except Exception as e:\n        logger.exception(e)\n        return None\n\n\ndef format_time(timestamp):\n    d = datetime.datetime.fromtimestamp(timestamp)\n    return d.strftime(""%Y/%m/%d-%H:%M:%S"")\n\n\ndef get_ip(ip_port):\n    """""" return 1.2.3.4 on 1.2.3.4:123 """"""\n    m = re.match(""([0-9]+[.][0-9]+[.][0-9]+[.][0-9]+):?.*"", ip_port)\n    if m:\n        return m.groups()[0]\n    return ip_port\n\n\nclass JobInfo(object):\n    def __init__(self, job_count=0, elapsed_time=0, cpu_sec=0, mem_sec=0, gpu_sec=0,\n            user=""unknown"", vc=""unknown"", start_time=0, finished_time=0, retries=0,\n            status=""unknown"", exit_code=""N/A"", max_mem_usage=""N/A""):\n        """""" elapsed_time is seconds, cpu_sec is vcore-seconds, mem_sec is\n        megabyte-seconds, gpu_sec is card-seconds """"""\n        self.job_count = job_count\n        self.elapsed_time = elapsed_time\n        self.cpu_sec = cpu_sec\n        self.mem_sec = mem_sec\n        self.gpu_sec = gpu_sec\n\n        self.user = user\n        self.vc = vc\n        self.start_time = start_time\n        self.finished_time = finished_time\n        self.retries = retries\n        self.status = status\n        self.exit_code = exit_code\n        self.max_mem_usage = max_mem_usage\n\n    def __iadd__(self, o):\n        self.job_count += o.job_count\n        self.elapsed_time += o.elapsed_time\n        self.cpu_sec += o.cpu_sec\n        self.mem_sec += o.mem_sec\n        self.gpu_sec += o.gpu_sec\n        return self\n\n    def __add__(self, o):\n        return JobInfo(\n                job_count=self.job_count + o.job_count,\n                elapsed_time=self.elapsed_time + o.elapsed_time,\n                cpu_sec=self.cpu_sec + o.cpu_sec,\n                mem_sec=self.mem_sec + o.mem_sec,\n                gpu_sec=self.gpu_sec + o.gpu_sec)\n\n    def values(self):\n        return [self.job_count, self.elapsed_time,\n                self.cpu_sec, self.mem_sec, self.gpu_sec]\n\n    def __repr__(self):\n        # NOTE this is used to generate final report\n        return "","".join(map(str, self.values()))\n\n\nclass JobReportEntries(object):\n    def __init__(self, username, vc, total_job_info, success_job_info,\n            failed_job_info, stopped_job_info, running_job_info, waiting_job_info):\n        self.username = username\n        self.vc = vc\n        self.total_job_info = total_job_info\n        self.success_job_info = success_job_info\n        self.failed_job_info = failed_job_info\n        self.stopped_job_info = stopped_job_info\n        self.running_job_info = running_job_info\n        self.waiting_job_info = waiting_job_info\n\n    def values(self):\n        result = [self.username, self.vc]\n        result.extend(self.total_job_info.values())\n        result.extend(self.success_job_info.values())\n        result.extend(self.failed_job_info.values())\n        result.extend(self.stopped_job_info.values())\n        result.extend(self.running_job_info.values())\n        result.extend(self.waiting_job_info.values())\n        return result\n\n    def __repr__(self):\n        # NOTE this is used to generate final report\n        return "","".join(map(str, self.values()))\n\n\nclass RawJob(object):\n    def __init__(self, user, vc, job,\n            start_time, finish_time, waiting_time, run_time,\n            retries, status, exit_code, cpu, mem, max_mem, gpu):\n        self.user = user\n        self.vc = vc\n        self.job = job\n        self.start_time = start_time\n        self.finish_time = finish_time\n        self.waiting_time = waiting_time\n        self.run_time = run_time\n        self.retries = retries\n        self.status = status\n        self.exit_code = exit_code\n        self.cpu = cpu\n        self.mem = mem\n        self.max_mem = max_mem\n        self.gpu = gpu\n\n    def values(self):\n        return [self.user, self.vc, self.job,\n                self.start_time, self.finish_time, self.waiting_time, self.run_time,\n                self.retries, self.status, self.exit_code,\n                self.cpu, self.mem, self.max_mem, self.gpu]\n\n    def __repr__(self):\n        # NOTE this is used to generate final report\n        return "","".join(map(str, self.values()))\n\n\nclass Alert(object):\n    default_get_ip = lambda a: get_ip(a[""instance""])\n    host_ip_mapping = {\n            ""NodeNotReady"": lambda a: get_ip(a[""name""]),\n            ""k8sApiServerNotOk"": lambda a: get_ip(a[""host_ip""]),\n            ""NodeDiskPressure"": lambda a: get_ip(a[""name""]),\n            ""NodeNotReady"": lambda a: get_ip(a[""name""]),\n            ""PaiServicePodNotRunning"": lambda a: get_ip(a[""host_ip""]),\n            ""PaiServicePodNotReady"": lambda a: get_ip(a[""host_ip""]),\n            }\n\n    src_mapping = {\n            ""NvidiaSmiEccError"": lambda a: a[""minor_number""],\n            ""NvidiaMemoryLeak"": lambda a: a[""minor_number""],\n            ""GpuUsedByExternalProcess"": lambda a: a[""minor_number""],\n            ""GpuUsedByZombieContainer"": lambda a: a[""minor_number""],\n            ""k8sApiServerNotOk"": lambda a: a[""error""],\n            ""k8sDockerDaemonNotOk"": lambda a: a[""error""],\n            ""NodeFilesystemUsage"": lambda a: a[""device""],\n            ""NodeDiskPressure"": lambda a: get_ip(a[""name""]),\n            ""NodeNotReady"": lambda a: get_ip(a[""name""]),\n            ""AzureAgentConsumeTooMuchMem"": lambda a: a[""cmd""],\n            ""PaiServicePodNotRunning"": lambda a: a[""name""],\n            ""PaiServicePodNotReady"": lambda a: a[""name""],\n            ""PaiServiceNotUp"": lambda a: a[""pai_service_name""],\n            ""JobExporterHangs"": lambda a: a[""name""],\n            }\n\n    def __init__(self, alert_name, start, durtion, labels):\n        """""" alert_name are derived from labels, start/durtion is timestamp\n        value """"""\n        self.alert_name = alert_name\n        self.start = start\n        self.durtion = durtion\n        self.labels = labels\n\n        #f.write(""alert_name,host_ip,source,start,durtion,labels\\n"")\n\n    @staticmethod\n    def get_info(alert_name, labels, mapping):\n        return mapping.get(alert_name, Alert.default_get_ip)(labels)\n\n    def labels_repr(self):\n        r = []\n        for k, v in self.labels.items():\n            if k in {""__name__"", ""alertname"", ""alertstate"", ""job"", ""type""}:\n                continue\n            r.append(""%s:%s"" % (k, v))\n        return ""|"".join(r)\n\n    def values(self):\n        return [self.alert_name,\n                Alert.get_info(self.alert_name, self.labels, Alert.host_ip_mapping),\n                Alert.get_info(self.alert_name, self.labels, Alert.src_mapping),\n                format_time(self.start),\n                self.durtion,\n                self.labels_repr()]\n\n    def __repr__(self):\n        # NOTE this is used to generate final report\n        return "","".join(map(str, self.values()))\n\n\nclass GPUEntry(object):\n    def __init__(self, node_ip, gpu_id, avg_util):\n        self.node_ip = node_ip\n        self.gpu_id = gpu_id\n        self.avg_util = avg_util\n\n    def values(self):\n        return [self.node_ip, self.gpu_id, self.avg_util]\n\n    def __repr__(self):\n        # NOTE this is used to generate final report\n        return "","".join(map(str, self.values()))\n\n\nclass DB(object):\n    # If app is running, the finished_time is 0, should not delete it in delete_old_data\n    CREATE_APPS_TABLE = """"""CREATE TABLE IF NOT EXISTS apps (\n                            app_id text NOT NULL,\n                            finished_time integer NOT NULL,\n                            content text NOT NULL\n                            )""""""\n    CREATE_APP_ID_INDEX = ""CREATE INDEX IF NOT EXISTS app_id_index ON apps (app_id);""\n    CREATE_APP_TIME_INDEX = ""CREATE INDEX IF NOT EXISTS app_time_index ON apps (finished_time);""\n\n    # If job is running, the finished_time is 0, should not delete it in delete_old_data\n    CREATE_FRAMEWORKS_TABLE = """"""CREATE TABLE IF NOT EXISTS frameworks (\n                            name text NOT NULL,\n                            start_time integer NOT NULL,\n                            finished_time integer NOT NULL,\n                            content text NOT NULL\n                            )""""""\n    CREATE_FRAMEWORK_NAME_INDEX = ""CREATE INDEX IF NOT EXISTS framework_name_index ON frameworks (name);""\n    CREATE_FRAMEWORK_TIME_INDEX = ""CREATE INDEX IF NOT EXISTS framework_time_index ON frameworks (start_time, finished_time);""\n\n    def __init__(self, db_path):\n        self.db_path = db_path\n        self.conn = sqlite3.connect(self.db_path)\n        cursor = self.conn.cursor()\n        cursor.execute(DB.CREATE_APPS_TABLE)\n        cursor.execute(DB.CREATE_APP_ID_INDEX)\n        cursor.execute(DB.CREATE_APP_TIME_INDEX)\n        cursor.execute(DB.CREATE_FRAMEWORKS_TABLE)\n        cursor.execute(DB.CREATE_FRAMEWORK_NAME_INDEX)\n        cursor.execute(DB.CREATE_FRAMEWORK_TIME_INDEX)\n        self.conn.commit()\n\n\ndef get_yarn_apps(yarn_url):\n    apps_url = urllib.parse.urljoin(yarn_url, ""/ws/v1/cluster/apps"")\n    result = []\n\n    obj = request_with_error_handling(apps_url)\n\n    apps = walk_json_field_safe(obj, ""apps"", ""app"")\n\n    if apps is None:\n        return result\n\n    for app in apps:\n        app_id = walk_json_field_safe(app, ""id"")\n        if app_id is None:\n            continue\n\n        finished_time = walk_json_field_safe(app, ""finishedTime"") or 0\n        finished_time = int(finished_time / 1000) # yarn\'s time is in millisecond\n        content = json.dumps(app)\n        result.append({""app_id"": app_id,\n            ""finished_time"": finished_time, ""content"": content})\n\n    return result\n\n\ndef get_frameworks(launcher_url):\n    launcher_url = urllib.parse.urljoin(launcher_url, ""/v1/Frameworks"")\n    result = []\n\n    obj = request_with_error_handling(launcher_url)\n\n    frameworks = walk_json_field_safe(obj, ""summarizedFrameworkInfos"")\n\n    if frameworks is None:\n        return result\n\n    for framework in frameworks:\n        name = walk_json_field_safe(framework, ""frameworkName"")\n        if name is None:\n            continue\n\n        finished_time = walk_json_field_safe(framework, ""frameworkCompletedTimestamp"") or 0\n        finished_time = int(finished_time / 1000) # yarn\'s time is in millisecond\n        start_time = walk_json_field_safe(framework, ""firstRequestTimestamp"") or 0\n        start_time = int(start_time / 1000) # yarn\'s time is in millisecond\n        content = json.dumps(framework)\n        result.append({""name"": name, ""start_time"": start_time,\n            ""finished_time"": finished_time, ""content"": content})\n\n    return result\n\n\ndef refresh_cache(database, yarn_url, launcher_url):\n    db = DB(database)\n\n    apps = get_yarn_apps(yarn_url)\n    logger.info(""get %d of apps from yarn"", len(apps))\n\n    with db.conn:\n        cursor = db.conn.cursor()\n\n        for app in apps:\n            cursor.execute(""""""SELECT COUNT(*) FROM apps\n                            WHERE app_id=?"""""",\n                            (app[""app_id""],))\n            result = cursor.fetchone()\n\n            if result[0] > 0:\n                cursor.execute(""""""UPDATE apps SET finished_time=?, content=?\n                                WHERE app_id=?"""""",\n                                (app[""finished_time""], app[""content""], app[""app_id""]))\n            else:\n                cursor.execute(""""""INSERT INTO apps(app_id,finished_time,content)\n                                VALUES(?,?,?)"""""",\n                                (app[""app_id""], app[""finished_time""], app[""content""]))\n\n        db.conn.commit()\n\n    frameworks = get_frameworks(launcher_url)\n    logger.info(""get %d of frameworks from launcher"", len(frameworks))\n\n    with db.conn:\n        cursor = db.conn.cursor()\n\n        for framework in frameworks:\n            cursor.execute(""""""SELECT COUNT(*) FROM frameworks\n                            WHERE name=?"""""",\n                            (framework[""name""],))\n            result = cursor.fetchone()\n\n            if result[0] > 0:\n                cursor.execute(""""""UPDATE frameworks SET finished_time=?, content=?\n                                WHERE name=?"""""",\n                                (framework[""finished_time""], framework[""content""], framework[""name""]))\n            else:\n                cursor.execute(""""""INSERT INTO frameworks(name,start_time,finished_time,content)\n                                VALUES(?,?,?,?)"""""",\n                                (framework[""name""],\n                                    framework[""start_time""],\n                                    framework[""finished_time""],\n                                    framework[""content""]))\n\n        db.conn.commit()\n\n\n# https://github.com/Microsoft/pai/blob/pai-0.9.y/src/rest-server/src/models/job.js#L45\n# https://github.com/microsoft/pai/blob/v0.13.0/src/job-exit-spec/config/job-exit-spec.md\ndef convert_job_state(framework_state, exit_code):\n    if framework_state in {\n            ""FRAMEWORK_WAITING"",\n            ""APPLICATION_CREATED"",\n            ""APPLICATION_LAUNCHED"",\n            ""APPLICATION_WAITING""}:\n        return ""WAITING""\n    elif framework_state in {\n            ""APPLICATION_RUNNING"",\n            ""APPLICATION_RETRIEVING_DIAGNOSTICS"",\n            ""APPLICATION_COMPLETED""}:\n        return ""RUNNING""\n    elif framework_state == ""FRAMEWORK_COMPLETED"":\n        if exit_code is not None:\n            if exit_code == 0:\n                return ""SUCCEEDED""\n            elif exit_code == -7351:\n                return ""STOPPED""\n            else:\n                return ""FAILED""\n        else:\n            return ""FAILED""\n\n    return ""UNKNOWN""\n\n\ndef get_job_report(database, since, until, max_mem_usage):\n    """""" return two values, one is aggregated job info, the other is raw job status """"""\n    db = DB(database)\n\n    with db.conn:\n        # Select more apps, since framework may retry, and previous retry\n        # may not finished in since~until range.\n        # Assume no retry will happen 1 month before framework finish.\n        app_since = datetime.datetime.fromtimestamp(since) - datetime.timedelta(days=31)\n        app_since = int(datetime.datetime.timestamp(app_since))\n        cursor = db.conn.cursor()\n        cursor.execute(""""""SELECT content FROM apps\n                        WHERE (finished_time>? AND finished_time<?)\n                            OR finished_time=0"""""",\n                        (app_since, until))\n        apps = cursor.fetchall()\n\n        logger.info(""get %d apps entries"", len(apps))\n\n        cursor.execute(""""""SELECT content FROM frameworks\n                        WHERE ((finished_time>? AND finished_time<?)\n                            OR finished_time=0)"""""",\n                        (since, until))\n        frameworks = cursor.fetchall()\n\n        logger.info(""get %d frameworks entries"", len(frameworks))\n\n    # key is framework_name, value is JobInfo\n    processed_apps = collections.defaultdict(lambda : JobInfo())\n\n    pattern = re.compile(u""\\[([^[\\]]+)\\]+_.*"")\n\n    for content, in apps:\n        app = json.loads(content)\n\n        name = walk_json_field_safe(app, ""name"")\n        if name is None:\n            continue\n\n        match = pattern.match(name)\n        if match is None:\n            continue\n\n        job_name = match.groups()[0]\n\n        elapsed_time = walk_json_field_safe(app, ""elapsedTime"") or 0\n        elapsed_time = int(elapsed_time / 1000)\n        cpu_sec = walk_json_field_safe(app, ""vcoreSeconds"") or 0\n        mem_sec = int((walk_json_field_safe(app, ""memorySeconds"") or 0) / 1024)\n        gpu_sec = walk_json_field_safe(app, ""gpuSeconds"") or 0\n\n        info = JobInfo(job_count=0, elapsed_time=elapsed_time,\n                cpu_sec=cpu_sec, mem_sec=mem_sec, gpu_sec=gpu_sec)\n\n        processed_apps[job_name] += info\n\n    statistic = collections.defaultdict(lambda : # key is username\n                collections.defaultdict(lambda : # key is vc\n                collections.defaultdict(lambda : JobInfo()))) # key is job_status\n\n    for content, in frameworks:\n        framework = json.loads(content)\n\n        name = walk_json_field_safe(framework, ""frameworkName"")\n        username = walk_json_field_safe(framework, ""userName"")\n        vc = walk_json_field_safe(framework, ""queue"")\n        start_time = walk_json_field_safe(framework, ""firstRequestTimestamp"") or 0\n        start_time = int(start_time / 1000)\n        finished_time = walk_json_field_safe(framework, ""frameworkCompletedTimestamp"") or 0\n        finished_time = int(finished_time / 1000)\n        retries = walk_json_field_safe(framework, ""frameworkRetryPolicyState"", ""retriedCount"")\n\n        state = walk_json_field_safe(framework, ""frameworkState"")\n        exit_code = walk_json_field_safe(framework, ""applicationExitCode"")\n        job_status = convert_job_state(state, exit_code)\n\n        if name in processed_apps:\n            job = processed_apps[name]\n\n            job.job_count = 1\n            job.user = username\n            job.vc = vc\n            job.start_time = start_time\n            job.finished_time = finished_time\n            job.retries = retries\n            job.status = job_status\n            if exit_code is not None:\n                job.exit_code = exit_code\n            else:\n                job.exit_code = ""N/A""\n\n            if name in max_mem_usage:\n                job.max_mem_usage = max_mem_usage[name] / 1024 / 1024 / 1024\n\n            statistic[username][vc][job_status] += job\n\n    # remove apps that not belongs to any framework\n    for job_name in list(processed_apps.keys()):\n        job = processed_apps[job_name]\n        if job.job_count == 0:\n            processed_apps.pop(job_name)\n\n    result = []\n    for username, val in statistic.items():\n        for vc, sub_val in val.items():\n            total_job_info = JobInfo()\n            mapping = {\n                    ""SUCCEEDED"": JobInfo(),\n                    ""FAILED"": JobInfo(),\n                    ""STOPPED"": JobInfo(),\n                    ""RUNNING"": JobInfo(),\n                    ""WAITING"": JobInfo()}\n\n            for job_status, job_info in sub_val.items():\n                if job_status in mapping:\n                    mapping[job_status] += job_info\n                total_job_info += job_info\n\n            result.append(JobReportEntries(username, vc, total_job_info,\n                mapping[""SUCCEEDED""], mapping[""FAILED""], mapping[""STOPPED""],\n                mapping[""RUNNING""], mapping[""WAITING""]))\n\n    return result, processed_apps\n\n\ndef get_alerts(prometheus_url, since, until):\n    args = urllib.parse.urlencode({\n        ""query"": ""ALERTS{alertstate=\\""firing\\""}"",\n        ""start"": str(since),\n        ""end"": str(until),\n        ""step"": ""5m"",\n        })\n\n    url = urllib.parse.urljoin(prometheus_url,\n            ""/prometheus/api/v1/query_range"") + ""?"" + args\n\n    logger.debug(""requesting %s"", url)\n    result = []\n\n    obj = request_with_error_handling(url)\n\n    if walk_json_field_safe(obj, ""status"") != ""success"":\n        logger.warning(""requesting %s failed, body is %s"", url, obj)\n        return result\n\n    gap = 5 * 60 # because the step is 5m, the gap between two data point should be this\n\n    metrics = walk_json_field_safe(obj, ""data"", ""result"")\n\n    for metric in metrics:\n        labels = walk_json_field_safe(metric, ""metric"")\n        alert_name = walk_json_field_safe(labels, ""alertname"") or ""unknown""\n\n        values = walk_json_field_safe(metric, ""values"")\n        if values is not None and len(values) > 0:\n            start = end = values[0][0]\n            events = []\n\n            for i, value in enumerate(values):\n                if i == len(values) - 1:\n                    events.append({""start"": start, ""end"": value[0]})\n                    break\n\n                if value[0] - end <= gap:\n                    end = value[0]\n                    continue\n                else:\n                    events.append({""start"": start, ""end"": end})\n                    start = end = value[0]\n\n            for event in events:\n                # because the end is the last time alert still happening, if we\n                # treat end - start equals to be the durtion of the alert,\n                # the alert with start == end will have durtion of 0, which is\n                # quite confusing, so we set durtion to be end - start + gap\n                result.append(Alert(alert_name, int(event[""start""]),\n                    int(event[""end""] - event[""start""] + gap),\n                    labels))\n        else:\n            logger.warning(""unexpected zero values in alert %s"", alert_name)\n\n    logger.info(""get %d alert entries"", len(result))\n\n    return result\n\n\ndef get_gpu_util(prometheus_url, since, until):\n    args = urllib.parse.urlencode({\n        ""query"": ""nvidiasmi_utilization_gpu"",\n        ""start"": str(since),\n        ""end"": str(until),\n        ""step"": ""10m"",\n        })\n\n    url = urllib.parse.urljoin(prometheus_url,\n            ""/prometheus/api/v1/query_range"") + ""?"" + args\n\n    logger.debug(""requesting %s"", url)\n    result = []\n\n    obj = request_with_error_handling(url)\n\n    if walk_json_field_safe(obj, ""status"") != ""success"":\n        logger.warning(""requesting %s failed, body is %s"", url, obj)\n        return result\n\n    metrics = walk_json_field_safe(obj, ""data"", ""result"")\n\n    for metric in metrics:\n        node_ip = get_ip(walk_json_field_safe(metric, ""metric"", ""instance""))\n        gpu_id = walk_json_field_safe(metric, ""metric"", ""minor_number"")\n\n        values = walk_json_field_safe(metric, ""values"")\n        sum_ = count = avg = 0\n        if values is not None and len(values) > 0:\n            for val in values:\n                sum_ += float(val[1])\n                count += 1\n            avg = sum_ / count\n        else:\n            logger.warning(""unexpected no values in gpu utils %s, %s, default avg to 0"",\n                    node_ip,\n                    gpu_id)\n\n        result.append(GPUEntry(node_ip, gpu_id, avg))\n\n    logger.info(""get %d gpu entries"", len(result))\n\n    return result\n\n\ndef delete_old_data(database, days):\n    db = DB(database)\n    now = datetime.datetime.now()\n    delta = datetime.timedelta(days=days)\n\n    ago = int(datetime.datetime.timestamp(now - delta))\n\n    with db.conn:\n        cursor = db.conn.cursor()\n\n        # should not delete entries if finished_time is 0, they are running apps\n        cursor.execute(""""""DELETE FROM apps WHERE finished_time<? AND finished_time!=0"""""",\n                        (ago,))\n\n        # should not delete entries if finished_time is 0, they are running jobs\n        cursor.execute(""""""DELETE FROM frameworks WHERE finished_time<? AND finished_time!=0"""""",\n                        (ago,))\n\n        db.conn.commit()\n\n\ndef get_max_mem_usage(prometheus_url, since, until):\n    return get_max_resource_usage(prometheus_url, since, until,\n            ""max (task_mem_usage_byte) by (job_name)"")\n\n\ndef get_max_resource_usage(prometheus_url, since, until, query):\n    args = urllib.parse.urlencode({\n        ""query"": query,\n        ""start"": str(since),\n        ""end"": str(until),\n        ""step"": ""5m"",\n        })\n\n    url = urllib.parse.urljoin(prometheus_url,\n            ""/prometheus/api/v1/query_range"") + ""?"" + args\n\n    logger.debug(""requesting %s"", url)\n    result = []\n\n    obj = request_with_error_handling(url)\n\n    if walk_json_field_safe(obj, ""status"") != ""success"":\n        logger.warning(""requesting %s failed, body is %s"", url, obj)\n        return result\n\n    metrics = walk_json_field_safe(obj, ""data"", ""result"")\n\n    result = {} # key is job_name, value is max resource usage.\n\n    for metric in metrics:\n        job_name = walk_json_field_safe(metric, ""metric"", ""job_name"")\n        if job_name is None:\n            continue\n\n        values = walk_json_field_safe(metric, ""values"")\n        if values is None or len(values) == 0:\n            continue\n\n        max_ = max(map(lambda x: float(x[1]), values))\n        result[job_name] = max_\n\n    logger.info(""get %d resource usage entries"", len(result))\n\n    return result\n\n\ndef gen_raw_job(processed_apps):\n    result = []\n\n    for job_name, job in processed_apps.items():\n        if job.user == ""unknown"" or job.vc == ""unknown"":\n            # this is due to Framework do not have job info, but yarn have\n            continue\n\n        elapsed_time = job.elapsed_time\n        cpu = math.ceil(job.cpu_sec / elapsed_time)\n        mem = math.ceil(job.mem_sec / elapsed_time)\n        gpu = math.ceil(job.gpu_sec / elapsed_time)\n\n        if job.finished_time == 0:\n            now = datetime.datetime.now()\n            now = int(datetime.datetime.timestamp(now))\n            waiting_time = now - job.start_time - elapsed_time\n        else:\n            waiting_time = job.finished_time - job.start_time - elapsed_time\n\n        result.append(RawJob(\n            job.user,\n            job.vc,\n            job_name,\n            format_time(job.start_time),\n            format_time(job.finished_time),\n            waiting_time,\n            job.elapsed_time,\n            job.retries,\n            job.status,\n            job.exit_code,\n            cpu,\n            mem,\n            job.max_mem_usage,\n            gpu))\n    return result\n\n\ndef gen_report(database, prometheus_url, path, since, until):\n    max_mem_usage = get_max_mem_usage(prometheus_url, since, until)\n    job_report, processed_apps = get_job_report(database, since, until, max_mem_usage)\n    job_file = ""%s_job.csv"" % path\n    with open(job_file, ""w"") as f:\n        f.write(""user,vc,"" +\n        ""total_count,total_time,total_cpu_sec,total_mem_sec,total_gpu_sec,"" +\n        ""succ_count,succ_time,succ_cpu_sec,succ_mem_sec,succ_gpu_sec,"" +\n        ""fail_count,fail_time,fail_cpu_sec,fail_mem_sec,fail_gpu_sec,"" +\n        ""stop_count,stop_time,stop_cpu_sec,stop_mem_sec,stop_gpu_sec,"" +\n        ""run_count,run_time,run_cpu_sec,run_mem_sec,run_gpu_sec,"" +\n        ""wait_count,wait_time,wait_cpu_sec,wait_mem_sec,wait_gpu_sec\\n"")\n        for r in job_report:\n            f.write(""%s\\n"" % r)\n\n    job_raw_file = ""%s_raw_job.csv"" % path\n    with open(job_raw_file, ""w"") as f:\n        f.write(""user,vc,job,start_time,finish_time,waiting_time,run_time,retries,status,exit_code,cpu,mem,max_mem,gpu\\n"")\n\n        for r in gen_raw_job(processed_apps):\n            f.write(""%s\\n"" % r)\n\n    alert_report = get_alerts(prometheus_url, since, until)\n    alert_file = ""%s_alert.csv"" % path\n    with open(alert_file, ""w"") as f:\n        f.write(""alert_name,host_ip,source,start,durtion,labels\\n"")\n        for r in alert_report:\n            f.write(""%s\\n"" % r)\n\n    gpu_report = get_gpu_util(prometheus_url, since, until)\n    gpu_file = ""%s_gpu.csv"" % path\n    with open(gpu_file, ""w"") as f:\n        f.write(""host_ip,gpu_id,avg\\n"")\n        for r in gpu_report:\n            f.write(""%s\\n"" % r)\n\n    logger.info(""write csv file into %s, %s, %s and %s"",\n            job_file, job_raw_file, alert_file, gpu_file)\n\n\ndef translate_span(span):\n    if span is None or span == ""week"":\n        delta = datetime.timedelta(days=7)\n    elif span == ""day"":\n        delta = datetime.timedelta(days=1)\n    elif span == ""month"":\n        delta = datetime.timedelta(days=31)\n    else:\n        delta = datetime.timedelta(days=7)\n        logger.warning(""unknown span %s, default to week"", span)\n\n    now = datetime.datetime.now()\n\n    ago = int(datetime.datetime.timestamp(now - delta))\n    now = int(datetime.datetime.timestamp(now))\n\n    return ago, now\n\n\ndef translate_to_map(keys, values):\n    result = []\n\n    for r in values:\n        element = {}\n        for i, value in enumerate(r.values()):\n            element[keys[i]] = value\n        result.append(element)\n\n    return result\n\n\ndef serve(database, prometheus_url, port):\n    app = Flask(__name__)\n\n    @app.route(""/job"", methods=[""GET""])\n    def get_job():\n        since, until = translate_span(request.args.get(""span""))\n\n        max_mem_usage = get_max_mem_usage(prometheus_url, since, until)\n        job_report, processed_apps = get_job_report(database, since, until, max_mem_usage)\n\n        keys =[""user"", ""vc"",\n            ""total_count"", ""total_time"", ""total_cpu_sec"", ""total_mem_sec"", ""total_gpu_sec"",\n            ""succ_count"", ""succ_time"", ""succ_cpu_sec"", ""succ_mem_sec"", ""succ_gpu_sec"",\n            ""fail_count"", ""fail_time"", ""fail_cpu_sec"", ""fail_mem_sec"", ""fail_gpu_sec"",\n            ""stop_count"", ""stop_time"", ""stop_cpu_sec"", ""stop_mem_sec"", ""stop_gpu_sec"",\n            ""run_count"", ""run_time"", ""run_cpu_sec"", ""run_mem_sec"", ""run_gpu_sec"",\n            ""wait_count"", ""wait_time"", ""wait_cpu_sec"", ""wait_mem_sec"", ""wait_gpu_sec""]\n\n        return flask.jsonify(translate_to_map(keys, job_report))\n\n    @app.route(""/raw_job"", methods=[""GET""])\n    def get_raw_job():\n        since, until = translate_span(request.args.get(""span""))\n\n        max_mem_usage = get_max_mem_usage(prometheus_url, since, until)\n        job_report, processed_apps = get_job_report(database, since, until, max_mem_usage)\n\n        keys = [""user"", ""vc"", ""job"",\n                ""start_time"", ""finish_time"", ""waiting_time"", ""run_time"",\n                ""retries"", ""status"", ""exit_code"", ""cpu"", ""mem"", ""max_mem"", ""gpu""]\n\n        return flask.jsonify(translate_to_map(keys, gen_raw_job(processed_apps)))\n\n    @app.route(""/alert"", methods=[""GET""])\n    def get_alert():\n        since, until = translate_span(request.args.get(""span""))\n\n        alert_report = get_alerts(prometheus_url, since, until)\n\n        keys = [""alert_name"", ""host_ip"", ""source"", ""start"", ""durtion"", ""labels""]\n\n        return flask.jsonify(translate_to_map(keys, alert_report))\n\n    @app.route(""/gpu"", methods=[""GET""])\n    def get_gpu():\n        since, until = translate_span(request.args.get(""span""))\n\n        gpu_report = get_gpu_util(prometheus_url, since, until)\n\n        keys = [""host_ip"", ""gpu_id"", ""avg""]\n\n        return flask.jsonify(translate_to_map(keys, gpu_report))\n\n    @app.route(""/proxy"", methods=[""GET""])\n    def proxy():\n        since, until = translate_span(request.args.get(""span""))\n        query = request.args.get(""query"")\n        if query is None:\n            return flask.jsonify({""error"": ""expect query parameter""})\n\n        args = urllib.parse.urlencode({\n            ""query"": query,\n            ""start"": str(since),\n            ""end"": str(until),\n            ""step"": ""10m"",\n            })\n\n        url = urllib.parse.urljoin(prometheus_url,\n                ""/prometheus/api/v1/query_range"") + ""?"" + args\n\n        logger.debug(""requesting %s"", url)\n        result = []\n\n        obj = request_with_error_handling(url)\n\n        if walk_json_field_safe(obj, ""status"") != ""success"":\n            logger.warning(""requesting %s failed, body is %s"", url, obj)\n        return flask.jsonify(obj)\n\n    app.run(host=""0.0.0.0"", port=port, debug=False)\n\n\ndef main(args):\n    if args.action == ""refresh"":\n        delete_old_data(args.database, args.retain)\n        refresh_cache(args.database, args.yarn_url, args.launcher_url)\n    elif args.action == ""report"":\n        gen_report(args.database, args.prometheus_url, args.file, args.since, args.until)\n    elif args.action == ""serve"":\n        serve(args.database, args.prometheus_url, args.port)\n    else:\n        sys.stderr.write(""unknown action %s\\n"" % (args.action))\n        sys.exit(1)\n\n\nif __name__ == ""__main__"":\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s"",\n            level=logging.INFO)\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""action"", choices=[""refresh"", ""report"", ""serve""])\n    parser.add_argument(""--yarn_url"", ""-y"", required=True,\n            help=""Yarn rest api address, eg: http://127.0.0.1:8088"")\n    parser.add_argument(""--prometheus_url"", ""-p"", required=True,\n            help=""Prometheus url, eg: http://127.0.0.1:9091"")\n    parser.add_argument(""--launcher_url"", ""-l"", required=True,\n            help=""Framework launcher url, eg: http://127.0.0.1:9086"")\n\n    parser.add_argument(""--retain"", ""-r"", type=int, default=6*31,\n            help=""How many days to retain cache"")\n    parser.add_argument(""--file"", ""-f"", required=False,\n            help=""Output file prefix, required argument when action is report"",\n            default=""cluster_report"")\n\n    parser.add_argument(""--database"", ""-d"", required=True,\n            help=""which sqlite db file to use"")\n\n    now = datetime.datetime.now()\n    delta = datetime.timedelta(days=31)\n    one_month_ago = int(datetime.datetime.timestamp(now - delta))\n    now = int(datetime.datetime.timestamp(now))\n\n    parser.add_argument(""--since"", ""-s"", type=int, default=one_month_ago,\n            help=""start time for generating report"")\n    parser.add_argument(""--until"", ""-u"", type=int, default=now,\n            help=""end time for generating report"")\n\n    parser.add_argument(""--port"", type=int, default=10240,\n            help=""port to listen when action is serve"")\n\n    args = parser.parse_args()\n\n    main(args)\n'"
src/utilities/doc_checker.py,0,"b'# Copyright (c) Microsoft Corporation.\n# Licensed under the MIT License.\n\nimport sys\nimport os\nimport codecs\nimport urllib.parse\nimport markdown # https://python-markdown.github.io/\n\n\nclass LinkChecker(markdown.treeprocessors.Treeprocessor):\n    def __init__(self, dir_name, file_name, *args, **kwargs):\n        markdown.treeprocessors.Treeprocessor.__init__(self, *args, **kwargs)\n\n        self.dir_name = dir_name\n        self.file_name = file_name\n        self.has_error = False\n\n    def run(self, root):\n        for node in root.iter():\n            if node.tag == ""a"":\n                link = node.get(""href"")\n                url = urllib.parse.urlparse(link)\n\n                # TODO check remot links\n                if url.scheme == """" and url.netloc == """": # local link\n                    path = os.path.join(self.dir_name, url.path)\n                    if url.path == """" and url.fragment != """":\n                        # ignore current file fragment checks\n                        continue\n\n                    if not os.path.exists(path):\n                        sys.stderr.write(""Error: %s has broken link %s\\n"" %\n                                (os.path.join(self.dir_name, self.file_name), link))\n                        self.has_error = True\n\n\nclass LinkCheckerExtension(markdown.Extension):\n    def __init__(self, dir_name, file_name, *args, **kwargs):\n        markdown.Extension.__init__(self, *args, **kwargs)\n\n        self.dir_name = dir_name\n        self.file_name = file_name\n\n    def extendMarkdown(self, md, md_globals):\n        md.treeprocessors.add(\n            ""link_checker"", LinkChecker(self.dir_name, self.file_name, md), "">inline"")\n\n\ndef check(doc_path):\n    """""" check doc file in `doc_path`. return True on error occurs,\n    broken links will be outputted to stderr """"""\n    dir_name = os.path.dirname(doc_path)\n    file_name = os.path.basename(doc_path)\n\n    md = markdown.Markdown(extensions=[LinkCheckerExtension(dir_name, file_name)])\n    with codecs.open(doc_path, ""r"", ""utf-8"") as f:\n        md.convert(f.read())\n\n    return md.treeprocessors.get(""link_checker"").has_error\n\n\ndef check_all(doc_root):\n    """""" check_all iteratively checks docs link. return True on error occurs,\n    broken links will be outputted to stderr """"""\n    has_error = False\n    for root, dirs, files in os.walk(doc_root):\n        dirs[:] = [d for d in dirs if d not in \'node_modules\']\n        for name in files:\n            if name.endswith("".md""):\n                path = os.path.join(root, name)\n                if ""/vendor/"" in path:\n                    # ignore 3rd party markdown check\n                    continue\n\n                md = markdown.Markdown(extensions=[LinkCheckerExtension(root, name)])\n                with codecs.open(path, ""r"", ""utf-8"") as f:\n                    sys.stdout.write(""analyzing %s\\n"" % path)\n                    md.convert(f.read())\n                has_error = has_error or md.treeprocessors.get(""link_checker"").has_error\n    return has_error\n\n\nif __name__ == ""__main__"":\n    if len(sys.argv) != 2:\n        sys.stderr.write(""Usage: doc_checker.py (doc_dir|doc_file)\\n"")\n        sys.exit(1)\n\n    doc_root = os.path.abspath(sys.argv[1])\n\n    if os.path.isdir(doc_root):\n        has_error = check_all(doc_root)\n    else:\n        has_error = check(doc_root)\n\n    if has_error:\n        sys.exit(2)\n    else:\n        sys.exit(0)\n'"
src/utilities/gen-amtool-config.py,0,"b'#!/usr/bin/python\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport codecs\nimport os\nimport sys\n\nproject_root = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\n\nsys.path.append(os.path.join(project_root, ""pai-management""))\n\nfrom paiLibrary.clusterObjectModel import objectModelFactory\n\namtool_config = """"""\n# see https://github.com/prometheus/alertmanager#config for detail\n# Define the path that amtool can find your `alertmanager` instance at\nalertmanager.url: ""http://{}:{}""\n\n# Override the default author. (unset defaults to your username)\n# author: me@example.com\n\n# Force amtool to give you an error if you don\'t include a comment on a silence\ncomment_required: true\n\n# Set a default output format. (unset defaults to simple)\noutput: extended\n""""""\n\ndef gen_amtool_config(args):\n    model = objectModelFactory.objectModelFactory(args.config_path)\n    service_config = model.objectModelPipeLine()\n\n    try:\n        prometheus_info = service_config[""service""][""clusterinfo""][""prometheusinfo""]\n        alerting = prometheus_info[""alerting""]\n        port = alerting[""alert_manager_port""]\n        alert_manager_hosts = alerting[""alert-manager-hosts""]\n        host = alert_manager_hosts[0] # TODO not sure if alert manager with HA workds this way\n    except KeyError:\n        sys.stderr.write(""no alert manager configured\\n"")\n        sys.exit(1)\n\n    home = os.path.expanduser(""~"")\n    amtool_dir = os.path.join(home, "".config/amtool"")\n    if not os.path.exists(amtool_dir):\n        os.makedirs(amtool_dir)\n    config = os.path.join(amtool_dir, ""config.yml"")\n    if os.path.isfile(config) and not args.force:\n        sys.stderr.write(""{} already exist, specify -f to overwrite\\n"".format(config))\n        sys.exit(1)\n\n    with codecs.open(config, ""w"", ""utf-8"") as f:\n        f.write(amtool_config.format(host,port))\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""-p"", ""--config-path"", dest=""config_path"", required=True,\n            help=""The path of your configuration directory."")\n    parser.add_argument(""-f"", ""--force"", dest=""force"", default=False, action=""store_true"",\n            help=""clean all the data forcefully"")\n    args = parser.parse_args()\n\n    gen_amtool_config(args)\n'"
contrib/debug-tools/openpaipdb/paipdb.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n""""""\nPaipdb is a simple wrapper of the pdb.\n\nAfter Paipdb server starts, it will wait for client connect and all stdin/stdout will\nredicret to the client side.\n""""""\n\nimport os\nimport sys\nimport pdb\nimport socket\nimport time\nimport traceback\nimport threading\nimport signal\nfrom functools import partial\n\nCURRENT_TASK_ROLE_NAME = os.environ.get(""PAI_CURRENT_TASK_ROLE_NAME"")\nTASK_ROLE_INDEX = os.environ.get(""PAI_TASK_ROLE_INDEX"")\nDEFAULT_ADDR = os.environ.get(""PAI_HOST_IP_"" + CURRENT_TASK_ROLE_NAME + ""_"" + TASK_ROLE_INDEX)\nDEBUG_PORT_NAME = os.environ.get(""DEBUG_PORT_NAME"", """")\nDEBUG_PORT_LIST = ""PAI_PORT_LIST_"" + CURRENT_TASK_ROLE_NAME + ""_"" + TASK_ROLE_INDEX + ""_"" + DEBUG_PORT_NAME\n\nif os.environ.get(DEBUG_PORT_LIST) is None:\n    print(""Can not get debug port, exit"")\n    sys.exit(1)\n\nDEFAULT_PORT = int(os.environ.get(DEBUG_PORT_LIST).split(\',\')[0])\n# Debug will exit if there is no connection in 600 seconds\nDEFAULT_TIMEOUT = int(os.environ.get(""DEBUG_TIMEOUT"", 600))\n\nclass FileObjectWrapper(object):\n    def __init__(self, fileobject, stdio):\n        self._obj = fileobject\n        self._io = stdio\n\n    def __getattr__(self, attr):\n        if hasattr(self._obj, attr):\n            attr = getattr(self._obj, attr)\n        elif hasattr(self._io, attr):\n            attr = getattr(self._io, attr)\n        else:\n            raise AttributeError(""Attribute {} is not found"".format(attr))\n        return attr\n\n\nclass Paipdb(pdb.Pdb):\n\n    def __init__(self, addr=DEFAULT_ADDR, port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT):\n        """"""Initialize the socket here.""""""\n\n        # Backup stdin and stdout before replacing them by the socket handle\n        self.old_stdout = sys.stdout\n        self.old_stdin = sys.stdin\n        self.port = port\n        self.addr = addr\n\n        # Open a \'reusable\' socket to let the webapp reload on the same port\n        self.skt = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.skt.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, True)\n        self.skt.settimeout(timeout)\n\n        self.skt.bind((addr, port))\n        self.skt.listen(1)\n        \n        # write to stderr for not buffering this message\n        sys.stderr.write(""paipdb is running on {}:{}\\n"".format(*self.skt.getsockname()))\n\n        (clientsocket, _) = self.skt.accept()\n        handle = clientsocket.makefile(\'rw\')\n        pdb.Pdb.__init__(self, completekey=\'tab\',\n                            stdin=FileObjectWrapper(handle, self.old_stdin),\n                            stdout=FileObjectWrapper(handle, self.old_stdin))\n        sys.stdout = sys.stdin = handle\n        OCCUPIED.claim(port, sys.stdout)\n\n    def stop_server(self):\n        """"""Revert the stdin & stdout, close the socket""""""\n\n        sys.stdout = self.old_stdout\n        sys.stdin = self.old_stdin\n        OCCUPIED.unclaim(self.port)\n        self.skt.close()\n\n\ndef set_trace(addr=DEFAULT_ADDR, port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT, frame=None):\n    """"""\n    wrapper function so we can user Paipdb.set_trace() as pdb usage\n    """"""\n    try:\n        debugger = Paipdb(addr=addr, port=port, timeout=timeout)\n    except socket.error:\n        if OCCUPIED.is_claimed(port, sys.stdout):\n            # paipdb is already on this port just ingnore and go on:\n            print(""(Recurrent paipdb invocation ignored)"")\n            return\n        else:\n            # Port occupied by something else.\n            raise\n    try:\n        debugger.set_trace(frame or sys._getframe().f_back)\n    except Exception:\n        traceback.print_exc()\n\n\ndef _trap_handler(addr, port, timeout, signum, frame):\n    set_trace(addr, port, timeout, frame=frame)\n\n\ndef handle_trap(addr=DEFAULT_ADDR, port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT):\n    """"""Register rpdb as the SIGTRAP signal handler""""""\n    signal.signal(signal.SIGTRAP, partial(_trap_handler, addr, port, timeout))\n\n\ndef post_mortem(addr=DEFAULT_ADDR, port=DEFAULT_PORT, timeout=DEFAULT_TIMEOUT):\n    debugger = Paipdb(addr=addr, port=port, timeout=timeout)\n    tb = sys.exc_info()[2]\n    traceback.print_exc()\n    debugger.reset()\n    debugger.interaction(None, tb)\n\n\n# This class is refer to rpbd: https://github.com/tamentis/rpdb/blob/0.1.6/rpdb/__init__.py\nclass OccupiedPorts(object):\n    """"""Maintain paipdb port versus stdin/out file handles.\n\n    Provides the means to determine whether or not a collision binding to a\n    particular port is with an already operating rpdb session.\n\n    Determination is according to whether a file handle is equal to what is\n    registered against the specified port.\n    """"""\n\n    def __init__(self):\n        self.lock = threading.RLock()\n        self.claims = {}\n\n    def claim(self, port, handle):\n        with self.lock:\n            self.claims[port] = id(handle)\n\n    def is_claimed(self, port, handle):\n        with self.lock:\n            got = (self.claims.get(port) == id(handle))\n        return got\n\n    def unclaim(self, port):\n        with self.lock:\n            del self.claims[port]\n\n# {port: sys.stdout} pairs to track recursive rpdb invocation on same port.\n# This scheme doesn\'t interfere with recursive invocations on separate ports -\n# useful, eg, for concurrently debugging separate threads.\nOCCUPIED = OccupiedPorts()\n\ndef main():\n    args = sys.argv[1:]\n    mainpyfile = args[0]\n    sys.argv[:] = args\n\n    # Do not buffer this message, print to stderr\n    sys.stderr.write(""debug host ip is set to {}, port is set to {}, timeout is set to {}\\n"".format(\n        DEFAULT_ADDR, DEFAULT_PORT, DEFAULT_TIMEOUT))\n\n    try:\n        currentPdb = Paipdb()\n    except socket.timeout:\n        print(""No client connect to server in {} seconds, exit"".format(DEFAULT_TIMEOUT))\n        sys.exit(1)\n    except:\n        traceback.print_exc()\n        print(\'Failed to start the remote debug\')\n        sys.exit(1)\n\n    # PDB will deal with the client close connection and stop the debugging\n    while True:\n        try:\n            currentPdb._runscript(mainpyfile)\n            if currentPdb._user_requested_quit:\n                break\n            print(""The program finished and will be restarted"")\n        except pdb.Restart:\n            print(""Restarting"", mainpyfile, ""with arguments:"")\n            print(""\\t"" + "" "".join(args))\n        except SystemExit:\n            # In most cases SystemExit does not warrant a post-mortem session.\n            print(""The program exited via sys.exit(). Exit status: "", sys.exc_info()[1])\n        except SyntaxError:\n            traceback.print_exc()\n            sys.exit(1)\n        except:\n            traceback.print_exc()\n            print(""Uncaught exception. Entering post mortem debugging"")\n            print(""Running \'cont\' or \'step\' will restart the program"")\n            t = sys.exc_info()[2]\n            currentPdb.interaction(None, t)\n            print(""Post mortem debugger finished. The "" + mainpyfile +\n                  "" will be restarted"")\n    currentPdb.stop_server()\n\nif __name__ == \'__main__\':\n    import paipdb\n    paipdb.main()\n'"
contrib/kubespray/script/k8s-generator.py,0,"b'import logging\nimport logging.config\nimport yaml\nimport os\nimport argparse\nimport csv\nimport jinja2\nimport sys\nimport time\n\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\n\ndef csv_reader(csv_path):\n    hosts_list = []\n    with open(csv_path) as fin:\n        hosts_csv = csv.reader(fin)\n        for row in hosts_csv:\n            hosts_list.append(\n                {\n                    ""hostname"": row[0],\n                    ""ip"": row[1]\n                }\n            )\n    return hosts_list\n\n\ndef load_yaml_config(config_path):\n    with open(config_path, ""r"") as f:\n        config_data = yaml.load(f, yaml.SafeLoader)\n    return config_data\n\n\ndef read_template(template_path):\n    with open(template_path, ""r"") as f:\n        template_data = f.read()\n    return template_data\n\n\ndef generate_from_template_dict(template_data, map_table):\n    generated_file = jinja2.Template(template_data).render(\n        map_table\n    )\n    return generated_file\n\n\ndef write_generated_file(file_path, content_data):\n    with open(file_path, ""w+"") as fout:\n        fout.write(content_data)\n\n\ndef generate_template_file(template_file_path, output_path, map_table):\n    template = read_template(template_file_path)\n    generated_template = generate_from_template_dict(template, map_table)\n    write_generated_file(output_path, generated_template)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-w\', \'--worker-list-csv\', dest=""worklist"", required=True,\n                        help=""worker-list"")\n    parser.add_argument(\'-m\', \'--master-list-csv\', dest=""masterlist"", required=True,\n                        help=""master-list"")\n    parser.add_argument(\'-c\', \'--configuration\', dest=""configuration"", required=True,\n                        help=""cluster configuration"")\n    parser.add_argument(\'-o\', \'--output\', dest=""output"", required=True,\n                        help=""cluster configuration"")\n    args = parser.parse_args()\n\n    output_path = os.path.expanduser(args.output)\n\n    master_list = csv_reader(args.masterlist)\n    head_node = master_list[0]\n    cluster_config = load_yaml_config(args.configuration)\n\n    if \'openpai_kube_network_plugin\' not in cluster_config or cluster_config[\'openpai_kube_network_plugin\'] != \'weave\':\n        count_input = 0\n        while True:\n            user_input = input(""Are your cluster is in Azure cloud or not? (Y/N) (case sensitive)"")\n            if user_input == ""N"":\n                break\n            elif user_input == ""Y"":\n                break\n            else:\n                print("" Please type Y or N. It\'s case sensitive."")\n            count_input = count_input + 1\n            if count_input == 3:\n                logger.warning(""3 Times.........  Sorry,  we will force stopping your operation."")\n                sys.exit(1)\n        if user_input == ""Y"" and (\'openpai_kube_network_plugin\' not in cluster_config or cluster_config[\'openpai_kube_network_plugin\'] == \'calico\'):\n            logger.warning(""Azure does not support calico, please change the openpai_kube_network_plugin to weave"")\n            logger.warning(""https://docs.projectcalico.org/reference/public-cloud/azure#why-doesnt-azure-support-calico-networking"")\n            sys.exit(1)\n\n    environment = {\n        \'master\': master_list,\n        \'worker\': csv_reader(args.worklist),\n        \'cfg\': cluster_config,\n        \'head_node\': head_node\n    }\n\n    map_table = {\n        ""env"": environment\n    }\n    generate_template_file(\n        ""quick-start/hosts.yml.template"",\n        ""{0}/hosts.yml"".format(output_path),\n        map_table\n    )\n    generate_template_file(\n        ""quick-start/openpai.yml.template"",\n        ""{0}/openpai.yml"".format(output_path),\n        map_table\n    )\n\n\nif __name__ == ""__main__"":\n    main()'"
contrib/kubespray/script/openpai-generator.py,0,"b'import logging\nimport logging.config\nimport yaml\nimport os\nimport argparse\nimport csv\nimport jinja2\nfrom kubernetes import client, config\nfrom kubernetes.utils import parse_quantity\nfrom kubernetes.client.rest import ApiException\nfrom pprint import pprint\nimport sys\nimport time\n\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\n\ndef csv_reader(csv_path):\n    hosts_list = []\n    with open(csv_path) as fin:\n        hosts_csv = csv.reader(fin)\n        for row in hosts_csv:\n            hosts_list.append(\n                {\n                    ""hostname"": row[0],\n                    ""ip"": row[1]\n                }\n            )\n    return hosts_list\n\n\ndef csv_reader_ret_dict(csv_path):\n    hosts_dict = {}\n    with open(csv_path) as fin:\n        hosts_csv = csv.reader(fin)\n        for row in hosts_csv:\n            hosts_dict[row[0]] = row[1]\n    return hosts_dict\n\n\ndef load_yaml_config(config_path):\n    with open(config_path, ""r"") as f:\n        config_data = yaml.load(f, yaml.SafeLoader)\n    return config_data\n\n\ndef read_template(template_path):\n    with open(template_path, ""r"") as f:\n        template_data = f.read()\n    return template_data\n\n\ndef generate_from_template_dict(template_data, map_table):\n    generated_file = jinja2.Template(template_data).render(\n        map_table\n    )\n    return generated_file\n\n\ndef write_generated_file(file_path, content_data):\n    with open(file_path, ""w+"") as fout:\n        fout.write(content_data)\n\n\ndef generate_template_file(template_file_path, output_path, map_table):\n    template = read_template(template_file_path)\n    generated_template = generate_from_template_dict(template, map_table)\n    write_generated_file(output_path, generated_template)\n\n\ndef pod_is_ready_or_not(label_key, label_value, service_name):\n\n    label_selector_str=""{0}={1}"".format(label_key, label_value)\n\n    config.load_kube_config()\n    v1 = client.CoreV1Api()\n\n    try:\n        pod_list = v1.list_pod_for_all_namespaces(label_selector=label_selector_str, watch=False)\n    except ApiException as e:\n        logger.error(""Exception when calling CoreV1Api->list_pod_for_all_namespaces: %s\\n"" % e)\n        return False\n\n    if len(pod_list.items) == 0:\n        logger.warning(""No pod can be dectected."")\n        return False\n\n    ready = 0\n    unready = 0\n    for pod in pod_list.items:\n        if pod.status.container_statuses is None:\n            unready = unready + 1\n            continue\n        flag = True\n        for container in pod.status.container_statuses:\n            if container.ready != True:\n                unready = unready + 1\n                flag = False\n                break\n        if flag:\n            ready = ready + 1\n\n    if unready != 0:\n        logger.info(""{0} is not ready."".format(service_name))\n        logger.info(""Total: {0}"".format(ready + unready))\n        logger.info(""Ready: {0}"".format(ready))\n        return False\n\n    return True\n\n\ndef get_kubernetes_node_info_from_API():\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    # https://github.com/kubernetes-client/python/blob/master/kubernetes/docs/CoreV1Api.md#list_node\n    pretty = \'true\'\n    timeout_seconds = 56\n\n    ret = dict()\n    try:\n        api_response = api_instance.list_node(pretty=pretty, timeout_seconds=timeout_seconds)\n        for node in api_response.items:\n            gpu_resource = 0\n            if \'nvidia.com/gpu\' in node.status.allocatable:\n                gpu_resource = int(parse_quantity(node.status.allocatable[\'nvidia.com/gpu\']))\n            if \'amd.com/gpu\' in node.status.allocatable:\n                gpu_resource = int(parse_quantity(node.status.allocatable[\'amd.com/gpu\']))\n            ret[node.metadata.name] = {\n                ""cpu-resource"": int(parse_quantity(node.status.allocatable[\'cpu\'])),\n                ""mem-resource"": int(parse_quantity(node.status.allocatable[\'memory\']) / 1024 / 1024 ),\n                ""gpu-resource"": gpu_resource,\n            }\n    except ApiException as e:\n        logger.error(""Exception when calling CoreV1Api->list_node: %s\\n"" % e)\n\n    return ret\n\n\ndef wait_nvidia_device_plugin_ready(total_time=3600):\n    while pod_is_ready_or_not(""name"", ""nvidia-device-plugin-ds"", ""Nvidia-Device-Plugin"") != True:\n        logger.info(""Nvidia-Device-Plugin is not ready yet. Please wait for a moment!"")\n        time.sleep(10)\n        total_time = total_time - 10\n        if total_time < 0:\n            logger.error(""An issue occure when starting up Nvidia-Device-Plugin"")\n            sys.exit(1)\n\n\ndef wait_amd_device_plugin_ready(total_time=3600):\n    while pod_is_ready_or_not(""name"", ""amdgpu-dp-ds"", ""AMD-Device-Plugin"") != True:\n        logger.info(""AMD-Device-Plugin is not ready yet. Please wait for a moment!"")\n        time.sleep(10)\n        total_time = total_time - 10\n        if total_time < 0:\n            logger.error(""An issue occure when starting up AMD-Device-Plugin"")\n            sys.exit(1)\n\n\ndef hived_config_prepare(worker_dict, node_resource_dict):\n    hived_config = dict()\n    hived_config[""nodelist""] = []\n\n    min_mem = 100000000\n    min_gpu = 100000000\n    min_cpu = 100000000\n\n    for key in node_resource_dict:\n        if key not in worker_dict:\n            continue\n        if node_resource_dict[key][""gpu-resource""] == 0:\n            logger.error(""Allocatable GPU number in {0} is 0, current quick start script does not allow."".format(key))\n            logger.error(""Please remove {0} from your workerlist, or check if the device plugin is running healthy on the node."".format(key))\n            sys.exit(1)\n        min_cpu = min(min_cpu, node_resource_dict[key][""cpu-resource""])\n        min_mem = min(min_mem, node_resource_dict[key][""mem-resource""])\n        min_gpu = min(min_gpu, node_resource_dict[key][""gpu-resource""])\n        hived_config[""nodelist""].append(key)\n    if not hived_config[""nodelist""]:\n        logger.error(""No worker node is detected."")\n        sys.exit(1)\n\n    hived_config[""min-gpu""] = min_gpu\n    hived_config[""unit-cpu""] = int( min_cpu / min_gpu )\n    hived_config[""unit-mem""] = int( min_mem / min_gpu )\n\n    return hived_config\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-w\', \'--worker-list-csv\', dest=""worklist"", required=True,\n                        help=""worker-list"")\n    parser.add_argument(\'-m\', \'--master-list-csv\', dest=""masterlist"", required=True,\n                        help=""master-list"")\n    parser.add_argument(\'-c\', \'--configuration\', dest=""configuration"", required=True,\n                        help=""cluster configuration"")\n    parser.add_argument(\'-o\', \'--output\', dest=""output"", required=True,\n                        help=""cluster configuration"")\n    args = parser.parse_args()\n\n    output_path = os.path.expanduser(args.output)\n\n    master_list = csv_reader(args.masterlist)\n    worker_list = csv_reader(args.worklist)\n    head_node = master_list[0]\n\n    worker_dict = csv_reader_ret_dict(args.worklist)\n    wait_nvidia_device_plugin_ready()\n    wait_amd_device_plugin_ready()\n    node_resource_dict = get_kubernetes_node_info_from_API()\n    hived_config = hived_config_prepare(worker_dict, node_resource_dict)\n\n    environment = {\n        \'master\': master_list,\n        \'worker\': worker_list,\n        \'cfg\': load_yaml_config(args.configuration),\n        \'head_node\': head_node,\n        \'hived\': hived_config\n    }\n\n    map_table = {\n        ""env"": environment\n    }\n    generate_template_file(\n        ""/quick-start-config/layout.yaml.template"",\n        ""{0}/layout.yaml"".format(output_path),\n        map_table\n    )\n    generate_template_file(\n        ""/quick-start-config/services-configuration.yaml.template"",\n        ""{0}/services-configuration.yaml"".format(output_path),\n        map_table\n    )\n\n\nif __name__ == ""__main__"":\n    main()'"
contrib/kubespray/script/pre-check-generator.py,0,"b'import logging\nimport logging.config\nimport yaml\nimport os\nimport argparse\nimport csv\nimport jinja2\nimport sys\nimport time\n\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\n\ndef csv_reader(csv_path):\n    hosts_list = []\n    with open(csv_path) as fin:\n        hosts_csv = csv.reader(fin)\n        for row in hosts_csv:\n            hosts_list.append(\n                {\n                    ""hostname"": row[0],\n                    ""ip"": row[1]\n                }\n            )\n    return hosts_list\n\n\ndef load_yaml_config(config_path):\n    with open(config_path, ""r"") as f:\n        config_data = yaml.load(f, yaml.SafeLoader)\n    return config_data\n\n\ndef read_template(template_path):\n    with open(template_path, ""r"") as f:\n        template_data = f.read()\n    return template_data\n\n\ndef generate_from_template_dict(template_data, map_table):\n    generated_file = jinja2.Template(template_data).render(\n        map_table\n    )\n    return generated_file\n\n\ndef write_generated_file(file_path, content_data):\n    with open(file_path, ""w+"") as fout:\n        fout.write(content_data)\n\n\ndef generate_template_file(template_file_path, output_path, map_table):\n    template = read_template(template_file_path)\n    generated_template = generate_from_template_dict(template, map_table)\n    write_generated_file(output_path, generated_template)\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-w\', \'--worker-list-csv\', dest=""worklist"", required=True,\n                        help=""worker-list"")\n    parser.add_argument(\'-m\', \'--master-list-csv\', dest=""masterlist"", required=True,\n                        help=""master-list"")\n    parser.add_argument(\'-c\', \'--configuration\', dest=""configuration"", required=True,\n                        help=""cluster configuration"")\n    parser.add_argument(\'-o\', \'--output\', dest=""output"", required=True,\n                        help=""cluster configuration"")\n    args = parser.parse_args()\n\n    output_path = os.path.expanduser(args.output)\n\n    master_list = csv_reader(args.masterlist)\n    head_node = master_list[0]\n\n    environment = {\n        \'master\': master_list,\n        \'worker\': csv_reader(args.worklist),\n        \'cfg\': load_yaml_config(args.configuration),\n        \'head_node\': head_node\n    }\n\n    map_table = {\n        ""env"": environment\n    }\n    generate_template_file(\n        ""quick-start/pre-check.yml.template"",\n        ""{0}/pre-check.yml"".format(output_path),\n        map_table\n    )\n\n\nif __name__ == ""__main__"":\n    main()'"
contrib/notebook-extension/openpai_submitter/data.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport json as openpai_ext_json\nimport threading as openpai_ext_threading\n\nif \'openpai_ext_lock\' not in vars():\n    openpai_ext_buffer_lock = openpai_ext_threading.Lock()\n\n\nclass openpai_ext_Storage(object):\n    \'\'\'\n    This class will not be run in multiple threads,\n    but it may be run in multiple processes.\n    It uses file system to store information and sync with each processes.\n    \'\'\'\n\n    def use_output(func):\n\n        def func_wrapper(*args, **kwargs):\n            token = args[1]\n            args = args[0:1] + args[2:]\n            ret = func(*args, **kwargs)\n            openpai_ext_buffer_lock.acquire()\n            print(""__openpai${}__"".format(token) + openpai_ext_json.dumps(\n                {\n                    \'code\': 0,\n                    \'message\': ret,\n                }\n            ), flush=True)\n            openpai_ext_buffer_lock.release()\n\n        return func_wrapper\n\n    def __init__(self, max_length=100):\n        import os\n        from openpaisdk import __flags__\n        self.os = os\n        self.max_length = max_length\n        self.dirname = os.path.join(os.path.expanduser(\'~\'), __flags__.cache)\n        self.lock_path = os.path.join(self.dirname, ""data.lock"")\n        self.data_path = os.path.join(self.dirname, ""data"")\n        if not(os.path.exists(self.data_path)):\n            self.data = []\n            self.write_to_file()\n        else:\n            self.read_file()\n\n    def acquire_lock(self):\n        if self.os.path.exists(self.lock_path):\n            raise Exception(\n                \'Unexpected lock file: {}! Please refresh the page or remove it manually!\'.format(self.lock_path))\n        with open(self.lock_path, \'w\'):\n            pass\n\n    def release_lock(self):\n        if not(self.os.path.exists(self.lock_path)):\n            raise Exception(\'Missing lock file: {}! Please refresh the page.\'.format(self.lock_path))\n        self.os.remove(self.lock_path)\n\n    def write_to_file(self):\n        self.acquire_lock()\n        try:\n            with open(self.data_path, \'w\') as f:\n                openpai_ext_json.dump(self.data, f)\n        except Exception:\n            pass\n        finally:\n            self.release_lock()\n\n    def read_file(self):\n        with open(self.data_path) as f:\n            self.data = openpai_ext_json.load(f)\n\n    @use_output\n    def get(self):\n        self.read_file()\n        return self.data\n\n    @use_output\n    def add(self, record):\n        self.read_file()\n        if len(self.data) == self.max_length:\n            self.data = self.data[1:]\n        self.data.append(record)\n        self.write_to_file()\n        return record\n\n    @use_output\n    def clear(self):\n        self.data = []\n        self.write_to_file()\n        return """"\n\n    @use_output\n    def save(self, data):\n        self.data = data\n        self.write_to_file()\n        return """"\n\n\nopenpai_ext_storage = openpai_ext_Storage()\n'"
contrib/notebook-extension/openpai_submitter/main.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport threading as openpai_ext_threading\nimport json as openpai_ext_json\nfrom openpaisdk import __flags__ as openpai_ext_flags\n\nopenpai_ext_flags.disable_to_screen = True\n\nif \'openpai_ext_lock\' not in vars():\n    openpai_ext_buffer_lock = openpai_ext_threading.Lock()\n\n\nclass openpai_ext_Thread(openpai_ext_threading.Thread):\n    \'\'\'\n    In Javascript:\n        Each time the code executed by Jupyter.notebook.kernel.execute gives output,\n        the callback function in callbacks.iopub.output will receive message.\n\n    In Python:\n        We run python code in a new thread to avoid blocking the notebook.\n        The handler is set to print json messages,\n        thus the callback in javascript will get noticed.\n    \'\'\'\n\n    def success_handler(self, ret):\n        openpai_ext_buffer_lock.acquire()\n        print(""__openpai${}__"".format(self.token) + openpai_ext_json.dumps(\n            {\n                \'code\': 0,\n                \'message\': ret,\n            }\n        ), flush=True)\n        openpai_ext_buffer_lock.release()\n\n    def err_handler(self, e):\n        openpai_ext_buffer_lock.acquire()\n        print(""__openpai${}__"".format(self.token) + openpai_ext_json.dumps(\n            {\n                \'code\': -1,\n                \'message\': str(e),\n            }\n        ), flush=True)\n        openpai_ext_buffer_lock.release()\n\n    def __init__(self, target, token, args=[], kwargs={}):\n        super(openpai_ext_Thread, self).__init__()\n        self.target = target\n        self.token = token\n        self.args = args\n        self.kwargs = kwargs\n\n    def run(self):\n        try:\n            ret = self.target(*self.args, **self.kwargs)\n            self.success_handler(ret)\n        except Exception as e:\n            import traceback\n            self.err_handler(traceback.format_exc())\n\n\nclass openpai_ext_Interface(object):\n\n    def __init__(self):\n        from openpaisdk import LayeredSettings, ClusterList\n        if LayeredSettings.get(\'container-sdk-branch\') != \'master\':\n            LayeredSettings.update(\'user_basic\', \'container-sdk-branch\', \'master\')\n        self.cll = ClusterList().load()\n\n    def execute(self, target, token, args=[], kwargs={}):\n        t = openpai_ext_Thread(target, token, args, kwargs)\n        t.start()\n\n    def tell_resources(self, token):\n        self.execute(self.cll.tell, token)\n\n    def available_resources(self, token):\n        self.execute(self.cll.available_resources, token)\n\n    def read_defaults(self, token):\n        def _read_defaults_helper():\n            from openpaisdk import LayeredSettings\n            from openpaisdk.job import JobResource\n            # add default settings\n            image_list = LayeredSettings.get(\'image-list\')\n            if image_list is None or len(image_list) == 0:\n                # add default images here\n                default_images = [\n                    \'openpai/pytorch-py36-cu90\',\n                    \'openpai/pytorch-py36-cpu\',\n                    \'openpai/tensorflow-py36-cu90\',\n                    \'openpai/tensorflow-py36-cpu\',\n                ]\n                for image in default_images:\n                    LayeredSettings.update(\'global_default\', \'image-list\', image)\n                image_list = LayeredSettings.get(\'image-list\')\n            resource_list = JobResource.parse_list(LayeredSettings.get(\'resource-list\'))\n            if resource_list is None or len(resource_list) == 0:\n                # add default resource here\n                default_resources = [\n                    \'1,4,8g\',\n                    \'1,8,16g\',\n                    \'0,4,8g\',\n                    \'2,8,16g\',\n                    \'4,16,32g\',\n                ]\n                for resource in default_resources:\n                    LayeredSettings.update(\'global_default\', \'resource-list\', resource)\n                resource_list = JobResource.parse_list(LayeredSettings.get(\'resource-list\'))\n            return {\n                \'image-list\': image_list,\n                \'resource-list\': resource_list,\n                \'web-default-form\': LayeredSettings.get(\'web-default-form\'),\n                \'web-default-image\': LayeredSettings.get(\'web-default-image\'),\n                \'web-default-resource\': LayeredSettings.get(\'web-default-resource\'),\n            }\n        self.execute(_read_defaults_helper, token)\n\n    def __set_selected(self, ctx):\n        from openpaisdk import LayeredSettings\n        LayeredSettings.update(\'global_default\', \'web-default-form\', ctx[\'form\'])\n        LayeredSettings.update(\'global_default\', \'web-default-image\', ctx[\'docker_image\'])\n        LayeredSettings.update(\'global_default\', \'web-default-resource\', \',\'.join([str(ctx[\'gpu\']), str(ctx[\'cpu\']), str(ctx[\'memoryMB\'])]))\n\n    def __submit_job_helper(self, ctx):\n        import tempfile\n        from openpaisdk import Job\n        import os\n        import sys\n        from openpaisdk.notebook import get_notebook_path\n        from openpaisdk import LayeredSettings\n        import yaml\n\n        # save settings\n        self.__set_selected(ctx)\n\n        # setting layers description\n        # layer name     | from                                 : priority\n        # user_advanced  | NotebookConfiguration.set            : 0\n        # user_basic     | extension panel selection            : 1\n        # local_default  | deaults in .openpai/defaults.yaml    : 2\n        # global_default | defaults in ~/.openpai/defaults.yaml : 3\n        # -              | predefined in flags.py               : 4\n        LayeredSettings.update(""user_basic"", ""cluster-alias"", ctx[\'cluster\'])\n        LayeredSettings.update(""user_basic"", ""virtual-cluster"", ctx[\'vc\'])\n        LayeredSettings.update(""user_basic"", ""image"", ctx[\'docker_image\'])\n        LayeredSettings.update(""user_basic"", ""cpu"", ctx[\'cpu\']),\n        LayeredSettings.update(""user_basic"", ""gpu"", ctx[\'gpu\']),\n        LayeredSettings.update(""user_basic"", ""memoryMB"", ctx[\'memoryMB\'])\n\n        cfgs = LayeredSettings.as_dict()\n\n        notebook_path = get_notebook_path()\n        _, _, sources = next(os.walk(\'.\'))\n\n        if ctx[\'form\'] == \'file\':\n            jobname = \'python_\' + tempfile.mkdtemp()[-8:]\n            mode = \'script\'\n        elif ctx[\'form\'] == \'notebook\':\n            jobname = \'jupyter_\' + tempfile.mkdtemp()[-8:]\n            mode = \'interactive\'\n        else:\n            jobname = \'silent_\' + tempfile.mkdtemp()[-8:]\n            mode = \'silent\'\n\n        job = Job(jobname)\\\n            .from_notebook(\n                nb_file=get_notebook_path(),\n                cluster={\n                    \'cluster_alias\': cfgs[\'cluster-alias\'],\n                    \'virtual_cluster\': cfgs[\'virtual-cluster\'],\n                    \'workspace\': cfgs[\'workspace\'],\n                },\n                mode=mode,\n                **{\n                    \'token\': \'\',\n                    \'image\': cfgs[""image""],\n                    \'resources\': {\n                        \'cpu\': cfgs[""cpu""],\n                        \'gpu\': cfgs[""gpu""],\n                        \'memoryMB\': cfgs[""memoryMB""],\n                        \'mem\': cfgs[\'mem\']\n                    },\n                    \'sources\': sources + cfgs[""sources""],\n                    \'pip_installs\': cfgs[""pip-installs""],\n                }\n        )\n        ctx[\'job_config\'] = yaml.dump(job.get_config(), default_flow_style=False)\n        ctx[\'jobname\'] = job.name\n        if ctx[\'type\'] == \'quick\':\n            ret = job.submit()\n            ctx[\'joblink\'] = ret[\'job_link\']\n            ctx[\'jobname\'] = ret[\'job_name\']\n        return ctx\n\n    def submit_job(self, token, ctx):\n        self.execute(self.__submit_job_helper, token, args=[ctx])\n\n    def __wait_jupyter_helper(self, ctx):\n        from openpaisdk import Job\n        job = Job(ctx[\'jobname\']).load(cluster_alias=ctx[\'cluster\'])\n        ret = job.wait()\n        ret = job.connect_jupyter()  # ret will be None if run in silent mode and without this\n        ctx[\'state\'] = ret[\'state\']\n        if ret[\'notebook\'] is None:\n            ctx[\'notebook_url\'] = \'-\'\n        else:\n            ctx[\'notebook_url\'] = ret[\'notebook\']\n        return ctx\n\n    def wait_jupyter(self, token, ctx):\n        self.execute(self.__wait_jupyter_helper, token, args=[ctx])\n\n    def __detect_jobs_helper(self, jobs_ctx):\n        from openpaisdk import Job\n        ret = []\n        for ctx in jobs_ctx:\n            try:\n                job = Job(ctx[\'jobname\']).load(cluster_alias=ctx[\'cluster\'])\n                job_info = job.connect_jupyter()\n                ctx[\'state\'] = job_info[\'state\']\n                ctx[\'notebook_url\'] = job_info[\'notebook\']\n                if ctx[\'notebook_url\'] is None:\n                    ctx[\'notebook_url\'] = \'-\'\n            except Exception as e:\n                ctx[\'state\'] = \'<span title=""{}"">UNKNOWN</span>\'.format(e)\n                ctx[\'notebook_url\'] = \'-\'\n            finally:\n                ret.append(ctx)\n        return ret\n\n    def detect_jobs(self, token, jobs_ctx):\n        self.execute(self.__detect_jobs_helper, token, args=[jobs_ctx])\n\n\nopenpai_ext_interface = openpai_ext_Interface()\n'"
contrib/python-sdk/examples/hello.py,0,"b'def say_hello():\n    print(""Hello, OpenPAI"")\n'"
contrib/python-sdk/examples/run_all_notebooks.py,0,"b'import os\nimport sys\nimport shutil\nfrom openpaisdk.utils import run_command\nfrom openpaisdk.io_utils import browser_open\n\n\ntry:\n    import nbmerge\nexcept:\n    run_command([sys.executable, \'-m pip install nbmerge\'])\n\ntest_notebooks = [\n    \'0-install-sdk-specify-openpai-cluster.ipynb\',\n    \'1-submit-and-query-via-command-line.ipynb\',\n    # \'2-submit-job-from-local-notebook.ipynb\',\n]\n\nmerged_file = ""integrated_tests.ipynb""\nhtml_file = os.path.splitext(merged_file)[0] + \'.html\'\nshutil.rmtree(merged_file, ignore_errors=True)\nshutil.rmtree(html_file, ignore_errors=True)\n\n# clear output for committing\nfor f in test_notebooks:\n    os.system(""jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace %s"" % f)\n\nos.system(\'nbmerge %s -o %s\' % (\' \'.join(test_notebooks), merged_file))\nos.system(\'jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.allow_errors=True --to html --execute %s\' % merged_file)\n\nbrowser_open(html_file)'"
contrib/python-sdk/openpaisdk/__init__.py,0,"b'from openpaisdk.flags import __flags__\nfrom openpaisdk.io_utils import to_screen\nfrom openpaisdk.defaults import get_defaults, update_default, LayeredSettings\nfrom openpaisdk.cluster import ClusterList, Cluster\nfrom openpaisdk.job import Job, JobStatusParser\n\n\n__version__ = \'0.4.00\'\n\n\ndef in_job_container(varname: str = \'PAI_CONTAINER_ID\'):\n    """"""in_job_container check whether it is inside a job container (by checking environmental variables)\n\n\n    Keyword Arguments:\n        varname {str} -- the variable to test (default: {\'PAI_CONTAINER_ID\'})\n\n    Returns:\n        [bool] -- return True is os.environ[varname] is set\n    """"""\n    if not os.environ.get(varname, \'\'):\n        return False\n    return True\n'"
contrib/python-sdk/openpaisdk/cli_arguments.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n""""""This file provides a mechanism to couple a Namespace (argparse) and pai protocol\n""""""\nimport argparse\nfrom openpaisdk.defaults import LayeredSettings\n\n\nclass ArgumentFactory:\n\n    def __init__(self):\n        self.factory = dict()\n\n        # deal with predefined defaults\n        for name, params in LayeredSettings.definitions.items():\n            args = [\'--\' + name]\n            abbr = params.get(\'abbreviation\', None)\n            if abbr:  # args = [\'--{name}\', \'-{abbr}\' or \'--{abbr}\']\n                args += [(\'-\' if len(abbr) == 1 else \'--\') + abbr]\n            kwargs = {k: v for k, v in params.items() if k not in [""name"", ""abbreviation""]}\n            kwargs[""default""] = LayeredSettings.get(name)\n            self.add_argument(*args, **kwargs)\n\n        # cluster\n        self.add_argument(\'cluster_alias\', help=\'cluster alias to select\')\n\n        self.add_argument(\'--pai-uri\', help=""uri of openpai cluster, in format of http://x.x.x.x"")\n        self.add_argument(\'--user\', help=\'username\')\n        self.add_argument(\'--password\', help=""password"")\n        self.add_argument(\'--authen-token\', \'--token\', dest=\'token\', help=""authentication token"")\n\n        self.add_argument(\'--editor\', default=""code"", help=""path to your editor used to open files"")\n\n        # job spec\n        self.add_argument(\'--job-name\', \'-j\', help=\'job name\')\n\n        self.add_argument(\'--is-global\', \'-g\', action=""store_true"",\n                          help=""set globally (not limited to current working folder)"", default=False)\n        self.add_argument(\'--update\', \'-u\', action=\'append\',\n                          help=\'replace current key-value pairs with new key=value (key1:key2:...=value for nested objects)\')\n        self.add_argument(\'--preview\', action=\'store_true\', help=\'preview result before doing action\')\n        self.add_argument(\'--no-browser\', action=\'store_true\', help=\'does not open the job link in web browser\')\n        self.add_argument(\'--interactive\', action=\'store_true\', help=\'enter the interactive mode after job starts\')\n        self.add_argument(\'--notebook-token\', \'--token\', dest=\'token\', default=""abcd"",\n                          help=\'jupyter notebook authentication token\')\n        self.add_argument(""--python"", default=""python"",\n                          help=""command or path of python, default is {python}, may be {python3}"")\n\n        self.add_argument(\'--cmd-sep\', default=""\\s*&&\\s*"", help=""command separator, default is (&&)"")\n        self.add_argument(\'commands\', nargs=argparse.REMAINDER, help=\'shell commands to execute\')\n\n        # runtime\n        self.add_argument(\'config\', nargs=\'?\', help=\'job config file\')\n        self.add_argument(\'notebook\', nargs=\'?\', help=\'Jupyter notebook file\')\n\n        # storage\n        self.add_argument(\'--recursive\', action=\'store_true\', default=False, help=""recursive target operation"")\n        self.add_argument(\'--overwrite\', action=\'store_true\', default=False, help=""enable overwrite if exists"")\n        self.add_argument(\'local_path\', help=""local path"")\n        self.add_argument(\'remote_path\', help=""remote path"")\n\n    def add_argument(self, *args, **kwargs):\n        self.factory[args[0]] = dict(args=args, kwargs=kwargs)\n\n    def get(self, key):\n        value = self.factory[key]\n        return value[\'args\'], value[\'kwargs\']\n\n\n__arguments_factory__ = ArgumentFactory()\n\n\ndef cli_add_arguments(parser: argparse.ArgumentParser, args: list):\n    for a in args:\n        args, kwargs = __arguments_factory__.get(a)\n        # assert parser.conflict_handler == \'resolve\', ""set conflict_handler to avoid duplicated""\n        parser.add_argument(*args, **kwargs)\n'"
contrib/python-sdk/openpaisdk/cli_factory.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport argparse\nfrom openpaisdk.io_utils import to_screen\nfrom openpaisdk.job import Job\nfrom openpaisdk.cluster import ClusterList\n\n\nclass ArgumentError(Exception):\n\n    pass\n\n\nclass Action:\n\n    def __init__(self, action: str, help_s: str):\n        self.action, self.help_s = action, help_s\n\n    def define_arguments(self, parser: argparse.ArgumentParser):\n        pass\n\n    def check_arguments(self, args):\n        pass\n\n    def restore(self, args):\n        pass\n\n    def store(self, args):\n        pass\n\n    def do_action(self, args):\n        raise NotImplementedError\n\n\nclass ActionFactory(Action):\n\n    def __init__(self, action: str, allowed_actions: dict):\n        assert action in allowed_actions, (""unsupported action of job"", action)\n        super().__init__(action, allowed_actions[action])\n        suffix = action.replace(\'-\', \'_\')\n        for attr in [""define_arguments"", ""check_arguments"", ""do_action""]:\n            if hasattr(self, f""{attr}_{suffix}""):\n                setattr(self, attr, getattr(self, f""{attr}_{suffix}""))\n            else:\n                assert attr != ""do_action"", f""must specify a method named {attr}_{suffix} in {self.__class__.__name__}""\n\n        self.__job__ = Job()\n        self.__clusters__ = ClusterList()\n        self.enable_svaing = dict(job=False, clusters=False)\n\n    def restore(self, args):\n        if getattr(args, \'job_name\', None):\n            self.__job__.load(job_name=args.job_name)\n        self.__clusters__.load()\n        return self\n\n    def store(self, args):\n        if self.enable_svaing[""job""]:\n            self.__job__.save()\n        if self.enable_svaing[""clusters""]:\n            self.__clusters__.save()\n        return self\n\n\nclass Scene:\n\n    def __init__(self, scene: str, help_s: str, parser: argparse.ArgumentParser,\n                 action_list  # type: list[Action]\n                 ):\n        self.scene, self.help_s = scene, help_s\n        self.single_action = len(action_list) == 1 and scene == action_list[0].action\n        if self.single_action:\n            self.actor = action_list[0]\n            self.actor.define_arguments(parser)\n        else:\n            self.actions, subparsers = dict(), parser.add_subparsers(dest=\'action\', help=help_s)\n            for a in action_list:\n                p = subparsers.add_parser(a.action, help=a.help_s)\n                a.define_arguments(p)\n                self.actions[a.action] = a\n\n    def process(self, args):\n        actor = self.actor if self.single_action else self.actions[args.action]\n        actor.check_arguments(args)\n        actor.restore(args)\n        result = actor.do_action(args)\n        actor.store(args)\n        return result\n\n\nclass EngineFactory:\n\n    def __init__(self, cli_structure):\n        self.parser = argparse.ArgumentParser(\n            description=\'command line interface for OpenPAI\',\n            formatter_class=argparse.ArgumentDefaultsHelpFormatter\n        )\n        subparsers = self.parser.add_subparsers(\n            dest=\'scene\',\n            help=\'openpai cli working scenarios\',\n        )\n        self.scenes = dict()\n        for k, v in cli_structure.items():\n            p = subparsers.add_parser(k, help=v[0])\n            self.scenes[k] = Scene(k, v[0], p, v[1])\n\n    def process(self, a: list):\n        to_screen(f\'Received arguments {a}\', _type=""debug"")\n        args = self.parser.parse_args(a)\n        return self.process_args(args)\n\n    def process_args(self, args):\n        to_screen(f\'Parsed arguments {args}\', _type=""debug"")\n        if not args.scene:\n            self.parser.print_help()\n            return\n        return self.scenes[args.scene].process(args)\n'"
contrib/python-sdk/openpaisdk/cluster.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom openpaisdk.io_utils import from_file, to_file, to_screen\nfrom openpaisdk.storage import Storage\nfrom openpaisdk.utils import OrganizedList\nfrom openpaisdk.utils import get_response, na, exception_free, RestSrvError, concurrent_map\n\n\ndef get_cluster(alias: str, fname: str = None, get_client: bool = True):\n    """"""the generalized function call to load cluster\n    return cluster client if assert get_client else return config""""""\n    if get_client:\n        return ClusterList().load(fname).get_client(alias)\n    else:\n        return ClusterList().load(fname).select(alias)\n\n\nclass ClusterList:\n    """"""Data structure corresponding to the contents of ~/.openpai/clusters.yaml\n    We use an OrganizedList to handle the operations to this class\n    """"""\n\n    def __init__(self, clusters: list = None):\n        self.clusters = OrganizedList(clusters, _key=""cluster_alias"") if clusters else []\n\n    def load(self, fname: str = None):\n        fname = na(fname, self.default_config_file)\n        self.clusters = OrganizedList(from_file(fname, default=[]), _key=""cluster_alias"")\n        return self\n\n    def save(self):\n        to_file(self.clusters.as_list, self.default_config_file)\n\n    @property\n    def default_config_file(self):\n        from openpaisdk.flags import __flags__\n        from openpaisdk.defaults import get_defaults\n        return __flags__.get_cluster_cfg_file(get_defaults()[""clusters-in-local""])\n\n    def tell(self):\n        return {\n            a: {\n                v: dict(GPUs=\'-\', memory=\'-\', vCores=\'-\', uri=cfg[""pai_uri""], user=cfg[""user""]) for v in cfg[""virtual_clusters""]\n            } for a, cfg in self.clusters.as_dict.items()\n        }\n\n    def add(self, cluster: dict):\n        cfg = Cluster().load(**cluster).check().config\n        self.clusters.add(cfg, replace=True)\n        return self\n\n    def update_all(self):\n        for a in self.aliases:\n            self.add(self.clusters.first(a))\n\n    def delete(self, alias: str):\n        return self.clusters.remove(alias)\n\n    def select(self, alias: str):\n        return self.clusters.first(alias)\n\n    def get_client(self, alias: str):\n        return Cluster().load(**self.select(alias))\n\n    def available_resources(self):\n        """"""concurrent version to get available resources""""""\n        aliases = self.aliases\n        ret = concurrent_map(Cluster.available_resources, (self.get_client(a) for a in aliases))\n        return {a: r for a, r in zip(aliases, ret) if r is not None}\n\n    @property\n    def aliases(self):\n        return [c[""cluster_alias""] for c in self.clusters if ""cluster_alias"" in c]\n\n    @property\n    def alias(self):\n        return self.config[""cluster_alias""]\n\n\nclass Cluster:\n    """"""A wrapper of cluster to access the REST APIs""""""\n\n    def __init__(self, toke_expiration: int = 3600):\n        # ! currently sdk will not handle toke refreshing\n        self.config = {}\n        self.__token_expire = toke_expiration\n        self.__token = None\n\n    def load(self, cluster_alias: str = None, pai_uri: str = None, user: str = None, password: str = None, token: str = None, **kwargs):\n        import re\n        self.config.update(\n            cluster_alias=cluster_alias,\n            pai_uri=pai_uri.strip(""/""),\n            user=user,\n            password=password,\n            token=token,\n        )\n        self.config.update(\n            {k: v for k, v in kwargs.items() if k in [""info"", ""storages"", ""virtual_clusters""]}\n        )\n        # validate\n        assert self.alias, ""cluster must have an alias""\n        assert self.user, ""must specify a user name""\n        assert re.match(""^(http|https)://(.*[^/])$"",\n                        self.pai_uri), ""pai_uri should be a uri in the format of http(s)://x.x.x.x""\n        return self\n\n    def check(self):\n        to_screen(""try to connect cluster {}"".format(self.alias))\n        storages = self.rest_api_storages()\n        for i, s in enumerate(storages):\n            s.setdefault(""storage_alias"", s[""protocol""] + f\'-{i}\')\n        cluster_info = na(self.rest_api_cluster_info(), {})\n        if cluster_info.get(""authnMethod"", ""basic"") == ""OIDC"":\n            assert self.config[""token""], ""must use authentication token (instead of password) in OIDC mode""\n        self.config.update(\n            info=cluster_info,\n            storages=storages,\n            virtual_clusters=self.virtual_clusters(),\n        )\n        # ! will check authentication types according to AAD enabled or not\n        return self\n\n    @property\n    def alias(self):\n        return self.config[""cluster_alias""]\n\n    @property\n    def pai_uri(self):\n        return self.config[""pai_uri""].strip(""/"")\n\n    @property\n    def user(self):\n        return self.config[""user""]\n\n    @property\n    def password(self):\n        return str(self.config[""password""])\n\n    @property\n    def token(self):\n        if self.config[""token""]:\n            return str(self.config[""token""])\n        if not self.__token:\n            self.__token = self.rest_api_token(self.__token_expire)\n        return self.__token\n\n    def get_storage(self, alias: str = None):\n        # ! every cluster should have a builtin storage\n        for sto in self.config.get(""storages"", []):\n            if alias is None or sto[""storage_alias""] == alias:\n                if sto[""protocol""] == \'hdfs\':\n                    return Storage(protocol=\'webHDFS\', url=sto[""webhdfs""], user=sto.get(\'user\', self.user))\n\n    def get_job_link(self, job_name: str):\n        return \'{}/job-detail.html?username={}&jobName={}\'.format(self.pai_uri, self.user, job_name)\n\n    @property\n    def rest_srv(self):\n        return \'{}/rest-server/api\'.format(self.pai_uri)\n\n    # ! for some older version that does not support this API\n    @exception_free(Exception, None, ""Cluster info API is not supported"")\n    def rest_api_cluster_info(self):\n        ""refer to https://github.com/microsoft/pai/pull/3281/""\n        return get_response(\'GET\', [self.rest_srv, \'v1\'], allowed_status=[200]).json()\n\n    def rest_api_storages(self):\n        # ! currently this is a fake\n        return [\n            {\n                ""protocol"": ""hdfs"",\n                ""webhdfs"": f""{self.pai_uri}/webhdfs""\n            },\n        ]\n\n    @exception_free(RestSrvError, None)\n    def rest_api_job_list(self, user: str = None):\n        return get_response(\n            \'GET\', [self.rest_srv, \'v1\', (\'user\', user), \'jobs\']\n        ).json()\n\n    @exception_free(RestSrvError, None)\n    def rest_api_job_info(self, job_name: str = None, info: str = None, user: str = None):\n        import json\n        import yaml\n        user = self.user if user is None else user\n        assert info in [None, \'config\', \'ssh\'], (\'unsupported query information\', info)\n        response = get_response(\n            \'GET\', [self.rest_srv, \'v1\', \'user\', user, \'jobs\', job_name, info]\n        )\n        try:\n            return response.json()\n        except json.decoder.JSONDecodeError:\n            return yaml.load(response.text, Loader=yaml.FullLoader)\n        else:\n            raise RestSrvError\n\n    @exception_free(Exception, None)\n    def rest_api_token(self, expiration=3600):\n        return get_response(\n            \'POST\', [self.rest_srv, \'v1\', \'token\'],\n            body={\n                \'username\': self.user, \'password\': self.password, \'expiration\': expiration\n            }\n        ).json()[\'token\']\n\n    def rest_api_submit(self, job: dict):\n        use_v2 = str(job.get(""protocolVersion"", 1)) == ""2""\n        if use_v2:\n            import yaml\n            return get_response(\n                \'POST\', [self.rest_srv, \'v2\', \'jobs\'],\n                headers={\n                    \'Authorization\': \'Bearer {}\'.format(self.token),\n                    \'Content-Type\': \'text/yaml\',\n                },\n                body=yaml.dump(job),\n                allowed_status=[202, 201]\n            )\n        else:\n            return get_response(\n                \'POST\', [self.rest_srv, \'v1\', \'user\', self.user, \'jobs\'],\n                headers={\n                    \'Authorization\': \'Bearer {}\'.format(self.token),\n                    \'Content-Type\': \'application/json\',\n                },\n                body=job,\n                allowed_status=[202, 201]\n            )\n\n    @exception_free(RestSrvError, None)\n    def rest_api_execute_job(self, job_name: str, e_type: str = ""STOP""):\n        assert e_type in [""START"", ""STOP""], ""unsupported execute type {}"".format(e_type)\n        return get_response(\n            \'PUT\', [self.rest_srv, \'v1\', \'user\', self.user, \'jobs\', job_name, \'executionType\'],\n            headers={\n                \'Authorization\': \'Bearer {}\'.format(self.token),\n            },\n            body={\n                ""value"": e_type\n            },\n            allowed_status=[200, 202],\n        ).json()\n\n    @exception_free(RestSrvError, None)\n    def rest_api_virtual_clusters(self):\n        return get_response(\n            \'GET\', [self.rest_srv, \'v1\', \'virtual-clusters\'],\n            headers={\n                \'Authorization\': \'Bearer {}\'.format(self.token),\n                \'Content-Type\': \'application/json\',\n            },\n            allowed_status=[200]\n        ).json()\n\n    @exception_free(RestSrvError, None)\n    def rest_api_user(self, user: str = None):\n        return get_response(\n            \'GET\', [self.rest_srv, \'v1\', \'user\', user if user else self.user],\n            headers={\n                \'Authorization\': \'Bearer {}\'.format(self.token),\n            },\n        ).json()\n\n    def virtual_clusters(self, user_info: dict = None):\n        user_info = na(user_info, self.rest_api_user())\n        assert user_info, f\'failed to get user information from {self.alias}\'\n        my_virtual_clusters = user_info[""virtualCluster""]\n        if isinstance(my_virtual_clusters, str):\n            my_virtual_clusters = my_virtual_clusters.split("","")\n        return my_virtual_clusters\n\n    def virtual_cluster_available_resources(self):\n        vc_info = self.rest_api_virtual_clusters()\n        dic = dict()\n        for key, vc in vc_info.items():\n            if ""resourcesTotal"" in vc:\n                used, total = vc[""resourcesUsed""], vc[""resourcesTotal""]\n                dic[key] = {\n                    k: max(0, int(total[k] - used[k])) for k in total\n                }\n            else:\n                # return -1 if the REST api not supported\n                dic[key] = dict(GPUs=-1, memory=-1, vCores=-1)\n        return dic\n\n    @exception_free(Exception, None)\n    def available_resources(self):\n        resources = self.virtual_cluster_available_resources()\n        return {k: v for k, v in resources.items() if k in self.config[""virtual_clusters""]}\n'"
contrib/python-sdk/openpaisdk/command_line.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport argparse\nimport os\nimport sys\nfrom openpaisdk.cli_arguments import cli_add_arguments\nfrom openpaisdk.cli_factory import ActionFactory, EngineFactory\nfrom openpaisdk.defaults import get_defaults, update_default\nfrom openpaisdk.io_utils import browser_open, to_screen\nfrom openpaisdk.utils import Nested, run_command, na, randstr\nfrom openpaisdk.defaults import __flags__\n\n\ndef extract_args(args: argparse.Namespace, get_list: list = None, ignore_list: list = [""scene"", ""action""]):\n    if get_list:\n        return {k: getattr(args, k) for k in get_list}\n    return {k: v for k, v in vars(args).items() if k not in ignore_list}\n\n\nclass ActionFactoryForDefault(ActionFactory):\n\n    def define_arguments(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--is-global\'])\n        parser.add_argument(\'contents\', nargs=\'*\', help=\'(variable=value) pair to be set as default\')\n\n    def do_action_set(self, args):\n        import re\n        if not args.contents:\n            return get_defaults(False, True, False) if args.is_global else get_defaults(True, True, False)\n        kv_pairs = []\n        for content in args.contents:\n            m = re.match(""^([^=]+?)([\\+|\\-]*=)([^=]*)$"", content)\n            if m:\n                kv_pairs.append(m.groups())\n            else:\n                kv_pairs.append((content, \'\', \'\'))\n        for kv_pair in kv_pairs:\n            assert kv_pair[0] and kv_pair[1] in [""="", ""+="", ""-=""] and kv_pair[2], \\\n                f""must specify a key=value pair ({kv_pair[0]}, {kv_pair[2]})""\n            update_default(kv_pair[0], kv_pair[2], is_global=args.is_global)\n\n    def do_action_unset(self, args):\n        for kv_pair in args.contents:\n            update_default(kv_pair[0], kv_pair[2], is_global=args.is_global, to_delete=True)\n\n\nclass ActionFactoryForCluster(ActionFactory):\n\n    def define_arguments_edit(self, parser):\n        cli_add_arguments(parser, [""--editor""])\n\n    def check_arguments_edit(self, args):\n        assert args.editor, ""cannot edit the file without an editor""\n\n    def do_action_edit(self, args):\n        run_command([args.editor, cluster_cfg_file])\n\n    def define_arguments_update(self, parser):\n        pass\n\n    def do_action_update(self, args):\n        self.enable_svaing[""clusters""] = True\n        return self.__clusters__.update_all()\n\n    def define_arguments_list(self, parser):\n        cli_add_arguments(parser, [])\n\n    @staticmethod\n    def tabulate_resources(dic: dict):\n        to_screen([\n            [c, i.get(""uri"", None), i.get(""user"", None), v, i[""GPUs""], i[""vCores""], i[""memory""]] for c in dic.keys() for v, i in dic[c].items()\n        ], _type=""table"", headers=[""cluster"", ""uri"", ""user"", ""virtual-cluster"", ""GPUs"", ""vCores"", ""memory""])\n        return dic\n\n    def do_action_list(self, args):\n        info = self.__clusters__.tell()\n        ActionFactoryForCluster.tabulate_resources(info)\n\n    def define_arguments_resources(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [])\n\n    def do_action_resources(self, args):\n        r = self.__clusters__.available_resources()\n        ActionFactoryForCluster.tabulate_resources(r)\n\n    def define_arguments_add(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--pai-uri\', \'--user\', \'--password\', \'--authen-token\'])\n\n    def check_arguments_add(self, args):\n        assert args.cluster_alias or args.pai_uri or args.user, ""must specify cluster-alias, pai-uri, user""\n        assert args.password or args.token, ""please add an authentication credential, password or token""\n\n    def do_action_add(self, args):\n        self.enable_svaing[""clusters""] = True\n        self.__clusters__.add(extract_args(args))\n\n    def define_arguments_delete(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'cluster_alias\'])\n\n    def do_action_delete(self, args):\n        if self.__clusters__.delete(args.cluster_alias):\n            to_screen(""cluster %s deleted"" % args.cluster_alias)\n        return None\n\n    def define_arguments_select(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--is-global\', \'cluster_alias\'])\n\n    def check_arguments_select(self, args):\n        assert args.cluster_alias, ""must specify a valid cluster-alias""\n\n    def do_action_select(self, args):\n        update_default(\'cluster-alias\', args.cluster_alias,\n                       is_global=args.is_global)\n\n\nclass ActionFactoryForJob(ActionFactory):\n\n    # basic commands\n    def define_arguments_list(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--cluster-alias\', \'--user\'])\n\n    def do_action_list(self, args):\n        client = self.__clusters__.get_client(args.cluster_alias)\n        if not args.user:\n            args.user = client.user\n            to_screen(""if not set, only your job will be listed, user `--user __all__` to list jobs of all users"")\n        if args.user == \'__all__\':\n            args.user = None\n        jobs = client.rest_api_job_list(user=args.user)\n        return [""%s [%s]"" % (j[""name""], j.get(""state"", ""UNKNOWN"")) for j in jobs]\n\n    def define_arguments_status(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--cluster-alias\', \'--user\'])\n        parser.add_argument(\'job_name\', help=\'job name\')\n        parser.add_argument(\'query\', nargs=\'?\', choices=[\'config\', \'ssh\'])\n\n    def check_arguments_status(self, args):\n        assert args.job_name, ""must specify a job name""\n\n    def do_action_status(self, args):\n        client = self.__clusters__.get_client(args.cluster_alias)\n        if not args.user:\n            args.user = client.user\n        return client.rest_api_job_info(args.job_name, args.query, user=args.user)\n\n    def define_arguments_stop(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--cluster-alias\'])\n        parser.add_argument(\'job_names\', nargs=\'+\', help=\'job name\')\n\n    def check_arguments_stop(self, args):\n        assert args.job_names, ""must specify a job name""\n\n    def do_action_stop(self, args):\n        client = self.__clusters__.get_client(args.cluster_alias)\n        for job_name in args.job_names:\n            to_screen(client.rest_api_execute_job(job_name, ""STOP""))\n\n    def define_arguments_submit(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--virtual-cluster\', \'--preview\', \'--update\', \'config\'])\n\n    def check_arguments_submit(self, args):\n        assert args.config, ""please specify a job config file (json or yaml format)""\n        assert os.path.isfile(args.config), ""%s cannot be read"" % args.config\n\n    def submit_it(self, args):\n        if args.preview:\n            return self.__job__.validate().get_config()\n        result = self.__job__.submit(args.cluster_alias, args.virtual_cluster)\n        if ""job_link"" in result and not getattr(args, \'no_browser\', False):\n            browser_open(result[""job_link""])\n        return result\n\n    def do_action_submit(self, args):\n        # key-value pair in --update option would support nested key, e.g. defaults->virtualCluster=<your-virtual-cluster>\n        self.__job__.load(fname=args.config)\n        if args.update:\n            for s in args.update:\n                key, value = s.split(""="")\n                Nested(self.__job__.protocol).set(key, value)\n        return self.submit_it(args)\n\n    def define_essentials(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\n            \'--job-name\',\n            \'--cluster-alias\', \'--virtual-cluster\', \'--workspace\',  # for cluster\n            \'--sources\', \'--pip-installs\',  # for sdk_template\n            \'--image\', \'--cpu\', \'--gpu\', \'--mem\', ""--memoryMB"",\n            \'--preview\', \'--no-browser\',\n            \'--python\',\n        ])\n\n    def check_essentials(self, args):\n        assert args.cluster_alias, ""must specify a cluster""\n        args.sources = [] if not args.sources else args.sources\n        args.pip_installs = [] if not args.pip_installs else args.pip_installs\n        if args.sources:\n            assert args.workspace, ""must specify --workspace if --sources used""\n            for s in args.sources:\n                assert os.path.isfile(s), ""file %s not found"" % s\n        assert args.image, ""must specify a docker image""\n        if args.job_name:\n            args.job_name = args.job_name.replace(""$"", randstr(10))\n\n    def define_arguments_sub(self, parser: argparse.ArgumentParser):\n        self.define_essentials(parser)\n        cli_add_arguments(parser, [\n            \'commands\'\n        ])\n\n    def check_arguments_sub(self, args):\n        self.check_essentials(args)\n\n    def do_action_sub(self, args):\n        self.__job__.new(args.job_name).one_liner(\n            commands="" "".join(args.commands),\n            image=args.image,\n            resources=extract_args(args, [""gpu"", ""cpu"", ""memoryMB"", ""mem""]),\n            cluster=extract_args(\n                args, [""cluster_alias"", ""virtual_cluster"", ""workspace""]),\n            sources=args.sources, pip_installs=args.pip_installs,\n        )\n        self.__job__.protocol[""parameters""][""python_path""] = args.python\n        return self.submit_it(args)\n\n    def define_arguments_notebook(self, parser: argparse.ArgumentParser):\n        self.define_essentials(parser)\n        cli_add_arguments(parser, [\n            \'--interactive\',\n            \'--notebook-token\',\n            \'notebook\'\n        ])\n\n    def check_arguments_notebook(self, args):\n        self.check_essentials(args)\n        assert args.notebook or args.interactive, ""must specify a notebook name unless in interactive mode""\n        if not args.job_name:\n            assert args.notebook or args.interactive, ""must specify a notebook if no job name defined""\n            args.job_name = os.path.splitext(os.path.basename(args.notebook))[\n                0] + ""_"" + randstr().hex if args.notebook else ""jupyter_server_{}"".format(randstr().hex)\n        if args.interactive and not args.token:\n            to_screen(""no authentication token is set"", _type=""warn"")\n\n    def connect_notebook(self):\n        result = self.__job__.wait()\n        if result.get(""notebook"", None) is not None:\n            browser_open(result[""notebook""])\n        return result\n\n    def do_action_notebook(self, args):\n        self.__job__.new(args.job_name).from_notebook(\n            nb_file=args.notebook, mode=""interactive"" if args.interactive else ""silent"", token=args.token,\n            image=args.image,\n            cluster=extract_args(\n                args, [""cluster_alias"", ""virtual_cluster"", ""workspace""]),\n            resources=extract_args(args, [""gpu"", ""cpu"", ""memoryMB"", ""mem""]),\n            sources=args.sources, pip_installs=args.pip_installs,\n        )\n        self.__job__.protocol[""parameters""][""python_path""] = args.python\n        result = self.submit_it(args)\n        if not args.preview:\n            result.update(na(self.connect_notebook(), {}))\n        return result\n\n    def define_arguments_connect(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--cluster-alias\'])\n        parser.add_argument(\'job_name\', help=""job name to connect"")\n\n    def check_arguments_connect(self, args):\n        assert args.cluster_alias, ""must specify a cluster""\n        assert args.job_name, ""must specify a job name""\n\n    def do_action_connect(self, args):\n        to_screen(""retrieving job config from cluster"")\n        self.__job__.load(job_name=args.job_name, cluster_alias=args.cluster_alias)\n        return self.connect_notebook()\n\n\nclass ActionFactoryForStorage(ActionFactory):\n\n    def define_arguments_list_storage(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\'--cluster-alias\'])\n\n    def do_action_list_storage(self, args):\n        return self.__clusters__.select(args.cluster_alias)[\'storages\']\n\n    def define_arguments_list(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--storage-alias\', \'remote_path\'])\n\n    def do_action_list(self, args):\n        return self.__clusters__.get_client(args.cluster_alias).get_storage(args.storage_alias).list(args.remote_path)\n\n    def define_arguments_status(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--storage-alias\', \'remote_path\'])\n\n    def do_action_status(self, args):\n        return self.__clusters__.get_client(args.cluster_alias).get_storage(args.storage_alias).status(args.remote_path)\n\n    def define_arguments_delete(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--storage-alias\', \'--recursive\', \'remote_path\'])\n\n    def do_action_delete(self, args):\n        return self.__clusters__.get_client(args.cluster_alias).get_storage(args.storage_alias).delete(args.remote_path, recursive=args.recursive)\n\n    def define_arguments_download(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(\n            parser, [\'--cluster-alias\', \'--storage-alias\', \'remote_path\', \'local_path\'])\n\n    def do_action_download(self, args):\n        return self.__clusters__.get_client(args.cluster_alias).get_storage(args.storage_alias).download(remote_path=args.remote_path, local_path=args.local_path)\n\n    def define_arguments_upload(self, parser: argparse.ArgumentParser):\n        cli_add_arguments(parser, [\n                          \'--cluster-alias\', \'--storage-alias\', \'--overwrite\', \'local_path\', \'remote_path\'])\n\n    def do_action_upload(self, args):\n        return self.__clusters__.get_client(args.cluster_alias).get_storage(args.storage_alias).upload(remote_path=args.remote_path, local_path=args.local_path, overwrite=getattr(args, ""overwrite"", False))\n\n\ncluster_cfg_file = __flags__.get_cluster_cfg_file(get_defaults()[""clusters-in-local""])\n\n\ndef generate_cli_structure(is_beta: bool):\n    cli_s = {\n        ""cluster"": {\n            ""help"": ""cluster management"",\n            ""factory"": ActionFactoryForCluster,\n            ""actions"": {\n                ""list"": ""list clusters in config file %s"" % cluster_cfg_file,\n                ""resources"": ""report the (available, used, total) resources of the cluster"",\n                ""update"": ""check the healthness of clusters and update the information"",\n                ""edit"": ""edit the config file in your editor %s"" % cluster_cfg_file,\n                ""add"": ""add a cluster to config file %s"" % cluster_cfg_file,\n                ""delete"": ""delete a cluster from config file %s"" % cluster_cfg_file,\n                ""select"": ""select a cluster as default"",\n            }\n        },\n        ""job"": {\n            ""help"": ""job operations"",\n            ""factory"": ActionFactoryForJob,\n            ""actions"": {\n                ""list"": ""list existing jobs"",\n                ""status"": ""query the status of a job"",\n                ""stop"": ""stop the job"",\n                ""submit"": ""submit the job from a config file"",\n                ""sub"": ""generate a config file from commands, and then `submit` it"",\n                ""notebook"": ""run a jupyter notebook remotely"",\n                ""connect"": ""connect to an existing job"",\n            }\n        },\n        ""storage"": {\n            ""help"": ""storage operations"",\n            ""factory"": ActionFactoryForStorage,\n            ""actions"": {\n                ""list-storage"": ""list storage attached to the cluster"",\n                ""list"": ""list items about the remote path"",\n                ""status"": ""get detailed information about remote path"",\n                ""upload"": ""upload"",\n                ""download"": ""download"",\n                ""delete"": ""delete"",\n            }\n        },\n    }\n    dic = {\n        key: [\n            value[""help""],\n            [value[""factory""](x, value[""actions""])\n             for x in value[""actions""].keys()]\n        ] for key, value in cli_s.items()\n    }\n    dic.update({\n        ""set"": [\n            ""set a (default) variable for cluster and job"", [\n                ActionFactoryForDefault(""set"", {""set"": [""set""]})]\n        ],\n        ""unset"": [\n            ""un-set a (default) variable for cluster and job"", [\n                ActionFactoryForDefault(""unset"", {""unset"": [""unset""]})]\n        ],\n    })\n    return dic\n\n\nclass Engine(EngineFactory):\n\n    def __init__(self):\n        super().__init__(generate_cli_structure(is_beta=False))\n\n\ndef main():\n    try:\n        eng = Engine()\n        result = eng.process(sys.argv[1:])\n        if result:\n            to_screen(result)\n        return 0\n    except AssertionError as identifier:\n        to_screen(f""Value error: {repr(identifier)}"", _type=""error"")\n        return 1\n    except Exception as identifier:\n        to_screen(f""Error: {repr(identifier)}"", _type=""error"")\n        return 2\n    else:\n        return -1\n\n\nif __name__ == \'__main__\':\n    main()\n'"
contrib/python-sdk/openpaisdk/defaults.py,0,"b'\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n"""""" this module is to set a way to control the predefined configurations\n""""""\nfrom openpaisdk.flags import __flags__\nfrom openpaisdk.utils import na, OrganizedList\nfrom openpaisdk.io_utils import from_file, to_file, to_screen\n\n\nclass CfgLayer:\n\n    def __init__(self, name: str, include: list = None, exclude: list = None, file: str = None, values: dict = None, allow_unknown: bool = True):\n        self.name = name\n        self.file = file\n        self.values = from_file(file, {}, silent=True) if file else na(values, {})\n        self.definitions = OrganizedList(\n            __flags__.default_var_definitions(),\n            _key=""name""\n        ).filter(None, include, exclude)  # type: OrganizedList\n\n    def update(self, key: str, value=None, delete: bool = False):\n        if not self.allow(key):\n            to_screen(f""{key} is not a recognized default variable, ignored"")\n            return\n        dic = self.values\n        if delete:\n            if key not in dic:\n                to_screen(f""key {key} not found in {self.name}, ignored"")\n            elif not self.act_append(key) or not value:  # delete the key when not append action\n                del dic[key]\n                to_screen(f""key {key} removed completely from {self.name} successfully"")\n            else:\n                dic[key].remove(value)\n                to_screen(f""{value} removed in {key} under {self.name} successfully"")\n        else:\n            if self.act_append(key):\n                def _append(dic, key, value):\n                    dic.setdefault(key, [])\n                    if value not in dic[key]:\n                        dic[key].append(value)\n                _append(dic, key, value)\n                to_screen(f""{value} added to {key} under {self.name} successfully"")\n            else:\n                dic[key] = value\n                to_screen(f""{key} set to {value} under {self.name} successfully"")\n        if self.file:\n            to_file(self.values, self.file)\n\n    def allow(self, key: str):\n        return self.definitions.first_index(key) is not None\n\n    def act_append(self, key: str):\n        if self.allow(key):\n            return self.definitions.first(key).get(""action"", None) == ""append""\n        return False\n\n\nclass LayeredSettings:\n    """"""key-value querying from a list of dicts, priority depends on list index\n    refer to [TestDefaults](../tests/test_utils.py) for more usage examples\n    """"""\n\n    layers = None\n    definitions = None\n\n    @classmethod\n    def init(cls):\n        if cls.layers is None:\n            cls.reset()\n\n    @classmethod\n    def reset(cls):\n        cls.definitions = OrganizedList(__flags__.default_var_definitions(), _key=""name"").as_dict\n        cls.layers = OrganizedList([\n            CfgLayer(\n                name=""user_advaced"",\n                exclude=[""clusters-in-local"", ""image-list"", ""resource-specs""]\n            ),\n            CfgLayer(\n                name=""user_basic"",\n                exclude=[""clusters-in-local"", ""image-list"", ""resource-specs""]\n            ),\n            CfgLayer(\n                name=""local_default"",\n                exclude=[], file=__flags__.get_default_file(is_global=False)\n            ),\n            CfgLayer(\n                name=""global_default"",\n                exclude=[], file=__flags__.get_default_file(is_global=True)\n            )\n        ], _key=""name"", _getter=getattr)\n\n    @classmethod\n    def keys(cls):\n        dic = set()\n        for layer in cls.layers:\n            for key in layer.values.keys():\n                dic.add(key)\n        dic = dic.union(cls.definitions.keys())\n        return list(dic)\n\n    @classmethod\n    def act_append(cls, key):\n        return cls.definitions.get(key, {}).get(""action"", None) == ""append""\n\n    @classmethod\n    def get(cls, key):\n        __not_found__ = ""==Not-Found==""\n        lst = [layer.values.get(key, __not_found__) for layer in cls.layers]\n        lst.append(cls.definitions.get(key, {}).get(""default"", None))\n        lst = [x for x in lst if x != __not_found__]\n\n        if cls.act_append(key):\n            from openpaisdk.utils import flatten\n            return list(flatten(lst))\n        else:\n            return lst[0] if lst else None\n\n    @classmethod\n    def update(cls, layer: str, key: str, value=None, delete: bool = False):\n        cls.layers.first(layer).update(key, value, delete)\n\n    @classmethod\n    def as_dict(cls):\n        return {key: cls.get(key) for key in cls.keys()}\n\n    @classmethod\n    def print_supported_items(cls):\n        headers = [\'name\', \'default\', \'help\']\n        return to_screen([\n            [x.get(k, None) for k in headers] for x in __flags__.default_var_definitions()\n        ], _type=""table"", headers=headers)\n\n\nLayeredSettings.init()\n\n\ndef get_defaults(en_local=True, en_global=True, en_predefined=True):\n    return LayeredSettings.as_dict()\n\n\ndef update_default(key: str, value: str = None, is_global: bool = False, to_delete: bool = False):\n    layer = ""global_default"" if is_global else ""local_default""\n    LayeredSettings.update(layer, key, value, to_delete)\n\n\ndef get_install_uri(ver: str = None):\n    ver = get_defaults()[""container-sdk-branch""] if not ver else ver\n    return \'-e ""git+https://github.com/Microsoft/pai@{}#egg=openpaisdk&subdirectory=contrib/python-sdk""\'.format(ver)\n'"
contrib/python-sdk/openpaisdk/flags.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\n\n\nclass __flags__(object):\n    ""store the flags and constants""\n    disable_to_screen = False  # A flag to disable to_screen output\n    debug_mode = os.path.isfile(\'debug_enable\')\n\n    # ! below attributes should not be changed\n    cache = \'.openpai\'\n    cluster_cfg_file = \'clusters.yaml\'\n    defaults_file = \'defaults.yaml\'\n    container_sdk_branch = \'master\'\n    resources_requirements = dict(cpu=2, gpu=0, memoryMB=4096, ports={})\n    storage_root = \'/openpai-sdk\'\n    custom_predefined = []\n\n    @staticmethod\n    def default_var_definitions():\n        return [\n            {\n                ""name"": ""clusters-in-local"",\n                ""default"": ""no"",\n                ""help"": f""[yes / no], if yes, clusters configuration stored in {__flags__.get_cluster_cfg_file(\'yes\')} other than ~/{__flags__.get_cluster_cfg_file(\'yes\')}"",\n            },\n            {\n                ""name"": ""cluster-alias"",\n                ""abbreviation"": ""a"",\n                ""help"": ""cluster alias"",\n            },\n            {\n                ""name"": ""virtual-cluster"",\n                ""abbreviation"": ""vc"",\n                ""help"": ""virtual cluster name""\n            },\n            {\n                ""name"": ""storage-alias"",\n                ""abbreviation"": ""s"",\n                ""help"": ""alias of storage to use""\n            },\n            {\n                ""name"": ""workspace"",\n                ""default"": None,\n                ""abbreviation"": ""w"",\n                ""help"": f""storage root for a job to store its codes / data / outputs ... (default is {__flags__.storage_root}/$user)""\n            },\n            {\n                ""name"": ""container-sdk-branch"",\n                ""default"": __flags__.container_sdk_branch,\n                ""help"": ""code branch to install sdk from (in a job container)""\n            },\n            {\n                ""name"": ""image"",\n                ""abbreviation"": ""i"",\n                ""help"": ""docker image""\n            },\n            {\n                ""name"": ""cpu"",\n                ""help"": f""cpu number per instance (default is {__flags__.resources_requirements[\'cpu\']})""\n            },\n            {\n                ""name"": ""gpu"",\n                ""help"": f""gpu number per instance (default is {__flags__.resources_requirements[\'gpu\']})""\n            },\n            {\n                ""name"": ""memoryMB"",\n                ""help"": f""memory (MB) per instance (default is {__flags__.resources_requirements[\'memoryMB\']}) (will be overridden by --mem)""\n            },\n            {\n                ""name"": ""mem"",\n                ""help"": ""memory (MB / GB) per instance (default is %.0fGB)"" % (__flags__.resources_requirements[""memoryMB""] / 1024.0)\n            },\n            {\n                ""name"": ""sources"",\n                ""default"": [],\n                ""abbreviation"": ""src"",\n                ""action"": ""append"",\n                ""help"": ""source files to upload (into container)""\n            },\n            {\n                ""name"": ""pip-installs"",\n                ""default"": [],\n                ""abbreviation"": ""pip"",\n                ""action"": ""append"",\n                ""help"": ""packages to install via pip""\n            },\n            {\n                ""name"": ""image-list"",\n                ""default"": [],\n                ""action"": ""append"",\n                ""help"": ""list of images that are frequently used""\n            },\n            {\n                ""name"": ""resource-list"",\n                ""default"": [],\n                ""action"": ""append"",\n                ""help"": ""list of resource specs that are frequently used""\n            },\n            {\n                ""name"": ""web-default-form"",\n                ""help"": ""web-default-form (in Submitter)""\n            },\n            {\n                ""name"": ""web-default-image"",\n                ""help"": ""web-default-image (in Submitter)""\n            },\n            {\n                ""name"": ""web-default-resource"",\n                ""help"": ""web-default-resource (in Submitter), format: \'<gpu>,<cpu>,<memoryMB>\'""\n            },\n        ] + __flags__.custom_predefined\n\n    @staticmethod\n    def get_cluster_cfg_file(clusters_in_local: str = \'no\') -> str:\n        assert clusters_in_local in [\'no\', \'yes\'], f""only allow yes / no, but {clusters_in_local} received""\n        pth = [__flags__.cache, __flags__.cluster_cfg_file]\n        if clusters_in_local == \'no\':\n            pth = [os.path.expanduser(\'~\')] + pth\n        return os.path.join(*pth)\n\n    @staticmethod\n    def get_default_file(is_global: bool) -> str:\n        pth = [__flags__.cache, __flags__.defaults_file]\n        pth = [os.path.expanduser(\'~\')] + pth if is_global else pth\n        return os.path.join(*pth)\n\n    @staticmethod\n    def print_predefined(exclude: list = None, include: list = None):\n        from tabulate import tabulate\n        citems = __flags__.predefined_defaults(exclude, include)\n        print(tabulate(citems, headers=citems[0]._asdict().keys()), flush=True)\n'"
contrib/python-sdk/openpaisdk/io_utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport errno\nimport shutil\nfrom webbrowser import open_new_tab\nfrom contextlib import contextmanager\nfrom functools import partial\nimport json\nimport yaml\nimport logging\nfrom urllib.request import urlopen\nfrom urllib.parse import urlsplit\nfrom urllib.request import urlretrieve\nimport cgi\nfrom openpaisdk.flags import __flags__\n\nlogging.basicConfig(format=\'%(name)s - %(levelname)s - %(message)s\')\n__logger__ = logging.getLogger(name=""openpai"")\n__logger__.setLevel(level=logging.DEBUG if __flags__.debug_mode else logging.INFO)\n\n\ndef to_screen(msg, _type: str = ""normal"", **kwargs):\n    """"""a general wrapping function to deal with interactive IO and logging\n    """"""\n    def print_out(msg, **kwargs):\n        out = yaml.dump(msg, default_flow_style=False, **kwargs) if not isinstance(msg, str) else msg\n        if not __flags__.disable_to_screen:\n            print(out, flush=True)\n        return out\n\n    def print_table(msg, **kwargs):\n        from tabulate import tabulate\n        out = tabulate(msg, **kwargs)\n        if not __flags__.disable_to_screen:\n            print(out, flush=True)\n        return out\n\n    func_dict = {\n        ""normal"": print_out,\n        ""table"": print_table,\n        ""warn"": partial(__logger__.warn, exc_info=__flags__.debug_mode),\n        ""debug"": __logger__.debug,\n        ""error"": partial(__logger__.error, exc_info=True),\n    }\n    assert _type in func_dict, f""unsupported output type {_type}, only {list(func_dict.keys(()))} are valid""\n    ret = func_dict[_type](msg, **kwargs)\n    return ret if _type == ""table"" else msg\n\n\ndef listdir(path):\n    assert os.path.isdir(path), ""{} is not a valid path of directory"".format(path)\n    root, dirs, files = next(os.walk(path))\n    return {\n        ""root"": root,\n        ""dirs"": dirs,\n        ""files"": files\n    }\n\n\ndef browser_open(url: str):\n    __logger__.info(""open in browser: %s"", url)\n    try:\n        open_new_tab(url)\n    except Exception as e:\n        to_screen(f""fail to open {url} due to {repx(e)}"", _type=""warn"")\n\n\ndef from_file(fname: str, default=None, silent: bool = False, **kwargs):\n    """"""read yaml or json file; return default if (only when default is not None)\n    - file non existing\n    - empty file or contents in file is not valid\n    - loaded content is not expected type (type(default))\n    """"""\n    import yaml\n    assert os.path.splitext(fname)[1] in __json_exts__ + __yaml_exts__, f""unrecognized {fname}""\n    try:\n        with open(fname) as fp:\n            dic = dict(kwargs)\n            dic.setdefault(\'Loader\', yaml.FullLoader)\n            ret = yaml.load(fp, **dic)\n            assert ret, f""read empty object ({ret}) from {fname}, return {default}""\n            assert default is None or isinstance(\n                ret, type(default)), f""read wrong type ({type(ret)}, expected {type(default)}) from {fname}, return {default}""\n            return ret\n    except Exception as identifier:\n        if default is None:\n            to_screen(f""{repr(identifier)} when reading {fname}"", _type=""error"")\n            raise identifier\n        if not silent:\n            to_screen(f""{repr(identifier)} when reading {fname}"", _type=""warn"")\n        return default\n\n\ndef get_url_filename_from_server(url):\n    try:\n        blah = urlopen(url).info()[\'Content-Disposition\']\n        _, params = cgi.parse_header(blah)\n        return params[""filename""]\n    except Exception as e:\n        to_screen(f\'Failed to get filename from server: {repr(e)}\', _type=""warn"")\n        return None\n\n\ndef web_download_to_folder(url: str, folder: str, filename: str = None):\n    if not filename:\n        split = urlsplit(url)\n        filename = split.path.split(""/"")[-1]\n    filename = os.path.join(folder, filename)\n    os.makedirs(folder, exist_ok=True)\n    try:\n        urlretrieve(url, filename)\n        __logger__.info(\'download from %s to %s\', url, filename)\n        return filename\n    except Exception:\n        __logger__.error(""failed to download"", exc_info=True)\n\n\ndef mkdir_for(pth: str):\n    d = os.path.dirname(pth)\n    if d:\n        os.makedirs(d, exist_ok=True)\n    return d\n\n\ndef file_func(kwargs: dict, func=shutil.copy2, tester: str = \'dst\'):\n    try:\n        return func(**kwargs)\n    except IOError as identifier:\n        # ENOENT(2): file does not exist, raised also on missing dest parent dir\n        if identifier.errno != errno.ENOENT:\n            print(identifier.__dict__)\n        assert tester in kwargs.keys(), \'wrong parameter {}\'.format(tester)\n        os.makedirs(os.path.dirname(kwargs[tester]), exist_ok=True)\n        return func(**kwargs)\n    except Exception as identifier:\n        print(identifier)\n        return None\n\n\n@contextmanager\ndef safe_open(filename: str, mode: str = \'r\', func=open, **kwargs):\n    ""if directory of filename does not exist, create it first""\n    mkdir_for(filename)\n    fn = func(filename, mode=mode, **kwargs)\n    yield fn\n    fn.close()\n\n\n@contextmanager\ndef safe_chdir(pth: str):\n    ""safely change directory to pth, and then go back""\n    currdir = os.getcwd()\n    try:\n        if not pth:\n            pth = currdir\n        os.chdir(pth)\n        __logger__.info(""changing directory to %s"", pth)\n        yield pth\n    finally:\n        os.chdir(currdir)\n        __logger__.info(""changing directory back to %s"", currdir)\n\n\ndef safe_copy(src: str, dst: str):\n    ""if directory of filename doesnot exist, create it first""\n    return file_func({\'src\': src, \'dst\': dst})\n\n\n__yaml_exts__, __json_exts__ = [\'.yaml\', \'.yml\'], [\'.json\', \'.jsn\']\n\n\ndef to_file(obj, fname: str, fmt=None, **kwargs):\n    if not fmt:\n        _, ext = os.path.splitext(fname)\n        if ext in __json_exts__:\n            fmt, dic = json, dict(indent=4)\n        elif ext in __yaml_exts__:\n            import yaml\n            fmt, dic = yaml, dict(default_flow_style=False)\n        else:\n            raise NotImplementedError\n        dic.update(kwargs)\n    else:\n        dic = kwargs\n    with safe_open(fname, \'w\') as fp:\n        fmt.dump(obj, fp, **dic)\n        __logger__.debug(""serialize object to file %s"", fname)\n'"
contrib/python-sdk/openpaisdk/job.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport json\nimport os\nimport re\nimport pathlib\nfrom typing import Union, List\nfrom copy import deepcopy\nfrom html2text import html2text\n\nfrom openpaisdk.flags import __flags__\nfrom openpaisdk.defaults import get_install_uri, LayeredSettings\nfrom openpaisdk.io_utils import from_file, safe_open, to_file, to_screen\nfrom openpaisdk.utils import Retry, concurrent_map, exception_free, find, get_response, na, na_lazy\nfrom openpaisdk.cluster import get_cluster\n\n__protocol_filename__ = ""job_protocol.yaml""\n__config_filename__ = ""job_config.json""\n__protocol_unit_types__ = [""job"", ""data"", ""script"", ""dockerimage"", ""output""]\n\n\nclass ProtocolUnit:\n\n    @staticmethod\n    def validate(u: dict):\n        # assert u[""protocolVersion""] in [""1"", ""2"", 1, 2], ""invalid protocolVersion (%s)"" % u[""protocolVersion""]\n        assert u[""type""] in __protocol_unit_types__, ""invalid type (%s)"" % u[""type""]\n        assert u[""name""], ""invalid name""\n        # uri: String or list, required # Only when the type is data can the uri be a list.\n        assert isinstance(u[""uri""], str) or u[""type""] == ""data"" and isinstance(u[""uri""], list), ""uri: String or list, required # Only when the type is data can the uri be a list. (Error: %s)"" % u\n\n\nclass TaskRole:\n\n    @staticmethod\n    def validate(t: dict):\n        assert t[""dockerImage""], ""unknown dockerImage""\n        assert t[""resourcePerInstance""][""cpu""] > 0, ""invalid cpu number (%d)"" % t[""resourcePerInstance""][""cpu""]\n        assert t[""resourcePerInstance""][""gpu""] >= 0, ""invalid gpu number (%d)"" % t[""resourcePerInstance""][""gpu""]\n        assert t[""resourcePerInstance""][""memoryMB""] > 0, ""invalid memoryMB number (%d)"" % t[""resourcePerInstance""][""memoryMB""]\n        for label, port in t[""resourcePerInstance""].get(""ports"", {}).items():\n            assert port >= 0, ""invalid port (%s : %d)"" % (label, port)\n        assert isinstance(t[""commands""], list) and t[""commands""], ""empty commands""\n\n\nclass Deployment:\n\n    @staticmethod\n    def validate(d: dict, task_role_names: list):\n        assert d[""name""], ""deployment should have a name""\n        for t, c in d[""taskRoles""].items():\n            assert t in task_role_names, ""invalid taskrole name (%s)"" % (t)\n            assert isinstance([""preCommands""], list), ""preCommands should be a list""\n            assert isinstance([""postCommands""], list), ""postCommands should be a list""\n\n\nclass JobResource:\n\n    def __init__(self, r: dict = None):\n        from copy import deepcopy\n\n        def gb2mb(m):\n            if not isinstance(m, str) or m.isnumeric():\n                return int(m)\n            if m.lower().endswith(\'g\'):\n                return int(m[:-1]) * 1024\n            if m.lower().endswith(\'gb\'):\n                return int(m[:-2]) * 1024\n            raise ValueError(m)\n\n        r = {} if not r else r\n        dic = deepcopy(__flags__.resources_requirements)\n        for key in [""cpu"", ""gpu"", ""memoryMB"", ""ports""]:\n            if r.get(key, None) is not None:\n                dic[key] = int(r[key]) if not key == ""ports"" else r[key]\n        if r.get(""mem"", None) is not None:\n            dic[""memoryMB""] = gb2mb(r[""mem""])\n        self.req = dic\n\n    def add_port(self, name: str, num: int = 1):\n        self.req.setdefault(""ports"", {})[name] = num\n        return self\n\n    @property\n    def as_dict(self):\n        return self.req\n\n    @staticmethod\n    def parse_list(lst: List[str]):\n        r = []\n        for spec in lst:\n            s = spec.replace("" "", \'\').split("","")\n            r.append(JobResource({\n                ""gpu"": s[0], ""cpu"": s[1], ""mem"": s[2],\n            }).as_dict)\n        return r\n\n\nclass Job:\n    """"""\n    the data structure and methods to describe a job compatible with https://github.com/microsoft/openpai-protocol/blob/master/schemas/v2/schema.yaml\n    external methods:\n    - I/O\n        - save(...) / load(...): store and restore to the disk\n    - Job protocol wizard\n        - sdk_job_template(...): generate a job template with the sdk (embedding cluster / storage information)\n        - one_liner(...): generate a single-taskrole job protocol from commands and other essential information\n        - from_notebook(...): generate a job protocol from a jupyter notebook\n    - Interaction with clusters\n        - submit(...): submit to a cluster, including archiving and uploading local source files\n        - wait(...): wait a job until completed\n        - log(...):\n    - Parse logs\n        - connect_jupyter(...): wait job running and connected to jupyter server\n    """"""\n\n    def __init__(self, name: str=None, **kwargs):\n        self.protocol = dict()  # follow the schema of https://github.com/microsoft/openpai-protocol/blob/master/schemas/v2/schema.yaml\n        self._client = None  # cluster client\n        self.new(name, **kwargs)\n\n    def new(self, name: str, **kwargs):\n        self.protocol = {\n            ""name"": name,\n            ""protocolVersion"": 2,\n            ""type"": ""job"",\n            ""prerequisites"": [],\n            ""parameters"": dict(),\n            ""secrets"": dict(),\n            ""taskRoles"": dict(),\n            ""deployments"": [],\n            ""defaults"": dict(),\n            ""extras"": dict(),\n        }\n        self.protocol.update(kwargs)\n        return self\n\n    def load(self, fname: str = None, job_name: str = None, cluster_alias: str = None):\n        if cluster_alias:  # load job config from cluster by REST api\n            job_name = na(job_name, self.name)\n            self.protocol = get_cluster(cluster_alias).rest_api_job_info(job_name, \'config\')\n        else:  # load from local file\n            if not fname:\n                fname = Job(job_name).protocol_file\n            if os.path.isfile(fname):\n                self.protocol = from_file(fname, default=""==FATAL=="")\n        self.protocol.setdefault(\'protocolVersion\', \'1\')  # v1 protocol (json) has no protocolVersion\n        return self\n\n    def save(self):\n        if self.name:\n            to_file(self.protocol, self.protocol_file)\n        return self\n\n    def validate(self):\n        assert self.protocolVersion in [""1"", ""2""], ""unknown protocolVersion (%s)"" % self.protocol[""protocolVersion""]\n        assert self.name is not None, ""job name is null %s"" % self.protocol\n        if self.protocolVersion == ""2"":\n            assert self.protocol[""type""] == ""job"", ""type must be job (%s)"" % self.protocol[""type""]\n            for t in self.protocol.get(""taskRoles"", {}).values():\n                TaskRole.validate(t)\n            for d in self.protocol.get(""deployments"", []):\n                Deployment.validate(d, list(self.protocol[""taskRoles""].keys()))\n            for u in self.protocol.get(""prerequisites"", []):\n                ProtocolUnit.validate(u)\n        return self\n\n    @property\n    def protocolVersion(self):\n        return str(self.protocol.get(""protocolVersion"", ""1""))\n\n    @property\n    def name(self):\n        return self.protocol.get(""name"" if self.protocolVersion == ""2"" else ""jobName"", None)\n\n    @property\n    def cache_dir(self):\n        assert self.name, ""cannot get cache directory for an empty job name""\n        return os.path.join(__flags__.cache, self.name)\n\n    def cache_file(self, fname):\n        return os.path.join(self.cache_dir, fname)\n\n    @property\n    def protocol_file(self):\n        return self.cache_file(__protocol_filename__)\n\n    @property\n    def temp_archive(self):\n        return self.cache_file(self.name + "".tar.gz"")\n\n    @staticmethod\n    def get_config_file(job_name: str, v2: bool=True):\n        return Job(job_name).cache_file(__protocol_filename__ if v2 else __config_filename__)\n\n    def param(self, key, default=None, field: str=""parameters""):\n        return self.protocol.get(field, {}).get(key, default)\n\n    def set_param(self, key, value, field: str=""parameters""):\n        self.protocol.setdefault(field, {})[key] = value\n\n    def secret(self, key, default=None):\n        return self.param(key, default, ""secrets"")\n\n    def set_secret(self, key, value):\n        self.set_param(key, value, ""secrets"")\n\n    def extra(self, key, default=None):\n        return self.param(key, default, ""extras"")\n\n    def set_extra(self, key, value):\n        self.set_param(key, value, ""extras"")\n\n    def tags(self):\n        return self.param(""tags"", [], ""extras"")\n\n    def add_tag(self, tag: str):\n        lst = self.tags()\n        if tag not in lst:\n            lst.append(tag)\n        self.set_param(""tags"", lst, ""extras"")\n        return self\n\n    def has_tag(self, tag: str):\n        return tag in self.tags()\n\n    def get_config(self):\n        if self.protocolVersion == ""2"":\n            self.interpret_sdk_plugin()\n            for d in self.protocol.get(""deployments"", []):\n                r = d[""taskRoles""]\n                t_lst = list(r.keys())\n                for t in t_lst:\n                    for k in [""preCommands"", ""postCommands""]:  # pre- / post-\n                        if k not in r[t]:\n                            continue\n                        if len(r[t][k]) == 0:\n                            del r[t][k]\n                    if len(r[t]) == 0:\n                        del r[t]\n            for key in [""deployments"", ""parameters""]:\n                if key in self.protocol and len(self.protocol[key]) == 0:\n                    del self.protocol[key]\n            for t in self.protocol[""taskRoles""].values():\n                if ""ports"" in t[""resourcePerInstance""] and len(t[""resourcePerInstance""][""ports""]) == 0:\n                    del t[""resourcePerInstance""][""ports""]\n            return self.protocol\n        else:\n            dic = deepcopy(self.protocol)\n            del dic[""protocolVersion""]\n            return dic\n\n    def sdk_job_template(self, cluster_alias_lst: str=[], workspace: str=None, sources: list=None, pip_installs: list=None):\n        ""generate the job template for a sdk-submitted job""\n        # secrets\n        clusters = [get_cluster(alias, get_client=False) for alias in cluster_alias_lst]\n        workspace = na(workspace, LayeredSettings.get(""workspace""))\n        workspace = na(workspace, f""{__flags__.storage_root}/{clusters[0][\'user\']}"")\n        self.set_secret(""clusters"", json.dumps(clusters))\n        self.set_param(""cluster_alias"", cluster_alias_lst[0] if cluster_alias_lst else None)\n        self.set_param(""work_directory"", \'{}/jobs/{}\'.format(workspace, self.name) if workspace else None)\n\n        # parameters\n        self.set_param(""python_path"", ""python"")\n\n        # signature\n        self.add_tag(__internal_tags__[""sdk""])\n\n        # sdk.plugins\n        sdk_install_uri = ""-U {}"".format(get_install_uri())\n        c_dir = \'~/{}\'.format(__flags__.cache)\n        c_file = \'%s/%s\' % (c_dir, __flags__.cluster_cfg_file)\n\n        plugins = []\n        if sources:\n            plugins.append({\n                ""plugin"": ""local.uploadFiles"",\n                ""parameters"": {\n                    ""files"": list(set([os.path.relpath(s) for s in sources])),\n                },\n            })\n\n        plugins.extend([\n            {\n                ""plugin"": ""container.preCommands"",  # commands to install essential pip packages\n                ""parameters"": {\n                    ""commands"": [\n                        ""<% $parameters.python_path %> -m pip install {}"".format(p) for p in [sdk_install_uri] + na(pip_installs, [])\n                    ]\n                }\n            },\n            {\n                ""plugin"": ""container.preCommands"",  # copy cluster information\n                ""parameters"": {\n                    ""commands"": [\n                        ""mkdir %s"" % c_dir,\n                        ""echo \\""write config to {}\\"""".format(c_file),\n                        ""echo <% $secrets.clusters %> > {}"".format(c_file),\n                        ""opai cluster select <% $parameters.cluster_alias %>"",\n                    ]\n                }\n            }\n        ])\n\n        if sources:\n            a_file = os.path.basename(self.temp_archive)\n            plugins.append({\n                ""plugin"": ""container.preCommands"",\n                ""parameters"": {\n                    ""commands"": [\n                        ""opai storage download <% $parameters.work_directory %>/source/{} {}"".format(a_file, a_file),\n                        ""tar xvfz {}"".format(a_file)\n                    ]\n                }\n            })\n        self.set_extra(""sdk.plugins"", plugins)\n        return self\n\n    def one_liner(self,\n                  commands: Union[list, str], image: str, cluster: dict, resources: dict=None,\n                  sources: list = None, pip_installs: list = None\n                  ):\n        """"""generate the single-task-role job protocol from essentials such as commands, docker image...\n        :param cluster (dict): a dictionary includes {cluster_alias, virtual_cluster, workspace}\n        """"""\n        self.sdk_job_template([cluster[""cluster_alias""]], cluster.get(""workspace"", None), sources, pip_installs)\n        self.protocol[""prerequisites""].append({\n            ""name"": ""docker_image"",\n            ""type"": ""dockerimage"",\n            ""protocolVersion"": ""2"",\n            ""uri"": image,\n        })\n        self.protocol.setdefault(""taskRoles"", {})[""main""] = {\n            ""dockerImage"": ""docker_image"",\n            ""resourcePerInstance"": JobResource(resources).as_dict,\n            ""commands"": commands if isinstance(commands, list) else [commands]\n        }\n        self.add_tag(__internal_tags__[""one_liner""])\n        return self\n\n    def from_notebook(self,\n                      nb_file: str, mode: str=""interactive"", token: str=""abcd"",\n                      image: str=None, cluster: dict=None, resources: dict=None,\n                      sources: list = None, pip_installs: list = None\n                      ):\n        """"""\n        mode: interactive / silent / script\n        """"""\n        assert mode in [""interactive"", ""silent"", ""script""], ""unsupported mode %s"" % mode\n        if not nb_file:\n            mode, nb_file = ""interactive"", """"\n        else:\n            assert os.path.isfile(nb_file), ""cannot read the ipython notebook {}"".format(nb_file)\n            sources = na(sources, [])\n            sources.append(nb_file)\n        self.set_param(""notebook_file"", os.path.splitext(os.path.basename(nb_file))[0] if nb_file else """")\n        resources = JobResource(resources)\n        if mode == ""interactive"":\n            resources.add_port(""jupyter"")\n            self.set_secret(""token"", token)\n            cmds = [\n                "" "".join([\n                    ""jupyter notebook"",\n                    ""--no-browser"", ""--ip 0.0.0.0"", ""--port $PAI_CONTAINER_HOST_jupyter_PORT_LIST"",\n                    ""--NotebookApp.token=<% $secrets.token %>"",\n                    ""--allow-root --NotebookApp.file_to_run=<% $parameters.notebook_file %>.ipynb"",\n                ]),\n            ]\n        elif mode == ""silent"":\n            cmds = [\n                "" "".join([\n                    ""jupyter nbconvert --ExecutePreprocessor.timeout=-1 --ExecutePreprocessor.allow_errors=True"",\n                    ""--to html --execute <% $parameters.notebook_file %>.ipynb"",\n                ]),\n                ""opai storage upload <% $parameters.notebook_file %>.html <% $parameters.work_directory %>/output/<% $parameters.notebook_file %>.html"",\n            ]\n        else:\n            cmds = [\n                ""jupyter nbconvert --to script <% $parameters.notebook_file %>.ipynb --output openpai_submitter_entry"",\n                ""echo ======================== Python Script Starts ========================"",\n                # execute notebook by iPython. To remove color information, we use ""--no-term-title"" and sed below\n                """"""ipython --no-term-title openpai_submitter_entry.py | sed -r ""s/\\\\x1B\\\\[([0-9]{1,2}(;[0-9]{1,2})?)?[mGK]//g"" | tr -dc \'[[:print:]]\\\\n\'"""""",\n            ]\n        self.one_liner(cmds, image, cluster, resources.as_dict, sources, na(pip_installs, []) + [""jupyter""])\n        mode_to_tag = {""interactive"": ""interactive_nb"", ""silent"": ""batch_nb"", ""script"": ""script_nb""}\n        self.add_tag(__internal_tags__[mode_to_tag[mode]])\n        return self\n\n    def interpret_sdk_plugin(self):\n        plugins = self.extra(""sdk.plugins"", [])\n        # concatenate commands\n        if len(self.protocol.setdefault(""deployments"", [])) == 0:  # will move to plugin fields when it is ready\n            # we could use a new deployments for every pre- / post- commands plugin\n            deployment_name, task_role_names = ""sdk_deployment"", list(self.protocol[""taskRoles""])\n            deployment = {key: dict(preCommands=[], postCommands=[]) for key in task_role_names}\n            plugins_to_remove = []\n            for i, plugin in enumerate(plugins):\n                target = find(""container.(\\w+)"", plugin[""plugin""])\n                if target not in [""preCommands"", ""postCommands""]:\n                    continue\n                for t in plugin.get(""taskRoles"", task_role_names):\n                    deployment[t][target].extend(plugin[""parameters""][""commands""])\n                plugins_to_remove.append(i)\n            if plugins_to_remove:\n                self.protocol[""deployments""].append({\n                    ""name"": deployment_name,\n                    ""taskRoles"": deployment,\n                })\n                self.protocol.setdefault(""defaults"", {})[""deployment""] = deployment_name\n                for i in reversed(plugins_to_remove):\n                    del plugins[i]\n        return self\n\n    @property\n    def client(self):\n        if self._client is None:\n            alias = self.param(""cluster_alias"")\n            if alias:\n                self._client = get_cluster(alias)\n        return self._client\n\n    def select_cluster(self, cluster_alias: str=None, virtual_cluster: str=None):\n        self._client = get_cluster(cluster_alias)\n        if virtual_cluster:\n            if self.protocolVersion == ""1"":\n                self.protocol[""virtualCluster""] = virtual_cluster\n            else:\n                self.set_param(""virtualCluster"", virtual_cluster, field=""defaults"")\n        return self\n\n    # methods only for SDK-enabled jobs\n    def submit(self, cluster_alias: str = None, virtual_cluster: str = None):\n        cluster_alias = na(cluster_alias, self.param(""cluster_alias"", None))\n        self.select_cluster(cluster_alias, virtual_cluster)\n        self.validate().local_process()\n        to_screen(""submit job %s to cluster %s"" % (self.name, cluster_alias))\n        try:\n            self.client.rest_api_submit(self.get_config())\n            job_link = self.client.get_job_link(self.name)\n            return {""job_link"": job_link, ""job_name"": self.name}\n        except Exception as identifier:\n            to_screen(f""submit failed due to {repr(identifier)}"", _type=""error"")\n            to_screen(self.get_config())\n            raise identifier\n\n    def stop(self):\n        return self.client.rest_api_execute_job(self.name)\n\n    def get_status(self):\n        return self.client.rest_api_job_info(self.name)\n\n    def wait(self, t_sleep: float = 10, timeout: float = 3600, silent: bool = False):\n        """"""for jupyter job, wait until ready to connect\n        for normal job, wait until completed""""""\n        exit_states = __job_states__[""completed""]\n        repeater = Retry(timeout=timeout, t_sleep=t_sleep, silent=silent)\n        interactive_nb = self.has_tag(__internal_tags__[""interactive_nb""])\n        batch_nb = self.has_tag(__internal_tags__[""batch_nb""])\n        if interactive_nb or batch_nb:\n            if interactive_nb:\n                to_screen(""{} is recognized to be an interactive jupyter notebook job"".format(self.name))\n                to_screen(""notebook job needs to be RUNNING state and the kernel started"")\n            if batch_nb:\n                to_screen(""{} is recognized to be a silent jupyter notebook job"".format(self.name))\n                to_screen(""notebook job needs to be SUCCEEDED state and the output is ready"")\n            return repeater.retry(\n                lambda x: x.get(\'state\', None) in exit_states or x.get(""notebook"", None) is not None,\n                self.connect_jupyter\n            )\n        to_screen(""wait until job to be completed ({})"".format(exit_states))\n        return repeater.retry(\n            lambda x: JobStatusParser.state(x) in exit_states,  # x: job status\n            self.get_status\n        )\n\n    def plugin_uploadFiles(self, plugin: dict):\n        import tarfile\n        to_screen(""archiving and uploading ..."")\n        work_directory = self.param(""work_directory"")\n        assert work_directory, ""must specify a storage to upload""\n        with safe_open(self.temp_archive, ""w:gz"", func=tarfile.open) as fn:\n            for src in plugin[""parameters""][""files""]:\n                src = os.path.relpath(src)\n                if os.path.dirname(src) != """":\n                    to_screen(""files not in current folder may cause wrong location when unarchived in the container, please check it {}"".format(src), _type=""warn"")\n                fn.add(src)\n                to_screen(""{} archived and wait to be uploaded"".format(src))\n        self.client.get_storage().upload(\n            local_path=self.temp_archive,\n            remote_path=""{}/source/{}"".format(work_directory, os.path.basename(self.temp_archive)),\n            overwrite=True\n        )\n\n    def local_process(self):\n        ""pre-process the job protocol locally, including uploading files, deal with pre-/post- commands""\n        self.validate()\n        plugins = self.protocol.get(""extras"", {}).get(""sdk.plugins"", [])\n        for plugin in plugins:\n            s = find(""local.(\\w+)"", plugin[""plugin""])\n            if not s:\n                continue\n            getattr(self, ""plugin_"" + s)(plugin)\n        return self\n\n    def connect_jupyter(self):\n        if self.has_tag(__internal_tags__[""script_nb""]):\n            return self.connect_jupyter_script()\n        if self.has_tag(__internal_tags__[""batch_nb""]):\n            return self.connect_jupyter_batch()\n        if self.has_tag(__internal_tags__[""interactive_nb""]):\n            return self.connect_jupyter_interactive()\n\n    def connect_jupyter_batch(self):\n        ""fetch the html result if ready""\n        status = self.get_status()\n        state = JobStatusParser.state(status)\n        url = None\n        if state in __job_states__[""successful""]:\n            html_file = self.param(""notebook_file"") + "".html""\n            local_path = html_file\n            remote_path = \'{}/output/{}\'.format(self.param(""work_directory""), html_file)\n            self.client.get_storage().download(remote_path=remote_path, local_path=local_path)\n            url = pathlib.Path(os.path.abspath(html_file)).as_uri()\n        return dict(state=state, notebook=url)\n\n    def connect_jupyter_interactive(self):\n        ""get the url of notebook if ready""\n        status = self.get_status()\n        nb_file = self.param(""notebook_file"") + "".ipynb"" if self.param(""notebook_file"") else None\n        return JobStatusParser.interactive_jupyter_url(status, nb_file)\n\n    def connect_jupyter_script(self):\n        status = self.get_status()\n        state = self.state(status)\n        return dict(state=state, notebook=None)\n\n\n__internal_tags__ = {\n    ""sdk"": ""py-sdk"",\n    ""one_liner"": \'py-sdk-one-liner\',\n    ""interactive_nb"": \'py-sdk-notebook-interactive\',\n    ""batch_nb"": \'py-sdk-notebook-batch\',\n    ""script_nb"": \'py-sdk-notebook-script\',\n}\n\n\n__job_states__ = {\n    ""successful"": [""SUCCEEDED""],\n    ""failed"": [""FAILED"", ""STOPPED""],\n    ""ongoing"": [""WAITING"", ""RUNNING"", ""COMPLETING""],\n}\n__job_states__[""completed""] = __job_states__[""successful""] + __job_states__[""failed""]\n__job_states__[""ready""] = __job_states__[""completed""] + [""RUNNING""]\n__job_states__[""valid""] = [s for sub in __job_states__.values() for s in sub]\n\n\nclass JobStatusParser:\n\n    @staticmethod\n    @exception_free(KeyError, None)\n    def state(status: dict):\n        return status[""jobStatus""][""state""]\n\n    @staticmethod\n    @exception_free(KeyError, None)\n    def single_task_logs(status: dict, task_role: str = \'main\', index: int = 0, log_type: dict=None, return_urls: bool=False):\n        """"""change to use containerLog""""""\n        log_type = na(log_type, {\n            ""stdout"": ""user.pai.stdout/?start=0"",\n            ""stderr"": ""user.pai.stderr/?start=0""\n        })\n        containers = status.get(""taskRoles"", {}).get(task_role, {}).get(""taskStatuses"", [])\n        if len(containers) < index + 1:\n            return None\n        containerLog = containers[index].get(""containerLog"", None)\n        if not containerLog:\n            return None\n        urls = {\n            k: ""{}{}"".format(containerLog, v)\n            for k, v in log_type.items()\n        }\n        if return_urls:\n            return urls\n        else:\n            html_contents = {k: get_response(\'GET\', v).text for k, v in urls.items()}\n            try:\n                from html2text import html2text\n                return {k: html2text(v) for k, v in html_contents.items()}\n            except ImportError:\n                return html_contents\n\n    @staticmethod\n    @exception_free(Exception, None)\n    def all_tasks_logs(status: dict):\n        """"""retrieve logs of all tasks""""""\n        logs = {\n            \'stdout\': {}, \'stderr\': {}\n        }\n        for tr_name, tf_info in status[\'taskRoles\'].items():\n            for task_status in tf_info[\'taskStatuses\']:\n                task_id = \'{}[{}]\'.format(tr_name, task_status[\'taskIndex\'])\n                task_logs = JobStatusParser.single_task_logs(status, tr_name, task_status[\'taskIndex\'])\n                for k, v in task_logs.items():\n                    logs.setdefault(k, {})[task_id] = v\n        return logs\n\n    @staticmethod\n    @exception_free(Exception, dict(state=None, notebook=None))\n    def interactive_jupyter_url(status: dict, nb_file: str=None, task_role: str=\'main\', index: int= 0):\n        ""get the url of notebook if ready""\n        state = JobStatusParser.state(status)\n        url = None\n        if state == ""RUNNING"":\n            job_log = JobStatusParser.single_task_logs(\n                status, task_role, index\n            )[""stderr""].split(\'\\n\')\n            for line in job_log:\n                if re.search(""The Jupyter Notebook is running at:"", line):\n                    from openpaisdk.utils import path_join\n                    container = status[""taskRoles""][task_role][""taskStatuses""][index]\n                    ip, port = container[""containerIp""], container[""containerPorts""][""jupyter""]\n                    url = path_join([f""http://{ip}:{port}"", ""notebooks"", nb_file])\n                    break\n        return dict(state=state, notebook=url)\n\n\ndef job_spider(cluster, jobs: list = None):\n    jobs = na_lazy(jobs, cluster.rest_api_job_list)\n    to_screen(""{} jobs to be captured in the cluster {}"".format(len(jobs), cluster.alias))\n    job_statuses = concurrent_map(\n        lambda j: cluster.rest_api_job_info(j[\'name\'], info=None, user=j[\'username\']),\n        jobs\n    )\n    job_configs = concurrent_map(\n        lambda j: cluster.rest_api_job_info(j[\'name\'], info=\'config\', user=j[\'username\']),\n        jobs\n    )\n    job_logs = concurrent_map(JobStatusParser.all_tasks_logs, job_statuses)\n    for job, sta, cfg, logs in zip(jobs, job_statuses, job_configs, job_logs):\n        job[\'status\'] = sta\n        job[\'config\'] = cfg\n        job[\'logs\'] = logs\n    return jobs\n'"
contrib/python-sdk/openpaisdk/notebook.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport json\nimport os.path\nimport re\nfrom openpaisdk.defaults import LayeredSettings, __flags__\n\n\ndef get_notebook_path():\n    """"""\n    Return the full path of the jupyter notebook.\n    Reference: https://github.com/jupyter/notebook/issues/1000#issuecomment-359875246\n    """"""\n    import requests\n    from requests.compat import urljoin\n    from notebook.notebookapp import list_running_servers\n    import ipykernel\n\n    kernel_id = re.search(\'kernel-(.*).json\',\n                          ipykernel.connect.get_connection_file()).group(1)\n    servers = list_running_servers()\n    for ss in servers:\n        response = requests.get(urljoin(ss[\'url\'], \'api/sessions\'),\n                                params={\'token\': ss.get(\'token\', \'\')})\n        info = json.loads(response.text)\n        if isinstance(info, dict) and info[\'message\'] == \'Forbidden\':\n            continue\n        for nn in info:\n            if nn[\'kernel\'][\'id\'] == kernel_id:\n                relative_path = nn[\'notebook\'][\'path\']\n                return os.path.join(ss[\'notebook_dir\'], relative_path)\n\n\ndef parse_notebook_path():\n    ""parse the running notebook path to name, folder, extension""\n    nb_file = get_notebook_path()\n    folder, fname = os.path.split(nb_file)\n    name, ext = os.path.splitext(fname)\n    return name, folder, ext\n\n\nclass NotebookConfiguration:\n    ""wrapper of LayeredSettings""\n\n    @staticmethod\n    def reset():\n        LayeredSettings.reset()\n\n    @staticmethod\n    def print_supported_items():\n        ret = LayeredSettings.print_supported_items()\n        if __flags__.disable_to_screen:\n            print(ret)\n\n    @staticmethod\n    def set(key, value):\n        LayeredSettings.update(""user_advaced"", key, value)\n\n    @staticmethod\n    def get(*args):\n        dic = LayeredSettings.as_dict()\n        if not args:\n            return dic\n        elif len(args) == 1:\n            return dic[args[0]]\n        else:\n            return [dic[a] for a in args]\n'"
contrib/python-sdk/openpaisdk/storage.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n""""""\n[summary]\n""""""\nfrom openpaisdk.io_utils import mkdir_for, to_screen\n\n\nclass Storage:\n\n    def __init__(self, protocol: str = \'webHDFS\', *args, **kwargs):\n        self.protocol, self.client = protocol.lower(), None\n        if protocol.lower() == \'webHDFS\'.lower():\n            from hdfs import InsecureClient\n            self.client = InsecureClient(*args, **kwargs)\n            for f in \'upload download list status delete\'.split():\n                setattr(self, f, getattr(self, \'%s_%s\' %\n                                         (f, protocol.lower())))\n\n    def upload_webhdfs(self, local_path: str, remote_path: str, **kwargs):\n        to_screen(""upload %s -> %s"" % (local_path, remote_path))\n        return self.client.upload(local_path=local_path, hdfs_path=remote_path, **kwargs)\n\n    def download_webhdfs(self, remote_path: str, local_path: str, **kwargs):\n        mkdir_for(local_path)\n        to_screen(""download %s -> %s"" % (remote_path, local_path))\n        return self.client.download(local_path=local_path, hdfs_path=remote_path, overwrite=True, **kwargs)\n\n    def list_webhdfs(self, remote_path: str, **kwargs):\n        return self.client.list(hdfs_path=remote_path, **kwargs)\n\n    def status_webhdfs(self, remote_path: str, **kwargs):\n        return self.client.status(hdfs_path=remote_path, **kwargs)\n\n    def delete_webhdfs(self, remote_path: str, **kwargs):\n        return self.client.delete(hdfs_path=remote_path, **kwargs)\n'"
contrib/python-sdk/openpaisdk/utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\n""""""\ncommon functions to\n""""""\nfrom openpaisdk.io_utils import safe_chdir, to_screen, __logger__\nimport subprocess\nimport importlib\nimport os\nimport time\nimport requests\nfrom typing import Union\nfrom functools import wraps\nfrom collections import Iterable\nfrom requests_toolbelt.utils import dump\nfrom urllib3.exceptions import InsecureRequestWarning\n\n# Suppress only the single warning from urllib3 needed.\nrequests.packages.urllib3.disable_warnings(category=InsecureRequestWarning)\n\n\ndef exception_free(err_type, default, err_msg: str = None):\n    ""return the default value if the exception is caught""\n    def inner_func(fn):\n        @wraps(fn)\n        def wrapper(*args, **kwargs):\n            try:\n                return fn(*args, **kwargs)\n            except err_type as e:\n                if not err_msg:\n                    to_screen(repr(e), _type=""warn"")\n                else:\n                    to_screen(err_msg, _type=""warn"")\n                return default\n            except Exception as e:\n                raise e\n        return wrapper\n    return inner_func\n\n\ndef concurrent_map(fn, it, max_workers=None):\n    ""a wrapper of concurrent.futures.ThreadPoolExecutor.map, retrieve the results""\n    from concurrent.futures import ThreadPoolExecutor\n    ret = []\n    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = executor.map(fn, it)\n        for f in futures:\n            ret.append(f)\n    return ret\n\n\nclass OrganizedList(list):\n\n    def __init__(self, lst: list, _key: str = None, _getter=dict.get):\n        super().__init__(lst)\n        self._getter = _getter\n        self._key = _key\n\n    @property\n    def _fn_get(self):\n        return lambda elem: self._getter(elem, self._key)\n\n    def first_index(self, target):\n        for i, elem in enumerate(self):\n            if self._fn_get(elem) == target:\n                return i\n        return None\n\n    def first(self, target):\n        i = self.first_index(target)\n        return self[i] if i is not None else None\n\n    def filter_index(self, target=None, include: list = None, exclude: list = None):\n        if include is not None:\n            return [i for i, elem in enumerate(self) if self._fn_get(elem) in include]\n        if exclude is not None:\n            return [i for i, elem in enumerate(self) if self._fn_get(elem) not in exclude]\n        return [i for i, elem in enumerate(self) if self._fn_get(elem) == target]\n\n    def filter(self, target=None, include=None, exclude=None):\n        return OrganizedList([self[i] for i in self.filter_index(target, include, exclude)], self._key, self._getter)\n\n    @property\n    def as_dict(self):\n        return {self._fn_get(elem): elem for elem in self}\n\n    @property\n    def as_list(self):\n        return [x for x in self]\n\n    def add(self, elem: dict, getter=dict.get, silent: bool = False, replace: bool = False):\n        for i in self.filter_index(self._fn_get(elem)):\n            if replace:\n                self[i] = elem\n                if not silent:\n                    to_screen(f""OrganizedList: {self._key} = {self._fn_get(elem)} already exists, replace it"")\n            else:\n                self[i].update(elem)\n                if not silent:\n                    to_screen(f""OrderedDict: {self._key} = {self._fn_get(elem)} already exists, update it"")\n            return self  # ~ return\n        self.append(elem)\n        if not silent:\n            to_screen(f""OrganizedList: {self._key} = {self._fn_get(elem)} added"")\n        return self\n\n    def remove(self, target):\n        indexes = self.filter_index(target)\n        if not indexes:\n            to_screen(f""OrganizedList: {self._key} = {target} cannot be deleted due to non-existence"")\n            return self\n        for index in sorted(indexes, reverse=True):\n            del self[index]\n            to_screen(f""OrganizedList: {self._key} = {target} removed"")\n        return self\n\n\nclass Nested:\n\n    def __init__(self, t, sep: str = "":""):\n        self.__sep__ = sep\n        self.content = t\n\n    def get(self, keys: str):\n        return Nested.s_get(self.content, keys.split(self.__sep__))\n\n    def set(self, keys: str, value):\n        return Nested.s_set(self.content, keys.split(self.__sep__), value)\n\n    @staticmethod\n    def _validate(context: Union[list, dict], idx: Union[str, int]):\n        return int(idx) if isinstance(context, list) else idx\n\n    @staticmethod\n    def s_get(target, keys: list):\n        k = Nested._validate(target, keys[0])\n        if len(keys) == 1:\n            return target[k]\n        return Nested.s_get(target[k], keys[1:])\n\n    @staticmethod\n    def s_set(target, keys: list, value):\n        # ! not allow to create a list\n        k = Nested._validate(target, keys[0])\n        if len(keys) == 1:\n            target[k] = value\n            return\n        if isinstance(target, dict) and k not in target:\n            target[k] = dict()\n        return Nested.s_set(target[k], keys[1:], value)\n\n\ndef getobj(name: str):\n    mod_name, func_name = name.rsplit(\'.\', 1)\n    mod = importlib.import_module(mod_name)\n    return getattr(mod, func_name)\n\n\nclass RestSrvError(Exception):\n    pass\n\n\nclass NotReadyError(Exception):\n    pass\n\n\nclass Retry:\n\n    def __init__(self, max_try: int = 10, t_sleep: float = 10, timeout: float = 600, silent: bool = True):\n        self.max_try = max_try\n        self.t_sleep = t_sleep\n        self.timeout = timeout\n        if self.timeout:\n            assert self.t_sleep, ""must specify a period to sleep if timeout is set""\n        self.silent = silent\n\n    def retry(self, f_exit, func, *args, **kwargs):\n        t, i = 0, 0\n        while True:\n            try:\n                x = func(*args, **kwargs)\n                if f_exit(x):\n                    if not self.silent:\n                        to_screen(""ready: {}"".format(x))\n                    return x\n            except NotReadyError as identifier:\n                __logger__.debug(""condition not satisfied"", identifier)\n            if not self.silent:\n                to_screen(""not ready yet: {}"".format(x))\n            i, t = i + 1, t + self.t_sleep\n            if self.max_try and i >= self.max_try or self.timeout and t >= self.timeout:\n                return None\n            if self.t_sleep:\n                time.sleep(self.t_sleep)\n\n\ndef path_join(path: Union[list, str], sep: str = \'/\'):\n    """""" join path from list or str\n    - [\'aaa\', \'bbb\', \'ccc\'] -> \'aaa/bbb/ccc\'\n    - [\'aaa\', \'bbb\', (\'xxx\', None), \'ddd\'] -> \'aaa/bbb/ccc\'\n    - [\'aaa\', \'bbb\', (\'xxx\', \'x-val\'), \'ddd\'] -> \'aaa/bbb/xxx/x-val/ccc\'\n    """"""\n    def is_single_element(x):\n        return isinstance(x, str) or not isinstance(x, Iterable)\n    if is_single_element(path):\n        return str(path)\n    p_lst = []\n    for p in path:\n        if not p:\n            continue\n        if is_single_element(p):\n            p_lst.append(str(p))\n        elif all(p):\n            p_lst.extend([str(x) for x in p])\n    return \'/\'.join(p_lst)\n\n\ndef get_response(method: str, path: Union[list, str], headers: dict = None, body: dict = None, allowed_status: list = [200], **kwargs):\n    """"""an easy wrapper of request, including:\n    - path accept a list of strings and more complicated input\n    - will checked the response status_code, raise RestSrvError if not in the allowed_status\n    """"""\n    path = path_join(path)\n    headers = na(headers, {})\n    body = na(body, {})\n    application_json = \'Content-Type\' not in headers or headers[\'Content-Type\'] == \'application/json\'\n    response = requests.request(method, path, headers=headers, ** kwargs, **{\n        ""json"" if application_json else ""data"": body,\n        ""verify"": False,  # support https\n    })\n    __logger__.debug(\'----------Response-------------\\n%s\', dump.dump_all(response).decode(\'utf-8\'))\n    if allowed_status and response.status_code not in allowed_status:\n        __logger__.warn(response.status_code, response.json())\n        raise RestSrvError(response.status_code, response.json())\n    return response\n\n\ndef run_command(commands,  # type: Union[list, str]\n                cwd=None,  # type: str\n                ):\n    command = commands if isinstance(commands, str) else "" "".join(commands)\n    with safe_chdir(cwd):\n        rtn_code = os.system(command)\n        if rtn_code:\n            raise subprocess.CalledProcessError(rtn_code, commands)\n\n\ndef sys_call(args, dec_mode: str = \'utf-8\'):\n    p = subprocess.Popen(args, shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n    out, err = p.communicate()\n    if dec_mode:\n        out, err = out.decode(dec_mode), err.decode(dec_mode)\n    if p.returncode:\n        raise subprocess.CalledProcessError(f""ErrCode: {p.returncode}, {err}"")\n    return out, err\n\n\ndef find(fmt: str, s: str, g: int = 1, func=None):\n    import re\n    func = na(func, re.match)\n    m = func(fmt, s)\n    return m.group(g) if m else None\n\n\ndef na(a, default):\n    return a if a is not None else default\n\n\ndef na_lazy(a, fn, *args, **kwargs):\n    return a if a is not None else fn(*args, **kwargs)\n\n\ndef flatten(lst: list):\n    return sum(lst, [])\n\n\ndef randstr(num: int = 10, letters=None):\n    ""get a random string with given length""\n    import string\n    import random\n    letters = na(letters, string.ascii_letters)\n    return \'\'.join(random.choice(letters) for i in range(num))\n'"
contrib/python-sdk/test/basic_test.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport unittest\nfrom typing import Union\nfrom openpaisdk.io_utils import to_screen, safe_chdir\n\n\ndef separated(method):\n    ""run the each test in a separated directory""\n    def func(*args, **kwargs):\n        dir_name = \'utdir_\' + method.__name__\n        os.makedirs(dir_name, exist_ok=True)\n        try:\n            with safe_chdir(dir_name):\n                method(*args, **kwargs)\n        except Exception as identifier:\n            raise identifier\n        finally:\n            to_screen(f""trying to remove {dir_name}"")\n            # ! rmtree not work on windows\n            os.system(f\'rm -rf {dir_name}\')\n    return func\n\n\nclass OrderedUnitTestCase(unittest.TestCase):\n\n    def get_steps(self):\n        for name in dir(self):  # dir() result is implicitly sorted\n            if name.lower().startswith(""step""):\n                yield name, getattr(self, name)\n\n    def run_steps(self):\n        for name, func in self.get_steps():\n            try:\n                to_screen(f""\\n==== begin to test {name} ===="")\n                func()\n            except Exception as identifier:\n                self.fail(""test {} failed ({}: {})"".format(name, type(identifier), repr(identifier)))\n\n    def cmd_exec(self, cmds: Union[list, str]):\n        if isinstance(cmds, list):\n            cmds = \' \'.join(cmds)\n        print(cmds)\n        exit_code = os.system(cmds)\n        self.assertEqual(exit_code, 0, f""fail to run {cmds}"")\n'"
contrib/python-sdk/test/test_command_line.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nfrom openpaisdk import get_defaults, ClusterList, JobStatusParser\nfrom openpaisdk.utils import run_command, randstr\nfrom openpaisdk.io_utils import to_screen\nfrom typing import Union\nfrom basic_test import OrderedUnitTestCase, separated\n\n\ndef get_cmd(cmd: Union[str, list], flags: dict, args: Union[list, str] = None):\n    lst = []\n    lst.extend(cmd if isinstance(cmd, list) else cmd.split())\n    for flag, value in flags.items():\n        lst.extend([""--"" + flag, value.__str__()])\n    if args:\n        lst.extend(args if isinstance(args, list) else args.split())\n    return lst\n\n\ndef run_commands(*cmds, sep: str = \'&&\'):\n    lst = []\n    for i, c in enumerate(cmds):\n        lst.extend(c)\n        if i != len(cmds) - 1:\n            lst.append(sep)\n    run_command(lst)\n\n\ndef run_test_command(cmd: Union[str, list], flags: dict, args: Union[list, str] = None):\n    run_command(get_cmd(cmd, flags, args))\n\n\ndef gen_expected(dic: dict, **kwargs):\n    dic2 = {k.replace(""-"", ""_""): v if k != ""password"" else ""******"" for k, v in dic.items()}\n    dic2.update(kwargs)\n    return dic2\n\n\nclass TestCommandLineInterface(OrderedUnitTestCase):\n\n    ut_init_shell = os.path.join(\'..\', \'ut_init.sh\')\n\n    def step1_init_clusters(self):\n        to_screen(""""""\\\ntesting REST APIs related to retrieving cluster info, including\n- rest_api_cluster_info\n- rest_api_user\n- rest_api_token\n- rest_api_virtual_clusters\n        """""")\n        with open(self.ut_init_shell) as fn:\n            for line in fn:\n                if line.startswith(\'#\'):\n                    continue\n                self.cmd_exec(line)\n        alias = get_defaults()[""cluster-alias""]\n        self.assertTrue(alias, ""not specify a cluster"")\n        self.cmd_exec(\'opai cluster resources\')\n\n    def step2_submit_job(self):\n        import time\n        to_screen(""""""\\\ntesting REST APIs related to submitting a job, including\n- rest_api_submit\n        """""")\n        self.job_name = \'ut_test_\' + randstr(10)\n        self.cmd_exec([\'opai\', \'job\', \'sub\', \'-i\', \'python:3\', \'-j\', self.job_name, \'opai cluster resources\'])\n        time.sleep(10)\n\n    def step3_job_monitoring(self):\n        to_screen(""""""\\\ntesting REST APIs related to querying a job, including\n- rest_api_job_list\n- rest_api_job_info\n        """""")\n        client = ClusterList().load().get_client(get_defaults()[""cluster-alias""])\n        self.cmd_exec([\'opai\', \'job\', \'list\'])\n        job_list = client.rest_api_job_list(client.user)  # ! only jobs from current user to reduce time\n        job_list = [job[\'name\'] for job in job_list]\n        assert self.job_name in job_list, job_list\n        to_screen(f""testing job monitoring with {self.job_name}"")\n        status = client.rest_api_job_info(self.job_name)\n        to_screen(f""retrieving job status and get its state {JobStatusParser.state(status)}"")\n        client.rest_api_job_info(self.job_name, \'config\')\n        to_screen(""retrieving job config"")\n        logs = JobStatusParser.all_tasks_logs(status)\n        assert logs, f""failed to read logs from status \\n{status}""\n        for k, v in logs.items():\n            for t, content in v.items():\n                to_screen(f""reading logs {k} for {t} and get {len(content)} Bytes"")\n\n    @separated\n    def test_commands_sequence(self):\n        self.run_steps()\n'"
contrib/python-sdk/test/test_format.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport unittest\n\n\nin_place_chaning = False\n\n\nclass TestFormat(unittest.TestCase):\n\n    folders = [os.path.join(\'..\', \'openpaisdk\'), \'.\']\n\n    def test_format(self):\n        for folder in self.folders:\n            root, dirs, files = next(os.walk(folder))\n            for src in [fn for fn in files if fn.endswith("".py"")]:\n                os.system(\' \'.join([\n                    sys.executable, \'-m\', \'autoflake\',\n                    \'--remove-unused-variables\',\n                    \'--remove-all-unused-imports\',\n                    \'--remove-duplicate-keys\',\n                    \'--ignore-init-module-imports\',\n                    \'-i\' if in_place_chaning else \'\',\n                    os.path.join(folder, src)\n                ]))\n\n    def clear_notebook_output(self):\n        folders = [\n            os.path.join(\'..\', \'examples\'),\n            os.path.join(\'..\', \'..\', \'notebook-extension\', \'examples\'),\n        ]\n        for folder in folders:\n            root, dirs, files = next(os.walk(folder))\n            for file in [fn for fn in files if fn.endswith(\'.ipynb\')]:\n                src = os.path.join(folder, file)\n                print(src)\n                os.system(f""jupyter nbconvert --ClearOutputPreprocessor.enabled=True --inplace {src}"")\n                os.system(f""dos2unix {src}"")\n\n\nif __name__ == \'__main__\':\n    in_place_chaning = True\n    TestFormat().test_format()\n    TestFormat().clear_notebook_output()\n'"
contrib/python-sdk/test/test_job.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom basic_test import OrderedUnitTestCase, separated\nfrom openpaisdk import to_screen\n\n\nclass TestJobResource(OrderedUnitTestCase):\n\n    def test_job_resource_parser(self):\n        from openpaisdk.job import JobResource\n        from openpaisdk import __flags__\n        self.assertDictEqual(__flags__.resources_requirements, JobResource(None).as_dict)\n        self.assertDictEqual(__flags__.resources_requirements, JobResource().as_dict)\n        self.assertDictEqual(__flags__.resources_requirements, JobResource({}).as_dict)\n        dic = dict(cpu=-1, gpu=-2, memoryMB=-1024)\n        for key, value in dic.items():\n            self.assertEqual(value, JobResource(dic).as_dict[key])\n        dic[\'mem\'] = \'-2gb\'\n        self.assertEqual(-2048, JobResource(dic).as_dict[""memoryMB""])\n        dic[\'mem\'] = \'-3g\'\n        self.assertEqual(-3072, JobResource(dic).as_dict[""memoryMB""])\n        dic[\'mem\'] = 10240\n        self.assertEqual(10240, JobResource(dic).as_dict[""memoryMB""])\n        self.assertEqual({""a"": 1}, JobResource(dic).add_port(""a"").as_dict[""ports""])\n\n    def test_job_resource_list(self):\n        from openpaisdk.job import JobResource\n        samples = {\n            ""3,3,3g"": dict(gpu=3, cpu=3, memoryMB=3072, ports={}),\n            ""3,1, 2g"": dict(gpu=3, cpu=1, memoryMB=2048, ports={}),\n        }\n        keys = list(samples.keys())\n        rets = JobResource.parse_list(keys)\n        for k, r in zip(keys, rets):\n            self.assertDictEqual(r, samples[k])\n'"
contrib/python-sdk/test/test_notebook.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom basic_test import OrderedUnitTestCase, separated\nfrom openpaisdk import to_screen\n\n\nclass TestNbExtCfg(OrderedUnitTestCase):\n\n    settings = dict(cpu=100, gpu=-2, mem=\'90g\')\n\n    def step1_init(self):\n        from openpaisdk.notebook import NotebookConfiguration\n        NotebookConfiguration.print_supported_items()\n\n    def step2_setup(self):\n        from openpaisdk.notebook import NotebookConfiguration\n        from openpaisdk import LayeredSettings\n        NotebookConfiguration.set(**self.settings)\n        for key in self.settings.keys():\n            LayeredSettings.update(\'user_basic\', key, -1)\n\n    def step3_check(self):\n        from openpaisdk.notebook import NotebookConfiguration\n        to_screen(NotebookConfiguration.get())\n        dic = {k: NotebookConfiguration.get(k) for k in self.settings}\n        self.assertDictEqual(dic, self.settings)\n\n    @separated\n    def test_nbext_configuration(self):\n        self.run_steps()\n'"
contrib/python-sdk/test/test_utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport unittest\nfrom copy import deepcopy\nfrom openpaisdk.utils import OrganizedList as ol\nfrom openpaisdk.utils import Nested\nfrom openpaisdk.utils import randstr\nfrom openpaisdk.io_utils import __flags__, from_file, to_screen\nfrom openpaisdk import get_defaults, update_default, LayeredSettings\nfrom basic_test import separated\n\n\nclass TestIOUtils(unittest.TestCase):\n\n    @separated\n    def test_reading_failures(self):\n        with self.assertRaises(Exception):  # non existing file\n            from_file(randstr(8) + \'.yaml\')\n        with self.assertRaises(AssertionError):  # unsupported file extension\n            from_file(randstr(10))\n        with self.assertRaises(Exception):\n            fname = randstr(10) + \'.json\'\n            os.system(f""touch {fname}"")\n            from_file(fname)\n\n    @separated\n    def test_returning_default(self):\n        for dval in [[], [\'a\', \'b\'], {}, {\'a\': \'b\'}]:\n            ass_fn = self.assertListEqual if isinstance(dval, list) else self.assertDictEqual\n            with self.assertRaises(AssertionError):  # unsupported file extension\n                from_file(randstr(10))\n            fname = randstr(8) + \'.yaml\'\n            ass_fn(from_file(fname, dval), dval)  # non existing\n            os.system(f""echo \'\' > {fname}"")\n            ass_fn(from_file(fname, dval), dval)\n            os.system(f""echo \'abcd\' > {fname}"")\n            ass_fn(from_file(fname, dval), dval)\n\n\nclass TestDefaults(unittest.TestCase):\n\n    global_default_file = __flags__.get_default_file(is_global=True)\n    local_default_file = __flags__.get_default_file(is_global=False)\n\n    def get_random_var_name(self):\n        import random\n        from openpaisdk import LayeredSettings\n        lst = [x for x in LayeredSettings.keys() if not LayeredSettings.act_append(x)]\n        ret = lst[random.randint(0, len(lst) - 1)]\n        to_screen(f""random select {ret} in {lst}"")\n        return ret\n\n    @separated\n    def test_update_defaults(self):\n        # ! not test global defaults updating, test it in integration tests\n        test_key, test_value = self.get_random_var_name(), randstr(10)\n        # add a default key\n        update_default(test_key, test_value, is_global=False, to_delete=False)\n        self.assertEqual(get_defaults()[test_key], test_value,\n                         msg=f""failed to check {test_key} in {LayeredSettings.as_dict()}"")\n        # should appear in local\n        self.assertEqual(from_file(self.local_default_file)[test_key], test_value)\n        # delete\n        update_default(test_key, test_value, is_global=False, to_delete=True)\n        with self.assertRaises(KeyError):\n            os.system(f""cat {self.local_default_file}"")\n            from_file(self.local_default_file, {})[test_key]\n        # add not allowed\n        test_key = randstr(10)\n        update_default(test_key, test_value, is_global=False, to_delete=False)\n        with self.assertRaises(KeyError):\n            from_file(self.local_default_file, {})[test_key]\n\n    @separated\n    def test_layered_settings(self):\n        from openpaisdk import LayeredSettings, __flags__\n        __flags__.custom_predefined = [\n            {\n                \'name\': \'test-key-1\',\n            },\n            {\n                \'name\': \'test-key-2\',\n                \'action\': \'append\',\n                \'default\': []\n            }\n        ]\n        LayeredSettings.reset()\n        # ? add / update append key\n        for test_key in [\'test-key-1\', \'test-key-2\']:\n            for i, layer in enumerate(LayeredSettings.layers):\n                LayeredSettings.update(layer.name, test_key, i)\n                if layer.act_append(test_key):\n                    self.assertTrue(isinstance(layer.values[test_key], list), msg=f""{layer.values}"")\n        self.assertEqual(0, LayeredSettings.get(\'test-key-1\'))\n        self.assertListEqual([0, 1, 2, 3], LayeredSettings.get(\'test-key-2\'))\n        # ? delete\n        for test_key in [\'test-key-1\', \'test-key-2\']:\n            for i, layer in enumerate(LayeredSettings.layers):\n                LayeredSettings.update(layer.name, test_key, None, delete=True)\n        # ? reset the predefined\n        __flags__.custom_predefined = []\n        LayeredSettings.reset()\n\n    @separated\n    def test_unknown_variable_defined(self):\n        from openpaisdk import LayeredSettings, __flags__\n        test_key, test_value = \'test-key-long-existing\', randstr(10)\n        __flags__.custom_predefined = [\n            {\n                \'name\': test_key,\n            },\n        ]\n        LayeredSettings.reset()\n        # ? add / update append key\n        LayeredSettings.update(\'local_default\', test_key, test_value)\n        # ? reset the predefined\n        __flags__.custom_predefined = []\n        LayeredSettings.reset()\n        self.assertEqual(test_value, LayeredSettings.get(test_key))\n        # cannot delete or change the unknown variable\n        LayeredSettings.update(\'local_default\', test_key, randstr(10))\n        LayeredSettings.reset()\n        self.assertEqual(test_value, LayeredSettings.get(test_key))\n        LayeredSettings.update(\'local_default\', test_key, delete=True)\n        LayeredSettings.reset()\n        self.assertEqual(test_value, LayeredSettings.get(test_key))\n\n\nclass TestOrganizedList(unittest.TestCase):\n\n    class foo:\n\n        def __init__(self, a=None, b=None, c=None, d=None):\n            self.a, self.b, self.c, self.d = a, b, c, d\n\n        @property\n        def as_dict(self):\n            return {k: v for k, v in vars(self).items() if v is not None}\n\n        def update(self, other):\n            for key, value in other.as_dict.items():\n                setattr(self, key, value)\n\n    lst_objs = [foo(""x"", 0), foo(""x"", 1), foo(""y"", 2), foo(""y"", c=1), foo(""z"", 4)]\n    lst = [obj.as_dict for obj in lst_objs]\n\n    def ol_test_run(self, lst, getter):\n        def to_dict(obj):\n            return obj if isinstance(obj, dict) else obj.as_dict\n        dut = ol(lst[:3], ""a"", getter)\n        # find\n        self.assertEqual(2, dut.first_index(""y""))\n        self.assertDictEqual(to_dict(lst[2]), to_dict(dut.first(""y"")))\n        # filter\n        self.assertListEqual([0, 1], dut.filter_index(""x""))\n        self.assertListEqual(lst[:2], dut.filter(""x"").as_list)\n        # as_dict\n        self.assertDictEqual(dict(x=lst[1], y=lst[2]), dut.as_dict)\n        # add (update)\n        elem = lst[-2]\n        dut.add(elem)\n        self.assertEqual(2, getter(lst[2], ""b""))\n        self.assertEqual(1, getter(lst[2], ""c""))\n        # add (replace)\n        elem = lst[-2]\n        dut.add(elem, replace=True)\n        self.assertEqual(None, getter(dut[2], ""b""))\n        # add (append)\n        elem = lst[-1]\n        dut.add(elem)\n        self.assertEqual(4, getter(dut[-1], ""b""))\n        # delete\n        dut.remove(""z"")\n        self.assertEqual(3, len(dut))\n        dut.remove(""z"")\n        self.assertEqual(3, len(dut))\n\n    def test_dict(self):\n        self.ol_test_run(deepcopy(self.lst), dict.get)\n\n    def test_obj(self):\n        self.ol_test_run(deepcopy(self.lst_objs), getattr)\n\n\nclass TestNested(unittest.TestCase):\n\n    def test_set(self):\n        nested_obj = {\n            ""a"": [\n                {\n                    ""aa0"": {\n                        ""aaa"": ""val_aaa""\n                    },\n                },\n                {\n                    ""aa1"": {\n                        ""aaa1"": ""val_aaa1""\n                    }\n                }\n\n            ],\n            ""b"": ""haha""\n        }\n        n = Nested(nested_obj, sep=""->"")\n        self.assertEqual(n.get(""a->0->aa0->aaa""), ""val_aaa"")\n        with self.assertRaises(KeyError):\n            nested_obj[""a""][1][""aa2""][""aaa""]\n        n.set(""a->1->aa2->aaa"", ""val_aaa2"")\n        self.assertEqual(nested_obj[""a""][1][""aa2""][""aaa""], ""val_aaa2"")\n'"
contrib/storage_plugin/utils/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
contrib/storage_plugin/utils/storage_util.py,0,"b'#!/usr/bin/env python\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport logging\nimport logging.config\nimport base64\n\nfrom kubernetes import client, config, watch\nfrom kubernetes.client.rest import ApiException\n\nlogger = logging.getLogger(__name__)\n\ndef confirm_namespace(namespace):\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    try:\n        api_response = api_instance.read_namespace(namespace)\n\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find namespace {0}. Create new namespace"".format(namespace))\n            try:\n                meta_data = client.V1ObjectMeta(name=namespace)\n                body = client.V1ConfigMap(metadata=meta_data)\n                api_response = api_instance.create_namespace(body)\n                logger.info(""Namesapce {0} is created"".format(namespace))\n            except ApiException as ie:\n                logger.error(""Exception when calling CoreV1Api->create_namespace: {0}"".format(str(ie)))\n                sys.exit(1)\n        else:\n            logger.error(""Exception when calling CoreV1Api->read_namespace: {0}"".format(str(e)))\n            sys.exit(1)\n\n\n# List usernames from pai-user secrets\ndef get_pai_users():\n    users = []\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    try:\n        api_response = api_instance.list_namespaced_secret(""pai-user"")\n        for item in api_response.items:\n            users.append(base64.b64decode(item.data[""username""]))\n\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find secret in namespace pai-user, exit"")\n            sys.exit(1)\n        else:\n            logger.error(""Exception when calling CoreV1Api->list_namespaced_secret: {0}"".format(str(e)))\n            sys.exit(1)\n\n    return users\n\n\ndef update_configmap(name, data_dict, namespace):\n    confirm_namespace(namespace)\n\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    meta_data = client.V1ObjectMeta()\n    meta_data.namespace = namespace\n    meta_data.name = name\n    body = client.V1ConfigMap(\n                metadata = meta_data,\n                data = data_dict)\n\n    try:\n        api_response = api_instance.patch_namespaced_config_map(name, namespace, body)\n        logger.info(""configmap named {0} is updated."".format(name))\n    except ApiException as e:\n        if e.status == 404:\n            try:\n                logger.info(""Couldn\'t find configmap named {0}. Create a new configmap"".format(name))\n                api_response = api_instance.create_namespaced_config_map(namespace, body)\n                logger.info(""Configmap named {0} is created"".format(name))\n            except ApiException as ie:\n                logger.error(""Exception when calling CoreV1Api->create_namespaced_config_map: {0}"".format(str(ie)))\n                sys.exit(1)\n        else:\n            logger.error(""Exception when calling CoreV1Api->patch_namespaced_config_map: {0}"".format(str(e)))\n            sys.exit(1)\n\n\ndef get_storage_config(storage_config_name, namespace):\n    confirm_namespace(namespace)\n\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    try:\n        api_response = api_instance.read_namespaced_config_map(storage_config_name, namespace)\n\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find configmap named {0}."".format(storage_config_name))\n            return None\n        else:\n            logger.error(""Exception when calling CoreV1Api->read_namespaced_config_map: {0}"".format(str(e)))\n            sys.exit(1)\n\n    return api_response.data\n\n\ndef patch_secret(name, data_dict, namespace):\n    confirm_namespace(namespace)\n\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    meta_data = client.V1ObjectMeta()\n    meta_data.namespace = namespace\n    meta_data.name = name\n    body = client.V1Secret(metadata = meta_data, data = data_dict)\n\n    try:\n        api_response = api_instance.patch_namespaced_secret(name, namespace, body)\n        logger.info(""Secret named {0} is updated."".format(name))\n    except ApiException as e:\n        logger.info(e)\n        if e.status == 404:\n            try:\n                logger.info(""Couldn\'t find secret named {0}. Create a new secret"".format(name))\n                api_response = api_instance.create_namespaced_secret(namespace, body)\n                logger.info(""Secret named {0} is created"".format(name))\n            except ApiException as ie:\n                logger.error(""Exception when calling CoreV1Api->create_namespaced_secret: {0}"".format(str(ie)))\n                sys.exit(1)\n        else:\n            logger.error(""Exception when calling CoreV1Api->patch_namespaced_secret: {0}"".format(str(e)))\n            sys.exit(1)\n\n\ndef get_secret(name, namespace):\n    confirm_namespace(namespace)\n\n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n\n    try:\n        api_response = api_instance.read_namespaced_secret(name, namespace)\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find secret named {0}."".format(name))\n            return None\n        else:\n            logger.error(""Exception when calling CoreV1Api->read_namespaced_config_map: {0}"".format(str(e)))\n            sys.exit(1)\n\n    return api_response.data\n\n\ndef delete_secret_content(name, key, namespace):\n    confirm_namespace(namespace)\n    \n    config.load_kube_config()\n    api_instance = client.CoreV1Api()\n    try:\n        api_response = api_instance.read_namespaced_secret(name, namespace)\n        if api_response is not None and type(api_response.data) is dict:\n            removed_content = api_response.data.pop(key, None)\n            if removed_content is not None:\n                meta_data = client.V1ObjectMeta()\n                meta_data.namespace = namespace\n                meta_data.name = name\n                body = client.V1Secret(metadata = meta_data, data = api_response.data)\n                api_instance.replace_namespaced_secret(name, namespace, body)\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find secret named {0}."".format(name))\n        else:\n            logger.error(""Exception when try to delete {0} from {1}: reason: {2}"".format(key, name, str(e)))\n            sys.exit(1)\n'"
deployment/clusterObjectModel/mainParser/__init__.py,0,b''
deployment/clusterObjectModel/mainParser/kubernetes.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport logging\nimport logging.config\n\nfrom ...k8sPaiLibrary.maintainlib import common as pai_k8s_common\n\n\nclass Kubernetes:\n\n    def __init__(self, cluster_configuration, kubernetes_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.kubernetes_configuration = kubernetes_configuration\n\n    def get_k8s_master_machine(self):\n        k8s_master_list = []\n        for host in self.cluster_configuration[""machine-list""]:\n            if host[""k8s-role""] == ""master"":\n                k8s_master_list.append(host)\n        return k8s_master_list\n\n    def generate_etcd_ip_list(self, master_list):\n        etcd_cluster_ips_peer = """"\n        etcd_cluster_ips_server = """"\n        separated = """"\n        for infra in master_list:\n            ip = infra[\'hostip\']\n            etcdid = infra[\'etcdid\']\n            ip_peer = ""{0}=http://{1}:2380"".format(etcdid, ip)\n            ip_server = ""http://{0}:4001"".format(ip)\n\n            etcd_cluster_ips_peer = etcd_cluster_ips_peer + separated + ip_peer\n            etcd_cluster_ips_server = etcd_cluster_ips_server + separated + ip_server\n\n            separated = "",""\n\n        return etcd_cluster_ips_peer, etcd_cluster_ips_server\n\n    def get_k8s_dashboard_node_ip(self):\n        hostip = """"\n        for host in self.cluster_configuration[""machine-list""]:\n            if ""dashboard"" in host and host[""dashboard""] == ""true"":\n                hostip = host[""hostip""]\n                break\n        if hostip == """":\n            print(""no machine labeled with dashboard = true"")\n            sys.exit(1)\n\n        return hostip\n\n    def run(self):\n        k8s_cfg = self.kubernetes_configuration[""kubernetes""]\n        com_kubernetes = dict()\n\n        com_kubernetes[""cluster-dns""] = k8s_cfg[""cluster-dns""]\n        com_kubernetes[""api-servers-ip""] = k8s_cfg[""load-balance-ip""]\n        com_kubernetes[""api-servers-port""] = k8s_cfg[""api-servers-port""] if (""api-servers-port"" in k8s_cfg) else ""8080""\n        com_kubernetes[""api-servers-http-schema""] = k8s_cfg[""api-servers-http-schema""] if (""api-servers-http-schema"" in k8s_cfg) else ""http""\n        com_kubernetes[""api-servers-url""] = ""{0}://{1}:{2}"".format(com_kubernetes[""api-servers-http-schema""], k8s_cfg[""load-balance-ip""], com_kubernetes[""api-servers-port""])\n        com_kubernetes[""docker-registry""] = k8s_cfg[""docker-registry""]\n        com_kubernetes[""hyperkube-version""] = k8s_cfg[""hyperkube-version""]\n        com_kubernetes[""etcd-version""] = k8s_cfg[""etcd-version""]\n        com_kubernetes[""service-cluster-ip-range""] = k8s_cfg[""service-cluster-ip-range""]\n        com_kubernetes[""apiserver-version""] = k8s_cfg[""apiserver-version""]\n        com_kubernetes[""storage-backend""] = k8s_cfg[""storage-backend""]\n        com_kubernetes[""kube-scheduler-version""] = k8s_cfg[""kube-scheduler-version""]\n        com_kubernetes[""kube-controller-manager-version""] = k8s_cfg[""kube-controller-manager-version""]\n        com_kubernetes[""dashboard-version""] = k8s_cfg[""dashboard-version""]\n        com_kubernetes[""dashboard-host""] = self.get_k8s_dashboard_node_ip()\n        com_kubernetes[""dashboard-url""] = ""http://{0}:9090"".format(self.get_k8s_dashboard_node_ip())\n\n        if ""etcd-data-path"" not in k8s_cfg:\n            com_kubernetes[""etcd-data-path""] = ""/var/etcd/data""\n        else:\n            com_kubernetes[""etcd-data-path""] = k8s_cfg[""etcd-data-path""]\n\n        com_kubernetes[""qos-switch""] = k8s_cfg[""qos-switch""] if (""qos-switch"" in k8s_cfg) else ""true""\n\n        k8s_master_list = self.get_k8s_master_machine()\n        etcd_cluster_ips_peer, etcd_cluster_ips_server = self.generate_etcd_ip_list(k8s_master_list)\n\n        # ETCD will communicate with each other through this address.\n        com_kubernetes[\'etcd_cluster_ips_peer\'] = etcd_cluster_ips_peer\n        # Other service will write and read data through this address.\n        com_kubernetes[\'etcd_cluster_ips_server\'] = etcd_cluster_ips_server\n        com_kubernetes[\'etcd-initial-cluster-state\'] = \'new\'\n\n        master_list = []\n        worker_list = []\n        proxy_list = []\n\n        for host in self.cluster_configuration[""machine-list""]:\n            if host[""k8s-role""] == ""master"":\n                master_list.append(host[""hostname""])\n            elif host[""k8s-role""] == ""worker"":\n                worker_list.append(host[""hostname""])\n            elif host[""k8s-role""] == ""proxy"":\n                proxy_list.append(host[""hostname""])\n\n        if len(master_list) != 0:\n            com_kubernetes[""master-list""] = master_list\n        if len(worker_list) != 0:\n            com_kubernetes[""worker-list""] = worker_list\n        if len(proxy_list) != 0:\n            com_kubernetes[""proxy-list""] = proxy_list\n\n        return com_kubernetes\n\n    def validation_pre(self):\n        k8s_cfg = self.kubernetes_configuration[""kubernetes""]\n\n        if ""cluster-dns"" not in k8s_cfg:\n            return False, ""cluster-dns is miss in kubernetes-configuration -> kubernetes. You can get this value with the command [cat /etc/resolv.conf]""\n        if pai_k8s_common.ipv4_address_validation(k8s_cfg[""cluster-dns""]) is False:\n            return False, ""cluster-dns in kubernetes-configuration is not a valid ipv4 address.""\n\n        if ""load-balance-ip"" not in k8s_cfg:\n            return False, ""load-balance-ip is miss in kubernetes-configuration -> kubernetes.""\n        if pai_k8s_common.ipv4_address_validation(k8s_cfg[""load-balance-ip""]) is False:\n            return False, ""load-balance-ip in kubernetes-configuration is not a valid ipv4 address""\n\n        if ""service-cluster-ip-range"" not in k8s_cfg:\n            return False, ""service-cluster-ip-range is miss in kubernetes-configuration -> kubernetes.""\n        if pai_k8s_common.cidr_validation(k8s_cfg[""service-cluster-ip-range""]) is False:\n            return False, ""service-cluster-ip-range in kubernetes-configuration is not a valid CIDR.""\n\n        if ""storage-backend"" not in k8s_cfg:\n            return False, ""storage-backend is miss in kubernetes-configuration -> kubernetes.""\n        if k8s_cfg[""storage-backend""] != ""etcd3"" and k8s_cfg[""storage-backend""] != ""etcd2"":\n            return False, ""storage-backend in kubernetes-configuration is not valid, please set corresponding value [etcd2 or etcd3] according to your etcd version.""\n\n        if ""docker-registry"" not in k8s_cfg:\n            return False, ""docker-registry is miss in kubernetes-configuration -> kubernetes.""\n\n        if ""hyperkube-version"" not in k8s_cfg:\n            return False, ""hyperkube-version is miss in kubernetes-configuration -> kubernetes.""\n\n        if ""etcd-version"" not in k8s_cfg:\n            return False, ""etcd-version is miss in kubernetes-configuration -> kubernetes.""\n\n        if ""apiserver-version"" not in k8s_cfg:\n            return False, ""apiserver-version is miss in kuberentes-configuration -> kubernetes.""\n\n        if ""kube-scheduler-version"" not in k8s_cfg:\n            return False, ""kube-scheduler-version is miss in kubernetes-configuration -> kubernetes.""\n\n        if ""kube-controller-manager-version"" not in k8s_cfg:\n            return False, ""kube-controller-manager-version is miss in kubernetes-configuration -> kubernetes.""\n\n        if ""dashboard-version"" not in k8s_cfg:\n            return False, ""dashboard-version is miss in kuberentes-configuration -> kubernetes.""\n\n        return True, None\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n'"
deployment/clusterObjectModel/mainParser/layout.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\nfrom ...k8sPaiLibrary.maintainlib import common as pai_k8s_common\n\n\nclass Layout:\n\n    def __init__(self, layout_configuration):\n        self.logger = logging.getLogger(__name__)\n        self.layout_configuration = layout_configuration\n\n\n\n    def validation_pre(self):\n        # TODO\n\n        # validate unique hostname\n        host_list = [host[""hostname""] for host in self.layout_configuration[""machine-list""]]\n        duplicate_host_list = set([host for host in host_list if host_list.count(host) > 1])\n        if duplicate_host_list:\n            return False, ""duplicate hostname [{}] in kubernetes-configuration"".format("", "".join(duplicate_host_list))\n\n        return True, None\n\n\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n\n\n    def run(self):\n        com_layout = dict()\n        com_layout[""machine-sku""] = self.layout_configuration[""machine-sku""]\n        com_layout[""kubernetes""] = self.layout_configuration[""kubernetes""]\n        com_layout[""machine-list""] = dict()\n\n        for host in self.layout_configuration[""machine-list""]:\n            com_layout[""machine-list""][host[""hostname""]] = host\n\n        return com_layout\n\n'"
deployment/clusterObjectModel/mainParser/machine.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\nfrom ...k8sPaiLibrary.maintainlib import common as pai_k8s_common\n\n\nclass Machine:\n\n    def __init__(self, cluster_configuration):\n        self.logger = logging.getLogger(__name__)\n        self.cluster_configuration = cluster_configuration\n\n\n\n    def validation_default_machine_properties(self):\n        cluster_cfg = self.cluster_configuration\n        if ""default-machine-properties"" not in cluster_cfg:\n            return False, ""default-machine-properties is miss in cluster-configuration.yaml""\n        if ""username"" not in cluster_cfg[""default-machine-properties""]:\n            return False, ""username is miss in cluster-configuration -> default-machine-properties.""\n        if ""sshport"" not in cluster_cfg[""default-machine-properties""]:\n            return False, ""sshport is miss in cluster-configuration -> default-machine-properties.""\n        if (""keyfile-path"" in cluster_cfg[""default-machine-properties""]) is (""password"" in cluster_cfg[""default-machine-properties""]):\n            return False, ""Please set one and only one property between keyfile-path and password in cluster-configuration -> default-machine-properties.""\n        return True, None\n\n\n\n    def validation_machine_sku(self):\n        cluster_cfg = self.cluster_configuration\n        if ""machine-sku"" not in cluster_cfg:\n            return False, ""machine-sku is miss.""\n        for sku_name in cluster_cfg[""machine-sku""]:\n            sku = cluster_cfg[""machine-sku""][sku_name]\n            if ""cpu"" not in sku:\n                return False, ""cpu is miss in the sku named [{0}]"".format(sku_name)\n            if ""mem"" not in sku:\n                return False, ""mem is miss in the sku named [{0}]"".format(sku_name)\n            if ""os"" not in sku:\n                return False, ""os is miss in the sku named [{0}]"".format(sku_name)\n        return True, False\n\n\n\n    def validation_host_properties(self):\n        cluster_cfg = self.cluster_configuration\n        etcd_id_visited = dict()\n        ip_visited = dict()\n        dashboard_count = 0\n\n        for host in cluster_cfg[""machine-list""]:\n            if ""hostip"" not in host:\n                return False, ""hostip is miss in the host [{0}]"".format(str(host))\n            if pai_k8s_common.ipv4_address_validation(host[""hostip""]) is False:\n                return False, ""Please ensure the hostip is right, in the host [{0}]"".format(str(host))\n            if host[""hostip""] in ip_visited:\n                return False, ""Duplicated IP address in machine-list. IP address is [{0}]"".format(str(host[""hostip""]))\n            ip_visited[host[""hostip""]] = True\n\n            if ""machine-type"" not in host:\n                return False, ""machine-type is miss in the host [{0}]"".format(str(host))\n            if host[""machine-type""] not in cluster_cfg[""machine-sku""]:\n                return False, ""machine-type [{0}] is not in machine-sku.""\n\n            if ""k8s-role"" not in host:\n                return False, ""k8s-role is miss in the host [{0}]"".format(str(host))\n            if ""k8s-role"" is ""master"":\n                if ""etcdid"" not in host:\n                    return False, ""etcdid is miss in one of the host with the [k8s-role: master].""\n                if host[""etcdid""] in etcd_id_visited:\n                    return False, ""Duplicated etcdid [{0}]. etcdid of each k8s master node shoule be unique."".format(host[""etcdid""])\n\n            if ""pai-master"" in host and ""zkid"" not in host:\n                return False, ""zkid is miss in one of the host with the [pai-master: true]""\n\n            if ""dashboard"" in host and host[""dashboard""] == ""true"":\n                dashboard_count = dashboard_count + 1\n\n        if dashboard_count == 0:\n            return False, ""dashboard label is miss.""\n\n        return True, None\n\n\n\n    def validation_pre(self):\n        ok, msg = self.validation_default_machine_properties()\n        if ok is False:\n            return False, msg\n\n        ok, msg = self.validation_machine_sku()\n        if ok is False:\n            return False, msg\n\n        ok, msg = self.validation_host_properties()\n        if ok is False:\n            return False, msg\n\n        return True, None\n\n\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n\n\n    def run(self):\n        com_machine = dict()\n        com_machine[""machine-sku""] = self.cluster_configuration[""machine-sku""]\n        com_machine[""default-machine-properties""] = self.cluster_configuration[""default-machine-properties""]\n        com_machine[""machine-list""] = dict()\n\n        for host in self.cluster_configuration[""machine-list""]:\n            if ""sshport"" not in host:\n                host[""sshport""] = com_machine[""default-machine-properties""][""sshport""]\n            if ""username"" not in host:\n                host[""username""] = com_machine[""default-machine-properties""][""username""]\n            if ""password"" not in host and ""keyfile-path"" not in host:\n                if ""password"" in com_machine[""default-machine-properties""]:\n                    host[""password""] = com_machine[""default-machine-properties""][""password""]\n                else:\n                    host[""keyfile-path""] = com_machine[""default-machine-properties""][""keyfile-path""]\n            if ""nodename"" not in host:\n                host[""nodename""] = host[""hostip""]\n            if ""docker-data"" not in host:\n                host[""docker-data""] = ""/var/lib/docker""\n            com_machine[""machine-list""][host[""hostname""]] = host\n\n        return com_machine\n'"
deployment/clusterObjectModel/test/__init__.py,0,b''
deployment/clusterObjectModel/test/test_cluster_object_model.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport unittest\nimport yaml\nimport logging\nimport logging.config\n\nfrom ...clusterObjectModel import cluster_object_model\n\n\npackage_directory_com = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestClusterObjectModel(unittest.TestCase):\n\n    def setUp(self):\n\n        try:\n\n            os.chdir(package_directory_com)\n\n        except:\n\n            pass\n\n        configuration_path = ""data/test_logging.yaml""\n\n        if os.path.exists(configuration_path):\n            with open(configuration_path, \'rt\') as f:\n                logging_configuration = yaml.safe_load(f.read())\n\n            logging.config.dictConfig(logging_configuration)\n\n            logging.getLogger()\n\n\n\n    def test_cluster_object_model_cfg_no_overwrite(self):\n\n        no_overwrite_path = ""data/configuration-none-overwrite/""\n        com_handler = cluster_object_model.cluster_object_model(no_overwrite_path)\n        com_handler.kubernetes_config()\n\n\n\n    def test_cluster_object_model_cfg_overwrite(self):\n\n        overwrite_path = ""data/configuration-overwrite/""\n        com_handler = cluster_object_model.cluster_object_model(overwrite_path)\n        com_handler.kubernetes_config()\n\n\n    def test_cluster_object_model_cfg_duplicate_hostname(self):\n\n        duplicate_hostname_path = ""data/configuration-duplicate-hostname/""\n        com_handler = cluster_object_model.cluster_object_model(duplicate_hostname_path)\n        with self.assertRaises(SystemExit) as cm:\n            com_handler.kubernetes_config()\n        self.assertEqual(cm.exception.code, 1)\n'"
deployment/clusterObjectModel/test/test_forward_compatibility.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport unittest\nimport yaml\nimport logging\nimport logging.config\n\nfrom ...clusterObjectModel import forward_compatibility\nfrom ...paiLibrary.common import file_handler\n\n\npackage_directory_com = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestClusterObjectModel(unittest.TestCase):\n\n    def setUp(self):\n\n        try:\n\n            os.chdir(package_directory_com)\n\n        except:\n\n            pass\n\n        configuration_path = ""data/test_logging.yaml""\n\n        if os.path.exists(configuration_path):\n            with open(configuration_path, \'rt\') as f:\n                logging_configuration = yaml.safe_load(f.read())\n\n            logging.config.dictConfig(logging_configuration)\n\n            logging.getLogger()\n\n\n\n    def test_service_configuration_convert(self):\n\n        ser_cfg_before_convert = ""data/forward_compatibility_data/service-configuration.yaml""\n        ser_cfg_after_convert = ""data/forward_compatibility_data/after-convert-service-configuration.yaml""\n\n        ser_cfg = file_handler.load_yaml_config(ser_cfg_before_convert)\n        ser_cfg_after_standard = file_handler.load_yaml_config(ser_cfg_after_convert)\n        ser_cfg_after, updated = forward_compatibility.service_configuration_convert(ser_cfg)\n\n        print ""Generated:""\n        print ser_cfg_after\n\n        print ""Standard:""\n        print ser_cfg_after_standard\n\n        self.assertTrue(\n            ser_cfg_after == ser_cfg_after_standard\n        )'"
deployment/clusterObjectModel/test/test_template_generate.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport unittest\nimport yaml\nimport logging\nimport logging.config\n\nfrom ...paiLibrary.common import directory_handler\nfrom ...paiLibrary.common import file_handler\nfrom ...clusterObjectModel.cluster_object_model import cluster_object_model\nfrom ...paiLibrary.paiService.service_template_generate import service_template_generate\nfrom ...paiLibrary.paiService.service_template_clean import  service_template_clean\n\n\npackage_directory_com = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestTemplateGenerate(unittest.TestCase):\n\n    def setUp(self):\n\n        try:\n\n            os.chdir(package_directory_com)\n\n        except:\n\n            pass\n\n        configuration_path = ""data/test_logging.yaml""\n\n        if os.path.exists(configuration_path):\n            with open(configuration_path, \'rt\') as f:\n                logging_configuration = yaml.safe_load(f.read())\n\n            logging.config.dictConfig(logging_configuration)\n\n            logging.getLogger()\n\n\n\n    def test_template_generate(self):\n\n        com_handler = cluster_object_model(""{0}/data/configuration-template-generate"".format(package_directory_com))\n        com = com_handler.service_config()\n        src_path = ""{0}/../../../src"".format(package_directory_com)\n\n        service_list = list()\n\n        subdir_list = directory_handler.get_subdirectory_list(src_path)\n        for subdir in subdir_list:\n            service_deploy_dir = ""{0}/{1}/deploy"".format(src_path, subdir)\n            service_deploy_conf_path = ""{0}/{1}/deploy/service.yaml"".format(src_path, subdir)\n            if file_handler.directory_exits(service_deploy_dir) and file_handler.file_exist_or_not(service_deploy_conf_path):\n                service_conf = file_handler.load_yaml_config(service_deploy_conf_path)\n                if (""cluster-type"" not in service_conf) or (""cluster-type"" in service_conf and ""k8s"" in service_conf[""cluster-type""]):\n                    service_list.append(subdir)\n\n        for serv in service_list:\n            service_conf = file_handler.load_yaml_config(""{0}/{1}/deploy/service.yaml"".format(src_path, serv))\n            service_template_generater = service_template_generate(com, serv, service_conf)\n            service_template_generater.run()\n            service_template_cleaner = service_template_clean(serv, service_conf)\n            service_template_cleaner.run()\n\n'"
deployment/confStorage/external_version_control/__init__.py,0,b''
deployment/confStorage/external_version_control/external_config.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport yaml\nimport readline\nimport logging\nimport logging.config\n\nfrom .. import conf_storage_util\nfrom ...paiLibrary.common import template_handler\nfrom ...paiLibrary.common import file_handler\nfrom ...paiLibrary.common import kubernetes_handler\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass getting_external_config:\n\n    def __init__(self, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        # Local cluster configuration path\n        self.local_cluster_conf_path = None\n        if ""local_cluster_configuration"" in kwargs and kwargs[""local_cluster_configuration""] != None:\n            self.local_cluster_conf_path = kwargs[""local_cluster_configuration""]\n\n        # Configuration for local conf\n        self.external_storage_conf_path = None\n        if ""external_storage_conf_path"" in kwargs and kwargs[""external_storage_conf_path""] != None:\n            self.external_storage_conf_path = kwargs[""external_storage_conf_path""]\n\n        # Configuration for configmap [Access to k8s through exist kube_config.]\n        self.kube_config_path = None\n        if ""kube_config_path"" in kwargs and kwargs[""kube_config_path""] != None:\n            self.kube_config_path = kwargs[""kube_config_path""]\n\n        self.external_storage_configuration = None\n\n\n\n    def load_yaml_config(self, config_path):\n        with open(config_path, ""r"") as f:\n            cluster_data = yaml.load(f, yaml.SafeLoader)\n\n        return cluster_data\n\n\n\n    def load_from_local_conf(self):\n        self.external_storage_configuration = self.load_yaml_config(self.external_storage_conf_path)\n\n\n\n    def load_from_k8s_configmap(self, KUBE_CONFIG_PATH = None):\n        if KUBE_CONFIG_PATH is None:\n            KUBE_CONFIG_PATH = self.kube_config_path\n        configmap_data_dict = kubernetes_handler.get_configmap(KUBE_CONFIG_PATH, ""pai-external-storage-conf"")\n        if configmap_data_dict is None:\n            self.logger.error(""Unable to get the external storage configuration from k8s cluster."")\n            self.logger.error(""Please check the configmap named [pai-external-storage] in the namespace [default]."")\n            sys.exit(1)\n\n        self.external_storage_configuration = yaml.load(configmap_data_dict[""data""][""external-storage-conf""], yaml.SafeLoader)\n\n\n\n    def construct_local_storage_type(self):\n\n        external_storage_conf_tmp = dict()\n        external_storage_conf_tmp[""type""] = ""local""\n        external_storage_conf_tmp[""path""] = self.local_cluster_conf_path\n        self.external_storage_configuration = external_storage_conf_tmp\n\n\n\n    def get_latest_external_configuration(self):\n        if self.local_cluster_conf_path != None:\n            self.construct_local_storage_type()\n        elif self.external_storage_conf_path != None:\n            self.load_from_local_conf()\n        elif self.kube_config_path != None:\n            self.load_from_k8s_configmap()\n        else:\n            self.logger.error(""Ops, unable to get configuration conf."")\n            self.logger.error(""Please check your command and the corresponding path in your parameters."")\n            sys.exit(1)\n\n        return self.external_storage_configuration\n\n\n\n\nclass uploading_external_config:\n\n\n    def __init__(self, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        # Configuration for local conf\n        self.external_storage_conf_path = None\n        if ""external_storage_conf_path"" in kwargs and kwargs[""external_storage_conf_path""] != None:\n            self.external_storage_conf_path = kwargs[""external_storage_conf_path""]\n\n        # Configuration for configmap [Access to k8s through exist kube_config.]\n        self.kube_config_path = None\n        if ""kube_config_path"" in kwargs and kwargs[""kube_config_path""] != None:\n            self.kube_config_path = kwargs[""kube_config_path""]\n\n\n\n    def read_file_from_path(self, file_path):\n        with open(file_path, ""r"") as fin:\n            file_data = fin.read().decode(\'utf-8\')\n\n        return file_data\n\n\n\n    def load_from_local_conf(self):\n        return self.read_file_from_path(self.external_storage_conf_path)\n\n\n\n    def check_cluster_id(self):\n\n        cluster_id = conf_storage_util.get_cluster_id(self.kube_config_path)\n\n        if cluster_id is None:\n            self.logger.warning(""No cluster_id found in your cluster."")\n            user_input = raw_input(""Please input the cluster-id for your cluster: "")\n            conf_storage_util.update_cluster_id(self.kube_config_path, user_input)\n            return False\n\n        user_input = raw_input(""Please input the cluster-id which you wanna operate: "")\n        if user_input != cluster_id:\n            self.logger.error(""Ops, maybe you find the wrong cluster. Please check your input and the target cluster."")\n            sys.exit(1)\n\n        self.logger.info(""Congratulations: Cluster-id checking passed."")\n        return True\n\n\n\n    def update_latest_external_configuration(self):\n\n        self.logger.info(""Begin to update the latest external configuration to k8s-cluster."")\n        KUBE_CONFIG_PATH = None\n\n        if self.kube_config_path != None:\n            KUBE_CONFIG_PATH = self.kube_config_path\n            self.logger.info(""The path of KUBE_CONFIG is detected: {0} "".format(self.kube_config_path))\n        else:\n            self.logger.error(""Unable to find the kubeconfig to connect to target cluster."")\n            sys.exit(1)\n\n        self.logger.info(""Begin to load external cluster configuration from the path: {0}"".format(self.external_storage_conf_path))\n        external_storage_conf_dict = dict()\n        external_storage_conf_dict[""external-storage-conf""] = self.load_from_local_conf()\n\n        self.check_cluster_id()\n\n        kubernetes_handler.update_configmap(KUBE_CONFIG_PATH, ""pai-external-storage-conf"", external_storage_conf_dict)\n        self.logger.info(""Successfully update the external storage configuration."")\n'"
deployment/confStorage/external_version_control/git_storage.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport git\nimport sys\nimport time\nimport logging\n\n\nfrom ...paiLibrary.common import file_handler\nfrom ...paiLibrary.common import directory_handler\n\n\n\nclass git_storage:\n\n    def __init__(self, storage_configuration, local_store = ""/tmp/pai-conf-sync/git-storage""):\n\n        self.repo_url = storage_configuration[""url""]\n        self.branch = storage_configuration[""branch""]\n        self.path = storage_configuration[""path""]\n\n        self.time = str(int(time.time()))\n        self.logger = logging.getLogger(__name__)\n        self.local_store = ""{0}-{1}"".format(local_store, self.time)\n\n\n\n    def git_clone(self):\n        try:\n            self.repo = git.Repo.clone_from(self.repo_url, self.local_store, branch=self.branch, depth=1)\n        except:\n            self.logger.error(""Failed to clone the repo from [ url: {0},  branch: {1} ]"".format(self.repo_url, self.branch))\n            sys.exit(1)\n\n\n\n    def git_file_clean(self):\n        file_handler.file_delete(self.local_store)\n\n\n\n    def rm_conf(self):\n        pai_temp_path = ""./tmp-config-{0}"".format(self.time)\n        directory_handler.directory_delete(pai_temp_path)\n\n\n\n    def get_conf(self):\n\n        configuation_path = ""{0}/{1}"".format(self.local_store, self.path)\n        if not directory_handler.directory_exist_or_not(configuation_path):\n            self.logger.error(""Unable to find configuration path in the repo."")\n            self.logger.error(""Path: {0}"".format(self.path))\n            self.logger.error(""Repo: {0}"".format(self.repo_url))\n            sys.exit(1)\n\n        pai_temp_path = ""./tmp-config-{0}"".format(self.time)\n        directory_handler.directory_copy(""{0}/*"".format(configuation_path), pai_temp_path)\n\n\n\n    def open(self):\n        self.git_clone()\n        self.get_conf()\n        return ""./tmp-config-{0}"".format(self.time)\n\n\n\n    def close(self):\n        self.rm_conf()\n        self.git_file_clean()\n\n\n\n    def __enter__(self):\n        return self.open()\n\n\n\n    def __exit__(self, type, value, trace):\n        self.close()\n'"
deployment/confStorage/external_version_control/local_storage.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport sys\nimport time\nimport logging\n\nfrom ...paiLibrary.common import file_handler\nfrom ...paiLibrary.common import directory_handler\n\nclass local_storage:\n\n    def __init__(self, storage_configuration, local_store = ""/tmp/pai-conf-sync/git-storage""):\n\n        self.local_path = storage_configuration[""path""]\n\n\n        self.time = str(int(time.time()))\n        self.logger = logging.getLogger(__name__)\n        self.local_store = ""{0}-{1}"".format(local_store, self.time)\n\n\n\n    def rm_conf(self):\n        pai_temp_path = ""./tmp-config-{0}"".format(self.time)\n        directory_handler.directory_delete(pai_temp_path)\n\n\n\n    def get_conf(self):\n\n        configuation_path = self.local_path\n        if not directory_handler.directory_exist_or_not(configuation_path):\n            self.logger.error(""Unable to find configuration path in the path."")\n            self.logger.error(""Path: {0}"".format(self.local_path))\n            sys.exit(1)\n\n        pai_temp_path = ""./tmp-config-{0}"".format(self.time)\n        directory_handler.directory_copy(""{0}/*"".format(configuation_path), pai_temp_path)\n\n\n\n    def open(self):\n        self.get_conf()\n        return ""./tmp-config-{0}"".format(self.time)\n\n\n\n    def close(self):\n        self.rm_conf()\n\n\n\n    def __enter__(self):\n        return self.open()\n\n\n\n    def __exit__(self, type, value, trace):\n        self.close()'"
deployment/confStorage/external_version_control/storage_factory.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport sys\nimport yaml\nimport logging\nimport logging.config\n\nfrom .external_config import getting_external_config\nfrom .git_storage import git_storage\nfrom .local_storage import local_storage\n\n\nlogger = logging.getLogger(__name__)\n\n\n\ndef get_external_storage(storage_conf):\n\n    if storage_conf[""type""] == ""git"":\n        return git_storage(storage_conf)\n    elif storage_conf[""type""] == ""local"":\n        return local_storage(storage_conf)\n    else:\n        logger.error(""External Storage Type [ {0} ] is not supported yet."".format(storage_conf[""type""]))\n        sys.exit(1)\n'"
deployment/k8sPaiLibrary/maintainlib/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
deployment/k8sPaiLibrary/maintainlib/add.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport paramiko\nimport common\nimport time\nimport logging\nimport logging.config\n\nfrom . import common as pai_common\n\npackage_directory_add = os.path.dirname(os.path.abspath(__file__))\n\n\nclass add:\n\n    """"""\n    An class to add new node\n    """"""\n\n    def __init__(self, cluster_object_model, node_config, clean):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = cluster_object_model\n        self.node_config = node_config\n        maintain_configuration_path = os.path.join(package_directory_add, ""../maintainconf/add.yaml"")\n        self.maintain_config = common.load_yaml_file(maintain_configuration_path)\n        self.clean_flag = clean\n\n        if node_config[\'k8s-role\'] == \'worker\':\n            self.jobname = ""add-worker-node""\n        elif node_config[\'k8s-role\'] == \'master\':\n            self.jobname = ""add-master-node""\n        else:\n            self.jobname = ""error""\n            self.logger.error(""[{0}] Error: {1} is an undefined role, quit add job in host [{2}]"".format(time.asctime(), node_config[\'k8s-role\'], node_config[\'nodename\']))\n            sys.exit(1)\n\n\n\n    def prepare_package(self, node_config, job_name):\n\n        common.maintain_package_wrapper(self.cluster_object_model, self.maintain_config, node_config, job_name)\n\n\n\n    def delete_packege(self, node_config):\n\n        common.maintain_package_cleaner(node_config)\n\n\n\n    def job_executer_add_work_node(self):\n\n        self.logger.info(""{0} job begins !"".format(self.jobname))\n\n        # sftp your script to remote host with paramiko.\n        srcipt_package = ""{0}.tar"".format(self.jobname)\n        src_local = ""parcel-center/{0}"".format(self.node_config[""nodename""])\n        dst_remote = common.get_user_dir(self.node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, srcipt_package, self.node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(self.jobname, self.node_config[\'hostip\'])\n        if common.ssh_shell_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to uncompress {0}.tar"".format(self.jobname))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/hosts-check.sh {1}"".format(self.jobname, self.node_config[\'hostip\'])\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to update the /etc/hosts on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/docker-ce-install.sh {0}"".format(self.jobname)\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to install docker-ce on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/kubelet-start.sh {0}"".format(self.jobname)\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to bootstrap kubelet on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        self.logger.info(""Successfully running {0} job on node {1}"".format(self.jobname, self.node_config[""nodename""]))\n\n\n\n    def remote_host_cleaner(self, node_config, jobname):\n\n        commandline = ""sudo rm -rf {0}*"".format(jobname)\n\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            sys.exit(1)\n\n\n\n    def run_add_work_node(self):\n\n        self.logger.info(""---- package wrapper is working now! ----"")\n        self.prepare_package(self.node_config, ""add-worker-node"")\n        self.logger.info(""---- package wrapper\'s work finished ----"")\n\n        self.job_executer_add_work_node()\n\n        if self.clean_flag == True:\n            self.logger.info(""---- package cleaner is working now! ----"")\n            self.delete_packege(self.node_config)\n            self.logger.info(""---- package cleaner\'s work finished! ----"")\n\n            self.logger.info(""---- remote host cleaner is working now! ----"")\n            self.remote_host_cleaner(self.node_config, self.jobname)\n            self.logger.info(""---- remote host cleaning job finished! "")\n\n\n\n    def job_executer_add_node_to_etcd_cluster(self):\n        com = self.cluster_object_model\n        self.logger.info(""Find a alive etcd node in the cluster"")\n\n        # Directly find the leader node.\n        good_node_config = pai_common.get_etcd_leader_node(com)\n        if good_node_config is None:\n            self.logger.error(""Unable to find the etcd leader node."")\n            sys.exit(1)\n\n        self.logger.info(""------------ package wrapper is working now ! -------------------- "")\n        self.prepare_package(good_node_config, ""add-master-node-task-one"")\n        self.logger.info(""------------ package wrapper\'s work finished ! ------------------- "")\n\n        script_package = ""add-master-node-task-one.tar""\n        src_local = ""parcel-center/{0}"".format(good_node_config[""nodename""])\n        dst_remote = common.get_user_dir(good_node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, good_node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}"".format(script_package)\n        if common.ssh_shell_with_password_input_paramiko(good_node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully extract the script package for add-master-node-task-one!"")\n\n        commandline = ""sudo /bin/bash {0}/{1}.sh {2} {3}"".format(""add-master-node-task-one"",\n                                                                 ""add-member-to-etcd-cluster"",\n                                                                 self.node_config[\'hostip\'],\n                                                                 self.node_config[\'etcdid\'])\n        if common.ssh_shell_with_password_input_paramiko(good_node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully add the new master into the etcd cluster."")\n\n        if self.clean_flag:\n            self.delete_packege(good_node_config)\n            self.remote_host_cleaner(good_node_config, ""add-master-node-task-one"")\n\n\n\n    def job_executer_starting_new_master_node(self):\n        com = self.cluster_object_model\n        new_etcd_cluster_ips_peer = pai_common.get_new_etcd_peer_ip_list(com, self.node_config)\n        com[\'kubernetes\'][\'etcd_cluster_ips_peer\'] = new_etcd_cluster_ips_peer\n        com[\'kubernetes\'][\'etcd-initial-cluster-state\'] = \'existing\'\n\n        self.logger.info(""---- package wrapper is working now! ----"")\n        self.prepare_package(self.node_config, ""add-master-node-task-two"")\n        self.logger.info(""---- package wrapper\'s work finished ----"")\n\n        script_package = ""add-master-node-task-two.tar""\n        src_local = ""parcel-center/{0}"".format(self.node_config[""nodename""])\n        dst_remote = common.get_user_dir(self.node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, self.node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}"".format(script_package)\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully extract the script package for add-master-node-task-two!"")\n\n        commandline = ""sudo ./{0}/hosts-check.sh {1}"".format(""add-master-node-task-two"", self.node_config[\'hostip\'])\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to update the /etc/hosts on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/docker-ce-install.sh {0}"".format(""add-master-node-task-two"")\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to install docker-ce on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/kubelet-start.sh {0}"".format(""add-master-node-task-two"")\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to bootstrap kubelet on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        self.logger.info(""Successfully running {0} job on node {1}!"".format(""add-master-node-task-two"", self.node_config[\'hostip\']))\n\n        if self.clean_flag:\n            self.delete_packege(self.node_config)\n            self.remote_host_cleaner(self.node_config, ""add-master-node-task-two"")\n\n\n\n    def run_add_master_node(self):\n\n        self.logger.info(""Begin to add master node into kubernetes cluster."")\n        self.logger.info(""TASK 1: ---- Begin to add the target node to the etcd cluster ---- "")\n        self.job_executer_add_node_to_etcd_cluster()\n\n        self.logger.info(""TASK 2: ---- Begin to new master node into your k8s cluster ------ "")\n        self.job_executer_starting_new_master_node()\n\n\n\n    def run(self):\n\n        if self.jobname == ""add-worker-node"":\n            self.run_add_work_node()\n\n        if self.jobname == ""add-master-node"":\n            self.run_add_master_node()'"
deployment/k8sPaiLibrary/maintainlib/clean.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport common\nimport logging\nimport logging.config\n\n\n\npackage_directory_clean = os.path.dirname(os.path.abspath(__file__))\n\n\nclass clean:\n\n    """"""\n\n    An class to destroy whole cluster.\n\n    """"""\n\n    def __init__(self, cluster_object_model, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = cluster_object_model\n        maintain_configuration_path = os.path.join(package_directory_clean, ""../maintainconf/clean.yaml"")\n        self.maintain_config = common.load_yaml_file(maintain_configuration_path)\n        self.clean_flag = kwargs[""clean""]\n        self.force_flag = kwargs[""force""]\n        self.jobname = ""clean""\n\n\n\n    def prepare_package(self, node_config):\n\n        common.maintain_package_wrapper(self.cluster_object_model, self.maintain_config, node_config, self.jobname)\n\n\n\n    def delete_packege(self, node_config):\n\n        common.maintain_package_cleaner(node_config)\n\n\n\n    def job_executer(self, node_config):\n\n        self.logger.info(""{0} job begins !"".format(self.jobname))\n\n        # sftp your script to remote host with paramiko.\n        srcipt_package = ""{0}.tar"".format(self.jobname)\n        src_local = ""parcel-center/{0}"".format(node_config[""nodename""])\n        dst_remote = common.get_user_dir(node_config)\n        if common.sftp_paramiko(src_local, dst_remote, srcipt_package, node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(self.jobname, node_config[\'hostip\'])\n        if common.ssh_shell_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to uncompress {0}.tar"".format(self.jobname))\n            sys.exit(1)\n\n        commandline = ""sudo /bin/bash {0}/kubernetes-cleanup.sh"".format(self.jobname)\n        if self.force_flag:\n            commandline += "" -f""\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to cleanup the kubernetes deployment on {0}"".format(node_config[\'hostip\']))\n            sys.exit(1)\n\n        self.logger.info(""Successfully running {0} job on node {1}"".format(self.jobname, node_config[""nodename""]))\n\n\n\n    def remote_host_cleaner(self, node_config):\n\n        commandline = ""sudo rm -rf {0}*"".format(self.jobname)\n\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            sys.exit(1)\n\n\n\n    def run(self):\n\n        com = self.cluster_object_model\n\n        self.logger.warning(""Begin to destroy whole cluster."")\n        self.logger.warning(""After destroying, all kubenretes\'s metadata will be deleted and etcd will be cleaned too."")\n        for role in [""proxy"", ""master"", ""worker""]:\n            if ""{0}-list"".format(role) not in com[""kubernetes""]:\n                continue\n            for hostname in com[""kubernetes""][""{0}-list"".format(role)]:\n                node_config = com[""layout""][""machine-list""][hostname]\n                self.logger.info(""Begin to clean data on host {0}"".format(node_config[""hostip""]))\n                self.prepare_package(node_config)\n                self.job_executer(node_config)\n\n                if self.clean_flag == True:\n                    self.logger.info("" package cleaner is working on the folder of {0}!"".format(node_config[""hostip""]))\n                    self.delete_packege(node_config)\n                    self.logger.info("" package cleaner\'s work finished! "")\n\n                    self.logger.info("" remote host cleaner is working on the host of {0}!"".format(node_config[""hostip""]))\n                    self.remote_host_cleaner(node_config)\n                    self.logger.info("" remote host cleaning job finished! "")\n\n        self.logger.info(""The kubernetes has been destroyed, and metadata has been removed"")\n'"
deployment/k8sPaiLibrary/maintainlib/common.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\n#\nimport yaml\nimport os\nimport errno\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport paramiko\nimport tarfile\nimport socket\nimport logging\nimport time\nimport etcd\nimport logging.config\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_yaml_file(path):\n\n    with open(path, ""r"") as f:\n        file_data = yaml.load(f, yaml.SafeLoader)\n\n    return file_data\n\n\n\ndef execute_shell(shell_cmd, error_msg):\n\n    try:\n        subprocess.check_call( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.error(error_msg)\n        sys.exit(1)\n\n\n\ndef execute_shell_retry(shell_cmd, error_msg, retry_count):\n\n    count = 0\n    while count < retry_count:\n        try:\n            subprocess.check_call( shell_cmd, shell=True )\n            break\n        except subprocess.CalledProcessError:\n            count += 1\n            logger.error(error_msg)\n            logger.info(""run command \\"" %s \\"" exception, retrying %d"", shell_cmd, count)\n            if count == retry_count:\n                sys.exit(1)\n            time.sleep(5)\n\n\n\ndef execute_shell_return(shell_cmd, error_msg):\n\n    try:\n        subprocess.check_call( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.warning(error_msg)\n        return False\n\n    return True\n\n\n\n\ndef read_template(template_path):\n\n    with open(template_path, ""r"") as fin:\n        template_data = fin.read().decode(\'utf-8\')\n\n    return template_data\n\n\n\ndef generate_from_template(template_data, cluster_object_model, host_config):\n\n    generated_file = jinja2.Template(template_data).render(\n        {\n            ""hostcofig"": host_config,\n            ""cluster_cfg"": cluster_object_model\n        }\n    )\n\n    return generated_file\n\n\n\ndef generate_from_template_dict(template_data, map_table):\n\n    generated_file = jinja2.Template(template_data).render(\n        map_table\n    )\n\n    return generated_file\n\n\n\ndef write_generated_file(generated_file, file_path):\n\n    with open(file_path, ""w+"") as fout:\n        fout.write(generated_file)\n\n\n\ndef ipv4_address_validation(ipv4_addr):\n\n    try:\n        socket.inet_aton(ipv4_addr)\n        ret = True\n    except socket.error:\n        ret = False\n        logger.error(""{0} is not a correct ipv4 address!"".format(ipv4_addr))\n\n    return ret\n\n\n\ndef cidr_validation(cidr):\n    str_list = cidr.split(""/"")\n\n    if len(str_list) != 2:\n        logger.error(""{0} is not a correct CIDR."".format(cidr))\n        return False\n\n    if ipv4_address_validation(str_list[0]) is not True:\n        return False\n\n    if int(str_list[1]) > 32 or int(str_list[1]) < 0:\n        logger.error(""{0} is not a correct CIDR."".format(cidr))\n        return False\n\n    return True\n\n\n\ndef port_validation(port):\n\n    if str(port).isdigit() == True and int(port) >= 0 and int(port) <= 65535 :\n\n        ret = True\n\n    else:\n\n        ret = False\n        logger.error(""{0} is not a correct port. A port can only contain digits!"".format(str(port)))\n\n    return ret\n\n\n\ndef sftp_paramiko(src, dst, filename, host_config):\n    hostip = str(host_config[\'hostip\'])\n    if ipv4_address_validation(hostip) == False:\n        return False\n    if \'password\' not in host_config and \'keyfile-path\' not in host_config:\n        logger.error(""At least, you should config a password or ssh key file path for a node."")\n        logger.error(""Both password and ssh key file path are missing in the node [ {0} ]."".format(host_config[\'hostip\']))\n        return False\n\n    username = str(host_config[\'username\'])\n    password = None\n    if \'password\' in host_config:\n        password = str(host_config[\'password\'])\n    port = 22\n    if \'sshport\' in host_config:\n        if port_validation(host_config[\'sshport\']) == False:\n            return False\n        port = int(host_config[\'sshport\'])\n    key_filename = None\n    if \'keyfile-path\' in host_config:\n        if os.path.isfile(str(host_config[\'keyfile-path\'])) and host_config[\'keyfile-path\'] is not None:\n            key_filename = str(host_config[\'keyfile-path\'])\n        else:\n            logger.warn(""The key file: {0} specified doesn\'t exist"".format(host_config[\'keyfile-path\']))\n    # First make sure the folder exist.\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(hostname=hostip, port=port, key_filename=key_filename, username=username, password=password, allow_agent=True)\n\n    password = password if password is not None else \'\'\n    stdin, stdout, stderr = ssh.exec_command(""echo \'{0}\' | sudo -S mkdir -p {1}"".format(password, dst), get_pty=True)\n    for response_msg in stdout:\n        print(response_msg.encode(\'utf-8\').strip(\'\\n\'))\n\n    # Put the file to target Path.\n    sftp = ssh.open_sftp()\n    sftp.put(\'{0}/{1}\'.format(src, filename), \'{0}/{1}\'.format(dst, filename))\n    sftp.close()\n\n    ssh.close()\n\n    return True\n\n\n# Support command with sudo? : No\n# Could you get the command result as the return value? : No\ndef ssh_shell_paramiko(host_config, commandline):\n    result_stdout, result_stderr = ssh_shell_paramiko_with_result(host_config, commandline)\n    if result_stdout is None or result_stderr is None:\n        return False\n    return True\n\n\n# Support command with sudo? : No\n# Could you get the command result as the return value? : Yes\ndef ssh_shell_paramiko_with_result(host_config, commandline):\n    hostip = str(host_config[\'hostip\'])\n    if ipv4_address_validation(hostip) == False:\n        return False\n    if \'password\' not in host_config and \'keyfile-path\' not in host_config:\n        logger.error(""At least, you should config a password or ssh key file path for a node."")\n        logger.error(""Both password and ssh key file path are missing in the node [ {0} ]."".format(host_config[\'hostip\']))\n        return False\n\n    username = str(host_config[\'username\'])\n    password = None\n    if \'password\' in host_config:\n        password = str(host_config[\'password\'])\n    port = 22\n    if \'sshport\' in host_config:\n        if port_validation(host_config[\'sshport\']) == False:\n            return (None, None)\n        port = int(host_config[\'sshport\'])\n    key_filename = None\n    if \'keyfile-path\' in host_config and host_config[\'keyfile-path\'] is not None:\n        if os.path.isfile(str(host_config[\'keyfile-path\'])):\n            key_filename = str(host_config[\'keyfile-path\'])\n        else:\n            logger.warn(""The key file: {0} specified doesn\'t exist"".format(host_config[\'keyfile-path\']))\n    logger.info(""Start executing the command on host [{0}]: {1}"".format(hostip, commandline))\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(hostname=hostip, port=port, key_filename=key_filename, username=username, password=password)\n    stdin, stdout, stderr = ssh.exec_command(commandline, get_pty=True)\n    logger.info(""Finished executing the command on host [{0}]: {1}"".format(hostip, commandline))\n    result_stdout = """"\n    for response_msg in stdout:\n        result_stdout += response_msg\n        print(response_msg.encode(\'utf-8\').strip(\'\\n\'))\n    result_stderr = """"\n    for response_msg in stderr:\n        result_stderr += response_msg\n\n    exit_code_ssh = stdout.channel.recv_exit_status()\n    if exit_code_ssh != 0:\n        sys.exit(exit_code_ssh)\n\n    ssh.close()\n    return (result_stdout, result_stderr)\n\n\n# Support command with sudo? : Yes\n# Could you get the command result as the return value? : No\ndef ssh_shell_with_password_input_paramiko(host_config, commandline):\n\n    hostip = str(host_config[\'hostip\'])\n    if ipv4_address_validation(hostip) == False:\n        return False\n    if \'password\' not in host_config and \'keyfile-path\' not in host_config:\n        logger.error(""At least, you should config a password or ssh key file path for a node."")\n        logger.error(""Both password and ssh key file path are missing in the node [ {0} ]."".format(host_config[\'hostip\']))\n        return False\n\n    username = str(host_config[\'username\'])\n    password = None\n    if \'password\' in host_config:\n        password = str(host_config[\'password\'])\n    port = 22\n    if \'sshport\' in host_config:\n        if port_validation(host_config[\'sshport\']) == False:\n            return False\n        port = int(host_config[\'sshport\'])\n    key_filename = None\n    if \'keyfile-path\' in host_config:\n        if os.path.isfile(str(host_config[\'keyfile-path\'])) and host_config[\'keyfile-path\'] is not None:\n            key_filename = str(host_config[\'keyfile-path\'])\n        else:\n            logger.warn(""The key file: {0} specified doesn\'t exist"".format(host_config[\'keyfile-path\']))\n\n    ssh = paramiko.SSHClient()\n    ssh.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n    ssh.connect(hostname=hostip, port=port, key_filename=key_filename, username=username, password=password)\n    password = password if password is not None else \'\'\n    if (commandline.strip().startswith(\'sudo\')):\n        commandline = commandline.replace(\'sudo\', \'sudo -S\', 1)\n    stdin, stdout, stderr = ssh.exec_command(""echo \'{0}\' | {1}"".format(password, commandline), get_pty=True)\n    logger.info(""Executing the command on host [{0}]: {1}"".format(hostip, commandline))\n    for response_msg in stdout:\n        print (response_msg.encode(\'utf-8\').strip(\'\\n\'))\n\n    exit_code_ssh = stdout.channel.recv_exit_status()\n    if exit_code_ssh != 0:\n        sys.exit(exit_code_ssh)\n\n    ssh.close()\n    return True\n\n\n\ndef get_user_dir(host_config):\n\n    cmd = ""getent passwd {0} | cut -d: -f6"".format(str(host_config[\'username\']))\n    result_stdout, result_stderr = ssh_shell_paramiko_with_result(host_config, cmd)\n    if result_stdout != None:\n        ret = result_stdout.encode(\'unicode-escape\').decode(\'string_escape\')\n        ret = ret.replace(\'\\n\', \'\')\n        ret = ret.replace(\'\\r\', \'\')\n        return ret\n\n    if str(host_config[\'username\']) == ""root"":\n        return ""/root""\n    else:\n        return ""/home/{0}"".format(host_config[""username""])\n\n\n\ndef create_path(path):\n\n    if not os.path.exists(""{0}"".format(path)):\n\n        try:\n            os.makedirs(path)\n        except OSError as exc:\n            if exc.errno == errno.EEXIST and os.path.isdir(path):\n                pass\n            else:\n                raise\n\n\n\ndef archive_tar(target, path):\n\n    tar = tarfile.open(target, ""w"")\n\n    for root,dir,files in os.walk(path):\n        for file in files:\n            fullpath = os.path.join(root, file)\n            tar.add(fullpath)\n\n    tar.close()\n\n\n\ndef maintain_package_wrapper(cluster_object_model, maintain_config, node_config, jobname):\n\n    create_path(""parcel-center/{0}/{1}"".format(node_config[\'nodename\'], jobname))\n\n    if ""template-list"" in maintain_config[jobname]:\n        for template_info in maintain_config[jobname][""template-list""]:\n\n            name = template_info[\'name\']\n            src = template_info[\'src\']\n            dst = template_info[\'dst\']\n\n            template_data = read_template(""{0}"".format(src))\n            template_file = generate_from_template(template_data, cluster_object_model, node_config)\n            create_path(""parcel-center/{0}/{1}"".format(node_config[\'nodename\'], dst))\n            write_generated_file(template_file, ""parcel-center/{0}/{1}/{2}"".format(node_config[\'nodename\'], dst, name))\n\n    if ""file-list"" in maintain_config[jobname]:\n        for file_info in maintain_config[jobname][""file-list""]:\n\n            name = file_info[\'name\']\n            src = file_info[\'src\']\n            dst = file_info[\'dst\']\n            create_path(""parcel-center/{0}/{1}"".format(node_config[\'nodename\'], dst))\n            execute_shell(\n                ""cp {0} parcel-center/{1}/{2}/{3}"".format(src, node_config[\'nodename\'], dst, name),\n                ""Failed copy {0} parcel-center/{1}/{2}/{3}"".format(src, node_config[\'nodename\'], dst, name)\n            )\n\n    execute_shell(""cp -r parcel-center/{0}/{1} ."".format(node_config[\'nodename\'], jobname), ""Failed cp job folder"")\n    archive_tar(""parcel-center/{0}/{1}.tar"".format(node_config[\'nodename\'], jobname), jobname)\n    execute_shell(""rm -rf {0}"".format(jobname), ""Failed to remove {0}"".format(jobname))\n\n\n\ndef maintain_package_cleaner(node_config):\n\n    execute_shell(\n        ""rm -rf parcel-center/{0}"".format(node_config[\'nodename\']),\n        ""Failed to remove parcel-center/{0}"".format(node_config[\'nodename\'])\n    )\n\n\n\ndef get_etcd_leader_node(cluster_cfg):\n    com = cluster_cfg\n    # Get leader node.\n    host_list = list()\n\n    for host in com[\'kubernetes\'][\'master-list\']:\n        host_list.append((com[\'layout\'][\'machine-list\'][host][\'hostip\'], 4001))\n\n    client = etcd.Client(host=tuple(host_list), allow_reconnect=True)\n\n    etcdid = client.leader[\'name\']\n    for host in com[\'kubernetes\'][\'master-list\']:\n        if etcdid == com[\'layout\'][\'machine-list\'][host][\'etcdid\']:\n            logger.debug(""Current leader of etcd-cluster: {0}"".format(com[\'layout\'][\'machine-list\'][host]))\n            return com[\'layout\'][\'machine-list\'][host]\n\n    logger.error(""Can\'t find the leader of etcd."")\n    return None\n\n\n\ndef get_new_etcd_peer_ip_list(cluster_cfg, new_node_config):\n    com = cluster_cfg\n    etcd_cluster_ips_peer = """"\n    separated = """"\n\n    host_list = list()\n    for host in com[\'kubernetes\'][\'master-list\']:\n        host_list.append((com[\'layout\'][\'machine-list\'][host][\'hostip\'], 4001))\n\n    client = etcd.Client(host=tuple(host_list), allow_reconnect=True)\n\n    member_dict = client.members\n    for member_hash in member_dict:\n\n        etcd_id = member_dict[member_hash][\'name\']\n        peer_url = member_dict[member_hash][\'peerURLs\'][0]\n\n        if etcd_id == """":\n            # new member before announcing, etcdid will be empty.\n            continue\n\n        ip_peer = ""{0}={1}"".format(etcd_id, peer_url)\n        etcd_cluster_ips_peer = etcd_cluster_ips_peer + separated + ip_peer\n        separated = "",""\n\n\n    if new_node_config != None:\n\n        new_etcd_id = new_node_config[\'etcdid\']\n        peer_url = new_node_config[\'hostip\']\n        ip_peer = ""{0}=http://{1}:2380"".format(new_etcd_id, peer_url)\n        etcd_cluster_ips_peer = etcd_cluster_ips_peer + separated + ip_peer\n\n        logger.debug(""New etcd-initial-cluster: {0}"".format(etcd_cluster_ips_peer))\n\n    return etcd_cluster_ips_peer\n\n\n\ndef get_etcd_peer_ip_list(cluster_config):\n\n    return get_new_etcd_peer_ip_list(cluster_config)\n'"
deployment/k8sPaiLibrary/maintainlib/deploy.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport paramiko\nimport common\nimport kubectl_install\nimport logging\nimport logging.config\n\nfrom ...paiLibrary.common import kubernetes_handler\n\n\npackage_directory_deploy = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass deploy:\n\n    """"""\n\n       A class to deploy a new cluster.\n\n    """"""\n\n    def __init__(self, cluster_object_model, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = cluster_object_model\n        maintain_configuration_path = os.path.join(package_directory_deploy, ""../maintainconf/deploy.yaml"")\n        self.maintain_config = common.load_yaml_file(maintain_configuration_path)\n        self.clean_flag = kwargs[""clean""]\n\n\n\n    def prepare_package(self, node_config, job_name):\n\n        common.maintain_package_wrapper(self.cluster_object_model, self.maintain_config, node_config, job_name)\n\n\n\n    def delete_packege(self, node_config):\n\n        common.maintain_package_cleaner(node_config)\n\n\n\n    def remote_host_cleaner(self, node_config, job_name):\n\n        commandline = ""sudo rm -rf {0}*"".format(job_name)\n\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            sys.exit(1)\n\n\n\n    def job_executer(self, node_config, job_name):\n\n        # sftp your script to remote host with paramiko.\n        srcipt_package = ""{0}.tar"".format(job_name)\n        src_local = ""parcel-center/{0}"".format(node_config[""nodename""])\n        dst_remote = common.get_user_dir(node_config)\n        if common.sftp_paramiko(src_local, dst_remote, srcipt_package, node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(job_name, node_config[\'hostip\'])\n        if common.ssh_shell_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to uncompress {0}.tar"".format(job_name))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/hosts-check.sh {1}"".format(job_name, node_config[\'hostip\'])\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to update the /etc/hosts on {0}"".format(node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/docker-ce-install.sh {0}"".format(job_name)\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to install docker-ce on {0}"".format(node_config[\'hostip\']))\n            sys.exit(1)\n\n        commandline = ""sudo ./{0}/kubelet-start.sh {0}"".format(job_name)\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            self.logger.error(""Failed to bootstrap kubelet on {0}"".format(node_config[\'hostip\']))\n            sys.exit(1)\n\n        self.logger.info(""Successfully running {0} job on node {1}!"".format(job_name, node_config[\'hostip\']))\n\n\n\n    def update_node_config(self):\n        com = self.cluster_object_model\n        node_config_from_cluster_conf = dict()\n\n        for role in [""proxy"", ""master"", ""worker""]:\n            if ""{0}-list"".format(role) not in com[""kubernetes""]:\n                continue\n\n            for hostname in com[""kubernetes""][""{0}-list"".format(role)]:\n                node_config = com[""layout""][""machine-list""][hostname]\n                node_config_from_cluster_conf[hostname] = node_config\n\n        kube_config_path = os.path.expanduser(""~/.kube/config"")\n        yaml_data = yaml.dump(node_config_from_cluster_conf, default_flow_style=False)\n        pai_node_list = {""node-list"": yaml_data}\n        kubernetes_handler.update_configmap(kube_config_path, ""pai-node-config"", pai_node_list, ""kube-system"")\n\n\n\n    def create_kube_proxy(self):\n        com = self.cluster_object_model\n        self.logger.info(""Create kube-proxy daemon for kuberentes cluster."")\n\n        file_path = ""deployment/k8sPaiLibrary/template/kube-proxy.yaml.template""\n        template_data = common.read_template(file_path)\n        dict_map = {\n            ""cluster_cfg"": com\n        }\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n\n        common.write_generated_file(generated_data, ""kube-proxy.yaml"")\n\n        retry_count = 5\n        common.execute_shell_retry(\n            ""kubectl apply --overwrite=true -f kube-proxy.yaml"",\n            ""Failed to create kube-proxy"",\n            retry_count\n        )\n\n        os.remove(""kube-proxy.yaml"")\n\n\n\n    def create_k8s_dashboard(self):\n        com = self.cluster_object_model\n        self.logger.info(""Create kubernetes dashboard deployment for kuberentes cluster."")\n\n        self.logger.info(""Create dashboard service."")\n        file_path = ""deployment/k8sPaiLibrary/template/dashboard-service.yaml.template""\n        template_data = common.read_template(file_path)\n        dict_map = {\n            ""cluster_cfg"": com\n        }\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n\n        common.write_generated_file(generated_data, ""dashboard-service.yaml"")\n\n        retry_count = 5\n        common.execute_shell_retry(\n            ""kubectl apply --overwrite=true -f dashboard-service.yaml"",\n            ""Failed to create dashboard-service"",\n            retry_count\n        )\n\n        os.remove(""dashboard-service.yaml"")\n\n        self.logger.info(""Create dashboard deployment."")\n        file_path = ""deployment/k8sPaiLibrary/template/dashboard-deployment.yaml.template""\n        template_data = common.read_template(file_path)\n        dict_map = {\n            ""cluster_cfg"": com\n        }\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n\n        common.write_generated_file(generated_data, ""dashboard-deployment.yaml"")\n        common.execute_shell(\n            ""kubectl apply --overwrite=true -f dashboard-deployment.yaml"",\n            ""Failed to create dashboard-deployment""\n        )\n\n        os.remove(""dashboard-deployment.yaml"")\n\n\n\n    def run(self):\n        com = self.cluster_object_model\n\n        self.logger.warning(""Begin to deploy a new cluster to your machine or vm."")\n        for role in [""proxy"", ""master"", ""worker""]:\n            if ""{0}-list"".format(role) not in com[""kubernetes""]:\n                continue\n\n            for hostname in com[""kubernetes""][""{0}-list"".format(role)]:\n                node_config = com[""layout""][""machine-list""][hostname]\n                self.logger.info(""Begin to deploy k8s on host {0}, the node role is [ {1} ]"".format(node_config[""hostip""], role))\n                self.prepare_package(node_config, ""{0}-deployment"".format(role))\n                self.job_executer(node_config, ""{0}-deployment"".format(role))\n\n                if self.clean_flag == True:\n                    self.logger.info("" package cleaner is working on the folder of {0}!"".format(node_config[""hostip""]))\n                    self.delete_packege(node_config)\n                    self.logger.info("" package cleaner\'s work finished! "")\n\n                    self.logger.info("" remote host cleaner is working on the host of {0}!"".format(node_config[""hostip""]))\n                    self.remote_host_cleaner(node_config, ""{0}-deployment"".format(role))\n                    self.logger.info("" remote host cleaning job finished! "")\n\n        kubectl_install_instance = kubectl_install.kubectl_install(com)\n        kubectl_install_instance.run()\n\n        # check the registered api resources\n        common.execute_shell_retry(""kubectl api-resources"", ""kubectl command failed!"", 5)\n\n        # create kube-proxy until daemonset resource is registered\n        common.execute_shell_retry(""kubectl api-resources | grep -q daemonsets"", ""Controller manager hasn\'t create daemonset object!"", 5)\n\n        self.create_kube_proxy()\n        self.create_k8s_dashboard()\n\n        self.logger.info(""Update node configuration into configmap in the namespace [ kube-system ] as the name [ pai-node-config ] "")\n        self.update_node_config()\n\n        self.logger.info(""The kubernetes deployment is finished!"")\n\n'"
deployment/k8sPaiLibrary/maintainlib/etcdfix.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport paramiko\nimport etcd\nimport common\nimport logging\nimport logging.config\n\n\n\npackage_directory_etcdfix = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass etcdfix_conf_validation:\n\n    """"""\n    A class to validation the cluster configuration.\n    """"""\n\n    def __init__(self, cluster_config, node_config):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_config = cluster_config\n        self.node_config = node_config\n\n\n\n    def node_conf_validation(self, node_cfg = None):\n\n        if node_cfg is None:\n            node_cfg = self.node_config\n\n        if \'nodename\' not in node_cfg:\n\n            self.logger.error(""nodename not in your node configuration."")\n\n            return False\n\n        if \'hostip\' not in node_cfg:\n\n            self.logger.error(""hostip not in your node configuration."")\n\n            return False\n\n        if common.ipv4_address_validation(node_cfg[\'hostip\']) == False:\n\n            self.logger.error(""The hostip in configuration is invalid."")\n\n            return False\n\n        if \'sshport\' in node_cfg and common.port_validation(node_cfg[\'sshport\']) == False:\n\n            self.logger.error(""The sshport in configuration is in valid."")\n\n            return False\n\n        if \'username\' not in node_cfg:\n\n            self.logger.error(""username not in your node configuration."")\n\n            return False\n\n        if \'password\' not in node_cfg:\n\n            self.logger.error(""password not in your node configuration."")\n\n            return False\n\n        if \'etcdid\' not in node_cfg:\n\n            self.logger.error(""etcdid not in your node configuration."")\n\n            return False\n\n        return True\n\n\n\n    def cluster_conf_validation(self):\n\n        com = self.cluster_config\n\n        if \'master-list\' not in com[""kubernetes""]:\n            self.logger.error(""mastermachinelist not in your cluster configuration."")\n            return False\n\n        ret = False\n\n        for host in com[""kubernetes""][""master-list""]:\n\n            hostObject = com[""layout""][""machine-list""][host]\n\n            if self.node_conf_validation(hostObject) == False:\n\n                return False\n\n            if str(hostObject[\'nodename\']) == str(self.node_config[\'nodename\']):\n\n                if str(hostObject[\'hostip\']) != str(self.node_config[\'hostip\']):\n                    self.logger.error(\n                        ""Hostip of the bad node in cluster configuration is inconsistent with node configuration"")\n                    return False\n\n                if str(hostObject[\'username\']) != str(self.node_config[\'username\']):\n                    self.logger.error(\n                        ""username of the bad node in cluster configuration is inconsistent with node configuration"")\n                    return False\n\n                if str(hostObject[\'password\']) != str(self.node_config[\'password\']):\n                    self.logger.error(\n                        ""password of the bad node in cluster configuration is inconsistent with node configuration"")\n                    return False\n\n                if \'sshport\' not in hostObject:\n                    hostObject[\'sshport\'] = 22\n\n                if \'sshport\' not in self.node_config:\n                    self.node_config[\'sshport\'] = 22\n\n                if str(hostObject[\'sshport\']) != str(self.node_config[\'sshport\']):\n                    self.logger.error(\n                        ""sshport of the bad node in cluster configuration is inconsistent with node configuration"")\n                    return False\n\n                if str(hostObject[\'etcdid\']) != str(self.node_config[\'etcdid\']):\n                    self.logger.error(\n                        ""etcdid of the bad node in cluster configuration is inconsistent with node configuration"")\n                    return False\n\n                ret = True\n\n\n        if ret == False:\n\n            self.logger.error(""Bad node not in your cluster configuration."")\n\n        return ret\n\n\n\n    def validation(self):\n\n        return self.node_conf_validation() and self.cluster_conf_validation()\n\n\n\nclass etcdfix:\n\n    """"""\n    A class to reconfiguration etcd. Fix the issue when etcd\'s data is corrupted\n    """"""\n\n    def __init__(self, cluster_object_model, node_config, clean):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.logger.info(""Initialize class etcdfix to fix the broken etcd member on {0}"".format(node_config[""nodename""]))\n        self.logger.debug(""Node-configuration: {0}"".format(str(node_config)))\n\n        self.cluster_object_model = cluster_object_model\n        self.bad_node_config = node_config\n        maintain_configuration_path = os.path.join(package_directory_etcdfix, ""../maintainconf/etcdfix.yaml"")\n        self.maintain_config = common.load_yaml_file(maintain_configuration_path)\n        self.clean_flag = clean\n\n\n\n    def prepare_package(self, node_config, jobname):\n\n        self.logger.debug(""Prepare package for {0} on {1}"".format(jobname, node_config[\'nodename\']))\n        self.logger.debug(""The job configuration: {0}"".format(self.maintain_config[jobname]))\n\n        common.maintain_package_wrapper(self.cluster_object_model, self.maintain_config, node_config, jobname)\n\n\n\n    def delete_packege(self, node_config):\n\n        self.logger.debug(""Cleanup all package of {0} on the package on directory "".format(node_config[\'nodename\']))\n\n        common.maintain_package_cleaner(node_config)\n\n\n\n    def stop_bad_etcd_server(self, bad_node_config):\n\n        self.prepare_package(bad_node_config, ""etcd-reconfiguration-stop"")\n\n        self.logger.info(""Begin to execute the job : etcd-reconfiguration-stop."")\n        self.logger.info(""Stop the bad etcd server on host [{0}]"".format(bad_node_config[\'nodename\']))\n\n        script_package = ""etcd-reconfiguration-stop.tar""\n        src_local = ""parcel-center/{0}"".format(bad_node_config[""nodename""])\n        dst_remote = common.get_user_dir(bad_node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, bad_node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar && sudo /bin/bash {0}/stop-etcd-server.sh"".format(""etcd-reconfiguration-stop"")\n\n        if common.ssh_shell_with_password_input_paramiko(bad_node_config, commandline) == False:\n            sys.exit(1)\n\n        self.logger.info(""Successfully stoping bad etcd server on node {0}"".format(bad_node_config[""nodename""]))\n\n        if self.clean_flag:\n            self.delete_packege(bad_node_config)\n\n\n\n    def update_etcd_cluster(self, good_node_config, bad_node_config):\n\n        self.prepare_package(good_node_config, ""etcd-reconfiguration-update"")\n\n        self.logger.info(""Begin to execute the job : etcd-reconfiguration-update."")\n        self.logger.info(""Update etcd cluster on host [{0}]."".format(good_node_config[\'nodename\']))\n\n        script_package = ""etcd-reconfiguration-update.tar""\n        src_local = ""parcel-center/{0}"".format(good_node_config[""nodename""])\n        dst_remote = common.get_user_dir(good_node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, good_node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(""etcd-reconfiguration-update"")\n        if common.ssh_shell_with_password_input_paramiko(good_node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully extract the script package for etcd-reconfiguration-update!"")\n\n        commandline = ""sudo /bin/bash {0}/{1}.sh {2} {3}"".format(""etcd-reconfiguration-update"",\n                                                                 ""remove-member-from-etcd-cluster"",\n                                                                 bad_node_config[\'hostip\'],\n                                                                 bad_node_config[\'etcdid\'])\n        if common.ssh_shell_with_password_input_paramiko(good_node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully remove the bad-member from the etcd cluster."")\n\n\n        commandline = ""sudo /bin/bash {0}/{1}.sh {2} {3}"".format(""etcd-reconfiguration-update"",\n                                                                 ""add-member-to-etcd-cluster"",\n                                                                 bad_node_config[\'hostip\'],\n                                                                 bad_node_config[\'etcdid\'])\n        if common.ssh_shell_with_password_input_paramiko(good_node_config, commandline) == False:\n            sys.exit(1)\n        self.logger.info(""Successfully add the bad-member into the etcd cluster again."")\n\n        self.logger.info(""Successfully update etcd cluster configuration on node {0}"".format(bad_node_config[""nodename""]))\n\n        if self.clean_flag:\n            self.delete_packege(good_node_config)\n\n\n\n    def restart_etcd_server(self, bad_node_config):\n\n        com = self.cluster_object_model\n\n        self.logger.info(""Begin to execute the job : etcd-reconfiguration-restart."")\n        self.logger.info(""Restart etcd server on host [{0}]."".format(bad_node_config[\'nodename\']))\n\n        new_etcd_cluster_ips_peer = self.get_etcd_peer_ip_list(bad_node_config)\n\n        com[\'kubernetes\'][\'etcd_cluster_ips_peer\'] = new_etcd_cluster_ips_peer\n        com[\'kubernetes\'][\'etcd-initial-cluster-state\'] = \'existing\'\n\n        self.prepare_package(bad_node_config, ""etcd-reconfiguration-restart"")\n\n        script_package = ""etcd-reconfiguration-restart.tar""\n        src_local = ""parcel-center/{0}"".format(bad_node_config[""nodename""])\n        dst_remote = common.get_user_dir(bad_node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, bad_node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar && sudo ./{0}/{1}.sh"".format(""etcd-reconfiguration-restart"", ""restart-etcd-server"")\n\n        if common.ssh_shell_with_password_input_paramiko(bad_node_config, commandline) == False:\n            sys.exit(1)\n\n        self.logger.info(""Successfully restarting bad etcd server on node {0}"".format(bad_node_config[""nodename""]))\n\n        if self.clean_flag:\n            self.delete_packege(bad_node_config)\n\n\n\n    def get_etcd_leader_node(self):\n        com = self.cluster_object_model\n\n        # Get leader node.\n        host_list = list()\n        for host in com[\'kubernetes\'][\'master-list\']:\n            host_list.append((com[\'layout\'][\'machine-list\'][host][\'hostip\'], 4001))\n        client = etcd.Client(host=tuple(host_list), allow_reconnect=True)\n\n        etcdid = client.leader[\'name\']\n        for host in com[\'kubernetes\'][\'master-list\']:\n            if etcdid == com[\'layout\'][\'machine-list\'][host][\'etcdid\']:\n                self.logger.debug(""Current leader of etcd-cluster: {0}"".format(com[\'layout\'][\'machine-list\'][host]))\n                return com[\'layout\'][\'machine-list\'][host]\n\n        self.logger.error(""Can\'t find the leader of etcd."")\n        return None\n\n\n\n    def get_etcd_peer_ip_list(self, bad_node_config):\n        com = self.cluster_object_model\n\n        etcd_cluster_ips_peer = """"\n        separated = """"\n\n        host_list = list()\n        for host in com[\'kubernetes\'][\'master-list\']:\n            host_list.append((com[\'layout\'][\'machine-list\'][host][\'hostip\'], 4001))\n        client = etcd.Client(host=tuple(host_list), allow_reconnect=True)\n\n        member_dict = client.members\n        for member_hash in member_dict:\n\n            etcd_id = member_dict[member_hash][\'name\']\n            peer_url = member_dict[member_hash][\'peerURLs\'][0]\n\n            if etcd_id == """":\n                # new member before announcing, etcdid will be empty.\n                continue\n\n            ip_peer = ""{0}={1}"".format(etcd_id, peer_url)\n\n            etcd_cluster_ips_peer = etcd_cluster_ips_peer + separated + ip_peer\n\n            separated = "",""\n\n        new_etcd_id = bad_node_config[\'etcdid\']\n        peer_url = bad_node_config[\'hostip\']\n        ip_peer = ""{0}=http://{1}:2380"".format(new_etcd_id, peer_url)\n        etcd_cluster_ips_peer = etcd_cluster_ips_peer + separated + ip_peer\n\n        self.logger.debug(""New etcd-initial-cluster: {0}"".format(etcd_cluster_ips_peer))\n\n        return etcd_cluster_ips_peer\n\n\n\n    def run(self):\n\n        validation = etcdfix_conf_validation(self.cluster_object_model, self.bad_node_config)\n        if validation.validation() == False:\n            sys.exit(1)\n\n        self.logger.info(""Begin to fix etcd-cluster\'s bad member!"")\n\n        bad_node_config = self.bad_node_config\n\n        self.logger.debug(""Bad node information: {0}"".format(str(bad_node_config)))\n\n        self.stop_bad_etcd_server(bad_node_config)\n\n        # Waiting for the bad node to demote from leader.\n        while True:\n            good_node_config = self.get_etcd_leader_node()\n\n            if good_node_config is None:\n                sys.exit(1)\n\n            if good_node_config[\'nodename\'] != self.bad_node_config[\'nodename\']:\n                break\n\n        self.logger.debug(""Good node information: {0}"".format(str(good_node_config)))\n\n        self.update_etcd_cluster(good_node_config, bad_node_config)\n\n        self.restart_etcd_server(bad_node_config)\n'"
deployment/k8sPaiLibrary/maintainlib/k8s_util.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport logging\nimport logging.config\nimport importlib\nfrom . import clean\nfrom . import deploy\nfrom . import common as pai_common\n\n\nlogger = logging.getLogger(__name__)\n\ndef maintain_cluster_k8s(cluster_config, **kwargs):\n\n    if kwargs[""option_name""] == ""deploy"":\n        job_instance = deploy.deploy(cluster_config, **kwargs)\n        job_instance.run()\n    elif kwargs[""option_name""] == ""clean"":\n        job_instance = clean.clean(cluster_config, **kwargs)\n        job_instance.run()\n\n'"
deployment/k8sPaiLibrary/maintainlib/kubectl_conf_check.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport common\nimport logging\nimport logging.config\n\n\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass kubectl_conf_check:\n\n    """"""\n\n       A class to install kubectl on your local dev-box\n\n    """"""\n\n    def __init__(self, cluster_object_model, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n        self.cluster_object_model = cluster_object_model\n        self.kube_conf_path = os.path.expanduser(""~/.kube"")\n\n\n\n    def check(self):\n\n        self.logger.info(""Checking kubectl\'s configuration for paictl."")\n\n        if not os.path.exists(self.kube_conf_path):\n            self.logger.warning(""CHECKING FAILED: The path {0} doesn\'t exist."".format(self.kube_conf_path))\n            return False\n        self.logger.info(""CHECKING PASS: The path {0} exists."".format(self.kube_conf_path))\n\n        if not os.path.isfile(""{0}/config"".format(self.kube_conf_path)):\n            self.logger.warning(""CHECKING FAILED: The configuration file {0}/config doesn\'t exist."".format(self.kube_conf_path))\n            return False\n        self.logger.info(""CHECKING PASS: The configuration file {0}/config exists."".format(self.kube_conf_path))\n\n\n        try:\n            local_kubectl_conf = common.load_yaml_file(""{0}/config"".format(self.kube_conf_path))\n            api_server_address = local_kubectl_conf[\'clusters\'][0][\'cluster\'][\'server\']\n\n            api_server_address_pai_conf = self.cluster_object_model[\'kubernetes\'][\'api-servers-url\']\n\n            if api_server_address != api_server_address_pai_conf:\n                self.logger.warning(""CHECKING FAILED: The api_server_address in local configuration is different from the one in pai\'s configuration."".format(self.kube_conf_path))\n                return False\n\n        except Exception as e:\n\n            self.logger.error(""CHECK FAILED:  Unable to compare api_server_address in the configuration."")\n            return False\n\n        self.logger.info(""Kubectl environment checking task is passed."")\n\n        return True\n\n\n\n\n\n\n\n\n\n'"
deployment/k8sPaiLibrary/maintainlib/kubectl_install.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport time\nimport common\nimport readline\nimport logging\nimport logging.config\n\n\n\npackage_directory_kubeinstall = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass kubectl_install:\n\n    """"""\n\n       A class to install kubectl on your local dev-box\n\n    """"""\n\n    def __init__(self, cluster_config, **kwargs):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_config = cluster_config\n\n\n\n    def kubectl_install(self):\n\n        self.logger.info(""Execute the script to install kubectl on your host!"")\n        commandline = ""./deployment/k8sPaiLibrary/maintaintool/kubectl-install.sh""\n        common.execute_shell(\n            commandline,\n            ""Failed to install kubectl on your dev-box""\n        )\n        self.logger.info(""Successfully install kubectl on the dev-box."")\n\n\n\n    def kubectl_configuration_generate(self):\n        com = self.cluster_config\n        self.logger.info(""Generate the configuation file of kubectl."")\n\n        if com != None:\n            self.logger.info(""Cluster configuration is detected."")\n            self.logger.info(""Generate the KUBECONIFG based on the cluster configuration."")\n            dict_map = {\n                ""cluster_cfg"": com\n            }\n        else:\n            self.logger.warning(""Unable to find the cluster configuration."")\n            self.logger.warning(""Please enter the required information, when prompted."")\n            user_input = raw_input(""Please input the api-server (or the api servers\' load-balancer) address in your cluster: "")\n            dict_map = {\n                ""cluster_cfg"": { ""kubernetes"": {""api-servers-ip"" : user_input} }\n            }\n\n        file_path = ""deployment/k8sPaiLibrary/template/config.template""\n        template_data = common.read_template(file_path)\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n\n        kube_config_path = os.path.expanduser(""~/.kube"")\n        common.write_generated_file(generated_data, ""{0}/config"".format(kube_config_path))\n        self.logger.info(""Successfully configure kubeconfig in the dev-box."")\n\n\n\n\n    def kubectl_ready_test(self):\n\n        times = 0\n\n        while True:\n            res = common.execute_shell_return(""kubectl get node"", ""There will be a delay after installing, please wait."")\n            times = times + 1\n            if res == True:\n                break\n            if times == 30:\n                self.logger.error(""kubectl ready test failed. Exit paictl."")\n                sys.exit(1)\n            self.logger.info(""Wait 5s, and retry it later."")\n            time.sleep(5)\n        self.logger.info(""Successfully install kubectl and configure it!"")\n\n\n\n    def run(self):\n\n        self.kubectl_install()\n        self.kubectl_configuration_generate()\n        self.kubectl_ready_test()\n\n'"
deployment/k8sPaiLibrary/maintainlib/remove.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\nimport paramiko\nimport common\nimport time\nimport logging\nimport logging.config\n\nfrom . import common as pai_common\n\n\n\npackage_directory_remove = os.path.dirname(os.path.abspath(__file__))\n\n\n\nclass remove:\n\n    """"""\n    An class to remove the node from current pai\'s k8s cluster.\n    """"""\n\n    def __init__(self, cluster_object_model, node_config, clean):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = cluster_object_model\n        self.node_config = node_config\n        maintain_configuration_path = os.path.join(package_directory_remove, ""../maintainconf/remove.yaml"")\n        self.maintain_config = common.load_yaml_file(maintain_configuration_path)\n        self.clean_flag = clean\n        self.jobname = ""remove-node""\n\n\n\n    def prepare_package(self, node_config, jobname):\n\n        common.maintain_package_wrapper(self.cluster_object_model, self.maintain_config, node_config, jobname)\n\n\n\n    def delete_packege(self, node_config):\n\n        common.maintain_package_cleaner(node_config)\n\n\n\n    def job_executer_clean_up_node(self):\n\n        self.logger.info(""{0} job begins !"".format(self.jobname))\n\n        commandline = ""kubectl delete node {0}"".format(self.node_config[\'nodename\'])\n        common.execute_shell_return(\n            commandline,\n            ""Failed to delete  node {0}"".format(self.node_config[\'nodename\'])\n        )\n\n        # sftp your script to remote host with paramiko.\n        srcipt_package = ""{0}.tar"".format(self.jobname)\n        src_local = ""parcel-center/{0}"".format(self.node_config[""nodename""])\n        dst_remote = common.get_user_dir(self.node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, srcipt_package, self.node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(self.jobname, self.node_config[\'hostip\'])\n        if common.ssh_shell_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to uncompress {0}.tar"".format(self.jobname))\n            sys.exit(1)\n\n        commandline = ""sudo /bin/bash {0}/kubernetes-cleanup.sh"".format(self.jobname)\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            self.logger.error(""Failed to cleanup the kubernetes deployment on {0}"".format(self.node_config[\'hostip\']))\n            sys.exit(1)\n\n        self.logger.info(""Successfully running {0} job on node {1}"".format(self.jobname, self.node_config[""nodename""]))\n\n\n\n    def remote_host_cleaner(self, node_config, jobname):\n\n        commandline = ""sudo rm -rf {0}*"".format(jobname)\n\n        if common.ssh_shell_with_password_input_paramiko(node_config, commandline) == False:\n            sys.exit(1)\n\n\n\n    def job_execute_stop_etcd_on_target_node(self):\n\n        self.logger.info(""---- package wrapper is working now! ----"")\n        self.prepare_package(self.node_config, ""stop-etcd-on-target-node"")\n        self.logger.info(""---- package wrapper\'s work finished ----"")\n\n        self.logger.info(""Begin to execute the job : stop-etcd-on-target-node."")\n        self.logger.info(""Stop the etcd server on host [{0}]"".format(self.node_config[\'nodename\']))\n\n        script_package = ""stop-etcd-on-target-node.tar""\n        src_local = ""parcel-center/{0}"".format(self.node_config[""nodename""])\n        dst_remote = common.get_user_dir(self.node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, self.node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar && sudo /bin/bash {0}/stop-etcd-server.sh"".format(""stop-etcd-on-target-node"")\n\n        if common.ssh_shell_with_password_input_paramiko(self.node_config, commandline) == False:\n            sys.exit(1)\n\n        self.logger.info(""Successfully stoping etcd server on node {0}"".format(self.node_config[""nodename""]))\n\n        if self.clean_flag == True:\n            self.logger.info(""---- package cleaner is working now! ----"")\n            self.delete_packege(self.node_config)\n            self.logger.info(""---- package cleaner\'s work finished! ----"")\n\n            self.logger.info(""---- remote host cleaner is working now! ----"")\n            self.remote_host_cleaner(self.node_config, ""stop-etcd-on-target-node"")\n            self.logger.info(""---- remote host cleaning job finished! "")\n\n\n\n    def job_execute_remove_node_from_etcd_cluster(self):\n\n        com = self.cluster_object_model\n        # Waiting for the bad node to remove from leader.\n        while True:\n\n            leader_node_config = pai_common.get_etcd_leader_node(com)\n\n            if leader_node_config is None:\n                self.logger.error(""Failed to find the leader node in the etcd cluster"")\n                sys.exit(1)\n\n            if leader_node_config[\'nodename\'] != self.node_config[\'nodename\']:\n                break\n\n        self.prepare_package(leader_node_config, ""remove-node-from-etcd-cluster"")\n\n        self.logger.info(""Begin to execute the job : remove-node-from-etcd-cluster."")\n        self.logger.info(""Update etcd cluster on host [{0}]."".format(leader_node_config[\'nodename\']))\n\n        script_package = ""remove-node-from-etcd-cluster.tar""\n        src_local = ""parcel-center/{0}"".format(leader_node_config[""nodename""])\n        dst_remote = common.get_user_dir(leader_node_config)\n\n        if common.sftp_paramiko(src_local, dst_remote, script_package, leader_node_config) == False:\n            sys.exit(1)\n\n        commandline = ""tar -xvf {0}.tar"".format(""remove-node-from-etcd-cluster"")\n        if common.ssh_shell_with_password_input_paramiko(leader_node_config, commandline) == False:\n            sys.exit(1)\n\n        commandline = ""sudo /bin/bash {0}/{1}.sh {2} {3}"".format(""remove-node-from-etcd-cluster"",\n                                                                 ""remove-member-from-etcd-cluster"",\n                                                                 self.node_config[\'hostip\'],\n                                                                 self.node_config[\'etcdid\'])\n        if common.ssh_shell_with_password_input_paramiko(leader_node_config, commandline) == False:\n            sys.exit(1)\n\n        self.logger.info(""Successfully remove target node from etcd cluster on node {0}"".format(leader_node_config[""nodename""]))\n\n        if self.clean_flag == True:\n            self.logger.info(""---- package cleaner is working now! ----"")\n            self.delete_packege(leader_node_config)\n            self.logger.info(""---- package cleaner\'s work finished! ----"")\n\n            self.logger.info(""---- remote host cleaner is working now! ----"")\n            self.remote_host_cleaner(leader_node_config, ""remove-node-from-etcd-cluster"")\n            self.logger.info(""---- remote host cleaning job finished! "")\n\n\n    def run(self):\n\n        if self.node_config[\'k8s-role\'] == \'master\':\n            self.logger.info(""The target node is master node."")\n\n            self.logger.info(""Task one before cleanup the node: stop target node\'s etcd."")\n            self.job_execute_stop_etcd_on_target_node()\n\n            self.logger.info(""Task two before cleanup the node: remove target node from etcd cluster"")\n            self.job_execute_remove_node_from_etcd_cluster()\n\n\n        self.logger.info(""---- package wrapper is working now! ----"")\n        self.prepare_package(self.node_config, self.jobname)\n        self.logger.info(""---- package wrapper\'s work finished ----"")\n\n        self.job_executer_clean_up_node()\n\n        if self.clean_flag == True:\n            self.logger.info(""---- package cleaner is working now! ----"")\n            self.delete_packege(self.node_config)\n            self.logger.info(""---- package cleaner\'s work finished! ----"")\n\n            self.logger.info(""---- remote host cleaner is working now! ----"")\n            self.remote_host_cleaner(self.node_config, self.jobname)\n            self.logger.info(""---- remote host cleaning job finished! "")\n\n'"
deployment/k8sPaiLibrary/maintainlib/update.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport time\nimport yaml\nimport requests\nimport logging\nimport logging.config\n\nfrom . import add as k8s_add\nfrom . import clean as k8s_clean\nfrom . import common as k8s_common\nfrom . import remove as k8s_remove\n\nfrom ...confStorage import conf_storage_util\nfrom ...confStorage.download import download_configuration\nfrom ...paiLibrary.common import directory_handler\nfrom ...paiLibrary.common import kubernetes_handler\n\nfrom ...clusterObjectModel.cluster_object_model import  cluster_object_model\n\n\nclass update:\n\n    def __init__(self, **kwargs):\n        self.logger = logging.getLogger(__name__)\n\n        self.kube_config_path = None\n        if ""kube_config_path"" in kwargs and kwargs[ ""kube_config_path"" ] != None:\n            self.kube_config_path = kwargs[ ""kube_config_path"" ]\n\n        if self.kube_config_path is None:\n            self.logger.error(""Unable to find KUBECONFIG. Please ensure that you have passed the correct path."")\n            sys.exit(1)\n\n        self.time = str(int(time.time()))\n        self.tmp_path = ""./tmp-machine-update-{0}"".format(self.time)\n\n        self.k8s_configuration = None\n        self.node_list_from_k8s = None\n        self.node_config_from_cluster_conf = None\n        self.node_config_from_k8s = None\n\n\n\n    def get_latest_configuration_from_pai(self):\n\n        self.logger.info(""Try to get latest configuration from kubernetes."")\n        directory_handler.directory_create(self.tmp_path)\n\n        config_get = download_configuration(config_output_path=self.tmp_path, kube_config_path=self.kube_config_path)\n        config_get.run()\n\n        cluster_object_model_instance = cluster_object_model(self.tmp_path)\n        com = cluster_object_model_instance.run()\n\n        directory_handler.directory_delete(self.tmp_path)\n        self.logger.info(""Successfully get latest configuration from Kubernetes."")\n        return com\n\n\n\n    def get_node_list_from_k8s_api(self):\n        self.logger.info(""Try to get node list from kubernetes api."")\n        node_list = kubernetes_handler.list_all_nodes(PAI_KUBE_CONFIG_PATH=self.kube_config_path)\n        self.logger.info(""Successfully get latest configuration from kubernetes."")\n        return node_list\n\n\n\n    def get_node_config_from_cluster_configuration(self):\n\n        self.logger.info(""Try to get node confguration from cluster configuration."")\n        com = self.k8s_configuration\n        node_config_from_cluster_conf = dict()\n\n        for role in [""proxy"", ""master"", ""worker""]:\n            if ""{0}-list"".format(role) not in com[""kubernetes""]:\n                continue\n\n            for node_key in com[\'kubernetes\'][""{0}-list"".format(role)]:\n                node_config = com[""layout""][""machine-list""][node_key]\n                node_config_from_cluster_conf[node_key] = node_config\n\n        self.logger.info(""Successfully get latest configuration from cluster configuration."")\n        return node_config_from_cluster_conf\n\n\n\n    """"""\n    Get node configuration from kubernetes configmap.\n    """"""\n    def get_node_config_from_k8s(self):\n        self.logger.info(""Try to get node configuration from kubernetes\' configmap."")\n        configmap_data = kubernetes_handler.get_configmap(self.kube_config_path, ""pai-node-config"", ""kube-system"")\n        pai_node_list = configmap_data[""data""][""node-list""]\n        self.logger.info(""Successfully get node configuration from kubernetes\' configmap."")\n        return yaml.load(pai_node_list, yaml.SafeLoader)\n\n\n\n    """"""\n    Update node configuration in kubernetes configmap based on cluster configuration\n    """"""\n    def update_node_config(self):\n        self.logger.info(""Try to update node configuration to kubernetes\' configmap."")\n        yaml_data = yaml.dump(self.node_config_from_cluster_conf, default_flow_style=False)\n        pai_node_list = {""node-list"": yaml_data}\n        kubernetes_handler.update_configmap(self.kube_config_path, ""pai-node-config"", pai_node_list, ""kube-system"")\n        self.logger.info(""Successfully update node configuration to kubernetes\' configmap."")\n\n\n\n    def check_node_healthz(self, address):\n        try:\n            r = requests.get(""http://{0}:10248/healthz"".format(address))\n            if r.status_code == 200:\n                return True\n        except Exception as e:\n            pass\n\n        return False\n\n\n\n    def remove(self, node_config, cluster_config):\n        remove_worker = k8s_remove.remove(cluster_config, node_config, True)\n        remove_worker.run()\n\n        if node_config[""k8s-role""] == ""master"":\n            self.logger.info(""master node is removed, sleep 60s for etcd cluster\'s updating"")\n            time.sleep(60)\n\n\n\n    def install(self, node_config, cluster_config):\n        add_worker = k8s_add.add(cluster_config, node_config, True)\n        add_worker.run()\n\n        if node_config[""k8s-role""] == ""master"":\n            self.logger.info(""Master Node is added, sleep 60s to wait it ready."")\n            time.sleep(60)\n\n\n\n    def node_status_check(self, node_config, node_list):\n        node_name = node_config[""nodename""]\n        if node_name not in node_list:\n            return False\n\n        for condition_instance in node_list[node_name][""condition""]:\n            if condition_instance[""type""] != ""Ready"":\n                continue\n            if condition_instance[""status""] != ""True"":\n                return False\n            break\n\n        if not self.check_node_healthz(node_config[""hostip""]):\n            return False\n\n        return True\n\n\n\n    """"""\n    Check all machine in the k8s configuration.\n    With the url to check the k8s node is setup or not.\n\n    URL: [ x.x.x.x:10248/healthz ]\n\n    If ok, the node is setup.\n    Or paictl will first do a clean on the target node and then bootstrap corresponding service according to the role of the node.\n    """"""\n    def add_machine(self):\n\n        node_list = self.node_list_from_k8s\n        com = self.k8s_configuration\n\n        for role in [""proxy"", ""master"", ""worker""]:\n            if ""{0}-list"".format(role) not in com[""kubernetes""]:\n                continue\n\n            for node_key in com[\'kubernetes\'][""{0}-list"".format(role)]:\n                node_config = com[""layout""][""machine-list""][node_key]\n\n                if not self.node_status_check(node_config, node_list):\n                    self.logger.info(""Begin to add new node into pai cluster."")\n                    self.logger.info(""Target node name: {0}"".format(node_config[""nodename""]))\n                    self.logger.info(""Target node address: {0}"".format(node_config[""hostip""]))\n\n                    self.logger.info(""[ 0/2 ] Cleaning the target node. "")\n                    self.remove(node_config, com)\n                    self.logger.info(""[ 1/2 ] Cleaning Done!"")\n\n                    self.logger.info(""[ 1/2 ] Install kubelet on the target node."")\n                    self.install(node_config, com)\n                    self.logger.info(""[ 2/2 ] Install Done!"")\n\n                    self.logger.info(""Node [{0}] is added into the cluster as a new kubernetes node."".format(node_config[""nodename""]))\n\n\n\n    """"""\n    Check all machine in the node list from k8s.\n    If the nodename not in the k8s configuration.\n    Paictl will clean the node.\n    Or do nothing.\n    """"""\n    def remove_machine(self):\n        for node in self.node_config_from_k8s:\n            if node not in self.node_config_from_cluster_conf:\n\n                node_config = self.node_config_from_k8s[node]\n\n                self.logger.info(""Begin to remove node from pai cluster."")\n                self.logger.info(""Target node name: {0}"".format(node_config[""nodename""]))\n                self.logger.info(""Target node address: {0}"".format(node_config[""hostip""]))\n\n                self.logger.info("" [ 0/1 ] Cleanning the target node, remove all service."")\n                self.remove(node_config, self.k8s_configuration)\n                self.logger.info("" [ 1/1 ] Cleanning Done."")\n\n                self.logger.info(""Node [{0}] is removed from kubernetes."")\n\n\n\n    def run(self):\n\n        self.k8s_configuration = self.get_latest_configuration_from_pai()\n        self.node_list_from_k8s = self.get_node_list_from_k8s_api()\n        self.node_config_from_cluster_conf = self.get_node_config_from_cluster_configuration()\n        self.node_config_from_k8s = self.get_node_config_from_k8s()\n\n        self.add_machine()\n        self.remove_machine()\n\n        self.update_node_config()\n        directory_handler.directory_delete(self.tmp_path)\n'"
deployment/k8sPaiLibrary/maintaintool/__init__.py,0,b''
deployment/k8sPaiLibrary/maintaintool/docker-config-update.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport json\nimport argparse\n\n\n# unable to handle list in list or dict in list.\n# Such as [ a, b, [c, d], e ]\n# Such as [ a, b, {k1:c, k2:d}, e ]\ndef dict_overwrite(subset_dict, superset_dict):\n    if subset_dict == None:\n        return False\n    updated = False\n    for key in subset_dict:\n        if key not in superset_dict:\n            superset_dict[key] = subset_dict[key]\n            updated = True\n        elif isinstance(subset_dict[key], dict) and isinstance(superset_dict[key], dict):\n            if dict_overwrite(subset_dict[key], superset_dict[key]) is True:\n                updated = True\n        elif isinstance(subset_dict[key], list) and isinstance(superset_dict[key], list):\n            if set(subset_dict[key]) != set(superset_dict[key]):\n                superset_dict[key] = subset_dict[key]\n                updated = True\n        elif subset_dict[key] != superset_dict[key]:\n            superset_dict[key] = subset_dict[key]\n            updated = True\n    return updated\n\n\nif __name__ == ""__main__"":\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-s\', \'--src-json\', dest=""src_json"", required=True,\n                        help=""The json with the data you wanna write"")\n    parser.add_argument(\'-d\', \'--dst-json\', dest=""dst_json"", required=True,\n                        help=""The json with the data you wanna update"")\n    args = parser.parse_args()\n\n    with open(args.src_json, ""r"") as jsonFile:\n        src_data = json.load(jsonFile)\n\n    with open(args.dst_json, ""r"") as jsonFile:\n        dst_data = json.load(jsonFile)\n\n    if dict_overwrite(src_data, dst_data) is False:\n        sys.exit(1)\n\n    with open(args.dst_json, \'w\') as jsonFile:\n        json.dump(dst_data, jsonFile)\n'"
deployment/k8sPaiLibrary/maintaintool/docker-info-root-dir.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import print_function\n\nimport docker\nimport sys\n\n\nclient = docker.from_env()\ndocker_info_dict = client.info()\n\nif \'DockerRootDir\' in docker_info_dict:\n    print(docker_info_dict[\'DockerRootDir\'])\nelse:\n    print(""Failed to get DockerRootDir through docker api."")\n    sys.exit(1)'"
deployment/k8sPaiLibrary/maintaintool/update_resource.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n""""""\nDelete k8s resources using AppsV1Api.\n""""""\n\nfrom __future__ import print_function\n\nimport time\nimport argparse\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\n\ndef get_api_resources(apps_v1_api):\n    api_resources = {\n        \'daemonset\': {\n            \'list\': apps_v1_api.list_namespaced_daemon_set,\n            \'delete\': apps_v1_api.delete_namespaced_daemon_set\n        },\n        \'statefulset\': {\n            \'list\': apps_v1_api.list_namespaced_stateful_set,\n            \'delete\': apps_v1_api.delete_namespaced_stateful_set,\n        },\n    }\n    return api_resources\n\n\ndef delete_resource(apps_v1_api, api_resource, name, namespace=\'default\'):\n    api_resources = get_api_resources(apps_v1_api)\n    body = client.V1DeleteOptions(\n        propagation_policy=\'Foreground\',\n        grace_period_seconds=0)\n\n    if api_resource in api_resources:\n        api = api_resources[api_resource]\n        while True:\n            items = api[\'list\'](namespace=namespace).items\n            if name not in map(lambda x: x.metadata.name, items):\n                break\n            print(\'Trying to stop {} ...\'.format(name))\n            try:\n                api[\'delete\'](name=name, namespace=namespace, body=body)\n            except ApiException as e:\n                continue\n            time.sleep(5)\n    else:\n        raise Exception(\'Unsupported resource {}.\'.format(api_resource))\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Update service resources.\')\n    parser.add_argument(\'--operation\', help=\'operation type\')\n    parser.add_argument(\'--resource\', help=\'api resource type\')\n    parser.add_argument(\'--name\', help=\'resource name\')\n    parser.add_argument(\'--namespace\', help=\'resource namespace\', default=\'default\')\n    args = parser.parse_args()\n\n    config.load_kube_config()\n    apps_v1_api = client.AppsV1Api()\n    if (args.operation == \'delete\'):\n        delete_resource(apps_v1_api, args.resource, args.name, args.namespace)\n    else:\n        raise Exception(\'Unknown operation {}.\'.format(args.operation))\n\n\nif __name__ == \'__main__\':\n    main()\n'"
deployment/k8sPaiLibrary/monitorTool/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n'"
deployment/k8sPaiLibrary/monitorTool/check_node_label_exist.py,0,"b'#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport sys\nimport time\n\nfrom ..monitorlib import nodestatus\n\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-k\', \'--label-key\', dest=""label_key"", required=True, help=""the data of app label-key in your service"")\n    parser.add_argument(\'-v\', \'--label-value\', dest=""label_value"", required=True, help=""the data of app label-value in your service"")\n\n    args = parser.parse_args()\n\n    if nodestatus.is_label_exist(args.label_key, args.label_value) != True:\n        sys.exit(1)\n\n\n\nif __name__ == ""__main__"":\n    main()'"
deployment/k8sPaiLibrary/monitorTool/check_pod_ready_status.py,0,"b'#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport argparse\nimport sys\nimport time\n\nfrom ..monitorlib import servicestatus\n\n\n# Designed for shell, so use the exit function to pass error code.\ndef service_status_check(label_key, label_value):\n\n    if servicestatus.pod_is_ready_or_not(label_key, label_value) != True:\n        print ""{0} is not ready yet!"".format(label_value)\n        sys.exit(1)\n\n\n\ndef waiting_until_service_ready(label_key, label_value, total_time=3600):\n\n    while servicestatus.pod_is_ready_or_not(label_key, label_value) != True:\n\n\n        print ""{0} is not ready yet. Please wait for a moment!"".format(label_value)\n        time.sleep(10)\n        total_time = total_time - 10\n\n        if total_time < 0:\n            print ""An issue occure when starting up {0}"".format(label_value)\n            sys.exit(1)\n\n    print ""{0} is ready!"".format(label_value)\n\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-w\', \'--wait_service\', action=""store_true"", help=""wait until the service is ready"")\n    parser.add_argument(\'-k\', \'--label-key\', dest=""label_key"", required=True, help=""the data of app label-key in your service"")\n    parser.add_argument(\'-v\', \'--label-value\', dest=""label_value"", required=True, help=""the data of app label-value in your service"")\n    parser.add_argument(\'-t\', \'--timeout\', type=int, default=3600, help=""the data of app label in your service"")\n\n    args = parser.parse_args()\n    service_label_key = args.label_key\n    service_label_value = args.label_value\n\n    timeout = args.timeout\n\n    if args.wait_service:\n        waiting_until_service_ready(service_label_key, service_label_value, timeout)\n    else:\n        service_status_check(service_label_key, service_label_value)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
deployment/k8sPaiLibrary/monitorlib/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
deployment/k8sPaiLibrary/monitorlib/nodestatus.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\n\n\n# To check a label on nodes exist or not.\n\ndef is_label_exist(key, value):\n    \n    config.load_kube_config()\n    v1 = client.CoreV1Api()\n\n    try:\n        node_list = v1.list_node(label_selector=""{0}={1}"".format(key, value), watch=False)\n    except ApiException as e:\n        print ""Exception when calling CoreV1Api->list_node: %s\\n"" % e\n        sys.exit(1)\n\n    if len(node_list.items) == 0:\n        return False\n    return True\n\n\n\n'"
deployment/k8sPaiLibrary/monitorlib/servicestatus.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\n\n\n# To check a service ready or not.\n# Note that service name should be same as the app-name in\n#     labels:\n#        app: app-name\ndef is_service_ready(servicename):\n\n    label_selector_str=""app={0}"".format(servicename)\n\n    config.load_kube_config()\n    v1 = client.CoreV1Api()\n\n    try:\n        pod_list = v1.list_pod_for_all_namespaces(label_selector=label_selector_str, watch=False)\n    except ApiException as e:\n        print ""Exception when calling CoreV1Api->list_pod_for_all_namespaces: %s\\n"" % e\n        sys.exit(1)\n\n    if len(pod_list.items) == 0:\n        return False\n\n    for pod in pod_list.items:\n        if pod.status.container_statuses is None:\n            return False\n        for container in pod.status.container_statuses:\n            if container.ready != True:\n                return False\n\n    return True\n\n\n\n\n# To check a service ready or not.\n# Note that service name should be same as the app-name in\n#     labels:\n#        key : value\ndef pod_is_ready_or_not(label_key, label_value):\n\n    label_selector_str=""{0}={1}"".format(label_key, label_value)\n\n    config.load_kube_config()\n    v1 = client.CoreV1Api()\n\n    try:\n        pod_list = v1.list_pod_for_all_namespaces(label_selector=label_selector_str, watch=False)\n    except ApiException as e:\n        print ""Exception when calling CoreV1Api->list_pod_for_all_namespaces: %s\\n"" % e\n        return False\n\n    if len(pod_list.items) == 0:\n        return False\n\n    for pod in pod_list.items:\n        if pod.status.container_statuses is None:\n            return False\n        for container in pod.status.container_statuses:\n            if container.ready != True:\n                return False\n\n    return True\n\n\n'"
deployment/paiLibrary/common/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n'"
deployment/paiLibrary/common/directory_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport os\nimport logging\nimport logging.config\n#\nfrom . import linux_shell\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_subdirectory_list(path):\n\n    return next(os.walk(path))[1]\n\n\n\ndef directory_create(path):\n\n    if os.path.exists(path) == False:\n        shell_cmd = ""mkdir -p {0}"".format(path)\n        error_msg = ""failed to mkdir -p {0}"".format(path)\n        linux_shell.execute_shell(shell_cmd, error_msg)\n\n\n\ndef directory_copy(src_item, dst):\n\n    directory_create(dst)\n\n    shell_cmd = ""cp -r {0} {1}"".format(src_item, dst)\n    error_msg = ""failed to copy {0}"".format(src_item)\n    linux_shell.execute_shell(shell_cmd, error_msg)\n\n\n\ndef directory_delete(path):\n\n    shell_cmd = ""rm -rf {0}"".format(path)\n    error_msg = ""failed to delete {0}"".format(path)\n    linux_shell.execute_shell(shell_cmd, error_msg)\n\n\n\ndef directory_exist_or_not(path):\n\n    return os.path.isfile(path) != True and os.path.exists(path) == True\n\n\n\n\n\n\n\n\n'"
deployment/paiLibrary/common/docker_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n#\n#import docker\nfrom . import file_handler\nfrom . import linux_shell\n\n\n# TODO: Change the command with linux_shell.execute_shell to docker lib.\n\n\n\n\n\n\nclass docker_handler:\n\n\n    def __init__(self, docker_registry, docker_namespace, docker_username, docker_password):\n\n        self.logger = logging.getLogger(__name__)\n\n        if docker_registry == ""public"":\n            docker_registry = """"\n\n        self.docker_registry = docker_registry\n        self.docker_namespace = docker_namespace\n        self.docker_username = docker_username\n        self.docker_password = docker_password\n\n        self.docker_login()\n        #self.docker_client = self.client_initialization()\n\n\n\n    def image_name_resolve(self, image_name):\n\n        if self.docker_registry == """":\n            prefix = """"\n        else:\n            prefix = self.docker_registry + ""/""\n\n        return ""{0}{1}/{2}"".format(prefix, self.docker_namespace, image_name)\n\n\n\n    def docker_login(self):\n\n        shell_cmd = ""docker login -u {0} -p {1} {2}"".format(self.docker_username, self.docker_password, self.docker_registry)\n        error_msg = ""docker registry login error""\n        linux_shell.execute_shell(shell_cmd, error_msg)\n        self.logger.info(""docker registry login successfully"")\n\n\n\n    #def client_initialization(self):\n    #\n    #    client = docker.from_env()\n    #    return client\n\n\n\n    def image_build(self, image_name, path_to_dockerfile):\n\n        # Comment the code with the docker python lib. Because I think the shell command\'s log\n        # is more friendly.\n        """"""""\n        image_obj, build_log = self.docker_client.images.build(path=path_to_dockerfile, tag=image_name, rm=True, pull=True)\n        for line in build_log:\n            self.logger.info(line)\n        """"""\n\n        cmd = ""docker build -t {0} {1}"".format(image_name, path_to_dockerfile)\n        err_msg = ""An error occurs, when building your image [ {0} ]"".format(image_name)\n\n        self.logger.info(""Begin to execute the command: {0}"".format(cmd))\n\n        linux_shell.execute_shell(cmd, err_msg)\n\n        self.logger.info(""Executing is successful."")\n\n\n\n\n    def image_tag_to_registry(self, origin_image_name, image_tag):\n\n        origin_tag = origin_image_name\n        target_tag = ""{0}:{1}"".format(self.image_name_resolve(origin_image_name), image_tag)\n\n        # Comment the code with the docker python lib. Because I think the shell command\'s log\n        # is more friendly.\n        """"""\n        target_image = self.docker_client.images.get(origin_tag)\n        target_image.tag(target_tag)\n        """"""\n\n        cmd = ""docker tag {0} {1}"".format(origin_tag, target_tag)\n        err_msg = ""An error occurs, when taging your image [ {0} ] to the target registry"".format(origin_image_name)\n\n        self.logger.info(""Begin to execute the command: {0}"".format(cmd))\n\n        linux_shell.execute_shell(cmd, err_msg)\n\n        self.logger.info(""Executing is successful."")\n\n\n\n    def image_push_to_registry(self, image_name, image_tag):\n\n        target_tag = ""{0}:{1}"".format(self.image_name_resolve(image_name), image_tag)\n\n        # Comment the code with the docker python lib. Because I think the shell command\'s log\n        # is more friendly.\n        """"""\n        push_logs = self.docker_client.images.push(target_tag)\n        self.logger.info(push_logs)\n        """"""\n\n        cmd = ""docker push {0}"".format(target_tag)\n        err_msg = ""An error occurs, when pushing your image [ {0} ] to the target registry"".format(image_name)\n\n        self.logger.info(""Begin to execute the command: {0}"".format(cmd))\n\n        linux_shell.execute_shell(cmd, err_msg)\n\n        self.logger.info(""Executing is successful."")\n\n\n\n'"
deployment/paiLibrary/common/file_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport yaml\nimport logging\nimport logging.config\n\nfrom . import linux_shell\n\nlogger = logging.getLogger(__name__)\n\n\ndef get_file_list_in_path(path):\n\n    return next(os.walk(path))[2]\n\n\n\ndef load_yaml_config(config_path):\n\n    with open(config_path, ""r"") as f:\n        cluster_data = yaml.load(f, yaml.SafeLoader)\n\n    return cluster_data\n\n\ndef dump_yaml_data(file_path, data):\n    with open(file_path, ""w"") as f:\n        yaml.dump(data, f, default_flow_style=False)\n\n\n\ndef read_template(template_path):\n\n    with open(template_path, ""r"") as f:\n        template_data = f.read().decode(\'utf-8\')\n\n    return template_data\n\n\n\ndef write_generated_file(file_path, content_data):\n\n    with open(file_path, ""w+"") as fout:\n        fout.write(content_data)\n\n\n\ndef file_exist_or_not(file_path):\n\n    return os.path.isfile(str(file_path))\n\n\n\ndef file_delete(file_path):\n\n    if file_exist_or_not(file_path):\n        try:\n            os.unlink(file_path)\n        except OSError as e:\n            logger.exception(e)\n\n\n\ndef directory_exits(dir_path):\n\n    return os.path.isdir(dir_path)\n\n\n\n\ndef create_folder_if_not_exist(folder_path):\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n'"
deployment/paiLibrary/common/kubernetes_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport sys\nimport logging\nimport logging.config\nimport kubernetes.client\n\nfrom kubernetes.client.rest import ApiException\nfrom kubernetes import client, config, watch\n\n\nlogger = logging.getLogger(__name__)\n\n\n\ndef get_kubernetes_corev1api(PAI_KUBE_CONFIG_PATH, **kwargs):\n\n    config.load_kube_config(config_file=PAI_KUBE_CONFIG_PATH)\n    api_instance = kubernetes.client.CoreV1Api()\n\n    return api_instance\n\n\n\ndef list_all_nodes(PAI_KUBE_CONFIG_PATH, include_uninitialized = True):\n\n    api_instance = get_kubernetes_corev1api(PAI_KUBE_CONFIG_PATH = PAI_KUBE_CONFIG_PATH)\n\n    try:\n        api_response = api_instance.list_node(\n            include_uninitialized = include_uninitialized\n        )\n        node_list = api_response.items\n\n    except ApiException as e:\n        logger.error(""Exception when calling kubernetes CoreV1Api->list_node: {0}"".format(e))\n        sys.exit(1)\n\n    except Exception as e:\n        logger.error(""Error happened when calling kubernetes CoreV1Api->list_node: {0}"".format(e))\n        sys.exit(1)\n\n    if len(node_list) == 0:\n        return None\n\n    resp = dict()\n    for node in node_list:\n        node_name = node.metadata.name\n\n        """"""\n         Example of address\n         [\n            {\'address\': \'10.240.0.10\', \'type\': \'InternalIP\'},\n            {\'address\': \'10.240.0.10\', \'type\': \'Hostname\'},\n            {\'address\': \'x.x.x.x\', \'type\': \'ExternalIP\'}\n         ]\n         before using this list, please check whether it exists or not.\n        """"""\n        node_addresses = list()\n        for node_address_instance in node.status.addresses:\n            node_addresses.append(\n                {\n                    ""type"": node_address_instance.type,\n                    ""address"": node_address_instance.address\n                }\n            )\n\n        node_conditions = list()\n        for node_conditions_instance in node.status.conditions:\n            node_conditions.append(\n                {\n                    ""type"": node_conditions_instance.type,\n                    # type str, value True, False, Unknown\n                    ""status"": node_conditions_instance.status\n                }\n            )\n\n        resp[node_name] = dict()\n        resp[node_name][""address""] = node_addresses\n        resp[node_name][""condition""] = node_conditions\n\n    return resp\n\n\n\ndef get_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, name, namespace = ""default""):\n\n    api_instance = get_kubernetes_corev1api(PAI_KUBE_CONFIG_PATH=PAI_KUBE_CONFIG_DEFAULT_LOCATION)\n    exact = True\n    export = True\n\n    target_configmap_data = None\n    target_configmap_metadata = None\n\n    try:\n        api_response = api_instance.read_namespaced_config_map(name, namespace, exact=exact, export=export)\n        target_configmap_data = api_response.data\n        target_configmap_metadata = api_response.metadata\n\n    except ApiException as e:\n        if e.status == 404:\n            logger.info(""Couldn\'t find configmap named {0}"".format(name))\n            return None\n        else:\n            logger.error(""Exception when calling CoreV1Api->read_namespaced_config_map: {0}"".format(str(e)))\n            sys.exit(1)\n\n    ret = {\n        ""metadata"" : target_configmap_metadata,\n        ""data""     : target_configmap_data\n    }\n\n    return ret\n\n\n\ndef update_configmap(PAI_KUBE_CONFIG_DEFAULT_LOCATION, name, data_dict, namespace = ""default""):\n\n    api_instance = get_kubernetes_corev1api(PAI_KUBE_CONFIG_PATH=PAI_KUBE_CONFIG_DEFAULT_LOCATION)\n\n    meta_data = kubernetes.client.V1ObjectMeta()\n    meta_data.namespace = namespace\n    meta_data.name = name\n    body = kubernetes.client.V1ConfigMap(\n                            metadata = meta_data,\n                            data = data_dict)\n\n    try:\n        api_response = api_instance.replace_namespaced_config_map(name, namespace, body)\n        logger.info(""configmap named {0} is updated."".format(name))\n\n    except ApiException as e:\n\n        if e.status == 404:\n\n            try:\n                logger.info(""Couldn\'t find configmap named {0}. Create a new configmap"".format(name))\n                api_response = api_instance.create_namespaced_config_map(namespace, body)\n                logger.info(""Configmap named {0} is created"".format(name))\n\n            except ApiException as ie:\n                logger.error(""Exception when calling CoreV1Api->create_namespaced_config_map: {0}"".format(str(e)))\n                sys.exit(1)\n\n        else:\n            logger.error(""Exception when calling CoreV1Api->replace_namespaced_config_map: {0}"".format(str(e)))\n            sys.exit(1)'"
deployment/paiLibrary/common/linux_shell.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport logging\nimport logging.config\nimport subprocess\n\n\nlogger = logging.getLogger(__name__)\n\n\n\ndef execute_shell_raise(shell_cmd, error_msg):\n\n    try:\n        subprocess.check_call( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.error(error_msg)\n        raise\n\n\n\n\ndef execute_shell(shell_cmd, error_msg):\n\n    try:\n        subprocess.check_call( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.error(error_msg)\n        sys.exit(1)\n\n\n\ndef execute_shell_with_output(shell_cmd, error_msg):\n\n    try:\n        res = subprocess.check_output( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.error(error_msg)\n        sys.exit(1)\n\n    return res\n\n\ndef execute_shell_return(shell_cmd, error_msg):\n\n    try:\n        subprocess.check_call( shell_cmd, shell=True )\n\n    except subprocess.CalledProcessError:\n        logger.warning(error_msg)\n        return False\n\n    return True'"
deployment/paiLibrary/common/template_handler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport jinja2\nimport logging\nimport logging.config\n\nlogger = logging.getLogger(__name__)\n\n\ndef generate_from_template_dict(template_data, map_table):\n\n    generated_file = jinja2.Template(template_data).render(\n        map_table\n    )\n\n    return generated_file'"
deployment/paiLibrary/paiService/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
deployment/paiLibrary/paiService/service_management_configuration.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport time\nimport os\nimport tempfile\nfrom ...confStorage.download import download_configuration\nfrom ...clusterObjectModel.cluster_object_model import cluster_object_model\nfrom ..common import directory_handler\nfrom ..common import file_handler\n\n\ndef gengerate_tmp_path():\n    time_in_seconds = str(int(time.time()))\n    sub_directory = ""tmp-service-config-{0}"".format(time_in_seconds)\n    return os.path.join(tempfile.gettempdir(), sub_directory)\n\n\ndef get_cluster_object_model_from_k8s(kube_config_path):\n    tmp_path = gengerate_tmp_path()\n\n    config_get_handler = download_configuration(config_output_path=tmp_path, kube_config_path=kube_config_path)\n    config_get_handler.run()\n\n    objectModelFactoryHandler = cluster_object_model(configuration_path=tmp_path)\n    cluster_object_service = objectModelFactoryHandler.service_config()\n\n    return cluster_object_service\n\n\ndef get_service_list(cluster_type=""yarn""):\n    service_list = list()\n    subdir_list = directory_handler.get_subdirectory_list(""src/"")\n    for subdir in subdir_list:\n        service_deploy_dir = ""src/{0}/deploy"".format(subdir)\n        service_deploy_conf_path = ""src/{0}/deploy/service.yaml"".format(subdir)\n        if file_handler.directory_exits(service_deploy_dir) and file_handler.file_exist_or_not(service_deploy_conf_path):\n            service_conf = file_handler.load_yaml_config(service_deploy_conf_path)\n            if (""cluster-type"" not in service_conf) or (""cluster-type"" in service_conf and cluster_type in service_conf[""cluster-type""]):\n                service_list.append(subdir)\n    return service_list\n'"
deployment/paiLibrary/paiService/service_management_delete.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport time\nimport logging\nimport logging.config\n#\nfrom deployment.paiLibrary.common import file_handler\nfrom . import service_template_generate\nfrom . import service_template_clean\nfrom . import service_management_configuration\n#\nfrom ..common import directory_handler\nfrom ..common import file_handler\n\n\nclass service_management_delete:\n\n    def __init__(self, kube_config_path=None, service_list=None, **kwargs):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = service_management_configuration.get_cluster_object_model_from_k8s(kube_config_path)\n        self.kube_config_path = kube_config_path\n        self.cluster_type = None\n\n        if service_list is None:\n            if ""cluster-type"" in self.cluster_object_model[""cluster""][""common""]:\n                self.cluster_type = self.cluster_object_model[""cluster""][""common""][""cluster-type""]\n            self.service_list = service_management_configuration.get_service_list(self.cluster_type)\n        else:\n            self.service_list = service_list\n        self.logger.info(""Get the service-list to manage : {0}"".format(str(self.service_list)))\n\n    def delete_service(self, service_conf, service_name):\n        from ..common import linux_shell\n        delete_script = ""src/{0}/deploy/{1}"".format(service_name, service_conf[""delete-script""])\n\n        cmd = ""/bin/bash {0}"".format(delete_script)\n        err_msg = ""Failed to execute the delete script of service {0}"".format(service_name)\n        self.logger.info(""Begin to execute service {0}\'s delete script."".format(service_name))\n        linux_shell.execute_shell(cmd, err_msg)\n\n    def start(self, serv):\n\n        service_conf = file_handler.load_yaml_config(""src/{0}/deploy/service.yaml"".format(serv))\n\n        self.logger.info(""----------------------------------------------------------------------"")\n        self.logger.info(""Begin to generate service {0}\'s template file"".format(serv))\n\n        service_template_generater = service_template_generate.service_template_generate(self.cluster_object_model,\n                                                                                         serv, service_conf)\n        service_template_generater.run()\n\n        self.logger.info(""Begin to delete service: [ {0} ]"".format(serv))\n        self.delete_service(service_conf, serv)\n\n        self.logger.info(""Begin to delete service\'s generated template file"".format(serv))\n        service_template_cleaner = service_template_clean.service_template_clean(serv, service_conf)\n        service_template_cleaner.run()\n\n        self.logger.info(""Successfully delete {0}"".format(serv))\n        self.logger.info(""----------------------------------------------------------------------"")\n\n    def run(self):\n        for serv in self.service_list:\n            if serv == ""cluster-configuration"":\n                continue\n            if file_handler.file_exist_or_not(""src/{0}/deploy/service.yaml"".format(serv)) == False:\n                self.logger.warning(""service.yaml can\'t be found on the directory of {0}"".format(serv))\n                self.logger.warning(""Please check your source code. The {0}\'s service will be skipped."".format(serv))\n                continue\n            self.start(serv)\n\n        if ""cluster-configuration"" in self.service_list:\n            self.start(serv)\n'"
deployment/paiLibrary/paiService/service_management_refresh.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\nfrom . import service_template_generate\nfrom . import service_template_clean\nfrom . import service_management_configuration\n\nfrom ..common import directory_handler\nfrom ..common import file_handler\nfrom ..common import linux_shell\n\n\nclass service_management_refresh:\n\n    def __init__(self, kube_config_path=None, service_list=None, **kwargs):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = service_management_configuration.get_cluster_object_model_from_k8s(kube_config_path)\n        self.kube_config_path = kube_config_path\n        self.cluster_type = None\n\n        if service_list is None:\n            if ""cluster-type"" in self.cluster_object_model[""cluster""][""common""]:\n                self.cluster_type = self.cluster_object_model[""cluster""][""common""][""cluster-type""]\n            self.service_list = service_management_configuration.get_service_list(self.cluster_type)\n        else:\n            self.service_list = service_list\n        self.logger.info(""Get the service-list to manage : {0}"".format(str(self.service_list)))\n\n        self.label_map = dict()\n\n    def refresh_all_label(self):\n        self.logger.info(""Begin to refresh all the nodes\' labels"")\n        machinelist = self.cluster_object_model[\'layout\'][\'machine-list\']\n\n        labels = [\'pai-master\', \'pai-worker\', \'pai-storage\', \'no-drivers\', \'no-nodeexporter\']\n        logging.info(""Currently supported labels: "" + str(labels))\n        for label in labels:\n            self.label_map[label] = list()\n\n        err_msg_prefix = ""Error refreshing all label when execute: ""\n        for host in machinelist:\n            nodename = machinelist[host][\'nodename\']\n            for label in labels:\n                cmd_checklabel = ""kubectl describe node "" + nodename + "" | grep -q "" + label + ""=\'true\'""\n                has_label = linux_shell.execute_shell_return(cmd_checklabel, """")\n                # If machinelist config has defined the label, but the node did\'t have, label it\n                if label in machinelist[host]:\n                    self.label_map[label].append(nodename)\n                    if not has_label:\n                        self.logger.info(""Label defined in cluster-configuration machinelist, label the node "" + str(nodename) + "" of "" + label)\n                        cmd = ""kubectl label --overwrite=true nodes "" + nodename + "" "" + label + ""=\'true\' || exit $?""\n                        linux_shell.execute_shell(cmd, err_msg_prefix + cmd)\n                # If machinelist config has not define the label, but the node has the label, remove it\n                else:\n                    if has_label:\n                        self.logger.info(""Remove Node "" + nodename + "" label "" + label + "", due to the cluster-configuration machinelist doesn\'t specify this label"")\n                        cmd = ""kubectl label nodes "" + nodename + "" "" + label + ""- || exit $?""\n                        linux_shell.execute_shell(cmd, err_msg_prefix + "" when kubectl label nodes"")\n\n    def refresh_service(self, service_conf, service_name, label_map):\n        # Check label definition in machinelist, keep nodes\' labels consistent with configuration\n        # Ensure pods sheduled correctly with labels\n        err_msg_prefix = ""Error refreshing service "" + service_name + "" when execute: ""\n        if \'deploy-rules\' in service_conf:\n            for rule in service_conf[\'deploy-rules\']:\n                if \'in\' in rule:\n                    # If service not runnning on labeled node, start the service\n                    if rule[\'in\'] not in label_map:\n                        self.logger.error(""Label defined error, "" + rule[\'in\'] + "" isn\'t supported in layout.yaml machinelist."")\n                    nodes = self.label_map[rule[\'in\']]\n                    for nodename in nodes:\n                        cmd_checkservice = ""kubectl get po -o wide | grep "" + nodename + "" | grep -q "" + service_name\n                        if not linux_shell.execute_shell_return(cmd_checkservice, """"):\n                            self.logger.info(""Start service "" + service_name + "" from Node "" + nodename +\n                                             "" for its deployment label is labeled on this node according to the cluster-configuration machinelist but service isn\'t running."")\n                            start_script = ""src/{0}/deploy/{1}"".format(service_name, service_conf[""start-script""])\n                            linux_shell.execute_shell(""/bin/bash "" + start_script, err_msg_prefix + "" start service "" + service_name)\n\n                    # If service run on not labeled node, delete it.\n                    cmd = ""kubectl get po -o wide | grep "" + service_name\n                    res = linux_shell.execute_shell_with_output(cmd, """")\n                    items = res.split(""\\n"")\n                    nodes_has_service = dict()\n                    for item in items:\n                        if len(item) > 10:\n                            item = item.split()\n                            nodes_has_service[item[-1]] = item[0]\n                    for n in nodes_has_service:\n                        if n not in nodes:\n                            self.logger.info(""Service "" + service_name + "" should not run on "" + n +\n                                             "" according to its deploy-rules of service.yaml config file. Deleting..."")\n                            cmd = ""kubectl delete pod "" + nodes_has_service[n]\n                            linux_shell.execute_shell(cmd, err_msg_prefix + cmd)\n\n                # for \'notin\' rule, it\'s Daemonset, needn\'t do anything\n                if \'notin\' in rule:\n                    if rule[\'notin\'] not in label_map:\n                        self.logger.error(""Label defined error, "" + rule[\'notin\'] + "" isn\'t defined in layout.yaml machinelist."")\n\n        refresh_script = ""src/{0}/deploy/{1}"".format(service_name, service_conf[""refresh-script""])\n        cmd = ""/bin/bash {0}"".format(refresh_script)\n        err_msg = ""Failed to execute the refresh script of service {0}"".format(service_name)\n        self.logger.info(""Begin to execute service {0}\'s refresh script."".format(service_name))\n        linux_shell.execute_shell(cmd, err_msg)\n\n    def start(self, serv):\n\n        if serv in self.done_dict and self.done_dict[serv] == True:\n            return\n\n        service_conf = file_handler.load_yaml_config(""src/{0}/deploy/service.yaml"".format(serv))\n\n        dependency_list = service_conf.get(""prerequisite"")\n        if dependency_list != None:\n            for fat_serv in dependency_list:\n                if fat_serv not in self.service_list:\n                    continue\n                if fat_serv in self.done_dict and self.done_dict[fat_serv] == True:\n                    continue\n                self.start(fat_serv)\n\n        self.logger.info(""-----------------------------------------------------------"")\n        self.logger.info(""Begin to generate service {0}\'s template file"".format(serv))\n        service_template_generater = service_template_generate.service_template_generate(self.cluster_object_model, serv, service_conf)\n        service_template_generater.run()\n\n        self.logger.info(""Begin to refresh service: [ {0} ]"".format(serv))\n        self.refresh_service(service_conf, serv, self.label_map)\n\n        self.logger.info(""Begin to clean all service\'s generated template file"".format(serv))\n        service_template_cleaner = service_template_clean.service_template_clean(serv, service_conf)\n        service_template_cleaner.run()\n\n        self.logger.info(""Successfully refresh {0}"".format(serv))\n        self.logger.info(""-----------------------------------------------------------"")\n\n        self.done_dict[serv] = True\n\n    def run(self):\n        self.done_dict = dict()\n        self.refresh_all_label()\n        for serv in self.service_list:\n            if file_handler.file_exist_or_not(""src/{0}/deploy/service.yaml"".format(serv)) == False:\n                self.logger.warning(""service.yaml can\'t be found on the directory of {0}"".format(serv))\n                self.logger.warning(""Please check your source code. The {0}\'s service will be skipped."".format(serv))\n                continue\n            if serv in self.done_dict and self.done_dict[serv] == True:\n                continue\n            self.start(serv)\n'"
deployment/paiLibrary/paiService/service_management_start.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport subprocess\nimport logging\nimport time\nimport logging.config\n#\nfrom . import service_template_generate\nfrom . import service_template_clean\nfrom . import service_management_configuration\n#\nfrom ..common import directory_handler\nfrom ..common import file_handler\n\n\nclass serivce_management_start:\n\n    def __init__(self, kube_config_path=None, service_list=None, **kwargs):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = service_management_configuration.get_cluster_object_model_from_k8s(kube_config_path)\n        self.kube_config_path = kube_config_path\n        self.cluster_type = None\n\n        if service_list is None:\n            if ""cluster-type"" in self.cluster_object_model[""cluster""][""common""]:\n                self.cluster_type = self.cluster_object_model[""cluster""][""common""][""cluster-type""]\n            self.service_list = service_management_configuration.get_service_list(self.cluster_type)\n        else:\n            self.service_list = service_list\n        if self.cluster_type == \'yarn\':\n            user_input = raw_input(\n                ""Cluster type `yarn` is not well tested. We recommend you to use `k8s` version or "" +\n                ""stick to 0.14.0 if you prefer yarn version. "" +\n                ""If you still want to continue, please input Y. "" +\n                ""Other inputs will stop installation immediately: "")\n            if user_input != ""Y"":\n                sys.exit(1)\n        self.logger.info(""Get the service-list to manage : {0}"".format(str(self.service_list)))\n\n        self.retry_times = 5\n        if ""retry_times"" in kwargs:\n            self.retry_times = kwargs[""retry_times""]\n\n    def start_service(self, service_conf, service_name):\n        from ..common import linux_shell\n        start_script = ""src/{0}/deploy/{1}"".format(service_name, service_conf[""start-script""])\n\n        cmd = ""/bin/bash {0}"".format(start_script)\n        err_msg = ""Failed to execute the start script of service {0}"".format(service_name)\n        self.logger.info(""Begin to execute service {0}\'s start script."".format(service_name))\n        linux_shell.execute_shell_raise(cmd, err_msg)\n\n    def start(self, serv):\n\n        if serv in self.done_dict and self.done_dict[serv] == True:\n            return\n\n        service_conf = file_handler.load_yaml_config(""src/{0}/deploy/service.yaml"".format(serv))\n\n        dependency_list = service_conf.get(""prerequisite"")\n        if dependency_list != None:\n            for fat_serv in dependency_list:\n                if fat_serv not in self.service_list:\n                    continue\n                if fat_serv in self.done_dict and self.done_dict[fat_serv] == True:\n                    continue\n                self.start(fat_serv)\n\n        try_counts = 0\n        while True:\n\n            try:\n                self.logger.info(""-----------------------------------------------------------"")\n                self.logger.info(""Begin to generate service {0}\'s template file"".format(serv))\n                service_template_generater = service_template_generate.service_template_generate(self.cluster_object_model, serv, service_conf)\n                service_template_generater.run()\n\n                self.logger.info(""Begin to start service: [ {0} ]"".format(serv))\n                self.start_service(service_conf, serv)\n\n                self.logger.info(""Begin to clean all service\'s generated template file"".format(serv))\n                service_template_cleaner = service_template_clean.service_template_clean(serv, service_conf)\n                service_template_cleaner.run()\n\n                self.logger.info(""Successfully start {0}"".format(serv))\n                self.logger.info(""-----------------------------------------------------------"")\n                break\n\n            except subprocess.CalledProcessError:\n                self.logger.error(""Failed to start service {0}"".format(serv))\n                self.logger.info(""-----------------------------------------------------------"")\n\n                try_counts = try_counts + 1\n                if try_counts >= self.retry_times:\n                    self.logger.error(""Have retried {0} times, but service {1} doesn\'t start. Please check it."".format(self.retry_times, serv))\n                    sys.exit(1)\n\n                time.sleep(10)\n\n            except Exception as error:\n                self.logger.exception(""Some error occurs when starting service {0}"".format(serv))\n                sys.exit(1)\n\n        self.done_dict[serv] = True\n\n    def run(self):\n        self.done_dict = dict()\n\n        for serv in self.service_list:\n            if file_handler.file_exist_or_not(""src/{0}/deploy/service.yaml"".format(serv)) == False:\n                self.logger.warning(""service.yaml can\'t be found on the directory of {0}"".format(serv))\n                self.logger.warning(""Please check your source code. The {0}\'s service will be skipped."".format(serv))\n                continue\n            if serv in self.done_dict and self.done_dict[serv] == True:\n                continue\n            self.start(serv)\n'"
deployment/paiLibrary/paiService/service_management_stop.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport time\nimport logging\nimport logging.config\n\nfrom . import service_template_generate\nfrom . import service_template_clean\nfrom . import service_management_configuration\n\nfrom ..common import directory_handler\nfrom ..common import file_handler\n\n\nclass service_management_stop:\n\n    def __init__(self, kube_config_path=None, service_list=None, **kwargs):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_model = service_management_configuration.get_cluster_object_model_from_k8s(kube_config_path)\n        self.kube_config_path = kube_config_path\n        self.cluster_type = None\n\n        if service_list is None:\n            if ""cluster-type"" in self.cluster_object_model[""cluster""][""common""]:\n                self.cluster_type = self.cluster_object_model[""cluster""][""common""][""cluster-type""]\n            self.service_list = service_management_configuration.get_service_list(self.cluster_type)\n        else:\n            self.service_list = service_list\n        self.logger.info(""Get the service-list to manage : {0}"".format(str(self.service_list)))\n\n\n    def stop_service(self, service_conf, service_name):\n        from ..common import linux_shell\n        stop_script = ""src/{0}/deploy/{1}"".format(service_name, service_conf[""stop-script""])\n\n        cmd = ""/bin/bash {0}"".format(stop_script)\n        err_msg = ""Failed to execute the stop script of service {0}"".format(service_name)\n        self.logger.info(""Begin to execute service {0}\'s stop script."".format(service_name))\n        linux_shell.execute_shell(cmd, err_msg)\n\n    def start(self, serv):\n\n        service_conf = file_handler.load_yaml_config(""src/{0}/deploy/service.yaml"".format(serv))\n\n        self.logger.info(""----------------------------------------------------------------------"")\n        self.logger.info(""Begin to generate service {0}\'s template file"".format(serv))\n\n        service_template_generater = service_template_generate.service_template_generate(self.cluster_object_model,\n                                                                                         serv, service_conf)\n        service_template_generater.run()\n\n        self.logger.info(""Begin to stop service: [ {0} ]"".format(serv))\n        self.stop_service(service_conf, serv)\n\n        self.logger.info(""Begin to clean service\'s generated template file"".format(serv))\n        service_template_cleaner = service_template_clean.service_template_clean(serv, service_conf)\n        service_template_cleaner.run()\n\n        self.logger.info(""Successfully stop {0}"".format(serv))\n        self.logger.info(""----------------------------------------------------------------------"")\n\n    def run(self):\n        for serv in self.service_list:\n            if serv == ""cluster-configuration"":\n                continue\n            if file_handler.file_exist_or_not(""src/{0}/deploy/service.yaml"".format(serv)) == False:\n                self.logger.warning(""service.yaml can\'t be found on the directory of {0}"".format(serv))\n                self.logger.warning(""Please check your source code. The {0}\'s service will be skipped."".format(serv))\n                continue\n            self.start(serv)\n\n        if ""cluster-configuration"" in self.service_list:\n            self.start(""cluster-configuration"")\n'"
deployment/paiLibrary/paiService/service_template_clean.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport logging\nimport logging.config\n\nfrom ..common import file_handler\n\npackage_directory_serv_template_clear = os.path.dirname(os.path.abspath(__file__))\n\n\nclass service_template_clean:\n\n\n    def __init__(self, service_name, service_conf):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.service_name = service_name\n        self.service_conf = service_conf\n\n        self.template_list = None\n        if ""template-list"" in self.service_conf:\n            self.template_list = self.service_conf[""template-list""]\n\n        self.src_path = ""{0}/../../../src"".format(package_directory_serv_template_clear)\n\n\n\n    def template_cleaner(self):\n\n        self.logger.info(""Begin to delete the generated template of {0}\'s service."".format(self.service_name))\n\n        if self.template_list is None:\n            self.logger.info(""There is no generated template of {0}\'s service to be deleted."".format(self.service_name))\n            return\n\n        for template_file in self.template_list:\n            file_path = ""{0}/{1}/deploy/{2}"".format(self.src_path, self.service_name, template_file)\n            if file_handler.file_exist_or_not(file_path) == True:\n                file_handler.file_delete(file_path)\n\n        self.logger.info(""The generated template files of {0}\'s  service have been cleaned up."".format(self.service_name))\n\n\n\n    def run(self):\n\n        self.template_cleaner()'"
deployment/paiLibrary/paiService/service_template_generate.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport logging\nimport logging.config\nimport yaml\n\nfrom ..common import template_handler\nfrom ..common import file_handler\n\n\npackage_directory_serv_template_gen = os.path.dirname(os.path.abspath(__file__))\n\n\nclass service_template_generate:\n\n    def __init__(self, cluster_object_model, service_name, service_conf):\n\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_object_mode = cluster_object_model\n        self.service_name = service_name\n        self.service_conf = service_conf\n\n        self.src_path = ""{0}/../../../src"".format(package_directory_serv_template_gen)\n\n    def template_mapper(self):\n\n        # Todo: read configuration from service.yaml?\n\n        self.logger.info(""Create template mapper for service {0}."".format(self.service_name))\n\n        service_conf_dict = {\n            ""cluster_cfg"": self.cluster_object_mode\n        }\n\n        self.logger.info(""Done. Template mapper for service {0} is created."".format(self.service_name))\n\n        return service_conf_dict\n\n    # Add ""NodeAffinity"" to service deployment yaml file\n    # according to the ""deploy-rules"" in service.yaml config file\n    # Currently support ""In"" and ""NotIn"" rules or the combination of them.\n\n    def add_deploy_rule_to_yaml(self, str_src_yaml):\n        service_deploy_kind_list = [\'DaemonSet\', \'Deployment\', \'StatefulSet\', \'Pod\']\n\n        config = yaml.load(str_src_yaml, yaml.SafeLoader)\n\n        # judge whether it\'s a service deploy file, eg. exclude configmap\n        # Some service may not being configured to run, for example when alert manager is not\n        # configure, alert-manager-deployment.yaml contains nothing, and hence config is None.\n        # In this case, return original content.\n        if config is not None and \'kind\' in config and config[\'kind\'] in service_deploy_kind_list:\n            match_expressions_arr = []\n\n            deploy_rules = self.service_conf[\'deploy-rules\']\n            for rule in deploy_rules:\n                for operator, label in rule.items():\n                    match_expression = dict()\n                    if operator.lower() == \'in\':\n                        match_expression[\'operator\'] = \'In\'\n                    if operator.lower() == \'notin\':\n                        match_expression[\'operator\'] = \'NotIn\'\n\n                    match_expression[\'key\'] = label\n                    match_expression[\'values\'] = [\'true\']\n                    match_expressions_arr.append(match_expression)\n\n                config[\'spec\'][\'template\'][\'spec\'][\'affinity\'] = {\'nodeAffinity\': {\'requiredDuringSchedulingIgnoredDuringExecution\': {\'nodeSelectorTerms\': [{\'matchExpressions\': match_expressions_arr}]}}}\n        else:\n            logging.info(""It is not a service deploy file! Only support "" + str(service_deploy_kind_list))\n            return str_src_yaml\n\n        return yaml.dump(config, default_flow_style=False)\n\n    def generate_template(self):\n\n        self.logger.info(""Begin to generate the template file in service {0}\'s configuration."".format(self.service_name))\n\n        service_conf_dict = self.template_mapper()\n\n        if ""template-list"" not in self.service_conf:\n            self.logger.warning(""There is no template-list in service {0}\'s configuration."".format(self.service_name))\n            self.logger.warning(""Please check the path bootstrap/{0}/service.yaml."".format(self.service_name))\n            return\n\n        for template_file in self.service_conf[""template-list""]:\n\n            template_path = ""{0}/{1}/deploy/{2}.template"".format(self.src_path, self.service_name, template_file)\n            target_path = ""{0}/{1}/deploy/{2}"".format(self.src_path, self.service_name, template_file)\n\n            self.logger.info(""Generate the template file {0}."".format(template_path))\n            self.logger.info(""Save the generated file to {0}."".format(target_path))\n\n            template_data = file_handler.read_template(template_path)\n            try:\n                generated_template = template_handler.generate_from_template_dict(template_data, service_conf_dict)\n            except Exception as e:\n                self.logger.exception(""failed to generate template file from %s with dict %s"", template_path, service_conf_dict)\n                raise e\n\n            # judge whether it\'s a service deploy file\n            if ""deploy-rules"" in self.service_conf and template_file.find(""yaml"") >= 0 and template_file.find(""delete"") == -1:\n                generated_template = self.add_deploy_rule_to_yaml(generated_template)\n\n            file_handler.write_generated_file(target_path,  generated_template)\n\n        self.logger.info(""The template file of service {0} is generated."".format(self.service_name))\n\n    def run(self):\n\n        self.generate_template()\n'"
examples/Dockerfiles/autobuild_docker/paitest.py,0,"b""print('hello world!')\n"""
src/alert-manager/config/alert_manager.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass AlertManager(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def get_master_ip(self):\n        for host_conf in self.cluster_conf[""machine-list""]:\n            if ""pai-master"" in host_conf and host_conf[""pai-master""] == ""true"":\n                return host_conf[""hostip""]\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        if result.get(""receiver"") is not None and \\\n                result.get(""smtp_url"") is not None and \\\n                result.get(""smtp_from"") is not None and \\\n                result.get(""smtp_auth_username"") is not None and \\\n                result.get(""smtp_auth_password"") is not None:\n            result[""configured""] = True\n        else:\n            result[""configured""] = False\n        result[""host""] = self.get_master_ip()\n        result[""url""] = ""http://{0}:{1}"".format(self.get_master_ip(), result[""port""])\n\n        return result\n\n    def validation_post(self, conf):\n        port = conf[""alert-manager""].get(""port"")\n        if type(port) != int:\n            msg = ""expect port in alert-manager to be int but get %s with type %s"" % \\\n                    (port, type(port))\n            return False, msg\n        return True, None\n'"
src/authentication/config/authentication.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport re\nimport json\n\nclass Authentication:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = default_service_configuration\n        if ""OIDC"" in service_configuration:\n            self.service_configuration[""OIDC""] = service_configuration[""OIDC""]\n        if ""OIDC-type"" in service_configuration:\n            self.service_configuration[""OIDC-type""] = service_configuration[""OIDC-type""]\n        if ""AAD"" in service_configuration:\n            self.service_configuration[""AAD""] = service_configuration[""AAD""]\n        if ""group-manager"" in service_configuration:\n            self.service_configuration[""group-manager""] = service_configuration[""group-manager""]\n\n    def validate_group_schema(self, groupitem):\n        pattern = re.compile(""^[A-Za-z0-9_]+$"")\n        if bool(pattern.match(groupitem[\'groupname\'])) is False:\n            return False, ""group name should only contain alpha-numeric and underscore characters""\n        if \'extension\' in groupitem and \'acls\' in groupitem[\'extension\']:\n            if \'virtualClusters\' in groupitem[\'extension\'][\'acls\'] \\\n                    and not isinstance(groupitem[\'extension\'][\'acls\'][\'virtualClusters\'], list):\n                return False, ""group.extension.acls.virtualClusters should be list""\n            if \'admin\' in groupitem[\'extension\'][\'acls\'] \\\n                    and not isinstance(groupitem[\'extension\'][\'acls\'][\'admin\'], bool):\n                return False, ""group.extension.acls.admin should be bool""\n        return True, None\n\n    def defaulting_group_schema(self, groupitem, virtualClusters=[], admin=False):\n        if \'extension\' not in groupitem:\n            groupitem[\'extension\'] = {}\n        if \'acls\' not in groupitem[\'extension\']:\n            groupitem[\'extension\'][\'acls\'] = {}\n        if \'virtualClusters\' not in groupitem[\'extension\'][\'acls\']:\n            groupitem[\'extension\'][\'acls\'][\'virtualClusters\'] = virtualClusters\n        if \'admin\' not in groupitem[\'extension\'][\'acls\']:\n            groupitem[\'extension\'][\'acls\'][\'admin\'] = admin\n\n    def validation_pre(self):\n        validated, error_info = self.validate_group_schema(self.service_configuration[\'group-manager\'][\'admin-group\'])\n        if not validated:\n            return False, error_info\n        validated, error_info = self.validate_group_schema(self.service_configuration[\'group-manager\'][\'default-group\'])\n        if not validated:\n            return False, error_info\n        if \'grouplist\' in self.service_configuration[\'group-manager\']:\n            for groupConfig in self.service_configuration[\'group-manager\'][\'grouplist\']:\n                validated, error_info = self.validate_group_schema(groupConfig)\n                if not validated:\n                    return False, error_info\n        if self.service_configuration[""OIDC""] is False:\n            return True, None\n        if ""OIDC-type"" not in self.service_configuration:\n            return False, ""OIDC-type is missing in service-configuration.yaml->authentication""\n        if self.service_configuration[""OIDC-type""] == ""AAD"":\n            if ""tenantID"" not in self.service_configuration[""AAD""]:\n                return False, ""tenantID is missing. If you wanna configure AAD-OIDC, you should configure service-configuration.yaml->authentication->AAD->tenantID""\n            if ""clientID"" not in self.service_configuration[""AAD""]:\n                return False, ""clientID is missing. If you wanna configure AAD-OIDC, you should configure service-configuration.yaml->authentication->AAD->clientID""\n            if ""clientSecret"" not in self.service_configuration[""AAD""]:\n                return False, ""ClientSecret is missing. If you wanna configure AAD-OIDC, you should configure service-configuration.yaml->authentication->AAD->ClientSecret""\n        return True, None\n\n    def run(self):\n        self.defaulting_group_schema(self.service_configuration[\'group-manager\'][\'admin-group\'], admin=True)\n        self.service_configuration[\'group-manager\'][\'admin-group\'][\'extension\'] = json.dumps(\n            self.service_configuration[\'group-manager\'][\'admin-group\'][\'extension\'])\n        self.defaulting_group_schema(self.service_configuration[\'group-manager\'][\'default-group\'], virtualClusters=[\'default\'])\n        self.service_configuration[\'group-manager\'][\'default-group\'][\'extension\'] = json.dumps(\n            self.service_configuration[\'group-manager\'][\'default-group\'][\'extension\'])\n        if \'grouplist\' in self.service_configuration[\'group-manager\']:\n            for groupConfig in self.service_configuration[\'group-manager\'][\'grouplist\']:\n                self.defaulting_group_schema(groupConfig)\n                groupConfig[\'extension\'] = json.dumps(groupConfig[\'extension\'])\n        return self.service_configuration\n\n    def validation_post(self, cluster_object_model):\n        if self.service_configuration[""OIDC""] is False:\n            return True, None\n        if ""uri"" not in cluster_object_model[""pylon""]:\n            return False, ""property named uri is missed in pylon configuration. Please check it.""\n        return True, None\n'"
src/base-image/build/host-configure.py,0,"b'#!/usr/bin/env python\n\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport os\nimport sys\nimport subprocess\nimport jinja2\nimport argparse\n\n\n\ndef load_yaml_config(config_path):\n\n    with open(config_path, ""r"") as f:\n        cluster_data = yaml.load(f)\n\n    return cluster_data\n\n\n\ndef read_template(template_path):\n\n    with open(template_path, ""r"") as f:\n        template_data = f.read().decode(\'utf-8\')\n\n    return template_data\n\n\n\ndef generate_from_template(template_data, cluster_config, hostname):\n\n    generated_file = jinja2.Template(template_data).render(\n        {\n            ""host_config"": cluster_config[ hostname ],\n            ""cluster_config"": cluster_config\n        }\n    )\n\n    return generated_file\n\n\n\ndef write_generated_file(file_path, content_data):\n\n    with open(file_path, ""w+"") as fout:\n        fout.write(content_data)\n\n\n\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'-c\', \'--clusterconfig\', required=True, help=""cluster configuration\'s path"")\n    parser.add_argument(\'-f\', \'--targetfile\', required=True, help=""target file\'s path"")\n    parser.add_argument(\'-n\', \'--hostname\', required=True, help=""the host\'s hostname"")\n\n    args = parser.parse_args()\n\n    config_path = args.clusterconfig\n    target_path = args.targetfile\n    hostname = args.hostname\n\n    cluster_config = load_yaml_config(config_path)\n\n    target_file_origin = read_template(target_path)\n    target_file_latest = generate_from_template(target_file_origin, cluster_config, hostname)\n    write_generated_file(target_path, target_file_latest)\n\n\n\nif __name__ == ""__main__"":\n    main()'"
src/cleaner/config/cleaner.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\nimport copy\n\nclass Cleaner(object):\n\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.logger = logging.getLogger(__name__)\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        threshold = conf[""cleaner""].get(""threshold"")\n        if type(threshold) != int:\n            msg = ""expect threshold in cleaner to be int but get %s with type %s"" % \\\n                    (threshold, type(threshold))\n            return False, msg\n        else:\n            if threshold < 0 or threshold > 100:\n                msg = ""expect threshold in [0, 100]""\n                return False, msg\n            \n        interval = conf[""cleaner""].get(""interval"")\n        if type(interval) != int:\n            msg = ""expect interval in cleaner to be int but get %s with type %s"" % \\\n                    (interval, type(interval))\n            return False, msg\n        \n        return True, None\n\n'"
src/cleaner/scripts/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/cleaner/scripts/check_deleted_files.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport multiprocessing\nfrom cleaner.utils import common\n\nlogger = multiprocessing.get_logger()\n\n# This command  will output the deleted files which has been opened by a process.\n# The output of the deleted file list is like:\n# COMMAND    PID USER   FD   TYPE DEVICE SIZE/OFF NLINK     NODE NAME\n# dhclient  1008 root  txt    REG    8,1   487248     0 12320783 /sbin/dhclient (deleted)\n# python   31848 root    3w   REG    8,1        0     0 29362883 /tmp/tmp_out.txt (deleted)\n#\n# We only retrieve the PID (second column) and NAME (10th column).\nDELETED_FILES_CMD = ""lsof +L1 2>/dev/null | awk \'{print $2, $10}\'""\n\n\ndef list_and_check_files(arg, log=logger):\n    files = common.run_cmd(DELETED_FILES_CMD, log)\n    if len(files) <= 1:\n        log.info(""no deleted files found."")\n        return\n    else:\n        # skip the field names from the command\n        files = files[1:]\n\n    for f in files:\n        f_fields = f.split("" "")\n        log.warning(""process [%s] opened file [%s] but the file has been deleted."", f_fields[0], f_fields[1])\n\n\ndef main():\n    common.setup_logging()\n    logger.info(""start to check the deleted files opened by each running process."")\n    list_and_check_files(None)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/cleaner/scripts/clean_docker.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom cleaner.utils.logger import LoggerMixin\nfrom cleaner.utils.timer import CountdownTimer, Timeout\nfrom cleaner.utils import common\nfrom datetime import timedelta\nimport subprocess\nimport multiprocessing\nimport re\nimport time\nimport os\n\nclass DockerCleaner(LoggerMixin):\n    def __init__(self, threshold, interval, timeout=timedelta(hours=1)):\n        self.__threshold = int(threshold)\n        self.__interval = int(interval)\n        self.__timeout = timeout\n\n    def _exec(self):\n        exc = None\n        try:\n            with CountdownTimer(duration=self.__timeout):\n                self.check_and_clean()\n        except Timeout as e:\n            self.logger.error(""Cleaner timeout."")\n            exc = e\n        except Exception as e:\n            self.logger.error(""Unexpected error to run cleaner."")\n            exc = e\n\n        if exc is not None:\n            self.logger.exception(exc)\n\n    def run(self):\n        while True:\n            # allow a delay before the cleaning\n            time.sleep(self.__interval)\n            self._exec()\n\n\n    def check_disk_usage(self, partition):\n        df = subprocess.Popen([""df"",""-h"", partition], stdout=subprocess.PIPE)\n        sized = 0\n        try:\n            for line in df.stdout:\n                splitline = line.decode().split()\n                if splitline[5] == partition:\n                    sized = splitline[1]\n                    used = splitline[2]\n                    usep = int(splitline[4][:-1])\n        except ValueError:\n            self.logger.error(""cannot get disk size, reset size to 0"")\n            sized = 0\n            used = 0\n            usep = 0\n        self.logger.info(""Checking disk, disk usage = {0}%"".format(usep))\n        return sized, used, usep\n\n\n    def check_and_clean(self):\n        sized, used, usep = self.check_disk_usage(""/"") \n        if usep >= self.__threshold:\n            self.logger.info(""Disk usage is above {0}%, Try to remove containers"".format(self.__threshold))\n            self.kill_largest_container(sized, used, usep)\n\n\n    # Clean logic v1: kill largest container\n    white_list = [""k8s_POD"", ""k8s_kube"", ""k8s_pylon"", ""k8s_zookeeper"", ""k8s_rest-server"", ""k8s_yarn"", ""k8s_hadoop"", ""k8s_job-exporter"", ""k8s_watchdog"", ""k8s_grafana"", ""k8s_node-exporter"", ""k8s_webportal"", ""k8s_prometheus"", ""k8s_nvidia-drivers"", ""k8s_etcd-container"", ""k8s_apiserver-container"", ""k8s_docker-cleaner"", ""kubelet"", ""dev-box""]\n    def kill_largest_container(self, sized, used, usep):\n        containers = []\n        # Only try to stop PAI jobs and user created containers\n        containers_source = subprocess.Popen([""docker"", ""ps"", ""-a"", ""--format"", r\'{{.ID}}\\t{{.Image}}\\t{{.Size}}\\t{{.Names}}\\t\'], stdout=subprocess.PIPE)\n        for line in containers_source.stdout:\n            splitline = line.split(""\\t"")\n            for prefix in self.white_list:\n                if (splitline[3].startswith(prefix)):\n                    break\n            else:\n                # Only check job containers\n                if re.search(r""container(_\\w+)?_\\d+_\\d+_\\d+_\\d+$"", splitline[3]) is not None:\n                    size_str = splitline[2].split()[0]\n                    size = common.calculate_size(size_str)\n                    containers.append([size, splitline[0], splitline[1], splitline[3], size_str])\n\n        containers.sort(key=lambda x:x[0], reverse=True)\n\n        if containers.count > 0 and containers[0][0] > 1024**3:\n            self.logger.warning(""Kill container {0} due to disk pressure. Container size: {1}"".format(containers[0][3], containers[0][4]))\n            \n            # Write error log\n            container_name = re.search(r""container(_\\w+)?_\\d+_\\d+_\\d+_\\d+$"", containers[0][3]).group()\n            application_name = ""application{0}"".format(re.search(r""^_\\d+_\\d+"", re.search(r""_\\d+_\\d+_\\d+_\\d+$"", container_name).group()).group())            \n            full_path = ""/logs/{0}/{1}"".format(application_name, container_name)\n\n            if not os.path.isdir(full_path):\n                self.logger.error(""Cannot find job log dir, creating path. Log may not be collected."")\n                try:\n                    os.makedirs(full_path)\n                except OSError as exc:\n                    self.logger.error(""Failed to create path {0}."".format(full_path))\n\n            if os.path.isdir(full_path):\n                error_filename = ""{0}/diskCleaner.pai.error"".format(full_path)\n                timestamp = int(time.time())\n                try:\n                    fp = open(error_filename, ""w"")\n                except IOError:\n                    self.logger.error(""Failed to write error log, skipped"")\n                else:\n                    fp.writelines([\n                        ""{0} ERROR ACTION \\""KILL\\""\\n"".format(timestamp),\n                        ""{0} ERROR REASON \\""{1} killed due to disk pressure. Disk size: {2}, Used: {3}, Cleaner threshold: {4}, Container cost: {5} \\""\\n"".format(timestamp, container_name, sized, ""{0}({1}%)"".format(used, usep), ""{0}%"".format(self.__threshold), containers[0][4]),\n                        ""{0} ERROR SOLUTION \\""Node disk is full, please try another time. If your job needs large space, please use NAS to store data.\\""\\n"".format(timestamp)\n                        ])\n                    fp.close()\n\n            subprocess.Popen([""docker"", ""kill"", ""--signal=10"", containers[0][1]])\n\n            # Because docker stop will not immedicately stop container, we can not remove docker image right after stop container\n            #container_image = subprocess.Popen([""docker"", ""inspect"", containers[0][1], r""--format=\'{{.Image}}\'""], stdout=subprocess.PIPE).stdout.readline()\n            #subprocess.Popen([""docker"", ""image"", ""rmi"", container_image])\n            return True\n        else:\n            return False\n\n'"
src/cleaner/scripts/clean_docker_cache.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom cleaner.utils import common\nimport multiprocessing\n\nlogger = multiprocessing.get_logger()\n\n\ndef get_cache_size():\n    out = common.run_cmd(""source ./scripts/reclaimable_docker_cache.sh 2> /dev/null"", logger)\n    size = 0\n    if len(out) == 0:\n        logger.error(""cannot retrieve cache size."")\n        return size\n    try:\n        size = float(out[0])\n    except ValueError:\n        logger.error(""cannot convert cache size, reset size to 0"")\n        size = 0\n    return size\n\n\ndef check_and_clean(threshold):\n    if get_cache_size() > threshold:\n        # to avoid possible race condition, only clean the containers, images and networks created 1h ago\n        common.run_cmd(""docker system prune -af --filter until=1h"", logger)\n\n\nif __name__ == ""__main__"":\n    common.setup_logging()\n    check_and_clean(10)\n'"
src/cleaner/test/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/cleaner/test/test_scripts.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom unittest import TestCase, main\nimport mock\nimport time\nimport psutil\nimport os\nimport multiprocessing\nfrom cleaner.utils.common import setup_logging, run_cmd\nfrom cleaner.scripts import clean_docker_cache, check_deleted_files\n\nCALLED_CMD = ""docker system prune -af""\nLOGGER = multiprocessing.get_logger()\n\n\nclass TestCacheClean(TestCase):\n\n    def setUp(self):\n        setup_logging()\n\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[])\n    def testCacheEmpty(self, mock_cmd):\n        self.assertEqual(clean_docker_cache.get_cache_size(), 0)\n\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""0""])\n    def testCacheZero(self, mock_cmd):\n        self.assertEqual(clean_docker_cache.get_cache_size(), 0)\n\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""error""])\n    def testCacheError(self, mock_cmd):\n        self.assertEqual(clean_docker_cache.get_cache_size(), 0)\n\n    @mock.patch(""cleaner.scripts.clean_docker_cache.get_cache_size"", return_value=1)\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""0""])\n    def testCleanTrue(self, mock_cmd, mock_size):\n        clean_docker_cache.check_and_clean(0)\n        mock_cmd.assert_called_once_with(CALLED_CMD, LOGGER)\n\n    @mock.patch(""cleaner.scripts.clean_docker_cache.get_cache_size"", return_value=0)\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""0""])\n    def testCleanFalse(self, mock_cmd, mock_size):\n        clean_docker_cache.check_and_clean(0)\n        mock_cmd.assert_not_called()\n\n\nclass TestDeletedFiles(TestCase):\n\n    def testDeletedCmd(self):\n        test_file = ""/tmp/deleted_test.txt""\n\n        def open_and_loop():\n            with open(test_file, ""w""):\n                while True:\n                    pass\n\n        proc = multiprocessing.Process(target=open_and_loop)\n        proc.start()\n        time.sleep(1)\n        os.remove(""/tmp/deleted_test.txt"")\n        time.sleep(1)\n\n        mock_logger = mock.Mock()\n        cmd_out = run_cmd(check_deleted_files.DELETED_FILES_CMD, mock_logger)\n        files = [f.split("" "")[1] for f in cmd_out[1:]]\n        self.assertTrue(test_file in files)\n\n        proc.terminate()\n        proc.join()\n\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""PID NAME""])\n    def testDeletedCheckEmpty(self, mock_cmd):\n        mock_log = mock.Mock()\n        check_deleted_files.list_and_check_files(None, mock_log)\n        mock_log.info.assert_called_once()\n\n    @mock.patch(""cleaner.utils.common.run_cmd"", return_value=[""PID NAME"", ""1, /test""])\n    def testDeletedCheckNonEmpty(self, mock_cmd):\n        mock_log = mock.Mock()\n        check_deleted_files.list_and_check_files(None, mock_log)\n        mock_log.info.assert_not_called()\n        mock_log.warning.assert_called_once()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/cleaner/test/test_utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom cleaner.utils.logger import LoggerMixin\nfrom cleaner.utils.timer import CountdownTimer, Timeout\nfrom cleaner.utils.common import *\nfrom datetime import timedelta\nfrom unittest import TestCase, main\nimport time\nimport mock\nimport subprocess as sp\nimport signal\nimport os\nimport psutil\n\n\ndef ps_raise(procs, timeout, callback):\n    raise psutil.Error\n\n\ndef kill_process_list_mock(procs, sig, timeout, logger):\n    for p in procs:\n        kill_process(p, sig, logger)\n    time.sleep(timeout)\n    return procs\n\n\nclass UtilsTest(TestCase, LoggerMixin):\n\n    def setUp(self):\n        setup_logging()\n\n    def testLogger(self):\n        self.assertTrue(self.logger is not None, ""logger cannot be None."")\n\n    def testTimerException(self):\n        count = 0\n        with self.assertRaises(Timeout):\n            with CountdownTimer(duration=timedelta(seconds=1)):\n                while count < 3:\n                    time.sleep(1)\n                    count += 1\n\n    def testTimerExceptionSleep(self):\n        with self.assertRaises(Timeout):\n            with CountdownTimer(duration=timedelta(seconds=1)):\n                time.sleep(10)\n\n    def testTimerNoException(self):\n        no_timeout = True\n        try:\n            with CountdownTimer(duration=timedelta(seconds=3)):\n                time.sleep(1)\n        except Timeout:\n            no_timeout = False\n        self.assertTrue(no_timeout)\n\n    def testNoTimer(self):\n        no_timer = True\n        try:\n            with CountdownTimer(duration=None):\n                time.sleep(1)\n        except Timeout:\n            no_timer = False\n        self.assertTrue(no_timer)\n\n    def testRunCmdOneLine(self):\n        out = run_cmd(""echo test"", self.logger)\n        self.assertEqual(out[0], ""test"")\n\n    def testRunCmdEmptyOut(self):\n        out = run_cmd(""echo test > /dev/null"", self.logger)\n        self.assertEqual(len(out), 0)\n\n    def testTerminateProcess(self):\n        proc = sp.Popen([""/bin/bash"", ""-c"", ""sleep 3600""])\n        kill_process(proc, signal.SIGTERM, self.logger)\n        time.sleep(1)\n        self.assertEqual(proc.poll(), -signal.SIGTERM)\n\n    def testKillProcess(self):\n        proc = sp.Popen([""/bin/bash"", ""-c"", ""sleep 3600""])\n        kill_process(proc, signal.SIGKILL, self.logger)\n        time.sleep(1)\n        self.assertEqual(proc.poll(), -signal.SIGKILL)\n\n    def testKillProcessList(self):\n        procs = []\n        procs.append(sp.Popen([""/bin/bash"", ""-c"", ""sleep 3600""]))\n        procs.append(sp.Popen([""/bin/bash"", ""-c"", ""sleep 3600""]))\n\n        ps_procs = [psutil.Process(p.pid) for p in procs]\n        alive = kill_process_list(ps_procs, signal.SIGTERM, 1, self.logger)\n        self.assertEqual(len(alive), 0)\n        self.assertTrue(procs[0].poll() is not None)\n        self.assertTrue(procs[1].poll() is not None)\n\n    @mock.patch(""psutil.wait_procs"", side_effect=ps_raise)\n    def testKillProcessListError(self, mock_wait):\n        proc = sp.Popen([""/bin/bash"", ""-c"", ""sleep 1200""])\n        ps_procs = [psutil.Process(proc.pid)]\n        alive = kill_process_list(ps_procs, signal.SIGTERM, 1, self.logger)\n        mock_wait.assert_called_once()\n        self.assertEqual(ps_procs, alive)\n        self.assertTrue(proc.poll() is not None)\n\n    def testKillProcessTree(self):\n        test_shell = ""#!/bin/bash \\n"" \\\n                     ""# create a background process as child \\n"" \\\n                     ""sleep 1000 & \\n"" \\\n                     ""# wait to block the foreground process \\n"" \\\n                     ""sleep 1000 \\n""\n        with open(""/tmp/subprocess.sh"", ""w"") as sh:\n            sh.write(test_shell)\n        proc = sp.Popen([""/bin/bash"", ""/tmp/subprocess.sh""])\n        time.sleep(1)\n        subproc = psutil.Process(proc.pid).children(recursive=True)\n        self.assertTrue(len(subproc) == 2)\n\n        kill_process_tree(proc.pid, 1, self.logger)\n        gone, alive = psutil.wait_procs(subproc, timeout=1)\n        self.assertTrue(len(alive) == 0)\n        self.assertTrue(proc.poll() is not None)\n\n    @mock.patch(""cleaner.utils.common.kill_process_list"", side_effect=kill_process_list_mock)\n    def testKillProcessTreeError(self, mock_kill):\n        proc = sp.Popen([""/bin/bash"", ""-c"", ""sleep 1200""])\n        kill_process_tree(proc.pid, 1, self.logger)\n        self.assertTrue(mock_kill.call_count == 2)\n        self.assertTrue(proc.poll() is not None)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/cleaner/test/test_worker.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom cleaner.worker import Worker\nfrom cleaner.utils import common\nimport datetime\nimport time\nfrom multiprocessing import Queue\nfrom unittest import TestCase, main\n\n\ndef called_by_worker(queue):\n    queue.put(1)\n\n\ndef timeout_worker(queue):\n    time.sleep(2)\n    queue.put(1)\n\n\nclass TestWorker(TestCase):\n\n    def setUp(self):\n        common.setup_logging()\n\n    def testWorkerRunOnce(self):\n        queue = Queue()\n        worker = Worker(called_by_worker, queue, long_run=False)\n        worker.start()\n        worker.join()\n        data = queue.get(timeout=2)\n        self.assertEqual(data, 1)\n\n    def testWorkerLongRun(self):\n        queue = Queue()\n        worker = Worker(called_by_worker, queue, cool_down_time=0.1)\n        worker.start()\n        time.sleep(3)\n        worker.terminate()\n        worker.join()\n        self.assertTrue(queue.qsize() > 1)\n\n    def testWorkerTimeout(self):\n        queue = Queue()\n        worker = Worker(timeout_worker, queue, long_run=False, timeout=datetime.timedelta(seconds=1))\n        worker.start()\n        worker.join()\n        self.assertEqual(queue.qsize(), 0)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/cleaner/utils/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/cleaner/utils/common.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport subprocess\nimport multiprocessing\nimport logging\nimport sys\nimport os\nimport psutil\nimport signal\nimport re\n\ndef kill_process_tree(pid, time_to_die, logger):\n    """"""\n    Kills a process and all its subprocesses in best effort.\n    The processes are first killed by sending SIGTERM.\n    If they cannot terminate in time_to_die seconds.\n    They will be killed by sending SIGKILL.\n\n    :param pid: id of process to be killed\n    :param time_to_die: the time period in which the process should terminate\n    :param logger: logger handler\n    """"""\n    if os.getpid() == pid:\n        logger.error(""I refuse to kill myself."")\n        return\n\n    try:\n        process = psutil.Process(pid)\n        processes = process.children(recursive=True)\n        processes.append(process)\n    except psutil.Error as e:\n        logger.error(""cannot get process %s and its subprocesses."", pid)\n        logger.exception(e)\n        return\n\n    alive = kill_process_list(processes, signal.SIGTERM, time_to_die, logger)\n\n    if alive:\n        # the processes survive SIGTERM so try to kill them by SIGKILL\n        alive = kill_process_list(alive, signal.SIGKILL, time_to_die, logger)\n        if alive:\n            for p in alive:\n                logger.error(""Process %s cannot be killed."", p.pid)\n\n\ndef kill_process_list(processes, sig, time_to_die, logger):\n    def on_kill(proc):\n        logger.info(""process %s is killed, exit code %s"", proc.pid, proc.returncode)\n\n    for p in processes:\n        kill_process(p, sig, logger)\n\n    try:\n        gone, alive = psutil.wait_procs(processes, timeout=time_to_die, callback=on_kill)\n    except psutil.Error as e:\n        logger.error(""error to wait the processes to terminate."")\n        logger.exception(e)\n        alive = processes\n    return alive\n\n\ndef kill_process(process, sig, logger):\n    """"""\n    kill a process by sending signal.\n\n    :param process: process to kill\n    :param sig: the signal\n    :param logger: logger handler\n    """"""\n    try:\n        logger.info(""kill process %s by sending %s"", process.pid, sig)\n        os.kill(process.pid, sig)\n    except Exception as e:\n        logger.error(""error to send %s to process %s."", sig, process.pid)\n        logger.exception(e)\n\n\ndef run_cmd(cmd, logger):\n    """"""\n    Runs a given command and returns its output. If exceptions occur and the command process is still running.\n    The command process and all its subprocesses will be terminated in best effort.\n\n    :param cmd: the command to run\n    :param logger: logger handler\n    :return the output of the command\n    """"""\n    proc = subprocess.Popen([""/bin/bash"", ""-c"", cmd], stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    lines = []\n    try:\n        while True:\n            line = proc.stdout.readline()\n            if not line:\n                break\n            line = line.encode(""UTF-8"").strip()\n            logger.info(""output from command [%s] : %s"", cmd, line)\n            lines.append(line)\n        proc.wait()\n        if proc.returncode:\n            logger.error(""failed to run command %s, error code is %s"", cmd, proc.returncode)\n    finally:\n        if proc.poll() is None:\n            # the process is till running and terminate it before exit\n            logger.error(""process %s is not completed and will terminate it before exit."", proc.pid)\n            kill_process_tree(proc.pid, 2, logger)\n\n    return lines\n\n\ndef setup_logging():\n    logger = multiprocessing.get_logger()\n    if len(logger.handlers) == 0:\n        handler = logging.StreamHandler(sys.stdout)\n        formatter = logging.Formatter(""%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s"")\n        handler.setFormatter(formatter)\n        logger.addHandler(handler)\n        logger.setLevel(logging.INFO)\n\n\nsize_defs={\'B\':1, \'K\':1024, \'M\':1024**2, \'G\':1024**3, \'T\':1024**4, \'b\':1, \'k\':1024, \'m\':1024**2, \'g\':1024**3, \'t\':1024**4}\ndef calculate_size(size_str):\n    size_search = re.search(r""[BbKkMmGgTt]"", size_str)\n    return float(size_str[0:size_search.start()]) * size_defs[size_search.group()]'"
src/cleaner/utils/logger.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport multiprocessing\n\n\nclass LoggerMixin(object):\n    """"""\n    This mixin is to add a logger property conveniently to classes derived from it.\n    The usage is like:\n\n        class A(LoggerMixin):\n            def do_something():\n                self.logger().info(""log message"")\n    """"""\n\n    @property\n    def logger(self):\n        try:\n            if self._logger is None:\n                self._logger = self._get_logger()\n        except AttributeError:\n            self._logger = self._get_logger()\n        return self._logger\n\n    def _get_logger(self):\n        return multiprocessing.get_logger().getChild(""."".join([self.__class__.__module__, self.__class__.__name__]))\n'"
src/cleaner/utils/timer.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport signal\nimport time\nfrom datetime import timedelta\nfrom cleaner.utils.logger import LoggerMixin\n\n\nclass Timeout(Exception):\n    pass\n\n\nclass CountdownTimer(LoggerMixin):\n    """"""\n    This class is to set a countdown with the given time. It will raise exceptions when the time is out.\n    """"""\n\n    def __init__(self, duration=timedelta(hours=1), name=""countdown_timer""):\n        self.duration_in_seconds = int(duration.total_seconds()) if duration else 0\n        self.name = name\n        self.enter_time = 0\n\n    def __enter__(self):\n        if self.duration_in_seconds == 0:\n            return\n\n        try:\n            signal.signal(signal.SIGALRM, self.on_alarm)\n            signal.alarm(self.duration_in_seconds)\n            self.logger.info(""setup countdown timer %s with duration %d"" % (self.name, self.duration_in_seconds))\n            self.enter_time = time.time()\n        except ValueError as e:\n            self.logger.error(""Failed to setup countdown timer %s"", self.name)\n            self.logger.exception(e)\n\n    def __exit__(self, type, value, traceback):\n        if self.duration_in_seconds == 0:\n            return\n\n        try:\n            signal.alarm(0)\n            self.logger.info(""exit the countdown timer %s after %d seconds"" % (self.name, time.time() - self.enter_time))\n        except ValueError as e:\n            self.logger.error(""Failed to setup countdown time %s"", self.name)\n            self.logger.exception(e)\n\n    def on_alarm(self, signum, frame):\n        self.logger.error(""%s : the maximum time duration %d reached and will exit."", self.name, self.duration_in_seconds)\n        raise Timeout()\n'"
src/cluster/config/cluster.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport sys\nimport subprocess\nimport logging\nimport logging.config\n\n\nclass Cluster:\n\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = self.merge_service_configuration(service_configuration, default_service_configuration)\n\n\n\n    def merge_service_configuration(self, overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg == None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            v = overwrite_srv_cfg[k]\n            if (k in srv_cfg and isinstance(overwrite_srv_cfg[k], dict) and isinstance(srv_cfg[k], dict)):\n                srv_cfg[k] = self.merge_service_configuration(overwrite_srv_cfg[k], srv_cfg[k])\n            else:\n                srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n\n\n    def validation_common(self, common_configuration):\n        if ""cluster-id"" not in common_configuration:\n            return False, ""cluster-id is missing in service-configuration.yaml -> cluster -> common -> cluster-id""\n        if ""cluster-type"" not in common_configuration or common_configuration[""cluster-type""] not in [""k8s""]:\n            return False, ""cluster-type is not defined or invalid. Please check service-configuration.yaml -> cluster -> common -> cluster-type""\n        if ""data-path"" not in common_configuration:\n            return False, ""data-path is missing in service-configuration.yaml -> cluster -> common -> data-path""\n        if ""job-history"" not in common_configuration:\n            return False, ""job-history is missing in service-configuration.yaml -> cluster -> common -> job-history""\n        if ""qos-switch"" not in common_configuration:\n            return False, ""qos-switch is missing in service-configuration.yaml -> cluster -> common -> qos-switch""\n        if ""az-rdma"" not in common_configuration:\n            return False, ""az-rdma is missing in service-configuration.yaml -> cluster -> common -> az-rdma""\n        if ""k8s-rbac"" not in common_configuration:\n            return False, ""k8s-rbac is missing in service-configuration.yaml -> cluster -> common -> k8s-rbac""\n        if ""deploy-in-aks"" not in common_configuration:\n            return False, ""deploy-in-aks is missing in service-configuration.yaml -> cluster ->common -> deploy-in-aks""\n        return True, None\n\n\n\n    def validation_docker_resgitry(self, docker_reg_configuration):\n        if ""namespace"" not in docker_reg_configuration:\n            return False, ""namespace is miss in service-configuration.yaml -> cluster -> docker-registry -> namespace""\n        if ""domain"" not in docker_reg_configuration:\n            return False, ""domain is miss in service-configuration.yaml -> cluster -> docker-registry -> domain""\n        if ""tag"" not in docker_reg_configuration:\n            return False, ""tag is miss in service-configuration.yaml -> cluster -> docker-registry -> tag""\n        if ""secret-name"" not in docker_reg_configuration:\n            return False, ""secret-name is miss in service-configuration.yaml -> cluster -> docker-registry -> secret-name""\n        if (""username"" in docker_reg_configuration) is not (""password"" in docker_reg_configuration):\n            return False, ""username and password should be coexist, or please comment all of them.""\n        return True, None\n\n\n\n    def validation_pre(self):\n        if ""common"" not in self.service_configuration:\n            return False, ""common is miss in service-configuration.yaml -> cluster -> common""\n        if ""docker-registry"" not in self.service_configuration:\n            return False, ""docker-registry is miss in service-configuration.yaml -> cluster -> docker-registry""\n\n        ok, msg = self.validation_common(self.service_configuration[""common""])\n        if ok is False:\n            return False, msg\n        ok, msg = self.validation_docker_resgitry(self.service_configuration[""docker-registry""])\n        if ok is False:\n            return False, msg\n\n        return True, None\n\n\n\n    def execute_shell(self, shell_cmd, error_msg):\n        try:\n            subprocess.check_call(shell_cmd, shell=True)\n\n        except subprocess.CalledProcessError:\n            self.logger.error(error_msg)\n            raise Exception(""Failed to execute the command [{0}]"".format(shell_cmd))\n\n\n\n    def execute_shell_with_output(self, shell_cmd, error_msg):\n        try:\n            res = subprocess.check_output(shell_cmd, shell=True)\n\n        except subprocess.CalledProcessError:\n            self.logger.error(error_msg)\n            raise Exception(""Failed to execute the command [{0}]"".format(shell_cmd))\n\n        return res\n\n\n\n    def login_docker_registry(self, docker_registry, docker_username, docker_password):\n        shell_cmd = ""docker login -u {0} -p {1} {2}"".format(docker_username, docker_password, docker_registry)\n        error_msg = ""docker registry login error""\n        self.execute_shell(shell_cmd, error_msg)\n        self.logger.info(""docker registry login successfully"")\n\n\n\n    def generate_secret_base64code(self, docker_registry_configuration):\n        domain = docker_registry_configuration[""domain""] and str(docker_registry_configuration[""domain""])\n        username = docker_registry_configuration[""username""] and str(docker_registry_configuration[""username""])\n        passwd = docker_registry_configuration[""password""] and str(docker_registry_configuration[""password""])\n\n        if domain == ""public"":\n            domain = """"\n\n        if username and passwd:\n            self.login_docker_registry(domain, username, passwd)\n\n            base64code = self.execute_shell_with_output(\n                ""cat ~/.docker/config.json | base64"",\n                ""Failed to base64 the docker\'s config.json""\n            )\n        else:\n            self.logger.info(""docker registry authentication not provided"")\n\n            base64code = ""{}"".encode(""base64"")\n\n        docker_registry_configuration[""base64code""] = base64code.replace(""\\n"", """")\n\n\n\n    def generate_docker_credential(self, docker_registry_configuration):\n        username = docker_registry_configuration[""username""] and str(docker_registry_configuration[""username""])\n        passwd = docker_registry_configuration[""password""] and str(docker_registry_configuration[""password""])\n\n        if username and passwd:\n            credential = self.execute_shell_with_output(\n                ""cat ~/.docker/config.json"",\n                ""Failed to get the docker\'s config.json""\n            )\n        else:\n            credential = ""{}""\n\n        docker_registry_configuration[""credential""] = credential\n\n\n\n    def generate_image_url_prefix(self, docker_registry_configuration):\n        domain = str(docker_registry_configuration[""domain""])\n        namespace = str(docker_registry_configuration[""namespace""])\n\n        if domain != ""docker.io"":\n            prefix = ""{0}/{1}/"".format(domain, namespace)\n        else:\n            prefix = ""{0}/"".format(namespace)\n\n        docker_registry_configuration[""prefix""] = prefix\n\n\n\n    def run(self):\n        cluster_com = self.service_configuration\n\n        self.generate_image_url_prefix(cluster_com[""docker-registry""])\n        if ""username"" not in cluster_com[""docker-registry""]:\n            cluster_com[""docker-registry""][""username""] = None\n        if ""password"" not in cluster_com[""docker-registry""]:\n            cluster_com[""docker-registry""][""password""] = None\n\n        try:\n            self.generate_secret_base64code(cluster_com[""docker-registry""])\n            self.generate_docker_credential(cluster_com[""docker-registry""])\n        except Exception as e:\n            self.logger.warning(""Failed to generate docker credential and base64code."")\n            self.logger.warning(""Please confirm docker is installed in your host."")\n\n        return cluster_com\n\n\n\n    def validation_post(self, cluster_object_model):\n        com = cluster_object_model\n        return True, None\n'"
src/device-plugin/config/device_plugin.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nclass DevicePlugin:\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        if \'devices\' not in self.service_conf:\n            self.service_conf[\'devices\'] = self.default_service_conf[\'devices\']\n        return True, None\n\n    def run(self):\n        return self.service_conf\n\n    def validation_post(self, conf):\n        return True, None\n'"
src/drivers/config/drivers.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass Drivers:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = self.merge_service_configuration(service_configuration, default_service_configuration)\n\n\n\n    def merge_service_configuration(self, overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg == None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            v = overwrite_srv_cfg[k]\n            if (k in srv_cfg and isinstance(overwrite_srv_cfg[k], dict) and isinstance(srv_cfg[k], dict)):\n                srv_cfg[k] = self.merge_service_configuration(overwrite_srv_cfg[k], srv_cfg[k])\n            else:\n                srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n\n\n    def validation_pre(self):\n        if ""set-nvidia-runtime"" not in self.service_configuration:\n            return False, ""set-nvidia-runtime is miss in service-configuration -> drivers.""\n        if self.service_configuration[""set-nvidia-runtime""] not in [False, True]:\n            return False, ""Value of set-nvidia-runtme should be false or true.""\n        if self.service_configuration[""enable-ib-installation""] not in [False, True]:\n            return False, ""Value of enable-ib-installation should be false or true.""\n        if ""version"" not in self.service_configuration:\n            return False, ""version is miss in service-configuration -> drivers.""\n        if self.service_configuration[""version""] not in [""384.111"", ""390.25"", ""410.73"", ""418.56""]:\n            return False, ""Value of version in drivers should be [384.111, 390.25, 410.73, 418.56].""\n        return True, None\n\n\n\n    def run(self):\n        drivers_com = self.service_configuration\n        return drivers_com\n\n\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/etcd-upgrade/build/upgrade.py,0,"b'#!/usr/bin/env python\n\nimport copy\nimport sys\nimport yaml\n\nlabels = {""app"": ""etcd-server""}\n\nprobe = {""httpGet"": {""path"": ""/health"", ""port"": 4001},\n         ""initialDelaySeconds"": 10,\n         ""periodSeconds"": 30,\n         ""timeoutSeconds"": 10}\n\ndef add_fields(obj):\n    obj = copy.deepcopy(obj)\n    assert obj[""apiVersion""] == ""v1""\n    assert obj[""kind""] == ""Pod""\n    assert obj[""metadata""][""name""] == ""etcd-server""\n    obj[""metadata""][""labels""] = labels\n    obj[""spec""][""containers""][0][""readinessProbe""] = probe\n    return obj\n\nif __name__ == \'__main__\':\n    print(yaml.dump(add_fields(yaml.load(sys.stdin))))\n'"
src/fluentd/config/fluentd.py,0,"b'#!/usr/bin/env python\nimport logging\n\n\nclass Fluentd(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.logger = logging.getLogger(__name__)\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        return {}\n\n    def validation_post(self, conf):\n        if \'job-history\' in conf[\'cluster\'][\'common\'] and conf[\'cluster\'][\'common\'][\'job-history\'] != ""false"":\n            if conf[\'postgresql\'][\'enable\'] is False:\n                return False, ""You must set postgresql.enable=true to use job history.""\n        return True, None\n'"
src/grafana/config/grafana.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass Grafana(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def get_master_ip(self):\n        for host_conf in self.cluster_conf[""machine-list""]:\n            if ""pai-master"" in host_conf and host_conf[""pai-master""] == ""true"":\n                return host_conf[""hostip""]\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        result[""url""] = ""http://{0}:{1}"".format(self.get_master_ip(), result[""port""])\n        return result\n\n    def validation_post(self, conf):\n        port = conf[""grafana""].get(""port"")\n        if type(port) != int:\n            msg = ""expect port in grafana to be int but get %s with type %s"" % \\\n                    (port, type(port))\n            return False, msg\n        return True, None\n'"
src/grafana/deploy/minify.py,0,"b'#!/usr/bin/env python\n\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport json\nimport sys\n\ndef minify(raw):\n    """""" minify json file """"""\n    obj = json.loads(raw)\n    return json.dumps(obj, separators=("","", "":""))\n\nif __name__ == \'__main__\':\n    sys.stdout.write(minify(sys.stdin.read()))\n'"
src/hadoop-batch-job/config/hadoop_batch_job.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopBatchJob:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        com = {}\n        return com\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/hadoop-data-node/config/hadoop_data_node.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopDataNode:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = self.merge_service_configuration(service_configuration,\n                                                                      default_service_configuration)\n\n    def merge_service_configuration(self, overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg is None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        com = {}\n        # com[""storage_path""] = self.service_configuration.get(""storage_path"") or \\\n        #                       ""{}/hdfs/data"".format(self.cluster_configuration[""cluster""][""common""][""data-path""])\n        com[""storage_path""] = self.service_configuration.get(""storage_path"")\n        return com\n\n    def validation_post(self, cluster_object_model):\n        if ""master-ip"" not in cluster_object_model[""hadoop-name-node""]:\n            return False, ""No hdfs master ip found""\n        return True, None\n\n'"
src/hadoop-jobhistory/config/hadoop_jobhistory.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopJobhistory:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n\n    def validation_pre(self):\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                return True, None\n\n        return False, ""No master node found in machine list""\n\n    def run(self):\n        com = {\n            ""log-server-ip"": None,\n            ""timeline-server-ip"": None,\n               }\n\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                com[""log-server-ip""] = host_config[""hostip""]\n                com[""timeline-server-ip""] = host_config[""hostip""]\n                break\n\n        return com\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/hadoop-name-node/config/hadoop_name_node.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopNameNode:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n\n\n    def validation_pre(self):\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                return True, None\n\n        return False, ""No master node found in machine list""\n\n\n\n    def run(self):\n        com = {""master-ip"": None}\n\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                com[""master-ip""] = host_config[""hostip""]\n                break\n\n        return com\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/hadoop-node-manager/config/hadoop_node_manager.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopNodeManager:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        com = {}\n\n        return com\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/hadoop-resource-manager/config/hadoop_resource_manager.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass HadoopResourceManager:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = self.merge_service_configuration(service_configuration, default_service_configuration)\n\n    def merge_service_configuration(self, overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg == None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n    def validation_pre(self):\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                break\n        else:\n            return False, ""No master node found in machine list""\n\n        total_capacity = 0\n        virtual_clusters_config = self.service_configuration[""virtualClusters""]\n        for vc_name in virtual_clusters_config:\n            if virtual_clusters_config[vc_name][""capacity""] < 0:\n                return False, ""Capacity of VC \'%s\' (=%f) should be a positive number."" % (vc_name, virtual_clusters_config[vc_name][""capacity""])\n            total_capacity += virtual_clusters_config[vc_name][""capacity""]\n        if total_capacity != 100:\n            return False, ""Total vc capacity doesn\'t equal to 100""\n\n        return True, None\n\n\n    def run(self):\n        com = {}\n\n        com[""yarn_exporter_port""] = self.service_configuration[""yarn_exporter_port""]\n        com[""yarn_log_retain_seconds""] = self.service_configuration[""yarn_log_retain_seconds""]\n\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                com[""master-ip""] = host_config[""hostip""]\n                break\n\n        virtual_clusters_config = self.service_configuration[""virtualClusters""]\n\n        hadoop_queues_config = {}\n        for vc_name in virtual_clusters_config:\n            hadoop_queues_config[vc_name] = {\n                ""description"": virtual_clusters_config[vc_name][""description""],\n                ""weight"": float(virtual_clusters_config[vc_name][""capacity""])\n            }\n\n        com[""virtualClusters""] = virtual_clusters_config\n        com[""hadoopQueues""] = hadoop_queues_config\n\n        return com\n\n    def validation_post(self, cluster_object_model):\n        return True, None\n\n'"
src/hivedscheduler/config/hivedscheduler.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\n\nclass Hivedscheduler:\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = dict(default_service_conf, **service_conf)\n\n    def validation_pre(self):\n        if \'webservice-port\' not in self.service_conf:\n            return False, \'webservice-port is missing in hivedscheduler service configuration\'\n        if \'config\' not in self.service_conf:\n            self.service_conf[\'config\'] = \'\'\n        return True, None\n\n    def run(self):\n        self.service_conf[\'structured-config\'] = {}\n        if self.service_conf[\'config\'] != \'\':\n            self.service_conf[\'structured-config\'] = yaml.load(self.service_conf[\'config\'], yaml.SafeLoader)\n        machine_list = self.cluster_conf[\'machine-list\']\n        master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n        self.service_conf[\'webservice\'] = \'http://{}:{}\'.format(master_ip, self.service_conf[\'webservice-port\'])\n        self.service_conf[\'config\'] = self.service_conf[\'config\'].replace(\'\\n\', \'\\n    \')\n        return self.service_conf\n\n    def validation_post(self, conf):\n        return True, None\n'"
src/internal-storage/config/internal_storage.py,0,"b'#!/usr/bin/env python\nimport copy\nimport logging\n\n\nclass InternalStorage(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = self.merge_service_configuration(service_conf, default_service_conf)\n        self.logger = logging.getLogger(__name__)\n\n    @staticmethod\n    def merge_service_configuration(overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg is None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n    def validation_pre(self):\n        if self.service_conf[\'enable\']:\n            type_ = self.service_conf.get(\'type\', \'\')\n            if type_ == \'hostPath\':\n                machine_list = self.cluster_conf[\'machine-list\']\n                if len([host for host in machine_list if host.get(\'pai-master\') == \'true\']) < 1:\n                    return False, \'""pai-master=true"" machine is required to deploy the internal storage\'\n                quotaGB = int(self.service_conf[\'quota-gb\'])\n                assert quotaGB >= 1\n                return True, None\n            else:\n                return False, \'Unknown internal storage type {}\'.format(type_)\n        else:\n            return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.service_conf)\n        if result[\'enable\']:\n            machine_list = self.cluster_conf[\'machine-list\']\n            master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n            result[\'master-ip\'] = master_ip\n            result[\'quota-gb\'] = int(result[\'quota-gb\'])\n        return result\n\n    def validation_post(self, conf):\n        return True, None\n'"
src/job-exit-spec/config/job_exit_spec.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass JobExitSpec(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        return True, None\n'"
src/job-exit-spec/config/update_markdown.py,0,"b'#!/usr/bin/env python3\n\nimport sys\n\nimport yaml\n\n\ndef escape(s):\n    return str(s) \\\n        .replace(\'<\', \'\\<\') \\\n        .replace(\'>\', \'\\>\') \\\n        .replace(\'|\', \'\\|\') \\\n        .replace(\'\\r\\n\', \'<br>\') \\\n        .replace(\'\\r\', \'<br>\') \\\n        .replace(\'\\n\', \'<br>\')\n\n\ndef bold(s):\n    return \'**\' + str(s) + \'**\'\n\n\ndef get(dic, key):\n    if key in dic:\n        value = dic[key]\n\n        if type(value) == list:\n            rows = \'\'\n            row_id = 1\n            for row in value:\n                rows += str(row_id) + \'. \' + escape(row) + \'<br>\'\n                row_id += 1\n            return rows\n\n        if type(value) == dict:\n            rows = \'\'\n            for row_key, row_value in sorted(value.items()):\n                rows += escape(row_key) + \': \' + escape(row_value) + \'<br>\'\n            return rows\n\n        return escape(value)\n    else:\n        return \'\'\n\n\ndef update_markdown():\n    sys.stdout = open(""job-exit-spec.md"", ""w"")\n    with open(\'job-exit-spec.yaml\', \'r\') as stream:\n        data = yaml.safe_load(stream)\n\n    schema = data[\'schema\']\n    spec = data[\'spec\']\n\n    print(\'# PAI Job Exit Spec\')\n    print(\'1. See details in [job-exit-spec.yaml](job-exit-spec.yaml)\')\n    print(\'2. This markdown file is generated by [update_markdown.py](update_markdown.py) with [job-exit-spec.yaml](job-exit-spec.yaml)\')\n    print(\'3. See full doc in [PAI Job Exit Spec User Manual](user-manual.md)\')\n    print(\'\')\n\n    print(\'## Spec Schema\')\n    print(\'|field|description|required|unique|type|range|\')\n    print(\'|-----|-----------|--------|------|----|----|\')\n    for field in schema:\n        print(\'|\', bold(get(field, \'field\')), \'|\',\n              get(field, \'description\'), \'|\',\n              get(field, \'required\'), \'|\',\n              get(field, \'unique\'), \'|\',\n              get(field, \'type\'), \'|\',\n              get(field, \'range\'), \'|\')\n    print(\'\')\n\n    print(\'## Spec Table\')\n    print(\'1. You may need to **scroll right side to see full table**.\')\n    print(\'2. The code **256** is just used to represent all **undefined \'\n          \'positive** exitcodes in this spec, and the specific undefined exitcode \'\n          \'will always override it to expose to user.\')\n    print(\'3. The code **-8000** is just used to represent all **undefined \'\n          \'negative** exitcodes in this spec, and the specific undefined exitcode \'\n          \'will always override it to expose to user.\')\n    print(\'\')\n    print(\'|code|phrase|issuer|causer|type|stage|behavior|reaction|reason|repro|solution|pattern|\')\n    print(\'|----|------|------|------|----|-----|--------|--------|------|-----|--------|-------|\')\n    for code in spec:\n        print(\'|\', bold(get(code, \'code\')), \'|\',\n              bold(get(code, \'phrase\')), \'|\',\n              get(code, \'issuer\'), \'|\',\n              get(code, \'causer\'), \'|\',\n              get(code, \'type\'), \'|\',\n              get(code, \'stage\'), \'|\',\n              get(code, \'behavior\'), \'|\',\n              get(code, \'reaction\'), \'|\',\n              get(code, \'reason\'), \'|\',\n              get(code, \'repro\'), \'|\',\n              get(code, \'solution\'), \'|\',\n              get(code, \'pattern\'), \'|\')\n    print(\'\')\n\n\ndef main():\n    update_markdown()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
src/job-exporter/config/job_exporter.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass JobExporter(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        port = conf[""job-exporter""].get(""port"")\n        if type(port) != int:\n            msg = ""expect port in job-exporter to be int but get %s with type %s"" % \\\n                    (port, type(port))\n            return False, msg\n        level = conf[""job-exporter""].get(""logging-level"")\n        if level not in {""DEBUG"", ""INFO"", ""WARNING""}:\n            msg = ""expect logging-level in job-exporter to be {\'DEBUG\', \'INFO\', \'WARNING\'} but got %s"" % \\\n                    (level)\n            return False, msg\n        return True, None\n'"
src/job-exporter/src/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
src/job-exporter/src/amd.py,0,"b'from dataclasses import dataclass\nimport json\nimport logging\nimport subprocess\n\nimport utils\n\nLOGGER = logging.getLogger(__name__)\n\n\n@dataclass\nclass AMDGpuStatus:\n    gpu_util: float\n    gpu_mem_util: float\n    uuid: str\n    temperature: float\n    pci_addr: str\n\n\ndef rocm_smi(histogram, timeout):\n    try:\n        smi_output = utils.exec_cmd(\n            [""rocm-smi"", ""--showmeminfo"", ""all"", ""-a"", ""--json""],\n            histogram=histogram,\n            timeout=timeout)\n\n        return parse_smi_json_result(smi_output)\n    except subprocess.CalledProcessError as e:\n        LOGGER.exception(""command \'%s\' return with error (code %d): %s"", e.cmd,\n                         e.returncode, e.output)\n    except subprocess.TimeoutExpired:\n        LOGGER.warning(""rocm-smi timeout"")\n    except Exception:\n        LOGGER.exception(""exec rocm-smi error"")\n\n    return None\n\n\ndef parse_smi_json_result(smi_output):\n    """""" return a map, key is PCI bus index, value is AMDGpuStatus """"""\n    res = {}\n    output = json.loads(smi_output)\n    gpu_infos = [v for k, v in output.items() if k.startswith(""card"")]\n    values = sorted(gpu_infos, key=lambda v: v[""PCI Bus""])\n    for index, value in enumerate(values):\n        gpu_util = float(value[""GPU use (%)""])\n        gpu_mem_vram_total = float(value[""vram Total Memory (B)""])\n        gpu_mem_vram_used = float(value[""vram Total Used Memory (B)""])\n        gpu_mem_vram_util = gpu_mem_vram_used / gpu_mem_vram_total * 100\n        gpu_temperature = float(value[""Temperature (Sensor edge) (C)""])\n        gpu_uuid = str(value[""Unique ID""]).strip()\n        pci_addr = value[""PCI Bus""]\n        res[str(index)] = AMDGpuStatus(gpu_util, gpu_mem_vram_util, gpu_uuid,\n                                       gpu_temperature, pci_addr)\n\n    return res\n'"
src/job-exporter/src/collector.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport re\nimport datetime\nimport logging\nimport threading\nimport subprocess\nimport time\nimport copy\nimport os\nimport collections\n\nfrom prometheus_client import make_wsgi_app, Counter, Gauge, Histogram\nfrom prometheus_client.core import GaugeMetricFamily\n\nimport amd\nimport network\nimport docker_inspect\nimport docker_stats\nimport nvidia\nimport ps\nimport utils\nfrom utils import GpuVendor\n\nlogger = logging.getLogger(__name__)\n\n\n##### collector will generate following metrics\n# Document about these metrics is in `` # TODO\n\niteration_counter = Counter(""collector_iteration_count"", ""total number of iteration"",\n        [""name""])\n\ndef gen_docker_daemon_counter():\n    return GaugeMetricFamily(""docker_daemon_count"",\n            ""count of docker daemon"",\n            labels=[""error""])\n\n# GPU Common metrics\ndef gen_gpu_util_gauge():\n    return GaugeMetricFamily(""gpu_utilization"",\n                             ""gpu core utilization of card"",\n                             labels=[""minor_number"", ""vender""])\n\n\ndef gen_gpu_mem_util_gauge():\n    return GaugeMetricFamily(""gpu_mem_utilization"",\n                             ""gpu memory utilization of card"",\n                             labels=[""minor_number"", ""vender""])\n\n# NVIDIA GPU metrics\ndef gen_nvidia_gpu_util_gauge():\n    return GaugeMetricFamily(""nvidiasmi_utilization_gpu"",\n            ""gpu core utilization of card"",\n            labels=[""minor_number""])\n\ndef gen_nvidia_gpu_mem_util_gauge():\n    return GaugeMetricFamily(""nvidiasmi_utilization_memory"",\n            ""gpu memory utilization of card"",\n            labels=[""minor_number""])\n\ndef gen_nvidia_gpu_temperature_gauge():\n    return GaugeMetricFamily(""nvidiasmi_temperature"",\n            ""gpu temperature of card"",\n            labels=[""minor_number""])\n\ndef gen_nvidia_gpu_ecc_counter():\n    return GaugeMetricFamily(""nvidiasmi_ecc_error_count"",\n            ""count of nvidia ecc error"",\n            labels=[""minor_number"", ""type""])\n\ndef gen_nvidia_gpu_memory_leak_counter():\n    return GaugeMetricFamily(""nvidiasmi_memory_leak_count"",\n            ""count of nvidia memory leak"",\n            labels=[""minor_number""])\n\n# AMD GPU metrics\ndef gen_amd_gpu_util_gauge():\n    return GaugeMetricFamily(""rocmsmi_utilization_gpu"",\n            ""gpu core utilization of card"",\n            labels=[""minor_number""])\n\ndef gen_amd_gpu_mem_util_gauge():\n    return GaugeMetricFamily(""rocmsmi_utilization_memory"",\n            ""gpu memory utilization of card"",\n            labels=[""minor_number""])\n\ndef gen_amd_gpu_temperature_gauge():\n    return GaugeMetricFamily(""rocmsmi_temperature"",\n            ""gpu temperature of card"",\n            labels=[""minor_number""])\n\ndef gen_zombie_process_counter():\n    return GaugeMetricFamily(""zombie_process_count"",\n            ""count of zombie process"",\n            labels=[""command""])\n\ndef gen_gpu_used_by_external_process_counter():\n    return GaugeMetricFamily(""gpu_used_by_external_process_count"",\n            ""count of gpu used by external process"",\n            labels=[""minor_number"", ""pid""])\n\ndef gen_gpu_used_by_zombie_container_counter():\n    return GaugeMetricFamily(""gpu_used_by_zombie_container_count"",\n            ""count of gpu used by zombie container"",\n            labels=[""minor_number"", ""container_id""])\n\ndef gen_process_mem_usage_gauge():\n    return GaugeMetricFamily(""process_mem_usage_byte"",\n            ""memory usage of process, to save space in prometheus, we only expose those who consume more than 500Mb of memory"",\n            labels=[""pid"", ""cmd""])\n\nclass ResourceGauges(object):\n    def __init__(self):\n        self.task_labels = [\n                ""username"",\n                ""job_name"",\n                ""role_name"",\n                ""task_index"",\n                ""job_instance_id"", # Used to distinguish job instance with same name but different retry number.\n                ]\n        self.service_labels = [""name""]\n\n        self.task_labels_gpu = copy.deepcopy(self.task_labels)\n        self.task_labels_gpu.append(""minor_number"")\n\n        self.gauges = {}\n\n        self.add_task_and_service_gauge(""{0}_cpu_percent"",\n                ""how much percent of cpu this {0} used"")\n        self.add_task_and_service_gauge(""{0}_mem_usage_byte"",\n                ""how much memory this {0} used"")\n        self.add_task_and_service_gauge(""{0}_mem_usage_percent"",\n                ""how much percent of memory this {0} used"")\n        self.add_task_and_service_gauge(""{0}_mem_limit_byte"",\n                ""how much memory this {0} are constrained to"")\n        self.add_task_and_service_gauge(""{0}_net_in_byte"",\n                ""how much network inbound this task used"")\n        self.add_task_and_service_gauge(""{0}_net_out_byte"",\n                ""how much network outbound this {0} used"")\n        self.add_task_and_service_gauge(""{0}_block_in_byte"",\n                ""how much block inbound this {0} used"")\n        self.add_task_and_service_gauge(""{0}_block_out_byte"",\n                ""how much block outbound this {0} used"")\n\n        self.add_gauge(""task_gpu_percent"",\n                ""how much percent of gpu core this task used"",\n                self.task_labels_gpu)\n        self.add_gauge(""task_gpu_mem_percent"",\n                ""how much percent of gpu memory this task used"",\n                self.task_labels_gpu)\n\n    def add_task_and_service_gauge(self, name_tmpl, desc_tmpl):\n        self.add_gauge(\n                name_tmpl.format(""task""),\n                desc_tmpl.format(""task""),\n                self.task_labels)\n\n        self.add_gauge(\n                name_tmpl.format(""service""),\n                desc_tmpl.format(""service""),\n                self.service_labels)\n\n    def add_gauge(self, name, desc, labels):\n        self.gauges[name] = GaugeMetricFamily(name, desc, labels=labels)\n\n    def add_value(self, metric_name, labels, val):\n        if metric_name not in self.gauges:\n            raise RuntimeError(\n                    ""{0} not found in gauges, all gauge names is {1}"".format(\n                        metric_name, "","".join(self.gauges.keys())))\n\n        gauge = self.gauges[metric_name]\n\n        # because prometheus library requires label provided as array, we\n        # preprocess the labels and check any missing labels\n        label_array = [None] * len(gauge._labelnames)\n\n        for k, v in labels.items():\n            try:\n                index = gauge._labelnames.index(k)\n                label_array[index] = v\n            except ValueError:\n                logger.warning(""unknown label %s with value %s for metrics %s"",\n                        k, v, metric_name)\n                continue\n\n        for i, label_val in enumerate(label_array):\n            if label_val is None:\n                logger.error(\n                        ""not provided %s as label value for metric %s, ignore this metric"",\n                        gauge._labelnames[i], metric_name)\n                return\n\n        gauge.add_metric(label_array, val)\n\n    def as_array(self):\n        return self.gauges.values()\n\n#####\n\nclass AtomicRef(object):\n    """""" a thread safe way to store and get object,\n    should not modify data get from this ref,\n    each get and set method should provide a time obj,\n    so this ref decide whether the data is out of date or not,\n    return None on expired """"""\n    def __init__(self, decay_time):\n        self.data = None\n        self.date_in_produced = datetime.datetime.now()\n        self.decay_time = decay_time\n        self.lock = threading.RLock()\n\n    def set(self, data, now):\n        with self.lock:\n            self.data, self.date_in_produced = data, now\n\n    def get(self, now):\n        with self.lock:\n            if self.date_in_produced + self.decay_time < now:\n                return None\n            return self.data\n\n\nclass Collector(object):\n    """""" collector is a model running in thread and responsible for collecting\n    some metrics, we use thread because we do not want to let hanging in one\n    collector can not have impact on other collectors. This is base class,\n    real collector should inhernit this class and implement collect_impl,\n    metrics are returned as an array.""""""\n    def __init__(self, name, sleep_time, atomic_ref, iteration_counter):\n        self.name = name\n        self.sleep_time = sleep_time\n        self.atomic_ref = atomic_ref\n        self.iteration_counter = iteration_counter\n\n        histogram_key = ""collector_%s_iteration_latency_seconds"" % self.name\n        histogram_desc = ""latency for execute one interation of %s collector (seconds)"" % \\\n                self.name\n        self.collector_histogram = Histogram(histogram_key, histogram_desc,\n                buckets=(.005, .01, .025, .05, .075, .1, .25, .5, .75, 1.0, 2.5, 5.0,\n                    7.5, 10.0, 12.5, 15.0, 17.5, 20.0, float(""inf"")))\n\n        logger.debug(""init %s with sleep_time %d"", self.name, self.sleep_time)\n\n    def collect(self):\n        while True:\n            logger.debug(""collecting metrics from %s"", self.name)\n\n            with self.collector_histogram.time():\n                self.iteration_counter.labels(name=self.name).inc()\n                try:\n                    self.atomic_ref.set(self.collect_impl(), datetime.datetime.now())\n                except Exception:\n                    logger.exception(""%s collector get an exception"", self.name)\n\n                logger.debug(""finished collect metrics from %s, will sleep for %s"",\n                        self.name, self.sleep_time)\n\n            time.sleep(self.sleep_time)\n\n    def collect_impl(self):\n        """""" implementations are expected to return an array of\n        prometheus_client\'s metrics or None on exception """"""\n        pass\n\n\ndef instantiate_collector(name, sleep_time, decay_time, collector_class, *args):\n    """""" test cases helper fn to instantiate a collector """"""\n    atomic_ref = AtomicRef(decay_time)\n    return atomic_ref, collector_class(name, sleep_time, atomic_ref, iteration_counter, *args)\n\n\ndef make_collector(name, sleep_time, decay_time, collector_class, *args):\n    """""" other module should use this fn to init a collector, this fn start a thread\n    to run the collector and return an atomic_ref so outside world can get metrics\n    collected by this collector """"""\n    atomic_ref, instance = instantiate_collector(name, sleep_time, decay_time, collector_class, *args)\n\n    t = threading.Thread(\n            target=instance.collect,\n            name=name,\n            args=(),\n            daemon=True)\n\n    t.start()\n\n    return atomic_ref\n\n\nclass DockerCollector(Collector):\n    cmd_histogram = Histogram(""cmd_docker_active_latency_seconds"",\n            ""Command call latency for checking docker daemon activeness (seconds)"")\n\n    cmd_timeout = 1 # 99th latency is 0.01s\n\n    def collect_impl(self):\n        cmd = [""docker"", ""info""]\n        error = ""ok""\n\n        try:\n            out = utils.exec_cmd(cmd,\n                    histogram=DockerCollector.cmd_histogram,\n                    timeout=DockerCollector.cmd_timeout)\n\n            logger.debug(""output for docker info is %s"", out)\n        except subprocess.CalledProcessError as e:\n            logger.exception(""command \'%s\' return with error (code %d): %s"",\n                    cmd, e.returncode, e.output)\n            error = str(e)\n        except subprocess.TimeoutExpired as e:\n            logger.warning(""check docker active timeout"")\n            error = ""timeout""\n        except Exception as e:\n            error = str(e)\n\n        counter = gen_docker_daemon_counter()\n        counter.add_metric([error], 1)\n\n        return [counter]\n\n\nclass GpuCollector(Collector):\n    nvidia_cmd_histogram = Histogram(\n        ""cmd_nvidia_smi_latency_seconds"",\n        ""Command call latency for nvidia-smi (seconds)"")\n    amd_cmd_hostogram = Histogram(\n        ""cmd_rocm_smi_latency_seconds"",\n        ""Command call latency for rocm-smi (seconds)"")\n\n    cmd_timeout = 60 # 99th latency is 0.97s\n\n    def __init__(self, name, sleep_time, atomic_ref, iteration_counter,\n                 gpu_info_ref, zombie_info_ref, mem_leak_thrashold):\n        Collector.__init__(self, name, sleep_time, atomic_ref,\n                           iteration_counter)\n        self.gpu_info_ref = gpu_info_ref\n        self.zombie_info_ref = zombie_info_ref\n        self.mem_leak_thrashold = mem_leak_thrashold\n        self.gpu_vendor = utils.get_gpu_vendor()\n\n    @staticmethod\n    def get_container_id(pid):\n        """""" return two values, the first one is if we found the corresponding\n        container_id, the second one is the container_id if found """"""\n        path = ""/proc/%d/cgroup"" % (pid)\n        if not os.path.isfile(path):\n            return False, """"\n\n        with open(path) as f:\n            content = f.read()\n\n        for line in content.split(""\\n""):\n            line = line.strip()\n            if ""pids"" in line:\n                if ""/docker/"" in line:\n                    parts = line.split(""/docker/"")\n                    if len(parts) == 2 and re.match(u""[0-9a-f]+"", parts[1]):\n                        return True, parts[1]\n                elif ""/kubepods/"" in line:\n                    parts = line.split(""/kubepods/"")\n                    if len(parts) == 2 and re.match(u""pod[0-9a-f-]+"", parts[1]):\n                        return True, parts[1]\n                else:\n                    logger.info(""unknown format in pid cgroup %s"", line)\n\n        return False, """"\n\n    @staticmethod\n    def gen_common_gpu_gauge():\n        return gen_gpu_util_gauge(), gen_gpu_mem_util_gauge()\n\n    @staticmethod\n    def convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info, pid_to_cid_fn, mem_leak_thrashold):\n        """""" This fn used to convert gpu_info & zombie_info into metrics, used to make\n        it easier to do unit test """"""\n        # common gpu metrics\n        gpu_core_util, gpu_mem_util = GpuCollector.gen_common_gpu_gauge()\n        # nvidia metrics\n        nvidia_core_utils = gen_nvidia_gpu_util_gauge()\n        nvidia_mem_utils = gen_nvidia_gpu_mem_util_gauge()\n        nvidia_gpu_temp = gen_nvidia_gpu_temperature_gauge()\n        nvidia_ecc_errors = gen_nvidia_gpu_ecc_counter()\n        nvidia_mem_leak = gen_nvidia_gpu_memory_leak_counter()\n        external_process = gen_gpu_used_by_external_process_counter()\n        zombie_container = gen_gpu_used_by_zombie_container_counter()\n\n        pids_use_gpu = {} # key is gpu minor, value is an array of pid\n\n        for minor, info in gpu_info.items():\n            if not minor.isdigit():\n                continue # ignore UUID\n\n            gpu_core_util.add_metric([minor, GpuVendor.NVIDIA.value], info.gpu_util)\n            gpu_mem_util.add_metric([minor, GpuVendor.NVIDIA.value], info.gpu_mem_util)\n            nvidia_core_utils.add_metric([minor], info.gpu_util)\n            nvidia_mem_utils.add_metric([minor], info.gpu_mem_util)\n            if info.temperature is not None:\n                nvidia_gpu_temp.add_metric([minor], info.temperature)\n            nvidia_ecc_errors.add_metric([minor, ""single""], info.ecc_errors.single)\n            nvidia_ecc_errors.add_metric([minor, ""double""], info.ecc_errors.double)\n\n            # TODO: this piece of code seems not corret, gpu_mem_util is\n            # a percentage number but mem_leak_thrashold is memory size. Need to fix it.\n            if info.gpu_mem_util > mem_leak_thrashold and len(info.pids) == 0:\n                # we found memory leak less than 20M can be mitigated automatically\n                nvidia_mem_leak.add_metric([minor], 1)\n\n            if len(info.pids) > 0:\n                pids_use_gpu[minor]= info.pids\n\n        logger.debug(""pids_use_gpu is %s, zombie_info is %s"", pids_use_gpu, zombie_info)\n        if len(pids_use_gpu) > 0:\n            if zombie_info is None:\n                zombie_info = []\n\n            for minor, pids in pids_use_gpu.items():\n                for pid in pids:\n                    found, z_id = pid_to_cid_fn(pid)\n                    logger.debug(""pid %s has found %s, z_id %s"", pid, found, z_id)\n                    if found:\n                        # NOTE: zombie_info is a set of short docker container id, but\n                        # z_id is full id.\n                        for zombie_id in zombie_info:\n                            if z_id.startswith(zombie_id):\n                                # found corresponding container\n                                zombie_container.add_metric([minor, zombie_id], 1)\n                    else:\n                        external_process.add_metric([minor, str(pid)], 1)\n            if len(zombie_container.samples) > 0 or len(external_process.samples) > 0:\n                logger.warning(""found gpu used by external %s, zombie container %s"",\n                        external_process, zombie_container)\n\n        return [\n            nvidia_core_utils, nvidia_mem_utils, nvidia_ecc_errors,\n            nvidia_mem_leak, external_process, zombie_container,\n            nvidia_gpu_temp, gpu_core_util, gpu_mem_util\n        ]\n\n    @staticmethod\n    def convert_amd_gpu_info_to_metrics(gpu_info):\n        # common gpu metrics\n        gpu_core_util, gpu_mem_util = GpuCollector.gen_common_gpu_gauge()\n\n        # amd metrics\n        amd_core_utils = gen_amd_gpu_util_gauge()\n        amd_mem_utils = gen_amd_gpu_mem_util_gauge()\n        amd_gpu_temp = gen_amd_gpu_temperature_gauge()\n\n        for minor, info in gpu_info.items():\n            gpu_core_util.add_metric([minor, GpuVendor.AMD.value],\n                                     info.gpu_util)\n            gpu_mem_util.add_metric([minor, GpuVendor.AMD.value],\n                                    info.gpu_mem_util)\n            amd_core_utils.add_metric([minor], info.gpu_util)\n            amd_mem_utils.add_metric([minor], info.gpu_mem_util)\n            amd_gpu_temp.add_metric([minor], info.temperature)\n        return [\n            amd_core_utils, amd_mem_utils, amd_gpu_temp, gpu_core_util,\n            gpu_mem_util\n        ]\n\n    def collect_impl(self):\n        if self.gpu_vendor == GpuVendor.UNKNOWN:\n            logger.warning(\n                ""Couldn\'t identify the GPU vendor, please make sure the GPU driver installed correctly""\n            )\n            return None\n        if self.gpu_vendor == GpuVendor.NVIDIA:\n            gpu_info = nvidia.nvidia_smi(GpuCollector.nvidia_cmd_histogram,\n                    GpuCollector.cmd_timeout)\n\n            logger.debug(""get nvidia gpu_info %s"", gpu_info)\n\n            now = datetime.datetime.now()\n            self.gpu_info_ref.set(gpu_info, now)\n            zombie_info = self.zombie_info_ref.get(now)\n\n            if gpu_info:\n                return GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                        GpuCollector.get_container_id, self.mem_leak_thrashold)\n            return None\n        if self.gpu_vendor == GpuVendor.AMD:\n            gpu_info = amd.rocm_smi(GpuCollector.amd_cmd_hostogram,\n                         GpuCollector.cmd_timeout)\n            logger.debug(""get amd gpu info %s"", gpu_info)\n\n            self.gpu_info_ref.set(gpu_info, datetime.datetime.now())\n            if gpu_info:\n                return GpuCollector.convert_amd_gpu_info_to_metrics(gpu_info)\n            return None\n        return None\n\n\nclass ContainerCollector(Collector):\n    stats_histogram = Histogram(""cmd_docker_stats_latency_seconds"",\n            ""Command call latency for docker stats (seconds)"")\n    stats_timeout = 20\n    # 99th latency may larger than 10s,\n    # Because prometheus\'s largest bucket for recording histogram is 10s,\n    # we can not get value higher than 10s.\n\n    inspect_histogram = Histogram(""cmd_docker_inspect_latency_seconds"",\n            ""Command call latency for docker inspect (seconds)"")\n    inspect_timeout = 1 # 99th latency is 0.042s\n\n    iftop_histogram = Histogram(""cmd_iftop_latency_seconds"",\n            ""Command call latency for iftop (seconds)"")\n    iftop_timeout = 10 # 99th latency is 7.4s\n\n    lsof_histogram = Histogram(""cmd_lsof_latency_seconds"",\n            ""Command call latency for lsof (seconds)"")\n    lsof_timeout = 2 # 99th latency is 0.5s\n\n    pai_services = list(map(lambda s: ""k8s_"" + s, [\n        ""rest-server"",\n        ""pylon"",\n        ""webportal"",\n        ""grafana"",\n        ""prometheus"",\n        ""alertmanager"",\n        ""watchdog"",\n        ""end-to-end-test"",\n        ""yarn-frameworklauncher"",\n        ""hadoop-jobhistory-service"",\n        ""hadoop-name-node"",\n        ""hadoop-node-manager"",\n        ""hadoop-resource-manager"",\n        ""hadoop-data-node"",\n        ""zookeeper"",\n        ""node-exporter"",\n        ""job-exporter"",\n        ""yarn-exporter"",\n        ""nvidia-drivers"",\n        ""docker-cleaner"",\n\n        # Below are DLTS services\n        ""nginx"",\n        ""restfulapi"",\n        ""weave"",\n        ""weave-npc"",\n        ""nvidia-device-plugin-ctr"",\n        ""mysql"",\n        ""jobmanager"",\n        ]))\n\n    def __init__(self, name, sleep_time, atomic_ref, iteration_counter, gpu_info_ref,\n            stats_info_ref, interface):\n        Collector.__init__(self, name, sleep_time, atomic_ref, iteration_counter)\n        self.gpu_info_ref = gpu_info_ref\n        self.stats_info_ref = stats_info_ref\n\n        self.network_interface = network.try_to_get_right_interface(interface)\n        logger.info(""found %s as potential network interface to listen network traffic"",\n                self.network_interface)\n\n        self.gpu_vendor = utils.get_gpu_vendor()\n\n        # k8s will prepend ""k8s_"" to pod name. There will also be a container name\n        # prepend with ""k8s_POD_"" which is a docker container used to construct\n        # network & pid namespace for specific container. These container prepend\n        # with ""k8s_POD"" consume nothing.\n\n    def collect_impl(self):\n        all_conns = network.iftop(self.network_interface,\n                ContainerCollector.iftop_histogram,\n                ContainerCollector.iftop_timeout)\n\n        stats_obj = docker_stats.stats(ContainerCollector.stats_histogram,\n                ContainerCollector.stats_timeout)\n\n        now = datetime.datetime.now()\n        gpu_infos = self.gpu_info_ref.get(now)\n        self.stats_info_ref.set(stats_obj, now)\n\n        logger.debug(""all_conns is %s"", all_conns)\n        logger.debug(""gpu_info is %s"", gpu_infos)\n        logger.debug(""stats_obj is %s"", stats_obj)\n\n        return self.collect_container_metrics(stats_obj, gpu_infos, all_conns)\n\n    @staticmethod\n    def parse_from_labels(inspect_info, gpu_infos):\n        gpu_ids = []\n        result_labels = {}\n\n        result_labels[""username""] = inspect_info.username or ""unknown""\n        result_labels[""job_name""] = inspect_info.job_name or ""unknown""\n        result_labels[""role_name""] = inspect_info.role_name or ""unknown""\n        result_labels[""task_index""] = inspect_info.task_index or ""unknown""\n        result_labels[""job_instance_id""] = inspect_info.job_instance_id or ""unknown""\n\n        if inspect_info.gpu_ids:\n            ids = inspect_info.gpu_ids.replace(""\\"""", """").split("","")\n            for id in ids:\n                # If the container was scheduled by yarn, we get its GPU usage\n                # info from label GPU_ID, value of the label is minor_number, and\n                # will be digits.\n                # If the container was scheduled by kube launcher, we get its GPU\n                # usage info from environment NVIDIA_VISIBLE_DEVICES, the value\n                # is like GPU-dc0671b0-61a4-443e-f456-f8fa6359b788. The mapping\n                # from uuid to minor_number is get via nvidia-smi, and gpu_infos\n                # should have key of this uuid.\n                if id.isdigit():\n                    gpu_ids.append(id)\n                elif id and gpu_infos is not None:\n                    # id is in form of UUID like\n                    if gpu_infos.get(id) is not None:\n                        gpu_ids.append(gpu_infos[id].minor)\n                    else:\n                        logger.warning(""gpu uuid %s can not be found in map %s"",\n                                id, gpu_infos)\n                else:\n                    logger.warning(""unknown gpu id %s, gpu_infos is %s"",\n                            id, gpu_infos)\n\n        return gpu_ids, result_labels\n\n    @classmethod\n    def infer_service_name(cls, container_name):\n        """""" try to infer service name from container_name, if it\'s container not belongs\n        to pai service, will return None """"""\n        if container_name.startswith(""k8s_POD_""):\n            # this is empty container created by k8s for pod\n            return None\n\n        # TODO speed this up, since this is O(n^2)\n        for service_name in cls.pai_services:\n            if container_name.startswith(service_name):\n                return service_name[4:] # remove ""k8s_"" prefix\n\n        return None\n\n    def process_one_container(self, container_id, stats, gpu_infos, all_conns, gauges):\n        container_name = utils.walk_json_field_safe(stats, ""name"")\n        pai_service_name = ContainerCollector.infer_service_name(container_name)\n\n        inspect_info = docker_inspect.inspect(container_id,\n                ContainerCollector.inspect_histogram,\n                ContainerCollector.inspect_timeout, self.gpu_vendor)\n\n        pid = inspect_info.pid\n        job_name = inspect_info.job_name\n\n        logger.debug(""%s has inspect result %s, service_name %s"",\n                container_name, inspect_info, pai_service_name)\n\n        if job_name is None and pai_service_name is None:\n            logger.debug(""%s is ignored"", container_name)\n            return # other container, maybe kubelet or api-server\n\n        # get network consumption, since all our services/jobs running in host\n        # network, and network statistic from docker is not specific to that\n        # container. We have to get network statistic by ourselves.\n        lsof_result = network.lsof(pid,\n                ContainerCollector.lsof_histogram,\n                ContainerCollector.lsof_timeout)\n\n        net_in, net_out = network.get_container_network_metrics(all_conns,\n                lsof_result)\n        if logger.isEnabledFor(logging.DEBUG):\n            debug_info = utils.exec_cmd(\n                    ""ps -o cmd fp {0} | tail -n 1"".format(pid),\n                    shell=True)\n\n            logger.debug(""pid %s with cmd `%s` has lsof result %s, in %d, out %d"",\n                    pid, debug_info.strip(), lsof_result, net_in, net_out)\n\n        if pai_service_name is None:\n            gpu_ids, container_labels = ContainerCollector.parse_from_labels(inspect_info, gpu_infos)\n\n            if gpu_infos:\n                for id in gpu_ids:\n                    if gpu_infos.get(id) is None:\n                        continue\n\n                    nvidia_gpu_status = gpu_infos[id]\n                    labels = copy.deepcopy(container_labels)\n                    labels[""minor_number""] = id\n\n                    gauges.add_value(""task_gpu_percent"",\n                            labels, nvidia_gpu_status.gpu_util)\n                    gauges.add_value(""task_gpu_mem_percent"",\n                            labels, nvidia_gpu_status.gpu_mem_util)\n\n            gauges.add_value(""task_cpu_percent"", container_labels, stats[""CPUPerc""])\n            gauges.add_value(""task_mem_usage_byte"", container_labels, stats[""MemUsage_Limit""][""usage""])\n            gauges.add_value(""task_mem_limit_byte"", container_labels, stats[""MemUsage_Limit""][""limit""])\n            gauges.add_value(""task_net_in_byte"", container_labels, net_in)\n            gauges.add_value(""task_net_out_byte"", container_labels, net_out)\n            gauges.add_value(""task_block_in_byte"", container_labels, stats[""BlockIO""][""in""])\n            gauges.add_value(""task_block_out_byte"", container_labels, stats[""BlockIO""][""out""])\n            gauges.add_value(""task_mem_usage_percent"", container_labels, stats[""MemPerc""])\n        else:\n            labels = {""name"": pai_service_name}\n            gauges.add_value(""service_cpu_percent"", labels, stats[""CPUPerc""])\n            gauges.add_value(""service_mem_usage_byte"", labels, stats[""MemUsage_Limit""][""usage""])\n            gauges.add_value(""service_mem_limit_byte"", labels, stats[""MemUsage_Limit""][""limit""])\n            gauges.add_value(""service_mem_usage_percent"", labels, stats[""MemPerc""])\n            gauges.add_value(""service_net_in_byte"", labels, net_in)\n            gauges.add_value(""service_net_out_byte"", labels, net_out)\n            gauges.add_value(""service_block_in_byte"", labels, stats[""BlockIO""][""in""])\n            gauges.add_value(""service_block_out_byte"", labels, stats[""BlockIO""][""out""])\n\n    def collect_container_metrics(self, stats_obj, gpu_infos, all_conns):\n        if stats_obj is None:\n            logger.warning(""docker stats returns None"")\n            return None\n\n        gauges = ResourceGauges()\n\n        for container_id, stats in stats_obj.items():\n            try:\n                self.process_one_container(container_id, stats, gpu_infos, all_conns, gauges)\n            except Exception:\n                logger.exception(""error when trying to process container %s with name %s"",\n                        container_id, utils.walk_json_field_safe(stats, ""name""))\n\n        return gauges.as_array()\n\n\nclass ZombieCollector(Collector):\n    logs_histogram = Histogram(""cmd_docker_logs_latency_seconds"",\n            ""Command call latency for docker logs (seconds)"")\n    logs_timeout = 1 # 99th latency is 0.04s\n\n    zombie_container_count = Gauge(""zombie_container_count"",\n            ""number of zombie container found for this node"",\n            [""type""])\n\n    class ZombieRecorder(object):\n        def __init__(self, type):\n            self.type = type\n            self.zombies = {} # key is container id, value is enter zombie time\n\n            # When we first meet zombie container, we only record time of that meet,\n            # we wait extra decay_time to report it as zombie. Because at the time\n            # of our recording, zombie just produced, and haven\'t been recycled, we\n            # wait 5 minutes to avoid possible cases of normal zombie.\n            self.decay_time = datetime.timedelta(minutes=5)\n\n        def update(self, zombie_ids, now):\n            """""" feed in new zombie ids and get id of decayed zombie """"""\n            # remove all records not exist anymore\n            for z_id in list(self.zombies.keys()):\n                if z_id not in zombie_ids:\n                    logger.debug(""pop zombie %s that not exist anymore"", z_id)\n                    self.zombies.pop(z_id)\n\n            result = set()\n            for current in zombie_ids:\n                if current in self.zombies:\n                    enter_zombie_time = self.zombies[current]\n                    if now - enter_zombie_time > self.decay_time:\n                        result.add(current)\n                else:\n                    logger.debug(""new zombie %s"", current)\n                    self.zombies[current] = now\n\n            ZombieCollector.zombie_container_count.labels(self.type).set(len(result))\n            return result\n\n        def __len__(self):\n            return len(self.zombies)\n\n    def __init__(self, name, sleep_time, atomic_ref, iteration_counter, stats_info_ref, zombie_ids_ref):\n        Collector.__init__(self, name, sleep_time, atomic_ref, iteration_counter)\n        self.stats_info_ref = stats_info_ref\n        self.zombie_ids_ref = zombie_ids_ref\n\n        self.type1_zombies = ZombieCollector.ZombieRecorder(""job_exit_hangs"")\n        self.type2_zombies = ZombieCollector.ZombieRecorder(""residual_job"")\n\n        self.yarn_pattern = u""container_\\w{3}_[0-9]{13}_[0-9]{4}_[0-9]{2}_[0-9]{6}""\n        self.yarn_container_reg = re.compile(u""^"" + self.yarn_pattern + ""$"")\n        self.job_container_reg = re.compile(u""^.+("" + self.yarn_pattern + u"")$"")\n\n    def update_zombie_count_type1(self, exited_containers, now):\n        """""" this fn will generate zombie container count for the first type,\n        exited_containers is container id set of which we believe exited """"""\n        return self.type1_zombies.update(exited_containers, now)\n\n    def update_zombie_count_type2(self, stats, now):\n        """""" this fn will generate zombie container count for the second type """"""\n        name_to_id = {}\n        for info in stats.values():\n            name_to_id[info[""name""]] = info[""id""]\n\n        # key is job name, value is tuple of corresponding\n        # yarn_container name and job container id\n        job_containers = {}\n\n        yarn_containers = set()\n\n        zombie_ids = set()\n\n        for name, id in name_to_id.items():\n            if re.match(self.yarn_container_reg, name) is not None:\n                yarn_containers.add(name)\n            elif re.match(self.job_container_reg, name) is not None:\n                match = re.match(self.job_container_reg, name)\n                value = match.groups()[0]\n                job_containers[name] = (value, id)\n            else:\n                pass # ignore\n\n        for _, val in job_containers.items():\n            yarn_name, job_id = val\n            if yarn_name not in yarn_containers:\n                zombie_ids.add(job_id)\n\n        return self.type2_zombies.update(zombie_ids, now)\n\n    def docker_logs(self, container_id, tail=""all""):\n        try:\n            return utils.exec_cmd(\n                    [""docker"", ""logs"", ""--tail"", str(tail), str(container_id)],\n                    histogram=ZombieCollector.logs_histogram,\n                    stderr=subprocess.STDOUT, # also capture stderr output\n                    timeout=ZombieCollector.logs_timeout)\n        except subprocess.TimeoutExpired as e:\n            logger.warning(""docker log timeout"")\n        except subprocess.CalledProcessError as e:\n            logger.warning(""docker logs returns %d, output %s"", e.returncode, e.output)\n        except Exception:\n            logger.exception(""exec docker logs error"")\n\n        return """"\n\n    def is_container_exited(self, container_id):\n        logs = self.docker_logs(container_id, tail=50)\n        if re.search(u""USER COMMAND END"", logs):\n            return True\n        return False\n\n    def update_zombie_count(self, stats):\n        """"""\n        There are two types of zombie:\n            1. container which outputted ""USER COMMAND END"" but did not exist for a long period of time\n            2. yarn container exited but job container didn\'t\n        return set of container id that deemed as zombie\n        """"""\n        if stats is None:\n            logger.warning(""docker stats is None"")\n            return\n\n        exited_containers = set(filter(self.is_container_exited, stats.keys()))\n\n        now = datetime.datetime.now()\n        type1_zombies = self.update_zombie_count_type1(exited_containers, now)\n        type2_zombies = self.update_zombie_count_type2(stats, now)\n        return type1_zombies.union(type2_zombies)\n\n    def collect_impl(self):\n        # set it to None so if docker-stats hangs till next time we get,\n        # we will get None\n        stats_info = self.stats_info_ref.get(datetime.datetime.now())\n        all_zombies = self.update_zombie_count(stats_info)\n        self.zombie_ids_ref.set(all_zombies, datetime.datetime.now())\n\n\nclass ProcessCollector(Collector):\n    cmd_histogram = Histogram(""cmd_ps_latency_seconds"",\n            ""Command call latency for ps (seconds)"")\n\n    cmd_timeout = 10 # TODO 99th latency is xxx\n\n    def __init__(self, name, sleep_time, atomic_ref, iteration_counter):\n        Collector.__init__(self, name, sleep_time, atomic_ref, iteration_counter)\n\n    def collect_impl(self):\n        process_info = ps.get_process_info(ProcessCollector.cmd_histogram,\n                ProcessCollector.cmd_timeout)\n\n        if len(process_info) > 0:\n            zombie_metrics = gen_zombie_process_counter()\n            process_mem_metrics = gen_process_mem_usage_gauge()\n            zombie_count = collections.defaultdict(lambda : 0)\n\n            for info in process_info:\n                if info.state == ""D"":\n                    if ""nvidia-smi"" in info.cmd:\n                        # override command name to make alert rule easier\n                        zombie_count[""nvidia-smi""] += 1\n                    else:\n                        cmd = info.cmd.split()[0] # remove args\n                        zombie_count[cmd] += 1\n\n                if info.rss > 500 * 1024 * 1024:\n                    # only record large memory consumption to save space in prometheus\n                    cmd = info.cmd.split()[0] # remove args\n                    process_mem_metrics.add_metric([str(info.pid), cmd], info.rss)\n\n            for cmd, count in zombie_count.items():\n                zombie_metrics.add_metric([cmd], count)\n\n            return [zombie_metrics, process_mem_metrics]\n\n        return None\n'"
src/job-exporter/src/docker_inspect.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport subprocess\nimport json\nimport sys\nimport logging\n\nimport utils\nfrom utils import GpuVendor\n\nlogger = logging.getLogger(__name__)\n\nclass InspectResult(object):\n    """""" Represents a task meta data, parsed from docker inspect result """"""\n    def __init__(self, username, job_name, role_name, task_index, gpu_ids, job_instance_id, pid):\n        self.username = username\n        self.job_name = job_name\n        self.role_name = role_name\n        self.task_index = task_index\n        self.gpu_ids = gpu_ids # comma separated str, str may be minor_number or UUID\n        self.pid = pid\n        self.job_instance_id = job_instance_id # Used to distinguish job instance with same name but different retry number.\n\n    def __repr__(self):\n        return ""username %s, job_name %s, role_name %s, task_index %s, gpu_ids %s, job_instance_id %s pid %s"" % \\\n                (self.username, self.job_name, self.role_name, self.task_index, self.gpu_ids, self.job_instance_id, self.pid)\n\n    def __eq__(self, o):\n        return self.username == o.username and \\\n                self.job_name == o.job_name and \\\n                self.role_name == o.role_name and \\\n                self.task_index == o.task_index and \\\n                self.gpu_ids == o.gpu_ids and \\\n                self.job_instance_id == o.job_instance_id and \\\n                self.pid == o.pid\n\n\nkeys = {""PAI_JOB_NAME"", ""PAI_USER_NAME"", ""PAI_CURRENT_TASK_ROLE_NAME"", ""GPU_ID"",\n        ""PAI_TASK_INDEX"", ""DLWS_JOB_ID"", ""DLWS_USER_NAME""}\n\n\ndef parse_docker_inspect(inspect_output, gpu_vender):\n    obj = json.loads(inspect_output)\n\n    m = {}\n\n    obj_labels = utils.walk_json_field_safe(obj, 0, ""Config"", ""Labels"")\n    if obj_labels is not None:\n        for k, v in obj_labels.items():\n            if k in keys:\n                m[k] = v\n\n    obj_env = utils.walk_json_field_safe(obj, 0, ""Config"", ""Env"")\n    if obj_env:\n        for env in obj_env:\n            k, v = env.split(""="", 1)\n            if k in keys:\n                m[k] = v\n\n            # for kube-launcher tasks\n            if k == ""FC_TASK_INDEX"":\n                m[""PAI_TASK_INDEX""] = v\n            else:\n                if k == ""NVIDIA_VISIBLE_DEVICES"" and gpu_vender == GpuVendor.NVIDIA and v \\\n                    and v != ""all"" and v != ""void"" and v != ""none"":\n                    m[""GPU_ID""] = v\n                if k == ""PAI_AMD_VISIBLE_DEVICES"" and gpu_vender == GpuVendor.AMD and v:\n                    m[""GPU_ID""] = v\n\n            if k == ""FC_FRAMEWORK_ATTEMPT_INSTANCE_UID"" or k == ""APP_ID"":\n                m[""JOB_INSTANCE_ID""] = v\n\n    pid = utils.walk_json_field_safe(obj, 0, ""State"", ""Pid"")\n\n    return InspectResult(\n            m.get(""PAI_USER_NAME"") or m.get(""DLWS_USER_NAME""),\n            m.get(""PAI_JOB_NAME"") or m.get(""DLWS_JOB_ID""),\n            m.get(""PAI_CURRENT_TASK_ROLE_NAME""),\n            m.get(""PAI_TASK_INDEX""),\n            m.get(""GPU_ID""),\n            m.get(""JOB_INSTANCE_ID""),\n            pid)\n\ndef inspect(container_id, histogram, timeout, gpu_vender):\n    try:\n        result = utils.exec_cmd(\n                [""docker"", ""inspect"", container_id],\n                histogram=histogram,\n                timeout=timeout)\n        return parse_docker_inspect(result, gpu_vender)\n    except subprocess.CalledProcessError as e:\n        logger.exception(""command \'%s\' return with error (code %d): %s"",\n                e.cmd, e.returncode, e.output)\n    except subprocess.TimeoutExpired:\n        logger.warning(""docker inspect timeout"")\n    except Exception:\n        logger.exception(""exec docker inspect error"")\n'"
src/job-exporter/src/docker_stats.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport subprocess\nimport sys\nimport re\nimport logging\n\nimport utils\n\nlogger = logging.getLogger(__name__)\n\ndef parse_percentile(data):\n    return float(data.replace(""%"", """"))\n\ndef parse_io(data):\n    inOut = data.split(""/"")\n    inByte = convert_to_byte(inOut[0])\n    outByte = convert_to_byte(inOut[1])\n    return {""in"": inByte, ""out"": outByte}\n\ndef parse_usage_limit(data):\n    usageLimit = data.split(""/"")\n    usageByte = convert_to_byte(usageLimit[0])\n    limitByte = convert_to_byte(usageLimit[1])\n    return {""usage"": usageByte, ""limit"": limitByte}\n\ndef convert_to_byte(data):\n    data = data.lower()\n    number = float(re.findall(r""[0-9.]+"", data)[0])\n    if ""tb"" in data:\n        return number * 10 ** 12\n    elif ""gb"" in data:\n        return number * 10 ** 9\n    elif ""mb"" in data:\n        return number * 10 ** 6\n    elif ""kb"" in data:\n        return number * 10 ** 3\n    elif ""tib"" in data:\n        return number * 2 ** 40\n    elif ""gib"" in data:\n        return number * 2 ** 30\n    elif ""mib"" in data:\n        return number * 2 ** 20\n    elif ""kib"" in data:\n        return number * 2 ** 10\n    else:\n        return number\n\ndef parse_docker_stats(stats):\n    data = [line.split("","") for line in stats.splitlines()]\n    # pop the headers\n    data.pop(0)\n    row_count = len(data)\n    container_stats = {}\n\n    for i in range(row_count):\n        id = data[i][0]\n        containerInfo = {\n            ""id"": data[i][0],\n            ""name"": data[i][1],\n            ""CPUPerc"": parse_percentile(data[i][2]),\n            ""MemUsage_Limit"": parse_usage_limit(data[i][3]),\n            ""NetIO"": parse_io(data[i][4]),\n            ""BlockIO"": parse_io(data[i][5]),\n            ""MemPerc"": parse_percentile(data[i][6])\n        }\n        container_stats[id] = containerInfo\n    return container_stats\n\ndef stats(histogram, timeout):\n    try:\n        result = utils.exec_cmd([\n            ""docker"", ""stats"", ""--no-stream"", ""--format"",\n            ""table {{.ID}},{{.Name}},{{.CPUPerc}},{{.MemUsage}},{{.NetIO}},{{.BlockIO}},{{.MemPerc}}""],\n            histogram=histogram,\n            timeout=timeout)\n        return parse_docker_stats(result)\n    except subprocess.CalledProcessError as e:\n        logger.error(""command \'{}\' return with error (code {}): {}"".format(e.cmd, e.returncode, e.output))\n    except subprocess.TimeoutExpired:\n        logger.warning(""docker stats timeout"")\n    except Exception:\n        logger.exception(""exec docker stats error"")\n'"
src/job-exporter/src/main.py,0,"b'#!/usr/bin/env python3\n\nimport argparse\nimport logging\nimport os\nimport json\nimport threading\nimport signal\nimport faulthandler\nimport gc\nimport datetime\nimport shutil\nimport sys\n\nimport prometheus_client\nfrom prometheus_client import Gauge\nfrom prometheus_client.core import REGISTRY\nfrom prometheus_client.twisted import MetricsResource\n\nfrom twisted.web.server import Site\nfrom twisted.web.resource import Resource\nfrom twisted.internet import reactor\n\nimport collector\n\nlogger = logging.getLogger(__name__)\n\n\nconfigured_gpu_counter = Gauge(""configured_gpu_count"",\n        ""total number of gpu configured for this node"")\n\n\nclass CustomCollector(object):\n    def __init__(self, atomic_refs):\n        self.atomic_refs = atomic_refs\n\n    def collect(self):\n        data = []\n\n        now = datetime.datetime.now()\n\n        for ref in self.atomic_refs:\n            d = ref.get(now)\n            if d is not None:\n                data.extend(d)\n\n        if len(data) > 0:\n            for datum in data:\n                yield datum\n        else:\n            # https://stackoverflow.com/a/6266586\n            # yield nothing\n            return\n            yield\n\n\ndef config_environ():\n    """""" since job-exporter needs to call nvidia-smi, we need to change\n    LD_LIBRARY_PATH and PATH to correct value """"""\n    deploy_env = os.environ.get(""DEPLOY_ENV"")\n    # Refer to issue: https://github.com/Azure/AKS/issues/1271, we need to set nvdia-smi manually\n    if deploy_env == ""aks"":\n        host_usr_bin_dir = os.environ.get(""HOST_USR_BIN_DIR"")\n        host_nvidia_dir = os.environ.get(""HOST_NVIDIA_DIR"")\n        if not host_usr_bin_dir or not host_nvidia_dir:\n            logger.error(""Failed to get HOST_USR_BIN_DIR or HOST_NVIDIA_DIR env"")\n            raise Exception(""Environment not set correctly, HOST_USR_BIN_DIR is{},\\\n                             HOST_NVIDIA_DIR is {}"".format(host_usr_bin_dir, host_nvidia_dir))\n\n        if os.path.isfile(os.path.join(host_usr_bin_dir, ""nvidia-smi"")):\n            logger.info(""nvidia-smi already under host /usr/bin dir"")\n            return\n\n        try:\n            shutil.copy(os.path.join(host_nvidia_dir, ""bin/nvidia-smi""), host_usr_bin_dir)\n        except Exception:\n            logger.exception(""Failed to copy nvidia-smi in aks"")\n            return\n\n        os.environ[""PATH""] = os.environ[""PATH""] + "":"" + os.path.join(host_nvidia_dir, ""bin"")\n        logger.info(""Copy nvidia-smi to %s successfully, PATH is %s"",\n                    host_usr_bin_dir, os.environ[""PATH""])\n        return\n\n    driver_path = os.environ.get(""NV_DRIVER"")\n    logger.debug(""NV_DRIVER is %s"", driver_path)\n\n    ld_path = os.environ.get(""LD_LIBRARY_PATH"", """")\n    os.environ[""LD_LIBRARY_PATH""] = ld_path + os.pathsep + \\\n            os.path.join(driver_path, ""lib"") + os.pathsep + \\\n            os.path.join(driver_path, ""lib64"")\n\n    driver_bin_path = os.path.join(driver_path, ""bin"")\n    os.environ[""PATH""] = os.environ[""PATH""] + "":"" + driver_bin_path\n\n    logger.info(""LD_LIBRARY_PATH is %s, PATH is %s"",\n            os.environ[""LD_LIBRARY_PATH""],\n            os.environ[""PATH""])\n\n\ndef try_remove_old_prom_file(path):\n    """""" try to remove old prom file, since old prom file are exposed by node-exporter,\n    if we do not remove, node-exporter will still expose old metrics """"""\n    if os.path.isfile(path):\n        try:\n            os.unlink(path)\n        except Exception:\n            logger.warning(""can not remove old prom file %s"", path)\n\n\ndef get_gpu_count(path):\n    hostname = os.environ.get(""HOSTNAME"")\n    ip = os.environ.get(""HOST_IP"")\n\n    logger.debug(""hostname is %s, ip is %s"", hostname, ip)\n\n    if os.path.isfile(path):\n        with open(path) as f:\n            gpu_config = json.load(f)\n\n        if hostname is not None and gpu_config[""nodes""].get(hostname) is not None:\n            return gpu_config[""nodes""][hostname][""gpuCount""]\n        elif ip is not None and gpu_config[""nodes""].get(ip) is not None:\n            return gpu_config[""nodes""][ip][""gpuCount""]\n\n    logger.warning(""failed to find gpu count from config %s"", path)\n    return 0\n\n\ndef register_stack_trace_dump():\n    faulthandler.register(signal.SIGTRAP, all_threads=True, chain=False)\n\n\nclass HealthResource(Resource):\n    def render_GET(self, request):\n        request.setHeader(""Content-Type"", ""text/html; charset=utf-8"")\n        return ""<html>Ok</html>"".encode(""utf-8"")\n\n\ndef main(args):\n    register_stack_trace_dump()\n    config_environ()\n    try_remove_old_prom_file(args.log + ""/gpu_exporter.prom"")\n    try_remove_old_prom_file(args.log + ""/job_exporter.prom"")\n    try_remove_old_prom_file(args.log + ""/docker.prom"")\n    try_remove_old_prom_file(args.log + ""/time.prom"")\n    try_remove_old_prom_file(args.log + ""/configured_gpu.prom"")\n\n    configured_gpu_counter.set(get_gpu_count(""/gpu-config/gpu-configuration.json""))\n\n    decay_time = datetime.timedelta(seconds=args.interval * 2)\n\n    # used to exchange gpu info between GpuCollector and ContainerCollector\n    gpu_info_ref = collector.AtomicRef(decay_time)\n\n    # used to exchange docker stats info between ContainerCollector and ZombieCollector\n    stats_info_ref = collector.AtomicRef(decay_time)\n\n    # used to exchange zombie info between GpuCollector and ZombieCollector\n    zombie_info_ref = collector.AtomicRef(decay_time)\n\n    interval = args.interval\n    # Because all collector except container_collector will spent little time in calling\n    # external command to get metrics, so they need to sleep 30s to align with prometheus\n    # scrape interval. The 99th latency of container_collector loop is around 20s, so it\n    # should only sleep 10s to adapt to scrape interval\n    collector_args = [\n            (""docker_daemon_collector"", interval, decay_time, collector.DockerCollector),\n            (""gpu_collector"", interval, decay_time, collector.GpuCollector, gpu_info_ref, zombie_info_ref, args.threshold),\n            (""container_collector"", max(0, interval - 18), decay_time, collector.ContainerCollector,\n                gpu_info_ref, stats_info_ref, args.interface),\n            (""zombie_collector"", interval, decay_time, collector.ZombieCollector, stats_info_ref, zombie_info_ref),\n            (""process_collector"", interval, decay_time, collector.ProcessCollector),\n            ]\n\n    refs = list(map(lambda x: collector.make_collector(*x), collector_args))\n\n    REGISTRY.register(CustomCollector(refs))\n\n    root = Resource()\n    root.putChild(b""metrics"", MetricsResource())\n    root.putChild(b""healthz"", HealthResource())\n    factory = Site(root)\n    reactor.listenTCP(int(args.port), factory)\n    reactor.run()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--log"", ""-l"", help=""log dir to store log"", default=""/datastorage/prometheus"")\n    parser.add_argument(""--port"", ""-p"", help=""port to expose metrics"", default=""9102"")\n    parser.add_argument(""--interval"", ""-i"", help=""prometheus scrape interval second"", type=int, default=30)\n    parser.add_argument(""--interface"", ""-n"", help=""network interface for job-exporter to listen on"", required=True)\n    parser.add_argument(""--threshold"", ""-t"", help=""memory threshold to consider gpu memory leak"", type=int, default=20 * 1024 * 1024)\n    args = parser.parse_args()\n\n    def get_logging_level():\n        mapping = {\n                ""DEBUG"": logging.DEBUG,\n                ""INFO"": logging.INFO,\n                ""WARNING"": logging.WARNING\n                }\n\n        result = logging.INFO\n\n        if os.environ.get(""LOGGING_LEVEL"") is not None:\n            level = os.environ[""LOGGING_LEVEL""]\n            result = mapping.get(level.upper())\n            if result is None:\n                sys.stderr.write(""unknown logging level "" + level + \\\n                        "", default to INFO\\n"")\n                result = logging.INFO\n\n        return result\n\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(threadName)s - %(filename)s:%(lineno)s - %(message)s"",\n            level=get_logging_level())\n\n    main(args)\n'"
src/job-exporter/src/network.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport re\nimport logging\nimport collections\nimport subprocess\nimport socket\nimport fcntl\nimport struct\nimport array\n\nimport utils\n\nlogger = logging.getLogger(__name__)\n\n# We relay on iftop & lsof to implement network consumption statistic. We first get all\n# network consuption by iftop, this command can run in any namespace, it will output\n# connetion & connection\'s consumption in some duration(40s), this consumption is node wide\n# statistic. We break this statistic down into container level by running lsof, lsof must\n# run in container\'s PID namespace. This will collect all connection pairs in that namespace.\n# If the container is running in host namespace, then, lsof will output connection pairs in\n# node level instead of container level.\n# So current implementation is not workable for container running in host PID namespace.\n# Also since lsof can not capture most of short-lived connections, this workaround is useful\n# only for long-lived connections.\n\n# To bring lsof in container\'s PID namespace, job exporter needs to be in host PID namespace\n# and is privileged. This has serious security issue since any people can exec into\n# job-exporter container and have root access to system wide resources, we prevent this by\n# enabling DenyEscalatingExec in kubernets,\n# see https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#denyescalatingexec\n\n\ndef convert_to_byte(data):\n    number = float(re.findall(r""(\\d+(\\.\\d+)?)"", data)[0][0])\n    if ""T"" in data:\n        return number * 1024 * 1024 * 1024 * 1024\n    elif ""G"" in data:\n        return number * 1024 * 1024 * 1024\n    elif ""M"" in data:\n        return number * 1024 * 1024\n    elif ""K"" in data:\n        return number * 1024\n    else:\n        return number\n\n\ndef iftop(interface, histogram, timeout):\n    cmd = [""iftop"", ""-t"", ""-P"", ""-s"", ""1"", ""-L"", ""10000"", ""-B"", ""-n"", ""-N""]\n    if interface is not None:\n        cmd.extend([""-i"", interface])\n\n    try:\n        output = utils.exec_cmd(\n                cmd,\n                stderr=subprocess.STDOUT, # also capture stderr output\n                histogram=histogram, timeout=timeout)\n        return parse_iftop(output)\n    except subprocess.TimeoutExpired:\n        logger.warning(""iftop timeout"")\n    except Exception:\n        logger.exception(""exec iftop error"")\n        return None\n\n\n# iftop return 2s, 10s, 40s connection duration data.\n# duration can only could be 2 or 10 or 40.\ndef parse_iftop(iftop_output, duration=40):\n    """""" parse output of iftop to map which key is ip:port value is map of in/out statistic """"""\n    result = collections.defaultdict(lambda : {""in"": 0, ""out"": 0})\n    raw = [line.strip() for line in iftop_output.splitlines()]\n    data = []\n\n    # only interested in part two divided by ----\n    part = 0\n    for line in raw:\n        if ""------------"" in line:\n            part += 1\n            if part == 2:\n                break\n        else:\n            if part == 1:\n                data.append(line)\n\n    for line_no in range(0, len(data), 2):\n        line1 = data[line_no].split()\n        line2 = data[line_no + 1].split()\n        src = line1[1]\n        dst = line2[0]\n\n        if duration == 2:\n            col_index = -4\n        elif duration == 10:\n            col_index = -3\n        elif duration == 40:\n            col_index = -2\n        else:\n            col_index = -2\n\n        out_byte = convert_to_byte(line1[col_index])\n        in_byte = convert_to_byte(line2[col_index])\n\n        result[src][""in""] += in_byte\n        result[src][""out""] += out_byte\n\n        result[dst][""in""] += out_byte\n        result[dst][""out""] += in_byte\n\n    return result\n\n\ndef lsof(pid, histogram, timeout):\n    """""" use infilter to do setns https://github.com/yadutaf/infilter """"""\n    if pid is None:\n        return None\n\n    try:\n        output = utils.exec_cmd([""infilter"", str(pid), ""/usr/bin/lsof"", ""-i"", ""-n"", ""-P""],\n                histogram=histogram,\n                stderr=subprocess.STDOUT, # also capture stderr output\n                timeout=timeout)\n        return parse_lsof(output)\n    except subprocess.TimeoutExpired:\n        logger.warning(""lsof timeout"")\n    except subprocess.CalledProcessError as e:\n        logger.warning(""infilter lsof returns %d, output %s"", e.returncode, e.output)\n    except Exception:\n        logger.exception(""exec lsof error"")\n        return None\n\n\ndef parse_lsof(lsof_output):\n    """""" parse output by lsof. For each socket link, lsof will output two lines,\n    return a map with string pid as key and list of ip:port that pid is connected from """"""\n    conns = collections.defaultdict(lambda : set())\n    data = [line.strip() for line in lsof_output.splitlines()[1:]]\n\n    for line in data:\n        if ""ESTABLISHED"" in line:\n            parts = line.split()\n            if len(parts) == 10:\n                pid = parts[1]\n                src = parts[8].split(""->"")[0]\n                conns[pid].add(src)\n            else:\n                logger.warning(""unknown format of lsof %s"", parts)\n                continue\n\n    return conns\n\n\n# NOTE: we assume lsof_result contains network consumption only in container,\n# This assumption will be break if container has host pid namespace, in this case\n# the network metrics contains all network consumption in its node.\ndef get_container_network_metrics(all_conns, lsof_result):\n    """""" return in_byte, out_byte of container """"""\n    if all_conns is None or lsof_result is None:\n        return 0, 0\n\n    in_byte = 0\n    out_byte = 0\n\n    for container_pid, container_conns in lsof_result.items():\n        for conn in container_conns:\n            if conn in all_conns:\n                in_byte += all_conns[conn][""in""]\n                out_byte += all_conns[conn][""out""]\n\n    return in_byte, out_byte\n\n\ndef format_ip(addr):\n    return str(addr[0]) + ""."" + \\\n           str(addr[1]) + ""."" + \\\n           str(addr[2]) + ""."" + \\\n           str(addr[3])\n\n\ndef get_interfaces():\n    """""" get all network interfaces we see, return map with interface name as key, and ip as value """"""\n    max_possible = 128\n    bytes = max_possible * 32\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    names = array.array(""B"", b""\\0"" * bytes)\n    outbytes = struct.unpack(""iL"", fcntl.ioctl(\n        s.fileno(),\n        0x8912,  # SIOCGIFCONF\n        struct.pack(""iL"", bytes, names.buffer_info()[0])\n    ))[0]\n\n    namestr = names.tostring()\n\n    result = {}\n    for i in range(0, outbytes, 40):\n        name = namestr[i:i+16].split(b""\\0"", 1)[0].decode(""ascii"")\n        ip   = namestr[i+20:i+24]\n        result[name] = format_ip(ip)\n    return result\n\n\ndef get_ip_can_access_internet(target=""hub.docker.com""):\n    """""" return None on error """"""\n    s = socket.socket(\n            socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((target, 80))\n    except socket.gaierror:\n        logger.exception(""failed to connect to %s"", target)\n        return None\n    except Exception:\n        logger.exception(""unknown exception when tying to connect %s"", target)\n\n    return s.getsockname()[0]\n\n\ndef try_to_get_right_interface(configured_ifs):\n    """""" try to return a right interface so that iftop can listen on """"""\n    ifs = get_interfaces()\n\n    logger.debug(""found interfaces %s"", ifs)\n\n    for interface in configured_ifs.split("",""):\n        interface = interface.strip()\n        if interface in ifs:\n            return interface\n\n    logger.info(""didn\'t find correct network interface in this node, configured %s, found %s"",\n            configured_ifs, ifs)\n\n    ip = get_ip_can_access_internet()\n    if ip is not None:\n        for if_name, if_ip in ifs.items():\n            if ip == if_ip:\n                return if_name\n\n    return None\n'"
src/job-exporter/src/nvidia.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport subprocess\nfrom xml.dom import minidom\nimport os\nimport logging\nimport re\n\nimport utils\n\nlogger = logging.getLogger(__name__)\n\ndef convert_to_byte(data):\n    data = data.lower()\n    number = float(re.findall(r""[0-9.]+"", data)[0])\n    if ""tb"" in data:\n        return number * 10 ** 12\n    elif ""gb"" in data:\n        return number * 10 ** 9\n    elif ""mb"" in data:\n        return number * 10 ** 6\n    elif ""kb"" in data:\n        return number * 10 ** 3\n    elif ""tib"" in data:\n        return number * 2 ** 40\n    elif ""gib"" in data:\n        return number * 2 ** 30\n    elif ""mib"" in data:\n        return number * 2 ** 20\n    elif ""kib"" in data:\n        return number * 2 ** 10\n    else:\n        return number\n\n\nclass EccError(object):\n    """""" EccError represents volatile count from one GPU card,\n    see https://developer.download.nvidia.com/compute/DCGM/docs/nvidia-smi-367.38.pdf for more info """"""\n    def __init__(self, single=0, double=0):\n        self.single = single\n        self.double = double\n\n    def __repr__(self):\n        return ""s: %d, d: %d"" % (self.single, self.double)\n\n    def __eq__(self, o):\n        return self.single == o.single and \\\n                self.double == o.double\n\n\nclass NvidiaGpuStatus(object):\n    """""" This object represents status of one GPU card, field meaning:\n        gpu_util the gpu util of this gpu, float number, range 0~100\n        gpu_mem_util the gpu memory usage/total of this gpu, float number, range 0~100\n        pids an array of pid numbers that uses this card\n        ecc_errors instance of EccError class\n        temperature will be None or float celsius """"""\n    def __init__(self, gpu_util, gpu_mem_util, pids, ecc_errors, minor, uuid, temperature):\n        self.gpu_util = gpu_util # float\n        self.gpu_mem_util = gpu_mem_util # float\n        self.pids = pids # list of int\n        self.ecc_errors = ecc_errors # list of EccError\n        self.minor = minor\n        self.uuid = uuid # str\n        self.temperature = temperature # None or float celsius\n\n    def __repr__(self):\n        return ""util: %.3f, mem_util: %.3f, pids: %s, ecc: %s, minor: %s, uuid: %s, temperature %.3f"" % \\\n                (self.gpu_util, self.gpu_mem_util, self.pids, self.ecc_errors, self.minor, self.uuid, self.temperature)\n\n    def __eq__(self, o): # for test\n        return self.gpu_util == o.gpu_util and \\\n                self.gpu_mem_util == o.gpu_mem_util and \\\n                self.pids == o.pids and \\\n                self.ecc_errors == o.ecc_errors and \\\n                self.minor == o.minor and \\\n                self.uuid == o.uuid and \\\n                self.temperature == o.temperature\n\n\ndef parse_smi_xml_result(smi):\n    """""" return a map, key is gpu_index(minor number or gpu sequence index) and gpu uuid, value is NvidiaGpuStatus """"""\n    xmldoc = minidom.parseString(smi)\n    gpus = xmldoc.getElementsByTagName(""gpu"")\n\n    result = {}\n\n    for index, gpu in enumerate(gpus):\n        if os.getenv(""LAUNCHER_TYPE"") == ""k8s"":\n            # For pai k8s, the minor number doesn\'t match the NVIDIA_VISIBLE_DEVICES number,\n            # use nvidia-smi gpu sequence index instead\n            gpu_index = index\n        else:\n            gpu_index = gpu.getElementsByTagName(""minor_number"")[0].childNodes[0].data\n        utilization = gpu.getElementsByTagName(""utilization"")[0]\n\n        gpu_util = utilization.getElementsByTagName(""gpu_util"")[0].childNodes[0].data.replace(""%"", """").strip()\n\n        gpu_mem_util = ""N/A""\n\n        memory_usage_list = gpu.getElementsByTagName(""fb_memory_usage"")\n        if len(memory_usage_list) != 0:\n            memory_usage = memory_usage_list[0]\n            mem_used = convert_to_byte(memory_usage.getElementsByTagName(""used"")[0].childNodes[0].data)\n            mem_total = convert_to_byte(memory_usage.getElementsByTagName(""total"")[0].childNodes[0].data)\n\n            if mem_total != 0:\n                gpu_mem_util = mem_used / mem_total * 100\n\n        if gpu_util == ""N/A"" or gpu_mem_util == ""N/A"":\n            continue\n\n        pids = []\n        processes = gpu.getElementsByTagName(""process_info"")\n        if len(processes) != 0:\n            for process in processes:\n                pids.append(int(\n                    process.getElementsByTagName(""pid"")[0].childNodes[0].data))\n\n        ecc_single = ecc_double = 0\n\n        """"""Here we try to get the ecc error count.\n        If there is no single_bit tag, it means that this GPU do not support \n        """"""\n        try:\n            ecc_errors = gpu.getElementsByTagName(""ecc_errors"")\n            if len(ecc_errors) > 0:\n                volatile = ecc_errors[0].getElementsByTagName(""volatile"")\n                if len(volatile) > 0:\n                    volatile = volatile[0]\n                    single = volatile.getElementsByTagName(""single_bit"")[0].getElementsByTagName(""total"")[0]\n                    double = volatile.getElementsByTagName(""double_bit"")[0].getElementsByTagName(""total"")[0]\n                    single = single.childNodes[0].data\n                    double = double.childNodes[0].data\n                    if single != ""N/A"":\n                        ecc_single = int(single)\n                    if double != ""N/A"":\n                        ecc_double = int(double)\n        except IndexError:\n            pass\n\n        uuid = gpu.getElementsByTagName(""uuid"")[0].childNodes[0].data\n\n        temperature = None\n        try:\n            temp_node = gpu.getElementsByTagName(""temperature"")\n            if len(temp_node) > 0:\n                temp_s = temp_node[0].getElementsByTagName(""gpu_temp"")[0].childNodes[0].data\n                temperature = float(re.findall(r""[0-9.]+"", temp_s)[0])\n        except Exception:\n            pass\n\n        status = NvidiaGpuStatus(\n                float(gpu_util),\n                float(gpu_mem_util),\n                pids,\n                EccError(single=ecc_single, double=ecc_double),\n                str(gpu_index),\n                uuid,\n                temperature)\n\n        result[str(gpu_index)] = result[uuid] = status\n\n    return result\n\ndef nvidia_smi(histogram, timeout):\n    try:\n        smi_output = utils.exec_cmd([""nvidia-smi"", ""-q"", ""-x""],\n                histogram=histogram, timeout=timeout)\n\n        return parse_smi_xml_result(smi_output)\n    except subprocess.CalledProcessError as e:\n        logger.exception(""command \'%s\' return with error (code %d): %s"",\n                e.cmd, e.returncode, e.output)\n    except subprocess.TimeoutExpired:\n        logger.warning(""nvidia-smi timeout"")\n    except Exception:\n        logger.exception(""exec nvidia-smi error"")\n\n    return None\n\ndef construct_gpu_info(statuses):\n    """""" util for unit test case """"""\n    m = {}\n    for status in statuses:\n        m[status.minor] = status\n        m[status.uuid] = status\n\n    return m\n'"
src/job-exporter/src/ps.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport subprocess\nimport logging\n\nimport utils\n\nlogger = logging.getLogger(__name__)\n\nclass ProcessInfo(object):\n    def __init__(self, pid, state, rss, cmd):\n        """""" pid is string type, rss is a number in byte """"""\n        self.pid = pid\n        self.state = state\n        self.rss = rss\n        self.cmd = cmd\n\ndef parse_result(ps):\n    result = []\n\n    for line in ps.split(""\\n""):\n        line = line.strip()\n        if len(line) == 0:\n            continue\n        parts = line.split()\n        state = parts[0]\n        rss = int(parts[1]) * 1024\n        pid = parts[2]\n        cmd = "" "".join(parts[3:])\n        result.append(ProcessInfo(pid, state, rss, cmd))\n\n    return result\n\ndef get_process_info(histogram, timeout):\n    try:\n        ps_output = utils.exec_cmd([""ps"", ""ax"", ""--no-headers"", ""--format"", ""state,rss,pid,cmd""],\n                histogram=histogram, timeout=timeout)\n\n        return parse_result(ps_output)\n    except subprocess.CalledProcessError as e:\n        logger.exception(""command \'%s\' return with error (code %d): %s"",\n                e.cmd, e.returncode, e.output)\n    except subprocess.TimeoutExpired:\n        logger.warning(""ps ax timeout"")\n    except Exception:\n        logger.exception(""exec ps ax error"")\n\n    return []\n'"
src/job-exporter/src/utils.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom enum import Enum\nimport os\nimport subprocess\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\ndef exec_cmd(*args, **kwargs):\n    """""" exec a cmd with timeout, also record time used using prometheus higtogram """"""\n    if kwargs.get(""histogram"") is not None:\n        histogram = kwargs.pop(""histogram"")\n    else:\n        histogram = None\n\n    logger.debug(""about to exec %s"", args[0])\n\n    if histogram is not None:\n        with histogram.time():\n            return subprocess.check_output(*args, **kwargs).decode(""utf-8"")\n    else:\n        return subprocess.check_output(*args, **kwargs).decode(""utf-8"")\n\n\ndef walk_json_field_safe(obj, *fields):\n    """""" for example a=[{""a"": {""b"": 2}}]\n    walk_json_field_safe(a, 0, ""a"", ""b"") will get 2\n    walk_json_field_safe(a, 0, ""not_exist"") will get None\n    """"""\n    try:\n        for f in fields:\n            obj = obj[f]\n        return obj\n    except:\n        return None\n\n\nclass GpuVendor(Enum):\n    UNKNOWN = ""unknown""\n    NVIDIA = ""nvidia""\n    AMD = ""amd""\n\ndef get_gpu_vendor():\n    nvidia_device_path = ""/dev/nvidiactl""\n    amd_device_path = ""/dev/kfd""\n    if os.path.exists(nvidia_device_path):\n        return GpuVendor.NVIDIA\n    if os.path.exists(amd_device_path):\n        return GpuVendor.AMD\n    return GpuVendor.UNKNOWN'"
src/job-exporter/test/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.'"
src/job-exporter/test/base.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport unittest\n\nclass TestBase(unittest.TestCase):\n    """"""\n    Test Base class for job-exporter\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s"",\n                level=logging.DEBUG)\n'"
src/job-exporter/test/test_amd.py,0,"b'import os\nimport sys\nimport unittest\n\nsys.path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../src""))\nimport amd\n\nPACKAGE_DIRECTORY_COM = os.path.dirname(os.path.abspath(__file__))\n\nclass TestAmd(unittest.TestCase):\n    def setUp(self):\n        try:\n            os.chdir(PACKAGE_DIRECTORY_COM)\n        except OSError:\n            pass\n\n    def test_parse_rocm_smi_result(self):\n        sample_path = ""data/rocm_smi.json""\n        with open(sample_path, ""r"") as f:\n            rocm_smi_result = f.read()\n        rocm_smi_parse_result = amd.parse_smi_json_result(rocm_smi_result)\n        expect = [{\n            ""pci_addr"": ""0000:03:00.0"",\n            ""temperature"": 31\n        }, {\n            ""pci_addr"": ""0000:06:00.0"",\n            ""temperature"": 25\n        }]\n        for e, v in zip(expect, rocm_smi_parse_result.values()):\n            self.assertEqual(e[""pci_addr""], v.pci_addr)\n            self.assertEqual(e[""temperature""], v.temperature)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_collector.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\nimport datetime\nimport time\nimport logging\n\nimport base\n\nsys.path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../src""))\n\nimport collector\nimport nvidia\nimport docker_inspect\nfrom collector import ContainerCollector\nfrom collector import GpuCollector\n\nlogger = logging.getLogger(__name__)\n\nclass TestContainerCollector(base.TestBase):\n    """"""\n    Test ContainerCollector in collecotr.py\n    """"""\n\n    def test_parse_from_labels(self):\n        inspect_result = docker_inspect.InspectResult(\n                ""openmindstudio"",\n                ""trialslot_nnimain_d65bc5ac"",\n                ""tuner"",\n                ""0"",\n                ""0,1,"",\n                ""application_1522829300813_1943"",\n                12345)\n\n        gpu_ids, labels = ContainerCollector.parse_from_labels(inspect_result, None)\n        self.assertEqual([""0"", ""1""], gpu_ids)\n\n        target_labels = {\n                ""username"": ""openmindstudio"",\n                ""job_name"": ""trialslot_nnimain_d65bc5ac"",\n                ""role_name"": ""tuner"",\n                ""task_index"": ""0"",\n                ""job_instance_id"": ""application_1522829300813_1943""}\n\n        self.assertEqual(target_labels, labels)\n\n    def test_infer_service_name(self):\n        self.assertIsNone(ContainerCollector.infer_service_name(\n            ""k8s_POD_alertmanager-7884c59f78-66r86_default_0a32e30a-f6ae-11e8""))\n\n        self.assertEqual(\n                ""alertmanager"",\n                ContainerCollector.infer_service_name(\n                    ""k8s_alertmanager_alertmanager-7884c59f78-66r86_default_0a32e30a-f6ae-11e8-a62d-000d3ab25bb6_2""))\n\n        self.assertIsNone(ContainerCollector.infer_service_name(\n            ""k8s_kube-scheduler_kube-scheduler-10.151.40.4_kube-system_f1164d931979939cf0601155df9c748a_6""))\n\n\nclass TestDockerCollector(base.TestBase):\n    """"""\n    Test DockerCollector in collector.py\n    """"""\n\n    def assert_metrics(self, metrics):\n        self.assertEqual(1, len(metrics))\n        self.assertEqual(1, len(metrics[0].samples))\n        sample = metrics[0].samples[0]\n        self.assertEqual(1, len(sample[1])) # label keys\n        self.assertEqual(1, sample[2]) # sample value\n\n    def test_impl(self):\n        _, c = collector.instantiate_collector(\n                ""test_docker_collector1"",\n                0.5,\n                datetime.timedelta(seconds=1),\n                collector.DockerCollector)\n\n        self.assert_metrics(c.collect_impl())\n\n    def test_base_collector(self):\n        """""" actually setup DockerCollector thread, and test, since this is multi-thread\n        test case, maybe sensitive to the system load """"""\n        ref = collector.make_collector(\n                ""test_docker_collector2"",\n                0.5,\n                datetime.timedelta(seconds=10),\n                collector.DockerCollector)\n\n        metrics = None\n        for i in range(20):\n            metrics = ref.get(datetime.datetime.now())\n            if metrics is not None:\n                break\n            time.sleep(0.1)\n\n        self.assert_metrics(metrics)\n\n\nclass TestZombieCollector(base.TestBase):\n    """"""\n    Test ZombieCollector in collector.py\n    """"""\n    def setUp(self):\n        # Because prometheus forbid same metric name, and we generate metric\n        # in from name, we need to differentiate name using time.\n        t = str(time.time()).replace(""."", ""_"")\n\n        decay_time = datetime.timedelta(seconds=1)\n        _, self.collector = collector.instantiate_collector(\n                ""test_zombie_collector"" + t,\n                0.5,\n                decay_time,\n                collector.ZombieCollector,\n                collector.AtomicRef(decay_time),\n                collector.AtomicRef(decay_time))\n\n    def test_update_zombie_count_type1(self):\n        start = datetime.datetime.now()\n\n        one_sec = datetime.timedelta(seconds=1)\n\n        type1_recorder = self.collector.type1_zombies\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type1({""a"", ""b""}, start))\n        self.assertEqual(2, len(type1_recorder))\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type1({""a"", ""b""},\n                    start + type1_recorder.decay_time - one_sec))\n        self.assertEqual(2, len(type1_recorder))\n\n        self.assertEqual({""a"", ""b""},\n                self.collector.update_zombie_count_type1({""a"", ""b""},\n                    start + type1_recorder.decay_time + one_sec))\n        self.assertEqual(2, len(type1_recorder))\n\n        self.assertEqual({""a""},\n                self.collector.update_zombie_count_type1({""a""},\n                    start + type1_recorder.decay_time + 2 *one_sec))\n        self.assertEqual(1, len(type1_recorder))\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type1({},\n                    start + type1_recorder.decay_time + 3 * one_sec))\n        self.assertEqual(0, len(type1_recorder))\n\n    def test_update_zombie_count_type2(self):\n        start = datetime.datetime.now()\n\n        one_sec = datetime.timedelta(seconds=1)\n\n        stats = {""43ffe701d883"":\n                    {""name"": ""core-caffe2_resnet50_20181012040921.586-container_e03_1539312078880_0780_01_000002"", ""id"": ""43ffe701d883""},\n                ""8de2f53e64cb"":\n                {""name"": ""container_e03_1539312078880_0780_01_000002"", ""id"": ""8de2f53e64cb""}}\n\n        type2_recorder = self.collector.type2_zombies\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type2(stats, start))\n\n        stats.pop(""8de2f53e64cb"")\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type2(stats, start + one_sec))\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type2(stats,\n                    start + type2_recorder.decay_time))\n\n        self.assertEqual({""43ffe701d883""},\n                self.collector.update_zombie_count_type2(stats,\n                    start + type2_recorder.decay_time + 2 * one_sec))\n\n        stats.pop(""43ffe701d883"")\n\n        self.assertEqual(set(),\n                self.collector.update_zombie_count_type2(stats,\n                    start + type2_recorder.decay_time + 3 * one_sec))\n\nclass TestGpuCollector(base.TestBase):\n    """"""\n    Test GpuCollector in collecotr.py\n    """"""\n\n    def make_pid_to_cid_fn(self, mapping):\n        def fn(pid):\n            if pid in mapping:\n                return True, mapping[pid]\n            return False, """"\n        return fn\n\n    def test_convert_to_metrics(self):\n        # sample may not ordered, and can not assertEqual directly, so tear them apart\n        gpu_info = nvidia.construct_gpu_info(\n                [nvidia.NvidiaGpuStatus(20, 21, [22, 33, 44], nvidia.EccError(), ""0"", ""GPU-uuid0"", 37.0)])\n\n        zombie_info = {""abc"", ""def""}\n\n        pid_to_cid_mapping = {33: ""def"", 22: ""ghi""} # only 33 is zombie\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        core_utils, mem_utils, ecc_errors, mem_leak, external_process, zombie_container, gpu_temp, _, _ = metrics\n\n        target_core_utils = collector.gen_nvidia_gpu_util_gauge()\n        target_core_utils.add_metric([""0""], 20)\n        self.assertEqual(target_core_utils, core_utils)\n\n        target_mem_utils = collector.gen_nvidia_gpu_mem_util_gauge()\n        target_mem_utils.add_metric([""0""], 21)\n        self.assertEqual(target_mem_utils, mem_utils)\n\n        target_ecc_errors = collector.gen_nvidia_gpu_ecc_counter()\n        target_ecc_errors.add_metric([""0"", ""single""], 0)\n        target_ecc_errors.add_metric([""0"", ""double""], 0)\n        self.assertEqual(target_ecc_errors, ecc_errors)\n\n        target_mem_leak = collector.gen_nvidia_gpu_memory_leak_counter()\n        self.assertEqual(target_mem_leak, mem_leak)\n\n        target_external_process = collector.gen_gpu_used_by_external_process_counter()\n        target_external_process.add_metric([""0"", ""44""], 1)\n        self.assertEqual(target_external_process, external_process)\n\n        target_zombie_container = collector.gen_gpu_used_by_zombie_container_counter()\n        target_zombie_container.add_metric([""0"", ""def""], 1)\n        self.assertEqual(target_zombie_container, zombie_container)\n\n        target_gpu_temp = collector.gen_nvidia_gpu_temperature_gauge()\n        target_gpu_temp.add_metric([""0""], 37.0)\n        self.assertEqual(target_gpu_temp, gpu_temp)\n\n        # test minor 1\n        gpu_info = nvidia.construct_gpu_info([\n            nvidia.NvidiaGpuStatus(30, 31, [55, 123], nvidia.EccError(single=2, double=3), ""1"", ""GPU-uuid1"", 24.0)])\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        core_utils, mem_utils, ecc_errors, mem_leak, external_process, zombie_container, gpu_temp, _, _ = metrics\n\n        target_core_utils = collector.gen_nvidia_gpu_util_gauge()\n        target_core_utils.add_metric([""1""], 30)\n        self.assertEqual(target_core_utils, core_utils)\n\n        target_mem_utils = collector.gen_nvidia_gpu_mem_util_gauge()\n        target_mem_utils.add_metric([""1""], 31)\n        self.assertEqual(target_mem_utils, mem_utils)\n\n        target_ecc_errors = collector.gen_nvidia_gpu_ecc_counter()\n        target_ecc_errors.add_metric([""1"", ""single""], 2)\n        target_ecc_errors.add_metric([""1"", ""double""], 3)\n        self.assertEqual(target_ecc_errors, ecc_errors)\n\n        target_mem_leak = collector.gen_nvidia_gpu_memory_leak_counter()\n        self.assertEqual(target_mem_leak, mem_leak)\n\n        target_external_process = collector.gen_gpu_used_by_external_process_counter()\n        target_external_process.add_metric([""1"", ""55""], 1)\n        target_external_process.add_metric([""1"", ""123""], 1)\n        self.assertEqual(target_external_process, external_process)\n\n        target_zombie_container = collector.gen_gpu_used_by_zombie_container_counter()\n        self.assertEqual(target_zombie_container, zombie_container)\n\n        target_gpu_temp = collector.gen_nvidia_gpu_temperature_gauge()\n        target_gpu_temp.add_metric([""1""], 24.0)\n        self.assertEqual(target_gpu_temp, gpu_temp)\n\n        # test minor 2\n        gpu_info = nvidia.construct_gpu_info([\n            nvidia.NvidiaGpuStatus(40, 20 * 1024 * 1024, [], nvidia.EccError(), ""2"", ""GPU-uuid2"", 30.0)])\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024 * 1024)\n\n        core_utils, mem_utils, ecc_errors, mem_leak, external_process, zombie_container, gpu_temp, _, _ = metrics\n\n        target_core_utils = collector.gen_nvidia_gpu_util_gauge()\n        target_core_utils.add_metric([""2""], 40)\n        self.assertEqual(target_core_utils, core_utils)\n\n        target_mem_utils = collector.gen_nvidia_gpu_mem_util_gauge()\n        target_mem_utils.add_metric([""2""], 20 * 1024 * 1024)\n        self.assertEqual(target_mem_utils, mem_utils)\n\n        target_ecc_errors = collector.gen_nvidia_gpu_ecc_counter()\n        target_ecc_errors.add_metric([""2"", ""single""], 0)\n        target_ecc_errors.add_metric([""2"", ""double""], 0)\n        self.assertEqual(target_ecc_errors, ecc_errors)\n\n        target_mem_leak = collector.gen_nvidia_gpu_memory_leak_counter()\n        self.assertEqual(target_mem_leak, mem_leak)\n\n        target_external_process = collector.gen_gpu_used_by_external_process_counter()\n        self.assertEqual(target_external_process, external_process)\n\n        target_zombie_container = collector.gen_gpu_used_by_zombie_container_counter()\n        self.assertEqual(target_zombie_container, zombie_container)\n\n        target_gpu_temp = collector.gen_nvidia_gpu_temperature_gauge()\n        target_gpu_temp.add_metric([""2""], 30.0)\n        self.assertEqual(target_gpu_temp, gpu_temp)\n\n        # test memory leak\n        gpu_info = nvidia.construct_gpu_info([\n            nvidia.NvidiaGpuStatus(40, 20 * 1024 * 1024 + 1, [], nvidia.EccError(), ""3"", ""GPU-uuid3"", 30.0)])\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        core_utils, mem_utils, ecc_errors, mem_leak, external_process, zombie_container, gpu_temp, _, _ = metrics\n\n        target_mem_leak = collector.gen_nvidia_gpu_memory_leak_counter()\n        target_mem_leak.add_metric([""3""], 1)\n        self.assertEqual(target_mem_leak, mem_leak)\n\n    def test_convert_to_metrics_with_no_zombie_info_BUGFIX(self):\n        gpu_info = nvidia.construct_gpu_info([\n            nvidia.NvidiaGpuStatus(20, 21, [22, 33, 44], nvidia.EccError(), ""0"", ""GPU-uuid0"", 40.0)])\n\n        # zombie_info is empty should also have external process metric\n        zombie_info = []\n\n        pid_to_cid_mapping = {33: ""def"", 22: ""ghi""} # only 44 is external process\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        _, _, _, _, external_process, zombie_container, _, _, _ = metrics\n\n        self.assertEqual(0, len(zombie_container.samples))\n        self.assertEqual(1, len(external_process.samples))\n        self.assertEqual(""0"", external_process.samples[0].labels[""minor_number""])\n        self.assertEqual(""44"", external_process.samples[0].labels[""pid""])\n\n        # zombie_info is None should also have external process metric\n        zombie_info = None\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        _, _, _, _, external_process, zombie_container, _, _, _ = metrics\n\n        self.assertEqual(0, len(zombie_container.samples))\n        self.assertEqual(1, len(external_process.samples))\n        self.assertEqual(""0"", external_process.samples[0].labels[""minor_number""])\n        self.assertEqual(""44"", external_process.samples[0].labels[""pid""])\n\n    def test_convert_to_metrics_with_real_id_BUGFIX(self):\n        gpu_info = nvidia.construct_gpu_info([\n            nvidia.NvidiaGpuStatus(20, 21, [22], nvidia.EccError(), ""0"", ""GPU-uuid0"", 50.0)])\n\n        # zombie_info is empty should also have external process metric\n        zombie_info = {""ce5de12d6275""}\n\n        pid_to_cid_mapping = {22: ""ce5de12d6275dc05c9ec5b7f58484f075f4775d8f54f6a4be3dc1439344df356""}\n\n        metrics = GpuCollector.convert_nvidia_gpu_info_to_metrics(gpu_info, zombie_info,\n                self.make_pid_to_cid_fn(pid_to_cid_mapping), 20 * 1024)\n\n        _, _, _, _, _, zombie_container, _, _, _ = metrics\n\n        self.assertEqual(1, len(zombie_container.samples))\n        self.assertEqual(""0"", zombie_container.samples[0].labels[""minor_number""])\n        self.assertEqual(""ce5de12d6275"", zombie_container.samples[0].labels[""container_id""])\n\nclass TestAtomicRef(base.TestBase):\n    """"""\n    Test AtomicRef in collecotr.py\n    """"""\n\n    def test_expiration(self):\n        ref = collector.AtomicRef(datetime.timedelta(seconds=10))\n\n        now = datetime.datetime.now()\n\n        delta = datetime.timedelta(seconds=1)\n\n        ref.set(1, now)\n\n        self.assertEquals(1, ref.get(now))\n        self.assertEquals(1, ref.get(now - delta))\n        self.assertEquals(1, ref.get(now + delta))\n        self.assertEquals(1, ref.get(now + delta * 10))\n        self.assertEquals(None, ref.get(now + delta * 11))\n        self.assertEquals(1, ref.get(now + delta * 10))\n\n        ref.set(2, now + delta)\n        self.assertEquals(2, ref.get(now))\n        self.assertEquals(2, ref.get(now + delta * 10))\n        self.assertEquals(2, ref.get(now + delta * 11))\n        self.assertEquals(None, ref.get(now + delta * 12))\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_docker_inspect.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nimport os\nimport unittest\n\nsys.path.append(\n    os.path.join(os.path.dirname(os.path.abspath(__file__)), ""../src""))\n\nfrom docker_inspect import parse_docker_inspect, InspectResult\nfrom utils import GpuVendor\n\nPACKAGE_DIRECTORY_COM = os.path.dirname(os.path.abspath(__file__))\n\n\nclass TestDockerInspect(unittest.TestCase):\n    """"""\n    Test docker_inspect.py\n    """"""\n\n    def setUp(self):\n        try:\n            os.chdir(PACKAGE_DIRECTORY_COM)\n        except OSError:\n            pass\n\n    def test_parse_docker_inspect(self):\n        sample_path = ""data/docker_inspect_sample.json""\n        with open(sample_path, ""r"") as f:\n            docker_inspect = f.read()\n\n        inspect_info = parse_docker_inspect(docker_inspect, GpuVendor.NVIDIA)\n        target_inspect_info = InspectResult(""openmindstudio"",\n                                            ""trialslot_nnimain_d65bc5ac"",\n                                            ""tuner"", ""0"", ""0,1,"",\n                                            ""application_1522829300813_1943"",\n                                            95539)\n\n        self.assertEqual(target_inspect_info, inspect_info)\n\n    def test_parse_docker_inspect_kube(self):\n        sample_path = ""data/docker_inspect_kube_launcher_task.json""\n        with open(sample_path, ""r"") as f:\n            docker_inspect = f.read()\n\n        inspect_info = parse_docker_inspect(docker_inspect, GpuVendor.NVIDIA)\n        target_inspect_info = InspectResult(\n            ""core"", ""core~tensorflowcifar10"", ""worker"", ""0"",\n            ""GPU-dc0671b0-61a4-443e-f456-f8fa6359b788"",\n            ""0_69c05215-46fa-11e9-8937-000d3ab38724"", 23774)\n        self.assertEqual(target_inspect_info, inspect_info)\n\n    def test_parse_docker_inspect_BUGFIX(self):\n        sample_path = ""data/inspect_result_bug_fix.json""\n        with open(sample_path, ""r"") as f:\n            docker_inspect = f.read()\n\n        inspect_info = parse_docker_inspect(docker_inspect, GpuVendor.NVIDIA)\n        target_inspect_info = InspectResult(\n            ""sokoya"",\n            ""sokoya~train-exp_offrl_sc_discard_0231-10th-beta07-lrfixed_13e9bf5_gCYv"",\n            ""train"", ""0"", ""3,2,1,0"", ""application_1553664769226_0080"", 30332)\n        self.assertEqual(target_inspect_info, inspect_info)\n\n    def test_adapt_dlts_jobs(self):\n        sample_path = ""data/dlts_docker_inspect.json""\n        with open(sample_path, ""r"") as f:\n            docker_inspect = f.read()\n\n        inspect_info = parse_docker_inspect(docker_inspect, GpuVendor.NVIDIA)\n        target_inspect_info = InspectResult(\n            ""dixu"", ""0c435eee-d31f-43d5-a1b3-442845fa1d0c"", None, None,\n            ""GPU-7c583998-b3ff-a885-8979-2d32d334cde4"", None, 3533)\n        self.assertEqual(target_inspect_info, inspect_info)\n\n    def test_parse_docker_inspect_amd(self):\n        sample_path = ""data/docker_inspect_amd.json""\n        with open(sample_path, ""r"") as f:\n            docker_inspect = f.read()\n\n        inspect_info = parse_docker_inspect(docker_inspect, GpuVendor.AMD)\n        target_inspect_info = InspectResult(\n            ""dgxadmin"", ""dgxadmin~rocm_pytorch_mnist_a"", ""worker"", ""0"",\n            ""0,1"", ""0_a12d82af-5ea6-11ea-8a2a-90b11c27f535"", 29647)\n        self.assertEqual(target_inspect_info, inspect_info)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_docker_stats.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\n\nimport base\n\nsys.path.append(os.path.abspath(""../src/""))\n\nfrom docker_stats import parse_docker_stats\nfrom docker_stats import convert_to_byte\nfrom docker_stats import parse_usage_limit\nfrom docker_stats import parse_io\nfrom docker_stats import parse_percentile\n\nclass TestDockerStats(base.TestBase):\n    """"""\n    Test docker_stats.py\n    """"""\n    def test_parse_docker_inspect(self):\n        sample_path = ""data/docker_stats_sample.txt""\n        with open(sample_path, ""r"") as f:\n            docker_stats = f.read()\n\n        stats_info = parse_docker_stats(docker_stats)\n        target_stats_info = {\'722dac0a62cf0243e63a268b8ef995e8386c185c712f545c0c403b295a529636\': {\'BlockIO\': {\'out\': 156000000.0, \'in\': 28600000.0}, \'NetIO\': {\'out\': 425000000000.0, \'in\': 1580000000000.0}, \'CPUPerc\': 0.00, \'MemPerc\': 0.19, \'id\': \'722dac0a62cf0243e63a268b8ef995e8386c185c712f545c0c403b295a529636\', \'MemUsage_Limit\': {\'usage\': 111149056.0, \'limit\': 59088012574.72}, \'name\': \'alert-manager\'}, \'33a22dcd4ba31ebc4a19fae865ee62285b6fae98a6ab72d2bc65e41cdc70e419\': {\'BlockIO\': {\'out\': 0.0, \'in\': 28000000.0}, \'NetIO\': {\'out\': 0.0, \'in\': 0.0}, \'CPUPerc\': 0.00, \'MemPerc\': 6.23, \'id\': \'33a22dcd4ba31ebc4a19fae865ee62285b6fae98a6ab72d2bc65e41cdc70e419\', \'MemUsage_Limit\': {\'usage\': 19587399.68, \'limit\': 314572800.0}, \'name\': \'prometheus\'}}\n        self.assertEqual(target_stats_info, stats_info)\n\n    def test_convert_to_byte(self):\n        self.assertEqual(380.4 * 2 ** 20, convert_to_byte(""380.4MiB""))\n        self.assertEqual(380.4 * 2 ** 20, convert_to_byte(""380.4mib""))\n        self.assertEqual(380.4 * 10 ** 6, convert_to_byte(""380.4MB""))\n\n    def test_parse_usage_limit(self):\n        data = ""380.4MiB / 55.03GiB""\n        result = parse_usage_limit(data)\n        target = {\'usage\': 380.4 * 2 ** 20, \'limit\': 55.03 * 2 ** 30}\n        self.assertEqual(target, result)\n\n    def test_parse_io(self):\n        data = ""0B / 0B""\n        result = parse_io(data)\n        target = {\'out\': 0.0, \'in\': 0.0}\n        self.assertEqual(target, result)\n\n    def test_parse_percentile(self):\n        data = ""24.45%""\n        result = parse_percentile(data)\n        target = 24.45\n        self.assertEqual(target, result)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_network.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\n\nimport base\n\nsys.path.append(os.path.abspath(""../src/""))\n\nimport network\n\nclass TestNetwork(base.TestBase):\n    """"""\n    Test network.py\n    """"""\n\n    def test_parse_lsof(self):\n        output = """"""COMMAND   PID USER   FD   TYPE    DEVICE SIZE/OFF NODE NAME\npython3 52485 dixu    5u  IPv4 420398429      0t0  TCP 10.150.148.166:43682->198.100.183.212:443 (ESTABLISHED)\n0t0 TCP 10.151.40.4:36090->10.151.40.4:8031 (ESTABLISHED)\n        """"""\n        result = network.parse_lsof(output)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_nvidia.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\n\nimport base\n\nsys.path.append(os.path.abspath(""../src/""))\n\nimport nvidia\n\nclass TestNvidia(base.TestBase):\n    """"""\n    Test nvidia.py\n    """"""\n    def test_parse_smi_xml_result(self):\n        sample_path = ""data/nvidia_smi_sample.xml""\n        with open(sample_path, ""r"") as f:\n            nvidia_smi_result = f.read()\n        nvidia_smi_parse_result = nvidia.parse_smi_xml_result(nvidia_smi_result)\n\n        zero = nvidia.NvidiaGpuStatus(100, 25, [1357, 2384, 3093], nvidia.EccError(),\n                ""0"", ""GPU-e511a7b2-f9d5-ba47-9b98-853732ca6c1b"", 60.0)\n        one = nvidia.NvidiaGpuStatus(98, 50, [3093], nvidia.EccError(),\n                ""1"", ""GPU-28daffaf-8abe-aaf8-c298-4bd13aecb5e6"", 59.0)\n\n        target_smi_info = {""1"": one, ""0"": zero, ""GPU-e511a7b2-f9d5-ba47-9b98-853732ca6c1b"": zero, ""GPU-28daffaf-8abe-aaf8-c298-4bd13aecb5e6"": one}\n\n        self.assertEqual(target_smi_info, nvidia_smi_parse_result)\n\n    def test_parse_smi_new_xml_result(self):\n        sample_path = ""data/nvidia_smi_sample_ecc_unsupported.xml""\n        with open(sample_path, ""r"") as f:\n            nvidia_smi_result = f.read()\n        nvidia_smi_parse_result = nvidia.parse_smi_xml_result(nvidia_smi_result)\n\n        zero = nvidia.NvidiaGpuStatus(0.000, 0.000, [], nvidia.EccError(),\n                ""0"", ""GPU-57567e11-0be2-381b-5132-2ad95c262e58"", 24.0)\n        one = nvidia.NvidiaGpuStatus(0.000, 0.000, [], nvidia.EccError(),\n                ""1"", ""GPU-ef1d0068-5bfd-f1e4-7e79-ff35d71d44b8"", 24.0)\n\n        target_smi_info = {""0"": zero, ""GPU-57567e11-0be2-381b-5132-2ad95c262e58"": zero, ""1"": one, ""GPU-ef1d0068-5bfd-f1e4-7e79-ff35d71d44b8"": one}\n\n        self.assertEqual(target_smi_info, nvidia_smi_parse_result)\n    \n    def test_parse_smi_out_of_order_xml_result(self):\n        sample_path = ""data/nvidia_smi_out_of_order.xml""\n        with open(sample_path, ""r"") as f:\n            nvidia_smi_result = f.read()\n\n        os.environ[""LAUNCHER_TYPE""] = ""k8s""\n        nvidia_smi_parse_result = nvidia.parse_smi_xml_result(nvidia_smi_result)\n\n        self.assertEqual(nvidia_smi_parse_result[""0""].gpu_util, 99.0)\n        self.assertEqual(nvidia_smi_parse_result[""1""].gpu_util, 99.0)\n        self.assertEqual(nvidia_smi_parse_result[""2""].gpu_util, 0.0)\n        self.assertEqual(nvidia_smi_parse_result[""3""].gpu_util, 0.0)\n\n        del os.environ[""LAUNCHER_TYPE""]\n\n    def test_exporter_will_not_report_unsupported_gpu(self):\n        sample_path = ""data/nvidia_smi_outdated_gpu.xml""\n        with open(sample_path, ""r"") as f:\n            nvidia_smi_result = f.read()\n        nvidia_smi_parse_result = nvidia.parse_smi_xml_result(nvidia_smi_result)\n\n        self.assertEqual({}, nvidia_smi_parse_result)\n\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_ps.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\n\nimport base\n\nsys.path.append(os.path.abspath(""../src/""))\n\nimport ps\n\nclass TestPS(base.TestBase):\n    """"""\n    Test ps.py\n    """"""\n    def test_parse_ps_result(self):\n        sample_path = ""data/ps_sample.txt""\n        with open(sample_path, ""r"") as f:\n            ps_result = f.read()\n        parse_result = ps.parse_result(ps_result)\n\n        self.assertEqual(4, len(parse_result))\n        self.assertEqual(""D"", parse_result[0].state)\n        self.assertEqual(""4"", parse_result[0].pid)\n        self.assertEqual(2 * 1024, parse_result[0].rss)\n        self.assertEqual(""/var/drivers/nvidia/current/bin/nvidia-smi -q -x"",\n                parse_result[0].cmd)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/job-exporter/test/test_utils.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\nimport subprocess\n\nimport base\n\nsys.path.append(os.path.abspath(""../src/""))\n\nimport utils\n\nclass TestUtils(base.TestBase):\n    """"""\n    Test utils.py\n    """"""\n    def test_walk_json_field_safe(self):\n        self.assertIsNone(utils.walk_json_field_safe(None, 1, ""abc""))\n        self.assertIsNone(utils.walk_json_field_safe([], 1, ""abc""))\n        self.assertIsNone(utils.walk_json_field_safe([{""abc""}], 1, ""abc""))\n        self.assertEqual(""345"",\n                utils.walk_json_field_safe([{""name"": ""123""}, {""name"": ""345""}], 1, ""name""))\n\n    def test_exec_cmd_with_0_return_value(self):\n        self.assertEqual(""10\\n"", utils.exec_cmd([""echo"", ""10""]))\n\n    def test_exec_cmd_with_timeout(self):\n        with self.assertRaises(subprocess.TimeoutExpired) as context:\n            utils.exec_cmd([""sleep"", ""10""], timeout=1)\n\n    def test_exec_cmd_with_non_0_return_value(self):\n        with self.assertRaises(subprocess.CalledProcessError) as context:\n            utils.exec_cmd([""false""])\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/k8s-dashboard/config/k8s_dashboard.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nfrom urlparse import urlparse\n\nclass K8SDashboard(object):\n\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def validation_post(self, conf):\n        return True, None\n\n    def run(self):\n        com_k8s_dashboard = {}\n\n        com_k8s_dashboard[\'api-servers-url\'] = self.cluster_conf[\'kubernetes\'][\'api-servers-url\']\n        dash_board_url = self.cluster_conf[\'kubernetes\'][\'dashboard-url\']\n        res = urlparse(dash_board_url)\n\n        machine_list = self.cluster_conf[\'machine-list\']\n        master_nodename = [host[\'nodename\'] for host in machine_list if host.get(\'hostip\') == res.hostname][0]\n        com_k8s_dashboard[\'dashboard-host\'] = master_nodename\n        com_k8s_dashboard[\'dashboard-port\'] = res.port\n\n        return com_k8s_dashboard'"
src/k8s-job-exit-spec/config/k8s_job_exit_spec.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass K8SJobExitSpec(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        return True, None\n'"
src/log-manager/config/log_manager.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass LogManager(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        port = conf[""log-manager""].get(""port"")\n        if type(port) != int:\n            msg = ""expect port in log-manager to be int but get %s with type %s"" % \\\n                    (port, type(port))\n            return False, msg\n        return True, None\n'"
src/node-exporter/config/node_exporter.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass NodeExporter(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        port = conf[""node-exporter""].get(""port"")\n        if type(port) != int:\n            msg = ""expect port in node-exporter to be int but get %s with type %s"" % \\\n                    (port, type(port))\n            return False, msg\n        return True, None\n'"
src/postgresql/config/postgresql.py,0,"b'#!/usr/bin/env python\n#\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport copy\nimport logging\n\n\nclass Postgresql(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = self.merge_service_configuration(service_conf, default_service_conf)\n        self.logger = logging.getLogger(__name__)\n\n    @staticmethod\n    def merge_service_configuration(overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg is None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n    def validation_pre(self):\n        if self.service_conf[\'enable\']:\n            machine_list = self.cluster_conf[\'machine-list\']\n            if len([host for host in machine_list if host.get(\'pai-master\') == \'true\']) < 1:\n                return False, \'""pai-master=true"" machine is required to deploy the postgresql service\'\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.service_conf)\n        if self.service_conf[\'enable\']:\n            machine_list = self.cluster_conf[\'machine-list\']\n            master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n            result[\'host\'] = master_ip\n            result[\'connection-str\'] = \'postgresql://{}:{}@{}:{}/{}\'.format(\n                result[\'user\'], result[\'passwd\'], result[\'host\'], result[\'port\'], result[\'db\'])\n        return result\n\n    def validation_post(self, conf):\n        if conf[\'internal-storage\'][\'enable\'] is False and conf[\'postgresql\'][\'enable\'] is True:\n            return False, ""You must set internal-storage.enable=true to use postgresql!""\n        return True, None\n'"
src/prometheus/config/prometheus.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass Prometheus(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def get_master_ip(self):\n        for host_conf in self.cluster_conf[""machine-list""]:\n            if ""pai-master"" in host_conf and host_conf[""pai-master""] == ""true"":\n                return host_conf[""hostip""]\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        result[""url""] = ""http://{0}:{1}"".format(self.get_master_ip(), result[""port""])\n        return result\n\n    def validation_post(self, conf):\n        error_msg =""expect %s in prometheus to be int but get %s with type %s""\n\n        port = conf[""prometheus""].get(""port"")\n        if type(port) != int:\n            return False, error_msg % (""port"", port, type(port))\n\n        interval = conf[""prometheus""].get(""scrape_interval"")\n        if type(interval) != int:\n            return False, error_msg % (""interval"", interval, type(interval))\n\n\n        return True, None\n'"
src/pylon/config/pylon.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nclass Pylon:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = dict(default_service_configuration,\n                                          **service_configuration)\n\n    #### Fist check, ensure all the configured data in cluster_configuration, service_configuration, default_service_configuration is right. And nothing is miss.\n    def validation_pre(self):\n        machine_list = self.cluster_configuration[\'machine-list\']\n        if len([host for host in machine_list if host.get(\'pai-master\') == \'true\']) != 1:\n            return False, \'1 and only 1 ""pai-master=true"" machine is required to deploy the rest server\'\n        return True, None\n\n    #### Generate the final service object model\n    def run(self):\n        # parse your service object model here, and return a generated dictionary\n\n        machine_list = self.cluster_configuration[\'machine-list\']\n        master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n        port = self.service_configuration[\'port\']\n        uri = \'http://{0}:{1}\'.format(master_ip, port)\n        uriHttps = \'https://{0}\'.format(master_ip)\n        if \'domain\' in self.service_configuration:\n            uri = \'http://{0}:{1}\'.format(self.service_configuration[\'domain\'], port)\n            uriHttps = \'https://{0}\'.format(self.service_configuration[\'domain\'])\n\n        webhdfs_legacy_port = self.service_configuration[\'webhdfs-legacy-port\']\n        ret = {\n            \'port\': port,\n            \'uri\': uri,\n            \'uri-https\': uriHttps,\n            \'webhdfs-legacy-port\': webhdfs_legacy_port,\n        }\n        if \'ssl\' in self.service_configuration:\n            ret[\'ssl\'] = self.service_configuration[\'ssl\']\n\n        return ret\n\n    #### All service and main module (kubrenetes, machine) is generated. And in this check steps, you could refer to the service object model which you will used in your own service, and check its existence and correctness.\n    def validation_post(self, cluster_object_model):\n        check_tuple = (\n            (\'rest-server\', \'uri\'),\n            (\'prometheus\', \'url\'),\n            (\'alert-manager\', \'url\'),\n            # TODO\n            # (\'kubernetes\', \'dashboard-url\'),\n            (\'grafana\', \'url\'),\n            (\'webportal\', \'uri\'),\n        )\n        if cluster_object_model[\'cluster\'][\'common\'][\'cluster-type\'] == \'yarn\':\n            check_tuple = ((\'hadoop-resource-manager\', \'master-ip\'), (\'hadoop-name-node\', \'master-ip\'),) + check_tuple\n        for (service, config) in check_tuple:\n            if service not in cluster_object_model or config not in cluster_object_model[service]:\n                return False, \'{0}.{1} is required\'.format(service, config)\n\n        return True, None\n'"
src/rest-server/config/rest_server.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nclass RestServer:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = dict(default_service_configuration,\n                                          **service_configuration)\n\n    #### Fist check, ensure all the configured data in cluster_configuration, service_configuration, default_service_configuration is right. And nothing is miss.\n    def validation_pre(self):\n        machine_list = self.cluster_configuration[\'machine-list\']\n        if \'default-pai-admin-username\' not in self.service_configuration:\n            return False, \'""default-pai-admin-username"" is required in rest-server\'\n        if \'default-pai-admin-password\' not in self.service_configuration:\n            return False, \'""default-pai-admin-password"" is required in rest-server\'\n        try:\n            reservation_time = int(self.service_configuration[\'debugging-reservation-seconds\'])\n        except ValueError:\n            return False, \'""debugging-reservation-seconds"" should be a positive integer.\'\n        if reservation_time <= 0:\n            return False, \'""debugging-reservation-seconds"" should be a positive integer.\'\n\n        return True, None\n\n    #### Generate the final service object model\n    def run(self):\n        # parse your service object model here, and return a generated dictionary\n\n        machine_list = self.cluster_configuration[\'machine-list\']\n        master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n        server_port = self.service_configuration[\'server-port\']\n\n        service_object_model = dict()\n\n        service_object_model[\'uri\'] = \'http://{0}:{1}\'.format(master_ip, server_port)\n        for k in [\n            \'server-port\', \'rate-limit-api-per-min\', \'rate-limit-list-job-per-min\',\n            \'rate-limit-submit-job-per-hour\', \'jwt-secret\', \'jwt-expire-time\',\n            \'default-pai-admin-username\', \'default-pai-admin-password\',\n            \'github-owner\', \'github-repository\', \'github-path\',\n            \'debugging-reservation-seconds\', \'enable-priority-class\',\n            \'schedule-port-start\', \'schedule-port-end\'\n        ]:\n            service_object_model[k] = self.service_configuration[k]\n        service_object_model[\'etcd-uris\'] = \',\'.join(\'http://{0}:4001\'.format(host[\'hostip\'])\n                                                     for host in machine_list\n                                                     if host.get(\'k8s-role\') == \'master\')\n        return service_object_model\n\n    #### All service and main module (kubrenetes, machine) is generated. And in this check steps, you could refer to the service object model which you will used in your own service, and check its existence and correctness.\n    def validation_post(self, cluster_object_model):\n        if cluster_object_model[\'cluster\'][\'common\'][\'cluster-type\'] == \'yarn\':\n            if \'yarn-frameworklauncher\' not in cluster_object_model or \'webservice\' not in cluster_object_model[\'yarn-frameworklauncher\']:\n                return False, \'yarn-frameworklauncher.webservice is required\'\n            if \'hadoop-name-node\' not in cluster_object_model or \'master-ip\' not in cluster_object_model[\'hadoop-name-node\']:\n                return False, \'hadoop-name-node.master-ip is required\'\n            if \'hadoop-resource-manager\' not in cluster_object_model or \'master-ip\' not in cluster_object_model[\'hadoop-resource-manager\']:\n                return False, \'hadoop-resource-manager.master-ip is required\'\n        if \'kubernetes\' not in cluster_object_model[\'layout\'] or \'api-servers-url\' not in cluster_object_model[\'layout\'][\'kubernetes\']:\n            return False, \'kubernetes.api-servers-url is required\'\n\n        return True, None\n'"
src/rest-server/deploy/legacy_user_migrate.py,0,"b'import logging\nimport logging.config\n\nimport sys\nimport argparse\nimport http.client\nimport json\nimport base64\n\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\nclass EtcdUser:\n\n    def __init__(self, user_name):\n        self.user_name = user_name\n        self.is_admin = \'false\'\n        self.pass_word = \'\'\n        self.virtual_cluster = \'default\'\n        self.github_PAT = \'\'\n\nclass TransferClient:\n\n    def __init__(self, etcd_uri, k8s_uri, in_cluster=False):\n        self.etcd_uri = etcd_uri\n        self.k8s_uri = k8s_uri\n        self.etcd_conn = http.client.HTTPConnection(self.etcd_uri)\n        self.flag_path = \'/v2/keys/transferFlag\'\n        self.etcd_prefix = \'/users/\'\n        self.secret_ns = ""pai-user""\n        self.in_cluster = in_cluster\n\n    def etcd_data_parse(self):\n        etcd_result = http_get(self.etcd_conn, ""/v2/keys/users?recursive=true"")\n        user_list = list()\n        if etcd_result[\'code\'] == 200:\n            user_data = json.loads(etcd_result[\'data\'])\n            for user in user_data[\'node\'][\'nodes\']:\n                user_name = user[\'key\'].replace(self.etcd_prefix,"""")\n                etcd_user = EtcdUser(user_name)\n                for info in user[\'nodes\']:\n                    if info[\'key\'] == user[\'key\'] + \'/passwd\':\n                        etcd_user.pass_word = info[\'value\']\n                    elif info[\'key\'] == user[\'key\'] + \'/admin\':\n                        etcd_user.is_admin = info[\'value\']\n                    elif info[\'key\'] == user[\'key\'] + \'/virtualClusters\':\n                        etcd_user.virtual_cluster = info[\'value\']\n                    elif info[\'key\'] == user[\'key\'] + \'/githubPAT\':\n                        etcd_user.github_PAT = info[\'value\']\n                user_list.append(etcd_user)\n        elif etcd_result[\'code\'] == 404:\n            logger.info(""No legacy user data found in etcd"")\n        else:\n            logger.error(""Check user data in etcd failed"")\n            sys.exit(1)\n        return user_list\n\n    def secret_data_prepare(self, user_info):\n        post_data_dict = dict()\n        meta_dict = dict()\n        user_dict = dict()\n        hex_key =    (\'\'.join([hex(ord(c)).replace(\'0x\', \'\') for c in user_info.user_name]))\n        meta_dict[\'name\'] = hex_key\n\n        encode_name = str(base64.b64encode(user_info.user_name.encode(\'utf-8\')), \'utf-8\')\n        encode_password = str(base64.b64encode(user_info.pass_word.encode(\'utf-8\')), \'utf-8\')\n        encode_admin = str(base64.b64encode(user_info.is_admin.encode(\'utf-8\')), \'utf-8\')\n        encode_vc = str(base64.b64encode(user_info.virtual_cluster.encode(\'utf-8\')), \'utf-8\')\n\n        user_dict[\'username\']= encode_name\n        user_dict[\'password\']= encode_password\n        user_dict[\'admin\'] = encode_admin\n        user_dict[\'virtualCluster\'] = encode_vc\n\n        if user_info.github_PAT:\n            encode_github = str(base64.b64encode((\'\'.join([hex(ord(c)).replace(\'0x\', \'\') for c in user_info.github_PAT])).encode(\'utf-8\')), \'utf-8\')\n            user_dict[\'githubPAT\'] = encode_github\n\n        post_data_dict[\'metadata\'] = meta_dict\n        post_data_dict[\'data\'] = user_dict\n\n        return post_data_dict\n\n    def list_all_secrets_from_namespace(self, namespace):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_response = api_instance.list_namespaced_secret(namespace)\n            return api_response.items\n        except ApiException as e:\n            if e.status == 404:\n                return []\n            logger.error(\'Exception when calling CoreV1Api->list_namespaced_secret: %s\\n\' % e)\n            sys.exit(1)\n\n    def create_group_if_not_exist(self, name):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_instance.read_namespace(name)\n        except ApiException as e:\n            if e.status == 404:\n                api_instance = client.CoreV1Api()\n                meta_data = client.V1ObjectMeta()\n                meta_data.name = name\n                body = client.V1Namespace(\n                  metadata=meta_data\n                )\n                api_instance.create_namespace(body)\n                return True\n            logger.error(""Failed to create namespace [{0}]"".format(name))\n            sys.exit(1)\n        return False\n\n    def create_secret_in_namespace_if_not_exist(self, payload, namespace):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_instance.read_namespaced_secret(payload[\'metadata\'][\'name\'], namespace)\n        except ApiException as e:\n            if e.status == 404:\n                try:\n                    api_instance = client.CoreV1Api()\n                    meta_data = client.V1ObjectMeta()\n                    meta_data.name = payload[\'metadata\'][\'name\']\n                    body = client.V1Secret(\n                        metadata=meta_data,\n                        data=payload[\'data\']\n                    )\n                    api_instance.create_namespaced_secret(namespace, body)\n                except ApiException as create_e:\n                    logger.error(""Exception when calling CoreV1Api->create_namespaced_secret: %s\\n"" % create_e)\n                    sys.exit(1)\n            else:\n                logger.error(""Exception when calling CoreV1Api->read_namespaced_secret: %s\\n"" % e)\n                sys.exit(1)\n\n    def prepare_secret_base_path(self):\n        self.create_group_if_not_exist(self.secret_ns)\n\n    def create_secret_user(self, payload):\n        self.create_secret_in_namespace_if_not_exist(payload, self.secret_ns)\n\n    def check_transfer_flag(self):\n        check_res = http_get(self.etcd_conn, self.flag_path)\n        if check_res[\'code\'] == 200:\n            return True\n        elif check_res[\'code\'] == 404:\n            return False\n        else:\n            logger.error(""Connect to etcd failed"")\n            sys.exit(1)\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\ndef http_get(conn, url, headers={}):\n    response_dict = dict()\n    conn.request(""GET"", url, headers=headers)\n    res = conn.getresponse()\n    data = res.read()\n    response_dict[\'code\'] = res.code\n    response_dict[\'data\'] = data.decode(\'utf-8\')\n    return response_dict\n\n\ndef http_post(conn, url, payload=None, headers={}):\n    response_dict = dict()\n    conn.request(""POST"", url, payload, headers)\n    res = conn.getresponse()\n    response_dict[\'code\'] = res.code\n    response_dict[\'data\'] = res.read().decode(\'utf-8\')\n    return response_dict\n\ndef main():\n    parser = argparse.ArgumentParser(description=""pai build client"")\n    parser.add_argument(\n        \'-e\', \'--etcdUri\',\n        type=str,\n        required=True)\n    parser.add_argument(\n        \'-k\', \'--k8sUri\',\n        type=str,\n        required=True)\n    parser.add_argument(\n        \'-i\', \'--incluster\',\n        required=False,\n        default=False,\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    etcd_uri = args.etcdUri.split(\',\')[0].replace(\'http://\',\'\')\n    in_cluster = args.incluster\n\n    logger.info(\'Starts to migrate legacy user data from etcd to kubernetes secrets\')\n\n    transferCli = TransferClient(etcd_uri, args.k8sUri.replace(\'http://\',\'\'), in_cluster)\n\n    if transferCli.check_transfer_flag() is False and in_cluster is False:\n        etcd_user_list = transferCli.etcd_data_parse()\n        if etcd_user_list:\n            transferCli.prepare_secret_base_path()\n            for user in etcd_user_list:\n                secret_post_data = transferCli.secret_data_prepare(user)\n                transferCli.create_secret_user(secret_post_data)\n        else:\n            logger.info(""No legacy data found"")\n        http_post(transferCli.etcd_conn, transferCli.flag_path)\n        logger.info(\'Legacy user data transfer from etcd to kubernetes secret (pai-user namespace) successfully\')\n    else:\n        logger.info(""Etcd data has already been transferred to k8s secret"")\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
src/rest-server/deploy/user_v2_migrate.py,0,"b'import logging\nimport logging.config\n\nimport sys\nimport argparse\nimport http.client\nimport json\nimport base64\n\nfrom kubernetes import client, config\nfrom kubernetes.client.rest import ApiException\n\ndef setup_logger_config(logger):\n    """"""\n    Setup logging configuration.\n    """"""\n    if len(logger.handlers) == 0:\n        logger.propagate = False\n        logger.setLevel(logging.DEBUG)\n        consoleHandler = logging.StreamHandler()\n        consoleHandler.setLevel(logging.DEBUG)\n        formatter = logging.Formatter(\'%(asctime)s [%(levelname)s] - %(filename)s:%(lineno)s : %(message)s\')\n        consoleHandler.setFormatter(formatter)\n        logger.addHandler(consoleHandler)\n\nlogger = logging.getLogger(__name__)\nsetup_logger_config(logger)\n\nclass EtcdUser:\n\n    def __init__(self, user_name):\n        self.user_name = user_name\n        self.is_admin = \'false\'\n        self.pass_word = \'\'\n        self.virtual_cluster = \'default\'\n        self.github_PAT = \'\'\n\nclass TransferClient:\n\n    def __init__(self, admin_groupname, in_cluster=False):\n        self.admin_group = admin_groupname\n        self.secret_ns = ""pai-user""\n        self.secret_ns_user_v2 = ""pai-user-v2""\n        self.secret_ns_group_v2 = ""pai-group""\n        self.vc_set = set()\n        self.in_cluster = in_cluster\n\n    def namespace_v1_data_prepare(self):\n        return self.list_all_secrets_from_namespace(\'pai-user\')\n\n    def secret_data_prepare_v2(self, user_info_item):\n        meta_dict = dict()\n        meta_dict[\'name\'] = user_info_item.metadata.name\n        grouplist = []\n        virtual_cluster = []\n        if base64.b64decode(user_info_item.data[\'admin\']).decode(\'utf-8\')== \'true\':\n            grouplist.append(self.admin_group)\n        if \'virtualCluster\' not in user_info_item.data:\n            user_info_item.data[\'virtualCluster\'] = \'\'\n        for vc_name in base64.b64decode(user_info_item.data[\'virtualCluster\']).decode(\'utf-8\').split(\',\'):\n            if vc_name == \'\':\n              continue\n            self.vc_set.add(vc_name)\n            grouplist.append(vc_name)\n            virtual_cluster.append(vc_name)\n        extension = {\n          \'virtualCluster\': virtual_cluster\n        }\n        if \'githubPAT\' in user_info_item.data and user_info_item.data[\'githubPAT\'] != \'\':\n            extension[\'githubPAT\'] = base64.b64decode(user_info_item.data[\'githubPAT\']).decode(\'utf-8\')\n        user_dict = {\n            \'username\': user_info_item.data[\'username\'],\n            \'password\': user_info_item.data[\'password\'],\n            \'email\': \'\',\n            \'grouplist\': str(base64.b64encode(json.dumps(grouplist).encode(\'utf-8\')), \'utf-8\'),\n            \'extension\': str(base64.b64encode(json.dumps(extension).encode(\'utf-8\')), \'utf-8\'),\n        }\n        post_data_dict = {}\n        post_data_dict[\'metadata\'] = meta_dict\n        post_data_dict[\'data\'] = user_dict\n\n        return post_data_dict\n\n    def secret_data_prepare_v2_group(self, groupname):\n        meta_dict = dict()\n        meta_dict[\'name\'] = (\'\'.join([hex(ord(c)).replace(\'0x\', \'\') for c in groupname]))\n        extension = {\n          \'groupType\': \'vc\'\n        }\n        group_dict = {\n            \'groupname\': str(base64.b64encode(groupname.encode(\'utf-8\')), \'utf-8\'),\n            \'description\': str(base64.b64encode(\'vc {0}\\\'s group\'.format(groupname).encode(\'utf-8\')), \'utf-8\'),\n            \'externalName\': str(base64.b64encode(\'\'.encode(\'utf-8\')), \'utf-8\'),\n            \'extension\': str(base64.b64encode(json.dumps(extension).encode(\'utf-8\')), \'utf-8\'),\n        }\n        post_data_dict = {}\n        post_data_dict[\'metadata\'] = meta_dict\n        post_data_dict[\'data\'] = group_dict\n        return post_data_dict\n\n    def load_v2_groups(self):\n        ns_pai_group_list = self.list_all_secrets_from_namespace(self.secret_ns_group_v2)\n        decode_group_list = []\n        vc_set = set()\n        for group in ns_pai_group_list:\n            try:\n                meta_dict = dict()\n                meta_dict[\'name\'] = bytes.fromhex(group.metadata.name).decode(\'utf-8\')\n                group_dict = {\n                    \'groupname\': str(base64.b64decode(group.data[\'groupname\'].encode(\'utf-8\')), \'utf-8\'),\n                    \'description\': str(base64.b64decode(group.data[\'description\'].encode(\'utf-8\')), \'utf-8\'),\n                    \'externalName\': str(base64.b64decode(group.data[\'externalName\'].encode(\'utf-8\')), \'utf-8\'),\n                    \'extension\': json.loads(str(base64.b64decode(group.data[\'extension\'].encode(\'utf-8\')), \'utf-8\')),\n                }\n                decode_group_list.append({\n                    \'metadata\': meta_dict,\n                    \'data\': group_dict,\n                })\n                if group_dict[\'extension\'].get(\'groupType\') == \'vc\':\n                    vc_set.add(group_dict[\'groupname\'])\n            except Exception as e:\n                logger.debug(""Filter the secret {0} in namespace {1} due to group schema."".format(group.metadata.name, self.secret_ns_group_v2))\n        return decode_group_list, vc_set\n\n    def convert_v2_group(self, data_dict, all_vcs):\n        extension_dict = data_dict[\'data\'][\'extension\']\n        if \'acls\' in extension_dict:\n            return data_dict, False\n\n        extension_dict[\'acls\'] = {\n            \'admin\': False,\n            \'virtualClusters\': [],\n        }\n        if \'groupType\' in extension_dict:\n            if extension_dict[\'groupType\'] == \'admin\':\n                extension_dict[\'acls\'][\'admin\'] = True\n                extension_dict[\'acls\'][\'virtualClusters\'].extend(all_vcs)\n            elif extension_dict[\'groupType\'] == \'vc\':\n                extension_dict[\'acls\'][\'virtualClusters\'].append(data_dict[\'data\'][\'groupname\'])\n\n        return data_dict, True\n\n    def update_v2_group(self, data_dict):\n        post_data_dict = {\n            \'metadata\': {\n                \'name\': data_dict[\'metadata\'][\'name\'].encode(\'utf-8\').hex()\n            },\n            \'data\': {\n                \'groupname\': str(base64.b64encode(data_dict[\'data\'][\'groupname\'].encode(\'utf-8\')), \'utf-8\'),\n                \'description\': str(base64.b64encode(data_dict[\'data\'][\'description\'].encode(\'utf-8\')), \'utf-8\'),\n                \'externalName\': str(base64.b64encode(data_dict[\'data\'][\'externalName\'].encode(\'utf-8\')), \'utf-8\'),\n                \'extension\': str(base64.b64encode(json.dumps(data_dict[\'data\'][\'extension\']).encode(\'utf-8\')), \'utf-8\'),\n            }\n\n        }\n        self.replace_secret_in_namespace(post_data_dict, self.secret_ns_group_v2)\n\n    def list_all_secrets_from_namespace(self, namespace):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_response = api_instance.list_namespaced_secret(namespace)\n            return api_response.items\n        except ApiException as e:\n            if e.status == 404:\n                return []\n            logger.error(\'Exception when calling CoreV1Api->list_namespaced_secret: %s\\n\' % e)\n            sys.exit(1)\n\n    def create_group_if_not_exist(self, name):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_instance.read_namespace(name)\n        except ApiException as e:\n            if e.status == 404:\n                api_instance = client.CoreV1Api()\n                meta_data = client.V1ObjectMeta()\n                meta_data.name = name\n                body = client.V1Namespace(\n                  metadata=meta_data\n                )\n                api_instance.create_namespace(body)\n                return True\n            logger.error(""Failed to create namespace [{0}]"".format(name))\n            sys.exit(1)\n        return False\n\n    def replace_secret_in_namespace(self, payload, namespace):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            meta_data = client.V1ObjectMeta()\n            meta_data.name = payload[\'metadata\'][\'name\']\n            body = client.V1Secret(\n              metadata=meta_data,\n              data=payload[\'data\']\n            )\n            # don\'t use patch, which can\'t handle empty string: https://github.com/kubernetes/kubernetes/issues/37216\n            api_instance.replace_namespaced_secret(payload[\'metadata\'][\'name\'], namespace, body)\n        except ApiException as e:\n            logger.error(""Exception when calling CoreV1Api->patch_namespaced_secret: %s\\n"" % e)\n            sys.exit(1)\n\n    def create_secret_in_namespace_if_not_exist(self, payload, namespace):\n        if self.in_cluster:\n            config.load_incluster_config()\n        else:\n            config.load_kube_config(config_file=""~/.kube/config"")\n        try:\n            api_instance = client.CoreV1Api()\n            api_instance.read_namespaced_secret(payload[\'metadata\'][\'name\'], namespace)\n        except ApiException as e:\n            if e.status == 404:\n                try:\n                    api_instance = client.CoreV1Api()\n                    meta_data = client.V1ObjectMeta()\n                    meta_data.name = payload[\'metadata\'][\'name\']\n                    body = client.V1Secret(\n                        metadata=meta_data,\n                        data=payload[\'data\']\n                    )\n                    api_instance.create_namespaced_secret(namespace, body)\n                except ApiException as create_e:\n                    logger.error(""Exception when calling CoreV1Api->create_namespaced_secret: %s\\n"" % create_e)\n                    sys.exit(1)\n            else:\n                logger.error(""Exception when calling CoreV1Api->read_namespaced_secret: %s\\n"" % e)\n                sys.exit(1)\n\n    def prepare_secret_base_path_v2(self):\n        status = [0, 0]\n        if self.create_group_if_not_exist(self.secret_ns_user_v2):\n            status[0] = 1\n        if self.create_group_if_not_exist(self.secret_ns_group_v2):\n            status[1] = 1\n        return status\n\n    def create_secret_user_v2(self, payload):\n        self.create_secret_in_namespace_if_not_exist(payload, self.secret_ns_user_v2)\n\n    def create_secret_group_v2(self, payload):\n        self.create_secret_in_namespace_if_not_exist(payload, self.secret_ns_group_v2)\n\ndef main():\n    parser = argparse.ArgumentParser(description=""pai build client"")\n    parser.add_argument(\n        \'-a\', \'--adminGroup\',\n        type=str,\n        required=True)\n    parser.add_argument(\n        \'-i\', \'--incluster\',\n        required=False,\n        default=False,\n        action=\'store_true\')\n    args = parser.parse_args()\n\n    admin_group_name = args.adminGroup\n    in_cluster = args.incluster\n\n    transferCli = TransferClient(admin_group_name, in_cluster)\n\n    res = transferCli.prepare_secret_base_path_v2()\n    if res[0] == 1 and res[1] == 1:\n        ns_pai_user_list = transferCli.namespace_v1_data_prepare()\n        for user in ns_pai_user_list:\n            try:\n                secret_post_data = transferCli.secret_data_prepare_v2(user)\n                transferCli.create_secret_user_v2(secret_post_data)\n            except Exception as e:\n                logger.debug(""skip the secret {0} in  secret_data_prepare_v2 "".format(user))\n        vc_set = transferCli.vc_set\n        for vc in vc_set:\n            secret_post_data = transferCli.secret_data_prepare_v2_group(vc)\n            transferCli.create_secret_group_v2(secret_post_data)\n        logger.info(\'Legacy user data transfer from namespace v1 to namespace v2 successfully\')\n    else:\n        logger.info(""Legacy data has already been transferred from v1 to v2. Skip it."")\n\n    group_list, vc_set = transferCli.load_v2_groups()\n    for group in group_list:\n        data_dict, updated = transferCli.convert_v2_group(group, vc_set)\n        if updated:\n            transferCli.update_v2_group(data_dict)\n\n\nif __name__ == ""__main__"":\n    main()\n\n'"
src/storage-manager/config/storage_manager.py,0,"b'#!/usr/bin/env python\n\nimport copy\n\nclass StorageManager(object):\n    def __init__(self, cluster_conf, service_conf, default_service_conf):\n        self.cluster_conf = cluster_conf\n        self.service_conf = service_conf\n        self.default_service_conf = default_service_conf\n\n    def validation_pre(self):\n        return True, None\n\n    def run(self):\n        result = copy.deepcopy(self.default_service_conf)\n        result.update(self.service_conf)\n        return result\n\n    def validation_post(self, conf):\n        security_type = conf[""storage-manager""].get(""security-type"")\n\n        if security_type != ""AUTO"" and security_type != ""ADS"":\n            msg = ""expect security_type in storage-manager to be AUTO or ADS but get %s"" % \\\n                    (security_type)\n            return False, msg\n        return True, None\n'"
src/tools/operator_wrapper/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\n\n__all__ = [""AlertOperator"", ""KubernetesOperator"", ""YarnOperator"", ""Resource"", ""RestserverOperator""]\n\n\nfrom .alert_operator import AlertOperator\nfrom .kubernetes_operator import KubernetesOperator\nfrom .yarn_operator import YarnOperator, Resource\nfrom .base_operator import BaseOperator\nfrom .restserver_operator import RestserverOperator\n'"
src/tools/operator_wrapper/alert_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport sys\n\nfrom .base_operator import BaseOperator\n\nlogger = logging.getLogger(__name__)\n\n\nclass AlertOperator(BaseOperator):\n    ALERT_TYPE = {\n        ""gpu_related"": {""NvidiaSmiLatencyTooLarge"", ""NvidiaSmiEccError"", ""NvidiaMemoryLeak"", ""NvidiaZombieProcess"", ""GpuUsedByExternalProcess"", ""GpuUsedByZombieContainer""},\n    }\n\n    def __init__(self, prometheus_ip, prometheus_port=9091):\n        super(AlertOperator, self).__init__(prometheus_ip, prometheus_port)\n\n    def get_gpu_alert_nodes(self):\n        api_path = ""/prometheus/api/v1/query?query=ALERTS""\n        alerts_info = self.request(api_path)\n\n        if alerts_info[""status""] != ""success"":\n            logger.error(""Alert response error: {}"".format(alerts_info[""data""]))\n            sys.exit(1)\n\n        alerts_info = alerts_info[""data""][""result""]\n        gpu_alert_nodes = {}\n        for alert in alerts_info:\n            metric = alert[""metric""]\n            if metric[""alertname""] in self.ALERT_TYPE[""gpu_related""] and metric[""alertstate""] == ""firing"":\n                node_ip = metric[""instance""].split(\':\')[0]\n                gpu_alert_nodes[node_ip] = metric[""alertname""]\n\n        return gpu_alert_nodes\n'"
src/tools/operator_wrapper/base_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport requests\nimport subprocess\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseOperator(object):\n    def __init__(self, master_ip, port):\n        self.master_ip = master_ip\n        self.port = port\n\n    def request(self, api_path, method=""get"", return_json=True, timeout=10,  **kwargs):\n\n        url = ""http://{}:{}{}"".format(self.master_ip, self.port, api_path)\n\n        logger.debug(""{}: {}"".format(method, url))\n        func = getattr(requests, method)\n        response = func(url, timeout=timeout, **kwargs)\n        response.raise_for_status()\n        if return_json:\n            return response.json()\n        else:\n            return response.text\n\n    def execute(self, command, redirect_stderr=True, shell=True, **kwargs):\n        logger.debug(command)\n        stderr = subprocess.STDOUT if redirect_stderr else None\n        output = subprocess.check_output(command, stderr=stderr, shell=shell, **kwargs)\n        try:\n            output = output.decode(""utf8"")\n        except AttributeError:\n            pass\n        return output\n\n\nif __name__ == ""__main__"":\n    pass\n\n'"
src/tools/operator_wrapper/kubernetes_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport sys\nsys.path.append(""../.."")\nfrom deployment.paiLibrary.common.kubernetes_handler import get_configmap, update_configmap\nfrom deployment.k8sPaiLibrary.maintainlib import common\n\n\nclass KubernetesOperator(object):\n    kubernetes_template = ""../../deployment/k8sPaiLibrary/template/config.template""\n    kube_config_path = ""./.config""\n    configmap_name = ""exclude-file""\n    configmap_data_key = ""nodes""\n\n    def __init__(self, master_ip):\n        self.master_ip = master_ip\n        self.setup_kubernetes_configfile(master_ip)\n\n    def setup_kubernetes_configfile(self, api_servers_ip):\n\n        template_data = common.read_template(self.kubernetes_template)\n        dict_map = {\n            ""cluster_cfg"": {""kubernetes"": {""api-servers-ip"": api_servers_ip}},\n        }\n        generated_data = common.generate_from_template_dict(template_data, dict_map)\n\n        common.write_generated_file(generated_data, self.kube_config_path)\n\n    def get_nodes(self):\n        configmap_info = get_configmap(self.kube_config_path, self.configmap_name)\n        nodes_str = configmap_info[""data""][self.configmap_data_key]\n        nodes = set(nodes_str.splitlines())\n        return nodes\n\n    def set_nodes(self, nodes):\n        nodes = set(nodes)\n        nodes_str = \'\\n\'.join(nodes)\n        data_dict = {self.configmap_data_key: nodes_str}\n        update_configmap(self.kube_config_path, self.configmap_name, data_dict)\n'"
src/tools/operator_wrapper/restserver_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport os\nimport json\nimport sys\n\nfrom base_operator import BaseOperator\n\nlogger = logging.getLogger(__name__)\n\n\nclass RestserverOperator(BaseOperator):\n\n    secret_file = "".restserver/user_info""\n\n    def __init__(self, restserver_ip, restserver_port=9186):\n        super(RestserverOperator, self).__init__(restserver_ip, restserver_port)\n        self.token = """"\n        self.load_token()\n\n    @classmethod\n    def setup_user(cls, username, password):\n        if not os.path.exists(os.path.dirname(cls.secret_file)):\n            os.mkdir(os.path.dirname(cls.secret_file))\n        with open(cls.secret_file, ""w"") as f:\n            data = {\n                ""username"": username,\n                ""password"": password\n            }\n            json.dump(data, f)\n\n    def load_token(self):\n        if not os.path.exists(self.secret_file):\n            return\n        with open(self.secret_file) as f:\n            data = json.load(f)\n        api_path = ""/api/v1/token""\n        headers = {\n            ""Content-Type"": ""application/x-www-form-urlencoded""\n        }\n        response = self.request(api_path, method=""post"", headers=headers, data=data)\n        self.token = response[""token""]\n\n    def get_vc(self):\n        api_path = ""/api/v1/virtual-clusters""\n        response = self.request(api_path)\n        return response\n\n    def add_vc(self, name, capacity=0, maxcapacity=0):\n        if self.token == """":\n            logger.error(""Anonymous user can\'t add vc, please setup user firstly"")\n            sys.exit(1)\n        api_path = ""/api/v1/virtual-clusters/{}"".format(name)\n        headers = {\n            ""Authorization"": ""Bearer "" + self.token\n        }\n        data = {\n            ""vcCapacity"": capacity,\n            ""vcMaxCapacity"": maxcapacity\n        }\n        response = self.request(api_path, method=""put"", headers=headers, data=data)\n        return response\n\n    def delete_vc(self, name):\n        if self.token == """":\n            logger.error(""Anonymous user can\'t delete vc, please setup user firstly"")\n            sys.exit(1)\n        api_path = ""/api/v1/virtual-clusters/{}"".format(name)\n        headers = {\n            ""Authorization"": ""Bearer "" + self.token\n        }\n        response = self.request(api_path, method=""delete"", headers=headers)\n        return response\n\n    def delete_group(self, name):\n        if self.token == """":\n            logger.error(""Anonymous user can\'t delete group, please setup user firstly"")\n            sys.exit(1)\n        api_path = ""/api/v2/group/{}"".format(name)\n        headers = {\n            ""Authorization"": ""Bearer "" + self.token\n        }\n        response = self.request(api_path, method=""delete"", headers=headers)\n        return response\n\n\nif __name__ == \'__main__\':\n    pass\n\n\n\n\n\n\n'"
src/tools/operator_wrapper/yarn_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport sys\nimport os\nimport re\nimport json\nfrom bs4 import BeautifulSoup\nimport dicttoxml\ndicttoxml.LOG.setLevel(logging.ERROR)\nimport time\nimport attr\nfrom attr.validators import instance_of\n\n\nfrom base_operator import BaseOperator\n\nlogger = logging.getLogger(__name__)\n\n@attr.s\nclass Resource(object):\n    cpus = attr.ib(converter=float, validator=instance_of(float))\n    gpus = attr.ib(converter=float, validator=instance_of(float))\n    memory = attr.ib(converter=float, validator=instance_of(float))\n\n    def __add__(self, other):\n        if isinstance(other, Resource):\n            cpus = self.cpus + other.cpus\n            gpus = self.gpus + other.gpus\n            memory = self.memory + other.memory\n            return Resource(cpus=cpus, gpus=gpus, memory=memory)\n        else:\n            raise NotImplemented\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        if isinstance(other, Resource):\n            cpus = self.cpus - other.cpus\n            gpus = self.gpus - other.gpus\n            memory = self.memory - other.memory\n            return Resource(cpus=cpus, gpus=gpus, memory=memory)\n        else:\n            raise NotImplemented\n\n    def __mul__(self, other):\n        if isinstance(other, (int, float)):\n            cpus = self.cpus * other\n            gpus = self.gpus * other\n            memory = self.memory * other\n            return Resource(cpus=cpus, gpus=gpus, memory=memory)\n        else:\n            raise NotImplemented\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __div__(self, other):\n        if isinstance(other, (int, float)):\n            cpus = self.cpus / other\n            gpus = self.gpus / other\n            memory = self.memory / other\n            return Resource(cpus=cpus, gpus=gpus, memory=memory)\n        else:\n            raise NotImplemented\n\n\nclass YarnOperator(BaseOperator):\n    yarn_config_path = ""./.hadoop""\n\n    def __init__(self, master_ip, port=8088):\n        super(YarnOperator, self).__init__(master_ip, port)\n        self.setup_yarn_configfile()\n\n    def setup_yarn_configfile(self):\n        if not os.path.exists(self.yarn_config_path):\n            os.mkdir(self.yarn_config_path)\n\n        yarn_config_str = \\\n        \'\'\'<configuration>\n            <property>\n                <name>yarn.resourcemanager.hostname</name>\n                <value>{}</value>\n            </property>\n        </configuration>\'\'\'.format(self.master_ip)\n\n        with open(os.path.join(self.yarn_config_path, ""yarn-site.xml""), \'w\') as f:\n            f.write(yarn_config_str)\n\n    def get_nodes_info(self):\n        api_path = ""/ws/v1/cluster/nodes""\n        nodes_info = self.request(api_path)\n        current_nodes = {}\n        for node in nodes_info[""nodes""][""node""]:\n            host = node[""nodeHostName""]\n            state = node[""state""]\n            node_label = node.get(""nodeLabels"", [""""])[0]\n            resource = Resource(**{\n                ""cpus"": node[""usedVirtualCores""] + node[""availableVirtualCores""],\n                ""memory"": node[""usedMemoryMB""] + node[""availMemoryMB""],\n                ""gpus"": node[""usedGPUs""] + node[""availableGPUs""]\n            })\n            current_nodes[host] = {\n                ""state"":  state,\n                ""nodeLabel"": node_label,\n                ""resource"": resource\n            }\n        return current_nodes\n\n    def decommission_nodes(self):\n        command = ""yarn --config {} rmadmin -refreshNodes -g -server"".format(self.yarn_config_path)\n        self.execute(command)\n\n    def get_cluster_labels(self):\n        # Sample output: ""Node Labels: <label_ex:exclusivity=true>,<label_non:exclusivity=false>""\n        # Sample output: ""Node Labels: ""\n        command = ""yarn --config {} cluster --list-node-labels"".format(self.yarn_config_path)\n\n        output = self.execute(command)\n\n        lines = output.split(""\\n"")\n        labels = dict()  # key: label name, value: exclusivity\n        for line in lines:\n            if not line.startswith(""Node Labels:""):\n                continue\n            line = line.lstrip(""Node Labels:"")\n            labels_str = line.split("","")\n            label_regex = r""<([a-zA-Z0-9][a-zA-Z0-9_\\-]*):exclusivity=(true|false)>""\n            for label_str in labels_str:\n                match = re.search(label_regex, label_str)\n                if match:\n                    label_name, exclusivity = match.groups()\n                    exclusivity = exclusivity == ""true""\n                    labels[label_name] = {""exclusive"": exclusivity}\n\n        return labels\n\n    def add_cluster_label(self, label, exclusivity=True):\n\n        label_str = ""{}(exclusive={})"".format(label, ""true"" if exclusivity else ""false"")\n\n        command = ""yarn --config {} rmadmin -addToClusterNodeLabels \\""{}\\"""".format(self.yarn_config_path, label_str)\n        self.execute(command)\n\n    def remove_cluster_label(self, label):\n\n        command = ""yarn --config {} rmadmin -removeFromClusterNodeLabels {}"".format(self.yarn_config_path, label)\n        self.execute(command)\n\n    def label_nodes(self, nodes, label):\n        if isinstance(nodes, str):\n            nodes = [nodes]\n\n        nodes_str_builder = []\n\n        for node in nodes:\n            node_str = ""{}={}"".format(node, label)\n            nodes_str_builder.append(node_str)\n\n        nodes_str = "" "".join(nodes_str_builder)\n\n        # yarn rmadmin -replaceLabelsOnNode ""node1[:port]=label1 node2=label2"" [-failOnUnknownNodes]\n        command = ""yarn --config {} rmadmin -replaceLabelsOnNode \\""{}\\"" -failOnUnknownNodes""\\\n            .format(self.yarn_config_path, nodes_str)\n\n        self.execute(command)\n\n    def get_queues_info(self):\n        api_path = ""/ws/v1/cluster/scheduler""\n        scheduler_info = self.request(api_path)\n\n        def traverse(queue_info, result_dict):\n            if queue_info[""type""] == ""capacitySchedulerLeafQueueInfo"":\n                result_dict[queue_info[""queueName""]] = {\n                    ""capacity"": queue_info[""absoluteCapacity""],\n                    ""maxCapacity"": queue_info[""absoluteMaxCapacity""],\n                    ""usedCapacity"": queue_info[""absoluteUsedCapacity""],\n                    ""numActiveJobs"": queue_info[""numActiveApplications""],\n                    ""numJobs"": queue_info[""numApplications""],\n                    ""numPendingJobs"": queue_info[""numPendingApplications""],\n                    ""resourcesUsed"": queue_info[""resourcesUsed""],\n                    ""state"": queue_info[""state""],\n                    ""nodeLabels"": queue_info[""nodeLabels""],\n                    ""capacities"": {\n                        partitionCapacities[""partitionName""]: {\n                            ""capacity"": partitionCapacities[""absoluteCapacity""],\n                            ""maxCapacity"": partitionCapacities[""absoluteMaxCapacity""],\n                            ""usedCapacity"": partitionCapacities[""absoluteUsedCapacity""],\n                        }\n                        for partitionCapacities in queue_info[""capacities""][""queueCapacitiesByPartition""]\n                    },\n                    ""preemptionDisabled"": queue_info.get(""preemptionDisabled"", False),\n                    ""defaultNodeLabelExpression"": queue_info.get(""defaultNodeLabelExpression"", """"),\n                }\n            elif queue_info[""type""] == ""capacityScheduler"":\n                for queue in queue_info[""queues""][""queue""]:\n                    traverse(queue, result_dict)\n            else:\n                logger.error(""unsupported scheduler type: {}"".format(queue_info[""type""]))\n                return\n\n        queues = {}\n        traverse(scheduler_info[""scheduler""][""schedulerInfo""], queues)\n        return queues\n\n    def get_resource_by_label(self):\n        api_path = ""/cluster/nodelabels""\n        html_text = self.request(api_path, return_json=False)\n\n        soup = BeautifulSoup(html_text)\n        result = soup.find(""table"", id=""nodelabels"")\n        tbody = result.find(""tbody"")\n        labels = tbody.find_all(""tr"")\n        labels_dict = {}\n        for label in labels:\n            label_dict = {}\n\n            label_name_raw, exclusive_raw, active_nm_raw, resources_raw = label.find_all(""td"")\n            label_name = label_name_raw.string.strip()\n            if label_name == ""<DEFAULT_PARTITION>"":\n                label_name = """"\n\n            exclusive = exclusive_raw.string.strip()\n            if exclusive == ""Exclusive Partition"":\n                label_dict[""exclusive""] = True\n            elif exclusive == ""Non Exclusive Partition"":\n                label_dict[""exclusive""] = False\n            else:\n                logger.error(""unknown exclusivity: {}"".format(exclusive))\n                sys.exit(1)\n\n            if active_nm_raw.find(\'a\'):\n                active_nm = active_nm_raw.find(\'a\').string.strip()\n            else:\n                active_nm = active_nm_raw.string.strip()\n            label_dict[""active_nm""] = int(active_nm)\n\n            resources = resources_raw.string.strip()\n            r_dict = {}\n            for resource in resources.strip(""<>"").split("",""):\n                r_type, r_quota = resource.split("":"")\n                r_dict[r_type.strip()] = int(r_quota)\n            label_dict[""resource""] = Resource(**{\n                ""cpus"": r_dict[""vCores""],\n                ""memory"": r_dict[""memory""],\n                ""gpus"": r_dict[""GPUs""]\n            })\n            labels_dict[label_name] = label_dict\n        return labels_dict\n\n    def add_dedicated_queue(self, label_name):\n\n        raw_dict = {\n            ""update-queue"": {\n                ""queue-name"": ""root.{}"".format(label_name),\n                ""params"": [\n                    {\n                        ""key"": ""capacity"",\n                        ""value"": 0\n                    },\n                    {\n                        ""key"": ""maximum-capacity"",\n                        ""value"": 0\n                    },\n                    {\n                        ""key"": ""default-node-label-expression"",\n                        ""value"": label_name\n                    },\n                    {\n                        ""key"": ""accessible-node-labels"",\n                        ""value"": label_name\n                    },\n                    {\n                        ""key"": ""disable_preemption"",\n                        ""value"": True\n                    },\n                    {\n                        ""key"": ""maximum-applications"",\n                        ""value"": 10000\n                    },\n                    {\n                        ""key"": ""user-limit-factor"",\n                        ""value"": 100\n                    }\n                ]\n\n            },\n            ""global-updates"": [\n                {\n                    ""key"": ""yarn.scheduler.capacity.root.accessible-node-labels.{}.capacity"".format(label_name),\n                    ""value"": 100\n                },\n                {\n                    ""key"": ""yarn.scheduler.capacity.root.{vc_name}.accessible-node-labels.{vc_name}.capacity"".format(vc_name=label_name),\n                    ""value"": 100\n                }\n            ]\n        }\n        request_xml = self.generate_queue_update_xml(raw_dict)\n\n        self.put_queue_update_xml(request_xml)\n\n    def remove_dedicated_queue(self, label_name):\n\n        raw_dict = {\n            ""update-queue"": {\n                ""queue-name"": ""root.{}"".format(label_name),\n                ""params"": [\n                    {\n                        ""key"": ""state"",\n                        ""value"": ""STOPPED""\n                    }\n                ]\n\n            },\n        }\n        request_xml = self.generate_queue_update_xml(raw_dict)\n\n        self.put_queue_update_xml(request_xml)\n        while True:\n            current_state = self.get_queues_info()[label_name][""state""]\n            if current_state == ""STOPPED"":\n                break\n            logger.info(""current vc status: {}. waiting..."".format(current_state))\n            time.sleep(5)\n\n        raw_dict = {\n            # ""remove-queue"": ""root.{}"".format(label_name),\n            ""global-updates"": [\n                {\n                    ""key"": ""yarn.scheduler.capacity.root.accessible-node-labels.{}.capacity"".format(label_name),\n                    ""value"": 0\n                },\n                {\n                    ""key"": ""yarn.scheduler.capacity.root.{vc_name}.accessible-node-labels.{vc_name}.capacity"".format(\n                        vc_name=label_name),\n                    ""value"": 0\n                },\n                {\n                    ""key"": ""yarn.scheduler.capacity.root.{vc_name}.default-node-label-expression"".format(\n                        vc_name=label_name),\n                    ""value"": None\n                }\n            ]\n        }\n        request_xml = self.generate_queue_update_xml(raw_dict)\n\n        self.put_queue_update_xml(request_xml)\n\n    def update_queue_capacity(self, update_dict):\n        # Todo: current we use global-updates to update capacity due to dicttoxml package limitation\n        # Todo: change it to update-queue after this pr: https://github.com/quandyfactory/dicttoxml/pull/64\n        raw_dict = {""global-updates"": []}\n        for queue, info in update_dict.items():\n            for attribute, value in info.items():\n                key = ""yarn.scheduler.capacity.root.{}.{}"".format(queue, attribute)\n                raw_dict[""global-updates""].append({\n                    ""key"": key,\n                    ""value"": value\n                })\n\n        request_xml = self.generate_queue_update_xml(raw_dict)\n        self.put_queue_update_xml(request_xml)\n\n    def generate_queue_update_xml(self, g_dict):\n        return dicttoxml.dicttoxml(g_dict, attr_type=False, custom_root=""sched-conf"", item_func=lambda x: ""entry"")\n\n    def put_queue_update_xml(self, update_xml):\n        api_path = ""/ws/v1/cluster/scheduler-conf""\n        headers = {""Content-Type"": ""application/xml""}\n        self.request(api_path, method=""put"", return_json=False, headers=headers, data=update_xml)\n\n\nif __name__ == ""__main__"":\n    pass\n\n'"
src/tools/tests/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/tools/tests/test_alert_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\n\nfrom mock import patch\nimport unittest\nfrom requests.exceptions import HTTPError\nimport json\nimport requests_mock\n\nfrom operator_wrapper.alert_operator import AlertOperator\n\n\nclass AlertOperatorTestCase(unittest.TestCase):\n    @staticmethod\n    def success_response():\n        response = {\n            ""status"": ""success"",\n            ""data"": {\n                ""resultType"": ""vector"",\n                ""result"": [\n                    {\n                        ""metric"": {\n                            ""__name__"": ""ALERTS"",\n                            ""alertname"": ""JobExporterHangs"",\n                            ""alertstate"": ""pending"",\n                            ""instance"": ""10.0.0.1:9102"",\n                            ""job"": ""pai_serivce_exporter"",\n                            ""name"": ""docker_daemon_collector"",\n                            ""pai_service_name"": ""job-exporter"",\n                            ""scraped_from"": ""job-exporter-p4skn"",\n                            ""type"": ""pai_service""\n                        },\n                        ""value"": [\n                            1558204199.489,\n                            ""1""\n                        ]\n                    },\n                    {\n                        ""metric"": {\n                            ""__name__"": ""ALERTS"",\n                            ""alertname"": ""NodeMemoryUsage"",\n                            ""alertstate"": ""firing"",\n                            ""instance"": ""10.0.0.2:9100"",\n                            ""job"": ""pai_serivce_exporter"",\n                            ""pai_service_name"": ""node-exporter"",\n                            ""scraped_from"": ""node-exporter-blkpp""\n                        },\n                        ""value"": [\n                            1558204199.489,\n                            ""1""\n                        ]\n                    },\n                    {\n                        ""metric"": {\n                            ""__name__"": ""ALERTS"",\n                            ""alertname"": ""NvidiaZombieProcess"",\n                            ""alertstate"": ""firing"",\n                            ""command"": ""nvidia-smi"",\n                            ""instance"": ""10.0.0.3:9102"",\n                            ""job"": ""pai_serivce_exporter"",\n                            ""pai_service_name"": ""job-exporter"",\n                            ""scraped_from"": ""job-exporter-t4sv6""\n                        },\n                        ""value"": [\n                            1558204199.489,\n                            ""1""\n                        ]\n                    }\n                ]\n            }\n        }\n\n        return response\n\n    @staticmethod\n    def failure_response():\n        response = {\n            ""status"": ""failure"",\n            ""data"": {\n            }\n        }\n        return response\n\n    def setUp(self):\n        self.alertOperator = AlertOperator(""localhost"")\n\n    def test__init__(self):\n        op = AlertOperator(""127.0.0.1"", ""5000"")\n        self.assertEqual(op.master_ip, ""127.0.0.1"")\n        self.assertEqual(op.port, ""5000"")\n\n    def test_get_gpu_alert_nodes_success(self):\n        with requests_mock.mock() as requests_get_mock:\n            requests_get_mock.get(""http://localhost:9091/prometheus/api/v1/query?query=ALERTS"", text=json.dumps(self.success_response()))\n\n            alerts = self.alertOperator.get_gpu_alert_nodes()\n\n            self.assertTrue(requests_get_mock.called)\n            self.assertDictEqual(alerts, {""10.0.0.3"": ""NvidiaZombieProcess""})\n\n    def test_get_gpu_alert_nodes_bad_request(self):\n        with requests_mock.mock() as requests_get_mock:\n            requests_get_mock.get(""http://localhost:9091/prometheus/api/v1/query?query=ALERTS"", status_code=404)\n\n            with self.assertRaises(HTTPError) as cm:\n                alerts = self.alertOperator.get_gpu_alert_nodes()\n\n            self.assertTrue(requests_get_mock.called)\n\n    def test_get_gpu_alert_nodes_fail_request(self):\n        with requests_mock.mock() as requests_get_mock:\n            requests_get_mock.get(""http://localhost:9091/prometheus/api/v1/query?query=ALERTS"", text=json.dumps(self.failure_response()))\n\n            with self.assertRaises(SystemExit) as cm:\n                alerts = self.alertOperator.get_gpu_alert_nodes()\n\n            self.assertTrue(requests_get_mock.called)\n            self.assertEqual(cm.exception.code, 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
src/tools/tests/test_kubernetes_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\n\nfrom mock import patch\nimport unittest\n\nfrom operator_wrapper.kubernetes_operator import KubernetesOperator\n\n\nclass KubernetesOperatorTestCase(unittest.TestCase):\n\n    def setUp(self):\n        with patch(""operator_wrapper.kubernetes_operator.KubernetesOperator.setup_kubernetes_configfile""):\n            self.kubernetesOperator = KubernetesOperator(""localhost"")\n\n    @patch(""operator_wrapper.kubernetes_operator.KubernetesOperator.setup_kubernetes_configfile"")\n    def test__init__(self, setup_kubernetes_configfile):\n        KubernetesOperator(""127.0.0.1"")\n        setup_kubernetes_configfile.assert_called_with(""127.0.0.1"")\n\n    @patch(""operator_wrapper.kubernetes_operator.common"")\n    def test_setup_kubernetes_configfile(self, common_mock):\n        common_mock.read_template.return_value = ""test""\n        common_mock.generate_from_template_dict.return_value = ""test2""\n\n        self.kubernetesOperator.setup_kubernetes_configfile(""localhost"")\n\n        common_mock.read_template.assert_called_with(self.kubernetesOperator.kubernetes_template)\n        dict_map = {\n            ""cluster_cfg"": {""kubernetes"": {""api-servers-ip"": ""localhost""}},\n        }\n        common_mock.generate_from_template_dict.assert_called_with(""test"", dict_map)\n        common_mock.write_generated_file.assert_called_with(""test2"", self.kubernetesOperator.kube_config_path)\n\n    @patch(""operator_wrapper.kubernetes_operator.get_configmap"")\n    def test_get_nodes(self, get_configmap_mock):\n        get_configmap_mock.return_value = {\n            ""data"": {self.kubernetesOperator.configmap_data_key: ""10.0.0.1\\n10.0.0.2""}\n        }\n\n        nodes = self.kubernetesOperator.get_nodes()\n\n        self.assertSetEqual(nodes, {""10.0.0.1"", ""10.0.0.2""})\n\n    @patch(""operator_wrapper.kubernetes_operator.update_configmap"")\n    def test_set_nodes(self, update_configmap_mock):\n        nodes = {""10.0.0.3"", ""10.0.0.4""}\n        nodes_str = ""\\n"".join(nodes)\n\n        self.kubernetesOperator.set_nodes(nodes)\n\n        update_configmap_mock.assert_called_with(self.kubernetesOperator.kube_config_path,\n                                                 self.kubernetesOperator.configmap_name,\n                                                 {self.kubernetesOperator.configmap_data_key: nodes_str})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
src/tools/tests/test_node_maintain.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import,print_function\n\nfrom mock import patch, call\nimport unittest\nimport requests_mock\nfrom collections import namedtuple\nimport json\nimport sys\nfrom StringIO import StringIO\nimport xmltodict\nimport os\n\nfrom node_maintain import add_dedicate_vc, get_dedicate_vc, remove_dedicate_vc\n\n\nclass NodeMaintainTestCase(unittest.TestCase):\n    ArgsMock = namedtuple(""ArgsMock"", [""resource_manager_ip"", ""vc_name"", ""nodes"", ""restserver_ip""])\n\n    @classmethod\n    def setUpClass(cls):\n        with open(""capacity_scheduler_case1.json"") as f:\n            cls.capacity_scheduler_response = f.read()\n        with open(""cluster_nodes_case1.json"") as f:\n            cls.cluster_nodes_response = f.read()\n\n    def setUp(self):\n        if not os.path.exists("".restserver""):\n            os.mkdir("".restserver"")\n        with open("".restserver/user_info"", ""w"") as f:\n            json.dump({""username"": ""test"", ""password"": ""test""}, f)\n\n    @requests_mock.Mocker()\n    def test_get_dedicate_vc(self, requests_get_mock):\n        args = self.ArgsMock(resource_manager_ip=""127.0.0.1"", vc_name=None, nodes=None)\n\n        requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/scheduler"", text=self.capacity_scheduler_response)\n        requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/nodes"", text=self.cluster_nodes_response)\n\n        with patch(""sys.stdout"", new=StringIO()) as stdout_mock:\n            get_dedicate_vc(args)\n            output = stdout_mock.getvalue().strip()\n\n        output_lines = set([line.strip() for line in output.split(""\\n"")])\n        self.assertSetEqual(\n            {u""test_vc:"", u""Nodes:"", u""Nodes: 10.151.40.132"", u""Resource: <CPUs:0.0, Memory:0.0MB, GPUs:0.0>"", u""label_ex:"",\n             u""Resource: <CPUs:24.0, Memory:208896.0MB, GPUs:4.0>""}, output_lines)\n\n    @patch(""node_maintain.YarnOperator.execute"")\n    def test_add_dedicate_vc(self, execute_mock):\n        args = self.ArgsMock(resource_manager_ip=""127.0.0.1"", restserver_ip=""127.0.0.1"", vc_name=""test_vc_2"", nodes={""10.151.40.132""})\n\n        execute_mock.side_effect = [\n            ""Node Labels: <label_ex:exclusivity=true>,<label_non:exclusivity=false>,<test_vc:exclusivity=true"",\n            None,\n            None,\n        ]\n        with requests_mock.mock() as requests_get_mock:\n            requests_get_mock.post(""http://127.0.0.1:9186/api/v1/token"", text=json.dumps({""token"": ""test""}))\n            requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/scheduler"", text=self.capacity_scheduler_response)\n            nodes_info = json.loads(self.cluster_nodes_response)\n            nodes_info[""nodes""][""node""][0].pop(""nodeLabels"")\n            requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/nodes"", text=json.dumps(nodes_info))\n            requests_get_mock.put(""http://127.0.0.1:9186/api/v1/virtual-clusters/test_vc_2"", text=""{}"")\n            requests_get_mock.put(""http://127.0.0.1:8088/ws/v1/cluster/scheduler-conf"")\n\n            add_dedicate_vc(args)\n\n            yarn_command_call = [\n                call(""yarn --config ./.hadoop cluster --list-node-labels""),\n                call(""yarn --config ./.hadoop rmadmin -addToClusterNodeLabels \\""test_vc_2(exclusive=true)\\""""),\n                call(""yarn --config ./.hadoop rmadmin -replaceLabelsOnNode \\""10.151.40.132=test_vc_2\\"" -failOnUnknownNodes"")\n            ]\n            execute_mock.assert_has_calls(yarn_command_call, any_order=False)\n\n            scheduler_conf_call = [request_object for request_object in requests_get_mock.request_history if\n                                   request_object.path == ""/ws/v1/cluster/scheduler-conf""]\n            self.assertEqual(len(scheduler_conf_call), 2)\n            add_queue, update_capacity = [xmltodict.parse(request_object.text) for request_object in\n                                                         scheduler_conf_call]\n            global_update = {or_dict[""key""]: or_dict[""value""] for or_dict in add_queue[""sched-conf""][""global-updates""][""entry""]}\n            global_update_expect = {\n                u""yarn.scheduler.capacity.root.test_vc_2.accessible-node-labels.test_vc_2.capacity"": u""100"",\n                u""yarn.scheduler.capacity.root.accessible-node-labels.test_vc_2.capacity"": u""100""\n            }\n            self.assertDictEqual(global_update, global_update_expect)\n            queue_name = add_queue[""sched-conf""][""update-queue""][""queue-name""]\n            queue_name_expect = u""root.test_vc_2""\n            self.assertEqual(queue_name, queue_name_expect)\n            queue_update = {or_dict[""key""]: or_dict[""value""] for or_dict in add_queue[""sched-conf""][""update-queue""][""params""][""entry""]}\n            queue_update_expect = {\n                u""capacity"": u""0"",\n                u""accessible-node-labels"": u""test_vc_2"",\n                u""user-limit-factor"": u""100"",\n                u""default-node-label-expression"": u""test_vc_2"",\n                u""maximum-applications"": u""10000"",\n                u""maximum-capacity"": u""0"",\n                u""disable_preemption"": u""True""\n            }\n            self.assertDictEqual(queue_update, queue_update_expect)\n\n            update_capacity = {or_dict[""key""]: or_dict[""value""] for or_dict in\n                               update_capacity[""sched-conf""][""global-updates""][""entry""]}\n            update_capacity_expect = {\n                u""yarn.scheduler.capacity.root.default.maximum-capacity"": u""100.0"",\n                u""yarn.scheduler.capacity.root.label_ex.disable_preemption"": u""True"",\n                u""yarn.scheduler.capacity.root.test_vc.capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.label_ex.maximum-capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.vc_a.disable_preemption"": u""True"",\n                u""yarn.scheduler.capacity.root.vc_a.capacity"": u""20.0"",\n                u""yarn.scheduler.capacity.root.vc_a.maximum-capacity"": u""20.0"",\n                u""yarn.scheduler.capacity.root.default.capacity"": u""80.0"",\n                u""yarn.scheduler.capacity.root.test_vc.disable_preemption"": u""True"",\n                u""yarn.scheduler.capacity.root.test_vc.maximum-capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.label_ex.capacity"": u""0.0""\n            }\n            self.assertDictEqual(update_capacity, update_capacity_expect)\n\n\n    @patch(""node_maintain.YarnOperator.execute"")\n    def test_remove_dedicate_vc(self, execute_mock):\n        args = self.ArgsMock(resource_manager_ip=""127.0.0.1"", restserver_ip=""127.0.0.1"", vc_name=""test_vc"", nodes=None)\n\n        execute_mock.side_effect = [\n            None,\n            ""Node Labels: <label_ex:exclusivity=true>,<label_non:exclusivity=false>,<test_vc:exclusivity=true>"",\n            None\n        ]\n        with requests_mock.mock() as requests_get_mock:\n            requests_get_mock.post(""http://127.0.0.1:9186/api/v1/token"", text=json.dumps({""token"": ""test""}))\n            requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/scheduler"",\n                                  text=self.capacity_scheduler_response)\n            requests_get_mock.get(""http://127.0.0.1:8088/ws/v1/cluster/nodes"", text=self.cluster_nodes_response)\n            requests_get_mock.put(""http://127.0.0.1:8088/ws/v1/cluster/scheduler-conf"")\n            requests_get_mock.delete(""http://127.0.0.1:9186/api/v1/virtual-clusters/test_vc"", text=""{}"")\n\n            remove_dedicate_vc(args)\n\n            yarn_command_call = [\n                call(""yarn --config ./.hadoop rmadmin -replaceLabelsOnNode \\""10.151.40.132=\\"" -failOnUnknownNodes""),\n                call(""yarn --config ./.hadoop cluster --list-node-labels""),\n                call(""yarn --config ./.hadoop rmadmin -removeFromClusterNodeLabels test_vc"")\n            ]\n            execute_mock.assert_has_calls(yarn_command_call, any_order=False)\n\n            scheduler_conf_call = [request_object for request_object in requests_get_mock.request_history if request_object.path == ""/ws/v1/cluster/scheduler-conf""]\n            self.assertEqual(len(scheduler_conf_call), 3)\n            update_capacity, stop_queue, remove_queue = [xmltodict.parse(request_object.text) for request_object in scheduler_conf_call]\n            update_capacity = {or_dict[""key""]: or_dict[""value""] for or_dict in update_capacity[""sched-conf""][""global-updates""][""entry""]}\n            update_capacity_expect = {\n                u""yarn.scheduler.capacity.root.default.maximum-capacity"": u""100.0"",\n                u""yarn.scheduler.capacity.root.test_vc.capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.label_ex.maximum-capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.vc_a.maximum-capacity"": u""5.0"",\n                u""yarn.scheduler.capacity.root.vc_a.capacity"": u""5.0"",\n                u""yarn.scheduler.capacity.root.default.capacity"": u""95.0"",\n                u""yarn.scheduler.capacity.root.test_vc.maximum-capacity"": u""0.0"",\n                u""yarn.scheduler.capacity.root.label_ex.capacity"": u""0.0""\n            }\n            self.assertDictEqual(update_capacity, update_capacity_expect)\n\n            remove_queue = {or_dict[""key""]: or_dict[""value""] for or_dict in\n                            remove_queue[""sched-conf""][""global-updates""][""entry""]}\n            remove_queue_expect = {\n                u""yarn.scheduler.capacity.root.accessible-node-labels.test_vc.capacity"": u""0"",\n                u""yarn.scheduler.capacity.root.test_vc.accessible-node-labels.test_vc.capacity"": u""0"",\n                u""yarn.scheduler.capacity.root.test_vc.default-node-label-expression"": None,\n            }\n            self.assertDictEqual(remove_queue, remove_queue_expect)\n\n\nif __name__ == ""__main__"":\n    assert not hasattr(sys.stdout, ""getvalue"")\n    unittest.main(module=__name__, buffer=True, exit=False)\n'"
src/tools/tests/test_yarn_operator.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nfrom __future__ import absolute_import\n\nfrom mock import patch\nimport unittest\n\nfrom operator_wrapper.yarn_operator import YarnOperator\n\n\nclass YarnOperatorTestCase(unittest.TestCase):\n\n    def setUp(self):\n        with patch(""operator_wrapper.yarn_operator.YarnOperator.setup_yarn_configfile""):\n            self.yarnOperator = YarnOperator(""localhost"")\n\n    @patch(""operator_wrapper.yarn_operator.YarnOperator.setup_yarn_configfile"")\n    def test__init__(self, setup_yarn_configfile):\n        YarnOperator(""127.0.0.1"")\n        setup_yarn_configfile.assert_called_with()\n\n\n    def test_generate_queue_update_xml(self):\n        from collections import OrderedDict\n        from xml.dom.minidom import parseString\n        raw_dict = OrderedDict([\n            (""global-updates"", [\n                OrderedDict([(""key"", ""yarn.scheduler.capacity.root.default.default-node-label-expression""),\n                             (""value"", ""label_non"")]),\n                OrderedDict([(""key"", ""yarn.scheduler.capacity.root.default.accessible-node-labels.label_ex.capacity""),\n                             (""value"", 0)]),\n\n            ])\n        ])\n        dom = parseString(self.yarnOperator.generate_queue_update_xml(raw_dict))\n        expect_output = \'\'\'<?xml version=""1.0"" ?>\n<sched-conf>\n\t<global-updates>\n\t\t<entry>\n\t\t\t<key>yarn.scheduler.capacity.root.default.default-node-label-expression</key>\n\t\t\t<value>label_non</value>\n\t\t</entry>\n\t\t<entry>\n\t\t\t<key>yarn.scheduler.capacity.root.default.accessible-node-labels.label_ex.capacity</key>\n\t\t\t<value>0</value>\n\t\t</entry>\n\t</global-updates>\n</sched-conf>\n\'\'\'\n        self.assertEquals(dom.toprettyxml(), expect_output)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
src/tools/utility/__init__.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n'"
src/tools/utility/common.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef safe_get(dct, *keys):\n    for key in keys:\n        try:\n            dct = dct[key]\n        except KeyError:\n            return None\n    return dct\n'"
src/tools/utility/log.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport yaml\nimport logging\nimport logging.config\nimport os\n\ndef setup_logging(default_path=""config/logging.yaml"", default_level=logging.INFO, env_key=""LOG_CFG""):\n    path = default_path\n    value = os.getenv(env_key, None)\n    if value:\n        path = value\n    if os.path.exists(path):\n        with open(path, ""rt"") as f:\n            config = yaml.safe_load(f)\n        logging.config.dictConfig(config)\n    else:\n        logging.basicConfig(level=default_level)'"
src/webportal/config/webportal.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport json\nimport urllib\nimport urlparse\n\n\nclass Webportal:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = dict(default_service_configuration,\n                                          **service_configuration)\n        self.service_configuration[\'plugins\'] = []\n        self.service_configuration[\'plugins\'].extend(default_service_configuration.get(\'plugins\', []))\n        self.service_configuration[\'plugins\'].extend(service_configuration.get(\'plugins\', []))\n\n    #### Fist check, ensure all the configured data in cluster_configuration, service_configuration, default_service_configuration is right. And nothing is miss.\n    def validation_pre(self):\n        machine_list = self.cluster_configuration[\'machine-list\']\n        if len([host for host in machine_list if host.get(\'pai-master\') == \'true\']) != 1:\n            return False, \'1 and only 1 ""pai-master=true"" machine is required to deploy the rest server\'\n        return True, None\n\n    #### Generate the final service object model\n    def run(self):\n        # parse your service object model here, and return a generated dictionary\n\n        def apply_config(plugin):\n            uri = plugin[\'uri\']\n            if \'config\' in plugin:\n                # Python 2 only uses urlquote_plus in urlencode\n                config_query = urllib.urlencode(plugin[\'config\'], True).replace(\'+\', \'%20\')\n                uri = urlparse.urljoin(uri, \'?\' + config_query)\n            return {\n                \'id\': plugin.get(\'id\'),\n                \'title\': plugin[\'title\'],\n                \'uri\': uri,\n            }\n\n        machine_list = self.cluster_configuration[\'machine-list\']\n        master_ip = [host[\'hostip\'] for host in machine_list if host.get(\'pai-master\') == \'true\'][0]\n        server_port = self.service_configuration[\'server-port\']\n        uri = \'http://{0}:{1}\'.format(master_ip, server_port)\n        plugins = self.service_configuration[\'plugins\']\n        return {\n            \'server-port\': server_port,\n            \'uri\': uri,\n            \'plugins\': json.dumps([apply_config(plugin) for plugin in plugins]),\n            \'webportal-address\': master_ip,\n        }\n\n    #### All service and main module (kubrenetes, machine) is generated. And in this check steps, you could refer to the service object model which you will used in your own service, and check its existence and correctness.\n    def validation_post(self, cluster_object_model):\n        check_tuple = (\n            (\'rest-server\', \'uri\'),\n            (\'prometheus\', \'url\'),\n            (\'grafana\', \'url\'),\n            # TODO\n            # (\'kubernetes\', \'dashboard-url\'),\n            (\'node-exporter\', \'port\'),\n            (\'prometheus\', \'scrape_interval\'),\n        )\n        if cluster_object_model[\'cluster\'][\'common\'][\'cluster-type\'] == \'yarn\':\n            check_tuple = ((\'hadoop-resource-manager\', \'master-ip\'),)+check_tuple\n        for (service, config) in check_tuple:\n            if service not in cluster_object_model or config not in cluster_object_model[service]:\n                return False, \'{0}.{1} is required\'.format(service, config)\n\n        return True, None\n'"
src/yarn-exporter/src/yarn_exporter.py,0,"b'#!/usr/bin/env python3\n# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport urllib.parse\nimport argparse\nimport signal\nimport faulthandler\nimport gc\nimport logging\nimport os\nimport sys\nfrom collections import defaultdict\n\nfrom prometheus_client.core import GaugeMetricFamily, REGISTRY\nfrom prometheus_client import Histogram\nfrom prometheus_client.twisted import MetricsResource\n\nimport requests\n\nfrom twisted.web.server import Site\nfrom twisted.web.resource import Resource\nfrom twisted.internet import reactor\n\nlogger = logging.getLogger(__name__)\n\n##### yarn-exporter will generate following metrics\n\ncluster_scheduler_histogram = Histogram(""yarn_api_cluster_scheduler_latency_seconds"",\n        ""Resource latency for requesting yarn api /ws/v1/cluster/scheduler"")\n\ncluster_nodes_histogram = Histogram(""yarn_api_cluster_nodes_latency_seconds"",\n        ""Resource latency for requesting yarn api /ws/v1/cluster/nodes"")\n\ndef gen_active_node_count():\n    return GaugeMetricFamily(""yarn_nodes_active"", ""active node count in yarn"")\n\ndef gen_queue_cpu_available():\n    return GaugeMetricFamily(""yarn_queue_cpu_available"", ""available cpu in queue"",\n            labels=[""queue""])\n\ndef gen_queue_cpu_cap():\n    return GaugeMetricFamily(""yarn_queue_cpu_total"", ""total cpu in queue"",\n            labels=[""queue""])\n\ndef gen_queue_mem_available():\n    return GaugeMetricFamily(""yarn_queue_mem_available"", ""available mem in queue"",\n            labels=[""queue""])\n\ndef gen_queue_mem_cap():\n    return GaugeMetricFamily(""yarn_queue_mem_total"", ""total mem in queue"",\n            labels=[""queue""])\n\ndef gen_queue_gpu_available():\n    return GaugeMetricFamily(""yarn_queue_gpu_available"", ""available gpu in queue"",\n            labels=[""queue""])\n\ndef gen_queue_gpu_cap():\n    return GaugeMetricFamily(""yarn_queue_gpu_total"", ""total gpu in queue"",\n            labels=[""queue""])\n\ndef gen_queue_running_jobs():\n    return GaugeMetricFamily(""yarn_queue_running_job"", ""total running job count in queue"",\n            labels=[""queue""])\n\ndef gen_queue_pending_jobs():\n    return GaugeMetricFamily(""yarn_queue_pending_job"", ""total pending job count in queue"",\n            labels=[""queue""])\n\ndef gen_queue_running_containers():\n    return GaugeMetricFamily(""yarn_queue_running_container"", ""total running container count in queue"",\n            labels=[""queue""])\n\ndef gen_queue_pending_containers():\n    return GaugeMetricFamily(""yarn_queue_pending_container"", ""total pending container count in queue"",\n            labels=[""queue""])\n\ndef gen_node_cpu_total():\n    return GaugeMetricFamily(""yarn_node_cpu_total"", ""total cpu core in node"",\n            labels=[""node_ip""])\n\ndef gen_node_cpu_available():\n    return GaugeMetricFamily(""yarn_node_cpu_available"", ""available cpu core in node"",\n            labels=[""node_ip""])\n\ndef gen_node_mem_total():\n    return GaugeMetricFamily(""yarn_node_mem_total"", ""total mem in node"",\n            labels=[""node_ip""])\n\ndef gen_node_mem_available():\n    return GaugeMetricFamily(""yarn_node_mem_available"", ""available mem in node"",\n            labels=[""node_ip""])\n\ndef gen_node_gpu_total():\n    return GaugeMetricFamily(""yarn_node_gpu_total"", ""total gpu in node"",\n            labels=[""node_ip""])\n\ndef gen_node_gpu_available():\n    return GaugeMetricFamily(""yarn_node_gpu_available"", ""available gpu in node"",\n            labels=[""node_ip""])\n\ndef gen_yarn_exporter_error():\n    return GaugeMetricFamily(""yarn_exporter_error_count"", ""error count yarn exporter encountered"",\n            labels=[""error""])\n\n##### yarn-exporter will generate above metrics\n\ndef request_with_histogram(url, histogram, *args, **kwargs):\n    with histogram.time():\n        return requests.get(url, *args, **kwargs)\n\nclass ResourceItem(object):\n    def __init__(self, cpu=0, mem=0, gpu=0):\n        self.cpu = cpu\n        self.mem = mem\n        self.gpu = gpu\n\n    def __add__(self, other):\n        if isinstance(other, ResourceItem):\n            cpu = self.cpu + other.cpu\n            gpu = self.gpu + other.gpu\n            mem = self.mem + other.mem\n            return ResourceItem(cpu=cpu, gpu=gpu, mem=mem)\n        else:\n            raise NotImplemented\n\n    def __radd__(self, other):\n        return self + other\n\n    def __sub__(self, other):\n        if isinstance(other, ResourceItem):\n            cpu = self.cpu - other.cpu\n            gpu = self.gpu - other.gpu\n            mem = self.mem - other.mem\n            return ResourceItem(cpu=cpu, gpu=gpu, mem=mem)\n        else:\n            raise NotImplemented\n\n    def __mul__(self, other):\n        if isinstance(other, (int, float)):\n            cpu = self.cpu * other\n            gpu = self.gpu * other\n            mem = self.mem * other\n            return ResourceItem(cpu=cpu, gpu=gpu, mem=mem)\n        else:\n            raise NotImplemented\n\n    def __rmul__(self, other):\n        return self * other\n\n    def __truediv__(self, other):\n        if isinstance(other, (int, float)):\n            cpu = self.cpu / other\n            gpu = self.gpu / other\n            mem = self.mem / other\n            return ResourceItem(cpu=cpu, gpu=gpu, mem=mem)\n        else:\n            raise NotImplemented\n\n    def __repr__(self):\n        return \'<cpu: {}, gpu: {}, mem: {}>\'.format(self.cpu, self.gpu, self.mem)\n\n    def __str__(self):\n        return \'<cpu: {}, gpu: {}, mem: {}>\'.format(self.cpu, self.gpu, self.mem)\n\n    def __eq__(self, other):\n        if isinstance(other, ResourceItem):\n            return self.cpu == other.cpu and self.gpu == other.gpu and self.mem == other.mem\n        else:\n            raise NotImplemented\n\n\nclass NodeCount(object):\n    def __init__(self, total=0, active=0):\n        self.total = total\n        self.active = active\n\n\nclass YarnCollector(object):\n    def __init__(self, yarn_url):\n        self.nodes_url = urllib.parse.urljoin(yarn_url, ""/ws/v1/cluster/nodes"")\n        self.scheduler_url = urllib.parse.urljoin(yarn_url, ""/ws/v1/cluster/scheduler"")\n\n    def collect(self):\n        error_counter = gen_yarn_exporter_error()\n\n        response = None\n\n        # nodes_url\n        try:\n            response = request_with_histogram(self.nodes_url, cluster_nodes_histogram,\n                    allow_redirects=True)\n        except Exception as e:\n            error_counter.add_metric([str(e)], 1)\n            logger.exception(e)\n\n        node_count = NodeCount()\n\n        labeled_resource = defaultdict(ResourceItem)\n\n        if response is not None:\n            if response.status_code != 200:\n                msg = ""requesting %s with code %d"" % (self.nodes_url, response.status_code)\n                logger.warning(msg)\n                error_counter.add_metric([msg], 1)\n            else:\n                try:\n                    metrics = YarnCollector.gen_nodes_metrics(response.json(),\n                            node_count, labeled_resource)\n                    for metric in metrics:\n                        yield metric\n                except Exception as e:\n                    error_counter.add_metric([str(e)], 1)\n                    logger.exception(e)\n\n        nodes_active = gen_active_node_count()\n        nodes_active.add_metric([], node_count.active)\n        yield nodes_active\n\n        # scheduler_url\n        response = None\n        try:\n            response = request_with_histogram(self.scheduler_url, cluster_scheduler_histogram,\n                    allow_redirects=True)\n        except Exception as e:\n            error_counter.add_metric([str(e)], 1)\n            logger.exception(e)\n\n        if response is not None:\n            if response.status_code != 200:\n                msg = ""requesting %s with code %d"" % (self.scheduler_url, response.status_code)\n                logger.warning(msg)\n                error_counter.add_metric([msg], 1)\n            else:\n                try:\n                    metrics = YarnCollector.gen_scheduler_metrics(response.json(),\n                            labeled_resource)\n                    for metric in metrics:\n                        yield metric\n                except Exception as e:\n                    error_counter.add_metric([str(e)], 1)\n                    logger.exception(e)\n\n        yield error_counter\n\n    @staticmethod\n    def gen_nodes_metrics(obj, node_count, labeled_resource):\n        if obj[""nodes""] is None:\n            return []\n\n        nodes = obj[""nodes""][""node""]\n\n        node_total_cpu = gen_node_cpu_total()\n        node_avail_cpu = gen_node_cpu_available()\n        node_total_mem = gen_node_mem_total()\n        node_avail_mem = gen_node_mem_available()\n        node_total_gpu = gen_node_gpu_total()\n        node_avail_gpu = gen_node_gpu_available()\n\n        total_node = active_node = 0\n\n        for node in nodes:\n            total_node += 1\n            if node[""state""] not in {""RUNNING"", ""DECOMMISSIONING""}:\n                continue\n            active_node += 1\n\n            total_cpu = node[""usedVirtualCores""] + node[""availableVirtualCores""]\n            avail_cpu = node[""availableVirtualCores""]\n            total_mem = (node[""usedMemoryMB""] + node[""availMemoryMB""]) * 1024 * 1024\n            avail_mem = node[""availMemoryMB""] * 1024 * 1024\n\n            ip = node[""nodeHostName""]\n            node_total_cpu.add_metric([ip],\n                    node[""usedVirtualCores""] + node[""availableVirtualCores""])\n            node_avail_cpu.add_metric([ip], node[""availableVirtualCores""])\n            node_total_mem.add_metric([ip],\n                    (node[""availMemoryMB""] + node[""usedMemoryMB""]) * 1024 * 1024)\n            node_avail_mem.add_metric([ip], node[""availMemoryMB""] * 1024 * 1024)\n            if node.get(""availableGPUs"") is None and node.get(""usedGPUs"") is None:\n                continue\n\n            total_gpu = node[""availableGPUs""] + node[""usedGPUs""]\n            avail_gpu = node[""availableGPUs""]\n\n            node_total_gpu.add_metric([ip], node[""availableGPUs""] + node[""usedGPUs""])\n            node_avail_gpu.add_metric([ip], node[""availableGPUs""])\n\n            node_label = node.get(""nodeLabels"", [""""])[0]\n\n            labeled_resource[node_label] += ResourceItem(cpu=total_cpu, mem=total_mem, gpu=total_gpu)\n\n        node_count.total = total_node\n        node_count.active = active_node\n\n        return [node_total_cpu, node_avail_cpu,\n                node_total_mem, node_avail_mem,\n                node_total_gpu, node_avail_gpu]\n\n    @staticmethod\n    def gen_scheduler_metrics(obj, labeled_resource):\n        if obj[""scheduler""] is None:\n            return []\n\n        scheduler_info = obj[""scheduler""][""schedulerInfo""]\n\n        cpu_cap = gen_queue_cpu_cap()\n        cpu_avail = gen_queue_cpu_available()\n        mem_cap = gen_queue_mem_cap()\n        mem_avail = gen_queue_mem_available()\n        gpu_cap = gen_queue_gpu_cap()\n        gpu_avail = gen_queue_gpu_available()\n\n        running_jobs = gen_queue_running_jobs()\n        pending_jobs = gen_queue_pending_jobs()\n        running_containers = gen_queue_running_containers()\n        pending_containers = gen_queue_pending_containers()\n\n        for queue in scheduler_info[""queues""][""queue""]:\n            queue_name = queue[""queueName""]\n            queue_nodelabel = queue.get(""defaultNodeLabelExpression"", """")\n            if queue_nodelabel == ""<DEFAULT_PARTITION>"":\n                queue_nodelabel = """"\n\n            queue_capacity = queue\n            for label_capacity in queue[""capacities""][""queueCapacitiesByPartition""]:\n                if label_capacity[""partitionName""] == queue_nodelabel:\n                    queue_capacity =label_capacity\n                    break\n\n            queue_resource_used = queue[""resourcesUsed""]\n            for label_used in queue[""resources""][""resourceUsagesByPartition""]:\n                if label_used[""partitionName""] == queue_nodelabel:\n                    queue_resource_used = label_used[""used""]\n                    break\n\n            cap = queue_capacity[""absoluteCapacity""] / 100.0\n            partition_resource = labeled_resource[queue_nodelabel]\n\n            cpu_cap.add_metric([queue_name], partition_resource.cpu * cap)\n            mem_cap.add_metric([queue_name], partition_resource.mem * cap)\n            gpu_cap.add_metric([queue_name], partition_resource.gpu * cap)\n\n            cpu_avail.add_metric([queue_name], partition_resource.cpu * cap - queue_resource_used[""vCores""])\n            mem_avail.add_metric([queue_name], partition_resource.mem * cap - queue_resource_used[""memory""] * 1024 * 1024)\n            gpu_avail.add_metric([queue_name], partition_resource.gpu * cap - queue_resource_used[""GPUs""])\n\n            running_jobs.add_metric([queue_name], queue[""numActiveApplications""])\n            pending_jobs.add_metric([queue_name], queue[""numPendingApplications""])\n            running_containers.add_metric([queue_name], queue[""numContainers""])\n            pending_containers.add_metric([queue_name], queue[""pendingContainers""])\n\n        return [cpu_cap, cpu_avail,\n                mem_cap, mem_avail,\n                gpu_cap, gpu_avail,\n                running_jobs, pending_jobs,\n                running_containers, pending_containers]\n\nclass HealthResource(Resource):\n    def render_GET(self, request):\n        request.setHeader(""Content-Type"", ""text/html; charset=utf-8"")\n        return ""<html>Ok</html>"".encode(""utf-8"")\n\ndef register_stack_trace_dump():\n    faulthandler.register(signal.SIGTRAP, all_threads=True, chain=False)\n\n # https://github.com/prometheus/client_python/issues/322#issuecomment-428189291\ndef burninate_gc_collector():\n    for callback in gc.callbacks[:]:\n        if callback.__qualname__.startswith(""GCCollector.""):\n            gc.callbacks.remove(callback)\n\n    for name, collector in list(REGISTRY._names_to_collectors.items()):\n        if name.startswith(""python_gc_""):\n            try:\n                REGISTRY.unregister(collector)\n            except KeyError:  # probably gone already\n                pass\n\ndef main(args):\n    register_stack_trace_dump()\n    burninate_gc_collector()\n\n    REGISTRY.register(YarnCollector(args.yarn_url))\n\n    root = Resource()\n    root.putChild(b""metrics"", MetricsResource())\n    root.putChild(b""healthz"", HealthResource())\n\n    factory = Site(root)\n    reactor.listenTCP(int(args.port), factory)\n    reactor.run()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""yarn_url"", help=""Yarn rest api address, eg: http://127.0.0.1:8088"")\n    parser.add_argument(""--cluster-name"", ""-n"", help=""Yarn cluster name"",\n                        default=""cluster_0"")\n    parser.add_argument(""--port"", ""-p"", help=""Exporter listen port"",default=""9459"")\n\n    args = parser.parse_args()\n\n    def get_logging_level():\n        mapping = {\n                ""DEBUG"": logging.DEBUG,\n                ""INFO"": logging.INFO,\n                ""WARNING"": logging.WARNING\n                }\n\n        result = logging.INFO\n\n        if os.environ.get(""LOGGING_LEVEL"") is not None:\n            level = os.environ[""LOGGING_LEVEL""]\n            result = mapping.get(level.upper())\n            if result is None:\n                sys.stderr.write(""unknown logging level "" + level + \\\n                        "", default to INFO\\n"")\n                result = logging.INFO\n\n        return result\n\n    logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(threadName)s - %(filename)s:%(lineno)s - %(message)s"",\n            level=get_logging_level())\n\n    main(args)\n'"
src/yarn-exporter/test/base.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport logging\nimport unittest\n\nclass TestBase(unittest.TestCase):\n    """"""\n    Test Base class for job-exporter\n    """"""\n    @classmethod\n    def setUpClass(cls):\n        logging.basicConfig(format=""%(asctime)s - %(levelname)s - %(filename)s:%(lineno)s - %(message)s"",\n                level=logging.DEBUG)\n'"
src/yarn-exporter/test/test_collector.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nimport sys\nimport unittest\nimport logging\nimport json\n\nimport base\nfrom collections import defaultdict\n\nsys.path.append(os.path.abspath(""../src/""))\n\nimport yarn_exporter\nfrom yarn_exporter import YarnCollector\n\nlogger = logging.getLogger(__name__)\n\nclass TestYarnCollector(base.TestBase):\n    """"""\n    Test YarnCollector in yarn_exporter.py\n    """"""\n\n    def test_gen_nodes_metrics(self):\n        with open(""data/nodes"") as f:\n            obj = json.load(f)\n\n        labeled_resource = defaultdict(yarn_exporter.ResourceItem)\n\n        node_count = yarn_exporter.NodeCount()\n        metrics = YarnCollector.gen_nodes_metrics(obj, node_count, labeled_resource)\n\n        self.assertEqual(1, node_count.total)\n        self.assertEqual(1, node_count.active)\n\n        self.assertEqual(24, labeled_resource[""""].cpu)\n        self.assertEqual(184320 * 1024 * 1024, labeled_resource[""""].mem)\n        self.assertEqual(4, labeled_resource[""""].gpu)\n\n        self.assertEqual(6, len(metrics))\n        # total cpu\n        self.assertEqual(1, len(metrics[0].samples))\n        self.assertEqual(24, metrics[0].samples[0].value)\n\n        # available cpu\n        self.assertEqual(1, len(metrics[1].samples))\n        self.assertEqual(15, metrics[1].samples[0].value)\n\n        # total mem\n        self.assertEqual(1, len(metrics[2].samples))\n        self.assertEqual(184320 * 1024 * 1024, metrics[2].samples[0].value)\n\n        # available mem\n        self.assertEqual(1, len(metrics[3].samples))\n        self.assertEqual(150528 * 1024 * 1024, metrics[3].samples[0].value)\n\n        # total gpu\n        self.assertEqual(1, len(metrics[4].samples))\n        self.assertEqual(4, metrics[4].samples[0].value)\n\n        # available gpu\n        self.assertEqual(1, len(metrics[5].samples))\n        self.assertEqual(3, metrics[5].samples[0].value)\n\n    def test_gen_scheduler_metrics(self):\n        with open(""data/nodes"") as f:\n            obj = json.load(f)\n\n        labeled_resource = defaultdict(yarn_exporter.ResourceItem)\n\n        node_count = yarn_exporter.NodeCount()\n        metrics = YarnCollector.gen_nodes_metrics(obj, node_count, labeled_resource)\n\n        with open(""data/scheduler"") as f:\n            obj = json.load(f)\n\n        metrics = YarnCollector.gen_scheduler_metrics(obj, labeled_resource)\n\n        self.assertEqual(10, len(metrics))\n\n        self.assertEqual(1, len(metrics[0].samples))\n\n        # queue cpu cap\n        self.assertEqual(1, len(metrics[0].samples))\n        self.assertEqual(24, metrics[0].samples[0].value)\n\n        # queue cpu available\n        self.assertEqual(1, len(metrics[1].samples))\n        self.assertEqual(15, metrics[1].samples[0].value)\n\n        # queue mem cap\n        self.assertEqual(1, len(metrics[2].samples))\n        self.assertEqual(184320 * 1024 * 1024, metrics[2].samples[0].value)\n\n        # queue mem available\n        self.assertEqual(1, len(metrics[3].samples))\n        self.assertEqual(150528 * 1024 * 1024, metrics[3].samples[0].value)\n\n        # queue gpu cap\n        self.assertEqual(1, len(metrics[4].samples))\n        self.assertEqual(4, metrics[4].samples[0].value)\n\n        # queue gpu available\n        self.assertEqual(1, len(metrics[5].samples))\n        self.assertEqual(3, metrics[5].samples[0].value)\n\n        # running jobs\n        self.assertEqual(1, len(metrics[6].samples))\n        self.assertEqual(1, metrics[6].samples[0].value)\n\n        # pending jobs\n        self.assertEqual(1, len(metrics[7].samples))\n        self.assertEqual(0, metrics[7].samples[0].value)\n\n        # running containers\n        self.assertEqual(1, len(metrics[8].samples))\n        self.assertEqual(2, metrics[8].samples[0].value)\n\n        # pending containers\n        self.assertEqual(1, len(metrics[9].samples))\n        self.assertEqual(0, metrics[9].samples[0].value)\n\n\n    def test_gen_metrics_with_label(self):\n        with open(""data/nodes_with_label.json"") as f:\n            obj = json.load(f)\n\n        labeled_resource = defaultdict(yarn_exporter.ResourceItem)\n\n        node_count = yarn_exporter.NodeCount()\n        metrics = YarnCollector.gen_nodes_metrics(obj, node_count, labeled_resource)\n\n        self.assertDictEqual(labeled_resource, {"""": yarn_exporter.ResourceItem(cpu=24, gpu=4, mem=204*1024*1024*1024),\n                                                ""test_vc"": yarn_exporter.ResourceItem(cpu=24, gpu=4, mem=204*1024*1024*1024)})\n\n        with open(""data/scheduler_with_label.json"") as f:\n            obj = json.load(f)\n\n        metrics = YarnCollector.gen_scheduler_metrics(obj, labeled_resource)\n\n        self.assertEqual(10, len(metrics))\n        self.assertEqual(4, len(metrics[0].samples))\n\n        ## total gpu\n        for sample in metrics[4].samples:\n            if sample.labels[""queue""] == ""default"":\n                self.assertEqual(sample.value, 3.6)\n            elif sample.labels[""queue""] == ""test_vc"":\n                self.assertEqual(sample.value, 4)\n            elif sample.labels[""queue""] == ""vc_a"":\n                self.assertEqual(sample.value, 0.4)\n\n        ## available gpu\n        for sample in metrics[5].samples:\n            if sample.labels[""queue""] == ""default"":\n                self.assertEqual(sample.value, 2.6)\n            elif sample.labels[""queue""] == ""test_vc"":\n                self.assertEqual(sample.value, 4)\n            elif sample.labels[""queue""] == ""vc_a"":\n                self.assertEqual(sample.value, 0.4)\n\n\n    def test_gen_nodes_metrics_using_empty_nodes(self):\n        with open(""data/empty_nodes"") as f:\n            obj = json.load(f)\n\n        labeled_resource = defaultdict(yarn_exporter.ResourceItem)\n\n        node_count = yarn_exporter.NodeCount()\n        metrics = YarnCollector.gen_nodes_metrics(obj, node_count, labeled_resource)\n\nif __name__ == \'__main__\':\n    unittest.main()\n'"
src/yarn-frameworklauncher/config/yarn_frameworklauncher.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass YarnFrameworklauncher:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        self.service_configuration = self.merge_service_configuration(service_configuration, default_service_configuration)\n\n    def merge_service_configuration(self, overwrite_srv_cfg, default_srv_cfg):\n        if overwrite_srv_cfg == None:\n            return default_srv_cfg\n        srv_cfg = default_srv_cfg.copy()\n        for k in overwrite_srv_cfg:\n            v = overwrite_srv_cfg[k]\n            if (k in srv_cfg and isinstance(overwrite_srv_cfg[k], dict) and isinstance(srv_cfg[k], dict)):\n                srv_cfg[k] = self.merge_service_configuration(overwrite_srv_cfg[k], srv_cfg[k])\n            else:\n                srv_cfg[k] = overwrite_srv_cfg[k]\n        return srv_cfg\n\n    def validation_pre(self):\n        if ""frameworklauncher-port"" not in self.service_configuration:\n            return False, ""frameworkerlaucnher-port is missed in service-configuration -> yarn-framworkerlauncher""\n        return True, None\n\n    def run(self):\n        yarn_launcher_com = self.service_configuration\n\n        yarn_launcher_com[""node-list""] = list()\n        yarn_launcher_com[""webservice""] = """"\n\n        # This properties is designed for single Instance, unable to support multiple launcher.\n        yarn_launcher_com[""launcher-address""] = """"\n\n        for host in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host and host[""pai-master""] == ""true"":\n                yarn_launcher_com[""node-list""].append(host[""hostname""])\n                yarn_launcher_com[""webservice""] = yarn_launcher_com[""webservice""] + ""http://{0}:{1}"".format(host[""hostip""], str(self.service_configuration[""frameworklauncher-port""]))\n                yarn_launcher_com[""launcher-address""] = host[""hostip""]\n\n\n        return yarn_launcher_com\n\n    def validation_post(self, cluster_object_model):\n        com = cluster_object_model\n\n        if ""hadoop-resource-manager"" not in com or ""master-ip"" not in com[""hadoop-resource-manager""]:\n            return False, ""hadoop-resource-manager.master-ip is missing in cluster-object-model.""\n\n        if ""hadoop-name-node"" not in com or ""master-ip"" not in com[""hadoop-name-node""]:\n            return False, ""hadoop-name-node.master-ip is missing in cluster-object-model.""\n\n        if ""hadoop-jobhistory"" not in com:\n            return False, ""hadoop-jobhistory is missing in cluster-object-model.""\n\n        if ""log-server-ip"" not in com[""hadoop-jobhistory""]:\n            return False, ""hadoop-jobhistory.log-server-ip is missing in cluster-object-model.""\n\n        if ""timeline-server-ip"" not in com[""hadoop-jobhistory""]:\n            return False, ""hadoop-jobhistory.timeline-server-ip is missing in cluster-object-model.""\n\n        return True, None\n'"
src/zookeeper/config/zookeeper.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n\nimport logging\nimport logging.config\n\n\nclass Zookeeper:\n\n    def __init__(self, cluster_configuration, service_configuration, default_service_configuration):\n        self.logger = logging.getLogger(__name__)\n\n        self.cluster_configuration = cluster_configuration\n        #self.service_configuration = None\n\n\n\n    def validation_pre(self):\n        zkid_visited = dict()\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                if ""zkid"" not in host_config:\n                    return False, ""zkid is missing in your pai-master machine [{0}] ."".format(host_config[""hostip""])\n                if host_config[""zkid""] in zkid_visited and zkid_visited[host_config[""zkid""]] is True :\n                    return False, ""Duplicated zkid [zkid: {0}]. "".format(host_config[""zkid""])\n                zkid_visited[host_config[""zkid""]] = True\n\n        return True, None\n\n\n\n    def run(self):\n        zookeeper_com = dict()\n        zookeeper_com[""host-list""] = list()\n        zookeeper_com[""quorum""] = """"\n\n        for host_config in self.cluster_configuration[""machine-list""]:\n            if ""pai-master"" in host_config and host_config[""pai-master""] == ""true"":\n                zookeeper_com[""host-list""].append(host_config[""hostname""])\n                if zookeeper_com[""quorum""] != """":\n                    zookeeper_com[""quorum""] = zookeeper_com[""quorum""] + "",""\n                zookeeper_com[""quorum""] = zookeeper_com[""quorum""] + host_config[""hostip""] + "":2181""\n\n        return zookeeper_com\n\n\n\n    def validation_post(self, cluster_object_model):\n        com = cluster_object_model\n        return True, None\n'"
contrib/samba-aad-server/build/infosrv/domaininfo.py,0,"b'import sys\nimport json\nimport os\n\nfrom flask import Flask\nfrom flask_restful import reqparse, abort, Api, Resource\nfrom flask import request, jsonify\nimport base64\nimport subprocess\n\n\napp = Flask(__name__)\napi = Api(app)\n\n\n\nparser = reqparse.RequestParser()\n\ndef cmd_exec(cmdStr):\n    try:\n        output = subprocess.check_output([""bash"",""-c"", cmdStr]).strip()\n    except Exception as e:\n        print(e)\n        output = """"\n    return output\n\nclass GetUserId(Resource):\n    def get(self):\n        parser.add_argument(\'userName\')\n        args = parser.parse_args()\n        ret = {}\n\n        if args[""userName""] is not None and len(args[""userName""].strip()) > 0:\n            # Replace with your corp domains\n            corpDomains = [\'ATHENA\']\n            ret[""uid""] = """"\n\n            for corpDomain in corpDomains:\n                if len(ret[""uid""].strip())==0:\n                    userName = str(args[""userName""]).strip().split(""@"")[0]\n                    uid = cmd_exec(""id -u %s\\\\\\\\%s"" % (corpDomain,userName))\n                    gid = cmd_exec(""id -g %s\\\\\\\\%s"" % (corpDomain,userName))\n                    groups = cmd_exec(""id -Gnz %s\\\\\\\\%s"" % (corpDomain,userName)).split(""\\0"")\n\n                    ret[""uid""] = uid\n                    ret[""gid""] = gid\n                    ret[""groups""] = groups\n\n\n        resp = jsonify(ret)\n        resp.headers[""Access-Control-Allow-Origin""] = ""*""\n        resp.headers[""dataType""] = ""json""\n\n        return resp\n\n##\n## Actually setup the Api resource routing here\n##\napi.add_resource(GetUserId, \'/GetUserId\')\n\nif __name__ == \'__main__\':\n    app.run(debug=False,host=""0.0.0.0"",threaded=True)\n'"
src/pylon/deploy/pylon-config/render.py,0,"b'# Copyright (c) Microsoft Corporation\n# All rights reserved.\n#\n# MIT License\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated\n# documentation files (the ""Software""), to deal in the Software without restriction, including without limitation\n# the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and\n# to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED *AS IS*, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING\n# BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\nimport os\nfrom hashlib import md5\nfrom jinja2 import Template\n\nenv = {}\nfor key in os.environ:\n    env[key] = os.environ[key]\n\ntemplateString = open(\'/pylon-config/nginx.conf.template\', \'r\').read()\nlocationCfgTemplateString = open(\'/pylon-config/location.conf.template\', \'r\').read()\n\nenv.setdefault(\'PYLON_CONF_ETAG\', md5(templateString).hexdigest())\n\nrenderedString = Template(templateString).render(env)\nopen(\'/root/nginx.conf\', \'w\').write(renderedString)\n\nlocationCfgRenderedString = Template(locationCfgTemplateString).render(env)\nopen(\'/root/location.conf\', \'w\').write(locationCfgRenderedString)'"
