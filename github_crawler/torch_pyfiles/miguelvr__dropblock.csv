file_path,api_count,code
setup.py,0,"b""from setuptools import setup, find_packages\n\n\nwith open('requirements.txt', encoding='utf-8') as f:\n    required = f.read().splitlines()\n\nwith open('README.md', encoding='utf-8') as f:\n    long_description = f.read()\n\nsetup(\n    name='dropblock',\n    version='0.3.0',\n    packages=find_packages(),\n    long_description=long_description,\n    long_description_content_type='text/markdown',\n    install_requires=required,\n    url='https://github.com/miguelvr/dropblock',\n    license='MIT',\n    author='Miguel Varela Ramos',\n    author_email='miguelvramos92@gmail.com',\n    description='Implementation of DropBlock: A regularization method for convolutional networks in PyTorch. '\n)\n"""
dropblock/__init__.py,0,"b""from .dropblock import DropBlock2D, DropBlock3D\nfrom .scheduler import LinearScheduler\n\n__all__ = ['DropBlock2D', 'DropBlock3D', 'LinearScheduler']\n"""
dropblock/dropblock.py,3,"b'import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass DropBlock2D(nn.Module):\n    r""""""Randomly zeroes 2D spatial blocks of the input tensor.\n\n    As described in the paper\n    `DropBlock: A regularization method for convolutional networks`_ ,\n    dropping whole blocks of feature map allows to remove semantic\n    information as compared to regular dropout.\n\n    Args:\n        drop_prob (float): probability of an element to be dropped.\n        block_size (int): size of the block to drop\n\n    Shape:\n        - Input: `(N, C, H, W)`\n        - Output: `(N, C, H, W)`\n\n    .. _DropBlock: A regularization method for convolutional networks:\n       https://arxiv.org/abs/1810.12890\n\n    """"""\n\n    def __init__(self, drop_prob, block_size):\n        super(DropBlock2D, self).__init__()\n\n        self.drop_prob = drop_prob\n        self.block_size = block_size\n\n    def forward(self, x):\n        # shape: (bsize, channels, height, width)\n\n        assert x.dim() == 4, \\\n            ""Expected input with 4 dimensions (bsize, channels, height, width)""\n\n        if not self.training or self.drop_prob == 0.:\n            return x\n        else:\n            # get gamma value\n            gamma = self._compute_gamma(x)\n\n            # sample mask\n            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n\n            # place mask on input device\n            mask = mask.to(x.device)\n\n            # compute block mask\n            block_mask = self._compute_block_mask(mask)\n\n            # apply block mask\n            out = x * block_mask[:, None, :, :]\n\n            # scale output\n            out = out * block_mask.numel() / block_mask.sum()\n\n            return out\n\n    def _compute_block_mask(self, mask):\n        block_mask = F.max_pool2d(input=mask[:, None, :, :],\n                                  kernel_size=(self.block_size, self.block_size),\n                                  stride=(1, 1),\n                                  padding=self.block_size // 2)\n\n        if self.block_size % 2 == 0:\n            block_mask = block_mask[:, :, :-1, :-1]\n\n        block_mask = 1 - block_mask.squeeze(1)\n\n        return block_mask\n\n    def _compute_gamma(self, x):\n        return self.drop_prob / (self.block_size ** 2)\n\n\nclass DropBlock3D(DropBlock2D):\n    r""""""Randomly zeroes 3D spatial blocks of the input tensor.\n\n    An extension to the concept described in the paper\n    `DropBlock: A regularization method for convolutional networks`_ ,\n    dropping whole blocks of feature map allows to remove semantic\n    information as compared to regular dropout.\n\n    Args:\n        drop_prob (float): probability of an element to be dropped.\n        block_size (int): size of the block to drop\n\n    Shape:\n        - Input: `(N, C, D, H, W)`\n        - Output: `(N, C, D, H, W)`\n\n    .. _DropBlock: A regularization method for convolutional networks:\n       https://arxiv.org/abs/1810.12890\n\n    """"""\n\n    def __init__(self, drop_prob, block_size):\n        super(DropBlock3D, self).__init__(drop_prob, block_size)\n\n    def forward(self, x):\n        # shape: (bsize, channels, depth, height, width)\n\n        assert x.dim() == 5, \\\n            ""Expected input with 5 dimensions (bsize, channels, depth, height, width)""\n\n        if not self.training or self.drop_prob == 0.:\n            return x\n        else:\n            # get gamma value\n            gamma = self._compute_gamma(x)\n\n            # sample mask\n            mask = (torch.rand(x.shape[0], *x.shape[2:]) < gamma).float()\n\n            # place mask on input device\n            mask = mask.to(x.device)\n\n            # compute block mask\n            block_mask = self._compute_block_mask(mask)\n\n            # apply block mask\n            out = x * block_mask[:, None, :, :, :]\n\n            # scale output\n            out = out * block_mask.numel() / block_mask.sum()\n\n            return out\n\n    def _compute_block_mask(self, mask):\n        block_mask = F.max_pool3d(input=mask[:, None, :, :, :],\n                                  kernel_size=(self.block_size, self.block_size, self.block_size),\n                                  stride=(1, 1, 1),\n                                  padding=self.block_size // 2)\n\n        if self.block_size % 2 == 0:\n            block_mask = block_mask[:, :, :-1, :-1, :-1]\n\n        block_mask = 1 - block_mask.squeeze(1)\n\n        return block_mask\n\n    def _compute_gamma(self, x):\n        return self.drop_prob / (self.block_size ** 3)\n'"
dropblock/scheduler.py,0,"b'import numpy as np\nfrom torch import nn\n\n\nclass LinearScheduler(nn.Module):\n    def __init__(self, dropblock, start_value, stop_value, nr_steps):\n        super(LinearScheduler, self).__init__()\n        self.dropblock = dropblock\n        self.i = 0\n        self.drop_values = np.linspace(start=start_value, stop=stop_value, num=nr_steps)\n\n    def forward(self, x):\n        return self.dropblock(x)\n\n    def step(self):\n        if self.i < len(self.drop_values):\n            self.dropblock.drop_prob = self.drop_values[self.i]\n\n        self.i += 1\n'"
examples/resnet-cifar10.py,5,"b'import time\nimport configargparse\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torchvision.models.resnet import BasicBlock, ResNet\nfrom ignite.engine import create_supervised_trainer, create_supervised_evaluator, Events\nfrom ignite.metrics import Accuracy\nfrom ignite.metrics import RunningAverage\nfrom ignite.contrib.handlers import ProgressBar\n\nfrom dropblock import DropBlock2D, LinearScheduler\n\nresults = []\n\n\nclass ResNetCustom(ResNet):\n\n    def __init__(self, block, layers, num_classes=1000, drop_prob=0., block_size=5):\n        super(ResNet, self).__init__()\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.dropblock = LinearScheduler(\n            DropBlock2D(drop_prob=drop_prob, block_size=block_size),\n            start_value=0.,\n            stop_value=drop_prob,\n            nr_steps=5e3\n        )\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        self.dropblock.step()  # increment number of iterations\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.dropblock(self.layer1(x))\n        x = self.dropblock(self.layer2(x))\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.shape[0], -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet9(**kwargs):\n    return ResNetCustom(BasicBlock, [1, 1, 1, 1], **kwargs)\n\n\ndef logger(engine, model, evaluator, loader, pbar):\n    evaluator.run(loader)\n    metrics = evaluator.state.metrics\n    avg_accuracy = metrics[\'accuracy\']\n    pbar.log_message(\n        ""Test Results - Avg accuracy: {:.2f}, drop_prob: {:.2f}"".format(avg_accuracy,\n                                                                        model.dropblock.dropblock.drop_prob)\n    )\n    results.append(avg_accuracy)\n\n\nif __name__ == \'__main__\':\n    parser = configargparse.ArgumentParser()\n\n    parser.add_argument(\'-c\', \'--config\', required=False,\n                        is_config_file=True, help=\'config file\')\n    parser.add_argument(\'--root\', required=False, type=str, default=\'./data\',\n                        help=\'data root path\')\n    parser.add_argument(\'--workers\', required=False, type=int, default=4,\n                        help=\'number of workers for data loader\')\n    parser.add_argument(\'--bsize\', required=False, type=int, default=256,\n                        help=\'batch size\')\n    parser.add_argument(\'--epochs\', required=False, type=int, default=50,\n                        help=\'number of epochs\')\n    parser.add_argument(\'--lr\', required=False, type=float, default=0.001,\n                        help=\'learning rate\')\n    parser.add_argument(\'--drop_prob\', required=False, type=float, default=0.,\n                        help=\'dropblock dropout probability\')\n    parser.add_argument(\'--block_size\', required=False, type=int, default=5,\n                        help=\'dropblock block size\')\n    parser.add_argument(\'--device\', required=False, default=None, type=int,\n                        help=\'CUDA device id for GPU training\')\n    options = parser.parse_args()\n\n    root = options.root\n    bsize = options.bsize\n    workers = options.workers\n    epochs = options.epochs\n    lr = options.lr\n    drop_prob = options.drop_prob\n    block_size = options.block_size\n    device = \'cpu\' if options.device is None \\\n        else torch.device(\'cuda:{}\'.format(options.device))\n\n    transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomCrop(32, padding=4),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n\n    train_set = torchvision.datasets.CIFAR10(root=root, train=True,\n                                             download=True, transform=transform)\n\n    train_loader = torch.utils.data.DataLoader(train_set, batch_size=bsize,\n                                               shuffle=True, num_workers=workers)\n\n    test_set = torchvision.datasets.CIFAR10(root=root, train=False,\n                                            download=True, transform=transform)\n\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=bsize,\n                                              shuffle=False, num_workers=workers)\n\n    classes = (\'plane\', \'car\', \'bird\', \'cat\',\n               \'deer\', \'dog\', \'frog\', \'horse\', \'ship\', \'truck\')\n\n    # define model\n    model = resnet9(num_classes=len(classes), drop_prob=drop_prob, block_size=block_size)\n\n    # define loss and optimizer\n    criterion = nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n\n    # create ignite engines\n    trainer = create_supervised_trainer(model=model,\n                                        optimizer=optimizer,\n                                        loss_fn=criterion,\n                                        device=device)\n\n    evaluator = create_supervised_evaluator(model,\n                                            metrics={\'accuracy\': Accuracy()},\n                                            device=device)\n\n    # ignite handlers\n    RunningAverage(output_transform=lambda x: x).attach(trainer, \'loss\')\n\n    pbar = ProgressBar()\n    pbar.attach(trainer, [\'loss\'])\n\n    trainer.add_event_handler(Events.EPOCH_COMPLETED, logger, model, evaluator, test_loader, pbar)\n\n    # start training\n    t0 = time.time()\n    trainer.run(train_loader, max_epochs=epochs)\n    t1 = time.time()\n    print(\'Best Accuracy:\', max(results))\n    print(\'Total time:\', t1 - t0)\n'"
tests/test_dropblock2d.py,28,"b'from unittest import mock\n\nimport pytest\nimport torch\n\nfrom dropblock import DropBlock2D\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_square_even():\n    db = DropBlock2D(block_size=2, drop_prob=0.1)\n    mask = torch.tensor([[[1., 0., 0., 0., 0.],\n                          [0., 0., 0., 1., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[0., 0., 1., 1., 1.],\n                              [0., 0., 1., 0., 0.],\n                              [1., 1., 1., 0., 0.],\n                              [1., 1., 1., 1., 1.],\n                              [1., 1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_Hw_even():\n    db = DropBlock2D(block_size=2, drop_prob=0.1)\n    mask = torch.tensor([[[1., 0., 0., 0.],\n                          [0., 0., 0., 1.],\n                          [0., 0., 0., 0.],\n                          [0., 0., 0., 0.],\n                          [0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[0., 0., 1., 1.],\n                              [0., 0., 1., 0.],\n                              [1., 1., 1., 0.],\n                              [1., 1., 1., 1.],\n                              [1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_hW_even():\n    db = DropBlock2D(block_size=2, drop_prob=0.1)\n    mask = torch.tensor([[[0., 0., 0., 1., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[1., 1., 1., 0., 0.],\n                              [1., 1., 1., 0., 0.],\n                              [1., 1., 1., 1., 1.],\n                              [1., 1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_square_odd():\n    db = DropBlock2D(block_size=3, drop_prob=0.1)\n    mask = torch.tensor([[[1., 0., 0., 0., 0.],\n                          [0., 0., 0., 1., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[0., 0., 0., 0., 0.],\n                              [0., 0., 0., 0., 0.],\n                              [1., 1., 0., 0., 0.],\n                              [1., 1., 1., 1., 1.],\n                              [1., 1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_Hw_odd():\n    db = DropBlock2D(block_size=3, drop_prob=0.1)\n    mask = torch.tensor([[[1., 0., 0., 0.],\n                          [0., 0., 0., 1.],\n                          [0., 0., 0., 0.],\n                          [0., 0., 0., 0.],\n                          [0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[0., 0., 0., 0.],\n                              [0., 0., 0., 0.],\n                              [1., 1., 0., 0.],\n                              [1., 1., 1., 1.],\n                              [1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_hW_odd():\n    db = DropBlock2D(block_size=3, drop_prob=0.1)\n    mask = torch.tensor([[[0., 0., 0., 1., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[1., 1., 0., 0., 0.],\n                              [1., 1., 0., 0., 0.],\n                              [1., 1., 1., 1., 1.],\n                              [1., 1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_overlap():\n    db = DropBlock2D(block_size=2, drop_prob=0.1)\n    mask = torch.tensor([[[1., 0., 0., 0., 0.],\n                          [0., 1., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.],\n                          [0., 0., 0., 0., 0.]]])\n\n    expected = torch.tensor([[[0., 0., 1., 1., 1.],\n                              [0., 0., 0., 1., 1.],\n                              [1., 0., 0., 1., 1.],\n                              [1., 1., 1., 1., 1.],\n                              [1., 1., 1., 1., 1.]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_forward_pass():\n    db = DropBlock2D(block_size=3, drop_prob=0.1)\n    block_mask = torch.tensor([[[0., 0., 0., 1., 1., 1., 1.],\n                                [0., 0., 0., 0., 0., 0., 1.],\n                                [0., 0., 0., 0., 0., 0., 1.],\n                                [1., 1., 1., 0., 0., 0., 1.],\n                                [1., 1., 1., 1., 1., 1., 1.],\n                                [1., 1., 1., 1., 1., 1., 1.],\n                                [1., 1., 1., 1., 1., 1., 1.]]])\n\n    db._compute_block_mask = mock.MagicMock(return_value=block_mask)\n\n    x = torch.ones(10, 10, 7, 7)\n    h = db(x)\n\n    expected = block_mask * block_mask.numel() / block_mask.sum()\n    expected = expected[:, None, :, :].expand_as(x)\n\n    assert tuple(h.shape) == (10, 10, 7, 7)\n    assert torch.equal(h, expected)\n\n\ndef test_forward_pass2():\n\n    block_sizes = [2, 3, 4, 5, 6, 7, 8]\n    heights = [5, 6, 8, 10, 11, 14, 15]\n    widths = [5, 7, 8, 10, 15, 14, 15]\n\n    for block_size, height, width in zip(block_sizes, heights, widths):\n        dropout = DropBlock2D(0.1, block_size=block_size)\n        input = torch.randn((5, 20, height, width))\n        output = dropout(input)\n        assert tuple(input.shape) == tuple(output.shape)\n\n\ndef test_large_block_size():\n    dropout = DropBlock2D(0.3, block_size=9)\n    x = torch.rand(100, 10, 16, 16)\n    output = dropout(x)\n\n    assert tuple(x.shape) == tuple(output.shape)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""CUDA not available"")\ndef test_forward_pass_with_cuda():\n    dropout = DropBlock2D(0.3, block_size=5).to(\'cuda\')\n    x = torch.rand(100, 10, 16, 16).to(\'cuda\')\n    output = dropout(x)\n\n    assert tuple(x.shape) == tuple(output.shape)\n'"
tests/test_dropblock3d.py,12,"b'import torch\nfrom dropblock import DropBlock3D\nfrom unittest import mock\nimport pytest\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_cube_even():\n    db = DropBlock3D(block_size=2, drop_prob=0.1)\n    mask = torch.tensor([[[[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[1., 0., 0., 0., 0.],\n                           [0., 0., 0., 1., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]]]])\n\n    expected = torch.tensor([[[[1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[0., 0., 1., 1., 1.],\n                               [0., 0., 1., 0., 0.],\n                               [1., 1., 1., 0., 0.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[0., 0., 1., 1., 1.],\n                               [0., 0., 1., 0., 0.],\n                               [1., 1., 1., 0., 0.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_block_mask_cube_odd():\n    db = DropBlock3D(block_size=3, drop_prob=0.1)\n    mask = torch.tensor([[[[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[1., 0., 0., 0., 0.],\n                           [0., 0., 0., 1., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]],\n                          [[0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.],\n                           [0., 0., 0., 0., 0.]]]])\n\n    expected = torch.tensor([[[[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.],\n                               [1., 1., 0., 0., 0.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.],\n                               [1., 1., 0., 0., 0.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[0., 0., 0., 0., 0.],\n                               [0., 0., 0., 0., 0.],\n                               [1., 1., 0., 0., 0.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]],\n                              [[1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.],\n                               [1., 1., 1., 1., 1.]]]])\n\n    block_mask = db._compute_block_mask(mask)\n    assert torch.equal(block_mask, expected)\n\n\n# noinspection PyCallingNonCallable\ndef test_forward_pass():\n    db = DropBlock3D(block_size=3, drop_prob=0.1)\n    block_mask = torch.tensor([[[[1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[0., 0., 0., 1., 1., 1., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [1., 1., 1., 0., 0., 0., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[0., 0., 0., 1., 1., 1., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [1., 1., 1., 0., 0., 0., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[0., 0., 0., 1., 1., 1., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [0., 0., 0., 0., 0., 0., 1.],\n                                 [1., 1., 1., 0., 0., 0., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]],\n                                [[1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.],\n                                 [1., 1., 1., 1., 1., 1., 1.]]]])\n\n    db._compute_block_mask = mock.MagicMock(return_value=block_mask)\n\n    x = torch.ones(10, 10, 7, 7, 7)\n    h = db(x)\n\n    expected = block_mask * block_mask.numel() / block_mask.sum()\n    expected = expected[:, None, :, :, :].expand_as(x)\n\n    assert tuple(h.shape) == (10, 10, 7, 7, 7)\n    assert torch.equal(h, expected)\n\n\ndef test_forward_pass2():\n    block_sizes = [2, 3, 4, 5, 6, 7, 8]\n    depths = [5, 6, 8, 10, 11, 14, 15]\n    heights = [5, 6, 8, 10, 11, 14, 15]\n    widths = [5, 7, 8, 10, 15, 14, 15]\n\n    for block_size, depth, height, width in zip(block_sizes, depths, heights, widths):\n        dropout = DropBlock3D(0.2, block_size=block_size)\n        input = torch.randn((5, 20, depth, height, width))\n        output = dropout(input)\n\n        assert tuple(input.shape) == tuple(output.shape)\n\n\n@pytest.mark.skipif(not torch.cuda.is_available(), reason=""CUDA not available"")\ndef test_forward_pass_with_cuda():\n    dropout = DropBlock3D(0.2, block_size=5).to(\'cuda\')\n    input = torch.randn((5, 20, 16, 16, 16)).to(\'cuda\')\n    output = dropout(input)\n\n    assert tuple(input.shape) == tuple(output.shape)\n'"
