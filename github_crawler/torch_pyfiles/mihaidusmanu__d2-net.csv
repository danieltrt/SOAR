file_path,api_count,code
extract_features.py,5,"b'import argparse\n\nimport numpy as np\n\nimport imageio\n\nimport torch\n\nfrom tqdm import tqdm\n\nimport scipy\nimport scipy.io\nimport scipy.misc\n\nfrom lib.model_test import D2Net\nfrom lib.utils import preprocess_image\nfrom lib.pyramid import process_multiscale\n\n# CUDA\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if use_cuda else ""cpu"")\n\n# Argument parsing\nparser = argparse.ArgumentParser(description=\'Feature extraction script\')\n\nparser.add_argument(\n    \'--image_list_file\', type=str, required=True,\n    help=\'path to a file containing a list of images to process\'\n)\n\nparser.add_argument(\n    \'--preprocessing\', type=str, default=\'caffe\',\n    help=\'image preprocessing (caffe or torch)\'\n)\nparser.add_argument(\n    \'--model_file\', type=str, default=\'models/d2_tf.pth\',\n    help=\'path to the full model\'\n)\n\nparser.add_argument(\n    \'--max_edge\', type=int, default=1600,\n    help=\'maximum image size at network input\'\n)\nparser.add_argument(\n    \'--max_sum_edges\', type=int, default=2800,\n    help=\'maximum sum of image sizes at network input\'\n)\n\nparser.add_argument(\n    \'--output_extension\', type=str, default=\'.d2-net\',\n    help=\'extension for the output\'\n)\nparser.add_argument(\n    \'--output_type\', type=str, default=\'npz\',\n    help=\'output file type (npz or mat)\'\n)\n\nparser.add_argument(\n    \'--multiscale\', dest=\'multiscale\', action=\'store_true\',\n    help=\'extract multiscale features\'\n)\nparser.set_defaults(multiscale=False)\n\nparser.add_argument(\n    \'--no-relu\', dest=\'use_relu\', action=\'store_false\',\n    help=\'remove ReLU after the dense feature extraction module\'\n)\nparser.set_defaults(use_relu=True)\n\nargs = parser.parse_args()\n\nprint(args)\n\n# Creating CNN model\nmodel = D2Net(\n    model_file=args.model_file,\n    use_relu=args.use_relu,\n    use_cuda=use_cuda\n)\n\n# Process the file\nwith open(args.image_list_file, \'r\') as f:\n    lines = f.readlines()\nfor line in tqdm(lines, total=len(lines)):\n    path = line.strip()\n\n    image = imageio.imread(path)\n    if len(image.shape) == 2:\n        image = image[:, :, np.newaxis]\n        image = np.repeat(image, 3, -1)\n\n    # TODO: switch to PIL.Image due to deprecation of scipy.misc.imresize.\n    resized_image = image\n    if max(resized_image.shape) > args.max_edge:\n        resized_image = scipy.misc.imresize(\n            resized_image,\n            args.max_edge / max(resized_image.shape)\n        ).astype(\'float\')\n    if sum(resized_image.shape[: 2]) > args.max_sum_edges:\n        resized_image = scipy.misc.imresize(\n            resized_image,\n            args.max_sum_edges / sum(resized_image.shape[: 2])\n        ).astype(\'float\')\n\n    fact_i = image.shape[0] / resized_image.shape[0]\n    fact_j = image.shape[1] / resized_image.shape[1]\n\n    input_image = preprocess_image(\n        resized_image,\n        preprocessing=args.preprocessing\n    )\n    with torch.no_grad():\n        if args.multiscale:\n            keypoints, scores, descriptors = process_multiscale(\n                torch.tensor(\n                    input_image[np.newaxis, :, :, :].astype(np.float32),\n                    device=device\n                ),\n                model\n            )\n        else:\n            keypoints, scores, descriptors = process_multiscale(\n                torch.tensor(\n                    input_image[np.newaxis, :, :, :].astype(np.float32),\n                    device=device\n                ),\n                model,\n                scales=[1]\n            )\n\n    # Input image coordinates\n    keypoints[:, 0] *= fact_i\n    keypoints[:, 1] *= fact_j\n    # i, j -> u, v\n    keypoints = keypoints[:, [1, 0, 2]]\n\n    if args.output_type == \'npz\':\n        with open(path + args.output_extension, \'wb\') as output_file:\n            np.savez(\n                output_file,\n                keypoints=keypoints,\n                scores=scores,\n                descriptors=descriptors\n            )\n    elif args.output_type == \'mat\':\n        with open(path + args.output_extension, \'wb\') as output_file:\n            scipy.io.savemat(\n                output_file,\n                {\n                    \'keypoints\': keypoints,\n                    \'scores\': scores,\n                    \'descriptors\': descriptors\n                }\n            )\n    else:\n        raise ValueError(\'Unknown output type.\')\n'"
train.py,8,"b'import argparse\n\nimport numpy as np\n\nimport os\n\nimport shutil\n\nimport torch\nimport torch.optim as optim\n\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm\n\nimport warnings\n\nfrom lib.dataset import MegaDepthDataset\nfrom lib.exceptions import NoGradientError\nfrom lib.loss import loss_function\nfrom lib.model import D2Net\n\n\n# CUDA\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if use_cuda else ""cpu"")\n\n# Seed\ntorch.manual_seed(1)\nif use_cuda:\n    torch.cuda.manual_seed(1)\nnp.random.seed(1)\n\n# Argument parsing\nparser = argparse.ArgumentParser(description=\'Training script\')\n\nparser.add_argument(\n    \'--dataset_path\', type=str, required=True,\n    help=\'path to the dataset\'\n)\nparser.add_argument(\n    \'--scene_info_path\', type=str, required=True,\n    help=\'path to the processed scenes\'\n)\n\nparser.add_argument(\n    \'--preprocessing\', type=str, default=\'caffe\',\n    help=\'image preprocessing (caffe or torch)\'\n)\nparser.add_argument(\n    \'--model_file\', type=str, default=\'models/d2_ots.pth\',\n    help=\'path to the full model\'\n)\n\nparser.add_argument(\n    \'--num_epochs\', type=int, default=10,\n    help=\'number of training epochs\'\n)\nparser.add_argument(\n    \'--lr\', type=float, default=1e-3,\n    help=\'initial learning rate\'\n)\nparser.add_argument(\n    \'--batch_size\', type=int, default=1,\n    help=\'batch size\'\n)\nparser.add_argument(\n    \'--num_workers\', type=int, default=4,\n    help=\'number of workers for data loading\'\n)\n\nparser.add_argument(\n    \'--use_validation\', dest=\'use_validation\', action=\'store_true\',\n    help=\'use the validation split\'\n)\nparser.set_defaults(use_validation=False)\n\nparser.add_argument(\n    \'--log_interval\', type=int, default=250,\n    help=\'loss logging interval\'\n)\n\nparser.add_argument(\n    \'--log_file\', type=str, default=\'log.txt\',\n    help=\'loss logging file\'\n)\n\nparser.add_argument(\n    \'--plot\', dest=\'plot\', action=\'store_true\',\n    help=\'plot training pairs\'\n)\nparser.set_defaults(plot=False)\n\nparser.add_argument(\n    \'--checkpoint_directory\', type=str, default=\'checkpoints\',\n    help=\'directory for training checkpoints\'\n)\nparser.add_argument(\n    \'--checkpoint_prefix\', type=str, default=\'d2\',\n    help=\'prefix for training checkpoints\'\n)\n\nargs = parser.parse_args()\n\nprint(args)\n\n# Create the folders for plotting if need be\nif args.plot:\n    plot_path = \'train_vis\'\n    if os.path.isdir(plot_path):\n        print(\'[Warning] Plotting directory already exists.\')\n    else:\n        os.mkdir(plot_path)\n\n# Creating CNN model\nmodel = D2Net(\n    model_file=args.model_file,\n    use_cuda=use_cuda\n)\n\n# Optimizer\noptimizer = optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr\n)\n\n# Dataset\nif args.use_validation:\n    validation_dataset = MegaDepthDataset(\n        scene_list_path=\'megadepth_utils/valid_scenes.txt\',\n        scene_info_path=args.scene_info_path,\n        base_path=args.dataset_path,\n        train=False,\n        preprocessing=args.preprocessing,\n        pairs_per_scene=25\n    )\n    validation_dataloader = DataLoader(\n        validation_dataset,\n        batch_size=args.batch_size,\n        num_workers=args.num_workers\n    )\n\ntraining_dataset = MegaDepthDataset(\n    scene_list_path=\'megadepth_utils/train_scenes.txt\',\n    scene_info_path=args.scene_info_path,\n    base_path=args.dataset_path,\n    preprocessing=args.preprocessing\n)\ntraining_dataloader = DataLoader(\n    training_dataset,\n    batch_size=args.batch_size,\n    num_workers=args.num_workers\n)\n\n\n# Define epoch function\ndef process_epoch(\n        epoch_idx,\n        model, loss_function, optimizer, dataloader, device,\n        log_file, args, train=True\n):\n    epoch_losses = []\n\n    torch.set_grad_enabled(train)\n\n    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader))\n    for batch_idx, batch in progress_bar:\n        if train:\n            optimizer.zero_grad()\n\n        batch[\'train\'] = train\n        batch[\'epoch_idx\'] = epoch_idx\n        batch[\'batch_idx\'] = batch_idx\n        batch[\'batch_size\'] = args.batch_size\n        batch[\'preprocessing\'] = args.preprocessing\n        batch[\'log_interval\'] = args.log_interval\n\n        try:\n            loss = loss_function(model, batch, device, plot=args.plot)\n        except NoGradientError:\n            continue\n\n        current_loss = loss.data.cpu().numpy()[0]\n        epoch_losses.append(current_loss)\n\n        progress_bar.set_postfix(loss=(\'%.4f\' % np.mean(epoch_losses)))\n\n        if batch_idx % args.log_interval == 0:\n            log_file.write(\'[%s] epoch %d - batch %d / %d - avg_loss: %f\\n\' % (\n                \'train\' if train else \'valid\',\n                epoch_idx, batch_idx, len(dataloader), np.mean(epoch_losses)\n            ))\n\n        if train:\n            loss.backward()\n            optimizer.step()\n\n    log_file.write(\'[%s] epoch %d - avg_loss: %f\\n\' % (\n        \'train\' if train else \'valid\',\n        epoch_idx,\n        np.mean(epoch_losses)\n    ))\n    log_file.flush()\n\n    return np.mean(epoch_losses)\n\n\n# Create the checkpoint directory\nif os.path.isdir(args.checkpoint_directory):\n    print(\'[Warning] Checkpoint directory already exists.\')\nelse:\n    os.mkdir(args.checkpoint_directory)\n    \n\n# Open the log file for writing\nif os.path.exists(args.log_file):\n    print(\'[Warning] Log file already exists.\')\nlog_file = open(args.log_file, \'a+\')\n\n# Initialize the history\ntrain_loss_history = []\nvalidation_loss_history = []\nif args.use_validation:\n    validation_dataset.build_dataset()\n    min_validation_loss = process_epoch(\n        0,\n        model, loss_function, optimizer, validation_dataloader, device,\n        log_file, args,\n        train=False\n    )\n\n# Start the training\nfor epoch_idx in range(1, args.num_epochs + 1):\n    # Process epoch\n    training_dataset.build_dataset()\n    train_loss_history.append(\n        process_epoch(\n            epoch_idx,\n            model, loss_function, optimizer, training_dataloader, device,\n            log_file, args\n        )\n    )\n\n    if args.use_validation:\n        validation_loss_history.append(\n            process_epoch(\n                epoch_idx,\n                model, loss_function, optimizer, validation_dataloader, device,\n                log_file, args,\n                train=False\n            )\n        )\n\n    # Save the current checkpoint\n    checkpoint_path = os.path.join(\n        args.checkpoint_directory,\n        \'%s.%02d.pth\' % (args.checkpoint_prefix, epoch_idx)\n    )\n    checkpoint = {\n        \'args\': args,\n        \'epoch_idx\': epoch_idx,\n        \'model\': model.state_dict(),\n        \'optimizer\': optimizer.state_dict(),\n        \'train_loss_history\': train_loss_history,\n        \'validation_loss_history\': validation_loss_history\n    }\n    torch.save(checkpoint, checkpoint_path)\n    if (\n        args.use_validation and\n        validation_loss_history[-1] < min_validation_loss\n    ):\n        min_validation_loss = validation_loss_history[-1]\n        best_checkpoint_path = os.path.join(\n            args.checkpoint_directory,\n            \'%s.best.pth\' % args.checkpoint_prefix\n        )\n        shutil.copy(checkpoint_path, best_checkpoint_path)\n\n# Close the log file\nlog_file.close()\n'"
lib/dataset.py,11,"b""import h5py\n\nimport numpy as np\n\nfrom PIL import Image\n\nimport os\n\nimport torch\nfrom torch.utils.data import Dataset\n\nimport time\n\nfrom tqdm import tqdm\n\nfrom lib.utils import preprocess_image\n\n\nclass MegaDepthDataset(Dataset):\n    def __init__(\n            self,\n            scene_list_path='megadepth_utils/train_scenes.txt',\n            scene_info_path='/local/dataset/megadepth/scene_info',\n            base_path='/local/dataset/megadepth',\n            train=True,\n            preprocessing=None,\n            min_overlap_ratio=.5,\n            max_overlap_ratio=1,\n            max_scale_ratio=np.inf,\n            pairs_per_scene=100,\n            image_size=256\n    ):\n        self.scenes = []\n        with open(scene_list_path, 'r') as f:\n            lines = f.readlines()\n            for line in lines:\n                self.scenes.append(line.strip('\\n'))\n\n        self.scene_info_path = scene_info_path\n        self.base_path = base_path\n\n        self.train = train\n\n        self.preprocessing = preprocessing\n\n        self.min_overlap_ratio = min_overlap_ratio\n        self.max_overlap_ratio = max_overlap_ratio\n        self.max_scale_ratio = max_scale_ratio\n\n        self.pairs_per_scene = pairs_per_scene\n\n        self.image_size = image_size\n\n        self.dataset = []\n\n    def build_dataset(self):\n        self.dataset = []\n        if not self.train:\n            np_random_state = np.random.get_state()\n            np.random.seed(42)\n            print('Building the validation dataset...')\n        else:\n            print('Building a new training dataset...')\n        for scene in tqdm(self.scenes, total=len(self.scenes)):\n            scene_info_path = os.path.join(\n                self.scene_info_path, '%s.npz' % scene\n            )\n            if not os.path.exists(scene_info_path):\n                continue\n            scene_info = np.load(scene_info_path, allow_pickle=True)\n            overlap_matrix = scene_info['overlap_matrix']\n            scale_ratio_matrix = scene_info['scale_ratio_matrix']\n\n            valid =  np.logical_and(\n                np.logical_and(\n                    overlap_matrix >= self.min_overlap_ratio,\n                    overlap_matrix <= self.max_overlap_ratio\n                ),\n                scale_ratio_matrix <= self.max_scale_ratio\n            )\n            \n            pairs = np.vstack(np.where(valid))\n            try:\n                selected_ids = np.random.choice(\n                    pairs.shape[1], self.pairs_per_scene\n                )\n            except:\n                continue\n            \n            image_paths = scene_info['image_paths']\n            depth_paths = scene_info['depth_paths']\n            points3D_id_to_2D = scene_info['points3D_id_to_2D']\n            points3D_id_to_ndepth = scene_info['points3D_id_to_ndepth']\n            intrinsics = scene_info['intrinsics']\n            poses = scene_info['poses']\n            \n            for pair_idx in selected_ids:\n                idx1 = pairs[0, pair_idx]\n                idx2 = pairs[1, pair_idx]\n                matches = np.array(list(\n                    points3D_id_to_2D[idx1].keys() &\n                    points3D_id_to_2D[idx2].keys()\n                ))\n\n                # Scale filtering\n                matches_nd1 = np.array([points3D_id_to_ndepth[idx1][match] for match in matches])\n                matches_nd2 = np.array([points3D_id_to_ndepth[idx2][match] for match in matches])\n                scale_ratio = np.maximum(matches_nd1 / matches_nd2, matches_nd2 / matches_nd1)\n                matches = matches[np.where(scale_ratio <= self.max_scale_ratio)[0]]\n                \n                point3D_id = np.random.choice(matches)\n                point2D1 = points3D_id_to_2D[idx1][point3D_id]\n                point2D2 = points3D_id_to_2D[idx2][point3D_id]\n                nd1 = points3D_id_to_ndepth[idx1][point3D_id]\n                nd2 = points3D_id_to_ndepth[idx2][point3D_id]\n                central_match = np.array([\n                    point2D1[1], point2D1[0],\n                    point2D2[1], point2D2[0]\n                ])\n                self.dataset.append({\n                    'image_path1': image_paths[idx1],\n                    'depth_path1': depth_paths[idx1],\n                    'intrinsics1': intrinsics[idx1],\n                    'pose1': poses[idx1],\n                    'image_path2': image_paths[idx2],\n                    'depth_path2': depth_paths[idx2],\n                    'intrinsics2': intrinsics[idx2],\n                    'pose2': poses[idx2],\n                    'central_match': central_match,\n                    'scale_ratio': max(nd1 / nd2, nd2 / nd1)\n                })\n        np.random.shuffle(self.dataset)\n        if not self.train:\n            np.random.set_state(np_random_state)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def recover_pair(self, pair_metadata):\n        depth_path1 = os.path.join(\n            self.base_path, pair_metadata['depth_path1']\n        )\n        with h5py.File(depth_path1, 'r') as hdf5_file:\n            depth1 = np.array(hdf5_file['/depth'])\n        assert(np.min(depth1) >= 0)\n        image_path1 = os.path.join(\n            self.base_path, pair_metadata['image_path1']\n        )\n        image1 = Image.open(image_path1)\n        if image1.mode != 'RGB':\n            image1 = image1.convert('RGB')\n        image1 = np.array(image1)\n        assert(image1.shape[0] == depth1.shape[0] and image1.shape[1] == depth1.shape[1])\n        intrinsics1 = pair_metadata['intrinsics1']\n        pose1 = pair_metadata['pose1']\n\n        depth_path2 = os.path.join(\n            self.base_path, pair_metadata['depth_path2']\n        )\n        with h5py.File(depth_path2, 'r') as hdf5_file:\n            depth2 = np.array(hdf5_file['/depth'])\n        assert(np.min(depth2) >= 0)\n        image_path2 = os.path.join(\n            self.base_path, pair_metadata['image_path2']\n        )\n        image2 = Image.open(image_path2)\n        if image2.mode != 'RGB':\n            image2 = image2.convert('RGB')\n        image2 = np.array(image2)\n        assert(image2.shape[0] == depth2.shape[0] and image2.shape[1] == depth2.shape[1])\n        intrinsics2 = pair_metadata['intrinsics2']\n        pose2 = pair_metadata['pose2']\n\n        central_match = pair_metadata['central_match']\n        image1, bbox1, image2, bbox2 = self.crop(image1, image2, central_match)\n\n        depth1 = depth1[\n            bbox1[0] : bbox1[0] + self.image_size,\n            bbox1[1] : bbox1[1] + self.image_size\n        ]\n        depth2 = depth2[\n            bbox2[0] : bbox2[0] + self.image_size,\n            bbox2[1] : bbox2[1] + self.image_size\n        ]\n\n        return (\n            image1, depth1, intrinsics1, pose1, bbox1,\n            image2, depth2, intrinsics2, pose2, bbox2\n        )\n\n    def crop(self, image1, image2, central_match):\n        bbox1_i = max(int(central_match[0]) - self.image_size // 2, 0)\n        if bbox1_i + self.image_size >= image1.shape[0]:\n            bbox1_i = image1.shape[0] - self.image_size\n        bbox1_j = max(int(central_match[1]) - self.image_size // 2, 0)\n        if bbox1_j + self.image_size >= image1.shape[1]:\n            bbox1_j = image1.shape[1] - self.image_size\n\n        bbox2_i = max(int(central_match[2]) - self.image_size // 2, 0)\n        if bbox2_i + self.image_size >= image2.shape[0]:\n            bbox2_i = image2.shape[0] - self.image_size\n        bbox2_j = max(int(central_match[3]) - self.image_size // 2, 0)\n        if bbox2_j + self.image_size >= image2.shape[1]:\n            bbox2_j = image2.shape[1] - self.image_size\n\n        return (\n            image1[\n                bbox1_i : bbox1_i + self.image_size,\n                bbox1_j : bbox1_j + self.image_size\n            ],\n            np.array([bbox1_i, bbox1_j]),\n            image2[\n                bbox2_i : bbox2_i + self.image_size,\n                bbox2_j : bbox2_j + self.image_size\n            ],\n            np.array([bbox2_i, bbox2_j])\n        )\n\n    def __getitem__(self, idx):\n        (\n            image1, depth1, intrinsics1, pose1, bbox1,\n            image2, depth2, intrinsics2, pose2, bbox2\n        ) = self.recover_pair(self.dataset[idx])\n\n        image1 = preprocess_image(image1, preprocessing=self.preprocessing)\n        image2 = preprocess_image(image2, preprocessing=self.preprocessing)\n\n        return {\n            'image1': torch.from_numpy(image1.astype(np.float32)),\n            'depth1': torch.from_numpy(depth1.astype(np.float32)),\n            'intrinsics1': torch.from_numpy(intrinsics1.astype(np.float32)),\n            'pose1': torch.from_numpy(pose1.astype(np.float32)),\n            'bbox1': torch.from_numpy(bbox1.astype(np.float32)),\n            'image2': torch.from_numpy(image2.astype(np.float32)),\n            'depth2': torch.from_numpy(depth2.astype(np.float32)),\n            'intrinsics2': torch.from_numpy(intrinsics2.astype(np.float32)),\n            'pose2': torch.from_numpy(pose2.astype(np.float32)),\n            'bbox2': torch.from_numpy(bbox2.astype(np.float32))\n        }\n"""
lib/exceptions.py,0,b'class EmptyTensorError(Exception):\n    pass\n\n\nclass NoGradientError(Exception):\n    pass\n'
lib/loss.py,39,"b""import matplotlib\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport torch\nimport torch.nn.functional as F\n\nfrom lib.utils import (\n    grid_positions,\n    upscale_positions,\n    downscale_positions,\n    savefig,\n    imshow_image\n)\nfrom lib.exceptions import NoGradientError, EmptyTensorError\n\nmatplotlib.use('Agg')\n\n\ndef loss_function(\n        model, batch, device, margin=1, safe_radius=4, scaling_steps=3, plot=False\n):\n    output = model({\n        'image1': batch['image1'].to(device),\n        'image2': batch['image2'].to(device)\n    })\n\n    loss = torch.tensor(np.array([0], dtype=np.float32), device=device)\n    has_grad = False\n\n    n_valid_samples = 0\n    for idx_in_batch in range(batch['image1'].size(0)):\n        # Annotations\n        depth1 = batch['depth1'][idx_in_batch].to(device)  # [h1, w1]\n        intrinsics1 = batch['intrinsics1'][idx_in_batch].to(device)  # [3, 3]\n        pose1 = batch['pose1'][idx_in_batch].view(4, 4).to(device)  # [4, 4]\n        bbox1 = batch['bbox1'][idx_in_batch].to(device)  # [2]\n\n        depth2 = batch['depth2'][idx_in_batch].to(device)\n        intrinsics2 = batch['intrinsics2'][idx_in_batch].to(device)\n        pose2 = batch['pose2'][idx_in_batch].view(4, 4).to(device)\n        bbox2 = batch['bbox2'][idx_in_batch].to(device)\n\n        # Network output\n        dense_features1 = output['dense_features1'][idx_in_batch]\n        c, h1, w1 = dense_features1.size()\n        scores1 = output['scores1'][idx_in_batch].view(-1)\n\n        dense_features2 = output['dense_features2'][idx_in_batch]\n        _, h2, w2 = dense_features2.size()\n        scores2 = output['scores2'][idx_in_batch]\n\n        all_descriptors1 = F.normalize(dense_features1.view(c, -1), dim=0)\n        descriptors1 = all_descriptors1\n\n        all_descriptors2 = F.normalize(dense_features2.view(c, -1), dim=0)\n\n        # Warp the positions from image 1 to image 2\n        fmap_pos1 = grid_positions(h1, w1, device)\n        pos1 = upscale_positions(fmap_pos1, scaling_steps=scaling_steps)\n        try:\n            pos1, pos2, ids = warp(\n                pos1,\n                depth1, intrinsics1, pose1, bbox1,\n                depth2, intrinsics2, pose2, bbox2\n            )\n        except EmptyTensorError:\n            continue\n        fmap_pos1 = fmap_pos1[:, ids]\n        descriptors1 = descriptors1[:, ids]\n        scores1 = scores1[ids]\n\n        # Skip the pair if not enough GT correspondences are available\n        if ids.size(0) < 128:\n            continue\n\n        # Descriptors at the corresponding positions\n        fmap_pos2 = torch.round(\n            downscale_positions(pos2, scaling_steps=scaling_steps)\n        ).long()\n        descriptors2 = F.normalize(\n            dense_features2[:, fmap_pos2[0, :], fmap_pos2[1, :]],\n            dim=0\n        )\n        positive_distance = 2 - 2 * (\n            descriptors1.t().unsqueeze(1) @ descriptors2.t().unsqueeze(2)\n        ).squeeze()\n\n        all_fmap_pos2 = grid_positions(h2, w2, device)\n        position_distance = torch.max(\n            torch.abs(\n                fmap_pos2.unsqueeze(2).float() -\n                all_fmap_pos2.unsqueeze(1)\n            ),\n            dim=0\n        )[0]\n        is_out_of_safe_radius = position_distance > safe_radius\n        distance_matrix = 2 - 2 * (descriptors1.t() @ all_descriptors2)\n        negative_distance2 = torch.min(\n            distance_matrix + (1 - is_out_of_safe_radius.float()) * 10.,\n            dim=1\n        )[0]\n\n        all_fmap_pos1 = grid_positions(h1, w1, device)\n        position_distance = torch.max(\n            torch.abs(\n                fmap_pos1.unsqueeze(2).float() -\n                all_fmap_pos1.unsqueeze(1)\n            ),\n            dim=0\n        )[0]\n        is_out_of_safe_radius = position_distance > safe_radius\n        distance_matrix = 2 - 2 * (descriptors2.t() @ all_descriptors1)\n        negative_distance1 = torch.min(\n            distance_matrix + (1 - is_out_of_safe_radius.float()) * 10.,\n            dim=1\n        )[0]\n\n        diff = positive_distance - torch.min(\n            negative_distance1, negative_distance2\n        )\n\n        scores2 = scores2[fmap_pos2[0, :], fmap_pos2[1, :]]\n\n        loss = loss + (\n            torch.sum(scores1 * scores2 * F.relu(margin + diff)) /\n            torch.sum(scores1 * scores2)\n        )\n\n        has_grad = True\n        n_valid_samples += 1\n\n        if plot and batch['batch_idx'] % batch['log_interval'] == 0:\n            pos1_aux = pos1.cpu().numpy()\n            pos2_aux = pos2.cpu().numpy()\n            k = pos1_aux.shape[1]\n            col = np.random.rand(k, 3)\n            n_sp = 4\n            plt.figure()\n            plt.subplot(1, n_sp, 1)\n            im1 = imshow_image(\n                batch['image1'][idx_in_batch].cpu().numpy(),\n                preprocessing=batch['preprocessing']\n            )\n            plt.imshow(im1)\n            plt.scatter(\n                pos1_aux[1, :], pos1_aux[0, :],\n                s=0.25**2, c=col, marker=',', alpha=0.5\n            )\n            plt.axis('off')\n            plt.subplot(1, n_sp, 2)\n            plt.imshow(\n                output['scores1'][idx_in_batch].data.cpu().numpy(),\n                cmap='Reds'\n            )\n            plt.axis('off')\n            plt.subplot(1, n_sp, 3)\n            im2 = imshow_image(\n                batch['image2'][idx_in_batch].cpu().numpy(),\n                preprocessing=batch['preprocessing']\n            )\n            plt.imshow(im2)\n            plt.scatter(\n                pos2_aux[1, :], pos2_aux[0, :],\n                s=0.25**2, c=col, marker=',', alpha=0.5\n            )\n            plt.axis('off')\n            plt.subplot(1, n_sp, 4)\n            plt.imshow(\n                output['scores2'][idx_in_batch].data.cpu().numpy(),\n                cmap='Reds'\n            )\n            plt.axis('off')\n            savefig('train_vis/%s.%02d.%02d.%d.png' % (\n                'train' if batch['train'] else 'valid',\n                batch['epoch_idx'],\n                batch['batch_idx'] // batch['log_interval'],\n                idx_in_batch\n            ), dpi=300)\n            plt.close()\n\n    if not has_grad:\n        raise NoGradientError\n\n    loss = loss / n_valid_samples\n\n    return loss\n\n\ndef interpolate_depth(pos, depth):\n    device = pos.device\n\n    ids = torch.arange(0, pos.size(1), device=device)\n\n    h, w = depth.size()\n\n    i = pos[0, :]\n    j = pos[1, :]\n\n    # Valid corners\n    i_top_left = torch.floor(i).long()\n    j_top_left = torch.floor(j).long()\n    valid_top_left = torch.min(i_top_left >= 0, j_top_left >= 0)\n\n    i_top_right = torch.floor(i).long()\n    j_top_right = torch.ceil(j).long()\n    valid_top_right = torch.min(i_top_right >= 0, j_top_right < w)\n\n    i_bottom_left = torch.ceil(i).long()\n    j_bottom_left = torch.floor(j).long()\n    valid_bottom_left = torch.min(i_bottom_left < h, j_bottom_left >= 0)\n\n    i_bottom_right = torch.ceil(i).long()\n    j_bottom_right = torch.ceil(j).long()\n    valid_bottom_right = torch.min(i_bottom_right < h, j_bottom_right < w)\n\n    valid_corners = torch.min(\n        torch.min(valid_top_left, valid_top_right),\n        torch.min(valid_bottom_left, valid_bottom_right)\n    )\n\n    i_top_left = i_top_left[valid_corners]\n    j_top_left = j_top_left[valid_corners]\n\n    i_top_right = i_top_right[valid_corners]\n    j_top_right = j_top_right[valid_corners]\n\n    i_bottom_left = i_bottom_left[valid_corners]\n    j_bottom_left = j_bottom_left[valid_corners]\n\n    i_bottom_right = i_bottom_right[valid_corners]\n    j_bottom_right = j_bottom_right[valid_corners]\n\n    ids = ids[valid_corners]\n    if ids.size(0) == 0:\n        raise EmptyTensorError\n\n    # Valid depth\n    valid_depth = torch.min(\n        torch.min(\n            depth[i_top_left, j_top_left] > 0,\n            depth[i_top_right, j_top_right] > 0\n        ),\n        torch.min(\n            depth[i_bottom_left, j_bottom_left] > 0,\n            depth[i_bottom_right, j_bottom_right] > 0\n        )\n    )\n\n    i_top_left = i_top_left[valid_depth]\n    j_top_left = j_top_left[valid_depth]\n\n    i_top_right = i_top_right[valid_depth]\n    j_top_right = j_top_right[valid_depth]\n\n    i_bottom_left = i_bottom_left[valid_depth]\n    j_bottom_left = j_bottom_left[valid_depth]\n\n    i_bottom_right = i_bottom_right[valid_depth]\n    j_bottom_right = j_bottom_right[valid_depth]\n\n    ids = ids[valid_depth]\n    if ids.size(0) == 0:\n        raise EmptyTensorError\n\n    # Interpolation\n    i = i[ids]\n    j = j[ids]\n    dist_i_top_left = i - i_top_left.float()\n    dist_j_top_left = j - j_top_left.float()\n    w_top_left = (1 - dist_i_top_left) * (1 - dist_j_top_left)\n    w_top_right = (1 - dist_i_top_left) * dist_j_top_left\n    w_bottom_left = dist_i_top_left * (1 - dist_j_top_left)\n    w_bottom_right = dist_i_top_left * dist_j_top_left\n\n    interpolated_depth = (\n        w_top_left * depth[i_top_left, j_top_left] +\n        w_top_right * depth[i_top_right, j_top_right] +\n        w_bottom_left * depth[i_bottom_left, j_bottom_left] +\n        w_bottom_right * depth[i_bottom_right, j_bottom_right]\n    )\n\n    pos = torch.cat([i.view(1, -1), j.view(1, -1)], dim=0)\n\n    return [interpolated_depth, pos, ids]\n\n\ndef uv_to_pos(uv):\n    return torch.cat([uv[1, :].view(1, -1), uv[0, :].view(1, -1)], dim=0)\n\n\ndef warp(\n        pos1,\n        depth1, intrinsics1, pose1, bbox1,\n        depth2, intrinsics2, pose2, bbox2\n):\n    device = pos1.device\n\n    Z1, pos1, ids = interpolate_depth(pos1, depth1)\n\n    # COLMAP convention\n    u1 = pos1[1, :] + bbox1[1] + .5\n    v1 = pos1[0, :] + bbox1[0] + .5\n\n    X1 = (u1 - intrinsics1[0, 2]) * (Z1 / intrinsics1[0, 0])\n    Y1 = (v1 - intrinsics1[1, 2]) * (Z1 / intrinsics1[1, 1])\n\n    XYZ1_hom = torch.cat([\n        X1.view(1, -1),\n        Y1.view(1, -1),\n        Z1.view(1, -1),\n        torch.ones(1, Z1.size(0), device=device)\n    ], dim=0)\n    XYZ2_hom = torch.chain_matmul(pose2, torch.inverse(pose1), XYZ1_hom)\n    XYZ2 = XYZ2_hom[: -1, :] / XYZ2_hom[-1, :].view(1, -1)\n\n    uv2_hom = torch.matmul(intrinsics2, XYZ2)\n    uv2 = uv2_hom[: -1, :] / uv2_hom[-1, :].view(1, -1)\n\n    u2 = uv2[0, :] - bbox2[1] - .5\n    v2 = uv2[1, :] - bbox2[0] - .5\n    uv2 = torch.cat([u2.view(1, -1),  v2.view(1, -1)], dim=0)\n\n    annotated_depth, pos2, new_ids = interpolate_depth(uv_to_pos(uv2), depth2)\n\n    ids = ids[new_ids]\n    pos1 = pos1[:, new_ids]\n    estimated_depth = XYZ2[2, new_ids]\n\n    inlier_mask = torch.abs(estimated_depth - annotated_depth) < 0.05\n\n    ids = ids[inlier_mask]\n    if ids.size(0) == 0:\n        raise EmptyTensorError\n\n    pos2 = pos2[:, inlier_mask]\n    pos1 = pos1[:, inlier_mask]\n\n    return pos1, pos2, ids\n"""
lib/model.py,10,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport torchvision.models as models\n\n\nclass DenseFeatureExtractionModule(nn.Module):\n    def __init__(self, finetune_feature_extraction=False, use_cuda=True):\n        super(DenseFeatureExtractionModule, self).__init__()\n\n        model = models.vgg16()\n        vgg16_layers = [\n            'conv1_1', 'relu1_1', 'conv1_2', 'relu1_2',\n            'pool1',\n            'conv2_1', 'relu2_1', 'conv2_2', 'relu2_2',\n            'pool2',\n            'conv3_1', 'relu3_1', 'conv3_2', 'relu3_2', 'conv3_3', 'relu3_3',\n            'pool3',\n            'conv4_1', 'relu4_1', 'conv4_2', 'relu4_2', 'conv4_3', 'relu4_3',\n            'pool4',\n            'conv5_1', 'relu5_1', 'conv5_2', 'relu5_2', 'conv5_3', 'relu5_3',\n            'pool5'\n        ]\n        conv4_3_idx = vgg16_layers.index('conv4_3')\n\n        self.model = nn.Sequential(\n            *list(model.features.children())[: conv4_3_idx + 1]\n        )\n\n        self.num_channels = 512\n\n        # Fix forward parameters\n        for param in self.model.parameters():\n            param.requires_grad = False\n        if finetune_feature_extraction:\n            # Unlock conv4_3\n            for param in list(self.model.parameters())[-2 :]:\n                param.requires_grad = True\n\n        if use_cuda:\n            self.model = self.model.cuda()\n\n    def forward(self, batch):\n        output = self.model(batch)\n        return output\n\n\nclass SoftDetectionModule(nn.Module):\n    def __init__(self, soft_local_max_size=3):\n        super(SoftDetectionModule, self).__init__()\n\n        self.soft_local_max_size = soft_local_max_size\n\n        self.pad = self.soft_local_max_size // 2\n\n    def forward(self, batch):\n        b = batch.size(0)\n\n        batch = F.relu(batch)\n\n        max_per_sample = torch.max(batch.view(b, -1), dim=1)[0]\n        exp = torch.exp(batch / max_per_sample.view(b, 1, 1, 1))\n        sum_exp = (\n            self.soft_local_max_size ** 2 *\n            F.avg_pool2d(\n                F.pad(exp, [self.pad] * 4, mode='constant', value=1.),\n                self.soft_local_max_size, stride=1\n            )\n        )\n        local_max_score = exp / sum_exp\n\n        depth_wise_max = torch.max(batch, dim=1)[0]\n        depth_wise_max_score = batch / depth_wise_max.unsqueeze(1)\n\n        all_scores = local_max_score * depth_wise_max_score\n        score = torch.max(all_scores, dim=1)[0]\n\n        score = score / torch.sum(score.view(b, -1), dim=1).view(b, 1, 1)\n\n        return score\n\n\nclass D2Net(nn.Module):\n    def __init__(self, model_file=None, use_cuda=True):\n        super(D2Net, self).__init__()\n\n        self.dense_feature_extraction = DenseFeatureExtractionModule(\n            finetune_feature_extraction=True,\n            use_cuda=use_cuda\n        )\n\n        self.detection = SoftDetectionModule()\n\n        if model_file is not None:\n            if use_cuda:\n                self.load_state_dict(torch.load(model_file)['model'])\n            else:\n                self.load_state_dict(torch.load(model_file, map_location='cpu')['model'])\n\n    def forward(self, batch):\n        b = batch['image1'].size(0)\n\n        dense_features = self.dense_feature_extraction(\n            torch.cat([batch['image1'], batch['image2']], dim=0)\n        )\n\n        scores = self.detection(dense_features)\n\n        dense_features1 = dense_features[: b, :, :, :]\n        dense_features2 = dense_features[b :, :, :, :]\n\n        scores1 = scores[: b, :, :]\n        scores2 = scores[b :, :, :]\n\n        return {\n            'dense_features1': dense_features1,\n            'scores1': scores1,\n            'dense_features2': dense_features2,\n            'scores2': scores2\n        }\n"""
lib/model_test.py,17,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass DenseFeatureExtractionModule(nn.Module):\n    def __init__(self, use_relu=True, use_cuda=True):\n        super(DenseFeatureExtractionModule, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(64, 64, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(2, stride=2),\n            nn.Conv2d(128, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, 3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(2, stride=1),\n            nn.Conv2d(256, 512, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(512, 512, 3, padding=2, dilation=2),\n        )\n        self.num_channels = 512\n\n        self.use_relu = use_relu\n\n        if use_cuda:\n            self.model = self.model.cuda()\n\n    def forward(self, batch):\n        output = self.model(batch)\n        if self.use_relu:\n            output = F.relu(output)\n        return output\n\n\nclass D2Net(nn.Module):\n    def __init__(self, model_file=None, use_relu=True, use_cuda=True):\n        super(D2Net, self).__init__()\n\n        self.dense_feature_extraction = DenseFeatureExtractionModule(\n            use_relu=use_relu, use_cuda=use_cuda\n        )\n\n        self.detection = HardDetectionModule()\n\n        self.localization = HandcraftedLocalizationModule()\n\n        if model_file is not None:\n            if use_cuda:\n                self.load_state_dict(torch.load(model_file)['model'])\n            else:\n                self.load_state_dict(torch.load(model_file, map_location='cpu')['model'])\n\n    def forward(self, batch):\n        _, _, h, w = batch.size()\n        dense_features = self.dense_feature_extraction(batch)\n\n        detections = self.detection(dense_features)\n\n        displacements = self.localization(dense_features)\n\n        return {\n            'dense_features': dense_features,\n            'detections': detections,\n            'displacements': displacements\n        }\n\n\nclass HardDetectionModule(nn.Module):\n    def __init__(self, edge_threshold=5):\n        super(HardDetectionModule, self).__init__()\n\n        self.edge_threshold = edge_threshold\n\n        self.dii_filter = torch.tensor(\n            [[0, 1., 0], [0, -2., 0], [0, 1., 0]]\n        ).view(1, 1, 3, 3)\n        self.dij_filter = 0.25 * torch.tensor(\n            [[1., 0, -1.], [0, 0., 0], [-1., 0, 1.]]\n        ).view(1, 1, 3, 3)\n        self.djj_filter = torch.tensor(\n            [[0, 0, 0], [1., -2., 1.], [0, 0, 0]]\n        ).view(1, 1, 3, 3)\n\n    def forward(self, batch):\n        b, c, h, w = batch.size()\n        device = batch.device\n\n        depth_wise_max = torch.max(batch, dim=1)[0]\n        is_depth_wise_max = (batch == depth_wise_max)\n        del depth_wise_max\n\n        local_max = F.max_pool2d(batch, 3, stride=1, padding=1)\n        is_local_max = (batch == local_max)\n        del local_max\n\n        dii = F.conv2d(\n            batch.view(-1, 1, h, w), self.dii_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        dij = F.conv2d(\n            batch.view(-1, 1, h, w), self.dij_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        djj = F.conv2d(\n            batch.view(-1, 1, h, w), self.djj_filter.to(device), padding=1\n        ).view(b, c, h, w)\n\n        det = dii * djj - dij * dij\n        tr = dii + djj\n        del dii, dij, djj\n\n        threshold = (self.edge_threshold + 1) ** 2 / self.edge_threshold\n        is_not_edge = torch.min(tr * tr / det <= threshold, det > 0)\n\n        detected = torch.min(\n            is_depth_wise_max,\n            torch.min(is_local_max, is_not_edge)\n        )\n        del is_depth_wise_max, is_local_max, is_not_edge\n\n        return detected\n\n\nclass HandcraftedLocalizationModule(nn.Module):\n    def __init__(self):\n        super(HandcraftedLocalizationModule, self).__init__()\n\n        self.di_filter = torch.tensor(\n            [[0, -0.5, 0], [0, 0, 0], [0,  0.5, 0]]\n        ).view(1, 1, 3, 3)\n        self.dj_filter = torch.tensor(\n            [[0, 0, 0], [-0.5, 0, 0.5], [0, 0, 0]]\n        ).view(1, 1, 3, 3)\n\n        self.dii_filter = torch.tensor(\n            [[0, 1., 0], [0, -2., 0], [0, 1., 0]]\n        ).view(1, 1, 3, 3)\n        self.dij_filter = 0.25 * torch.tensor(\n            [[1., 0, -1.], [0, 0., 0], [-1., 0, 1.]]\n        ).view(1, 1, 3, 3)\n        self.djj_filter = torch.tensor(\n            [[0, 0, 0], [1., -2., 1.], [0, 0, 0]]\n        ).view(1, 1, 3, 3)\n\n    def forward(self, batch):\n        b, c, h, w = batch.size()\n        device = batch.device\n\n        dii = F.conv2d(\n            batch.view(-1, 1, h, w), self.dii_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        dij = F.conv2d(\n            batch.view(-1, 1, h, w), self.dij_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        djj = F.conv2d(\n            batch.view(-1, 1, h, w), self.djj_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        det = dii * djj - dij * dij\n\n        inv_hess_00 = djj / det\n        inv_hess_01 = -dij / det\n        inv_hess_11 = dii / det\n        del dii, dij, djj, det\n\n        di = F.conv2d(\n            batch.view(-1, 1, h, w), self.di_filter.to(device), padding=1\n        ).view(b, c, h, w)\n        dj = F.conv2d(\n            batch.view(-1, 1, h, w), self.dj_filter.to(device), padding=1\n        ).view(b, c, h, w)\n\n        step_i = -(inv_hess_00 * di + inv_hess_01 * dj)\n        step_j = -(inv_hess_01 * di + inv_hess_11 * dj)\n        del inv_hess_00, inv_hess_01, inv_hess_11, di, dj\n\n        return torch.stack([step_i, step_j], dim=1)\n"""
lib/pyramid.py,19,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom lib.exceptions import EmptyTensorError\nfrom lib.utils import interpolate_dense_features, upscale_positions\n\n\ndef process_multiscale(image, model, scales=[.5, 1, 2]):\n    b, _, h_init, w_init = image.size()\n    device = image.device\n    assert(b == 1)\n\n    all_keypoints = torch.zeros([3, 0])\n    all_descriptors = torch.zeros([\n        model.dense_feature_extraction.num_channels, 0\n    ])\n    all_scores = torch.zeros(0)\n\n    previous_dense_features = None\n    banned = None\n    for idx, scale in enumerate(scales):\n        current_image = F.interpolate(\n            image, scale_factor=scale,\n            mode='bilinear', align_corners=True\n        )\n        _, _, h_level, w_level = current_image.size()\n\n        dense_features = model.dense_feature_extraction(current_image)\n        del current_image\n\n        _, _, h, w = dense_features.size()\n\n        # Sum the feature maps.\n        if previous_dense_features is not None:\n            dense_features += F.interpolate(\n                previous_dense_features, size=[h, w],\n                mode='bilinear', align_corners=True\n            )\n            del previous_dense_features\n\n        # Recover detections.\n        detections = model.detection(dense_features)\n        if banned is not None:\n            banned = F.interpolate(banned.float(), size=[h, w]).bool()\n            detections = torch.min(detections, ~banned)\n            banned = torch.max(\n                torch.max(detections, dim=1)[0].unsqueeze(1), banned\n            )\n        else:\n            banned = torch.max(detections, dim=1)[0].unsqueeze(1)\n        fmap_pos = torch.nonzero(detections[0].cpu()).t()\n        del detections\n\n        # Recover displacements.\n        displacements = model.localization(dense_features)[0].cpu()\n        displacements_i = displacements[\n            0, fmap_pos[0, :], fmap_pos[1, :], fmap_pos[2, :]\n        ]\n        displacements_j = displacements[\n            1, fmap_pos[0, :], fmap_pos[1, :], fmap_pos[2, :]\n        ]\n        del displacements\n\n        mask = torch.min(\n            torch.abs(displacements_i) < 0.5,\n            torch.abs(displacements_j) < 0.5\n        )\n        fmap_pos = fmap_pos[:, mask]\n        valid_displacements = torch.stack([\n            displacements_i[mask],\n            displacements_j[mask]\n        ], dim=0)\n        del mask, displacements_i, displacements_j\n\n        fmap_keypoints = fmap_pos[1 :, :].float() + valid_displacements\n        del valid_displacements\n\n        try:\n            raw_descriptors, _, ids = interpolate_dense_features(\n                fmap_keypoints.to(device),\n                dense_features[0]\n            )\n        except EmptyTensorError:\n            continue\n        fmap_pos = fmap_pos[:, ids]\n        fmap_keypoints = fmap_keypoints[:, ids]\n        del ids\n\n        keypoints = upscale_positions(fmap_keypoints, scaling_steps=2)\n        del fmap_keypoints\n\n        descriptors = F.normalize(raw_descriptors, dim=0).cpu()\n        del raw_descriptors\n\n        keypoints[0, :] *= h_init / h_level\n        keypoints[1, :] *= w_init / w_level\n\n        fmap_pos = fmap_pos.cpu()\n        keypoints = keypoints.cpu()\n\n        keypoints = torch.cat([\n            keypoints,\n            torch.ones([1, keypoints.size(1)]) * 1 / scale,\n        ], dim=0)\n\n        scores = dense_features[\n            0, fmap_pos[0, :], fmap_pos[1, :], fmap_pos[2, :]\n        ].cpu() / (idx + 1)\n        del fmap_pos\n\n        all_keypoints = torch.cat([all_keypoints, keypoints], dim=1)\n        all_descriptors = torch.cat([all_descriptors, descriptors], dim=1)\n        all_scores = torch.cat([all_scores, scores], dim=0)\n        del keypoints, descriptors\n\n        previous_dense_features = dense_features\n        del dense_features\n    del previous_dense_features, banned\n\n    keypoints = all_keypoints.t().numpy()\n    del all_keypoints\n    scores = all_scores.numpy()\n    del all_scores\n    descriptors = all_descriptors.t().numpy()\n    del all_descriptors\n    return keypoints, scores, descriptors\n"""
lib/utils.py,26,"b""import matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport torch\n\nfrom lib.exceptions import EmptyTensorError\n\n\ndef preprocess_image(image, preprocessing=None):\n    image = image.astype(np.float32)\n    image = np.transpose(image, [2, 0, 1])\n    if preprocessing is None:\n        pass\n    elif preprocessing == 'caffe':\n        # RGB -> BGR\n        image = image[:: -1, :, :]\n        # Zero-center by mean pixel\n        mean = np.array([103.939, 116.779, 123.68])\n        image = image - mean.reshape([3, 1, 1])\n    elif preprocessing == 'torch':\n        image /= 255.0\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = (image - mean.reshape([3, 1, 1])) / std.reshape([3, 1, 1])\n    else:\n        raise ValueError('Unknown preprocessing parameter.')\n    return image\n\n\ndef imshow_image(image, preprocessing=None):\n    if preprocessing is None:\n        pass\n    elif preprocessing == 'caffe':\n        mean = np.array([103.939, 116.779, 123.68])\n        image = image + mean.reshape([3, 1, 1])\n        # RGB -> BGR\n        image = image[:: -1, :, :]\n    elif preprocessing == 'torch':\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n        image = image * std.reshape([3, 1, 1]) + mean.reshape([3, 1, 1])\n        image *= 255.0\n    else:\n        raise ValueError('Unknown preprocessing parameter.')\n    image = np.transpose(image, [1, 2, 0])\n    image = np.round(image).astype(np.uint8)\n    return image\n\n\ndef grid_positions(h, w, device, matrix=False):\n    lines = torch.arange(\n        0, h, device=device\n    ).view(-1, 1).float().repeat(1, w)\n    columns = torch.arange(\n        0, w, device=device\n    ).view(1, -1).float().repeat(h, 1)\n    if matrix:\n        return torch.stack([lines, columns], dim=0)\n    else:\n        return torch.cat([lines.view(1, -1), columns.view(1, -1)], dim=0)\n\n\ndef upscale_positions(pos, scaling_steps=0):\n    for _ in range(scaling_steps):\n        pos = pos * 2 + 0.5\n    return pos\n\n\ndef downscale_positions(pos, scaling_steps=0):\n    for _ in range(scaling_steps):\n        pos = (pos - 0.5) / 2\n    return pos\n\n\ndef interpolate_dense_features(pos, dense_features, return_corners=False):\n    device = pos.device\n\n    ids = torch.arange(0, pos.size(1), device=device)\n\n    _, h, w = dense_features.size()\n\n    i = pos[0, :]\n    j = pos[1, :]\n\n    # Valid corners\n    i_top_left = torch.floor(i).long()\n    j_top_left = torch.floor(j).long()\n    valid_top_left = torch.min(i_top_left >= 0, j_top_left >= 0)\n\n    i_top_right = torch.floor(i).long()\n    j_top_right = torch.ceil(j).long()\n    valid_top_right = torch.min(i_top_right >= 0, j_top_right < w)\n\n    i_bottom_left = torch.ceil(i).long()\n    j_bottom_left = torch.floor(j).long()\n    valid_bottom_left = torch.min(i_bottom_left < h, j_bottom_left >= 0)\n\n    i_bottom_right = torch.ceil(i).long()\n    j_bottom_right = torch.ceil(j).long()\n    valid_bottom_right = torch.min(i_bottom_right < h, j_bottom_right < w)\n\n    valid_corners = torch.min(\n        torch.min(valid_top_left, valid_top_right),\n        torch.min(valid_bottom_left, valid_bottom_right)\n    )\n\n    i_top_left = i_top_left[valid_corners]\n    j_top_left = j_top_left[valid_corners]\n\n    i_top_right = i_top_right[valid_corners]\n    j_top_right = j_top_right[valid_corners]\n\n    i_bottom_left = i_bottom_left[valid_corners]\n    j_bottom_left = j_bottom_left[valid_corners]\n\n    i_bottom_right = i_bottom_right[valid_corners]\n    j_bottom_right = j_bottom_right[valid_corners]\n\n    ids = ids[valid_corners]\n    if ids.size(0) == 0:\n        raise EmptyTensorError\n\n    # Interpolation\n    i = i[ids]\n    j = j[ids]\n    dist_i_top_left = i - i_top_left.float()\n    dist_j_top_left = j - j_top_left.float()\n    w_top_left = (1 - dist_i_top_left) * (1 - dist_j_top_left)\n    w_top_right = (1 - dist_i_top_left) * dist_j_top_left\n    w_bottom_left = dist_i_top_left * (1 - dist_j_top_left)\n    w_bottom_right = dist_i_top_left * dist_j_top_left\n\n    descriptors = (\n        w_top_left * dense_features[:, i_top_left, j_top_left] +\n        w_top_right * dense_features[:, i_top_right, j_top_right] +\n        w_bottom_left * dense_features[:, i_bottom_left, j_bottom_left] +\n        w_bottom_right * dense_features[:, i_bottom_right, j_bottom_right]\n    )\n\n    pos = torch.cat([i.view(1, -1), j.view(1, -1)], dim=0)\n\n    if not return_corners:\n        return [descriptors, pos, ids]\n    else:\n        corners = torch.stack([\n            torch.stack([i_top_left, j_top_left], dim=0),\n            torch.stack([i_top_right, j_top_right], dim=0),\n            torch.stack([i_bottom_left, j_bottom_left], dim=0),\n            torch.stack([i_bottom_right, j_bottom_right], dim=0)\n        ], dim=0)\n        return [descriptors, pos, ids, corners]\n\n\ndef savefig(filepath, fig=None, dpi=None):\n    # TomNorway - https://stackoverflow.com/a/53516034\n    if not fig:\n        fig = plt.gcf()\n\n    plt.subplots_adjust(0, 0, 1, 1, 0, 0)\n    for ax in fig.axes:\n        ax.axis('off')\n        ax.margins(0, 0)\n        ax.xaxis.set_major_locator(plt.NullLocator())\n        ax.yaxis.set_major_locator(plt.NullLocator())\n\n    fig.savefig(filepath, pad_inches=0, bbox_inches='tight', dpi=dpi)\n"""
megadepth_utils/preprocess_scene.py,0,"b""import argparse\n\nimport imagesize\n\nimport numpy as np\n\nimport os\n\nparser = argparse.ArgumentParser(description='MegaDepth preprocessing script')\n\nparser.add_argument(\n    '--base_path', type=str, required=True,\n    help='path to MegaDepth'\n)\nparser.add_argument(\n    '--scene_id', type=str, required=True,\n    help='scene ID'\n)\n\nparser.add_argument(\n    '--output_path', type=str, required=True,\n    help='path to the output directory'\n)\n\nargs = parser.parse_args()\n\nbase_path = args.base_path\n# Remove the trailing / if need be.\nif base_path[-1] in ['/', '\\\\']:\n    base_path = base_path[: - 1]\nscene_id = args.scene_id\n\nbase_depth_path = os.path.join(\n    base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\nbase_undistorted_sfm_path = os.path.join(\n    base_path, 'Undistorted_SfM'\n)\n\nundistorted_sparse_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'sparse-txt'\n)\nif not os.path.exists(undistorted_sparse_path):\n    exit()\n\ndepths_path = os.path.join(\n    base_depth_path, scene_id, 'dense0', 'depths'\n)\nif not os.path.exists(depths_path):\n    exit()\n\nimages_path = os.path.join(\n    base_undistorted_sfm_path, scene_id, 'images'\n)\nif not os.path.exists(images_path):\n    exit()\n\n# Process cameras.txt\nwith open(os.path.join(undistorted_sparse_path, 'cameras.txt'), 'r') as f:\n    raw = f.readlines()[3 :]  # skip the header\n\ncamera_intrinsics = {}\nfor camera in raw:\n    camera = camera.split(' ')\n    camera_intrinsics[int(camera[0])] = [float(elem) for elem in camera[2 :]]\n\n# Process points3D.txt\nwith open(os.path.join(undistorted_sparse_path, 'points3D.txt'), 'r') as f:\n    raw = f.readlines()[3 :]  # skip the header\n\npoints3D = {}\nfor point3D in raw:\n    point3D = point3D.split(' ')\n    points3D[int(point3D[0])] = np.array([\n        float(point3D[1]), float(point3D[2]), float(point3D[3])\n    ])\n    \n# Process images.txt\nwith open(os.path.join(undistorted_sparse_path, 'images.txt'), 'r') as f:\n    raw = f.readlines()[4 :]  # skip the header\n\nimage_id_to_idx = {}\nimage_names = []\nraw_pose = []\ncamera = []\npoints3D_id_to_2D = []\nn_points3D = []\nfor idx, (image, points) in enumerate(zip(raw[:: 2], raw[1 :: 2])):\n    image = image.split(' ')\n    points = points.split(' ')\n\n    image_id_to_idx[int(image[0])] = idx\n\n    image_name = image[-1].strip('\\n')\n    image_names.append(image_name)\n\n    raw_pose.append([float(elem) for elem in image[1 : -2]])\n    camera.append(int(image[-2]))\n    current_points3D_id_to_2D = {}\n    for x, y, point3D_id in zip(points[:: 3], points[1 :: 3], points[2 :: 3]):\n        if int(point3D_id) == -1:\n            continue\n        current_points3D_id_to_2D[int(point3D_id)] = [float(x), float(y)]\n    points3D_id_to_2D.append(current_points3D_id_to_2D)\n    n_points3D.append(len(current_points3D_id_to_2D))\nn_images = len(image_names)\n\n# Image and depthmaps paths\nimage_paths = []\ndepth_paths = []\nfor image_name in image_names:\n    image_path = os.path.join(images_path, image_name)\n   \n    # Path to the depth file\n    depth_path = os.path.join(\n        depths_path, '%s.h5' % os.path.splitext(image_name)[0]\n    )\n    \n    if os.path.exists(depth_path):\n        # Check if depth map or background / foreground mask\n        file_size = os.stat(depth_path).st_size\n        # Rough estimate - 75KB might work as well\n        if file_size < 100 * 1024:\n            depth_paths.append(None)\n            image_paths.append(None)\n        else:\n            depth_paths.append(depth_path[len(base_path) + 1 :])\n            image_paths.append(image_path[len(base_path) + 1 :])\n    else:\n        depth_paths.append(None)\n        image_paths.append(None)\n\n# Camera configuration\nintrinsics = []\nposes = []\nprincipal_axis = []\npoints3D_id_to_ndepth = []\nfor idx, image_name in enumerate(image_names):\n    if image_paths[idx] is None:\n        intrinsics.append(None)\n        poses.append(None)\n        principal_axis.append([0, 0, 0])\n        points3D_id_to_ndepth.append({})\n        continue\n    image_intrinsics = camera_intrinsics[camera[idx]]\n    K = np.zeros([3, 3])\n    K[0, 0] = image_intrinsics[2]\n    K[0, 2] = image_intrinsics[4]\n    K[1, 1] = image_intrinsics[3]\n    K[1, 2] = image_intrinsics[5]\n    K[2, 2] = 1\n    intrinsics.append(K)\n\n    image_pose = raw_pose[idx]\n    qvec = image_pose[: 4]\n    qvec = qvec / np.linalg.norm(qvec)\n    w, x, y, z = qvec\n    R = np.array([\n        [\n            1 - 2 * y * y - 2 * z * z,\n            2 * x * y - 2 * z * w,\n            2 * x * z + 2 * y * w\n        ],\n        [\n            2 * x * y + 2 * z * w,\n            1 - 2 * x * x - 2 * z * z,\n            2 * y * z - 2 * x * w\n        ],\n        [\n            2 * x * z - 2 * y * w,\n            2 * y * z + 2 * x * w,\n            1 - 2 * x * x - 2 * y * y\n        ]\n    ])\n    principal_axis.append(R[2, :])\n    t = image_pose[4 : 7]\n    # World-to-Camera pose\n    current_pose = np.zeros([4, 4])\n    current_pose[: 3, : 3] = R\n    current_pose[: 3, 3] = t\n    current_pose[3, 3] = 1\n    # Camera-to-World pose\n    # pose = np.zeros([4, 4])\n    # pose[: 3, : 3] = np.transpose(R)\n    # pose[: 3, 3] = -np.matmul(np.transpose(R), t)\n    # pose[3, 3] = 1\n    poses.append(current_pose)\n    \n    current_points3D_id_to_ndepth = {}\n    for point3D_id in points3D_id_to_2D[idx].keys():\n        p3d = points3D[point3D_id]\n        current_points3D_id_to_ndepth[point3D_id] = (np.dot(R[2, :], p3d) + t[2]) / (.5 * (K[0, 0] + K[1, 1])) \n    points3D_id_to_ndepth.append(current_points3D_id_to_ndepth)\nprincipal_axis = np.array(principal_axis)\nangles = np.rad2deg(np.arccos(\n    np.clip(\n        np.dot(principal_axis, np.transpose(principal_axis)),\n        -1, 1\n    )\n))\n\n# Compute overlap score\noverlap_matrix = np.full([n_images, n_images], -1.)\nscale_ratio_matrix = np.full([n_images, n_images], -1.)\nfor idx1 in range(n_images):\n    if image_paths[idx1] is None or depth_paths[idx1] is None:\n        continue\n    for idx2 in range(idx1 + 1, n_images):\n        if image_paths[idx2] is None or depth_paths[idx2] is None:\n            continue\n        matches = (\n            points3D_id_to_2D[idx1].keys() &\n            points3D_id_to_2D[idx2].keys()\n        )\n        min_num_points3D = min(\n            len(points3D_id_to_2D[idx1]), len(points3D_id_to_2D[idx2])\n        )\n        overlap_matrix[idx1, idx2] = len(matches) / len(points3D_id_to_2D[idx1])  # min_num_points3D\n        overlap_matrix[idx2, idx1] = len(matches) / len(points3D_id_to_2D[idx2])  # min_num_points3D\n        if len(matches) == 0:\n            continue\n        points3D_id_to_ndepth1 = points3D_id_to_ndepth[idx1]\n        points3D_id_to_ndepth2 = points3D_id_to_ndepth[idx2]\n        nd1 = np.array([points3D_id_to_ndepth1[match] for match in matches])\n        nd2 = np.array([points3D_id_to_ndepth2[match] for match in matches])\n        min_scale_ratio = np.min(np.maximum(nd1 / nd2, nd2 / nd1))\n        scale_ratio_matrix[idx1, idx2] = min_scale_ratio\n        scale_ratio_matrix[idx2, idx1] = min_scale_ratio\n\nnp.savez(\n    os.path.join(args.output_path, '%s.npz' % scene_id),\n    image_paths=image_paths,\n    depth_paths=depth_paths,\n    intrinsics=intrinsics,\n    poses=poses,\n    overlap_matrix=overlap_matrix,\n    scale_ratio_matrix=scale_ratio_matrix,\n    angles=angles,\n    n_points3D=n_points3D,\n    points3D_id_to_2D=points3D_id_to_2D,\n    points3D_id_to_ndepth=points3D_id_to_ndepth\n)\n"""
megadepth_utils/undistort_reconstructions.py,0,"b""import argparse\n\nimport imagesize\n\nimport os\n\nimport subprocess\n\nparser = argparse.ArgumentParser(description='MegaDepth Undistortion')\n\nparser.add_argument(\n    '--colmap_path', type=str, required=True,\n    help='path to colmap executable'\n)\nparser.add_argument(\n    '--base_path', type=str, required=True,\n    help='path to MegaDepth'\n)\n\nargs = parser.parse_args()\n\nsfm_path = os.path.join(\n    args.base_path, 'MegaDepth_v1_SfM'\n)\nbase_depth_path = os.path.join(\n    args.base_path, 'phoenix/S6/zl548/MegaDepth_v1'\n)\noutput_path = os.path.join(\n    args.base_path, 'Undistorted_SfM'\n)\n\nos.mkdir(output_path)\n\nfor scene_name in os.listdir(base_depth_path):\n    current_output_path = os.path.join(output_path, scene_name)\n    os.mkdir(current_output_path)\n\n    image_path = os.path.join(\n        base_depth_path, scene_name, 'dense0', 'imgs'\n    )\n    if not os.path.exists(image_path):\n        continue\n    \n    # Find the maximum image size in scene.\n    max_image_size = 0\n    for image_name in os.listdir(image_path):\n        max_image_size = max(\n            max_image_size,\n            max(imagesize.get(os.path.join(image_path, image_name)))\n        )\n\n    # Undistort the images and update the reconstruction.\n    subprocess.call([\n        os.path.join(args.colmap_path, 'colmap'), 'image_undistorter', \n        '--image_path', os.path.join(sfm_path, scene_name, 'images'),\n        '--input_path', os.path.join(sfm_path, scene_name, 'sparse', 'manhattan', '0'),\n        '--output_path',  current_output_path,\n        '--max_image_size', str(max_image_size)\n    ])\n\n    # Transform the reconstruction to raw text format.\n    sparse_txt_path = os.path.join(current_output_path, 'sparse-txt')\n    os.mkdir(sparse_txt_path)\n    subprocess.call([\n        os.path.join(args.colmap_path, 'colmap'), 'model_converter',\n        '--input_path', os.path.join(current_output_path, 'sparse'),\n        '--output_path', sparse_txt_path, \n        '--output_type', 'TXT'\n    ])"""
