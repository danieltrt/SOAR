file_path,api_count,code
__init__.py,0,"b'from mindsdb import *\nname = ""mindsdb""'"
setup.py,0,"b'import setuptools\nimport sys\nimport os\n\n\ndef remove_requirements(requirements, name, replace=None):\n    new_requirements = []\n    for requirement in requirements:\n        if requirement.split(\' \')[0] != name:\n            new_requirements.append(requirement)\n        elif replace is not None:\n            new_requirements.append(replace)\n    return new_requirements\n\nsys_platform = sys.platform\n\nabout = {}\nwith open(""mindsdb/__about__.py"") as fp:\n    exec(fp.read(), about)\n\nlong_description = open(\'README.md\', encoding=\'utf-8\').read()\n\nwith open(\'requirements.txt\', \'r\') as req_file:\n    requirements = [req.strip() for req in req_file.read().splitlines()]\n\nextra_data_sources_requirements = []\nwith open(\'optional_requirements_extra_data_sources.txt\', \'r\') as fp:\n    for line in fp:\n        extra_data_sources_requirements.append(line.rstrip(\'\\n\'))\n\nludwig_model_requirements = []\nbeta_requirements = []\nwith open(\'optional_requirements_ludwig_model.txt\', \'r\') as fp:\n    for line in fp:\n        ludwig_model_requirements.append(line.rstrip(\'\\n\'))\n\ndependency_links = []\n\n# Linux specific requirements\nif sys_platform == \'linux\' or sys_platform.startswith(\'linux\'):\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'tensorflow-estimator\')\n\n# OSX specific requirements\nelif sys_platform == \'darwin\':\n    requirements = requirements\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'tensorflow\', \'tensorflow == 1.13.1\')\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'tensorflow-estimator\', \'tensorflow-estimator == 1.13.0\')\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'ludwig\', \'ludwig == 0.1.2\')\n\n# Windows specific requirements\nelif sys_platform in [\'win32\',\'cygwin\',\'windows\']:\n    requirements = [\'cwrap\',*requirements]\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'tensorflow\', \'tensorflow == 1.13.1\')\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'ludwig\', \'ludwig == 0.1.2\')\n    ludwig_model_requirements = remove_requirements(ludwig_model_requirements, \'tensorflow-estimator\')\n    requirements = remove_requirements(requirements,\'wheel\', replace=\'wheel == 0.26.0\')\n\nelse:\n    print(\'\\n\\n====================\\n\\nError, platform {sys_platform} not recognized, proceeding to install anyway, but mindsdb might not work properly !\\n\\n====================\\n\\n\')\n\n\nsetuptools.setup(\n    name=about[\'__title__\'],\n    version=about[\'__version__\'],\n    url=about[\'__github__\'],\n    download_url=about[\'__pypi__\'],\n    license=about[\'__license__\'],\n    author=about[\'__author__\'],\n    author_email=about[\'__email__\'],\n    description=about[\'__description__\'],\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    packages=setuptools.find_packages(),\n    install_requires=requirements,\n    extras_require = {\n        \'extra_data_sources\': extra_data_sources_requirements\n        ,\'ludwig_model\': ludwig_model_requirements\n        ,\'beta\': beta_requirements\n    },\n    dependency_links=dependency_links,\n    classifiers=(\n        ""Programming Language :: Python :: 3"",\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ),\n    python_requires="">=3.6""\n)\n'"
test.py,0,"b'from mindsdb import Predictor\nimport sys\nimport pandas as pd\nimport json\nimport time\nimport lightwood\n\n\nlightwood.config.config.CONFIG.HELPER_MIXERS = False\nmdb = Predictor(name=\'test_predictor\')\n#\'rental_price\',\n#mdb.learn(to_predict=[\'neighborhood\',\'rental_price\'],from_data=""https://mindsdb-example-data.s3.eu-west-2.amazonaws.com/home_rentals.csv"",use_gpu=False,stop_training_in_x_seconds=1000, backend=\'lightwood\', unstable_parameters_dict={\'use_selfaware_model\':True})\n\np = mdb.predict(when={\'number_of_rooms\': 3, \'number_of_bathrooms\': 2, \'sqft\':2411, \'initial_price\':3000}, run_confidence_variation_analysis=True, use_gpu=True)\ne = p[0].explanation\n\np_arr = mdb.predict(when_data=\'https://mindsdb-example-data.s3.eu-west-2.amazonaws.com/home_rentals.csv\', use_gpu=True)\nprint(p_arr[0].explanation)\n\nfor p in p_arr:\n    e = p.explanation\n\np = mdb.predict(when={\'number_of_rooms\': 3, \'number_of_bathrooms\': 2, \'neighborhood\': \'south_side\', \'sqft\':2411}, run_confidence_variation_analysis=True, use_gpu=True)\n\nfor p in p_arr:\n    exp_s = p.epitomize()\n    exp = p.explanation\n    print(exp_s)\n\n    print(p.as_dict())\n    print(p.as_list())\n    print(p.raw_predictions())\n'"
mindsdb/__about__.py,0,"b'__title__ = \'MindsDB\'\n__package_name__ = \'mindsdb\'\n__version__ = \'1.20.1\'\n__description__ = ""MindsDB\'s goal is to make it very simple for developers to use the power of artificial neural networks in their projects.""\n__email__ = ""jorge@mindsdb.com""\n__author__ = \'MindsDB Inc\'\n__github__ = \'https://github.com/mindsdb/mindsdb\'\n__pypi__ = \'https://pypi.org/project/mindsdb\'\n__license__ = \'MIT\'\n__copyright__ = \'Copyright 2018- mindsdb\'\n'"
mindsdb/__init__.py,0,"b'import sys\nif sys.version_info < (3,6):\n    raise Exception(\'Sorry, For MindsDB Python < 3.6 is not supported\')\n\n# @TODO: FIND A WAY TO ACTUALLY SOLVE THIS ASAP !!!\n# HORRIBLE HACK TO AVOID SEGFAULT\nimport lightwood\n# HORRIBLE HACK TO AVOID SEGFAULT\n# @TODO: FIND A WAY TO ACTUALLY SOLVE THIS ASAP !!!\n\nfrom mindsdb.config import CONFIG\nimport mindsdb.libs.constants.mindsdb as CONST\n\nfrom mindsdb.__about__ import __package_name__ as name, __version__\nfrom mindsdb.libs.controllers.predictor import Predictor\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n# Data Sources\nfrom mindsdb.libs.data_sources.file_ds import FileDS\n\n# These might not initialized properly since they require optional dependencies, so we wrap them in a try-except and don\'t export them if the dependencies aren\'t installed\ntry:\n    from mindsdb.libs.data_sources.s3_ds import S3DS\nexcept:\n    pass\n\ntry:\n    from mindsdb.libs.data_sources.mysql_ds import MySqlDS\nexcept:\n    log.warning(""MySQL Datasource is not avaiable by default. If you wish to use it, please install mysqlclient or mindsdb[extra_data_sources]"")\n\ntry:\n    from mindsdb.libs.data_sources.postgres_ds import PostgresDS\nexcept:\n    log.warning(""PostgresDS Datasource is not avaiable by default. If you wish to use it, please install psycopg2 or mindsdb[extra_data_sources]"")\n\n\n\nfrom mindsdb.libs.data_sources.clickhouse_ds import ClickhouseDS\n\nMindsDB = Predictor\n'"
mindsdb/scraps.py,0,"b'\'\'\'\nThis file contains bits of codes that we might want to keep for later use,\n, but that are are currently not used anywhere in the codebase.\n\'\'\'\n\n# flake8: noqa\n\nfrom itertools import combinations, permutations\n\n\n# Previously in: mindsdb/libs/helpers/train_helpers.py\ndef getAllButOnePermutations(possible_columns):\n\n    permutations = {}\n\n    for col in possible_columns:\n        possible_columns_2 = [col3 for col3 in possible_columns if col3 != col ]\n        n_perms = "":"".join(possible_columns_2)\n\n        permutations[n_perms] = 1\n\n    ret = [perm.split(\':\') for perm in list(permutations.keys())]\n    return ret\n\n\n# Previously in mindsdb/libs/phases/stats_generator.py\n# NOTE: This function used to confuse permutations & combinations,\n# so I made both get_col_combinations() and get_col_permutations()\ndef get_col_combinations(columns, n=100):\n    """"""\n    Given a list of column names, finds first :param:n:\n    combinations (without replacement).\n    \n    :param columns: list of column names\n    :param n: max number of combinations\n    \n    :yields: example: [a, b, c] -> [[a], [b], [c], [a, b], [a, c], [b, c]]\n    """"""\n\n    count = 0\n    for i in range(1, len(columns)):\n        for combo in combinations(columns, i):\n            if count < n:\n                count += 1\n                yield combo\n\n\ndef get_col_permutations(columns, n=100):\n    """"""\n    Given a list of column names, finds first :param:n: permutations\n    \n    :param columns: list of column names\n    :param n: max number of permutations\n    \n    :yields: example: [a, b, c] -> [[a], [b], [c], [a, b], [b, a], [a, c], [c, a], [b, c], [c, b]]\n    """"""\n\n    count = 0\n    for i in range(1, len(columns)):\n        for perm in permutations(columns, i):\n            if count < n:\n                count += 1\n                yield perm\n\n\ndef getBestFitDistribution(self, data, bins=40):\n    """"""Model data by finding best fit distribution to data""""""\n    # Get histogram of original data\n\n    y, x = np.histogram(data, bins=bins, density=False)\n    x = (x + np.roll(x, -1))[:-1] / 2.0\n    # Distributions to check\n    DISTRIBUTIONS = [\n        st.bernoulli, st.beta,  st.cauchy, st.expon,  st.gamma, st.halfcauchy, st.lognorm,\n        st.norm, st.uniform, st.poisson\n    ]\n\n    # Best holders\n    best_distribution = st.norm\n    best_params = (0.0, 1.0)\n    best_sse = np.inf\n    # Estimate distribution parameters from data\n    for i, distribution in enumerate(DISTRIBUTIONS):\n        try:\n            # Ignore warnings from data that can\'t be fit\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\'ignore\')\n                # fit dist to data\n                params = distribution.fit(data)\n                # Separate parts of parameters\n                arg = params[:-2]\n                loc = params[-2]\n                scale = params[-1]\n\n                # Calculate fitted PDF and error with fit in distribution\n                pdf = distribution.pdf(x, loc=loc, scale=scale, *arg)\n                sse = np.sum(np.power(y - pdf, 2.0))\n                # identify if this distribution is better\n                if best_sse > sse > 0:\n                    best_distribution = distribution\n                    best_params = params\n                    best_sse = sse\n\n        except Exception:\n            pass\n\n    return (best_distribution.name, best_params, x.tolist(), y.tolist())\n\n\ndef _get_params_as_dictionary(self, params):\n    """""" Returns a dictionary with the params of the distribution """"""\n    arg = params[:-2]\n    loc = params[-2]\n    scale = params[-1]\n    ret = {\n        \'loc\': loc,\n        \'scale\': scale,\n        \'shape\': arg\n    }\n    return ret\n\n# def isFullText(self, data):\n#     """"""\n#     It determines if the column is full text right\n#     Right now we assume its full text if any cell contains any of the WORD_SEPARATORS\n#\n#     :param data: a list containing all the column\n#     :return: Boolean\n#     """"""\n#     for cell in data:\n#         try:\n#             if any(separator in cell for separator in WORD_SEPARATORS):\n#                 return True\n#         except:\n#             exc_type, exc_value, exc_traceback = sys.exc_info()\n#             error = traceback.format_exception(exc_type, exc_value,\n#                                       exc_traceback)\n#             return False\n#     return False\n\n# Bool to number and vice-versa\n\nboolean_dictionary = {True: \'True\', False: \'False\'}\nnumeric_dictionary = {True: 1, False: 0}\nfor column in df:\n    if is_numeric_dtype(df[column]):\n        df[column] = df[column].replace(numeric_dictionary)\n    else:\n        df[column] = df[column].replace(boolean_dictionary)\n'"
docs/video_docs/learn.py,0,"b""import mindsdb\n\nmodel = mindsdb.Predictor(name='wine_model')\nmodel.learn(from_data='wine_data_train.tsv', to_predict='Cultivar')\n"""
docs/video_docs/predict.py,0,"b""import mindsdb\n\nmodel = mindsdb.Predictor(name='wine_model')\npredictions = model.predict(when_data='wine_data_predict.tsv')\n\nfor index, prediction in enumerate(predictions):\n    Cultivar = prediction['Cultivar']\n    Cultivar_confidence = prediction['Cultivar_confidence']\n    print(f'Predicted cultivar {Cultivar} for row with index {index}')\n"""
mindsdb/config/__init__.py,0,"b'""""""\n*******************************************************\n * Copyright (C) 2017 MindsDB Inc. <copyright@mindsdb.com>\n *******************************************************\n""""""\n\n\nimport logging\nfrom .helpers import *\nimport  mindsdb.libs.constants.mindsdb as CONST\n\nclass Config:\n    # These are the paths for storing data regarding mindsdb models and model info\n    MINDSDB_STORAGE_PATH = if_env_else(\'MINDSDB_STORAGE_PATH\', get_and_create_default_storage_path())\n\n    # What percentage of data do we want to keep as test, and what as train default 10% is test\n    TEST_TRAIN_RATIO = if_env_else(\'TEST_TRAIN_RATIO\', 0.1)\n\n    # IF YOU CAN TO MOVE THE TRAINING OPERATION TO A DIFFERENT EXECUTION THREAD (DEFAULT True)\n    EXEC_LEARN_IN_THREAD = if_env_else(\'EXEC_LEARN_IN_THREAD\', False)\n\n    # LOG Config settings\n    DEFAULT_LOG_LEVEL = if_env_else(\'DEFAULT_LOG_LEVEL\', CONST.DEBUG_LOG_LEVEL)\n\n    CHECK_FOR_UPDATES = if_env_else(\'CHECK_FOR_UPDATES\', True)\n    IS_CI_TEST = if_env_else(\'IS_CI_TEST\', False)\n\n    # Default options for unning on sagemaker\n    SAGEMAKER = if_env_else(\'SAGEMAKER\', False)\n\n\nCONFIG = Config()\n'"
mindsdb/config/helpers.py,0,"b'import inspect\nimport os\nfrom pathlib import Path\nimport traceback\n\nfrom mindsdb.__about__ import __version__\n\n\ndef if_env_else(env_var, else_value):\n    """"""\n    return else_value if env_var is not set in environment variables\n    :return:\n    """"""\n    return else_value if env_var not in os.environ else os.environ[env_var]\n\n\ndef create_directory(path):\n    path = Path(path)\n    path.mkdir(mode=0o777, exist_ok=True, parents=True)\n\n\ndef get_and_create_default_storage_path():\n    this_file_path = os.path.abspath(inspect.getfile(inspect.currentframe()))\n    mindsdb_path = os.path.abspath(Path(this_file_path).parent.parent)\n    version_path = __version__.replace(\'.\', \'_\')\n    path = os.path.abspath(f\'{mindsdb_path}/mindsdb_storage/{version_path}\')\n\n    try:\n        create_directory(path)\n        correct_permissions = os.access(path, os.W_OK)\n    except:\n        print(f\'MindsDB storage directory: {path} does not exist and could not be created, trying another directory\')\n        correct_permissions = False\n\n    if not correct_permissions:\n        home = os.path.expanduser(\'~\')\n        path = os.path.join(home, \'.mindsdb_storage\')\n        try:\n            create_directory(path)\n        except:\n            print(f\'MindsDB storage directory: {path} does not exist and could not be created\')\n\n    return path\n'"
mindsdb/external_libs/__init__.py,0,b''
mindsdb/external_libs/stats.py,0,"b'# Cochran\xe2\x80\x99s sample size calculator\nfrom scipy.stats import norm\n\n\ndef calculate_sample_size(\n    population_size,\n    margin_error=.05,\n    confidence_level=.99,\n    sigma=1/2\n):\n    """"""\n    Calculate the minimal sample size to use to achieve a certain\n    margin of error and confidence level for a sample estimate\n    of the population mean.\n    Inputs\n    -------\n    population_size: integer\n        Total size of the population that the sample is to be drawn from.\n    margin_error: number\n        Maximum expected difference between the true population parameter,\n        such as the mean, and the sample estimate.\n    confidence_level: number in the interval (0, 1)\n        If we were to draw a large number of equal-size samples\n        from the population, the true population parameter\n        should lie within this percentage\n        of the intervals (sample_parameter - e, sample_parameter + e)\n        where e is the margin_error.\n    sigma: number\n        The standard deviation of the population.  For the case\n        of estimating a parameter in the interval [0, 1], sigma=1/2\n        should be sufficient.\n    """"""\n    alpha = 1 - (confidence_level)\n    # dictionary of confidence levels and corresponding z-scores\n    # computed via norm.ppf(1 - (alpha/2)), where norm is\n    # a normal distribution object in scipy.stats.\n    # Here, ppf is the percentile point function.\n    zdict = {\n        .90: 1.645,\n        .91: 1.695,\n        .99: 2.576,\n        .97: 2.17,\n        .94: 1.881,\n        .93: 1.812,\n        .95: 1.96,\n        .98: 2.326,\n        .96: 2.054,\n        .92: 1.751\n    }\n    if confidence_level in zdict:\n        z = zdict[confidence_level]\n    else:\n        #Inf fix\n        if alpha == 0.0:\n            alpha += 0.001\n        z = norm.ppf(1 - (alpha/2))\n    N = population_size\n    M = margin_error\n    numerator = z**2 * sigma**2 * (N / (N-1))\n    denom = M**2 + ((z**2 * sigma**2)/(N-1))\n    return numerator/denom\n'"
mindsdb/libs/__init__.py,0,"b'try:\n    import pkg_resources\n    pkg_resources.declare_namespace(__name__)\nexcept ImportError:\n    import pkgutil\n    __path__ = pkgutil.extend_path(__path__, __name__)\n'"
tests/accuracy_benchmarking/benchmark.py,0,"b'import json\nimport sys\nimport uuid\nimport os\nimport shutil\nimport importlib\nimport datetime\n\nimport requests\nimport mindsdb\nimport lightwood\ntry:\n    import ludwig\nexcept:\n    pass\n\nfrom helpers import *\n\n\ndef run_benchmarks():\n    logger = setup_logger()\n    con, cur, cfg = get_mysql(sys.argv[1])\n\n    cur.execute(""""""CREATE DATABASE IF NOT EXISTS mindsdb_accuracy"""""")\n    cur.execute(""""""CREATE TABLE IF NOT EXISTS mindsdb_accuracy.tests (\n        batch_id                Text\n        ,batch_started          Datetime\n        ,test_name              Text\n        ,dataset_name           Text\n        ,accuracy               Float\n        ,accuracy_function      Text\n        ,accuracy_description   Text\n        ,runtime                BIGINT\n        ,started                Datetime\n        ,ended                  Datetime\n        ,mindsdb_version        Text\n        ,lightwood_version      Text\n        ,ludwig_version         Text\n        ,backend                Text\n        ,label                  Text\n    ) ENGINE=InnoDB"""""")\n\n    batch_id = uuid.uuid4().hex\n    batch_started = datetime.datetime.now()\n    try:\n        shutil.rmtree(\'tmp_downloads\')\n    except:\n        pass\n\n    TESTS = [\'default_of_credit\', \'cancer50\', \'pulsar_stars\', \'cifar_100\', \'imdb_movie_review\', \'german_credit_data\', \'wine_quality\']\n    #TESTS = [\'cancer50\', \'pulsar_stars\']\n\n    status = os.system(\'cp -r ~/mindsdb_examples tmp_downloads\')\n\n    if str(status) != \'0\':\n        os.system(\'git clone https://github.com/mindsdb/mindsdb-examples tmp_downloads\')\n\n    test_data_arr = []\n    for test_name in TESTS:\n        \'\'\'\n            @TODO: (async)\n            Launch ec2 GPU machine\n            Rsync this script onto it\n            Execute the script there\n            Download accuracy data and add it to the test_data_arr\n            (await) Shut down machine\n        \'\'\'\n        logger.debug(f\'\\n\\n=================================\\nRunning test: {test_name}\\n=================================\\n\\n\')\n\n        os.chdir(f\'tmp_downloads/benchmarks/{test_name}\')\n        run_test = importlib.import_module(f\'tmp_downloads.benchmarks.{test_name}.mindsdb_acc\').run\n\n        started = datetime.datetime.now()\n        accuracy_data = run_test(False)\n        ended = datetime.datetime.now()\n\n        os.chdir(\'../../..\')\n\n        accuracy = accuracy_data[\'accuracy\']\n        accuracy_function = accuracy_data[\'accuracy_function\'] if \'accuracy_function\' in accuracy_data else \'accuracy_score\'\n        accuracy_description = accuracy_data[\'accuracy_description\'] if \'accuracy_description\' in accuracy_data else \'\'\n        backend_used = accuracy_data[\'backend\'] if \'backend\' in accuracy_data else \'unknown\'\n\n        test_data_arr.append({\n            \'test_name\': test_name\n            ,\'dataset_name\': test_name\n            ,\'accuracy\': accuracy\n            ,\'accuracy_function\': accuracy_function\n            ,\'accuracy_description\': accuracy_description\n            ,\'started\': started\n            ,\'ended\': ended\n            ,\'runtime\': (ended - started).total_seconds()\n            ,\'backend_used\': backend_used\n        })\n\n    try:\n        ludwig_version = ludwig.__version__\n    except:\n        ludwig_version = \'not installed\'\n\n    for test_data in test_data_arr:\n        cur.execute(""""""INSERT INTO mindsdb_accuracy.tests VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)"""""",(\n            batch_id, batch_started, test_data[\'test_name\'], test_data[\'dataset_name\'],test_data[\'accuracy\'],\n            test_data[\'accuracy_function\'], test_data[\'accuracy_description\'],test_data[\'runtime\'], test_data[\'started\'],\n            test_data[\'ended\'], mindsdb.__version__, lightwood.__version__, ludwig_version, test_data[\'backend_used\'],\n            sys.argv[2]\n        ))\n        con.commit()\n\n    shutil.rmtree(\'tmp_downloads\')\n\n    con.commit()\n    con.close()\n\nif __name__ == ""__main__"":\n    run_benchmarks()\n'"
tests/accuracy_benchmarking/compare.py,0,"b'import sys\n\nfrom helpers import *\n\n\ndef compare():\n    logger = setup_logger()\n    con, cur, cfg = get_mysql(sys.argv[1])\n\n    con = MySQLdb.connect(cfg[\'mysql\'][\'host\'], cfg[\'mysql\'][\'user\'], cfg[\'mysql\'][\'password\'], cfg[\'mysql\'][\'database\'])\n    cur = con.cursor()\n\n    if len(sys.argv) > 2:\n        batch_id = sys.argv[2]\n    # Go with the latest batch by default\n    else:\n        cur.execute(\'SELECT batch_id FROM mindsdb_accuracy.tests WHERE batch_started=(SELECT max(batch_started) FROM mindsdb_accuracy.tests)\')\n        batch_id = cur.fetchall()[0][0]\n\n    cur.execute(\'SELECT test_name, accuracy_function, accuracy, runtime, mindsdb_version, lightwood_version, ludwig_version, backend, label FROM mindsdb_accuracy.tests WHERE batch_id=%s\', [batch_id])\n    batch_tests_arr = cur.fetchall()\n\n    for entity in batch_tests_arr:\n        test_name = entity[0]\n        accuracy_function = entity[1]\n        accuracy = entity[2]\n        runtime = entity[3]\n\n        mindsdb_version = entity[4]\n        lightwood_version = entity[5]\n        ludwig_version = entity[6]\n        backend = entity[7]\n        label = entity[8]\n\n        logger.info(f\'\\nRunning checks for test {test_name} in batch {batch_id}, the test took {runtime} seconds and had an accuracy of {accuracy} using {accuracy_function} and training on backend {backend}\\nSystem info when ran: (mindsdb_version: {mindsdb_version}, lightwood_version: {lightwood_version}, ludwig_version: {ludwig_version}, label: {label})\\n\')\n\n        cur.execute(\'SELECT accuracy, batch_id, batch_started, runtime, mindsdb_version, lightwood_version, ludwig_version, backend FROM mindsdb_accuracy.tests WHERE test_name=%s AND accuracy_function=%s AND (accuracy - 0.0000001) > %s\', [test_name,accuracy_function,accuracy])\n        better_tests = cur.fetchall()\n\n        for row in better_tests:\n            old_accuracy = row[0]\n            old_batch_id = row[1]\n            old_batch_started = row[2]\n            old_runtime = row[3]\n\n            old_mindsdb_version = row[4]\n            old_lightwood_version = row[5]\n            old_ludwig_version = row[6]\n            old_backend = row[7]\n\n\n            # There\'s going to be some variation within accuracy, so le\'ts only worry about somewhat sizeable gaps\n            if old_accuracy > accuracy*1.05:\n                worseness_prefix = \'significantly\'\n                func = logger.error\n            elif old_accuracy > accuracy*1.01:\n                worseness_prefix = \'\'\n                func = logger.warning\n            elif old_accuracy > accuracy*1.0001:\n                worseness_prefix = \'possibly\'\n                func = logger.debug\n            else:\n                continue\n\n            func(f\'There was a previous test for {test_name} which yielded {worseness_prefix} better results than the current one. The results were:  \\n * Accuracy: {old_accuracy} \\n * Batch id: {old_batch_id} \\n * Batch started: {old_batch_started} \\n * Runtime: {old_runtime} \\n * Backend: {old_backend} \\n * System info when ran: (mindsdb_version: {old_mindsdb_version}, lightwood_version: {old_lightwood_version}, ludwig_version: {old_ludwig_version})\\n\')\n\n\n    con.commit()\n    con.close()\n\nif __name__ == ""__main__"":\n    compare()\n'"
tests/accuracy_benchmarking/helpers.py,0,"b'import logging\nimport json\n\nfrom colorlog import ColoredFormatter\nimport MySQLdb\n\n\ndef setup_logger():\n    formatter = ColoredFormatter(\n        ""%(log_color)s%(levelname)-8s%(reset)s %(log_color)s%(message)s"",\n        datefmt=None,\n        reset=True,\n        log_colors={\n            \'DEBUG\':    \'cyan\',\n            \'INFO\':     \'green\',\n            \'WARNING\':  \'yellow\',\n            \'ERROR\':    \'red\',\n            \'CRITICAL\': \'red\',\n        }\n    )\n\n    logger = logging.getLogger(\'test-logger\')\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n    return logger\n\n\ndef get_mysql(cfg_file):\n    with open(cfg_file, \'rb\') as fp:\n        cfg = json.load(fp)\n\n    con = MySQLdb.connect(cfg[\'mysql\'][\'host\'], cfg[\'mysql\'][\'user\'], cfg[\'mysql\'][\'password\'], cfg[\'mysql\'][\'database\'])\n    cur = con.cursor()\n    return con, cur, cfg\n'"
tests/accuracy_benchmarking/launch_client.py,0,"b""import os\nimport sys\nimport time\nimport json\nfrom subprocess import Popen, PIPE\n\ninstance_id = 'i-0d6aa6ae788b28ccb'\n\noutput = Popen(['aws', 'ec2', 'start-instances', '--instance-ids' ,instance_id],stdout=PIPE)\nresponse = output.communicate()\n\nprint('Booted up instance, waiting 2 minutes for it to start')\ntime.sleep(120)\n\noutput = Popen(['aws', 'ec2', 'describe-instances', '--instance-ids' ,instance_id],stdout=PIPE)\nresponse = output.communicate()\n\nj = json.loads(response[0])\nstatus = j['Reservations'][0]['Instances'][0]['State']['Name']\nip = j['Reservations'][0]['Instances'][0]['PublicIpAddress']\n\nos.system(f'ssh -t ubuntu@{ip} \\' sudo /var/benchmarks/mindsdb/tests/accuracy_benchmarking/launch_server.sh {sys.argv[1]} {sys.argv[2]} {sys.argv[3]}\\'')\n"""
tests/ci_tests/fast_test.py,1,"b'from tests import basic_test\nimport torch\n\n# Test with a few basic options\nif __name__ == ""__main__"":\n    basic_test(backend=\'lightwood\',use_gpu=torch.cuda.is_available(), IS_CI_TEST=True)\n    print(\'\\n\\n=============[Success]==============\\n     Finished running quick test !\\n=============[Success]==============\\n\\n\')\n'"
tests/ci_tests/full_test.py,1,"b'import torch\nimport mindsdb\nfrom tests import basic_test\n\n\nif __name__ == ""__main__"":\n    use_gpu_settings = [False]\n    if torch.cuda.is_available():\n        use_gpu_settings.append(True)\n\n    # Cycle through a few options:\n    for backend in [\'lightwood\']:\n        for use_gpu in use_gpu_settings:\n            print(f\'use_gpu is set to {use_gpu}, backend is set to {backend}\')\n            basic_test(backend=backend,use_gpu=use_gpu, IS_CI_TEST=True, run_extra=True)\n\n    print(\'\\n\\n=============[Success]==============\\n     Finished running full test suite !\\n=============[Success]==============\\n\\n\')\n'"
tests/ci_tests/tests.py,0,"b'import mindsdb\nimport sys\nimport os\nfrom sklearn.metrics import r2_score\n\ndef test_data_analysis(amd, to_predict):\n    data_analysis = amd[\'data_analysis\']\n    for k in data_analysis:\n        assert (len(data_analysis[k]) > 0)\n        assert isinstance(data_analysis[k][0], dict)\n\ndef test_model_analysis(amd, to_predict):\n    model_analysis = amd[\'model_analysis\']\n    assert (len(model_analysis) > 0)\n    assert isinstance(model_analysis[0], dict)\n    input_importance = model_analysis[0][""overall_input_importance""]\n    assert (len(input_importance) > 0)\n    assert isinstance(input_importance, dict)\n\n    for column, importance in zip(input_importance[""x""], input_importance[""y""]):\n        assert isinstance(column, str)\n        assert (len(column) > 0)\n        assert isinstance(importance, (float, int))\n        assert (importance >= 0 and importance <= 10)\n\ndef test_force_vectors(amd, to_predict):\n    force_vectors = amd[\'force_vectors\']\n    for k in force_vectors[to_predict][\'normal_data_distribution\']:\n        assert (len(force_vectors[to_predict][\'normal_data_distribution\'][k]) > 0)\n\n    for k in force_vectors[to_predict][\'missing_data_distribution\']:\n        for sk in force_vectors[to_predict][\'missing_data_distribution\'][k]:\n            assert (len(force_vectors[to_predict][\'missing_data_distribution\'][k][sk]) > 0)\n\ndef test_adapted_model_data(amd, to_predict):\n    amd = amd\n    for k in [\'status\', \'name\', \'version\', \'data_source\', \'current_phase\', \'updated_at\', \'created_at\',\n              \'train_end_at\']:\n        assert isinstance(amd[k], str)\n\n    assert isinstance(amd[\'predict\'], (list, str))\n    assert isinstance(amd[\'is_active\'], bool)\n\n    for k in [\'validation_set_accuracy\', \'accuracy\']:\n        assert isinstance(amd[k], float)\n\n    for k in amd[\'data_preparation\']:\n        assert isinstance(amd[\'data_preparation\'][k], (int, float))\n    test_data_analysis(amd, to_predict)\n    test_model_analysis(amd, to_predict)\n    #test_force_vectors(amd, to_predict)\n\n\ndef basic_test(backend=\'lightwood\',use_gpu=True, run_extra=False, IS_CI_TEST=False):\n    mindsdb.CONFIG.IS_CI_TEST = IS_CI_TEST\n    if run_extra:\n        for py_file in [x for x in os.listdir(\'../functional_testing\') if \'.py\' in x]:\n            # Skip data source tests since installing dependencies is annoying\n            # @TODO: Figure out a way to make travis install required dependencies on osx\n\n            if any(x in py_file for x in [\'all_data_sources\', \'custom_model\']):\n                continue\n\n            code = os.system(f\'python3 ../functional_testing/{py_file}\')\n            if code != 0:\n                raise Exception(f\'Test failed with status code: {code} !\')\n\n    # Create & Learn\n    to_predict = \'rental_price\'\n    mdb = mindsdb.Predictor(name=\'home_rentals_price\')\n    mdb.learn(to_predict=to_predict,from_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"",backend=backend, stop_training_in_x_seconds=120,use_gpu=use_gpu)\n\n    # Reload & Predict\n    model_name = \'home_rentals_price\'\n    if run_extra:\n        mdb.rename_model(\'home_rentals_price\', \'home_rentals_price_renamed\')\n        model_name = \'home_rentals_price_renamed\'\n\n    mdb = mindsdb.Predictor(name=model_name)\n    # Try predicting from a file and from a dictionary\n    prediction = mdb.predict(when_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"", use_gpu=use_gpu)\n\n    mdb.test(when_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"",accuracy_score_functions=r2_score,predict_args={\'use_gpu\': use_gpu})\n\n    prediction = mdb.predict(when={\'sqft\':300}, use_gpu=use_gpu)\n\n    # Test all different forms of output\n    # No need to print them in order to run these checks, we\'re just doing so for quick-debugging purposes, we just want to see if the interfaces will crash when we call them\n    print(prediction)\n    print(prediction[0])\n\n    for item in prediction:\n        print(item)\n\n\n    for p in prediction:\n        print(p)\n    print(prediction[0].as_dict())\n    print(prediction[0].as_list())\n    print(prediction[0][\'rental_price_confidence\'])\n    print(type(prediction[0][\'rental_price_confidence\']))\n\n    print(\'\\n\\n========================\\n\\n\')\n    print(prediction[0].explain())\n    print(prediction[0].explanation)\n    print(prediction[0].raw_predictions())\n    print(\'\\n\\n\')\n\n    # See if we can get the adapted model data\n    amd = mdb.get_model_data(model_name)\n    test_adapted_model_data(amd, to_predict)\n\nif __name__ == ""__main__"":\n    basic_test()\n'"
tests/functional_testing/analyse_dataset.py,0,"b'from mindsdb import Predictor\n\n\nmdb = Predictor(name=\'analyse_dataset_test_predictor\')\nresults = mdb.analyse_dataset(from_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"")\nprint(\'\\n\\n\\n\\n========================\\n\\n\')\nprint(results)\n'"
tests/functional_testing/custom_model.py,0,"b'from mindsdb import Predictor\nimport pandas as pd\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LinearRegression\n\nclass CustomDTModel():\n    def __init__(self):\n        self.clf = LinearRegression()\n        le = preprocessing.LabelEncoder()\n\n    def set_transaction(self, transaction):\n        self.transaction = transaction\n        self.output_columns = self.transaction.lmd[\'predict_columns\']\n        self.input_columns = [x for x in self.transaction.lmd[\'columns\'] if x not in self.output_columns]\n        self.train_df = self.transaction.input_data.train_df\n        self.test_dt = train_df = self.transaction.input_data.test_df\n\n\n    def train(self):\n        self.le_arr = {}\n        for col in [*self.output_columns, *self.input_columns]:\n            self.le_arr[col] = preprocessing.LabelEncoder()\n            self.le_arr[col].fit(pd.concat([self.transaction.input_data.train_df,self.transaction.input_data.test_df,self.transaction.input_data.validation_df])[col])\n\n        X = []\n        for col in self.input_columns:\n            X.append(self.le_arr[col].transform(self.transaction.input_data.train_df[col]))\n\n        X = np.swapaxes(X,1,0)\n\n        # Only works with one output column\n        Y = self.le_arr[self.output_columns[0]].transform(self.transaction.input_data.train_df[self.output_columns[0]])\n\n        self.clf.fit(X, Y)\n\n    def predict(self, mode=\'predict\', ignore_columns=[]):\n        if mode == \'predict\':\n            df = self.transaction.input_data.data_frame\n        if mode == \'validate\':\n            df = self.transaction.input_data.validation_df\n        elif mode == \'test\':\n            df = self.transaction.input_data.test_df\n\n        X = []\n        for col in self.input_columns:\n            X.append(self.le_arr[col].transform(df[col]))\n\n        X = np.swapaxes(X,1,0)\n\n        predictions = self.clf.predict(X)\n\n        formated_predictions = {self.output_columns[0]: predictions}\n\n        return formated_predictions\n\n\npredictor = Predictor(name=\'custom_model_test_predictor\')\n\ndt_model = CustomDTModel()\n\npredictor.learn(to_predict=\'rental_price\',from_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"", backend=dt_model)\npredictions = predictor.predict(when_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"", backend=dt_model)\n'"
tests/functional_testing/data_source_setting.py,0,"b""from mindsdb.libs.data_sources.file_ds import FileDS\nfrom mindsdb.libs.constants.mindsdb import DATA_TYPES, DATA_SUBTYPES\n\n\ndata_source = FileDS('https://raw.githubusercontent.com/mindsdb/mindsdb-examples/master/benchmarks/german_credit_data/processed_data/test.csv')\ndata_source.set_subtypes({})\n\ndata_source_mod = FileDS('https://raw.githubusercontent.com/mindsdb/mindsdb-examples/master/benchmarks/german_credit_data/processed_data/test.csv')\ndata_source_mod.set_subtypes({'credit_usage': 'Int', 'Average_Credit_Balance': 'Text','existing_credits': 'Binary Category'})\n\nimport mindsdb\nanalysis = mindsdb.Predictor('analyzer1').analyse_dataset(data_source)\nanalysis_mod = mindsdb.Predictor('analyzer2').analyse_dataset(data_source_mod)\n\na1 = analysis['data_analysis_v2']\na2 = analysis_mod['data_analysis_v2']\nassert(len(a1) == len(a2))\nassert(a1['over_draft']['typing']['data_type'] == a2['over_draft']['typing']['data_type'])\n\nassert(a1['credit_usage']['typing']['data_type'] == a2['credit_usage']['typing']['data_type'])\nassert(a1['credit_usage']['typing']['data_subtype'] != a2['credit_usage']['typing']['data_subtype'])\nassert(a2['credit_usage']['typing']['data_subtype'] == DATA_SUBTYPES.INT)\n\nassert(a1['Average_Credit_Balance']['typing']['data_type'] != a2['Average_Credit_Balance']['typing']['data_type'])\nassert(a1['Average_Credit_Balance']['typing']['data_subtype'] != a2['Average_Credit_Balance']['typing']['data_subtype'])\nassert(a2['Average_Credit_Balance']['typing']['data_subtype'] == DATA_SUBTYPES.TEXT)\nassert(a2['Average_Credit_Balance']['typing']['data_type'] == DATA_TYPES.SEQUENTIAL)\n\nassert(a1['existing_credits']['typing']['data_type'] == a2['existing_credits']['typing']['data_type'])\nassert(a1['existing_credits']['typing']['data_subtype'] != a2['existing_credits']['typing']['data_subtype'])\nassert(a2['existing_credits']['typing']['data_subtype'] == DATA_SUBTYPES.SINGLE)\n"""
tests/integration_tests/data_generators.py,0,"b'import random\nimport string\nimport datetime\nfrom math import log\n\n\ndef generate_timeseries(length, bounds=(0,1852255420), _type=\'timestamp\',period=24*3600, swing=0, separator=\',\'):\n    column = []\n\n    for n in range(*bounds,period):\n        if len(column) >= length:\n            break\n        column.append(n)\n\n    if _type == \'timestamp\':\n        return column\n    elif _type == \'datetime\':\n        return list(map(str, map(datetime.datetime.fromtimestamp ,column)))\n    elif _type == \'date\':\n        return list(map(str, map(lambda x: datetime.datetime.fromtimestamp(x).date() ,column)))\n\n\ndef rand_str(length=random.randrange(4,120)):\n    # Create a list of unicode characters within the range 0000-D7FF\n    random_unicodes = [chr(random.randrange(0xD7FF)) for _ in range(0, length)]\n    return u"""".join(random_unicodes)\n\n\ndef rand_ascii_str(length=None, give_nulls=True, only_letters=False):\n    if only_letters:\n        charlist = [*string.ascii_letters]\n    else:\n        #other = [\' \', \'_\', \'-\', \'?\', \'.\', \'<\', \'>\', \')\', \'(\']\n        other = []\n        charlist = [*other, *string.ascii_letters]\n    if length == None:\n        length = random.randrange(1,120)\n    if length % 4 == 0 and give_nulls==True:\n        return \'\'\n    #Sometimes we should return a number instead of a string\n    #if length % 7 == 0:\n    #    return str(length)\n    return \'\'.join(random.choice(charlist) for _ in range(length))\n\n\ndef rand_int():\n    return int(random.randrange(-pow(2,18), pow(2,18)))\n\ndef rand_numerical_cat():\n    return int(random.randrange(-pow(2,3), pow(2,3)))\n\n\ndef rand_float():\n    return random.randrange(-pow(2,18), pow(2,18)) * random.random()\n\n\ndef generate_value_cols(types, length, separator=\',\', ts_period=48*3600):\n    columns = []\n    for t in types:\n        columns.append([])\n        # This is a header of sorts\n        columns[-1].append(rand_ascii_str(random.randrange(8,10),give_nulls=False,only_letters=True))\n\n        # Figure out which random generation function to use for this column\n        if t == \'str\':\n            gen_fun = rand_str\n        elif t == \'ascii\':\n            gen_fun = rand_ascii_str\n        elif t == \'int\':\n            gen_fun = rand_int\n        elif t == \'nr_category\':\n            gen_fun = rand_numerical_cat\n        elif t == \'float\':\n            gen_fun = rand_float\n        else:\n            columns[-1].extend(generate_timeseries(length=length,_type=t,period=ts_period, separator=separator))\n            continue\n\n        for n in range(length):\n            val = gen_fun()\n            # @TODO: Maybe escpae the separator rather than replace them\n            if isinstance(val, str):\n                val = val.replace(separator,\'_\').replace(\'\\n\',\'_\').replace(\'\\r\',\'_\')\n            columns[-1].append(val)\n\n    return columns\n\n\n# Ignore all but flaots and ints\n# Adds up the log of all floats and ints\ndef generate_labels_1(columns, separator=\',\'):\n    labels = []\n    # This is a header of sorts\n    labels.append(rand_ascii_str(random.randrange(14,28),give_nulls=False,only_letters=True))\n\n    for n in range(1, len(columns[-1])):\n        value = 0\n        for i in range(len(columns)):\n            try:\n                value += log(abs(columns[i][n]))\n            except:\n                pass\n        labels.append(value)\n\n    return labels\n\n\ndef generate_labels_2(columns, separator=\',\'):\n    labels = []\n    # This is a header of sorts\n    labels.append(rand_ascii_str(random.randrange(5,11),give_nulls=False,only_letters=True))\n\n    for n in range(1, len(columns[-1])):\n        value = 1\n        for i in range(len(columns)):\n            if isinstance(columns[i][n], str):\n                operand = len(columns[i][n])\n            else:\n                operand = columns[i][n]\n\n            if i % 2 == 0:\n                value = value * operand\n            else:\n                try:\n                    value = value / operand\n                except:\n                    value = 1\n\n        labels.append(value)\n\n    return labels\n\n\ndef generate_labels_3(columns, separator=\',\'):\n    labels = []\n    # This is a header of sorts\n    labels.append(rand_ascii_str(random.randrange(14,18),give_nulls=False,only_letters=True))\n\n    col_nr = random.randrange(0,len(columns))\n    labels.extend(columns[col_nr][1:])\n\n    return labels\n\n\ndef columns_to_file(columns, filename, separator=\',\', headers=None):\n    with open(filename, \'w\', encoding=\'utf-8\') as fp:\n        fp.write(\'\')\n\n    with open(filename, \'a\', encoding=\'utf-8\') as fp:\n        if headers is not None:\n            fp.write(separator.join(headers) + \'\\r\\n\')\n        for i in range(len(columns[-1])):\n            row = \'\'\n            for col in columns:\n                row += str(col[i]) + separator\n\n            fp.write(row.rstrip(separator) + \'\\r\\n\')\n'"
tests/integration_tests/generated_data_tests.py,0,"b'from data_generators import *\nimport traceback\nimport sys\nimport os\nimport itertools\nimport logging\nfrom colorlog import ColoredFormatter\nimport time\n\nimport mindsdb\nfrom mindsdb import CONST\n\n\ntypes_that_fail = [\'str\']\ntypes_that_work = [\'int\',\'float\',\'date\',\'datetime\',\'timestamp\',\'ascii\']\n\nlogger = None\n\ndef setup_testing_logger():\n    global logger\n    formatter = ColoredFormatter(\n        ""%(log_color)s%(message)s"",\n        datefmt=None,\n        reset=True,\n        log_colors={\n            \'DEBUG\':    \'black,bg_white\',\n            \'INFO\':     \'blue,bg_white\',\n            \'WARNING\':  \'orange,bg_white\',\n            \'ERROR\':    \'red,bg_white\',\n            \'CRITICAL\': \'red,bg_white\',\n        }\n    )\n\n    logger = logging.getLogger(\'mindsdb_integration_testing\')\n    logger.handlers = []\n    handler = logging.StreamHandler()\n    handler.setFormatter(formatter)\n    logger.addHandler(handler)\n    logger.setLevel(logging.DEBUG)\n\ndef test_timeseries(backend=\'lightwood\'):\n    logger.info(\'Starting timeseries test !\')\n    ts_hours = 12\n    separator = \',\'\n    data_len = 1200\n    train_file_name = \'train_data.csv\'\n    test_file_name = \'test_data.csv\'\n\n    # Create the full dataset\n    logger.debug(f\'Creating timeseries test datasets and saving them to {train_file_name} and {test_file_name}, total dataset size will be {data_len} rows\')\n\n    try:\n        # add ,\'ascii\' in the features list to re-implement the group by\n        features = generate_value_cols([\'date\',\'int\'],data_len, separator, ts_hours * 3600)\n        #features[3] = list(map(lambda x: str(x[0]) if len(x) > 0 else \'Nrmm\', features[3]))\n        labels = [generate_labels_2(features, separator)]\n\n        feature_headers = list(map(lambda col: col[0], features))\n        label_headers = list(map(lambda col: col[0], labels))\n\n        # Create the training dataset and save it to a file\n        columns_train = list(map(lambda col: col[1:int(len(col)*3/4)], features))\n        columns_train.extend(list(map(lambda col: col[1:int(len(col)*3/4)], labels)))\n        columns_to_file(columns_train, train_file_name, separator, headers=[*feature_headers,*label_headers])\n\n        # Create the testing dataset and save it to a file\n        columns_test = list(map(lambda col: col[int(len(col)*3/4):], features))\n        columns_to_file(columns_test, test_file_name, separator, headers=feature_headers)\n        logger.debug(f\'Datasets generate and saved to files successfully\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to generate datasets !\')\n        exit(1)\n\n    # Train\n\n    mdb = None\n    try:\n        mdb = mindsdb.Predictor(name=\'test_date_timeseries_2\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n\n    try:\n        mdb.learn(\n            from_data=train_file_name,\n            to_predict=label_headers\n            # timeseries specific argsw\n            ,order_by=feature_headers[0]\n            #,window_size_seconds=ts_hours* 3600 * 1.5\n            ,window_size=3\n            #,group_by = feature_headers[3]\n            ,use_gpu=False\n            ,backend=backend\n        )\n        logger.info(f\'--------------- Learning ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed during the training !\')\n        exit(1)\n\n    # Predict\n    try:\n        mdb = mindsdb.Predictor(name=\'test_date_timeseries_2\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n    try:\n        results = mdb.predict(when_data=test_file_name,use_gpu=False)\n\n        for row in results:\n            expect_columns = [label_headers[0] ,label_headers[0] + \'_confidence\']\n            for col in expect_columns:\n                if col not in row:\n                    logger.error(f\'Prediction failed to return expected column: {col}\')\n                    logger.debug(\'Got row: {}\'.format(row))\n                    exit(1)\n\n        models = mdb.get_models()\n        print(models)\n        mdb.get_model_data(models[0][\'name\'])\n\n        logger.info(f\'--------------- Predicting ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed whilst predicting\')\n        exit(1)\n\n    logger.info(\'Timeseries test ran succesfully !\')\n\n\ndef test_one_label_prediction(backend=\'lightwood\'):\n    logger.info(\'Starting one-label test\')\n    separator = \',\'\n    train_file_name = \'train_data.csv\'\n    test_file_name = \'test_data.csv\'\n    data_len = 8000\n\n    # Create the full dataset\n    logger.debug(f\'Creating one-labe test datasets and saving them to {train_file_name} and {test_file_name}, total dataset size will be {data_len} rows\')\n\n    try:\n        features = generate_value_cols([\'int\',\'float\',\'nr_category\'],data_len, separator)\n        labels = [generate_labels_2(features, separator)]\n\n        feature_headers = list(map(lambda col: col[0], features))\n        label_headers = list(map(lambda col: col[0], labels))\n\n        # Create the training dataset and save it to a file\n        columns_train = list(map(lambda col: col[1:int(len(col)*3/4)], features))\n        columns_train.extend(list(map(lambda col: col[1:int(len(col)*3/4)], labels)))\n        columns_to_file(columns_train, train_file_name, separator, headers=[*feature_headers,*label_headers])\n\n        # Create the testing dataset and save it to a file\n        columns_test = list(map(lambda col: col[int(len(col)*3/4):], features))\n        columns_to_file(columns_test, test_file_name, separator, headers=feature_headers)\n        logger.debug(f\'Datasets generate and saved to files successfully\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to generate datasets !\')\n        exit(1)\n\n    # Train\n    mdb = None\n    try:\n        mdb = mindsdb.Predictor(name=\'test_one_label_prediction\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n\n    try:\n        mdb.learn(from_data=train_file_name, to_predict=label_headers, backend=backend)\n        logger.info(f\'--------------- Learning ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed during the training !\')\n        exit(1)\n\n    # Predict\n    try:\n        mdb = mindsdb.Predictor(name=\'test_one_label_prediction\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n    try:\n        results = mdb.predict(when_data=test_file_name)\n        models = mdb.get_models()\n        mdb.get_model_data(models[0][\'name\'])\n        for row in results:\n            expect_columns = [label_headers[0] ,label_headers[0] + \'_confidence\']\n            for col in expect_columns:\n                if col not in row:\n                    logger.error(f\'Prediction failed to return expected column: {col}\')\n                    logger.debug(\'Got row: {}\'.format(row))\n                    exit(1)\n\n        logger.info(f\'--------------- Predicting ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed whilst predicting\')\n        exit(1)\n\n    logger.info(\'One-label prediction test ran succesfully !\')\n\ndef test_one_label_prediction_wo_strings(backend=\'lightwood\'):\n    logger.info(\'Starting one-label test\')\n    separator = \',\'\n    train_file_name = \'train_data.csv\'\n    test_file_name = \'test_data.csv\'\n    data_len = 8000\n\n    # Create the full dataset\n    logger.debug(f\'Creating one-labe test datasets and saving them to {train_file_name} and {test_file_name}, total dataset size will be {data_len} rows\')\n\n    try:\n        features = generate_value_cols([\'int\',\'float\',\'datetime\',\'date\',\'int\'],data_len, separator)\n        labels = [generate_labels_2(features, separator)]\n\n        feature_headers = list(map(lambda col: col[0], features))\n        label_headers = list(map(lambda col: col[0], labels))\n\n        # Create the training dataset and save it to a file\n        columns_train = list(map(lambda col: col[1:int(len(col)*3/4)], features))\n        columns_train.extend(list(map(lambda col: col[1:int(len(col)*3/4)], labels)))\n        columns_to_file(columns_train, train_file_name, separator, headers=[*feature_headers,*label_headers])\n\n        # Create the testing dataset and save it to a file\n        columns_test = list(map(lambda col: col[int(len(col)*3/4):], features))\n        columns_to_file(columns_test, test_file_name, separator, headers=feature_headers)\n        logger.debug(f\'Datasets generate and saved to files successfully\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to generate datasets !\')\n        exit(1)\n\n    # Train\n    mdb = None\n    try:\n        mdb = mindsdb.Predictor(name=\'test_one_label_prediction_wo_strings\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n\n    try:\n        mdb.learn(from_data=train_file_name, to_predict=label_headers, backend=backend)\n        logger.info(f\'--------------- Learning ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed during the training !\')\n        exit(1)\n\n    # Predict\n    try:\n        mdb = mindsdb.Predictor(name=\'test_one_label_prediction_wo_strings\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n    try:\n        results = mdb.predict(when_data=test_file_name)\n        models = mdb.get_models()\n        mdb.get_model_data(models[0][\'name\'])\n        for row in results:\n            expect_columns = [label_headers[0] ,label_headers[0] + \'_confidence\']\n            for col in expect_columns:\n                if col not in row:\n                    logger.error(f\'Prediction failed to return expected column: {col}\')\n                    logger.debug(\'Got row: {}\'.format(row))\n                    exit(1)\n\n        logger.info(f\'--------------- Predicting ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed whilst predicting\')\n        exit(1)\n\n    logger.info(\'One-label prediction test ran succesfully !\')\n\ndef test_multilabel_prediction(backend=\'lightwood\'):\n    logger.info(\'Starting multilabel prediction test\')\n    separator = \',\'\n    train_file_name = \'train_data.csv\'\n    test_file_name = \'test_data.csv\'\n    data_len = 600\n\n    # Create the full dataset\n    logger.debug(f\'Creating multilabel test datasets and saving them to {train_file_name} and {test_file_name}, total dataset size will be {data_len} rows\')\n\n    try:\n        features = generate_value_cols([\'int\',\'float\',\'int\',\'float\'], data_len, separator)\n        labels = []\n        labels.append(generate_labels_3(features, separator))\n        labels.append(generate_labels_2(features, separator))\n        labels.append(generate_labels_1(features, separator))\n\n        feature_headers = list(map(lambda col: col[0], features))\n        label_headers = list(map(lambda col: col[0], labels))\n\n        # Create the training dataset and save it to a file\n        columns_train = list(map(lambda col: col[1:int(len(col)*3/4)], features))\n        columns_train.extend(list(map(lambda col: col[1:int(len(col)*3/4)], labels)))\n        columns_to_file(columns_train, train_file_name, separator, headers=[*feature_headers,*label_headers])\n\n        # Create the testing dataset and save it to a file\n        columns_test = list(map(lambda col: col[int(len(col)*3/4):], features))\n        columns_to_file(columns_test, test_file_name, separator, headers=feature_headers)\n        logger.debug(f\'Multilabel datasets generate and saved to files successfully\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to generate datasets !\')\n        exit(1)\n\n    # Train\n    mdb = None\n    try:\n        mdb = mindsdb.Predictor(name=\'test_multilabel_prediction\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n\n    try:\n        mdb.learn(from_data=train_file_name, to_predict=label_headers, backend=backend)\n        logger.info(f\'--------------- Learning ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed during the training !\')\n        exit(1)\n\n    # Predict\n    try:\n        mdb = mindsdb.Predictor(name=\'test_multilabel_prediction\')\n        logger.debug(f\'Succesfully create mindsdb Predictor\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed to create mindsdb Predictor\')\n        exit(1)\n\n    try:\n        results = mdb.predict(when_data=test_file_name)\n        models = mdb.get_models()\n        mdb.get_model_data(models[0][\'name\'])\n        for i in range(len(results)):\n            row = results[i]\n            expect_columns = [label_headers[0] ,label_headers[0] + \'_confidence\']\n            for col in expect_columns:\n                print(row[col])\n                if col not in row:\n                    logger.error(f\'Prediction failed to return expected column: {col}\')\n                    logger.debug(\'Got row: {}\'.format(row))\n                    exit(1)\n\n        logger.info(f\'--------------- Predicting ran succesfully ---------------\')\n    except:\n        print(traceback.format_exc())\n        logger.error(f\'Failed whilst predicting\')\n        exit(1)\n\n    logger.info(\'Multilabel predict test ran succesfully !\')\n\n    separator = \',\'\n    data_file_name = \'test_data.csv\'\n    data_len = 10000\n\nsetup_testing_logger()\n\nif __name__ == \'__main__\':\n    test_timeseries()\n    test_one_label_prediction_wo_strings()\n    test_multilabel_prediction()\n    test_one_label_prediction()\n'"
tests/integration_tests/run_example.py,0,"b""import requests\nimport sys\nimport os\nimport shutil\nimport tarfile\nimport atexit\n\n\ndef cleanup(name):\n    shutil.rmtree(f'{name}',ignore_errors=True)\n    try:\n        os.remove(f'{name}.tar.gz')\n    except:\n        pass\n\n    shutil.rmtree(os.path.join('..', f'{name}'),ignore_errors=True)\n\n    try:\n        os.remove(os.path.join('..', f'{name}.tar.gz'))\n    except:\n        pass\n\n\ndef run_example(example_name, sample=False):\n    atexit.register(cleanup,name=example_name)\n\n    with open(f'{example_name}.tar.gz', 'wb') as f:\n        r = requests.get(f'https://mindsdb-example-data.s3.eu-west-2.amazonaws.com/{example_name}.tar.gz')\n        f.write(r.content)\n\n    try:\n        tar = tarfile.open(f'{example_name}.tar.gz', 'r:gz')\n    except:\n        tar = tarfile.open(f'{example_name}.tar.gz', 'r')\n\n    tar.extractall()\n    tar.close()\n\n    os.chdir(example_name)\n    module = __import__(f'{example_name}.mindsdb_acc', fromlist=['run'])\n    run_func = getattr(module,'run')\n    try:\n        res = run_func(sample)\n        os.chdir('..')\n    except:\n        os.chdir('..')\n        sys.exit()\n\n    return res\n\n\nif __name__ == '__main__':\n    run_example(sys.argv[1])\n"""
tests/integration_tests/test_suite.py,0,"b""from run_example import run_example\nfrom generated_data_tests import *\nimport multiprocessing\nimport os\n\n\n# Run the generated data tests\nfor backend in ['ludwig', 'lightwood']:\n    test_one_label_prediction_wo_strings(backend)\n    test_timeseries(backend)\n    test_multilabel_prediction(backend)\n    test_one_label_prediction(backend)\n\n# Run the CI tests\nos.system('cd ..; cd ci_tests; python3 full_test.py')\n\n# Run the example datassts\ndatasets = [{\n    'name':'default_of_credit',\n    'sample':True,\n    'expect_accuracy_above':72\n},{\n    'name':'imdb_movie_review',\n    'sample':False,\n    'expect_accuracy_above':83\n},{\n    'name':'cifar_100',\n    'sample':True,\n    'expect_accuracy_above': 40 # For full dataset: 69\n}]\n\n\nfor dataset in datasets:\n    dataset_name = dataset['name']\n\n    res = run_example(dataset_name, sample=dataset['sample'])\n\n    acc = res['accuracy']\n    ex_acc = dataset['expect_accuracy_above']\n\n    if acc < ex_acc:\n        print('\\n\\n\\n============WARNING===============\\n\\n\\n')\n        print(f'Expected an accuracy above {ex_acc} for dataset {dataset_name}.')\n        print(f'Got accuracy of {acc} instead.')\n        print('\\n\\n\\n==================================\\n\\n\\n')\n    else:\n        print('\\n\\n\\n============SUCCESS===============\\n\\n\\n')\n        print(f'Example dataset {dataset_name}, ran with success')\n        print(f'Got accuracy of {acc} !')\n        print('\\n\\n\\n==================================\\n\\n\\n')\n\n#with multiprocessing.Pool(max(len(datasets),6)) as pool:\n#    pool.map(run_example,datasets)\n"""
docs/examples/basic/predict.py,0,"b'""""""\n\nThis example we will walk you over the basics of MindsDB\n\nThe example code objective here is to predict the best retail price for a given property.\n\n""""""\n\nfrom mindsdb import Predictor\n\n# use the model to make predictions\nresult = Predictor(name=\'home_rentals_price\').predict(when={\'number_of_rooms\': 2,\'number_of_bathrooms\':1, \'sqft\': 1190})\n\n# you can now print the results\nprint(\'The predicted price is ${price} with {conf} confidence\'.format(price=result[0][\'rental_price\'], conf=result[0][\'rental_price_confidence\']))\n'"
docs/examples/basic/test.py,0,"b'from mindsdb import *\n\nmdb = Predictor(name=\'home_rentals\')\n\nmdb.learn(\n    from_data=""home_rentals.csv"",\n    # the path to the file where we can learn from, (note: can be url)\n    to_predict=\'rental_price\',  # the column we want to learn to predict given all the data in the file\n)\n\n#use the model to make predictions\nresult = mdb.predict(\n    when={""number_of_rooms"": 2, ""sqft"": 1100, \'location\': \'great\', \'days_on_market\': 10, ""number_of_bathrooms"": 1})\n\nprint(result[0][\'rental_price\'])\nprint(result[0])\n#3306 (5%)\n#3837 (37%)\n#3836 (26%)\n#3559 (12%)\n#3559 (4%)\n#3837 (14%)\n#3306 (4%)\n\nwhen = {""sqft"": 700}\nresult = mdb.predict(\n    when=when)\nprint(result[0][\'rental_price\'])\n#2205\n#828\n#3306\n#3076\n#2677\n'"
docs/examples/basic/train.py,0,"b'""""""\n\nThis example we will walk you over the basics of MindsDB\n\nThe example code objective here is to:\n\n- learn a model to predict the best retal price for a given property.\n\nIn order to to this we have a dataset ""data_sources/home_rentals.csv"" (or download from https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv)\n\n""""""\n\nfrom mindsdb import Predictor\n\n\n# We tell mindsDB what we want to learn and from what data\nPredictor(name=\'home_rentals_price\').learn(\n    to_predict=\'days_on_market\', # the column we want to learn to predict given all the data in the file\n    from_data=""home_rentals.csv"" # the path to the file where we can learn from, (note: can be url)\n)\n'"
docs/examples/nlp/__init__.py,0,b'\n'
docs/examples/nlp/predict.py,0,"b'from mindsdb import *\n\nmdb = Predictor(name=\'real_estate_desc\')\n\n# Here we use the model to make predictions (NOTE: You need to run train.py first)\nresult = mdb.predict(\n    when={\n        ""description"": """"""A true gem\n rooms: 2\n  bathrooms: 0\n  neighboorhood: thowsand_oaks\n   amenities: parking\n  area: 84.0291068642868\n  condition: great !\n        """"""\n    }\n)\n\n# you can now print the results\nprint(\'The predicted number of rooms\')\nprint(result)\n'"
docs/examples/nlp/train.py,0,"b'from mindsdb import *\n\nmdb = Predictor(name=\'real_estate_desc\')\n\n# We tell mindsDB what we want to learn and from what data\nmdb.learn(\n    from_data=""real_estate_description.xlsx"",\n    to_predict=\'\xe8\xa1\x8c\xe9\xa1\xb9\xe7\x9b\xae\xe6\x95\xb0\'\n)\n'"
docs/examples/time_series/__init__.py,0,b''
docs/examples/time_series/predict.py,0,"b'""""""\n\n""""""\n\nfrom mindsdb import Predictor\n\n# Here we use the model to make predictions (NOTE: You need to run train.py first)\nresult = Predictor(name=\'fuel\').predict(when_data = \'fuel_predict.csv\')\n\n# you can now print the results\nprint(\'The predicted main engine fuel consumption\')\nfor row in result:\n  print(row)\n'"
docs/examples/time_series/train.py,0,"b""from mindsdb import Predictor\n\n\nPredictor(name='fuel').learn(\n    to_predict='Main_Engine_Fuel_Consumption_MT_day',\n    from_data = 'fuel.csv',\n\n    # Time series arguments:\n\n    order_by='Time',\n    group_by='id',\n    window_size=24, # just 24 hours\n\n)"""
mindsdb/libs/backends/__init__.py,0,b''
mindsdb/libs/backends/lightwood.py,0,"b""import os\nimport logging\n\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.config import *\n\nimport pandas as pd\nimport lightwood\n\n\nclass LightwoodBackend():\n\n    def __init__(self, transaction):\n        self.transaction = transaction\n        self.predictor = None\n\n    def _get_group_by_key(self, group_by, row):\n        gb_lookup_key = '!!@@!!'\n        for column in group_by:\n            gb_lookup_key += f'{column}_{row[column]}_!!@@!!'\n        return gb_lookup_key\n\n    def _create_timeseries_df(self, original_df):\n        group_by = self.transaction.lmd['model_group_by']\n        order_by = [x[0] for x in self.transaction.lmd['model_order_by']]\n        nr_samples = self.transaction.lmd['window_size']\n\n        group_by_ts_map = {}\n\n        for _, row in original_df.iterrows():\n            gb_lookup_key = self._get_group_by_key(group_by, row)\n            if gb_lookup_key not in group_by_ts_map:\n                group_by_ts_map[gb_lookup_key] = []\n\n            for col in order_by:\n                if row[col] is None:\n                    row[col] = 0.0\n                try:\n                    row[col] = float(row[col])\n                except:\n                    try:\n                        row[col] = float(row[col].timestamp())\n                    except:\n                        error_msg = f'Backend Lightwood does not support ordering by the column: {col} !, Faulty value: {row[col]}'\n                        self.transaction.log.error(error_msg)\n                        raise ValueError(error_msg)\n\n            group_by_ts_map[gb_lookup_key].append(row)\n\n        for k in group_by_ts_map:\n            group_by_ts_map[k] = pd.DataFrame.from_records(group_by_ts_map[k], columns=original_df.columns)\n            group_by_ts_map[k] = group_by_ts_map[k].sort_values(by=order_by)\n\n            for order_col in order_by:\n                for i in range(0,len(group_by_ts_map[k])):\n                    group_by_ts_map[k][order_col] = group_by_ts_map[k][order_col].astype(object)\n\n\n                    numerical_value = float(group_by_ts_map[k][order_col].iloc[i])\n                    arr_val = [str(numerical_value)]\n\n                    group_by_ts_map[k][order_col].iat[i] = arr_val\n\n                    previous_indexes = list(range(i - nr_samples, i))\n                    previous_indexes = [x for x in previous_indexes if x >= 0]\n                    previous_indexes.reverse()\n\n                    for prev_i in previous_indexes:\n                        group_by_ts_map[k].iloc[i][order_col].append(group_by_ts_map[k][order_col].iloc[prev_i].split(' ')[-1])\n\n                    while len(group_by_ts_map[k].iloc[i][order_col]) <= nr_samples:\n                        group_by_ts_map[k].iloc[i][order_col].append('0')\n\n                    group_by_ts_map[k].iloc[i][order_col].reverse()\n                    group_by_ts_map[k][order_col].iat[i] = ' '.join(group_by_ts_map[k].iloc[i][order_col])\n\n        combined_df = pd.concat(list(group_by_ts_map.values()))\n        return combined_df\n\n    def _create_lightwood_config(self):\n        config = {}\n\n        #config['name'] = 'lightwood_predictor_' + self.transaction.lmd['name']\n\n        config['input_features'] = []\n        config['output_features'] = []\n\n        for col_name in self.transaction.input_data.columns:\n            if col_name in self.transaction.lmd['columns_to_ignore']:\n                continue\n\n            col_stats = self.transaction.lmd['column_stats'][col_name]\n            data_subtype = col_stats['data_subtype']\n            data_type = col_stats['data_type']\n\n            lightwood_data_type = None\n\n            other_keys = {'encoder_attrs': {}}\n            if data_type in (DATA_TYPES.NUMERIC):\n                lightwood_data_type = 'numeric'\n\n            elif data_type in (DATA_TYPES.CATEGORICAL):\n                lightwood_data_type = 'categorical'\n\n            elif data_subtype in (DATA_SUBTYPES.TIMESTAMP, DATA_SUBTYPES.DATE):\n                lightwood_data_type = 'datetime'\n\n            elif data_subtype in (DATA_SUBTYPES.IMAGE):\n                lightwood_data_type = 'image'\n                other_keys['encoder_attrs']['aim'] = 'balance'\n\n            elif data_subtype in (DATA_SUBTYPES.AUDIO):\n                lightwood_data_type = 'audio'\n\n            elif data_subtype in (DATA_SUBTYPES.TEXT):\n                lightwood_data_type = 'text'\n\n            elif data_subtype in (DATA_SUBTYPES.ARRAY):\n                lightwood_data_type = 'time_series'\n\n            else:\n                self.transaction.log.error(f'The lightwood model backend is unable to handle data of type {data_type} and subtype {data_subtype} !')\n                raise Exception('Failed to build data definition for Lightwood model backend')\n\n            if col_name in [x[0] for x in self.transaction.lmd['model_order_by']]:\n                lightwood_data_type = 'time_series'\n\n            col_config = {\n                'name': col_name,\n                'type': lightwood_data_type\n            }\n\n            if col_name in self.transaction.lmd['weight_map']:\n                col_config['weights'] = self.transaction.lmd['weight_map'][col_name]\n\n            col_config.update(other_keys)\n\n            if col_name not in self.transaction.lmd['predict_columns']:\n                config['input_features'].append(col_config)\n            else:\n                config['output_features'].append(col_config)\n\n        if self.transaction.lmd['optimize_model']:\n            config['optimizer'] = lightwood.model_building.BasicAxOptimizer\n\n        config['data_source'] = {}\n        config['data_source']['cache_transformed_data'] = not self.transaction.lmd['force_disable_cache']\n\n        config['mixer'] = {}\n        config['mixer']['selfaware'] = self.transaction.lmd['use_selfaware_model']\n\n        return config\n\n    def callback_on_iter(self, epoch, mix_error, test_error, delta_mean, accuracy):\n        test_error_rounded = round(test_error,4)\n        for col in accuracy:\n            value = accuracy[col]['value']\n            if accuracy[col]['function'] == 'r2_score':\n                value_rounded = round(value,3)\n                self.transaction.log.debug(f'We\\'ve reached training epoch nr {epoch} with an r2 score of {value_rounded} on the testing dataset')\n            else:\n                value_pct = round(value * 100,2)\n                self.transaction.log.debug(f'We\\'ve reached training epoch nr {epoch} with an accuracy of {value_pct}% on the testing dataset')\n\n    def train(self):\n        if self.transaction.lmd['use_gpu'] is not None:\n            lightwood.config.config.CONFIG.USE_CUDA = self.transaction.lmd['use_gpu']\n\n        if self.transaction.lmd['model_order_by'] is not None and len(self.transaction.lmd['model_order_by']) > 0:\n            self.transaction.log.debug('Reshaping data into timeseries format, this may take a while !')\n            train_df = self._create_timeseries_df(self.transaction.input_data.train_df)\n            test_df = self._create_timeseries_df(self.transaction.input_data.test_df)\n            self.transaction.log.debug('Done reshaping data into timeseries format !')\n        else:\n            train_df = self.transaction.input_data.train_df\n            test_df = self.transaction.input_data.test_df\n\n        lightwood_config = self._create_lightwood_config()\n\n        if self.transaction.lmd['skip_model_training'] == True:\n            self.predictor = lightwood.Predictor(load_from_path=os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.transaction.lmd['name'] + '_lightwood_data'))\n        else:\n            self.predictor = lightwood.Predictor(lightwood_config)\n\n            # Evaluate less often for larger datasets and vice-versa\n            eval_every_x_epochs = int(round(1 * pow(10,6) * (1/len(train_df))))\n\n            # Within some limits\n            if eval_every_x_epochs > 200:\n                eval_every_x_epochs = 200\n            if eval_every_x_epochs < 3:\n                eval_every_x_epochs = 3\n\n            logging.getLogger().setLevel(logging.DEBUG)\n            if self.transaction.lmd['stop_training_in_x_seconds'] is None:\n                self.predictor.learn(from_data=train_df, test_data=test_df, callback_on_iter=self.callback_on_iter, eval_every_x_epochs=eval_every_x_epochs)\n            else:\n                self.predictor.learn(from_data=train_df, test_data=test_df, stop_training_after_seconds=self.transaction.lmd['stop_training_in_x_seconds'], callback_on_iter=self.callback_on_iter, eval_every_x_epochs=eval_every_x_epochs)\n\n            self.transaction.log.info('Training accuracy of: {}'.format(self.predictor.train_accuracy))\n\n        self.transaction.lmd['lightwood_data']['save_path'] = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.transaction.lmd['name'] + '_lightwood_data')\n        self.predictor.save(path_to=self.transaction.lmd['lightwood_data']['save_path'])\n\n    def predict(self, mode='predict', ignore_columns=None):\n        if ignore_columns is None:\n            ignore_columns = []\n        if self.transaction.lmd['use_gpu'] is not None:\n            lightwood.config.config.CONFIG.USE_CUDA = self.transaction.lmd['use_gpu']\n\n        if mode == 'predict':\n            df = self.transaction.input_data.data_frame\n        if mode == 'validate':\n            df = self.transaction.input_data.validation_df\n        elif mode == 'test':\n            df = self.transaction.input_data.test_df\n\n        if self.transaction.lmd['model_order_by'] is not None and len(self.transaction.lmd['model_order_by']) > 0:\n            df = self._create_timeseries_df(df)\n\n        if self.predictor is None:\n            self.predictor = lightwood.Predictor(load_from_path=self.transaction.lmd['lightwood_data']['save_path'])\n\n        # not the most efficient but least prone to bug and should be fast enough\n        if len(ignore_columns)  > 0:\n            run_df = df.copy(deep=True)\n            for col_name in ignore_columns:\n                run_df[col_name] = [None] * len(run_df[col_name])\n        else:\n            run_df = df\n\n        predictions = self.predictor.predict(when_data=run_df)\n\n        formated_predictions = {}\n        for k in predictions:\n            formated_predictions[k] = predictions[k]['predictions']\n\n            confidence_arr = []\n            for confidence_name in ['selfaware_confidences','loss_confidences', 'quantile_confidences']:\n                if confidence_name in predictions[k]:\n                    conf_arr = [x if x > 0 else 0 for x in predictions[k][confidence_name]]\n                    conf_arr = [x if x < 1 else 1 for x in conf_arr]\n                    confidence_arr.append(conf_arr)\n                    \n            if len(confidence_arr) > 0:\n                confidences = []\n                for n in range(len(confidence_arr[0])):\n                    confidences.append([])\n                    for i in range(len(confidence_arr)):\n                        confidences[-1].append(confidence_arr[i][n])\n                    confidences[-1] = sum(confidences[-1])/len(confidences[-1])\n                formated_predictions[f'{k}_model_confidence'] = confidences\n\n            if 'confidence_range' in predictions[k]:\n                formated_predictions[f'{k}_confidence_range'] = predictions[k]['confidence_range']\n\n        return formated_predictions\n"""
mindsdb/libs/backends/ludwig.py,0,"b'from dateutil.parser import parse as parse_datetime\nimport os, sys\nimport shutil\nimport subprocess\n\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.config import *\nfrom mindsdb.libs.helpers.general_helpers import disable_console_output, get_tensorflow_colname\n\nfrom tensorflow.python.client import device_lib\nfrom ludwig.api import LudwigModel\nfrom ludwig.data.preprocessing import build_metadata\nimport pandas as pd\nfrom imageio import imread\n\n\nclass LudwigBackend():\n\n    def __init__(self, transaction):\n        try:\n            subprocess.call([\'python3\',\'-m\',\'spacy\',\'download\',\'en_core_web_sm\'])\n        except:\n            try:\n                subprocess.call([\'python\',\'-m\',\'spacy\',\'download\',\'en_core_web_sm\'])\n            except:\n                print(\'Can\\\'t download spacy vocabulary, ludwig backend may fail when processing text input\')\n        try:\n            subprocess.call([\'python3\',\'-m\',\'spacy\',\'download\',\'en\'])\n        except:\n            try:\n                subprocess.call([\'python\',\'-m\',\'spacy\',\'download\',\'en\'])\n            except:\n                print(\'Can\\\'t download spacy vocabulary, ludwig backend may fail when processing text input\')\n\n        self.transaction = transaction\n\n    def _translate_df_to_timeseries_format(self, df, model_definition, timeseries_cols, mode=\'predict\'):\n        timeseries_col_name = timeseries_cols[0]\n\n        previous_predict_col_names = []\n        predict_col_names = []\n        for feature_def in model_definition[\'output_features\']:\n            if mode == \'train\':\n                predict_col_names.append(feature_def[\'name\'])\n            else:\n                predict_col_names = []\n            previous_predict_col_name = \'previous_\' + feature_def[\'name\']\n\n            previous_predict_col_already_in = False\n\n            for definition in model_definition[\'input_features\']:\n                if definition[\'name\'] == previous_predict_col_name:\n                     previous_predict_col_already_in = True\n\n            if not previous_predict_col_already_in:\n                model_definition[\'input_features\'].append({\n                    \'name\': previous_predict_col_name\n                    ,\'type\': \'sequence\'\n                })\n\n        other_col_names = []\n        for feature_def in model_definition[\'input_features\']:\n            if feature_def[\'name\'] not in self.transaction.lmd[\'model_group_by\'] and feature_def[\'name\'] not in previous_predict_col_names:\n                feature_def[\'type\'] = \'sequence\'\n                if feature_def[\'name\'] not in [timeseries_col_name]:\n                    other_col_names.append(feature_def[\'name\'])\n\n\n            previous_predict_col_names.append(previous_predict_col_name)\n\n        new_cols = {}\n        for col in [*other_col_names,*previous_predict_col_names,timeseries_col_name,*predict_col_names,*self.transaction.lmd[\'model_group_by\']]:\n            new_cols[col] = []\n\n        nr_ele = len(df[timeseries_col_name])\n\n        i = 0\n        while i < nr_ele:\n            new_row = {}\n\n            timeseries_row = [df[timeseries_col_name][i]]\n\n            for col in other_col_names:\n                new_row[col] = [df[col][i]]\n            for col in previous_predict_col_names:\n                new_row[col] = []\n            for col in predict_col_names:\n                new_row[col] = df[col][i]\n            for col in self.transaction.lmd[\'model_group_by\']:\n                new_row[col] = df[col][i]\n\n            inverted_index_range = list(range(i))\n            inverted_index_range.reverse()\n            ii = 0\n            for ii in inverted_index_range:\n                if (i - ii) > self.transaction.lmd[\'window_size\']:\n                    break\n                timeseries_row.append(df[timeseries_col_name][ii])\n\n                for col in other_col_names:\n                    new_row[col].append(df[col][ii])\n                for col in previous_predict_col_names:\n                    try:\n                        new_row[col].append(df[col.replace(\'previous_\', \'\')][ii])\n                    except:\n                        try:\n                            new_row[col].append(df[col][ii])\n                        except:\n                            self.transaction.log.warning(\'Missing previous predicted values for output column: {}, these should be included in your input under the name: {}\'.format(col.replace(\'previous_\', \'\'), col))\n\n            if mode == \'train\':\n                i = max(i + 1, (i + round((i - ii)/2)))\n            else:\n                i = i + 1\n\n            new_row[timeseries_col_name] = timeseries_row\n\n            for col in new_row:\n                if col not in predict_col_names and col not in self.transaction.lmd[\'model_group_by\']:\n                    new_row[col].reverse()\n                new_cols[col].append(new_row[col])\n\n        new_df = pd.DataFrame(data=new_cols)\n        df = new_df\n        return df, model_definition\n\n    def _create_ludwig_dataframe(self, mode):\n        has_heavy_data = False\n        col_map = {}\n\n        if mode == \'train\':\n            df = self.transaction.input_data.train_df\n        elif mode == \'predict\':\n            df = self.transaction.input_data.data_frame\n        elif mode == \'validate\':\n            df = self.transaction.input_data.validation_df\n        elif mode == \'test\':\n            df = self.transaction.input_data.test_df\n        else:\n            raise Exception(f\'Unknown mode specified: ""{mode}""\')\n        model_definition = {\'input_features\': [], \'output_features\': []}\n        data = {}\n\n        if self.transaction.lmd[\'model_order_by\'] is None:\n            timeseries_cols = []\n        else:\n            timeseries_cols = list(map(lambda x: x[0], self.transaction.lmd[\'model_order_by\']))\n\n        for col in df.columns:\n            tf_col = get_tensorflow_colname(col)\n            col_map[tf_col] = col\n\n            # Handle malformed columns\n            if col in self.transaction.lmd[\'columns_to_ignore\']:\n                continue\n\n            data[tf_col] = []\n\n            col_stats = self.transaction.lmd[\'column_stats\'][col]\n            data_subtype = col_stats[\'data_subtype\']\n\n            ludwig_dtype = None\n            encoder = None\n            cell_type = None\n            in_memory = None\n            height = None\n            width = None\n\n            if col in timeseries_cols:\n                encoder = \'rnn\'\n                cell_type = \'rnn\'\n                ludwig_dtype = \'order_by_col\'\n\n            if data_subtype in DATA_SUBTYPES.ARRAY:\n                encoder = \'rnn\'\n                cell_type = \'rnn\'\n                ludwig_dtype = \'sequence\'\n\n            elif data_subtype in (DATA_SUBTYPES.INT, DATA_SUBTYPES.FLOAT):\n                ludwig_dtype = \'numerical\'\n\n            elif data_subtype in (DATA_SUBTYPES.BINARY):\n                ludwig_dtype = \'category\'\n\n            elif data_subtype in (DATA_SUBTYPES.DATE):\n                if col not in self.transaction.lmd[\'predict_columns\']:\n                    ludwig_dtype = \'date\'\n                else:\n                    ludwig_dtype = \'category\'\n\n            elif data_subtype in (DATA_SUBTYPES.TIMESTAMP):\n                ludwig_dtype = \'numerical\'\n\n            elif data_subtype in (DATA_SUBTYPES.SINGLE, DATA_SUBTYPES.MULTIPLE):\n                ludwig_dtype = \'category\'\n\n            elif data_subtype in (DATA_SUBTYPES.IMAGE):\n                has_heavy_data = True\n                ludwig_dtype = \'image\'\n                encoder = \'stacked_cnn\'\n                in_memory = True\n                height = 256\n                width = 256\n\n            elif data_subtype in (DATA_SUBTYPES.TEXT):\n                ludwig_dtype = \'text\'\n\n            else:\n                # @TODO Maybe regress to some other similar subtype or use the principal data type for certain values\n                self.transaction.log.error(f\'The Ludwig backend doesn\\\'t support the ""{data_subtype}"" data type !\')\n                estr = f\'Data subtype ""{data_subtype}"" no supported by Ludwig model backend\'\n                raise Exception(estr)\n\n            custom_logic_continue = False\n\n            for index, row in df.iterrows():\n                if ludwig_dtype == \'order_by_col\':\n                    ts_data_point = row[col]\n\n                    try:\n                        ts_data_point = float(ts_data_point)\n                    except:\n                        ts_data_point = parse_datetime(ts_data_point).timestamp()\n                    data[tf_col].append(ts_data_point)\n\n                elif ludwig_dtype == \'sequence\':\n                    arr_str = row[col]\n                    if arr_str is not None:\n                        arr = list(map(float,arr_str.rstrip(\']\').lstrip(\'[\').split(self.transaction.lmd[\'column_stats\'][col][\'separator\'])))\n                    else:\n                        arr = \'\'\n                    data[tf_col].append(arr)\n\n                # Date isn\'t supported yet, so we hack around it\n                elif ludwig_dtype == \'date\':\n                    if col in data:\n                        data.pop(col)\n                        data[tf_col + \'_year\'] = []\n                        data[tf_col + \'_month\'] = []\n                        data[tf_col + \'_day\'] = []\n\n                        model_definition[\'input_features\'].append({\n                            \'name\': col + \'_year\'\n                            ,\'type\': \'category\'\n                        })\n                        model_definition[\'input_features\'].append({\n                            \'name\': col + \'_month\'\n                            ,\'type\': \'category\'\n                        })\n                        model_definition[\'input_features\'].append({\n                            \'name\': col + \'_day\'\n                            ,\'type\': \'numerical\'\n                        })\n\n                    date = parse_datetime(row[col])\n\n                    data[tf_col + \'_year\'].append(date.year)\n                    data[tf_col + \'_month\'].append(date.month)\n                    data[tf_col + \'_day\'].append(date.day)\n\n                    custom_logic_continue = True\n\n                    if col in timeseries_cols:\n                        timeseries_cols.remove(col)\n                        timeseries_cols.append(col + \'_day\')\n                        timeseries_cols.append(col + \'_month\')\n                        timeseries_cols.append(col + \'_year\')\n\n                elif data_subtype in (DATA_SUBTYPES.TIMESTAMP):\n                    if row[col] is None:\n                        unix_ts = 0\n                    else:\n                        unix_ts = parse_datetime(row[col]).timestamp()\n\n                    data[tf_col].append(unix_ts)\n\n                elif data_subtype in (DATA_SUBTYPES.FLOAT):\n                    if isinstance(row[col], str):\n                        data[tf_col].append(float(str(row[col]).replace(\',\', \'.\')))\n                    else:\n                        data[tf_col].append(row[col])\n\n                elif data_subtype in (DATA_SUBTYPES.INT):\n                    if isinstance(row[col], str):\n                        data[tf_col].append(round(float(str(row[col]).replace(\',\', \'.\'))))\n                    else:\n                        data[tf_col].append(row[col])\n\n                elif data_subtype in (DATA_SUBTYPES.IMAGE):\n                    if os.path.isabs(row[col]):\n                        data[tf_col].append(row[col])\n                    else:\n                        data[tf_col].append(os.path.join(os.getcwd(), row[col]))\n                else:\n                    data[tf_col].append(row[col])\n\n            if custom_logic_continue:\n                continue\n\n            if col not in self.transaction.lmd[\'predict_columns\']:\n                input_def = {\n                    \'name\': tf_col\n                    ,\'type\': ludwig_dtype\n                }\n                if encoder is not None:\n                    input_def[\'encoder\'] = encoder\n                if cell_type is not None:\n                    input_def[\'cell_type\'] = cell_type\n                if in_memory is not None:\n                    input_def[\'in_memory\'] = in_memory\n\n                if height is not None and width is not None:\n                    input_def[\'height\'] = height\n                    input_def[\'width\'] = width\n                    input_def[\'resize_image\'] = True\n                    input_def[\'resize_method\'] = \'crop_or_pad\'\n                    model_definition[\'preprocessing\'] = {\n                        \'image\': {\n                            \'height\': height\n                            ,\'width\': width\n                            ,\'resize_image\': True\n                            ,\'resize_method\': \'crop_or_pad\'\n                            ,\'num_channels\': 3\n                        }\n                    }\n\n                model_definition[\'input_features\'].append(input_def)\n            else:\n                output_def = {\n                    \'name\': tf_col\n                    ,\'type\': ludwig_dtype\n                }\n                model_definition[\'output_features\'].append(output_def)\n\n        df = pd.DataFrame(data=data)\n        if len(timeseries_cols) > 0:\n            df.sort_values(timeseries_cols)\n\n        return df, model_definition, timeseries_cols, has_heavy_data, col_map\n\n    def _get_model_dir(self):\n        model_dir = None\n        for thing in os.listdir(self.transaction.lmd[\'ludwig_data\'][\'ludwig_save_path\']):\n            if \'api_experiment\' in thing:\n                model_dir = os.path.join(self.transaction.lmd[\'ludwig_data\'][\'ludwig_save_path\'],thing,\'model\')\n        if model_dir is None:\n            model_dir = os.path.join(self.transaction.lmd[\'ludwig_data\'][\'ludwig_save_path\'],\'model\')\n        return model_dir\n\n    def _get_useable_gpus(self):\n        if self.transaction.lmd[\'use_gpu\'] == False:\n            return []\n        local_device_protos = device_lib.list_local_devices()\n        gpus = [x for x in local_device_protos if x.device_type == \'GPU\']\n        #bus_ids = [x.locality.bus_id for x in gpus]\n        gpu_indices = [i for i in range(len(gpus))]\n        if len(gpu_indices) == 0:\n            return None\n        else:\n            return gpu_indices\n\n    def train(self):\n        training_dataframe, model_definition, timeseries_cols, has_heavy_data, self.transaction.lmd[\'ludwig_tf_self_col_map\'] = self._create_ludwig_dataframe(\'train\')\n\n        if len(timeseries_cols) > 0:\n            training_dataframe, model_definition =  self._translate_df_to_timeseries_format(training_dataframe, model_definition, timeseries_cols, \'train\')\n\n        with disable_console_output(True):\n            # <---- Ludwig currently broken, since mode can\'t be initialized without train_set_metadata and train_set_metadata can\'t be obtained without running train... see this issue for any updates on the matter: https://github.com/uber/ludwig/issues/295\n            #model.initialize_model(train_set_metadata={})\n            #train_stats = model.train_online(data_df=training_dataframe) # ??Where to add model_name?? ----> model_name=self.transaction.lmd[\'name\']\n\n            ludwig_save_is_working = False\n\n            if not ludwig_save_is_working:\n                shutil.rmtree(\'results\',ignore_errors=True)\n\n            if self.transaction.lmd[\'rebuild_model\'] is True:\n                model = LudwigModel(model_definition)\n                merged_model_definition = model.model_definition\n                train_set_metadata = build_metadata(\n                    training_dataframe,\n                    (merged_model_definition[\'input_features\'] +\n                    merged_model_definition[\'output_features\']),\n                    merged_model_definition[\'preprocessing\']\n                )\n                model.initialize_model(train_set_metadata=train_set_metadata, gpus=self._get_useable_gpus())\n            else:\n                model = LudwigModel.load(model_dir=self._get_model_dir())\n\n\n            split_by = int(20 * pow(10,6))\n            if has_heavy_data:\n                split_by = 40\n            df_len = len(training_dataframe[training_dataframe.columns[0]])\n            if df_len > split_by:\n                i = 0\n                while i < df_len:\n                    end = i + split_by\n                    self.transaction.log.info(f\'Training with batch from index {i} to index {end}\')\n                    training_sample = training_dataframe.iloc[i:end]\n                    training_sample = training_sample.reset_index()\n\n                    if len(training_sample) < 1:\n                        continue\n\n                    train_stats = model.train(data_df=training_sample, model_name=self.transaction.lmd[\'name\'], skip_save_model=ludwig_save_is_working, skip_save_progress=True, gpus=self._get_useable_gpus())\n                    i = end\n            else:\n                train_stats = model.train(data_df=training_dataframe, model_name=self.transaction.lmd[\'name\'], skip_save_model=ludwig_save_is_working, skip_save_progress=True, gpus=self._get_useable_gpus())\n\n            for k in train_stats[\'train\']:\n                if k not in self.transaction.lmd[\'model_accuracy\'][\'train\']:\n                    self.transaction.lmd[\'model_accuracy\'][\'train\'][k] = []\n                    self.transaction.lmd[\'model_accuracy\'][\'test\'][k] = []\n                elif k != \'combined\':\n                    # We should be adding the accuracy here but we only have it for combined, so, for now use that, will only affect multi-output scenarios anyway\n                    pass\n                else:\n                    self.transaction.lmd[\'model_accuracy\'][\'train\'][k].extend(train_stats[\'train\'][k][\'accuracy\'])\n                    self.transaction.lmd[\'model_accuracy\'][\'test\'][k].extend(train_stats[\'test\'][k][\'accuracy\'])\n\n            \'\'\'\n            @ TRAIN ONLINE BIT That\'s not working\n            model = LudwigModel.load(self.transaction.lmd[\'ludwig_data\'][\'ludwig_save_path\'])\n            for i in range(0,100):\n                train_stats = model.train_online(data_df=training_dataframe)\n                # The resulting train_stats are ""None""... wonderful -_-\n            \'\'\'\n\n        ludwig_model_savepath = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.transaction.lmd[\'name\'] + \'_ludwig_data\')\n        if ludwig_save_is_working:\n            model.save(ludwig_model_savepath)\n            model.close()\n        else:\n            shutil.rmtree(ludwig_model_savepath,ignore_errors=True)\n            shutil.move(os.path.join(\'results\',os.listdir(\'results\')[0]),ludwig_model_savepath)\n        self.transaction.lmd[\'ludwig_data\'] = {\'ludwig_save_path\': ludwig_model_savepath}\n        self.transaction.hmd[\'ludwig_data\'] = {\'model_definition\': model_definition}\n\n    def predict(self, mode=\'predict\', ignore_columns=None):\n        if ignore_columns is None:\n            ignore_columns = []\n\n        predict_dataframe, model_definition, timeseries_cols, has_heavy_data, _ = self._create_ludwig_dataframe(mode)\n        model_definition = self.transaction.hmd[\'ludwig_data\'][\'model_definition\']\n\n        if len(timeseries_cols) > 0:\n            predict_dataframe, model_definition =  self._translate_df_to_timeseries_format(predict_dataframe, model_definition, timeseries_cols)\n\n        for ignore_col in ignore_columns:\n            for tf_col in self.transaction.lmd[\'ludwig_tf_self_col_map\']:\n                if ignore_col == self.transaction.lmd[\'ludwig_tf_self_col_map\'][tf_col]:\n                    ignore_col = tf_col\n            try:\n                predict_dataframe[ignore_col] = [None] * len(predict_dataframe[ignore_col])\n            except:\n                for date_appendage in [\'_year\', \'_month\',\'_day\']:\n                    predict_dataframe[ignore_col + date_appendage] = [None] * len(predict_dataframe[ignore_col + date_appendage])\n\n        with disable_console_output(True):\n            model_dir = self._get_model_dir()\n            model = LudwigModel.load(model_dir=model_dir)\n            predictions = model.predict(data_df=predict_dataframe, gpus=self._get_useable_gpus())\n\n        for col_name in predictions:\n            col_name_normalized = col_name.replace(\'_predictions\', \'\')\n            if col_name_normalized in self.transaction.lmd[\'ludwig_tf_self_col_map\']:\n                col_name_normalized = self.transaction.lmd[\'ludwig_tf_self_col_map\'][col_name_normalized]\n            predictions = predictions.rename(columns = {col_name: col_name_normalized})\n\n        return predictions\n'"
mindsdb/libs/constants/__init__.py,0,b''
mindsdb/libs/constants/mindsdb.py,0,"b'TRANSACTION_LEARN = \'learn\'\nTRANSACTION_PREDICT = \'predict\'\nTRANSACTION_ANALYSE = \'analyse\'\nTRANSACTION_NORMAL_SELECT = \'normal_select\'\nTRANSACTION_NORMAL_MODIFY = \'normal_modify\'\nTRANSACTION_BAD_QUERY = \'bad_query\'\nTRANSACTION_DROP_MODEL =\'drop_model\'\n\nSTOP_TRAINING = \'stop_training\'\nKILL_TRAINING = \'kill_training\'\n\n\nclass DATA_SUBTYPES:\n    # Numeric\n    INT = \'Int\'\n    FLOAT = \'Float\'\n    BINARY = \'Binary\' # Should we have this ?\n\n    # DATETIME\n    DATE = \'Date\' # YYYY-MM-DD\n    TIMESTAMP = \'Timestamp\' # YYYY-MM-DD hh:mm:ss or 1852362464\n\n    # CATEGORICAL\n    SINGLE = \'Binary Category\'\n    MULTIPLE = \'Category\'\n\n    # FILE_PATH\n    IMAGE = \'Image\'\n    VIDEO = \'Video\'\n    AUDIO = \'Audio\'\n\n    # SEQUENTIAL\n    TEXT = \'Text\'\n    ARRAY = \'Array\' # Do we even want to support arrays / structs / nested ... etc ?\n\nclass DATA_TYPES:\n    NUMERIC = \'Numeric\'\n    DATE = \'Date\'\n    CATEGORICAL = \'Categorical\'\n    FILE_PATH = \'File Path\'\n    SEQUENTIAL = \'Sequential\'\n\nclass DATA_TYPES_SUBTYPES:\n    subtypes = {\n        DATA_TYPES.NUMERIC: (DATA_SUBTYPES.INT, DATA_SUBTYPES.FLOAT, DATA_SUBTYPES.BINARY)\n        ,DATA_TYPES.DATE:(DATA_SUBTYPES.DATE, DATA_SUBTYPES.TIMESTAMP)\n        ,DATA_TYPES.CATEGORICAL:(DATA_SUBTYPES.SINGLE, DATA_SUBTYPES.MULTIPLE)\n        ,DATA_TYPES.FILE_PATH:(DATA_SUBTYPES.IMAGE, DATA_SUBTYPES.VIDEO, DATA_SUBTYPES.AUDIO)\n        ,DATA_TYPES.SEQUENTIAL:(DATA_SUBTYPES.TEXT, DATA_SUBTYPES.ARRAY)\n    }\n\n\nclass ORDER_BY_KEYS:\n    COLUMN = 0\n    ASCENDING_VALUE = 1\n\nMODEL_STATUS_TRAINED = ""Trained""\nMODEL_STATUS_PREPARING = ""Preparing""\nMODEL_STATUS_DATA_ANALYSIS = ""Data Analysis""\nMODEL_STATUS_TRAINING= ""Training""\nMODEL_STATUS_ANALYZING = ""Analyzing""\nMODEL_STATUS_ERROR = ""Error""\nMODEL_STATUS_DONE = ""Done""\n\nWORD_SEPARATORS = [\',\', ""\\t"", \' \']\n\nDEBUG_LOG_LEVEL = 10\nINFO_LOG_LEVEL = 20\nWARNING_LOG_LEVEL = 30\nERROR_LOG_LEVEL = 40\nNO_LOGS_LOG_LEVEL = 50\n\n# Magic numbers for extracting significant clusters out of the confidence distribution\nPEAK_CONFIDENCE_THRESHOLD = 0.12\nCLUSTER_MEMBER_CONFIDENCE_THRESHOLD = 0.06\n'"
mindsdb/libs/controllers/__init__.py,0,b''
mindsdb/libs/controllers/predictor.py,0,"b'import shutil\nimport zipfile\nimport os\nimport uuid\nimport traceback\nimport pickle\n\nfrom mindsdb.libs.data_types.mindsdb_logger import MindsdbLogger\nfrom mindsdb.libs.helpers.multi_data_source import getDS\nfrom mindsdb.__about__ import __version__\n\nfrom mindsdb.config import CONFIG\nfrom mindsdb.libs.controllers.transaction import Transaction\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.helpers.general_helpers import check_for_updates\n\nfrom pathlib import Path\n\nclass Predictor:\n\n    def __init__(self, name, root_folder=CONFIG.MINDSDB_STORAGE_PATH, log_level=CONFIG.DEFAULT_LOG_LEVEL):\n        """"""\n        This controller defines the API to a MindsDB \'mind\', a mind is an object that can learn and predict from data\n\n        :param name: the namespace you want to identify this mind instance with\n        :param root_folder: the folder where you want to store this mind or load from\n        :param log_level: the desired log level\n        """"""\n\n        # initialize variables\n        self.name = name\n        self.root_folder = root_folder\n        self.uuid = str(uuid.uuid1())\n        # initialize log\n        self.log = MindsdbLogger(log_level=log_level, uuid=self.uuid)\n\n        if CONFIG.CHECK_FOR_UPDATES:\n            try:\n                check_for_updates()\n            except:\n                self.log.warning(\'Could not check for updates !\')\n\n        if not CONFIG.SAGEMAKER:\n            # If storage path is not writable, raise an exception as this can no longer be\n            if not os.access(CONFIG.MINDSDB_STORAGE_PATH, os.W_OK):\n                error_message = \'\'\'Cannot write into storage path, please either set the config variable mindsdb.config.set(\'MINDSDB_STORAGE_PATH\',<path>) or give write access to {folder}\'\'\'\n                self.log.warning(error_message.format(folder=CONFIG.MINDSDB_STORAGE_PATH))\n                raise ValueError(error_message.format(folder=CONFIG.MINDSDB_STORAGE_PATH))\n\n\n            # If storage path is not writable, raise an exception as this can no longer be\n            if not os.access(CONFIG.MINDSDB_STORAGE_PATH, os.R_OK):\n                error_message = \'\'\'Cannot read from storage path, please either set the config variable mindsdb.config.set(\'MINDSDB_STORAGE_PATH\',<path>) or give write access to {folder}\'\'\'\n                self.log.warning(error_message.format(folder=CONFIG.MINDSDB_STORAGE_PATH))\n                raise ValueError(error_message.format(folder=CONFIG.MINDSDB_STORAGE_PATH))\n\n    def get_models(self):\n        models = []\n        for fn in os.listdir(CONFIG.MINDSDB_STORAGE_PATH):\n            if \'_light_model_metadata.pickle\' in fn:\n                model_name = fn.replace(\'_light_model_metadata.pickle\',\'\')\n                try:\n                    amd = self.get_model_data(model_name)\n                    model = {}\n                    for k in [\'name\', \'version\', \'is_active\', \'data_source\', \'predict\',\n                    \'status\', \'train_end_at\', \'updated_at\', \'created_at\',\'current_phase\', \'accuracy\']:\n                        if k in amd:\n                            model[k] = amd[k]\n                        else:\n                            model[k] = None\n\n                    models.append(model)\n                except Exception as e:\n                    print(e)\n                    print(traceback.format_exc())\n                    print(f""Can\'t adapt metadata for model: \'{model_name}\' when calling `get_models()`"")\n\n        return models\n\n    def _adapt_column(self, col_stats, col):\n        icm = {}\n        icm[\'column_name\'] = col\n        icm[\'data_type\'] = col_stats[\'data_type\']\n        icm[\'data_subtype\'] = col_stats[\'data_subtype\']\n\n        icm[\'data_type_distribution\'] = {\n            \'type\': ""categorical""\n            ,\'x\': []\n            ,\'y\': []\n        }\n        for k in col_stats[\'data_type_dist\']:\n            icm[\'data_type_distribution\'][\'x\'].append(k)\n            icm[\'data_type_distribution\'][\'y\'].append(col_stats[\'data_type_dist\'][k])\n\n        icm[\'data_subtype_distribution\'] = {\n            \'type\': ""categorical""\n            ,\'x\': []\n            ,\'y\': []\n        }\n        for k in col_stats[\'data_subtype_dist\']:\n            icm[\'data_subtype_distribution\'][\'x\'].append(k)\n            icm[\'data_subtype_distribution\'][\'y\'].append(col_stats[\'data_subtype_dist\'][k])\n\n        icm[\'data_distribution\'] = {}\n        icm[\'data_distribution\'][\'data_histogram\'] = {\n            ""type"": ""categorical"",\n            \'x\': [],\n            \'y\': []\n        }\n        icm[\'data_distribution\'][\'clusters\'] =  [\n             {\n                 ""group"": [],\n                 ""members"": []\n             }\n         ]\n\n\n        for i in range(len(col_stats[\'histogram\'][\'x\'])):\n            icm[\'data_distribution\'][\'data_histogram\'][\'x\'].append(col_stats[\'histogram\'][\'x\'][i])\n            icm[\'data_distribution\'][\'data_histogram\'][\'y\'].append(col_stats[\'histogram\'][\'y\'][i])\n\n        scores = [\'consistency_score\', \'redundancy_score\', \'variability_score\']\n        for score in scores:\n            metrics = []\n            if score == \'consistency_score\':\n                simple_description = ""A low value indicates the data is not very consistent, it\'s either missing a lot of valus or the type (e.g. number, text, category, date) of values varries quite a lot.""\n                metrics.append({\n                      ""type"": ""score"",\n                      ""name"": ""Type Distribution"",\n                      ""score"": col_stats[\'data_type_distribution_score\'],\n                      #""description"": col_stats[\'data_type_distribution_score_description\'],\n                      ""description"": ""A low value indicates that we can\'t consistently determine a single data type (e.g. number, text, category, date) for most values in this column"",\n                      ""warning"": col_stats[\'data_type_distribution_score_warning\']\n                })\n                metrics.append({\n                      ""type"": ""score"",\n                      ""score"": col_stats[\'empty_cells_score\'],\n                      ""name"": ""Empty Cells"",\n                      #""description"": col_stats[\'empty_cells_score_description\'],\n                      ""description"": ""A low value indicates that a lot of the values in this column are empty or null. A value of 10 means no cell is missing data, a value of 0 means no cell has any data."",\n                      ""warning"": col_stats[\'empty_cells_score_warning\']\n                })\n                if \'duplicates_score\' in col_stats:\n                    metrics.append({\n                          ""type"": ""score"",\n                          ""name"": ""Value Duplication"",\n                          ""score"": col_stats[\'duplicates_score\'],\n                          #""description"": col_stats[\'duplicates_score_description\'],\n                          ""description"": ""A low value indicates that a lot of the values in this columns are duplicates, as in, the same value shows up more than once in the column. This is not necessarily bad and could be normal for certain data types."",\n                          ""warning"": col_stats[\'duplicates_score_warning\']\n                    })\n\n            if score == \'variability_score\':\n                simple_description = ""A low value indicates a high possibility of some noise affecting your data collection process. This could mean that the values for this column are not collected or processed correctly.""\n                if \'lof_based_outlier_score\' in col_stats and \'z_test_based_outlier_score\' in col_stats:\n                    metrics.append({\n                          ""type"": ""score"",\n                          ""name"": ""Z Outlier Score"",\n                          ""score"": col_stats[\'lof_based_outlier_score\'],\n                          #""description"": col_stats[\'lof_based_outlier_score_description\'],\n                          ""description"": ""A low value indicates a large number of outliers in your dataset. This is based on distance from the center of 20 clusters as constructed via KNN."",\n                          ""warning"": col_stats[\'lof_based_outlier_score_warning\']\n                    })\n                    metrics.append({\n                          ""type"": ""score"",\n                          ""name"": ""Z Outlier Score"",\n                          ""score"": col_stats[\'z_test_based_outlier_score\'],\n                          #""description"": col_stats[\'z_test_based_outlier_score_description\'],\n                          ""description"": ""A low value indicates a large number of data points are more than 3 standard deviations away from the mean value of this column. This means that this column likely has a large amount of outliers"",\n                          ""warning"": col_stats[\'z_test_based_outlier_score_warning\']\n                    })\n                metrics.append({\n                      ""type"": ""score"",\n                      ""name"":""Value Distribution"",\n                      ""score"": col_stats[\'value_distribution_score\'],\n                      #""description"": col_stats[\'value_distribution_score_description\'],\n                      ""description"": ""A low value indicates the possibility of a large number of outliers, the clusters in which your data is distributed aren\'t evenly sized."",\n                      ""warning"": col_stats[\'value_distribution_score_warning\']\n                })\n\n            if score == \'redundancy_score\':\n                # CLF based score to be included here once we find a faster way of computing it...\n                similarity_score_based_most_correlated_column = col_stats[\'most_similar_column_name\']\n\n                simple_description = f""A low value indicates that the data in this column is highly redundant (useless) for making any sort of prediction. You should make sure that values heavily related to this column are not already expressed in the \\""{similarity_score_based_most_correlated_column}\\"" column (e.g. if this column is a timestamp, make sure you don\'t have another column representing the exact same time in ISO datetime format)""\n\n\n                metrics.append({\n                      ""type"": ""score"",\n                      ""name"": ""Matthews Correlation Score"",\n                      ""score"": col_stats[\'similarity_score\'],\n                      #""description"": col_stats[\'similarity_score_description\'],\n                      ""description"": f""A low value indicates a large number of values in this column are similar to values in the \\""{similarity_score_based_most_correlated_column}\\"" column"",\n                      ""warning"": col_stats[\'similarity_score_warning\']\n                })\n\n            icm[score.replace(\'_score\',\'\')] = {\n                ""score"": col_stats[score],\n                ""metrics"": metrics,\n                #""description"": col_stats[f\'{score}_description\'],\n                ""description"": simple_description,\n                ""warning"": col_stats[f\'{score}_warning\']\n            }\n\n        return icm\n\n    def get_model_data(self, model_name=None, lmd=None):\n        if model_name is None:\n            model_name = self.name\n\n        if lmd is None:\n            with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, f\'{model_name}_light_model_metadata.pickle\'), \'rb\') as fp:\n                lmd = pickle.load(fp)\n        # ADAPTOR CODE\n        amd = {}\n\n        if \'stats_v2\' in lmd:\n            amd[\'data_analysis_v2\'] = lmd[\'stats_v2\']\n\n        if lmd[\'current_phase\'] == MODEL_STATUS_TRAINED:\n            amd[\'status\'] = \'complete\'\n        elif lmd[\'current_phase\'] == MODEL_STATUS_ERROR:\n            amd[\'status\'] = \'error\'\n        else:\n            amd[\'status\'] = \'training\'\n\n        # Shared keys\n        for k in [\'name\', \'version\', \'is_active\', \'data_source\', \'predict\', \'current_phase\',\n        \'train_end_at\', \'updated_at\', \'created_at\',\'data_preparation\', \'validation_set_accuracy\']:\n            if k == \'predict\':\n                amd[k] = lmd[\'predict_columns\']\n            elif k in lmd:\n                amd[k] = lmd[k]\n                if k == \'validation_set_accuracy\':\n                    if lmd[\'validation_set_accuracy\'] is not None:\n                        amd[\'accuracy\'] = round(lmd[\'validation_set_accuracy\'],3)\n                    else:\n                        amd[\'accuracy\'] = None\n            else:\n                amd[k] = None\n\n        amd[\'data_analysis\'] = {\n            \'target_columns_metadata\': []\n            ,\'input_columns_metadata\': []\n        }\n\n        amd[\'model_analysis\'] = []\n\n        for col in lmd[\'model_columns_map\'].keys():\n            if col in lmd[\'columns_to_ignore\']:\n                continue\n\n            try:\n                icm = self._adapt_column(lmd[\'column_stats\'][col],col)\n            except Exception as e:\n                icm = {\'column_name\': col}\n                #continue\n\n            amd[\'force_vectors\'] = {}\n            if col in lmd[\'predict_columns\']:\n                # Histograms for plotting the force vectors\n                if \'all_columns_prediction_distribution\' in lmd and lmd[\'all_columns_prediction_distribution\'] is not None:\n                    amd[\'force_vectors\'][col] = {}\n                    amd[\'force_vectors\'][col][\'normal_data_distribution\'] = lmd[\'all_columns_prediction_distribution\'][col]\n                    amd[\'force_vectors\'][col][\'normal_data_distribution\'][\'type\'] = \'categorical\'\n\n                    amd[\'force_vectors\'][col][\'missing_data_distribution\'] = {}\n                    for missing_column in lmd[\'columnless_prediction_distribution\'][col]:\n                        amd[\'force_vectors\'][col][\'missing_data_distribution\'][missing_column] = lmd[\'columnless_prediction_distribution\'][col][missing_column]\n                        amd[\'force_vectors\'][col][\'missing_data_distribution\'][missing_column][\'type\'] = \'categorical\'\n\n                    icm[\'importance_score\'] = None\n                amd[\'data_analysis\'][\'target_columns_metadata\'].append(icm)\n\n                if \'confusion_matrices\' in lmd and col in lmd[\'confusion_matrices\']:\n                    confusion_matrix = lmd[\'confusion_matrices\'][col]\n                else:\n                    confusion_matrix = None\n\n                if \'accuracy_samples\' in lmd and col in lmd[\'accuracy_samples\']:\n                    accuracy_samples = lmd[\'accuracy_samples\'][col]\n                else:\n                    accuracy_samples = None\n\n\n\n                # Model analysis building for each of the predict columns\n                mao = {\n                    \'column_name\': col\n                    ,\'overall_input_importance\': {\n                        ""type"": ""categorical""\n                        ,""x"": []\n                        ,""y"": []\n                    }\n                  ,""train_accuracy_over_time"": {\n                    ""type"": ""categorical"",\n                    ""x"": [],\n                    ""y"": []\n                  }\n                  ,""test_accuracy_over_time"": {\n                    ""type"": ""categorical"",\n                    ""x"": [],\n                    ""y"": []\n                  }\n                  ,""accuracy_histogram"": {\n                        ""x"": []\n                        ,""y"": []\n                        ,\'x_explained\': []\n                  }\n                  ,""confusion_matrix"": confusion_matrix\n                  ,""accuracy_samples"": accuracy_samples\n                }\n\n\n                # This is a check to see if model analysis has run on this data\n                if \'model_accuracy\' in lmd and lmd[\'model_accuracy\'] is not None and \'train\' in lmd[\'model_accuracy\'] and \'combined\' in lmd[\'model_accuracy\'][\'train\'] and lmd[\'model_accuracy\'][\'train\'][\'combined\'] is not None:\n                    train_acc = lmd[\'model_accuracy\'][\'train\'][\'combined\']\n                    test_acc = lmd[\'model_accuracy\'][\'test\'][\'combined\']\n\n                    for i in range(0,len(train_acc)):\n                        mao[\'train_accuracy_over_time\'][\'x\'].append(i)\n                        mao[\'train_accuracy_over_time\'][\'y\'].append(train_acc[i])\n\n                    for i in range(0,len(test_acc)):\n                        mao[\'test_accuracy_over_time\'][\'x\'].append(i)\n                        mao[\'test_accuracy_over_time\'][\'y\'].append([i])\n\n                if \'model_accuracy\' in lmd and lmd[\'model_accuracy\'] is not None and lmd[\'column_importances\'] is not None:\n                    mao[\'accuracy_histogram\'][\'x\'] = [f\'{x}\' for x in lmd[\'accuracy_histogram\'][col][\'buckets\']]\n                    mao[\'accuracy_histogram\'][\'y\'] = lmd[\'accuracy_histogram\'][col][\'accuracies\']\n\n                    if lmd[\'columns_buckets_importances\'] is not None and col in lmd[\'columns_buckets_importances\']:\n                        for output_col_bucket in lmd[\'columns_buckets_importances\'][col]:\n                            x_explained_member = []\n                            for input_col in lmd[\'columns_buckets_importances\'][col][output_col_bucket]:\n                                stats = lmd[\'columns_buckets_importances\'][col][output_col_bucket][input_col]\n                                adapted_sub_incol = self._adapt_column(stats, input_col)\n                                x_explained_member.append(adapted_sub_incol)\n                            mao[\'accuracy_histogram\'][\'x_explained\'].append(x_explained_member)\n\n                    for icol in lmd[\'model_columns_map\'].keys():\n                        if icol in lmd[\'columns_to_ignore\']:\n                            continue\n                        if icol not in lmd[\'predict_columns\']:\n                            try:\n                                mao[\'overall_input_importance\'][\'x\'].append(icol)\n                                mao[\'overall_input_importance\'][\'y\'].append(round(lmd[\'column_importances\'][icol],1))\n                            except:\n                                print(f\'No column importances found for {icol} !\')\n\n                amd[\'model_analysis\'].append(mao)\n            else:\n                if \'column_importances\' in lmd and lmd[\'column_importances\'] is not None:\n                    icm[\'importance_score\'] = lmd[\'column_importances\'][col]\n                amd[\'data_analysis\'][\'input_columns_metadata\'].append(icm)\n\n        return amd\n\n    def export(self, mindsdb_storage_dir=\'mindsdb_storage\'):\n        """"""\n        If you want to export this mindsdb\'s instance storage to a file\n\n        :param mindsdb_storage_dir: this is the full_path where you want to store a mind to, it will be a zip file\n        :return: bool (True/False) True if mind was exported successfully\n        """"""\n        try:\n            shutil.make_archive(base_name=mindsdb_storage_dir, format=\'zip\', root_dir=CONFIG.MINDSDB_STORAGE_PATH)\n            print(f\'Exported mindsdb storage to {mindsdb_storage_dir}.zip\')\n            return True\n        except:\n            return False\n\n    def export_model(self, model_name=None):\n        """"""\n        If you want to export a model to a file\n\n        :param model_name: this is the name of the model you wish to export (defaults to the name of the current Predictor)\n        :return: bool (True/False) True if mind was exported successfully\n        """"""\n        if model_name is None:\n            model_name = self.name\n        try:\n            storage_file = model_name + \'.zip\'\n            with zipfile.ZipFile(storage_file, \'w\') as zip_fp:\n                for file_name in [model_name + \'_heavy_model_metadata.pickle\', model_name + \'_light_model_metadata.pickle\', model_name + \'_lightwood_data\']:\n                    full_path = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, file_name)\n                    zip_fp.write(full_path, os.path.basename(full_path))\n\n                # If the backend is ludwig, save the ludwig files\n                try:\n                    ludwig_model_path = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, model_name + \'_ludwig_data\')\n                    for root, dirs, files in os.walk(ludwig_model_path):\n                        for file in files:\n                            full_path = os.path.join(root, file)\n                            zip_fp.write(full_path, full_path[len(CONFIG.MINDSDB_STORAGE_PATH):])\n                except:\n                    pass\n\n            print(f\'Exported model to {storage_file}\')\n            return True\n        except Exception as e:\n            print(e)\n            return False\n\n    def load(self, model_archive_path):\n        """"""\n        If you want to import a mindsdb instance storage from a file\n\n        :param mindsdb_storage_dir: full_path that contains your mindsdb predictor zip file\n        :return: bool (True/False) True if mind was importerd successfully\n        """"""\n        previous_models = os.listdir(CONFIG.MINDSDB_STORAGE_PATH)\n        shutil.unpack_archive(model_archive_path, extract_dir=CONFIG.MINDSDB_STORAGE_PATH)\n\n        new_model_files = set(os.listdir(CONFIG.MINDSDB_STORAGE_PATH)) - set(previous_models)\n        model_names = []\n        for file in new_model_files:\n            if \'_light_model_metadata.pickle\' in file:\n                model_name = file.replace(\'_light_model_metadata.pickle\', \'\')\n                model_names.append(model_name)\n\n\n        for model_name in model_names:\n            with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, model_name + \'_light_model_metadata.pickle\'), \'rb\') as fp:\n                lmd = pickle.load(fp)\n\n            if \'ludwig_data\' in lmd and \'ludwig_save_path\' in lmd[\'ludwig_data\']:\n                lmd[\'ludwig_data\'][\'ludwig_save_path\'] = str(os.path.join(CONFIG.MINDSDB_STORAGE_PATH,os.path.basename(lmd[\'ludwig_data\'][\'ludwig_save_path\'])))\n\n            if \'lightwood_data\' in lmd and \'save_path\' in lmd[\'lightwood_data\']:\n                lmd[\'lightwood_data\'][\'save_path\'] = str(os.path.join(CONFIG.MINDSDB_STORAGE_PATH,os.path.basename(lmd[\'lightwood_data\'][\'save_path\'])))\n\n            with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, model_name + \'_light_model_metadata.pickle\'), \'wb\') as fp:\n                pickle.dump(lmd, fp,protocol=pickle.HIGHEST_PROTOCOL)\n\n\n    def load_model(self, model_archive_path=None):\n        """"""\n        If you want to load a model to a file\n\n        :param model_archive_path: this is the path to the archive where your model resides\n        :return: bool (True/False) True if mind was importerd successfully\n        """"""\n        self.load(model_archive_path)\n\n    def rename_model(self, old_model_name, new_model_name):\n        """"""\n        If you want to export a model to a file\n\n        :param old_model_name: this is the name of the model you wish to rename\n        :param new_model_name: this is the new name of the model\n        :return: bool (True/False) True if mind was exported successfully\n        """"""\n\n        if old_model_name == new_model_name:\n            return True\n\n        moved_a_backend = False\n        for extension in [\'_lightwood_data\', \'_ludwig_data\']:\n            try:\n                shutil.move(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, old_model_name + extension), os.path.join(CONFIG.MINDSDB_STORAGE_PATH, new_model_name + extension))\n                moved_a_backend = True\n            except:\n                pass\n\n        if not moved_a_backend:\n            return False\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, old_model_name + \'_light_model_metadata.pickle\'), \'rb\') as fp:\n            lmd = pickle.load(fp)\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, old_model_name + \'_heavy_model_metadata.pickle\'), \'rb\') as fp:\n            hmd = pickle.load(fp)\n\n        lmd[\'name\'] = new_model_name\n        hmd[\'name\'] = new_model_name\n\n        renamed_one_backend = False\n        try:\n            lmd[\'ludwig_data\'][\'ludwig_save_path\'] = lmd[\'ludwig_data\'][\'ludwig_save_path\'].replace(old_model_name, new_model_name)\n            renamed_one_backend = True\n        except:\n            pass\n\n        try:\n            lmd[\'lightwood_data\'][\'save_path\'] = lmd[\'lightwood_data\'][\'save_path\'].replace(old_model_name, new_model_name)\n            renamed_one_backend = True\n        except:\n            pass\n\n        if not renamed_one_backend:\n            return False\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, new_model_name + \'_light_model_metadata.pickle\'), \'wb\') as fp:\n            pickle.dump(lmd, fp,protocol=pickle.HIGHEST_PROTOCOL)\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, new_model_name + \'_heavy_model_metadata.pickle\'), \'wb\') as fp:\n            pickle.dump(hmd, fp,protocol=pickle.HIGHEST_PROTOCOL)\n\n\n        os.remove(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, old_model_name + \'_light_model_metadata.pickle\'))\n        os.remove(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, old_model_name + \'_heavy_model_metadata.pickle\'))\n        return True\n\n\n    def delete_model(self, model_name=None):\n        """"""\n        If you want to export a model to a file\n\n        :param model_name: this is the name of the model you wish to export (defaults to the name of the current Predictor)\n        :return: bool (True/False) True if mind was exported successfully\n        """"""\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, model_name + \'_light_model_metadata.pickle\'), \'rb\') as fp:\n            lmd = pickle.load(fp)\n\n            try:\n                os.remove(lmd[\'lightwood_data\'][\'save_path\'])\n            except:\n                pass\n\n            try:\n                shutil.rmtree(lmd[\'ludwig_data\'][\'ludwig_save_path\'])\n            except:\n                pass\n\n        if model_name is None:\n            model_name = self.name\n        try:\n            for file_name in [model_name + \'_heavy_model_metadata.pickle\', model_name + \'_light_model_metadata.pickle\']:\n                os.remove(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, file_name))\n            return True\n        except Exception as e:\n            print(e)\n            return False\n\n    def analyse_dataset(self, from_data, sample_margin_of_error=0.005):\n        """"""\n        Analyse the particular dataset being given\n        """"""\n\n        from_ds = getDS(from_data)\n        transaction_type = TRANSACTION_ANALYSE\n        sample_confidence_level = 1 - sample_margin_of_error\n\n        heavy_transaction_metadata = {}\n        heavy_transaction_metadata[\'name\'] = self.name\n        heavy_transaction_metadata[\'from_data\'] = from_ds\n\n        light_transaction_metadata = {}\n        light_transaction_metadata[\'version\'] = str(__version__)\n        light_transaction_metadata[\'name\'] = self.name\n        light_transaction_metadata[\'model_columns_map\'] = from_ds._col_map\n        light_transaction_metadata[\'type\'] = transaction_type\n        light_transaction_metadata[\'sample_margin_of_error\'] = sample_margin_of_error\n        light_transaction_metadata[\'sample_confidence_level\'] = sample_confidence_level\n        light_transaction_metadata[\'model_is_time_series\'] = False\n        light_transaction_metadata[\'model_group_by\'] = []\n        light_transaction_metadata[\'model_order_by\'] = []\n        light_transaction_metadata[\'columns_to_ignore\'] = []\n        light_transaction_metadata[\'data_preparation\'] = {}\n        light_transaction_metadata[\'predict_columns\'] = []\n        light_transaction_metadata[\'empty_columns\'] = []\n\n        light_transaction_metadata[\'handle_foreign_keys\'] = True\n        light_transaction_metadata[\'force_categorical_encoding\'] = []\n        light_transaction_metadata[\'handle_text_as_categorical\'] = False\n\n        light_transaction_metadata[\'data_types\'] = {}\n        light_transaction_metadata[\'data_subtypes\'] = {}\n\n        Transaction(session=self, light_transaction_metadata=light_transaction_metadata, heavy_transaction_metadata=heavy_transaction_metadata, logger=self.log)\n        return self.get_model_data(model_name=None, lmd=light_transaction_metadata)\n\n\n    def learn(self, to_predict, from_data, test_from_data=None, group_by=None, window_size=None, order_by=None, sample_margin_of_error=0.005, ignore_columns=None, stop_training_in_x_seconds=None, stop_training_in_accuracy=None, backend=\'lightwood\', rebuild_model=True, use_gpu=None, disable_optional_analysis=False, equal_accuracy_for_all_output_categories=True, output_categories_importance_dictionary=None, unstable_parameters_dict=None):\n        """"""\n        Learn to predict a column or columns from the data in \'from_data\'\n\n        Mandatory arguments:\n        :param to_predict: what column or columns you want to predict\n        :param from_data: the data that you want to learn from, this can be either a file, a pandas data frame, or url or a mindsdb data source\n\n        Optional arguments:\n        :param test_from_data: If you would like to test this learning from a different data set\n\n        Optional Time series arguments:\n        :param order_by: this order by defines the time series, it can be a list. By default it sorts each sort by column in ascending manner, if you want to change this pass a touple (\'column_name\', \'boolean_for_ascending <default=true>\')\n        :param group_by: This argument tells the time series that it should learn by grouping rows by a given id\n        :param window_size: The number of samples to learn from in the time series\n\n        Optional data transformation arguments:\n        :param ignore_columns: mindsdb will ignore this column\n\n        Optional sampling parameters:\n        :param sample_margin_of_error (DEFAULT 0): Maximum expected difference between the true population parameter, such as the mean, and the sample estimate.\n\n        Optional debug arguments:\n        :param stop_training_in_x_seconds: (default None), if set, you want training to finish in a given number of seconds\n\n        :return:\n        """"""\n\n        if ignore_columns is None:\n            ignore_columns = []\n\n        if group_by is None:\n            group_by = []\n\n        if order_by is None:\n            order_by = []\n\n        # lets turn into lists: predict, ignore, group_by, order_by\n        predict_columns = to_predict if isinstance(to_predict, list) else [to_predict]\n        ignore_columns = ignore_columns if isinstance(ignore_columns, list) else [ignore_columns]\n        group_by = group_by if isinstance(group_by, list) else [group_by]\n        order_by = order_by if isinstance(order_by, list) else [order_by]\n\n        # lets turn order by into list of tuples if not already\n        # each element (\'column_name\', \'boolean_for_ascending <default=true>\')\n        order_by = [col_name if isinstance(col_name, tuple) else (col_name, True) for col_name in order_by]\n\n        if unstable_parameters_dict is None:\n            unstable_parameters_dict = {}\n\n        from_ds = getDS(from_data)\n\n        test_from_ds = None if test_from_data is None else getDS(test_from_data)\n\n        transaction_type = TRANSACTION_LEARN\n        sample_confidence_level = 1 - sample_margin_of_error\n\n        if len(predict_columns) == 0:\n            error = \'You need to specify a column to predict\'\n            self.log.error(error)\n            raise ValueError(error)\n\n        is_time_series = True if len(order_by) > 0 else False\n\n        \'\'\'\n        We don\'t implement ""name"" as a concept in mindsdbd data sources, this is only available for files,\n        the server doesn\'t handle non-file data sources at the moment, so this shouldn\'t prove an issue,\n        once we want to support datasources such as s3 and databases for the server we need to add name as a concept (or, preferably, before that)\n        \'\'\'\n        data_source_name = from_data if isinstance(from_data, str) else \'Unkown\'\n\n        heavy_transaction_metadata = {}\n        heavy_transaction_metadata[\'name\'] = self.name\n        heavy_transaction_metadata[\'from_data\'] = from_ds\n        heavy_transaction_metadata[\'test_from_data\'] = test_from_ds\n        heavy_transaction_metadata[\'bucketing_algorithms\'] = {}\n        heavy_transaction_metadata[\'predictions\'] = None\n        heavy_transaction_metadata[\'model_backend\'] = backend\n\n        light_transaction_metadata = {}\n        light_transaction_metadata[\'version\'] = str(__version__)\n        light_transaction_metadata[\'name\'] = self.name\n        light_transaction_metadata[\'data_preparation\'] = {}\n        light_transaction_metadata[\'predict_columns\'] = predict_columns\n        light_transaction_metadata[\'model_columns_map\'] = from_ds._col_map\n        light_transaction_metadata[\'model_group_by\'] = group_by\n        light_transaction_metadata[\'model_order_by\'] = order_by\n        light_transaction_metadata[\'model_is_time_series\'] = is_time_series\n        light_transaction_metadata[\'data_source\'] = data_source_name\n        light_transaction_metadata[\'type\'] = transaction_type\n        light_transaction_metadata[\'window_size\'] = window_size\n        light_transaction_metadata[\'sample_margin_of_error\'] = sample_margin_of_error\n        light_transaction_metadata[\'sample_confidence_level\'] = sample_confidence_level\n        light_transaction_metadata[\'stop_training_in_x_seconds\'] = stop_training_in_x_seconds\n        light_transaction_metadata[\'rebuild_model\'] = rebuild_model\n        light_transaction_metadata[\'model_accuracy\'] = {\'train\': {}, \'test\': {}}\n        light_transaction_metadata[\'column_importances\'] = None\n        light_transaction_metadata[\'columns_buckets_importances\'] = None\n        light_transaction_metadata[\'columnless_prediction_distribution\'] = None\n        light_transaction_metadata[\'all_columns_prediction_distribution\'] = None\n        light_transaction_metadata[\'use_gpu\'] = use_gpu\n        light_transaction_metadata[\'columns_to_ignore\'] = ignore_columns\n        light_transaction_metadata[\'disable_optional_analysis\'] = disable_optional_analysis\n        light_transaction_metadata[\'validation_set_accuracy\'] = None\n        light_transaction_metadata[\'lightwood_data\'] = {}\n        light_transaction_metadata[\'ludwig_data\'] = {}\n        light_transaction_metadata[\'weight_map\'] = {}\n        light_transaction_metadata[\'confusion_matrices\'] = {}\n        light_transaction_metadata[\'empty_columns\'] = []\n        light_transaction_metadata[\'data_types\'] = {}\n        light_transaction_metadata[\'data_subtypes\'] = {}\n        \n        light_transaction_metadata[\'equal_accuracy_for_all_output_categories\'] = equal_accuracy_for_all_output_categories\n        light_transaction_metadata[\'output_categories_importance_dictionary\'] = output_categories_importance_dictionary if output_categories_importance_dictionary is not None else {}\n\n        if \'skip_model_training\' in unstable_parameters_dict:\n            light_transaction_metadata[\'skip_model_training\'] = unstable_parameters_dict[\'skip_model_training\']\n        else:\n            light_transaction_metadata[\'skip_model_training\'] = False\n\n        if \'skip_stats_generation\' in unstable_parameters_dict:\n            light_transaction_metadata[\'skip_stats_generation\'] = unstable_parameters_dict[\'skip_stats_generation\']\n        else:\n            light_transaction_metadata[\'skip_stats_generation\'] = False\n\n        if \'optimize_model\' in unstable_parameters_dict:\n            light_transaction_metadata[\'optimize_model\'] = unstable_parameters_dict[\'optimize_model\']\n        else:\n            light_transaction_metadata[\'optimize_model\'] = False\n\n        if \'force_disable_cache\' in unstable_parameters_dict:\n            light_transaction_metadata[\'force_disable_cache\'] = unstable_parameters_dict[\'force_disable_cache\']\n        else:\n            light_transaction_metadata[\'force_disable_cache\'] = False\n\n        if \'force_categorical_encoding\' in unstable_parameters_dict:\n            light_transaction_metadata[\'force_categorical_encoding\'] = unstable_parameters_dict[\'force_categorical_encoding\']\n        else:\n            light_transaction_metadata[\'force_categorical_encoding\'] = []\n\n        if \'handle_foreign_keys\' in unstable_parameters_dict:\n            light_transaction_metadata[\'handle_foreign_keys\'] = unstable_parameters_dict[\'handle_foreign_keys\']\n        else:\n            light_transaction_metadata[\'handle_foreign_keys\'] = False\n\n        if \'handle_text_as_categorical\' in unstable_parameters_dict:\n            light_transaction_metadata[\'handle_text_as_categorical\'] = unstable_parameters_dict[\'handle_text_as_categorical\']\n        else:\n            light_transaction_metadata[\'handle_text_as_categorical\'] = False\n\n        if \'use_selfaware_model\' in unstable_parameters_dict:\n            light_transaction_metadata[\'use_selfaware_model\'] = unstable_parameters_dict[\'use_selfaware_model\']\n        else:\n            light_transaction_metadata[\'use_selfaware_model\'] = True\n\n        if rebuild_model is False:\n            old_lmd = {}\n            for k in light_transaction_metadata: old_lmd[k] = light_transaction_metadata[k]\n\n            old_hmd = {}\n            for k in heavy_transaction_metadata: old_hmd[k] = heavy_transaction_metadata[k]\n\n            with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, light_transaction_metadata[\'name\'] + \'_light_model_metadata.pickle\'), \'rb\') as fp:\n                light_transaction_metadata = pickle.load(fp)\n\n            with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, heavy_transaction_metadata[\'name\'] + \'_heavy_model_metadata.pickle\'), \'rb\') as fp:\n                heavy_transaction_metadata= pickle.load(fp)\n\n            for k in [\'data_preparation\', \'rebuild_model\', \'data_source\', \'type\', \'columns_to_ignore\', \'sample_margin_of_error\', \'sample_confidence_level\', \'stop_training_in_x_seconds\']:\n                if old_lmd[k] is not None: light_transaction_metadata[k] = old_lmd[k]\n\n            for k in [\'from_data\', \'test_from_data\']:\n                if old_hmd[k] is not None: heavy_transaction_metadata[k] = old_hmd[k]\n        Transaction(session=self, light_transaction_metadata=light_transaction_metadata, heavy_transaction_metadata=heavy_transaction_metadata, logger=self.log)\n\n    def test(self, when_data, accuracy_score_functions, score_using=\'predicted_value\', predict_args=None):\n        """"""\n        :param when_data: use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from\n        :param accuracy_score_functions: a single function or  a dictionary for the form `{f\'{target_name}\': acc_func}` for when we have multiple targets\n        :param score_using: what values from the `explanation` of the target to use in the score function, defaults to the\n        :param predict_args: dictionary of arguments to be passed to `predict`, e.g: `predict_args={\'use_gpu\': True}`\n\n        :return: a dictionary for the form `{f\'{target_name}_accuracy\': accuracy_func_return}`, e.g. {\'rental_price_accuracy\':0.99}\n        """"""\n        if predict_args is None:\n            predict_args = {}\n\n        predictions = self.predict(when_data=when_data, **predict_args)\n\n        with open(os.path.join(CONFIG.MINDSDB_STORAGE_PATH, f\'{self.name}_light_model_metadata.pickle\'), \'rb\') as fp:\n            lmd = pickle.load(fp)\n\n        accuracy_dict = {}\n        for col in lmd[\'predict_columns\']:\n            if isinstance(accuracy_score_functions, dict):\n                acc_f = accuracy_score_functions[col]\n            else:\n                acc_f = accuracy_score_functions\n\n            accuracy_dict[f\'{col}_accuracy\'] = acc_f([x[f\'__observed_{col}\'] for x in predictions], [x.explanation[col][score_using] for x in predictions])\n\n        return accuracy_dict\n\n\n    def predict(self, when=None, when_data=None, update_cached_model = False, use_gpu=None, unstable_parameters_dict=None, backend=None, run_confidence_variation_analysis=False):\n        """"""\n        You have a mind trained already and you want to make a prediction\n\n        :param when: use this if you have certain conditions for a single prediction\n        :param when_data: use this when you have data in either a file, a pandas data frame, or url to a file that you want to predict from\n        :param update_cached_model: (optional, default:False) when you run predict for the first time, it loads the latest model in memory, you can force it to do this on this run by flipping it to True\n        :param run_confidence_variation_analysis: Run a confidence variation analysis on each of the given input column, currently only works when making single predictions via `when`\n\n        :return: TransactionOutputData object\n        """"""\n\n        if unstable_parameters_dict is None:\n            unstable_parameters_dict = {}\n\n        if run_confidence_variation_analysis is True and when_data is not None:\n            error_msg = \'run_confidence_variation_analysis=True is a valid option only when predicting a single data point via `when`\'\n            self.log.error(error_msg)\n            raise ValueError(error_msg)\n\n        transaction_type = TRANSACTION_PREDICT\n        when_ds = None if when_data is None else getDS(when_data)\n\n        # lets turn into lists: when\n        when = [when] if isinstance(when, dict) else when if when is not None else []\n\n        heavy_transaction_metadata = {}\n        if when_ds is None:\n            heavy_transaction_metadata[\'when_data\'] = None\n        else:\n            heavy_transaction_metadata[\'when_data\'] = when_ds\n        heavy_transaction_metadata[\'model_when_conditions\'] = when\n        heavy_transaction_metadata[\'name\'] = self.name\n\n        if backend is not None:\n            heavy_transaction_metadata[\'model_backend\'] = backend\n\n        light_transaction_metadata = {}\n        light_transaction_metadata[\'name\'] = self.name\n        light_transaction_metadata[\'type\'] = transaction_type\n        light_transaction_metadata[\'use_gpu\'] = use_gpu\n        light_transaction_metadata[\'data_preparation\'] = {}\n        light_transaction_metadata[\'run_confidence_variation_analysis\'] = run_confidence_variation_analysis\n\n        if \'force_disable_cache\' in unstable_parameters_dict:\n            light_transaction_metadata[\'force_disable_cache\'] = unstable_parameters_dict[\'force_disable_cache\']\n        else:\n            light_transaction_metadata[\'force_disable_cache\'] = False\n\n        transaction = Transaction(session=self, light_transaction_metadata=light_transaction_metadata, heavy_transaction_metadata=heavy_transaction_metadata)\n\n        return transaction.output_data\n'"
mindsdb/libs/controllers/transaction.py,0,"b'from mindsdb.libs.helpers.general_helpers import unpickle_obj\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.helpers.general_helpers import *\nfrom mindsdb.libs.data_types.transaction_data import TransactionData\nfrom mindsdb.libs.data_types.transaction_output_data import PredictTransactionOutputData, TrainTransactionOutputData\nfrom mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.helpers.probabilistic_validator import ProbabilisticValidator\nfrom mindsdb.config import CONFIG\n\nimport time\nimport _thread\nimport traceback\nimport importlib\nimport copy\nimport pickle\nimport datetime\nimport sys\nfrom copy import deepcopy\nimport pandas as pd\nimport numpy as np\n\nclass Transaction:\n\n    def __init__(self, session, light_transaction_metadata, heavy_transaction_metadata, logger =  log):\n        """"""\n        A transaction is the interface to start some MindsDB operation within a session\n\n        :param session:\n        :type session: utils.controllers.session_controller.SessionController\n        :param transaction_type:\n        :param transaction_metadata:\n        :type transaction_metadata: dict\n        :type heavy_transaction_metadata: dict\n        """"""\n\n        self.session = session\n        self.lmd = light_transaction_metadata\n        self.lmd[\'created_at\'] = str(datetime.datetime.now())\n        self.hmd = heavy_transaction_metadata\n\n        # variables to de defined by setup\n        self.error = None\n        self.errorMsg = None\n\n        self.input_data = TransactionData()\n        self.output_data = TrainTransactionOutputData()\n\n        # variables that can be persisted\n\n\n        self.log = logger\n\n        self.run()\n\n\n    def load_metadata(self):\n        try:\n            import resource\n            resource.setrlimit(resource.RLIMIT_STACK, [0x10000000, resource.RLIM_INFINITY])\n            sys.setrecursionlimit(0x100000)\n        except:\n            pass\n\n\n        fn = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.lmd[\'name\'] + \'_light_model_metadata.pickle\')\n        try:\n            with open(fn, \'rb\') as fp:\n                self.lmd = pickle.load(fp)\n        except Exception as e:\n            self.log.error(e)\n            self.log.error(f\'Could not load mindsdb light metadata from the file: {fn}\')\n\n        fn = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.hmd[\'name\'] + \'_heavy_model_metadata.pickle\')\n        try:\n            with open(fn, \'rb\') as fp:\n                self.hmd = pickle.load(fp)\n        except Exception as e:\n            self.log.error(e)\n            self.log.error(f\'Could not load mindsdb heavy metadata in the file: {fn}\')\n\n\n    def save_metadata(self):\n        fn = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.lmd[\'name\'] + \'_light_model_metadata.pickle\')\n        self.lmd[\'updated_at\'] = str(datetime.datetime.now())\n        try:\n            with open(fn, \'wb\') as fp:\n                pickle.dump(self.lmd, fp,protocol=pickle.HIGHEST_PROTOCOL)\n        except Exception as e:\n            self.log.error(e)\n            self.log.error(f\'Could not save mindsdb light metadata in the file: {fn}\')\n\n        fn = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, self.hmd[\'name\'] + \'_heavy_model_metadata.pickle\')\n        save_hmd = {}\n        null_out_fields = [\'test_from_data\', \'from_data\']\n        for k in null_out_fields:\n            save_hmd[k] = None\n\n\n        for k in self.hmd:\n            if k not in null_out_fields:\n                save_hmd[k] = self.hmd[k]\n            if k == \'model_backend\' and not isinstance(self.hmd[\'model_backend\'], str):\n                save_hmd[k] = None\n\n        try:\n            with open(fn, \'wb\') as fp:\n                # Don\'t save data for now\n                pickle.dump(save_hmd, fp,protocol=pickle.HIGHEST_PROTOCOL)\n        except Exception as e:\n            self.log.error(e)\n            self.log.error(f\'Could not save mindsdb heavy metadata in the file: {fn}\')\n\n    def _call_phase_module(self, module_name, **kwargs):\n        """"""\n        Loads the module and runs it\n\n        :param module_name:\n        :return:\n        """"""\n\n        self.lmd[\'is_active\'] = True\n        module_path = convert_cammelcase_to_snake_string(module_name)\n        module_full_path = \'mindsdb.libs.phases.{module_path}.{module_path}\'.format(module_path=module_path)\n        try:\n            main_module = importlib.import_module(module_full_path)\n            module = getattr(main_module, module_name)\n            return module(self.session, self)(**kwargs)\n        except:\n            error = \'Could not load module {module_name}\'.format(module_name=module_name)\n            self.log.error(\'Could not load module {module_name}\'.format(module_name=module_name))\n            self.log.error(traceback.format_exc())\n            raise Exception(error)\n        finally:\n            self.lmd[\'is_active\'] = False\n\n    def _execute_analyze(self):\n        self.lmd[\'current_phase\'] = MODEL_STATUS_PREPARING\n        self.save_metadata()\n\n        self._call_phase_module(module_name=\'DataExtractor\')\n        self.save_metadata()\n\n        self._call_phase_module(module_name=\'DataCleaner\', stage=0)\n        self.save_metadata()\n\n        self.lmd[\'current_phase\'] = MODEL_STATUS_DATA_ANALYSIS\n        self._call_phase_module(module_name=\'StatsGenerator\', input_data=self.input_data, hmd=self.hmd)\n        self.save_metadata()\n\n        self.lmd[\'current_phase\'] = MODEL_STATUS_DONE\n        self.save_metadata()\n        return\n\n    def _execute_learn(self):\n        """"""\n        :return:\n        """"""\n        try:\n            self.lmd[\'current_phase\'] = MODEL_STATUS_PREPARING\n            self.save_metadata()\n\n            self._call_phase_module(module_name=\'DataExtractor\')\n            self.save_metadata()\n\n            self._call_phase_module(module_name=\'DataCleaner\', stage=0)\n            self.save_metadata()\n\n            self.lmd[\'current_phase\'] = MODEL_STATUS_DATA_ANALYSIS\n            self._call_phase_module(module_name=\'StatsGenerator\', input_data=self.input_data, hmd=self.hmd)\n            self.save_metadata()\n\n            self._call_phase_module(module_name=\'DataCleaner\', stage=0)\n            self.save_metadata()\n\n            self._call_phase_module(module_name=\'DataSplitter\')\n\n            self._call_phase_module(module_name=\'DataTransformer\', input_data=self.input_data)\n            self.lmd[\'current_phase\'] = MODEL_STATUS_TRAINING\n            self.save_metadata()\n            self._call_phase_module(module_name=\'ModelInterface\', mode=\'train\')\n\n            self.lmd[\'current_phase\'] = MODEL_STATUS_ANALYZING\n            self.save_metadata()\n            self._call_phase_module(module_name=\'ModelAnalyzer\')\n\n            self.lmd[\'current_phase\'] = MODEL_STATUS_TRAINED\n            self.save_metadata()\n            return\n\n        except Exception as e:\n            self.lmd[\'is_active\'] = False\n            self.lmd[\'current_phase\'] = MODEL_STATUS_ERROR\n            self.lmd[\'error_msg\'] = traceback.print_exc()\n            self.log.error(str(e))\n            raise e\n\n\n    def _execute_predict(self):\n        """"""\n        :return:\n        """"""\n        old_lmd = {}\n        for k in self.lmd: old_lmd[k] = self.lmd[k]\n\n        old_hmd = {}\n        for k in self.hmd: old_hmd[k] = self.hmd[k]\n        self.load_metadata()\n\n        for k in old_lmd:\n            if old_lmd[k] is not None:\n                self.lmd[k] = old_lmd[k]\n            else:\n                if k not in self.lmd:\n                    self.lmd[k] = None\n\n        for k in old_hmd:\n            if old_hmd[k] is not None:\n                self.hmd[k] = old_hmd[k]\n            else:\n                if k not in self.hmd:\n                    self.hmd[k] = None\n\n        if self.lmd is None:\n            self.log.error(\'No metadata found for this model\')\n            return\n\n        self._call_phase_module(module_name=\'DataExtractor\')\n\n        if self.input_data.data_frame.shape[0] <= 0:\n            self.log.error(\'No input data provided !\')\n            return\n        if self.lmd[\'model_is_time_series\']:\n            self._call_phase_module(module_name=\'DataSplitter\')\n\n        # @TODO Maybe move to a separate ""PredictionAnalysis"" phase ?\n        if self.lmd[\'run_confidence_variation_analysis\']:\n            nulled_out_data = []\n            nulled_out_columns = []\n            for column in self.input_data.columns:\n                # Only adapted for a single `when`\n                if self.input_data.data_frame.iloc[0][column] is not None:\n                    nulled_out_data.append(self.input_data.data_frame.iloc[0].copy())\n                    nulled_out_data[-1][column] = None\n                    nulled_out_columns.append(column)\n\n            nulled_out_data = pd.DataFrame(nulled_out_data)\n\n        for mode in [\'predict\', \'analyze_confidence\']:\n            if mode == \'analyze_confidence\':\n                if not self.lmd[\'run_confidence_variation_analysis\']:\n                    continue\n                else:\n                    self.input_data.data_frame = nulled_out_data\n\n            self._call_phase_module(module_name=\'DataTransformer\', input_data=self.input_data)\n\n            self._call_phase_module(module_name=\'ModelInterface\', mode=\'predict\')\n\n            output_data = {col: [] for col in self.lmd[\'columns\']}\n\n            for column in self.input_data.columns:\n                if column in self.lmd[\'predict_columns\']:\n                    output_data[f\'__observed_{column}\'] = list(self.input_data.data_frame[column])\n                else:\n                    output_data[column] = list(self.input_data.data_frame[column])\n\n            for predicted_col in self.lmd[\'predict_columns\']:\n                output_data[predicted_col] = list(self.hmd[\'predictions\'][predicted_col])\n                for extra_column in [f\'{predicted_col}_model_confidence\', f\'{predicted_col}_confidence_range\']:\n                    if extra_column in self.hmd[\'predictions\']:\n                        output_data[extra_column] = self.hmd[\'predictions\'][extra_column]\n\n                probabilistic_validator = unpickle_obj(self.hmd[\'probabilistic_validators\'][predicted_col])\n                output_data[f\'{predicted_col}_confidence\'] = [None] * len(output_data[predicted_col])\n\n                output_data[f\'model_{predicted_col}\'] = deepcopy(output_data[predicted_col])\n                for row_number, predicted_value in enumerate(output_data[predicted_col]):\n\n                    # Compute the feature existance vector\n                    input_columns = [col for col in self.input_data.columns if col not in self.lmd[\'predict_columns\']]\n                    features_existance_vector = [False if  str(output_data[col][row_number]) in (\'None\', \'nan\', \'\', \'Nan\', \'NAN\', \'NaN\') else True for col in input_columns if col not in self.lmd[\'columns_to_ignore\']]\n\n                    # Create the probabilsitic evaluation\n                    probability_true_prediction = probabilistic_validator.evaluate_prediction_accuracy(features_existence=features_existance_vector, predicted_value=predicted_value)\n\n                    output_data[f\'{predicted_col}_confidence\'][row_number] = probability_true_prediction\n\n            if mode == \'predict\':\n                self.output_data = PredictTransactionOutputData(transaction=self, data=output_data)\n            else:\n                nulled_out_predictions = PredictTransactionOutputData(transaction=self, data=output_data)\n\n        if self.lmd[\'run_confidence_variation_analysis\']:\n            input_confidence = {}\n            extra_insights = {}\n\n            for predicted_col in self.lmd[\'predict_columns\']:\n                input_confidence[predicted_col] = {}\n                extra_insights[predicted_col] = {\'if_missing\':[]}\n\n                actual_confidence = self.output_data[0].explanation[predicted_col][\'confidence\']\n\n                for i, nulled_col_name in enumerate(nulled_out_columns):\n                    nulled_out_predicted_value = nulled_out_predictions[i].explanation[predicted_col][\'predicted_value\']\n                    nulled_confidence = nulled_out_predictions[i].explanation[predicted_col][\'confidence\']\n                    print(actual_confidence - nulled_confidence, actual_confidence, nulled_confidence)\n                    confidence_variation = actual_confidence - nulled_confidence\n\n                    input_confidence[predicted_col][nulled_col_name] = round(confidence_variation,3)\n                    extra_insights[predicted_col][\'if_missing\'].append({nulled_col_name: nulled_out_predicted_value})\n\n            self.output_data.input_confidence = input_confidence\n            self.output_data.extra_insights = extra_insights\n        return\n\n\n    def run(self):\n        """"""\n\n        :return:\n        """"""\n\n        if self.lmd[\'type\'] == TRANSACTION_BAD_QUERY:\n            self.log.error(self.errorMsg)\n            self.error = True\n            return\n\n        if self.lmd[\'type\'] == TRANSACTION_LEARN:\n            if CONFIG.EXEC_LEARN_IN_THREAD == False:\n                self._execute_learn()\n            else:\n                _thread.start_new_thread(self._execute_learn, ())\n            return\n\n        if self.lmd[\'type\'] == TRANSACTION_ANALYSE:\n            self._execute_analyze()\n\n        elif self.lmd[\'type\'] == TRANSACTION_PREDICT:\n            self._execute_predict()\n        elif self.lmd[\'type\'] == TRANSACTION_NORMAL_SELECT:\n            self._execute_normal_select()\n'"
mindsdb/libs/data_sources/__init__.py,0,b''
mindsdb/libs/data_sources/clickhouse_ds.py,0,"b'import pandas as pd\nimport requests\n\nfrom mindsdb.libs.data_types.data_source import DataSource\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n\nclass ClickhouseDS(DataSource):\n\n    def _setup(self, query, host=\'localhost\', user=\'default\', password=None, port=8123, protocol=\'http\'):\n        if protocol not in (\'https\', \'http\'):\n            raise ValueError(\'Unexpected protocol {}\'.fomat(protocol))\n\n        if \' format \' in query.lower():\n            err_msg = \'Please refrain from adding a ""FROAMT"" statement to the query\'\n            log.error(err_msg)\n            raise Exception(err_msg)\n        \n        query = f\'{query.rstrip("" ;"")} FORMAT JSON\'\n        log.info(f\'Getting data via the query: ""{query}""""\')\n\n        params = {\'user\': user}\n        if password is not None:\n            params[\'password\'] = password\n\n        response = requests.post(f\'{protocol}://{host}:{port}\', data=query, params=params)\n        \n        try:\n            data = response.json()[\'data\']\n        except:\n            log.error(f\'Got an invalid response from the database: {response.text}\')\n            raise Exception(response.text)\n\n        df = pd.DataFrame(data)\n        \n        col_map = {}\n        for col in df.columns:\n            col_map[col] = col\n\n        return df, col_map\n\nif __name__ == ""__main__"":\n    from mindsdb import Predictor\n\n    log.info(\'Starting ClickhouseDS tests !\')\n\n    log.info(\'Inserting data\')\n    requests.post(\'http://localhost:8123\', data=\'CREATE DATABASE IF NOT EXISTS test\')\n    requests.post(\'http://localhost:8123\', data=\'DROP TABLE IF EXISTS test.mock\')\n    requests.post(\'http://localhost:8123\', data=""""""CREATE TABLE test.mock(\n        col1 String\n        ,col2 Int64\n        ,col3 Array(UInt8)\n    ) ENGINE=Memory"""""")\n    requests.post(\'http://localhost:8123\', data=""""""INSERT INTO test.mock VALUES (\'a\',1,[1,2,3])"""""")\n    requests.post(\'http://localhost:8123\', data=""""""INSERT INTO test.mock VALUES (\'b\',2,[2,3,1])"""""")\n    requests.post(\'http://localhost:8123\', data=""""""INSERT INTO test.mock VALUES (\'c\',3,[3,1,2])"""""")\n\n    log.info(\'Querying data\')\n    clickhouse_ds = ClickhouseDS(\'SELECT * FROM test.mock ORDER BY col2 DESC LIMIT 2\')\n\n    log.info(\'Validating data integrity\')\n    assert(len(clickhouse_ds.df) == 2)\n    assert(sum(map(int,clickhouse_ds.df[\'col2\'])) == 5)\n    assert(len(list(clickhouse_ds.df[\'col3\'][1])) == 3)\n    assert(set(clickhouse_ds.df.columns) == set([\'col1\',\'col2\',\'col3\']))\n\n    mdb = Predictor(name=\'analyse_dataset_test_predictor\')\n    mdb.analyse_dataset(from_data=clickhouse_ds)\n\n    log.info(\'Finished running ClickhouseDS tests successfully !\')\n\n\n'"
mindsdb/libs/data_sources/file_ds.py,0,"b'import re\nfrom io import BytesIO, StringIO\nimport csv\nimport codecs\nimport json\nimport traceback\n\nimport pandas as pd\nfrom pandas.io.json import json_normalize\nimport requests\n\nfrom mindsdb.libs.data_types.data_source import DataSource\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n\nclass FileDS(DataSource):\n\n    def cleanRow(self, row):\n        n_row = []\n        for cell in row:\n            if str(cell) in [\'\', \' \', \'  \', \'NaN\', \'nan\', \'NA\']:\n                cell = None\n            n_row.append(cell)\n\n        return n_row\n\n    def _getDataIo(self, file):\n        """"""\n        This gets a file either url or local file and defiens what the format is as well as dialect\n        :param file: file path or url\n        :return: data_io, format, dialect\n        """"""\n\n        ############\n        # get file as io object\n        ############\n\n        data = BytesIO()\n\n        # get data from either url or file load in memory\n        if file.startswith(\'http:\') or file.startswith(\'https:\'):\n            r = requests.get(file, stream=True)\n            if r.status_code == 200:\n                for chunk in r:\n                    data.write(chunk)\n            data.seek(0)\n\n        # else read file from local file system\n        else:\n            try:\n                data = open(file, \'rb\')\n            except Exception as e:\n                error = \'Could not load file, possible exception : {exception}\'.format(exception = e)\n                log.error(error)\n                raise ValueError(error)\n\n\n        dialect = None\n\n        ############\n        # check for file type\n        ############\n\n        # try to guess if its an excel file\n        xlsx_sig = b\'\\x50\\x4B\\x05\\06\'\n        xlsx_sig2 = b\'\\x50\\x4B\\x03\\x04\'\n        xls_sig = b\'\\x09\\x08\\x10\\x00\\x00\\x06\\x05\\x00\'\n\n        # different whence, offset, size for different types\n        excel_meta = [ (\'xls\', 0, 512, 8), (\'xlsx\', 2, -22, 4)]\n\n        for filename, whence, offset, size in excel_meta:\n\n            try:\n                data.seek(offset, whence)  # Seek to the offset.\n                bytes = data.read(size)  # Capture the specified number of bytes.\n                data.seek(0)\n                codecs.getencoder(\'hex\')(bytes)\n\n                if bytes == xls_sig:\n                    return data, \'xls\', dialect\n                elif bytes == xlsx_sig:\n                    return data, \'xlsx\', dialect\n\n            except:\n                data.seek(0)\n\n        # if not excel it can be a json file or a CSV, convert from binary to stringio\n\n        byte_str = data.read()\n        # Move it to StringIO\n        try:\n            # Handle Microsoft\'s BOM ""special"" UTF-8 encoding\n            if byte_str.startswith(codecs.BOM_UTF8):\n                data = StringIO(byte_str.decode(\'utf-8-sig\'))\n            else:\n                data = StringIO(byte_str.decode(\'utf-8\'))\n\n        except:\n            log.error(traceback.format_exc())\n            log.error(\'Could not load into string\')\n\n        # see if its JSON\n        buffer = data.read(100)\n        data.seek(0)\n        text = buffer.strip()\n        # analyze first n characters\n        if len(text) > 0:\n            text = text.strip()\n            # it it looks like a json, then try to parse it\n            if text.startswith(\'{\') or text.startswith(\'[\'):\n                try:\n                    json.loads(data.read())\n                    data.seek(0)\n                    return data, \'json\', dialect\n                except:\n                    data.seek(0)\n                    return data, None, dialect\n\n        # lets try to figure out if its a csv\n        try:\n            data.seek(0)\n            first_few_lines = []\n            i = 0\n            for line in data:\n                if line in [\'\\r\\n\',\'\\n\']:\n                    continue\n                first_few_lines.append(line)\n                i += 1\n                if i > 0:\n                    break\n\n            accepted_delimiters = [\',\',\'\\t\', \';\']\n            dialect = csv.Sniffer().sniff(\'\'.join(first_few_lines[0]), delimiters=accepted_delimiters)\n            data.seek(0)\n            # if csv dialect identified then return csv\n            if dialect:\n                return data, \'csv\', dialect\n            else:\n                return data, None, dialect\n        except:\n            data.seek(0)\n            log.error(\'Could not detect format for this file\')\n            log.error(traceback.format_exc())\n            # No file type identified\n            return data, None, dialect\n\n\n\n\n    def _setup(self,file, clean_rows = True, custom_parser = None):\n        """"""\n        Setup from file\n        :param file: fielpath or url\n        :param clean_rows:  if you want to clean rows for strange null values\n        :param custom_parser: if you want to parse the file with some custom parser\n        """"""\n\n        col_map = {}\n        # get file data io, format and dialect\n        data, fmt, dialect = self._getDataIo(file)\n        data.seek(0) # make sure we are at 0 in file pointer\n\n        if custom_parser:\n            header, file_data = custom_parser(data, fmt)\n\n        elif fmt == \'csv\':\n            csv_reader = list(csv.reader(data, dialect))\n            header = csv_reader[0]\n            file_data =  csv_reader[1:]\n\n        elif fmt in [\'xlsx\', \'xls\']:\n            data.seek(0)\n            df = pd.read_excel(data)\n            header = df.columns.values.tolist()\n            file_data = df.values.tolist()\n\n        elif fmt == \'json\':\n            data.seek(0)\n            json_doc = json.loads(data.read())\n            df = json_normalize(json_doc)\n            header = df.columns.values.tolist()\n            file_data = df.values.tolist()\n        \n        else:\n            raise ValueError(\'Could not load file into any format, supported formats are csv, json, xls, xslx\')\n\n        for col in header:\n            col_map[col] = col\n\n        if clean_rows == True:\n            file_list_data = []\n            for row in file_data:\n                row = self.cleanRow(row)\n                file_list_data.append(row)\n        else:\n            file_list_data = file_data\n\n        try:\n            return pd.DataFrame(file_list_data, columns=header), col_map\n        except:\n            return pd.read_csv(file), col_map\n'"
mindsdb/libs/data_sources/mysql_ds.py,0,"b'import pandas as pd\nimport MySQLdb\n\nfrom mindsdb.libs.data_types.data_source import DataSource\n\n\nclass MySqlDS(DataSource):\n\n    def _setup(self, query=None, host=\'localhost\', user=\'root\', password=\'\', database=\'mysql\', port=3306, table=None):\n\n        if query is None:\n            query = f\'SELECT * FROM {table}\'\n\n        con = MySQLdb.connect(host, user, password, database, port=port)\n        df = pd.read_sql(query, con=con)\n        con.close()\n\n        col_map = {}\n        for col in df.columns:\n            col_map[col] = col\n\n        return df, col_map\n\nif __name__ == ""__main__"":\n    from mindsdb import Predictor\n\n    HOST = \'localhost\'\n    USER = \'root\'\n    PASSWORD = \'\'\n    DATABASE = \'mysql\'\n    PORT = 3306\n\n    con = MySQLdb.connect(HOST, USER, PASSWORD, DATABASE)\n    cur = con.cursor()\n\n    cur.execute(\'DROP TABLE IF EXISTS test_mindsdb\')\n    cur.execute(\'CREATE TABLE test_mindsdb(col_1 Text, col_2 BIGINT, col_3 BOOL)\')\n    for i in range(0,200):\n        cur.execute(f\'INSERT INTO test_mindsdb VALUES (""This is tring number {i}"", {i}, {i % 2 == 0})\')\n    con.commit()\n    con.close()\n\n    mysql_ds = MySqlDS(table=\'test_mindsdb\', host=HOST, user=USER, password=PASSWORD, database=DATABASE, port=PORT)\n    assert(len(mysql_ds._df) == 200)\n\n    mdb = Predictor(name=\'analyse_dataset_test_predictor\')\n    mdb.analyse_dataset(from_data=mysql_ds)\n'"
mindsdb/libs/data_sources/postgres_ds.py,0,"b'import os\n\nimport pandas as pd\nimport psycopg2\n\nfrom mindsdb.libs.data_types.data_source import DataSource\n\n\nclass PostgresDS(DataSource):\n\n    def _setup(self, query=None, host=\'localhost\', user=\'postgres\', password=\'\', database=\'postgres\', port=5432, table=None):\n\n        if query is None:\n            query = f\'SELECT * FROM {table}\'\n\n        con = psycopg2.connect(dbname=database, user=user, password=password, host=host, port=port)\n        df = pd.read_sql(query, con=con)\n        con.close()\n\n        col_map = {}\n        for col in df.columns:\n            col_map[col] = col\n\n        return df, col_map\n\nif __name__ == ""__main__"":\n    from mindsdb import Predictor\n\n    HOST = \'localhost\'\n    USER = \'postgres\'\n    PASSWORD = \'\'\n    DBNAME = \'postgres\'\n    PORT = 5432\n\n    con = psycopg2.connect(dbname=DBNAME, user=USER, password=PASSWORD, host=HOST, port=PORT)\n    cur = con.cursor()\n\n    cur.execute(\'DROP TABLE IF EXISTS test_mindsdb\')\n    cur.execute(\'CREATE TABLE test_mindsdb(col_1 Text, col_2 Int, col_3 Boolean)\')\n    for i in range(0,200):\n        cur.execute(f\'INSERT INTO test_mindsdb VALUES (\\\'This is tring number {i}\\\', {i}, {i % 2 == 0})\')\n    con.commit()\n    con.close()\n\n    mysql_ds = PostgresDS(table=\'test_mindsdb\', host=HOST, user=USER, password=PASSWORD, database=DBNAME, port=PORT)\n    assert(len(mysql_ds._df) == 200)\n\n    mdb = Predictor(name=\'analyse_dataset_test_predictor\')\n    mdb.analyse_dataset(from_data=mysql_ds)'"
mindsdb/libs/data_sources/s3_ds.py,0,"b'import os\n\nimport boto3\nfrom botocore import UNSIGNED\nfrom botocore.client import Config\n\n\nfrom mindsdb.libs.data_types.data_source import DataSource\nfrom mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.data_sources.file_ds import FileDS\n\n\nclass S3DS(DataSource):\n\n    def _setup(self, bucket_name, file_path, access_key=None, secret_key=None, use_default_credentails=False):\n        if access_key is not None and secret_key is not None:\n            s3 = boto3.client(\'s3\', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n        elif use_default_credentails:\n            s3 = boto3.client(\'s3\')\n        else:\n            s3 = boto3.client(\'s3\', config=Config(signature_version=UNSIGNED))\n\n        self.tmp_file_name = \'.tmp_mindsdb_data_file\'\n\n        with open(self.tmp_file_name, \'wb\') as fw:\n            s3.download_fileobj(bucket_name, file_path, fw)\n\n        file_ds = FileDS(self.tmp_file_name)\n        return file_ds._df, file_ds._col_map\n\n    def _cleanup(self):\n        os.remove(self.tmp_file_name)\n\nif __name__ == ""__main__"":\n    from mindsdb import Predictor\n    mdb = Predictor(name=\'analyse_dataset_test_predictor\')\n    s3_ds = S3DS(bucket_name=\'mindsdb-example-data\',file_path=\'home_rentals.csv\', access_key=None, secret_key=None)\n    mdb.analyse_dataset(from_data=s3_ds)'"
mindsdb/libs/data_types/__init__.py,0,b''
mindsdb/libs/data_types/data_source.py,0,"b'from mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.constants.mindsdb import DATA_TYPES_SUBTYPES, DATA_TYPES, DATA_SUBTYPES\n\n\nclass DataSource:\n\n    def __init__(self, *args, **kwargs):\n        self.log = log\n        self.data_types = {}\n        self.data_subtypes = {}\n        df, col_map = self._setup(*args, **kwargs)\n        self._set_df(df, col_map)\n        self._cleanup()\n\n    def _setup(self, df, **kwargs):\n        col_map = {}\n\n        for col in df.columns:\n            col_map[col] = col\n\n        return df, col_map\n\n    def _cleanup(self):\n        pass\n\n    @property\n    def df(self):\n        return self._df\n\n    def set_subtypes(self, data_subtypes):\n        if data_subtypes is not None:\n            for col in data_subtypes:\n                if col not in self._col_map:\n                    del data_subtypes[col]\n                    log.warning(f\'Column {col} not present in your data, ignoring the ""{data_subtypes[col]}"" subtype you specified for it\')\n\n            self.data_subtypes = data_subtypes\n            for col in self.data_subtypes:\n                col_subtype = self.data_subtypes[col]\n                if col_subtype not in [getattr(DATA_SUBTYPES,x) for x in DATA_SUBTYPES.__dict__ if \'__\' not in x]:\n                    raise Exception(f\'Invalid data subtype: {col_subtype}\')\n\n                for col_type in DATA_TYPES_SUBTYPES.subtypes:\n                    if col_subtype in DATA_TYPES_SUBTYPES.subtypes[col_type]:\n                        self.data_types[col] = col_type\n\n    def _set_df(self, df, col_map):\n\n        self._df = df\n        self._col_map = col_map\n\n    def dropColumns(self, column_list):\n        """"""\n        Drop columns by original names\n\n        :param column_list: a list of columns that you want to drop\n        :return: None\n        """"""\n\n        cols = [col if col not in self._col_map else self._col_map[col] for col in column_list]\n        self._df = self._df.drop(columns=cols)\n\n    def __getattr__(self, item):\n        """"""\n        Map all other functions to the DataFrame\n\n        :param item: the attribute to get\n        :return: the dataframe attribute\n        """"""\n\n        return getattr(self._df, item)\n\n\n    def __getitem__(self, key):\n        """"""\n        Map all other items to the DataFrame\n\n        :param key: the key to get\n        :return: the dataframe attribute\n        """"""\n        return self._df[key]\n\n\n    def __setitem__(self, key, value):\n        """"""\n        Support item assignment, mapped ot dataframe\n        :param key:\n        :param value:\n        :return:\n        """"""\n        self._df[key] = value\n'"
mindsdb/libs/data_types/mindsdb_logger.py,0,"b'import pprint\nimport logging\nimport colorlog\nimport uuid\n\nfrom mindsdb import CONFIG\nfrom mindsdb.libs.helpers.text_helpers import gen_chars\nfrom inspect import getframeinfo, stack\n\n\nclass MindsdbLogger():\n    internal_logger = None\n    id = None\n\n    def __init__(self, log_level, uuid):\n        \'\'\'\n        # Initialize the log module, should only be called once at the begging of the program\n\n        :param log_level: What logs to display\n        :param uuid: The unique id for this MindsDB instance or training/prediction session\n        \'\'\'\n\n        self.id = uuid\n        self.internal_logger = logging.getLogger(\'mindsdb-logger-{}\'.format(self.id))\n\n        self.internal_logger.handlers = []\n        self.internal_logger.propagate = False\n\n        stream_handler = logging.StreamHandler()\n        stream_handler.setFormatter(colorlog.ColoredFormatter(\'%(log_color)s%(levelname)s:%(name)s:%(message)s\'))\n        self.internal_logger.addHandler(stream_handler)\n\n        self.internal_logger.setLevel(log_level)\n\n\n    def log_message(self, message, func):\n        \'\'\'\n        # Internal function used for logging, adds the id and caller to the log and prettifies the message\n\n        :param message: message that the logger shoud log\n        :param chracter: logger function to use (example: \'info\' or \'error\')\n        \'\'\'\n        caller = getframeinfo(stack()[2][0])\n        #message = pprint.pformat(str(message))\n        message = str(message) + \'\\n\'\n\n        call = getattr(self.internal_logger, func)\n        call(""%s:%d - %s"" % (caller.filename.split(\'mindsdb/\')[-1], caller.lineno, message))\n\n    def debug(self, message):\n        self.log_message(message, \'debug\')\n\n    def info(self, message):\n        self.log_message(message, \'info\')\n\n    def warning(self, message):\n        self.log_message(message, \'warning\')\n\n    def error(self, message):\n        self.log_message(message, \'error\')\n\n    def infoChart(self, message, type, uid=None):\n        """"""\n        It will add the specific markdown plus tags or format it for stdout logs\n\n        :param message: its the chart payload\n        :param type: the type of chart\n        :param uid: the unique id of the chart so markdown can update properly\n\n        :return: None\n        """"""\n\n        if uid is None:\n            uid = str(uuid.uuid1())\n        else:\n            self.info(gen_chars(10, \'-\'))\n            if type in [\'pie\']:\n                total = sum([i[0] for i in message[\'subsets\']]) if \'total\' not in message else message[\'total\'][0]\n                max_len = max([len(i[1]) for i in message[\'subsets\']])\n                len_format = "" {: >"" + str(max_len) + ""}: ""\n\n                if \'label\' in message:\n                    label = message[\'label\']\n                    self.info(\'{label}\'.format(label=label))\n                for i in message[\'subsets\']:\n                    p = 100.0*i[0]/total\n                    l = int(p/5)\n                    info_str =len_format.format(i[1]) + ""[{:-<20}]"".format(gen_chars(l,\'#\')) + \' {val} ({p}% of Total)\'.format(label=str(i[1]), val=i[0], p=format(p, \'.2f\'))\n                    self.info(info_str)\n                if \'total\' in message:\n                    label = \'{label} ({count})\'.format(label=message[\'total\'][1], count=message[\'total\'][0])\n                    self.info(\' {label}\'.format(label=label))\n\n            if type in [\'histogram\']:\n\n                total = sum(message[\'y\'])\n                max_len = max([len(str(i)) for i in message[\'x\']])\n                len_format = "" {: >"" + str(max_len) + ""}: ""\n                max_val = max(message[\'y\'])\n\n                if \'label\' in message:\n                    label = message[\'label\']\n                    self.info(\'{label}\'.format(label=label))\n\n                for i,v in enumerate(message[\'y\']):\n                    p = 100.0 * v / max_val\n                    prob = 100.0 *v /total\n                    l = int(p / 5)\n                    info_str = len_format.format(message[\'x\'][i]) + ""[{: <20}"".format(\n                        gen_chars(l, \'#\')) + \'  ({p}% likely)\'.format(label=message[\'x\'][i], p=format(prob, \'.2f\'))\n                    self.info(info_str)\n\n\n\n            elif type in [\'list\']:\n                max_len = max([len(i) for i in message.keys()])\n                len_format = "" {: >"" + str(max_len) + ""}: ""\n                for key in message:\n                    self.info(len_format.format(key) + \'{val}\'.format(val=message[key]))\n            else:\n                self.info(message)\n                self.info(\'info type: {type}\'.format(type=type))\n            self.info(gen_chars(10, \'-\'))\n\n\n\nmain_logger_uuid = \'core-logger\'\nlog = MindsdbLogger(log_level=CONFIG.DEFAULT_LOG_LEVEL, uuid=main_logger_uuid)\n'"
mindsdb/libs/data_types/transaction_data.py,0,b'\nclass TransactionData():\n\n    def __init__(self):\n        self.data_frame = None\n        self.train_df = None\n        self.test_df = None\n        self.validation_df = None\n        self.columns = []\n'
mindsdb/libs/data_types/transaction_output_data.py,0,"b""from mindsdb.libs.constants.mindsdb import *\n\nfrom mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.data_types.transaction_output_row import TransactionOutputRow\n\n\nclass TrainTransactionOutputData():\n    def __init__(self):\n        self.data_frame = None\n        self.columns = None\n\n\nclass PredictTransactionOutputData():\n    def __init__(self, transaction, data):\n        self.data = data\n        self.transaction = transaction\n        self.input_confidence = None\n        self.extra_insights = None\n\n    def __iter__(self):\n        for i, value in enumerate(self.data[self.transaction.lmd['columns'][0]]):\n            yield TransactionOutputRow(self, i)\n\n    def __getitem__(self, item):\n        return TransactionOutputRow(self, item)\n\n    def __str__(self):\n        return str(self.data)\n\n    def __len__(self):\n        return len(self.data[self.transaction.lmd['columns'][0]])\n"""
mindsdb/libs/data_types/transaction_output_row.py,0,"b'from mindsdb.libs.helpers.explain_prediction import explain_prediction, get_important_missing_cols\nfrom mindsdb.libs.constants.mindsdb import *\n\n\nclass TransactionOutputRow:\n    def __init__(self, transaction_output, row_index):\n        self.transaction_output = transaction_output\n        self.predict_columns = self.transaction_output.transaction.lmd[\'predict_columns\']\n        self.row_index = row_index\n        self.col_stats = self.transaction_output.transaction.lmd[\'column_stats\']\n        self.data = self.transaction_output.data\n        self.explanation = self.new_explain()\n\n    def __getitem__(self, item):\n        return self.data[item][self.row_index]\n\n    def __contains__(self, item):\n        return item in self.data.keys()\n\n    def new_explain(self):\n        answers = {}\n        for pred_col in self.predict_columns:\n            answers[pred_col] = {}\n\n            prediction_row = {col: self.data[col][self.row_index] for col in self.data.keys()}\n\n            answers[pred_col][\'predicted_value\'] = prediction_row[pred_col]\n\n            if f\'{pred_col}_model_confidence\' in prediction_row:\n                answers[pred_col][\'confidence\'] = round((prediction_row[f\'{pred_col}_model_confidence\'] * 3 + prediction_row[f\'{pred_col}_confidence\'] * 1)/4, 4)\n            else:\n                answers[pred_col][\'confidence\'] = prediction_row[f\'{pred_col}_confidence\']\n\n            quality = \'very confident\'\n            if answers[pred_col][\'confidence\'] < 0.8:\n                quality = \'confident\'\n            if answers[pred_col][\'confidence\'] < 0.6:\n                quality = \'somewhat confident\'\n            if answers[pred_col][\'confidence\'] < 0.4:\n                quality = \'not very confident\'\n            if answers[pred_col][\'confidence\'] < 0.2:\n                quality = \'not confident\'\n\n            answers[pred_col][\'explanation\'] = {\n                \'prediction_quality\': quality\n            }\n\n            if self.col_stats[pred_col][\'data_type\'] in (DATA_TYPES.NUMERIC, DATA_TYPES.DATE):\n                if f\'{pred_col}_confidence_range\' in prediction_row:\n                    answers[pred_col][\'explanation\'][\'confidence_interval\'] = prediction_row[f\'{pred_col}_confidence_range\']\n\n            important_missing_cols = get_important_missing_cols(self.transaction_output.transaction.lmd, prediction_row, pred_col)\n            answers[pred_col][\'explanation\'][\'important_missing_information\'] = important_missing_cols\n\n            if self.transaction_output.input_confidence is not None:\n                answers[pred_col][\'explanation\'][\'confidence_composition\'] = {k:v for (k,v) in self.transaction_output.input_confidence[pred_col].items() if v > 0}\n\n            if self.transaction_output.extra_insights is not None:\n                answers[pred_col][\'explanation\'][\'extra_insights\'] = self.transaction_output.extra_insights[pred_col]\n\n            for k in answers[pred_col][\'explanation\']:\n                answers[pred_col][k] = answers[pred_col][\'explanation\'][k]\n\n        return answers\n\n    def explain(self):\n        answers = {}\n        for pred_col in self.predict_columns:\n            answers[pred_col] = []\n\n            prediction_row = {col: self.data[col][self.row_index] for col in list(self.data.keys())}\n\n            clusters = [{\'value\': prediction_row[pred_col], \'confidence\': prediction_row[f\'{pred_col}_confidence\']}]\n\n            for cluster in clusters:\n                pct_confidence = round(cluster[\'confidence\'] * 100)\n                predicted_value = cluster[\'value\']\n\n                if f\'{pred_col}_model_confidence\' in prediction_row:\n                    new_conf = round((prediction_row[f\'{pred_col}_model_confidence\'] * 3 + cluster[\'confidence\'] * 1)/4, 4)\n                else:\n                    new_conf = cluster[\'confidence\']\n\n                explanation = explain_prediction(self.transaction_output.transaction.lmd, prediction_row, cluster[\'confidence\'], pred_col)\n                answers[pred_col].append({\n                    \'value\': predicted_value,\n                    \'confidence\': new_conf,\n                    \'explanation\': explanation,\n                    \'explaination\': explanation,\n                    \'simple\': f\'We are {pct_confidence}% confident the value of ""{pred_col}"" is {predicted_value}\'\n                })\n\n                if self.transaction_output.input_confidence is not None:\n                    for i in range(len(answers[pred_col])):\n                        answers[pred_col][i][\'confidence_influence_scores\'] = {\n                            \'confidence_variation_score\': []\n                            ,\'column_names\': []\n                        }\n                        for c in self.transaction_output.input_confidence:\n                            answers[pred_col][i][\'confidence_influence_scores\'][\'confidence_variation_score\'].append(self.transaction_output.input_confidence[c])\n                            answers[pred_col][i][\'confidence_influence_scores\'][\'column_names\'].append(str(c))\n\n                model_result = {\n                    \'value\': prediction_row[f\'model_{pred_col}\']\n                }\n\n                if f\'{pred_col}_model_confidence\' in prediction_row:\n                    model_result[\'confidence\'] = prediction_row[f\'{pred_col}_model_confidence\']\n\n                answers[pred_col][-1][\'model_result\'] = model_result\n\n            answers[pred_col] = sorted(answers[pred_col], key=lambda x: x[\'confidence\'], reverse=True)\n\n        return answers\n\n    def epitomize(self):\n        answers = self.new_explain()\n        simple_answers = []\n\n        for pred_col in answers:\n            confidence = answers[pred_col][\'confidence\']\n            value = answers[pred_col][\'predicted_value\']\n            simple_col_answer = f\'We are {confidence}% confident the value of ""{pred_col}"" is {value}\'\n            simple_answers.append(simple_col_answer)\n\n        return \'* \' + \'\\n* \'.join(simple_answers)\n\n    def __str__(self):\n        return str(self.epitomize())\n\n    def as_dict(self):\n        return {key: self.data[key][self.row_index] for key in list(self.data.keys()) if not key.startswith(\'model_\')}\n\n    def as_list(self):\n        #Note that here we will not output the confidence columns\n        return [self.data[col][self.row_index] for col in list(self.data.keys()) if not col.startswith(\'model_\')]\n\n    def raw_predictions(self):\n        return {key: self.data[key][self.row_index] for key in list(self.data.keys()) if key.startswith(\'model_\')}\n\n    @property\n    def _predicted_values(self):\n        return {pred_col: evaluations[pred_col][self.row_index].predicted_value for pred_col in evaluations}\n'"
mindsdb/libs/helpers/__init__.py,0,b''
mindsdb/libs/helpers/debugging.py,0,"b""import os\n\n\ndef print_key_and_type(d, nl=''):\n    for k in d:\n        print(nl + str(k) + ' -- '+ str(type(d[k])))\n        if isinstance(d[k], dict):\n            print_key_and_type(d[k], nl + '  ')\n        if isinstance(d[k], list):\n            try:\n                if isinstance(d[k][0], dict):\n                    print('[\\n')\n                    print_key_and_type(d[k][0], nl + '  ')\n                    print('\\n]')\n                else:\n                    print(nl + '[' + str(type(d[k][0])) + ']')\n            except:\n                print('Empty list for key: ' + k)\n"""
mindsdb/libs/helpers/explain_prediction.py,0,"b'import numpy as np\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.helpers.general_helpers import value_isnan\n\n\ndef get_important_missing_cols(lmd, prediction_row, pred_col):\n    if lmd[\'column_importances\'] is None or len(lmd[\'column_importances\']) < 2:\n        important_cols = [col for col in lmd[\'columns\'] if col not in lmd[\'predict_columns\'] and not col.startswith(\'model_\')]\n    else:\n        top_30_val = np.percentile(list(lmd[\'column_importances\'].values()),70)\n        important_cols = [col for col in lmd[\'column_importances\'] if lmd[\'column_importances\'][col] >= top_30_val]\n\n    important_missing_cols = []\n    for col in important_cols:\n        if col not in prediction_row or prediction_row[col] is None or str(prediction_row[col]) == \'\'  or str(prediction_row[col]) == \'None\' or value_isnan(prediction_row[col]):\n                important_missing_cols.append(col)\n\n    return important_missing_cols\n\n\ndef explain_prediction(lmd, prediction_row, confidence, pred_col):\n    \'\'\'\n        Given a row contianing a prediction, it tries to use the information in it plus the metadata generated by the ModelAnalyzer and StatsGenerator in order to explain ""why"" the prediction took said value.\n        :param prediction_row: The row that was predicted by the model backend and processed by mindsdb\n        :return: A, hopefully human readable, string containing the explanation\n    \'\'\'\n    if lmd[\'column_importances\'] is None or len(lmd[\'column_importances\']) < 2:\n        important_cols = [col for col in lmd[\'columns\'] if col not in lmd[\'predict_columns\']]\n        useless_cols = []\n    else:\n        top_20_val = np.percentile(list(lmd[\'column_importances\'].values()),80)\n        bottom_20_val = np.percentile(list(lmd[\'column_importances\'].values()),20)\n\n        important_cols = [col for col in lmd[\'column_importances\'] if lmd[\'column_importances\'][col] >= top_20_val]\n        useless_cols = [col for col in lmd[\'column_importances\'] if lmd[\'column_importances\'][col] <= bottom_20_val]\n\n    explain_predictions = {}\n\n    predicted_val = prediction_row[pred_col]\n    col_stats = lmd[\'column_stats\'][pred_col]\n    col_type = col_stats[\'data_type\']\n\n    if \'histogram\' in col_stats and [\'histogram\'] is not None:\n        histogram_x = None\n        histogram_keys = col_stats[\'histogram\'][\'x\']\n        if col_type == DATA_TYPES.NUMERIC:\n            for i in range(len(histogram_keys)):\n                if i == 0:\n                    if histogram_keys[i] >= predicted_val:\n                        histogram_x = histogram_keys[0]\n                if i == len(histogram_keys) - 1:\n                        if histogram_keys[i] <= predicted_val:\n                            histogram_x = histogram_keys[0]\n                else:\n                    if histogram_keys[i] <= predicted_val or predicted_val > histogram_keys[i+1]:\n                        histogram_x = histogram_keys[0]\n        else:\n            histogram_x = predicted_val\n\n        bucket_occurances = col_stats[\'histogram\'][\'y\'][col_stats[\'histogram\'][\'x\'].index(histogram_x)]\n        total_occuraces = sum(col_stats[\'histogram\'][\'y\'])\n\n        percentage_bucket_percentage = round(100*bucket_occurances/total_occuraces, 2)\n\n        column_confidence = confidence * 100\n\n        confidence_str = \'very confident\'\n        if confidence < 0.80:\n            confidence_str = \'confident\'\n        if confidence < 0.60:\n            confidence_str = \'somewhat confident\'\n        if confidence < 0.40:\n            confidence_str = \'not very confident\'\n        if confidence < 0.20:\n            confidence_str = \'not confident\'\n\n        if percentage_bucket_percentage < 2:\n            column_explanation = f\'A similar value for the predicted column {pred_col} occurs rarely in your dataset\'\n\n            if column_confidence >= 70:\n                column_explanation += \', in spite of this, due to the quality of the input data and the model, we are very confident this prediction is correct.\'\n\n            if column_confidence < 70 and column_confidence > 30:\n                column_explanation += \', it is partially because of this reason that we are only somewhat confident this prediction is correct.\'\n\n            if column_confidence <= 30:\n                column_explanation += \', it is partially because of this reason that we aren\\\'t confident this prediction is correct.\'\n\n        elif percentage_bucket_percentage < 12.5:\n            column_explanation = f\'A similar value for the predicted column {pred_col} occurs a moderate amount of times in your dataset\'\n\n            if column_confidence >= 70:\n                column_explanation += \', we are very confident this prediction is correct.\'\n\n            if column_confidence < 70 and column_confidence > 30:\n                column_explanation += \', we are only somewhat confident this prediction is correct.\'\n\n            if column_confidence <= 30:\n                column_explanation += \', you\\\'r input data might be of sub-par qualkity, since we aren\\\'t confident this prediction is correct.\'\n\n        else:\n            column_explanation = f\'A similar value for the predicted column {pred_col} occurs very often in your dataset\'\n\n            if column_confidence >= 70:\n                column_explanation += \', it\\\'s partially because of this plaethora of examples that we can be very confident this prediction is correct.\'\n\n            if column_confidence < 70 and column_confidence > 30:\n                column_explanation += \', still we can only be somewhat confident this prediction is correct.\'\n\n            if column_confidence <= 30:\n                column_explanation += \', in spite of this, possibly due to poor input data quality, we aren\\\'t confident this prediction is correct.\'\n\n        explain_predictions[pred_col] = column_explanation\n\n    else:\n        explain_predictions[pred_col] = f\'Column {pred_col} is of type {col_type} and thus it\\\'s impossible for us to make statistical inferences about it.\'\n\n    explain_inputs = {}\n\n    for icol in important_cols:\n        if prediction_row[icol] is None or value_isnan(prediction_row[icol]):\n            explain_inputs[icol] = f\'The column {icol} is very important for this model to predict correctly. Since it\\\'s missing it\\\'s quite likely that the quality of this prediction is lacking because of this.\'\n        else:\n            explain_inputs[icol] = f\'The value of the column {icol} played a large role in generating this prediction.\'\n\n    for icol in useless_cols:\n        if prediction_row[icol] is None or value_isnan(prediction_row[icol]):\n            explain_inputs[icol] = f\'The fact that {icol} is missing is probably not very relevant for this prediction.\'\n        else:\n            explain_inputs[icol] = f\'The column {icol} is probably not very relevant for this prediction.\'\n\n    join_str = \'\\n\\n*\'\n    explanation = join_str + join_str.join(explain_predictions.values())\n    explanation += join_str + join_str.join(explain_inputs.values())\n    return explanation\n'"
mindsdb/libs/helpers/general_helpers.py,0,"b'import platform\nimport re\nimport pickle\nimport urllib\nimport requests\nfrom pathlib import Path\nimport uuid\nfrom contextlib import contextmanager\nimport os, sys\n\n\nfrom mindsdb.__about__ import __version__\nfrom mindsdb.config import CONFIG\nfrom mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.constants.mindsdb import *\nimport imagehash\nfrom PIL import Image\n\n\ndef check_for_updates():\n    """"""\n    Check for updates of mindsdb\n    it will ask the mindsdb server if there are new versions, if there are it will log a message\n\n    :return: None\n    """"""\n\n    # tmp files\n    uuid_file = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, \'..\', \'uuid.mdb_base\')\n    mdb_file = os.path.join(CONFIG.MINDSDB_STORAGE_PATH, \'start.mdb_base\')\n\n    if Path(uuid_file).is_file():\n        uuid_str = open(uuid_file).read()\n    else:\n        uuid_str = str(uuid.uuid4())\n        try:\n            with open(uuid_file, \'w\') as fp:\n                fp.write(uuid_str)\n        except:\n            log.warning(f\'Cannot store token, Please add write permissions to file: {uuid_file}\')\n            uuid_str = f\'{uuid_str}.NO_WRITE\'\n\n    if Path(mdb_file).is_file():\n        token = open(mdb_file, \'r\').read()\n    else:\n        if CONFIG.IS_CI_TEST:\n            uuid_str = \'travis\'\n        token = \'{system}|{version}|{uid}\'.format(system=platform.system(), version=__version__, uid=uuid_str)\n        try:\n            open(mdb_file,\'w\').write(token)\n        except:\n            log.warning(f\'Cannot store token, Please add write permissions to file: {mdb_file}\')\n            token = f\'{token}.NO_WRITE\'\n\n    try:\n        ret = requests.get(\'https://public.api.mindsdb.com/updates/check/{token}\'.format(token=token), headers={\'referer\': \'http://check.mindsdb.com/?token={token}\'.format(token=token)})\n        ret = ret.json()\n    except Exception as e:\n        try:\n            log.warning(f\'Got reponse: {ret} from update check server !\')\n        except:\n            log.warning(f\'Got no response from update check server !\')\n        log.warning(f\'Could not check for updates, got excetpion {e} !\')\n        return\n\n    try:\n        if \'version\' in ret and ret[\'version\']!= __version__:\n            log.warning(""There is a new version of MindsDB {version}, please upgrade using:\\npip3 uninstall mindsdb --upgrade"".format(version=ret[\'version\']))\n        else:\n            log.debug(\'MindsDB is up to date!\')\n    except:\n        log.warning(\'could not check for MindsDB updates\')\n\n\ndef convert_cammelcase_to_snake_string(cammel_string):\n    """"""\n    Converts snake string to cammelcase\n\n    :param cammel_string: as described\n    :return: the snake string AsSaid -> as_said\n    """"""\n\n    s1 = re.sub(\'(.)([A-Z][a-z]+)\', r\'\\1_\\2\', cammel_string)\n    return re.sub(\'([a-z0-9])([A-Z])\', r\'\\1_\\2\', s1).lower()\n\n\ndef pickle_obj(object_to_pickle):\n    """"""\n    Returns a version of self that can be serialized into mongodb or tinydb\n    :return: The data of an object serialized via pickle and decoded as a latin1 string\n    """"""\n\n    return pickle.dumps(object_to_pickle,protocol=pickle.HIGHEST_PROTOCOL).decode(encoding=\'latin1\')\n\n\ndef unpickle_obj(pickle_string):\n    """"""\n    :param pickle_string: A latin1 encoded python str containing the pickle data\n    :return: Returns an object generated from the pickle string\n    """"""\n    return pickle.loads(pickle_string.encode(encoding=\'latin1\'))\n\n\ndef closest(arr, value):\n    """"""\n    :return: The index of the member of `arr` which is closest to `value`\n    """"""\n\n    if value == None:\n        return -1\n\n    for i,ele in enumerate(arr):\n        value = float(str(value).replace(\',\', \'.\'))\n        if ele > value:\n            return i - 1\n\n    return len(arr)-1\n\n\ndef get_value_bucket(value, buckets, col_stats, hmd=None):\n    """"""\n    :return: The bucket in the `histogram` in which our `value` falls\n    """"""\n    if buckets is None:\n        return None\n\n    if col_stats[\'data_subtype\'] in (DATA_SUBTYPES.SINGLE, DATA_SUBTYPES.MULTIPLE):\n        if value in buckets:\n            bucket = buckets.index(value)\n        else:\n            bucket = len(buckets) # for null values\n\n    elif col_stats[\'data_subtype\'] in (DATA_SUBTYPES.BINARY, DATA_SUBTYPES.INT, DATA_SUBTYPES.FLOAT):\n        bucket = closest(buckets, value)\n    elif col_stats[\'data_subtype\'] in (DATA_SUBTYPES.IMAGE) and hmd is not None:\n        bucket = hmd[\'bucketing_algorithms\'][col_name].predict(np.array(imagehash.phash(Image.open(value)).reshape(1, -1)))[0]\n    else:\n        bucket = len(buckets) # for null values\n\n    return bucket\n\n\ndef evaluate_accuracy(predictions, full_dataset, col_stats, output_columns, hmd=None):\n    score = 0\n    for output_column in output_columns:\n        cummulative_scores = 0\n        if \'percentage_buckets\' in col_stats[output_column]:\n            buckets = col_stats[output_column][\'percentage_buckets\']\n        else:\n            buckets = None\n\n        i = 0\n        for real_value in full_dataset[output_column]:\n            pred_val_bucket = get_value_bucket(predictions[output_column][i], buckets, col_stats[output_column], hmd)\n            if pred_val_bucket is None:\n                if predictions[output_column][i] == real_value:\n                    cummulative_scores += 1\n            elif pred_val_bucket == get_value_bucket(real_value, buckets, col_stats[output_column], hmd):\n                cummulative_scores += 1\n            i += 1\n\n        score += cummulative_scores/len(predictions[output_column])\n    score = score/len(output_columns)\n    if score == 0:\n        score = 0.00000001\n    return score\n\nclass suppress_stdout_stderr(object):\n    def __init__(self):\n        try:\n            crash = \'fileno\' in dir(sys.stdout)\n            # Open a pair of null files\n            self.null_fds =  [os.open(os.devnull,os.O_RDWR) for x in range(2)]\n            # Save the actual stdout (1) and stderr (2) file descriptors.\n            self.c_stdout = sys.stdout.fileno()\n            self.c_stderr = sys.stderr.fileno()\n\n            self.save_fds = [os.dup(self.c_stdout), os.dup(self.c_stderr)]\n        except:\n            print(\'Can\\\'t disable output on Jupyter notebook\')\n\n    def __enter__(self):\n        try:\n            crash = \'dup2\' in dir(os)\n            # Assign the null pointers to stdout and stderr.\n            os.dup2(self.null_fds[0],self.c_stdout)\n            os.dup2(self.null_fds[1],self.c_stderr)\n        except:\n            print(\'Can\\\'t disable output on Jupyter notebook\')\n\n    def __exit__(self, *_):\n        try:\n            crash = \'dup2\' in dir(os)\n            # Re-assign the real stdout/stderr back to (1) and (2)\n            os.dup2(self.save_fds[0],self.c_stdout)\n            os.dup2(self.save_fds[1],self.c_stderr)\n            # Close all file descriptors\n            for fd in self.null_fds + self.save_fds:\n                os.close(fd)\n        except:\n            print(\'Can\\\'t disable output on Jupyter notebook\')\n\ndef get_tensorflow_colname(col):\n    replace_chars = """""" ,./;\'[]!@#$%^&*()+{-=+~`}\\\\|:""<>?""""""\n\n    for char in replace_chars:\n        col = col.replace(char,\'_\')\n    col = re.sub(\'_+\',\'_\',col)\n\n    return col\n\n@contextmanager\ndef disable_console_output(activate=True):\n    try:\n        try:\n            old_tf_loglevel = os.environ[\'TF_CPP_MIN_LOG_LEVEL\']\n        except:\n            old_tf_loglevel = \'2\'\n        # Maybe get rid of this to not supress all errors and stdout\n        if activate:\n            with suppress_stdout_stderr():\n                yield\n        else:\n            yield\n    finally:\n        os.environ[\'TF_CPP_MIN_LOG_LEVEL\'] = old_tf_loglevel\n\n\ndef value_isnan(value):\n    try:\n        if isinstance(value, float):\n            a = int(value)\n        isnan = False\n    except:\n        isnan = True\n    return isnan\n'"
mindsdb/libs/helpers/multi_data_source.py,0,"b""from mindsdb.libs.data_types.data_source import DataSource\nfrom mindsdb.libs.data_sources.file_ds import FileDS\nfrom pandas import DataFrame\n\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n\ndef getDS(from_data):\n    '''\n    Get a datasource give the input\n\n    :param input: a string or an object\n    :return: a datasource\n    '''\n\n    if isinstance(from_data, DataSource):\n        from_ds = from_data\n\n    elif isinstance(from_data, DataFrame):\n        from_ds = DataSource(from_data)\n\n\n    else:  # assume is a file\n        from_ds = FileDS(from_data)\n        if from_ds is None:\n            log.error('No data matched the input data')\n\n    return from_ds\n"""
mindsdb/libs/helpers/parser.py,0,"b'from mindsdb.libs.data_types.mindsdb_logger import log\n\ntest = """"""\n\nSELECT DISTINCT c as cosa, col2,\ncol3 as ""year"", col5 as ""select"", * FROM (\n  select * FROM schemab.my_teable WHERE   c = 10 AND col3 < 5 and col2 in (20, 30) Or ( c in (SELECT count(distinct names) from name_list where area = 1) and col2 >= 5 )\n) table_2\n\nwhere (col4 > 10   AND col5  lIKE \'%noname\'\'myname )\'  ) OR ( col5 = \'(select * Fom ) ()\' AND cold3 = 2 )\n\n""""""\n\n\n#first step replace \'\' for __ESC_QUOTES__\n\n# replace all text with __VAR_varindex__ build a map\n\n# replace all double space  or greater with single space\n# replace all \'( \' and \' )\' with \'(\' \')\'\n# upper case all reserved words that are not enclosed by """"\n\n# replace each sub statement with  __SUB:INDEX.child.grandchild..___ build a map\n\n\n\ndef replaceTexts(str):\n    """"""\n    The whole point of this function is to\n    replace strings with __TEXTVAR_index__ and return a variable map dictionary\n\n    :param str: the string that we want to make replacements on\n    :return: replaced_str, text_var_map\n    """"""\n\n    # these are the variables to be returned\n    text_var_map = {}\n    ret = \'\'\n\n\n    inside = False # as we walk the string, if this is true is because we are inside a text variable\n    str_len = len(str)\n    collected_text = \'\' # as we find one text variable we collect it here\n    text_var_count = 0 # the variable count\n    skip_next = False\n\n    # here we walk oall the string in search for \' and store its contents replace the string and update the map\n    for i,c in enumerate(str):\n\n        if skip_next == True:\n            skip_next = False\n            continue\n\n        next = \'\' if i>=(str_len-1) else str[i+1]\n        if c == ""\'"" and not inside:\n            inside = True\n            text_var_count +=1\n            continue\n        if c == ""\'"" and inside and next!=""\'"":\n            inside = False\n            map_key = \'__TEXTVAR_{text_var_count}__\'.format(text_var_count=text_var_count)\n            text_var_map[map_key] = collected_text\n            collected_text = \'\'\n            ret += map_key\n            continue\n        elif c == ""\'"" and inside and next==""\'"":\n            skip_next = True\n            collected_text += ""\'""\n\n        if inside == True:\n            collected_text += c\n        else:\n            ret += c\n\n    return ret, text_var_map\n\n\ndef cleanStr(str):\n    """"""\n    Do some cleaning, remove double spaces new lines, clean commas and others\n    :param str:\n    :return:\n    """"""\n    str = str.replace(""\\n"", \' \')\n\n    clean_space_from = [\'( \', \' )\', \', \', \'[ \', \' ]\']\n    for str_to_replace in clean_space_from:\n        actual = str_to_replace.replace(\' \', \'\')\n        str = str.replace(actual, str_to_replace)\n\n    str = \' (\' + str + \') \'\n    str = \' \'.join(str.split())\n\n    return str\n\ndef replaceSubStatements(str, count = 0, node=\'0\'):\n\n    ret = \'\'\n\n    ret_map = {}\n\n    for i, c in enumerate(str):\n\n        if c == \'(\':\n            count += 1\n            subnode = \'.\'.join(node.split(\'.\')[-1])\n            node = \'{node}.{count}\'.format(node=subnode,count=count)\n\n            node_str, count,  map = replaceSubStatements(str[i+1:], count, node)\n            ret_map[node] = map\n\n            ret += \'__NODE:{node}__\'.format(node=node)\n\n            break\n\n        if c == \')\':\n            count -= 1\n\n            return ret, count, ret_map\n\n\n        ret += c\n\n    return ret, count, ret_map\n\n\n\ndef parse(str):\n\n    str, text_var_map = replaceTexts(str)\n    str = cleanStr(str)\n\n\n    log.info(str)\n\n\n\'\'\'\n ( select a from ( select u from b ) where u > 10 )\n\'\'\'\n\n\n\nmap = {\n\n    \'expr\': \'__0__\',\n    \'parts\': [\n        {\n            \'expr\': \' select a from __0__ where u > 10 \',\n            \'parts\': [\n                {\n                    \'expr\': \' select u from b \',\n                    \'parts\': []\n                }\n            ]\n        }\n    ]\n}\n\n\n\nparse(test)\n'"
mindsdb/libs/helpers/probabilistic_validator.py,0,"b'from mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.helpers.general_helpers import get_value_bucket\n\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.metrics import confusion_matrix\nimport numpy as np\nimport random\n\n\nclass ProbabilisticValidator():\n    """"""\n    # The probabilistic validator is a quick to train model used for validating the predictions of our main model\n    # It is fit to the results our model gets on the validation set\n    """"""\n    _probabilistic_model = None\n    _X_buff = None\n    _Y_buff = None\n\n\n    def __init__(self, col_stats, col_name, input_columns):\n        """"""\n        Chose the algorithm to use for the rest of the model\n        As of right now we go with BernoulliNB\xc2\xb6\n        """"""\n        self.col_stats = col_stats\n        self.col_name = col_name\n        self.input_columns = input_columns\n\n        if \'percentage_buckets\' in col_stats:\n            self.buckets = col_stats[\'percentage_buckets\']\n\n        self._probabilistic_model = BernoulliNB()\n\n    def fit(self, real_df, predictions_arr, missing_col_arr, hmd=None):\n        """"""\n        # Fit the probabilistic validator\n\n        :param real_df: A dataframe with the real inputs and outputs for every row\n        :param predictions_arr: An array containing arrays of predictions, one containing the ""normal"" predictions and the rest containing predictions with various missing column\n        :param missing_col_arr: The missing columns for each of the prediction arrays, same order as the arrays in `predictions_arr`, starting from the second element of `predictions_arr` (The first is assumed to have no missing columns)\n\n\n        """"""\n        self.real_values_bucketized = []\n        self.normal_predictions_bucketized = []\n        self.numerical_samples_arr = []\n\n        column_indexes = {}\n        for i, col in enumerate(self.input_columns):\n            column_indexes[col] = i\n\n        real_present_inputs_arr = []\n        for _, row in real_df.iterrows():\n            present_inputs = [1] * len(self.input_columns)\n            for i, col in enumerate(self.input_columns):\n                if str(row[col]) in (\'None\', \'nan\', \'\', \'Nan\', \'NAN\', \'NaN\'):\n                    present_inputs[i] = 0\n            real_present_inputs_arr.append(present_inputs)\n\n        X = []\n        Y = []\n\n        for n in range(len(predictions_arr)):\n            for m in range(len(real_df)):\n                row = real_df.iloc[m]\n                predicted_value = predictions_arr[n][self.col_name][m]\n\n                if f\'{self.col_name}_confidence_range\' in predictions_arr[n]:\n                    predicted_range = predictions_arr[n][f\'{self.col_name}_confidence_range\'][m]\n\n                real_value = row[self.col_name]\n                try:\n                    predicted_value = predicted_value if self.col_stats[\'data_type\'] != DATA_TYPES.NUMERIC else float(predicted_value)\n                except:\n                    predicted_value = None\n\n                try:\n                    real_value = real_value if self.col_stats[\'data_type\'] != DATA_TYPES.NUMERIC else float(str(real_value).replace(\',\',\'.\'))\n                except:\n                    real_value = None\n\n                if self.buckets is not None:\n                    predicted_value_b = get_value_bucket(predicted_value, self.buckets, self.col_stats, hmd)\n                    real_value_b = get_value_bucket(real_value, self.buckets, self.col_stats, hmd)\n                    X.append([0] * (len(self.buckets) + 1))\n                    X[-1][predicted_value_b] = 1\n                else:\n                    predicted_value_b = predicted_value\n                    real_value_b = real_value_b\n\n                    X.append([])\n\n                if self.col_stats[\'data_type\'] == DATA_TYPES.NUMERIC:\n                    Y.append(predicted_range[0] < real_value < predicted_range[1])\n                else:\n                    Y.append(real_value_b == predicted_value_b)\n\n                if n == 0:\n                    self.real_values_bucketized.append(real_value_b)\n                    self.normal_predictions_bucketized.append(predicted_value_b)\n                    if self.col_stats[\'data_type\'] == DATA_TYPES.NUMERIC:\n                        self.numerical_samples_arr.append((real_value,predicted_range))\n\n                feature_existance = real_present_inputs_arr[m]\n                if n > 0:\n                    for missing_col in missing_col_arr[n - 1]:\n                        feature_existance[self.input_columns.index(missing_col)] = 0\n\n                X[-1] += feature_existance\n\n        log_types = np.seterr()\n        np.seterr(divide=\'ignore\')\n        self._probabilistic_model.fit(X, Y)\n        np.seterr(divide=log_types[\'divide\'])\n\n    def evaluate_prediction_accuracy(self, features_existence, predicted_value):\n        """"""\n        # Fit the probabilistic validator on an observation\n        :param features_existence: A vector of 0 and 1 representing the existence of all the features (0 == not exists, 1 == exists)\n        :param predicted_value: The predicted value/label\n        :return: The probability (from 0 to 1) of our prediction being accurate (within the same histogram bucket as the real value)\n        """"""\n        if self.buckets is not None:\n            predicted_value_b = get_value_bucket(predicted_value, self.buckets, self.col_stats)\n            X = [0] * (len(self.buckets) + 1)\n            X[predicted_value_b] = 1\n            X = [X + features_existence]\n        else:\n            X = [features_existence]\n\n        try:\n            true_index = self._probabilistic_model.classes_.tolist().index(True)\n        except:\n            print(\'Only got classes: \', str(self._probabilistic_model.classes_.tolist()), \' in the probabilistic model\\\'s Y vector !\')\n            true_index = None\n\n        if true_index is None:\n            probability_true_prediction = 0\n        else:\n            probability_true_prediction = self._probabilistic_model.predict_proba(np.array(X))[0][true_index]\n\n        return probability_true_prediction\n\n\n    def get_accuracy_stats(self):\n\n        bucket_accuracy = {}\n        bucket_acc_counts = {}\n        for i, bucket in enumerate(self.normal_predictions_bucketized):\n            if bucket not in bucket_acc_counts:\n                bucket_acc_counts[bucket] = []\n\n            if len(self.numerical_samples_arr) != 0:\n                bucket_acc_counts[bucket].append(self.numerical_samples_arr[i][1][0] < self.numerical_samples_arr[i][0] < self.numerical_samples_arr[i][1][1])\n            else:\n                bucket_acc_counts[bucket].append(1 if bucket == self.real_values_bucketized[i] else 0)\n\n        for bucket in bucket_accuracy:\n            bucket_accuracy[bucket] = sum(bucket_acc_counts[bucket])/len(bucket_acc_counts[bucket])\n\n        accuracy_count = []\n        for counts in list(bucket_acc_counts.values()):\n            accuracy_count += counts\n\n        overall_accuracy = sum(accuracy_count)/len(accuracy_count)\n\n        for bucket in range(len(self.buckets)):\n            if bucket not in bucket_accuracy:\n                if bucket in self.real_values_bucketized:\n                    # If it was never predicted, but it did exist as a real value, then assume 0% confidence when it does get predicted\n                    bucket_accuracy[bucket] = 0\n\n        for bucket in range(len(self.buckets)):\n            if bucket not in bucket_accuracy:\n                # If it wasn\'t seen either in the real values or in the predicted values, assume average confidence (maybe should be 0 instead ?)\n                bucket_accuracy[bucket] = overall_accuracy\n\n        accuracy_histogram = {\n            \'buckets\': list(bucket_accuracy.keys())\n            ,\'accuracies\': list(bucket_accuracy.values())\n        }\n\n        labels= list(set(self.real_values_bucketized))\n        matrix = confusion_matrix(self.real_values_bucketized, self.normal_predictions_bucketized, labels=labels)\n        matrix = [[int(y) if str(y) != \'nan\' else 0 for y in x] for x in matrix]\n\n        bucket_values = [self.buckets[i] if i < len(self.buckets) else None for i in labels]\n\n        cm = {\n            \'matrix\': matrix,\n            \'predicted\': bucket_values,\n            \'real\': bucket_values\n        }\n\n        accuracy_samples = None\n        if len(self.numerical_samples_arr) > 0:\n            nr_samples = min(400,len(self.numerical_samples_arr))\n            sampled_numerical_samples_arr = random.sample(self.numerical_samples_arr, nr_samples)\n            accuracy_samples = {\n                \'y\': [x[0] for x in sampled_numerical_samples_arr]\n                ,\'x\': [x[1] for x in sampled_numerical_samples_arr]\n            }\n\n        return overall_accuracy, accuracy_histogram, cm, accuracy_samples\n\nif __name__ == ""__main__"":\n    pass\n    # Removing test for now, as tets for the new one stand-alone would require the creation of a bunch of dataframes mimicking those inputed into mindsdb and those predicted by lightwood.\n'"
mindsdb/libs/helpers/sqlite_helpers.py,0,"b'\nimport json\n\n\n\n\nclass FirstValueAgg:\n    def __init__(self):\n        self.first = None\n\n    def step(self, value):\n        if self.first == None:\n            self.first=value\n\n    def finalize(self):\n        return self.first\n\nclass ArrayAggJSON:\n    def __init__(self):\n        self.array = []\n        self.limit = 80\n\n    def step(self, value, limit):\n        self.array += [value]\n        self.limit = limit\n\n    def finalize(self):\n\n        arr =  self.array[-self.limit or None: ]\n        if len(arr) < self.limit:\n            diff_arra = [None]*(self.limit-len(arr))\n            arr = diff_arra + arr\n        return json.dumps(arr)\n'"
mindsdb/libs/helpers/text_helpers.py,0,"b'""""""\n*******************************************************\n * Copyright (C) 2017 MindsDB Inc. <copyright@mindsdb.com>\n *\n * This file is part of MindsDB Server.\n *\n * MindsDB Server can not be copied and/or distributed without the express\n * permission of MindsDB Inc\n *******************************************************\n""""""\n\nfrom mindsdb.libs.constants.mindsdb import *\nimport json\nimport hashlib\nimport numpy\n\n\ndef clean_float(val):\n    if isinstance(val, (int, float)):\n        return float(val)\n\n    if isinstance(val, numpy.float64):\n        return val\n\n    val = str(val)\n    val = val.replace(\',\',\'.\')\n    val = val.rstrip(\'""\').lstrip(\'""\')\n\n    if val == \'\' or val == \'None\' or val == \'nan\':\n        return None\n\n    return float(val)\n\n\ndef gen_chars(length, character):\n    """"""\n    # lambda to Generates a string consisting of `length` consiting of repeating `character`\n    :param length:\n    :param character:\n    :return:\n    """"""\n    return \'\'.join([character for i in range(length)])\n\ndef cast_string_to_python_type(string):\n    """""" Returns an integer, float or a string from a string""""""\n    try:\n        if string is None:\n            return None\n        return int(string)\n    except:\n        try:\n            return clean_float(string)\n        except ValueError:\n            if string == \'\':\n                return None\n            else:\n                return string\n\ndef splitRecursive(word, tokens):\n    words = [str(word)]\n    for token in tokens:\n        new_split = []\n        for word in words:\n            new_split += word.split(token)\n        words = new_split\n    words = [word for word in words if word not in [\'\', None] ]\n    return words\n\ndef hashtext(cell):\n    text = json.dumps(cell)\n    hash = hashlib.md5(text.encode(\'utf8\')).hexdigest()\n    return hash\n\ndef test():\n    log.info(splitRecursive(\'ABC.C HELLO, one:123.45 67\', WORD_SEPARATORS))\n\n# only run the test if this file is called from debugger\nif __name__ == ""__main__"":\n    test()\n'"
mindsdb/libs/helpers/train_helpers.py,0,"b""def getOneColPermutations(possible_columns):\n    permutations = {col: 1 for col in possible_columns}\n    ret = [perm.split(':') for perm in list(permutations.keys())]\n    return ret\n"""
mindsdb/libs/phases/__init__.py,0,b''
mindsdb/libs/phases/base_module.py,0,"b'""""""\n*******************************************************\n * Copyright (C) 2017 MindsDB Inc. <copyright@mindsdb.com>\n *\n * This file is part of MindsDB Server.\n *\n * MindsDB Server can not be copied and/or distributed without the express\n * permission of MindsDB Inc\n *******************************************************\n""""""\n\n""""""\nThis class works as an interface to code any module within MindsDB\nWe use this to standarize the way that code parts of what makes mindsDB in a way such that those can be replaced so long the interfaces remain\n\nThe principle is very simple, all you need within the module is set in the method __call__\n\nwhatever you pass to call is the input and whatever it returns is the ourput of the module\n\nThe interesting part is that in a module you should not instantiate data sources or any global resources\nThose should be available via self.session\n\n""""""\n\nfrom mindsdb.libs.constants.mindsdb import *\nimport time\n\n\nclass BaseModule():\n    def __init__(self, session, transaction, **kwargs):\n        \'\'\'\n        Initialize the base module and the basic global variables\n        :param session: the session under which this transaction is happening\n        :type transaction: libs.controllers.transaction_controller.TransactionController\n        :param transaction: the transaction under which this phases is being called\n        :param kwargs: extra arguments passed to run\n        \'\'\'\n        self.kwargs = kwargs\n        self.session = session\n        self.transaction = transaction\n        self.output = {}\n        self.setup(**kwargs)\n        self.log = self.transaction.log\n\n    def run(self):\n        pass\n\n    def __call__(self, **kwargs):\n        start = time.time()\n        class_name = type(self).__name__\n\n        self.log.info(\'[START] {class_name}\'.format(class_name=class_name))\n\n        ret = self.run(**kwargs)\n        execution_time = time.time() - start\n\n        self.log.info(\'[END] {class_name}, execution time: {execution_time:.3f} seconds\'.format(class_name=class_name, execution_time=execution_time))\n        return ret\n\n    def setup(self, **kwargs):\n        # This is to be implemented by the child classes\n        pass\n'"
mindsdb/libs/phases/data_cleaner/__init__.py,0,b''
mindsdb/libs/phases/data_cleaner/data_cleaner.py,0,"b'from mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n\nclass DataCleaner(BaseModule):\n    def _cleanup_w_missing_targets(self, df):\n        initial_len = len(df)\n        df = df.dropna(subset=self.transaction.lmd[\'predict_columns\'])\n        no_dropped = len(df) - initial_len\n        if no_dropped > 0:\n            self.log.warning(f\'Dropped {no_dropped} rows because they had null values in one or more of the columns that we are trying to predict. Please always provide non-null values in the columns you want to predict !\')\n        return df\n\n    def _cleanup_ignored(self, df):\n        for col_name in df.columns.values:\n            if len(df[col_name].dropna()) < 1:\n                self.transaction.lmd[\'columns_to_ignore\'].append(col_name)\n                self.transaction.lmd[\'empty_columns\'].append(col_name)\n                self.log.warning(f\'Column ""{col_name}"" is empty ! We\\\'ll go ahead and ignore it, please make sure you gave mindsdb the correct data.\')\n\n        df = df.drop(columns=self.transaction.lmd[\'columns_to_ignore\'])\n        return df\n\n    def run(self, stage):\n        if stage == 0:\n            self.transaction.input_data.data_frame = self._cleanup_w_missing_targets(self.transaction.input_data.data_frame)\n            self.transaction.input_data.data_frame = self._cleanup_ignored(self.transaction.input_data.data_frame)\n'"
mindsdb/libs/phases/data_extractor/__init__.py,0,b''
mindsdb/libs/phases/data_extractor/data_extractor.py,0,"b'from mindsdb.config import CONFIG\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.data_types.mindsdb_logger import log\nfrom mindsdb.libs.helpers.text_helpers import hashtext\nfrom mindsdb.external_libs.stats import calculate_sample_size\n\nfrom pandas.api.types import is_numeric_dtype\nimport random\nimport traceback\nimport pandas as pd\nimport numpy as np\n\n\nclass DataExtractor(BaseModule):\n    def _get_data_frame_from_when_conditions(self):\n        """"""\n        :return:\n        """"""\n        when_conditions = self.transaction.hmd[\'model_when_conditions\']\n\n        when_conditions_list = []\n        # here we want to make a list of the type  ( ValueForField1, ValueForField2,..., ValueForFieldN ), ...\n        for when_condition in when_conditions:\n            cond_list = [None] * len(self.transaction.lmd[\'columns\'])  # empty list with blanks for values\n\n            for condition_col in when_condition:\n                col_index = self.transaction.lmd[\'columns\'].index(condition_col)\n                cond_list[col_index] = when_condition[condition_col]\n\n            when_conditions_list.append(cond_list)\n\n        result = pd.DataFrame(when_conditions_list, columns=self.transaction.lmd[\'columns\'])\n\n        return result\n\n\n    def _apply_sort_conditions_to_df(self, df):\n        """"""\n\n        :param df:\n        :return:\n        """"""\n\n        # apply order by (group_by, order_by)\n        if self.transaction.lmd[\'model_is_time_series\']:\n            asc_values = [order_tuple[ORDER_BY_KEYS.ASCENDING_VALUE] for order_tuple in self.transaction.lmd[\'model_order_by\']]\n            sort_by = [order_tuple[ORDER_BY_KEYS.COLUMN] for order_tuple in self.transaction.lmd[\'model_order_by\']]\n\n            if self.transaction.lmd[\'model_group_by\']:\n                sort_by = self.transaction.lmd[\'model_group_by\'] + sort_by\n                asc_values = [True for i in self.transaction.lmd[\'model_group_by\']] + asc_values\n            df = df.sort_values(sort_by, ascending=asc_values)\n\n        elif self.transaction.lmd[\'type\'] == TRANSACTION_LEARN:\n            # if its not a time series, randomize the input data and we are learning\n            df = df.sample(frac=1, random_state=len(df))\n\n        return df\n\n\n    def _get_prepared_input_df(self):\n        """"""\n\n        :return:\n        """"""\n        df = None\n\n        # if transaction metadata comes with some data as from_data create the data frame\n        if \'from_data\' in self.transaction.hmd and self.transaction.hmd[\'from_data\'] is not None:\n            # make sure we build a dataframe that has all the columns we need\n            df = self.transaction.hmd[\'from_data\']\n            df = df.where((pd.notnull(df)), None)\n\n        # if this is a predict statement, create use model_when_conditions to shape the dataframe\n        if  self.transaction.lmd[\'type\'] == TRANSACTION_PREDICT:\n            if self.transaction.hmd[\'when_data\'] is not None:\n                df = self.transaction.hmd[\'when_data\']\n                df = df.where((pd.notnull(df)), None)\n\n                for col in self.transaction.lmd[\'columns\']:\n                    if col not in df.columns:\n                        df[col] = [None] * len(df)\n\n            elif self.transaction.hmd[\'model_when_conditions\'] is not None:\n\n                # if no data frame yet, make one\n                df = self._get_data_frame_from_when_conditions()\n\n\n        # if by now there is no DF, throw an error\n        if df is None:\n            error = \'Could not create a data frame for transaction\'\n            self.log.error(error)\n            raise ValueError(error)\n            return None\n\n        df = self._apply_sort_conditions_to_df(df)\n        groups = df.columns.to_series().groupby(df.dtypes).groups\n\n        if np.dtype(\'datetime64[ns]\') in groups:\n            for colname in groups[np.dtype(\'datetime64[ns]\')]:\n                df[colname] = df[colname].astype(str)\n\n        return df\n\n\n    def _validate_input_data_integrity(self):\n        """"""\n        :return:\n        """"""\n        if self.transaction.input_data.data_frame.shape[0] <= 0:\n            error = \'Input Data has no rows, please verify from_data or when_conditions\'\n            self.log.error(error)\n            raise ValueError(error)\n\n        if self.transaction.lmd[\'type\'] == TRANSACTION_LEARN:\n            for col_target in self.transaction.lmd[\'predict_columns\']:\n                if col_target not in self.transaction.input_data.columns:\n                    err = \'Trying to predict column {column} but column not in source data\'.format(column=col_target)\n                    self.log.error(err)\n                    self.transaction.error = True\n                    self.transaction.errorMsg = err\n                    raise ValueError(err)\n                    return\n\n    def _set_user_data_subtypes(self):\n        if \'from_data\' in self.transaction.hmd and self.transaction.hmd[\'from_data\'] is not None:\n            for col in self.transaction.hmd[\'from_data\'].data_subtypes:\n                self.transaction.lmd[\'data_types\'][col] = self.transaction.hmd[\'from_data\'].data_types[col]\n                self.transaction.lmd[\'data_subtypes\'][col] = self.transaction.hmd[\'from_data\'].data_subtypes[col]\n\n    def run(self):\n        # --- Dataset gets randomized or sorted (if timeseries) --- #\n        result = self._get_prepared_input_df()\n        # --- Dataset gets randomized or sorted (if timeseries) --- #\n\n        # --- Some information about the dataset gets transplanted into transaction level variables --- #\n        self.transaction.input_data.columns = result.columns.values.tolist()\n        self.transaction.lmd[\'columns\'] = self.transaction.input_data.columns\n        self.transaction.input_data.data_frame = result\n        # --- Some information about the dataset gets transplanted into transaction level variables --- #\n\n        self._set_user_data_subtypes()\n\n        # --- Some preliminary dataset integrity checks --- #\n        self._validate_input_data_integrity()\n        # --- Some preliminary dataset integrity checks --- #\n'"
mindsdb/libs/phases/data_splitter/__init__.py,0,b''
mindsdb/libs/phases/data_splitter/data_splitter.py,0,"b""from mindsdb.config import CONFIG\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.data_types.mindsdb_logger import log\n\n\nclass DataSplitter(BaseModule):\n    def run(self):\n        group_by = self.transaction.lmd['model_group_by']\n        if group_by is None or len(group_by) == 0:\n            group_by = []\n            # @TODO: Group by seems not to work on certain datasets and the values get split complete unevenly between train/test/validation\n            #for col in self.transaction.lmd['predict_columns']:\n            #    if self.transaction.lmd['column_stats'][col]['data_type'] == DATA_TYPES.CATEGORICAL:\n            #        group_by.append(col)\n            if len(group_by) > 0:\n                try:\n                    self.transaction.input_data.data_frame = self.transaction.input_data.data_frame.sort_values(group_by)\n                except Exception as e:\n                    # If categories can't be sroted because of various issues, that's fine, no need for the prediction logic to fail\n                    if len(self.transaction.lmd['model_group_by']) == 0:\n                        group_by = []\n                    else:\n                        raise Exception(e)\n\n\n        KEY_NO_GROUP_BY = '{PLEASE_DONT_TELL_ME_ANYONE_WOULD_CALL_A_COLUMN_THIS}##ALL_ROWS_NO_GROUP_BY##{PLEASE_DONT_TELL_ME_ANYONE_WOULD_CALL_A_COLUMN_THIS}'\n\n        # create all indexes by group by, that is all the rows that belong to each group by\n        all_indexes = {}\n        train_indexes = {}\n        test_indexes = {}\n        validation_indexes = {}\n\n        all_indexes[KEY_NO_GROUP_BY] = []\n        train_indexes[KEY_NO_GROUP_BY] = []\n        test_indexes[KEY_NO_GROUP_BY] = []\n        validation_indexes[KEY_NO_GROUP_BY] = []\n        for i, row in self.transaction.input_data.data_frame.iterrows():\n\n            if len(group_by) > 0:\n                group_by_value = '_'.join([str(row[group_by_index]) for group_by_index in [self.transaction.input_data.columns.index(group_by_col) for group_by_col in group_by]])\n\n                if group_by_value not in all_indexes:\n                    all_indexes[group_by_value] = []\n\n                all_indexes[group_by_value] += [i]\n\n            all_indexes[KEY_NO_GROUP_BY] += [i]\n\n        # move indexes to corresponding train, test, validation, etc and trim input data accordingly\n        if self.transaction.lmd['type'] == TRANSACTION_LEARN:\n            for key in all_indexes:\n                should_split_by_group = isinstance(group_by, list) and len(group_by) > 0\n\n                #If this is a group by, skip the `KEY_NO_GROUP_BY` key\n                if should_split_by_group and key == KEY_NO_GROUP_BY:\n                    continue\n\n                length = len(all_indexes[key])\n                # this evals True if it should send the entire group data into test, train or validation as opposed to breaking the group into the subsets\n                if should_split_by_group:\n                    train_indexes[key] = all_indexes[key][0:round(length - length*CONFIG.TEST_TRAIN_RATIO)]\n                    train_indexes[KEY_NO_GROUP_BY].extend(train_indexes[key])\n\n                    test_indexes[key] = all_indexes[key][round(length - length*CONFIG.TEST_TRAIN_RATIO):int(round(length - length*CONFIG.TEST_TRAIN_RATIO) + round(length*CONFIG.TEST_TRAIN_RATIO/2))]\n                    test_indexes[KEY_NO_GROUP_BY].extend(test_indexes[key])\n\n                    validation_indexes[key] = all_indexes[key][(round(length - length*CONFIG.TEST_TRAIN_RATIO) + round(length*CONFIG.TEST_TRAIN_RATIO/2)):]\n                    validation_indexes[KEY_NO_GROUP_BY].extend(validation_indexes[key])\n\n                else:\n                    # make sure that the last in the time series are also the subset used for test\n\n                    train_window = (0,int(length*(1-2*CONFIG.TEST_TRAIN_RATIO)))\n                    train_indexes[key] = all_indexes[key][train_window[0]:train_window[1]]\n                    validation_window = (train_window[1],train_window[1] + int(length*CONFIG.TEST_TRAIN_RATIO))\n                    test_window = (validation_window[1],length)\n                    test_indexes[key] = all_indexes[key][test_window[0]:test_window[1]]\n                    validation_indexes[key] = all_indexes[key][validation_window[0]:validation_window[1]]\n\n            self.transaction.input_data.train_df = self.transaction.input_data.data_frame.iloc[train_indexes[KEY_NO_GROUP_BY]].copy()\n            self.transaction.input_data.test_df = self.transaction.input_data.data_frame.iloc[test_indexes[KEY_NO_GROUP_BY]].copy()\n            self.transaction.input_data.validation_df = self.transaction.input_data.data_frame.iloc[validation_indexes[KEY_NO_GROUP_BY]].copy()\n\n            try:\n                self.transaction.input_data.data_frame = None\n                del self.transaction.input_data.data_frame\n                # Importing here on the off chance w'ere running on an interp where the gc can't be accessed directly\n                import gc\n                gc.collect()\n            except:\n                self.log.warning('Failed to cleanup memory after data splitting !')\n\n            self.transaction.lmd['data_preparation']['test_row_count'] = len(self.transaction.input_data.test_df)\n            self.transaction.lmd['data_preparation']['train_row_count'] = len(self.transaction.input_data.train_df)\n            self.transaction.lmd['data_preparation']['validation_row_count'] = len(self.transaction.input_data.validation_df)\n\n        # log some stats\n        if self.transaction.lmd['type'] == TRANSACTION_LEARN:\n            data = {\n                'subsets': [\n                    [len(self.transaction.input_data.train_df), 'Train'],\n                    [len(self.transaction.input_data.test_df), 'Test'],\n                    [len(self.transaction.input_data.validation_df), 'Validation']\n                ],\n                'label': 'Number of rows per subset'\n            }\n\n            self.log.info('We have split the input data into:')\n            self.log.infoChart(data, type='pie')\n"""
mindsdb/libs/phases/data_transformer/__init__.py,0,b''
mindsdb/libs/phases/data_transformer/data_transformer.py,0,"b""from dateutil.parser import parse as parse_datetime\nimport datetime\nimport math\nimport sys\n\nimport pandas as pd\n\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.helpers.text_helpers import clean_float\nfrom mindsdb.libs.helpers.debugging import *\n\n\nclass DataTransformer(BaseModule):\n\n    @staticmethod\n    def _handle_nan(x):\n        if x is not None and math.isnan(x):\n            return 0\n        else:\n            return x\n\n    @staticmethod\n    def _try_round(x):\n        try:\n            return round(x)\n        except:\n            return None\n\n    @staticmethod\n    def _standardize_date(date_str):\n        try:\n            # will return a datetime object\n            date = parse_datetime(date_str)\n        except:\n            try:\n                date = datetime.datetime.utcfromtimestamp(date_str)\n            except:\n                return None\n        return date.strftime('%Y-%m-%d')\n\n    @staticmethod\n    def _standardize_datetime(date_str):\n        try:\n            # will return a datetime object\n            dt = parse_datetime(str(date_str))\n        except:\n            try:\n                dt = datetime.datetime.utcfromtimestamp(date_str)\n            except:\n                return None\n\n        return dt.strftime('%Y-%m-%d %H:%M:%S')\n\n    @staticmethod\n    def _lightwood_datetime_processing(dt):\n        dt = pd.to_datetime(dt, errors = 'coerce')\n        try:\n            return dt.timestamp()\n        except:\n            return None\n\n    @staticmethod\n    def _aply_to_all_data(input_data, column, func, transaction_type):\n        if transaction_type == TRANSACTION_LEARN:\n            input_data.train_df[column] = input_data.train_df[column].apply(func)\n            input_data.test_df[column] = input_data.test_df[column].apply(func)\n            input_data.validation_df[column] = input_data.validation_df[column].apply(func)\n        else:\n            input_data.data_frame[column] = input_data.data_frame[column].apply(func)\n\n    @staticmethod\n    def _cast_all_data(input_data, column, cast_to_type, transaction_type):\n        if transaction_type == TRANSACTION_LEARN:\n            input_data.train_df[column] = input_data.train_df[column].astype(cast_to_type)\n            input_data.test_df[column] = input_data.test_df[column].astype(cast_to_type)\n            input_data.validation_df[column] = input_data.validation_df[column].astype(cast_to_type)\n        else:\n            input_data.data_frame[column] = input_data.data_frame[column].astype(cast_to_type)\n\n    def run(self, input_data):\n        for column in input_data.columns:\n            if column in self.transaction.lmd['columns_to_ignore']:\n                continue\n\n            data_type = self.transaction.lmd['column_stats'][column]['data_type']\n            data_subtype = self.transaction.lmd['column_stats'][column]['data_subtype']\n\n            if data_type == DATA_TYPES.NUMERIC:\n                self._aply_to_all_data(input_data, column, clean_float, self.transaction.lmd['type'])\n                self._aply_to_all_data(input_data, column, self._handle_nan, self.transaction.lmd['type'])\n\n                if data_subtype == DATA_SUBTYPES.INT:\n                    self._aply_to_all_data(input_data, column, DataTransformer._try_round, self.transaction.lmd['type'])\n\n            if data_type == DATA_TYPES.DATE:\n                if data_subtype == DATA_SUBTYPES.DATE:\n                    self._aply_to_all_data(input_data, column, self._standardize_date, self.transaction.lmd['type'])\n\n                elif data_subtype == DATA_SUBTYPES.TIMESTAMP:\n                    self._aply_to_all_data(input_data, column, self._standardize_datetime, self.transaction.lmd['type'])\n\n            if data_type == DATA_TYPES.CATEGORICAL:\n                self._aply_to_all_data(input_data, column, str, self.transaction.lmd['type'])\n                self._cast_all_data(input_data, column, 'category', self.transaction.lmd['type'])\n\n            if data_subtype == DATA_SUBTYPES.TEXT:\n                self._aply_to_all_data(input_data, column, str, self.transaction.lmd['type'])\n\n            if self.transaction.hmd['model_backend'] == 'lightwood':\n                if data_type == DATA_TYPES.DATE:\n                    self._aply_to_all_data(input_data, column, self._standardize_datetime, self.transaction.lmd['type'])\n                    self._aply_to_all_data(input_data, column, self._lightwood_datetime_processing, self.transaction.lmd['type'])\n                    self._aply_to_all_data(input_data, column, self._handle_nan, self.transaction.lmd['type'])\n\n        # Initialize this here, will be overwritten if `equal_accuracy_for_all_output_categories` is specified to be True in order to account for it\n        self.transaction.lmd['weight_map'] = self.transaction.lmd['output_categories_importance_dictionary']\n\n        # Un-bias dataset for training\n        for column in self.transaction.lmd['predict_columns']:\n            if self.transaction.lmd['column_stats'][column]['data_type'] == DATA_TYPES.CATEGORICAL and self.transaction.lmd['equal_accuracy_for_all_output_categories'] == True and self.transaction.lmd['type'] == TRANSACTION_LEARN:\n\n                occurance_map = {}\n                ciclying_map = {}\n\n                for i in range(0,len(self.transaction.lmd['column_stats'][column]['histogram']['x'])):\n                    ciclying_map[self.transaction.lmd['column_stats'][column]['histogram']['x'][i]] = 0\n                    occurance_map[self.transaction.lmd['column_stats'][column]['histogram']['x'][i]] = self.transaction.lmd['column_stats'][column]['histogram']['y'][i]\n\n                max_val_occurances = max(occurance_map.values())\n\n                if self.transaction.hmd['model_backend'] in ('lightwood'):\n                    lightwood_weight_map = {}\n                    for val in occurance_map:\n                        lightwood_weight_map[val] = 1/occurance_map[val] #sum(occurance_map.values())\n\n                        if column in self.transaction.lmd['output_categories_importance_dictionary']:\n                            if val in self.transaction.lmd['output_categories_importance_dictionary'][column]:\n                                lightwood_weight_map[val] = self.transaction.lmd['output_categories_importance_dictionary'][column][val]\n                            elif '<default>' in self.transaction.lmd['output_categories_importance_dictionary'][column]:\n                                lightwood_weight_map[val] = self.transaction.lmd['output_categories_importance_dictionary'][column]['<default>']\n\n                    self.transaction.lmd['weight_map'][column] = lightwood_weight_map\n\n                #print(self.transaction.lmd['weight_map'])\n                column_is_weighted_in_train = column in self.transaction.lmd['weight_map']\n\n                if column_is_weighted_in_train:\n                    dfs = ['input_data.validation_df']\n                else:\n                    dfs = ['input_data.train_df','input_data.test_df','input_data.validation_df']\n\n                total_len = (len(input_data.train_df) + len(input_data.test_df) + len(input_data.validation_df))\n                # Since pandas doesn't support append in-place we'll just do some eval-based hacks\n\n                for dfn in dfs:\n                    max_val_occurances_in_set = int(round(max_val_occurances * len(eval(dfn))/total_len))\n                    for val in occurance_map:\n                        valid_rows = eval(dfn)[eval(dfn)[column] == val]\n                        if len(valid_rows) == 0:\n                            continue\n\n                        appended_times = 0\n                        while max_val_occurances_in_set > len(valid_rows) * (2 + appended_times):\n                            exec(f'{dfn} = {dfn}.append(valid_rows)')\n                            appended_times += 1\n\n                        if int(max_val_occurances_in_set - len(valid_rows) * (1 + appended_times)) > 0:\n                            exec(f'{dfn} = {dfn}.append(valid_rows[0:int(max_val_occurances_in_set - len(valid_rows) * (1 + appended_times))])')\n"""
mindsdb/libs/phases/model_analyzer/__init__.py,0,b''
mindsdb/libs/phases/model_analyzer/model_analyzer.py,0,"b'from mindsdb.libs.helpers.general_helpers import pickle_obj, disable_console_output\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.helpers.general_helpers import evaluate_accuracy\nfrom mindsdb.libs.helpers.probabilistic_validator import ProbabilisticValidator\n\nimport pandas as pd\nimport numpy as np\n\n\nclass ModelAnalyzer(BaseModule):\n    def run(self):\n        np.seterr(divide=\'warn\', invalid=\'warn\')\n        """"""\n        # Runs the model on the validation set in order to fit a probabilistic model that will evaluate the accuracy of future predictions\n        """"""\n\n        output_columns = self.transaction.lmd[\'predict_columns\']\n        input_columns = [col for col in self.transaction.lmd[\'columns\'] if col not in output_columns and col not in self.transaction.lmd[\'columns_to_ignore\']]\n\n        # Make predictions on the validation dataset normally and with various columns missing\n        normal_predictions = self.transaction.model_backend.predict(\'validate\')\n        normal_accuracy = evaluate_accuracy(normal_predictions, self.transaction.input_data.validation_df, self.transaction.lmd[\'column_stats\'], output_columns)\n\n        empty_input_predictions = {}\n        empty_inpurt_accuracy = {}\n\n        ignorable_input_columns = [x for x in input_columns if self.transaction.lmd[\'column_stats\'][x][\'data_type\'] != DATA_TYPES.FILE_PATH and x not in [y[0] for y in self.transaction.lmd[\'model_order_by\']]]\n        for col in ignorable_input_columns:\n            empty_input_predictions[col] = self.transaction.model_backend.predict(\'validate\', ignore_columns=[col])\n            empty_inpurt_accuracy[col] = evaluate_accuracy(empty_input_predictions[col], self.transaction.input_data.validation_df, self.transaction.lmd[\'column_stats\'], output_columns)\n\n        # Get some information about the importance of each column\n        if not self.transaction.lmd[\'disable_optional_analysis\']:\n            self.transaction.lmd[\'column_importances\'] = {}\n            for col in ignorable_input_columns:\n                column_importance = (1 - empty_inpurt_accuracy[col]/normal_accuracy)\n                column_importance = np.ceil(10*column_importance)\n                self.transaction.lmd[\'column_importances\'][col] = float(10 if column_importance > 10 else column_importance)\n\n        # Run Probabilistic Validator\n        overall_accuracy_arr = []\n        self.transaction.lmd[\'accuracy_histogram\'] = {}\n        self.transaction.lmd[\'confusion_matrices\'] = {}\n        self.transaction.lmd[\'accuracy_samples\'] = {}\n        self.transaction.hmd[\'probabilistic_validators\'] = {}\n\n        for col in output_columns:\n            pval = ProbabilisticValidator(col_stats=self.transaction.lmd[\'column_stats\'][col], col_name=col, input_columns=input_columns)\n            predictions_arr = [normal_predictions] + [empty_input_predictions[col] for col in ignorable_input_columns]\n\n            pval.fit(self.transaction.input_data.validation_df, predictions_arr, [[x] for x in ignorable_input_columns])\n            overall_accuracy, accuracy_histogram, cm, accuracy_samples = pval.get_accuracy_stats()\n            overall_accuracy_arr.append(overall_accuracy)\n\n            self.transaction.lmd[\'accuracy_histogram\'][col] = accuracy_histogram\n            self.transaction.lmd[\'confusion_matrices\'][col] = cm\n            self.transaction.lmd[\'accuracy_samples\'][col] = accuracy_samples\n            self.transaction.hmd[\'probabilistic_validators\'][col] = pickle_obj(pval)\n\n        print(overall_accuracy_arr)\n        self.transaction.lmd[\'validation_set_accuracy\'] = sum(overall_accuracy_arr)/len(overall_accuracy_arr)\n\ndef test():\n    from mindsdb.libs.controllers.predictor import Predictor\n    from mindsdb import CONFIG\n\n    mdb = Predictor(name=\'home_rentals\')\n\n    mdb.learn(\n        from_data=""https://s3.eu-west-2.amazonaws.com/mindsdb-example-data/home_rentals.csv"",\n        # the path to the file where we can learn from, (note: can be url)\n        to_predict=\'rental_price\',  # the column we want to learn to predict given all the data in the file\n        #sample_margin_of_error=0.02,\n        stop_training_in_x_seconds=6\n    )\n\n    #use the model to make predictions\n    result = mdb.predict(\n        when={""number_of_rooms"": 2, ""sqft"": 1384})\n\n    result[0].explain()\n\n    when = {""number_of_rooms"": 1,""sqft"": 384}\n\n    # use the model to make predictions\n    result = mdb.predict(\n        when=when)\n\n    result[0].explain()\n\n\n# only run the test if this file is called from debugger\nif __name__ == ""__main__"":\n    test()\n'"
mindsdb/libs/phases/model_interface/__init__.py,0,b''
mindsdb/libs/phases/model_interface/model_interface.py,0,"b""from mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.constants.mindsdb import *\n\nimport datetime\n\n\nclass ModelInterface(BaseModule):\n    def run(self, mode='train'):\n        try:\n            from mindsdb.libs.backends.ludwig import LudwigBackend\n        except ImportError as e:\n            # Ludwig is optional, so this is fine\n            pass\n\n        try:\n            from mindsdb.libs.backends.lightwood import LightwoodBackend\n        except ImportError as e:\n            self.log.warning(e)\n\n        if self.transaction.hmd['model_backend'] == 'ludwig':\n            self.transaction.model_backend = LudwigBackend(self.transaction)\n        elif self.transaction.hmd['model_backend'] == 'lightwood':\n            self.transaction.model_backend = LightwoodBackend(self.transaction)\n        else:\n            self.transaction.model_backend = self.transaction.hmd['model_backend']\n\n        if hasattr(self.transaction.model_backend, 'set_transaction'):\n            self.transaction.model_backend.set_transaction(self.transaction)\n\n        if mode == 'train':\n            self.transaction.model_backend.train()\n            self.transaction.lmd['train_end_at'] = str(datetime.datetime.now())\n        elif mode == 'predict':\n            self.transaction.hmd['predictions'] = self.transaction.model_backend.predict()\n"""
mindsdb/libs/phases/stats_generator/__init__.py,0,b''
mindsdb/libs/phases/stats_generator/data_preparation.py,0,"b""import random\n\nimport numpy as np\nfrom dateutil.parser import parse as parse_datetime\n\nfrom mindsdb.external_libs.stats import calculate_sample_size\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.helpers.text_helpers import clean_float\n\n\ndef sample_data(df, sample_margin_of_error, sample_confidence_level, log):\n    population_size = len(df)\n\n    sample_size = int(calculate_sample_size(population_size, sample_margin_of_error, sample_confidence_level)) if population_size > 50 else population_size\n    sample_size_pct = sample_size*100/population_size\n\n    # get the indexes of randomly selected rows given the population size\n    input_data_sample_indexes = random.sample(range(population_size), sample_size)\n\n    log.info(f'Analyzing a sample of {sample_size} from a total population of {population_size}, this is equivalent to {sample_size_pct}% of your data.')\n\n    return df.iloc[input_data_sample_indexes]\n\n\ndef clean_int_and_date_data(col_data, log):\n    cleaned_data = []\n\n    for ele in col_data:\n        if str(ele) not in ['', str(None), str(False), str(np.nan), 'NaN', 'nan', 'NA', 'null'] and (not ele or not str(ele).isspace()):\n            try:\n                cleaned_data.append(clean_float(ele))\n            except Exception as e1:\n                try:\n                    cleaned_data.append(parse_datetime(str(ele)).timestamp())\n                except Exception as e2:\n                    log.warning(f'Failed to parser numerical value with error chain:\\n {e1} -> {e2}\\n')\n                    cleaned_data.append(0)\n\n    return cleaned_data\n"""
mindsdb/libs/phases/stats_generator/scores.py,0,"b'from collections import Counter\n\nimport numpy as np\nimport scipy.stats as st\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import matthews_corrcoef\n\nfrom mindsdb.libs.constants.mindsdb import *\n\n\n\ndef compute_value_distribution_score(stats, columns, col_name):\n    """"""\n    # Looks at the histogram and transforms it into a proability mapping for each\n    bucket, then generates a quality score (value_distribution_score) based on that\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        bucket_probabilities: A value distribution score, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n        value_distribution_score: A dictioanry of with the probabilities that a value values in a bucket, for each of he buckets in the histogram\n    """"""\n\n    bucket_probabilities = {}\n    pair = stats[col_name][\'histogram\']\n    total_vals = sum(pair[\'y\'])\n    for i in range(len(pair[\'x\'])):\n        bucket_probabilities[pair[\'x\'][i]] = pair[\'y\'][i]/total_vals\n\n    probabilities = list(bucket_probabilities.values())\n\n    max_probability = max(probabilities)\n    max_probability_key = max(bucket_probabilities, key=lambda k: bucket_probabilities[k])\n\n    value_distribution_score = 1 - np.mean(probabilities)/max_probability\n\n    data = {\n        \'bucket_probabilities\': bucket_probabilities\n        ,\'value_distribution_score\': round(10 * (1 - value_distribution_score))\n        ,\'max_probability_key\': max_probability_key\n        ,\'value_distribution_score_description\': """"""\n        This score can indicate either biasing towards one specific value in the column or a large number of outliers. So it is a reliable quality indicator but we can\'t know for which of the two reasons.\n        """"""\n    }\n\n    return data\n\n\ndef compute_duplicates_score(stats, columns, col_name):\n    """"""\n    # Looks at the set of distinct values for all the data and computes a quality\n    socre based on how many of the values are duplicates\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        nr_duplicates: the nr of cells which contain values that are found more than once\n        duplicates_percentage: % of the values that are found more than once\n        duplicates_score: a quality based on the duplicate percentage, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n\n    occurances = Counter(columns[col_name])\n    values_that_occur_twice_or_more = filter(lambda val: occurances[val] < 2, occurances)\n    nr_of_occurances = map(lambda val: occurances[val], values_that_occur_twice_or_more)\n    nr_duplicates = sum(nr_of_occurances)\n    data = {\n        \'nr_duplicates\': nr_duplicates\n        ,\'duplicates_percentage\': nr_duplicates*100/len(columns[col_name])\n        ,\'duplicates_score_description\':""""""\n        The duplicates score consists in the % of duplicate values / 100. So, it can range from 0 (no duplicates) to 1 (all the values have one or more duplicates). This score being large, on it\'s own, is not necessarily an indicator that your data is of poor quality.\n        """"""\n    }\n\n    if stats[col_name][\'data_type\'] != DATA_TYPES.CATEGORICAL and stats[col_name][\'data_type\'] != DATA_TYPES.DATE:\n        data[\'duplicates_score\'] = data[\'duplicates_percentage\']/100\n    else:\n        data[\'c\'] = 0\n\n    return data\n\ndef compute_empty_cells_score(stats, columns, col_name):\n    """"""\n    # Creates a quality socre based on the percentage of empty cells (empty_percentage)\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        empty_cells_score: A quality score based on the nr of empty cells, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n\n    return {\'empty_cells_score\': round(10 * (1 - stats[col_name][\'empty_percentage\']/100))\n            ,\'empty_cells_score_description\':""""""This score is computed as the % of empty values / 100. Empty values in a column are always bad for training correctly on that data.""""""}\n\ndef compute_data_type_dist_score(stats, columns, col_name):\n    """"""\n    # Creates a quality socre based on the data type distribution, this score is based on\n    the difference between the nr of values with the ""main"" data type\n    and all the nr of values with all other data types\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        data_type_distribution_score: A quality score based on the nr of empty cells, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n\n    vals = stats[col_name][\'data_type_dist\'].values()\n    principal = max(vals)\n    total = len(columns[col_name])\n    data_type_dist_score = (total - principal)/total\n    return {\'data_type_distribution_score\': round(10 * (1 - data_type_dist_score))\n    ,\'data_type_distribution_score_description\':""""""\n    This score indicates the amount of data that are not of the same data type as the most commonly detected data type in this column. Note, the most commonly occuring data type is not necessarily the type mindsdb will use to label the column when learning or predicting.\n    """"""}\n\ndef compute_z_score(stats, columns, col_name):\n    """"""\n    # Computes the z_score for each value in our column.\n    # This score represents the distance from the mean over the standard deviation.\n    # Based on this, compute a quality metrics.\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        z_score_outliers: The indexs of values which we consider outliers based on the z score\n        mean_z_score: The mean z score for the column\n        z_test_based_outlier_score: A quality score based on the nr of outliers as determined by their z score, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n    if stats[col_name][\'data_type\'] != DATA_TYPES.NUMERIC:\n        return {}\n\n    z_scores = list(map(abs,(st.zscore(columns[col_name]))))\n    threshold = 3\n    z_score_outlier_indexes = [i for i in range(len(z_scores)) if z_scores[i] > threshold]\n    data = {\n        \'z_score_outliers\': z_score_outlier_indexes\n        ,\'mean_z_score\': round(10 * (1 - np.mean(z_scores)))\n        ,\'z_test_based_outlier_score\': round(10 * (1 - len(z_score_outlier_indexes)/len(columns[col_name])))\n        ,\'z_test_based_outlier_score_description\':""""""\n        This score indicates the amount of data that are 3 STDs or more away from the mean. That is to say, the amount of data that we consider to be an outlir. A hgih z socre means your data contains a large amount of outliers.\n        """"""\n    }\n    return data\n\ndef compute_lof_score(stats, columns, col_name):\n    """"""\n    # Uses LocalOutlierFactor (a KNN clustering based method from sklearn)\n    to determine outliers within our column\n    # All data that has a small score after we call `fit_predict` has a high chance of being an outlier\n    based on the distance from the clusters created by LOF\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        lof_outliers: The indexs of values which we consider outliers based on LOF\n        lof_based_outlier_score: A quality score based on the nr of outliers as determined by their LOF score, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n\n    if stats[col_name][\'data_type\'] != DATA_TYPES.NUMERIC:\n        return {}\n\n    np_col_data = np.array(columns[col_name]).reshape(-1, 1)\n    lof = LocalOutlierFactor(contamination=\'auto\')\n    outlier_scores = lof.fit_predict(np_col_data)\n\n    outliers = [columns[col_name][i] for i in range(len(columns[col_name])) if outlier_scores[i] < -0.8]\n\n    return {\n        \'lof_outliers\': outliers\n        ,\'lof_based_outlier_score\': round(10 * (1 - len(outliers)/len(columns[col_name])))\n        ,\'percentage_of_log_based_outliers\': (len(outliers)/len(columns[col_name])) * 100\n        ,\'lof_based_outlier_score_description\':""""""\n        The higher this score, the more outliers your dataset has. This is based on distance from the center of 20 clusters as constructed via KNN.\n        """"""\n    }\n\n\ndef compute_similariy_score(stats, columns, col_name):\n    """"""\n    # Uses equality between values in the same position to determine up what % of their cells two columns are identical\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        similarities: How similar this column is to other columns (with 0 being completely different and 1 being an exact copy).\n        similarity_score: A score equal to the highest similarity found, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n    """"""\n    col_data = columns[col_name]\n\n    similarities = []\n    for other_col_name in columns.columns:\n        if other_col_name == col_name:\n            continue\n        else:\n            # @TODO Figure out why computing matthews_corrcoef is so slow, possibly find a better implementation and replace it with that. Matthews corrcoef code was: similarity = matthews_corrcoef(list(map(str,col_data)), list(map(str,columns[other_col_name])))\n            similarity = 0\n            X1 = list(map(str,col_data))\n            X2 = list(map(str,columns[other_col_name]))\n            for ii in range(len(X1)):\n                if X1[ii] == X2[ii]:\n                    similarity += 1\n\n            similarity = similarity/len(X1)\n            similarities.append((other_col_name,similarity))\n\n\n    max_similarity = max(map(lambda x: x[1], similarities))\n    most_similar_column_name = list(filter(lambda x: x[1] == max_similarity, similarities))[0][0]\n\n    if max_similarity < 0:\n        max_similarity = 0\n\n    return {\n        \'max_similarity\': max_similarity\n        ,\'similarities\': similarities\n        ,\'similarity_score\': round(10 * (1 - max_similarity))\n        ,\'most_similar_column_name\': most_similar_column_name\n        ,\'similarity_score_description\':""""""\n        This score is simple element-wise equality applied between this column and all other column.\n        The score * 100 is the number of values which are similar in the column that is most similar to the scored column.\n        """"""\n    }\n\n\ndef compute_clf_based_correlation_score(stats, columns, col_name):\n    """"""\n    # Tries to find correlated columns by trying to predict the values in one\n    column based on all the others using a simple DT classifier\n    # The purpose of this is not to see if a column is predictable, but rather, to\n    see if it can be predicted accurately based on a single other column, or if\n    all other columns play an equally important role\n    # A good prediction score, based on all the columns, doesn\'t necessarily imply\n    a correlation between columns, it could just mean the data is very garnular (few repeat values)\n\n    :param stats: The stats extracted up until this point for all columns\n    :param columns: All the columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        correlation_score: A score equal to the prediction accuracy * the importance (from 0 to 1)\n            of the most imporant column in making said prediciton,\n            ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n        highest_correlation: The importance of the most_correlated_column in the DT classifier\n        most_correlated_column: The column with which our column is correlated most based on the DT\n    """"""\n    full_col_data = columns[col_name]\n\n    dt_clf = DecisionTreeClassifier()\n\n    other_feature_names = []\n    other_features = []\n    for other_col_name in columns.columns:\n        if other_col_name == col_name:\n            continue\n\n        other_feature_names.append(other_col_name)\n        le = LabelEncoder()\n        _stringified_col = list(map(str,columns[other_col_name]))\n        le.fit(_stringified_col)\n        other_features.append(list(le.transform(_stringified_col)))\n\n    other_features_t = np.array(other_features, dtype=object).transpose()\n\n    le = LabelEncoder()\n    _stringified_col = list(map(str,full_col_data))\n    le.fit(_stringified_col)\n    y = le.transform(_stringified_col)\n    dt_clf.fit(other_features_t,y)\n    prediction_score = dt_clf.score(other_features_t,y)\n    corr_scores = list(dt_clf.feature_importances_)\n    highest_correlated_column = max(corr_scores)\n    return {\n        \'correlation_score\': round(10 * (1 - prediction_score * highest_correlated_column))\n        ,\'highest_correlation\': max(corr_scores)\n        ,\'most_correlated_column\': other_feature_names[corr_scores.index(max(corr_scores))]\n        ,\'similarity_score_description\':""""""\n        A high value for this score means that two of your columns are highly similar. This is done by trying to predict one column using the other via a simple DT.\n        """"""\n    }\n\ndef compute_consistency_score(stats, col_name):\n    """"""\n    # Attempts to determine the consistency of the data in a column\n    by taking into account the ty[e distribution, nr of empty cells and duplicates\n\n    :param stats: The stats extracted up until this point for all columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        consistency_score: The socre, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality\n    """"""\n    col_stats = stats[col_name]\n    if \'duplicates_score\' in col_stats:\n        consistency_score = (col_stats[\'data_type_distribution_score\'] + col_stats[\'empty_cells_score\'])/2.5 + col_stats[\'duplicates_score\']/5\n    else:\n        consistency_score = (col_stats[\'data_type_distribution_score\'] + col_stats[\'empty_cells_score\'])/2\n    return {\'consistency_score\': consistency_score\n    ,\'consistency_score_description\':""""""\n    A high value for this score indicates that the data in a column is not very consistent, it\'s either missing a lot of valus or the type of values it has varries quite a lot (e.g. combination of strings, dates, integers and floats).\n    The data consistency score is mainly based upon the Data Type Distribution Score and the Empty Cells Score, the Duplicates Score is also taken into account if present but with a smaller (2x smaller) bias.\n    """"""}\n\ndef compute_redundancy_score(stats, col_name):\n    """"""\n    # Attempts to determine the redundancy of the column by taking into account correlation and\n    similarity with other columns\n\n    :param stats: The stats extracted up until this point for all columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        consistency_score: The socre, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality\n    """"""\n    col_stats = stats[col_name]\n    redundancy_score = (col_stats[\'similarity_score\'])/1\n    return {\'redundancy_score\': redundancy_score\n        ,\'redundancy_score_description\':""""""\n        A high value in this score indicates the data in this column is highly redundant for making any sort of prediction, you should make sure that values heavily related to this column are not already expressed in another column (e.g. if this column is a timestamp, make sure you don\'t have another column representing the exact same time in ISO datetime format).\n        The value is based in equal part on the Similarity Score and the Correlation Score.\n        """"""}\n\ndef compute_variability_score(stats, col_name):\n    """"""\n    # Attempts to determine the variability/randomness of a column by taking into account\n    the z and lof outlier scores and the value distribution score (histogram biasing towards a few buckets)\n\n    :param stats: The stats extracted up until this point for all columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        consistency_score: The socre, ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality\n    """"""\n    col_stats = stats[col_name]\n    if \'lof_based_outlier_score\' in col_stats and \'z_test_based_outlier_score\' in col_stats:\n        variability_score = (col_stats[\'z_test_based_outlier_score\'] + col_stats[\'lof_based_outlier_score\']\n         + col_stats[\'value_distribution_score\'])/3\n    else:\n        variability_score = col_stats[\'value_distribution_score\']/2\n\n    return {\'variability_score\': variability_score\n    ,\'variability_score_description\':""""""\n    A high value for this score indicates the data in this column seems to be very variable, indicating a large possibility of some random noise affecting your data. This could mean that the values for this column are not collected or processed correctly.\n    The value is based in equal part on the Z Test based outliers score, the LOG based outlier score and the Value Distribution Score.\n    """"""}\n\n\ndef compute_data_quality_score(stats, col_name):\n    """"""\n    # Attempts to determine the quality of the column through aggregating all quality score\n    we could compute about it\n\n    :param stats: The stats extracted up until this point for all columns\n    :param col_name: The name of the column we should compute the new stats for\n    :return: Dictioanry containing:\n        quality_score: An aggreagted quality socre that attempts to asses the overall quality of the column,\n            , ranges from 1 to 0, where 1 is lowest quality and 0 is highest quality.\n        bad_scores: The socres which lead to use rating this column poorly\n    """"""\n\n    col_stats = stats[col_name]\n    scores = [\'consistency_score\', \'redundancy_score\', \'variability_score\']\n    quality_score = 0\n    for score in scores:\n        quality_score += col_stats[score]\n    quality_score = quality_score/len(scores)\n\n    return {\'quality_score\': quality_score\n    ,\'quality_score_description\':""""""\n    The higher this score is, the lower the quality of a given column.\n    """"""}\n'"
mindsdb/libs/phases/stats_generator/stats_generator.py,0,"b'import random\nimport time\nimport warnings\nimport imghdr\nimport sndhdr\nimport logging\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nfrom scipy.stats import entropy\nfrom dateutil.parser import parse as parse_datetime\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.cluster import MiniBatchKMeans\nimport imagehash\nfrom PIL import Image\n\nfrom mindsdb.config import CONFIG\nfrom mindsdb.libs.constants.mindsdb import *\nfrom mindsdb.libs.phases.base_module import BaseModule\nfrom mindsdb.libs.helpers.text_helpers import splitRecursive, clean_float\nfrom mindsdb.libs.helpers.general_helpers import get_value_bucket\nfrom mindsdb.libs.helpers.debugging import *\nfrom mindsdb.libs.phases.stats_generator.scores import *\nfrom mindsdb.libs.phases.stats_generator.data_preparation import sample_data, clean_int_and_date_data\n\n\nclass StatsGenerator(BaseModule):\n    """"""\n    # The stats generator phase is responsible for generating the insights we need about the data in order to vectorize it\n    # Additionally, the stats generator also provides the user with some extra meaningful information about his data,\n    though this functionality may be moved to a different step (after vectorization) in the future\n    """"""\n    def _get_file_type(self, potential_path):\n        could_be_fp = False\n        for char in (\'/\', \'\\\\\', \':\\\\\'):\n            if char in potential_path:\n                could_be_fp = True\n\n        if not could_be_fp:\n            return False\n\n        try:\n            is_img = imghdr.what(potential_path)\n            if is_img is None:\n                return False\n            else:\n                return DATA_SUBTYPES.IMAGE\n        except:\n            # Not a file or file doesn\'t exist\n            return False\n\n        # @TODO: CURRENTLY DOESN\'T DIFFERENTIATE BETWEEN AUDIO AND VIDEO\n        is_audio = sndhdr.what(potential_path)\n        if is_audio is not None:\n            return DATA_SUBTYPES.AUDIO\n\n        return False\n\n    def _is_number(self, string):\n        """""" Returns True if string is a number. """"""\n        try:\n            # Should crash if not number\n            clean_float(str(string))\n            if \'.\' in str(string) or \',\' in str(string):\n                return DATA_SUBTYPES.FLOAT\n            else:\n                return DATA_SUBTYPES.INT\n        except ValueError:\n            return False\n\n    def _get_date_type(self, string):\n        """""" Returns True if string is a valid date format """"""\n        try:\n            dt = parse_datetime(string)\n\n            # Not accurate 100% for a single datetime str, but should work in aggregate\n            if dt.hour == 0 and dt.minute == 0 and dt.second == 0 and len(string) <= 16:\n                return DATA_SUBTYPES.DATE\n            else:\n                return DATA_SUBTYPES.TIMESTAMP\n        except:\n            return False\n\n    def _get_text_type(self, data):\n        """"""\n        Takes in column data and defines if its categorical or full_text\n\n        :param data: a list of cells in a column\n        :return: DATA_TYPES.CATEGORICAL or DATA_TYPES.FULL_TEXT\n        """"""\n\n        total_length = len(data)\n        key_count = {}\n        max_number_of_words = 0\n\n        for cell in data:\n\n            if cell not in key_count:\n                key_count[cell] = 1\n            else:\n                key_count[cell] += 1\n\n            cell_wseparator = cell\n            sep_tag = \'{#SEP#}\'\n            for separator in WORD_SEPARATORS:\n                cell_wseparator = str(cell_wseparator).replace(separator,sep_tag)\n\n            words_split = cell_wseparator.split(sep_tag)\n            words = len([ word for word in words_split if word not in [\'\', None] ])\n\n            if max_number_of_words < words:\n                max_number_of_words += words\n\n        # If all sentences are less than or equal and 3 words, assume it\'s a category rather than a sentence\n        if max_number_of_words <= 3:\n            if len(key_count.keys()) < 3:\n                return DATA_TYPES.CATEGORICAL, DATA_SUBTYPES.SINGLE\n            else:\n                return DATA_TYPES.CATEGORICAL, DATA_SUBTYPES.MULTIPLE\n        else:\n            return DATA_TYPES.SEQUENTIAL, DATA_SUBTYPES.TEXT\n\n\n    def _get_column_data_type(self, data, data_frame, col_name):\n        """"""\n        Provided the column data, define if its numeric, data or class\n\n        :param data: a list containing each of the cells in a column\n\n        :return: type and type distribution, we can later use type_distribution to determine data quality\n        NOTE: type distribution is the count that this column has for belonging cells to each DATA_TYPE\n        """"""\n\n        type_dist = {}\n        subtype_dist = {}\n        additional_info = {\'other_potential_subtypes\': [], \'other_potential_types\': []}\n\n        if col_name in self.transaction.lmd[\'data_subtypes\']:\n            curr_data_type = self.transaction.lmd[\'data_types\'][col_name]\n            curr_data_subtype = self.transaction.lmd[\'data_subtypes\'][col_name]\n            type_dist[curr_data_type] = len(data)\n            subtype_dist[curr_data_subtype] = len(data)\n            self.log.info(f\'Manually setting the types for column {col_name} to {curr_data_type}->{curr_data_subtype}\')\n            return curr_data_type, curr_data_subtype, type_dist, subtype_dist, additional_info, \'Column ok\'\n\n        # calculate type_dist\n        if len(data) < 1:\n            self.log.warning(f\'Column {col_name} has no data in it. Please remove {col_name} from the training file or fill in some of the values !\')\n            return None, None, None, None, None, \'Column empty\'\n\n        for element in data:\n            # Maybe use list of functions in the future\n            element = str(element)\n            current_subtype_guess = \'Unknown\'\n            current_type_guess = \'Unknown\'\n\n            # Check if Nr\n            if current_subtype_guess == \'Unknown\' or current_type_guess == \'Unknown\':\n                subtype = self._is_number(element)\n                if subtype is not False:\n                    current_type_guess = DATA_TYPES.NUMERIC\n                    current_subtype_guess = subtype\n\n            # Check if date\n            if current_subtype_guess == \'Unknown\' or current_type_guess == \'Unknown\':\n                subtype = self._get_date_type(element)\n                if subtype is not False:\n                    current_type_guess = DATA_TYPES.DATE\n                    current_subtype_guess = subtype\n\n            # Check if sequence\n            if current_subtype_guess == \'Unknown\' or current_type_guess == \'Unknown\':\n                for char in [\',\',\'\\t\',\'|\',\' \']:\n                    try:\n                        all_nr = True\n                        eles = element.rstrip(\']\').lstrip(\'[\').split(char)\n                        for ele in eles:\n                            if not self._is_number(ele):\n                                all_nr = False\n                    except:\n                        all_nr = False\n                        pass\n                    if all_nr is True:\n                        additional_info[\'separator\'] = char\n                        current_type_guess = DATA_TYPES.SEQUENTIAL\n                        current_subtype_guess = DATA_SUBTYPES.ARRAY\n                        break\n\n            # Check if file\n            if current_subtype_guess == \'Unknown\' or current_type_guess == \'Unknown\':\n                subtype = self._get_file_type(element)\n                if subtype is not False:\n                    current_type_guess = DATA_TYPES.FILE_PATH\n                    current_subtype_guess = subtype\n\n            if current_type_guess not in type_dist:\n                type_dist[current_type_guess] = 1\n            else:\n                type_dist[current_type_guess] += 1\n\n            if current_subtype_guess not in subtype_dist:\n                subtype_dist[current_subtype_guess] = 1\n            else:\n                subtype_dist[current_subtype_guess] += 1\n\n\n        curr_data_type = \'Unknown\'\n        curr_data_subtype = \'Unknown\'\n        max_data_type = 0\n\n        # assume that the type is the one with the most prevalent type_dist\n        for data_type in type_dist:\n            # If any of the members are Unknown, use that data type (later to be turned into CATEGORICAL or SEQUENTIAL), since otherwise the model will crash when casting\n            # @TODO consider removing or flagging rows where data type is unknown in the future, might just be corrupt data... a bit hard to imply currently\n            if data_type == \'Unknown\':\n                curr_data_type = \'Unknown\'\n                break\n            if type_dist[data_type] > max_data_type:\n                curr_data_type = data_type\n                max_data_type = type_dist[data_type]\n\n        # If a mix of dates and numbers interpret all as dates\n        if DATA_TYPES.DATE in type_dist and len(set(type_dist.keys()) - set([DATA_TYPES.NUMERIC])) == 1:\n            if DATA_TYPES.NUMERIC in type_dist:\n                type_dist[DATA_TYPES.DATE] += type_dist[DATA_TYPES.NUMERIC]\n                del type_dist[DATA_TYPES.NUMERIC]\n\n            if DATA_SUBTYPES.FLOAT in subtype_dist:\n                subtype_dist[DATA_SUBTYPES.TIMESTAMP] += subtype_dist[DATA_SUBTYPES.FLOAT]\n                del subtype_dist[DATA_SUBTYPES.FLOAT]\n\n            if DATA_SUBTYPES.INT in subtype_dist:\n                subtype_dist[DATA_SUBTYPES.TIMESTAMP] += subtype_dist[DATA_SUBTYPES.INT]\n                del subtype_dist[DATA_SUBTYPES.INT]\n\n            curr_data_type = DATA_TYPES.DATE\n\n        # Set subtype\n        max_data_subtype = 0\n        if curr_data_type != \'Unknown\':\n            for data_subtype in subtype_dist:\n                if subtype_dist[data_subtype] > max_data_subtype and data_subtype in DATA_TYPES_SUBTYPES.subtypes[curr_data_type]:\n                    curr_data_subtype = data_subtype\n                    max_data_subtype = subtype_dist[data_subtype]\n\n        # If it finds that the type is categorical it should determine if its categorical or actual text\n        if curr_data_type == \'Unknown\':\n            curr_data_type, curr_data_subtype = self._get_text_type(data)\n            type_dist[curr_data_type] = type_dist.pop(\'Unknown\')\n            subtype_dist[curr_data_subtype] = subtype_dist.pop(\'Unknown\')\n\n        if curr_data_type != DATA_TYPES.CATEGORICAL and curr_data_subtype != DATA_SUBTYPES.DATE:\n            all_values = data_frame[col_name]\n            all_distinct_vals = set(all_values)\n\n            # The numbers here are picked randomly, the gist of it is that if values repeat themselves a lot we should consider the column to be categorical\n            nr_vals = len(all_values)\n            nr_distinct_vals = len(all_distinct_vals)\n\n            if ( nr_vals/20 > nr_distinct_vals and (curr_data_type not in [DATA_TYPES.NUMERIC, DATA_TYPES.DATE] or nr_distinct_vals < 20) ) or (curr_data_subtype == DATA_SUBTYPES.TEXT and self.transaction.lmd[\'handle_text_as_categorical\']):\n                additional_info[\'other_potential_subtypes\'].append(curr_data_type)\n                additional_info[\'other_potential_types\'].append(curr_data_subtype)\n                curr_data_type = DATA_TYPES.CATEGORICAL\n                if len(all_distinct_vals) < 3:\n                    curr_data_subtype = DATA_SUBTYPES.SINGLE\n                else:\n                    curr_data_subtype = DATA_SUBTYPES.MULTIPLE\n                type_dist = {}\n                subtype_dist = {}\n\n                type_dist[curr_data_type] = len(data)\n                subtype_dist[curr_data_subtype] = len(data)\n\n        if col_name in self.transaction.lmd[\'force_categorical_encoding\']:\n            curr_data_type = DATA_TYPES.CATEGORICAL\n            curr_data_subtype = DATA_SUBTYPES.MULTIPLE\n            type_dist[curr_data_type] = len(data)\n            subtype_dist[curr_data_subtype] = len(data)\n\n        return curr_data_type, curr_data_subtype, type_dist, subtype_dist, additional_info, \'Column ok\'\n\n    @staticmethod\n    def get_words_histogram(data, is_full_text=False):\n        """""" Returns an array of all the words that appear in the dataset and the number of times each word appears in the dataset """"""\n\n        splitter = lambda w, t: [wi.split(t) for wi in w] if isinstance(w, list) else splitter(w, t)\n\n        if is_full_text:\n            # get all words in every cell and then calculate histograms\n            words = []\n            for cell in data:\n                words += splitRecursive(cell, WORD_SEPARATORS)\n\n            hist = {i: words.count(i) for i in words}\n        else:\n            hist = {i: data.count(i) for i in data}\n\n        return {\n            \'x\': list(hist.keys()),\n            \'y\': list(hist.values())\n        }\n\n    @staticmethod\n    def get_histogram(data, data_type=None, data_subtype=None, full_text=None, hmd=None):\n        """""" Returns a histogram for the data and [optionaly] the percentage buckets""""""\n        if data_type == DATA_TYPES.SEQUENTIAL:\n            is_full_text = True if data_subtype == DATA_SUBTYPES.TEXT else False\n            return StatsGenerator.get_words_histogram(data, is_full_text), None\n        elif data_type == DATA_TYPES.NUMERIC or data_subtype == DATA_SUBTYPES.TIMESTAMP:\n            Y, X = np.histogram(data, bins=min(50,len(set(data))), range=(min(data),max(data)), density=False)\n            if data_subtype == DATA_SUBTYPES.INT:\n                Y, X = np.histogram(data, bins=[int(round(x)) for x in X], density=False)\n\n            X = X[:-1].tolist()\n            Y = Y.tolist()\n\n            return {\n                \'x\': X\n                ,\'y\': Y\n            }, X\n        elif data_type == DATA_TYPES.CATEGORICAL or data_subtype == DATA_SUBTYPES.DATE :\n            histogram = Counter(data)\n            X = np.array([str(x) for x in histogram.keys()])\n            Y = np.array(list(histogram.values()))\n\n            sorted_idx = np.argsort(Y)[::-1]\n\n            X = X[sorted_idx].tolist()\n            Y = Y[sorted_idx].tolist()\n\n            return {\n                \'x\': X,\n                \'y\': Y\n            }, X\n\n        elif data_subtype == DATA_SUBTYPES.IMAGE:\n            image_hashes = []\n            for img_path in data:\n                img_hash = imagehash.phash(Image.open(img_path))\n                seq_hash = []\n                for hash_row in img_hash.hash:\n                    seq_hash.extend(hash_row)\n\n                image_hashes.append(np.array(seq_hash))\n\n            kmeans = MiniBatchKMeans(n_clusters=20, batch_size=round(len(image_hashes)/4))\n\n            kmeans.fit(image_hashes)\n\n            if hmd is not None:\n                hmd[\'bucketing_algorithms\'][col_name] = kmeans\n\n            x = []\n            y = [0] * len(kmeans.cluster_centers_)\n\n            for cluster in kmeans.cluster_centers_:\n                similarities = cosine_similarity(image_hashes,kmeans.cluster_centers_)\n\n                similarities = list(map(lambda x: sum(x), similarities))\n\n                index_of_most_similar = similarities.index(max(similarities))\n                x.append(data.iloc[index_of_most_similar])\n\n            indices = kmeans.predict(image_hashes)\n            for index in indices:\n                y[index] +=1\n\n            return {\n                \'x\': x,\n                \'y\': y\n            }, list(kmeans.cluster_centers_)\n        else:\n            return None, None\n\n    @staticmethod\n    def is_foreign_key(column_name, column_stats, data):\n        foregin_key_type = DATA_SUBTYPES.INT in column_stats[\'other_potential_subtypes\'] or DATA_SUBTYPES.INT == column_stats[\'data_subtype\']\n\n        data_looks_like_id = True\n\n        # No need to run this check if the type already indicates a foreign key like value\n        if not foregin_key_type:\n            val_length = None\n            for val in data:\n                is_uuid = True\n                is_same_length = False\n\n                for char in str(val):\n                    if char not in [\'0\', \'1\',\'2\',\'3\',\'4\',\'5\',\'6\',\'7\',\'8\',\'9\',\'a\',\'b\',\'c\',\'d\',\'e\',\'f\',\'-\']:\n                        is_uuid = False\n\n                if val_length is not False:\n                    if val_length is None:\n                        val_length = len(str(val))\n                    if len(str(val)) == val_length:\n                        is_same_length = True\n\n                if not is_uuid and not is_same_length:\n                    data_looks_like_id = False\n                    break\n\n        foreign_key_name = False\n        for endings in [\'-id\', \'_id\', \'ID\', \'Id\']:\n            if column_name.endswith(endings):\n                foreign_key_name = True\n        for keyword in [\'account\', \'uuid\', \'identifier\', \'user\']:\n            if keyword in column_name:\n                foreign_key_name = True\n\n        return foreign_key_name and (foregin_key_type or data_looks_like_id)\n\n\n    def _log_interesting_stats(self, stats):\n        """"""\n        # Provide interesting insights about the data to the user and send them to the logging server in order for it to generate charts\n\n        :param stats: The stats extracted up until this point for all columns\n        """"""\n        for col_name in stats:\n            col_stats = stats[col_name]\n            # Overall quality\n            if \'quality_score\' in col_stats and col_stats[\'quality_score\'] < 6:\n                # Some scores are not that useful on their own, so we should only warn users about them if overall quality is bad.\n                self.log.warning(\'Column ""{}"" is considered of low quality, the scores that influenced this decision will be listed below\')\n                if \'duplicates_score\' in col_stats and col_stats[\'duplicates_score\'] < 6:\n                    duplicates_percentage = col_stats[\'duplicates_percentage\']\n                    w = f\'{duplicates_percentage}% of the values in column {col_name} seem to be repeated, this might indicate that your data is of poor quality.\'\n                    self.log.warning(w)\n                    col_stats[\'duplicates_score_warning\'] = w\n                else:\n                    col_stats[\'duplicates_score_warning\'] = None\n            else:\n                col_stats[\'duplicates_score_warning\'] = None\n\n            #Compound scores\n            if \'consistency_score\' in col_stats and  col_stats[\'consistency_score\'] < 3:\n                w = f\'The values in column {col_name} rate poorly in terms of consistency. This means that the data has too many empty values, values with a hard to determine type and duplicate values. Please see the detailed logs below for more info\'\n                self.log.warning(w)\n                col_stats[\'consistency_score_warning\'] = w\n            else:\n                col_stats[\'consistency_score_warning\'] = None\n\n            if \'redundancy_score\' in col_stats and  col_stats[\'redundancy_score\'] < 5:\n                w = f\'The data in the column {col_name} is likely somewhat redundant, any insight it can give us can already by deduced from your other columns. Please see the detailed logs below for more info\'\n                self.log.warning(w)\n                col_stats[\'redundancy_score_warning\'] = w\n            else:\n                col_stats[\'redundancy_score_warning\'] = None\n\n            if \'variability_score\' in col_stats and  col_stats[\'variability_score\'] < 6:\n                w = f\'The data in the column {col_name} seems to contain too much noise/randomness based on the value variability. That is to say, the data is too unevenly distributed and has too many outliers. Please see the detailed logs below for more info.\'\n                self.log.warning(w)\n                col_stats[\'variability_score_warning\'] = w\n            else:\n                col_stats[\'variability_score_warning\'] = None\n\n            # Some scores are meaningful on their own, and the user should be warned if they fall below a certain threshold\n            if col_stats[\'empty_cells_score\'] < 8:\n                empty_cells_percentage = col_stats[\'empty_percentage\']\n                w = f\'{empty_cells_percentage}% of the values in column {col_name} are empty, this might indicate that your data is of poor quality.\'\n                self.log.warning(w)\n                col_stats[\'empty_cells_score_warning\'] = w\n            else:\n                col_stats[\'empty_cells_score_warning\'] = None\n\n            if col_stats[\'data_type_distribution_score\'] < 7:\n                percentage_of_data_not_of_principal_type = col_stats[\'data_type_distribution_score\'] * 100\n                principal_data_type = col_stats[\'data_type\']\n                w = f\'{percentage_of_data_not_of_principal_type}% of your data is not of type {principal_data_type}, which was detected to be the data type for column {col_name}, this might indicate that your data is of poor quality.\'\n                self.log.warning(w)\n                col_stats[\'data_type_distribution_score_warning\'] = w\n            else:\n                col_stats[\'data_type_distribution_score_warning\'] = None\n\n            if \'z_test_based_outlier_score\' in col_stats and col_stats[\'z_test_based_outlier_score\'] < 6:\n                percentage_of_outliers = col_stats[\'z_test_based_outlier_score\']*100\n                w = f""""""Column {col_name} has a very high amount of outliers, {percentage_of_outliers}% of your data is more than 3 standard deviations away from the mean, this means that there might\n                be too much randomness in this column for us to make an accurate prediction based on it.""""""\n                self.log.warning(w)\n                col_stats[\'z_test_based_outlier_score_warning\'] = w\n            else:\n                col_stats[\'z_test_based_outlier_score_warning\'] = None\n\n            if \'lof_based_outlier_score\' in col_stats and col_stats[\'lof_based_outlier_score\'] < 4:\n                percentage_of_outliers = col_stats[\'percentage_of_log_based_outliers\']\n                w = f""""""Column {col_name} has a very high amount of outliers, {percentage_of_outliers}% of your data doesn\'t fit closely in any cluster using the KNN algorithm (20n) to cluster your data, this means that there might\n                be too much randomness in this column for us to make an accurate prediction based on it.""""""\n                self.log.warning(w)\n                col_stats[\'lof_based_outlier_score_warning\'] = w\n            else:\n                col_stats[\'lof_based_outlier_score_warning\'] = None\n\n            if \'value_distribution_score\' in col_stats and col_stats[\'value_distribution_score\'] < 3:\n                max_probability_key = col_stats[\'max_probability_key\']\n                w = f""""""Column {col_name} is very biased towards the value {max_probability_key}, please make sure that the data in this column is correct !""""""\n                self.log.warning(w)\n                col_stats[\'value_distribution_score_warning\'] = w\n            else:\n                col_stats[\'value_distribution_score_warning\'] = None\n\n            if \'similarity_score\' in col_stats and col_stats[\'similarity_score\'] < 6:\n                similar_percentage = col_stats[\'max_similarity\'] * 100\n                similar_col_name = col_stats[\'most_similar_column_name\']\n                w = f\'Column {col_name} and {similar_col_name} are {similar_percentage}% the same, please make sure these represent two distinct features of your data !\'\n                self.log.warning(w)\n                col_stats[\'similarity_score_warning\'] = w\n            else:\n                col_stats[\'similarity_score_warning\'] = None\n\n            \'\'\'\n            if col_stats[\'correlation_score\'] < 5:\n                not_quite_correlation_percentage = col_stats[\'correlation_score\'] * 100\n                most_correlated_column = col_stats[\'most_correlated_column\']\n                self.log.warning(f""""""Using a statistical predictor we\\\'v discovered a correlation of roughly {not_quite_correlation_percentage}% between column\n                {col_name} and column {most_correlated_column}"""""")\n            \'\'\'\n\n            # We might want to inform the user about a few stats regarding his column regardless of the score, this is done below\n            self.log.info(f""""""Data distribution for column ""{col_name}"" of type ""{stats[col_name][\'data_type\']}"" and subtype  ""{stats[col_name][\'data_subtype\']}"""""")\n            try:\n                self.log.infoChart(stats[col_name][\'data_subtype_dist\'], type=\'list\', uid=\'Data Type Distribution for column ""{}""\'.format(col_name))\n            except:\n                # Functionality is specific to mindsdb logger\n                pass\n\n    def run(self, input_data, hmd=None, print_logs=True):\n        """"""\n        # Runs the stats generation phase\n        # This shouldn\'t alter the columns themselves, but rather provide the `stats` metadata object and update the types for each column\n        # A lot of information about the data distribution and quality will  also be logged to the server in this phase\n        """"""\n\n        stats = {}\n        stats_v2 = {}\n        col_data_dict = {}\n\n        if print_logs == False:\n            self.log = logging.getLogger(\'null-logger\')\n            self.log.propagate = False\n\n        sample_df = sample_data(input_data.data_frame, self.transaction.lmd[\'sample_margin_of_error\'], self.transaction.lmd[\'sample_confidence_level\'], self.log)\n\n        for col_name in self.transaction.lmd[\'empty_columns\']:\n            stats_v2[col_name] = {}\n            stats_v2[col_name][\'empty\'] = {\'is_empty\': True}\n\n        for col_name in sample_df.columns.values:\n            stats_v2[col_name] = {}\n            stats[col_name] = {}\n\n            len_wo_nulls = len(input_data.data_frame[col_name].dropna())\n            len_w_nulls = len(input_data.data_frame[col_name])\n            len_unique = len(set(input_data.data_frame[col_name]))\n            nr_missing_values = len_w_nulls - len_wo_nulls\n\n            stats_v2[col_name][\'empty\'] = {\n                \'empty_cells\': nr_missing_values\n                ,\'empty_percentage\': 100 * round(nr_missing_values/len_w_nulls,3)\n                ,\'is_empty\': False\n                ,\'description\': \'TBD\'\n            }\n\n            if nr_missing_values > 0:\n                stats_v2[col_name][\'empty\'][\'warning\'] = f\'Your column has {nr_missing_values} values missing\'\n\n            col_data = sample_df[col_name].dropna()\n\n            data_type, data_subtype, data_type_dist, data_subtype_dist, additional_info, column_status = self._get_column_data_type(col_data, input_data.data_frame, col_name)\n\n            stats_v2[col_name][\'typing\'] = {\n                \'data_type\': data_type\n                ,\'data_subtype\': data_subtype\n                ,\'data_type_dist\': data_type_dist\n                ,\'data_subtype_dist\': data_subtype_dist\n                ,\'description\': \'TBD\'\n            }\n\n            for k  in stats_v2[col_name][\'typing\']: stats[col_name][k] = stats_v2[col_name][\'typing\'][k]\n\n            # Do some temporary processing for timestamp and numerical values\n            if data_type == DATA_TYPES.NUMERIC or data_subtype == DATA_SUBTYPES.TIMESTAMP:\n                col_data = clean_int_and_date_data(col_data, self.log)\n\n            hist_data = col_data\n            if data_type == DATA_TYPES.CATEGORICAL:\n                hist_data = input_data.data_frame[col_name]\n                stats_v2[col_name][\'unique\'] = {\n                    \'unique_values\': len_unique\n                    ,\'unique_percentage\': 100 * round(len_unique/len_w_nulls,8)\n                    ,\'description\': \'TBD\'\n                }\n\n            if len_unique == 1:\n                stats_v2[col_name][\'unique\'][\'warning\'] = \'This column contains no information because it has a single possible value.\'\n\n            histogram, percentage_buckets = StatsGenerator.get_histogram(hist_data, data_type=data_type, data_subtype=data_subtype)\n\n            stats[col_name][\'histogram\'] = histogram\n            stats[col_name][\'percentage_buckets\'] = percentage_buckets\n            stats_v2[col_name][\'histogram\'] = histogram\n            stats_v2[col_name][\'percentage_buckets\'] = percentage_buckets\n\n            stats[col_name][\'empty_cells\'] = stats_v2[col_name][\'empty\'][\'empty_cells\']\n            stats[col_name][\'empty_percentage\'] = stats_v2[col_name][\'empty\'][\'empty_percentage\']\n\n            stats_v2[col_name][\'additional_info\'] = additional_info\n            for k in additional_info:\n                stats[col_name][k] = additional_info[k]\n\n            col_data_dict[col_name] = col_data\n\n        for col_name in sample_df.columns:\n            data_type = stats_v2[col_name][\'typing\'][\'data_type\']\n            data_subtype = stats_v2[col_name][\'typing\'][\'data_subtype\']\n\n            # For now there\'s only one and computing it takes way too long, so this is not enabled\n            scores = []\n\n            for score_promise in scores:\n                # Wait for function on process to finish running\n                score = score_promise.get()\n                stats[col_name].update(score)\n\n            for score_func in [compute_duplicates_score, compute_empty_cells_score, compute_data_type_dist_score, compute_z_score, compute_lof_score, compute_similariy_score, compute_value_distribution_score]:\n                start_time = time.time()\n\n                try:\n                    if \'compute_z_score\' in str(score_func) or \'compute_lof_score\' in str(score_func):\n                        stats[col_name].update(score_func(stats, col_data_dict, col_name))\n                    else:\n                        stats[col_name].update(score_func(stats, sample_df, col_name))\n                except Exception as e:\n                    self.log.warning(e)\n\n                fun_name = str(score_func)\n                run_duration = round(time.time() - start_time, 2)\n\n            for score_func in [compute_consistency_score, compute_redundancy_score, compute_variability_score, compute_data_quality_score]:\n                try:\n                    stats[col_name].update(score_func(stats, col_name))\n                except Exception as e:\n                    self.log.warning(e)\n\n            stats[col_name][\'is_foreign_key\'] = self.is_foreign_key(col_name, stats[col_name], col_data_dict[col_name])\n            if stats[col_name][\'is_foreign_key\'] and self.transaction.lmd[\'handle_foreign_keys\']:\n                self.transaction.lmd[\'columns_to_ignore\'].append(col_name)\n\n            # New logic\n            col_data = sample_df[col_name]\n\n            if data_type in (DATA_TYPES.NUMERIC,DATA_TYPES.DATE,DATA_TYPES.CATEGORICAL) or data_subtype in (DATA_SUBTYPES.IMAGE):\n                nr_values = sum(stats_v2[col_name][\'histogram\'][\'y\'])\n                S = entropy([x/nr_values for x in stats_v2[col_name][\'histogram\'][\'y\']],base=max(2,len(stats_v2[col_name][\'histogram\'][\'y\'])))\n                stats_v2[col_name][\'bias\'] = {\n                    \'entropy\': float(S)\n                    ,\'description\': \'TBD\'\n                }\n                if S < 0.8:\n                    if data_type in (DATA_TYPES.CATEGORICAL):\n                        stats_v2[col_name][\'bias\'][\'warning\'] =  """"""You may to check if some categories occur too often to too little in this columns. This doesn\'t necessarily mean there\'s an issue with your data, it just indicates a higher than usual probability there might be some issue.""""""\n                    else:\n                        stats_v2[col_name][\'bias\'][\'warning\'] = """"""You may want to check if you see something suspicious on the right-hand-side graph. This doesn\'t necessarily mean there\'s an issue with your data, it just indicates a higher than usual probability there might be some issue""""""\n\n            if \'lof_outliers\' in stats[col_name]:\n                if data_subtype in (DATA_SUBTYPES.INT):\n                    stats[col_name][\'lof_outliers\'] = [int(x) for x in stats[col_name][\'lof_outliers\']]\n\n                stats_v2[col_name][\'outliers\'] = {\n                    \'outlier_values\': stats[col_name][\'lof_outliers\']\n                    ,\'outlier_score\': stats[col_name][\'lof_based_outlier_score\']\n                    ,\'description\': \'TBD\'\n                }\n\n                # map each bucket to list of outliers in it\n                bucket_outliers = defaultdict(list)\n                for value in stats_v2[col_name][\'outliers\'][\'outlier_values\']:\n                    vb_index = get_value_bucket(value, stats_v2[col_name][\'percentage_buckets\'], stats[col_name])\n                    vb = stats_v2[col_name][\'percentage_buckets\'][vb_index]\n                    bucket_outliers[vb].append(value)\n\n                # Filter out buckets without outliers,\n                # then sort by number of outliers in ascending order\n                buckets_with_outliers = sorted(filter(\n                    lambda kv: len(kv[1]) > 0, bucket_outliers.items()\n                ), key=lambda kv: len(kv[1]))\n\n                stats_v2[col_name][\'outliers\'][\'outlier_buckets\'] = []\n\n                for i, (bucket, outlier_values) in enumerate(buckets_with_outliers):\n                    bucket_index = stats_v2[col_name][\'histogram\'][\'x\'].index(bucket)\n\n                    bucket_values_num = stats_v2[col_name][\'histogram\'][\'y\'][bucket_index]\n                    bucket_outliers_num = len(outlier_values)\n\n                    # Is the bucket in the 95th percentile by number of outliers?\n                    percentile_outlier = ((i + 1) / len(buckets_with_outliers)) >= 0.95\n\n                    # Are half of values in the bucket outliers?\n                    predominantly_outlier = (bucket_outliers_num / bucket_values_num) > 0.5\n\n                    if predominantly_outlier or percentile_outlier:\n                        stats_v2[col_name][\'outliers\'][\'outlier_buckets\'].append(bucket)\n\n            stats_v2[col_name][\'nr_warnings\'] = len([1 for x in stats_v2[col_name].values() if isinstance(x, dict) and \'warning\' in x and x[\'warning\'] is not None])\n\n        self.transaction.lmd[\'column_stats\'] = stats\n        self.transaction.lmd[\'stats_v2\'] = stats_v2\n\n        self.transaction.lmd[\'data_preparation\'][\'accepted_margin_of_error\'] = self.transaction.lmd[\'sample_margin_of_error\']\n\n        self.transaction.lmd[\'data_preparation\'][\'total_row_count\'] = len(input_data.data_frame)\n        self.transaction.lmd[\'data_preparation\'][\'used_row_count\'] = len(sample_df)\n\n        self._log_interesting_stats(stats)\n'"
