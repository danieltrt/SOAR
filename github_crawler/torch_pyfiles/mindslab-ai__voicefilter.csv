file_path,api_count,code
generator.py,2,"b'import os\nimport glob\nimport tqdm\nimport torch\nimport random\nimport librosa\nimport argparse\nimport numpy as np\nfrom multiprocessing import Pool, cpu_count\n\nfrom utils.audio import Audio\nfrom utils.hparams import HParam\n\n\ndef formatter(dir_, form, num):\n    return os.path.join(dir_, form.replace(\'*\', \'%06d\' % num))\n\ndef vad_merge(w):\n    intervals = librosa.effects.split(w, top_db=20)\n    temp = list()\n    for s, e in intervals:\n        temp.append(w[s:e])\n    return np.concatenate(temp, axis=None)\n\n\ndef mix(hp, args, audio, num, s1_dvec, s1_target, s2, train):\n    srate = hp.audio.sample_rate\n    dir_ = os.path.join(args.out_dir, \'train\' if train else \'test\')\n\n    d, _ = librosa.load(s1_dvec, sr=srate)\n    w1, _ = librosa.load(s1_target, sr=srate)\n    w2, _ = librosa.load(s2, sr=srate)\n    assert len(d.shape) == len(w1.shape) == len(w2.shape) == 1, \\\n        \'wav files must be mono, not stereo\'\n\n    d, _ = librosa.effects.trim(d, top_db=20)\n    w1, _ = librosa.effects.trim(w1, top_db=20)\n    w2, _ = librosa.effects.trim(w2, top_db=20)\n\n    # if reference for d-vector is too short, discard it\n    if d.shape[0] < 1.1 * hp.embedder.window * hp.audio.hop_length:\n        return\n\n    # LibriSpeech dataset have many silent interval, so let\'s vad-merge them\n    # VoiceFilter paper didn\'t do that. To test SDR in same way, don\'t vad-merge.\n    if args.vad == 1:\n        w1, w2 = vad_merge(w1), vad_merge(w2)\n\n    # I think random segment length will be better, but let\'s follow the paper first\n    # fit audio to `hp.data.audio_len` seconds.\n    # if merged audio is shorter than `L`, discard it\n    L = int(srate * hp.data.audio_len)\n    if w1.shape[0] < L or w2.shape[0] < L:\n        return\n    w1, w2 = w1[:L], w2[:L]\n\n    mixed = w1 + w2\n\n    norm = np.max(np.abs(mixed)) * 1.1\n    w1, w2, mixed = w1/norm, w2/norm, mixed/norm\n\n    # save vad & normalized wav files\n    target_wav_path = formatter(dir_, hp.form.target.wav, num)\n    mixed_wav_path = formatter(dir_, hp.form.mixed.wav, num)\n    librosa.output.write_wav(target_wav_path, w1, srate)\n    librosa.output.write_wav(mixed_wav_path, mixed, srate)\n\n    # save magnitude spectrograms\n    target_mag, _ = audio.wav2spec(w1)\n    mixed_mag, _ = audio.wav2spec(mixed)\n    target_mag_path = formatter(dir_, hp.form.target.mag, num)\n    mixed_mag_path = formatter(dir_, hp.form.mixed.mag, num)\n    torch.save(torch.from_numpy(target_mag), target_mag_path)\n    torch.save(torch.from_numpy(mixed_mag), mixed_mag_path)\n\n    # save selected sample as text file. d-vec will be calculated soon\n    dvec_text_path = formatter(dir_, hp.form.dvec, num)\n    with open(dvec_text_path, \'w\') as f:\n        f.write(s1_dvec)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', type=str, required=True,\n                        help=""yaml file for configuration"")\n    parser.add_argument(\'-d\', \'--libri_dir\', type=str, default=None,\n                        help=""Directory of LibriSpeech dataset, containing folders of train-clean-100, train-clean-360, dev-clean."")\n    parser.add_argument(\'-v\', \'--voxceleb_dir\', type=str, default=None,\n                        help=""Directory of VoxCeleb2 dataset, ends with \'aac\'"")\n    parser.add_argument(\'-o\', \'--out_dir\', type=str, required=True,\n                        help=""Directory of output training triplet"")\n    parser.add_argument(\'-p\', \'--process_num\', type=int, default=None,\n                        help=\'number of processes to run. default: cpu_count\')\n    parser.add_argument(\'--vad\', type=int, default=0,\n                        help=\'apply vad to wav file. yes(1) or no(0, default)\')\n    args = parser.parse_args()\n\n    os.makedirs(args.out_dir, exist_ok=True)\n    os.makedirs(os.path.join(args.out_dir, \'train\'), exist_ok=True)\n    os.makedirs(os.path.join(args.out_dir, \'test\'), exist_ok=True)\n\n    hp = HParam(args.config)\n\n    cpu_num = cpu_count() if args.process_num is None else args.process_num\n\n    if args.libri_dir is None and args.voxceleb_dir is None:\n        raise Exception(""Please provide directory of data"")\n\n    if args.libri_dir is not None:\n        train_folders = [x for x in glob.glob(os.path.join(args.libri_dir, \'train-clean-100\', \'*\'))\n                            if os.path.isdir(x)] + \\\n                        [x for x in glob.glob(os.path.join(args.libri_dir, \'train-clean-360\', \'*\'))\n                            if os.path.isdir(x)]\n                        # we recommned to exclude train-other-500\n                        # See https://github.com/mindslab-ai/voicefilter/issues/5#issuecomment-497746793\n                        # + \\\n                        #[x for x in glob.glob(os.path.join(args.libri_dir, \'train-other-500\', \'*\'))\n                        #    if os.path.isdir(x)]\n        test_folders = [x for x in glob.glob(os.path.join(args.libri_dir, \'dev-clean\', \'*\'))]\n\n    elif args.voxceleb_dir is not None:\n        all_folders = [x for x in glob.glob(os.path.join(args.voxceleb_dir, \'*\'))\n                            if os.path.isdir(x)]\n        train_folders = all_folders[:-20]\n        test_folders = all_folders[-20:]\n\n    train_spk = [glob.glob(os.path.join(spk, \'**\', hp.form.input), recursive=True)\n                    for spk in train_folders]\n    train_spk = [x for x in train_spk if len(x) >= 2]\n\n    test_spk = [glob.glob(os.path.join(spk, \'**\', hp.form.input), recursive=True)\n                    for spk in test_folders]\n    test_spk = [x for x in test_spk if len(x) >= 2]\n\n    audio = Audio(hp)\n\n    def train_wrapper(num):\n        spk1, spk2 = random.sample(train_spk, 2)\n        s1_dvec, s1_target = random.sample(spk1, 2)\n        s2 = random.choice(spk2)\n        mix(hp, args, audio, num, s1_dvec, s1_target, s2, train=True)\n\n    def test_wrapper(num):\n        spk1, spk2 = random.sample(test_spk, 2)\n        s1_dvec, s1_target = random.sample(spk1, 2)\n        s2 = random.choice(spk2)\n        mix(hp, args, audio, num, s1_dvec, s1_target, s2, train=False)\n\n    arr = list(range(10**5))\n    with Pool(cpu_num) as p:\n        r = list(tqdm.tqdm(p.imap(train_wrapper, arr), total=len(arr)))\n\n    arr = list(range(10**2))\n    with Pool(cpu_num) as p:\n        r = list(tqdm.tqdm(p.imap(test_wrapper, arr), total=len(arr)))\n'"
inference.py,5,"b'import os\nimport glob\nimport torch\nimport librosa\nimport argparse\n\nfrom utils.audio import Audio\nfrom utils.hparams import HParam\nfrom model.model import VoiceFilter\nfrom model.embedder import SpeechEmbedder\n\n\ndef main(args, hp):\n    with torch.no_grad():\n        model = VoiceFilter(hp).cuda()\n        chkpt_model = torch.load(args.checkpoint_path)[\'model\']\n        model.load_state_dict(chkpt_model)\n        model.eval()\n\n        embedder = SpeechEmbedder(hp).cuda()\n        chkpt_embed = torch.load(args.embedder_path)\n        embedder.load_state_dict(chkpt_embed)\n        embedder.eval()\n\n        audio = Audio(hp)\n        dvec_wav, _ = librosa.load(args.reference_file, sr=16000)\n        dvec_mel = audio.get_mel(dvec_wav)\n        dvec_mel = torch.from_numpy(dvec_mel).float().cuda()\n        dvec = embedder(dvec_mel)\n        dvec = dvec.unsqueeze(0)\n\n        mixed_wav, _ = librosa.load(args.mixed_file, sr=16000)\n        mag, phase = audio.wav2spec(mixed_wav)\n        mag = torch.from_numpy(mag).float().cuda()\n\n        mag = mag.unsqueeze(0)\n        mask = model(mag, dvec)\n        est_mag = mag * mask\n\n        est_mag = est_mag[0].cpu().detach().numpy()\n        est_wav = audio.spec2wav(est_mag, phase)\n\n        os.makedirs(args.out_dir, exist_ok=True)\n        out_path = os.path.join(args.out_dir, \'result.wav\')\n        librosa.output.write_wav(out_path, est_wav, sr=16000)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-c\', \'--config\', type=str, required=True,\n                        help=""yaml file for configuration"")\n    parser.add_argument(\'-e\', \'--embedder_path\', type=str, required=True,\n                        help=""path of embedder model pt file"")\n    parser.add_argument(\'--checkpoint_path\', type=str, default=None,\n                        help=""path of checkpoint pt file"")\n    parser.add_argument(\'-m\', \'--mixed_file\', type=str, required=True,\n                        help=\'path of mixed wav file\')\n    parser.add_argument(\'-r\', \'--reference_file\', type=str, required=True,\n                        help=\'path of reference wav file\')\n    parser.add_argument(\'-o\', \'--out_dir\', type=str, required=True,\n                        help=\'directory of output\')\n\n    args = parser.parse_args()\n\n    hp = HParam(args.config)\n\n    main(args, hp)\n'"
trainer.py,0,"b'import os\nimport time\nimport logging\nimport argparse\n\nfrom utils.train import train\nfrom utils.hparams import HParam\nfrom utils.writer import MyWriter\nfrom datasets.dataloader import create_dataloader\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'-b\', \'--base_dir\', type=str, default=\'.\',\n                        help=""Root directory of run."")\n    parser.add_argument(\'-c\', \'--config\', type=str, required=True,\n                        help=""yaml file for configuration"")\n    parser.add_argument(\'-e\', \'--embedder_path\', type=str, required=True,\n                        help=""path of embedder model pt file"")\n    parser.add_argument(\'--checkpoint_path\', type=str, default=None,\n                        help=""path of checkpoint pt file"")\n    parser.add_argument(\'-m\', \'--model\', type=str, required=True,\n                        help=""Name of the model. Used for both logging and saving checkpoints."")\n    args = parser.parse_args()\n\n    hp = HParam(args.config)\n    with open(args.config, \'r\') as f:\n        # store hparams as string\n        hp_str = \'\'.join(f.readlines())\n\n    pt_dir = os.path.join(args.base_dir, hp.log.chkpt_dir, args.model)\n    os.makedirs(pt_dir, exist_ok=True)\n\n    log_dir = os.path.join(args.base_dir, hp.log.log_dir, args.model)\n    os.makedirs(log_dir, exist_ok=True)\n\n    chkpt_path = args.checkpoint_path if args.checkpoint_path is not None else None\n\n    logging.basicConfig(\n        level=logging.INFO,\n        format=\'%(asctime)s - %(levelname)s - %(message)s\',\n        handlers=[\n            logging.FileHandler(os.path.join(log_dir,\n                \'%s-%d.log\' % (args.model, time.time()))),\n            logging.StreamHandler()\n        ]\n    )\n    logger = logging.getLogger()\n\n    if hp.data.train_dir == \'\' or hp.data.test_dir == \'\':\n        logger.error(""train_dir, test_dir cannot be empty."")\n        raise Exception(""Please specify directories of data in %s"" % args.config)\n\n    writer = MyWriter(hp, log_dir)\n\n    trainloader = create_dataloader(hp, args, train=True)\n    testloader = create_dataloader(hp, args, train=False)\n\n    train(args, pt_dir, chkpt_path, trainloader, testloader, writer, logger, hp, hp_str)\n'"
datasets/dataloader.py,9,"b'import os\nimport glob\nimport torch\nimport librosa\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom utils.audio import Audio\n\n\ndef create_dataloader(hp, args, train):\n    def train_collate_fn(batch):\n        dvec_list = list()\n        target_mag_list = list()\n        mixed_mag_list = list()\n\n        for dvec_mel, target_mag, mixed_mag in batch:\n            dvec_list.append(dvec_mel)\n            target_mag_list.append(target_mag)\n            mixed_mag_list.append(mixed_mag)\n        target_mag_list = torch.stack(target_mag_list, dim=0)\n        mixed_mag_list = torch.stack(mixed_mag_list, dim=0)\n\n        return dvec_list, target_mag_list, mixed_mag_list\n\n    def test_collate_fn(batch):\n        return batch\n\n    if train:\n        return DataLoader(dataset=VFDataset(hp, args, True),\n                          batch_size=hp.train.batch_size,\n                          shuffle=True,\n                          num_workers=hp.train.num_workers,\n                          collate_fn=train_collate_fn,\n                          pin_memory=True,\n                          drop_last=True,\n                          sampler=None)\n    else:\n        return DataLoader(dataset=VFDataset(hp, args, False),\n                          collate_fn=test_collate_fn,\n                          batch_size=1, shuffle=False, num_workers=0)\n\n\nclass VFDataset(Dataset):\n    def __init__(self, hp, args, train):\n        def find_all(file_format):\n            return sorted(glob.glob(os.path.join(self.data_dir, file_format)))\n        self.hp = hp\n        self.args = args\n        self.train = train\n        self.data_dir = hp.data.train_dir if train else hp.data.test_dir\n\n        self.dvec_list = find_all(hp.form.dvec)\n        self.target_wav_list = find_all(hp.form.target.wav)\n        self.mixed_wav_list = find_all(hp.form.mixed.wav)\n        self.target_mag_list = find_all(hp.form.target.mag)\n        self.mixed_mag_list = find_all(hp.form.mixed.mag)\n\n        assert len(self.dvec_list) == len(self.target_wav_list) == len(self.mixed_wav_list) == \\\n            len(self.target_mag_list) == len(self.mixed_mag_list), ""number of training files must match""\n        assert len(self.dvec_list) != 0, \\\n            ""no training file found""\n\n        self.audio = Audio(hp)\n\n    def __len__(self):\n        return len(self.dvec_list)\n\n    def __getitem__(self, idx):\n        with open(self.dvec_list[idx], \'r\') as f:\n            dvec_path = f.readline().strip()\n\n        dvec_wav, _ = librosa.load(dvec_path, sr=self.hp.audio.sample_rate)\n        dvec_mel = self.audio.get_mel(dvec_wav)\n        dvec_mel = torch.from_numpy(dvec_mel).float()\n\n        if self.train: # need to be fast\n            target_mag = torch.load(self.target_mag_list[idx])\n            mixed_mag = torch.load(self.mixed_mag_list[idx])\n            return dvec_mel, target_mag, mixed_mag\n        else:\n            target_wav, _ = librosa.load(self.target_wav_list[idx], self.hp.audio.sample_rate)\n            mixed_wav, _ = librosa.load(self.mixed_wav_list[idx], self.hp.audio.sample_rate)\n            target_mag, _ = self.wav2magphase(self.target_wav_list[idx])\n            mixed_mag, mixed_phase = self.wav2magphase(self.mixed_wav_list[idx])\n            target_mag = torch.from_numpy(target_mag)\n            mixed_mag = torch.from_numpy(mixed_mag)\n            # mixed_phase = torch.from_numpy(mixed_phase)\n            return dvec_mel, target_wav, mixed_wav, target_mag, mixed_mag, mixed_phase\n\n    def wav2magphase(self, path):\n        wav, _ = librosa.load(path, self.hp.audio.sample_rate)\n        mag, phase = self.audio.wav2spec(wav)\n        return mag, phase\n'"
model/embedder.py,2,"b""import torch\nimport torch.nn as nn\n\n\nclass LinearNorm(nn.Module):\n    def __init__(self, hp):\n        super(LinearNorm, self).__init__()\n        self.linear_layer = nn.Linear(hp.embedder.lstm_hidden, hp.embedder.emb_dim)\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass SpeechEmbedder(nn.Module):\n    def __init__(self, hp):\n        super(SpeechEmbedder, self).__init__()\n        self.lstm = nn.LSTM(hp.embedder.num_mels,\n                            hp.embedder.lstm_hidden,\n                            num_layers=hp.embedder.lstm_layers,\n                            batch_first=True)\n        self.proj = LinearNorm(hp)\n        self.hp = hp\n\n    def forward(self, mel):\n        # (num_mels, T)\n        mels = mel.unfold(1, self.hp.embedder.window, self.hp.embedder.stride) # (num_mels, T', window)\n        mels = mels.permute(1, 2, 0) # (T', window, num_mels)\n        x, _ = self.lstm(mels) # (T', window, lstm_hidden)\n        x = x[:, -1, :] # (T', lstm_hidden), use last frame only\n        x = self.proj(x) # (T', emb_dim)\n        x = x / torch.norm(x, p=2, dim=1, keepdim=True) # (T', emb_dim)\n        x = x.sum(0) / x.size(0) # (emb_dim), average pooling over time frames\n        return x\n"""
model/model.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass VoiceFilter(nn.Module):\n    def __init__(self, hp):\n        super(VoiceFilter, self).__init__()\n        self.hp = hp\n        assert hp.audio.n_fft // 2 + 1 == hp.audio.num_freq == hp.model.fc2_dim, \\\n            ""stft-related dimension mismatch""\n\n        self.conv = nn.Sequential(\n            # cnn1\n            nn.ZeroPad2d((3, 3, 0, 0)),\n            nn.Conv2d(1, 64, kernel_size=(1, 7), dilation=(1, 1)),\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn2\n            nn.ZeroPad2d((0, 0, 3, 3)),\n            nn.Conv2d(64, 64, kernel_size=(7, 1), dilation=(1, 1)),\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn3\n            nn.ZeroPad2d(2),\n            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(1, 1)),\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn4\n            nn.ZeroPad2d((2, 2, 4, 4)),\n            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(2, 1)), # (9, 5)\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn5\n            nn.ZeroPad2d((2, 2, 8, 8)),\n            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(4, 1)), # (17, 5)\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn6\n            nn.ZeroPad2d((2, 2, 16, 16)),\n            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(8, 1)), # (33, 5)\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn7\n            nn.ZeroPad2d((2, 2, 32, 32)),\n            nn.Conv2d(64, 64, kernel_size=(5, 5), dilation=(16, 1)), # (65, 5)\n            nn.BatchNorm2d(64), nn.ReLU(),\n\n            # cnn8\n            nn.Conv2d(64, 8, kernel_size=(1, 1), dilation=(1, 1)), \n            nn.BatchNorm2d(8), nn.ReLU(),\n        )\n\n        self.lstm = nn.LSTM(\n            8*hp.audio.num_freq + hp.embedder.emb_dim,\n            hp.model.lstm_dim,\n            batch_first=True,\n            bidirectional=True)\n\n        self.fc1 = nn.Linear(2*hp.model.lstm_dim, hp.model.fc1_dim)\n        self.fc2 = nn.Linear(hp.model.fc1_dim, hp.model.fc2_dim)\n\n    def forward(self, x, dvec):\n        # x: [B, T, num_freq]\n        x = x.unsqueeze(1)\n        # x: [B, 1, T, num_freq]\n        x = self.conv(x)\n        # x: [B, 8, T, num_freq]\n        x = x.transpose(1, 2).contiguous()\n        # x: [B, T, 8, num_freq]\n        x = x.view(x.size(0), x.size(1), -1)\n        # x: [B, T, 8*num_freq]\n\n        # dvec: [B, emb_dim]\n        dvec = dvec.unsqueeze(1)\n        dvec = dvec.repeat(1, x.size(1), 1)\n        # dvec: [B, T, emb_dim]\n\n        x = torch.cat((x, dvec), dim=2) # [B, T, 8*num_freq + emb_dim]\n\n        x, _ = self.lstm(x) # [B, T, 2*lstm_dim]\n        x = F.relu(x)\n        x = self.fc1(x) # x: [B, T, fc1_dim]\n        x = F.relu(x)\n        x = self.fc2(x) # x: [B, T, fc2_dim], fc2_dim == num_freq\n        x = torch.sigmoid(x)\n        return x\n'"
utils/adabound.py,6,"b'# from https://github.com/Luolc/AdaBound, commit 6fa8260\n# https://github.com/Luolc/AdaBound/tree/6fa826003f41a57501bde3e2baab1488410fe2da\n\nimport math\nimport torch\nfrom torch.optim import Optimizer\n\n\nclass AdaBound(Optimizer):\n    """"""Implements AdaBound algorithm.\n    It has been proposed in `Adaptive Gradient Methods with Dynamic Bound of Learning Rate`_.\n    Arguments:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float, optional): Adam learning rate (default: 1e-3)\n        betas (Tuple[float, float], optional): coefficients used for computing\n            running averages of gradient and its square (default: (0.9, 0.999))\n        final_lr (float, optional): final (SGD) learning rate (default: 0.1)\n        gamma (float, optional): convergence speed of the bound functions (default: 1e-3)\n        eps (float, optional): term added to the denominator to improve\n            numerical stability (default: 1e-8)\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        amsbound (boolean, optional): whether to use the AMSBound variant of this algorithm\n    .. Adaptive Gradient Methods with Dynamic Bound of Learning Rate:\n        https://openreview.net/forum?id=Bkg3g2R9FX\n    """"""\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), final_lr=0.1, gamma=1e-3,\n                 eps=1e-8, weight_decay=0, amsbound=False):\n        if not 0.0 <= lr:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n        if not 0.0 <= final_lr:\n            raise ValueError(""Invalid final learning rate: {}"".format(final_lr))\n        if not 0.0 <= gamma < 1.0:\n            raise ValueError(""Invalid gamma parameter: {}"".format(gamma))\n        defaults = dict(lr=lr, betas=betas, final_lr=final_lr, gamma=gamma, eps=eps,\n                        weight_decay=weight_decay, amsbound=amsbound)\n        super(AdaBound, self).__init__(params, defaults)\n\n        self.base_lrs = list(map(lambda group: group[\'lr\'], self.param_groups))\n\n    def __setstate__(self, state):\n        super(AdaBound, self).__setstate__(state)\n        for group in self.param_groups:\n            group.setdefault(\'amsbound\', False)\n\n    def step(self, closure=None):\n        """"""Performs a single optimization step.\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        """"""\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group, base_lr in zip(self.param_groups, self.base_lrs):\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                if grad.is_sparse:\n                    raise RuntimeError(\n                        \'Adam does not support sparse gradients, please consider SparseAdam instead\')\n                amsbound = group[\'amsbound\']\n\n                state = self.state[p]\n\n                # State initialization\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    # Exponential moving average of gradient values\n                    state[\'exp_avg\'] = torch.zeros_like(p.data)\n                    # Exponential moving average of squared gradient values\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p.data)\n                    if amsbound:\n                        # Maintains max of all exp. moving avg. of sq. grad. values\n                        state[\'max_exp_avg_sq\'] = torch.zeros_like(p.data)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                if amsbound:\n                    max_exp_avg_sq = state[\'max_exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                state[\'step\'] += 1\n\n                if group[\'weight_decay\'] != 0:\n                    grad = grad.add(group[\'weight_decay\'], p.data)\n\n                # Decay the first and second moment running average coefficient\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                if amsbound:\n                    # Maintains the maximum of all 2nd moment running avg. till now\n                    torch.max(max_exp_avg_sq, exp_avg_sq, out=max_exp_avg_sq)\n                    # Use the max. for normalizing running avg. of gradient\n                    denom = max_exp_avg_sq.sqrt().add_(group[\'eps\'])\n                else:\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n\n                bias_correction1 = 1 - beta1 ** state[\'step\']\n                bias_correction2 = 1 - beta2 ** state[\'step\']\n                step_size = group[\'lr\'] * math.sqrt(bias_correction2) / bias_correction1\n\n                # Applies bounds on actual learning rate\n                # lr_scheduler cannot affect final_lr, this is a workaround to apply lr decay\n                final_lr = group[\'final_lr\'] * group[\'lr\'] / base_lr\n                lower_bound = final_lr * (1 - 1 / (group[\'gamma\'] * state[\'step\'] + 1))\n                upper_bound = final_lr * (1 + 1 / (group[\'gamma\'] * state[\'step\']))\n                step_size = torch.full_like(denom, step_size)\n                step_size.div_(denom).clamp_(lower_bound, upper_bound).mul_(exp_avg)\n\n                p.data.add_(-step_size)\n\n        return loss'"
utils/audio.py,0,"b""# adapted from Keith Ito's tacotron implementation\n# https://github.com/keithito/tacotron/blob/master/util/audio.py\n\nimport librosa\nimport numpy as np\n\n\nclass Audio():\n    def __init__(self, hp):\n        self.hp = hp\n        self.mel_basis = librosa.filters.mel(sr=hp.audio.sample_rate,\n                                             n_fft=hp.embedder.n_fft,\n                                             n_mels=hp.embedder.num_mels)\n\n    def get_mel(self, y):\n        y = librosa.core.stft(y=y, n_fft=self.hp.embedder.n_fft,\n                              hop_length=self.hp.audio.hop_length,\n                              win_length=self.hp.audio.win_length,\n                              window='hann')\n        magnitudes = np.abs(y) ** 2\n        mel = np.log10(np.dot(self.mel_basis, magnitudes) + 1e-6)\n        return mel\n\n    def wav2spec(self, y):\n        D = self.stft(y)\n        S = self.amp_to_db(np.abs(D)) - self.hp.audio.ref_level_db\n        S, D = self.normalize(S), np.angle(D)\n        S, D = S.T, D.T # to make [time, freq]\n        return S, D\n\n    def spec2wav(self, spectrogram, phase):\n        spectrogram, phase = spectrogram.T, phase.T\n        # used during inference only\n        # spectrogram: enhanced output\n        # phase: use noisy input's phase, so no GLA is required\n        S = self.db_to_amp(self.denormalize(spectrogram) + self.hp.audio.ref_level_db)\n        return self.istft(S, phase)\n\n    def stft(self, y):\n        return librosa.stft(y=y, n_fft=self.hp.audio.n_fft,\n                            hop_length=self.hp.audio.hop_length,\n                            win_length=self.hp.audio.win_length)\n\n    def istft(self, mag, phase):\n        stft_matrix = mag * np.exp(1j*phase)\n        return librosa.istft(stft_matrix,\n                             hop_length=self.hp.audio.hop_length,\n                             win_length=self.hp.audio.win_length)\n\n    def amp_to_db(self, x):\n        return 20.0 * np.log10(np.maximum(1e-5, x))\n\n    def db_to_amp(self, x):\n        return np.power(10.0, x * 0.05)\n\n    def normalize(self, S):\n        return np.clip(S / -self.hp.audio.min_level_db, -1.0, 0.0) + 1.0\n\n    def denormalize(self, S):\n        return (np.clip(S, 0.0, 1.0) - 1.0) * -self.hp.audio.min_level_db\n"""
utils/evaluation.py,2,"b'import torch\nimport torch.nn as nn\nfrom mir_eval.separation import bss_eval_sources\n\n\ndef validate(audio, model, embedder, testloader, writer, step):\n    model.eval()\n    \n    criterion = nn.MSELoss()\n    with torch.no_grad():\n        for batch in testloader:\n            dvec_mel, target_wav, mixed_wav, target_mag, mixed_mag, mixed_phase = batch[0]\n\n            dvec_mel = dvec_mel.cuda()\n            target_mag = target_mag.unsqueeze(0).cuda()\n            mixed_mag = mixed_mag.unsqueeze(0).cuda()\n\n            dvec = embedder(dvec_mel)\n            dvec = dvec.unsqueeze(0)\n            est_mask = model(mixed_mag, dvec)\n            est_mag = est_mask * mixed_mag\n            test_loss = criterion(target_mag, est_mag).item()\n\n            mixed_mag = mixed_mag[0].cpu().detach().numpy()\n            target_mag = target_mag[0].cpu().detach().numpy()\n            est_mag = est_mag[0].cpu().detach().numpy()\n            est_wav = audio.spec2wav(est_mag, mixed_phase)\n            est_mask = est_mask[0].cpu().detach().numpy()\n\n            sdr = bss_eval_sources(target_wav, est_wav, False)[0][0]\n            writer.log_evaluation(test_loss, sdr,\n                                  mixed_wav, target_wav, est_wav,\n                                  mixed_mag.T, target_mag.T, est_mag.T, est_mask.T,\n                                  step)\n            break\n\n    model.train()\n'"
utils/hparams.py,0,"b'# modified from https://github.com/HarryVolek/PyTorch_Speaker_Verification\n\nimport os\nimport yaml\n\n\ndef load_hparam_str(hp_str):\n    path = os.path.join(\'config\', \'temp-restore.yaml\')\n    with open(path, \'w\') as f:\n        f.write(hp_str)\n    return HParam(path)\n\n\ndef load_hparam(filename):\n    stream = open(filename, \'r\')\n    docs = yaml.load_all(stream)\n    hparam_dict = dict()\n    for doc in docs:\n        for k, v in doc.items():\n            hparam_dict[k] = v\n    return hparam_dict\n\n\ndef merge_dict(user, default):\n    if isinstance(user, dict) and isinstance(default, dict):\n        for k, v in default.items():\n            if k not in user:\n                user[k] = v\n            else:\n                user[k] = merge_dict(user[k], v)\n    return user\n\n\nclass Dotdict(dict):\n    """"""\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({\'val1\':\'first\'})\n    set attributes: d.val2 = \'second\' or d[\'val2\'] = \'second\'\n    get attributes: d.val2 or d[\'val2\']\n    """"""\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, \'keys\'):\n                value = Dotdict(value)\n            self[key] = value\n\n\nclass HParam(Dotdict):\n\n    def __init__(self, file):\n        super(Dotdict, self).__init__()\n        hp_dict = load_hparam(file)\n        hp_dotdict = Dotdict(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n            \n    __getattr__ = Dotdict.__getitem__\n    __setattr__ = Dotdict.__setitem__\n    __delattr__ = Dotdict.__delitem__\n'"
utils/plotting.py,0,"b""import matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pylab as plt\nimport numpy as np\n\n\ndef fig2np(fig):\n    data = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep='')\n    data = data.reshape(fig.canvas.get_width_height()[::-1] + (3,))\n    return data\n\ndef plot_spectrogram_to_numpy(spectrogram):\n    fig, ax = plt.subplots(figsize=(12, 3))\n    im = ax.imshow(spectrogram, aspect='auto', origin='lower',\n                   interpolation='none')\n    plt.colorbar(im, ax=ax)\n    plt.xlabel('Frames')\n    plt.ylabel('Channels')\n    plt.tight_layout()\n\n    fig.canvas.draw()\n    data = fig2np(fig)\n    plt.close()\n    return data\n"""
utils/train.py,8,"b'import os\nimport math\nimport torch\nimport torch.nn as nn\nimport traceback\n\nfrom .adabound import AdaBound\nfrom .audio import Audio\nfrom .evaluation import validate\nfrom model.model import VoiceFilter\nfrom model.embedder import SpeechEmbedder\n\n\ndef train(args, pt_dir, chkpt_path, trainloader, testloader, writer, logger, hp, hp_str):\n    # load embedder\n    embedder_pt = torch.load(args.embedder_path)\n    embedder = SpeechEmbedder(hp).cuda()\n    embedder.load_state_dict(embedder_pt)\n    embedder.eval()\n\n    audio = Audio(hp)\n    model = VoiceFilter(hp).cuda()\n    if hp.train.optimizer == \'adabound\':\n        optimizer = AdaBound(model.parameters(),\n                             lr=hp.train.adabound.initial,\n                             final_lr=hp.train.adabound.final)\n    elif hp.train.optimizer == \'adam\':\n        optimizer = torch.optim.Adam(model.parameters(),\n                                     lr=hp.train.adam)\n    else:\n        raise Exception(""%s optimizer not supported"" % hp.train.optimizer)\n\n    step = 0\n\n    if chkpt_path is not None:\n        logger.info(""Resuming from checkpoint: %s"" % chkpt_path)\n        checkpoint = torch.load(chkpt_path)\n        model.load_state_dict(checkpoint[\'model\'])\n        optimizer.load_state_dict(checkpoint[\'optimizer\'])\n        step = checkpoint[\'step\']\n\n        # will use new given hparams.\n        if hp_str != checkpoint[\'hp_str\']:\n            logger.warning(""New hparams is different from checkpoint."")\n    else:\n        logger.info(""Starting new training run"")\n\n    try:\n        criterion = nn.MSELoss()\n        while True:\n            model.train()\n            for dvec_mels, target_mag, mixed_mag in trainloader:\n                target_mag = target_mag.cuda()\n                mixed_mag = mixed_mag.cuda()\n\n                dvec_list = list()\n                for mel in dvec_mels:\n                    mel = mel.cuda()\n                    dvec = embedder(mel)\n                    dvec_list.append(dvec)\n                dvec = torch.stack(dvec_list, dim=0)\n                dvec = dvec.detach()\n\n                mask = model(mixed_mag, dvec)\n                output = mixed_mag * mask\n\n                # output = torch.pow(torch.clamp(output, min=0.0), hp.audio.power)\n                # target_mag = torch.pow(torch.clamp(target_mag, min=0.0), hp.audio.power)\n                loss = criterion(output, target_mag)\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                step += 1\n\n                loss = loss.item()\n                if loss > 1e8 or math.isnan(loss):\n                    logger.error(""Loss exploded to %.02f at step %d!"" % (loss, step))\n                    raise Exception(""Loss exploded"")\n\n                # write loss to tensorboard\n                if step % hp.train.summary_interval == 0:\n                    writer.log_training(loss, step)\n                    logger.info(""Wrote summary at step %d"" % step)\n\n                # 1. save checkpoint file to resume training\n                # 2. evaluate and save sample to tensorboard\n                if step % hp.train.checkpoint_interval == 0:\n                    save_path = os.path.join(pt_dir, \'chkpt_%d.pt\' % step)\n                    torch.save({\n                        \'model\': model.state_dict(),\n                        \'optimizer\': optimizer.state_dict(),\n                        \'step\': step,\n                        \'hp_str\': hp_str,\n                    }, save_path)\n                    logger.info(""Saved checkpoint to: %s"" % save_path)\n                    validate(audio, model, embedder, testloader, writer, step)\n    except Exception as e:\n        logger.info(""Exiting due to exception: %s"" % e)\n        traceback.print_exc()\n'"
utils/writer.py,0,"b""import numpy as np\nfrom tensorboardX import SummaryWriter\n\nfrom .plotting import plot_spectrogram_to_numpy\n\n\nclass MyWriter(SummaryWriter):\n    def __init__(self, hp, logdir):\n        super(MyWriter, self).__init__(logdir)\n        self.hp = hp\n\n    def log_training(self, train_loss, step):\n        self.add_scalar('train_loss', train_loss, step)\n\n    def log_evaluation(self, test_loss, sdr,\n                       mixed_wav, target_wav, est_wav,\n                       mixed_spec, target_spec, est_spec, est_mask,\n                       step):\n        \n        self.add_scalar('test_loss', test_loss, step)\n        self.add_scalar('SDR', sdr, step)\n\n        self.add_audio('mixed_wav', mixed_wav, step, self.hp.audio.sample_rate)\n        self.add_audio('target_wav', target_wav, step, self.hp.audio.sample_rate)\n        self.add_audio('estimated_wav', est_wav, step, self.hp.audio.sample_rate)\n\n        self.add_image('data/mixed_spectrogram',\n            plot_spectrogram_to_numpy(mixed_spec), step, dataformats='HWC')\n        self.add_image('data/target_spectrogram',\n            plot_spectrogram_to_numpy(target_spec), step, dataformats='HWC')\n        self.add_image('result/estimated_spectrogram',\n            plot_spectrogram_to_numpy(est_spec), step, dataformats='HWC')\n        self.add_image('result/estimated_mask',\n            plot_spectrogram_to_numpy(est_mask), step, dataformats='HWC')\n        self.add_image('result/estimation_error_sq',\n            plot_spectrogram_to_numpy(np.square(est_spec - target_spec)), step, dataformats='HWC')\n"""
