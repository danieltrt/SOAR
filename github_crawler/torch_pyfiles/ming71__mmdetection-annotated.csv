file_path,api_count,code
demo.py,0,"b""\nimport os\nfrom mmdet.apis import init_detector, inference_detector, show_result\n\nif __name__ == '__main__':\n\tconfig_file = 'configs/faster_rcnn_r50_fpn_1x.py'\n\tcheckpoint_file = 'weights/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth'\n\t# checkpoint_file = 'tools/work_dirs/mask_rcnn_r101_fpn_1x/epoch_1200.pth'\n\timg_path = '/home/bit/\xe4\xb8\x8b\xe8\xbd\xbd/n07753592'\n\n\tmodel = init_detector(config_file, checkpoint_file, device='cuda:0')\n\n\t# print(model)\n\nif os.path.isdir(img_path):\n\timgs= os.listdir(img_path)\n\tfor i in range(len(imgs)):\n\t\timgs[i]=os.path.join(img_path,imgs[i])\n\tfor i, result in enumerate(inference_detector(model, imgs)):\t# \xe6\x94\xaf\xe6\x8c\x81\xe5\x8f\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe8\xbe\x93\xe5\x85\xa5imgs\n\t    print(i, imgs[i])\n\t    show_result(imgs[i], result, model.CLASSES, out_file='output/result_{}.jpg'.format(i))\n\nelif os.path.isfile(img_path):\n\tresult = inference_detector(model, img_path)\n\tshow_result(img_path, result, model.CLASSES)\n\n"""
hook.py,1,"b""# \xe4\xbb\x85\xe7\x94\xa8\xe4\xbd\x9c\xe7\xae\x80\xe5\x8d\x95\xe7\x9a\x84\xe4\xb8\xad\xe9\x97\xb4\xe5\x8f\x98\xe9\x87\x8f\xe8\xbe\x93\xe5\x87\xba\xe5\x92\x8c\xe8\xb0\x83\xe7\x94\xa8hook\xef\xbc\x8c\xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8ammdetection\xe9\x9b\x86\xe6\x88\x90\xe4\xba\x86\xe8\xbe\x83\xe5\xae\x8c\xe5\xa4\x87\xe7\x9a\x84hook\xe7\xb3\xbb\xe7\xbb\x9f\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe8\xaf\xbb\xe6\x87\x82\xe5\xba\x95\xe5\xb1\x82\xe4\xbb\xa3\xe7\xa0\x81\xe6\x97\xa0\xe9\x9c\x80\xe8\xbf\x99\xe6\xa0\xb7\xe8\x87\xaa\xe5\xb7\xb1\xe5\x86\x99hook\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xb0\x83\xe7\x94\xa8\xe5\xb0\xb1\xe8\xa1\x8c\nimport mmcv\nimport torch\nfrom mmcv.runner import load_checkpoint\nfrom mmdet.models import build_detector\nfrom mmdet.apis import inference_detector, show_result\nimport ipdb\n\ndef roialign_forward(module,input,output):\n\tprint('\\n\\ninput:')\n\tprint(input[0].shape,'\\n',input[1].shape)\n\tprint('\\n\\noutput:')\n\tprint(output.shape)\n\t# print(type(input))\n\n\nif __name__ == '__main__':\n\tparams=[]\n\tdef hook(module,input):\n\t\t# print('breakpoint')\n\t\tparams.append(input)\n\t\t# print(input[0].shape)\n\t\t# data=input\n\tcfg = mmcv.Config.fromfile('configs/faster_rcnn_r50_fpn_1x.py')\n\tcfg.model.pretrained = None\n\n\ttorch.cuda.empty_cache()\n\n\t# ipdb.set_trace()\n\n\t# construct the model and load checkpoint\n\tmodel = build_detector(cfg.model, test_cfg=cfg.test_cfg)\n\tprint(model)\n\thandle=model.backbone.conv1.register_forward_pre_hook(hook)\n\t# model.bbox_roi_extractor.roi_layers[0].register_forward_hook(roialign_forward)\n\t\n\t_ = load_checkpoint(model, 'weights/faster_rcnn_r50_fpn_1x_20181010-3d1b3351.pth')\n\t\n\t# test a single image\n\timg= mmcv.imread('/py/pic/2.jpg')\n\tresult = inference_detector(model, img, cfg)\n\t# print(params)\n\n\t\n\tshow_result(img, result)\n\thandle.remove()\n\t# # test a list of images\n\t# imgs = ['/py/pic/4.jpg', '/py/pic/5.jpg']\n\t# for i, result in enumerate(inference_detector(model, imgs, cfg, device='cuda:0')):\n\t#     print(i, imgs[i])\n\t#     show_result(imgs[i], result)\n\n"""
setup.py,1,"b'import os\nimport platform\nimport subprocess\nimport time\nfrom setuptools import Extension, find_packages, setup\n\nimport numpy as np\nfrom Cython.Build import cythonize\nfrom torch.utils.cpp_extension import BuildExtension, CUDAExtension\n\n\ndef readme():\n    with open(\'README.md\', encoding=\'utf-8\') as f:\n        content = f.read()\n    return content\n\n\nMAJOR = 0\nMINOR = 6\nPATCH = 0\nSUFFIX = \'\'\nSHORT_VERSION = \'{}.{}.{}{}\'.format(MAJOR, MINOR, PATCH, SUFFIX)\n\nversion_file = \'mmdet/version.py\'\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in [\'SYSTEMROOT\', \'PATH\', \'HOME\']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env[\'LANGUAGE\'] = \'C\'\n        env[\'LANG\'] = \'C\'\n        env[\'LC_ALL\'] = \'C\'\n        out = subprocess.Popen(\n            cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd([\'git\', \'rev-parse\', \'HEAD\'])\n        sha = out.strip().decode(\'ascii\')\n    except OSError:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists(\'.git\'):\n        sha = get_git_hash()[:7]\n    elif os.path.exists(version_file):\n        try:\n            from mmdet.version import __version__\n            sha = __version__.split(\'+\')[-1]\n        except ImportError:\n            raise ImportError(\'Unable to get git version\')\n    else:\n        sha = \'unknown\'\n\n    return sha\n\n\ndef write_version_py():\n    content = """"""# GENERATED VERSION FILE\n# TIME: {}\n\n__version__ = \'{}\'\nshort_version = \'{}\'\n""""""\n    sha = get_hash()\n    VERSION = SHORT_VERSION + \'+\' + sha\n\n    with open(version_file, \'w\') as f:\n        f.write(content.format(time.asctime(), VERSION, SHORT_VERSION))\n\n\ndef get_version():\n    with open(version_file, \'r\') as f:\n        exec(compile(f.read(), version_file, \'exec\'))\n    return locals()[\'__version__\']\n\n\ndef make_cuda_ext(name, module, sources):\n\n    return CUDAExtension(\n        name=\'{}.{}\'.format(module, name),\n        sources=[os.path.join(*module.split(\'.\'), p) for p in sources],\n        extra_compile_args={\n            \'cxx\': [],\n            \'nvcc\': [\n                \'-D__CUDA_NO_HALF_OPERATORS__\',\n                \'-D__CUDA_NO_HALF_CONVERSIONS__\',\n                \'-D__CUDA_NO_HALF2_OPERATORS__\',\n            ]\n        })\n\n\ndef make_cython_ext(name, module, sources):\n    extra_compile_args = None\n    if platform.system() != \'Windows\':\n        extra_compile_args = {\n            \'cxx\': [\'-Wno-unused-function\', \'-Wno-write-strings\']\n        }\n\n    extension = Extension(\n        \'{}.{}\'.format(module, name),\n        [os.path.join(*module.split(\'.\'), p) for p in sources],\n        include_dirs=[np.get_include()],\n        language=\'c++\',\n        extra_compile_args=extra_compile_args)\n    extension, = cythonize(extension)\n    return extension\n\n\nif __name__ == \'__main__\':\n    write_version_py()\n    setup(\n        name=\'mmdet\',\n        version=get_version(),\n        description=\'Open MMLab Detection Toolbox\',\n        long_description=readme(),\n        keywords=\'computer vision, object detection\',\n        url=\'https://github.com/open-mmlab/mmdetection\',\n        packages=find_packages(exclude=(\'configs\', \'tools\', \'demo\')),\n        package_data={\'mmdet.ops\': [\'*/*.so\']},\n        classifiers=[\n            \'Development Status :: 4 - Beta\',\n            \'License :: OSI Approved :: Apache Software License\',\n            \'Operating System :: OS Independent\',\n            \'Programming Language :: Python :: 2\',\n            \'Programming Language :: Python :: 2.7\',\n            \'Programming Language :: Python :: 3\',\n            \'Programming Language :: Python :: 3.4\',\n            \'Programming Language :: Python :: 3.5\',\n            \'Programming Language :: Python :: 3.6\',\n        ],\n        license=\'Apache License 2.0\',\n        setup_requires=[\'pytest-runner\', \'cython\', \'numpy\'],\n        tests_require=[\'pytest\'],\n        install_requires=[\n            \'mmcv>=0.2.6\', \'numpy\', \'matplotlib\', \'six\', \'terminaltables\',\n            \'pycocotools\', \'torch>=1.1\'\n        ],\n        ext_modules=[\n            make_cython_ext(\n                name=\'soft_nms_cpu\',\n                module=\'mmdet.ops.nms\',\n                sources=[\'src/soft_nms_cpu.pyx\']),\n            make_cuda_ext(\n                name=\'roi_align_cuda\',\n                module=\'mmdet.ops.roi_align\',\n                sources=[\'src/roi_align_cuda.cpp\', \'src/roi_align_kernel.cu\']),\n            make_cuda_ext(\n                name=\'roi_pool_cuda\',\n                module=\'mmdet.ops.roi_pool\',\n                sources=[\'src/roi_pool_cuda.cpp\', \'src/roi_pool_kernel.cu\']),\n            make_cuda_ext(\n                name=\'nms_cpu\',\n                module=\'mmdet.ops.nms\',\n                sources=[\'src/nms_cpu.cpp\']),\n            make_cuda_ext(\n                name=\'nms_cuda\',\n                module=\'mmdet.ops.nms\',\n                sources=[\'src/nms_cuda.cpp\', \'src/nms_kernel.cu\']),\n            make_cuda_ext(\n                name=\'deform_conv_cuda\',\n                module=\'mmdet.ops.dcn\',\n                sources=[\n                    \'src/deform_conv_cuda.cpp\',\n                    \'src/deform_conv_cuda_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'deform_pool_cuda\',\n                module=\'mmdet.ops.dcn\',\n                sources=[\n                    \'src/deform_pool_cuda.cpp\',\n                    \'src/deform_pool_cuda_kernel.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'sigmoid_focal_loss_cuda\',\n                module=\'mmdet.ops.sigmoid_focal_loss\',\n                sources=[\n                    \'src/sigmoid_focal_loss.cpp\',\n                    \'src/sigmoid_focal_loss_cuda.cu\'\n                ]),\n            make_cuda_ext(\n                name=\'masked_conv2d_cuda\',\n                module=\'mmdet.ops.masked_conv\',\n                sources=[\n                    \'src/masked_conv2d_cuda.cpp\', \'src/masked_conv2d_kernel.cu\'\n                ]),\n        ],\n        cmdclass={\'build_ext\': BuildExtension},\n        zip_safe=False)\n'"
annotation/faster_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x9a inference\xe6\x97\xb6\xef\xbc\x8cdemo\xe7\xa8\x8b\xe5\xba\x8f\xe9\x9c\x80\xe8\xa6\x81\xe8\xa6\x86\xe7\x9b\x96\xe4\xb8\xbaNone\xef\xbc\x9btraining \xe6\x97\xb6\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x89\xe6\x9b\xb4\xe6\x94\xb9\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe7\x94\xa8\xe4\xb8\x8d\xe4\xba\x86\xe7\x9a\x84\xef\xbc\x8c\xe4\xb9\x9f\xe8\xa6\x81\xe5\x86\x99None\xe9\x87\x8d\xe6\x96\xb0\xe8\x87\xaa\xe5\xb7\xb1\xe8\xae\xad\xe7\xbb\x83\n    pretrained='modelzoo://resnet50',   \n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,               # stage\xe7\x89\xb9\xe5\xbe\x81\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xb1\x82\xe7\xba\xa7\n        out_indices=(0, 1, 2, 3),   # \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe4\xb8\x8a\xe9\x9d\xa2stage1-4\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\n        frozen_stages=1,            # \xe5\x86\xbb\xe7\xbb\x93\xe7\x9a\x84stage\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe8\xaf\xa5stage\xe4\xb8\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x881-4\xef\xbc\x89\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84stage\xe9\x83\xbd\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048], # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaastage\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0,\xe9\x83\xbd\xe6\x98\xafstage\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\n        out_channels=256,                   # FPN\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        num_outs=5),                        # FPN\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x88\xe6\x9c\x89\xe4\xb8\x80\xe4\xb8\xaa\xe7\x9b\xb4\xe6\x8e\xa5\xe7\x9a\x84\xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xe6\x89\x80\xe4\xbb\xa5\xe5\xa4\x9a\xe4\xba\x86\xe4\xb8\x80\xe5\xb1\x82\xef\xbc\x89\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,                    # RPN\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        feat_channels=256,                  # \xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,          # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x95\xb0\xe9\x87\x8f\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,           # \xe6\x98\xaf\xe5\x90\xa6\xe9\x87\x87\xe7\x94\xa8class_agnostic\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9d\xa5\xe5\x9b\x9e\xe5\xbd\x92\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',  # RPN\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x92\xe5\x88\x86\n            pos_iou_thr=0.7,        # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84iou\xe9\x98\x88\xe5\x80\xbc\n            neg_iou_thr=0.3,        # \xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xaciou\n            min_pos_iou=0.3,        # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84iou\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe5\x88\x86\xe9\x85\x8d\xe7\xbb\x99gt\xe7\x9a\x84anchors\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7IOU\xe4\xbd\x8e\xe4\xba\x8e0.3\xef\xbc\x8c\xe5\x88\x99\xe5\xbf\xbd\xe7\x95\xa5\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84anchors\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xbf\x9d\xe7\x95\x99\xe6\x9c\x80\xe5\xa4\xa7IOU\xe7\x9a\x84anchor\n            ignore_iof_thr=-1),     # \xe5\xbf\xbd\xe7\x95\xa5bbox\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe5\xbd\x93ground truth\xe4\xb8\xad\xe5\x8c\x85\xe5\x90\xab\xe9\x9c\x80\xe8\xa6\x81\xe5\xbf\xbd\xe7\x95\xa5\xe7\x9a\x84bbox\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe5\xbf\xbd\xe7\x95\xa5\n        sampler=dict(\n            type='RandomSampler',   # \xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\xe7\xb1\xbb\xe5\x9e\x8b\n            num=256,                # \xe9\x9c\x80\xe6\x8f\x90\xe5\x8f\x96\xe7\x9a\x84\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f  \n            pos_fraction=0.5,       # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\xaf\x94\xe4\xbe\x8b\n            neg_pos_ub=-1,          # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe8\xaf\xa5\xe6\xaf\x94\xe4\xbe\x8b\xe7\x9a\x84\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\xbf\xbd\xe7\x95\xa5\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe5\xbf\xbd\xe7\x95\xa5\n            add_gt_as_proposals=False), # \xe6\x8a\x8aground truth\xe5\x8a\xa0\xe5\x85\xa5proposal\xe4\xbd\x9c\xe4\xb8\xba\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n        allowed_border=0,           # \xe5\x85\x81\xe8\xae\xb8\xe5\x9c\xa8bbox\xe5\x91\xa8\xe5\x9b\xb4\xe5\xa4\x96\xe6\x89\xa9\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\n        pos_weight=-1,              # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n        debug=False),   \n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(                    # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x90\xad\xe5\xbb\xba\xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8cforward\xe6\x97\xb6\xe4\xbc\x9a\xe8\xb0\x83\xe7\x94\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xbe\xe7\xbd\xae\xef\xbc\x88train\xe6\x97\xb6\xe4\xbc\xa0\xe5\x85\xa5\xe4\xba\x86train\xe5\x92\x8ctest\xe4\xb8\xa4\xe4\xb8\xaacfg\xe9\x85\x8d\xe7\xbd\xae\xef\xbc\x89\xe3\x80\x82\xe6\x8e\xa8\xe6\x96\xad\xe6\x97\xb6\xe7\x9a\x84RPN\xe5\x8f\x82\xe6\x95\xb0\n    rpn=dict(\n        nms_across_levels=False,    # \xe5\x9c\xa8\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84fpn\xe5\xb1\x82\xe5\x86\x85\xe5\x81\x9anms\n        nms_pre=1000,               # \xe5\x9c\xa8nms\xe4\xb9\x8b\xe5\x89\x8d\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        nms_post=1000,              # \xe5\x9c\xa8nms\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        max_num=1000,               # \xe5\x9c\xa8\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xe5\xae\x8c\xe6\x88\x90\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        nms_thr=0.7,\n        min_bbox_size=0),           # \xe6\x9c\x80\xe5\xb0\x8fbbox\xe5\xb0\xba\xe5\xaf\xb8\n    rcnn=dict(  \n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)   # max_per_img\xe4\xb8\xba\xe6\x9c\x80\xe7\xbb\x88\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84det bbox\xe6\x95\xb0\xe9\x87\x8f\xe4\xb8\x8a\xe9\x99\x90\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,     # \xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8cresize\xe6\x97\xb6\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8d\x95\xe4\xbd\x8d\xef\xbc\x8c32\xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe9\x83\xbd\xe4\xbc\x9a\xe8\xa2\xabresize\xe6\x88\x9032\xe7\x9a\x84\xe5\x80\x8d\xe6\x95\xb0\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(               # inference\xe9\x98\xb6\xe6\xae\xb5\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]       # \xe5\xb7\xa5\xe4\xbd\x9c\xe6\xb5\x81\xe7\xa8\x8b\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8train\xe4\xb8\x80\xe8\xbd\xae\xe3\x80\x82\xe6\x9b\xb4\xe8\xaf\xa6\xe7\xbb\x86\xe7\x9a\x84\xe8\xa7\xa3\xe9\x87\x8a\xe8\xa7\x81runner\xe6\xb3\xa8\xe9\x87\x8a\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\xb1\xe4\xb8\x8d\xe5\xb1\x95\xe5\xbc\x80\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe5\x8f\xaa\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe5\xb0\xb1\xe8\xa1\x8c\n"""
annotation/mask_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(                       # \xe6\xa8\xa1\xe5\x9e\x8bbuild\xe5\x8f\x82\xe6\x95\xb0\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet101',  # \xe6\xb3\xa8\xe6\x84\x8f\xe8\xbf\x99\xe4\xb8\xaa\xe5\x9c\xb0\xe6\x96\xb9\xef\xbc\x9a inference\xe6\x97\xb6\xef\xbc\x8c\xe9\x9c\x80\xe8\xa6\x81\xe8\xa6\x86\xe7\x9b\x96\xe4\xb8\xbaNone\xef\xbc\x8c\n                                        #              training \xe6\x97\xb6\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe5\xaf\xb9\xe7\xbd\x91\xe7\xbb\x9c\xe6\x9c\x89\xe6\x9b\xb4\xe6\x94\xb9\xef\xbc\x8c\xe8\xbf\x99\xe4\xb8\xaa\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe7\x94\xa8\xe4\xb8\x8d\xe4\xba\x86\xe7\x9a\x84\xef\xbc\x8c\xe4\xb9\x9f\xe8\xa6\x81\xe5\x86\x99None\xe9\x87\x8d\xe6\x96\xb0\xe8\x87\xaa\xe5\xb7\xb1\xe8\xae\xad\xe7\xbb\x83\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,               # stage\xe7\x89\xb9\xe5\xbe\x81\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xb1\x82\xe7\xba\xa7\n        out_indices=(0, 1, 2, 3),   # \xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe4\xb8\x8a\xe9\x9d\xa2stage1-4\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xbe\x93\xe5\x87\xba\xe7\xbb\x93\xe6\x9e\x9c\xe7\x9a\x84\xe7\xb4\xa2\xe5\xbc\x95\n        frozen_stages=1,            # \xe5\x86\xbb\xe7\xbb\x93\xe7\x9a\x84stage\xe6\x95\xb0\xe9\x87\x8f\xef\xbc\x8c\xe5\x8d\xb3\xe8\xaf\xa5stage\xe4\xb8\x8d\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x881-4\xef\xbc\x89\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84stage\xe9\x83\xbd\xe6\x9b\xb4\xe6\x96\xb0\xe5\x8f\x82\xe6\x95\xb0  \n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],     # \xe8\xbe\x93\xe5\x85\xa5\xe7\x9a\x84\xe5\x90\x84\xe4\xb8\xaastage\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0,\xe9\x83\xbd\xe6\x98\xafstage\xe7\x9a\x84\xe6\x9c\x80\xe5\x90\x8e\xe4\xb8\x80\xe5\xb1\x82\n        out_channels=256,                       # FPN\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        num_outs=5),                            # FPN\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe6\x95\xb0\xe9\x87\x8f\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,                        # RPN\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe8\xbe\x93\xe5\x85\xa5\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        feat_channels=256,                      # \xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe7\x9a\x84\xe9\x80\x9a\xe9\x81\x93\xe6\x95\xb0\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True),                  # \xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8sigmoid\xe6\x9d\xa5\xe8\xbf\x9b\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9cFalse\xe5\x88\x99\xe4\xbd\xbf\xe7\x94\xa8softmax\xe6\x9d\xa5\xe5\x88\x86\xe7\xb1\xbb\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',              # RoIExtractor\xe7\xb1\xbb\xe5\x9e\x8b    \n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,                              # \xe5\x85\xa8\xe8\xbf\x9e\xe6\x8e\xa5\xe5\xb1\x82\xe6\x95\xb0\xe9\x87\x8f\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),              # \xe6\x98\xaf\xe5\x90\xa6\xe9\x87\x87\xe7\x94\xa8class_agnostic\xe7\x9a\x84\xe6\x96\xb9\xe5\xbc\x8f\xe6\x9d\xa5\xe9\xa2\x84\xe6\xb5\x8b\xef\xbc\x8c\n                                                # class_agnostic\xe8\xa1\xa8\xe7\xa4\xba\xe8\xbe\x93\xe5\x87\xbabbox\xe6\x97\xb6\xe5\x8f\xaa\xe8\x80\x83\xe8\x99\x91\xe5\x85\xb6\xe6\x98\xaf\xe5\x90\xa6\xe4\xb8\xba\xe5\x89\x8d\xe6\x99\xaf\xef\xbc\x8c\n                                                # \xe5\x90\x8e\xe7\xbb\xad\xe5\x88\x86\xe7\xb1\xbb\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x86\x8d\xe6\xa0\xb9\xe6\x8d\xae\xe8\xaf\xa5bbox\xe5\x9c\xa8\xe7\xbd\x91\xe7\xbb\x9c\xe4\xb8\xad\xe7\x9a\x84\xe7\xb1\xbb\xe5\x88\xab\xe5\xbe\x97\xe5\x88\x86\xe6\x9d\xa5\xe5\x88\x86\xe7\xb1\xbb\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xaf\xe8\xaf\xb4\xe4\xb8\x80\xe4\xb8\xaa\xe6\xa1\x86\xe5\x8f\xaf\xe4\xbb\xa5\xe5\xaf\xb9\xe5\xba\x94\xe5\xa4\x9a\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(                         # detection\xe7\xbb\x8f\xe8\xbf\x87RoIAlign\xe5\x90\x8e\xef\xbc\x8c\xe5\x86\x8d\xe7\x94\xa84\xe7\xbb\x84\xe5\x8d\xb7\xe7\xa7\xaf\xe9\x87\x8d\xe7\xbb\x84\xe7\x89\xb9\xe5\xbe\x81   \n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',          # RPN\xe7\xbd\x91\xe7\xbb\x9c\xe7\x9a\x84\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x92\xe5\x88\x86\n            pos_iou_thr=0.7,                # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84iou\xe9\x98\x88\xe5\x80\xbc\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,    # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84iou\xe6\x9c\x80\xe5\xb0\x8f\xe5\x80\xbc\xe3\x80\x82\xe5\xa6\x82\xe6\x9e\x9c\xe5\x88\x86\xe9\x85\x8d\xe7\xbb\x99gt\xe7\x9a\x84anchors\xe4\xb8\xad\xe6\x9c\x80\xe5\xa4\xa7IOU\xe4\xbd\x8e\xe4\xba\x8e0.3\xef\xbc\x8c\xe5\x88\x99\xe5\xbf\xbd\xe7\x95\xa5\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84anchors\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xbf\x9d\xe7\x95\x99\xe6\x9c\x80\xe5\xa4\xa7IOU\xe7\x9a\x84anchor\n            ignore_iof_thr=-1),             # \xe5\xbf\xbd\xe7\x95\xa5bbox\xe7\x9a\x84\xe9\x98\x88\xe5\x80\xbc\xef\xbc\x8c\xe5\xbd\x93ground truth\xe4\xb8\xad\xe5\x8c\x85\xe5\x90\xab\xe9\x9c\x80\xe8\xa6\x81\xe5\xbf\xbd\xe7\x95\xa5\xe7\x9a\x84bbox\xe6\x97\xb6\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe5\xbf\xbd\xe7\x95\xa5\n        sampler=dict(\n            type='RandomSampler',           # \xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x8f\x90\xe5\x8f\x96\xe5\x99\xa8\xe7\xb1\xbb\xe5\x9e\x8b\n            num=256,                        # \xe9\x9c\x80\xe6\x8f\x90\xe5\x8f\x96\xe7\x9a\x84\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\x95\xb0\xe9\x87\x8f    \n            pos_fraction=0.5,               # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\xaf\x94\xe4\xbe\x8b\n            neg_pos_ub=-1,                  # \xe6\x9c\x80\xe5\xa4\xa7\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe6\xaf\x94\xe4\xbe\x8b\xef\xbc\x8c\xe5\xa4\xa7\xe4\xba\x8e\xe8\xaf\xa5\xe6\xaf\x94\xe4\xbe\x8b\xe7\x9a\x84\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\xbf\xbd\xe7\x95\xa5\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe5\xbf\xbd\xe7\x95\xa5\n            add_gt_as_proposals=False),     # \xe6\x8a\x8aground truth\xe5\x8a\xa0\xe5\x85\xa5proposal\xe4\xbd\x9c\xe4\xb8\xba\xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\n        allowed_border=0,                   # \xe5\x85\x81\xe8\xae\xb8\xe5\x9c\xa8bbox\xe5\x91\xa8\xe5\x9b\xb4\xe5\xa4\x96\xe6\x89\xa9\xe4\xb8\x80\xe5\xae\x9a\xe7\x9a\x84\xe5\x83\x8f\xe7\xb4\xa0\n        pos_weight=-1,                      # \xe6\xad\xa3\xe6\xa0\xb7\xe6\x9c\xac\xe6\x9d\x83\xe9\x87\x8d\xef\xbc\x8c-1\xe8\xa1\xa8\xe7\xa4\xba\xe4\xb8\x8d\xe6\x94\xb9\xe5\x8f\x98\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84\xe6\x9d\x83\xe9\x87\x8d\n        smoothl1_beta=1 / 9.0,              # \xe5\xb9\xb3\xe6\xbb\x91L1\xe7\xb3\xbb\xe6\x95\xb0\n        debug=False),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',          # RCNN\xe7\xbd\x91\xe7\xbb\x9c\xe6\xad\xa3\xe8\xb4\x9f\xe6\xa0\xb7\xe6\x9c\xac\xe5\x88\x92\xe5\x88\x86\xef\xbc\x8c\xe4\xb8\x8b\xe9\x9d\xa2\xe7\x95\xa5\xef\xbc\x8c\xe5\x90\x8c\xe4\xb8\x8a\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(                            # \xe6\xa8\xa1\xe5\x9e\x8b\xe6\x90\xad\xe5\xbb\xba\xe4\xb8\x8d\xe4\xbc\x9a\xe4\xbd\xbf\xe7\x94\xa8\xef\xbc\x8c\xe5\x9c\xa8forward\xe6\x97\xb6\xe4\xbc\x9a\xe8\xb0\x83\xe7\x94\xa8\xe8\xbf\x9b\xe8\xa1\x8c\xe8\xae\xbe\xe7\xbd\xae\xe3\x80\x82\xe6\x8e\xa8\xe6\x96\xad\xe6\x97\xb6\xe7\x9a\x84RPN\xe5\x8f\x82\xe6\x95\xb0\n    rpn=dict(\n        nms_across_levels=False,            # \xe5\x9c\xa8\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84fpn\xe5\xb1\x82\xe5\x86\x85\xe5\x81\x9anms\n        nms_pre=2000,                       # \xe5\x9c\xa8nms\xe4\xb9\x8b\xe5\x89\x8d\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        nms_post=2000,                      # \xe5\x9c\xa8nms\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84\xe5\xbe\x97\xe5\x88\x86\xe6\x9c\x80\xe9\xab\x98\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        max_num=2000,                       # \xe5\x9c\xa8\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xe5\xae\x8c\xe6\x88\x90\xe4\xb9\x8b\xe5\x90\x8e\xe4\xbf\x9d\xe7\x95\x99\xe7\x9a\x84proposal\xe6\x95\xb0\xe9\x87\x8f\n        nms_thr=0.7,\n        min_bbox_size=0),                   # \xe6\x9c\x80\xe5\xb0\x8fbbox\xe5\xb0\xba\xe5\xaf\xb8\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,                    # max_per_img\xe8\xa1\xa8\xe7\xa4\xba\xe6\x9c\x80\xe7\xbb\x88\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84det bbox\xe6\x95\xb0\xe9\x87\x8f\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n    # data\xe9\x83\xa8\xe5\x88\x86\xe8\xae\xbe\xe7\xbd\xae\xe7\x9a\x84\xe5\xb0\xb1\xe6\x98\xaf\xe8\xbe\x93\xe5\x85\xa5\xe5\x9b\xbe\xe7\x89\x87\xe8\xbf\x9b\xe8\xa1\x8c\xe5\xa4\x84\xe7\x90\x86\xe7\x9a\x84\xe7\x9b\xb8\xe5\x85\xb3\xe5\x8f\x82\xe6\x95\xb0,\xe5\x9c\xa8\xe8\xae\xad\xe7\xbb\x83/\xe6\xb5\x8b\xe8\xaf\x95\xe4\xb9\x8b\xe5\x89\x8d\xe5\x85\x88\xe5\x8a\xa0\xe8\xbd\xbd\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,      # 'CocoDataset'\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,                    # \xe5\xaf\xb9\xe5\x9b\xbe\xe5\x83\x8f\xe8\xbf\x9b\xe8\xa1\x8cresize\xe6\x97\xb6\xe7\x9a\x84\xe6\x9c\x80\xe5\xb0\x8f\xe5\x8d\x95\xe4\xbd\x8d\xef\xbc\x8c32\xe8\xa1\xa8\xe7\xa4\xba\xe6\x89\x80\xe6\x9c\x89\xe7\x9a\x84\xe5\x9b\xbe\xe5\x83\x8f\xe9\x83\xbd\xe4\xbc\x9a\xe8\xa2\xabresize\xe6\x88\x9032\xe7\x9a\x84\xe5\x80\x8d\xe6\x95\xb0\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(      # inference\xe9\x98\xb6\xe6\xae\xb5\xe7\x9a\x84\xe9\x85\x8d\xe7\xbd\xae\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),      #\xe5\x9b\xbe\xe7\x89\x87\xe7\xbc\xa9\xe6\x94\xbe\xe7\x9a\x84\xe5\xb0\xba\xe5\xaf\xb8\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)                # \xe8\xae\xbe\xe7\xbd\xae\xe6\x96\xad\xe7\x82\xb9\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')    # \xe9\x80\x89\xe6\x8b\xa9\xe4\xb8\xad\xe9\x97\xb4\xe6\x97\xa5\xe5\xbf\x97\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\xe5\xbd\xa2\xe5\xbc\x8f\xef\xbc\x8ctext\xe6\x98\xaf\xe7\xbb\x88\xe7\xab\xaf\xe6\x98\xbe\xe7\xa4\xba\xe3\x80\x82\xe6\x8e\xa8\xe8\x8d\x90Tensorboard\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9b\xb4\xe5\xa5\xbd\xe5\x9c\xb0\xe5\x8f\xaf\xe8\xa7\x86\xe5\x8c\x96\xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_1x'      # \xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84\xe5\xb7\xa5\xe4\xbd\x9c\xe7\x9b\xae\xe5\xbd\x95\xef\xbc\x8c\xe5\x86\x8d\xe6\xac\xa1\xe5\xad\x98\xe5\x82\xa8\xe6\x9d\x83\xe9\x87\x8d\xe7\xad\x89\xe6\x96\x87\xe4\xbb\xb6\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]       # \xe5\xb7\xa5\xe4\xbd\x9c\xe6\xb5\x81\xe7\xa8\x8b\xef\xbc\x8c\xe4\xbb\xa3\xe8\xa1\xa8train\xe4\xb8\x80\xe8\xbd\xae\xe3\x80\x82\xe6\x9b\xb4\xe8\xaf\xa6\xe7\xbb\x86\xe7\x9a\x84\xe8\xa7\xa3\xe9\x87\x8a\xe8\xa7\x81runner\xe6\xb3\xa8\xe9\x87\x8a\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe5\xb0\xb1\xe4\xb8\x8d\xe5\xb1\x95\xe5\xbc\x80\xef\xbc\x8c\xe4\xb8\x80\xe8\x88\xac\xe5\x8f\xaa\xe7\x94\xa8\xe8\xbf\x99\xe4\xb8\xaa\xe5\xb0\xb1\xe8\xa1\x8c\n"""
configs/cascade_mask_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_mask_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_scales=[2, 4, 8, 16, 32],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[16],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=[\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=None,\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=0,\n        in_channels=2048,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_mask_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_mask_rcnn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_mask_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'MyDataset'\ndata_root = '/py/R2CNN-tensorflow/data/VOCdevkit/VOC2007/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '' ,\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=10000)\n# yapf:disable\nlog_config = dict(\n    interval=500,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 120000\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_scales=[2, 4, 8, 16, 32],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[16],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=[\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='BBoxHead',\n            with_avg_pool=True,\n            roi_feat_size=7,\n            in_channels=2048,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=14,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r50_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'MyDataset'\ndata_root = '/py/datasets/tiny_ships/VOCdevkit/VOC2007/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=2)\n# yapf:disable\nlog_config = dict(\n    interval=1,\n    hooks=[\n        # dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 120000\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_rcnn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = '/py/datasets/tiny_ships/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'train.json',\n        img_prefix=data_root + 'image/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'train.json',\n        img_prefix=data_root + 'image/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'train.json',\n        img_prefix=data_root + 'image/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        # dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/cascade_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_mask_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_mask_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_mask_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='FastRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=dict(\n        type='BBoxHead',\n        with_avg_pool=True,\n        roi_feat_size=7,\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    mask_roi_extractor=None,\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=0,\n        in_channels=2048,\n        conv_out_channels=256,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=14,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_mask_rcnn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_mask_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_mask_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss',\n            use_sigmoid=False,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='FastRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=dict(\n        type='BBoxHead',\n        with_avg_pool=True,\n        roi_feat_size=7,\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss',\n            use_sigmoid=False,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_c4_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_rcnn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fast_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss',\n            use_sigmoid=False,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fast_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_ohem_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='OHEMSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_scales=[2, 4, 8, 16, 32],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[16],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=dict(\n        type='BBoxHead',\n        with_avg_pool=True,\n        roi_feat_size=7,\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'MyDataset'\ndata_root = '/py/R2CNN-tensorflow/data/VOCdevkit/VOC2007/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'Main/train.txt',\n        img_prefix=data_root + '',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1000)\n# yapf:disable\nlog_config = dict(\n    interval=1,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 1200000\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/faster_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_r101_(test).py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = '/py/mmdetection-master/data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/car.json',\n        img_prefix=data_root + 'train2014/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/car.json',\n        img_prefix=data_root + 'val2014/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/car.json',\n        img_prefix=data_root + 'val2014/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.0025, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=100)\n# yapf:disable\nlog_config = dict(\n    interval=100,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 1200\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_rcnn_r50_caffe_c4_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='BN', requires_grad=False)\nmodel = dict(\n    type='MaskRCNN',\n    # pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=norm_cfg,\n        norm_eval=True,\n        style='caffe'),\n    shared_head=dict(\n        type='ResLayer',\n        depth=50,\n        stage=3,\n        stride=2,\n        dilation=1,\n        style='caffe',\n        norm_cfg=norm_cfg,\n        norm_eval=True),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_scales=[2, 4, 8, 16, 32],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[16],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=1024,\n        featmap_strides=[16]),\n    bbox_head=dict(\n        type='BBoxHead',\n        with_avg_pool=True,\n        roi_feat_size=7,\n        in_channels=2048,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=None,\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=0,\n        in_channels=2048,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=14,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=6000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\nevaluation = dict(interval=1)\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_rcnn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/mask_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/retinanet_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/retinanet_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/retinanet_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/retinanet_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/rpn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/rpn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/rpn_r50_caffe_c4_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=3,\n        strides=(1, 2, 2),\n        dilations=(1, 1, 1),\n        out_indices=(2, ),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=None,\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=1024,\n        feat_channels=1024,\n        anchor_scales=[2, 4, 8, 16, 32],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[16],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=12000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/rpn_r50_caffe_c4_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/rpn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/rpn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/rpn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/rpn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/rpn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/rpn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ssd300_coco.py,0,"b""# model settings\ninput_size = 300\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(512, 1024, 512, 256, 256, 256),\n        num_classes=81,\n        anchor_strides=(8, 16, 32, 64, 100, 300),\n        basesize_ratio_range=(0.15, 0.9),\n        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2)))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        type='RepeatDataset',\n        times=5,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/instances_train2017.json',\n            img_prefix=data_root + 'train2017/',\n            img_scale=(300, 300),\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=None,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=False,\n            with_label=True,\n            test_mode=False,\n            extra_aug=dict(\n                photo_metric_distortion=dict(\n                    brightness_delta=32,\n                    contrast_range=(0.5, 1.5),\n                    saturation_range=(0.5, 1.5),\n                    hue_delta=18),\n                expand=dict(\n                    mean=img_norm_cfg['mean'],\n                    to_rgb=img_norm_cfg['to_rgb'],\n                    ratio_range=(1, 4)),\n                random_crop=dict(\n                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),\n            resize_keep_ratio=False)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False))\n# optimizer\noptimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ssd300_coco'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ssd512_coco.py,0,"b""# model settings\ninput_size = 512\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(512, 1024, 512, 256, 256, 256, 256),\n        num_classes=81,\n        anchor_strides=(8, 16, 32, 64, 128, 256, 512),\n        basesize_ratio_range=(0.1, 0.9),\n        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2)))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=8,\n    workers_per_gpu=3,\n    train=dict(\n        type='RepeatDataset',\n        times=5,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=data_root + 'annotations/instances_train2017.json',\n            img_prefix=data_root + 'train2017/',\n            img_scale=(512, 512),\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=None,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=False,\n            with_label=True,\n            test_mode=False,\n            extra_aug=dict(\n                photo_metric_distortion=dict(\n                    brightness_delta=32,\n                    contrast_range=(0.5, 1.5),\n                    saturation_range=(0.5, 1.5),\n                    hue_delta=18),\n                expand=dict(\n                    mean=img_norm_cfg['mean'],\n                    to_rgb=img_norm_cfg['to_rgb'],\n                    ratio_range=(1, 4)),\n                random_crop=dict(\n                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),\n            resize_keep_ratio=False)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(512, 512),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(512, 512),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False))\n# optimizer\noptimizer = dict(type='SGD', lr=2e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ssd512_coco'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
mmdet/__init__.py,0,"b""from .version import __version__, short_version\n\n__all__ = ['__version__', 'short_version']\n"""
mmdet/version.py,0,"b""# GENERATED VERSION FILE\n# TIME: Mon Jul 15 15:38:29 2019\n\n__version__ = '0.6.0+unknown'\nshort_version = '0.6.0'\n"""
tools/analyze_logs.py,0,"b""import argparse\nimport json\nfrom collections import defaultdict\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\n\ndef cal_train_time(log_dicts, args):\n    for i, log_dict in enumerate(log_dicts):\n        print('{}Analyze train time of {}{}'.format('-' * 5, args.json_logs[i],\n                                                    '-' * 5))\n        all_times = []\n        for epoch in log_dict.keys():\n            if args.include_outliers:\n                all_times.append(log_dict[epoch]['time'])\n            else:\n                all_times.append(log_dict[epoch]['time'][1:])\n        all_times = np.array(all_times)\n        epoch_ave_time = all_times.mean(-1)\n        slowest_epoch = epoch_ave_time.argmax()\n        fastest_epoch = epoch_ave_time.argmin()\n        std_over_epoch = epoch_ave_time.std()\n        print('slowest epoch {}, average time is {:.4f}'.format(\n            slowest_epoch + 1, epoch_ave_time[slowest_epoch]))\n        print('fastest epoch {}, average time is {:.4f}'.format(\n            fastest_epoch + 1, epoch_ave_time[fastest_epoch]))\n        print('time std over epochs is {:.4f}'.format(std_over_epoch))\n        print('average iter time: {:.4f} s/iter'.format(np.mean(all_times)))\n        print()\n\n\ndef plot_curve(log_dicts, args):\n    if args.backend is not None:\n        plt.switch_backend(args.backend)\n    sns.set_style(args.style)\n    # if legend is None, use {filename}_{key} as legend\n    legend = args.legend\n    if legend is None:\n        legend = []\n        for json_log in args.json_logs:\n            for metric in args.keys:\n                legend.append('{}_{}'.format(json_log, metric))\n    assert len(legend) == (len(args.json_logs) * len(args.keys))\n    metrics = args.keys\n\n    num_metrics = len(metrics)\n    for i, log_dict in enumerate(log_dicts):\n        epochs = list(log_dict.keys())\n        for j, metric in enumerate(metrics):\n            print('plot curve of {}, metric is {}'.format(\n                args.json_logs[i], metric))\n            assert metric in log_dict[epochs[\n                0]], '{} does not contain metric {}'.format(\n                    args.json_logs[i], metric)\n\n            if 'mAP' in metric:\n                xs = np.arange(1, max(epochs) + 1)\n                ys = []\n                for epoch in epochs:\n                    ys += log_dict[epoch][metric]\n                ax = plt.gca()\n                ax.set_xticks(xs)\n                plt.xlabel('epoch')\n                plt.plot(xs, ys, label=legend[i * num_metrics + j], marker='o')\n            else:\n                xs = []\n                ys = []\n                num_iters_per_epoch = log_dict[epochs[0]]['iter'][-1]\n                for epoch in epochs:\n                    iters = log_dict[epoch]['iter']\n                    if log_dict[epoch]['mode'][-1] == 'val':\n                        iters = iters[:-1]\n                    xs.append(\n                        np.array(iters) + (epoch - 1) * num_iters_per_epoch)\n                    ys.append(np.array(log_dict[epoch][metric][:len(iters)]))\n                xs = np.concatenate(xs)\n                ys = np.concatenate(ys)\n                plt.xlabel('iter')\n                plt.plot(\n                    xs, ys, label=legend[i * num_metrics + j], linewidth=0.5)\n            plt.legend()\n        if args.title is not None:\n            plt.title(args.title)\n    if args.out is None:\n        plt.show()\n    else:\n        print('save curve to: {}'.format(args.out))\n        plt.savefig(args.out)\n        plt.cla()\n\n\ndef add_plot_parser(subparsers):\n    parser_plt = subparsers.add_parser(\n        'plot_curve', help='parser for plotting curves')\n    parser_plt.add_argument(\n        'json_logs',\n        type=str,\n        nargs='+',\n        help='path of train log in json format')\n    parser_plt.add_argument(\n        '--keys',\n        type=str,\n        nargs='+',\n        default=['bbox_mAP'],\n        help='the metric that you want to plot')\n    parser_plt.add_argument('--title', type=str, help='title of figure')\n    parser_plt.add_argument(\n        '--legend',\n        type=str,\n        nargs='+',\n        default=None,\n        help='legend of each plot')\n    parser_plt.add_argument(\n        '--backend', type=str, default=None, help='backend of plt')\n    parser_plt.add_argument(\n        '--style', type=str, default='dark', help='style of plt')\n    parser_plt.add_argument('--out', type=str, default=None)\n\n\ndef add_time_parser(subparsers):\n    parser_time = subparsers.add_parser(\n        'cal_train_time',\n        help='parser for computing the average time per training iteration')\n    parser_time.add_argument(\n        'json_logs',\n        type=str,\n        nargs='+',\n        help='path of train log in json format')\n    parser_time.add_argument(\n        '--include-outliers',\n        action='store_true',\n        help='include the first value of every epoch when computing '\n        'the average time')\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Analyze Json Log')\n    # currently only support plot curve and calculate average train time\n    subparsers = parser.add_subparsers(dest='task', help='task parser')\n    add_plot_parser(subparsers)\n    add_time_parser(subparsers)\n    args = parser.parse_args()\n    return args\n\n\ndef load_json_logs(json_logs):\n    # load and convert json_logs to log_dict, key is epoch, value is a sub dict\n    # keys of sub dict is different metrics, e.g. memory, bbox_mAP\n    # value of sub dict is a list of corresponding values of all iterations\n    log_dicts = [dict() for _ in json_logs]\n    for json_log, log_dict in zip(json_logs, log_dicts):\n        with open(json_log, 'r') as log_file:\n            for l in log_file:\n                log = json.loads(l.strip())\n                epoch = log.pop('epoch')\n                if epoch not in log_dict:\n                    log_dict[epoch] = defaultdict(list)\n                for k, v in log.items():\n                    log_dict[epoch][k].append(v)\n    return log_dicts\n\n\ndef main():\n    args = parse_args()\n\n    json_logs = args.json_logs\n    for json_log in json_logs:\n        assert json_log.endswith('.json')\n\n    log_dicts = load_json_logs(json_logs)\n\n    eval(args.task)(log_dicts, args)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/coco_eval.py,0,"b""from argparse import ArgumentParser\n\nfrom mmdet.core import coco_eval\n\n\ndef main():\n    parser = ArgumentParser(description='COCO Evaluation')\n    parser.add_argument('result', help='result file path')\n    parser.add_argument('--ann', help='annotation file path')\n    parser.add_argument(\n        '--types',\n        type=str,\n        nargs='+',\n        choices=['proposal_fast', 'proposal', 'bbox', 'segm', 'keypoint'],\n        default=['bbox'],\n        help='result types')\n    parser.add_argument(\n        '--max-dets',\n        type=int,\n        nargs='+',\n        default=[100, 300, 1000],\n        help='proposal numbers, only used for recall evaluation')\n    args = parser.parse_args()\n    coco_eval(args.result, args.types, args.ann, args.max_dets)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/detectron2pytorch.py,7,"b'import argparse\nfrom collections import OrderedDict\n\nimport mmcv\nimport torch\n\narch_settings = {50: (3, 4, 6, 3), 101: (3, 4, 23, 3)}\n\n\ndef convert_bn(blobs, state_dict, caffe_name, torch_name, converted_names):\n    # detectron replace bn with affine channel layer\n    state_dict[torch_name + \'.bias\'] = torch.from_numpy(blobs[caffe_name +\n                                                              \'_b\'])\n    state_dict[torch_name + \'.weight\'] = torch.from_numpy(blobs[caffe_name +\n                                                                \'_s\'])\n    bn_size = state_dict[torch_name + \'.weight\'].size()\n    state_dict[torch_name + \'.running_mean\'] = torch.zeros(bn_size)\n    state_dict[torch_name + \'.running_var\'] = torch.ones(bn_size)\n    converted_names.add(caffe_name + \'_b\')\n    converted_names.add(caffe_name + \'_s\')\n\n\ndef convert_conv_fc(blobs, state_dict, caffe_name, torch_name,\n                    converted_names):\n    state_dict[torch_name + \'.weight\'] = torch.from_numpy(blobs[caffe_name +\n                                                                \'_w\'])\n    converted_names.add(caffe_name + \'_w\')\n    if caffe_name + \'_b\' in blobs:\n        state_dict[torch_name + \'.bias\'] = torch.from_numpy(blobs[caffe_name +\n                                                                  \'_b\'])\n        converted_names.add(caffe_name + \'_b\')\n\n\ndef convert(src, dst, depth):\n    """"""Convert keys in detectron pretrained ResNet models to pytorch style.""""""\n    # load arch_settings\n    if depth not in arch_settings:\n        raise ValueError(\'Only support ResNet-50 and ResNet-101 currently\')\n    block_nums = arch_settings[depth]\n    # load caffe model\n    caffe_model = mmcv.load(src, encoding=\'latin1\')\n    blobs = caffe_model[\'blobs\'] if \'blobs\' in caffe_model else caffe_model\n    # convert to pytorch style\n    state_dict = OrderedDict()\n    converted_names = set()\n    convert_conv_fc(blobs, state_dict, \'conv1\', \'conv1\', converted_names)\n    convert_bn(blobs, state_dict, \'res_conv1_bn\', \'bn1\', converted_names)\n    for i in range(1, len(block_nums) + 1):\n        for j in range(block_nums[i - 1]):\n            if j == 0:\n                convert_conv_fc(blobs, state_dict,\n                                \'res{}_{}_branch1\'.format(i + 1, j),\n                                \'layer{}.{}.downsample.0\'.format(i, j),\n                                converted_names)\n                convert_bn(blobs, state_dict,\n                           \'res{}_{}_branch1_bn\'.format(i + 1, j),\n                           \'layer{}.{}.downsample.1\'.format(i, j),\n                           converted_names)\n            for k, letter in enumerate([\'a\', \'b\', \'c\']):\n                convert_conv_fc(blobs, state_dict,\n                                \'res{}_{}_branch2{}\'.format(i + 1, j, letter),\n                                \'layer{}.{}.conv{}\'.format(i, j, k + 1),\n                                converted_names)\n                convert_bn(blobs, state_dict,\n                           \'res{}_{}_branch2{}_bn\'.format(i + 1, j, letter),\n                           \'layer{}.{}.bn{}\'.format(i, j,\n                                                    k + 1), converted_names)\n    # check if all layers are converted\n    for key in blobs:\n        if key not in converted_names:\n            print(\'Not Convert: {}\'.format(key))\n    # save checkpoint\n    checkpoint = dict()\n    checkpoint[\'state_dict\'] = state_dict\n    torch.save(checkpoint, dst)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Convert model keys\')\n    parser.add_argument(\'src\', help=\'src detectron model path\')\n    parser.add_argument(\'dst\', help=\'save path\')\n    parser.add_argument(\'depth\', type=int, help=\'ResNet model depth\')\n    args = parser.parse_args()\n    convert(args.src, args.dst, args.depth)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/publish_model.py,2,"b""import argparse\nimport subprocess\nimport torch\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description='Process a checkpoint to be published')\n    parser.add_argument('in_file', help='input checkpoint filename')\n    parser.add_argument('out_file', help='output checkpoint filename')\n    args = parser.parse_args()\n    return args\n\n\ndef process_checkpoint(in_file, out_file):\n    checkpoint = torch.load(in_file, map_location='cpu')\n    # remove optimizer for smaller file size\n    if 'optimizer' in checkpoint:\n        del checkpoint['optimizer']\n    # if it is necessary to remove some sensitive data in checkpoint['meta'],\n    # add the code here.\n    torch.save(checkpoint, out_file)\n    sha = subprocess.check_output(['sha256sum', out_file]).decode()\n    final_file = out_file.rstrip('.pth') + '-{}.pth'.format(sha[:8])\n    subprocess.Popen(['mv', out_file, final_file])\n\n\ndef main():\n    args = parse_args()\n    process_checkpoint(args.in_file, args.out_file)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/test.py,8,"b""import argparse\nimport os\nimport os.path as osp\nimport shutil\nimport tempfile\n\nimport mmcv\nimport torch\nimport torch.distributed as dist\nfrom mmcv.runner import load_checkpoint, get_dist_info\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n\nfrom mmdet.apis import init_dist\nfrom mmdet.core import results2json, coco_eval, wrap_fp16_model\nfrom mmdet.datasets import build_dataloader, build_dataset\nfrom mmdet.models import build_detector\n\n\ndef single_gpu_test(model, data_loader, show=False):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=not show, **data)\n        results.append(result)\n\n        if show:\n            model.module.show_result(data, result, dataset.img_norm_cfg)\n\n        batch_size = data['img'][0].size(0)\n        for _ in range(batch_size):\n            prog_bar.update()\n    return results\n\n\ndef multi_gpu_test(model, data_loader, tmpdir=None):\n    model.eval()\n    results = []\n    dataset = data_loader.dataset\n    rank, world_size = get_dist_info()\n    if rank == 0:\n        prog_bar = mmcv.ProgressBar(len(dataset))\n    for i, data in enumerate(data_loader):\n        with torch.no_grad():\n            result = model(return_loss=False, rescale=True, **data)\n        results.append(result)\n\n        if rank == 0:\n            batch_size = data['img'][0].size(0)\n            for _ in range(batch_size * world_size):\n                prog_bar.update()\n\n    # collect results from all ranks\n    results = collect_results(results, len(dataset), tmpdir)\n\n    return results\n\n\ndef collect_results(result_part, size, tmpdir=None):\n    rank, world_size = get_dist_info()\n    # create a tmp dir if it is not specified\n    if tmpdir is None:\n        MAX_LEN = 512\n        # 32 is whitespace\n        dir_tensor = torch.full((MAX_LEN, ),\n                                32,\n                                dtype=torch.uint8,\n                                device='cuda')\n        if rank == 0:\n            tmpdir = tempfile.mkdtemp()\n            tmpdir = torch.tensor(\n                bytearray(tmpdir.encode()), dtype=torch.uint8, device='cuda')\n            dir_tensor[:len(tmpdir)] = tmpdir\n        dist.broadcast(dir_tensor, 0)\n        tmpdir = dir_tensor.cpu().numpy().tobytes().decode().rstrip()\n    else:\n        mmcv.mkdir_or_exist(tmpdir)\n    # dump the part result to the dir\n    mmcv.dump(result_part, osp.join(tmpdir, 'part_{}.pkl'.format(rank)))\n    dist.barrier()\n    # collect all parts\n    if rank != 0:\n        return None\n    else:\n        # load results of all parts from tmp dir\n        part_list = []\n        for i in range(world_size):\n            part_file = osp.join(tmpdir, 'part_{}.pkl'.format(i))\n            part_list.append(mmcv.load(part_file))\n        # sort the results\n        ordered_results = []\n        for res in zip(*part_list):\n            ordered_results.extend(list(res))\n        # the dataloader may pad some samples\n        ordered_results = ordered_results[:size]\n        # remove tmp dir\n        shutil.rmtree(tmpdir)\n        return ordered_results\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='MMDet test detector')\n    parser.add_argument('--config', help='test config file path',default='../configs/cascade_rcnn_x101_64x4d_fpn_1x.py')\n    parser.add_argument('--checkpoint', help='checkpoint file',default='../weights/cascade_rcnn_x101_64x4d_fpn_2x_20181218-5add321e.pth')\n    parser.add_argument('--out', help='output result file')\n    parser.add_argument(\n        '--eval',\n        type=str,\n        nargs='+',\n        choices=['proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints'],\n        help='eval types')\n    parser.add_argument('--show', action='store_true', help='show results')\n    parser.add_argument('--tmpdir', help='tmp dir for writing some results')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    if args.out is not None and not args.out.endswith(('.pkl', '.pickle')):\n        raise ValueError('The output file must be a pkl file.')     # test\xe7\xbb\x93\xe6\x9e\x9c\xe8\xbe\x93\xe5\x87\xba\n\n    cfg = mmcv.Config.fromfile(args.config)     # \xe8\xaf\xbb\xe5\x8f\x96\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe5\x8a\xa0\xe8\xbd\xbddict\n    # set cudnn_benchmark\n    if cfg.get('cudnn_benchmark', False):       # \xe5\x8d\x95\xe5\xb0\xba\xe5\xba\xa6\xe5\xbc\x80\xe5\x90\xaf\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8a\xa0\xe9\x80\x9f\n        torch.backends.cudnn.benchmark = True\n    cfg.model.pretrained = None                 \n    cfg.data.test.test_mode = True\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # build the dataloader\n    # TODO: support multiple images per gpu (only minor changes are needed)\n    dataset = build_dataset(cfg.data.test)      \n    data_loader = build_dataloader(\n        dataset,\n        imgs_per_gpu=1,\n        workers_per_gpu=cfg.data.workers_per_gpu,\n        dist=distributed,\n        shuffle=False)\n\n    # build the model and load checkpoint\n    model = build_detector(cfg.model, train_cfg=None, test_cfg=cfg.test_cfg)\n    fp16_cfg = cfg.get('fp16', None)\n    if fp16_cfg is not None:\n        wrap_fp16_model(model)\n    checkpoint = load_checkpoint(model, args.checkpoint, map_location='cpu')\n    # old versions did not save class info in checkpoints, this walkaround is\n    # for backward compatibility\n    if 'CLASSES' in checkpoint['meta']:\n        model.CLASSES = checkpoint['meta']['CLASSES']\n    else:\n        model.CLASSES = dataset.CLASSES\n\n    if not distributed:\n        model = MMDataParallel(model, device_ids=[0])\n        outputs = single_gpu_test(model, data_loader, args.show)\n    else:\n        model = MMDistributedDataParallel(model.cuda())\n        outputs = multi_gpu_test(model, data_loader, args.tmpdir)\n\n    rank, _ = get_dist_info()\n    if args.out and rank == 0:\n        print('\\nwriting results to {}'.format(args.out))\n        mmcv.dump(outputs, args.out)\n        eval_types = args.eval\n        if eval_types:\n            print('Starting evaluate {}'.format(' and '.join(eval_types)))\n            if eval_types == ['proposal_fast']:\n                result_file = args.out\n                coco_eval(result_file, eval_types, dataset.coco)\n            else:\n                if not isinstance(outputs[0], dict):\n                    result_files = results2json(dataset, outputs, args.out)\n                    coco_eval(result_files, eval_types, dataset.coco)\n                else:\n                    for name in outputs[0]:\n                        print('\\nEvaluating {}'.format(name))\n                        outputs_ = [out[name] for out in outputs]\n                        result_file = args.out + '.{}'.format(name)\n                        result_files = results2json(dataset, outputs_,\n                                                    result_file)\n                        coco_eval(result_files, eval_types, dataset.coco)\n\n\nif __name__ == '__main__':\n    main()\n"""
tools/train.py,1,"b""from __future__ import division\n\nimport argparse\nimport os\nfrom mmcv import Config\n\nfrom mmdet import __version__\nfrom mmdet.datasets import build_dataset\nfrom mmdet.apis import (train_detector, init_dist, get_root_logger,\n                        set_random_seed)\nfrom mmdet.models import build_detector\nimport torch\n\nimport ipdb\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train a detector')\n    # \xe6\x94\xb9\xe5\x8a\xa8\xe4\xba\x86\xef\xbc\x9a\xe5\xb0\x86config\xe8\xae\xbe\xe7\xbd\xae\xe4\xb8\xba\xe5\x8f\xaf\xe9\x80\x89\xe6\x8b\xa9\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe8\xbf\x99\xe6\xa0\xb7\xe5\xb0\xb1\xe4\xb8\x8d\xe7\x94\xa8\xe9\x94\xae\xe5\x85\xa5\xe4\xba\x86\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe6\x94\xb9\xe8\xb7\xaf\xe5\xbe\x84\xef\xbc\x8c\xe6\x96\xb9\xe4\xbe\xbf\n    parser.add_argument('--config', help='train config file path',default='../configs/faster_rcnn_r50_fpn_1x.py')\n    parser.add_argument('--work_dir', help='the dir to save logs and models')\n    parser.add_argument(\n        '--resume_from', help='the checkpoint file to resume from')\n    parser.add_argument(\n        '--validate',\n        action='store_true',\n        help='whether to evaluate the checkpoint during training')\n    parser.add_argument(\n        '--gpus',\n        type=int,\n        default=1,\n        help='number of gpus to use '\n        '(only applicable to non-distributed training)')\n    parser.add_argument('--seed', type=int, default=None, help='random seed')\n    parser.add_argument(\n        '--launcher',\n        choices=['none', 'pytorch', 'slurm', 'mpi'],\n        default='none',\n        help='job launcher')\n    parser.add_argument('--local_rank', type=int, default=0)\n    args = parser.parse_args()\n    if 'LOCAL_RANK' not in os.environ:\n        os.environ['LOCAL_RANK'] = str(args.local_rank)\n\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    cfg = Config.fromfile(args.config)\n    # set cudnn_benchmark \n    # \xe5\x9c\xa8\xe5\x9b\xbe\xe7\x89\x87\xe8\xbe\x93\xe5\x85\xa5\xe5\xb0\xba\xe5\xba\xa6\xe5\x9b\xba\xe5\xae\x9a\xe6\x97\xb6\xe5\xbc\x80\xe5\x90\xaf\xef\xbc\x8c\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8a\xa0\xe9\x80\x9f.\xe4\xb8\x80\xe8\x88\xac\xe9\x83\xbd\xe6\x98\xaf\xe5\x85\xb3\xe7\x9a\x84\xef\xbc\x8c\xe5\x8f\xaa\xe6\x9c\x89\xe5\x9c\xa8\xe5\x9b\xba\xe5\xae\x9a\xe5\xb0\xba\xe5\xba\xa6\xe7\x9a\x84\xe7\xbd\x91\xe7\xbb\x9c\xe5\xa6\x82SSD512\xe4\xb8\xad\xe6\x89\x8d\xe5\xbc\x80\xe5\x90\xaf\n    if cfg.get('cudnn_benchmark', False):\n        torch.backends.cudnn.benchmark = True\n    # update configs according to CLI args\n    if args.work_dir is not None:\n        # \xe5\x88\x9b\xe5\xbb\xba\xe5\xb7\xa5\xe4\xbd\x9c\xe7\x9b\xae\xe5\xbd\x95\xe5\xad\x98\xe6\x94\xbe\xe8\xae\xad\xe7\xbb\x83\xe6\x96\x87\xe4\xbb\xb6\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe9\x94\xae\xe5\x85\xa5\xef\xbc\x8c\xe4\xbc\x9a\xe8\x87\xaa\xe5\x8a\xa8\xe6\x8c\x89\xe7\x85\xa7py\xe9\x85\x8d\xe7\xbd\xae\xe6\x96\x87\xe4\xbb\xb6\xe7\x94\x9f\xe6\x88\x90\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe7\x9b\xae\xe5\xbd\x95\n        cfg.work_dir = args.work_dir\n    if args.resume_from is not None:    \n        # \xe6\x96\xad\xe7\x82\xb9\xe7\xbb\xa7\xe7\xbb\xad\xe8\xae\xad\xe7\xbb\x83\xe7\x9a\x84\xe6\x9d\x83\xe5\x80\xbc\xe6\x96\x87\xe4\xbb\xb6\n        cfg.resume_from = args.resume_from\n    cfg.gpus = args.gpus\n\n    # init distributed env first, since logger depends on the dist info.\n    if args.launcher == 'none':\n        distributed = False\n    else:\n        distributed = True\n        init_dist(args.launcher, **cfg.dist_params)\n\n    # init logger before other steps\n    logger = get_root_logger(cfg.log_level)\n    logger.info('Distributed training: {}'.format(distributed))\n\n    # set random seeds\n    if args.seed is not None:\n        logger.info('Set random seed to {}'.format(args.seed))\n        set_random_seed(args.seed)\n\n    # ipdb.set_trace(context=35)\n    #  \xe6\x90\xad\xe5\xbb\xba\xe6\xa8\xa1\xe5\x9e\x8b\n    model = build_detector(\n        cfg.model, train_cfg=cfg.train_cfg, test_cfg=cfg.test_cfg)\n\n     # \xe5\xb0\x86\xe8\xae\xad\xe7\xbb\x83\xe9\x85\x8d\xe7\xbd\xae\xe4\xbc\xa0\xe5\x85\xa5\n    train_dataset = build_dataset(cfg.data.train)\n    if cfg.checkpoint_config is not None:\n        # save mmdet version, config file content and class names in checkpoints as meta data\n        # \xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xe7\x9a\x84\xe6\x98\xaf\xef\xbc\x8c\xe4\xbb\xa5\xe5\x89\x8d\xe5\x8f\x91\xe5\xb8\x83\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\x98\xaf\xe4\xb8\x8d\xe5\xad\x98\xe8\xbf\x99\xe4\xb8\xaa\xe7\xb1\xbb\xe5\x88\xab\xe7\xad\x89\xe4\xbf\xa1\xe6\x81\xaf\xe7\x9a\x84\xef\xbc\x8c\n        # \xe7\x94\xa8\xe7\x9a\x84\xe9\xbb\x98\xe8\xae\xa4COCO\xe6\x88\x96\xe8\x80\x85VOC\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe6\x89\x80\xe4\xbb\xa5\xe5\xa6\x82\xe6\x9e\x9c\xe7\x94\xa8\xe4\xbb\xa5\xe5\x89\x8d\xe8\xae\xad\xe7\xbb\x83\xe5\xa5\xbd\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8b\xe6\xa3\x80\xe6\xb5\x8b\xe6\x97\xb6\xe4\xbc\x9a\xe6\x8f\x90\xe9\x86\x92warning\xe4\xb8\x80\xe4\xb8\x8b\xef\xbc\x8c\xe6\x97\xa0\xe4\xbc\xa4\xe5\xa4\xa7\xe9\x9b\x85\n        cfg.checkpoint_config.meta = dict(\n            mmdet_version=__version__,\n            config=cfg.text,\n            CLASSES=train_dataset.CLASSES)\n\n    # add an attribute for visualization convenience\n    model.CLASSES = train_dataset.CLASSES   # model\xe7\x9a\x84CLASSES\xe5\xb1\x9e\xe6\x80\xa7\xe6\x9c\xac\xe6\x9d\xa5\xe6\xb2\xa1\xe6\x9c\x89\xe7\x9a\x84\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xafpython\xe4\xb8\x8d\xe7\x94\xa8\xe6\x8f\x90\xe5\x89\x8d\xe5\xa3\xb0\xe6\x98\x8e\xef\xbc\x8c\xe5\x86\x8d\xe8\xb5\x8b\xe5\x80\xbc\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe8\x87\xaa\xe5\x8a\xa8\xe5\xae\x9a\xe4\xb9\x89\xe5\x8f\x98\xe9\x87\x8f\n    train_detector(\n        model,\n        train_dataset,\n        cfg,\n        distributed=distributed,\n        validate=args.validate,\n        logger=logger)\n\n\nif __name__ == '__main__':\n    main()\n\n"""
tools/upgrade_model_version.py,2,"b'import argparse\nimport re\nfrom collections import OrderedDict\n\nimport torch\n\n\ndef convert(in_file, out_file):\n    """"""Convert keys in checkpoints.\n\n    There can be some breaking changes during the development of mmdetection,\n    and this tool is used for upgrading checkpoints trained with old versions\n    to the latest one.\n    """"""\n    checkpoint = torch.load(in_file)\n    in_state_dict = checkpoint.pop(\'state_dict\')\n    out_state_dict = OrderedDict()\n    for key, val in in_state_dict.items():\n        # Use ConvModule instead of nn.Conv2d in RetinaNet\n        # cls_convs.0.weight -> cls_convs.0.conv.weight\n        m = re.search(r\'(cls_convs|reg_convs).\\d.(weight|bias)\', key)\n        if m is not None:\n            param = m.groups()[1]\n            new_key = key.replace(param, \'conv.{}\'.format(param))\n            out_state_dict[new_key] = val\n            continue\n\n        out_state_dict[key] = val\n    checkpoint[\'state_dict\'] = out_state_dict\n    torch.save(checkpoint, out_file)\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Upgrade model version\')\n    parser.add_argument(\'in_file\', help=\'input checkpoint file\')\n    parser.add_argument(\'out_file\', help=\'output checkpoint file\')\n    args = parser.parse_args()\n    convert(args.in_file, args.out_file)\n\n\nif __name__ == \'__main__\':\n    main()\n'"
tools/voc_eval.py,0,"b""from argparse import ArgumentParser\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet import datasets\nfrom mmdet.core import eval_map\n\n\ndef voc_eval(result_file, dataset, iou_thr=0.5):\n    det_results = mmcv.load(result_file)\n    gt_bboxes = []\n    gt_labels = []\n    gt_ignore = []\n    for i in range(len(dataset)):\n        ann = dataset.get_ann_info(i)\n        bboxes = ann['bboxes']\n        labels = ann['labels']\n        if 'bboxes_ignore' in ann:\n            ignore = np.concatenate([\n                np.zeros(bboxes.shape[0], dtype=np.bool),\n                np.ones(ann['bboxes_ignore'].shape[0], dtype=np.bool)\n            ])\n            gt_ignore.append(ignore)\n            bboxes = np.vstack([bboxes, ann['bboxes_ignore']])\n            labels = np.concatenate([labels, ann['labels_ignore']])\n        gt_bboxes.append(bboxes)\n        gt_labels.append(labels)\n    if not gt_ignore:\n        gt_ignore = gt_ignore\n    if hasattr(dataset, 'year') and dataset.year == 2007:\n        dataset_name = 'voc07'\n    else:\n        dataset_name = dataset.CLASSES\n    eval_map(\n        det_results,\n        gt_bboxes,\n        gt_labels,\n        gt_ignore=gt_ignore,\n        scale_ranges=None,\n        iou_thr=iou_thr,\n        dataset=dataset_name,\n        print_summary=True)\n\n\ndef main():\n    parser = ArgumentParser(description='VOC Evaluation')\n    parser.add_argument('result', help='result file path')\n    parser.add_argument('config', help='config file path')\n    parser.add_argument(\n        '--iou-thr',\n        type=float,\n        default=0.5,\n        help='IoU threshold for evaluation')\n    args = parser.parse_args()\n    cfg = mmcv.Config.fromfile(args.config)\n    test_dataset = mmcv.runner.obj_from_dict(cfg.data.test, datasets)\n    voc_eval(args.result, test_dataset, args.iou_thr)\n\n\nif __name__ == '__main__':\n    main()\n"""
configs/dcn/cascade_mask_rcnn_dconv_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False, deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n            loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_mask_rcnn_dconv_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/cascade_rcnn_dconv_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False,\n            deformable_groups=1,\n            fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_dconv_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/faster_rcnn_dconv_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False, deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_dconv_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/faster_rcnn_dconv_c3-c5_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False,\n            groups=32,\n            deformable_groups=1,\n            fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_dconv_c3-c5_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/faster_rcnn_dpool_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(\n            type='DeformRoIPoolingPack',\n            out_size=7,\n            out_channels=256,\n            no_trans=False,\n            group_size=1,\n            trans_std=0.1),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_dpool_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/faster_rcnn_mdconv_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=True, deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_mdconv_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/faster_rcnn_mdpool_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(\n            type='ModulatedDeformRoIPoolingPack',\n            out_size=7,\n            out_channels=256,\n            no_trans=False,\n            group_size=1,\n            trans_std=0.1),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_mdpool_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/dcn/mask_rcnn_dconv_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False,\n            deformable_groups=1,\n            fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_dconv_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_0010_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gen_attention=dict(\n            spatial_range=-1, num_heads=8, attention_type='0010', kv_stride=2),\n        stage_with_gen_attention=[[], [], [0, 1, 2, 3, 4, 5], [0, 1, 2]],\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_attention_0010_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_0010_dcn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gen_attention=dict(\n            spatial_range=-1, num_heads=8, attention_type='0010', kv_stride=2),\n        stage_with_gen_attention=[[], [], [0, 1, 2, 3, 4, 5], [0, 1, 2]],\n        dcn=dict(\n            modulated=False, deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True),\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_attention_0010_dcn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_1111_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gen_attention=dict(\n            spatial_range=-1, num_heads=8, attention_type='1111', kv_stride=2),\n        stage_with_gen_attention=[[], [], [0, 1, 2, 3, 4, 5], [0, 1, 2]],\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_attention_1111_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/empirical_attention/faster_rcnn_r50_fpn_attention_1111_dcn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gen_attention=dict(\n            spatial_range=-1, num_heads=8, attention_type='1111', kv_stride=2),\n        stage_with_gen_attention=[[], [], [0, 1, 2, 3, 4, 5], [0, 1, 2]],\n        dcn=dict(\n            modulated=False, deformable_groups=1, fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True),\n    ),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_attention_1111_dcn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fcos/fcos_mstrain_640_800_r101_caffe_fpn_gn_2x_4gpu.py,0,"b""# model settings\nmodel = dict(\n    type='FCOS',\n    pretrained='open-mmlab://resnet101_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=4,\n    workers_per_gpu=4,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_options=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='constant',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(4)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fcos_mstrain_640_800_r101_caffe_fpn_gn_2x_4gpu'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fcos/fcos_mstrain_640_800_x101_64x4d_fpn_gn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='FCOS',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=[(1333, 640), (1333, 800)],\n        multiscale_mode='value',\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_options=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='constant',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fcos_mstrain_640_800_x101_64x4d_fpn_gn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fcos/fcos_r50_caffe_fpn_gn_1x_4gpu.py,0,"b""# model settings\nmodel = dict(\n    type='FCOS',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        extra_convs_on_inputs=False,  # use P5\n        num_outs=5,\n        relu_before_extra_convs=True),\n    bbox_head=dict(\n        type='FCOSHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        strides=[8, 16, 32, 64, 128],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='IoULoss', loss_weight=1.0),\n        loss_centerness=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=4,\n    workers_per_gpu=4,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.01,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_options=dict(bias_lr_mult=2., bias_decay_mult=0.))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='constant',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(4)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/fcos_r50_caffe_fpn_gn_1x_4gpu'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fp16/faster_rcnn_r50_fpn_fp16_1x.py,0,"b""# fp16 settings\nfp16 = dict(loss_scale=512.)\n\n# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_fp16_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fp16/mask_rcnn_r50_fpn_fp16_1x.py,0,"b""# fp16 settings\nfp16 = dict(loss_scale=512.)\n\n# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\nevaluation = dict(interval=1)\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_fp16_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/fp16/retinanet_r50_fpn_fp16_1x.py,0,"b""# fp16 settings\nfp16 = dict(loss_scale=512.)\n\n# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.11, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/retinanet_r50_fpn_fp16_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gcnet/mask_rcnn_r16_gcb_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gcb=dict(\n            ratio=1./16.,\n        ),\n        stage_with_gcb=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r16_gcb_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gcnet/mask_rcnn_r16_gcb_c3-c5_r50_fpn_syncbn_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gcb=dict(\n            ratio=1./16.,\n        ),\n        stage_with_gcb=(False, True, True, True),\n        norm_eval=False,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r16_gcb_c3-c5_r50_fpn_syncbn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gcnet/mask_rcnn_r4_gcb_c3-c5_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gcb=dict(\n            ratio=1./4.,\n        ),\n        stage_with_gcb=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r4_gcb_c3-c5_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gcnet/mask_rcnn_r4_gcb_c3-c5_r50_fpn_syncbn_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        gcb=dict(\n            ratio=1./4.,\n        ),\n        stage_with_gcb=(False, True, True, True),\n        norm_eval=False,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r4_gcb_c3-c5_r50_fpn_syncbn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gcnet/mask_rcnn_r50_fpn_sbn_1x.py,0,"b""# model settings\nnorm_cfg = dict(type='SyncBN', requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        norm_eval=False,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_sbn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ghm/retinanet_ghm_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='GHMC',\n            bins=30,\n            momentum=0.75,\n            use_sigmoid=True,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            type='GHMR',\n            mu=0.02,\n            bins=10,\n            momentum=0.7,\n            loss_weight=10.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ghm'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn+ws/faster_rcnn_r50_fpn_gn_ws_1x.py,0,"b""# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://jhu/resnet50_gn_ws',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_gn_ws_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn+ws/mask_rcnn_r50_fpn_gn_ws_20_23_24e.py,0,"b""# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://jhu/resnet50_gn_ws',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[20, 23])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_gn_ws_20_23_24e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn+ws/mask_rcnn_r50_fpn_gn_ws_2x.py,0,"b""# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://jhu/resnet50_gn_ws',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_gn_ws_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn+ws/mask_rcnn_x101_32x4d_fpn_gn_ws_2x.py,0,"b""# model settings\nconv_cfg = dict(type='ConvWS')\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://jhu/resnext101_32x4d_gn_ws',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        conv_cfg=conv_cfg,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_x101_32x4d_fpn_gn_ws_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn/mask_rcnn_r101_fpn_gn_2x.py,0,"b""# model settings\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://detectron/resnet101_gn',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r101_fpn_gn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn/mask_rcnn_r50_fpn_gn_2x.py,0,"b""# model settings\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://detectron/resnet50_gn',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_gn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/gn/mask_rcnn_r50_fpn_gn_contrib_2x.py,0,"b""# model settings\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\n\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://contrib/resnet50_gn',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 22])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_r50_fpn_gn_contrib_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/grid_rcnn/grid_rcnn_gn_head_r50_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='GridRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        with_reg=False,\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    grid_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    grid_head=dict(\n        type='GridHead',\n        grid_points=9,\n        num_convs=8,\n        in_channels=256,\n        point_feat_channels=64,\n        norm_cfg=dict(type='GN', num_groups=36),\n        loss_grid=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=15)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_radius=1,\n        pos_weight=-1,\n        max_num_grid=192,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.03, nms=dict(type='nms', iou_thr=0.3), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=3665,\n    warmup_ratio=1.0 / 80,\n    step=[17, 23])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 25\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/grid_rcnn_gn_head_r50_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/grid_rcnn/grid_rcnn_gn_head_x101_32x4d_fpn_2x.py,0,"b""# model settings\nmodel = dict(\n    type='GridRCNN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        with_reg=False,\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False),\n    grid_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    grid_head=dict(\n        type='GridHead',\n        grid_points=9,\n        num_convs=8,\n        in_channels=256,\n        point_feat_channels=64,\n        norm_cfg=dict(type='GN', num_groups=36),\n        loss_grid=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=15)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_radius=1,\n        pos_weight=-1,\n        max_num_grid=192,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.03, nms=dict(type='nms', iou_thr=0.3), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=3665,\n    warmup_ratio=1.0 / 80,\n    step=[17, 23])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 25\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/grid_rcnn_gn_head_x101_32x4d_fpn_2x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_fast_r50_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.05, 0.05, 0.1, 0.1],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.6,\n            neg_iou_thr=0.6,\n            min_pos_iou=0.6,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=1e-3, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        num_max_proposals=300,\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        num_max_proposals=300,\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        num_max_proposals=300,\n        proposal_file=data_root + 'proposals/ga_rpn_r50_fpn_1x_val2017.pkl',\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_fast_rcnn_r50_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_faster_r50_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        octave_base_scale=8,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[0.07, 0.07, 0.14, 0.14],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[0.07, 0.07, 0.11, 0.11],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.05, 0.05, 0.1, 0.1],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=300,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.6,\n            neg_iou_thr=0.6,\n            min_pos_iou=0.6,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=300,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=1e-3, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_faster_rcnn_r50_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_faster_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        octave_base_scale=8,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[0.07, 0.07, 0.14, 0.14],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[0.07, 0.07, 0.11, 0.11],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.05, 0.05, 0.1, 0.1],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=300,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.6,\n            neg_iou_thr=0.6,\n            min_pos_iou=0.6,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=300,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=1e-3, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_faster_rcnn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_retinanet_r50_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='GARetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[1.0, 1.0, 1.0, 1.0],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.04, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    ga_assigner=dict(\n        type='ApproxMaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.4,\n        ignore_iof_thr=-1),\n    ga_sampler=dict(\n        type='RandomSampler',\n        num=256,\n        pos_fraction=0.5,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=False),\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    center_ratio=0.2,\n    ignore_ratio=0.5,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_retinanet_r50_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_retinanet_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        start_level=1,\n        add_extra_convs=True,\n        num_outs=5),\n    bbox_head=dict(\n        type='GARetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[1.0, 1.0, 1.0, 1.0],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=0.04, loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    ga_assigner=dict(\n        type='ApproxMaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0.4,\n        ignore_iof_thr=-1),\n    ga_sampler=dict(\n        type='RandomSampler',\n        num=256,\n        pos_fraction=0.5,\n        neg_pos_ub=-1,\n        add_gt_as_proposals=False),\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.0,\n        ignore_iof_thr=-1),\n    allowed_border=-1,\n    pos_weight=-1,\n    center_ratio=0.2,\n    ignore_ratio=0.5,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_retinanet_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_rpn_r101_caffe_rpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnet101_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        octave_base_scale=8,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[0.07, 0.07, 0.14, 0.14],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[0.07, 0.07, 0.11, 0.11],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_rpn_r101_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_rpn_r50_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        norm_eval=True,\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        octave_base_scale=8,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[0.07, 0.07, 0.14, 0.14],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[0.07, 0.07, 0.11, 0.11],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_rpn_r50_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/guided_anchoring/ga_rpn_x101_32x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RPN',\n    pretrained='open-mmlab://resnext101_32x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='GARPNHead',\n        in_channels=256,\n        feat_channels=256,\n        octave_base_scale=8,\n        scales_per_octave=3,\n        octave_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        anchor_base_sizes=None,\n        anchoring_means=[.0, .0, .0, .0],\n        anchoring_stds=[0.07, 0.07, 0.14, 0.14],\n        target_means=(.0, .0, .0, .0),\n        target_stds=[0.07, 0.07, 0.11, 0.11],\n        loc_filter_thr=0.01,\n        loss_loc=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_shape=dict(type='BoundedIoULoss', beta=0.2, loss_weight=1.0),\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        ga_assigner=dict(\n            type='ApproxMaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        ga_sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        center_ratio=0.2,\n        ignore_ratio=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\n# runner configs\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ga_rpn_x101_32x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/cascade_rcnn_hrnetv2p_w32_20e.py,0,"b""# model settings\nmodel = dict(\n    type='CascadeRCNN',\n    num_stages=3,\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4, ),\n                num_channels=(64, )),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(\n            type='RoIAlign',\n            out_size=7,\n            sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n    ])\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53],\n    std=[58.395, 57.12, 57.375],\n    to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/cascade_rcnn_hrnetv2p_w32'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w18_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4,),\n                num_channels=(64,)),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(18, 36)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(18, 36, 72)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(18, 36, 72, 144)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[18, 36, 72, 144],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=4,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_hrnetv2p_w18_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w32_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4,),\n                num_channels=(64,)),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_hrnetv2p_w32_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/faster_rcnn_hrnetv2p_w40_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://msra/hrnetv2_w40',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4,),\n                num_channels=(64,)),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(40, 80)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(40, 80, 160)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(40, 80, 160, 320)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[40, 80, 160, 320],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=4,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_hrnetv2p_w40_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w18_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://msra/hrnetv2_w18',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4,),\n                num_channels=(64,)),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(18, 36)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(18, 36, 72)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(18, 36, 72, 144)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[18, 36, 72, 144],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\n# if you use 8 GPUs for training, please change lr to 0.02\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_hrnetv2p_w18_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/hrnet/mask_rcnn_hrnetv2p_w32_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskRCNN',\n    pretrained='open-mmlab://msra/hrnetv2_w32',\n    backbone=dict(\n        type='HRNet',\n        extra=dict(\n            stage1=dict(\n                num_modules=1,\n                num_branches=1,\n                block='BOTTLENECK',\n                num_blocks=(4,),\n                num_channels=(64,)),\n            stage2=dict(\n                num_modules=1,\n                num_branches=2,\n                block='BASIC',\n                num_blocks=(4, 4),\n                num_channels=(32, 64)),\n            stage3=dict(\n                num_modules=4,\n                num_branches=3,\n                block='BASIC',\n                num_blocks=(4, 4, 4),\n                num_channels=(32, 64, 128)),\n            stage4=dict(\n                num_modules=3,\n                num_branches=4,\n                block='BASIC',\n                num_blocks=(4, 4, 4, 4),\n                num_channels=(32, 64, 128, 256)))),\n    neck=dict(\n        type='HRFPN',\n        in_channels=[32, 64, 128, 256],\n        out_channels=256),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53],\n                    std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/mask_rcnn_hrnetv2p_w32_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_64x4d',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch',\n        dcn=dict(\n            modulated=False,\n            groups=64,\n            deformable_groups=1,\n            fallback_on_stride=False),\n        stage_with_dcn=(False, True, True, True)),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=1,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=[(1600, 400), (1600, 1400)],\n        multiscale_mode='range',\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_dconv_c3-c5_mstrain_400_1400_x101_64x4d_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_r101_fpn_20e.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='modelzoo://resnet101',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_r101_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_r50_fpn_20e.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_r50_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_without_semantic_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='modelzoo://resnet50',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_without_semantic_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_x101_32x4d_fpn_20e_16gpu.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_32x4d',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=32,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=1,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_x101_32x4d_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/htc/htc_x101_64x4d_fpn_20e_16gpu.py,0,"b""# model settings\nmodel = dict(\n    type='HybridTaskCascade',\n    num_stages=3,\n    pretrained='open-mmlab://resnext101_64x4d',\n    interleaved=True,\n    mask_info_flow=True,\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=[\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.1, 0.1, 0.2, 0.2],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.05, 0.05, 0.1, 0.1],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0)),\n        dict(\n            type='SharedFCBBoxHead',\n            num_fcs=2,\n            in_channels=256,\n            fc_out_channels=1024,\n            roi_feat_size=7,\n            num_classes=81,\n            target_means=[0., 0., 0., 0.],\n            target_stds=[0.033, 0.033, 0.067, 0.067],\n            reg_class_agnostic=True,\n            loss_cls=dict(\n                type='CrossEntropyLoss',\n                use_sigmoid=False,\n                loss_weight=1.0),\n            loss_bbox=dict(\n                type='SmoothL1Loss',\n                beta=1.0,\n                loss_weight=1.0))\n    ],\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='HTCMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    semantic_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[8]),\n    semantic_head=dict(\n        type='FusedSemanticHead',\n        num_ins=5,\n        fusion_level=1,\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=183,\n        ignore_label=255,\n        loss_weight=0.2))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=[\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.5,\n                neg_iou_thr=0.5,\n                min_pos_iou=0.5,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.6,\n                neg_iou_thr=0.6,\n                min_pos_iou=0.6,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False),\n        dict(\n            assigner=dict(\n                type='MaxIoUAssigner',\n                pos_iou_thr=0.7,\n                neg_iou_thr=0.7,\n                min_pos_iou=0.7,\n                ignore_iof_thr=-1),\n            sampler=dict(\n                type='RandomSampler',\n                num=512,\n                pos_fraction=0.25,\n                neg_pos_ub=-1,\n                add_gt_as_proposals=True),\n            mask_size=28,\n            pos_weight=-1,\n            debug=False)\n    ],\n    stage_loss_weights=[1, 0.5, 0.25])\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.001,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5),\n    keep_all_stages=False)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=1,\n    workers_per_gpu=1,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        seg_prefix=data_root + 'stuffthingmaps/train2017/',\n        seg_scale_factor=1 / 8,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True,\n        with_semantic_seg=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 19])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 20\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/htc_x101_64x4d_fpn_20e'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/libra_rcnn/libra_fast_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FastRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=1.0,\n            loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3)),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=0,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        proposal_file=data_root +\n        'libra_proposals/rpn_r50_fpn_1x_train2017.pkl',\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'libra_proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        proposal_file=data_root + 'libra_proposals/rpn_r50_fpn_1x_val2017.pkl',\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/libra_fast_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/libra_rcnn/libra_faster_rcnn_r101_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet101',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=1.0,\n            loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=5,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3)),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/libra_faster_rcnn_r101_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/libra_rcnn/libra_faster_rcnn_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=1.0,\n            loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=5,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3)),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/libra_faster_rcnn_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/libra_rcnn/libra_faster_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=2,\n            refine_type='non_local')\n    ],\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=1.0,\n            loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=5,\n            add_gt_as_proposals=False),\n        allowed_border=-1,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='CombinedSampler',\n            num=512,\n            pos_fraction=0.25,\n            add_gt_as_proposals=True,\n            pos_sampler=dict(type='InstanceBalancedPosSampler'),\n            neg_sampler=dict(\n                type='IoUBalancedNegSampler',\n                floor_thr=-1,\n                floor_fraction=0,\n                num_bins=3)),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/libra_faster_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/libra_rcnn/libra_retinanet_r50_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='RetinaNet',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=[\n        dict(\n            type='FPN',\n            in_channels=[256, 512, 1024, 2048],\n            out_channels=256,\n            start_level=1,\n            extra_convs_on_inputs=True,\n            add_extra_convs=True,\n            num_outs=5),\n        dict(\n            type='BFP',\n            in_channels=256,\n            num_levels=5,\n            refine_level=1,\n            refine_type='non_local')\n    ],\n    bbox_head=dict(\n        type='RetinaHead',\n        num_classes=81,\n        in_channels=256,\n        stacked_convs=4,\n        feat_channels=256,\n        octave_base_scale=4,\n        scales_per_octave=3,\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[8, 16, 32, 64, 128],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='FocalLoss',\n            use_sigmoid=True,\n            gamma=2.0,\n            alpha=0.25,\n            loss_weight=1.0),\n        loss_bbox=dict(\n            type='BalancedL1Loss',\n            alpha=0.5,\n            gamma=1.5,\n            beta=0.11,\n            loss_weight=1.0)))\n# training and testing settings\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.4,\n        min_pos_iou=0,\n        ignore_iof_thr=-1),\n    smoothl1_beta=0.11,\n    gamma=2.0,\n    alpha=0.25,\n    allowed_border=-1,\n    pos_weight=-1,\n    debug=False)\ntest_cfg = dict(\n    nms_pre=1000,\n    min_bbox_size=0,\n    score_thr=0.05,\n    nms=dict(type='nms', iou_thr=0.5),\n    max_per_img=100)\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndevice_ids = range(8)\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/libra_retinanet_r50_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ms_rcnn/ms_rcnn_r101_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskScoringRCNN',\n    pretrained='open-mmlab://resnet101_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=101,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    mask_iou_head=dict(\n        type='MaskIoUHead',\n        num_convs=4,\n        num_fcs=2,\n        roi_feat_size=14,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        mask_thr_binary=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ms_rcnn_r101_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ms_rcnn/ms_rcnn_r50_caffe_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskScoringRCNN',\n    pretrained='open-mmlab://resnet50_caffe',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        norm_cfg=dict(type='BN', requires_grad=False),\n        style='caffe'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    mask_iou_head=dict(\n        type='MaskIoUHead',\n        num_convs=4,\n        num_fcs=2,\n        roi_feat_size=14,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        mask_thr_binary=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[102.9801, 115.9465, 122.7717], std=[1.0, 1.0, 1.0], to_rgb=False)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ms_rcnn_r50_caffe_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/ms_rcnn/ms_rcnn_x101_64x4d_fpn_1x.py,0,"b""# model settings\nmodel = dict(\n    type='MaskScoringRCNN',\n    pretrained='open-mmlab://resnext101_64x4d',\n    backbone=dict(\n        type='ResNeXt',\n        depth=101,\n        groups=64,\n        base_width=4,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)),\n    mask_iou_head=dict(\n        type='MaskIoUHead',\n        num_convs=4,\n        num_fcs=2,\n        roi_feat_size=14,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        num_classes=81))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        mask_thr_binary=0.5,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.02, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[8, 11])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ms_rcnn_x101_64x4d_fpn_1x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/pascal_voc/faster_rcnn_r50_fpn_1x_voc0712.py,0,"b""# model settings\nmodel = dict(\n    type='FasterRCNN',\n    pretrained='modelzoo://resnet50',\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=1,\n        style='pytorch'),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='SharedFCBBoxHead',\n        num_fcs=2,\n        in_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=21,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100)\n    # soft-nms is also supported for rcnn testing\n    # e.g., nms=dict(type='soft_nms', iou_thr=0.5, min_score=0.05)\n)\n# dataset settings\ndataset_type = 'VOCDataset'\ndata_root = 'data/VOCdevkit/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',  # to avoid reloading datasets frequently\n        times=3,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=[\n                data_root + 'VOC2007/ImageSets/Main/trainval.txt',\n                data_root + 'VOC2012/ImageSets/Main/trainval.txt'\n            ],\n            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],\n            img_scale=(1000, 600),\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=32,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=True,\n            with_label=True)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(1000, 600),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(1000, 600),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(type='SGD', lr=0.01, momentum=0.9, weight_decay=0.0001)\noptimizer_config = dict(grad_clip=dict(max_norm=35, norm_type=2))\n# learning policy\nlr_config = dict(policy='step', step=[3])  # actual epoch = 3 * 3 = 9\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 4  # actual epoch = 4 * 3 = 12\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/faster_rcnn_r50_fpn_1x_voc0712'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/pascal_voc/ssd300_voc.py,0,"b""# model settings\ninput_size = 300\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(512, 1024, 512, 256, 256, 256),\n        num_classes=21,\n        anchor_strides=(8, 16, 32, 64, 100, 300),\n        basesize_ratio_range=(0.2, 0.9),\n        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2)))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'VOCDataset'\ndata_root = 'data/VOCdevkit/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=4,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=10,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=[\n                data_root + 'VOC2007/ImageSets/Main/trainval.txt',\n                data_root + 'VOC2012/ImageSets/Main/trainval.txt'\n            ],\n            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],\n            img_scale=(300, 300),\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=None,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=False,\n            with_label=True,\n            test_mode=False,\n            extra_aug=dict(\n                photo_metric_distortion=dict(\n                    brightness_delta=32,\n                    contrast_range=(0.5, 1.5),\n                    saturation_range=(0.5, 1.5),\n                    hue_delta=18),\n                expand=dict(\n                    mean=img_norm_cfg['mean'],\n                    to_rgb=img_norm_cfg['to_rgb'],\n                    ratio_range=(1, 4)),\n                random_crop=dict(\n                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),\n            resize_keep_ratio=False)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False))\n# optimizer\noptimizer = dict(type='SGD', lr=1e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 20])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ssd300_voc'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/pascal_voc/ssd512_voc.py,0,"b""# model settings\ninput_size = 512\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(512, 1024, 512, 256, 256, 256, 256),\n        num_classes=21,\n        anchor_strides=(8, 16, 32, 64, 128, 256, 512),\n        basesize_ratio_range=(0.15, 0.9),\n        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2, 3], [2], [2]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2)))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'VOCDataset'\ndata_root = 'data/VOCdevkit/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=4,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=10,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=[\n                data_root + 'VOC2007/ImageSets/Main/trainval.txt',\n                data_root + 'VOC2012/ImageSets/Main/trainval.txt'\n            ],\n            img_prefix=[data_root + 'VOC2007/', data_root + 'VOC2012/'],\n            img_scale=(512, 512),\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=None,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=False,\n            with_label=True,\n            test_mode=False,\n            extra_aug=dict(\n                photo_metric_distortion=dict(\n                    brightness_delta=32,\n                    contrast_range=(0.5, 1.5),\n                    saturation_range=(0.5, 1.5),\n                    hue_delta=18),\n                expand=dict(\n                    mean=img_norm_cfg['mean'],\n                    to_rgb=img_norm_cfg['to_rgb'],\n                    ratio_range=(1, 4)),\n                random_crop=dict(\n                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),\n            resize_keep_ratio=False)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(512, 512),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n        img_prefix=data_root + 'VOC2007/',\n        img_scale=(512, 512),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False))\n# optimizer\noptimizer = dict(type='SGD', lr=1e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=1.0 / 3,\n    step=[16, 20])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ssd512_voc'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/scratch/scratch_faster_rcnn_r50_fpn_gn_6x.py,0,"b""# model settings\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='FasterRCNN',\n    pretrained=None,\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=-1,\n        style='pytorch',\n        zero_init_residual=False,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05, nms=dict(type='nms', iou_thr=0.5), max_per_img=100))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.02,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_options=dict(norm_decay_mult=0))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.1,\n    step=[65, 71])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 73\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/scratch_faster_rcnn_r50_fpn_gn_6x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/scratch/scratch_mask_rcnn_r50_fpn_gn_6x.py,0,"b""# model settings\nnorm_cfg = dict(type='GN', num_groups=32, requires_grad=True)\nmodel = dict(\n    type='MaskRCNN',\n    pretrained=None,\n    backbone=dict(\n        type='ResNet',\n        depth=50,\n        num_stages=4,\n        out_indices=(0, 1, 2, 3),\n        frozen_stages=-1,\n        style='pytorch',\n        zero_init_residual=False,\n        norm_cfg=norm_cfg),\n    neck=dict(\n        type='FPN',\n        in_channels=[256, 512, 1024, 2048],\n        out_channels=256,\n        num_outs=5,\n        norm_cfg=norm_cfg),\n    rpn_head=dict(\n        type='RPNHead',\n        in_channels=256,\n        feat_channels=256,\n        anchor_scales=[8],\n        anchor_ratios=[0.5, 1.0, 2.0],\n        anchor_strides=[4, 8, 16, 32, 64],\n        target_means=[.0, .0, .0, .0],\n        target_stds=[1.0, 1.0, 1.0, 1.0],\n        use_sigmoid_cls=True,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=True, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0 / 9.0, loss_weight=1.0)),\n    bbox_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=7, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    bbox_head=dict(\n        type='ConvFCBBoxHead',\n        num_shared_convs=4,\n        num_shared_fcs=1,\n        in_channels=256,\n        conv_out_channels=256,\n        fc_out_channels=1024,\n        roi_feat_size=7,\n        num_classes=81,\n        target_means=[0., 0., 0., 0.],\n        target_stds=[0.1, 0.1, 0.2, 0.2],\n        reg_class_agnostic=False,\n        norm_cfg=norm_cfg,\n        loss_cls=dict(\n            type='CrossEntropyLoss', use_sigmoid=False, loss_weight=1.0),\n        loss_bbox=dict(type='SmoothL1Loss', beta=1.0, loss_weight=1.0)),\n    mask_roi_extractor=dict(\n        type='SingleRoIExtractor',\n        roi_layer=dict(type='RoIAlign', out_size=14, sample_num=2),\n        out_channels=256,\n        featmap_strides=[4, 8, 16, 32]),\n    mask_head=dict(\n        type='FCNMaskHead',\n        num_convs=4,\n        in_channels=256,\n        conv_out_channels=256,\n        num_classes=81,\n        norm_cfg=norm_cfg,\n        loss_mask=dict(\n            type='CrossEntropyLoss', use_mask=True, loss_weight=1.0)))\n# model training and testing settings\ntrain_cfg = dict(\n    rpn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.7,\n            neg_iou_thr=0.3,\n            min_pos_iou=0.3,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=256,\n            pos_fraction=0.5,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=False),\n        allowed_border=0,\n        pos_weight=-1,\n        debug=False),\n    rpn_proposal=dict(\n        nms_across_levels=False,\n        nms_pre=2000,\n        nms_post=2000,\n        max_num=2000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        assigner=dict(\n            type='MaxIoUAssigner',\n            pos_iou_thr=0.5,\n            neg_iou_thr=0.5,\n            min_pos_iou=0.5,\n            ignore_iof_thr=-1),\n        sampler=dict(\n            type='RandomSampler',\n            num=512,\n            pos_fraction=0.25,\n            neg_pos_ub=-1,\n            add_gt_as_proposals=True),\n        mask_size=28,\n        pos_weight=-1,\n        debug=False))\ntest_cfg = dict(\n    rpn=dict(\n        nms_across_levels=False,\n        nms_pre=1000,\n        nms_post=1000,\n        max_num=1000,\n        nms_thr=0.7,\n        min_bbox_size=0),\n    rcnn=dict(\n        score_thr=0.05,\n        nms=dict(type='nms', iou_thr=0.5),\n        max_per_img=100,\n        mask_thr_binary=0.5))\n# dataset settings\ndataset_type = 'CocoDataset'\ndata_root = 'data/coco/'\nimg_norm_cfg = dict(\n    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=2,\n    workers_per_gpu=2,\n    train=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_train2017.json',\n        img_prefix=data_root + 'train2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0.5,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=True,\n        with_crowd=True,\n        with_label=True),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + 'annotations/instances_val2017.json',\n        img_prefix=data_root + 'val2017/',\n        img_scale=(1333, 800),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=32,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True))\n# optimizer\noptimizer = dict(\n    type='SGD',\n    lr=0.02,\n    momentum=0.9,\n    weight_decay=0.0001,\n    paramwise_options=dict(norm_decay_mult=0))\noptimizer_config = dict(grad_clip=None)\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.1,\n    step=[65, 71])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        # dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 73\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/scratch_mask_rcnn_r50_fpn_gn_6x'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
configs/wider_face/ssd300_wider_face.py,0,"b""# model settings\ninput_size = 300\nmodel = dict(\n    type='SingleStageDetector',\n    pretrained='open-mmlab://vgg16_caffe',\n    backbone=dict(\n        type='SSDVGG',\n        input_size=input_size,\n        depth=16,\n        with_last_pool=False,\n        ceil_mode=True,\n        out_indices=(3, 4),\n        out_feature_indices=(22, 34),\n        l2_norm_scale=20),\n    neck=None,\n    bbox_head=dict(\n        type='SSDHead',\n        input_size=input_size,\n        in_channels=(512, 1024, 512, 256, 256, 256),\n        num_classes=2,\n        anchor_strides=(8, 16, 32, 64, 100, 300),\n        basesize_ratio_range=(0.15, 0.9),\n        anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n        target_means=(.0, .0, .0, .0),\n        target_stds=(0.1, 0.1, 0.2, 0.2)))\ncudnn_benchmark = True\ntrain_cfg = dict(\n    assigner=dict(\n        type='MaxIoUAssigner',\n        pos_iou_thr=0.5,\n        neg_iou_thr=0.5,\n        min_pos_iou=0.,\n        ignore_iof_thr=-1,\n        gt_max_assign_all=False),\n    smoothl1_beta=1.,\n    allowed_border=-1,\n    pos_weight=-1,\n    neg_pos_ratio=3,\n    debug=False)\ntest_cfg = dict(\n    nms=dict(type='nms', iou_thr=0.45),\n    min_bbox_size=0,\n    score_thr=0.02,\n    max_per_img=200)\n# model training and testing settings\n# dataset settings\ndataset_type = 'WIDERFaceDataset'\ndata_root = 'data/WIDERFace/'\nimg_norm_cfg = dict(mean=[123.675, 116.28, 103.53], std=[1, 1, 1], to_rgb=True)\ndata = dict(\n    imgs_per_gpu=60,\n    workers_per_gpu=2,\n    train=dict(\n        type='RepeatDataset',\n        times=2,\n        dataset=dict(\n            type=dataset_type,\n            ann_file=[\n                data_root + 'train.txt',\n            ],\n            img_prefix=[data_root + 'WIDER_train/'],\n            img_scale=(300, 300),\n            min_size=17,  # throw away very small faces to improve training,\n            # because 300x300 is too low resolution to detect them\n            img_norm_cfg=img_norm_cfg,\n            size_divisor=None,\n            flip_ratio=0.5,\n            with_mask=False,\n            with_crowd=False,\n            with_label=True,\n            test_mode=False,\n            extra_aug=dict(\n                photo_metric_distortion=dict(\n                    brightness_delta=32,\n                    contrast_range=(0.5, 1.5),\n                    saturation_range=(0.5, 1.5),\n                    hue_delta=18),\n                expand=dict(\n                    mean=img_norm_cfg['mean'],\n                    to_rgb=img_norm_cfg['to_rgb'],\n                    ratio_range=(1, 4)),\n                random_crop=dict(\n                    min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3)),\n            resize_keep_ratio=False)),\n    val=dict(\n        type=dataset_type,\n        ann_file=data_root + '/val.txt',\n        img_prefix=data_root + 'WIDER_val/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False),\n    test=dict(\n        type=dataset_type,\n        ann_file=data_root + '/val.txt',\n        img_prefix=data_root + 'WIDER_val/',\n        img_scale=(300, 300),\n        img_norm_cfg=img_norm_cfg,\n        size_divisor=None,\n        flip_ratio=0,\n        with_mask=False,\n        with_label=False,\n        test_mode=True,\n        resize_keep_ratio=False))\n# optimizer\noptimizer = dict(type='SGD', lr=1e-3, momentum=0.9, weight_decay=5e-4)\noptimizer_config = dict()\n# learning policy\nlr_config = dict(\n    policy='step',\n    warmup='linear',\n    warmup_iters=1000,\n    warmup_ratio=1.0 / 3,\n    step=[16, 20])\ncheckpoint_config = dict(interval=1)\n# yapf:disable\nlog_config = dict(\n    interval=1,\n    hooks=[\n        dict(type='TextLoggerHook'),\n        dict(type='TensorboardLoggerHook')\n    ])\n# yapf:enable\n# runtime settings\ntotal_epochs = 24\ndist_params = dict(backend='nccl')\nlog_level = 'INFO'\nwork_dir = './work_dirs/ssd300_wider'\nload_from = None\nresume_from = None\nworkflow = [('train', 1)]\n"""
mmdet/apis/__init__.py,0,"b""from .env import init_dist, get_root_logger, set_random_seed\nfrom .train import train_detector\nfrom .inference import init_detector, inference_detector, show_result\n\n__all__ = [\n    'init_dist', 'get_root_logger', 'set_random_seed', 'train_detector',\n    'init_detector', 'inference_detector', 'show_result'\n]\n"""
mmdet/apis/env.py,8,"b""import logging\nimport os\nimport random\nimport subprocess\n\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom mmcv.runner import get_dist_info\n\n\ndef init_dist(launcher, backend='nccl', **kwargs):\n    if mp.get_start_method(allow_none=True) is None:\n        mp.set_start_method('spawn')\n    if launcher == 'pytorch':\n        _init_dist_pytorch(backend, **kwargs)\n    elif launcher == 'mpi':\n        _init_dist_mpi(backend, **kwargs)\n    elif launcher == 'slurm':\n        _init_dist_slurm(backend, **kwargs)\n    else:\n        raise ValueError('Invalid launcher type: {}'.format(launcher))\n\n\ndef _init_dist_pytorch(backend, **kwargs):\n    # TODO: use local_rank instead of rank % num_gpus\n    rank = int(os.environ['RANK'])\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(rank % num_gpus)\n    dist.init_process_group(backend=backend, **kwargs)\n\n\ndef _init_dist_mpi(backend, **kwargs):\n    raise NotImplementedError\n\n\ndef _init_dist_slurm(backend, port=29500, **kwargs):\n    proc_id = int(os.environ['SLURM_PROCID'])\n    ntasks = int(os.environ['SLURM_NTASKS'])\n    node_list = os.environ['SLURM_NODELIST']\n    num_gpus = torch.cuda.device_count()\n    torch.cuda.set_device(proc_id % num_gpus)\n    addr = subprocess.getoutput(\n        'scontrol show hostname {} | head -n1'.format(node_list))\n    os.environ['MASTER_PORT'] = str(port)\n    os.environ['MASTER_ADDR'] = addr\n    os.environ['WORLD_SIZE'] = str(ntasks)\n    os.environ['RANK'] = str(proc_id)\n    dist.init_process_group(backend=backend)\n\n\ndef set_random_seed(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef get_root_logger(log_level=logging.INFO):\n    logger = logging.getLogger()\n    if not logger.hasHandlers():\n        logging.basicConfig(\n            format='%(asctime)s - %(levelname)s - %(message)s',\n            level=log_level)\n    rank, _ = get_dist_info()\n    if rank != 0:\n        logger.setLevel('ERROR')\n    return logger\n"""
mmdet/apis/inference.py,1,"b'import warnings\n\nimport mmcv\nimport numpy as np\nimport pycocotools.mask as maskUtils\nimport torch\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.core import get_classes\nfrom mmdet.datasets import to_tensor\nfrom mmdet.datasets.transforms import ImageTransform\nfrom mmdet.models import build_detector\n\n\ndef init_detector(config, checkpoint=None, device=\'cuda:0\'):\n    """"""Initialize a detector from config file.\n\n    Args:\n        config (str or :obj:`mmcv.Config`): Config file path or the config\n            object.\n        checkpoint (str, optional): Checkpoint path. If left as None, the model\n            will not load any weights.\n\n    Returns:\n        nn.Module: The constructed detector.\n    """"""\n    if isinstance(config, str):\n        config = mmcv.Config.fromfile(config)\n    elif not isinstance(config, mmcv.Config):\n        raise TypeError(\'config must be a filename or Config object, \'\n                        \'but got {}\'.format(type(config)))\n    config.model.pretrained = None\n    model = build_detector(config.model, test_cfg=config.test_cfg)\n    if checkpoint is not None:\n        checkpoint = load_checkpoint(model, checkpoint)\n        if \'CLASSES\' in checkpoint[\'meta\']:\n            model.CLASSES = checkpoint[\'meta\'][\'CLASSES\']\n        else:\n            warnings.warn(\'Class names are not saved in the checkpoint\\\'s \'\n                          \'meta data, use COCO classes by default.\')\n            model.CLASSES = get_classes(\'coco\')\n    model.cfg = config  # save the config in the model for convenience\n    model.to(device)\n    model.eval()\n    return model\n\n\ndef inference_detector(model, imgs):\n    """"""Inference image(s) with the detector.\n\n    Args:\n        model (nn.Module): The loaded detector.\n        imgs (str/ndarray or list[str/ndarray]): Either image files or loaded\n            images.\n\n    Returns:\n        If imgs is a str, a generator will be returned, otherwise return the\n        detection results directly.\n    """"""\n    cfg = model.cfg\n    img_transform = ImageTransform(\n        size_divisor=cfg.data.test.size_divisor, **cfg.img_norm_cfg)\n\n    device = next(model.parameters()).device  # model device\n    if not isinstance(imgs, list):\n        return _inference_single(model, imgs, img_transform, device)\n    else:\n        return _inference_generator(model, imgs, img_transform, device)\n\n\ndef _prepare_data(img, img_transform, cfg, device):\n    ori_shape = img.shape\n    img, img_shape, pad_shape, scale_factor = img_transform(\n        img,\n        scale=cfg.data.test.img_scale,\n        keep_ratio=cfg.data.test.get(\'resize_keep_ratio\', True))\n    img = to_tensor(img).to(device).unsqueeze(0)\n    img_meta = [\n        dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            flip=False)\n    ]\n    return dict(img=[img], img_meta=[img_meta])\n\n\ndef _inference_single(model, img, img_transform, device):\n    img = mmcv.imread(img)\n    data = _prepare_data(img, img_transform, model.cfg, device)\n    with torch.no_grad():\n        result = model(return_loss=False, rescale=True, **data)\n    return result\n\n\ndef _inference_generator(model, imgs, img_transform, device):\n    for img in imgs:\n        yield _inference_single(model, img, img_transform, device)\n\n\n# TODO: merge this method with the one in BaseDetector\ndef show_result(img, result, class_names, score_thr=0.3, out_file=None):\n    """"""Visualize the detection results on the image.\n\n    Args:\n        img (str or np.ndarray): Image filename or loaded image.\n        result (tuple[list] or list): The detection result, can be either\n            (bbox, segm) or just bbox.\n        class_names (list[str] or tuple[str]): A list of class names.\n        score_thr (float): The threshold to visualize the bboxes and masks.\n        out_file (str, optional): If specified, the visualization result will\n            be written to the out file instead of shown in a window.\n    """"""\n    assert isinstance(class_names, (tuple, list))\n    img = mmcv.imread(img)\n    if isinstance(result, tuple):\n        bbox_result, segm_result = result\n    else:\n        bbox_result, segm_result = result, None\n    bboxes = np.vstack(bbox_result)\n    # draw segmentation masks\n    if segm_result is not None:\n        segms = mmcv.concat_list(segm_result)\n        inds = np.where(bboxes[:, -1] > score_thr)[0]\n        for i in inds:\n            color_mask = np.random.randint(0, 256, (1, 3), dtype=np.uint8)\n            mask = maskUtils.decode(segms[i]).astype(np.bool)\n            img[mask] = img[mask] * 0.5 + color_mask * 0.5\n    # draw bounding boxes\n    labels = [\n        np.full(bbox.shape[0], i, dtype=np.int32)\n        for i, bbox in enumerate(bbox_result)\n    ]\n    labels = np.concatenate(labels)\n    mmcv.imshow_det_bboxes(\n        img.copy(),\n        bboxes,\n        labels,\n        class_names=class_names,\n        score_thr=score_thr,\n        show=out_file is None,\n        out_file=out_file)\n'"
mmdet/apis/train.py,4,"b'from __future__ import division\n\nimport re\nfrom collections import OrderedDict\n\nimport torch\nfrom mmcv.runner import Runner, DistSamplerSeedHook, obj_from_dict\nfrom mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n\nfrom mmdet import datasets\nfrom mmdet.core import (DistOptimizerHook, DistEvalmAPHook,\n                        CocoDistEvalRecallHook, CocoDistEvalmAPHook,\n                        Fp16OptimizerHook)\nfrom mmdet.datasets import build_dataloader\nfrom mmdet.models import RPN\nfrom .env import get_root_logger\n\nimport ipdb\n\n\n# \xe4\xbe\x8b\xe5\xad\x90\xe5\x8f\x82\xe8\xa7\x81\xe6\xb3\xa8\xe9\x87\x8a\xe6\x96\x87\xe6\xa1\xa3\ndef parse_losses(losses):\n    log_vars = OrderedDict()\n    for loss_name, loss_value in losses.items():\n        if isinstance(loss_value, torch.Tensor):\n            log_vars[loss_name] = loss_value.mean()\n        # \xe5\xa4\x9a\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe8\xbe\x93\xe5\x87\xba\xe6\x97\xb6\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaaloss\xe7\x9a\x84\xe5\x80\xbc\xe6\x9c\x89\xe5\xa4\x9a\xe4\xb8\xaa\xe7\x94\xa8list\xe8\xa1\xa8\xe7\xa4\xba\n        elif isinstance(loss_value, list):\n            log_vars[loss_name] = sum(_loss.mean() for _loss in loss_value)\n        else:\n            raise TypeError(\n                \'{} is not a tensor or list of tensors\'.format(loss_name))\n    loss = sum(_value for _key, _value in log_vars.items() if \'loss\' in _key)\n\n    log_vars[\'loss\'] = loss\n    for name in log_vars:\n        log_vars[name] = log_vars[name].item()\n\n    return loss, log_vars\n\ndef batch_processor(model, data, train_mode):\n    losses = model(**data)  \n    loss, log_vars = parse_losses(losses)\n\n    outputs = dict(\n        loss=loss, log_vars=log_vars, num_samples=len(data[\'img\'].data))\n\n    return outputs\n\n# \xe8\xae\xad\xe7\xbb\x83\xe6\x8e\xa5\xe5\x8f\xa3\ndef train_detector(model,\n                   dataset,\n                   cfg,\n                   distributed=False,\n                   validate=False,\n                   logger=None):\n    if logger is None:\n        logger = get_root_logger(cfg.log_level)\n\n    # start training\n    if distributed:\n        _dist_train(model, dataset, cfg, validate=validate)\n    else:\n        _non_dist_train(model, dataset, cfg, validate=validate)\n\n\ndef build_optimizer(model, optimizer_cfg):\n    """"""Build optimizer from configs.\n\n    Args:\n        model (:obj:`nn.Module`): The model with parameters to be optimized.\n        optimizer_cfg (dict): The config dict of the optimizer.\n            Positional fields are:\n                - type: class name of the optimizer.\n                - lr: base learning rate.\n            Optional fields are:\n                - any arguments of the corresponding optimizer type, e.g.,\n                  weight_decay, momentum, etc.\n                - paramwise_options: a dict with 3 accepted fileds\n                  (bias_lr_mult, bias_decay_mult, norm_decay_mult).\n                  `bias_lr_mult` and `bias_decay_mult` will be multiplied to\n                  the lr and weight decay respectively for all bias parameters\n                  (except for the normalization layers), and\n                  `norm_decay_mult` will be multiplied to the weight decay\n                  for all weight and bias parameters of normalization layers.\n\n    Returns:\n        torch.optim.Optimizer: The initialized optimizer.\n    """"""\n    if hasattr(model, \'module\'):\n        model = model.module\n\n    optimizer_cfg = optimizer_cfg.copy()\n    paramwise_options = optimizer_cfg.pop(\'paramwise_options\', None)\n    # if no paramwise option is specified, just use the global setting\n    if paramwise_options is None:\n        return obj_from_dict(optimizer_cfg, torch.optim,\n                             dict(params=model.parameters()))\n    else:\n        assert isinstance(paramwise_options, dict)\n        # get base lr and weight decay\n        base_lr = optimizer_cfg[\'lr\']\n        base_wd = optimizer_cfg.get(\'weight_decay\', None)\n        # weight_decay must be explicitly specified if mult is specified\n        if (\'bias_decay_mult\' in paramwise_options\n                or \'norm_decay_mult\' in paramwise_options):\n            assert base_wd is not None\n        # get param-wise options\n        bias_lr_mult = paramwise_options.get(\'bias_lr_mult\', 1.)\n        bias_decay_mult = paramwise_options.get(\'bias_decay_mult\', 1.)\n        norm_decay_mult = paramwise_options.get(\'norm_decay_mult\', 1.)\n        # set param-wise lr and weight decay\n        params = []\n        for name, param in model.named_parameters():\n            param_group = {\'params\': [param]}\n            if not param.requires_grad:\n                # FP16 training needs to copy gradient/weight between master\n                # weight copy and model weight, it is convenient to keep all\n                # parameters here to align with model.parameters()\n                params.append(param_group)\n                continue\n\n            # for norm layers, overwrite the weight decay of weight and bias\n            # TODO: obtain the norm layer prefixes dynamically\n            if re.search(r\'(bn|gn)(\\d+)?.(weight|bias)\', name):\n                if base_wd is not None:\n                    param_group[\'weight_decay\'] = base_wd * norm_decay_mult\n            # for other layers, overwrite both lr and weight decay of bias\n            elif name.endswith(\'.bias\'):\n                param_group[\'lr\'] = base_lr * bias_lr_mult\n                if base_wd is not None:\n                    param_group[\'weight_decay\'] = base_wd * bias_decay_mult\n            # otherwise use the global settings\n\n            params.append(param_group)\n\n        optimizer_cls = getattr(torch.optim, optimizer_cfg.pop(\'type\'))\n        return optimizer_cls(params, **optimizer_cfg)\n\n\ndef _dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.imgs_per_gpu,\n            cfg.data.workers_per_gpu,\n            dist=True)\n    ]\n    # put model on gpus\n    model = MMDistributedDataParallel(model.cuda())\n\n    # build runner\n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model, batch_processor, optimizer, cfg.work_dir,\n                    cfg.log_level)\n\n    # fp16 setting\n    fp16_cfg = cfg.get(\'fp16\', None)\n    if fp16_cfg is not None:\n        optimizer_config = Fp16OptimizerHook(**cfg.optimizer_config,\n                                             **fp16_cfg)\n    else:\n        optimizer_config = DistOptimizerHook(**cfg.optimizer_config)\n\n    # register hooks\n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n    runner.register_hook(DistSamplerSeedHook())\n    # register eval hooks\n    if validate:\n        val_dataset_cfg = cfg.data.val\n        eval_cfg = cfg.get(\'evaluation\', {})\n        if isinstance(model.module, RPN):\n            # TODO: implement recall hooks for other datasets\n            runner.register_hook(\n                CocoDistEvalRecallHook(val_dataset_cfg, **eval_cfg))\n        else:\n            dataset_type = getattr(datasets, val_dataset_cfg.type)\n            if issubclass(dataset_type, datasets.CocoDataset):\n                runner.register_hook(\n                    CocoDistEvalmAPHook(val_dataset_cfg, **eval_cfg))\n            else:\n                runner.register_hook(\n                    DistEvalmAPHook(val_dataset_cfg, **eval_cfg))\n\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n\n\ndef _non_dist_train(model, dataset, cfg, validate=False):\n    # prepare data loaders\n    # \xe8\xbf\x94\xe5\x9b\x9edataloader\xe7\x9a\x84\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8pytorch\xe7\x9a\x84DataLoader\xe6\x96\xb9\xe6\xb3\x95\xe5\xb0\x81\xe8\xa3\x85\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\n    data_loaders = [\n        build_dataloader(\n            dataset,\n            cfg.data.imgs_per_gpu,\n            cfg.data.workers_per_gpu,\n            cfg.gpus,\n            dist=False)\n    ]\n    # put model on gpus \xe8\xbf\x99\xe9\x87\x8c\xe5\xa4\x9aGPU\xe8\xbe\x93\xe5\x85\xa5\xe6\xb2\xa1\xe7\x94\xa8list\xe8\x80\x8c\xe6\x98\xaf\xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8f\xe5\x8d\x95GPU\xe6\x98\xafrange(0,1),\xe9\x81\x8d\xe5\x8e\x86\xe7\x9a\x84\xe6\x97\xb6\xe5\x80\x99\xe5\x8f\xaa\xe6\x9c\x890\n    model = MMDataParallel(model, device_ids=range(cfg.gpus)).cuda()\n\n    # build runner \n    optimizer = build_optimizer(model, cfg.optimizer)\n    runner = Runner(model, batch_processor, optimizer, cfg.work_dir,\n                    cfg.log_level)\n    # fp16 setting\n    fp16_cfg = cfg.get(\'fp16\', None)\n    if fp16_cfg is not None:\n        optimizer_config = Fp16OptimizerHook(\n            **cfg.optimizer_config, **fp16_cfg, distributed=False)\n    else:\n        optimizer_config = cfg.optimizer_config\n\n    # \xe6\xb3\xa8\xe5\x86\x8c\xe9\x92\xa9\xe5\xad\x90    \n    runner.register_training_hooks(cfg.lr_config, optimizer_config,\n                                   cfg.checkpoint_config, cfg.log_config)\n    # \xe6\x96\xad\xe7\x82\xb9\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x88\x96\xe6\x96\x87\xe4\xbb\xb6\xe5\x8a\xa0\xe8\xbd\xbd\xe6\x95\xb0\xe6\x8d\xae\n    if cfg.resume_from:\n        runner.resume(cfg.resume_from)\n    elif cfg.load_from:\n        runner.load_checkpoint(cfg.load_from)\n    runner.run(data_loaders, cfg.workflow, cfg.total_epochs)\n'"
mmdet/core/__init__.py,0,"b'from .anchor import *  # noqa: F401, F403\nfrom .bbox import *  # noqa: F401, F403\nfrom .evaluation import *  # noqa: F401, F403\nfrom .fp16 import *  # noqa: F401, F403\nfrom .mask import *  # noqa: F401, F403\nfrom .post_processing import *  # noqa: F401, F403\nfrom .utils import *  # noqa: F401, F403\n'"
mmdet/datasets/__init__.py,0,"b""from .custom import CustomDataset\nfrom .xml_style import XMLDataset\nfrom .coco import CocoDataset\nfrom .voc import VOCDataset\nfrom .wider_face import WIDERFaceDataset\nfrom .my_dataset import MyDataset\nfrom .loader import GroupSampler, DistributedGroupSampler, build_dataloader\nfrom .utils import to_tensor, random_scale, show_ann\nfrom .dataset_wrappers import ConcatDataset, RepeatDataset\nfrom .extra_aug import ExtraAugmentation\nfrom .registry import DATASETS\nfrom .builder import build_dataset\n\n__all__ = [\n    'CustomDataset', 'XMLDataset', 'CocoDataset', 'VOCDataset', 'GroupSampler',\n    'DistributedGroupSampler', 'build_dataloader', 'to_tensor', 'random_scale',\n    'show_ann', 'ConcatDataset', 'RepeatDataset', 'ExtraAugmentation',\n    'WIDERFaceDataset', 'DATASETS', 'build_dataset','MyDataset'\n]\n"""
mmdet/datasets/builder.py,0,"b""import copy\n\nfrom mmdet.utils import build_from_cfg\nfrom .dataset_wrappers import ConcatDataset, RepeatDataset\nfrom .registry import DATASETS\n\nimport ipdb\n\n\n\ndef _concat_dataset(cfg):\n    ann_files = cfg['ann_file']\n    img_prefixes = cfg.get('img_prefix', None)\n    seg_prefixes = cfg.get('seg_prefixes', None)\n    proposal_files = cfg.get('proposal_file', None)\n\n    datasets = []\n    num_dset = len(ann_files)\n    for i in range(num_dset):\n        data_cfg = copy.deepcopy(cfg)\n        data_cfg['ann_file'] = ann_files[i]\n        if isinstance(img_prefixes, (list, tuple)):\n            data_cfg['img_prefix'] = img_prefixes[i]\n        if isinstance(seg_prefixes, (list, tuple)):\n            data_cfg['seg_prefix'] = seg_prefixes[i]\n        if isinstance(proposal_files, (list, tuple)):\n            data_cfg['proposal_file'] = proposal_files[i]\n        datasets.append(build_dataset(data_cfg))\n\n    return ConcatDataset(datasets)\n\n\ndef build_dataset(cfg):\n    if cfg['type'] == 'RepeatDataset':\n        dataset = RepeatDataset(build_dataset(cfg['dataset']), cfg['times'])\n    elif isinstance(cfg['ann_file'], (list, tuple)):\n        dataset = _concat_dataset(cfg)\n    else:\n        dataset = build_from_cfg(cfg, DATASETS)\n        # ipdb.set_trace(context=35)\n\n    return dataset\n"""
mmdet/datasets/coco.py,0,"b'import numpy as np\nfrom pycocotools.coco import COCO\n\nfrom .custom import CustomDataset\nfrom .registry import DATASETS\n\n\n@DATASETS.register_module\nclass CocoDataset(CustomDataset):\n\n    CLASSES = (\'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\',\n               \'train\', \'truck\', \'boat\', \'traffic_light\', \'fire_hydrant\',\n               \'stop_sign\', \'parking_meter\', \'bench\', \'bird\', \'cat\', \'dog\',\n               \'horse\', \'sheep\', \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\',\n               \'backpack\', \'umbrella\', \'handbag\', \'tie\', \'suitcase\', \'frisbee\',\n               \'skis\', \'snowboard\', \'sports_ball\', \'kite\', \'baseball_bat\',\n               \'baseball_glove\', \'skateboard\', \'surfboard\', \'tennis_racket\',\n               \'bottle\', \'wine_glass\', \'cup\', \'fork\', \'knife\', \'spoon\', \'bowl\',\n               \'banana\', \'apple\', \'sandwich\', \'orange\', \'broccoli\', \'carrot\',\n               \'hot_dog\', \'pizza\', \'donut\', \'cake\', \'chair\', \'couch\',\n               \'potted_plant\', \'bed\', \'dining_table\', \'toilet\', \'tv\', \'laptop\',\n               \'mouse\', \'remote\', \'keyboard\', \'cell_phone\', \'microwave\',\n               \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\',\n               \'vase\', \'scissors\', \'teddy_bear\', \'hair_drier\', \'toothbrush\')\n\n    def load_annotations(self, ann_file):\n        self.coco = COCO(ann_file)\n        self.cat_ids = self.coco.getCatIds()\n        self.cat2label = {\n            cat_id: i + 1\n            for i, cat_id in enumerate(self.cat_ids)\n        }\n        self.img_ids = self.coco.getImgIds()\n        img_infos = []\n        for i in self.img_ids:\n            info = self.coco.loadImgs([i])[0]\n            info[\'filename\'] = info[\'file_name\']\n            img_infos.append(info)\n        return img_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.img_infos[idx][\'id\']\n        ann_ids = self.coco.getAnnIds(imgIds=[img_id])\n        ann_info = self.coco.loadAnns(ann_ids)\n        return self._parse_ann_info(ann_info, self.with_mask)\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small or without ground truths.""""""\n        valid_inds = []\n        ids_with_ann = set(_[\'image_id\'] for _ in self.coco.anns.values())\n        for i, img_info in enumerate(self.img_infos):\n            if self.img_ids[i] not in ids_with_ann:\n                continue\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _parse_ann_info(self, ann_info, with_mask=True):\n        """"""Parse bbox and mask annotation.\n\n        Args:\n            ann_info (list[dict]): Annotation info of an image.\n            with_mask (bool): Whether to parse mask annotations.\n\n        Returns:\n            dict: A dict containing the following keys: bboxes, bboxes_ignore,\n                labels, masks, mask_polys, poly_lens.\n        """"""\n        gt_bboxes = []\n        gt_labels = []\n        gt_bboxes_ignore = []\n        # Two formats are provided.\n        # 1. mask: a binary map of the same size of the image.\n        # 2. polys: each mask consists of one or several polys, each poly is a\n        # list of float.\n        if with_mask:\n            gt_masks = []\n            gt_mask_polys = []\n            gt_poly_lens = []\n        for i, ann in enumerate(ann_info):\n            if ann.get(\'ignore\', False):\n                continue\n            x1, y1, w, h = ann[\'bbox\']\n            if ann[\'area\'] <= 0 or w < 1 or h < 1:\n                continue\n            bbox = [x1, y1, x1 + w - 1, y1 + h - 1]\n            if ann[\'iscrowd\']:\n                gt_bboxes_ignore.append(bbox)\n            else:\n                gt_bboxes.append(bbox)\n                gt_labels.append(self.cat2label[ann[\'category_id\']])\n            if with_mask:\n                gt_masks.append(self.coco.annToMask(ann))\n                mask_polys = [\n                    p for p in ann[\'segmentation\'] if len(p) >= 6\n                ]  # valid polygons have >= 3 points (6 coordinates)\n                poly_lens = [len(p) for p in mask_polys]\n                gt_mask_polys.append(mask_polys)\n                gt_poly_lens.extend(poly_lens)\n        if gt_bboxes:\n            gt_bboxes = np.array(gt_bboxes, dtype=np.float32)\n            gt_labels = np.array(gt_labels, dtype=np.int64)\n        else:\n            gt_bboxes = np.zeros((0, 4), dtype=np.float32)\n            gt_labels = np.array([], dtype=np.int64)\n\n        if gt_bboxes_ignore:\n            gt_bboxes_ignore = np.array(gt_bboxes_ignore, dtype=np.float32)\n        else:\n            gt_bboxes_ignore = np.zeros((0, 4), dtype=np.float32)\n\n        ann = dict(\n            bboxes=gt_bboxes, labels=gt_labels, bboxes_ignore=gt_bboxes_ignore)\n\n        if with_mask:\n            ann[\'masks\'] = gt_masks\n            # poly format is not used in the current implementation\n            ann[\'mask_polys\'] = gt_mask_polys\n            ann[\'poly_lens\'] = gt_poly_lens\n        return ann\n'"
mmdet/datasets/custom.py,1,"b'import os.path as osp\n\nimport mmcv\nimport numpy as np\nfrom mmcv.parallel import DataContainer as DC\nfrom torch.utils.data import Dataset\n\nfrom .registry import DATASETS\nfrom .transforms import (ImageTransform, BboxTransform, MaskTransform,\n                         SegMapTransform, Numpy2Tensor)\nfrom .utils import to_tensor, random_scale\nfrom .extra_aug import ExtraAugmentation\n\n\n@DATASETS.register_module\nclass CustomDataset(Dataset):\n    """"""Custom dataset for detection.\n\n    Annotation format:\n    [\n        {\n            \'filename\': \'a.jpg\',\n            \'width\': 1280,\n            \'height\': 720,\n            \'ann\': {\n                \'bboxes\': <np.ndarray> (n, 4),\n                \'labels\': <np.ndarray> (n, ),\n                \'bboxes_ignore\': <np.ndarray> (k, 4),\n                \'labels_ignore\': <np.ndarray> (k, 4) (optional field)\n            }\n        },\n        ...\n    ]\n\n    The `ann` field is optional for testing.\n    """"""\n\n    CLASSES = None\n\n    def __init__(self,\n                 ann_file,\n                 img_prefix,\n                 img_scale,\n                 img_norm_cfg,\n                 multiscale_mode=\'value\',\n                 size_divisor=None,\n                 proposal_file=None,\n                 num_max_proposals=1000,\n                 flip_ratio=0,\n                 with_mask=True,\n                 with_crowd=True,\n                 with_label=True,\n                 with_semantic_seg=False,\n                 seg_prefix=None,\n                 seg_scale_factor=1,\n                 extra_aug=None,\n                 resize_keep_ratio=True,\n                 test_mode=False):\n        # prefix of images path\n        self.img_prefix = img_prefix\n\n        # load annotations (and proposals)\n        self.img_infos = self.load_annotations(ann_file)\n        if proposal_file is not None:\n            self.proposals = self.load_proposals(proposal_file)\n        else:\n            self.proposals = None\n        # filter images with no annotation during training\n        if not test_mode:\n            valid_inds = self._filter_imgs()\n            self.img_infos = [self.img_infos[i] for i in valid_inds]\n            if self.proposals is not None:\n                self.proposals = [self.proposals[i] for i in valid_inds]\n\n        # (long_edge, short_edge) or [(long1, short1), (long2, short2), ...]\n        self.img_scales = img_scale if isinstance(img_scale,\n                                                  list) else [img_scale]\n        assert mmcv.is_list_of(self.img_scales, tuple)\n        # normalization configs\n        self.img_norm_cfg = img_norm_cfg\n\n        # multi-scale mode (only applicable for multi-scale training)\n        self.multiscale_mode = multiscale_mode\n        assert multiscale_mode in [\'value\', \'range\']\n\n        # max proposals per image\n        self.num_max_proposals = num_max_proposals\n        # flip ratio\n        self.flip_ratio = flip_ratio\n        assert flip_ratio >= 0 and flip_ratio <= 1\n        # padding border to ensure the image size can be divided by\n        # size_divisor (used for FPN)\n        self.size_divisor = size_divisor\n\n        # with mask or not (reserved field, takes no effect)\n        self.with_mask = with_mask\n        # some datasets provide bbox annotations as ignore/crowd/difficult,\n        # if `with_crowd` is True, then these info is returned.\n        self.with_crowd = with_crowd\n        # with label is False for RPN\n        self.with_label = with_label\n        # with semantic segmentation (stuff) annotation or not\n        self.with_seg = with_semantic_seg\n        # prefix of semantic segmentation map path\n        self.seg_prefix = seg_prefix\n        # rescale factor for segmentation maps\n        self.seg_scale_factor = seg_scale_factor\n        # in test mode or not\n        self.test_mode = test_mode\n\n        # set group flag for the sampler\n        if not self.test_mode:\n            self._set_group_flag()\n        # transforms\n        self.img_transform = ImageTransform(\n            size_divisor=self.size_divisor, **self.img_norm_cfg)\n        self.bbox_transform = BboxTransform()\n        self.mask_transform = MaskTransform()\n        self.seg_transform = SegMapTransform(self.size_divisor)\n        self.numpy2tensor = Numpy2Tensor()\n\n        # if use extra augmentation\n        if extra_aug is not None:\n            self.extra_aug = ExtraAugmentation(**extra_aug)\n        else:\n            self.extra_aug = None\n\n        # image rescale if keep ratio\n        self.resize_keep_ratio = resize_keep_ratio\n\n    def __len__(self):\n        return len(self.img_infos)\n\n    def load_annotations(self, ann_file):\n        return mmcv.load(ann_file)\n\n    def load_proposals(self, proposal_file):\n        return mmcv.load(proposal_file)\n\n    def get_ann_info(self, idx):\n        return self.img_infos[idx][\'ann\']\n\n    def _filter_imgs(self, min_size=32):\n        """"""Filter images too small.""""""\n        valid_inds = []\n        for i, img_info in enumerate(self.img_infos):\n            if min(img_info[\'width\'], img_info[\'height\']) >= min_size:\n                valid_inds.append(i)\n        return valid_inds\n\n    def _set_group_flag(self):\n        """"""Set flag according to image aspect ratio.\n\n        Images with aspect ratio greater than 1 will be set as group 1,\n        otherwise group 0.\n        """"""\n        self.flag = np.zeros(len(self), dtype=np.uint8)\n        for i in range(len(self)):\n            img_info = self.img_infos[i]\n            if img_info[\'width\'] / img_info[\'height\'] > 1:\n                self.flag[i] = 1\n\n    def _rand_another(self, idx):\n        pool = np.where(self.flag == self.flag[idx])[0]\n        return np.random.choice(pool)\n\n    def __getitem__(self, idx):\n        if self.test_mode:\n            return self.prepare_test_img(idx)\n        while True:\n            data = self.prepare_train_img(idx)\n            if data is None:\n                idx = self._rand_another(idx)\n                continue\n            return data\n\n    def prepare_train_img(self, idx):\n        img_info = self.img_infos[idx]\n        # load image\n        img = mmcv.imread(osp.join(self.img_prefix, img_info[\'filename\']))\n        # load proposals if necessary\n        if self.proposals is not None:\n            proposals = self.proposals[idx][:self.num_max_proposals]\n            # TODO: Handle empty proposals properly. Currently images with\n            # no proposals are just ignored, but they can be used for\n            # training in concept.\n            if len(proposals) == 0:\n                return None\n            if not (proposals.shape[1] == 4 or proposals.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n, 5), \'\n                    \'but found {}\'.format(proposals.shape))\n            if proposals.shape[1] == 5:\n                scores = proposals[:, 4, None]\n                proposals = proposals[:, :4]\n            else:\n                scores = None\n\n        ann = self.get_ann_info(idx)\n        gt_bboxes = ann[\'bboxes\']\n        gt_labels = ann[\'labels\']\n        if self.with_crowd:\n            gt_bboxes_ignore = ann[\'bboxes_ignore\']\n\n        # skip the image if there is no valid gt bbox\n        if len(gt_bboxes) == 0:\n            return None\n\n        # extra augmentation\n        if self.extra_aug is not None:\n            img, gt_bboxes, gt_labels = self.extra_aug(img, gt_bboxes,\n                                                       gt_labels)\n\n        # apply transforms\n        flip = True if np.random.rand() < self.flip_ratio else False\n        # randomly sample a scale\n        img_scale = random_scale(self.img_scales, self.multiscale_mode)\n        img, img_shape, pad_shape, scale_factor = self.img_transform(\n            img, img_scale, flip, keep_ratio=self.resize_keep_ratio)\n        img = img.copy()\n        if self.with_seg:\n            gt_seg = mmcv.imread(\n                osp.join(self.seg_prefix, img_info[\'file_name\'].replace(\n                    \'jpg\', \'png\')),\n                flag=\'unchanged\')\n            gt_seg = self.seg_transform(gt_seg.squeeze(), img_scale, flip)\n            gt_seg = mmcv.imrescale(\n                gt_seg, self.seg_scale_factor, interpolation=\'nearest\')\n            gt_seg = gt_seg[None, ...]\n        if self.proposals is not None:\n            proposals = self.bbox_transform(proposals, img_shape, scale_factor,\n                                            flip)\n            proposals = np.hstack(\n                [proposals, scores]) if scores is not None else proposals\n        gt_bboxes = self.bbox_transform(gt_bboxes, img_shape, scale_factor,\n                                        flip)\n        if self.with_crowd:\n            gt_bboxes_ignore = self.bbox_transform(gt_bboxes_ignore, img_shape,\n                                                   scale_factor, flip)\n        if self.with_mask:\n            gt_masks = self.mask_transform(ann[\'masks\'], pad_shape,\n                                           scale_factor, flip)\n\n        ori_shape = (img_info[\'height\'], img_info[\'width\'], 3)\n        img_meta = dict(\n            ori_shape=ori_shape,\n            img_shape=img_shape,\n            pad_shape=pad_shape,\n            scale_factor=scale_factor,\n            flip=flip)\n\n        data = dict(\n            img=DC(to_tensor(img), stack=True),\n            img_meta=DC(img_meta, cpu_only=True),\n            gt_bboxes=DC(to_tensor(gt_bboxes)))\n        if self.proposals is not None:\n            data[\'proposals\'] = DC(to_tensor(proposals))\n        if self.with_label:\n            data[\'gt_labels\'] = DC(to_tensor(gt_labels))\n        if self.with_crowd:\n            data[\'gt_bboxes_ignore\'] = DC(to_tensor(gt_bboxes_ignore))\n        if self.with_mask:\n            data[\'gt_masks\'] = DC(gt_masks, cpu_only=True)\n        if self.with_seg:\n            data[\'gt_semantic_seg\'] = DC(to_tensor(gt_seg), stack=True)\n        return data\n\n    def prepare_test_img(self, idx):\n        """"""Prepare an image for testing (multi-scale and flipping)""""""\n        img_info = self.img_infos[idx]\n        img = mmcv.imread(osp.join(self.img_prefix, img_info[\'filename\']))\n        if self.proposals is not None:\n            proposal = self.proposals[idx][:self.num_max_proposals]\n            if not (proposal.shape[1] == 4 or proposal.shape[1] == 5):\n                raise AssertionError(\n                    \'proposals should have shapes (n, 4) or (n, 5), \'\n                    \'but found {}\'.format(proposal.shape))\n        else:\n            proposal = None\n\n        def prepare_single(img, scale, flip, proposal=None):\n            _img, img_shape, pad_shape, scale_factor = self.img_transform(\n                img, scale, flip, keep_ratio=self.resize_keep_ratio)\n            _img = to_tensor(_img)\n            _img_meta = dict(\n                ori_shape=(img_info[\'height\'], img_info[\'width\'], 3),\n                img_shape=img_shape,\n                pad_shape=pad_shape,\n                scale_factor=scale_factor,\n                flip=flip)\n            if proposal is not None:\n                if proposal.shape[1] == 5:\n                    score = proposal[:, 4, None]\n                    proposal = proposal[:, :4]\n                else:\n                    score = None\n                _proposal = self.bbox_transform(proposal, img_shape,\n                                                scale_factor, flip)\n                _proposal = np.hstack(\n                    [_proposal, score]) if score is not None else _proposal\n                _proposal = to_tensor(_proposal)\n            else:\n                _proposal = None\n            return _img, _img_meta, _proposal\n\n        imgs = []\n        img_metas = []\n        proposals = []\n        for scale in self.img_scales:\n            _img, _img_meta, _proposal = prepare_single(\n                img, scale, False, proposal)\n            imgs.append(_img)\n            img_metas.append(DC(_img_meta, cpu_only=True))\n            proposals.append(_proposal)\n            if self.flip_ratio > 0:\n                _img, _img_meta, _proposal = prepare_single(\n                    img, scale, True, proposal)\n                imgs.append(_img)\n                img_metas.append(DC(_img_meta, cpu_only=True))\n                proposals.append(_proposal)\n        data = dict(img=imgs, img_meta=img_metas)\n        if self.proposals is not None:\n            data[\'proposals\'] = proposals\n        return data\n'"
mmdet/datasets/dataset_wrappers.py,2,"b'import numpy as np\nfrom torch.utils.data.dataset import ConcatDataset as _ConcatDataset\n\nfrom .registry import DATASETS\n\n\n@DATASETS.register_module\nclass ConcatDataset(_ConcatDataset):\n    """"""A wrapper of concatenated dataset.\n\n    Same as :obj:`torch.utils.data.dataset.ConcatDataset`, but\n    concat the group flag for image aspect ratio.\n\n    Args:\n        datasets (list[:obj:`Dataset`]): A list of datasets.\n    """"""\n\n    def __init__(self, datasets):\n        super(ConcatDataset, self).__init__(datasets)\n        self.CLASSES = datasets[0].CLASSES\n        if hasattr(datasets[0], \'flag\'):\n            flags = []\n            for i in range(0, len(datasets)):\n                flags.append(datasets[i].flag)\n            self.flag = np.concatenate(flags)\n\n\n@DATASETS.register_module\nclass RepeatDataset(object):\n    """"""A wrapper of repeated dataset.\n\n    The length of repeated dataset will be `times` larger than the original\n    dataset. This is useful when the data loading time is long but the dataset\n    is small. Using RepeatDataset can reduce the data loading time between\n    epochs.\n\n    Args:\n        dataset (:obj:`Dataset`): The dataset to be repeated.\n        times (int): Repeat times.\n    """"""\n\n    def __init__(self, dataset, times):\n        self.dataset = dataset\n        self.times = times\n        self.CLASSES = dataset.CLASSES\n        if hasattr(self.dataset, \'flag\'):\n            self.flag = np.tile(self.dataset.flag, times)\n\n        self._ori_len = len(self.dataset)\n\n    def __getitem__(self, idx):\n        return self.dataset[idx % self._ori_len]\n\n    def __len__(self):\n        return self.times * self._ori_len\n'"
mmdet/datasets/extra_aug.py,0,"b'import mmcv\nimport numpy as np\nfrom numpy import random\n\nfrom mmdet.core.evaluation.bbox_overlaps import bbox_overlaps\n\n\nclass PhotoMetricDistortion(object):\n\n    def __init__(self,\n                 brightness_delta=32,\n                 contrast_range=(0.5, 1.5),\n                 saturation_range=(0.5, 1.5),\n                 hue_delta=18):\n        self.brightness_delta = brightness_delta\n        self.contrast_lower, self.contrast_upper = contrast_range\n        self.saturation_lower, self.saturation_upper = saturation_range\n        self.hue_delta = hue_delta\n\n    def __call__(self, img, boxes, labels):\n        # random brightness\n        if random.randint(2):\n            delta = random.uniform(-self.brightness_delta,\n                                   self.brightness_delta)\n            img += delta\n\n        # mode == 0 --> do random contrast first\n        # mode == 1 --> do random contrast last\n        mode = random.randint(2)\n        if mode == 1:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # convert color from BGR to HSV\n        img = mmcv.bgr2hsv(img)\n\n        # random saturation\n        if random.randint(2):\n            img[..., 1] *= random.uniform(self.saturation_lower,\n                                          self.saturation_upper)\n\n        # random hue\n        if random.randint(2):\n            img[..., 0] += random.uniform(-self.hue_delta, self.hue_delta)\n            img[..., 0][img[..., 0] > 360] -= 360\n            img[..., 0][img[..., 0] < 0] += 360\n\n        # convert color from HSV to BGR\n        img = mmcv.hsv2bgr(img)\n\n        # random contrast\n        if mode == 0:\n            if random.randint(2):\n                alpha = random.uniform(self.contrast_lower,\n                                       self.contrast_upper)\n                img *= alpha\n\n        # randomly swap channels\n        if random.randint(2):\n            img = img[..., random.permutation(3)]\n\n        return img, boxes, labels\n\n\nclass Expand(object):\n\n    def __init__(self, mean=(0, 0, 0), to_rgb=True, ratio_range=(1, 4)):\n        if to_rgb:\n            self.mean = mean[::-1]\n        else:\n            self.mean = mean\n        self.min_ratio, self.max_ratio = ratio_range\n\n    def __call__(self, img, boxes, labels):\n        if random.randint(2):\n            return img, boxes, labels\n\n        h, w, c = img.shape\n        ratio = random.uniform(self.min_ratio, self.max_ratio)\n        expand_img = np.full((int(h * ratio), int(w * ratio), c),\n                             self.mean).astype(img.dtype)\n        left = int(random.uniform(0, w * ratio - w))\n        top = int(random.uniform(0, h * ratio - h))\n        expand_img[top:top + h, left:left + w] = img\n        img = expand_img\n        boxes += np.tile((left, top), 2)\n        return img, boxes, labels\n\n\nclass RandomCrop(object):\n\n    def __init__(self, min_ious=(0.1, 0.3, 0.5, 0.7, 0.9), min_crop_size=0.3):\n        # 1: return ori img\n        self.sample_mode = (1, *min_ious, 0)\n        self.min_crop_size = min_crop_size\n\n    def __call__(self, img, boxes, labels):\n        h, w, c = img.shape\n        while True:\n            mode = random.choice(self.sample_mode)\n            if mode == 1:\n                return img, boxes, labels\n\n            min_iou = mode\n            for i in range(50):\n                new_w = random.uniform(self.min_crop_size * w, w)\n                new_h = random.uniform(self.min_crop_size * h, h)\n\n                # h / w in [0.5, 2]\n                if new_h / new_w < 0.5 or new_h / new_w > 2:\n                    continue\n\n                left = random.uniform(w - new_w)\n                top = random.uniform(h - new_h)\n\n                patch = np.array((int(left), int(top), int(left + new_w),\n                                  int(top + new_h)))\n                overlaps = bbox_overlaps(\n                    patch.reshape(-1, 4), boxes.reshape(-1, 4)).reshape(-1)\n                if overlaps.min() < min_iou:\n                    continue\n\n                # center of boxes should inside the crop img\n                center = (boxes[:, :2] + boxes[:, 2:]) / 2\n                mask = (center[:, 0] > patch[0]) * (\n                    center[:, 1] > patch[1]) * (center[:, 0] < patch[2]) * (\n                        center[:, 1] < patch[3])\n                if not mask.any():\n                    continue\n                boxes = boxes[mask]\n                labels = labels[mask]\n\n                # adjust boxes\n                img = img[patch[1]:patch[3], patch[0]:patch[2]]\n                boxes[:, 2:] = boxes[:, 2:].clip(max=patch[2:])\n                boxes[:, :2] = boxes[:, :2].clip(min=patch[:2])\n                boxes -= np.tile(patch[:2], 2)\n\n                return img, boxes, labels\n\n\nclass ExtraAugmentation(object):\n\n    def __init__(self,\n                 photo_metric_distortion=None,\n                 expand=None,\n                 random_crop=None):\n        self.transforms = []\n        if photo_metric_distortion is not None:\n            self.transforms.append(\n                PhotoMetricDistortion(**photo_metric_distortion))\n        if expand is not None:\n            self.transforms.append(Expand(**expand))\n        if random_crop is not None:\n            self.transforms.append(RandomCrop(**random_crop))\n\n    def __call__(self, img, boxes, labels):\n        img = img.astype(np.float32)\n        for transform in self.transforms:\n            img, boxes, labels = transform(img, boxes, labels)\n        return img, boxes, labels\n'"
mmdet/datasets/my_dataset.py,0,"b""from .voc import VOCDataset\nfrom .registry import DATASETS\n\n\n@DATASETS.register_module\nclass MyDataset(VOCDataset):\n\n\tCLASSES = ('large-vehicle', 'swimming-pool', 'helicopter', 'bridge', 'plane','ship',\n\t\t\t\t'soccer-ball-field','basketball-court','airport','container-crane',\n\t\t\t\t'ground-track-field','small-vehicle','harbor','baseball-diamond','tennis-court',\n\t\t\t\t'roundabout','storage-tank','helipad')\n\n\t# CLASSES = ('ship','cruiser','carrier')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n"""
mmdet/datasets/registry.py,0,"b""from mmdet.utils import Registry\n\nDATASETS = Registry('dataset')\n"""
mmdet/datasets/transforms.py,2,"b'import mmcv\nimport numpy as np\nimport torch\n\n__all__ = [\n    \'ImageTransform\', \'BboxTransform\', \'MaskTransform\', \'SegMapTransform\',\n    \'Numpy2Tensor\'\n]\n\n\nclass ImageTransform(object):\n    """"""Preprocess an image.\n\n    1. rescale the image to expected size\n    2. normalize the image\n    3. flip the image (if needed)\n    4. pad the image (if needed)\n    5. transpose to (c, h, w)\n    """"""\n\n    def __init__(self,\n                 mean=(0, 0, 0),\n                 std=(1, 1, 1),\n                 to_rgb=True,\n                 size_divisor=None):\n        self.mean = np.array(mean, dtype=np.float32)\n        self.std = np.array(std, dtype=np.float32)\n        self.to_rgb = to_rgb\n        self.size_divisor = size_divisor\n\n    def __call__(self, img, scale, flip=False, keep_ratio=True):\n        if keep_ratio:\n            img, scale_factor = mmcv.imrescale(img, scale, return_scale=True)\n        else:\n            img, w_scale, h_scale = mmcv.imresize(\n                img, scale, return_scale=True)\n            scale_factor = np.array(\n                [w_scale, h_scale, w_scale, h_scale], dtype=np.float32)\n        img_shape = img.shape\n        img = mmcv.imnormalize(img, self.mean, self.std, self.to_rgb)\n        if flip:\n            img = mmcv.imflip(img)\n        if self.size_divisor is not None:\n            img = mmcv.impad_to_multiple(img, self.size_divisor)\n            pad_shape = img.shape\n        else:\n            pad_shape = img_shape\n        img = img.transpose(2, 0, 1)\n        return img, img_shape, pad_shape, scale_factor\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n\n    Args:\n        bboxes(ndarray): shape (..., 4*k)\n        img_shape(tuple): (height, width)\n    """"""\n    assert bboxes.shape[-1] % 4 == 0\n    w = img_shape[1]\n    flipped = bboxes.copy()\n    flipped[..., 0::4] = w - bboxes[..., 2::4] - 1\n    flipped[..., 2::4] = w - bboxes[..., 0::4] - 1\n    return flipped\n\n\nclass BboxTransform(object):\n    """"""Preprocess gt bboxes.\n\n    1. rescale bboxes according to image size\n    2. flip bboxes (if needed)\n    3. pad the first dimension to `max_num_gts`\n    """"""\n\n    def __init__(self, max_num_gts=None):\n        self.max_num_gts = max_num_gts\n\n    def __call__(self, bboxes, img_shape, scale_factor, flip=False):\n        gt_bboxes = bboxes * scale_factor\n        if flip:\n            gt_bboxes = bbox_flip(gt_bboxes, img_shape)\n        gt_bboxes[:, 0::2] = np.clip(gt_bboxes[:, 0::2], 0, img_shape[1] - 1)\n        gt_bboxes[:, 1::2] = np.clip(gt_bboxes[:, 1::2], 0, img_shape[0] - 1)\n        if self.max_num_gts is None:\n            return gt_bboxes\n        else:\n            num_gts = gt_bboxes.shape[0]\n            padded_bboxes = np.zeros((self.max_num_gts, 4), dtype=np.float32)\n            padded_bboxes[:num_gts, :] = gt_bboxes\n            return padded_bboxes\n\n\nclass MaskTransform(object):\n    """"""Preprocess masks.\n\n    1. resize masks to expected size and stack to a single array\n    2. flip the masks (if needed)\n    3. pad the masks (if needed)\n    """"""\n\n    def __call__(self, masks, pad_shape, scale_factor, flip=False):\n        masks = [\n            mmcv.imrescale(mask, scale_factor, interpolation=\'nearest\')\n            for mask in masks\n        ]\n        if flip:\n            masks = [mask[:, ::-1] for mask in masks]\n        padded_masks = [\n            mmcv.impad(mask, pad_shape[:2], pad_val=0) for mask in masks\n        ]\n        padded_masks = np.stack(padded_masks, axis=0)\n        return padded_masks\n\n\nclass SegMapTransform(object):\n    """"""Preprocess semantic segmentation maps.\n\n    1. rescale the segmentation map to expected size\n    3. flip the image (if needed)\n    4. pad the image (if needed)\n    """"""\n\n    def __init__(self, size_divisor=None):\n        self.size_divisor = size_divisor\n\n    def __call__(self, img, scale, flip=False, keep_ratio=True):\n        if keep_ratio:\n            img = mmcv.imrescale(img, scale, interpolation=\'nearest\')\n        else:\n            img = mmcv.imresize(img, scale, interpolation=\'nearest\')\n        if flip:\n            img = mmcv.imflip(img)\n        if self.size_divisor is not None:\n            img = mmcv.impad_to_multiple(img, self.size_divisor)\n        return img\n\n\nclass Numpy2Tensor(object):\n\n    def __init__(self):\n        pass\n\n    def __call__(self, *args):\n        if len(args) == 1:\n            return torch.from_numpy(args[0])\n        else:\n            return tuple([torch.from_numpy(np.array(array)) for array in args])\n'"
mmdet/datasets/utils.py,7,"b'from collections import Sequence\n\nimport matplotlib.pyplot as plt\nimport mmcv\nimport numpy as np\nimport torch\n\n\ndef to_tensor(data):\n    """"""Convert objects of various python types to :obj:`torch.Tensor`.\n\n    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n    :class:`Sequence`, :class:`int` and :class:`float`.\n    """"""\n    if isinstance(data, torch.Tensor):\n        return data\n    elif isinstance(data, np.ndarray):\n        return torch.from_numpy(data)\n    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n        return torch.tensor(data)\n    elif isinstance(data, int):\n        return torch.LongTensor([data])\n    elif isinstance(data, float):\n        return torch.FloatTensor([data])\n    else:\n        raise TypeError(\'type {} cannot be converted to tensor.\'.format(\n            type(data)))\n\n\ndef random_scale(img_scales, mode=\'range\'):\n    """"""Randomly select a scale from a list of scales or scale ranges.\n\n    Args:\n        img_scales (list[tuple]): Image scale or scale range.\n        mode (str): ""range"" or ""value"".\n\n    Returns:\n        tuple: Sampled image scale.\n    """"""\n    num_scales = len(img_scales)\n    if num_scales == 1:  # fixed scale is specified\n        img_scale = img_scales[0]\n    elif num_scales == 2:  # randomly sample a scale\n        if mode == \'range\':\n            img_scale_long = [max(s) for s in img_scales]\n            img_scale_short = [min(s) for s in img_scales]\n            long_edge = np.random.randint(\n                min(img_scale_long),\n                max(img_scale_long) + 1)\n            short_edge = np.random.randint(\n                min(img_scale_short),\n                max(img_scale_short) + 1)\n            img_scale = (long_edge, short_edge)\n        elif mode == \'value\':\n            img_scale = img_scales[np.random.randint(num_scales)]\n    else:\n        if mode != \'value\':\n            raise ValueError(\n                \'Only ""value"" mode supports more than 2 image scales\')\n        img_scale = img_scales[np.random.randint(num_scales)]\n    return img_scale\n\n\ndef show_ann(coco, img, ann_info):\n    plt.imshow(mmcv.bgr2rgb(img))\n    plt.axis(\'off\')\n    coco.showAnns(ann_info)\n    plt.show()\n'"
mmdet/datasets/voc.py,0,"b""from .registry import DATASETS\nfrom .xml_style import XMLDataset\n\n\n@DATASETS.register_module\nclass VOCDataset(XMLDataset):\n\n    CLASSES = ('aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car',\n               'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse',\n               'motorbike', 'person', 'pottedplant', 'sheep', 'sofa', 'train',\n               'tvmonitor')\n\n    def __init__(self, **kwargs):\n        super(VOCDataset, self).__init__(**kwargs)\n        if 'VOC2007' in self.img_prefix:\n            self.year = 2007\n        elif 'VOC2012' in self.img_prefix:\n            self.year = 2012\n        else:\n            raise ValueError('Cannot infer dataset year from img_prefix')\n"""
mmdet/datasets/wider_face.py,0,"b'import os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\n\nfrom .registry import DATASETS\nfrom .xml_style import XMLDataset\n\n\n@DATASETS.register_module\nclass WIDERFaceDataset(XMLDataset):\n    """"""\n    Reader for the WIDER Face dataset in PASCAL VOC format.\n    Conversion scripts can be found in\n    https://github.com/sovrasov/wider-face-pascal-voc-annotations\n    """"""\n    CLASSES = (\'face\', )\n\n    def __init__(self, **kwargs):\n        super(WIDERFaceDataset, self).__init__(**kwargs)\n\n    def load_annotations(self, ann_file):\n        img_infos = []\n        img_ids = mmcv.list_from_file(ann_file)\n        for img_id in img_ids:\n            filename = \'{}.jpg\'.format(img_id)\n            xml_path = osp.join(self.img_prefix, \'Annotations\',\n                                \'{}.xml\'.format(img_id))\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            size = root.find(\'size\')\n            width = int(size.find(\'width\').text)\n            height = int(size.find(\'height\').text)\n            folder = root.find(\'folder\').text\n            img_infos.append(\n                dict(\n                    id=img_id,\n                    filename=osp.join(folder, filename),\n                    width=width,\n                    height=height))\n\n        return img_infos\n'"
mmdet/datasets/xml_style.py,0,"b""import os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\n\nfrom .custom import CustomDataset\nfrom .registry import DATASETS\n\n\n@DATASETS.register_module\nclass XMLDataset(CustomDataset):\n\n    def __init__(self, min_size=None, **kwargs):\n        super(XMLDataset, self).__init__(**kwargs)\n        self.cat2label = {cat: i + 1 for i, cat in enumerate(self.CLASSES)}\n        self.min_size = min_size\n\n    def load_annotations(self, ann_file):\n        img_infos = []\n        img_ids = mmcv.list_from_file(ann_file)\n        for img_id in img_ids:\n            filename = 'JPEGImages/{}.jpg'.format(img_id)\n            xml_path = osp.join(self.img_prefix, 'Annotations',\n                                '{}.xml'.format(img_id))\n            tree = ET.parse(xml_path)\n            root = tree.getroot()\n            size = root.find('size')\n            width = int(size.find('width').text)\n            height = int(size.find('height').text)\n            img_infos.append(\n                dict(id=img_id, filename=filename, width=width, height=height))\n        return img_infos\n\n    def get_ann_info(self, idx):\n        img_id = self.img_infos[idx]['id']\n        xml_path = osp.join(self.img_prefix, 'Annotations',\n                            '{}.xml'.format(img_id))\n        tree = ET.parse(xml_path)\n        root = tree.getroot()\n        bboxes = []\n        labels = []\n        bboxes_ignore = []\n        labels_ignore = []\n        for obj in root.findall('object'):\n            name = obj.find('name').text\n            label = self.cat2label[name]\n            difficult = int(obj.find('difficult').text)\n            bnd_box = obj.find('bndbox')\n            bbox = [\n                int(bnd_box.find('xmin').text),\n                int(bnd_box.find('ymin').text),\n                int(bnd_box.find('xmax').text),\n                int(bnd_box.find('ymax').text)\n            ]\n            ignore = False\n            if self.min_size:\n                assert not self.test_mode\n                w = bbox[2] - bbox[0]\n                h = bbox[3] - bbox[1]\n                if w < self.min_size or h < self.min_size:\n                    ignore = True\n            if difficult or ignore:\n                bboxes_ignore.append(bbox)\n                labels_ignore.append(label)\n            else:\n                bboxes.append(bbox)\n                labels.append(label)\n        if not bboxes:\n            bboxes = np.zeros((0, 4))\n            labels = np.zeros((0, ))\n        else:\n            bboxes = np.array(bboxes, ndmin=2) - 1\n            labels = np.array(labels)\n        if not bboxes_ignore:\n            bboxes_ignore = np.zeros((0, 4))\n            labels_ignore = np.zeros((0, ))\n        else:\n            bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n            labels_ignore = np.array(labels_ignore)\n        ann = dict(\n            bboxes=bboxes.astype(np.float32),\n            labels=labels.astype(np.int64),\n            bboxes_ignore=bboxes_ignore.astype(np.float32),\n            labels_ignore=labels_ignore.astype(np.int64))\n        return ann\n"""
mmdet/models/__init__.py,0,"b""from .backbones import *  # noqa: F401,F403\nfrom .necks import *  # noqa: F401,F403\nfrom .roi_extractors import *  # noqa: F401,F403\nfrom .anchor_heads import *  # noqa: F401,F403\nfrom .shared_heads import *  # noqa: F401,F403\nfrom .bbox_heads import *  # noqa: F401,F403\nfrom .mask_heads import *  # noqa: F401,F403\nfrom .losses import *  # noqa: F401,F403\nfrom .detectors import *  # noqa: F401,F403\nfrom .registry import (BACKBONES, NECKS, ROI_EXTRACTORS, SHARED_HEADS, HEADS,\n                       LOSSES, DETECTORS)\nfrom .builder import (build_backbone, build_neck, build_roi_extractor,\n                      build_shared_head, build_head, build_loss,\n                      build_detector)\n\n__all__ = [\n    'BACKBONES', 'NECKS', 'ROI_EXTRACTORS', 'SHARED_HEADS', 'HEADS', 'LOSSES',\n    'DETECTORS', 'build_backbone', 'build_neck', 'build_roi_extractor',\n    'build_shared_head', 'build_head', 'build_loss', 'build_detector'\n]\n"""
mmdet/models/builder.py,0,"b'from torch import nn\n\nfrom mmdet.utils import build_from_cfg\n\n#\xe6\xad\xa4\xe5\xa4\x84\xe4\xb8\x8d\xe4\xbc\x9a\xe5\x9c\xa8\xe6\x89\xa7\xe8\xa1\x8cregistry\xe8\x80\x8c\xe6\x98\xaf\xe7\x9b\xb4\xe6\x8e\xa5\xe8\xbf\x9b\xe8\xa1\x8csys.modules\xe6\x9f\xa5\xe8\xaf\xa2\xe5\xbe\x97\xe5\x88\xb0\nfrom .registry import (BACKBONES, NECKS, ROI_EXTRACTORS, SHARED_HEADS, HEADS,\n                       LOSSES, DETECTORS)\n\n\ndef build(cfg, registry, default_args=None):\n    if isinstance(cfg, list):\n        modules = [\n            build_from_cfg(cfg_, registry, default_args) for cfg_ in cfg\n        ]\n        return nn.Sequential(*modules)\n    else:\n        return build_from_cfg(cfg, registry, default_args)\n\n\ndef build_backbone(cfg):\n    return build(cfg, BACKBONES)\n\n\ndef build_neck(cfg):\n    return build(cfg, NECKS)\n\n\ndef build_roi_extractor(cfg):\n    return build(cfg, ROI_EXTRACTORS)\n\n\ndef build_shared_head(cfg):\n    return build(cfg, SHARED_HEADS)\n\n\ndef build_head(cfg):\n    return build(cfg, HEADS)\n\n\ndef build_loss(cfg):\n    return build(cfg, LOSSES)\n\n\ndef build_detector(cfg, train_cfg=None, test_cfg=None):\n    return build(cfg, DETECTORS, dict(train_cfg=train_cfg, test_cfg=test_cfg))\n'"
mmdet/models/registry.py,0,"b""from mmdet.utils import Registry\n\n# registry\xe7\x9a\x84\xe7\x9c\x9f\xe6\xad\xa3\xe5\xae\x9e\xe7\x8e\xb0\xe9\x83\xa8\xe5\x88\x86\xe5\x9c\xa8mmdet.utils\xe7\x9a\x84regisry.py\n\nBACKBONES = Registry('backbone')\nNECKS = Registry('neck')\nROI_EXTRACTORS = Registry('roi_extractor')\nSHARED_HEADS = Registry('shared_head')\nHEADS = Registry('head')\nLOSSES = Registry('loss')\nDETECTORS = Registry('detector')\n"""
mmdet/utils/__init__.py,0,"b""from .registry import Registry, build_from_cfg\n\n__all__ = ['Registry', 'build_from_cfg']\n"""
mmdet/utils/registry.py,0,"b'import inspect\n\nimport mmcv\nimport ipdb\n\n\nclass Registry(object):\n\n    def __init__(self, name):\n        self._name = name\n        self._module_dict = dict()\n\n    def __repr__(self):\n        format_str = self.__class__.__name__ + \'(name={}, items={})\'.format(\n            self._name, list(self._module_dict.keys()))\n        return format_str\n\n    @property\n    def name(self):\n        return self._name\n\n    \n    @property\n    def module_dict(self):\n        return self._module_dict\n\n    def get(self, key):\n        return self._module_dict.get(key, None)\n\n\n    def _register_module(self, module_class):\n        """"""Register a module.\n\n        Args:\n            module (:obj:`nn.Module`): Module to be registered.\n        """"""\n        if not inspect.isclass(module_class):\n            raise TypeError(\'module must be a class, but got {}\'.format(\n                type(module_class)))\n        module_name = module_class.__name__\n        if module_name in self._module_dict:\n            raise KeyError(\'{} is already registered in {}\'.format(\n                module_name, self.name))\n        self._module_dict[module_name] = module_class\n\n    def register_module(self, cls):\n        self._register_module(cls)\n        return cls\n\n\ndef build_from_cfg(cfg, registry, default_args=None):\n    """"""Build a module from config dict.\n\n    Args:\n        cfg (dict): Config dict. It should at least contain the key ""type"".\n        registry (:obj:`Registry`): The registry to search the type from.\n        default_args (dict, optional): Default initialization arguments.\n\n    Returns:\n        obj: The constructed object.\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    assert isinstance(default_args, dict) or default_args is None\n    args = cfg.copy()\n    obj_type = args.pop(\'type\')  #   \xe6\xa8\xa1\xe5\x9e\x8b\xe5\x90\x8d\n    if mmcv.is_str(obj_type):\n        # \xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84registry\xe7\x9a\x84get\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84_module_dict\xe5\xb1\x9e\xe6\x80\xa7\xe4\xb8\xad\xe5\x8c\x85\xe5\x90\xab\xe7\x9a\x84\xe6\x98\xafdetector\xe4\xb8\x8b\xe7\x9a\x84\xe6\xa8\xa1\xe5\x9e\x8btype\n        # \xe7\xb4\xa2\xe5\xbc\x95key\xe5\xbe\x97\xe5\x88\xb0\xe7\x9b\xb8\xe5\xba\x94\xe7\x9a\x84class\n        obj_type = registry.get(obj_type)\n        if obj_type is None:\n            raise KeyError(\'{} is not in the {} registry\'.format(\n                obj_type, registry.name))\n    elif not inspect.isclass(obj_type):\n        raise TypeError(\'type must be a str or valid type, but got {}\'.format(\n            type(obj_type)))\n    if default_args is not None:\n        for name, value in default_args.items():    #items()\xe8\xbf\x94\xe5\x9b\x9e\xe5\xad\x97\xe5\x85\xb8\xe7\x9a\x84\xe9\x94\xae\xe5\x80\xbc\xe5\xaf\xb9\xe7\x94\xa8\xe4\xba\x8e\xe9\x81\x8d\xe5\x8e\x86\n            args.setdefault(name, value)            #\xe5\xb0\x86default_args\xe7\x9a\x84\xe9\x94\xae\xe5\x80\xbc\xe5\xaf\xb9\xe5\x8a\xa0\xe5\x85\xa5\xe5\x88\xb0args\xe4\xb8\xad\xef\xbc\x8c\xe5\xb0\x86\xe6\xa8\xa1\xe5\x9e\x8b\xe5\x92\x8c\xe8\xae\xad\xe7\xbb\x83\xe9\x85\x8d\xe7\xbd\xae\xe8\xbf\x9b\xe8\xa1\x8c\xe6\x95\xb4\xe5\x90\x88\xe9\x80\x81\xe5\x85\xa5\xe7\xb1\xbb\xe4\xb8\xad\n    # \xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x9a\xe6\x97\xa0\xe8\xae\xba\xe8\xae\xad\xe7\xbb\x83/\xe6\xa3\x80\xe6\xb5\x8b\xef\xbc\x8c\xe9\x83\xbd\xe4\xbc\x9abuild DETECTORS\xef\xbc\x8c\xef\xbc\x9b\n    # **args\xe6\x98\xaf\xe5\xb0\x86\xe5\xad\x97\xe5\x85\xb8unpack\xe5\xbe\x97\xe5\x88\xb0\xe5\x90\x84\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xef\xbc\x8c\xe5\x88\x86\xe5\x88\xab\xe4\xb8\x8e\xe5\xbd\xa2\xe5\x8f\x82\xe5\x8c\xb9\xe9\x85\x8d\xe9\x80\x81\xe5\x85\xa5\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xef\xbc\x9b\n    return obj_type(**args)\n'"
tools/convert_datasets/pascal_voc.py,0,"b'import argparse\nimport os.path as osp\nimport xml.etree.ElementTree as ET\n\nimport mmcv\nimport numpy as np\n\nfrom mmdet.core import voc_classes\n\nlabel_ids = {name: i + 1 for i, name in enumerate(voc_classes())}\n\n\ndef parse_xml(args):\n    xml_path, img_path = args\n    tree = ET.parse(xml_path)\n    root = tree.getroot()\n    size = root.find(\'size\')\n    w = int(size.find(\'width\').text)\n    h = int(size.find(\'height\').text)\n    bboxes = []\n    labels = []\n    bboxes_ignore = []\n    labels_ignore = []\n    for obj in root.findall(\'object\'):\n        name = obj.find(\'name\').text\n        label = label_ids[name]\n        difficult = int(obj.find(\'difficult\').text)\n        bnd_box = obj.find(\'bndbox\')\n        bbox = [\n            int(bnd_box.find(\'xmin\').text),\n            int(bnd_box.find(\'ymin\').text),\n            int(bnd_box.find(\'xmax\').text),\n            int(bnd_box.find(\'ymax\').text)\n        ]\n        if difficult:\n            bboxes_ignore.append(bbox)\n            labels_ignore.append(label)\n        else:\n            bboxes.append(bbox)\n            labels.append(label)\n    if not bboxes:\n        bboxes = np.zeros((0, 4))\n        labels = np.zeros((0, ))\n    else:\n        bboxes = np.array(bboxes, ndmin=2) - 1\n        labels = np.array(labels)\n    if not bboxes_ignore:\n        bboxes_ignore = np.zeros((0, 4))\n        labels_ignore = np.zeros((0, ))\n    else:\n        bboxes_ignore = np.array(bboxes_ignore, ndmin=2) - 1\n        labels_ignore = np.array(labels_ignore)\n    annotation = {\n        \'filename\': img_path,\n        \'width\': w,\n        \'height\': h,\n        \'ann\': {\n            \'bboxes\': bboxes.astype(np.float32),\n            \'labels\': labels.astype(np.int64),\n            \'bboxes_ignore\': bboxes_ignore.astype(np.float32),\n            \'labels_ignore\': labels_ignore.astype(np.int64)\n        }\n    }\n    return annotation\n\n\ndef cvt_annotations(devkit_path, years, split, out_file):\n    if not isinstance(years, list):\n        years = [years]\n    annotations = []\n    for year in years:\n        filelist = osp.join(devkit_path, \'VOC{}/ImageSets/Main/{}.txt\'.format(\n            year, split))\n        if not osp.isfile(filelist):\n            print(\'filelist does not exist: {}, skip voc{} {}\'.format(\n                filelist, year, split))\n            return\n        img_names = mmcv.list_from_file(filelist)\n        xml_paths = [\n            osp.join(devkit_path, \'VOC{}/Annotations/{}.xml\'.format(\n                year, img_name)) for img_name in img_names\n        ]\n        img_paths = [\n            \'VOC{}/JPEGImages/{}.jpg\'.format(year, img_name)\n            for img_name in img_names\n        ]\n        part_annotations = mmcv.track_progress(parse_xml,\n                                               list(zip(xml_paths, img_paths)))\n        annotations.extend(part_annotations)\n    mmcv.dump(annotations, out_file)\n    return annotations\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Convert PASCAL VOC annotations to mmdetection format\')\n    parser.add_argument(\'devkit_path\', help=\'pascal voc devkit path\')\n    parser.add_argument(\'-o\', \'--out-dir\', help=\'output path\')\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    devkit_path = args.devkit_path\n    out_dir = args.out_dir if args.out_dir else devkit_path\n    mmcv.mkdir_or_exist(out_dir)\n\n    years = []\n    if osp.isdir(osp.join(devkit_path, \'VOC2007\')):\n        years.append(\'2007\')\n    if osp.isdir(osp.join(devkit_path, \'VOC2012\')):\n        years.append(\'2012\')\n    if \'2007\' in years and \'2012\' in years:\n        years.append([\'2007\', \'2012\'])\n    if not years:\n        raise IOError(\'The devkit path {} contains neither ""VOC2007"" nor \'\n                      \'""VOC2012"" subfolder\'.format(devkit_path))\n    for year in years:\n        if year == \'2007\':\n            prefix = \'voc07\'\n        elif year == \'2012\':\n            prefix = \'voc12\'\n        elif year == [\'2007\', \'2012\']:\n            prefix = \'voc0712\'\n        for split in [\'train\', \'val\', \'trainval\']:\n            dataset_name = prefix + \'_\' + split\n            print(\'processing {} ...\'.format(dataset_name))\n            cvt_annotations(devkit_path, year, split,\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n        if not isinstance(year, list):\n            dataset_name = prefix + \'_test\'\n            print(\'processing {} ...\'.format(dataset_name))\n            cvt_annotations(devkit_path, year, \'test\',\n                            osp.join(out_dir, dataset_name + \'.pkl\'))\n    print(\'Done!\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
mmdet/core/anchor/__init__.py,0,"b""from .anchor_generator import AnchorGenerator\nfrom .anchor_target import anchor_target, anchor_inside_flags\nfrom .guided_anchor_target import ga_loc_target, ga_shape_target\n\n__all__ = [\n    'AnchorGenerator', 'anchor_target', 'anchor_inside_flags', 'ga_loc_target',\n    'ga_shape_target'\n]\n"""
mmdet/core/anchor/anchor_generator.py,9,"b""import torch\n\n\nclass AnchorGenerator(object):\n\n    def __init__(self, base_size, scales, ratios, scale_major=True, ctr=None):\n        self.base_size = base_size\n        self.scales = torch.Tensor(scales)\n        self.ratios = torch.Tensor(ratios)\n        self.scale_major = scale_major\n        self.ctr = ctr\n        self.base_anchors = self.gen_base_anchors()\n\n    @property\n    def num_base_anchors(self):\n        return self.base_anchors.size(0)\n\n    def gen_base_anchors(self):\n        w = self.base_size\n        h = self.base_size\n        if self.ctr is None:\n            x_ctr = 0.5 * (w - 1)\n            y_ctr = 0.5 * (h - 1)\n        else:\n            x_ctr, y_ctr = self.ctr\n\n        h_ratios = torch.sqrt(self.ratios)\n        w_ratios = 1 / h_ratios\n        if self.scale_major:\n            ws = (w * w_ratios[:, None] * self.scales[None, :]).view(-1)\n            hs = (h * h_ratios[:, None] * self.scales[None, :]).view(-1)\n        else:\n            ws = (w * self.scales[:, None] * w_ratios[None, :]).view(-1)\n            hs = (h * self.scales[:, None] * h_ratios[None, :]).view(-1)\n\n        base_anchors = torch.stack(\n            [\n                x_ctr - 0.5 * (ws - 1), y_ctr - 0.5 * (hs - 1),\n                x_ctr + 0.5 * (ws - 1), y_ctr + 0.5 * (hs - 1)\n            ],\n            dim=-1).round()\n\n        return base_anchors\n\n    def _meshgrid(self, x, y, row_major=True):\n        xx = x.repeat(len(y))\n        yy = y.view(-1, 1).repeat(1, len(x)).view(-1)\n        if row_major:\n            return xx, yy\n        else:\n            return yy, xx\n\n    def grid_anchors(self, featmap_size, stride=16, device='cuda'):\n        base_anchors = self.base_anchors.to(device)\n\n        feat_h, feat_w = featmap_size\n        shift_x = torch.arange(0, feat_w, device=device) * stride\n        shift_y = torch.arange(0, feat_h, device=device) * stride\n        shift_xx, shift_yy = self._meshgrid(shift_x, shift_y)\n        shifts = torch.stack([shift_xx, shift_yy, shift_xx, shift_yy], dim=-1)\n        shifts = shifts.type_as(base_anchors)\n        # first feat_w elements correspond to the first row of shifts\n        # add A anchors (1, A, 4) to K shifts (K, 1, 4) to get\n        # shifted anchors (K, A, 4), reshape to (K*A, 4)\n\n        all_anchors = base_anchors[None, :, :] + shifts[:, None, :]\n        all_anchors = all_anchors.view(-1, 4)\n        # first A rows correspond to A anchors of (0, 0) in feature map,\n        # then (0, 1), (0, 2), ...\n        return all_anchors\n\n    def valid_flags(self, featmap_size, valid_size, device='cuda'):\n        feat_h, feat_w = featmap_size\n        valid_h, valid_w = valid_size\n        assert valid_h <= feat_h and valid_w <= feat_w\n        valid_x = torch.zeros(feat_w, dtype=torch.uint8, device=device)\n        valid_y = torch.zeros(feat_h, dtype=torch.uint8, device=device)\n        valid_x[:valid_w] = 1\n        valid_y[:valid_h] = 1\n        valid_xx, valid_yy = self._meshgrid(valid_x, valid_y)\n        valid = valid_xx & valid_yy\n        valid = valid[:, None].expand(\n            valid.size(0), self.num_base_anchors).contiguous().view(-1)\n        return valid\n"""
mmdet/core/anchor/anchor_target.py,7,"b'import torch\n\nfrom ..bbox import assign_and_sample, build_assigner, PseudoSampler, bbox2delta\nfrom ..utils import multi_apply\n\n\ndef anchor_target(anchor_list,\n                  valid_flag_list,\n                  gt_bboxes_list,\n                  img_metas,\n                  target_means,\n                  target_stds,\n                  cfg,\n                  gt_bboxes_ignore_list=None,\n                  gt_labels_list=None,\n                  label_channels=1,\n                  sampling=True,\n                  unmap_outputs=True):\n    """"""Compute regression and classification targets for anchors.\n\n    Args:\n        anchor_list (list[list]): Multi level anchors of each image.\n        valid_flag_list (list[list]): Multi level valid flags of each image.\n        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n        img_metas (list[dict]): Meta info of each image.\n        target_means (Iterable): Mean value of regression targets.\n        target_stds (Iterable): Std value of regression targets.\n        cfg (dict): RPN train configs.\n\n    Returns:\n        tuple\n    """"""\n    num_imgs = len(img_metas)\n    assert len(anchor_list) == len(valid_flag_list) == num_imgs\n\n    # anchor number of multi levels\n    num_level_anchors = [anchors.size(0) for anchors in anchor_list[0]]\n    # concat all level anchors and flags to a single tensor\n    for i in range(num_imgs):\n        assert len(anchor_list[i]) == len(valid_flag_list[i])\n        anchor_list[i] = torch.cat(anchor_list[i])\n        valid_flag_list[i] = torch.cat(valid_flag_list[i])\n\n    # compute targets for each image\n    if gt_bboxes_ignore_list is None:\n        gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n    if gt_labels_list is None:\n        gt_labels_list = [None for _ in range(num_imgs)]\n    (all_labels, all_label_weights, all_bbox_targets, all_bbox_weights,\n     pos_inds_list, neg_inds_list) = multi_apply(\n         anchor_target_single,\n         anchor_list,\n         valid_flag_list,\n         gt_bboxes_list,\n         gt_bboxes_ignore_list,\n         gt_labels_list,\n         img_metas,\n         target_means=target_means,\n         target_stds=target_stds,\n         cfg=cfg,\n         label_channels=label_channels,\n         sampling=sampling,\n         unmap_outputs=unmap_outputs)\n    # no valid anchors\n    if any([labels is None for labels in all_labels]):\n        return None\n    # sampled anchors of all images\n    num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n    num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n    # split targets to a list w.r.t. multiple levels\n    labels_list = images_to_levels(all_labels, num_level_anchors)\n    label_weights_list = images_to_levels(all_label_weights, num_level_anchors)\n    bbox_targets_list = images_to_levels(all_bbox_targets, num_level_anchors)\n    bbox_weights_list = images_to_levels(all_bbox_weights, num_level_anchors)\n    return (labels_list, label_weights_list, bbox_targets_list,\n            bbox_weights_list, num_total_pos, num_total_neg)\n\n\ndef images_to_levels(target, num_level_anchors):\n    """"""Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    """"""\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_level_anchors:\n        end = start + n\n        level_targets.append(target[:, start:end].squeeze(0))\n        start = end\n    return level_targets\n\n\ndef anchor_target_single(flat_anchors,\n                         valid_flags,\n                         gt_bboxes,\n                         gt_bboxes_ignore,\n                         gt_labels,\n                         img_meta,\n                         target_means,\n                         target_stds,\n                         cfg,\n                         label_channels=1,\n                         sampling=True,\n                         unmap_outputs=True):\n    inside_flags = anchor_inside_flags(flat_anchors, valid_flags,\n                                       img_meta[\'img_shape\'][:2],\n                                       cfg.allowed_border)\n    if not inside_flags.any():\n        return (None, ) * 6\n    # assign gt and sample anchors\n    anchors = flat_anchors[inside_flags, :]\n\n    if sampling:\n        assign_result, sampling_result = assign_and_sample(\n            anchors, gt_bboxes, gt_bboxes_ignore, None, cfg)\n    else:\n        bbox_assigner = build_assigner(cfg.assigner)\n        assign_result = bbox_assigner.assign(anchors, gt_bboxes,\n                                             gt_bboxes_ignore, gt_labels)\n        bbox_sampler = PseudoSampler()\n        sampling_result = bbox_sampler.sample(assign_result, anchors,\n                                              gt_bboxes)\n\n    num_valid_anchors = anchors.shape[0]\n    bbox_targets = torch.zeros_like(anchors)\n    bbox_weights = torch.zeros_like(anchors)\n    labels = anchors.new_zeros(num_valid_anchors, dtype=torch.long)\n    label_weights = anchors.new_zeros(num_valid_anchors, dtype=torch.float)\n\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    if len(pos_inds) > 0:\n        pos_bbox_targets = bbox2delta(sampling_result.pos_bboxes,\n                                      sampling_result.pos_gt_bboxes,\n                                      target_means, target_stds)\n        bbox_targets[pos_inds, :] = pos_bbox_targets\n        bbox_weights[pos_inds, :] = 1.0\n        if gt_labels is None:\n            labels[pos_inds] = 1\n        else:\n            labels[pos_inds] = gt_labels[sampling_result.pos_assigned_gt_inds]\n        if cfg.pos_weight <= 0:\n            label_weights[pos_inds] = 1.0\n        else:\n            label_weights[pos_inds] = cfg.pos_weight\n    if len(neg_inds) > 0:\n        label_weights[neg_inds] = 1.0\n\n    # map up to original set of anchors\n    if unmap_outputs:\n        num_total_anchors = flat_anchors.size(0)\n        labels = unmap(labels, num_total_anchors, inside_flags)\n        label_weights = unmap(label_weights, num_total_anchors, inside_flags)\n        bbox_targets = unmap(bbox_targets, num_total_anchors, inside_flags)\n        bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n    return (labels, label_weights, bbox_targets, bbox_weights, pos_inds,\n            neg_inds)\n\n\ndef anchor_inside_flags(flat_anchors, valid_flags, img_shape,\n                        allowed_border=0):\n    img_h, img_w = img_shape[:2]\n    if allowed_border >= 0:\n        inside_flags = valid_flags & \\\n            (flat_anchors[:, 0] >= -allowed_border) & \\\n            (flat_anchors[:, 1] >= -allowed_border) & \\\n            (flat_anchors[:, 2] < img_w + allowed_border) & \\\n            (flat_anchors[:, 3] < img_h + allowed_border)\n    else:\n        inside_flags = valid_flags\n    return inside_flags\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n'"
mmdet/core/anchor/guided_anchor_target.py,18,"b'import torch\n\nfrom ..bbox import build_assigner, build_sampler, PseudoSampler\nfrom ..utils import unmap, multi_apply\n\n\ndef calc_region(bbox, ratio, featmap_size=None):\n    """"""Calculate a proportional bbox region.\n\n    The bbox center are fixed and the new h\' and w\' is h * ratio and w * ratio.\n\n    Args:\n        bbox (Tensor): Bboxes to calculate regions, shape (n, 4)\n        ratio (float): Ratio of the output region.\n        featmap_size (tuple): Feature map size used for clipping the boundary.\n\n    Returns:\n        tuple: x1, y1, x2, y2\n    """"""\n    x1 = torch.round((1 - ratio) * bbox[0] + ratio * bbox[2]).long()\n    y1 = torch.round((1 - ratio) * bbox[1] + ratio * bbox[3]).long()\n    x2 = torch.round(ratio * bbox[0] + (1 - ratio) * bbox[2]).long()\n    y2 = torch.round(ratio * bbox[1] + (1 - ratio) * bbox[3]).long()\n    if featmap_size is not None:\n        x1 = x1.clamp(min=0, max=featmap_size[1] - 1)\n        y1 = y1.clamp(min=0, max=featmap_size[0] - 1)\n        x2 = x2.clamp(min=0, max=featmap_size[1] - 1)\n        y2 = y2.clamp(min=0, max=featmap_size[0] - 1)\n    return (x1, y1, x2, y2)\n\n\ndef ga_loc_target(gt_bboxes_list,\n                  featmap_sizes,\n                  anchor_scale,\n                  anchor_strides,\n                  center_ratio=0.2,\n                  ignore_ratio=0.5):\n    """"""Compute location targets for guided anchoring.\n\n    Each feature map is divided into positive, negative and ignore regions.\n    - positive regions: target 1, weight 1\n    - ignore regions: target 0, weight 0\n    - negative regions: target 0, weight 0.1\n\n    Args:\n        gt_bboxes_list (list[Tensor]): Gt bboxes of each image.\n        featmap_sizes (list[tuple]): Multi level sizes of each feature maps.\n        anchor_scale (int): Anchor scale.\n        anchor_strides ([list[int]]): Multi level anchor strides.\n        center_ratio (float): Ratio of center region.\n        ignore_ratio (float): Ratio of ignore region.\n\n    Returns:\n        tuple\n    """"""\n    img_per_gpu = len(gt_bboxes_list)\n    num_lvls = len(featmap_sizes)\n    r1 = (1 - center_ratio) / 2\n    r2 = (1 - ignore_ratio) / 2\n    all_loc_targets = []\n    all_loc_weights = []\n    all_ignore_map = []\n    for lvl_id in range(num_lvls):\n        h, w = featmap_sizes[lvl_id]\n        loc_targets = torch.zeros(img_per_gpu,\n                                  1,\n                                  h,\n                                  w,\n                                  device=gt_bboxes_list[0].device,\n                                  dtype=torch.float32)\n        loc_weights = torch.full_like(loc_targets, -1)\n        ignore_map = torch.zeros_like(loc_targets)\n        all_loc_targets.append(loc_targets)\n        all_loc_weights.append(loc_weights)\n        all_ignore_map.append(ignore_map)\n    for img_id in range(img_per_gpu):\n        gt_bboxes = gt_bboxes_list[img_id]\n        scale = torch.sqrt((gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) *\n                           (gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1))\n        min_anchor_size = scale.new_full(\n            (1, ), float(anchor_scale * anchor_strides[0]))\n        # assign gt bboxes to different feature levels w.r.t. their scales\n        target_lvls = torch.floor(\n            torch.log2(scale) - torch.log2(min_anchor_size) + 0.5)\n        target_lvls = target_lvls.clamp(min=0, max=num_lvls - 1).long()\n        for gt_id in range(gt_bboxes.size(0)):\n            lvl = target_lvls[gt_id].item()\n            # rescaled to corresponding feature map\n            gt_ = gt_bboxes[gt_id, :4] / anchor_strides[lvl]\n            # calculate ignore regions\n            ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                gt_, r2, featmap_sizes[lvl])\n            # calculate positive (center) regions\n            ctr_x1, ctr_y1, ctr_x2, ctr_y2 = calc_region(\n                gt_, r1, featmap_sizes[lvl])\n            all_loc_targets[lvl][img_id, 0, ctr_y1:ctr_y2 + 1, ctr_x1:ctr_x2 +\n                                 1] = 1\n            all_loc_weights[lvl][img_id, 0, ignore_y1:ignore_y2 +\n                                 1, ignore_x1:ignore_x2 + 1] = 0\n            all_loc_weights[lvl][img_id, 0, ctr_y1:ctr_y2 + 1, ctr_x1:ctr_x2 +\n                                 1] = 1\n            # calculate ignore map on nearby low level feature\n            if lvl > 0:\n                d_lvl = lvl - 1\n                # rescaled to corresponding feature map\n                gt_ = gt_bboxes[gt_id, :4] / anchor_strides[d_lvl]\n                ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                    gt_, r2, featmap_sizes[d_lvl])\n                all_ignore_map[d_lvl][img_id, 0, ignore_y1:ignore_y2 +\n                                      1, ignore_x1:ignore_x2 + 1] = 1\n            # calculate ignore map on nearby high level feature\n            if lvl < num_lvls - 1:\n                u_lvl = lvl + 1\n                # rescaled to corresponding feature map\n                gt_ = gt_bboxes[gt_id, :4] / anchor_strides[u_lvl]\n                ignore_x1, ignore_y1, ignore_x2, ignore_y2 = calc_region(\n                    gt_, r2, featmap_sizes[u_lvl])\n                all_ignore_map[u_lvl][img_id, 0, ignore_y1:ignore_y2 +\n                                      1, ignore_x1:ignore_x2 + 1] = 1\n    for lvl_id in range(num_lvls):\n        # ignore negative regions w.r.t. ignore map\n        all_loc_weights[lvl_id][(all_loc_weights[lvl_id] < 0)\n                                & (all_ignore_map[lvl_id] > 0)] = 0\n        # set negative regions with weight 0.1\n        all_loc_weights[lvl_id][all_loc_weights[lvl_id] < 0] = 0.1\n    # loc average factor to balance loss\n    loc_avg_factor = sum(\n        [t.size(0) * t.size(-1) * t.size(-2) for t in all_loc_targets]) / 200\n    return all_loc_targets, all_loc_weights, loc_avg_factor\n\n\ndef ga_shape_target(approx_list,\n                    inside_flag_list,\n                    square_list,\n                    gt_bboxes_list,\n                    img_metas,\n                    approxs_per_octave,\n                    cfg,\n                    gt_bboxes_ignore_list=None,\n                    sampling=True,\n                    unmap_outputs=True):\n    """"""Compute guided anchoring targets.\n\n    Args:\n        approx_list (list[list]): Multi level approxs of each image.\n        inside_flag_list (list[list]): Multi level inside flags of each image.\n        square_list (list[list]): Multi level squares of each image.\n        gt_bboxes_list (list[Tensor]): Ground truth bboxes of each image.\n        img_metas (list[dict]): Meta info of each image.\n        approxs_per_octave (int): number of approxs per octave\n        cfg (dict): RPN train configs.\n        gt_bboxes_ignore_list (list[Tensor]): ignore list of gt bboxes.\n        sampling (bool): sampling or not.\n        unmap_outputs (bool): unmap outputs or not.\n\n    Returns:\n        tuple\n    """"""\n    num_imgs = len(img_metas)\n    assert len(approx_list) == len(inside_flag_list) == len(\n        square_list) == num_imgs\n    # anchor number of multi levels\n    num_level_squares = [squares.size(0) for squares in square_list[0]]\n    # concat all level anchors and flags to a single tensor\n    inside_flag_flat_list = []\n    approx_flat_list = []\n    square_flat_list = []\n    for i in range(num_imgs):\n        assert len(square_list[i]) == len(inside_flag_list[i])\n        inside_flag_flat_list.append(torch.cat(inside_flag_list[i]))\n        approx_flat_list.append(torch.cat(approx_list[i]))\n        square_flat_list.append(torch.cat(square_list[i]))\n\n    # compute targets for each image\n    if gt_bboxes_ignore_list is None:\n        gt_bboxes_ignore_list = [None for _ in range(num_imgs)]\n    (all_bbox_anchors, all_bbox_gts, all_bbox_weights, pos_inds_list,\n     neg_inds_list) = multi_apply(ga_shape_target_single,\n                                  approx_flat_list,\n                                  inside_flag_flat_list,\n                                  square_flat_list,\n                                  gt_bboxes_list,\n                                  gt_bboxes_ignore_list,\n                                  img_metas,\n                                  approxs_per_octave=approxs_per_octave,\n                                  cfg=cfg,\n                                  sampling=sampling,\n                                  unmap_outputs=unmap_outputs)\n    # no valid anchors\n    if any([bbox_anchors is None for bbox_anchors in all_bbox_anchors]):\n        return None\n    # sampled anchors of all images\n    num_total_pos = sum([max(inds.numel(), 1) for inds in pos_inds_list])\n    num_total_neg = sum([max(inds.numel(), 1) for inds in neg_inds_list])\n    # split targets to a list w.r.t. multiple levels\n    bbox_anchors_list = images_to_levels(all_bbox_anchors, num_level_squares)\n    bbox_gts_list = images_to_levels(all_bbox_gts, num_level_squares)\n    bbox_weights_list = images_to_levels(all_bbox_weights, num_level_squares)\n    return (bbox_anchors_list, bbox_gts_list, bbox_weights_list, num_total_pos,\n            num_total_neg)\n\n\ndef images_to_levels(target, num_level_anchors):\n    """"""Convert targets by image to targets by feature level.\n\n    [target_img0, target_img1] -> [target_level0, target_level1, ...]\n    """"""\n    target = torch.stack(target, 0)\n    level_targets = []\n    start = 0\n    for n in num_level_anchors:\n        end = start + n\n        level_targets.append(target[:, start:end].squeeze(0))\n        start = end\n    return level_targets\n\n\ndef ga_shape_target_single(flat_approxs,\n                           inside_flags,\n                           flat_squares,\n                           gt_bboxes,\n                           gt_bboxes_ignore,\n                           img_meta,\n                           approxs_per_octave,\n                           cfg,\n                           sampling=True,\n                           unmap_outputs=True):\n    """"""Compute guided anchoring targets.\n\n    This function returns sampled anchors and gt bboxes directly\n    rather than calculates regression targets.\n\n    Args:\n        flat_approxs (Tensor): flat approxs of a single image,\n            shape (n, 4)\n        inside_flags (Tensor): inside flags of a single image,\n            shape (n, ).\n        flat_squares (Tensor): flat squares of a single image,\n            shape (approxs_per_octave * n, 4)\n        gt_bboxes (Tensor): Ground truth bboxes of a single image.\n        img_meta (dict): Meta info of a single image.\n        approxs_per_octave (int): number of approxs per octave\n        cfg (dict): RPN train configs.\n        sampling (bool): sampling or not.\n        unmap_outputs (bool): unmap outputs or not.\n\n    Returns:\n        tuple\n    """"""\n    if not inside_flags.any():\n        return (None, ) * 6\n    # assign gt and sample anchors\n    expand_inside_flags = inside_flags[:, None].expand(\n        -1, approxs_per_octave).reshape(-1)\n    approxs = flat_approxs[expand_inside_flags, :]\n    squares = flat_squares[inside_flags, :]\n\n    bbox_assigner = build_assigner(cfg.ga_assigner)\n    assign_result = bbox_assigner.assign(approxs, squares, approxs_per_octave,\n                                         gt_bboxes, gt_bboxes_ignore)\n    if sampling:\n        bbox_sampler = build_sampler(cfg.ga_sampler)\n    else:\n        bbox_sampler = PseudoSampler()\n    sampling_result = bbox_sampler.sample(assign_result, squares, gt_bboxes)\n\n    bbox_anchors = torch.zeros_like(squares)\n    bbox_gts = torch.zeros_like(squares)\n    bbox_weights = torch.zeros_like(squares)\n\n    pos_inds = sampling_result.pos_inds\n    neg_inds = sampling_result.neg_inds\n    if len(pos_inds) > 0:\n        bbox_anchors[pos_inds, :] = sampling_result.pos_bboxes\n        bbox_gts[pos_inds, :] = sampling_result.pos_gt_bboxes\n        bbox_weights[pos_inds, :] = 1.0\n\n    # map up to original set of anchors\n    if unmap_outputs:\n        num_total_anchors = flat_squares.size(0)\n        bbox_anchors = unmap(bbox_anchors, num_total_anchors, inside_flags)\n        bbox_gts = unmap(bbox_gts, num_total_anchors, inside_flags)\n        bbox_weights = unmap(bbox_weights, num_total_anchors, inside_flags)\n\n    return (bbox_anchors, bbox_gts, bbox_weights, pos_inds, neg_inds)\n'"
mmdet/core/bbox/__init__.py,0,"b""from .geometry import bbox_overlaps\nfrom .assigners import BaseAssigner, MaxIoUAssigner, AssignResult\nfrom .samplers import (BaseSampler, PseudoSampler, RandomSampler,\n                       InstanceBalancedPosSampler, IoUBalancedNegSampler,\n                       CombinedSampler, SamplingResult)\nfrom .assign_sampling import build_assigner, build_sampler, assign_and_sample\nfrom .transforms import (bbox2delta, delta2bbox, bbox_flip, bbox_mapping,\n                         bbox_mapping_back, bbox2roi, roi2bbox, bbox2result,\n                         distance2bbox)\nfrom .bbox_target import bbox_target\n\n__all__ = [\n    'bbox_overlaps', 'BaseAssigner', 'MaxIoUAssigner', 'AssignResult',\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'SamplingResult', 'build_assigner', 'build_sampler', 'assign_and_sample',\n    'bbox2delta', 'delta2bbox', 'bbox_flip', 'bbox_mapping',\n    'bbox_mapping_back', 'bbox2roi', 'roi2bbox', 'bbox2result',\n    'distance2bbox', 'bbox_target'\n]\n"""
mmdet/core/bbox/assign_sampling.py,0,"b""import mmcv\n\nfrom . import assigners, samplers\n\n\ndef build_assigner(cfg, **kwargs):\n    if isinstance(cfg, assigners.BaseAssigner):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(cfg, assigners, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef build_sampler(cfg, **kwargs):\n    if isinstance(cfg, samplers.BaseSampler):\n        return cfg\n    elif isinstance(cfg, dict):\n        return mmcv.runner.obj_from_dict(cfg, samplers, default_args=kwargs)\n    else:\n        raise TypeError('Invalid type {} for building a sampler'.format(\n            type(cfg)))\n\n\ndef assign_and_sample(bboxes, gt_bboxes, gt_bboxes_ignore, gt_labels, cfg):\n    bbox_assigner = build_assigner(cfg.assigner)\n    bbox_sampler = build_sampler(cfg.sampler)\n    assign_result = bbox_assigner.assign(bboxes, gt_bboxes, gt_bboxes_ignore,\n                                         gt_labels)\n    sampling_result = bbox_sampler.sample(assign_result, bboxes, gt_bboxes,\n                                          gt_labels)\n    return assign_result, sampling_result\n"""
mmdet/core/bbox/bbox_target.py,6,"b'import torch\n\nfrom .transforms import bbox2delta\nfrom ..utils import multi_apply\n\n\ndef bbox_target(pos_bboxes_list,\n                neg_bboxes_list,\n                pos_gt_bboxes_list,\n                pos_gt_labels_list,\n                cfg,\n                reg_classes=1,\n                target_means=[.0, .0, .0, .0],\n                target_stds=[1.0, 1.0, 1.0, 1.0],\n                concat=True):\n    labels, label_weights, bbox_targets, bbox_weights = multi_apply(\n        bbox_target_single,\n        pos_bboxes_list,\n        neg_bboxes_list,\n        pos_gt_bboxes_list,\n        pos_gt_labels_list,\n        cfg=cfg,\n        reg_classes=reg_classes,\n        target_means=target_means,\n        target_stds=target_stds)\n\n    if concat:\n        labels = torch.cat(labels, 0)\n        label_weights = torch.cat(label_weights, 0)\n        bbox_targets = torch.cat(bbox_targets, 0)\n        bbox_weights = torch.cat(bbox_weights, 0)\n    return labels, label_weights, bbox_targets, bbox_weights\n\n\ndef bbox_target_single(pos_bboxes,\n                       neg_bboxes,\n                       pos_gt_bboxes,\n                       pos_gt_labels,\n                       cfg,\n                       reg_classes=1,\n                       target_means=[.0, .0, .0, .0],\n                       target_stds=[1.0, 1.0, 1.0, 1.0]):\n    num_pos = pos_bboxes.size(0)\n    num_neg = neg_bboxes.size(0)\n    num_samples = num_pos + num_neg\n    labels = pos_bboxes.new_zeros(num_samples, dtype=torch.long)\n    label_weights = pos_bboxes.new_zeros(num_samples)\n    bbox_targets = pos_bboxes.new_zeros(num_samples, 4)\n    bbox_weights = pos_bboxes.new_zeros(num_samples, 4)\n    if num_pos > 0:\n        labels[:num_pos] = pos_gt_labels\n        pos_weight = 1.0 if cfg.pos_weight <= 0 else cfg.pos_weight\n        label_weights[:num_pos] = pos_weight\n        pos_bbox_targets = bbox2delta(pos_bboxes, pos_gt_bboxes, target_means,\n                                      target_stds)\n        bbox_targets[:num_pos, :] = pos_bbox_targets\n        bbox_weights[:num_pos, :] = 1\n    if num_neg > 0:\n        label_weights[-num_neg:] = 1.0\n\n    return labels, label_weights, bbox_targets, bbox_weights\n\n\ndef expand_target(bbox_targets, bbox_weights, labels, num_classes):\n    bbox_targets_expand = bbox_targets.new_zeros((bbox_targets.size(0),\n                                                  4 * num_classes))\n    bbox_weights_expand = bbox_weights.new_zeros((bbox_weights.size(0),\n                                                  4 * num_classes))\n    for i in torch.nonzero(labels > 0).squeeze(-1):\n        start, end = labels[i] * 4, (labels[i] + 1) * 4\n        bbox_targets_expand[i, start:end] = bbox_targets[i, :]\n        bbox_weights_expand[i, start:end] = bbox_weights[i, :]\n    return bbox_targets_expand, bbox_weights_expand\n'"
mmdet/core/bbox/geometry.py,4,"b'import torch\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\', is_aligned=False):\n    """"""Calculate overlap between two set of bboxes.\n\n    If ``is_aligned`` is ``False``, then calculate the ious between each bbox\n    of bboxes1 and bboxes2, otherwise the ious between each aligned pair of\n    bboxes1 and bboxes2.\n\n    Args:\n        bboxes1 (Tensor): shape (m, 4)\n        bboxes2 (Tensor): shape (n, 4), if is_aligned is ``True``, then m and n\n            must be equal.\n        mode (str): ""iou"" (intersection over union) or iof (intersection over\n            foreground).\n\n    Returns:\n        ious(Tensor): shape (m, n) if is_aligned == False else shape (m, 1)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    rows = bboxes1.size(0)\n    cols = bboxes2.size(0)\n    if is_aligned:\n        assert rows == cols\n\n    if rows * cols == 0:\n        return bboxes1.new(rows, 1) if is_aligned else bboxes1.new(rows, cols)\n\n    if is_aligned:\n        lt = torch.max(bboxes1[:, :2], bboxes2[:, :2])  # [rows, 2]\n        rb = torch.min(bboxes1[:, 2:], bboxes2[:, 2:])  # [rows, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, 2]\n        overlap = wh[:, 0] * wh[:, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1 + area2 - overlap)\n        else:\n            ious = overlap / area1\n    else:\n        lt = torch.max(bboxes1[:, None, :2], bboxes2[:, :2])  # [rows, cols, 2]\n        rb = torch.min(bboxes1[:, None, 2:], bboxes2[:, 2:])  # [rows, cols, 2]\n\n        wh = (rb - lt + 1).clamp(min=0)  # [rows, cols, 2]\n        overlap = wh[:, :, 0] * wh[:, :, 1]\n        area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n            bboxes1[:, 3] - bboxes1[:, 1] + 1)\n\n        if mode == \'iou\':\n            area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n                bboxes2[:, 3] - bboxes2[:, 1] + 1)\n            ious = overlap / (area1[:, None] + area2 - overlap)\n        else:\n            ious = overlap / (area1[:, None])\n\n    return ious\n'"
mmdet/core/bbox/transforms.py,11,"b'import mmcv\nimport numpy as np\nimport torch\n\n\ndef bbox2delta(proposals, gt, means=[0, 0, 0, 0], stds=[1, 1, 1, 1]):\n    assert proposals.size() == gt.size()\n\n    proposals = proposals.float()\n    gt = gt.float()\n    px = (proposals[..., 0] + proposals[..., 2]) * 0.5\n    py = (proposals[..., 1] + proposals[..., 3]) * 0.5\n    pw = proposals[..., 2] - proposals[..., 0] + 1.0\n    ph = proposals[..., 3] - proposals[..., 1] + 1.0\n\n    gx = (gt[..., 0] + gt[..., 2]) * 0.5\n    gy = (gt[..., 1] + gt[..., 3]) * 0.5\n    gw = gt[..., 2] - gt[..., 0] + 1.0\n    gh = gt[..., 3] - gt[..., 1] + 1.0\n\n    dx = (gx - px) / pw\n    dy = (gy - py) / ph\n    dw = torch.log(gw / pw)\n    dh = torch.log(gh / ph)\n    deltas = torch.stack([dx, dy, dw, dh], dim=-1)\n\n    means = deltas.new_tensor(means).unsqueeze(0)\n    stds = deltas.new_tensor(stds).unsqueeze(0)\n    deltas = deltas.sub_(means).div_(stds)\n\n    return deltas\n\n\ndef delta2bbox(rois,\n               deltas,\n               means=[0, 0, 0, 0],\n               stds=[1, 1, 1, 1],\n               max_shape=None,\n               wh_ratio_clip=16 / 1000):\n    means = deltas.new_tensor(means).repeat(1, deltas.size(1) // 4)\n    stds = deltas.new_tensor(stds).repeat(1, deltas.size(1) // 4)\n    denorm_deltas = deltas * stds + means\n    dx = denorm_deltas[:, 0::4]\n    dy = denorm_deltas[:, 1::4]\n    dw = denorm_deltas[:, 2::4]\n    dh = denorm_deltas[:, 3::4]\n    max_ratio = np.abs(np.log(wh_ratio_clip))\n    dw = dw.clamp(min=-max_ratio, max=max_ratio)\n    dh = dh.clamp(min=-max_ratio, max=max_ratio)\n    px = ((rois[:, 0] + rois[:, 2]) * 0.5).unsqueeze(1).expand_as(dx)\n    py = ((rois[:, 1] + rois[:, 3]) * 0.5).unsqueeze(1).expand_as(dy)\n    pw = (rois[:, 2] - rois[:, 0] + 1.0).unsqueeze(1).expand_as(dw)\n    ph = (rois[:, 3] - rois[:, 1] + 1.0).unsqueeze(1).expand_as(dh)\n    gw = pw * dw.exp()\n    gh = ph * dh.exp()\n    gx = torch.addcmul(px, 1, pw, dx)  # gx = px + pw * dx\n    gy = torch.addcmul(py, 1, ph, dy)  # gy = py + ph * dy\n    x1 = gx - gw * 0.5 + 0.5\n    y1 = gy - gh * 0.5 + 0.5\n    x2 = gx + gw * 0.5 - 0.5\n    y2 = gy + gh * 0.5 - 0.5\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1] - 1)\n        y1 = y1.clamp(min=0, max=max_shape[0] - 1)\n        x2 = x2.clamp(min=0, max=max_shape[1] - 1)\n        y2 = y2.clamp(min=0, max=max_shape[0] - 1)\n    bboxes = torch.stack([x1, y1, x2, y2], dim=-1).view_as(deltas)\n    return bboxes\n\n\ndef bbox_flip(bboxes, img_shape):\n    """"""Flip bboxes horizontally.\n\n    Args:\n        bboxes(Tensor or ndarray): Shape (..., 4*k)\n        img_shape(tuple): Image shape.\n\n    Returns:\n        Same type as `bboxes`: Flipped bboxes.\n    """"""\n    if isinstance(bboxes, torch.Tensor):\n        assert bboxes.shape[-1] % 4 == 0\n        flipped = bboxes.clone()\n        flipped[:, 0::4] = img_shape[1] - bboxes[:, 2::4] - 1\n        flipped[:, 2::4] = img_shape[1] - bboxes[:, 0::4] - 1\n        return flipped\n    elif isinstance(bboxes, np.ndarray):\n        return mmcv.bbox_flip(bboxes, img_shape)\n\n\ndef bbox_mapping(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from the original image scale to testing scale""""""\n    new_bboxes = bboxes * scale_factor\n    if flip:\n        new_bboxes = bbox_flip(new_bboxes, img_shape)\n    return new_bboxes\n\n\ndef bbox_mapping_back(bboxes, img_shape, scale_factor, flip):\n    """"""Map bboxes from testing scale to original image scale""""""\n    new_bboxes = bbox_flip(bboxes, img_shape) if flip else bboxes\n    new_bboxes = new_bboxes / scale_factor\n    return new_bboxes\n\n\ndef bbox2roi(bbox_list):\n    """"""Convert a list of bboxes to roi format.\n\n    Args:\n        bbox_list (list[Tensor]): a list of bboxes corresponding to a batch\n            of images.\n\n    Returns:\n        Tensor: shape (n, 5), [batch_ind, x1, y1, x2, y2]\n    """"""\n    rois_list = []\n    for img_id, bboxes in enumerate(bbox_list):\n        if bboxes.size(0) > 0:\n            img_inds = bboxes.new_full((bboxes.size(0), 1), img_id)\n            rois = torch.cat([img_inds, bboxes[:, :4]], dim=-1)\n        else:\n            rois = bboxes.new_zeros((0, 5))\n        rois_list.append(rois)\n    rois = torch.cat(rois_list, 0)\n    return rois\n\n\ndef roi2bbox(rois):\n    bbox_list = []\n    img_ids = torch.unique(rois[:, 0].cpu(), sorted=True)\n    for img_id in img_ids:\n        inds = (rois[:, 0] == img_id.item())\n        bbox = rois[inds, 1:]\n        bbox_list.append(bbox)\n    return bbox_list\n\n\ndef bbox2result(bboxes, labels, num_classes):\n    """"""Convert detection results to a list of numpy arrays.\n\n    Args:\n        bboxes (Tensor): shape (n, 5)\n        labels (Tensor): shape (n, )\n        num_classes (int): class number, including background class\n\n    Returns:\n        list(ndarray): bbox results of each class\n    """"""\n    if bboxes.shape[0] == 0:\n        return [\n            np.zeros((0, 5), dtype=np.float32) for i in range(num_classes - 1)\n        ]\n    else:\n        bboxes = bboxes.cpu().numpy()\n        labels = labels.cpu().numpy()\n        return [bboxes[labels == i, :] for i in range(num_classes - 1)]\n\n\ndef distance2bbox(points, distance, max_shape=None):\n    """"""Decode distance prediction to bounding box.\n\n    Args:\n        points (Tensor): Shape (n, 2), [x, y].\n        distance (Tensor): Distance from the given point to 4\n            boundaries (left, top, right, bottom).\n        max_shape (tuple): Shape of the image.\n\n    Returns:\n        Tensor: Decoded bboxes.\n    """"""\n    x1 = points[:, 0] - distance[:, 0]\n    y1 = points[:, 1] - distance[:, 1]\n    x2 = points[:, 0] + distance[:, 2]\n    y2 = points[:, 1] + distance[:, 3]\n    if max_shape is not None:\n        x1 = x1.clamp(min=0, max=max_shape[1] - 1)\n        y1 = y1.clamp(min=0, max=max_shape[0] - 1)\n        x2 = x2.clamp(min=0, max=max_shape[1] - 1)\n        y2 = y2.clamp(min=0, max=max_shape[0] - 1)\n    return torch.stack([x1, y1, x2, y2], -1)\n'"
mmdet/core/evaluation/__init__.py,0,"b""from .class_names import (voc_classes, imagenet_det_classes,\n                          imagenet_vid_classes, coco_classes, dataset_aliases,\n                          get_classes)\nfrom .coco_utils import coco_eval, fast_eval_recall, results2json\nfrom .eval_hooks import (DistEvalHook, DistEvalmAPHook, CocoDistEvalRecallHook,\n                         CocoDistEvalmAPHook)\nfrom .mean_ap import average_precision, eval_map, print_map_summary\nfrom .recall import (eval_recalls, print_recall_summary, plot_num_recall,\n                     plot_iou_recall)\n\n__all__ = [\n    'voc_classes', 'imagenet_det_classes', 'imagenet_vid_classes',\n    'coco_classes', 'dataset_aliases', 'get_classes', 'coco_eval',\n    'fast_eval_recall', 'results2json', 'DistEvalHook', 'DistEvalmAPHook',\n    'CocoDistEvalRecallHook', 'CocoDistEvalmAPHook', 'average_precision',\n    'eval_map', 'print_map_summary', 'eval_recalls', 'print_recall_summary',\n    'plot_num_recall', 'plot_iou_recall'\n]\n"""
mmdet/core/evaluation/bbox_overlaps.py,0,"b'import numpy as np\n\n\ndef bbox_overlaps(bboxes1, bboxes2, mode=\'iou\'):\n    """"""Calculate the ious between each bbox of bboxes1 and bboxes2.\n\n    Args:\n        bboxes1(ndarray): shape (n, 4)\n        bboxes2(ndarray): shape (k, 4)\n        mode(str): iou (intersection over union) or iof (intersection\n            over foreground)\n\n    Returns:\n        ious(ndarray): shape (n, k)\n    """"""\n\n    assert mode in [\'iou\', \'iof\']\n\n    bboxes1 = bboxes1.astype(np.float32)\n    bboxes2 = bboxes2.astype(np.float32)\n    rows = bboxes1.shape[0]\n    cols = bboxes2.shape[0]\n    ious = np.zeros((rows, cols), dtype=np.float32)\n    if rows * cols == 0:\n        return ious\n    exchange = False\n    if bboxes1.shape[0] > bboxes2.shape[0]:\n        bboxes1, bboxes2 = bboxes2, bboxes1\n        ious = np.zeros((cols, rows), dtype=np.float32)\n        exchange = True\n    area1 = (bboxes1[:, 2] - bboxes1[:, 0] + 1) * (\n        bboxes1[:, 3] - bboxes1[:, 1] + 1)\n    area2 = (bboxes2[:, 2] - bboxes2[:, 0] + 1) * (\n        bboxes2[:, 3] - bboxes2[:, 1] + 1)\n    for i in range(bboxes1.shape[0]):\n        x_start = np.maximum(bboxes1[i, 0], bboxes2[:, 0])\n        y_start = np.maximum(bboxes1[i, 1], bboxes2[:, 1])\n        x_end = np.minimum(bboxes1[i, 2], bboxes2[:, 2])\n        y_end = np.minimum(bboxes1[i, 3], bboxes2[:, 3])\n        overlap = np.maximum(x_end - x_start + 1, 0) * np.maximum(\n            y_end - y_start + 1, 0)\n        if mode == \'iou\':\n            union = area1[i] + area2 - overlap\n        else:\n            union = area1[i] if not exchange else area2\n        ious[i, :] = overlap / union\n    if exchange:\n        ious = ious.T\n    return ious\n'"
mmdet/core/evaluation/class_names.py,0,"b'import mmcv\n\n\ndef wider_face_classes():\n    return [\'face\']\n\n\ndef voc_classes():\n    return [\n        \'aeroplane\', \'bicycle\', \'bird\', \'boat\', \'bottle\', \'bus\', \'car\', \'cat\',\n        \'chair\', \'cow\', \'diningtable\', \'dog\', \'horse\', \'motorbike\', \'person\',\n        \'pottedplant\', \'sheep\', \'sofa\', \'train\', \'tvmonitor\'\n    ]\n\n\ndef imagenet_det_classes():\n    return [\n        \'accordion\', \'airplane\', \'ant\', \'antelope\', \'apple\', \'armadillo\',\n        \'artichoke\', \'axe\', \'baby_bed\', \'backpack\', \'bagel\', \'balance_beam\',\n        \'banana\', \'band_aid\', \'banjo\', \'baseball\', \'basketball\', \'bathing_cap\',\n        \'beaker\', \'bear\', \'bee\', \'bell_pepper\', \'bench\', \'bicycle\', \'binder\',\n        \'bird\', \'bookshelf\', \'bow_tie\', \'bow\', \'bowl\', \'brassiere\', \'burrito\',\n        \'bus\', \'butterfly\', \'camel\', \'can_opener\', \'car\', \'cart\', \'cattle\',\n        \'cello\', \'centipede\', \'chain_saw\', \'chair\', \'chime\', \'cocktail_shaker\',\n        \'coffee_maker\', \'computer_keyboard\', \'computer_mouse\', \'corkscrew\',\n        \'cream\', \'croquet_ball\', \'crutch\', \'cucumber\', \'cup_or_mug\', \'diaper\',\n        \'digital_clock\', \'dishwasher\', \'dog\', \'domestic_cat\', \'dragonfly\',\n        \'drum\', \'dumbbell\', \'electric_fan\', \'elephant\', \'face_powder\', \'fig\',\n        \'filing_cabinet\', \'flower_pot\', \'flute\', \'fox\', \'french_horn\', \'frog\',\n        \'frying_pan\', \'giant_panda\', \'goldfish\', \'golf_ball\', \'golfcart\',\n        \'guacamole\', \'guitar\', \'hair_dryer\', \'hair_spray\', \'hamburger\',\n        \'hammer\', \'hamster\', \'harmonica\', \'harp\', \'hat_with_a_wide_brim\',\n        \'head_cabbage\', \'helmet\', \'hippopotamus\', \'horizontal_bar\', \'horse\',\n        \'hotdog\', \'iPod\', \'isopod\', \'jellyfish\', \'koala_bear\', \'ladle\',\n        \'ladybug\', \'lamp\', \'laptop\', \'lemon\', \'lion\', \'lipstick\', \'lizard\',\n        \'lobster\', \'maillot\', \'maraca\', \'microphone\', \'microwave\', \'milk_can\',\n        \'miniskirt\', \'monkey\', \'motorcycle\', \'mushroom\', \'nail\', \'neck_brace\',\n        \'oboe\', \'orange\', \'otter\', \'pencil_box\', \'pencil_sharpener\', \'perfume\',\n        \'person\', \'piano\', \'pineapple\', \'ping-pong_ball\', \'pitcher\', \'pizza\',\n        \'plastic_bag\', \'plate_rack\', \'pomegranate\', \'popsicle\', \'porcupine\',\n        \'power_drill\', \'pretzel\', \'printer\', \'puck\', \'punching_bag\', \'purse\',\n        \'rabbit\', \'racket\', \'ray\', \'red_panda\', \'refrigerator\',\n        \'remote_control\', \'rubber_eraser\', \'rugby_ball\', \'ruler\',\n        \'salt_or_pepper_shaker\', \'saxophone\', \'scorpion\', \'screwdriver\',\n        \'seal\', \'sheep\', \'ski\', \'skunk\', \'snail\', \'snake\', \'snowmobile\',\n        \'snowplow\', \'soap_dispenser\', \'soccer_ball\', \'sofa\', \'spatula\',\n        \'squirrel\', \'starfish\', \'stethoscope\', \'stove\', \'strainer\',\n        \'strawberry\', \'stretcher\', \'sunglasses\', \'swimming_trunks\', \'swine\',\n        \'syringe\', \'table\', \'tape_player\', \'tennis_ball\', \'tick\', \'tie\',\n        \'tiger\', \'toaster\', \'traffic_light\', \'train\', \'trombone\', \'trumpet\',\n        \'turtle\', \'tv_or_monitor\', \'unicycle\', \'vacuum\', \'violin\',\n        \'volleyball\', \'waffle_iron\', \'washer\', \'water_bottle\', \'watercraft\',\n        \'whale\', \'wine_bottle\', \'zebra\'\n    ]\n\n\ndef imagenet_vid_classes():\n    return [\n        \'airplane\', \'antelope\', \'bear\', \'bicycle\', \'bird\', \'bus\', \'car\',\n        \'cattle\', \'dog\', \'domestic_cat\', \'elephant\', \'fox\', \'giant_panda\',\n        \'hamster\', \'horse\', \'lion\', \'lizard\', \'monkey\', \'motorcycle\', \'rabbit\',\n        \'red_panda\', \'sheep\', \'snake\', \'squirrel\', \'tiger\', \'train\', \'turtle\',\n        \'watercraft\', \'whale\', \'zebra\'\n    ]\n\n\ndef coco_classes():\n    return [\n        \'person\', \'bicycle\', \'car\', \'motorcycle\', \'airplane\', \'bus\', \'train\',\n        \'truck\', \'boat\', \'traffic_light\', \'fire_hydrant\', \'stop_sign\',\n        \'parking_meter\', \'bench\', \'bird\', \'cat\', \'dog\', \'horse\', \'sheep\',\n        \'cow\', \'elephant\', \'bear\', \'zebra\', \'giraffe\', \'backpack\', \'umbrella\',\n        \'handbag\', \'tie\', \'suitcase\', \'frisbee\', \'skis\', \'snowboard\',\n        \'sports_ball\', \'kite\', \'baseball_bat\', \'baseball_glove\', \'skateboard\',\n        \'surfboard\', \'tennis_racket\', \'bottle\', \'wine_glass\', \'cup\', \'fork\',\n        \'knife\', \'spoon\', \'bowl\', \'banana\', \'apple\', \'sandwich\', \'orange\',\n        \'broccoli\', \'carrot\', \'hot_dog\', \'pizza\', \'donut\', \'cake\', \'chair\',\n        \'couch\', \'potted_plant\', \'bed\', \'dining_table\', \'toilet\', \'tv\',\n        \'laptop\', \'mouse\', \'remote\', \'keyboard\', \'cell_phone\', \'microwave\',\n        \'oven\', \'toaster\', \'sink\', \'refrigerator\', \'book\', \'clock\', \'vase\',\n        \'scissors\', \'teddy_bear\', \'hair_drier\', \'toothbrush\'\n    ]\n\n\ndataset_aliases = {\n    \'voc\': [\'voc\', \'pascal_voc\', \'voc07\', \'voc12\'],\n    \'imagenet_det\': [\'det\', \'imagenet_det\', \'ilsvrc_det\'],\n    \'imagenet_vid\': [\'vid\', \'imagenet_vid\', \'ilsvrc_vid\'],\n    \'coco\': [\'coco\', \'mscoco\', \'ms_coco\'],\n    \'wider_face\': [\'WIDERFaceDataset\', \'wider_face\', \'WDIERFace\']\n}\n\n\ndef get_classes(dataset):\n    """"""Get class names of a dataset.""""""\n    alias2name = {}\n    for name, aliases in dataset_aliases.items():\n        for alias in aliases:\n            alias2name[alias] = name\n\n    if mmcv.is_str(dataset):\n        if dataset in alias2name:\n            labels = eval(alias2name[dataset] + \'_classes()\')\n        else:\n            raise ValueError(\'Unrecognized dataset: {}\'.format(dataset))\n    else:\n        raise TypeError(\'dataset must a str, but got {}\'.format(type(dataset)))\n    return labels\n'"
mmdet/core/evaluation/coco_utils.py,0,"b""import mmcv\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom pycocotools.cocoeval import COCOeval\n\nfrom .recall import eval_recalls\n\n\ndef coco_eval(result_files, result_types, coco, max_dets=(100, 300, 1000)):\n    for res_type in result_types:\n        assert res_type in [\n            'proposal', 'proposal_fast', 'bbox', 'segm', 'keypoints'\n        ]\n\n    if mmcv.is_str(coco):\n        coco = COCO(coco)\n    assert isinstance(coco, COCO)\n\n    if result_types == ['proposal_fast']:\n        ar = fast_eval_recall(result_files, coco, np.array(max_dets))\n        for i, num in enumerate(max_dets):\n            print('AR@{}\\t= {:.4f}'.format(num, ar[i]))\n        return\n\n    for res_type in result_types:\n        result_file = result_files[res_type]\n        assert result_file.endswith('.json')\n\n        coco_dets = coco.loadRes(result_file)\n        img_ids = coco.getImgIds()\n        iou_type = 'bbox' if res_type == 'proposal' else res_type\n        cocoEval = COCOeval(coco, coco_dets, iou_type)\n        cocoEval.params.imgIds = img_ids\n        if res_type == 'proposal':\n            cocoEval.params.useCats = 0\n            cocoEval.params.maxDets = list(max_dets)\n        cocoEval.evaluate()\n        cocoEval.accumulate()\n        cocoEval.summarize()\n\n\ndef fast_eval_recall(results,\n                     coco,\n                     max_dets,\n                     iou_thrs=np.arange(0.5, 0.96, 0.05)):\n    if mmcv.is_str(results):\n        assert results.endswith('.pkl')\n        results = mmcv.load(results)\n    elif not isinstance(results, list):\n        raise TypeError(\n            'results must be a list of numpy arrays or a filename, not {}'.\n            format(type(results)))\n\n    gt_bboxes = []\n    img_ids = coco.getImgIds()\n    for i in range(len(img_ids)):\n        ann_ids = coco.getAnnIds(imgIds=img_ids[i])\n        ann_info = coco.loadAnns(ann_ids)\n        if len(ann_info) == 0:\n            gt_bboxes.append(np.zeros((0, 4)))\n            continue\n        bboxes = []\n        for ann in ann_info:\n            if ann.get('ignore', False) or ann['iscrowd']:\n                continue\n            x1, y1, w, h = ann['bbox']\n            bboxes.append([x1, y1, x1 + w - 1, y1 + h - 1])\n        bboxes = np.array(bboxes, dtype=np.float32)\n        if bboxes.shape[0] == 0:\n            bboxes = np.zeros((0, 4))\n        gt_bboxes.append(bboxes)\n\n    recalls = eval_recalls(\n        gt_bboxes, results, max_dets, iou_thrs, print_summary=False)\n    ar = recalls.mean(axis=1)\n    return ar\n\n\ndef xyxy2xywh(bbox):\n    _bbox = bbox.tolist()\n    return [\n        _bbox[0],\n        _bbox[1],\n        _bbox[2] - _bbox[0] + 1,\n        _bbox[3] - _bbox[1] + 1,\n    ]\n\n\ndef proposal2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        bboxes = results[idx]\n        for i in range(bboxes.shape[0]):\n            data = dict()\n            data['image_id'] = img_id\n            data['bbox'] = xyxy2xywh(bboxes[i])\n            data['score'] = float(bboxes[i][4])\n            data['category_id'] = 1\n            json_results.append(data)\n    return json_results\n\n\ndef det2json(dataset, results):\n    json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        result = results[idx]\n        for label in range(len(result)):\n            bboxes = result[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                json_results.append(data)\n    return json_results\n\n\ndef segm2json(dataset, results):\n    bbox_json_results = []\n    segm_json_results = []\n    for idx in range(len(dataset)):\n        img_id = dataset.img_ids[idx]\n        det, seg = results[idx]\n        for label in range(len(det)):\n            # bbox results\n            bboxes = det[label]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['bbox'] = xyxy2xywh(bboxes[i])\n                data['score'] = float(bboxes[i][4])\n                data['category_id'] = dataset.cat_ids[label]\n                bbox_json_results.append(data)\n\n            # segm results\n            # some detectors use different score for det and segm\n            if len(seg) == 2:\n                segms = seg[0][label]\n                mask_score = seg[1][label]\n            else:\n                segms = seg[label]\n                mask_score = [bbox[4] for bbox in bboxes]\n            for i in range(bboxes.shape[0]):\n                data = dict()\n                data['image_id'] = img_id\n                data['score'] = float(mask_score[i])\n                data['category_id'] = dataset.cat_ids[label]\n                segms[i]['counts'] = segms[i]['counts'].decode()\n                data['segmentation'] = segms[i]\n                segm_json_results.append(data)\n    return bbox_json_results, segm_json_results\n\n\ndef results2json(dataset, results, out_file):\n    result_files = dict()\n    if isinstance(results[0], list):\n        json_results = det2json(dataset, results)\n        result_files['bbox'] = '{}.{}.json'.format(out_file, 'bbox')\n        result_files['proposal'] = '{}.{}.json'.format(out_file, 'bbox')\n        mmcv.dump(json_results, result_files['bbox'])\n    elif isinstance(results[0], tuple):\n        json_results = segm2json(dataset, results)\n        result_files['bbox'] = '{}.{}.json'.format(out_file, 'bbox')\n        result_files['proposal'] = '{}.{}.json'.format(out_file, 'bbox')\n        result_files['segm'] = '{}.{}.json'.format(out_file, 'segm')\n        mmcv.dump(json_results[0], result_files['bbox'])\n        mmcv.dump(json_results[1], result_files['segm'])\n    elif isinstance(results[0], np.ndarray):\n        json_results = proposal2json(dataset, results)\n        result_files['proposal'] = '{}.{}.json'.format(out_file, 'proposal')\n        mmcv.dump(json_results, result_files['proposal'])\n    else:\n        raise TypeError('invalid type of results')\n    return result_files\n"""
mmdet/core/evaluation/eval_hooks.py,4,"b""import os\nimport os.path as osp\n\nimport mmcv\nimport numpy as np\nimport torch\nimport torch.distributed as dist\nfrom mmcv.runner import Hook, obj_from_dict\nfrom mmcv.parallel import scatter, collate\nfrom pycocotools.cocoeval import COCOeval\nfrom torch.utils.data import Dataset\n\nfrom .coco_utils import results2json, fast_eval_recall\nfrom .mean_ap import eval_map\nfrom mmdet import datasets\n\n\nclass DistEvalHook(Hook):\n\n    def __init__(self, dataset, interval=1):\n        if isinstance(dataset, Dataset):\n            self.dataset = dataset\n        elif isinstance(dataset, dict):\n            self.dataset = obj_from_dict(dataset, datasets,\n                                         {'test_mode': True})\n        else:\n            raise TypeError(\n                'dataset must be a Dataset object or a dict, not {}'.format(\n                    type(dataset)))\n        self.interval = interval\n\n    def after_train_epoch(self, runner):\n        if not self.every_n_epochs(runner, self.interval):\n            return\n        runner.model.eval()\n        results = [None for _ in range(len(self.dataset))]\n        if runner.rank == 0:\n            prog_bar = mmcv.ProgressBar(len(self.dataset))\n        for idx in range(runner.rank, len(self.dataset), runner.world_size):\n            data = self.dataset[idx]\n            data_gpu = scatter(\n                collate([data], samples_per_gpu=1),\n                [torch.cuda.current_device()])[0]\n\n            # compute output\n            with torch.no_grad():\n                result = runner.model(\n                    return_loss=False, rescale=True, **data_gpu)\n            results[idx] = result\n\n            batch_size = runner.world_size\n            if runner.rank == 0:\n                for _ in range(batch_size):\n                    prog_bar.update()\n\n        if runner.rank == 0:\n            print('\\n')\n            dist.barrier()\n            for i in range(1, runner.world_size):\n                tmp_file = osp.join(runner.work_dir, 'temp_{}.pkl'.format(i))\n                tmp_results = mmcv.load(tmp_file)\n                for idx in range(i, len(results), runner.world_size):\n                    results[idx] = tmp_results[idx]\n                os.remove(tmp_file)\n            self.evaluate(runner, results)\n        else:\n            tmp_file = osp.join(runner.work_dir,\n                                'temp_{}.pkl'.format(runner.rank))\n            mmcv.dump(results, tmp_file)\n            dist.barrier()\n        dist.barrier()\n\n    def evaluate(self):\n        raise NotImplementedError\n\n\nclass DistEvalmAPHook(DistEvalHook):\n\n    def evaluate(self, runner, results):\n        gt_bboxes = []\n        gt_labels = []\n        gt_ignore = [] if self.dataset.with_crowd else None\n        for i in range(len(self.dataset)):\n            ann = self.dataset.get_ann_info(i)\n            bboxes = ann['bboxes']\n            labels = ann['labels']\n            if gt_ignore is not None:\n                ignore = np.concatenate([\n                    np.zeros(bboxes.shape[0], dtype=np.bool),\n                    np.ones(ann['bboxes_ignore'].shape[0], dtype=np.bool)\n                ])\n                gt_ignore.append(ignore)\n                bboxes = np.vstack([bboxes, ann['bboxes_ignore']])\n                labels = np.concatenate([labels, ann['labels_ignore']])\n            gt_bboxes.append(bboxes)\n            gt_labels.append(labels)\n        # If the dataset is VOC2007, then use 11 points mAP evaluation.\n        if hasattr(self.dataset, 'year') and self.dataset.year == 2007:\n            ds_name = 'voc07'\n        else:\n            ds_name = self.dataset.CLASSES\n        mean_ap, eval_results = eval_map(\n            results,\n            gt_bboxes,\n            gt_labels,\n            gt_ignore=gt_ignore,\n            scale_ranges=None,\n            iou_thr=0.5,\n            dataset=ds_name,\n            print_summary=True)\n        runner.log_buffer.output['mAP'] = mean_ap\n        runner.log_buffer.ready = True\n\n\nclass CocoDistEvalRecallHook(DistEvalHook):\n\n    def __init__(self,\n                 dataset,\n                 interval=1,\n                 proposal_nums=(100, 300, 1000),\n                 iou_thrs=np.arange(0.5, 0.96, 0.05)):\n        super(CocoDistEvalRecallHook, self).__init__(\n            dataset, interval=interval)\n        self.proposal_nums = np.array(proposal_nums, dtype=np.int32)\n        self.iou_thrs = np.array(iou_thrs, dtype=np.float32)\n\n    def evaluate(self, runner, results):\n        # the official coco evaluation is too slow, here we use our own\n        # implementation instead, which may get slightly different results\n        ar = fast_eval_recall(results, self.dataset.coco, self.proposal_nums,\n                              self.iou_thrs)\n        for i, num in enumerate(self.proposal_nums):\n            runner.log_buffer.output['AR@{}'.format(num)] = ar[i]\n        runner.log_buffer.ready = True\n\n\nclass CocoDistEvalmAPHook(DistEvalHook):\n\n    def evaluate(self, runner, results):\n        tmp_file = osp.join(runner.work_dir, 'temp_0')\n        result_files = results2json(self.dataset, results, tmp_file)\n\n        res_types = ['bbox', 'segm'\n                     ] if runner.model.module.with_mask else ['bbox']\n        cocoGt = self.dataset.coco\n        imgIds = cocoGt.getImgIds()\n        for res_type in res_types:\n            cocoDt = cocoGt.loadRes(result_files[res_type])\n            iou_type = res_type\n            cocoEval = COCOeval(cocoGt, cocoDt, iou_type)\n            cocoEval.params.imgIds = imgIds\n            cocoEval.evaluate()\n            cocoEval.accumulate()\n            cocoEval.summarize()\n            metrics = ['mAP', 'mAP_50', 'mAP_75', 'mAP_s', 'mAP_m', 'mAP_l']\n            for i in range(len(metrics)):\n                key = '{}_{}'.format(res_type, metrics[i])\n                val = float('{:.3f}'.format(cocoEval.stats[i]))\n                runner.log_buffer.output[key] = val\n            runner.log_buffer.output['{}_mAP_copypaste'.format(res_type)] = (\n                '{ap[0]:.3f} {ap[1]:.3f} {ap[2]:.3f} {ap[3]:.3f} '\n                '{ap[4]:.3f} {ap[5]:.3f}').format(ap=cocoEval.stats[:6])\n        runner.log_buffer.ready = True\n        for res_type in res_types:\n            os.remove(result_files[res_type])\n"""
mmdet/core/evaluation/mean_ap.py,0,"b'import mmcv\nimport numpy as np\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\nfrom .class_names import get_classes\n\n\ndef average_precision(recalls, precisions, mode=\'area\'):\n    """"""Calculate average precision (for single or multiple scales).\n\n    Args:\n        recalls (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        precisions (ndarray): shape (num_scales, num_dets) or (num_dets, )\n        mode (str): \'area\' or \'11points\', \'area\' means calculating the area\n            under precision-recall curve, \'11points\' means calculating\n            the average precision of recalls at [0, 0.1, ..., 1]\n\n    Returns:\n        float or ndarray: calculated average precision\n    """"""\n    no_scale = False\n    if recalls.ndim == 1:\n        no_scale = True\n        recalls = recalls[np.newaxis, :]\n        precisions = precisions[np.newaxis, :]\n    assert recalls.shape == precisions.shape and recalls.ndim == 2\n    num_scales = recalls.shape[0]\n    ap = np.zeros(num_scales, dtype=np.float32)\n    if mode == \'area\':\n        zeros = np.zeros((num_scales, 1), dtype=recalls.dtype)\n        ones = np.ones((num_scales, 1), dtype=recalls.dtype)\n        mrec = np.hstack((zeros, recalls, ones))\n        mpre = np.hstack((zeros, precisions, zeros))\n        for i in range(mpre.shape[1] - 1, 0, -1):\n            mpre[:, i - 1] = np.maximum(mpre[:, i - 1], mpre[:, i])\n        for i in range(num_scales):\n            ind = np.where(mrec[i, 1:] != mrec[i, :-1])[0]\n            ap[i] = np.sum(\n                (mrec[i, ind + 1] - mrec[i, ind]) * mpre[i, ind + 1])\n    elif mode == \'11points\':\n        for i in range(num_scales):\n            for thr in np.arange(0, 1 + 1e-3, 0.1):\n                precs = precisions[i, recalls[i, :] >= thr]\n                prec = precs.max() if precs.size > 0 else 0\n                ap[i] += prec\n            ap /= 11\n    else:\n        raise ValueError(\n            \'Unrecognized mode, only ""area"" and ""11points"" are supported\')\n    if no_scale:\n        ap = ap[0]\n    return ap\n\n\ndef tpfp_imagenet(det_bboxes,\n                  gt_bboxes,\n                  gt_ignore,\n                  default_iou_thr,\n                  area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): the detected bbox\n        gt_bboxes (ndarray): ground truth bboxes of this image\n        gt_ignore (ndarray): indicate if gts are ignored for evaluation or not\n        default_iou_thr (float): the iou thresholds for medium and large bboxes\n        area_ranges (list or None): gt bbox area ranges\n\n    Returns:\n        tuple: two arrays (tp, fp) whose elements are 0 and 1\n    """"""\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp\n    # of a certain scale.\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0] + 1) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1] + 1)\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n    ious = bbox_overlaps(det_bboxes, gt_bboxes - 1)\n    gt_w = gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1\n    gt_h = gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1\n    iou_thrs = np.minimum((gt_w * gt_h) / ((gt_w + 10.0) * (gt_h + 10.0)),\n                          default_iou_thr)\n    # sort all detections by scores in descending order\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore, dtype=bool)\n        else:\n            gt_areas = gt_w * gt_h\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            max_iou = -1\n            matched_gt = -1\n            # find best overlapped available gt\n            for j in range(num_gts):\n                # different from PASCAL VOC: allow finding other gts if the\n                # best overlaped ones are already matched by other det bboxes\n                if gt_covered[j]:\n                    continue\n                elif ious[i, j] >= iou_thrs[j] and ious[i, j] > max_iou:\n                    max_iou = ious[i, j]\n                    matched_gt = j\n            # there are 4 cases for a det bbox:\n            # 1. it matches a gt, tp = 1, fp = 0\n            # 2. it matches an ignored gt, tp = 0, fp = 0\n            # 3. it matches no gt and within area range, tp = 0, fp = 1\n            # 4. it matches no gt but is beyond area range, tp = 0, fp = 0\n            if matched_gt >= 0:\n                gt_covered[matched_gt] = 1\n                if not (gt_ignore[matched_gt] or gt_area_ignore[matched_gt]):\n                    tp[k, i] = 1\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0] + 1) * (bbox[3] - bbox[1] + 1)\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef tpfp_default(det_bboxes, gt_bboxes, gt_ignore, iou_thr, area_ranges=None):\n    """"""Check if detected bboxes are true positive or false positive.\n\n    Args:\n        det_bbox (ndarray): the detected bbox\n        gt_bboxes (ndarray): ground truth bboxes of this image\n        gt_ignore (ndarray): indicate if gts are ignored for evaluation or not\n        iou_thr (float): the iou thresholds\n\n    Returns:\n        tuple: (tp, fp), two arrays whose elements are 0 and 1\n    """"""\n    num_dets = det_bboxes.shape[0]\n    num_gts = gt_bboxes.shape[0]\n    if area_ranges is None:\n        area_ranges = [(None, None)]\n    num_scales = len(area_ranges)\n    # tp and fp are of shape (num_scales, num_gts), each row is tp or fp of\n    # a certain scale\n    tp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    fp = np.zeros((num_scales, num_dets), dtype=np.float32)\n    # if there is no gt bboxes in this image, then all det bboxes\n    # within area range are false positives\n    if gt_bboxes.shape[0] == 0:\n        if area_ranges == [(None, None)]:\n            fp[...] = 1\n        else:\n            det_areas = (det_bboxes[:, 2] - det_bboxes[:, 0] + 1) * (\n                det_bboxes[:, 3] - det_bboxes[:, 1] + 1)\n            for i, (min_area, max_area) in enumerate(area_ranges):\n                fp[i, (det_areas >= min_area) & (det_areas < max_area)] = 1\n        return tp, fp\n    ious = bbox_overlaps(det_bboxes, gt_bboxes)\n    ious_max = ious.max(axis=1)\n    ious_argmax = ious.argmax(axis=1)\n    sort_inds = np.argsort(-det_bboxes[:, -1])\n    for k, (min_area, max_area) in enumerate(area_ranges):\n        gt_covered = np.zeros(num_gts, dtype=bool)\n        # if no area range is specified, gt_area_ignore is all False\n        if min_area is None:\n            gt_area_ignore = np.zeros_like(gt_ignore, dtype=bool)\n        else:\n            gt_areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * (\n                gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1)\n            gt_area_ignore = (gt_areas < min_area) | (gt_areas >= max_area)\n        for i in sort_inds:\n            if ious_max[i] >= iou_thr:\n                matched_gt = ious_argmax[i]\n                if not (gt_ignore[matched_gt] or gt_area_ignore[matched_gt]):\n                    if not gt_covered[matched_gt]:\n                        gt_covered[matched_gt] = True\n                        tp[k, i] = 1\n                    else:\n                        fp[k, i] = 1\n                # otherwise ignore this detected bbox, tp = 0, fp = 0\n            elif min_area is None:\n                fp[k, i] = 1\n            else:\n                bbox = det_bboxes[i, :4]\n                area = (bbox[2] - bbox[0] + 1) * (bbox[3] - bbox[1] + 1)\n                if area >= min_area and area < max_area:\n                    fp[k, i] = 1\n    return tp, fp\n\n\ndef get_cls_results(det_results, gt_bboxes, gt_labels, gt_ignore, class_id):\n    """"""Get det results and gt information of a certain class.""""""\n    cls_dets = [det[class_id]\n                for det in det_results]  # det bboxes of this class\n    cls_gts = []  # gt bboxes of this class\n    cls_gt_ignore = []\n    for j in range(len(gt_bboxes)):\n        gt_bbox = gt_bboxes[j]\n        cls_inds = (gt_labels[j] == class_id + 1)\n        cls_gt = gt_bbox[cls_inds, :] if gt_bbox.shape[0] > 0 else gt_bbox\n        cls_gts.append(cls_gt)\n        if gt_ignore is None:\n            cls_gt_ignore.append(np.zeros(cls_gt.shape[0], dtype=np.int32))\n        else:\n            cls_gt_ignore.append(gt_ignore[j][cls_inds])\n    return cls_dets, cls_gts, cls_gt_ignore\n\n\ndef eval_map(det_results,\n             gt_bboxes,\n             gt_labels,\n             gt_ignore=None,\n             scale_ranges=None,\n             iou_thr=0.5,\n             dataset=None,\n             print_summary=True):\n    """"""Evaluate mAP of a dataset.\n\n    Args:\n        det_results (list): a list of list, [[cls1_det, cls2_det, ...], ...]\n        gt_bboxes (list): ground truth bboxes of each image, a list of K*4\n            array.\n        gt_labels (list): ground truth labels of each image, a list of K array\n        gt_ignore (list): gt ignore indicators of each image, a list of K array\n        scale_ranges (list, optional): [(min1, max1), (min2, max2), ...]\n        iou_thr (float): IoU threshold\n        dataset (None or str or list): dataset name or dataset classes, there\n            are minor differences in metrics for different datsets, e.g.\n            ""voc07"", ""imagenet_det"", etc.\n        print_summary (bool): whether to print the mAP summary\n\n    Returns:\n        tuple: (mAP, [dict, dict, ...])\n    """"""\n    assert len(det_results) == len(gt_bboxes) == len(gt_labels)\n    if gt_ignore is not None:\n        assert len(gt_ignore) == len(gt_labels)\n        for i in range(len(gt_ignore)):\n            assert len(gt_labels[i]) == len(gt_ignore[i])\n    area_ranges = ([(rg[0]**2, rg[1]**2) for rg in scale_ranges]\n                   if scale_ranges is not None else None)\n    num_scales = len(scale_ranges) if scale_ranges is not None else 1\n    eval_results = []\n    num_classes = len(det_results[0])  # positive class num\n    gt_labels = [\n        label if label.ndim == 1 else label[:, 0] for label in gt_labels\n    ]\n    for i in range(num_classes):\n        # get gt and det bboxes of this class\n        cls_dets, cls_gts, cls_gt_ignore = get_cls_results(\n            det_results, gt_bboxes, gt_labels, gt_ignore, i)\n        # calculate tp and fp for each image\n        tpfp_func = (\n            tpfp_imagenet if dataset in [\'det\', \'vid\'] else tpfp_default)\n        tpfp = [\n            tpfp_func(cls_dets[j], cls_gts[j], cls_gt_ignore[j], iou_thr,\n                      area_ranges) for j in range(len(cls_dets))\n        ]\n        tp, fp = tuple(zip(*tpfp))\n        # calculate gt number of each scale, gts ignored or beyond scale\n        # are not counted\n        num_gts = np.zeros(num_scales, dtype=int)\n        for j, bbox in enumerate(cls_gts):\n            if area_ranges is None:\n                num_gts[0] += np.sum(np.logical_not(cls_gt_ignore[j]))\n            else:\n                gt_areas = (bbox[:, 2] - bbox[:, 0] + 1) * (\n                    bbox[:, 3] - bbox[:, 1] + 1)\n                for k, (min_area, max_area) in enumerate(area_ranges):\n                    num_gts[k] += np.sum(\n                        np.logical_not(cls_gt_ignore[j]) &\n                        (gt_areas >= min_area) & (gt_areas < max_area))\n        # sort all det bboxes by score, also sort tp and fp\n        cls_dets = np.vstack(cls_dets)\n        num_dets = cls_dets.shape[0]\n        sort_inds = np.argsort(-cls_dets[:, -1])\n        tp = np.hstack(tp)[:, sort_inds]\n        fp = np.hstack(fp)[:, sort_inds]\n        # calculate recall and precision with tp and fp\n        tp = np.cumsum(tp, axis=1)\n        fp = np.cumsum(fp, axis=1)\n        eps = np.finfo(np.float32).eps\n        recalls = tp / np.maximum(num_gts[:, np.newaxis], eps)\n        precisions = tp / np.maximum((tp + fp), eps)\n        # calculate AP\n        if scale_ranges is None:\n            recalls = recalls[0, :]\n            precisions = precisions[0, :]\n            num_gts = num_gts.item()\n        mode = \'area\' if dataset != \'voc07\' else \'11points\'\n        ap = average_precision(recalls, precisions, mode)\n        eval_results.append({\n            \'num_gts\': num_gts,\n            \'num_dets\': num_dets,\n            \'recall\': recalls,\n            \'precision\': precisions,\n            \'ap\': ap\n        })\n    if scale_ranges is not None:\n        # shape (num_classes, num_scales)\n        all_ap = np.vstack([cls_result[\'ap\'] for cls_result in eval_results])\n        all_num_gts = np.vstack(\n            [cls_result[\'num_gts\'] for cls_result in eval_results])\n        mean_ap = [\n            all_ap[all_num_gts[:, i] > 0, i].mean()\n            if np.any(all_num_gts[:, i] > 0) else 0.0\n            for i in range(num_scales)\n        ]\n    else:\n        aps = []\n        for cls_result in eval_results:\n            if cls_result[\'num_gts\'] > 0:\n                aps.append(cls_result[\'ap\'])\n        mean_ap = np.array(aps).mean().item() if aps else 0.0\n    if print_summary:\n        print_map_summary(mean_ap, eval_results, dataset)\n\n    return mean_ap, eval_results\n\n\ndef print_map_summary(mean_ap, results, dataset=None):\n    """"""Print mAP and results of each class.\n\n    Args:\n        mean_ap(float): calculated from `eval_map`\n        results(list): calculated from `eval_map`\n        dataset(None or str or list): dataset name or dataset classes.\n    """"""\n    num_scales = len(results[0][\'ap\']) if isinstance(results[0][\'ap\'],\n                                                     np.ndarray) else 1\n    num_classes = len(results)\n\n    recalls = np.zeros((num_scales, num_classes), dtype=np.float32)\n    precisions = np.zeros((num_scales, num_classes), dtype=np.float32)\n    aps = np.zeros((num_scales, num_classes), dtype=np.float32)\n    num_gts = np.zeros((num_scales, num_classes), dtype=int)\n    for i, cls_result in enumerate(results):\n        if cls_result[\'recall\'].size > 0:\n            recalls[:, i] = np.array(cls_result[\'recall\'], ndmin=2)[:, -1]\n            precisions[:, i] = np.array(\n                cls_result[\'precision\'], ndmin=2)[:, -1]\n        aps[:, i] = cls_result[\'ap\']\n        num_gts[:, i] = cls_result[\'num_gts\']\n\n    if dataset is None:\n        label_names = [str(i) for i in range(1, num_classes + 1)]\n    elif mmcv.is_str(dataset):\n        label_names = get_classes(dataset)\n    else:\n        label_names = dataset\n\n    if not isinstance(mean_ap, list):\n        mean_ap = [mean_ap]\n    header = [\'class\', \'gts\', \'dets\', \'recall\', \'precision\', \'ap\']\n    for i in range(num_scales):\n        table_data = [header]\n        for j in range(num_classes):\n            row_data = [\n                label_names[j], num_gts[i, j], results[j][\'num_dets\'],\n                \'{:.3f}\'.format(recalls[i, j]), \'{:.3f}\'.format(\n                    precisions[i, j]), \'{:.3f}\'.format(aps[i, j])\n            ]\n            table_data.append(row_data)\n        table_data.append([\'mAP\', \'\', \'\', \'\', \'\', \'{:.3f}\'.format(mean_ap[i])])\n        table = AsciiTable(table_data)\n        table.inner_footing_row_border = True\n        print(table.table)\n'"
mmdet/core/evaluation/recall.py,0,"b'import numpy as np\nfrom terminaltables import AsciiTable\n\nfrom .bbox_overlaps import bbox_overlaps\n\n\ndef _recalls(all_ious, proposal_nums, thrs):\n\n    img_num = all_ious.shape[0]\n    total_gt_num = sum([ious.shape[0] for ious in all_ious])\n\n    _ious = np.zeros((proposal_nums.size, total_gt_num), dtype=np.float32)\n    for k, proposal_num in enumerate(proposal_nums):\n        tmp_ious = np.zeros(0)\n        for i in range(img_num):\n            ious = all_ious[i][:, :proposal_num].copy()\n            gt_ious = np.zeros((ious.shape[0]))\n            if ious.size == 0:\n                tmp_ious = np.hstack((tmp_ious, gt_ious))\n                continue\n            for j in range(ious.shape[0]):\n                gt_max_overlaps = ious.argmax(axis=1)\n                max_ious = ious[np.arange(0, ious.shape[0]), gt_max_overlaps]\n                gt_idx = max_ious.argmax()\n                gt_ious[j] = max_ious[gt_idx]\n                box_idx = gt_max_overlaps[gt_idx]\n                ious[gt_idx, :] = -1\n                ious[:, box_idx] = -1\n            tmp_ious = np.hstack((tmp_ious, gt_ious))\n        _ious[k, :] = tmp_ious\n\n    _ious = np.fliplr(np.sort(_ious, axis=1))\n    recalls = np.zeros((proposal_nums.size, thrs.size))\n    for i, thr in enumerate(thrs):\n        recalls[:, i] = (_ious >= thr).sum(axis=1) / float(total_gt_num)\n\n    return recalls\n\n\ndef set_recall_param(proposal_nums, iou_thrs):\n    """"""Check proposal_nums and iou_thrs and set correct format.\n    """"""\n    if isinstance(proposal_nums, list):\n        _proposal_nums = np.array(proposal_nums)\n    elif isinstance(proposal_nums, int):\n        _proposal_nums = np.array([proposal_nums])\n    else:\n        _proposal_nums = proposal_nums\n\n    if iou_thrs is None:\n        _iou_thrs = np.array([0.5])\n    elif isinstance(iou_thrs, list):\n        _iou_thrs = np.array(iou_thrs)\n    elif isinstance(iou_thrs, float):\n        _iou_thrs = np.array([iou_thrs])\n    else:\n        _iou_thrs = iou_thrs\n\n    return _proposal_nums, _iou_thrs\n\n\ndef eval_recalls(gts,\n                 proposals,\n                 proposal_nums=None,\n                 iou_thrs=None,\n                 print_summary=True):\n    """"""Calculate recalls.\n\n    Args:\n        gts(list or ndarray): a list of arrays of shape (n, 4)\n        proposals(list or ndarray): a list of arrays of shape (k, 4) or (k, 5)\n        proposal_nums(int or list of int or ndarray): top N proposals\n        thrs(float or list or ndarray): iou thresholds\n\n    Returns:\n        ndarray: recalls of different ious and proposal nums\n    """"""\n\n    img_num = len(gts)\n    assert img_num == len(proposals)\n\n    proposal_nums, iou_thrs = set_recall_param(proposal_nums, iou_thrs)\n\n    all_ious = []\n    for i in range(img_num):\n        if proposals[i].ndim == 2 and proposals[i].shape[1] == 5:\n            scores = proposals[i][:, 4]\n            sort_idx = np.argsort(scores)[::-1]\n            img_proposal = proposals[i][sort_idx, :]\n        else:\n            img_proposal = proposals[i]\n        prop_num = min(img_proposal.shape[0], proposal_nums[-1])\n        if gts[i] is None or gts[i].shape[0] == 0:\n            ious = np.zeros((0, img_proposal.shape[0]), dtype=np.float32)\n        else:\n            ious = bbox_overlaps(gts[i], img_proposal[:prop_num, :4])\n        all_ious.append(ious)\n    all_ious = np.array(all_ious)\n    recalls = _recalls(all_ious, proposal_nums, iou_thrs)\n    if print_summary:\n        print_recall_summary(recalls, proposal_nums, iou_thrs)\n    return recalls\n\n\ndef print_recall_summary(recalls,\n                         proposal_nums,\n                         iou_thrs,\n                         row_idxs=None,\n                         col_idxs=None):\n    """"""Print recalls in a table.\n\n    Args:\n        recalls(ndarray): calculated from `bbox_recalls`\n        proposal_nums(ndarray or list): top N proposals\n        iou_thrs(ndarray or list): iou thresholds\n        row_idxs(ndarray): which rows(proposal nums) to print\n        col_idxs(ndarray): which cols(iou thresholds) to print\n    """"""\n    proposal_nums = np.array(proposal_nums, dtype=np.int32)\n    iou_thrs = np.array(iou_thrs)\n    if row_idxs is None:\n        row_idxs = np.arange(proposal_nums.size)\n    if col_idxs is None:\n        col_idxs = np.arange(iou_thrs.size)\n    row_header = [\'\'] + iou_thrs[col_idxs].tolist()\n    table_data = [row_header]\n    for i, num in enumerate(proposal_nums[row_idxs]):\n        row = [\n            \'{:.3f}\'.format(val)\n            for val in recalls[row_idxs[i], col_idxs].tolist()\n        ]\n        row.insert(0, num)\n        table_data.append(row)\n    table = AsciiTable(table_data)\n    print(table.table)\n\n\ndef plot_num_recall(recalls, proposal_nums):\n    """"""Plot Proposal_num-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        proposal_nums(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(proposal_nums, np.ndarray):\n        _proposal_nums = proposal_nums.tolist()\n    else:\n        _proposal_nums = proposal_nums\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot([0] + _proposal_nums, [0] + _recalls)\n    plt.xlabel(\'Proposal num\')\n    plt.ylabel(\'Recall\')\n    plt.axis([0, proposal_nums.max(), 0, 1])\n    f.show()\n\n\ndef plot_iou_recall(recalls, iou_thrs):\n    """"""Plot IoU-Recalls curve.\n\n    Args:\n        recalls(ndarray or list): shape (k,)\n        iou_thrs(ndarray or list): same shape as `recalls`\n    """"""\n    if isinstance(iou_thrs, np.ndarray):\n        _iou_thrs = iou_thrs.tolist()\n    else:\n        _iou_thrs = iou_thrs\n    if isinstance(recalls, np.ndarray):\n        _recalls = recalls.tolist()\n    else:\n        _recalls = recalls\n\n    import matplotlib.pyplot as plt\n    f = plt.figure()\n    plt.plot(_iou_thrs + [1.0], _recalls + [0.])\n    plt.xlabel(\'IoU\')\n    plt.ylabel(\'Recall\')\n    plt.axis([iou_thrs.min(), 1, 0, 1])\n    f.show()\n'"
mmdet/core/fp16/__init__.py,0,"b""from .decorators import auto_fp16, force_fp32\nfrom .hooks import Fp16OptimizerHook, wrap_fp16_model\n\n__all__ = ['auto_fp16', 'force_fp32', 'Fp16OptimizerHook', 'wrap_fp16_model']\n"""
mmdet/core/fp16/decorators.py,8,"b'import functools\nfrom inspect import getfullargspec\n\nimport torch\n\nfrom .utils import cast_tensor_type\n\n\ndef auto_fp16(apply_to=None, out_fp32=False):\n    """"""Decorator to enable fp16 training automatically.\n\n    This decorator is useful when you write custom modules and want to support\n    mixed precision training. If inputs arguments are fp32 tensors, they will\n    be converted to fp16 automatically. Arguments other than fp32 tensors are\n    ignored.\n\n    Args:\n        apply_to (Iterable, optional): The argument names to be converted.\n            `None` indicates all arguments.\n        out_fp32 (bool): Whether to convert the output back to fp32.\n\n    :Example:\n\n        class MyModule1(nn.Module)\n\n            # Convert x and y to fp16\n            @auto_fp16()\n            def forward(self, x, y):\n                pass\n\n        class MyModule2(nn.Module):\n\n            # convert pred to fp16\n            @auto_fp16(apply_to=(\'pred\', ))\n            def do_something(self, pred, others):\n                pass\n    """"""\n\n    def auto_fp16_wrapper(old_func):\n\n        @functools.wraps(old_func)\n        def new_func(*args, **kwargs):\n            # check if the module has set the attribute `fp16_enabled`, if not,\n            # just fallback to the original method.\n            if not isinstance(args[0], torch.nn.Module):\n                raise TypeError(\'@auto_fp16 can only be used to decorate the \'\n                                \'method of nn.Module\')\n            if not (hasattr(args[0], \'fp16_enabled\') and args[0].fp16_enabled):\n                return old_func(*args, **kwargs)\n            # get the arg spec of the decorated method\n            args_info = getfullargspec(old_func)\n            # get the argument names to be casted\n            args_to_cast = args_info.args if apply_to is None else apply_to\n            # convert the args that need to be processed\n            new_args = []\n            # NOTE: default args are not taken into consideration\n            if args:\n                arg_names = args_info.args[:len(args)]\n                for i, arg_name in enumerate(arg_names):\n                    if arg_name in args_to_cast:\n                        new_args.append(\n                            cast_tensor_type(args[i], torch.float, torch.half))\n                    else:\n                        new_args.append(args[i])\n            # convert the kwargs that need to be processed\n            new_kwargs = {}\n            if kwargs:\n                for arg_name, arg_value in kwargs.items():\n                    if arg_name in args_to_cast:\n                        new_kwargs[arg_name] = cast_tensor_type(\n                            arg_value, torch.float, torch.half)\n                    else:\n                        new_kwargs[arg_name] = arg_value\n            # apply converted arguments to the decorated method\n            output = old_func(*new_args, **new_kwargs)\n            # cast the results back to fp32 if necessary\n            if out_fp32:\n                output = cast_tensor_type(output, torch.half, torch.float)\n            return output\n\n        return new_func\n\n    return auto_fp16_wrapper\n\n\ndef force_fp32(apply_to=None, out_fp16=False):\n    """"""Decorator to convert input arguments to fp32 in force.\n\n    This decorator is useful when you write custom modules and want to support\n    mixed precision training. If there are some inputs that must be processed\n    in fp32 mode, then this decorator can handle it. If inputs arguments are\n    fp16 tensors, they will be converted to fp32 automatically. Arguments other\n    than fp16 tensors are ignored.\n\n    Args:\n        apply_to (Iterable, optional): The argument names to be converted.\n            `None` indicates all arguments.\n        out_fp16 (bool): Whether to convert the output back to fp16.\n\n    :Example:\n\n        class MyModule1(nn.Module)\n\n            # Convert x and y to fp32\n            @force_fp32()\n            def loss(self, x, y):\n                pass\n\n        class MyModule2(nn.Module):\n\n            # convert pred to fp32\n            @force_fp32(apply_to=(\'pred\', ))\n            def post_process(self, pred, others):\n                pass\n    """"""\n\n    def force_fp32_wrapper(old_func):\n\n        @functools.wraps(old_func)\n        def new_func(*args, **kwargs):\n            # check if the module has set the attribute `fp16_enabled`, if not,\n            # just fallback to the original method.\n            if not isinstance(args[0], torch.nn.Module):\n                raise TypeError(\'@force_fp32 can only be used to decorate the \'\n                                \'method of nn.Module\')\n            if not (hasattr(args[0], \'fp16_enabled\') and args[0].fp16_enabled):\n                return old_func(*args, **kwargs)\n            # get the arg spec of the decorated method\n            args_info = getfullargspec(old_func)\n            # get the argument names to be casted\n            args_to_cast = args_info.args if apply_to is None else apply_to\n            # convert the args that need to be processed\n            new_args = []\n            if args:\n                arg_names = args_info.args[:len(args)]\n                for i, arg_name in enumerate(arg_names):\n                    if arg_name in args_to_cast:\n                        new_args.append(\n                            cast_tensor_type(args[i], torch.half, torch.float))\n                    else:\n                        new_args.append(args[i])\n            # convert the kwargs that need to be processed\n            new_kwargs = dict()\n            if kwargs:\n                for arg_name, arg_value in kwargs.items():\n                    if arg_name in args_to_cast:\n                        new_kwargs[arg_name] = cast_tensor_type(\n                            arg_value, torch.half, torch.float)\n                    else:\n                        new_kwargs[arg_name] = arg_value\n            # apply converted arguments to the decorated method\n            output = old_func(*new_args, **new_kwargs)\n            # cast the results back to fp32 if necessary\n            if out_fp16:\n                output = cast_tensor_type(output, torch.float, torch.half)\n            return output\n\n        return new_func\n\n    return force_fp32_wrapper\n'"
mmdet/core/fp16/hooks.py,5,"b'import copy\nimport torch\nimport torch.nn as nn\nfrom mmcv.runner import OptimizerHook\n\nfrom .utils import cast_tensor_type\nfrom ..utils.dist_utils import allreduce_grads\n\n\nclass Fp16OptimizerHook(OptimizerHook):\n    """"""FP16 optimizer hook.\n\n    The steps of fp16 optimizer is as follows.\n    1. Scale the loss value.\n    2. BP in the fp16 model.\n    2. Copy gradients from fp16 model to fp32 weights.\n    3. Update fp32 weights.\n    4. Copy updated parameters from fp32 weights to fp16 model.\n\n    Refer to https://arxiv.org/abs/1710.03740 for more details.\n\n    Args:\n        loss_scale (float): Scale factor multiplied with loss.\n    """"""\n\n    def __init__(self,\n                 grad_clip=None,\n                 coalesce=True,\n                 bucket_size_mb=-1,\n                 loss_scale=512.,\n                 distributed=True):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n        self.loss_scale = loss_scale\n        self.distributed = distributed\n\n    def before_run(self, runner):\n        # keep a copy of fp32 weights\n        runner.optimizer.param_groups = copy.deepcopy(\n            runner.optimizer.param_groups)\n        # convert model to fp16\n        wrap_fp16_model(runner.model)\n\n    def copy_grads_to_fp32(self, fp16_net, fp32_weights):\n        """"""Copy gradients from fp16 model to fp32 weight copy.""""""\n        for fp32_param, fp16_param in zip(fp32_weights, fp16_net.parameters()):\n            if fp16_param.grad is not None:\n                if fp32_param.grad is None:\n                    fp32_param.grad = fp32_param.data.new(fp32_param.size())\n                fp32_param.grad.copy_(fp16_param.grad)\n\n    def copy_params_to_fp16(self, fp16_net, fp32_weights):\n        """"""Copy updated params from fp32 weight copy to fp16 model.""""""\n        for fp16_param, fp32_param in zip(fp16_net.parameters(), fp32_weights):\n            fp16_param.data.copy_(fp32_param.data)\n\n    def after_train_iter(self, runner):\n        # clear grads of last iteration\n        runner.model.zero_grad()\n        runner.optimizer.zero_grad()\n        # scale the loss value\n        scaled_loss = runner.outputs[\'loss\'] * self.loss_scale\n        scaled_loss.backward()\n        # copy fp16 grads in the model to fp32 params in the optimizer\n        fp32_weights = []\n        for param_group in runner.optimizer.param_groups:\n            fp32_weights += param_group[\'params\']\n        self.copy_grads_to_fp32(runner.model, fp32_weights)\n        # allreduce grads\n        if self.distributed:\n            allreduce_grads(fp32_weights, self.coalesce, self.bucket_size_mb)\n        # scale the gradients back\n        for param in fp32_weights:\n            if param.grad is not None:\n                param.grad.div_(self.loss_scale)\n        if self.grad_clip is not None:\n            self.clip_grads(fp32_weights)\n        # update fp32 params\n        runner.optimizer.step()\n        # copy fp32 params to the fp16 model\n        self.copy_params_to_fp16(runner.model, fp32_weights)\n\n\ndef wrap_fp16_model(model):\n    # convert model to fp16\n    model.half()\n    # patch the normalization layers to make it work in fp32 mode\n    patch_norm_fp32(model)\n    # set `fp16_enabled` flag\n    for m in model.modules():\n        if hasattr(m, \'fp16_enabled\'):\n            m.fp16_enabled = True\n\n\ndef patch_norm_fp32(module):\n    if isinstance(module, (nn.modules.batchnorm._BatchNorm, nn.GroupNorm)):\n        module.float()\n        module.forward = patch_forward_method(module.forward, torch.half,\n                                              torch.float)\n    for child in module.children():\n        patch_norm_fp32(child)\n    return module\n\n\ndef patch_forward_method(func, src_type, dst_type, convert_output=True):\n    """"""Patch the forward method of a module.\n\n    Args:\n        func (callable): The original forward method.\n        src_type (torch.dtype): Type of input arguments to be converted from.\n        dst_type (torch.dtype): Type of input arguments to be converted to.\n        convert_output (bool): Whether to convert the output back to src_type.\n\n    Returns:\n        callable: The patched forward method.\n    """"""\n\n    def new_forward(*args, **kwargs):\n        output = func(*cast_tensor_type(args, src_type, dst_type),\n                      **cast_tensor_type(kwargs, src_type, dst_type))\n        if convert_output:\n            output = cast_tensor_type(output, dst_type, src_type)\n        return output\n\n    return new_forward\n'"
mmdet/core/fp16/utils.py,1,"b'from collections import abc\n\nimport numpy as np\nimport torch\n\n\ndef cast_tensor_type(inputs, src_type, dst_type):\n    if isinstance(inputs, torch.Tensor):\n        return inputs.to(dst_type)\n    elif isinstance(inputs, str):\n        return inputs\n    elif isinstance(inputs, np.ndarray):\n        return inputs\n    elif isinstance(inputs, abc.Mapping):\n        return type(inputs)({\n            k: cast_tensor_type(v, src_type, dst_type)\n            for k, v in inputs.items()\n        })\n    elif isinstance(inputs, abc.Iterable):\n        return type(inputs)(\n            cast_tensor_type(item, src_type, dst_type) for item in inputs)\n    else:\n        return inputs\n'"
mmdet/core/mask/__init__.py,0,"b""from .utils import split_combined_polys\nfrom .mask_target import mask_target\n\n__all__ = ['split_combined_polys', 'mask_target']\n"""
mmdet/core/mask/mask_target.py,2,"b'import torch\nimport numpy as np\nimport mmcv\n\n\ndef mask_target(pos_proposals_list, pos_assigned_gt_inds_list, gt_masks_list,\n                cfg):\n    cfg_list = [cfg for _ in range(len(pos_proposals_list))]\n    mask_targets = map(mask_target_single, pos_proposals_list,\n                       pos_assigned_gt_inds_list, gt_masks_list, cfg_list)\n    mask_targets = torch.cat(list(mask_targets))\n    return mask_targets\n\n\ndef mask_target_single(pos_proposals, pos_assigned_gt_inds, gt_masks, cfg):\n    mask_size = cfg.mask_size\n    num_pos = pos_proposals.size(0)\n    mask_targets = []\n    if num_pos > 0:\n        proposals_np = pos_proposals.cpu().numpy()\n        pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()\n        for i in range(num_pos):\n            gt_mask = gt_masks[pos_assigned_gt_inds[i]]\n            bbox = proposals_np[i, :].astype(np.int32)\n            x1, y1, x2, y2 = bbox\n            w = np.maximum(x2 - x1 + 1, 1)\n            h = np.maximum(y2 - y1 + 1, 1)\n            # mask is uint8 both before and after resizing\n            target = mmcv.imresize(gt_mask[y1:y1 + h, x1:x1 + w],\n                                   (mask_size, mask_size))\n            mask_targets.append(target)\n        mask_targets = torch.from_numpy(np.stack(mask_targets)).float().to(\n            pos_proposals.device)\n    else:\n        mask_targets = pos_proposals.new_zeros((0, mask_size, mask_size))\n    return mask_targets\n'"
mmdet/core/mask/utils.py,0,"b'import mmcv\n\n\ndef split_combined_polys(polys, poly_lens, polys_per_mask):\n    """"""Split the combined 1-D polys into masks.\n\n    A mask is represented as a list of polys, and a poly is represented as\n    a 1-D array. In dataset, all masks are concatenated into a single 1-D\n    tensor. Here we need to split the tensor into original representations.\n\n    Args:\n        polys (list): a list (length = image num) of 1-D tensors\n        poly_lens (list): a list (length = image num) of poly length\n        polys_per_mask (list): a list (length = image num) of poly number\n            of each mask\n\n    Returns:\n        list: a list (length = image num) of list (length = mask num) of\n            list (length = poly num) of numpy array\n    """"""\n    mask_polys_list = []\n    for img_id in range(len(polys)):\n        polys_single = polys[img_id]\n        polys_lens_single = poly_lens[img_id].tolist()\n        polys_per_mask_single = polys_per_mask[img_id].tolist()\n\n        split_polys = mmcv.slice_list(polys_single, polys_lens_single)\n        mask_polys = mmcv.slice_list(split_polys, polys_per_mask_single)\n        mask_polys_list.append(mask_polys)\n    return mask_polys_list\n'"
mmdet/core/post_processing/__init__.py,0,"b""from .bbox_nms import multiclass_nms\nfrom .merge_augs import (merge_aug_proposals, merge_aug_bboxes,\n                         merge_aug_scores, merge_aug_masks)\n\n__all__ = [\n    'multiclass_nms', 'merge_aug_proposals', 'merge_aug_bboxes',\n    'merge_aug_scores', 'merge_aug_masks'\n]\n"""
mmdet/core/post_processing/bbox_nms.py,5,"b'import torch\n\nfrom mmdet.ops.nms import nms_wrapper\n\n\ndef multiclass_nms(multi_bboxes,\n                   multi_scores,\n                   score_thr,\n                   nms_cfg,\n                   max_num=-1,\n                   score_factors=None):\n    """"""NMS for multi-class bboxes.\n\n    Args:\n        multi_bboxes (Tensor): shape (n, #class*4) or (n, 4)\n        multi_scores (Tensor): shape (n, #class)\n        score_thr (float): bbox threshold, bboxes with scores lower than it\n            will not be considered.\n        nms_thr (float): NMS IoU threshold\n        max_num (int): if there are more than max_num bboxes after NMS,\n            only top max_num will be kept.\n        score_factors (Tensor): The factors multiplied to scores before\n            applying NMS\n\n    Returns:\n        tuple: (bboxes, labels), tensors of shape (k, 5) and (k, 1). Labels\n            are 0-based.\n    """"""\n    num_classes = multi_scores.shape[1]\n    bboxes, labels = [], []\n    nms_cfg_ = nms_cfg.copy()\n    nms_type = nms_cfg_.pop(\'type\', \'nms\')\n    nms_op = getattr(nms_wrapper, nms_type)\n    for i in range(1, num_classes):\n        cls_inds = multi_scores[:, i] > score_thr\n        if not cls_inds.any():\n            continue\n        # get bboxes and scores of this class\n        if multi_bboxes.shape[1] == 4:\n            _bboxes = multi_bboxes[cls_inds, :]\n        else:\n            _bboxes = multi_bboxes[cls_inds, i * 4:(i + 1) * 4]\n        _scores = multi_scores[cls_inds, i]\n        if score_factors is not None:\n            _scores *= score_factors[cls_inds]\n        cls_dets = torch.cat([_bboxes, _scores[:, None]], dim=1)\n        cls_dets, _ = nms_op(cls_dets, **nms_cfg_)\n        cls_labels = multi_bboxes.new_full(\n            (cls_dets.shape[0], ), i - 1, dtype=torch.long)\n        bboxes.append(cls_dets)\n        labels.append(cls_labels)\n    if bboxes:\n        bboxes = torch.cat(bboxes)\n        labels = torch.cat(labels)\n        if bboxes.shape[0] > max_num:\n            _, inds = bboxes[:, -1].sort(descending=True)\n            inds = inds[:max_num]\n            bboxes = bboxes[inds]\n            labels = labels[inds]\n    else:\n        bboxes = multi_bboxes.new_zeros((0, 5))\n        labels = multi_bboxes.new_zeros((0, ), dtype=torch.long)\n\n    return bboxes, labels\n'"
mmdet/core/post_processing/merge_augs.py,5,"b'import torch\n\nimport numpy as np\n\nfrom mmdet.ops import nms\nfrom ..bbox import bbox_mapping_back\n\n\ndef merge_aug_proposals(aug_proposals, img_metas, rpn_test_cfg):\n    """"""Merge augmented proposals (multiscale, flip, etc.)\n\n    Args:\n        aug_proposals (list[Tensor]): proposals from different testing\n            schemes, shape (n, 5). Note that they are not rescaled to the\n            original image size.\n        img_metas (list[dict]): image info including ""shape_scale"" and ""flip"".\n        rpn_test_cfg (dict): rpn test config.\n\n    Returns:\n        Tensor: shape (n, 4), proposals corresponding to original image scale.\n    """"""\n    recovered_proposals = []\n    for proposals, img_info in zip(aug_proposals, img_metas):\n        img_shape = img_info[\'img_shape\']\n        scale_factor = img_info[\'scale_factor\']\n        flip = img_info[\'flip\']\n        _proposals = proposals.clone()\n        _proposals[:, :4] = bbox_mapping_back(_proposals[:, :4], img_shape,\n                                              scale_factor, flip)\n        recovered_proposals.append(_proposals)\n    aug_proposals = torch.cat(recovered_proposals, dim=0)\n    merged_proposals, _ = nms(aug_proposals, rpn_test_cfg.nms_thr)\n    scores = merged_proposals[:, 4]\n    _, order = scores.sort(0, descending=True)\n    num = min(rpn_test_cfg.max_num, merged_proposals.shape[0])\n    order = order[:num]\n    merged_proposals = merged_proposals[order, :]\n    return merged_proposals\n\n\ndef merge_aug_bboxes(aug_bboxes, aug_scores, img_metas, rcnn_test_cfg):\n    """"""Merge augmented detection bboxes and scores.\n\n    Args:\n        aug_bboxes (list[Tensor]): shape (n, 4*#class)\n        aug_scores (list[Tensor] or None): shape (n, #class)\n        img_shapes (list[Tensor]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_bboxes = []\n    for bboxes, img_info in zip(aug_bboxes, img_metas):\n        img_shape = img_info[0][\'img_shape\']\n        scale_factor = img_info[0][\'scale_factor\']\n        flip = img_info[0][\'flip\']\n        bboxes = bbox_mapping_back(bboxes, img_shape, scale_factor, flip)\n        recovered_bboxes.append(bboxes)\n    bboxes = torch.stack(recovered_bboxes).mean(dim=0)\n    if aug_scores is None:\n        return bboxes\n    else:\n        scores = torch.stack(aug_scores).mean(dim=0)\n        return bboxes, scores\n\n\ndef merge_aug_scores(aug_scores):\n    """"""Merge augmented bbox scores.""""""\n    if isinstance(aug_scores[0], torch.Tensor):\n        return torch.mean(torch.stack(aug_scores), dim=0)\n    else:\n        return np.mean(aug_scores, axis=0)\n\n\ndef merge_aug_masks(aug_masks, img_metas, rcnn_test_cfg, weights=None):\n    """"""Merge augmented mask prediction.\n\n    Args:\n        aug_masks (list[ndarray]): shape (n, #class, h, w)\n        img_shapes (list[ndarray]): shape (3, ).\n        rcnn_test_cfg (dict): rcnn test config.\n\n    Returns:\n        tuple: (bboxes, scores)\n    """"""\n    recovered_masks = [\n        mask if not img_info[0][\'flip\'] else mask[..., ::-1]\n        for mask, img_info in zip(aug_masks, img_metas)\n    ]\n    if weights is None:\n        merged_masks = np.mean(recovered_masks, axis=0)\n    else:\n        merged_masks = np.average(\n            np.array(recovered_masks), axis=0, weights=np.array(weights))\n    return merged_masks\n'"
mmdet/core/utils/__init__.py,0,"b""from .dist_utils import allreduce_grads, DistOptimizerHook\nfrom .misc import tensor2imgs, unmap, multi_apply\n\n__all__ = [\n    'allreduce_grads', 'DistOptimizerHook', 'tensor2imgs', 'unmap',\n    'multi_apply'\n]\n"""
mmdet/core/utils/dist_utils.py,2,"b""from collections import OrderedDict\n\nimport torch.distributed as dist\nfrom torch._utils import (_flatten_dense_tensors, _unflatten_dense_tensors,\n                          _take_tensors)\nfrom mmcv.runner import OptimizerHook\n\n\ndef _allreduce_coalesced(tensors, world_size, bucket_size_mb=-1):\n    if bucket_size_mb > 0:\n        bucket_size_bytes = bucket_size_mb * 1024 * 1024\n        buckets = _take_tensors(tensors, bucket_size_bytes)\n    else:\n        buckets = OrderedDict()\n        for tensor in tensors:\n            tp = tensor.type()\n            if tp not in buckets:\n                buckets[tp] = []\n            buckets[tp].append(tensor)\n        buckets = buckets.values()\n\n    for bucket in buckets:\n        flat_tensors = _flatten_dense_tensors(bucket)\n        dist.all_reduce(flat_tensors)\n        flat_tensors.div_(world_size)\n        for tensor, synced in zip(\n                bucket, _unflatten_dense_tensors(flat_tensors, bucket)):\n            tensor.copy_(synced)\n\n\ndef allreduce_grads(params, coalesce=True, bucket_size_mb=-1):\n    grads = [\n        param.grad.data for param in params\n        if param.requires_grad and param.grad is not None\n    ]\n    world_size = dist.get_world_size()\n    if coalesce:\n        _allreduce_coalesced(grads, world_size, bucket_size_mb)\n    else:\n        for tensor in grads:\n            dist.all_reduce(tensor.div_(world_size))\n\n\nclass DistOptimizerHook(OptimizerHook):\n\n    def __init__(self, grad_clip=None, coalesce=True, bucket_size_mb=-1):\n        self.grad_clip = grad_clip\n        self.coalesce = coalesce\n        self.bucket_size_mb = bucket_size_mb\n\n    def after_train_iter(self, runner):\n        runner.optimizer.zero_grad()\n        runner.outputs['loss'].backward()\n        allreduce_grads(runner.model.parameters(), self.coalesce,\n                        self.bucket_size_mb)\n        if self.grad_clip is not None:\n            self.clip_grads(runner.model.parameters())\n        runner.optimizer.step()\n"""
mmdet/core/utils/misc.py,0,"b'from functools import partial\n\nimport mmcv\nimport numpy as np\nfrom six.moves import map, zip\n\nimport ipdb\n\n\n\ndef tensor2imgs(tensor, mean=(0, 0, 0), std=(1, 1, 1), to_rgb=True):\n    num_imgs = tensor.size(0)\n    mean = np.array(mean, dtype=np.float32)\n    std = np.array(std, dtype=np.float32)\n    imgs = []\n    for img_id in range(num_imgs):\n        img = tensor[img_id, ...].cpu().numpy().transpose(1, 2, 0)\n        img = mmcv.imdenormalize(\n            img, mean, std, to_bgr=to_rgb).astype(np.uint8)\n        imgs.append(np.ascontiguousarray(img))\n    return imgs\n\n\ndef multi_apply(func, *args, **kwargs):\n    pfunc = partial(func, **kwargs) if kwargs else func\n    map_results = map(pfunc, *args)\n    return tuple(map(list, zip(*map_results)))\n\n\ndef unmap(data, count, inds, fill=0):\n    """""" Unmap a subset of item (data) back to the original set of items (of\n    size count) """"""\n    if data.dim() == 1:\n        ret = data.new_full((count, ), fill)\n        ret[inds] = data\n    else:\n        new_size = (count, ) + data.size()[1:]\n        ret = data.new_full(new_size, fill)\n        ret[inds, :] = data\n    return ret\n'"
mmdet/datasets/loader/__init__.py,0,"b""from .build_loader import build_dataloader\nfrom .sampler import GroupSampler, DistributedGroupSampler\n\n__all__ = ['GroupSampler', 'DistributedGroupSampler', 'build_dataloader']\n"""
mmdet/datasets/loader/build_loader.py,1,"b""import platform\nfrom functools import partial\n\nfrom mmcv.runner import get_dist_info\nfrom mmcv.parallel import collate\nfrom torch.utils.data import DataLoader\n\nfrom .sampler import GroupSampler, DistributedGroupSampler, DistributedSampler\n\nimport ipdb\n\nif platform.system() != 'Windows':\n    # https://github.com/pytorch/pytorch/issues/973\n    import resource\n    rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n    resource.setrlimit(resource.RLIMIT_NOFILE, (4096, rlimit[1]))\n\n\ndef build_dataloader(dataset,\n                     imgs_per_gpu,\n                     workers_per_gpu,\n                     num_gpus=1,\n                     dist=True,\n                     **kwargs):\n    shuffle = kwargs.get('shuffle', True)\n    if dist:\n        rank, world_size = get_dist_info()\n        if shuffle:\n            sampler = DistributedGroupSampler(dataset, imgs_per_gpu,\n                                              world_size, rank)\n        else:\n            sampler = DistributedSampler(\n                dataset, world_size, rank, shuffle=False)\n        batch_size = imgs_per_gpu\n        num_workers = workers_per_gpu\n    else:\n        # \xe9\x9d\x9e\xe5\x88\x86\xe5\xb8\x83\xe5\xbc\x8f\xe8\xae\xad\xe7\xbb\x83\n        sampler = GroupSampler(dataset, imgs_per_gpu) if shuffle else None  # batch\xe4\xb8\xad\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe9\x87\x87\xe6\xa0\xb7\xe6\x96\xb9\xe5\xbc\x8f\n        batch_size = num_gpus * imgs_per_gpu        # \xe5\x9c\xa8\xe8\xbf\x99\xe9\x87\x8c\xe5\xae\x9a\xe4\xb9\x89batch size\n        num_workers = num_gpus * workers_per_gpu    # \xe5\xa4\x9a\xe7\xba\xbf\xe7\xa8\x8b\xe8\xaf\xbb\xe5\x8f\x96\xe5\x8f\xaf\xe4\xbb\xa5\xe5\x8a\xa0\xe5\xbf\xab\xe6\x95\xb0\xe6\x8d\xae\xe7\x9a\x84\xe8\xaf\xbb\xe5\x8f\x96\xe9\x80\x9f\xe5\xba\xa6\n\n    # \xe9\x87\x87\xe7\x94\xa8pytorch\xe5\x86\x85\xe7\xbd\xae\xe7\x9a\x84DataLoader\xe6\x96\xb9\xe6\xb3\x95\n    # DataLoader\xe6\x98\xaf\xe4\xb8\x80\xe4\xb8\xaa \xe8\xbf\xad\xe4\xbb\xa3\xe5\x99\xa8\n    # collate_fn\xef\xbc\x9a\xe5\x9c\xa8\xe6\x95\xb0\xe6\x8d\xae\xe5\xa4\x84\xe7\x90\x86\xe4\xb8\xad\xef\xbc\x8c\xe6\x9c\x89\xe6\x97\xb6\xe4\xbc\x9a\xe5\x87\xba\xe7\x8e\xb0\xe6\x9f\x90\xe4\xb8\xaa\xe6\xa0\xb7\xe6\x9c\xac\xe6\x97\xa0\xe6\xb3\x95\xe8\xaf\xbb\xe5\x8f\x96\xe7\xad\x89\xe9\x97\xae\xe9\xa2\x98\xef\xbc\x8c\xe6\xaf\x94\xe5\xa6\x82\xe6\x9f\x90\xe5\xbc\xa0\xe5\x9b\xbe\xe7\x89\x87\xe6\x8d\x9f\xe5\x9d\x8f\xe3\x80\x82\n    #             \xe8\xbf\x99\xe6\x97\xb6\xe5\x9c\xa8_ getitem _\xe5\x87\xbd\xe6\x95\xb0\xe4\xb8\xad\xe5\xb0\x86\xe5\x87\xba\xe7\x8e\xb0\xe5\xbc\x82\xe5\xb8\xb8\xef\xbc\x8c\xe6\xad\xa4\xe6\x97\xb6\xe6\x9c\x80\xe5\xa5\xbd\xe7\x9a\x84\xe8\xa7\xa3\xe5\x86\xb3\xe6\x96\xb9\xe6\xa1\x88\xe5\x8d\xb3\xe6\x98\xaf\xe5\xb0\x86\xe5\x87\xba\xe9\x94\x99\xe7\x9a\x84\xe6\xa0\xb7\xe6\x9c\xac\xe5\x89\x94\xe9\x99\xa4\xe3\x80\x82\n    #             \xe5\xa6\x82\xe6\x9e\x9c\xe5\xae\x9e\xe5\x9c\xa8\xe6\x98\xaf\xe9\x81\x87\xe5\x88\xb0\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe6\x97\xa0\xe6\xb3\x95\xe5\xa4\x84\xe7\x90\x86\xef\xbc\x8c\xe5\x88\x99\xe5\x8f\xaf\xe4\xbb\xa5\xe8\xbf\x94\xe5\x9b\x9eNone\xe5\xaf\xb9\xe8\xb1\xa1\xef\xbc\x8c\xe7\x84\xb6\xe5\x90\x8e\xe5\x9c\xa8Dataloader\xe4\xb8\xad\xe5\xae\x9e\xe7\x8e\xb0\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe7\x9a\x84collate_fn\xef\xbc\x8c\xe5\xb0\x86\xe7\xa9\xba\xe5\xaf\xb9\xe8\xb1\xa1\xe8\xbf\x87\xe6\xbb\xa4\xe6\x8e\x89\xe3\x80\x82\n    #             \xe4\xbd\x86\xe8\xa6\x81\xe6\xb3\xa8\xe6\x84\x8f\xef\xbc\x8c\xe5\x9c\xa8\xe8\xbf\x99\xe7\xa7\x8d\xe6\x83\x85\xe5\x86\xb5\xe4\xb8\x8bdataloader\xe8\xbf\x94\xe5\x9b\x9e\xe7\x9a\x84batch\xe6\x95\xb0\xe7\x9b\xae\xe4\xbc\x9a\xe5\xb0\x91\xe4\xba\x8ebatch_size\xe3\x80\x82\n\n    # sampler\xef\xbc\x9a\xe8\x87\xaa\xe5\xae\x9a\xe4\xb9\x89\xe4\xbb\x8e\xe6\x95\xb0\xe6\x8d\xae\xe9\x9b\x86\xe4\xb8\xad\xe5\x8f\x96\xe6\xa0\xb7\xe6\x9c\xac\xe7\x9a\x84\xe7\xad\x96\xe7\x95\xa5\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\x8c\x87\xe5\xae\x9a\xe8\xbf\x99\xe4\xb8\xaa\xe5\x8f\x82\xe6\x95\xb0\xef\xbc\x8c\xe9\x82\xa3\xe4\xb9\x88shuffle\xe5\xbf\x85\xe9\xa1\xbb\xe4\xb8\xbaFalse\n    data_loader = DataLoader(\n        dataset,\n        batch_size=batch_size,\n        sampler=sampler,\n        num_workers=num_workers,\n        collate_fn=partial(collate, samples_per_gpu=imgs_per_gpu),\n        pin_memory=False,\n        **kwargs)\n\n    return data_loader\n"""
mmdet/datasets/loader/sampler.py,9,"b'from __future__ import division\n\nimport math\nimport torch\nimport numpy as np\n\nfrom mmcv.runner.utils import get_dist_info\nfrom torch.utils.data import Sampler\nfrom torch.utils.data import DistributedSampler as _DistributedSampler\n\n\nclass DistributedSampler(_DistributedSampler):\n\n    def __init__(self, dataset, num_replicas=None, rank=None, shuffle=True):\n        super().__init__(dataset, num_replicas=num_replicas, rank=rank)\n        self.shuffle = shuffle\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        if self.shuffle:\n            g = torch.Generator()\n            g.manual_seed(self.epoch)\n            indices = torch.randperm(len(self.dataset), generator=g).tolist()\n        else:\n            indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n\nclass GroupSampler(Sampler):\n\n    def __init__(self, dataset, samples_per_gpu=1):\n        assert hasattr(dataset, \'flag\')\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.flag = dataset.flag.astype(np.int64)\n        self.group_sizes = np.bincount(self.flag)\n        self.num_samples = 0\n        for i, size in enumerate(self.group_sizes):\n            self.num_samples += int(np.ceil(\n                size / self.samples_per_gpu)) * self.samples_per_gpu\n\n    def __iter__(self):\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size == 0:\n                continue\n            indice = np.where(self.flag == i)[0]\n            assert len(indice) == size\n            np.random.shuffle(indice)\n            num_extra = int(np.ceil(size / self.samples_per_gpu)\n                            ) * self.samples_per_gpu - len(indice)\n            indice = np.concatenate([indice, indice[:num_extra]])\n            indices.append(indice)\n        indices = np.concatenate(indices)\n        indices = [\n            indices[i * self.samples_per_gpu:(i + 1) * self.samples_per_gpu]\n            for i in np.random.permutation(\n                range(len(indices) // self.samples_per_gpu))\n        ]\n        indices = np.concatenate(indices)\n        indices = indices.astype(np.int64).tolist()\n        assert len(indices) == self.num_samples\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n\nclass DistributedGroupSampler(Sampler):\n    """"""Sampler that restricts data loading to a subset of the dataset.\n    It is especially useful in conjunction with\n    :class:`torch.nn.parallel.DistributedDataParallel`. In such case, each\n    process can pass a DistributedSampler instance as a DataLoader sampler,\n    and load a subset of the original dataset that is exclusive to it.\n    .. note::\n        Dataset is assumed to be of constant size.\n    Arguments:\n        dataset: Dataset used for sampling.\n        num_replicas (optional): Number of processes participating in\n            distributed training.\n        rank (optional): Rank of the current process within num_replicas.\n    """"""\n\n    def __init__(self,\n                 dataset,\n                 samples_per_gpu=1,\n                 num_replicas=None,\n                 rank=None):\n        _rank, _num_replicas = get_dist_info()\n        if num_replicas is None:\n            num_replicas = _num_replicas\n        if rank is None:\n            rank = _rank\n        self.dataset = dataset\n        self.samples_per_gpu = samples_per_gpu\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n\n        assert hasattr(self.dataset, \'flag\')\n        self.flag = self.dataset.flag\n        self.group_sizes = np.bincount(self.flag)\n\n        self.num_samples = 0\n        for i, j in enumerate(self.group_sizes):\n            self.num_samples += int(\n                math.ceil(self.group_sizes[i] * 1.0 / self.samples_per_gpu /\n                          self.num_replicas)) * self.samples_per_gpu\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        # deterministically shuffle based on epoch\n        g = torch.Generator()\n        g.manual_seed(self.epoch)\n\n        indices = []\n        for i, size in enumerate(self.group_sizes):\n            if size > 0:\n                indice = np.where(self.flag == i)[0]\n                assert len(indice) == size\n                indice = indice[list(torch.randperm(int(size),\n                                                    generator=g))].tolist()\n                extra = int(\n                    math.ceil(\n                        size * 1.0 / self.samples_per_gpu / self.num_replicas)\n                ) * self.samples_per_gpu * self.num_replicas - len(indice)\n                indice += indice[:extra]\n                indices += indice\n\n        assert len(indices) == self.total_size\n\n        indices = [\n            indices[j] for i in list(\n                torch.randperm(\n                    len(indices) // self.samples_per_gpu, generator=g))\n            for j in range(i * self.samples_per_gpu, (i + 1) *\n                           self.samples_per_gpu)\n        ]\n\n        # subsample\n        offset = self.num_samples * self.rank\n        indices = indices[offset:offset + self.num_samples]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n'"
mmdet/models/anchor_heads/__init__.py,0,"b""from .anchor_head import AnchorHead\nfrom .guided_anchor_head import GuidedAnchorHead, FeatureAdaption\nfrom .fcos_head import FCOSHead\nfrom .rpn_head import RPNHead\nfrom .ga_rpn_head import GARPNHead\nfrom .retina_head import RetinaHead\nfrom .ga_retina_head import GARetinaHead\nfrom .ssd_head import SSDHead\n\n__all__ = [\n    'AnchorHead', 'GuidedAnchorHead', 'FeatureAdaption', 'RPNHead',\n    'GARPNHead', 'RetinaHead', 'GARetinaHead', 'SSDHead', 'FCOSHead'\n]\n"""
mmdet/models/anchor_heads/anchor_head.py,4,"b'from __future__ import division\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import (AnchorGenerator, anchor_target, delta2bbox,\n                        multi_apply, multiclass_nms, force_fp32)\nfrom ..builder import build_loss\nfrom ..registry import HEADS\n\nimport ipdb\n\n\n@HEADS.register_module\nclass AnchorHead(nn.Module):\n    """"""Anchor-based head (RPN, RetinaNet, SSD, etc.).\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of channels of the feature map.\n        anchor_scales (Iterable): Anchor scales.\n        anchor_ratios (Iterable): Anchor aspect ratios.\n        anchor_strides (Iterable): Anchor strides.\n        anchor_base_sizes (Iterable): Anchor base sizes.\n        target_means (Iterable): Mean values of regression targets.\n        target_stds (Iterable): Std values of regression targets.\n        loss_cls (dict): Config of classification loss.\n        loss_bbox (dict): Config of localization loss.\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_classes,   \n                 in_channels,\n                 feat_channels=256,\n                 anchor_scales=[8, 16, 32],\n                 anchor_ratios=[0.5, 1.0, 2.0],\n                 anchor_strides=[4, 8, 16, 32, 64],  # \xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xe6\xad\xa5\xe9\x95\xbf\n                 anchor_base_sizes=None,\n                 target_means=(.0, .0, .0, .0),\n                 target_stds=(1.0, 1.0, 1.0, 1.0),\n                 loss_cls=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=True,\n                     loss_weight=1.0),\n                 loss_bbox=dict(\n                     type=\'SmoothL1Loss\', beta=1.0 / 9.0, loss_weight=1.0)):\n        super(AnchorHead, self).__init__()\n        # ipdb.set_trace(context=35)\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.anchor_scales = anchor_scales\n        self.anchor_ratios = anchor_ratios\n        self.anchor_strides = anchor_strides\n        # self.anchor_base_sizes\xe8\xa2\xab\xe7\xbd\xae\xe4\xb8\xbaself.anchor_strides\xe5\x8d\xb3\xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xe6\xad\xa5\xe9\x95\xbf\xef\xbc\x88\xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe8\xae\xbe\xe7\xbd\xaeanchor_base_sizes\xe7\x9a\x84\xe5\x9b\xba\xe5\xae\x9a\xe5\xb0\xba\xe5\xaf\xb8\xef\xbc\x89\n        self.anchor_base_sizes = list(\n            anchor_strides) if anchor_base_sizes is None else anchor_base_sizes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        \n        self.use_sigmoid_cls = loss_cls.get(\'use_sigmoid\', False) # \xe6\x9f\xa5\xe7\x9c\x8b\xe6\x98\xaf\xe5\x90\xa6\xe4\xbd\xbf\xe7\x94\xa8sigmoid\xef\xbc\x8c\xe5\xa6\x82\xe6\x9e\x9c\xe6\xb2\xa1\xe5\x86\x99\xe9\xbb\x98\xe8\xae\xa4False\n        self.sampling = loss_cls[\'type\'] not in [\'FocalLoss\', \'GHMC\'] # \xe5\xa6\x82\xe6\x9e\x9c\xe4\xb8\x8d\xe6\x98\xaffocal\xe6\x88\x96\xe8\x80\x85GHMC\xe5\x88\x99\xe9\x87\x87\xe6\xa0\xb7\xe6\xa0\x87\xe5\xbf\x97\xe4\xb8\xbaTrue\xef\xbc\x8c\xe5\x90\xa6\xe5\x88\x99\xe4\xb8\x8d\xe5\x8f\xaf\xe7\x9b\xb4\xe6\x8e\xa5\xe9\x87\x87\xe6\xa0\xb7\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = num_classes - 1\n        else:\n            self.cls_out_channels = num_classes\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.fp16_enabled = False\n\n        self.anchor_generators = []\n        for anchor_base in self.anchor_base_sizes:\n            self.anchor_generators.append(\n                AnchorGenerator(anchor_base, anchor_scales, anchor_ratios))\n\n        self.num_anchors = len(self.anchor_ratios) * len(self.anchor_scales)\n        self._init_layers()\n\n    def _init_layers(self):\n        self.conv_cls = nn.Conv2d(self.feat_channels,\n                                  self.num_anchors * self.cls_out_channels, 1)\n        self.conv_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_score = self.conv_cls(x)\n        bbox_pred = self.conv_reg(x)\n        return cls_score, bbox_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_anchors(self, featmap_sizes, img_metas):\n        """"""Get anchors according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n\n        Returns:\n            tuple: anchors of each image, valid flags of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # anchors for one time\n        multi_level_anchors = []\n        for i in range(num_levels):\n            anchors = self.anchor_generators[i].grid_anchors(\n                featmap_sizes[i], self.anchor_strides[i])\n            multi_level_anchors.append(anchors)\n        anchor_list = [multi_level_anchors for _ in range(num_imgs)]\n\n        # for each image, we compute valid flags of multi level anchors\n        valid_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            for i in range(num_levels):\n                anchor_stride = self.anchor_strides[i]\n                feat_h, feat_w = featmap_sizes[i]\n                h, w, _ = img_meta[\'pad_shape\']\n                valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)\n                valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)\n                flags = self.anchor_generators[i].valid_flags(\n                    (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n                multi_level_flags.append(flags)\n            valid_flag_list.append(multi_level_flags)\n\n        return anchor_list, valid_flag_list\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples, cfg):\n        # classification loss\n        labels = labels.reshape(-1)\n        label_weights = label_weights.reshape(-1)\n        cls_score = cls_score.permute(0, 2, 3,\n                                      1).reshape(-1, self.cls_out_channels)\n        loss_cls = self.loss_cls(\n            cls_score, labels, label_weights, avg_factor=num_total_samples)\n        # regression loss\n        bbox_targets = bbox_targets.reshape(-1, 4)\n        bbox_weights = bbox_weights.reshape(-1, 4)\n        bbox_pred = bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n        loss_bbox = self.loss_bbox(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            avg_factor=num_total_samples)\n        return loss_cls, loss_bbox\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generators)\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas)\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = anchor_target(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n            sampling=self.sampling)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos + num_total_neg if self.sampling else num_total_pos)\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples,\n            cfg=cfg)\n        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\'))\n    def get_bboxes(self, cls_scores, bbox_preds, img_metas, cfg,\n                   rescale=False):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)    # \xe5\x87\xa0\xe5\xbc\xa0\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\n\n        # \xe5\xbe\x97\xe5\x88\xb0\xe9\x95\xbf\xe5\xba\xa6\xe4\xb8\xba5\xe7\x9a\x84list\xef\xbc\x8c\xe5\xaf\xb9\xe5\xba\x94\xe6\xaf\x8f\xe5\xbc\xa0\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe7\x9a\x84shape\xe4\xb8\xba[N,4]\xef\xbc\x8cN\xe6\x98\xaf\xe7\x94\x9f\xe6\x88\x90\xe7\x9a\x84\xe6\x80\xbb\xe7\x9a\x84anchor\xe6\x95\xb0\xe7\x9b\xae\n        mlvl_anchors = [\n            self.anchor_generators[i].grid_anchors(cls_scores[i].size()[-2:],\n                                                   self.anchor_strides[i])\n            for i in range(num_levels)\n        ]\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            # \xe8\xbf\x99\xe4\xb8\x80\xe6\xad\xa5\xe5\xb0\x86\xe6\x89\x80\xe6\x9c\x89\xe9\xa2\x84\xe6\xb5\x8b\xe7\x9a\x84anchor bbox\xe5\x89\x94\xe9\x99\xa4\xe5\x88\xb02000\xe4\xb8\xaa\xef\xbc\x8c\xe5\xb9\xb6\xe4\xb8\x94\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84\xe5\xb7\xb2\xe7\xbb\x8f\xe6\x98\xaf\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9d\x90\xe6\xa0\x87\xe4\xba\x86\n            # \xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84proposal\xe5\xa6\x82\xe6\x9e\x9c\xe5\xb0\x8f\xe4\xba\x8e2000\xe4\xb8\x8d\xe5\x89\x94\xe9\x99\xa4\n            proposals = self.get_bboxes_single(cls_score_list, bbox_pred_list,\n                                               mlvl_anchors, img_shape,\n                                               scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors in zip(cls_scores, bbox_preds,\n                                                 mlvl_anchors):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    max_scores, _ = scores[:, 1:].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = delta2bbox(anchors, bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n        det_bboxes, det_labels = multiclass_nms(mlvl_bboxes, mlvl_scores,\n                                                cfg.score_thr, cfg.nms,\n                                                cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmdet/models/anchor_heads/fcos_head.py,23,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import multi_apply, multiclass_nms, distance2bbox, force_fp32\nfrom ..builder import build_loss\nfrom ..registry import HEADS\nfrom ..utils import bias_init_with_prob, Scale, ConvModule\n\nINF = 1e8\n\n\n@HEADS.register_module\nclass FCOSHead(nn.Module):\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 feat_channels=256,\n                 stacked_convs=4,\n                 strides=(4, 8, 16, 32, 64),\n                 regress_ranges=((-1, 64), (64, 128), (128, 256), (256, 512),\n                                 (512, INF)),\n                 loss_cls=dict(\n                     type=\'FocalLoss\',\n                     use_sigmoid=True,\n                     gamma=2.0,\n                     alpha=0.25,\n                     loss_weight=1.0),\n                 loss_bbox=dict(type=\'IoULoss\', loss_weight=1.0),\n                 loss_centerness=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=True,\n                     loss_weight=1.0),\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'GN\', num_groups=32, requires_grad=True)):\n        super(FCOSHead, self).__init__()\n\n        self.num_classes = num_classes\n        self.cls_out_channels = num_classes - 1\n        self.in_channels = in_channels\n        self.feat_channels = feat_channels\n        self.stacked_convs = stacked_convs\n        self.strides = strides\n        self.regress_ranges = regress_ranges\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n        self.loss_centerness = build_loss(loss_centerness)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=self.norm_cfg is None))\n        self.fcos_cls = nn.Conv2d(\n            self.feat_channels, self.cls_out_channels, 3, padding=1)\n        self.fcos_reg = nn.Conv2d(self.feat_channels, 4, 3, padding=1)\n        self.fcos_centerness = nn.Conv2d(self.feat_channels, 1, 3, padding=1)\n\n        self.scales = nn.ModuleList([Scale(1.0) for _ in self.strides])\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.fcos_cls, std=0.01, bias=bias_cls)\n        normal_init(self.fcos_reg, std=0.01)\n        normal_init(self.fcos_centerness, std=0.01)\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats, self.scales)\n\n    def forward_single(self, x, scale):\n        cls_feat = x\n        reg_feat = x\n\n        for cls_layer in self.cls_convs:\n            cls_feat = cls_layer(cls_feat)\n        cls_score = self.fcos_cls(cls_feat)\n        centerness = self.fcos_centerness(cls_feat)\n\n        for reg_layer in self.reg_convs:\n            reg_feat = reg_layer(reg_feat)\n        # scale the bbox_pred of different level\n        # float to avoid overflow when enabling FP16\n        bbox_pred = scale(self.fcos_reg(reg_feat)).float().exp()\n        return cls_score, bbox_pred, centerness\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             centernesses,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        assert len(cls_scores) == len(bbox_preds) == len(centernesses)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        all_level_points = self.get_points(featmap_sizes, bbox_preds[0].dtype,\n                                           bbox_preds[0].device)\n        labels, bbox_targets = self.fcos_target(all_level_points, gt_bboxes,\n                                                gt_labels)\n\n        num_imgs = cls_scores[0].size(0)\n        # flatten cls_scores, bbox_preds and centerness\n        flatten_cls_scores = [\n            cls_score.permute(0, 2, 3, 1).reshape(-1, self.cls_out_channels)\n            for cls_score in cls_scores\n        ]\n        flatten_bbox_preds = [\n            bbox_pred.permute(0, 2, 3, 1).reshape(-1, 4)\n            for bbox_pred in bbox_preds\n        ]\n        flatten_centerness = [\n            centerness.permute(0, 2, 3, 1).reshape(-1)\n            for centerness in centernesses\n        ]\n        flatten_cls_scores = torch.cat(flatten_cls_scores)\n        flatten_bbox_preds = torch.cat(flatten_bbox_preds)\n        flatten_centerness = torch.cat(flatten_centerness)\n        flatten_labels = torch.cat(labels)\n        flatten_bbox_targets = torch.cat(bbox_targets)\n        # repeat points to align with bbox_preds\n        flatten_points = torch.cat(\n            [points.repeat(num_imgs, 1) for points in all_level_points])\n\n        pos_inds = flatten_labels.nonzero().reshape(-1)\n        num_pos = len(pos_inds)\n        loss_cls = self.loss_cls(\n            flatten_cls_scores, flatten_labels,\n            avg_factor=num_pos + num_imgs)  # avoid num_pos is 0\n\n        pos_bbox_preds = flatten_bbox_preds[pos_inds]\n        pos_bbox_targets = flatten_bbox_targets[pos_inds]\n        pos_centerness = flatten_centerness[pos_inds]\n        pos_centerness_targets = self.centerness_target(pos_bbox_targets)\n\n        if num_pos > 0:\n            pos_points = flatten_points[pos_inds]\n            pos_decoded_bbox_preds = distance2bbox(pos_points, pos_bbox_preds)\n            pos_decoded_target_preds = distance2bbox(pos_points,\n                                                     pos_bbox_targets)\n            # centerness weighted iou loss\n            loss_bbox = self.loss_bbox(\n                pos_decoded_bbox_preds,\n                pos_decoded_target_preds,\n                weight=pos_centerness_targets,\n                avg_factor=pos_centerness_targets.sum())\n            loss_centerness = self.loss_centerness(pos_centerness,\n                                                   pos_centerness_targets)\n        else:\n            loss_bbox = pos_bbox_preds.sum()\n            loss_centerness = pos_centerness.sum()\n\n        return dict(\n            loss_cls=loss_cls,\n            loss_bbox=loss_bbox,\n            loss_centerness=loss_centerness)\n\n    @force_fp32(apply_to=(\'cls_scores\', \'bbox_preds\', \'centernesses\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   centernesses,\n                   img_metas,\n                   cfg,\n                   rescale=None):\n        assert len(cls_scores) == len(bbox_preds)\n        num_levels = len(cls_scores)\n\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        mlvl_points = self.get_points(featmap_sizes, bbox_preds[0].dtype,\n                                      bbox_preds[0].device)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            centerness_pred_list = [\n                centernesses[i][img_id].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            det_bboxes = self.get_bboxes_single(cls_score_list, bbox_pred_list,\n                                                centerness_pred_list,\n                                                mlvl_points, img_shape,\n                                                scale_factor, cfg, rescale)\n            result_list.append(det_bboxes)\n        return result_list\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          centernesses,\n                          mlvl_points,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_points)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        mlvl_centerness = []\n        for cls_score, bbox_pred, centerness, points in zip(\n                cls_scores, bbox_preds, centernesses, mlvl_points):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            scores = cls_score.permute(1, 2, 0).reshape(\n                -1, self.cls_out_channels).sigmoid()\n            centerness = centerness.permute(1, 2, 0).reshape(-1).sigmoid()\n\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                max_scores, _ = (scores * centerness[:, None]).max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                points = points[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n                centerness = centerness[topk_inds]\n            bboxes = distance2bbox(points, bbox_pred, max_shape=img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n            mlvl_centerness.append(centerness)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n        mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n        mlvl_centerness = torch.cat(mlvl_centerness)\n        det_bboxes, det_labels = multiclass_nms(\n            mlvl_bboxes,\n            mlvl_scores,\n            cfg.score_thr,\n            cfg.nms,\n            cfg.max_per_img,\n            score_factors=mlvl_centerness)\n        return det_bboxes, det_labels\n\n    def get_points(self, featmap_sizes, dtype, device):\n        """"""Get points according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            dtype (torch.dtype): Type of points.\n            device (torch.device): Device of points.\n\n        Returns:\n            tuple: points of each image.\n        """"""\n        mlvl_points = []\n        for i in range(len(featmap_sizes)):\n            mlvl_points.append(\n                self.get_points_single(featmap_sizes[i], self.strides[i],\n                                       dtype, device))\n        return mlvl_points\n\n    def get_points_single(self, featmap_size, stride, dtype, device):\n        h, w = featmap_size\n        x_range = torch.arange(\n            0, w * stride, stride, dtype=dtype, device=device)\n        y_range = torch.arange(\n            0, h * stride, stride, dtype=dtype, device=device)\n        y, x = torch.meshgrid(y_range, x_range)\n        points = torch.stack(\n            (x.reshape(-1), y.reshape(-1)), dim=-1) + stride // 2\n        return points\n\n    def fcos_target(self, points, gt_bboxes_list, gt_labels_list):\n        assert len(points) == len(self.regress_ranges)\n        num_levels = len(points)\n        # expand regress ranges to align with points\n        expanded_regress_ranges = [\n            points[i].new_tensor(self.regress_ranges[i])[None].expand_as(\n                points[i]) for i in range(num_levels)\n        ]\n        # concat all levels points and regress ranges\n        concat_regress_ranges = torch.cat(expanded_regress_ranges, dim=0)\n        concat_points = torch.cat(points, dim=0)\n        # get labels and bbox_targets of each image\n        labels_list, bbox_targets_list = multi_apply(\n            self.fcos_target_single,\n            gt_bboxes_list,\n            gt_labels_list,\n            points=concat_points,\n            regress_ranges=concat_regress_ranges)\n\n        # split to per img, per level\n        num_points = [center.size(0) for center in points]\n        labels_list = [labels.split(num_points, 0) for labels in labels_list]\n        bbox_targets_list = [\n            bbox_targets.split(num_points, 0)\n            for bbox_targets in bbox_targets_list\n        ]\n\n        # concat per level image\n        concat_lvl_labels = []\n        concat_lvl_bbox_targets = []\n        for i in range(num_levels):\n            concat_lvl_labels.append(\n                torch.cat([labels[i] for labels in labels_list]))\n            concat_lvl_bbox_targets.append(\n                torch.cat(\n                    [bbox_targets[i] for bbox_targets in bbox_targets_list]))\n        return concat_lvl_labels, concat_lvl_bbox_targets\n\n    def fcos_target_single(self, gt_bboxes, gt_labels, points, regress_ranges):\n        num_points = points.size(0)\n        num_gts = gt_labels.size(0)\n\n        areas = (gt_bboxes[:, 2] - gt_bboxes[:, 0] + 1) * (\n            gt_bboxes[:, 3] - gt_bboxes[:, 1] + 1)\n        # TODO: figure out why these two are different\n        # areas = areas[None].expand(num_points, num_gts)\n        areas = areas[None].repeat(num_points, 1)\n        regress_ranges = regress_ranges[:, None, :].expand(\n            num_points, num_gts, 2)\n        gt_bboxes = gt_bboxes[None].expand(num_points, num_gts, 4)\n        xs, ys = points[:, 0], points[:, 1]\n        xs = xs[:, None].expand(num_points, num_gts)\n        ys = ys[:, None].expand(num_points, num_gts)\n\n        left = xs - gt_bboxes[..., 0]\n        right = gt_bboxes[..., 2] - xs\n        top = ys - gt_bboxes[..., 1]\n        bottom = gt_bboxes[..., 3] - ys\n        bbox_targets = torch.stack((left, top, right, bottom), -1)\n\n        # condition1: inside a gt bbox\n        inside_gt_bbox_mask = bbox_targets.min(-1)[0] > 0\n\n        # condition2: limit the regression range for each location\n        max_regress_distance = bbox_targets.max(-1)[0]\n        inside_regress_range = (\n            max_regress_distance >= regress_ranges[..., 0]) & (\n                max_regress_distance <= regress_ranges[..., 1])\n\n        # if there are still more than one objects for a location,\n        # we choose the one with minimal area\n        areas[inside_gt_bbox_mask == 0] = INF\n        areas[inside_regress_range == 0] = INF\n        min_area, min_area_inds = areas.min(dim=1)\n\n        labels = gt_labels[min_area_inds]\n        labels[min_area == INF] = 0\n        bbox_targets = bbox_targets[range(num_points), min_area_inds]\n\n        return labels, bbox_targets\n\n    def centerness_target(self, pos_bbox_targets):\n        # only calculate pos centerness targets, otherwise there may be nan\n        left_right = pos_bbox_targets[:, [0, 2]]\n        top_bottom = pos_bbox_targets[:, [1, 3]]\n        centerness_targets = (\n            left_right.min(dim=-1)[0] / left_right.max(dim=-1)[0]) * (\n                top_bottom.min(dim=-1)[0] / top_bottom.max(dim=-1)[0])\n        return torch.sqrt(centerness_targets)\n'"
mmdet/models/anchor_heads/ga_retina_head.py,1,"b'import torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom .guided_anchor_head import GuidedAnchorHead, FeatureAdaption\nfrom ..registry import HEADS\nfrom ..utils import bias_init_with_prob, ConvModule\nfrom mmdet.ops import MaskedConv2d\n\n\n@HEADS.register_module\nclass GARetinaHead(GuidedAnchorHead):\n    """"""Guided-Anchor-based RetinaNet head.""""""\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        super(GARetinaHead, self).__init__(num_classes, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(chn,\n                           self.feat_channels,\n                           3,\n                           stride=1,\n                           padding=1,\n                           conv_cfg=self.conv_cfg,\n                           norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(chn,\n                           self.feat_channels,\n                           3,\n                           stride=1,\n                           padding=1,\n                           conv_cfg=self.conv_cfg,\n                           norm_cfg=self.norm_cfg))\n\n        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)\n        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,\n                                    1)\n        self.feature_adaption_cls = FeatureAdaption(\n            self.feat_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.feature_adaption_reg = FeatureAdaption(\n            self.feat_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.retina_cls = MaskedConv2d(self.feat_channels,\n                                       self.num_anchors *\n                                       self.cls_out_channels,\n                                       3,\n                                       padding=1)\n        self.retina_reg = MaskedConv2d(self.feat_channels,\n                                       self.num_anchors * 4,\n                                       3,\n                                       padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n\n        self.feature_adaption_cls.init_weights()\n        self.feature_adaption_reg.init_weights()\n\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.conv_loc, std=0.01, bias=bias_cls)\n        normal_init(self.conv_shape, std=0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n\n        loc_pred = self.conv_loc(cls_feat)\n        shape_pred = self.conv_shape(reg_feat)\n\n        cls_feat = self.feature_adaption_cls(cls_feat, shape_pred)\n        reg_feat = self.feature_adaption_reg(reg_feat, shape_pred)\n\n        if not self.training:\n            mask = loc_pred.sigmoid()[0] >= self.loc_filter_thr\n        else:\n            mask = None\n        cls_score = self.retina_cls(cls_feat, mask)\n        bbox_pred = self.retina_reg(reg_feat, mask)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n'"
mmdet/models/anchor_heads/ga_rpn_head.py,5,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import delta2bbox\nfrom mmdet.ops import nms\nfrom .guided_anchor_head import GuidedAnchorHead\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass GARPNHead(GuidedAnchorHead):\n    """"""Guided-Anchor-based RPN head.""""""\n\n    def __init__(self, in_channels, **kwargs):\n        super(GARPNHead, self).__init__(2, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(self.in_channels,\n                                  self.feat_channels,\n                                  3,\n                                  padding=1)\n        super(GARPNHead, self)._init_layers()\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        super(GARPNHead, self).init_weights()\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        (cls_score, bbox_pred, shape_pred,\n         loc_pred) = super(GARPNHead, self).forward_single(x)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             shape_preds,\n             loc_preds,\n             gt_bboxes,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        losses = super(GARPNHead, self).loss(cls_scores,\n                                             bbox_preds,\n                                             shape_preds,\n                                             loc_preds,\n                                             gt_bboxes,\n                                             None,\n                                             img_metas,\n                                             cfg,\n                                             gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(loss_rpn_cls=losses[\'loss_cls\'],\n                    loss_rpn_bbox=losses[\'loss_bbox\'],\n                    loss_anchor_shape=losses[\'loss_shape\'],\n                    loss_anchor_loc=losses[\'loss_loc\'])\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          mlvl_masks,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        mlvl_proposals = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            anchors = mlvl_anchors[idx]\n            mask = mlvl_masks[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            # if no location is kept, end.\n            if mask.sum() == 0:\n                continue\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                scores = rpn_cls_score.softmax(dim=1)[:, 1]\n            # filter scores, bbox_pred w.r.t. mask.\n            # anchors are filtered in get_anchors() beforehand.\n            scores = scores[mask]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1,\n                                                                   4)[mask, :]\n            if scores.dim() == 0:\n                rpn_bbox_pred = rpn_bbox_pred.unsqueeze(0)\n                anchors = anchors.unsqueeze(0)\n                scores = scores.unsqueeze(0)\n            # filter anchors, bbox_pred, scores w.r.t. scores\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                _, topk_inds = scores.topk(cfg.nms_pre)\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n                scores = scores[topk_inds]\n            # get proposals w.r.t. anchors and rpn_bbox_pred\n            proposals = delta2bbox(anchors, rpn_bbox_pred, self.target_means,\n                                   self.target_stds, img_shape)\n            # filter out too small bboxes\n            if cfg.min_bbox_size > 0:\n                w = proposals[:, 2] - proposals[:, 0] + 1\n                h = proposals[:, 3] - proposals[:, 1] + 1\n                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &\n                                           (h >= cfg.min_bbox_size)).squeeze()\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n            proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n            # NMS in current level\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.nms_post, :]\n            mlvl_proposals.append(proposals)\n        proposals = torch.cat(mlvl_proposals, 0)\n        if cfg.nms_across_levels:\n            # NMS across multi levels\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.max_num, :]\n        else:\n            scores = proposals[:, 4]\n            num = min(cfg.max_num, proposals.shape[0])\n            _, topk_inds = scores.topk(num)\n            proposals = proposals[topk_inds, :]\n        return proposals\n'"
mmdet/models/anchor_heads/guided_anchor_head.py,6,"b'from __future__ import division\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import (AnchorGenerator, anchor_target, anchor_inside_flags,\n                        ga_loc_target, ga_shape_target, delta2bbox,\n                        multi_apply, multiclass_nms, force_fp32)\nfrom mmdet.ops import DeformConv, MaskedConv2d\nfrom ..builder import build_loss\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\nfrom ..utils import bias_init_with_prob\n\n\nclass FeatureAdaption(nn.Module):\n    """"""Feature Adaption Module.\n\n    Feature Adaption Module is implemented based on DCN v1.\n    It uses anchor shape prediction rather than feature map to\n    predict offsets of deformable conv layer.\n\n    Args:\n        in_channels (int): Number of channels in the input feature map.\n        out_channels (int): Number of channels in the output feature map.\n        kernel_size (int): Deformable conv kernel size.\n        deformable_groups (int): Deformable conv group size.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size=3,\n                 deformable_groups=4):\n        super(FeatureAdaption, self).__init__()\n        offset_channels = kernel_size * kernel_size * 2\n        self.conv_offset = nn.Conv2d(\n            2, deformable_groups * offset_channels, 1, bias=False)\n        self.conv_adaption = DeformConv(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            padding=(kernel_size - 1) // 2,\n            deformable_groups=deformable_groups)\n        self.relu = nn.ReLU(inplace=True)\n\n    def init_weights(self):\n        normal_init(self.conv_offset, std=0.1)\n        normal_init(self.conv_adaption, std=0.01)\n\n    def forward(self, x, shape):\n        offset = self.conv_offset(shape.detach())\n        x = self.relu(self.conv_adaption(x, offset))\n        return x\n\n\n@HEADS.register_module\nclass GuidedAnchorHead(AnchorHead):\n    """"""Guided-Anchor-based head (GA-RPN, GA-RetinaNet, etc.).\n\n    This GuidedAnchorHead will predict high-quality feature guided\n    anchors and locations where anchors will be kept in inference.\n    There are mainly 3 categories of bounding-boxes.\n    - Sampled (9) pairs for target assignment. (approxes)\n    - The square boxes where the predicted anchors are based on.\n        (squares)\n    - Guided anchors.\n    Please refer to https://arxiv.org/abs/1901.03278 for more details.\n\n    Args:\n        num_classes (int): Number of classes.\n        in_channels (int): Number of channels in the input feature map.\n        feat_channels (int): Number of channels of the feature map.\n        octave_base_scale (int): Base octave scale of each level of\n            feature map.\n        scales_per_octave (int): Number of octave scales in each level of\n            feature map\n        octave_ratios (Iterable): octave aspect ratios.\n        anchor_strides (Iterable): Anchor strides.\n        anchor_base_sizes (Iterable): Anchor base sizes.\n        anchoring_means (Iterable): Mean values of anchoring targets.\n        anchoring_stds (Iterable): Std values of anchoring targets.\n        target_means (Iterable): Mean values of regression targets.\n        target_stds (Iterable): Std values of regression targets.\n        deformable_groups: (int): Group number of DCN in\n            FeatureAdaption module.\n        loc_filter_thr (float): Threshold to filter out unconcerned regions.\n        loss_loc (dict): Config of location loss.\n        loss_shape (dict): Config of anchor shape loss.\n        loss_cls (dict): Config of classification loss.\n        loss_bbox (dict): Config of bbox regression loss.\n    """"""\n\n    def __init__(\n            self,\n            num_classes,\n            in_channels,\n            feat_channels=256,\n            octave_base_scale=8,\n            scales_per_octave=3,\n            octave_ratios=[0.5, 1.0, 2.0],\n            anchor_strides=[4, 8, 16, 32, 64],\n            anchor_base_sizes=None,\n            anchoring_means=(.0, .0, .0, .0),\n            anchoring_stds=(1.0, 1.0, 1.0, 1.0),\n            target_means=(.0, .0, .0, .0),\n            target_stds=(1.0, 1.0, 1.0, 1.0),\n            deformable_groups=4,\n            loc_filter_thr=0.01,\n            loss_loc=dict(\n                type=\'FocalLoss\',\n                use_sigmoid=True,\n                gamma=2.0,\n                alpha=0.25,\n                loss_weight=1.0),\n            loss_shape=dict(type=\'BoundedIoULoss\', beta=0.2, loss_weight=1.0),\n            loss_cls=dict(\n                type=\'CrossEntropyLoss\', use_sigmoid=True, loss_weight=1.0),\n            loss_bbox=dict(type=\'SmoothL1Loss\', beta=1.0, loss_weight=1.0)):\n        super(AnchorHead, self).__init__()\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.feat_channels = feat_channels\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.octave_scales = octave_base_scale * np.array(\n            [2**(i / scales_per_octave) for i in range(scales_per_octave)])\n        self.approxs_per_octave = len(self.octave_scales) * len(octave_ratios)\n        self.octave_ratios = octave_ratios\n        self.anchor_strides = anchor_strides\n        self.anchor_base_sizes = list(\n            anchor_strides) if anchor_base_sizes is None else anchor_base_sizes\n        self.anchoring_means = anchoring_means\n        self.anchoring_stds = anchoring_stds\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.deformable_groups = deformable_groups\n        self.loc_filter_thr = loc_filter_thr\n        self.approx_generators = []\n        self.square_generators = []\n        for anchor_base in self.anchor_base_sizes:\n            # Generators for approxs\n            self.approx_generators.append(\n                AnchorGenerator(anchor_base, self.octave_scales,\n                                self.octave_ratios))\n            # Generators for squares\n            self.square_generators.append(\n                AnchorGenerator(anchor_base, [self.octave_base_scale], [1.0]))\n        # one anchor per location\n        self.num_anchors = 1\n        self.use_sigmoid_cls = loss_cls.get(\'use_sigmoid\', False)\n        self.cls_focal_loss = loss_cls[\'type\'] in [\'FocalLoss\']\n        self.loc_focal_loss = loss_loc[\'type\'] in [\'FocalLoss\']\n        if self.use_sigmoid_cls:\n            self.cls_out_channels = self.num_classes - 1\n        else:\n            self.cls_out_channels = self.num_classes\n\n        # build losses\n        self.loss_loc = build_loss(loss_loc)\n        self.loss_shape = build_loss(loss_shape)\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n\n        self.fp16_enabled = False\n\n        self._init_layers()\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.conv_loc = nn.Conv2d(self.feat_channels, 1, 1)\n        self.conv_shape = nn.Conv2d(self.feat_channels, self.num_anchors * 2,\n                                    1)\n        self.feature_adaption = FeatureAdaption(\n            self.feat_channels,\n            self.feat_channels,\n            kernel_size=3,\n            deformable_groups=self.deformable_groups)\n        self.conv_cls = MaskedConv2d(self.feat_channels,\n                                     self.num_anchors * self.cls_out_channels,\n                                     1)\n        self.conv_reg = MaskedConv2d(self.feat_channels, self.num_anchors * 4,\n                                     1)\n\n    def init_weights(self):\n        normal_init(self.conv_cls, std=0.01)\n        normal_init(self.conv_reg, std=0.01)\n\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.conv_loc, std=0.01, bias=bias_cls)\n        normal_init(self.conv_shape, std=0.01)\n\n        self.feature_adaption.init_weights()\n\n    def forward_single(self, x):\n        loc_pred = self.conv_loc(x)\n        shape_pred = self.conv_shape(x)\n        x = self.feature_adaption(x, shape_pred)\n        # masked conv is only used during inference for speed-up\n        if not self.training:\n            mask = loc_pred.sigmoid()[0] >= self.loc_filter_thr\n        else:\n            mask = None\n        cls_score = self.conv_cls(x, mask)\n        bbox_pred = self.conv_reg(x, mask)\n        return cls_score, bbox_pred, shape_pred, loc_pred\n\n    def forward(self, feats):\n        return multi_apply(self.forward_single, feats)\n\n    def get_sampled_approxs(self, featmap_sizes, img_metas, cfg):\n        """"""Get sampled approxs and inside flags according to feature map sizes.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            img_metas (list[dict]): Image meta info.\n\n        Returns:\n            tuple: approxes of each image, inside flags of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # approxes for one time\n        multi_level_approxs = []\n        for i in range(num_levels):\n            approxs = self.approx_generators[i].grid_anchors(\n                featmap_sizes[i], self.anchor_strides[i])\n            multi_level_approxs.append(approxs)\n        approxs_list = [multi_level_approxs for _ in range(num_imgs)]\n\n        # for each image, we compute inside flags of multi level approxes\n        inside_flag_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_flags = []\n            multi_level_approxs = approxs_list[img_id]\n            for i in range(num_levels):\n                approxs = multi_level_approxs[i]\n                anchor_stride = self.anchor_strides[i]\n                feat_h, feat_w = featmap_sizes[i]\n                h, w, _ = img_meta[\'pad_shape\']\n                valid_feat_h = min(int(np.ceil(h / anchor_stride)), feat_h)\n                valid_feat_w = min(int(np.ceil(w / anchor_stride)), feat_w)\n                flags = self.approx_generators[i].valid_flags(\n                    (feat_h, feat_w), (valid_feat_h, valid_feat_w))\n                inside_flags_list = []\n                for i in range(self.approxs_per_octave):\n                    split_valid_flags = flags[i::self.approxs_per_octave]\n                    split_approxs = approxs[i::self.approxs_per_octave, :]\n                    inside_flags = anchor_inside_flags(\n                        split_approxs, split_valid_flags,\n                        img_meta[\'img_shape\'][:2], cfg.allowed_border)\n                    inside_flags_list.append(inside_flags)\n                # inside_flag for a position is true if any anchor in this\n                # position is true\n                inside_flags = (\n                    torch.stack(inside_flags_list, 0).sum(dim=0) > 0)\n                multi_level_flags.append(inside_flags)\n            inside_flag_list.append(multi_level_flags)\n        return approxs_list, inside_flag_list\n\n    def get_anchors(self,\n                    featmap_sizes,\n                    shape_preds,\n                    loc_preds,\n                    img_metas,\n                    use_loc_filter=False):\n        """"""Get squares according to feature map sizes and guided\n        anchors.\n\n        Args:\n            featmap_sizes (list[tuple]): Multi-level feature map sizes.\n            shape_preds (list[tensor]): Multi-level shape predictions.\n            loc_preds (list[tensor]): Multi-level location predictions.\n            img_metas (list[dict]): Image meta info.\n            use_loc_filter (bool): Use loc filter or not.\n\n        Returns:\n            tuple: square approxs of each image, guided anchors of each image,\n                loc masks of each image\n        """"""\n        num_imgs = len(img_metas)\n        num_levels = len(featmap_sizes)\n\n        # since feature map sizes of all images are the same, we only compute\n        # squares for one time\n        multi_level_squares = []\n        for i in range(num_levels):\n            squares = self.square_generators[i].grid_anchors(\n                featmap_sizes[i], self.anchor_strides[i])\n            multi_level_squares.append(squares)\n        squares_list = [multi_level_squares for _ in range(num_imgs)]\n\n        # for each image, we compute multi level guided anchors\n        guided_anchors_list = []\n        loc_mask_list = []\n        for img_id, img_meta in enumerate(img_metas):\n            multi_level_guided_anchors = []\n            multi_level_loc_mask = []\n            for i in range(num_levels):\n                squares = squares_list[img_id][i]\n                shape_pred = shape_preds[i][img_id]\n                loc_pred = loc_preds[i][img_id]\n                guided_anchors, loc_mask = self.get_guided_anchors_single(\n                    squares,\n                    shape_pred,\n                    loc_pred,\n                    use_loc_filter=use_loc_filter)\n                multi_level_guided_anchors.append(guided_anchors)\n                multi_level_loc_mask.append(loc_mask)\n            guided_anchors_list.append(multi_level_guided_anchors)\n            loc_mask_list.append(multi_level_loc_mask)\n        return squares_list, guided_anchors_list, loc_mask_list\n\n    def get_guided_anchors_single(self,\n                                  squares,\n                                  shape_pred,\n                                  loc_pred,\n                                  use_loc_filter=False):\n        """"""Get guided anchors and loc masks for a single level.\n\n        Args:\n            square (tensor): Squares of a single level.\n            shape_pred (tensor): Shape predections of a single level.\n            loc_pred (tensor): Loc predections of a single level.\n            use_loc_filter (list[tensor]): Use loc filter or not.\n\n        Returns:\n            tuple: guided anchors, location masks\n        """"""\n        # calculate location filtering mask\n        loc_pred = loc_pred.sigmoid().detach()\n        if use_loc_filter:\n            loc_mask = loc_pred >= self.loc_filter_thr\n        else:\n            loc_mask = loc_pred >= 0.0\n        mask = loc_mask.permute(1, 2, 0).expand(-1, -1, self.num_anchors)\n        mask = mask.contiguous().view(-1)\n        # calculate guided anchors\n        squares = squares[mask]\n        anchor_deltas = shape_pred.permute(1, 2, 0).contiguous().view(\n            -1, 2).detach()[mask]\n        bbox_deltas = anchor_deltas.new_full(squares.size(), 0)\n        bbox_deltas[:, 2:] = anchor_deltas\n        guided_anchors = delta2bbox(\n            squares,\n            bbox_deltas,\n            self.anchoring_means,\n            self.anchoring_stds,\n            wh_ratio_clip=1e-6)\n        return guided_anchors, mask\n\n    def loss_shape_single(self, shape_pred, bbox_anchors, bbox_gts,\n                          anchor_weights, anchor_total_num):\n        shape_pred = shape_pred.permute(0, 2, 3, 1).contiguous().view(-1, 2)\n        bbox_anchors = bbox_anchors.contiguous().view(-1, 4)\n        bbox_gts = bbox_gts.contiguous().view(-1, 4)\n        anchor_weights = anchor_weights.contiguous().view(-1, 4)\n        bbox_deltas = bbox_anchors.new_full(bbox_anchors.size(), 0)\n        bbox_deltas[:, 2:] += shape_pred\n        # filter out negative samples to speed-up weighted_bounded_iou_loss\n        inds = torch.nonzero(anchor_weights[:, 0] > 0).squeeze(1)\n        bbox_deltas_ = bbox_deltas[inds]\n        bbox_anchors_ = bbox_anchors[inds]\n        bbox_gts_ = bbox_gts[inds]\n        anchor_weights_ = anchor_weights[inds]\n        pred_anchors_ = delta2bbox(\n            bbox_anchors_,\n            bbox_deltas_,\n            self.anchoring_means,\n            self.anchoring_stds,\n            wh_ratio_clip=1e-6)\n        loss_shape = self.loss_shape(\n            pred_anchors_,\n            bbox_gts_,\n            anchor_weights_,\n            avg_factor=anchor_total_num)\n        return loss_shape\n\n    def loss_loc_single(self, loc_pred, loc_target, loc_weight, loc_avg_factor,\n                        cfg):\n        loss_loc = self.loss_loc(\n            loc_pred.reshape(-1, 1),\n            loc_target.reshape(-1, 1).long(),\n            loc_weight.reshape(-1, 1),\n            avg_factor=loc_avg_factor)\n        return loss_loc\n\n    @force_fp32(\n        apply_to=(\'cls_scores\', \'bbox_preds\', \'shape_preds\', \'loc_preds\'))\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             shape_preds,\n             loc_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.approx_generators)\n\n        # get loc targets\n        loc_targets, loc_weights, loc_avg_factor = ga_loc_target(\n            gt_bboxes,\n            featmap_sizes,\n            self.octave_base_scale,\n            self.anchor_strides,\n            center_ratio=cfg.center_ratio,\n            ignore_ratio=cfg.ignore_ratio)\n\n        # get sampled approxes\n        approxs_list, inside_flag_list = self.get_sampled_approxs(\n            featmap_sizes, img_metas, cfg)\n        # get squares and guided anchors\n        squares_list, guided_anchors_list, _ = self.get_anchors(\n            featmap_sizes, shape_preds, loc_preds, img_metas)\n\n        # get shape targets\n        sampling = False if not hasattr(cfg, \'ga_sampler\') else True\n        shape_targets = ga_shape_target(\n            approxs_list,\n            inside_flag_list,\n            squares_list,\n            gt_bboxes,\n            img_metas,\n            self.approxs_per_octave,\n            cfg,\n            sampling=sampling)\n        if shape_targets is None:\n            return None\n        (bbox_anchors_list, bbox_gts_list, anchor_weights_list, anchor_fg_num,\n         anchor_bg_num) = shape_targets\n        anchor_total_num = (\n            anchor_fg_num if not sampling else anchor_fg_num + anchor_bg_num)\n\n        # get anchor targets\n        sampling = False if self.cls_focal_loss else True\n        label_channels = self.cls_out_channels if self.use_sigmoid_cls else 1\n        cls_reg_targets = anchor_target(\n            guided_anchors_list,\n            inside_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=label_channels,\n            sampling=sampling)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n        num_total_samples = (\n            num_total_pos if self.cls_focal_loss else num_total_pos +\n            num_total_neg)\n\n        # get classification and bbox regression losses\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            cls_scores,\n            bbox_preds,\n            labels_list,\n            label_weights_list,\n            bbox_targets_list,\n            bbox_weights_list,\n            num_total_samples=num_total_samples,\n            cfg=cfg)\n\n        # get anchor location loss\n        losses_loc = []\n        for i in range(len(loc_preds)):\n            loss_loc = self.loss_loc_single(\n                loc_preds[i],\n                loc_targets[i],\n                loc_weights[i],\n                loc_avg_factor=loc_avg_factor,\n                cfg=cfg)\n            losses_loc.append(loss_loc)\n\n        # get anchor shape loss\n        losses_shape = []\n        for i in range(len(shape_preds)):\n            loss_shape = self.loss_shape_single(\n                shape_preds[i],\n                bbox_anchors_list[i],\n                bbox_gts_list[i],\n                anchor_weights_list[i],\n                anchor_total_num=anchor_total_num)\n            losses_shape.append(loss_shape)\n\n        return dict(\n            loss_cls=losses_cls,\n            loss_bbox=losses_bbox,\n            loss_shape=losses_shape,\n            loss_loc=losses_loc)\n\n    @force_fp32(\n        apply_to=(\'cls_scores\', \'bbox_preds\', \'shape_preds\', \'loc_preds\'))\n    def get_bboxes(self,\n                   cls_scores,\n                   bbox_preds,\n                   shape_preds,\n                   loc_preds,\n                   img_metas,\n                   cfg,\n                   rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(shape_preds) == len(\n            loc_preds)\n        num_levels = len(cls_scores)\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        # get guided anchors\n        _, guided_anchors, loc_masks = self.get_anchors(\n            featmap_sizes,\n            shape_preds,\n            loc_preds,\n            img_metas,\n            use_loc_filter=not self.training)\n        result_list = []\n        for img_id in range(len(img_metas)):\n            cls_score_list = [\n                cls_scores[i][img_id].detach() for i in range(num_levels)\n            ]\n            bbox_pred_list = [\n                bbox_preds[i][img_id].detach() for i in range(num_levels)\n            ]\n            guided_anchor_list = [\n                guided_anchors[img_id][i].detach() for i in range(num_levels)\n            ]\n            loc_mask_list = [\n                loc_masks[img_id][i].detach() for i in range(num_levels)\n            ]\n            img_shape = img_metas[img_id][\'img_shape\']\n            scale_factor = img_metas[img_id][\'scale_factor\']\n            proposals = self.get_bboxes_single(cls_score_list, bbox_pred_list,\n                                               guided_anchor_list,\n                                               loc_mask_list, img_shape,\n                                               scale_factor, cfg, rescale)\n            result_list.append(proposals)\n        return result_list\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          mlvl_masks,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        assert len(cls_scores) == len(bbox_preds) == len(mlvl_anchors)\n        mlvl_bboxes = []\n        mlvl_scores = []\n        for cls_score, bbox_pred, anchors, mask in zip(cls_scores, bbox_preds,\n                                                       mlvl_anchors,\n                                                       mlvl_masks):\n            assert cls_score.size()[-2:] == bbox_pred.size()[-2:]\n            # if no location is kept, end.\n            if mask.sum() == 0:\n                continue\n            # reshape scores and bbox_pred\n            cls_score = cls_score.permute(1, 2,\n                                          0).reshape(-1, self.cls_out_channels)\n            if self.use_sigmoid_cls:\n                scores = cls_score.sigmoid()\n            else:\n                scores = cls_score.softmax(-1)\n            bbox_pred = bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            # filter scores, bbox_pred w.r.t. mask.\n            # anchors are filtered in get_anchors() beforehand.\n            scores = scores[mask, :]\n            bbox_pred = bbox_pred[mask, :]\n            if scores.dim() == 0:\n                anchors = anchors.unsqueeze(0)\n                scores = scores.unsqueeze(0)\n                bbox_pred = bbox_pred.unsqueeze(0)\n            # filter anchors, bbox_pred, scores w.r.t. scores\n            nms_pre = cfg.get(\'nms_pre\', -1)\n            if nms_pre > 0 and scores.shape[0] > nms_pre:\n                if self.use_sigmoid_cls:\n                    max_scores, _ = scores.max(dim=1)\n                else:\n                    max_scores, _ = scores[:, 1:].max(dim=1)\n                _, topk_inds = max_scores.topk(nms_pre)\n                anchors = anchors[topk_inds, :]\n                bbox_pred = bbox_pred[topk_inds, :]\n                scores = scores[topk_inds, :]\n            bboxes = delta2bbox(anchors, bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n            mlvl_bboxes.append(bboxes)\n            mlvl_scores.append(scores)\n        mlvl_bboxes = torch.cat(mlvl_bboxes)\n        if rescale:\n            mlvl_bboxes /= mlvl_bboxes.new_tensor(scale_factor)\n        mlvl_scores = torch.cat(mlvl_scores)\n        if self.use_sigmoid_cls:\n            padding = mlvl_scores.new_zeros(mlvl_scores.shape[0], 1)\n            mlvl_scores = torch.cat([padding, mlvl_scores], dim=1)\n        # multi class NMS\n        det_bboxes, det_labels = multiclass_nms(mlvl_bboxes, mlvl_scores,\n                                                cfg.score_thr, cfg.nms,\n                                                cfg.max_per_img)\n        return det_bboxes, det_labels\n'"
mmdet/models/anchor_heads/retina_head.py,1,"b'import numpy as np\nimport torch.nn as nn\nfrom mmcv.cnn import normal_init\n\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\nfrom ..utils import bias_init_with_prob, ConvModule\n\n\n@HEADS.register_module\nclass RetinaHead(AnchorHead):\n\n    def __init__(self,\n                 num_classes,\n                 in_channels,\n                 stacked_convs=4,\n                 octave_base_scale=4,\n                 scales_per_octave=3,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 **kwargs):\n        self.stacked_convs = stacked_convs\n        self.octave_base_scale = octave_base_scale\n        self.scales_per_octave = scales_per_octave\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        octave_scales = np.array(\n            [2**(i / scales_per_octave) for i in range(scales_per_octave)])\n        anchor_scales = octave_scales * octave_base_scale\n        super(RetinaHead, self).__init__(\n            num_classes, in_channels, anchor_scales=anchor_scales, **kwargs)\n\n    def _init_layers(self):\n        self.relu = nn.ReLU(inplace=True)\n        self.cls_convs = nn.ModuleList()\n        self.reg_convs = nn.ModuleList()\n        for i in range(self.stacked_convs):\n            chn = self.in_channels if i == 0 else self.feat_channels\n            self.cls_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n            self.reg_convs.append(\n                ConvModule(\n                    chn,\n                    self.feat_channels,\n                    3,\n                    stride=1,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.retina_cls = nn.Conv2d(\n            self.feat_channels,\n            self.num_anchors * self.cls_out_channels,\n            3,\n            padding=1)\n        self.retina_reg = nn.Conv2d(\n            self.feat_channels, self.num_anchors * 4, 3, padding=1)\n\n    def init_weights(self):\n        for m in self.cls_convs:\n            normal_init(m.conv, std=0.01)\n        for m in self.reg_convs:\n            normal_init(m.conv, std=0.01)\n        bias_cls = bias_init_with_prob(0.01)\n        normal_init(self.retina_cls, std=0.01, bias=bias_cls)\n        normal_init(self.retina_reg, std=0.01)\n\n    def forward_single(self, x):\n        cls_feat = x\n        reg_feat = x\n        for cls_conv in self.cls_convs:\n            cls_feat = cls_conv(cls_feat)\n        for reg_conv in self.reg_convs:\n            reg_feat = reg_conv(reg_feat)\n        cls_score = self.retina_cls(cls_feat)\n        bbox_pred = self.retina_reg(reg_feat)\n        return cls_score, bbox_pred\n'"
mmdet/models/anchor_heads/rpn_head.py,5,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import normal_init\n\nfrom mmdet.core import delta2bbox\nfrom mmdet.ops import nms\nfrom .anchor_head import AnchorHead\nfrom ..registry import HEADS\n\nimport ipdb\n\n@HEADS.register_module\nclass RPNHead(AnchorHead):\n\n    def __init__(self, in_channels, **kwargs):\n        super(RPNHead, self).__init__(2, in_channels, **kwargs)\n\n    def _init_layers(self):\n        self.rpn_conv = nn.Conv2d(\n            self.in_channels, self.feat_channels, 3, padding=1)\n        self.rpn_cls = nn.Conv2d(self.feat_channels,\n                                 self.num_anchors * self.cls_out_channels, 1)\n        self.rpn_reg = nn.Conv2d(self.feat_channels, self.num_anchors * 4, 1)\n\n    def init_weights(self):\n        normal_init(self.rpn_conv, std=0.01)\n        normal_init(self.rpn_cls, std=0.01)\n        normal_init(self.rpn_reg, std=0.01)\n\n    def forward_single(self, x):\n        x = self.rpn_conv(x)\n        x = F.relu(x, inplace=True)\n        rpn_cls_score = self.rpn_cls(x)\n        rpn_bbox_pred = self.rpn_reg(x)\n        return rpn_cls_score, rpn_bbox_pred\n\n    # \xe5\xae\x9e\xe9\x99\x85\xe4\xb8\x8a\xe6\x98\xafsuper\xe8\xb0\x83\xe7\x94\xa8\xe8\xb6\x85\xe7\xb1\xbbAnchorHead\xe7\x9a\x84loss\xe6\x96\xb9\xe6\xb3\x95\xe8\xae\xa1\xe7\xae\x97\xe6\x8d\x9f\xe5\xa4\xb1\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        losses = super(RPNHead, self).loss(\n            cls_scores,\n            bbox_preds,\n            gt_bboxes,\n            None,\n            img_metas,\n            cfg,\n            gt_bboxes_ignore=gt_bboxes_ignore)\n        return dict(\n            loss_rpn_cls=losses['loss_cls'], loss_rpn_bbox=losses['loss_bbox'])\n\n    def get_bboxes_single(self,\n                          cls_scores,\n                          bbox_preds,\n                          mlvl_anchors,\n                          img_shape,\n                          scale_factor,\n                          cfg,\n                          rescale=False):\n        mlvl_proposals = []\n        for idx in range(len(cls_scores)):\n            rpn_cls_score = cls_scores[idx]\n            rpn_bbox_pred = bbox_preds[idx]\n            assert rpn_cls_score.size()[-2:] == rpn_bbox_pred.size()[-2:]\n            anchors = mlvl_anchors[idx]\n            rpn_cls_score = rpn_cls_score.permute(1, 2, 0)\n            if self.use_sigmoid_cls:\n                rpn_cls_score = rpn_cls_score.reshape(-1)\n                scores = rpn_cls_score.sigmoid()\n            else:\n                rpn_cls_score = rpn_cls_score.reshape(-1, 2)\n                scores = rpn_cls_score.softmax(dim=1)[:, 1]\n            rpn_bbox_pred = rpn_bbox_pred.permute(1, 2, 0).reshape(-1, 4)\n            if cfg.nms_pre > 0 and scores.shape[0] > cfg.nms_pre:\n                _, topk_inds = scores.topk(cfg.nms_pre)\n                rpn_bbox_pred = rpn_bbox_pred[topk_inds, :]\n                anchors = anchors[topk_inds, :]\n                scores = scores[topk_inds]\n            proposals = delta2bbox(anchors, rpn_bbox_pred, self.target_means,\n                                   self.target_stds, img_shape)\n            if cfg.min_bbox_size > 0:\n                w = proposals[:, 2] - proposals[:, 0] + 1\n                h = proposals[:, 3] - proposals[:, 1] + 1\n                valid_inds = torch.nonzero((w >= cfg.min_bbox_size) &\n                                           (h >= cfg.min_bbox_size)).squeeze()\n                proposals = proposals[valid_inds, :]\n                scores = scores[valid_inds]\n            proposals = torch.cat([proposals, scores.unsqueeze(-1)], dim=-1)\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.nms_post, :]\n            mlvl_proposals.append(proposals)\n        proposals = torch.cat(mlvl_proposals, 0)\n        if cfg.nms_across_levels:\n            proposals, _ = nms(proposals, cfg.nms_thr)\n            proposals = proposals[:cfg.max_num, :]\n        else:\n            scores = proposals[:, 4]\n            num = min(cfg.max_num, proposals.shape[0])\n            _, topk_inds = scores.topk(num)\n            proposals = proposals[topk_inds, :]\n        return proposals\n"""
mmdet/models/anchor_heads/ssd_head.py,10,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom mmdet.core import AnchorGenerator, anchor_target, multi_apply\nfrom .anchor_head import AnchorHead\nfrom ..losses import smooth_l1_loss\nfrom ..registry import HEADS\n\n\n# TODO: add loss evaluator for SSD\n@HEADS.register_module\nclass SSDHead(AnchorHead):\n\n    def __init__(self,\n                 input_size=300,\n                 num_classes=81,\n                 in_channels=(512, 1024, 512, 256, 256, 256),\n                 anchor_strides=(8, 16, 32, 64, 100, 300),\n                 basesize_ratio_range=(0.1, 0.9),\n                 anchor_ratios=([2], [2, 3], [2, 3], [2, 3], [2], [2]),\n                 target_means=(.0, .0, .0, .0),\n                 target_stds=(1.0, 1.0, 1.0, 1.0)):\n        super(AnchorHead, self).__init__()\n        self.input_size = input_size\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.cls_out_channels = num_classes\n        num_anchors = [len(ratios) * 2 + 2 for ratios in anchor_ratios]\n        reg_convs = []\n        cls_convs = []\n        for i in range(len(in_channels)):\n            reg_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * 4,\n                    kernel_size=3,\n                    padding=1))\n            cls_convs.append(\n                nn.Conv2d(\n                    in_channels[i],\n                    num_anchors[i] * num_classes,\n                    kernel_size=3,\n                    padding=1))\n        self.reg_convs = nn.ModuleList(reg_convs)\n        self.cls_convs = nn.ModuleList(cls_convs)\n\n        min_ratio, max_ratio = basesize_ratio_range\n        min_ratio = int(min_ratio * 100)\n        max_ratio = int(max_ratio * 100)\n        step = int(np.floor(max_ratio - min_ratio) / (len(in_channels) - 2))\n        min_sizes = []\n        max_sizes = []\n        for r in range(int(min_ratio), int(max_ratio) + 1, step):\n            min_sizes.append(int(input_size * r / 100))\n            max_sizes.append(int(input_size * (r + step) / 100))\n        if input_size == 300:\n            if basesize_ratio_range[0] == 0.15:  # SSD300 COCO\n                min_sizes.insert(0, int(input_size * 7 / 100))\n                max_sizes.insert(0, int(input_size * 15 / 100))\n            elif basesize_ratio_range[0] == 0.2:  # SSD300 VOC\n                min_sizes.insert(0, int(input_size * 10 / 100))\n                max_sizes.insert(0, int(input_size * 20 / 100))\n        elif input_size == 512:\n            if basesize_ratio_range[0] == 0.1:  # SSD512 COCO\n                min_sizes.insert(0, int(input_size * 4 / 100))\n                max_sizes.insert(0, int(input_size * 10 / 100))\n            elif basesize_ratio_range[0] == 0.15:  # SSD512 VOC\n                min_sizes.insert(0, int(input_size * 7 / 100))\n                max_sizes.insert(0, int(input_size * 15 / 100))\n        self.anchor_generators = []\n        self.anchor_strides = anchor_strides\n        for k in range(len(anchor_strides)):\n            base_size = min_sizes[k]\n            stride = anchor_strides[k]\n            ctr = ((stride - 1) / 2., (stride - 1) / 2.)\n            scales = [1., np.sqrt(max_sizes[k] / min_sizes[k])]\n            ratios = [1.]\n            for r in anchor_ratios[k]:\n                ratios += [1 / r, r]  # 4 or 6 ratio\n            anchor_generator = AnchorGenerator(\n                base_size, scales, ratios, scale_major=False, ctr=ctr)\n            indices = list(range(len(ratios)))\n            indices.insert(1, len(indices))\n            anchor_generator.base_anchors = torch.index_select(\n                anchor_generator.base_anchors, 0, torch.LongTensor(indices))\n            self.anchor_generators.append(anchor_generator)\n\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.use_sigmoid_cls = False\n        self.cls_focal_loss = False\n        self.fp16_enabled = False\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform', bias=0)\n\n    def forward(self, feats):\n        cls_scores = []\n        bbox_preds = []\n        for feat, reg_conv, cls_conv in zip(feats, self.reg_convs,\n                                            self.cls_convs):\n            cls_scores.append(cls_conv(feat))\n            bbox_preds.append(reg_conv(feat))\n        return cls_scores, bbox_preds\n\n    def loss_single(self, cls_score, bbox_pred, labels, label_weights,\n                    bbox_targets, bbox_weights, num_total_samples, cfg):\n        loss_cls_all = F.cross_entropy(\n            cls_score, labels, reduction='none') * label_weights\n        pos_inds = (labels > 0).nonzero().view(-1)\n        neg_inds = (labels == 0).nonzero().view(-1)\n\n        num_pos_samples = pos_inds.size(0)\n        num_neg_samples = cfg.neg_pos_ratio * num_pos_samples\n        if num_neg_samples > neg_inds.size(0):\n            num_neg_samples = neg_inds.size(0)\n        topk_loss_cls_neg, _ = loss_cls_all[neg_inds].topk(num_neg_samples)\n        loss_cls_pos = loss_cls_all[pos_inds].sum()\n        loss_cls_neg = topk_loss_cls_neg.sum()\n        loss_cls = (loss_cls_pos + loss_cls_neg) / num_total_samples\n\n        loss_bbox = smooth_l1_loss(\n            bbox_pred,\n            bbox_targets,\n            bbox_weights,\n            beta=cfg.smoothl1_beta,\n            avg_factor=num_total_samples)\n        return loss_cls[None], loss_bbox\n\n    def loss(self,\n             cls_scores,\n             bbox_preds,\n             gt_bboxes,\n             gt_labels,\n             img_metas,\n             cfg,\n             gt_bboxes_ignore=None):\n        featmap_sizes = [featmap.size()[-2:] for featmap in cls_scores]\n        assert len(featmap_sizes) == len(self.anchor_generators)\n\n        anchor_list, valid_flag_list = self.get_anchors(\n            featmap_sizes, img_metas)\n        cls_reg_targets = anchor_target(\n            anchor_list,\n            valid_flag_list,\n            gt_bboxes,\n            img_metas,\n            self.target_means,\n            self.target_stds,\n            cfg,\n            gt_bboxes_ignore_list=gt_bboxes_ignore,\n            gt_labels_list=gt_labels,\n            label_channels=1,\n            sampling=False,\n            unmap_outputs=False)\n        if cls_reg_targets is None:\n            return None\n        (labels_list, label_weights_list, bbox_targets_list, bbox_weights_list,\n         num_total_pos, num_total_neg) = cls_reg_targets\n\n        num_images = len(img_metas)\n        all_cls_scores = torch.cat([\n            s.permute(0, 2, 3, 1).reshape(\n                num_images, -1, self.cls_out_channels) for s in cls_scores\n        ], 1)\n        all_labels = torch.cat(labels_list, -1).view(num_images, -1)\n        all_label_weights = torch.cat(label_weights_list,\n                                      -1).view(num_images, -1)\n        all_bbox_preds = torch.cat([\n            b.permute(0, 2, 3, 1).reshape(num_images, -1, 4)\n            for b in bbox_preds\n        ], -2)\n        all_bbox_targets = torch.cat(bbox_targets_list,\n                                     -2).view(num_images, -1, 4)\n        all_bbox_weights = torch.cat(bbox_weights_list,\n                                     -2).view(num_images, -1, 4)\n\n        losses_cls, losses_bbox = multi_apply(\n            self.loss_single,\n            all_cls_scores,\n            all_bbox_preds,\n            all_labels,\n            all_label_weights,\n            all_bbox_targets,\n            all_bbox_weights,\n            num_total_samples=num_total_pos,\n            cfg=cfg)\n        return dict(loss_cls=losses_cls, loss_bbox=losses_bbox)\n"""
mmdet/models/backbones/__init__.py,0,"b""from .resnet import ResNet, make_res_layer\nfrom .resnext import ResNeXt\nfrom .ssd_vgg import SSDVGG\nfrom .hrnet import HRNet\n\n__all__ = ['ResNet', 'make_res_layer', 'ResNeXt', 'SSDVGG', 'HRNet']\n"""
mmdet/models/backbones/hrnet.py,2,"b'import logging\n\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom ..registry import BACKBONES\nfrom ..utils import build_norm_layer, build_conv_layer\nfrom .resnet import BasicBlock, Bottleneck\n\n\nclass HRModule(nn.Module):\n    """""" High-Resolution Module for HRNet. In this module, every branch\n    has 4 BasicBlocks/Bottlenecks. Fusion/Exchange is in this module.\n    """"""\n\n    def __init__(self,\n                 num_branches,\n                 blocks,\n                 num_blocks,\n                 in_channels,\n                 num_channels,\n                 multiscale_output=True,\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\')):\n        super(HRModule, self).__init__()\n        self._check_branches(num_branches, num_blocks, in_channels,\n                             num_channels)\n\n        self.in_channels = in_channels\n        self.num_branches = num_branches\n\n        self.multiscale_output = multiscale_output\n        self.norm_cfg = norm_cfg\n        self.conv_cfg = conv_cfg\n        self.with_cp = with_cp\n        self.branches = self._make_branches(num_branches, blocks, num_blocks,\n                                            num_channels)\n        self.fuse_layers = self._make_fuse_layers()\n        self.relu = nn.ReLU(inplace=False)\n\n    def _check_branches(self, num_branches, num_blocks, in_channels,\n                        num_channels):\n        if num_branches != len(num_blocks):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_BLOCKS({})\'.format(\n                num_branches, len(num_blocks))\n            raise ValueError(error_msg)\n\n        if num_branches != len(num_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_CHANNELS({})\'.format(\n                num_branches, len(num_channels))\n            raise ValueError(error_msg)\n\n        if num_branches != len(in_channels):\n            error_msg = \'NUM_BRANCHES({}) <> NUM_INCHANNELS({})\'.format(\n                num_branches, len(in_channels))\n            raise ValueError(error_msg)\n\n    def _make_one_branch(self,\n                         branch_index,\n                         block,\n                         num_blocks,\n                         num_channels,\n                         stride=1):\n        downsample = None\n        if stride != 1 or \\\n                self.in_channels[branch_index] != \\\n                num_channels[branch_index] * block.expansion:\n            downsample = nn.Sequential(\n                build_conv_layer(\n                    self.conv_cfg,\n                    self.in_channels[branch_index],\n                    num_channels[branch_index] * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, num_channels[branch_index] *\n                                 block.expansion)[1])\n\n        layers = []\n        layers.append(\n            block(\n                self.in_channels[branch_index],\n                num_channels[branch_index],\n                stride,\n                downsample=downsample,\n                with_cp=self.with_cp,\n                norm_cfg=self.norm_cfg,\n                conv_cfg=self.conv_cfg))\n        self.in_channels[branch_index] = \\\n            num_channels[branch_index] * block.expansion\n        for i in range(1, num_blocks[branch_index]):\n            layers.append(\n                block(\n                    self.in_channels[branch_index],\n                    num_channels[branch_index],\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*layers)\n\n    def _make_branches(self, num_branches, block, num_blocks, num_channels):\n        branches = []\n\n        for i in range(num_branches):\n            branches.append(\n                self._make_one_branch(i, block, num_blocks, num_channels))\n\n        return nn.ModuleList(branches)\n\n    def _make_fuse_layers(self):\n        if self.num_branches == 1:\n            return None\n\n        num_branches = self.num_branches\n        in_channels = self.in_channels\n        fuse_layers = []\n        num_out_branches = num_branches if self.multiscale_output else 1\n        for i in range(num_out_branches):\n            fuse_layer = []\n            for j in range(num_branches):\n                if j > i:\n                    fuse_layer.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                in_channels[j],\n                                in_channels[i],\n                                kernel_size=1,\n                                stride=1,\n                                padding=0,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg, in_channels[i])[1],\n                            nn.Upsample(\n                                scale_factor=2**(j - i), mode=\'nearest\')))\n                elif j == i:\n                    fuse_layer.append(None)\n                else:\n                    conv_downsamples = []\n                    for k in range(i - j):\n                        if k == i - j - 1:\n                            conv_downsamples.append(\n                                nn.Sequential(\n                                    build_conv_layer(\n                                        self.conv_cfg,\n                                        in_channels[j],\n                                        in_channels[i],\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        bias=False),\n                                    build_norm_layer(self.norm_cfg,\n                                                     in_channels[i])[1]))\n                        else:\n                            conv_downsamples.append(\n                                nn.Sequential(\n                                    build_conv_layer(\n                                        self.conv_cfg,\n                                        in_channels[j],\n                                        in_channels[j],\n                                        kernel_size=3,\n                                        stride=2,\n                                        padding=1,\n                                        bias=False),\n                                    build_norm_layer(self.norm_cfg,\n                                                     in_channels[j])[1],\n                                    nn.ReLU(inplace=False)))\n                    fuse_layer.append(nn.Sequential(*conv_downsamples))\n            fuse_layers.append(nn.ModuleList(fuse_layer))\n\n        return nn.ModuleList(fuse_layers)\n\n    def forward(self, x):\n        if self.num_branches == 1:\n            return [self.branches[0](x[0])]\n\n        for i in range(self.num_branches):\n            x[i] = self.branches[i](x[i])\n\n        x_fuse = []\n        for i in range(len(self.fuse_layers)):\n            y = 0\n            for j in range(self.num_branches):\n                if i == j:\n                    y += x[j]\n                else:\n                    y += self.fuse_layers[i][j](x[j])\n            x_fuse.append(self.relu(y))\n        return x_fuse\n\n\n@BACKBONES.register_module\nclass HRNet(nn.Module):\n    """"""HRNet backbone.\n\n    High-Resolution Representations for Labeling Pixels and Regions\n    arXiv: https://arxiv.org/abs/1904.04514\n\n    Args:\n        extra (dict): detailed configuration for each stage of HRNet.\n        conv_cfg (dict): dictionary to construct and config conv layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    blocks_dict = {\'BASIC\': BasicBlock, \'BOTTLENECK\': Bottleneck}\n\n    def __init__(self,\n                 extra,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 norm_eval=True,\n                 with_cp=False,\n                 zero_init_residual=False):\n        super(HRNet, self).__init__()\n        self.extra = extra\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.norm_eval = norm_eval\n        self.with_cp = with_cp\n        self.zero_init_residual = zero_init_residual\n\n        # stem net\n        self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, 64, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(self.norm_cfg, 64, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            3,\n            64,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            self.conv_cfg,\n            64,\n            64,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n\n        self.add_module(self.norm2_name, norm2)\n        self.relu = nn.ReLU(inplace=True)\n\n        # stage 1\n        self.stage1_cfg = self.extra[\'stage1\']\n        num_channels = self.stage1_cfg[\'num_channels\'][0]\n        block_type = self.stage1_cfg[\'block\']\n        num_blocks = self.stage1_cfg[\'num_blocks\'][0]\n\n        block = self.blocks_dict[block_type]\n        stage1_out_channels = num_channels * block.expansion\n        self.layer1 = self._make_layer(block, 64, num_channels, num_blocks)\n\n        # stage 2\n        self.stage2_cfg = self.extra[\'stage2\']\n        num_channels = self.stage2_cfg[\'num_channels\']\n        block_type = self.stage2_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition1 = self._make_transition_layer([stage1_out_channels],\n                                                       num_channels)\n        self.stage2, pre_stage_channels = self._make_stage(\n            self.stage2_cfg, num_channels)\n\n        # stage 3\n        self.stage3_cfg = self.extra[\'stage3\']\n        num_channels = self.stage3_cfg[\'num_channels\']\n        block_type = self.stage3_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition2 = self._make_transition_layer(pre_stage_channels,\n                                                       num_channels)\n        self.stage3, pre_stage_channels = self._make_stage(\n            self.stage3_cfg, num_channels)\n\n        # stage 4\n        self.stage4_cfg = self.extra[\'stage4\']\n        num_channels = self.stage4_cfg[\'num_channels\']\n        block_type = self.stage4_cfg[\'block\']\n\n        block = self.blocks_dict[block_type]\n        num_channels = [channel * block.expansion for channel in num_channels]\n        self.transition3 = self._make_transition_layer(pre_stage_channels,\n                                                       num_channels)\n        self.stage4, pre_stage_channels = self._make_stage(\n            self.stage4_cfg, num_channels)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def _make_transition_layer(self, num_channels_pre_layer,\n                               num_channels_cur_layer):\n        num_branches_cur = len(num_channels_cur_layer)\n        num_branches_pre = len(num_channels_pre_layer)\n\n        transition_layers = []\n        for i in range(num_branches_cur):\n            if i < num_branches_pre:\n                if num_channels_cur_layer[i] != num_channels_pre_layer[i]:\n                    transition_layers.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                num_channels_pre_layer[i],\n                                num_channels_cur_layer[i],\n                                kernel_size=3,\n                                stride=1,\n                                padding=1,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg,\n                                             num_channels_cur_layer[i])[1],\n                            nn.ReLU(inplace=True)))\n                else:\n                    transition_layers.append(None)\n            else:\n                conv_downsamples = []\n                for j in range(i + 1 - num_branches_pre):\n                    in_channels = num_channels_pre_layer[-1]\n                    out_channels = num_channels_cur_layer[i] \\\n                        if j == i - num_branches_pre else in_channels\n                    conv_downsamples.append(\n                        nn.Sequential(\n                            build_conv_layer(\n                                self.conv_cfg,\n                                in_channels,\n                                out_channels,\n                                kernel_size=3,\n                                stride=2,\n                                padding=1,\n                                bias=False),\n                            build_norm_layer(self.norm_cfg, out_channels)[1],\n                            nn.ReLU(inplace=True)))\n                transition_layers.append(nn.Sequential(*conv_downsamples))\n\n        return nn.ModuleList(transition_layers)\n\n    def _make_layer(self, block, inplanes, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                build_conv_layer(\n                    self.conv_cfg,\n                    inplanes,\n                    planes * block.expansion,\n                    kernel_size=1,\n                    stride=stride,\n                    bias=False),\n                build_norm_layer(self.norm_cfg, planes * block.expansion)[1])\n\n        layers = []\n        layers.append(\n            block(\n                inplanes,\n                planes,\n                stride,\n                downsample=downsample,\n                with_cp=self.with_cp,\n                norm_cfg=self.norm_cfg,\n                conv_cfg=self.conv_cfg))\n        inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(\n                block(\n                    inplanes,\n                    planes,\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*layers)\n\n    def _make_stage(self, layer_config, in_channels, multiscale_output=True):\n        num_modules = layer_config[\'num_modules\']\n        num_branches = layer_config[\'num_branches\']\n        num_blocks = layer_config[\'num_blocks\']\n        num_channels = layer_config[\'num_channels\']\n        block = self.blocks_dict[layer_config[\'block\']]\n\n        hr_modules = []\n        for i in range(num_modules):\n            # multi_scale_output is only used for the last module\n            if not multiscale_output and i == num_modules - 1:\n                reset_multiscale_output = False\n            else:\n                reset_multiscale_output = True\n\n            hr_modules.append(\n                HRModule(\n                    num_branches,\n                    block,\n                    num_blocks,\n                    in_channels,\n                    num_channels,\n                    reset_multiscale_output,\n                    with_cp=self.with_cp,\n                    norm_cfg=self.norm_cfg,\n                    conv_cfg=self.conv_cfg))\n\n        return nn.Sequential(*hr_modules), in_channels\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.norm2(x)\n        x = self.relu(x)\n        x = self.layer1(x)\n\n        x_list = []\n        for i in range(self.stage2_cfg[\'num_branches\']):\n            if self.transition1[i] is not None:\n                x_list.append(self.transition1[i](x))\n            else:\n                x_list.append(x)\n        y_list = self.stage2(x_list)\n\n        x_list = []\n        for i in range(self.stage3_cfg[\'num_branches\']):\n            if self.transition2[i] is not None:\n                x_list.append(self.transition2[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage3(x_list)\n\n        x_list = []\n        for i in range(self.stage4_cfg[\'num_branches\']):\n            if self.transition3[i] is not None:\n                x_list.append(self.transition3[i](y_list[-1]))\n            else:\n                x_list.append(y_list[i])\n        y_list = self.stage4(x_list)\n\n        return y_list\n\n    def train(self, mode=True):\n        super(HRNet, self).train(mode)\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n'"
mmdet/models/backbones/resnet.py,3,"b'import logging\n\nimport torch.nn as nn\nimport torch.utils.checkpoint as cp\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv, ContextBlock\nfrom mmdet.models.plugins import GeneralizedAttention\n\nfrom ..registry import BACKBONES\nfrom ..utils import build_conv_layer, build_norm_layer\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 dcn=None,\n                 gcb=None,\n                 gen_attention=None):\n        super(BasicBlock, self).__init__()\n        assert dcn is None, ""Not implemented yet.""\n        assert gen_attention is None, ""Not implemented yet.""\n        assert gcb is None, ""Not implemented yet.""\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            3,\n            stride=stride,\n            padding=dilation,\n            dilation=dilation,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        self.conv2 = build_conv_layer(\n            conv_cfg, planes, planes, 3, padding=1, bias=False)\n        self.add_module(self.norm2_name, norm2)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n        self.dilation = dilation\n        assert not with_cp\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self,\n                 inplanes,\n                 planes,\n                 stride=1,\n                 dilation=1,\n                 downsample=None,\n                 style=\'pytorch\',\n                 with_cp=False,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\'),\n                 dcn=None,\n                 gcb=None,\n                 gen_attention=None):\n        """"""Bottleneck block for ResNet.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__()\n        assert style in [\'pytorch\', \'caffe\']\n        assert dcn is None or isinstance(dcn, dict)\n        assert gcb is None or isinstance(gcb, dict)\n        assert gen_attention is None or isinstance(gen_attention, dict)\n\n        self.inplanes = inplanes\n        self.planes = planes\n        self.stride = stride\n        self.dilation = dilation\n        self.style = style\n        self.with_cp = with_cp\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.dcn = dcn\n        self.with_dcn = dcn is not None\n        self.gcb = gcb\n        self.with_gcb = gcb is not None\n        self.gen_attention = gen_attention\n        self.with_gen_attention = gen_attention is not None\n\n        if self.style == \'pytorch\':\n            self.conv1_stride = 1\n            self.conv2_stride = stride\n        else:\n            self.conv1_stride = stride\n            self.conv2_stride = 1\n\n        self.norm1_name, norm1 = build_norm_layer(norm_cfg, planes, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(norm_cfg, planes, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            norm_cfg, planes * self.expansion, postfix=3)\n\n        self.conv1 = build_conv_layer(\n            conv_cfg,\n            inplanes,\n            planes,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = build_conv_layer(\n                conv_cfg,\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                bias=False)\n        else:\n            assert conv_cfg is None, \'conv_cfg must be None for DCN\'\n            deformable_groups = dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                planes,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation)\n            self.conv2 = conv_op(\n                planes,\n                planes,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=dilation,\n                dilation=dilation,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = build_conv_layer(\n            conv_cfg,\n            planes,\n            planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n\n        if self.with_gcb:\n            gcb_inplanes = planes * self.expansion\n            self.context_block = ContextBlock(inplanes=gcb_inplanes, **gcb)\n\n        # gen_attention\n        if self.with_gen_attention:\n            self.gen_attention_block = GeneralizedAttention(\n                planes, **gen_attention)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    @property\n    def norm2(self):\n        return getattr(self, self.norm2_name)\n\n    @property\n    def norm3(self):\n        return getattr(self, self.norm3_name)\n\n    def forward(self, x):\n\n        def _inner_forward(x):\n            identity = x\n\n            out = self.conv1(x)\n            out = self.norm1(out)\n            out = self.relu(out)\n\n            if not self.with_dcn:\n                out = self.conv2(out)\n            elif self.with_modulated_dcn:\n                offset_mask = self.conv2_offset(out)\n                offset = offset_mask[:, :18, :, :]\n                mask = offset_mask[:, -9:, :, :].sigmoid()\n                out = self.conv2(out, offset, mask)\n            else:\n                offset = self.conv2_offset(out)\n                out = self.conv2(out, offset)\n            out = self.norm2(out)\n            out = self.relu(out)\n\n            if self.with_gen_attention:\n                out = self.gen_attention_block(out)\n\n            out = self.conv3(out)\n            out = self.norm3(out)\n\n            if self.with_gcb:\n                out = self.context_block(out)\n\n            if self.downsample is not None:\n                identity = self.downsample(x)\n\n            out += identity\n\n            return out\n\n        if self.with_cp and x.requires_grad:\n            out = cp.checkpoint(_inner_forward, x)\n        else:\n            out = _inner_forward(x)\n\n        out = self.relu(out)\n\n        return out\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   conv_cfg=None,\n                   norm_cfg=dict(type=\'BN\'),\n                   dcn=None,\n                   gcb=None,\n                   gen_attention=None,\n                   gen_attention_blocks=[]):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            build_conv_layer(\n                conv_cfg,\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(norm_cfg, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes=inplanes,\n            planes=planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            style=style,\n            with_cp=with_cp,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            dcn=dcn,\n            gcb=gcb,\n            gen_attention=gen_attention if\n            (0 in gen_attention_blocks) else None))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes=inplanes,\n                planes=planes,\n                stride=1,\n                dilation=dilation,\n                style=style,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                dcn=dcn,\n                gcb=gcb,\n                gen_attention=gen_attention if\n                (i in gen_attention_blocks) else None))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNet(nn.Module):\n    """"""ResNet backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (stop grad and set eval mode).\n            -1 means not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        18: (BasicBlock, (2, 2, 2, 2)),\n        34: (BasicBlock, (3, 4, 6, 3)),\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self,\n                 depth,\n                 num_stages=4,\n                 strides=(1, 2, 2, 2),\n                 dilations=(1, 1, 1, 1),\n                 out_indices=(0, 1, 2, 3),\n                 style=\'pytorch\',\n                 frozen_stages=-1,\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'BN\', requires_grad=True),\n                 norm_eval=True,\n                 dcn=None,\n                 stage_with_dcn=(False, False, False, False),\n                 gcb=None,\n                 stage_with_gcb=(False, False, False, False),\n                 gen_attention=None,\n                 stage_with_gen_attention=((), (), (), ()),\n                 with_cp=False,\n                 zero_init_residual=True):\n        super(ResNet, self).__init__()\n        if depth not in self.arch_settings:\n            raise KeyError(\'invalid depth {} for resnet\'.format(depth))\n        self.depth = depth\n        self.num_stages = num_stages\n        assert num_stages >= 1 and num_stages <= 4\n        self.strides = strides\n        self.dilations = dilations\n        assert len(strides) == len(dilations) == num_stages\n        self.out_indices = out_indices\n        assert max(out_indices) < num_stages\n        self.style = style\n        self.frozen_stages = frozen_stages\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.with_cp = with_cp\n        self.norm_eval = norm_eval\n        self.dcn = dcn\n        self.stage_with_dcn = stage_with_dcn\n        if dcn is not None:\n            assert len(stage_with_dcn) == num_stages\n        self.gen_attention = gen_attention\n        self.gcb = gcb\n        self.stage_with_gcb = stage_with_gcb\n        if gcb is not None:\n            assert len(stage_with_gcb) == num_stages\n        self.zero_init_residual = zero_init_residual\n        self.block, stage_blocks = self.arch_settings[depth]\n        self.stage_blocks = stage_blocks[:num_stages]\n        self.inplanes = 64\n\n        self._make_stem_layer()\n\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = strides[i]\n            dilation = dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            gcb = self.gcb if self.stage_with_gcb[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                style=self.style,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                dcn=dcn,\n                gcb=gcb,\n                gen_attention=gen_attention,\n                gen_attention_blocks=stage_with_gen_attention[i])\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n\n        self.feat_dim = self.block.expansion * 64 * 2**(\n            len(self.stage_blocks) - 1)\n\n    @property\n    def norm1(self):\n        return getattr(self, self.norm1_name)\n\n    def _make_stem_layer(self):\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            3,\n            64,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False)\n        self.norm1_name, norm1 = build_norm_layer(self.norm_cfg, 64, postfix=1)\n        self.add_module(self.norm1_name, norm1)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n\n    def _freeze_stages(self):\n        if self.frozen_stages >= 0:\n            self.norm1.eval()\n            for m in [self.conv1, self.norm1]:\n                for param in m.parameters():\n                    param.requires_grad = False\n\n        for i in range(1, self.frozen_stages + 1):\n            m = getattr(self, \'layer{}\'.format(i))\n            m.eval()\n            for param in m.parameters():\n                param.requires_grad = False\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, (_BatchNorm, nn.GroupNorm)):\n                    constant_init(m, 1)\n\n            if self.dcn is not None:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck) and hasattr(\n                            m, \'conv2_offset\'):\n                        constant_init(m.conv2_offset, 0)\n\n            if self.zero_init_residual:\n                for m in self.modules():\n                    if isinstance(m, Bottleneck):\n                        constant_init(m.norm3, 0)\n                    elif isinstance(m, BasicBlock):\n                        constant_init(m.norm2, 0)\n        else:\n            raise TypeError(\'pretrained must be a str or None\')\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.norm1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        outs = []\n        for i, layer_name in enumerate(self.res_layers):\n            res_layer = getattr(self, layer_name)\n            x = res_layer(x)\n            if i in self.out_indices:\n                outs.append(x)\n        return tuple(outs)\n\n    def train(self, mode=True):\n        super(ResNet, self).train(mode)\n        self._freeze_stages()\n        if mode and self.norm_eval:\n            for m in self.modules():\n                # trick: eval have effect on BatchNorm only\n                if isinstance(m, _BatchNorm):\n                    m.eval()\n'"
mmdet/models/backbones/resnext.py,1,"b'import math\n\nimport torch.nn as nn\n\nfrom mmdet.ops import DeformConv, ModulatedDeformConv\nfrom .resnet import Bottleneck as _Bottleneck\nfrom .resnet import ResNet\nfrom ..registry import BACKBONES\nfrom ..utils import build_conv_layer, build_norm_layer\n\n\nclass Bottleneck(_Bottleneck):\n\n    def __init__(self, inplanes, planes, groups=1, base_width=4, **kwargs):\n        """"""Bottleneck block for ResNeXt.\n        If style is ""pytorch"", the stride-two layer is the 3x3 conv layer,\n        if it is ""caffe"", the stride-two layer is the first 1x1 conv layer.\n        """"""\n        super(Bottleneck, self).__init__(inplanes, planes, **kwargs)\n\n        if groups == 1:\n            width = self.planes\n        else:\n            width = math.floor(self.planes * (base_width / 64)) * groups\n\n        self.norm1_name, norm1 = build_norm_layer(\n            self.norm_cfg, width, postfix=1)\n        self.norm2_name, norm2 = build_norm_layer(\n            self.norm_cfg, width, postfix=2)\n        self.norm3_name, norm3 = build_norm_layer(\n            self.norm_cfg, self.planes * self.expansion, postfix=3)\n\n        self.conv1 = build_conv_layer(\n            self.conv_cfg,\n            self.inplanes,\n            width,\n            kernel_size=1,\n            stride=self.conv1_stride,\n            bias=False)\n        self.add_module(self.norm1_name, norm1)\n        fallback_on_stride = False\n        self.with_modulated_dcn = False\n        if self.with_dcn:\n            fallback_on_stride = self.dcn.get(\'fallback_on_stride\', False)\n            self.with_modulated_dcn = self.dcn.get(\'modulated\', False)\n        if not self.with_dcn or fallback_on_stride:\n            self.conv2 = build_conv_layer(\n                self.conv_cfg,\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                bias=False)\n        else:\n            assert self.conv_cfg is None, \'conv_cfg must be None for DCN\'\n            groups = self.dcn.get(\'groups\', 1)\n            deformable_groups = self.dcn.get(\'deformable_groups\', 1)\n            if not self.with_modulated_dcn:\n                conv_op = DeformConv\n                offset_channels = 18\n            else:\n                conv_op = ModulatedDeformConv\n                offset_channels = 27\n            self.conv2_offset = nn.Conv2d(\n                width,\n                deformable_groups * offset_channels,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation)\n            self.conv2 = conv_op(\n                width,\n                width,\n                kernel_size=3,\n                stride=self.conv2_stride,\n                padding=self.dilation,\n                dilation=self.dilation,\n                groups=groups,\n                deformable_groups=deformable_groups,\n                bias=False)\n        self.add_module(self.norm2_name, norm2)\n        self.conv3 = build_conv_layer(\n            self.conv_cfg,\n            width,\n            self.planes * self.expansion,\n            kernel_size=1,\n            bias=False)\n        self.add_module(self.norm3_name, norm3)\n\n\ndef make_res_layer(block,\n                   inplanes,\n                   planes,\n                   blocks,\n                   stride=1,\n                   dilation=1,\n                   groups=1,\n                   base_width=4,\n                   style=\'pytorch\',\n                   with_cp=False,\n                   conv_cfg=None,\n                   norm_cfg=dict(type=\'BN\'),\n                   dcn=None,\n                   gcb=None):\n    downsample = None\n    if stride != 1 or inplanes != planes * block.expansion:\n        downsample = nn.Sequential(\n            build_conv_layer(\n                conv_cfg,\n                inplanes,\n                planes * block.expansion,\n                kernel_size=1,\n                stride=stride,\n                bias=False),\n            build_norm_layer(norm_cfg, planes * block.expansion)[1],\n        )\n\n    layers = []\n    layers.append(\n        block(\n            inplanes=inplanes,\n            planes=planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            groups=groups,\n            base_width=base_width,\n            style=style,\n            with_cp=with_cp,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            dcn=dcn,\n            gcb=gcb))\n    inplanes = planes * block.expansion\n    for i in range(1, blocks):\n        layers.append(\n            block(\n                inplanes=inplanes,\n                planes=planes,\n                stride=1,\n                dilation=dilation,\n                groups=groups,\n                base_width=base_width,\n                style=style,\n                with_cp=with_cp,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                dcn=dcn,\n                gcb=gcb))\n\n    return nn.Sequential(*layers)\n\n\n@BACKBONES.register_module\nclass ResNeXt(ResNet):\n    """"""ResNeXt backbone.\n\n    Args:\n        depth (int): Depth of resnet, from {18, 34, 50, 101, 152}.\n        num_stages (int): Resnet stages, normally 4.\n        groups (int): Group of resnext.\n        base_width (int): Base width of resnext.\n        strides (Sequence[int]): Strides of the first block of each stage.\n        dilations (Sequence[int]): Dilation of each stage.\n        out_indices (Sequence[int]): Output from which stages.\n        style (str): `pytorch` or `caffe`. If set to ""pytorch"", the stride-two\n            layer is the 3x3 conv layer, otherwise the stride-two layer is\n            the first 1x1 conv layer.\n        frozen_stages (int): Stages to be frozen (all param fixed). -1 means\n            not freezing any parameters.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        norm_eval (bool): Whether to set norm layers to eval mode, namely,\n            freeze running stats (mean and var). Note: Effect on Batch Norm\n            and its variants only.\n        with_cp (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n        zero_init_residual (bool): whether to use zero init for last norm layer\n            in resblocks to let them behave as identity.\n    """"""\n\n    arch_settings = {\n        50: (Bottleneck, (3, 4, 6, 3)),\n        101: (Bottleneck, (3, 4, 23, 3)),\n        152: (Bottleneck, (3, 8, 36, 3))\n    }\n\n    def __init__(self, groups=1, base_width=4, **kwargs):\n        super(ResNeXt, self).__init__(**kwargs)\n        self.groups = groups\n        self.base_width = base_width\n\n        self.inplanes = 64\n        self.res_layers = []\n        for i, num_blocks in enumerate(self.stage_blocks):\n            stride = self.strides[i]\n            dilation = self.dilations[i]\n            dcn = self.dcn if self.stage_with_dcn[i] else None\n            gcb = self.gcb if self.stage_with_gcb[i] else None\n            planes = 64 * 2**i\n            res_layer = make_res_layer(\n                self.block,\n                self.inplanes,\n                planes,\n                num_blocks,\n                stride=stride,\n                dilation=dilation,\n                groups=self.groups,\n                base_width=self.base_width,\n                style=self.style,\n                with_cp=self.with_cp,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg,\n                dcn=dcn,\n                gcb=gcb)\n            self.inplanes = planes * self.block.expansion\n            layer_name = \'layer{}\'.format(i + 1)\n            self.add_module(layer_name, res_layer)\n            self.res_layers.append(layer_name)\n\n        self._freeze_stages()\n'"
mmdet/models/backbones/ssd_vgg.py,3,"b""import logging\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import (VGG, xavier_init, constant_init, kaiming_init,\n                      normal_init)\nfrom mmcv.runner import load_checkpoint\n\nfrom ..registry import BACKBONES\n\n\n@BACKBONES.register_module\nclass SSDVGG(VGG):\n    extra_setting = {\n        300: (256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256),\n        512: (256, 'S', 512, 128, 'S', 256, 128, 'S', 256, 128, 'S', 256, 128),\n    }\n\n    def __init__(self,\n                 input_size,\n                 depth,\n                 with_last_pool=False,\n                 ceil_mode=True,\n                 out_indices=(3, 4),\n                 out_feature_indices=(22, 34),\n                 l2_norm_scale=20.):\n        super(SSDVGG, self).__init__(\n            depth,\n            with_last_pool=with_last_pool,\n            ceil_mode=ceil_mode,\n            out_indices=out_indices)\n        assert input_size in (300, 512)\n        self.input_size = input_size\n\n        self.features.add_module(\n            str(len(self.features)),\n            nn.MaxPool2d(kernel_size=3, stride=1, padding=1))\n        self.features.add_module(\n            str(len(self.features)),\n            nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.features.add_module(\n            str(len(self.features)), nn.Conv2d(1024, 1024, kernel_size=1))\n        self.features.add_module(\n            str(len(self.features)), nn.ReLU(inplace=True))\n        self.out_feature_indices = out_feature_indices\n\n        self.inplanes = 1024\n        self.extra = self._make_extra_layers(self.extra_setting[input_size])\n        self.l2_norm = L2Norm(\n            self.features[out_feature_indices[0] - 1].out_channels,\n            l2_norm_scale)\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.features.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n                elif isinstance(m, nn.Linear):\n                    normal_init(m, std=0.01)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n        for m in self.extra.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n        constant_init(self.l2_norm, self.l2_norm.scale)\n\n    def forward(self, x):\n        outs = []\n        for i, layer in enumerate(self.features):\n            x = layer(x)\n            if i in self.out_feature_indices:\n                outs.append(x)\n        for i, layer in enumerate(self.extra):\n            x = F.relu(layer(x), inplace=True)\n            if i % 2 == 1:\n                outs.append(x)\n        outs[0] = self.l2_norm(outs[0])\n        if len(outs) == 1:\n            return outs[0]\n        else:\n            return tuple(outs)\n\n    def _make_extra_layers(self, outplanes):\n        layers = []\n        kernel_sizes = (1, 3)\n        num_layers = 0\n        outplane = None\n        for i in range(len(outplanes)):\n            if self.inplanes == 'S':\n                self.inplanes = outplane\n                continue\n            k = kernel_sizes[num_layers % 2]\n            if outplanes[i] == 'S':\n                outplane = outplanes[i + 1]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=2, padding=1)\n            else:\n                outplane = outplanes[i]\n                conv = nn.Conv2d(\n                    self.inplanes, outplane, k, stride=1, padding=0)\n            layers.append(conv)\n            self.inplanes = outplanes[i]\n            num_layers += 1\n        if self.input_size == 512:\n            layers.append(nn.Conv2d(self.inplanes, 256, 4, padding=1))\n\n        return nn.Sequential(*layers)\n\n\nclass L2Norm(nn.Module):\n\n    def __init__(self, n_dims, scale=20., eps=1e-10):\n        super(L2Norm, self).__init__()\n        self.n_dims = n_dims\n        self.weight = nn.Parameter(torch.Tensor(self.n_dims))\n        self.eps = eps\n        self.scale = scale\n\n    def forward(self, x):\n        # normalization layer convert to FP32 in FP16 training\n        x_float = x.float()\n        norm = x_float.pow(2).sum(1, keepdim=True).sqrt() + self.eps\n        return (self.weight[None, :, None, None].float().expand_as(x_float) *\n                x_float / norm).type_as(x)\n"""
mmdet/models/bbox_heads/__init__.py,0,"b""from .bbox_head import BBoxHead\nfrom .convfc_bbox_head import ConvFCBBoxHead, SharedFCBBoxHead\n\n__all__ = ['BBoxHead', 'ConvFCBBoxHead', 'SharedFCBBoxHead']\n"""
mmdet/models/bbox_heads/bbox_head.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmdet.core import (delta2bbox, multiclass_nms, bbox_target, force_fp32,\n                        auto_fp16)\nfrom ..builder import build_loss\nfrom ..losses import accuracy\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass BBoxHead(nn.Module):\n    """"""Simplest RoI head, with only two fc layers for classification and\n    regression respectively""""""\n\n    def __init__(self,\n                 with_avg_pool=False,   # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8a\xa0\xe4\xb8\x80\xe5\xb1\x82\xe5\x9d\x87\xe5\x80\xbc\xe6\xb1\xa0\xe5\x8c\x96\n                 with_cls=True,         # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8a\xa0cls\xe7\x9a\x84FC\xe5\xb1\x82    \n                 with_reg=True,         # \xe6\x98\xaf\xe5\x90\xa6\xe5\x8a\xa0reg\xe7\x9a\x84FC\xe5\xb1\x82\n                 roi_feat_size=7,\n                 in_channels=256,\n                 num_classes=81,\n                 target_means=[0., 0., 0., 0.],\n                 target_stds=[0.1, 0.1, 0.2, 0.2],\n                 reg_class_agnostic=False,\n                 loss_cls=dict(\n                     type=\'CrossEntropyLoss\',\n                     use_sigmoid=False,\n                     loss_weight=1.0),\n                 loss_bbox=dict(\n                     type=\'SmoothL1Loss\', beta=1.0, loss_weight=1.0)):\n        super(BBoxHead, self).__init__()\n        assert with_cls or with_reg\n        self.with_avg_pool = with_avg_pool\n        self.with_cls = with_cls\n        self.with_reg = with_reg\n        self.roi_feat_size = roi_feat_size\n        self.in_channels = in_channels\n        self.num_classes = num_classes\n        self.target_means = target_means\n        self.target_stds = target_stds\n        self.reg_class_agnostic = reg_class_agnostic\n        self.fp16_enabled = False\n\n        self.loss_cls = build_loss(loss_cls)\n        self.loss_bbox = build_loss(loss_bbox)\n\n        in_channels = self.in_channels\n        if self.with_avg_pool:\n            self.avg_pool = nn.AvgPool2d(roi_feat_size)\n        else:\n            in_channels *= (self.roi_feat_size * self.roi_feat_size)\n        if self.with_cls:\n            self.fc_cls = nn.Linear(in_channels, num_classes)\n        if self.with_reg:\n            out_dim_reg = 4 if reg_class_agnostic else 4 * num_classes\n            self.fc_reg = nn.Linear(in_channels, out_dim_reg)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        if self.with_cls:\n            nn.init.normal_(self.fc_cls.weight, 0, 0.01)\n            nn.init.constant_(self.fc_cls.bias, 0)\n        if self.with_reg:\n            nn.init.normal_(self.fc_reg.weight, 0, 0.001)\n            nn.init.constant_(self.fc_reg.bias, 0)\n\n    @auto_fp16()\n    def forward(self, x):\n        if self.with_avg_pool:\n            x = self.avg_pool(x)\n        x = x.view(x.size(0), -1)\n        cls_score = self.fc_cls(x) if self.with_cls else None\n        bbox_pred = self.fc_reg(x) if self.with_reg else None\n        return cls_score, bbox_pred\n\n    def get_target(self, sampling_results, gt_bboxes, gt_labels,\n                   rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        neg_proposals = [res.neg_bboxes for res in sampling_results]\n        pos_gt_bboxes = [res.pos_gt_bboxes for res in sampling_results]\n        pos_gt_labels = [res.pos_gt_labels for res in sampling_results]\n        reg_classes = 1 if self.reg_class_agnostic else self.num_classes\n        cls_reg_targets = bbox_target(\n            pos_proposals,\n            neg_proposals,\n            pos_gt_bboxes,\n            pos_gt_labels,\n            rcnn_train_cfg,\n            reg_classes,\n            target_means=self.target_means,\n            target_stds=self.target_stds)\n        return cls_reg_targets\n\n    @force_fp32(apply_to=(\'cls_score\', \'bbox_pred\'))\n    def loss(self,\n             cls_score,\n             bbox_pred,\n             labels,\n             label_weights,\n             bbox_targets,\n             bbox_weights,\n             reduction_override=None):\n        losses = dict()\n        if cls_score is not None:\n            avg_factor = max(torch.sum(label_weights > 0).float().item(), 1.)\n            losses[\'loss_cls\'] = self.loss_cls(\n                cls_score,\n                labels,\n                label_weights,\n                avg_factor=avg_factor,\n                reduction_override=reduction_override)\n            losses[\'acc\'] = accuracy(cls_score, labels)\n        if bbox_pred is not None:\n            pos_inds = labels > 0\n            if self.reg_class_agnostic:\n                pos_bbox_pred = bbox_pred.view(bbox_pred.size(0), 4)[pos_inds]\n            else:\n                pos_bbox_pred = bbox_pred.view(bbox_pred.size(0), -1,\n                                               4)[pos_inds, labels[pos_inds]]\n            losses[\'loss_bbox\'] = self.loss_bbox(\n                pos_bbox_pred,\n                bbox_targets[pos_inds],\n                bbox_weights[pos_inds],\n                avg_factor=bbox_targets.size(0),\n                reduction_override=reduction_override)\n        return losses\n\n    @force_fp32(apply_to=(\'cls_score\', \'bbox_pred\'))\n    def get_det_bboxes(self,\n                       rois,\n                       cls_score,\n                       bbox_pred,\n                       img_shape,\n                       scale_factor,\n                       rescale=False,\n                       cfg=None):\n        if isinstance(cls_score, list):\n            cls_score = sum(cls_score) / float(len(cls_score))\n        scores = F.softmax(cls_score, dim=1) if cls_score is not None else None\n\n        if bbox_pred is not None:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_shape)\n        else:\n            bboxes = rois[:, 1:].clone()\n            if img_shape is not None:\n                bboxes[:, [0, 2]].clamp_(min=0, max=img_shape[1] - 1)\n                bboxes[:, [1, 3]].clamp_(min=0, max=img_shape[0] - 1)\n\n        if rescale:\n            bboxes /= scale_factor\n\n        if cfg is None:\n            return bboxes, scores\n        else:\n            det_bboxes, det_labels = multiclass_nms(bboxes, scores,\n                                                    cfg.score_thr, cfg.nms,\n                                                    cfg.max_per_img)\n\n            return det_bboxes, det_labels\n\n    @force_fp32(apply_to=(\'bbox_preds\', ))\n    def refine_bboxes(self, rois, labels, bbox_preds, pos_is_gts, img_metas):\n        """"""Refine bboxes during training.\n\n        Args:\n            rois (Tensor): Shape (n*bs, 5), where n is image number per GPU,\n                and bs is the sampled RoIs per image.\n            labels (Tensor): Shape (n*bs, ).\n            bbox_preds (Tensor): Shape (n*bs, 4) or (n*bs, 4*#class).\n            pos_is_gts (list[Tensor]): Flags indicating if each positive bbox\n                is a gt bbox.\n            img_metas (list[dict]): Meta info of each image.\n\n        Returns:\n            list[Tensor]: Refined bboxes of each image in a mini-batch.\n        """"""\n        img_ids = rois[:, 0].long().unique(sorted=True)\n        assert img_ids.numel() == len(img_metas)\n\n        bboxes_list = []\n        for i in range(len(img_metas)):\n            inds = torch.nonzero(rois[:, 0] == i).squeeze()\n            num_rois = inds.numel()\n\n            bboxes_ = rois[inds, 1:]\n            label_ = labels[inds]\n            bbox_pred_ = bbox_preds[inds]\n            img_meta_ = img_metas[i]\n            pos_is_gts_ = pos_is_gts[i]\n\n            bboxes = self.regress_by_class(bboxes_, label_, bbox_pred_,\n                                           img_meta_)\n            # filter gt bboxes\n            pos_keep = 1 - pos_is_gts_\n            keep_inds = pos_is_gts_.new_ones(num_rois)\n            keep_inds[:len(pos_is_gts_)] = pos_keep\n\n            bboxes_list.append(bboxes[keep_inds])\n\n        return bboxes_list\n\n    @force_fp32(apply_to=(\'bbox_pred\', ))\n    def regress_by_class(self, rois, label, bbox_pred, img_meta):\n        """"""Regress the bbox for the predicted class. Used in Cascade R-CNN.\n\n        Args:\n            rois (Tensor): shape (n, 4) or (n, 5)\n            label (Tensor): shape (n, )\n            bbox_pred (Tensor): shape (n, 4*(#class+1)) or (n, 4)\n            img_meta (dict): Image meta info.\n\n        Returns:\n            Tensor: Regressed bboxes, the same shape as input rois.\n        """"""\n        assert rois.size(1) == 4 or rois.size(1) == 5\n\n        if not self.reg_class_agnostic:\n            label = label * 4\n            inds = torch.stack((label, label + 1, label + 2, label + 3), 1)\n            bbox_pred = torch.gather(bbox_pred, 1, inds)\n        assert bbox_pred.size(1) == 4\n\n        if rois.size(1) == 4:\n            new_rois = delta2bbox(rois, bbox_pred, self.target_means,\n                                  self.target_stds, img_meta[\'img_shape\'])\n        else:\n            bboxes = delta2bbox(rois[:, 1:], bbox_pred, self.target_means,\n                                self.target_stds, img_meta[\'img_shape\'])\n            new_rois = torch.cat((rois[:, [0]], bboxes), dim=1)\n\n        return new_rois\n'"
mmdet/models/bbox_heads/convfc_bbox_head.py,1,"b'import torch.nn as nn\n\nfrom .bbox_head import BBoxHead\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\n\n# \xe6\x89\xa7\xe8\xa1\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe5\xbe\x97\xe5\x88\xb0\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe5\x88\x86\xe6\x95\xb0\xe4\xb8\x8e\xe5\x81\x8f\xe7\xa7\xbb\n@HEADS.register_module\nclass ConvFCBBoxHead(BBoxHead):\n    """"""More general bbox head, with shared conv and fc layers and two optional\n    separated branches.\n\n                                /-> cls convs -> cls fcs -> cls\n    shared convs -> shared fcs\n                                \\-> reg convs -> reg fcs -> reg\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_shared_convs=0,\n                 num_shared_fcs=0,\n                 num_cls_convs=0,\n                 num_cls_fcs=0,\n                 num_reg_convs=0,\n                 num_reg_fcs=0,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 *args,\n                 **kwargs):\n        super(ConvFCBBoxHead, self).__init__(*args, **kwargs)\n        assert (num_shared_convs + num_shared_fcs + num_cls_convs +\n                num_cls_fcs + num_reg_convs + num_reg_fcs > 0)\n        if num_cls_convs > 0 or num_reg_convs > 0:\n            assert num_shared_fcs == 0\n        if not self.with_cls:\n            assert num_cls_convs == 0 and num_cls_fcs == 0\n        if not self.with_reg:\n            assert num_reg_convs == 0 and num_reg_fcs == 0\n        self.num_shared_convs = num_shared_convs\n        self.num_shared_fcs = num_shared_fcs\n        self.num_cls_convs = num_cls_convs\n        self.num_cls_fcs = num_cls_fcs\n        self.num_reg_convs = num_reg_convs\n        self.num_reg_fcs = num_reg_fcs\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        # add shared convs and fcs\n        self.shared_convs, self.shared_fcs, last_layer_dim = \\\n            self._add_conv_fc_branch(\n                self.num_shared_convs, self.num_shared_fcs, self.in_channels,\n                True)\n        self.shared_out_channels = last_layer_dim\n\n        # add cls specific branch\n        self.cls_convs, self.cls_fcs, self.cls_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_cls_convs, self.num_cls_fcs, self.shared_out_channels)\n\n        # add reg specific branch\n        self.reg_convs, self.reg_fcs, self.reg_last_dim = \\\n            self._add_conv_fc_branch(\n                self.num_reg_convs, self.num_reg_fcs, self.shared_out_channels)\n\n        if self.num_shared_fcs == 0 and not self.with_avg_pool:\n            if self.num_cls_fcs == 0:\n                self.cls_last_dim *= (self.roi_feat_size * self.roi_feat_size)\n            if self.num_reg_fcs == 0:\n                self.reg_last_dim *= (self.roi_feat_size * self.roi_feat_size)\n\n        self.relu = nn.ReLU(inplace=True)\n        # reconstruct fc_cls and fc_reg since input channels are changed\n        if self.with_cls:\n            self.fc_cls = nn.Linear(self.cls_last_dim, self.num_classes)\n        if self.with_reg:\n            out_dim_reg = (4 if self.reg_class_agnostic else 4 *\n                           self.num_classes)\n            self.fc_reg = nn.Linear(self.reg_last_dim, out_dim_reg)\n\n    def _add_conv_fc_branch(self,\n                            num_branch_convs,\n                            num_branch_fcs,\n                            in_channels,\n                            is_shared=False):\n        """"""Add shared or separable branch\n\n        convs -> avg pool (optional) -> fcs\n        """"""\n        last_layer_dim = in_channels\n        # add branch specific conv layers\n        branch_convs = nn.ModuleList()\n        if num_branch_convs > 0:\n            for i in range(num_branch_convs):\n                conv_in_channels = (\n                    last_layer_dim if i == 0 else self.conv_out_channels)\n                branch_convs.append(\n                    ConvModule(\n                        conv_in_channels,\n                        self.conv_out_channels,\n                        3,\n                        padding=1,\n                        conv_cfg=self.conv_cfg,\n                        norm_cfg=self.norm_cfg))\n            last_layer_dim = self.conv_out_channels\n        # add branch specific fc layers\n        branch_fcs = nn.ModuleList()\n        if num_branch_fcs > 0:\n            # for shared branch, only consider self.with_avg_pool\n            # for separated branches, also consider self.num_shared_fcs\n            if (is_shared\n                    or self.num_shared_fcs == 0) and not self.with_avg_pool:\n                last_layer_dim *= (self.roi_feat_size * self.roi_feat_size)\n            for i in range(num_branch_fcs):\n                fc_in_channels = (\n                    last_layer_dim if i == 0 else self.fc_out_channels)\n                branch_fcs.append(\n                    nn.Linear(fc_in_channels, self.fc_out_channels))\n            last_layer_dim = self.fc_out_channels\n        return branch_convs, branch_fcs, last_layer_dim\n\n    def init_weights(self):\n        super(ConvFCBBoxHead, self).init_weights()\n        for module_list in [self.shared_fcs, self.cls_fcs, self.reg_fcs]:\n            for m in module_list.modules():\n                if isinstance(m, nn.Linear):\n                    nn.init.xavier_uniform_(m.weight)\n                    nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        # shared part\n        if self.num_shared_convs > 0:\n            for conv in self.shared_convs:\n                x = conv(x)\n\n        # \xe5\x85\xb1\xe4\xba\xab\xe7\x9a\x84fc\xe5\xb1\x82\n        if self.num_shared_fcs > 0:\n            if self.with_avg_pool:\n                x = self.avg_pool(x)\n            x = x.view(x.size(0), -1)\n            for fc in self.shared_fcs:\n                x = self.relu(fc(x))\n                \n        # separate branches\xe5\x88\x86\xe6\x88\x90\xe5\x88\x86\xe7\xb1\xbb\xe5\x92\x8c\xe5\x9b\x9e\xe5\xbd\x92\xe4\xb8\xa4\xe4\xb8\xaa\xe5\x88\x86\xe6\x94\xaf\n        x_cls = x\n        x_reg = x\n\n        for conv in self.cls_convs:\n            x_cls = conv(x_cls)\n        if x_cls.dim() > 2:\n            if self.with_avg_pool:\n                x_cls = self.avg_pool(x_cls)\n            x_cls = x_cls.view(x_cls.size(0), -1)\n        for fc in self.cls_fcs:\n            x_cls = self.relu(fc(x_cls))\n\n        for conv in self.reg_convs:\n            x_reg = conv(x_reg)\n        if x_reg.dim() > 2:\n            if self.with_avg_pool:\n                x_reg = self.avg_pool(x_reg)\n            x_reg = x_reg.view(x_reg.size(0), -1)\n        for fc in self.reg_fcs:\n            x_reg = self.relu(fc(x_reg))\n\n        cls_score = self.fc_cls(x_cls) if self.with_cls else None\n        bbox_pred = self.fc_reg(x_reg) if self.with_reg else None\n        return cls_score, bbox_pred\n\n\n# \xe9\x80\x90\xe5\xb1\x82\xe7\xbb\xa7\xe6\x89\xbf\xe7\x88\xb6\xe7\xb1\xbb\xef\xbc\x8c\xe7\x9b\xb4\xe5\x88\xb0BBoxHead\xe7\x94\xb1\xe5\x86\x85\xe5\x90\x91\xe5\xa4\x96\xe9\x80\x90\xe6\xad\xa5\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n# \xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe5\x88\xb0\xe8\xbf\x99\xe9\x87\x8c\xe6\x97\xb6\xef\xbc\x8c\xe6\x89\xbe\xe4\xb8\x8d\xe5\x88\xb0forward\xe7\xbb\xa7\xe7\xbb\xad\xe5\x90\x91\xe4\xb8\x8a\xef\xbc\x8c\xe5\x9c\xa8\xe7\x88\xb6\xe7\xb1\xbbConvFCBBoxHead\xe4\xb8\xad\xe6\x89\xbe\xe5\x88\xb0forward\xef\xbc\x8c\xe4\xbb\x8e\xe9\x82\xa3\xe9\x87\x8c\xe5\xbc\x80\xe5\xa7\x8b\xe6\x89\xa7\xe8\xa1\x8c\n\n@HEADS.register_module\nclass SharedFCBBoxHead(ConvFCBBoxHead):\n\n    def __init__(self, num_fcs=2, fc_out_channels=1024, *args, **kwargs):\n        assert num_fcs >= 1\n        super(SharedFCBBoxHead, self).__init__(\n            num_shared_convs=0,\n            num_shared_fcs=num_fcs,\n            num_cls_convs=0,\n            num_cls_fcs=0,\n            num_reg_convs=0,\n            num_reg_fcs=0,\n            fc_out_channels=fc_out_channels,\n            *args,\n            **kwargs)\n'"
mmdet/models/detectors/__init__.py,0,"b""from .base import BaseDetector\nfrom .single_stage import SingleStageDetector\nfrom .two_stage import TwoStageDetector\nfrom .rpn import RPN\nfrom .fast_rcnn import FastRCNN\nfrom .faster_rcnn import FasterRCNN\nfrom .mask_rcnn import MaskRCNN\nfrom .cascade_rcnn import CascadeRCNN\nfrom .htc import HybridTaskCascade\nfrom .retinanet import RetinaNet\nfrom .fcos import FCOS\nfrom .grid_rcnn import GridRCNN\nfrom .mask_scoring_rcnn import MaskScoringRCNN\n\n__all__ = [\n    'BaseDetector', 'SingleStageDetector', 'TwoStageDetector', 'RPN',\n    'FastRCNN', 'FasterRCNN', 'MaskRCNN', 'CascadeRCNN', 'HybridTaskCascade',\n    'RetinaNet', 'FCOS', 'GridRCNN', 'MaskScoringRCNN'\n]\n"""
mmdet/models/detectors/base.py,1,"b'import logging\nfrom abc import ABCMeta, abstractmethod\n\nimport mmcv\nimport numpy as np\nimport torch.nn as nn\nimport pycocotools.mask as maskUtils\n\nfrom mmdet.core import tensor2imgs, get_classes, auto_fp16\n\nimport ipdb\n\nclass BaseDetector(nn.Module):\n    """"""Base class for detectors""""""\n\n    __metaclass__ = ABCMeta\n\n    def __init__(self):\n        super(BaseDetector, self).__init__()\n        self.fp16_enabled = False\n\n    @property\n    def with_neck(self):\n        return hasattr(self, \'neck\') and self.neck is not None\n\n    @property\n    def with_shared_head(self):\n        return hasattr(self, \'shared_head\') and self.shared_head is not None\n\n    @property\n    def with_bbox(self):\n        return hasattr(self, \'bbox_head\') and self.bbox_head is not None\n\n    @property\n    def with_mask(self):\n        return hasattr(self, \'mask_head\') and self.mask_head is not None\n\n    @abstractmethod\n    def extract_feat(self, imgs):\n        pass\n\n    def extract_feats(self, imgs):\n        assert isinstance(imgs, list)\n        for img in imgs:\n            yield self.extract_feat(img)\n\n    @abstractmethod\n    def forward_train(self, imgs, img_metas, **kwargs):\n        pass\n\n    @abstractmethod\n    def simple_test(self, img, img_meta, **kwargs):\n        pass\n\n    @abstractmethod\n    def aug_test(self, imgs, img_metas, **kwargs):\n        pass\n\n    def init_weights(self, pretrained=None):\n        if pretrained is not None:\n            logger = logging.getLogger()\n            logger.info(\'load model from: {}\'.format(pretrained))\n\n    def forward_test(self, imgs, img_metas, **kwargs):\n        for var, name in [(imgs, \'imgs\'), (img_metas, \'img_metas\')]:\n            if not isinstance(var, list):\n                raise TypeError(\'{} must be a list, but got {}\'.format(\n                    name, type(var)))\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                \'num of augmentations ({}) != num of image meta ({})\'.format(\n                    len(imgs), len(img_metas)))\n        # TODO: remove the restriction of imgs_per_gpu == 1 when prepared\n        imgs_per_gpu = imgs[0].size(0)\n        assert imgs_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], **kwargs)\n        else:\n            return self.aug_test(imgs, img_metas, **kwargs)\n\n    @auto_fp16(apply_to=(\'img\', ))\n    def forward(self, img, img_meta, return_loss=True, **kwargs):\n        if return_loss:\n            return self.forward_train(img, img_meta, **kwargs)\n        else:\n            return self.forward_test(img, img_meta, **kwargs)\n\n    def show_result(self,\n                    data,\n                    result,\n                    img_norm_cfg,\n                    dataset=None,\n                    score_thr=0.3):\n        if isinstance(result, tuple):\n            bbox_result, segm_result = result\n        else:\n            bbox_result, segm_result = result, None\n\n        img_tensor = data[\'img\'][0]\n        img_metas = data[\'img_meta\'][0].data[0]\n        imgs = tensor2imgs(img_tensor, **img_norm_cfg)\n        assert len(imgs) == len(img_metas)\n\n        if dataset is None:\n            class_names = self.CLASSES\n        elif isinstance(dataset, str):\n            class_names = get_classes(dataset)\n        elif isinstance(dataset, (list, tuple)):\n            class_names = dataset\n        else:\n            raise TypeError(\n                \'dataset must be a valid dataset name or a sequence\'\n                \' of class names, not {}\'.format(type(dataset)))\n\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n\n            bboxes = np.vstack(bbox_result)\n            # draw segmentation masks\n            if segm_result is not None:\n                segms = mmcv.concat_list(segm_result)\n                inds = np.where(bboxes[:, -1] > score_thr)[0]\n                for i in inds:\n                    color_mask = np.random.randint(\n                        0, 256, (1, 3), dtype=np.uint8)\n                    mask = maskUtils.decode(segms[i]).astype(np.bool)\n                    img_show[mask] = img_show[mask] * 0.5 + color_mask * 0.5\n            # draw bounding boxes\n            labels = [\n                np.full(bbox.shape[0], i, dtype=np.int32)\n                for i, bbox in enumerate(bbox_result)\n            ]\n            labels = np.concatenate(labels)\n            mmcv.imshow_det_bboxes(\n                img_show,\n                bboxes,\n                labels,\n                class_names=class_names,\n                score_thr=score_thr)\n'"
mmdet/models/detectors/cascade_rcnn.py,8,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import (build_assigner, bbox2roi, bbox2result, build_sampler,\n                        merge_aug_masks)\n\n\n@DETECTORS.register_module\nclass CascadeRCNN(BaseDetector, RPNTestMixin):\n\n    def __init__(self,\n                 num_stages,\n                 backbone,\n                 neck=None,\n                 shared_head=None,\n                 rpn_head=None,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        assert bbox_roi_extractor is not None\n        assert bbox_head is not None\n        super(CascadeRCNN, self).__init__()\n\n        self.num_stages = num_stages\n        self.backbone = builder.build_backbone(backbone)\n\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n\n        if rpn_head is not None:\n            self.rpn_head = builder.build_head(rpn_head)\n\n        if shared_head is not None:\n            self.shared_head = builder.build_shared_head(shared_head)\n\n        if bbox_head is not None:\n            self.bbox_roi_extractor = nn.ModuleList()\n            self.bbox_head = nn.ModuleList()\n            if not isinstance(bbox_roi_extractor, list):\n                bbox_roi_extractor = [\n                    bbox_roi_extractor for _ in range(num_stages)\n                ]\n            if not isinstance(bbox_head, list):\n                bbox_head = [bbox_head for _ in range(num_stages)]\n            assert len(bbox_roi_extractor) == len(bbox_head) == self.num_stages\n            for roi_extractor, head in zip(bbox_roi_extractor, bbox_head):\n                self.bbox_roi_extractor.append(\n                    builder.build_roi_extractor(roi_extractor))\n                self.bbox_head.append(builder.build_head(head))\n\n        if mask_head is not None:\n            self.mask_head = nn.ModuleList()\n            if not isinstance(mask_head, list):\n                mask_head = [mask_head for _ in range(num_stages)]\n            assert len(mask_head) == self.num_stages\n            for head in mask_head:\n                self.mask_head.append(builder.build_head(head))\n            if mask_roi_extractor is not None:\n                self.share_roi_extractor = False\n                self.mask_roi_extractor = nn.ModuleList()\n                if not isinstance(mask_roi_extractor, list):\n                    mask_roi_extractor = [\n                        mask_roi_extractor for _ in range(num_stages)\n                    ]\n                assert len(mask_roi_extractor) == self.num_stages\n                for roi_extractor in mask_roi_extractor:\n                    self.mask_roi_extractor.append(\n                        builder.build_roi_extractor(roi_extractor))\n            else:\n                self.share_roi_extractor = True\n                self.mask_roi_extractor = self.bbox_roi_extractor\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    def init_weights(self, pretrained=None):\n        super(CascadeRCNN, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        if self.with_shared_head:\n            self.shared_head.init_weights(pretrained=pretrained)\n        for i in range(self.num_stages):\n            if self.with_bbox:\n                self.bbox_roi_extractor[i].init_weights()\n                self.bbox_head[i].init_weights()\n            if self.with_mask:\n                if not self.share_roi_extractor:\n                    self.mask_roi_extractor[i].init_weights()\n                self.mask_head[i].init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)\n            proposal_inputs = rpn_outs + (img_meta, proposal_cfg)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        for i in range(self.num_stages):\n            self.current_stage = i\n            rcnn_train_cfg = self.train_cfg.rcnn[i]\n            lw = self.train_cfg.stage_loss_weights[i]\n\n            # assign gts and sample proposals\n            sampling_results = []\n            if self.with_bbox or self.with_mask:\n                bbox_assigner = build_assigner(rcnn_train_cfg.assigner)\n                bbox_sampler = build_sampler(\n                    rcnn_train_cfg.sampler, context=self)\n                num_imgs = img.size(0)\n                if gt_bboxes_ignore is None:\n                    gt_bboxes_ignore = [None for _ in range(num_imgs)]\n\n                for j in range(num_imgs):\n                    assign_result = bbox_assigner.assign(\n                        proposal_list[j], gt_bboxes[j], gt_bboxes_ignore[j],\n                        gt_labels[j])\n                    sampling_result = bbox_sampler.sample(\n                        assign_result,\n                        proposal_list[j],\n                        gt_bboxes[j],\n                        gt_labels[j],\n                        feats=[lvl_feat[j][None] for lvl_feat in x])\n                    sampling_results.append(sampling_result)\n\n            # bbox head forward and loss\n            bbox_roi_extractor = self.bbox_roi_extractor[i]\n            bbox_head = self.bbox_head[i]\n\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            bbox_feats = bbox_roi_extractor(x[:bbox_roi_extractor.num_inputs],\n                                            rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n            cls_score, bbox_pred = bbox_head(bbox_feats)\n\n            bbox_targets = bbox_head.get_target(sampling_results, gt_bboxes,\n                                                gt_labels, rcnn_train_cfg)\n            loss_bbox = bbox_head.loss(cls_score, bbox_pred, *bbox_targets)\n            for name, value in loss_bbox.items():\n                losses[\'s{}.{}\'.format(i, name)] = (\n                    value * lw if \'loss\' in name else value)\n\n            # mask head forward and loss\n            if self.with_mask:\n                if not self.share_roi_extractor:\n                    mask_roi_extractor = self.mask_roi_extractor[i]\n                    pos_rois = bbox2roi(\n                        [res.pos_bboxes for res in sampling_results])\n                    mask_feats = mask_roi_extractor(\n                        x[:mask_roi_extractor.num_inputs], pos_rois)\n                    if self.with_shared_head:\n                        mask_feats = self.shared_head(mask_feats)\n                else:\n                    # reuse positive bbox feats\n                    pos_inds = []\n                    device = bbox_feats.device\n                    for res in sampling_results:\n                        pos_inds.append(\n                            torch.ones(\n                                res.pos_bboxes.shape[0],\n                                device=device,\n                                dtype=torch.uint8))\n                        pos_inds.append(\n                            torch.zeros(\n                                res.neg_bboxes.shape[0],\n                                device=device,\n                                dtype=torch.uint8))\n                    pos_inds = torch.cat(pos_inds)\n                    mask_feats = bbox_feats[pos_inds]\n                mask_head = self.mask_head[i]\n                mask_pred = mask_head(mask_feats)\n                mask_targets = mask_head.get_target(sampling_results, gt_masks,\n                                                    rcnn_train_cfg)\n                pos_labels = torch.cat(\n                    [res.pos_gt_labels for res in sampling_results])\n                loss_mask = mask_head.loss(mask_pred, mask_targets, pos_labels)\n                for name, value in loss_mask.items():\n                    losses[\'s{}.{}\'.format(i, name)] = (\n                        value * lw if \'loss\' in name else value)\n\n            # refine bboxes\n            if i < self.num_stages - 1:\n                pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                roi_labels = bbox_targets[0]  # bbox_targets is a tuple\n                with torch.no_grad():\n                    proposal_list = bbox_head.refine_bboxes(\n                        rois, roi_labels, bbox_pred, pos_is_gts, img_meta)\n\n        return losses\n\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        img_shape = img_meta[0][\'img_shape\']\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n\n        # ""ms"" in variable names means multi-stage\n        ms_bbox_result = {}\n        ms_segm_result = {}\n        ms_scores = []\n        rcnn_test_cfg = self.test_cfg.rcnn\n\n        rois = bbox2roi(proposal_list)\n        for i in range(self.num_stages):\n            bbox_roi_extractor = self.bbox_roi_extractor[i]\n            bbox_head = self.bbox_head[i]\n\n            bbox_feats = bbox_roi_extractor(\n                x[:len(bbox_roi_extractor.featmap_strides)], rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n\n            cls_score, bbox_pred = bbox_head(bbox_feats)\n            ms_scores.append(cls_score)\n\n            if self.test_cfg.keep_all_stages:\n                det_bboxes, det_labels = bbox_head.get_det_bboxes(\n                    rois,\n                    cls_score,\n                    bbox_pred,\n                    img_shape,\n                    scale_factor,\n                    rescale=rescale,\n                    cfg=rcnn_test_cfg)\n                bbox_result = bbox2result(det_bboxes, det_labels,\n                                          bbox_head.num_classes)\n                ms_bbox_result[\'stage{}\'.format(i)] = bbox_result\n\n                if self.with_mask:\n                    mask_roi_extractor = self.mask_roi_extractor[i]\n                    mask_head = self.mask_head[i]\n                    if det_bboxes.shape[0] == 0:\n                        segm_result = [\n                            [] for _ in range(mask_head.num_classes - 1)\n                        ]\n                    else:\n                        _bboxes = (\n                            det_bboxes[:, :4] * scale_factor\n                            if rescale else det_bboxes)\n                        mask_rois = bbox2roi([_bboxes])\n                        mask_feats = mask_roi_extractor(\n                            x[:len(mask_roi_extractor.featmap_strides)],\n                            mask_rois)\n                        if self.with_shared_head:\n                            mask_feats = self.shared_head(mask_feats, i)\n                        mask_pred = mask_head(mask_feats)\n                        segm_result = mask_head.get_seg_masks(\n                            mask_pred, _bboxes, det_labels, rcnn_test_cfg,\n                            ori_shape, scale_factor, rescale)\n                    ms_segm_result[\'stage{}\'.format(i)] = segm_result\n\n            if i < self.num_stages - 1:\n                bbox_label = cls_score.argmax(dim=1)\n                rois = bbox_head.regress_by_class(rois, bbox_label, bbox_pred,\n                                                  img_meta[0])\n\n        cls_score = sum(ms_scores) / self.num_stages\n        det_bboxes, det_labels = self.bbox_head[-1].get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n        ms_bbox_result[\'ensemble\'] = bbox_result\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                segm_result = [\n                    [] for _ in range(self.mask_head[-1].num_classes - 1)\n                ]\n            else:\n                _bboxes = (\n                    det_bboxes[:, :4] * scale_factor\n                    if rescale else det_bboxes)\n                mask_rois = bbox2roi([_bboxes])\n                aug_masks = []\n                for i in range(self.num_stages):\n                    mask_roi_extractor = self.mask_roi_extractor[i]\n                    mask_feats = mask_roi_extractor(\n                        x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n                    if self.with_shared_head:\n                        mask_feats = self.shared_head(mask_feats)\n                    mask_pred = self.mask_head[i](mask_feats)\n                    aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n                merged_masks = merge_aug_masks(aug_masks,\n                                               [img_meta] * self.num_stages,\n                                               self.test_cfg.rcnn)\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks, _bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, scale_factor, rescale)\n            ms_segm_result[\'ensemble\'] = segm_result\n\n        if not self.test_cfg.keep_all_stages:\n            if self.with_mask:\n                results = (ms_bbox_result[\'ensemble\'],\n                           ms_segm_result[\'ensemble\'])\n            else:\n                results = ms_bbox_result[\'ensemble\']\n        else:\n            if self.with_mask:\n                results = {\n                    stage: (ms_bbox_result[stage], ms_segm_result[stage])\n                    for stage in ms_bbox_result\n                }\n            else:\n                results = ms_bbox_result\n\n        return results\n\n    def aug_test(self, img, img_meta, proposals=None, rescale=False):\n        raise NotImplementedError\n\n    def show_result(self, data, result, img_norm_cfg, **kwargs):\n        if self.with_mask:\n            ms_bbox_result, ms_segm_result = result\n            if isinstance(ms_bbox_result, dict):\n                result = (ms_bbox_result[\'ensemble\'],\n                          ms_segm_result[\'ensemble\'])\n        else:\n            if isinstance(result, dict):\n                result = result[\'ensemble\']\n        super(CascadeRCNN, self).show_result(data, result, img_norm_cfg,\n                                             **kwargs)\n'"
mmdet/models/detectors/fast_rcnn.py,0,"b""from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FastRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 shared_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 pretrained=None):\n        super(FastRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            pretrained=pretrained)\n\n    def forward_test(self, imgs, img_metas, proposals, **kwargs):\n        for var, name in [(imgs, 'imgs'), (img_metas, 'img_metas')]:\n            if not isinstance(var, list):\n                raise TypeError('{} must be a list, but got {}'.format(\n                    name, type(var)))\n\n        num_augs = len(imgs)\n        if num_augs != len(img_metas):\n            raise ValueError(\n                'num of augmentations ({}) != num of image meta ({})'.format(\n                    len(imgs), len(img_metas)))\n        # TODO: remove the restriction of imgs_per_gpu == 1 when prepared\n        imgs_per_gpu = imgs[0].size(0)\n        assert imgs_per_gpu == 1\n\n        if num_augs == 1:\n            return self.simple_test(imgs[0], img_metas[0], proposals[0],\n                                    **kwargs)\n        else:\n            return self.aug_test(imgs, img_metas, proposals, **kwargs)\n"""
mmdet/models/detectors/faster_rcnn.py,0,"b""from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\nimport ipdb\n\n# \xe5\x88\xa9\xe7\x94\xa8super\xe7\x9a\x84\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe7\xbb\xa7\xe6\x89\xbf\xef\xbc\x8c\xe9\x80\x92\xe5\xbd\x92\xe5\x9c\xb0\xe6\x90\xad\xe5\xbb\xba\xe4\xba\x86\xe8\xae\xa1\xe7\xae\x97\xe5\x9b\xbe\xef\xbc\x81\n\n@DETECTORS.register_module  \nclass FasterRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 shared_head=None,\n                 pretrained=None):\n        super(FasterRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n\n'''\n\xe8\xae\xad\xe7\xbb\x83\xe6\x97\xb6\xe4\xbc\xa0\xe5\x85\xa5\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xef\xbc\x9a\nself:Faster RCNN\xe7\x9a\x84\xe7\xbb\x84\xe5\xbb\xba\nbackbone = {'type': 'ResNet', 'depth': 50, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'frozen_stages': 1, 'style': 'pytorch'}\nrpn_head = {'type': 'RPNHead', 'in_channels': 256, 'feat_channels': 256, 'anchor_scales': [8], 'anchor_ratios': [0.5, 1.0, 2.0], 'anchor_strides': [4, 8, 16, 32, 64], 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': True, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 0.1111111111111111, 'loss_weight': 1.0}}\nbbox_roi_extractor = {'type': 'SingleRoIExtractor', 'roi_layer': {'type': 'RoIAlign', 'out_size': 7, 'sample_num': 2}, 'out_channels': 256, 'featmap_strides': [4, 8, 16, 32]}\nbbox_head = {'type': 'SharedFCBBoxHead', 'num_fcs': 2, 'in_channels': 256, 'fc_out_channels': 1024, 'roi_feat_size': 7, 'num_classes': 81, 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2], 'reg_class_agnostic': False, 'loss_cls': {'type': 'CrossEntropyLoss', 'use_sigmoid': False, 'loss_weight': 1.0}, 'loss_bbox': {'type': 'SmoothL1Loss', 'beta': 1.0, 'loss_weight': 1.0}}\ntrain_cfg = {'rpn': {'assigner': {'type': 'MaxIoUAssigner', 'pos_iou_thr': 0.7, 'neg_iou_thr': 0.3, 'min_pos_iou': 0.3, 'ignore_iof_thr': -1}, 'sampler': {'type': 'RandomSampler', 'num': 256, 'pos_fraction': 0.5, 'neg_pos_ub': -1, 'add_gt_as_proposals': False}, 'allowed_border': 0, 'pos_weight': -1, 'debug': False}, 'rpn_proposal': {'nms_across_levels': False, 'nms_pre': 2000, 'nms_post': 2000, 'max_num': 2000, 'nms_thr': 0.7, 'min_bbox_size': 0}, 'rcnn': {'assigner': {'type': 'MaxIoUAssigner', 'pos_iou_thr': 0.5, 'neg_iou_thr': 0.5, 'min_pos_iou': 0.5, 'ignore_iof_thr': -1}, 'sampler': {'type': 'RandomSampler', 'num': 512, 'pos_fraction': 0.25, 'neg_pos_ub': -1, 'add_gt_as_proposals': True}, 'pos_weight': -1, 'debug': False}}\ntest_cfg = {'rpn': {'nms_across_levels': False, 'nms_pre': 1000, 'nms_post': 1000, 'max_num': 1000, 'nms_thr': 0.7, 'min_bbox_size': 0}, 'rcnn': {'score_thr': 0.05, 'nms': {'type': 'nms', 'iou_thr': 0.5}, 'max_per_img': 100}}\nneck = {'type': 'FPN', 'in_channels': [256, 512, 1024, 2048], 'out_channels': 256, 'num_outs': 5}\nshared_head = None\npretrained = 'modelzoo://resnet50'\n\n\xe5\x90\x8e\xe9\x9d\xa2\xe7\xbb\xa7\xe6\x89\xbf\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe9\x83\xbd\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\n'''\n"""
mmdet/models/detectors/fcos.py,0,"b'from .single_stage import SingleStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass FCOS(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(FCOS, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                   test_cfg, pretrained)\n'"
mmdet/models/detectors/grid_rcnn.py,3,"b'from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\nimport torch\n\nfrom .. import builder\nfrom mmdet.core import bbox2roi, bbox2result, build_assigner, build_sampler\n\n\n@DETECTORS.register_module\nclass GridRCNN(TwoStageDetector):\n    """"""Grid R-CNN.\n\n    This detector is the implementation of:\n    - Grid R-CNN (https://arxiv.org/abs/1811.12030)\n    - Grid R-CNN Plus: Faster and Better (https://arxiv.org/abs/1906.05688)\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 grid_roi_extractor,\n                 grid_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 shared_head=None,\n                 pretrained=None):\n        assert grid_head is not None\n        super(GridRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n        if grid_roi_extractor is not None:\n            self.grid_roi_extractor = builder.build_roi_extractor(\n                grid_roi_extractor)\n            self.share_roi_extractor = False\n        else:\n            self.share_roi_extractor = True\n            self.grid_roi_extractor = self.bbox_roi_extractor\n        self.grid_head = builder.build_head(grid_head)\n\n        self.init_extra_weights()\n\n    def init_extra_weights(self):\n        self.grid_head.init_weights()\n        if not self.share_roi_extractor:\n            self.grid_roi_extractor.init_weights()\n\n    def _random_jitter(self, sampling_results, img_metas, amplitude=0.15):\n        """"""Ramdom jitter positive proposals for training.""""""\n        for sampling_result, img_meta in zip(sampling_results, img_metas):\n            bboxes = sampling_result.pos_bboxes\n            random_offsets = bboxes.new_empty(bboxes.shape[0], 4).uniform_(\n                -amplitude, amplitude)\n            # before jittering\n            cxcy = (bboxes[:, 2:4] + bboxes[:, :2]) / 2\n            wh = (bboxes[:, 2:4] - bboxes[:, :2]).abs()\n            # after jittering\n            new_cxcy = cxcy + wh * random_offsets[:, :2]\n            new_wh = wh * (1 + random_offsets[:, 2:])\n            # xywh to xyxy\n            new_x1y1 = (new_cxcy - new_wh / 2)\n            new_x2y2 = (new_cxcy + new_wh / 2)\n            new_bboxes = torch.cat([new_x1y1, new_x2y2], dim=1)\n            # clip bboxes\n            max_shape = img_meta[\'img_shape\']\n            if max_shape is not None:\n                new_bboxes[:, 0::2].clamp_(min=0, max=max_shape[1] - 1)\n                new_bboxes[:, 1::2].clamp_(min=0, max=max_shape[0] - 1)\n\n            sampling_result.pos_bboxes = new_bboxes\n        return sampling_results\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)\n            proposal_inputs = rpn_outs + (img_meta, proposal_cfg)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        if self.with_bbox:\n            # assign gts and sample proposals\n            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n            bbox_sampler = build_sampler(\n                self.train_cfg.rcnn.sampler, context=self)\n            num_imgs = img.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = bbox_assigner.assign(proposal_list[i],\n                                                     gt_bboxes[i],\n                                                     gt_bboxes_ignore[i],\n                                                     gt_labels[i])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n            # bbox head forward and loss\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            # TODO: a more flexible way to decide which feature maps to use\n            bbox_feats = self.bbox_roi_extractor(\n                x[:self.bbox_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n            cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n            bbox_targets = self.bbox_head.get_target(sampling_results,\n                                                     gt_bboxes, gt_labels,\n                                                     self.train_cfg.rcnn)\n            loss_bbox = self.bbox_head.loss(cls_score, bbox_pred,\n                                            *bbox_targets)\n            losses.update(loss_bbox)\n\n            # Grid head forward and loss\n            sampling_results = self._random_jitter(sampling_results, img_meta)\n            pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n            grid_feats = self.grid_roi_extractor(\n                x[:self.grid_roi_extractor.num_inputs], pos_rois)\n            if self.with_shared_head:\n                grid_feats = self.shared_head(grid_feats)\n            # Accelerate training\n            max_sample_num_grid = self.train_cfg.rcnn.get(\'max_num_grid\', 192)\n            sample_idx = torch.randperm(\n                grid_feats.shape[0])[:min(grid_feats.\n                                          shape[0], max_sample_num_grid)]\n            grid_feats = grid_feats[sample_idx]\n\n            grid_pred = self.grid_head(grid_feats)\n\n            grid_targets = self.grid_head.get_target(sampling_results,\n                                                     self.train_cfg.rcnn)\n            grid_targets = grid_targets[sample_idx]\n\n            loss_grid = self.grid_head.loss(grid_pred, grid_targets)\n            losses.update(loss_grid)\n\n        return losses\n\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, ""Bbox head must be implemented.""\n\n        x = self.extract_feat(img)\n\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_meta, proposal_list, self.test_cfg.rcnn, rescale=False)\n\n        # pack rois into bboxes\n        grid_rois = bbox2roi([det_bboxes[:, :4]])\n        grid_feats = self.grid_roi_extractor(\n            x[:len(self.grid_roi_extractor.featmap_strides)], grid_rois)\n        if grid_rois.shape[0] != 0:\n            self.grid_head.test_mode = True\n            grid_pred = self.grid_head(grid_feats)\n            det_bboxes = self.grid_head.get_bboxes(det_bboxes,\n                                                   grid_pred[\'fused\'],\n                                                   img_meta)\n            if rescale:\n                det_bboxes[:, :4] /= img_meta[0][\'scale_factor\']\n        else:\n            det_bboxes = torch.Tensor([])\n\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        return bbox_results\n'"
mmdet/models/detectors/htc.py,4,"b'import torch\nimport torch.nn.functional as F\n\nfrom .cascade_rcnn import CascadeRCNN\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import (bbox2roi, bbox2result, build_assigner, build_sampler,\n                        merge_aug_masks)\n\n\n@DETECTORS.register_module\nclass HybridTaskCascade(CascadeRCNN):\n\n    def __init__(self,\n                 num_stages,\n                 backbone,\n                 semantic_roi_extractor=None,\n                 semantic_head=None,\n                 semantic_fusion=(\'bbox\', \'mask\'),\n                 interleaved=True,\n                 mask_info_flow=True,\n                 **kwargs):\n        super(HybridTaskCascade, self).__init__(num_stages, backbone, **kwargs)\n        assert self.with_bbox and self.with_mask\n        assert not self.with_shared_head  # shared head not supported\n        if semantic_head is not None:\n            self.semantic_roi_extractor = builder.build_roi_extractor(\n                semantic_roi_extractor)\n            self.semantic_head = builder.build_head(semantic_head)\n\n        self.semantic_fusion = semantic_fusion\n        self.interleaved = interleaved\n        self.mask_info_flow = mask_info_flow\n\n    @property\n    def with_semantic(self):\n        if hasattr(self, \'semantic_head\') and self.semantic_head is not None:\n            return True\n        else:\n            return False\n\n    def _bbox_forward_train(self,\n                            stage,\n                            x,\n                            sampling_results,\n                            gt_bboxes,\n                            gt_labels,\n                            rcnn_train_cfg,\n                            semantic_feat=None):\n        rois = bbox2roi([res.bboxes for res in sampling_results])\n        bbox_roi_extractor = self.bbox_roi_extractor[stage]\n        bbox_head = self.bbox_head[stage]\n        bbox_feats = bbox_roi_extractor(x[:bbox_roi_extractor.num_inputs],\n                                        rois)\n        # semantic feature fusion\n        # element-wise sum for original features and pooled semantic features\n        if self.with_semantic and \'bbox\' in self.semantic_fusion:\n            bbox_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             rois)\n            if bbox_semantic_feat.shape[-2:] != bbox_feats.shape[-2:]:\n                bbox_semantic_feat = F.adaptive_avg_pool2d(\n                    bbox_semantic_feat, bbox_feats.shape[-2:])\n            bbox_feats += bbox_semantic_feat\n\n        cls_score, bbox_pred = bbox_head(bbox_feats)\n\n        bbox_targets = bbox_head.get_target(sampling_results, gt_bboxes,\n                                            gt_labels, rcnn_train_cfg)\n        loss_bbox = bbox_head.loss(cls_score, bbox_pred, *bbox_targets)\n        return loss_bbox, rois, bbox_targets, bbox_pred\n\n    def _mask_forward_train(self,\n                            stage,\n                            x,\n                            sampling_results,\n                            gt_masks,\n                            rcnn_train_cfg,\n                            semantic_feat=None):\n        mask_roi_extractor = self.mask_roi_extractor[stage]\n        mask_head = self.mask_head[stage]\n        pos_rois = bbox2roi([res.pos_bboxes for res in sampling_results])\n        mask_feats = mask_roi_extractor(x[:mask_roi_extractor.num_inputs],\n                                        pos_rois)\n\n        # semantic feature fusion\n        # element-wise sum for original features and pooled semantic features\n        if self.with_semantic and \'mask\' in self.semantic_fusion:\n            mask_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             pos_rois)\n            if mask_semantic_feat.shape[-2:] != mask_feats.shape[-2:]:\n                mask_semantic_feat = F.adaptive_avg_pool2d(\n                    mask_semantic_feat, mask_feats.shape[-2:])\n            mask_feats += mask_semantic_feat\n\n        # mask information flow\n        # forward all previous mask heads to obtain last_feat, and fuse it\n        # with the normal mask feature\n        if self.mask_info_flow:\n            last_feat = None\n            for i in range(stage):\n                last_feat = self.mask_head[i](\n                    mask_feats, last_feat, return_logits=False)\n            mask_pred = mask_head(mask_feats, last_feat, return_feat=False)\n        else:\n            mask_pred = mask_head(mask_feats)\n\n        mask_targets = mask_head.get_target(sampling_results, gt_masks,\n                                            rcnn_train_cfg)\n        pos_labels = torch.cat([res.pos_gt_labels for res in sampling_results])\n        loss_mask = mask_head.loss(mask_pred, mask_targets, pos_labels)\n        return loss_mask\n\n    def _bbox_forward_test(self, stage, x, rois, semantic_feat=None):\n        bbox_roi_extractor = self.bbox_roi_extractor[stage]\n        bbox_head = self.bbox_head[stage]\n        bbox_feats = bbox_roi_extractor(\n            x[:len(bbox_roi_extractor.featmap_strides)], rois)\n        if self.with_semantic and \'bbox\' in self.semantic_fusion:\n            bbox_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             rois)\n            if bbox_semantic_feat.shape[-2:] != bbox_feats.shape[-2:]:\n                bbox_semantic_feat = F.adaptive_avg_pool2d(\n                    bbox_semantic_feat, bbox_feats.shape[-2:])\n            bbox_feats += bbox_semantic_feat\n        cls_score, bbox_pred = bbox_head(bbox_feats)\n        return cls_score, bbox_pred\n\n    def _mask_forward_test(self, stage, x, bboxes, semantic_feat=None):\n        mask_roi_extractor = self.mask_roi_extractor[stage]\n        mask_head = self.mask_head[stage]\n        mask_rois = bbox2roi([bboxes])\n        mask_feats = mask_roi_extractor(\n            x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n        if self.with_semantic and \'mask\' in self.semantic_fusion:\n            mask_semantic_feat = self.semantic_roi_extractor([semantic_feat],\n                                                             mask_rois)\n            if mask_semantic_feat.shape[-2:] != mask_feats.shape[-2:]:\n                mask_semantic_feat = F.adaptive_avg_pool2d(\n                    mask_semantic_feat, mask_feats.shape[-2:])\n            mask_feats += mask_semantic_feat\n        if self.mask_info_flow:\n            last_feat = None\n            last_pred = None\n            for i in range(stage):\n                mask_pred, last_feat = self.mask_head[i](mask_feats, last_feat)\n                if last_pred is not None:\n                    mask_pred = mask_pred + last_pred\n                last_pred = mask_pred\n            mask_pred = mask_head(mask_feats, last_feat, return_feat=False)\n            if last_pred is not None:\n                mask_pred = mask_pred + last_pred\n        else:\n            mask_pred = mask_head(mask_feats)\n        return mask_pred\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      gt_semantic_seg=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN part, the same as normal two-stage detectors\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)\n            proposal_inputs = rpn_outs + (img_meta, proposal_cfg)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        # semantic segmentation part\n        # 2 outputs: segmentation prediction and embedded features\n        if self.with_semantic:\n            semantic_pred, semantic_feat = self.semantic_head(x)\n            loss_seg = self.semantic_head.loss(semantic_pred, gt_semantic_seg)\n            losses[\'loss_semantic_seg\'] = loss_seg\n        else:\n            semantic_feat = None\n\n        for i in range(self.num_stages):\n            self.current_stage = i\n            rcnn_train_cfg = self.train_cfg.rcnn[i]\n            lw = self.train_cfg.stage_loss_weights[i]\n\n            # assign gts and sample proposals\n            sampling_results = []\n            bbox_assigner = build_assigner(rcnn_train_cfg.assigner)\n            bbox_sampler = build_sampler(rcnn_train_cfg.sampler, context=self)\n            num_imgs = img.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n\n            for j in range(num_imgs):\n                assign_result = bbox_assigner.assign(\n                    proposal_list[j], gt_bboxes[j], gt_bboxes_ignore[j],\n                    gt_labels[j])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[j],\n                    gt_bboxes[j],\n                    gt_labels[j],\n                    feats=[lvl_feat[j][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n            # bbox head forward and loss\n            loss_bbox, rois, bbox_targets, bbox_pred = \\\n                self._bbox_forward_train(\n                    i, x, sampling_results, gt_bboxes, gt_labels,\n                    rcnn_train_cfg, semantic_feat)\n            roi_labels = bbox_targets[0]\n\n            for name, value in loss_bbox.items():\n                losses[\'s{}.{}\'.format(i, name)] = (\n                    value * lw if \'loss\' in name else value)\n\n            # mask head forward and loss\n            if self.with_mask:\n                # interleaved execution: use regressed bboxes by the box branch\n                # to train the mask branch\n                if self.interleaved:\n                    pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                    with torch.no_grad():\n                        proposal_list = self.bbox_head[i].refine_bboxes(\n                            rois, roi_labels, bbox_pred, pos_is_gts, img_meta)\n                        # re-assign and sample 512 RoIs from 512 RoIs\n                        sampling_results = []\n                        for j in range(num_imgs):\n                            assign_result = bbox_assigner.assign(\n                                proposal_list[j], gt_bboxes[j],\n                                gt_bboxes_ignore[j], gt_labels[j])\n                            sampling_result = bbox_sampler.sample(\n                                assign_result,\n                                proposal_list[j],\n                                gt_bboxes[j],\n                                gt_labels[j],\n                                feats=[lvl_feat[j][None] for lvl_feat in x])\n                            sampling_results.append(sampling_result)\n                loss_mask = self._mask_forward_train(i, x, sampling_results,\n                                                     gt_masks, rcnn_train_cfg,\n                                                     semantic_feat)\n                for name, value in loss_mask.items():\n                    losses[\'s{}.{}\'.format(i, name)] = (\n                        value * lw if \'loss\' in name else value)\n\n            # refine bboxes (same as Cascade R-CNN)\n            if i < self.num_stages - 1 and not self.interleaved:\n                pos_is_gts = [res.pos_is_gt for res in sampling_results]\n                with torch.no_grad():\n                    proposal_list = self.bbox_head[i].refine_bboxes(\n                        rois, roi_labels, bbox_pred, pos_is_gts, img_meta)\n\n        return losses\n\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        if self.with_semantic:\n            _, semantic_feat = self.semantic_head(x)\n        else:\n            semantic_feat = None\n\n        img_shape = img_meta[0][\'img_shape\']\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n\n        # ""ms"" in variable names means multi-stage\n        ms_bbox_result = {}\n        ms_segm_result = {}\n        ms_scores = []\n        rcnn_test_cfg = self.test_cfg.rcnn\n\n        rois = bbox2roi(proposal_list)\n        for i in range(self.num_stages):\n            bbox_head = self.bbox_head[i]\n            cls_score, bbox_pred = self._bbox_forward_test(\n                i, x, rois, semantic_feat=semantic_feat)\n            ms_scores.append(cls_score)\n\n            if self.test_cfg.keep_all_stages:\n                det_bboxes, det_labels = bbox_head.get_det_bboxes(\n                    rois,\n                    cls_score,\n                    bbox_pred,\n                    img_shape,\n                    scale_factor,\n                    rescale=rescale,\n                    nms_cfg=rcnn_test_cfg)\n                bbox_result = bbox2result(det_bboxes, det_labels,\n                                          bbox_head.num_classes)\n                ms_bbox_result[\'stage{}\'.format(i)] = bbox_result\n\n                if self.with_mask:\n                    mask_head = self.mask_head[i]\n                    if det_bboxes.shape[0] == 0:\n                        segm_result = [\n                            [] for _ in range(mask_head.num_classes - 1)\n                        ]\n                    else:\n                        _bboxes = (\n                            det_bboxes[:, :4] * scale_factor\n                            if rescale else det_bboxes)\n                        mask_pred = self._mask_forward_test(\n                            i, x, _bboxes, semantic_feat=semantic_feat)\n                        segm_result = mask_head.get_seg_masks(\n                            mask_pred, _bboxes, det_labels, rcnn_test_cfg,\n                            ori_shape, scale_factor, rescale)\n                    ms_segm_result[\'stage{}\'.format(i)] = segm_result\n\n            if i < self.num_stages - 1:\n                bbox_label = cls_score.argmax(dim=1)\n                rois = bbox_head.regress_by_class(rois, bbox_label, bbox_pred,\n                                                  img_meta[0])\n\n        cls_score = sum(ms_scores) / float(len(ms_scores))\n        det_bboxes, det_labels = self.bbox_head[-1].get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        bbox_result = bbox2result(det_bboxes, det_labels,\n                                  self.bbox_head[-1].num_classes)\n        ms_bbox_result[\'ensemble\'] = bbox_result\n\n        if self.with_mask:\n            if det_bboxes.shape[0] == 0:\n                segm_result = [\n                    [] for _ in range(self.mask_head[-1].num_classes - 1)\n                ]\n            else:\n                _bboxes = (\n                    det_bboxes[:, :4] * scale_factor\n                    if rescale else det_bboxes)\n\n                mask_rois = bbox2roi([_bboxes])\n                aug_masks = []\n                mask_roi_extractor = self.mask_roi_extractor[-1]\n                mask_feats = mask_roi_extractor(\n                    x[:len(mask_roi_extractor.featmap_strides)], mask_rois)\n                if self.with_semantic and \'mask\' in self.semantic_fusion:\n                    mask_semantic_feat = self.semantic_roi_extractor(\n                        [semantic_feat], mask_rois)\n                    mask_feats += mask_semantic_feat\n                last_feat = None\n                for i in range(self.num_stages):\n                    mask_head = self.mask_head[i]\n                    if self.mask_info_flow:\n                        mask_pred, last_feat = mask_head(mask_feats, last_feat)\n                    else:\n                        mask_pred = mask_head(mask_feats)\n                    aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n                merged_masks = merge_aug_masks(aug_masks,\n                                               [img_meta] * self.num_stages,\n                                               self.test_cfg.rcnn)\n                segm_result = self.mask_head[-1].get_seg_masks(\n                    merged_masks, _bboxes, det_labels, rcnn_test_cfg,\n                    ori_shape, scale_factor, rescale)\n            ms_segm_result[\'ensemble\'] = segm_result\n\n        if not self.test_cfg.keep_all_stages:\n            if self.with_mask:\n                results = (ms_bbox_result[\'ensemble\'],\n                           ms_segm_result[\'ensemble\'])\n            else:\n                results = ms_bbox_result[\'ensemble\']\n        else:\n            if self.with_mask:\n                results = {\n                    stage: (ms_bbox_result[stage], ms_segm_result[stage])\n                    for stage in ms_bbox_result\n                }\n            else:\n                results = ms_bbox_result\n\n        return results\n\n    def aug_test(self, img, img_meta, proposals=None, rescale=False):\n        raise NotImplementedError\n'"
mmdet/models/detectors/mask_rcnn.py,0,"b""from .two_stage import TwoStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass MaskRCNN(TwoStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 mask_roi_extractor,\n                 mask_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 shared_head=None,\n                 pretrained=None):\n        super(MaskRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n#\xe4\xbc\xa0\xe5\x85\xa5\xe8\xbf\x99\xe9\x87\x8c\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe6\x98\xaf\xef\xbc\x9a\n'''\nself = MaskRCNN()\nbackbone = {'type': 'ResNet', 'depth': 101, 'num_stages': 4, 'out_indices': (0, 1, 2, 3), 'frozen_stages': 1, 'style': 'pytorch'}\nneck = {'type': 'FPN', 'in_channels': [256, 512, 1024, 2048], 'out_channels': 256, 'num_outs': 5}\nrpn_head = {'type': 'RPNHead', 'in_channels': 256, 'feat_channels': 256, 'anchor_scales': [8], 'anchor_ratios': [0.5, 1.0, 2.0], 'anchor_strides': [4, 8, 16, 32, 64], 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [1.0, 1.0, 1.0, 1.0], 'use_sigmoid_cls': True}\nbbox_roi_extractor = {'type': 'SingleRoIExtractor', 'roi_layer': {'type': 'RoIAlign', 'out_size': 7, 'sample_num': 2}, 'out_channels': 256, 'featmap_strides': [4, 8, 16, 32]}\nbbox_head = {'type': 'SharedFCBBoxHead', 'num_fcs': 2, 'in_channels': 256, 'fc_out_channels': 1024, 'roi_feat_size': 7, 'num_classes': 81, 'target_means': [0.0, 0.0, 0.0, 0.0], 'target_stds': [0.1, 0.1, 0.2, 0.2], 'reg_class_agnostic': False}\nmask_roi_extractor = {'type': 'SingleRoIExtractor', 'roi_layer': {'type': 'RoIAlign', 'out_size': 14, 'sample_num': 2}, 'out_channels': 256, 'featmap_strides': [4, 8, 16, 32]}\nmask_head = {'type': 'FCNMaskHead', 'num_convs': 4, 'in_channels': 256, 'conv_out_channels': 256, 'num_classes': 81}\ntrain_cfg = None\ntest_cfg = {'rpn': {'nms_across_levels': False, 'nms_pre': 2000, 'nms_post': 2000, 'max_num': 2000, 'nms_thr': 0.7, 'min_bbox_size': 0}, 'rcnn': {'score_thr': 0.05, 'nms': {'type': 'nms', 'iou_thr': 0.5}, 'max_per_img': 100, 'mask_thr_binary': 0.5}}\npretrained = None\n\xe5\x90\x8e\xe9\x9d\xa2\xe7\xbb\xa7\xe6\x89\xbf\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\xe4\xbc\xa0\xe5\x85\xa5\xe7\x9a\x84\xe9\x83\xbd\xe6\x98\xaf\xe8\xbf\x99\xe4\xb8\xaa\n'''"""
mmdet/models/detectors/mask_scoring_rcnn.py,6,"b'import torch\n\nfrom mmdet.core import bbox2roi, build_assigner, build_sampler\nfrom .two_stage import TwoStageDetector\nfrom .. import builder\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass MaskScoringRCNN(TwoStageDetector):\n    """"""Mask Scoring RCNN.\n\n    https://arxiv.org/abs/1903.00241\n    """"""\n\n    def __init__(self,\n                 backbone,\n                 rpn_head,\n                 bbox_roi_extractor,\n                 bbox_head,\n                 mask_roi_extractor,\n                 mask_head,\n                 train_cfg,\n                 test_cfg,\n                 neck=None,\n                 shared_head=None,\n                 mask_iou_head=None,\n                 pretrained=None):\n        super(MaskScoringRCNN, self).__init__(\n            backbone=backbone,\n            neck=neck,\n            shared_head=shared_head,\n            rpn_head=rpn_head,\n            bbox_roi_extractor=bbox_roi_extractor,\n            bbox_head=bbox_head,\n            mask_roi_extractor=mask_roi_extractor,\n            mask_head=mask_head,\n            train_cfg=train_cfg,\n            test_cfg=test_cfg,\n            pretrained=pretrained)\n\n        self.mask_iou_head = builder.build_head(mask_iou_head)\n        self.mask_iou_head.init_weights()\n\n    # TODO: refactor forward_train in two stage to reduce code redundancy\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        x = self.extract_feat(img)\n\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            rpn_outs = self.rpn_head(x)\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n            losses.update(rpn_losses)\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)\n            proposal_inputs = rpn_outs + (img_meta, proposal_cfg)\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        else:\n            proposal_list = proposals\n\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n            bbox_sampler = build_sampler(\n                self.train_cfg.rcnn.sampler, context=self)\n            num_imgs = img.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = bbox_assigner.assign(proposal_list[i],\n                                                     gt_bboxes[i],\n                                                     gt_bboxes_ignore[i],\n                                                     gt_labels[i])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        # bbox head forward and loss\n        if self.with_bbox:\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            # TODO: a more flexible way to decide which feature maps to use\n            bbox_feats = self.bbox_roi_extractor(\n                x[:self.bbox_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n            cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n            bbox_targets = self.bbox_head.get_target(sampling_results,\n                                                     gt_bboxes, gt_labels,\n                                                     self.train_cfg.rcnn)\n            loss_bbox = self.bbox_head.loss(cls_score, bbox_pred,\n                                            *bbox_targets)\n            losses.update(loss_bbox)\n\n        # mask head forward and loss\n        if self.with_mask:\n            if not self.share_roi_extractor:\n                pos_rois = bbox2roi(\n                    [res.pos_bboxes for res in sampling_results])\n                mask_feats = self.mask_roi_extractor(\n                    x[:self.mask_roi_extractor.num_inputs], pos_rois)\n                if self.with_shared_head:\n                    mask_feats = self.shared_head(mask_feats)\n            else:\n                pos_inds = []\n                device = bbox_feats.device\n                for res in sampling_results:\n                    pos_inds.append(\n                        torch.ones(\n                            res.pos_bboxes.shape[0],\n                            device=device,\n                            dtype=torch.uint8))\n                    pos_inds.append(\n                        torch.zeros(\n                            res.neg_bboxes.shape[0],\n                            device=device,\n                            dtype=torch.uint8))\n                pos_inds = torch.cat(pos_inds)\n                mask_feats = bbox_feats[pos_inds]\n            mask_pred = self.mask_head(mask_feats)\n\n            mask_targets = self.mask_head.get_target(sampling_results,\n                                                     gt_masks,\n                                                     self.train_cfg.rcnn)\n            pos_labels = torch.cat(\n                [res.pos_gt_labels for res in sampling_results])\n            loss_mask = self.mask_head.loss(mask_pred, mask_targets,\n                                            pos_labels)\n            losses.update(loss_mask)\n\n            # mask iou head forward and loss\n            pos_mask_pred = mask_pred[range(mask_pred.size(0)), pos_labels]\n            mask_iou_pred = self.mask_iou_head(mask_feats, pos_mask_pred)\n            pos_mask_iou_pred = mask_iou_pred[range(mask_iou_pred.size(0)\n                                                    ), pos_labels]\n            mask_iou_targets = self.mask_iou_head.get_target(\n                sampling_results, gt_masks, pos_mask_pred, mask_targets,\n                self.train_cfg.rcnn)\n            loss_mask_iou = self.mask_iou_head.loss(pos_mask_iou_pred,\n                                                    mask_iou_targets)\n            losses.update(loss_mask_iou)\n        return losses\n\n    def simple_test_mask(self,\n                         x,\n                         img_meta,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n            mask_scores = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            _bboxes = (\n                det_bboxes[:, :4] * scale_factor if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_feats = self.mask_roi_extractor(\n                x[:len(self.mask_roi_extractor.featmap_strides)], mask_rois)\n            if self.with_shared_head:\n                mask_feats = self.shared_head(mask_feats)\n            mask_pred = self.mask_head(mask_feats)\n            segm_result = self.mask_head.get_seg_masks(mask_pred, _bboxes,\n                                                       det_labels,\n                                                       self.test_cfg.rcnn,\n                                                       ori_shape, scale_factor,\n                                                       rescale)\n            # get mask scores with mask iou head\n            mask_iou_pred = self.mask_iou_head(\n                mask_feats,\n                mask_pred[range(det_labels.size(0)), det_labels + 1])\n            mask_scores = self.mask_iou_head.get_mask_scores(\n                mask_iou_pred, det_bboxes, det_labels)\n        return segm_result, mask_scores\n'"
mmdet/models/detectors/retinanet.py,0,"b'from .single_stage import SingleStageDetector\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass RetinaNet(SingleStageDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 bbox_head,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(RetinaNet, self).__init__(backbone, neck, bbox_head, train_cfg,\n                                        test_cfg, pretrained)\n'"
mmdet/models/detectors/rpn.py,0,"b'import mmcv\n\nfrom mmdet.core import tensor2imgs, bbox_mapping\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\n\n\n@DETECTORS.register_module\nclass RPN(BaseDetector, RPNTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck,\n                 rpn_head,\n                 train_cfg,\n                 test_cfg,\n                 pretrained=None):\n        super(RPN, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        self.neck = builder.build_neck(neck) if neck is not None else None\n        self.rpn_head = builder.build_head(rpn_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(RPN, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            self.neck.init_weights()\n        self.rpn_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes=None,\n                      gt_bboxes_ignore=None):\n        if self.train_cfg.rpn.get(\'debug\', False):\n            self.rpn_head.debug_imgs = tensor2imgs(img)\n\n        x = self.extract_feat(img)\n        rpn_outs = self.rpn_head(x)\n\n        rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta, self.train_cfg.rpn)\n        losses = self.rpn_head.loss(\n            *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_meta, rescale=False):\n        x = self.extract_feat(img)\n        proposal_list = self.simple_test_rpn(x, img_meta, self.test_cfg.rpn)\n        if rescale:\n            for proposals, meta in zip(proposal_list, img_meta):\n                proposals[:, :4] /= meta[\'scale_factor\']\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        proposal_list = self.aug_test_rpn(\n            self.extract_feats(imgs), img_metas, self.test_cfg.rpn)\n        if not rescale:\n            for proposals, img_meta in zip(proposal_list, img_metas[0]):\n                img_shape = img_meta[\'img_shape\']\n                scale_factor = img_meta[\'scale_factor\']\n                flip = img_meta[\'flip\']\n                proposals[:, :4] = bbox_mapping(proposals[:, :4], img_shape,\n                                                scale_factor, flip)\n        # TODO: remove this restriction\n        return proposal_list[0].cpu().numpy()\n\n    def show_result(self, data, result, img_norm_cfg, dataset=None, top_k=20):\n        """"""Show RPN proposals on the image.\n\n        Although we assume batch size is 1, this method supports arbitrary\n        batch size.\n        """"""\n        img_tensor = data[\'img\'][0]\n        img_metas = data[\'img_meta\'][0].data[0]\n        imgs = tensor2imgs(img_tensor, **img_norm_cfg)\n        assert len(imgs) == len(img_metas)\n        for img, img_meta in zip(imgs, img_metas):\n            h, w, _ = img_meta[\'img_shape\']\n            img_show = img[:h, :w, :]\n            mmcv.imshow_bboxes(img_show, result, top_k=top_k)\n'"
mmdet/models/detectors/single_stage.py,1,"b'import torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import bbox2result\n\n\n@DETECTORS.register_module\nclass SingleStageDetector(BaseDetector):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 bbox_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(SingleStageDetector, self).__init__()\n        self.backbone = builder.build_backbone(backbone)\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n        self.bbox_head = builder.build_head(bbox_head)\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n        self.init_weights(pretrained=pretrained)\n\n    def init_weights(self, pretrained=None):\n        super(SingleStageDetector, self).init_weights(pretrained)\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        self.bbox_head.init_weights()\n\n    def extract_feat(self, img):\n        x = self.backbone(img)\n        if self.with_neck:\n            x = self.neck(x)\n        return x\n\n    def forward_train(self,\n                      img,\n                      img_metas,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        loss_inputs = outs + (gt_bboxes, gt_labels, img_metas, self.train_cfg)\n        losses = self.bbox_head.loss(\n            *loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)\n        return losses\n\n    def simple_test(self, img, img_meta, rescale=False):\n        x = self.extract_feat(img)\n        outs = self.bbox_head(x)\n        bbox_inputs = outs + (img_meta, self.test_cfg, rescale)\n        bbox_list = self.bbox_head.get_bboxes(*bbox_inputs)\n        bbox_results = [\n            bbox2result(det_bboxes, det_labels, self.bbox_head.num_classes)\n            for det_bboxes, det_labels in bbox_list\n        ]\n        return bbox_results[0]\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        raise NotImplementedError\n'"
mmdet/models/detectors/test_mixins.py,0,"b'from mmdet.core import (bbox2roi, bbox_mapping, merge_aug_proposals,\n                        merge_aug_bboxes, merge_aug_masks, multiclass_nms)\n\n\nclass RPNTestMixin(object):\n\n    def simple_test_rpn(self, x, img_meta, rpn_test_cfg):\n        rpn_outs = self.rpn_head(x)\n        proposal_inputs = rpn_outs + (img_meta, rpn_test_cfg)\n        proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)\n        return proposal_list\n\n    def aug_test_rpn(self, feats, img_metas, rpn_test_cfg):\n        imgs_per_gpu = len(img_metas[0])\n        aug_proposals = [[] for _ in range(imgs_per_gpu)]\n        for x, img_meta in zip(feats, img_metas):\n            proposal_list = self.simple_test_rpn(x, img_meta, rpn_test_cfg)\n            for i, proposals in enumerate(proposal_list):\n                aug_proposals[i].append(proposals)\n        # reorganize the order of \'img_metas\' to match the dimensions\n        # of \'aug_proposals\'\n        aug_img_metas = []\n        for i in range(imgs_per_gpu):\n            aug_img_meta = []\n            for j in range(len(img_metas)):\n                aug_img_meta.append(img_metas[j][i])\n            aug_img_metas.append(aug_img_meta)\n        # after merging, proposals will be rescaled to the original image size\n        merged_proposals = [\n            merge_aug_proposals(proposals, aug_img_meta, rpn_test_cfg)\n            for proposals, aug_img_meta in zip(aug_proposals, aug_img_metas)\n        ]\n        return merged_proposals\n\n\nclass BBoxTestMixin(object):\n\n    def simple_test_bboxes(self,\n                           x,\n                           img_meta,\n                           proposals,\n                           rcnn_test_cfg,\n                           rescale=False):\n        """"""Test only det bboxes without augmentation.""""""\n        rois = bbox2roi(proposals)\n        roi_feats = self.bbox_roi_extractor(\n            x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n        if self.with_shared_head:\n            roi_feats = self.shared_head(roi_feats)\n        cls_score, bbox_pred = self.bbox_head(roi_feats)\n        img_shape = img_meta[0][\'img_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n        det_bboxes, det_labels = self.bbox_head.get_det_bboxes(\n            rois,\n            cls_score,\n            bbox_pred,\n            img_shape,\n            scale_factor,\n            rescale=rescale,\n            cfg=rcnn_test_cfg)\n        return det_bboxes, det_labels\n\n    def aug_test_bboxes(self, feats, img_metas, proposal_list, rcnn_test_cfg):\n        aug_bboxes = []\n        aug_scores = []\n        for x, img_meta in zip(feats, img_metas):\n            # only one image in the batch\n            img_shape = img_meta[0][\'img_shape\']\n            scale_factor = img_meta[0][\'scale_factor\']\n            flip = img_meta[0][\'flip\']\n            # TODO more flexible\n            proposals = bbox_mapping(proposal_list[0][:, :4], img_shape,\n                                     scale_factor, flip)\n            rois = bbox2roi([proposals])\n            # recompute feature maps to save GPU memory\n            roi_feats = self.bbox_roi_extractor(\n                x[:len(self.bbox_roi_extractor.featmap_strides)], rois)\n            if self.with_shared_head:\n                roi_feats = self.shared_head(roi_feats)\n            cls_score, bbox_pred = self.bbox_head(roi_feats)\n            bboxes, scores = self.bbox_head.get_det_bboxes(\n                rois,\n                cls_score,\n                bbox_pred,\n                img_shape,\n                scale_factor,\n                rescale=False,\n                cfg=None)\n            aug_bboxes.append(bboxes)\n            aug_scores.append(scores)\n        # after merging, bboxes will be rescaled to the original image size\n        merged_bboxes, merged_scores = merge_aug_bboxes(\n            aug_bboxes, aug_scores, img_metas, rcnn_test_cfg)\n        det_bboxes, det_labels = multiclass_nms(\n            merged_bboxes, merged_scores, rcnn_test_cfg.score_thr,\n            rcnn_test_cfg.nms, rcnn_test_cfg.max_per_img)\n        return det_bboxes, det_labels\n\n\nclass MaskTestMixin(object):\n\n    def simple_test_mask(self,\n                         x,\n                         img_meta,\n                         det_bboxes,\n                         det_labels,\n                         rescale=False):\n        # image shape of the first image in the batch (only one)\n        ori_shape = img_meta[0][\'ori_shape\']\n        scale_factor = img_meta[0][\'scale_factor\']\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            # if det_bboxes is rescaled to the original image size, we need to\n            # rescale it back to the testing scale to obtain RoIs.\n            _bboxes = (\n                det_bboxes[:, :4] * scale_factor if rescale else det_bboxes)\n            mask_rois = bbox2roi([_bboxes])\n            mask_feats = self.mask_roi_extractor(\n                x[:len(self.mask_roi_extractor.featmap_strides)], mask_rois)\n            if self.with_shared_head:\n                mask_feats = self.shared_head(mask_feats)\n            mask_pred = self.mask_head(mask_feats)\n            segm_result = self.mask_head.get_seg_masks(\n                mask_pred, _bboxes, det_labels, self.test_cfg.rcnn, ori_shape,\n                scale_factor, rescale)\n        return segm_result\n\n    def aug_test_mask(self, feats, img_metas, det_bboxes, det_labels):\n        if det_bboxes.shape[0] == 0:\n            segm_result = [[] for _ in range(self.mask_head.num_classes - 1)]\n        else:\n            aug_masks = []\n            for x, img_meta in zip(feats, img_metas):\n                img_shape = img_meta[0][\'img_shape\']\n                scale_factor = img_meta[0][\'scale_factor\']\n                flip = img_meta[0][\'flip\']\n                _bboxes = bbox_mapping(det_bboxes[:, :4], img_shape,\n                                       scale_factor, flip)\n                mask_rois = bbox2roi([_bboxes])\n                mask_feats = self.mask_roi_extractor(\n                    x[:len(self.mask_roi_extractor.featmap_strides)],\n                    mask_rois)\n                if self.with_shared_head:\n                    mask_feats = self.shared_head(mask_feats)\n                mask_pred = self.mask_head(mask_feats)\n                # convert to numpy array to save memory\n                aug_masks.append(mask_pred.sigmoid().cpu().numpy())\n            merged_masks = merge_aug_masks(aug_masks, img_metas,\n                                           self.test_cfg.rcnn)\n\n            ori_shape = img_metas[0][0][\'ori_shape\']\n            segm_result = self.mask_head.get_seg_masks(\n                merged_masks,\n                det_bboxes,\n                det_labels,\n                self.test_cfg.rcnn,\n                ori_shape,\n                scale_factor=1.0,\n                rescale=False)\n        return segm_result\n'"
mmdet/models/detectors/two_stage.py,7,"b'import torch\nimport torch.nn as nn\n\nfrom .base import BaseDetector\nfrom .test_mixins import RPNTestMixin, BBoxTestMixin, MaskTestMixin\nfrom .. import builder\nfrom ..registry import DETECTORS\nfrom mmdet.core import bbox2roi, bbox2result, build_assigner, build_sampler\n\nimport ipdb\n\n# \xe6\xb3\xa8\xe6\x84\x8f\xe5\xa4\x9a\xe7\xbb\xa7\xe6\x89\xbf\xe7\x9a\x84\xe7\xbb\xa7\xe6\x89\xbf\xe9\xa1\xba\xe5\xba\x8f\xef\xbc\x8c\xe6\x98\xaf\xe5\xb9\xbf\xe5\xba\xa6\xe4\xbc\x98\xe5\x85\x88\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87__mro__\xe5\x86\x85\xe7\xbd\xae\xe6\x96\xb9\xe6\xb3\x95\xe5\x8f\xaf\xe4\xbb\xa5\xe6\x9f\xa5\xe7\x9c\x8b\n# \xe4\xbd\x86\xe6\x98\xaf\xe5\x90\x8e\xe9\x9d\xa2\xe5\x87\xa0\xe4\xb8\xaa\xe9\x83\xbd\xe6\xb2\xa1\xe6\x9c\x89\xe6\x9e\x84\xe9\x80\xa0\xe5\x87\xbd\xe6\x95\xb0\n@DETECTORS.register_module\nclass TwoStageDetector(BaseDetector, RPNTestMixin, BBoxTestMixin,\n                       MaskTestMixin):\n\n    def __init__(self,\n                 backbone,\n                 neck=None,\n                 shared_head=None,\n                 rpn_head=None,\n                 bbox_roi_extractor=None,\n                 bbox_head=None,\n                 mask_roi_extractor=None,\n                 mask_head=None,\n                 train_cfg=None,\n                 test_cfg=None,\n                 pretrained=None):\n        super(TwoStageDetector, self).__init__()\n        # ipdb.set_trace(context=35)\n        self.backbone = builder.build_backbone(backbone)\n        if neck is not None:\n            self.neck = builder.build_neck(neck)\n\n        if shared_head is not None:\n            self.shared_head = builder.build_shared_head(shared_head)\n\n        if rpn_head is not None:\n            # RPN\xe6\x90\xad\xe5\xbb\xba\xe5\x90\x8e\xef\xbc\x8c\xe6\x9c\x89\xe5\x87\xa0\xe5\xbc\xa0\xe5\xb0\xba\xe5\xaf\xb8\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x8c\xe6\x89\xa7\xe8\xa1\x8c\xe5\x87\xa0\xe6\xac\xa1\xe5\x85\xb6forward\n            self.rpn_head = builder.build_head(rpn_head)\n\n        if bbox_head is not None:\n            self.bbox_roi_extractor = builder.build_roi_extractor(\n                bbox_roi_extractor)\n            self.bbox_head = builder.build_head(bbox_head)\n\n        if mask_head is not None:\n            if mask_roi_extractor is not None:\n                self.mask_roi_extractor = builder.build_roi_extractor(\n                    mask_roi_extractor)\n                self.share_roi_extractor = False\n            else:\n                self.share_roi_extractor = True\n                self.mask_roi_extractor = self.bbox_roi_extractor\n            self.mask_head = builder.build_head(mask_head)\n\n        self.train_cfg = train_cfg\n        self.test_cfg = test_cfg\n\n        # \xe5\x9c\xa8\xe6\xad\xa4\xe5\xa4\x84\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\xe6\xa8\xa1\xe5\x9e\x8b\xe3\x80\x82\xe5\xae\x9e\xe9\x99\x85\xe6\x98\xaf\xe9\x80\x90\xe6\xa8\xa1\xe5\x9d\x97\xe8\xb0\x83\xe7\x94\xa8\xef\xbc\x8c\xe5\xb1\x82\xe5\xb1\x82\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        self.init_weights(pretrained=pretrained)\n\n    @property\n    def with_rpn(self):\n        return hasattr(self, \'rpn_head\') and self.rpn_head is not None\n\n    def init_weights(self, pretrained=None):\n        super(TwoStageDetector, self).init_weights(pretrained)\n        # \xe8\xae\xad\xe7\xbb\x83\xe9\x98\xb6\xe6\xae\xb5\xe5\x8f\xaa\xe6\x9c\x89backbone\xe6\x98\xaf\xe9\xa2\x84\xe8\xae\xad\xe7\xbb\x83\xe5\x88\x9d\xe5\xa7\x8b\xe5\x8c\x96\n        self.backbone.init_weights(pretrained=pretrained)\n        if self.with_neck:\n            if isinstance(self.neck, nn.Sequential):\n                for m in self.neck:\n                    m.init_weights()\n            else:\n                self.neck.init_weights()\n        if self.with_shared_head:\n            self.shared_head.init_weights(pretrained=pretrained)\n        if self.with_rpn:\n            self.rpn_head.init_weights()\n        if self.with_bbox:\n            self.bbox_roi_extractor.init_weights()\n            self.bbox_head.init_weights()\n        if self.with_mask:\n            self.mask_head.init_weights()\n            if not self.share_roi_extractor:\n                self.mask_roi_extractor.init_weights()\n    \n    # \xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe6\x8f\x90\xe5\x8f\x96\xe5\xb1\x82\xef\xbc\x9abackbone\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba+FPN\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n    def extract_feat(self, img):\n        x = self.backbone(img)  # \xe7\xbb\x8f\xe8\xbf\x87backbone\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97   \n        if self.with_neck:\n            x = self.neck(x)    # neck FPN\xe5\x89\x8d\xe5\x90\x91\xe8\xae\xa1\xe7\xae\x97\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84tuple\n        return x\n\n    # \xe8\xae\xad\xe7\xbb\x83\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xe8\xae\xa1\xe7\xae\x97 \n    # \xe4\xb8\xba\xe4\xba\x86\xe9\x98\xb2\xe6\xad\xa2\xe6\x97\xa0\xe6\x84\x8f\xe7\xaf\xa1\xe6\x94\xb9\xe4\xb8\xad\xe9\x97\xb4\xe7\x89\xb9\xe5\xbe\x81\xe5\xb1\x82\xe5\xaf\xbc\xe8\x87\xb4\xe9\x9a\xbe\xe4\xbb\xa5\xe6\x9f\xa5\xe9\x94\x99\xef\xbc\x8c\xef\xbc\x8c\xe4\xb8\xad\xe9\x97\xb4\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe8\xae\xa1\xe7\xae\x97\xe5\x85\xa8\xe9\x83\xa8\xe6\x98\xaftuple\xe5\xbd\xa2\xe5\xbc\x8f\xe8\xbf\x9b\xe8\xa1\x8c\xe4\xbc\xa0\xe9\x80\x92\n    def forward_train(self,\n                      img,\n                      img_meta,\n                      gt_bboxes,\n                      gt_labels,\n                      gt_bboxes_ignore=None,\n                      gt_masks=None,\n                      proposals=None):\n        # \xe4\xb8\x8b\xe9\x9d\xa2\xe8\xbf\x99\xe4\xb8\x80\xe6\xad\xa5\xe8\xae\xa1\xe7\xae\x97\xe4\xba\x86\xe5\x89\x8d\xe5\x90\x91\xe7\x9a\x84backbone\xe4\xbc\xa0\xe6\x92\xad\xe5\x92\x8cFPN\n        x = self.extract_feat(img)\n\n        # \xe4\xbb\x8eRPN\xe5\xbc\x80\xe5\xa7\x8b\xe6\x9c\x89loss\xe4\xba\x86\n        losses = dict()\n\n        # RPN forward and loss\n        if self.with_rpn:\n            # ipdb.set_trace(context=35)\n            rpn_outs = self.rpn_head(x)     # \xe8\xae\xa1\xe7\xae\x97RPN\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\xef\xbc\x8c\xe9\x9a\x90\xe5\xbc\x8f\xe8\xb0\x83\xe7\x94\xa8forward\xe6\xb3\xa8\xe6\x84\x8f\xe9\x81\xb5\xe7\x85\xa7MRO\xe9\xa1\xba\xe5\xba\x8f\xe6\x90\x9c\xe7\xb4\xa2\n            # tuple\xe5\x8f\xaf\xe4\xbb\xa5\xe7\x9b\xb4\xe6\x8e\xa5\xe4\xbd\x9c\xe5\x8a\xa0\xe6\xb3\x95\xef\xbc\x8c\xe7\x9b\xb8\xe5\xbd\x93\xe4\xba\x8e\xe5\x85\x83\xe7\xbb\x84\xe5\x90\x88\xe5\xb9\xb6\n            rpn_loss_inputs = rpn_outs + (gt_bboxes, img_meta,\n                                          self.train_cfg.rpn)\n            rpn_losses = self.rpn_head.loss(\n                *rpn_loss_inputs, gt_bboxes_ignore=gt_bboxes_ignore)    # \xe8\xae\xa1\xe7\xae\x97\xe5\x89\x8d\xe5\x90\x91loss\xef\xbc\x8c\xe6\xb3\xa8\xe6\x84\x8frpn\xe7\x9a\x84loss\xe8\xb0\x83\xe7\x94\xa8\xe6\x96\xb9\xe6\xb3\x95\n            losses.update(rpn_losses)   # \xe5\xad\x97\xe5\x85\xb8\xe7\x9a\x84\xe5\x90\x88\xe5\xb9\xb6\xe6\x96\xb9\xe6\xb3\x95\n\n            proposal_cfg = self.train_cfg.get(\'rpn_proposal\',\n                                              self.test_cfg.rpn)    \n            proposal_inputs = rpn_outs + (img_meta, proposal_cfg)       # \xe5\xb0\x86RPN\xe8\xbe\x93\xe5\x87\xba\xe7\x9a\x84box\xe5\x92\x8c\xe7\x9b\xb8\xe5\x85\xb3\xe8\xae\xbe\xe7\xbd\xae\xe4\xbf\xa1\xe6\x81\xaf\xe8\xbe\x93\xe5\x85\xa5proposal\n            proposal_list = self.rpn_head.get_bboxes(*proposal_inputs)  # \xe8\x8e\xb7\xe5\xbe\x97\xe5\x9b\x9e\xe5\xbd\x92proposal\n        else:\n            proposal_list = proposals   # \xe7\x9b\xb4\xe6\x8e\xa5\xe6\x8c\x87\xe5\xae\x9aproposals\n\n        # assign gts and sample proposals\n        if self.with_bbox or self.with_mask:\n            bbox_assigner = build_assigner(self.train_cfg.rcnn.assigner)\n            bbox_sampler = build_sampler(\n                self.train_cfg.rcnn.sampler, context=self)\n            num_imgs = img.size(0)\n            if gt_bboxes_ignore is None:\n                gt_bboxes_ignore = [None for _ in range(num_imgs)]\n            sampling_results = []\n            for i in range(num_imgs):\n                assign_result = bbox_assigner.assign(\n                    proposal_list[i], gt_bboxes[i], gt_bboxes_ignore[i],\n                    gt_labels[i])\n                sampling_result = bbox_sampler.sample(\n                    assign_result,\n                    proposal_list[i],\n                    gt_bboxes[i],\n                    gt_labels[i],\n                    feats=[lvl_feat[i][None] for lvl_feat in x])\n                sampling_results.append(sampling_result)\n\n        # bbox head forward and loss\n        if self.with_bbox:\n            rois = bbox2roi([res.bboxes for res in sampling_results])\n            # TODO: a more flexible way to decide which feature maps to use\n            bbox_feats = self.bbox_roi_extractor(\n                x[:self.bbox_roi_extractor.num_inputs], rois)\n            if self.with_shared_head:\n                bbox_feats = self.shared_head(bbox_feats)\n            cls_score, bbox_pred = self.bbox_head(bbox_feats)\n\n            bbox_targets = self.bbox_head.get_target(\n                sampling_results, gt_bboxes, gt_labels, self.train_cfg.rcnn)\n            loss_bbox = self.bbox_head.loss(cls_score, bbox_pred,\n                                            *bbox_targets)\n            losses.update(loss_bbox)\n\n        # mask head forward and loss\n        if self.with_mask:\n            if not self.share_roi_extractor:\n                pos_rois = bbox2roi(\n                    [res.pos_bboxes for res in sampling_results])\n                mask_feats = self.mask_roi_extractor(\n                    x[:self.mask_roi_extractor.num_inputs], pos_rois)\n                if self.with_shared_head:\n                    mask_feats = self.shared_head(mask_feats)\n            else:\n                pos_inds = []\n                device = bbox_feats.device\n                for res in sampling_results:\n                    pos_inds.append(\n                        torch.ones(\n                            res.pos_bboxes.shape[0],\n                            device=device,\n                            dtype=torch.uint8))\n                    pos_inds.append(\n                        torch.zeros(\n                            res.neg_bboxes.shape[0],\n                            device=device,\n                            dtype=torch.uint8))\n                pos_inds = torch.cat(pos_inds)\n                mask_feats = bbox_feats[pos_inds]\n            mask_pred = self.mask_head(mask_feats)\n\n            mask_targets = self.mask_head.get_target(\n                sampling_results, gt_masks, self.train_cfg.rcnn)\n            pos_labels = torch.cat(\n                [res.pos_gt_labels for res in sampling_results])\n            loss_mask = self.mask_head.loss(mask_pred, mask_targets,\n                                            pos_labels)\n            losses.update(loss_mask)\n\n        return losses\n\n    # \xe6\xa3\x80\xe6\xb5\x8b\xe8\xbf\x87\xe7\xa8\x8b\xe7\x9a\x84\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xadforward\xe8\xb0\x83\xe7\x94\xa8\xe7\x9a\x84\xe5\x87\xbd\xe6\x95\xb0\xef\xbc\x8c\xe9\x80\x9a\xe8\xbf\x87\xe6\x9c\x80\xe5\x8e\x9f\xe5\xa7\x8b\xe7\x9a\x84nn.Module\xe5\x88\xb0\xe7\x88\xb6\xe7\xb1\xbbBaseDetector\xe7\x9a\x84forward\xef\xbc\x8c\xe7\xbb\xa7\xe7\xbb\xad\xe7\x94\xb1\xe5\xba\x95\xe5\xb1\x82 \xe5\xb1\x82\xe5\xb1\x82\xe5\x90\x91\xe4\xb8\x8a\xe8\xb0\x83\xe7\x94\xa8\xe5\x88\xb0\xe8\xbf\x99\xe9\x87\x8c\n    def simple_test(self, img, img_meta, proposals=None, rescale=False):\n        """"""Test without augmentation.""""""\n        assert self.with_bbox, ""Bbox head must be implemented.""\n\n        # \xe7\xbb\x8f\xe8\xbf\x87backbone\xe5\x92\x8cFPN\xe7\x89\xb9\xe5\xbe\x81\xe6\x8f\x90\xe5\x8f\x96\xe5\xb1\x82\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe8\x8b\xa5\xe5\xb9\xb2\xe4\xb8\xaa\xe8\xbe\x93\xe5\x87\xba\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\xbb\x84\xe6\x88\x90tuple\n        x = self.extract_feat(img)\n\n        # \xe5\xae\x9a\xe4\xb9\x89\xe5\x9c\xa8RPNTestMixin\xe4\xb8\xad\xef\xbc\x8c\xe7\x94\xa8\xe4\xba\x8e\xe7\x94\x9f\xe6\x88\x90RPN\xe8\xbe\x93\xe5\x87\xba\xe5\x90\x8e\xe5\xa4\x84\xe7\x90\x86\xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84proposals\xef\xbc\x88\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9d\x90\xe6\xa0\x87\xef\xbc\x89\xef\xbc\x88NMS\xe8\xbf\x87\xef\xbc\x89\n        # \xe6\xb3\xa8\xe6\x84\x8flist\xe7\x9a\x84\xe9\x95\xbf\xe5\xba\xa6\xe5\xaf\xb9\xe5\xba\x94\xe7\x9a\x84\xe6\x98\xaf\xe5\x9b\xbe\xe7\x89\x87\xe7\x9a\x84\xe6\x95\xb0\xe7\x9b\xae\n        #\xe8\xbe\x93\xe5\x85\xa5\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90anchors\xef\xbc\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8cNMS\xef\xbc\x8c\xe5\x9d\x90\xe6\xa0\x87\xe8\xbf\x98\xe5\x8e\x9f\xef\xbc\x8c\xe7\x94\x9f\xe6\x88\x90proposals\n        proposal_list = self.simple_test_rpn(\n            x, img_meta, self.test_cfg.rpn) if proposals is None else proposals\n\n        # \xe5\xae\x9a\xe4\xb9\x89\xe5\x9c\xa8BBoxTestMixin\xe4\xb8\xad\n        # \xe8\xbe\x93\xe5\x85\xa5proposals\xef\xbc\x8c\xe8\xbf\x9b\xe8\xa1\x8cRoI\xe6\xb1\xa0\xe5\x8c\x96\xef\xbc\x8c\xe5\x88\x86\xe7\xb1\xbb\xe5\x9b\x9e\xe5\xbd\x92\xef\xbc\x8cNMS\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\x9c\x9f\xe6\xad\xa3\xe7\x9a\x84\xe6\xa3\x80\xe6\xb5\x8b\xe7\x9b\xae\xe6\xa0\x87\xe5\x92\x8c\xe5\x9d\x90\xe6\xa0\x87\xe7\x9a\x84\xe5\x9d\x90\xe6\xa0\x87\xe5\x81\x8f\xe7\xa7\xbb\n        det_bboxes, det_labels = self.simple_test_bboxes(\n            x, img_meta, proposal_list, self.test_cfg.rcnn, rescale=rescale)\n        # \xe5\x81\x8f\xe7\xa7\xbb\xe8\xbf\x98\xe5\x8e\x9f\xe4\xb8\xba\xe7\x9c\x9f\xe5\xae\x9e\xe5\x9d\x90\xe5\x9d\x90\xe6\xa0\x87\n        bbox_results = bbox2result(det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        if not self.with_mask:\n            return bbox_results\n        else:\n            # \xe5\x88\x86\xe5\x89\xb2\xe4\xbb\xbb\xe5\x8a\xa1\xe5\x8a\xa0mask\n            segm_results = self.simple_test_mask(\n                x, img_meta, det_bboxes, det_labels, rescale=rescale)\n            return bbox_results, segm_results\n\n    def aug_test(self, imgs, img_metas, rescale=False):\n        """"""Test with augmentations.\n\n        If rescale is False, then returned bboxes and masks will fit the scale\n        of imgs[0].\n        """"""\n        # recompute feats to save memory\n        proposal_list = self.aug_test_rpn(\n            self.extract_feats(imgs), img_metas, self.test_cfg.rpn)\n        det_bboxes, det_labels = self.aug_test_bboxes(\n            self.extract_feats(imgs), img_metas, proposal_list,\n            self.test_cfg.rcnn)\n\n        if rescale:\n            _det_bboxes = det_bboxes\n        else:\n            _det_bboxes = det_bboxes.clone()\n            _det_bboxes[:, :4] *= img_metas[0][0][\'scale_factor\']\n        bbox_results = bbox2result(_det_bboxes, det_labels,\n                                   self.bbox_head.num_classes)\n\n        # det_bboxes always keep the original scale\n        if self.with_mask:\n            segm_results = self.aug_test_mask(\n                self.extract_feats(imgs), img_metas, det_bboxes, det_labels)\n            return bbox_results, segm_results\n        else:\n            return bbox_results\n'"
mmdet/models/losses/__init__.py,0,"b""from .accuracy import accuracy, Accuracy\nfrom .cross_entropy_loss import (cross_entropy, binary_cross_entropy,\n                                 mask_cross_entropy, CrossEntropyLoss)\nfrom .focal_loss import sigmoid_focal_loss, FocalLoss\nfrom .smooth_l1_loss import smooth_l1_loss, SmoothL1Loss\nfrom .ghm_loss import GHMC, GHMR\nfrom .balanced_l1_loss import balanced_l1_loss, BalancedL1Loss\nfrom .mse_loss import mse_loss, MSELoss\nfrom .iou_loss import iou_loss, bounded_iou_loss, IoULoss, BoundedIoULoss\nfrom .utils import reduce_loss, weight_reduce_loss, weighted_loss\n\n__all__ = [\n    'accuracy', 'Accuracy', 'cross_entropy', 'binary_cross_entropy',\n    'mask_cross_entropy', 'CrossEntropyLoss', 'sigmoid_focal_loss',\n    'FocalLoss', 'smooth_l1_loss', 'SmoothL1Loss', 'balanced_l1_loss',\n    'BalancedL1Loss', 'mse_loss', 'MSELoss', 'iou_loss', 'bounded_iou_loss',\n    'IoULoss', 'BoundedIoULoss', 'GHMC', 'GHMR', 'reduce_loss',\n    'weight_reduce_loss', 'weighted_loss'\n]\n"""
mmdet/models/losses/accuracy.py,1,"b'import torch.nn as nn\n\n\ndef accuracy(pred, target, topk=1):\n    assert isinstance(topk, (int, tuple))\n    if isinstance(topk, int):\n        topk = (topk, )\n        return_single = True\n    else:\n        return_single = False\n\n    maxk = max(topk)\n    _, pred_label = pred.topk(maxk, dim=1)\n    pred_label = pred_label.t()\n    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))\n\n    res = []\n    for k in topk:\n        correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n        res.append(correct_k.mul_(100.0 / pred.size(0)))\n    return res[0] if return_single else res\n\n\nclass Accuracy(nn.Module):\n\n    def __init__(self, topk=(1, )):\n        super().__init__()\n        self.topk = topk\n\n    def forward(self, pred, target):\n        return accuracy(pred, target, self.topk)\n'"
mmdet/models/losses/balanced_l1_loss.py,4,"b'import numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom .utils import weighted_loss\nfrom ..registry import LOSSES\n\n\n@weighted_loss\ndef balanced_l1_loss(pred,\n                     target,\n                     beta=1.0,\n                     alpha=0.5,\n                     gamma=1.5,\n                     reduction=\'mean\'):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n\n    diff = torch.abs(pred - target)\n    b = np.e**(gamma / alpha) - 1\n    loss = torch.where(\n        diff < beta, alpha / b *\n        (b * diff + 1) * torch.log(b * diff / beta + 1) - alpha * diff,\n        gamma * diff + gamma / b - alpha * beta)\n\n    return loss\n\n\n@LOSSES.register_module\nclass BalancedL1Loss(nn.Module):\n    """"""Balanced L1 Loss\n\n    arXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n    """"""\n\n    def __init__(self,\n                 alpha=0.5,\n                 gamma=1.5,\n                 beta=1.0,\n                 reduction=\'mean\',\n                 loss_weight=1.0):\n        super(BalancedL1Loss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * balanced_l1_loss(\n            pred,\n            target,\n            weight,\n            alpha=self.alpha,\n            gamma=self.gamma,\n            beta=self.beta,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_bbox\n'"
mmdet/models/losses/cross_entropy_loss.py,4,"b""import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .utils import weight_reduce_loss\nfrom ..registry import LOSSES\n\n\ndef cross_entropy(pred, label, weight=None, reduction='mean', avg_factor=None):\n    # element-wise losses\n    loss = F.cross_entropy(pred, label, reduction='none')\n\n    # apply weights and do the reduction\n    if weight is not None:\n        weight = weight.float()\n    loss = weight_reduce_loss(\n        loss, weight=weight, reduction=reduction, avg_factor=avg_factor)\n\n    return loss\n\n\ndef _expand_binary_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    if label_weights is None:\n        bin_label_weights = None\n    else:\n        bin_label_weights = label_weights.view(-1, 1).expand(\n            label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\ndef binary_cross_entropy(pred,\n                         label,\n                         weight=None,\n                         reduction='mean',\n                         avg_factor=None):\n    if pred.dim() != label.dim():\n        label, weight = _expand_binary_labels(label, weight, pred.size(-1))\n\n    # weighted element-wise losses\n    if weight is not None:\n        weight = weight.float()\n    loss = F.binary_cross_entropy_with_logits(\n        pred, label.float(), weight, reduction='none')\n    # do the reduction for the weighted loss\n    loss = weight_reduce_loss(loss, reduction=reduction, avg_factor=avg_factor)\n\n    return loss\n\n\ndef mask_cross_entropy(pred, target, label, reduction='mean', avg_factor=None):\n    # TODO: handle these two reserved arguments\n    assert reduction == 'mean' and avg_factor is None\n    num_rois = pred.size()[0]\n    inds = torch.arange(0, num_rois, dtype=torch.long, device=pred.device)\n    pred_slice = pred[inds, label].squeeze(1)\n    return F.binary_cross_entropy_with_logits(\n        pred_slice, target, reduction='mean')[None]\n\n\n@LOSSES.register_module\nclass CrossEntropyLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=False,\n                 use_mask=False,\n                 reduction='mean',\n                 loss_weight=1.0):\n        super(CrossEntropyLoss, self).__init__()\n        assert (use_sigmoid is False) or (use_mask is False)\n        self.use_sigmoid = use_sigmoid\n        self.use_mask = use_mask\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n        if self.use_sigmoid:\n            self.cls_criterion = binary_cross_entropy\n        elif self.use_mask:\n            self.cls_criterion = mask_cross_entropy\n        else:\n            self.cls_criterion = cross_entropy\n\n    def forward(self,\n                cls_score,\n                label,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_cls = self.loss_weight * self.cls_criterion(\n            cls_score,\n            label,\n            weight,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_cls\n"""
mmdet/models/losses/focal_loss.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom mmdet.ops import sigmoid_focal_loss as _sigmoid_focal_loss\nfrom .utils import weight_reduce_loss\nfrom ..registry import LOSSES\n\n\n# This method is only for debugging\ndef py_sigmoid_focal_loss(pred,\n                          target,\n                          weight=None,\n                          gamma=2.0,\n                          alpha=0.25,\n                          reduction=\'mean\',\n                          avg_factor=None):\n    pred_sigmoid = pred.sigmoid()\n    target = target.type_as(pred)\n    pt = (1 - pred_sigmoid) * target + pred_sigmoid * (1 - target)\n    focal_weight = (alpha * target + (1 - alpha) *\n                    (1 - target)) * pt.pow(gamma)\n    loss = F.binary_cross_entropy_with_logits(\n        pred, target, reduction=\'none\') * focal_weight\n    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n    return loss\n\n\ndef sigmoid_focal_loss(pred,\n                       target,\n                       weight=None,\n                       gamma=2.0,\n                       alpha=0.25,\n                       reduction=\'mean\',\n                       avg_factor=None):\n    # Function.apply does not accept keyword arguments, so the decorator\n    # ""weighted_loss"" is not applicable\n    loss = _sigmoid_focal_loss(pred, target, gamma, alpha)\n    # TODO: find a proper way to handle the shape of weight\n    if weight is not None:\n        weight = weight.view(-1, 1)\n    loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n    return loss\n\n\n@LOSSES.register_module\nclass FocalLoss(nn.Module):\n\n    def __init__(self,\n                 use_sigmoid=True,\n                 gamma=2.0,\n                 alpha=0.25,\n                 reduction=\'mean\',\n                 loss_weight=1.0):\n        super(FocalLoss, self).__init__()\n        assert use_sigmoid is True, \'Only sigmoid focal loss supported now.\'\n        self.use_sigmoid = use_sigmoid\n        self.gamma = gamma\n        self.alpha = alpha\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None):\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        if self.use_sigmoid:\n            loss_cls = self.loss_weight * sigmoid_focal_loss(\n                pred,\n                target,\n                weight,\n                gamma=self.gamma,\n                alpha=self.alpha,\n                reduction=reduction,\n                avg_factor=avg_factor)\n        else:\n            raise NotImplementedError\n        return loss_cls\n'"
mmdet/models/losses/ghm_loss.py,12,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom ..registry import LOSSES\n\n\ndef _expand_binary_labels(labels, label_weights, label_channels):\n    bin_labels = labels.new_full((labels.size(0), label_channels), 0)\n    inds = torch.nonzero(labels >= 1).squeeze()\n    if inds.numel() > 0:\n        bin_labels[inds, labels[inds] - 1] = 1\n    bin_label_weights = label_weights.view(-1, 1).expand(\n        label_weights.size(0), label_channels)\n    return bin_labels, bin_label_weights\n\n\n# TODO: code refactoring to make it consistent with other losses\n@LOSSES.register_module\nclass GHMC(nn.Module):\n    """"""GHM Classification Loss.\n\n    Details of the theorem can be viewed in the paper\n    ""Gradient Harmonized Single-stage Detector"".\n    https://arxiv.org/abs/1811.05181\n\n    Args:\n        bins (int): Number of the unit regions for distribution calculation.\n        momentum (float): The parameter for moving average.\n        use_sigmoid (bool): Can only be true for BCE based loss now.\n        loss_weight (float): The weight of the total GHM-C loss.\n    """"""\n\n    def __init__(self, bins=10, momentum=0, use_sigmoid=True, loss_weight=1.0):\n        super(GHMC, self).__init__()\n        self.bins = bins\n        self.momentum = momentum\n        self.edges = torch.arange(bins + 1).float().cuda() / bins\n        self.edges[-1] += 1e-6\n        if momentum > 0:\n            self.acc_sum = torch.zeros(bins).cuda()\n        self.use_sigmoid = use_sigmoid\n        if not self.use_sigmoid:\n            raise NotImplementedError\n        self.loss_weight = loss_weight\n\n    def forward(self, pred, target, label_weight, *args, **kwargs):\n        """"""Calculate the GHM-C loss.\n\n        Args:\n            pred (float tensor of size [batch_num, class_num]):\n                The direct prediction of classification fc layer.\n            target (float tensor of size [batch_num, class_num]):\n                Binary class target for each sample.\n            label_weight (float tensor of size [batch_num, class_num]):\n                the value is 1 if the sample is valid and 0 if ignored.\n        Returns:\n            The gradient harmonized loss.\n        """"""\n        # the target should be binary class label\n        if pred.dim() != target.dim():\n            target, label_weight = _expand_binary_labels(\n                target, label_weight, pred.size(-1))\n        target, label_weight = target.float(), label_weight.float()\n        edges = self.edges\n        mmt = self.momentum\n        weights = torch.zeros_like(pred)\n\n        # gradient length\n        g = torch.abs(pred.sigmoid().detach() - target)\n\n        valid = label_weight > 0\n        tot = max(valid.float().sum().item(), 1.0)\n        n = 0  # n valid bins\n        for i in range(self.bins):\n            inds = (g >= edges[i]) & (g < edges[i + 1]) & valid\n            num_in_bin = inds.sum().item()\n            if num_in_bin > 0:\n                if mmt > 0:\n                    self.acc_sum[i] = mmt * self.acc_sum[i] \\\n                        + (1 - mmt) * num_in_bin\n                    weights[inds] = tot / self.acc_sum[i]\n                else:\n                    weights[inds] = tot / num_in_bin\n                n += 1\n        if n > 0:\n            weights = weights / n\n\n        loss = F.binary_cross_entropy_with_logits(\n            pred, target, weights, reduction=\'sum\') / tot\n        return loss * self.loss_weight\n\n\n# TODO: code refactoring to make it consistent with other losses\n@LOSSES.register_module\nclass GHMR(nn.Module):\n    """"""GHM Regression Loss.\n\n    Details of the theorem can be viewed in the paper\n    ""Gradient Harmonized Single-stage Detector""\n    https://arxiv.org/abs/1811.05181\n\n    Args:\n        mu (float): The parameter for the Authentic Smooth L1 loss.\n        bins (int): Number of the unit regions for distribution calculation.\n        momentum (float): The parameter for moving average.\n        loss_weight (float): The weight of the total GHM-R loss.\n    """"""\n\n    def __init__(self, mu=0.02, bins=10, momentum=0, loss_weight=1.0):\n        super(GHMR, self).__init__()\n        self.mu = mu\n        self.bins = bins\n        self.edges = torch.arange(bins + 1).float().cuda() / bins\n        self.edges[-1] = 1e3\n        self.momentum = momentum\n        if momentum > 0:\n            self.acc_sum = torch.zeros(bins).cuda()\n        self.loss_weight = loss_weight\n\n    # TODO: support reduction parameter\n    def forward(self, pred, target, label_weight, avg_factor=None):\n        """"""Calculate the GHM-R loss.\n\n        Args:\n            pred (float tensor of size [batch_num, 4 (* class_num)]):\n                The prediction of box regression layer. Channel number can be 4\n                or 4 * class_num depending on whether it is class-agnostic.\n            target (float tensor of size [batch_num, 4 (* class_num)]):\n                The target regression values with the same size of pred.\n            label_weight (float tensor of size [batch_num, 4 (* class_num)]):\n                The weight of each sample, 0 if ignored.\n        Returns:\n            The gradient harmonized loss.\n        """"""\n        mu = self.mu\n        edges = self.edges\n        mmt = self.momentum\n\n        # ASL1 loss\n        diff = pred - target\n        loss = torch.sqrt(diff * diff + mu * mu) - mu\n\n        # gradient length\n        g = torch.abs(diff / torch.sqrt(mu * mu + diff * diff)).detach()\n        weights = torch.zeros_like(g)\n\n        valid = label_weight > 0\n        tot = max(label_weight.float().sum().item(), 1.0)\n        n = 0  # n: valid bins\n        for i in range(self.bins):\n            inds = (g >= edges[i]) & (g < edges[i + 1]) & valid\n            num_in_bin = inds.sum().item()\n            if num_in_bin > 0:\n                n += 1\n                if mmt > 0:\n                    self.acc_sum[i] = mmt * self.acc_sum[i] \\\n                        + (1 - mmt) * num_in_bin\n                    weights[inds] = tot / self.acc_sum[i]\n                else:\n                    weights[inds] = tot / num_in_bin\n        if n > 0:\n            weights /= n\n\n        loss = loss * weights\n        loss = loss.sum() / tot\n        return loss * self.loss_weight\n'"
mmdet/models/losses/iou_loss.py,12,"b'import torch\nimport torch.nn as nn\n\nfrom mmdet.core import bbox_overlaps\nfrom .utils import weighted_loss\nfrom ..registry import LOSSES\n\n\n@weighted_loss\ndef iou_loss(pred, target, eps=1e-6):\n    """"""IoU loss.\n\n    Computing the IoU loss between a set of predicted bboxes and target bboxes.\n    The loss is calculated as negative log of IoU.\n\n    Args:\n        pred (Tensor): Predicted bboxes of format (x1, y1, x2, y2),\n            shape (n, 4).\n        target (Tensor): Corresponding gt bboxes, shape (n, 4).\n        eps (float): Eps to avoid log(0).\n\n    Return:\n        Tensor: Loss tensor.\n    """"""\n    ious = bbox_overlaps(pred, target, is_aligned=True).clamp(min=eps)\n    loss = -ious.log()\n    return loss\n\n\n@weighted_loss\ndef bounded_iou_loss(pred, target, beta=0.2, eps=1e-3):\n    """"""Improving Object Localization with Fitness NMS and Bounded IoU Loss,\n    https://arxiv.org/abs/1711.00164.\n\n    Args:\n        pred (tensor): Predicted bboxes.\n        target (tensor): Target bboxes.\n        beta (float): beta parameter in smoothl1.\n        eps (float): eps to avoid NaN.\n    """"""\n    pred_ctrx = (pred[:, 0] + pred[:, 2]) * 0.5\n    pred_ctry = (pred[:, 1] + pred[:, 3]) * 0.5\n    pred_w = pred[:, 2] - pred[:, 0] + 1\n    pred_h = pred[:, 3] - pred[:, 1] + 1\n    with torch.no_grad():\n        target_ctrx = (target[:, 0] + target[:, 2]) * 0.5\n        target_ctry = (target[:, 1] + target[:, 3]) * 0.5\n        target_w = target[:, 2] - target[:, 0] + 1\n        target_h = target[:, 3] - target[:, 1] + 1\n\n    dx = target_ctrx - pred_ctrx\n    dy = target_ctry - pred_ctry\n\n    loss_dx = 1 - torch.max(\n        (target_w - 2 * dx.abs()) /\n        (target_w + 2 * dx.abs() + eps), torch.zeros_like(dx))\n    loss_dy = 1 - torch.max(\n        (target_h - 2 * dy.abs()) /\n        (target_h + 2 * dy.abs() + eps), torch.zeros_like(dy))\n    loss_dw = 1 - torch.min(target_w / (pred_w + eps), pred_w /\n                            (target_w + eps))\n    loss_dh = 1 - torch.min(target_h / (pred_h + eps), pred_h /\n                            (target_h + eps))\n    loss_comb = torch.stack([loss_dx, loss_dy, loss_dw, loss_dh],\n                            dim=-1).view(loss_dx.size(0), -1)\n\n    loss = torch.where(loss_comb < beta, 0.5 * loss_comb * loss_comb / beta,\n                       loss_comb - 0.5 * beta)\n    return loss\n\n\n@LOSSES.register_module\nclass IoULoss(nn.Module):\n\n    def __init__(self, eps=1e-6, reduction=\'mean\', loss_weight=1.0):\n        super(IoULoss, self).__init__()\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * iou_loss(\n            pred,\n            target,\n            weight,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss\n\n\n@LOSSES.register_module\nclass BoundedIoULoss(nn.Module):\n\n    def __init__(self, beta=0.2, eps=1e-3, reduction=\'mean\', loss_weight=1.0):\n        super(BoundedIoULoss, self).__init__()\n        self.beta = beta\n        self.eps = eps\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        if weight is not None and not torch.any(weight > 0):\n            return (pred * weight).sum()  # 0\n        assert reduction_override in (None, \'none\', \'mean\', \'sum\')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss = self.loss_weight * bounded_iou_loss(\n            pred,\n            target,\n            weight,\n            beta=self.beta,\n            eps=self.eps,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss\n'"
mmdet/models/losses/mse_loss.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\n\nfrom .utils import weighted_loss\nfrom ..registry import LOSSES\n\nmse_loss = weighted_loss(F.mse_loss)\n\n\n@LOSSES.register_module\nclass MSELoss(nn.Module):\n\n    def __init__(self, reduction='mean', loss_weight=1.0):\n        super().__init__()\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self, pred, target, weight=None, avg_factor=None):\n        loss = self.loss_weight * mse_loss(\n            pred,\n            target,\n            weight,\n            reduction=self.reduction,\n            avg_factor=avg_factor)\n        return loss\n"""
mmdet/models/losses/smooth_l1_loss.py,3,"b""import torch\nimport torch.nn as nn\n\nfrom .utils import weighted_loss\nfrom ..registry import LOSSES\n\n\n@weighted_loss\ndef smooth_l1_loss(pred, target, beta=1.0):\n    assert beta > 0\n    assert pred.size() == target.size() and target.numel() > 0\n    diff = torch.abs(pred - target)\n    loss = torch.where(diff < beta, 0.5 * diff * diff / beta,\n                       diff - 0.5 * beta)\n    return loss\n\n\n@LOSSES.register_module\nclass SmoothL1Loss(nn.Module):\n\n    def __init__(self, beta=1.0, reduction='mean', loss_weight=1.0):\n        super(SmoothL1Loss, self).__init__()\n        self.beta = beta\n        self.reduction = reduction\n        self.loss_weight = loss_weight\n\n    def forward(self,\n                pred,\n                target,\n                weight=None,\n                avg_factor=None,\n                reduction_override=None,\n                **kwargs):\n        assert reduction_override in (None, 'none', 'mean', 'sum')\n        reduction = (\n            reduction_override if reduction_override else self.reduction)\n        loss_bbox = self.loss_weight * smooth_l1_loss(\n            pred,\n            target,\n            weight,\n            beta=self.beta,\n            reduction=reduction,\n            avg_factor=avg_factor,\n            **kwargs)\n        return loss_bbox\n"""
mmdet/models/losses/utils.py,4,"b'import functools\n\nimport torch.nn.functional as F\n\n\ndef reduce_loss(loss, reduction):\n    """"""Reduce loss as specified.\n\n    Args:\n        loss (Tensor): Elementwise loss tensor.\n        reduction (str): Options are ""none"", ""mean"" and ""sum"".\n\n    Return:\n        Tensor: Reduced loss tensor.\n    """"""\n    reduction_enum = F._Reduction.get_enum(reduction)\n    # none: 0, elementwise_mean:1, sum: 2\n    if reduction_enum == 0:\n        return loss\n    elif reduction_enum == 1:\n        return loss.mean()\n    elif reduction_enum == 2:\n        return loss.sum()\n\n\ndef weight_reduce_loss(loss, weight=None, reduction=\'mean\', avg_factor=None):\n    """"""Apply element-wise weight and reduce loss.\n\n    Args:\n        loss (Tensor): Element-wise loss.\n        weight (Tensor): Element-wise weights.\n        reduction (str): Same as built-in losses of PyTorch.\n        avg_factor (float): Avarage factor when computing the mean of losses.\n\n    Returns:\n        Tensor: Processed loss values.\n    """"""\n    # if weight is specified, apply element-wise weight\n    if weight is not None:\n        loss = loss * weight\n\n    # if avg_factor is not specified, just reduce the loss\n    if avg_factor is None:\n        loss = reduce_loss(loss, reduction)\n    else:\n        # if reduction is mean, then average the loss by avg_factor\n        if reduction == \'mean\':\n            loss = loss.sum() / avg_factor\n        # if reduction is \'none\', then do nothing, otherwise raise an error\n        elif reduction != \'none\':\n            raise ValueError(\'avg_factor can not be used with reduction=""sum""\')\n    return loss\n\n\ndef weighted_loss(loss_func):\n    """"""Create a weighted version of a given loss function.\n\n    To use this decorator, the loss function must have the signature like\n    `loss_func(pred, target, **kwargs)`. The function only needs to compute\n    element-wise loss without any reduction. This decorator will add weight\n    and reduction arguments to the function. The decorated function will have\n    the signature like `loss_func(pred, target, weight=None, reduction=\'mean\',\n    avg_factor=None, **kwargs)`.\n\n    :Example:\n\n    >>> @weighted_loss\n    >>> def l1_loss(pred, target):\n    >>>     return (pred - target).abs()\n\n    >>> pred = torch.Tensor([0, 2, 3])\n    >>> target = torch.Tensor([1, 1, 1])\n    >>> weight = torch.Tensor([1, 0, 1])\n\n    >>> l1_loss(pred, target)\n    tensor(1.3333)\n    >>> l1_loss(pred, target, weight)\n    tensor(1.)\n    >>> l1_loss(pred, target, reduction=\'none\')\n    tensor([1., 1., 2.])\n    >>> l1_loss(pred, target, weight, avg_factor=2)\n    tensor(1.5000)\n    """"""\n\n    @functools.wraps(loss_func)\n    def wrapper(pred,\n                target,\n                weight=None,\n                reduction=\'mean\',\n                avg_factor=None,\n                **kwargs):\n        # get element-wise loss\n        loss = loss_func(pred, target, **kwargs)\n        loss = weight_reduce_loss(loss, weight, reduction, avg_factor)\n        return loss\n\n    return wrapper\n'"
mmdet/models/mask_heads/__init__.py,0,"b""from .fcn_mask_head import FCNMaskHead\nfrom .fused_semantic_head import FusedSemanticHead\nfrom .grid_head import GridHead\nfrom .htc_mask_head import HTCMaskHead\nfrom .maskiou_head import MaskIoUHead\n\n__all__ = [\n    'FCNMaskHead', 'HTCMaskHead', 'FusedSemanticHead', 'GridHead',\n    'MaskIoUHead'\n]\n"""
mmdet/models/mask_heads/fcn_mask_head.py,3,"b'import mmcv\nimport numpy as np\nimport pycocotools.mask as mask_util\nimport torch\nimport torch.nn as nn\n\nfrom ..builder import build_loss\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\nfrom mmdet.core import mask_target, force_fp32, auto_fp16\n\n\n@HEADS.register_module\nclass FCNMaskHead(nn.Module):\n\n    def __init__(self,\n                 num_convs=4,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 conv_out_channels=256,\n                 upsample_method=\'deconv\',\n                 upsample_ratio=2,\n                 num_classes=81,\n                 class_agnostic=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 loss_mask=dict(\n                     type=\'CrossEntropyLoss\', use_mask=True, loss_weight=1.0)):\n        super(FCNMaskHead, self).__init__()\n        if upsample_method not in [None, \'deconv\', \'nearest\', \'bilinear\']:\n            raise ValueError(\n                \'Invalid upsample method {}, accepted methods \'\n                \'are ""deconv"", ""nearest"", ""bilinear""\'.format(upsample_method))\n        self.num_convs = num_convs\n        self.roi_feat_size = roi_feat_size  # WARN: not used and reserved\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.conv_out_channels = conv_out_channels\n        self.upsample_method = upsample_method\n        self.upsample_ratio = upsample_ratio\n        self.num_classes = num_classes\n        self.class_agnostic = class_agnostic\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n        self.loss_mask = build_loss(loss_mask)\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            in_channels = (\n                self.in_channels if i == 0 else self.conv_out_channels)\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    padding=padding,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg))\n        upsample_in_channels = (\n            self.conv_out_channels if self.num_convs > 0 else in_channels)\n        if self.upsample_method is None:\n            self.upsample = None\n        elif self.upsample_method == \'deconv\':\n            self.upsample = nn.ConvTranspose2d(\n                upsample_in_channels,\n                self.conv_out_channels,\n                self.upsample_ratio,\n                stride=self.upsample_ratio)\n        else:\n            self.upsample = nn.Upsample(\n                scale_factor=self.upsample_ratio, mode=self.upsample_method)\n\n        out_channels = 1 if self.class_agnostic else self.num_classes\n        logits_in_channel = (\n            self.conv_out_channels\n            if self.upsample_method == \'deconv\' else upsample_in_channels)\n        self.conv_logits = nn.Conv2d(logits_in_channel, out_channels, 1)\n        self.relu = nn.ReLU(inplace=True)\n        self.debug_imgs = None\n\n    def init_weights(self):\n        for m in [self.upsample, self.conv_logits]:\n            if m is None:\n                continue\n            nn.init.kaiming_normal_(\n                m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            nn.init.constant_(m.bias, 0)\n\n    @auto_fp16()\n    def forward(self, x):\n        for conv in self.convs:\n            x = conv(x)\n        if self.upsample is not None:\n            x = self.upsample(x)\n            if self.upsample_method == \'deconv\':\n                x = self.relu(x)\n        mask_pred = self.conv_logits(x)\n        return mask_pred\n\n    def get_target(self, sampling_results, gt_masks, rcnn_train_cfg):\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n        mask_targets = mask_target(pos_proposals, pos_assigned_gt_inds,\n                                   gt_masks, rcnn_train_cfg)\n        return mask_targets\n\n    @force_fp32(apply_to=(\'mask_pred\', ))\n    def loss(self, mask_pred, mask_targets, labels):\n        loss = dict()\n        if self.class_agnostic:\n            loss_mask = self.loss_mask(mask_pred, mask_targets,\n                                       torch.zeros_like(labels))\n        else:\n            loss_mask = self.loss_mask(mask_pred, mask_targets, labels)\n        loss[\'loss_mask\'] = loss_mask\n        return loss\n\n    def get_seg_masks(self, mask_pred, det_bboxes, det_labels, rcnn_test_cfg,\n                      ori_shape, scale_factor, rescale):\n        """"""Get segmentation masks from mask_pred and bboxes.\n\n        Args:\n            mask_pred (Tensor or ndarray): shape (n, #class+1, h, w).\n                For single-scale testing, mask_pred is the direct output of\n                model, whose type is Tensor, while for multi-scale testing,\n                it will be converted to numpy array outside of this method.\n            det_bboxes (Tensor): shape (n, 4/5)\n            det_labels (Tensor): shape (n, )\n            img_shape (Tensor): shape (3, )\n            rcnn_test_cfg (dict): rcnn testing config\n            ori_shape: original image size\n\n        Returns:\n            list[list]: encoded masks\n        """"""\n        if isinstance(mask_pred, torch.Tensor):\n            mask_pred = mask_pred.sigmoid().cpu().numpy()\n        assert isinstance(mask_pred, np.ndarray)\n        # when enabling mixed precision training, mask_pred may be float16\n        # numpy array\n        mask_pred = mask_pred.astype(np.float32)\n\n        cls_segms = [[] for _ in range(self.num_classes - 1)]\n        bboxes = det_bboxes.cpu().numpy()[:, :4]\n        labels = det_labels.cpu().numpy() + 1\n\n        if rescale:\n            img_h, img_w = ori_shape[:2]\n        else:\n            img_h = np.round(ori_shape[0] * scale_factor).astype(np.int32)\n            img_w = np.round(ori_shape[1] * scale_factor).astype(np.int32)\n            scale_factor = 1.0\n\n        for i in range(bboxes.shape[0]):\n            bbox = (bboxes[i, :] / scale_factor).astype(np.int32)\n            label = labels[i]\n            w = max(bbox[2] - bbox[0] + 1, 1)\n            h = max(bbox[3] - bbox[1] + 1, 1)\n\n            if not self.class_agnostic:\n                mask_pred_ = mask_pred[i, label, :, :]\n            else:\n                mask_pred_ = mask_pred[i, 0, :, :]\n            im_mask = np.zeros((img_h, img_w), dtype=np.uint8)\n\n            bbox_mask = mmcv.imresize(mask_pred_, (w, h))\n            bbox_mask = (bbox_mask > rcnn_test_cfg.mask_thr_binary).astype(\n                np.uint8)\n            im_mask[bbox[1]:bbox[1] + h, bbox[0]:bbox[0] + w] = bbox_mask\n            rle = mask_util.encode(\n                np.array(im_mask[:, :, np.newaxis], order=\'F\'))[0]\n            cls_segms[label - 1].append(rle)\n\n        return cls_segms\n'"
mmdet/models/mask_heads/fused_semantic_head.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import kaiming_init\n\nfrom mmdet.core import auto_fp16, force_fp32\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\n\n\n@HEADS.register_module\nclass FusedSemanticHead(nn.Module):\n    """"""Multi-level fused semantic segmentation head.\n\n    in_1 -> 1x1 conv ---\n                        |\n    in_2 -> 1x1 conv -- |\n                       ||\n    in_3 -> 1x1 conv - ||\n                      |||                  /-> 1x1 conv (mask prediction)\n    in_4 -> 1x1 conv -----> 3x3 convs (*4)\n                        |                  \\-> 1x1 conv (feature)\n    in_5 -> 1x1 conv ---\n    """"""  # noqa: W605\n\n    def __init__(self,\n                 num_ins,\n                 fusion_level,\n                 num_convs=4,\n                 in_channels=256,\n                 conv_out_channels=256,\n                 num_classes=183,\n                 ignore_label=255,\n                 loss_weight=0.2,\n                 conv_cfg=None,\n                 norm_cfg=None):\n        super(FusedSemanticHead, self).__init__()\n        self.num_ins = num_ins\n        self.fusion_level = fusion_level\n        self.num_convs = num_convs\n        self.in_channels = in_channels\n        self.conv_out_channels = conv_out_channels\n        self.num_classes = num_classes\n        self.ignore_label = ignore_label\n        self.loss_weight = loss_weight\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.fp16_enabled = False\n\n        self.lateral_convs = nn.ModuleList()\n        for i in range(self.num_ins):\n            self.lateral_convs.append(\n                ConvModule(\n                    self.in_channels,\n                    self.in_channels,\n                    1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    inplace=False))\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_convs):\n            in_channels = self.in_channels if i == 0 else conv_out_channels\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    conv_out_channels,\n                    3,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg))\n        self.conv_embedding = ConvModule(\n            conv_out_channels,\n            conv_out_channels,\n            1,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg)\n        self.conv_logits = nn.Conv2d(conv_out_channels, self.num_classes, 1)\n\n        self.criterion = nn.CrossEntropyLoss(ignore_index=ignore_label)\n\n    def init_weights(self):\n        kaiming_init(self.conv_logits)\n\n    @auto_fp16()\n    def forward(self, feats):\n        x = self.lateral_convs[self.fusion_level](feats[self.fusion_level])\n        fused_size = tuple(x.shape[-2:])\n        for i, feat in enumerate(feats):\n            if i != self.fusion_level:\n                feat = F.interpolate(\n                    feat, size=fused_size, mode=\'bilinear\', align_corners=True)\n                x += self.lateral_convs[i](feat)\n\n        for i in range(self.num_convs):\n            x = self.convs[i](x)\n\n        mask_pred = self.conv_logits(x)\n        x = self.conv_embedding(x)\n        return mask_pred, x\n\n    @force_fp32(apply_to=(\'mask_pred\',))\n    def loss(self, mask_pred, labels):\n        labels = labels.squeeze(1).long()\n        loss_semantic_seg = self.criterion(mask_pred, labels)\n        loss_semantic_seg *= self.loss_weight\n        return loss_semantic_seg\n'"
mmdet/models/mask_heads/grid_head.py,10,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import kaiming_init, normal_init\n\nfrom ..builder import build_loss\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\n\n\n@HEADS.register_module\nclass GridHead(nn.Module):\n\n    def __init__(self,\n                 grid_points=9,\n                 num_convs=8,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_kernel_size=3,\n                 point_feat_channels=64,\n                 deconv_kernel_size=4,\n                 class_agnostic=False,\n                 loss_grid=dict(\n                     type=\'CrossEntropyLoss\', use_sigmoid=True,\n                     loss_weight=15),\n                 conv_cfg=None,\n                 norm_cfg=dict(type=\'GN\', num_groups=36)):\n        super(GridHead, self).__init__()\n        self.grid_points = grid_points\n        self.num_convs = num_convs\n        self.roi_feat_size = roi_feat_size\n        self.in_channels = in_channels\n        self.conv_kernel_size = conv_kernel_size\n        self.point_feat_channels = point_feat_channels\n        self.conv_out_channels = self.point_feat_channels * self.grid_points\n        self.class_agnostic = class_agnostic\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        if isinstance(norm_cfg, dict) and norm_cfg[\'type\'] == \'GN\':\n            assert self.conv_out_channels % norm_cfg[\'num_groups\'] == 0\n\n        assert self.grid_points >= 4\n        self.grid_size = int(np.sqrt(self.grid_points))\n        if self.grid_size * self.grid_size != self.grid_points:\n            raise ValueError(\'grid_points must be a square number\')\n\n        # the predicted heatmap is half of whole_map_size\n        self.whole_map_size = self.roi_feat_size * 4\n\n        # compute point-wise sub-regions\n        self.sub_regions = self.calc_sub_regions()\n\n        self.convs = []\n        for i in range(self.num_convs):\n            in_channels = (\n                self.in_channels if i == 0 else self.conv_out_channels)\n            stride = 2 if i == 0 else 1\n            padding = (self.conv_kernel_size - 1) // 2\n            self.convs.append(\n                ConvModule(\n                    in_channels,\n                    self.conv_out_channels,\n                    self.conv_kernel_size,\n                    stride=stride,\n                    padding=padding,\n                    conv_cfg=self.conv_cfg,\n                    norm_cfg=self.norm_cfg,\n                    bias=True))\n        self.convs = nn.Sequential(*self.convs)\n\n        self.deconv1 = nn.ConvTranspose2d(\n            self.conv_out_channels,\n            self.conv_out_channels,\n            kernel_size=deconv_kernel_size,\n            stride=2,\n            padding=(deconv_kernel_size - 2) // 2,\n            groups=grid_points)\n        self.norm1 = nn.GroupNorm(grid_points, self.conv_out_channels)\n        self.deconv2 = nn.ConvTranspose2d(\n            self.conv_out_channels,\n            grid_points,\n            kernel_size=deconv_kernel_size,\n            stride=2,\n            padding=(deconv_kernel_size - 2) // 2,\n            groups=grid_points)\n\n        # find the 4-neighbor of each grid point\n        self.neighbor_points = []\n        grid_size = self.grid_size\n        for i in range(grid_size):  # i-th column\n            for j in range(grid_size):  # j-th row\n                neighbors = []\n                if i > 0:  # left: (i - 1, j)\n                    neighbors.append((i - 1) * grid_size + j)\n                if j > 0:  # up: (i, j - 1)\n                    neighbors.append(i * grid_size + j - 1)\n                if j < grid_size - 1:  # down: (i, j + 1)\n                    neighbors.append(i * grid_size + j + 1)\n                if i < grid_size - 1:  # right: (i + 1, j)\n                    neighbors.append((i + 1) * grid_size + j)\n                self.neighbor_points.append(tuple(neighbors))\n        # total edges in the grid\n        self.num_edges = sum([len(p) for p in self.neighbor_points])\n\n        self.forder_trans = nn.ModuleList()  # first-order feature transition\n        self.sorder_trans = nn.ModuleList()  # second-order feature transition\n        for neighbors in self.neighbor_points:\n            fo_trans = nn.ModuleList()\n            so_trans = nn.ModuleList()\n            for _ in range(len(neighbors)):\n                # each transition module consists of a 5x5 depth-wise conv and\n                # 1x1 conv.\n                fo_trans.append(\n                    nn.Sequential(\n                        nn.Conv2d(\n                            self.point_feat_channels,\n                            self.point_feat_channels,\n                            5,\n                            stride=1,\n                            padding=2,\n                            groups=self.point_feat_channels),\n                        nn.Conv2d(self.point_feat_channels,\n                                  self.point_feat_channels, 1)))\n                so_trans.append(\n                    nn.Sequential(\n                        nn.Conv2d(\n                            self.point_feat_channels,\n                            self.point_feat_channels,\n                            5,\n                            1,\n                            2,\n                            groups=self.point_feat_channels),\n                        nn.Conv2d(self.point_feat_channels,\n                                  self.point_feat_channels, 1)))\n            self.forder_trans.append(fo_trans)\n            self.sorder_trans.append(so_trans)\n\n        self.loss_grid = build_loss(loss_grid)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                # TODO: compare mode = ""fan_in"" or ""fan_out""\n                kaiming_init(m)\n        for m in self.modules():\n            if isinstance(m, nn.ConvTranspose2d):\n                normal_init(m, std=0.001)\n        nn.init.constant_(self.deconv2.bias, -np.log(0.99 / 0.01))\n\n    def forward(self, x):\n        assert x.shape[-1] == x.shape[-2] == self.roi_feat_size\n        # RoI feature transformation, downsample 2x\n        x = self.convs(x)\n\n        c = self.point_feat_channels\n        # first-order fusion\n        x_fo = [None for _ in range(self.grid_points)]\n        for i, points in enumerate(self.neighbor_points):\n            x_fo[i] = x[:, i * c:(i + 1) * c]\n            for j, point_idx in enumerate(points):\n                x_fo[i] = x_fo[i] + self.forder_trans[i][j](\n                    x[:, point_idx * c:(point_idx + 1) * c])\n\n        # second-order fusion\n        x_so = [None for _ in range(self.grid_points)]\n        for i, points in enumerate(self.neighbor_points):\n            x_so[i] = x[:, i * c:(i + 1) * c]\n            for j, point_idx in enumerate(points):\n                x_so[i] = x_so[i] + self.sorder_trans[i][j](x_fo[point_idx])\n\n        # predicted heatmap with fused features\n        x2 = torch.cat(x_so, dim=1)\n        x2 = self.deconv1(x2)\n        x2 = F.relu(self.norm1(x2), inplace=True)\n        heatmap = self.deconv2(x2)\n\n        # predicted heatmap with original features (applicable during training)\n        if self.training:\n            x1 = x\n            x1 = self.deconv1(x1)\n            x1 = F.relu(self.norm1(x1), inplace=True)\n            heatmap_unfused = self.deconv2(x1)\n        else:\n            heatmap_unfused = heatmap\n\n        return dict(fused=heatmap, unfused=heatmap_unfused)\n\n    def calc_sub_regions(self):\n        """"""Compute point specific representation regions.\n\n        See Grid R-CNN Plus (https://arxiv.org/abs/1906.05688) for details.\n        """"""\n        # to make it consistent with the original implementation, half_size\n        # is computed as 2 * quarter_size, which is smaller\n        half_size = self.whole_map_size // 4 * 2\n        sub_regions = []\n        for i in range(self.grid_points):\n            x_idx = i // self.grid_size\n            y_idx = i % self.grid_size\n            if x_idx == 0:\n                sub_x1 = 0\n            elif x_idx == self.grid_size - 1:\n                sub_x1 = half_size\n            else:\n                ratio = x_idx / (self.grid_size - 1) - 0.25\n                sub_x1 = max(int(ratio * self.whole_map_size), 0)\n\n            if y_idx == 0:\n                sub_y1 = 0\n            elif y_idx == self.grid_size - 1:\n                sub_y1 = half_size\n            else:\n                ratio = y_idx / (self.grid_size - 1) - 0.25\n                sub_y1 = max(int(ratio * self.whole_map_size), 0)\n            sub_regions.append(\n                (sub_x1, sub_y1, sub_x1 + half_size, sub_y1 + half_size))\n        return sub_regions\n\n    def get_target(self, sampling_results, rcnn_train_cfg):\n        # mix all samples (across images) together.\n        pos_bboxes = torch.cat([res.pos_bboxes for res in sampling_results],\n                               dim=0).cpu()\n        pos_gt_bboxes = torch.cat(\n            [res.pos_gt_bboxes for res in sampling_results], dim=0).cpu()\n        assert pos_bboxes.shape == pos_gt_bboxes.shape\n\n        # expand pos_bboxes to 2x of original size\n        x1 = pos_bboxes[:, 0] - (pos_bboxes[:, 2] - pos_bboxes[:, 0]) / 2\n        y1 = pos_bboxes[:, 1] - (pos_bboxes[:, 3] - pos_bboxes[:, 1]) / 2\n        x2 = pos_bboxes[:, 2] + (pos_bboxes[:, 2] - pos_bboxes[:, 0]) / 2\n        y2 = pos_bboxes[:, 3] + (pos_bboxes[:, 3] - pos_bboxes[:, 1]) / 2\n        pos_bboxes = torch.stack([x1, y1, x2, y2], dim=-1)\n        pos_bbox_ws = (pos_bboxes[:, 2] - pos_bboxes[:, 0]).unsqueeze(-1)\n        pos_bbox_hs = (pos_bboxes[:, 3] - pos_bboxes[:, 1]).unsqueeze(-1)\n\n        num_rois = pos_bboxes.shape[0]\n        map_size = self.whole_map_size\n        # this is not the final target shape\n        targets = torch.zeros((num_rois, self.grid_points, map_size, map_size),\n                              dtype=torch.float)\n\n        # pre-compute interpolation factors for all grid points.\n        # the first item is the factor of x-dim, and the second is y-dim.\n        # for a 9-point grid, factors are like (1, 0), (0.5, 0.5), (0, 1)\n        factors = []\n        for j in range(self.grid_points):\n            x_idx = j // self.grid_size\n            y_idx = j % self.grid_size\n            factors.append((1 - x_idx / (self.grid_size - 1),\n                            1 - y_idx / (self.grid_size - 1)))\n\n        radius = rcnn_train_cfg.pos_radius\n        radius2 = radius**2\n        for i in range(num_rois):\n            # ignore small bboxes\n            if (pos_bbox_ws[i] <= self.grid_size\n                    or pos_bbox_hs[i] <= self.grid_size):\n                continue\n            # for each grid point, mark a small circle as positive\n            for j in range(self.grid_points):\n                factor_x, factor_y = factors[j]\n                gridpoint_x = factor_x * pos_gt_bboxes[i, 0] + (\n                    1 - factor_x) * pos_gt_bboxes[i, 2]\n                gridpoint_y = factor_y * pos_gt_bboxes[i, 1] + (\n                    1 - factor_y) * pos_gt_bboxes[i, 3]\n\n                cx = int((gridpoint_x - pos_bboxes[i, 0]) / pos_bbox_ws[i] *\n                         map_size)\n                cy = int((gridpoint_y - pos_bboxes[i, 1]) / pos_bbox_hs[i] *\n                         map_size)\n\n                for x in range(cx - radius, cx + radius + 1):\n                    for y in range(cy - radius, cy + radius + 1):\n                        if x >= 0 and x < map_size and y >= 0 and y < map_size:\n                            if (x - cx)**2 + (y - cy)**2 <= radius2:\n                                targets[i, j, y, x] = 1\n        # reduce the target heatmap size by a half\n        # proposed in Grid R-CNN Plus (https://arxiv.org/abs/1906.05688).\n        sub_targets = []\n        for i in range(self.grid_points):\n            sub_x1, sub_y1, sub_x2, sub_y2 = self.sub_regions[i]\n            sub_targets.append(targets[:, [i], sub_y1:sub_y2, sub_x1:sub_x2])\n        sub_targets = torch.cat(sub_targets, dim=1)\n        sub_targets = sub_targets.cuda()\n        return sub_targets\n\n    def loss(self, grid_pred, grid_targets):\n        loss_fused = self.loss_grid(grid_pred[\'fused\'], grid_targets)\n        loss_unfused = self.loss_grid(grid_pred[\'unfused\'], grid_targets)\n        loss_grid = loss_fused + loss_unfused\n        return dict(loss_grid=loss_grid)\n\n    def get_bboxes(self, det_bboxes, grid_pred, img_meta):\n        # TODO: refactoring\n        assert det_bboxes.shape[0] == grid_pred.shape[0]\n        det_bboxes = det_bboxes.cpu()\n        cls_scores = det_bboxes[:, [4]]\n        det_bboxes = det_bboxes[:, :4]\n        grid_pred = grid_pred.sigmoid().cpu()\n\n        R, c, h, w = grid_pred.shape\n        half_size = self.whole_map_size // 4 * 2\n        assert h == w == half_size\n        assert c == self.grid_points\n\n        # find the point with max scores in the half-sized heatmap\n        grid_pred = grid_pred.view(R * c, h * w)\n        pred_scores, pred_position = grid_pred.max(dim=1)\n        xs = pred_position % w\n        ys = pred_position // w\n\n        # get the position in the whole heatmap instead of half-sized heatmap\n        for i in range(self.grid_points):\n            xs[i::self.grid_points] += self.sub_regions[i][0]\n            ys[i::self.grid_points] += self.sub_regions[i][1]\n\n        # reshape to (num_rois, grid_points)\n        pred_scores, xs, ys = tuple(\n            map(lambda x: x.view(R, c), [pred_scores, xs, ys]))\n\n        # get expanded pos_bboxes\n        widths = (det_bboxes[:, 2] - det_bboxes[:, 0]).unsqueeze(-1)\n        heights = (det_bboxes[:, 3] - det_bboxes[:, 1]).unsqueeze(-1)\n        x1 = (det_bboxes[:, 0, None] - widths / 2)\n        y1 = (det_bboxes[:, 1, None] - heights / 2)\n        # map the grid point to the absolute coordinates\n        abs_xs = (xs.float() + 0.5) / w * widths + x1\n        abs_ys = (ys.float() + 0.5) / h * heights + y1\n\n        # get the grid points indices that fall on the bbox boundaries\n        x1_inds = [i for i in range(self.grid_size)]\n        y1_inds = [i * self.grid_size for i in range(self.grid_size)]\n        x2_inds = [\n            self.grid_points - self.grid_size + i\n            for i in range(self.grid_size)\n        ]\n        y2_inds = [(i + 1) * self.grid_size - 1 for i in range(self.grid_size)]\n\n        # voting of all grid points on some boundary\n        bboxes_x1 = (abs_xs[:, x1_inds] * pred_scores[:, x1_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, x1_inds].sum(dim=1, keepdim=True))\n        bboxes_y1 = (abs_ys[:, y1_inds] * pred_scores[:, y1_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, y1_inds].sum(dim=1, keepdim=True))\n        bboxes_x2 = (abs_xs[:, x2_inds] * pred_scores[:, x2_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, x2_inds].sum(dim=1, keepdim=True))\n        bboxes_y2 = (abs_ys[:, y2_inds] * pred_scores[:, y2_inds]).sum(\n            dim=1, keepdim=True) / (\n                pred_scores[:, y2_inds].sum(dim=1, keepdim=True))\n\n        bbox_res = torch.cat(\n            [bboxes_x1, bboxes_y1, bboxes_x2, bboxes_y2, cls_scores], dim=1)\n        bbox_res[:, [0, 2]].clamp_(min=0, max=img_meta[0][\'img_shape\'][1] - 1)\n        bbox_res[:, [1, 3]].clamp_(min=0, max=img_meta[0][\'img_shape\'][0] - 1)\n\n        return bbox_res\n'"
mmdet/models/mask_heads/htc_mask_head.py,0,"b""from .fcn_mask_head import FCNMaskHead\nfrom ..registry import HEADS\nfrom ..utils import ConvModule\n\n\n@HEADS.register_module\nclass HTCMaskHead(FCNMaskHead):\n\n    def __init__(self, *args, **kwargs):\n        super(HTCMaskHead, self).__init__(*args, **kwargs)\n        self.conv_res = ConvModule(\n            self.conv_out_channels,\n            self.conv_out_channels,\n            1,\n            conv_cfg=self.conv_cfg,\n            norm_cfg=self.norm_cfg)\n\n    def init_weights(self):\n        super(HTCMaskHead, self).init_weights()\n        self.conv_res.init_weights()\n\n    def forward(self, x, res_feat=None, return_logits=True, return_feat=True):\n        if res_feat is not None:\n            res_feat = self.conv_res(res_feat)\n            x = x + res_feat\n        for conv in self.convs:\n            x = conv(x)\n        res_feat = x\n        outs = []\n        if return_logits:\n            x = self.upsample(x)\n            if self.upsample_method == 'deconv':\n                x = self.relu(x)\n            mask_pred = self.conv_logits(x)\n            outs.append(mask_pred)\n        if return_feat:\n            outs.append(res_feat)\n        return outs if len(outs) > 1 else outs[0]\n"""
mmdet/models/mask_heads/maskiou_head.py,4,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom mmcv.cnn import kaiming_init, normal_init\nfrom mmdet.core import force_fp32\n\nfrom ..builder import build_loss\nfrom ..registry import HEADS\n\n\n@HEADS.register_module\nclass MaskIoUHead(nn.Module):\n    """"""Mask IoU Head.\n\n    This head predicts the IoU of predicted masks and corresponding gt masks.\n    """"""\n\n    def __init__(self,\n                 num_convs=4,\n                 num_fcs=2,\n                 roi_feat_size=14,\n                 in_channels=256,\n                 conv_out_channels=256,\n                 fc_out_channels=1024,\n                 num_classes=81,\n                 loss_iou=dict(type=\'MSELoss\', loss_weight=0.5)):\n        super(MaskIoUHead, self).__init__()\n        self.in_channels = in_channels\n        self.conv_out_channels = conv_out_channels\n        self.fc_out_channels = fc_out_channels\n        self.num_classes = num_classes\n        self.fp16_enabled = False\n\n        self.convs = nn.ModuleList()\n        for i in range(num_convs):\n            if i == 0:\n                # concatenation of mask feature and mask prediction\n                in_channels = self.in_channels + 1\n            else:\n                in_channels = self.conv_out_channels\n            stride = 2 if i == num_convs - 1 else 1\n            self.convs.append(\n                nn.Conv2d(\n                    in_channels,\n                    self.conv_out_channels,\n                    3,\n                    stride=stride,\n                    padding=1))\n\n        self.fcs = nn.ModuleList()\n        for i in range(num_fcs):\n            in_channels = self.conv_out_channels * (\n                roi_feat_size // 2)**2 if i == 0 else self.fc_out_channels\n            self.fcs.append(nn.Linear(in_channels, self.fc_out_channels))\n\n        self.fc_mask_iou = nn.Linear(self.fc_out_channels, self.num_classes)\n        self.relu = nn.ReLU()\n        self.max_pool = nn.MaxPool2d(2, 2)\n        self.loss_iou = build_loss(loss_iou)\n\n    def init_weights(self):\n        for conv in self.convs:\n            kaiming_init(conv)\n        for fc in self.fcs:\n            kaiming_init(\n                fc,\n                a=1,\n                mode=\'fan_in\',\n                nonlinearity=\'leaky_relu\',\n                distribution=\'uniform\')\n        normal_init(self.fc_mask_iou, std=0.01)\n\n    def forward(self, mask_feat, mask_pred):\n        mask_pred = mask_pred.sigmoid()\n        mask_pred_pooled = self.max_pool(mask_pred.unsqueeze(1))\n\n        x = torch.cat((mask_feat, mask_pred_pooled), 1)\n\n        for conv in self.convs:\n            x = self.relu(conv(x))\n        x = x.view(x.size(0), -1)\n        for fc in self.fcs:\n            x = self.relu(fc(x))\n        mask_iou = self.fc_mask_iou(x)\n        return mask_iou\n\n    @force_fp32(apply_to=(\'mask_iou_pred\', ))\n    def loss(self, mask_iou_pred, mask_iou_targets):\n        pos_inds = mask_iou_targets > 0\n        if pos_inds.sum() > 0:\n            loss_mask_iou = self.loss_iou(mask_iou_pred[pos_inds],\n                                          mask_iou_targets[pos_inds])\n        else:\n            loss_mask_iou = mask_iou_pred * 0\n        return dict(loss_mask_iou=loss_mask_iou)\n\n    @force_fp32(apply_to=(\'mask_pred\', ))\n    def get_target(self, sampling_results, gt_masks, mask_pred, mask_targets,\n                   rcnn_train_cfg):\n        """"""Compute target of mask IoU.\n\n        Mask IoU target is the IoU of the predicted mask (inside a bbox) and\n        the gt mask of corresponding gt mask (the whole instance).\n        The intersection area is computed inside the bbox, and the gt mask area\n        is computed with two steps, firstly we compute the gt area inside the\n        bbox, then divide it by the area ratio of gt area inside the bbox and\n        the gt area of the whole instance.\n\n        Args:\n            sampling_results (list[:obj:`SamplingResult`]): sampling results.\n            gt_masks (list[ndarray]): Gt masks (the whole instance) of each\n                image, binary maps with the same shape of the input image.\n            mask_pred (Tensor): Predicted masks of each positive proposal,\n                shape (num_pos, h, w).\n            mask_targets (Tensor): Gt mask of each positive proposal,\n                binary map of the shape (num_pos, h, w).\n            rcnn_train_cfg (dict): Training config for R-CNN part.\n\n        Returns:\n            Tensor: mask iou target (length == num positive).\n        """"""\n        pos_proposals = [res.pos_bboxes for res in sampling_results]\n        pos_assigned_gt_inds = [\n            res.pos_assigned_gt_inds for res in sampling_results\n        ]\n\n        # compute the area ratio of gt areas inside the proposals and\n        # the whole instance\n        area_ratios = map(self._get_area_ratio, pos_proposals,\n                          pos_assigned_gt_inds, gt_masks)\n        area_ratios = torch.cat(list(area_ratios))\n        assert mask_targets.size(0) == area_ratios.size(0)\n\n        mask_pred = (mask_pred > rcnn_train_cfg.mask_thr_binary).float()\n        mask_pred_areas = mask_pred.sum((-1, -2))\n\n        # mask_pred and mask_targets are binary maps\n        overlap_areas = (mask_pred * mask_targets).sum((-1, -2))\n\n        # compute the mask area of the whole instance\n        gt_full_areas = mask_targets.sum((-1, -2)) / (area_ratios + 1e-7)\n\n        mask_iou_targets = overlap_areas / (\n            mask_pred_areas + gt_full_areas - overlap_areas)\n        return mask_iou_targets\n\n    def _get_area_ratio(self, pos_proposals, pos_assigned_gt_inds, gt_masks):\n        """"""Compute area ratio of the gt mask inside the proposal and the gt\n        mask of the corresponding instance""""""\n        num_pos = pos_proposals.size(0)\n        if num_pos > 0:\n            area_ratios = []\n            proposals_np = pos_proposals.cpu().numpy()\n            pos_assigned_gt_inds = pos_assigned_gt_inds.cpu().numpy()\n            # compute mask areas of gt instances (batch processing for speedup)\n            gt_instance_mask_area = gt_masks.sum((-1, -2))\n            for i in range(num_pos):\n                gt_mask = gt_masks[pos_assigned_gt_inds[i]]\n\n                # crop the gt mask inside the proposal\n                x1, y1, x2, y2 = proposals_np[i, :].astype(np.int32)\n                gt_mask_in_proposal = gt_mask[y1:y2 + 1, x1:x2 + 1]\n\n                ratio = gt_mask_in_proposal.sum() / (\n                    gt_instance_mask_area[pos_assigned_gt_inds[i]] + 1e-7)\n                area_ratios.append(ratio)\n            area_ratios = torch.from_numpy(np.stack(area_ratios)).float().to(\n                pos_proposals.device)\n        else:\n            area_ratios = pos_proposals.new_zeros((0, ))\n        return area_ratios\n\n    @force_fp32(apply_to=(\'mask_iou_pred\', ))\n    def get_mask_scores(self, mask_iou_pred, det_bboxes, det_labels):\n        """"""Get the mask scores.\n\n        mask_score = bbox_score * mask_iou\n        """"""\n        inds = range(det_labels.size(0))\n        mask_scores = mask_iou_pred[inds, det_labels +\n                                    1] * det_bboxes[inds, -1]\n        mask_scores = mask_scores.cpu().numpy()\n        det_labels = det_labels.cpu().numpy()\n        return [\n            mask_scores[det_labels == i] for i in range(self.num_classes - 1)\n        ]\n'"
mmdet/models/necks/__init__.py,0,"b""from .fpn import FPN\nfrom .bfp import BFP\nfrom .hrfpn import HRFPN\n\n__all__ = ['FPN', 'BFP', 'HRFPN']\n"""
mmdet/models/necks/bfp.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom ..plugins import NonLocal2D\nfrom ..registry import NECKS\nfrom ..utils import ConvModule\n\n\n@NECKS.register_module\nclass BFP(nn.Module):\n    """"""BFP (Balanced Feature Pyrmamids)\n\n    BFP takes multi-level features as inputs and gather them into a single one,\n    then refine the gathered feature and scatter the refined results to\n    multi-level features. This module is used in Libra R-CNN (CVPR 2019), see\n    https://arxiv.org/pdf/1904.02701.pdf for details.\n\n    Args:\n        in_channels (int): Number of input channels (feature maps of all levels\n            should have the same channels).\n        num_levels (int): Number of input feature levels.\n        conv_cfg (dict): The config dict for convolution layers.\n        norm_cfg (dict): The config dict for normalization layers.\n        refine_level (int): Index of integration and refine level of BSF in\n            multi-level features from bottom to top.\n        refine_type (str): Type of the refine op, currently support\n            [None, \'conv\', \'non_local\'].\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 num_levels,\n                 refine_level=2,\n                 refine_type=None,\n                 conv_cfg=None,\n                 norm_cfg=None):\n        super(BFP, self).__init__()\n        assert refine_type in [None, \'conv\', \'non_local\']\n\n        self.in_channels = in_channels\n        self.num_levels = num_levels\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        self.refine_level = refine_level\n        self.refine_type = refine_type\n        assert 0 <= self.refine_level < self.num_levels\n\n        if self.refine_type == \'conv\':\n            self.refine = ConvModule(\n                self.in_channels,\n                self.in_channels,\n                3,\n                padding=1,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg)\n        elif self.refine_type == \'non_local\':\n            self.refine = NonLocal2D(\n                self.in_channels,\n                reduction=1,\n                use_scale=False,\n                conv_cfg=self.conv_cfg,\n                norm_cfg=self.norm_cfg)\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution=\'uniform\')\n\n    def forward(self, inputs):\n        assert len(inputs) == self.num_levels\n\n        # step 1: gather multi-level features by resize and average\n        feats = []\n        gather_size = inputs[self.refine_level].size()[2:]\n        for i in range(self.num_levels):\n            if i < self.refine_level:\n                gathered = F.adaptive_max_pool2d(\n                    inputs[i], output_size=gather_size)\n            else:\n                gathered = F.interpolate(\n                    inputs[i], size=gather_size, mode=\'nearest\')\n            feats.append(gathered)\n\n        bsf = sum(feats) / len(feats)\n\n        # step 2: refine gathered features\n        if self.refine_type is not None:\n            bsf = self.refine(bsf)\n\n        # step 3: scatter refined features to multi-levels by a residual path\n        outs = []\n        for i in range(self.num_levels):\n            out_size = inputs[i].size()[2:]\n            if i < self.refine_level:\n                residual = F.interpolate(bsf, size=out_size, mode=\'nearest\')\n            else:\n                residual = F.adaptive_max_pool2d(bsf, output_size=out_size)\n            outs.append(residual + inputs[i])\n\n        return tuple(outs)\n'"
mmdet/models/necks/fpn.py,2,"b""import torch.nn as nn\nimport torch.nn.functional as F\nfrom mmcv.cnn import xavier_init\n\nfrom mmdet.core import auto_fp16\nfrom ..registry import NECKS\nfrom ..utils import ConvModule\n\n\n@NECKS.register_module\nclass FPN(nn.Module):\n\n    def __init__(self,\n                 in_channels,   # [256, 512, 1024, 2048]\n                 out_channels,  # 256\n                 num_outs,      # 5\n                 start_level=0,\n                 end_level=-1,\n                 add_extra_convs=False,\n                 extra_convs_on_inputs=True,\n                 relu_before_extra_convs=False,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=None):  # \xe7\x94\x9a\xe8\x87\xb3\xe8\xbf\x99\xe9\x87\x8c\xe8\xbf\x98\xe8\x83\xbd\xe5\x8a\xa0\xe6\xbf\x80\xe6\xb4\xbb\xe5\x87\xbd\xe6\x95\xb0\n        super(FPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.activation = activation\n        self.relu_before_extra_convs = relu_before_extra_convs\n        self.fp16_enabled = False\n\n        if end_level == -1:\n            self.backbone_end_level = self.num_ins  # 4     \n            assert num_outs >= self.num_ins - start_level\n        else:\n            # if end_level < inputs, no extra level is allowed\n            self.backbone_end_level = end_level\n            assert end_level <= len(in_channels)\n            assert num_outs == end_level - start_level\n        self.start_level = start_level\n        self.end_level = end_level\n        self.add_extra_convs = add_extra_convs\n        self.extra_convs_on_inputs = extra_convs_on_inputs\n\n        self.lateral_convs = nn.ModuleList()\n        self.fpn_convs = nn.ModuleList()\n\n        for i in range(self.start_level, self.backbone_end_level):\n            # \xe8\xaf\xa5\xe6\xa8\xa1\xe5\x9d\x97\xe6\x98\xafconv + bn/bias + activation\n            l_conv = ConvModule(    \n                in_channels[i],\n                out_channels,\n                1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                activation=self.activation,\n                inplace=False)\n            # \xe4\xb8\x8d\xe9\x9a\xbe\xe7\x9c\x8b\xe5\x87\xba\xef\xbc\x8c\xe8\xbf\x99\xe9\x87\x8c\xe8\x99\xbd\xe7\x84\xb6\xe5\x8a\xa0\xe4\xba\x86\xe5\x9b\x9b\xe4\xb8\xaa\xe5\x8d\xb7\xe7\xa7\xaf\xe6\xa0\xb8\xef\xbc\x88\xe6\xaf\x8f\xe6\xac\xa1\xe9\x81\x8d\xe5\x8e\x86\xe4\xb8\x80\xe4\xb8\xaa\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xef\xbc\x89\xef\xbc\x8c\xe4\xbd\x86\xe6\x98\xaf\xe5\xb0\xba\xe5\xaf\xb8\xe4\xb8\x80\xe6\xa0\xb7\xef\xbc\x88\xe8\xbf\x99\xe9\x87\x8c\xe4\xb8\x8d\xe5\x85\xb1\xe7\x94\xa8\xef\xbc\x8c\xe4\xb8\x8d\xe5\x90\x8c\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe5\xad\xa6\xe4\xb9\xa0\xe7\x9a\x84\xe5\x8f\x82\xe6\x95\xb0\xe4\xb8\x8d\xe5\x90\x8c\xef\xbc\x89\n            fpn_conv = ConvModule(\n                out_channels,\n                out_channels,\n                3,\n                padding=1,\n                conv_cfg=conv_cfg,\n                norm_cfg=norm_cfg,\n                activation=self.activation,\n                inplace=False)\n\n            self.lateral_convs.append(l_conv)\n            self.fpn_convs.append(fpn_conv)\n\n        # add extra conv layers (e.g., RetinaNet)\n        extra_levels = num_outs - self.backbone_end_level + self.start_level\n        if add_extra_convs and extra_levels >= 1:\n            for i in range(extra_levels):\n                if i == 0 and self.extra_convs_on_inputs:\n                    in_channels = self.in_channels[self.backbone_end_level - 1]\n                else:\n                    in_channels = out_channels\n                extra_fpn_conv = ConvModule(\n                    in_channels,\n                    out_channels,\n                    3,\n                    stride=2,\n                    padding=1,\n                    conv_cfg=conv_cfg,\n                    norm_cfg=norm_cfg,\n                    activation=self.activation,\n                    inplace=False)\n                self.fpn_convs.append(extra_fpn_conv)\n\n    # default init_weights for conv(msra) and norm in ConvModule\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                xavier_init(m, distribution='uniform')\n\n    @auto_fp16()\n    def forward(self, inputs):\n        # len(inputs)\xe6\x8f\x8f\xe8\xbf\xb0\xe4\xbc\xa0\xe9\x80\x92\xe8\xbf\x87\xe6\x9d\xa5\xe7\x9a\x84\xe5\x90\x84stage\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe6\x95\xb0\xe7\x9b\xae\n        assert len(inputs) == len(self.in_channels)\n\n        # build laterals\n        laterals = [\n            lateral_conv(inputs[i + self.start_level])\n            for i, lateral_conv in enumerate(self.lateral_convs)\n        ]\n\n        # build top-down path  \xe5\xbc\x80\xe5\xa7\x8b\xe8\xbf\x9b\xe8\xa1\x8c\xe7\x89\xb9\xe5\xbe\x81\xe7\x9a\x84\xe8\x87\xaa\xe9\xa1\xb6\xe5\x90\x91\xe4\xb8\x8b\xe8\x9e\x8d\xe5\x90\x88\n        used_backbone_levels = len(laterals)\n        for i in range(used_backbone_levels - 1, 0, -1):\n            laterals[i - 1] += F.interpolate(\n                laterals[i], scale_factor=2, mode='nearest')\n\n        # build outputs\n        # part 1: from original levels\n        # \xe5\xb0\x86\xe6\xaf\x8f\xe4\xb8\xaa\xe8\x9e\x8d\xe5\x90\x88\xe7\x9a\x84\xe5\xb1\x82\xe8\xbf\x9b\xe8\xa1\x8c3*3\xe5\x8d\xb7\xe7\xa7\xaf\xe8\xbe\x93\xe5\x87\xba\xef\xbc\x8c\xe6\xb2\xa1\xe8\x9e\x8d\xe5\x90\x88\xe7\x9a\x842048\xe6\x9c\x80\xe9\xab\x98\xe5\xb1\x82\xe4\xb9\x9f\xe8\xa6\x81\xe5\x8d\xb7\xe7\xa7\xaf\n        outs = [\n            self.fpn_convs[i](laterals[i]) for i in range(used_backbone_levels)\n        ]\n        # part 2: add extra levels\n        if self.num_outs > len(outs):\n            # \xe8\xbe\x93\xe5\x87\xba\xe6\x98\xaf5\xe5\xb1\x82\xe4\xbd\x86\xe6\x98\xaf\xe8\x9e\x8d\xe5\x90\x88\xe5\x8f\xaa\xe6\x9c\x89\xe5\x9b\x9b\xe5\xb1\x82\xef\xbc\x8c\xe4\xb8\xba\xe4\xba\x86\xe8\x8e\xb7\xe5\xbe\x97\xe6\x9b\xb4\xe5\xa4\x9a\xe7\x9a\x84\xe7\x89\xb9\xe5\xbe\x81\xef\xbc\x8c\xe9\x87\x87\xe7\x94\xa8maxpooling\xe5\xaf\xb9\xe6\x9c\x80\xe9\xab\x98\xe5\xb1\x82\xe8\xbf\x9b\xe4\xb8\x80\xe6\xad\xa5\xe9\x99\x8d\xe9\x87\x87\xe6\xa0\xb7\xef\xbc\x8c\xe4\xb9\x9f\xe5\xb0\xb1\xe6\x98\xafP6\n            # use max pool to get more levels on top of outputs\n            # (e.g., Faster R-CNN, Mask R-CNN)\n            if not self.add_extra_convs:\n                for i in range(self.num_outs - used_backbone_levels):\n                    outs.append(F.max_pool2d(outs[-1], 1, stride=2))\n            # add conv layers on top of original feature maps (RetinaNet)\n            else:\n                if self.extra_convs_on_inputs:\n                    orig = inputs[self.backbone_end_level - 1]\n                    outs.append(self.fpn_convs[used_backbone_levels](orig))\n                else:\n                    outs.append(self.fpn_convs[used_backbone_levels](outs[-1]))\n                for i in range(used_backbone_levels + 1, self.num_outs):\n                    if self.relu_before_extra_convs:\n                        outs.append(self.fpn_convs[i](F.relu(outs[-1])))\n                    else:\n                        outs.append(self.fpn_convs[i](outs[-1]))\n        return tuple(outs)\n"""
mmdet/models/necks/hrfpn.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.checkpoint import checkpoint\nfrom mmcv.cnn.weight_init import caffe2_xavier_init\n\nfrom ..utils import ConvModule\nfrom ..registry import NECKS\n\n\n@NECKS.register_module\nclass HRFPN(nn.Module):\n    """"""HRFPN (High Resolution Feature Pyrmamids)\n\n    arXiv: https://arxiv.org/abs/1904.04514\n\n    Args:\n        in_channels (list): number of channels for each branch.\n        out_channels (int): output channels of feature pyramids.\n        num_outs (int): number of output stages.\n        pooling_type (str): pooling for generating feature pyramids\n            from {MAX, AVG}.\n        conv_cfg (dict): dictionary to construct and config conv layer.\n        norm_cfg (dict): dictionary to construct and config norm layer.\n        with_cp  (bool): Use checkpoint or not. Using checkpoint will save some\n            memory while slowing down the training speed.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 num_outs=5,\n                 pooling_type=\'AVG\',\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 with_cp=False):\n        super(HRFPN, self).__init__()\n        assert isinstance(in_channels, list)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.num_ins = len(in_channels)\n        self.num_outs = num_outs\n        self.with_cp = with_cp\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n\n        self.reduction_conv = ConvModule(\n            sum(in_channels),\n            out_channels,\n            kernel_size=1,\n            conv_cfg=self.conv_cfg,\n            activation=None)\n\n        self.fpn_convs = nn.ModuleList()\n        for i in range(self.num_outs):\n            self.fpn_convs.append(\n                ConvModule(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    padding=1,\n                    conv_cfg=self.conv_cfg,\n                    activation=None))\n\n        if pooling_type == \'MAX\':\n            self.pooling = F.max_pool2d\n        else:\n            self.pooling = F.avg_pool2d\n\n    def init_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                caffe2_xavier_init(m)\n\n    def forward(self, inputs):\n        assert len(inputs) == self.num_ins\n        outs = [inputs[0]]\n        for i in range(1, self.num_ins):\n            outs.append(\n                F.interpolate(inputs[i], scale_factor=2**i, mode=\'bilinear\'))\n        out = torch.cat(outs, dim=1)\n        if out.requires_grad and self.with_cp:\n            out = checkpoint(self.reduction_conv, out)\n        else:\n            out = self.reduction_conv(out)\n        outs = [out]\n        for i in range(1, self.num_outs):\n            outs.append(self.pooling(out, kernel_size=2**i, stride=2**i))\n        outputs = []\n\n        for i in range(self.num_outs):\n            if outs[i].requires_grad and self.with_cp:\n                tmp_out = checkpoint(self.fpn_convs[i], outs[i])\n            else:\n                tmp_out = self.fpn_convs[i](outs[i])\n            outputs.append(tmp_out)\n        return tuple(outputs)\n'"
mmdet/models/plugins/__init__.py,0,"b""from .non_local import NonLocal2D\nfrom .generalized_attention import GeneralizedAttention\n\n__all__ = ['NonLocal2D', 'GeneralizedAttention']\n"""
mmdet/models/plugins/generalized_attention.py,26,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport math\nimport numpy as np\nfrom mmcv.cnn import kaiming_init\n\n\nclass GeneralizedAttention(nn.Module):\n    """"""GeneralizedAttention module.\n\n    See \'An Empirical Study of Spatial Attention Mechanisms in Deep Networks\'\n    (https://arxiv.org/abs/1711.07971) for details.\n\n    Args:\n        in_dim (int): Channels of the input feature map.\n        spatial_range (int): The spatial range.\n            -1 indicates no spatial range constraint.\n        num_heads (int): The head number of empirical_attention module.\n        position_embedding_dim (int): The position embedding dimension.\n        position_magnitude (int): A multiplier acting on coord difference.\n        kv_stride (int): The feature stride acting on key/value feature map.\n        q_stride (int): The feature stride acting on query feature map.\n        attention_type (str): A binary indicator string for indicating which\n            items in generalized empirical_attention module are used.\n            \'1000\' indicates \'query and key content\' (appr - appr) item,\n            \'0100\' indicates \'query content and relative position\'\n              (appr - position) item,\n            \'0010\' indicates \'key content only\' (bias - appr) item,\n            \'0001\' indicates \'relative position only\' (bias - position) item.\n    """"""\n\n    def __init__(self,\n                 in_dim,\n                 spatial_range=-1,\n                 num_heads=9,\n                 position_embedding_dim=-1,\n                 position_magnitude=1,\n                 kv_stride=2,\n                 q_stride=1,\n                 attention_type=\'1111\'):\n\n        super(GeneralizedAttention, self).__init__()\n\n        # hard range means local range for non-local operation\n        self.position_embedding_dim = (\n            position_embedding_dim if position_embedding_dim > 0 else in_dim)\n\n        self.position_magnitude = position_magnitude\n        self.num_heads = num_heads\n        self.channel_in = in_dim\n        self.spatial_range = spatial_range\n        self.kv_stride = kv_stride\n        self.q_stride = q_stride\n        self.attention_type = [bool(int(_)) for _ in attention_type]\n        self.qk_embed_dim = in_dim // num_heads\n        out_c = self.qk_embed_dim * num_heads\n\n        if self.attention_type[0] or self.attention_type[1]:\n            self.query_conv = nn.Conv2d(\n                in_channels=in_dim,\n                out_channels=out_c,\n                kernel_size=1,\n                bias=False)\n            self.query_conv.kaiming_init = True\n\n        if self.attention_type[0] or self.attention_type[2]:\n            self.key_conv = nn.Conv2d(\n                in_channels=in_dim,\n                out_channels=out_c,\n                kernel_size=1,\n                bias=False)\n            self.key_conv.kaiming_init = True\n\n        self.v_dim = in_dim // num_heads\n        self.value_conv = nn.Conv2d(\n            in_channels=in_dim,\n            out_channels=self.v_dim * num_heads,\n            kernel_size=1,\n            bias=False)\n        self.value_conv.kaiming_init = True\n\n        if self.attention_type[1] or self.attention_type[3]:\n            self.appr_geom_fc_x = nn.Linear(\n                self.position_embedding_dim // 2, out_c, bias=False)\n            self.appr_geom_fc_x.kaiming_init = True\n\n            self.appr_geom_fc_y = nn.Linear(\n                self.position_embedding_dim // 2, out_c, bias=False)\n            self.appr_geom_fc_y.kaiming_init = True\n\n        if self.attention_type[2]:\n            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)\n            appr_bias_value = -2 * stdv * torch.rand(out_c) + stdv\n            self.appr_bias = nn.Parameter(appr_bias_value)\n\n        if self.attention_type[3]:\n            stdv = 1.0 / math.sqrt(self.qk_embed_dim * 2)\n            geom_bias_value = -2 * stdv * torch.rand(out_c) + stdv\n            self.geom_bias = nn.Parameter(geom_bias_value)\n\n        self.proj_conv = nn.Conv2d(\n            in_channels=self.v_dim * num_heads,\n            out_channels=in_dim,\n            kernel_size=1,\n            bias=True)\n        self.proj_conv.kaiming_init = True\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        if self.spatial_range >= 0:\n            # only works when non local is after 3*3 conv\n            if in_dim == 256:\n                max_len = 84\n            elif in_dim == 512:\n                max_len = 42\n\n            max_len_kv = int((max_len - 1.0) / self.kv_stride + 1)\n            local_constraint_map = np.ones(\n                (max_len, max_len, max_len_kv, max_len_kv), dtype=np.int)\n            for iy in range(max_len):\n                for ix in range(max_len):\n                    local_constraint_map[iy, ix,\n                                         max((iy - self.spatial_range) //\n                                             self.kv_stride, 0):min(\n                                                 (iy + self.spatial_range +\n                                                  1) // self.kv_stride +\n                                                 1, max_len),\n                                         max((ix - self.spatial_range) //\n                                             self.kv_stride, 0):min(\n                                                 (ix + self.spatial_range +\n                                                  1) // self.kv_stride +\n                                                 1, max_len)] = 0\n\n            self.local_constraint_map = nn.Parameter(\n                torch.from_numpy(local_constraint_map).byte(),\n                requires_grad=False)\n\n        if self.q_stride > 1:\n            self.q_downsample = nn.AvgPool2d(\n                kernel_size=1, stride=self.q_stride)\n        else:\n            self.q_downsample = None\n\n        if self.kv_stride > 1:\n            self.kv_downsample = nn.AvgPool2d(\n                kernel_size=1, stride=self.kv_stride)\n        else:\n            self.kv_downsample = None\n\n        self.init_weights()\n\n    def get_position_embedding(self,\n                               h,\n                               w,\n                               h_kv,\n                               w_kv,\n                               q_stride,\n                               kv_stride,\n                               device,\n                               feat_dim,\n                               wave_length=1000):\n        h_idxs = torch.linspace(0, h - 1, h).cuda(device)\n        h_idxs = h_idxs.view((h, 1)) * q_stride\n\n        w_idxs = torch.linspace(0, w - 1, w).cuda(device)\n        w_idxs = w_idxs.view((w, 1)) * q_stride\n\n        h_kv_idxs = torch.linspace(0, h_kv - 1, h_kv).cuda(device)\n        h_kv_idxs = h_kv_idxs.view((h_kv, 1)) * kv_stride\n\n        w_kv_idxs = torch.linspace(0, w_kv - 1, w_kv).cuda(device)\n        w_kv_idxs = w_kv_idxs.view((w_kv, 1)) * kv_stride\n\n        # (h, h_kv, 1)\n        h_diff = h_idxs.unsqueeze(1) - h_kv_idxs.unsqueeze(0)\n        h_diff *= self.position_magnitude\n\n        # (w, w_kv, 1)\n        w_diff = w_idxs.unsqueeze(1) - w_kv_idxs.unsqueeze(0)\n        w_diff *= self.position_magnitude\n\n        feat_range = torch.arange(0, feat_dim / 4).cuda(device)\n\n        dim_mat = torch.Tensor([wave_length]).cuda(device)\n        dim_mat = dim_mat**((4. / feat_dim) * feat_range)\n        dim_mat = dim_mat.view((1, 1, -1))\n\n        embedding_x = torch.cat(\n            ((w_diff / dim_mat).sin(), (w_diff / dim_mat).cos()), dim=2)\n\n        embedding_y = torch.cat(\n            ((h_diff / dim_mat).sin(), (h_diff / dim_mat).cos()), dim=2)\n\n        return embedding_x, embedding_y\n\n    def forward(self, x_input):\n        num_heads = self.num_heads\n\n        # use empirical_attention\n        if self.q_downsample is not None:\n            x_q = self.q_downsample(x_input)\n        else:\n            x_q = x_input\n        n, _, h, w = x_q.shape\n\n        if self.kv_downsample is not None:\n            x_kv = self.kv_downsample(x_input)\n        else:\n            x_kv = x_input\n        _, _, h_kv, w_kv = x_kv.shape\n\n        if self.attention_type[0] or self.attention_type[1]:\n            proj_query = self.query_conv(x_q).view(\n                (n, num_heads, self.qk_embed_dim, h * w))\n            proj_query = proj_query.permute(0, 1, 3, 2)\n\n        if self.attention_type[0] or self.attention_type[2]:\n            proj_key = self.key_conv(x_kv).view(\n                (n, num_heads, self.qk_embed_dim, h_kv * w_kv))\n\n        if self.attention_type[1] or self.attention_type[3]:\n            position_embed_x, position_embed_y = self.get_position_embedding(\n                h, w, h_kv, w_kv, self.q_stride, self.kv_stride,\n                x_input.device, self.position_embedding_dim)\n            # (n, num_heads, w, w_kv, dim)\n            position_feat_x = self.appr_geom_fc_x(position_embed_x).\\\n                view(1, w, w_kv, num_heads, self.qk_embed_dim).\\\n                permute(0, 3, 1, 2, 4).\\\n                repeat(n, 1, 1, 1, 1)\n\n            # (n, num_heads, h, h_kv, dim)\n            position_feat_y = self.appr_geom_fc_y(position_embed_y).\\\n                view(1, h, h_kv, num_heads, self.qk_embed_dim).\\\n                permute(0, 3, 1, 2, 4).\\\n                repeat(n, 1, 1, 1, 1)\n\n            position_feat_x /= math.sqrt(2)\n            position_feat_y /= math.sqrt(2)\n\n        # accelerate for saliency only\n        if (np.sum(self.attention_type) == 1) and self.attention_type[2]:\n            appr_bias = self.appr_bias.\\\n                view(1, num_heads, 1, self.qk_embed_dim).\\\n                repeat(n, 1, 1, 1)\n\n            energy = torch.matmul(appr_bias, proj_key).\\\n                view(n, num_heads, 1, h_kv * w_kv)\n\n            h = 1\n            w = 1\n        else:\n            # (n, num_heads, h*w, h_kv*w_kv), query before key, 540mb for\n            if not self.attention_type[0]:\n                energy = torch.zeros(\n                    n,\n                    num_heads,\n                    h,\n                    w,\n                    h_kv,\n                    w_kv,\n                    dtype=x_input.dtype,\n                    device=x_input.device)\n\n            # attention_type[0]: appr - appr\n            # attention_type[1]: appr - position\n            # attention_type[2]: bias - appr\n            # attention_type[3]: bias - position\n            if self.attention_type[0] or self.attention_type[2]:\n                if self.attention_type[0] and self.attention_type[2]:\n                    appr_bias = self.appr_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim)\n                    energy = torch.matmul(proj_query + appr_bias, proj_key).\\\n                        view(n, num_heads, h, w, h_kv, w_kv)\n\n                elif self.attention_type[0]:\n                    energy = torch.matmul(proj_query, proj_key).\\\n                        view(n, num_heads, h, w, h_kv, w_kv)\n\n                elif self.attention_type[2]:\n                    appr_bias = self.appr_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim).\\\n                        repeat(n, 1, 1, 1)\n\n                    energy += torch.matmul(appr_bias, proj_key).\\\n                        view(n, num_heads, 1, 1, h_kv, w_kv)\n\n            if self.attention_type[1] or self.attention_type[3]:\n                if self.attention_type[1] and self.attention_type[3]:\n                    geom_bias = self.geom_bias.\\\n                        view(1, num_heads, 1, self.qk_embed_dim)\n\n                    proj_query_reshape = (proj_query + geom_bias).\\\n                        view(n, num_heads, h, w, self.qk_embed_dim)\n\n                    energy_x = torch.matmul(\n                        proj_query_reshape.permute(0, 1, 3, 2, 4),\n                        position_feat_x.permute(0, 1, 2, 4, 3))\n                    energy_x = energy_x.\\\n                        permute(0, 1, 3, 2, 4).unsqueeze(4)\n\n                    energy_y = torch.matmul(\n                        proj_query_reshape,\n                        position_feat_y.permute(0, 1, 2, 4, 3))\n                    energy_y = energy_y.unsqueeze(5)\n\n                    energy += energy_x + energy_y\n\n                elif self.attention_type[1]:\n                    proj_query_reshape = proj_query.\\\n                        view(n, num_heads, h, w, self.qk_embed_dim)\n                    proj_query_reshape = proj_query_reshape.\\\n                        permute(0, 1, 3, 2, 4)\n                    position_feat_x_reshape = position_feat_x.\\\n                        permute(0, 1, 2, 4, 3)\n                    position_feat_y_reshape = position_feat_y.\\\n                        permute(0, 1, 2, 4, 3)\n\n                    energy_x = torch.matmul(proj_query_reshape,\n                                            position_feat_x_reshape)\n                    energy_x = energy_x.permute(0, 1, 3, 2, 4).unsqueeze(4)\n\n                    energy_y = torch.matmul(proj_query_reshape,\n                                            position_feat_y_reshape)\n                    energy_y = energy_y.unsqueeze(5)\n\n                    energy += energy_x + energy_y\n\n                elif self.attention_type[3]:\n                    geom_bias = self.geom_bias.\\\n                        view(1, num_heads, self.qk_embed_dim, 1).\\\n                        repeat(n, 1, 1, 1)\n\n                    position_feat_x_reshape = position_feat_x.\\\n                        view(n, num_heads, w*w_kv, self.qk_embed_dim)\n\n                    position_feat_y_reshape = position_feat_y.\\\n                        view(n, num_heads, h * h_kv, self.qk_embed_dim)\n\n                    energy_x = torch.matmul(position_feat_x_reshape, geom_bias)\n                    energy_x = energy_x.view(n, num_heads, 1, w, 1, w_kv)\n\n                    energy_y = torch.matmul(position_feat_y_reshape, geom_bias)\n                    energy_y = energy_y.view(n, num_heads, h, 1, h_kv, 1)\n\n                    energy += energy_x + energy_y\n\n            energy = energy.view(n, num_heads, h * w, h_kv * w_kv)\n\n        if self.spatial_range >= 0:\n            cur_local_constraint_map = \\\n                self.local_constraint_map[:h, :w, :h_kv, :w_kv].\\\n                contiguous().\\\n                view(1, 1, h*w, h_kv*w_kv)\n\n            energy = energy.masked_fill_(cur_local_constraint_map,\n                                         float(\'-inf\'))\n\n        attention = F.softmax(energy, 3)\n\n        proj_value = self.value_conv(x_kv)\n        proj_value_reshape = proj_value.\\\n            view((n, num_heads, self.v_dim, h_kv * w_kv)).\\\n            permute(0, 1, 3, 2)\n\n        out = torch.matmul(attention, proj_value_reshape).\\\n            permute(0, 1, 3, 2).\\\n            contiguous().\\\n            view(n, self.v_dim * self.num_heads, h, w)\n\n        out = self.proj_conv(out)\n        out = self.gamma * out + x_input\n        return out\n\n    def init_weights(self):\n        for m in self.modules():\n            if hasattr(m, \'kaiming_init\') and m.kaiming_init:\n                kaiming_init(\n                    m,\n                    mode=\'fan_in\',\n                    nonlinearity=\'leaky_relu\',\n                    bias=0,\n                    distribution=\'uniform\',\n                    a=1)\n'"
mmdet/models/plugins/non_local.py,4,"b'import torch\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, normal_init\n\nfrom ..utils import ConvModule\n\n\nclass NonLocal2D(nn.Module):\n    """"""Non-local module.\n\n    See https://arxiv.org/abs/1711.07971 for details.\n\n    Args:\n        in_channels (int): Channels of the input feature map.\n        reduction (int): Channel reduction ratio.\n        use_scale (bool): Whether to scale pairwise_weight by 1/inter_channels.\n        conv_cfg (dict): The config dict for convolution layers.\n            (only applicable to conv_out)\n        norm_cfg (dict): The config dict for normalization layers.\n            (only applicable to conv_out)\n        mode (str): Options are `embedded_gaussian` and `dot_product`.\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 reduction=2,\n                 use_scale=True,\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 mode=\'embedded_gaussian\'):\n        super(NonLocal2D, self).__init__()\n        self.in_channels = in_channels\n        self.reduction = reduction\n        self.use_scale = use_scale\n        self.inter_channels = in_channels // reduction\n        self.mode = mode\n        assert mode in [\'embedded_gaussian\', \'dot_product\']\n\n        # g, theta, phi are actually `nn.Conv2d`. Here we use ConvModule for\n        # potential usage.\n        self.g = ConvModule(\n            self.in_channels,\n            self.inter_channels,\n            kernel_size=1,\n            activation=None)\n        self.theta = ConvModule(\n            self.in_channels,\n            self.inter_channels,\n            kernel_size=1,\n            activation=None)\n        self.phi = ConvModule(\n            self.in_channels,\n            self.inter_channels,\n            kernel_size=1,\n            activation=None)\n        self.conv_out = ConvModule(\n            self.inter_channels,\n            self.in_channels,\n            kernel_size=1,\n            conv_cfg=conv_cfg,\n            norm_cfg=norm_cfg,\n            activation=None)\n\n        self.init_weights()\n\n    def init_weights(self, std=0.01, zeros_init=True):\n        for m in [self.g, self.theta, self.phi]:\n            normal_init(m.conv, std=std)\n        if zeros_init:\n            constant_init(self.conv_out.conv, 0)\n        else:\n            normal_init(self.conv_out.conv, std=std)\n\n    def embedded_gaussian(self, theta_x, phi_x):\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = torch.matmul(theta_x, phi_x)\n        if self.use_scale:\n            # theta_x.shape[-1] is `self.inter_channels`\n            pairwise_weight /= theta_x.shape[-1]**-0.5\n        pairwise_weight = pairwise_weight.softmax(dim=-1)\n        return pairwise_weight\n\n    def dot_product(self, theta_x, phi_x):\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = torch.matmul(theta_x, phi_x)\n        pairwise_weight /= pairwise_weight.shape[-1]\n        return pairwise_weight\n\n    def forward(self, x):\n        n, _, h, w = x.shape\n\n        # g_x: [N, HxW, C]\n        g_x = self.g(x).view(n, self.inter_channels, -1)\n        g_x = g_x.permute(0, 2, 1)\n\n        # theta_x: [N, HxW, C]\n        theta_x = self.theta(x).view(n, self.inter_channels, -1)\n        theta_x = theta_x.permute(0, 2, 1)\n\n        # phi_x: [N, C, HxW]\n        phi_x = self.phi(x).view(n, self.inter_channels, -1)\n\n        pairwise_func = getattr(self, self.mode)\n        # pairwise_weight: [N, HxW, HxW]\n        pairwise_weight = pairwise_func(theta_x, phi_x)\n\n        # y: [N, HxW, C]\n        y = torch.matmul(pairwise_weight, g_x)\n        # y: [N, C, H, W]\n        y = y.permute(0, 2, 1).reshape(n, self.inter_channels, h, w)\n\n        output = x + self.conv_out(y)\n\n        return output\n'"
mmdet/models/roi_extractors/__init__.py,0,"b""from .single_level import SingleRoIExtractor\n\n__all__ = ['SingleRoIExtractor']\n"""
mmdet/models/roi_extractors/single_level.py,4,"b'from __future__ import division\n\nimport torch\nimport torch.nn as nn\n\nfrom mmdet import ops  # \xe7\x94\xa8\xe4\xba\x8e\xe5\xaf\xbc\xe5\x85\xa5RoIAlign\xef\xbc\x8cNMS\xe7\xad\x89\nfrom mmdet.core import force_fp32\nfrom ..registry import ROI_EXTRACTORS\n\n# \xe5\x9c\xa8registry\xe6\xb3\xa8\xe5\x86\x8c\xe5\xb9\xb6\xe9\x80\x9a\xe8\xbf\x87\xe4\xbf\xae\xe9\xa5\xb0\xe5\x99\xa8\xe6\xb7\xbb\xe5\x8a\xa0\xe8\xaf\xa5\xe5\xa4\x84\xe7\x9a\x84\xe5\xad\x97\xe5\x85\xb8\xe5\xb1\x9e\xe6\x80\xa7\n@ROI_EXTRACTORS.register_module\nclass SingleRoIExtractor(nn.Module):\n    """"""Extract RoI features from a single level feature map.\n\n    If there are mulitple input feature levels, each RoI is mapped to a level\n    according to its scale.\n\n    Args:\n        roi_layer (dict): Specify RoI layer type and arguments.\n        out_channels (int): Output channels of RoI layers.\n        featmap_strides (int): Strides of input feature maps.\n        finest_scale (int): Scale threshold of mapping to level 0.\n    """"""\n\n    def __init__(self,\n                 roi_layer,\n                 out_channels,\n                 featmap_strides,\n                 finest_scale=56):\n        super(SingleRoIExtractor, self).__init__()\n        self.roi_layers = self.build_roi_layers(roi_layer, featmap_strides)\n        self.out_channels = out_channels\n        self.featmap_strides = featmap_strides\n        self.finest_scale = finest_scale\n        self.fp16_enabled = False\n\n    @property\n    def num_inputs(self):\n        """"""int: Input feature map levels.""""""\n        return len(self.featmap_strides)\n\n    def init_weights(self):\n        pass\n\n    def build_roi_layers(self, layer_cfg, featmap_strides):\n        cfg = layer_cfg.copy()\n        layer_type = cfg.pop(\'type\')\n        assert hasattr(ops, layer_type)\n        # \xe4\xb8\x8b\xe9\x9d\xa2\xe5\xaf\xbc\xe5\x85\xa5\xe4\xba\x86ops\xe7\x9a\x84__init__.py,getattr(ops, layer_type)\xe5\x8f\x96\xe5\x87\xba\xe5\x85\xb6\xe4\xb8\xad\xe7\x9a\x84RoIAlign\xe7\xb1\xbb\xef\xbc\x8c\xe5\xbe\x97\xe5\x88\xb0\xe7\xbb\x93\xe6\x9e\x9c\xef\xbc\x9a\n        # <class \'mmdet.ops.roi_align.modules.roi_align.RoIAlign\'>\n        layer_cls = getattr(ops, layer_type)\n        # \xe7\x94\xb1\xe4\xba\x8e\xe6\x9c\x89\xe9\x94\xae\xe5\x80\xbc\xe5\xaf\xb9\xef\xbc\x8c\xe4\xb8\x8d\xe7\x94\xa8\xe5\x9c\xa8\xe6\x84\x8f\xe9\xa1\xba\xe5\xba\x8f\n        roi_layers = nn.ModuleList(\n            [layer_cls(spatial_scale=1 / s, **cfg) for s in featmap_strides])\n        return roi_layers\n\n    # \xe5\xbe\x97\xe5\x88\xb0\xe7\x9a\x84target_lvls\xe6\x98\xaf\xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x80\xe4\xb8\xaaproposal\xe7\x9a\x84level\n    def map_roi_levels(self, rois, num_levels):\n        """"""Map rois to corresponding feature levels by scales.\n\n        - scale < finest_scale * 2: level 0\n        - finest_scale * 2 <= scale < finest_scale * 4: level 1\n        - finest_scale * 4 <= scale < finest_scale * 8: level 2\n        - scale >= finest_scale * 8: level 3\n\n        Args:\n            rois (Tensor): Input RoIs, shape (k, 5).\n            num_levels (int): Total level number.\n\n        Returns:\n            Tensor: Level index (0-based) of each RoI, shape (k, )\n        """"""\n        scale = torch.sqrt(\n            (rois[:, 3] - rois[:, 1] + 1) * (rois[:, 4] - rois[:, 2] + 1))\n        target_lvls = torch.floor(torch.log2(scale / self.finest_scale + 1e-6))\n        target_lvls = target_lvls.clamp(min=0, max=num_levels - 1).long()\n        return target_lvls\n\n    @force_fp32(apply_to=(\'feats\',), out_fp16=True)\n    def forward(self, feats, rois):\n        if len(feats) == 1:\n            return self.roi_layers[0](feats[0], rois)\n\n        out_size = self.roi_layers[0].out_size\n        num_levels = len(feats) # \xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe7\x9a\x84\xe5\xbc\xa0\xe6\x95\xb0\xe5\x86\xb3\xe5\xae\x9a\xe4\xba\x86\xe6\x9c\x89\xe5\x87\xa0\xe4\xb8\xaalevel\xe7\x9a\x84\xe8\xbe\x93\xe5\x87\xba\n        # \xe4\xb8\x80\xe7\xbb\xb4\xe5\x90\x91\xe9\x87\x8f\xef\xbc\x8c\xe6\xaf\x8f\xe4\xb8\xaa\xe5\x85\x83\xe7\xb4\xa0\xe5\xaf\xb9\xe5\xba\x94\xe4\xb8\x80\xe4\xb8\xaaproposal\xe7\x9a\x84level\xef\xbc\x9b\xe5\xb0\x86rois\xe7\xbc\xa9\xe6\x94\xbe\xe5\x88\xb0\xe7\x89\xb9\xe5\xbe\x81\xe5\x9b\xbe\xe4\xb8\x8a\n        target_lvls = self.map_roi_levels(rois, num_levels)\n        # roi_feats\xe4\xb8\xba torch.Size([2000, 256, 7, 7])\n        roi_feats = feats[0].new_zeros(rois.size()[0], self.out_channels,\n                                       out_size, out_size)\n        for i in range(num_levels):\n            inds = target_lvls == i\n            if inds.any():\n                rois_ = rois[inds, :]\n                # \xe9\x9a\x90\xe5\xbc\x8f\xe5\x89\x8d\xe5\x90\x91\xe4\xbc\xa0\xe6\x92\xad\n                roi_feats_t = self.roi_layers[i](feats[i], rois_)\n                roi_feats[inds] += roi_feats_t\n        return roi_feats\n'"
mmdet/models/shared_heads/__init__.py,0,"b""from .res_layer import ResLayer\n\n__all__ = ['ResLayer']\n"""
mmdet/models/shared_heads/res_layer.py,1,"b""import logging\n\nimport torch.nn as nn\nfrom mmcv.cnn import constant_init, kaiming_init\nfrom mmcv.runner import load_checkpoint\n\nfrom mmdet.core import auto_fp16\nfrom ..backbones import ResNet, make_res_layer\nfrom ..registry import SHARED_HEADS\n\n\n@SHARED_HEADS.register_module\nclass ResLayer(nn.Module):\n\n    def __init__(self,\n                 depth,\n                 stage=3,\n                 stride=2,\n                 dilation=1,\n                 style='pytorch',\n                 norm_cfg=dict(type='BN', requires_grad=True),\n                 norm_eval=True,\n                 with_cp=False,\n                 dcn=None):\n        super(ResLayer, self).__init__()\n        self.norm_eval = norm_eval\n        self.norm_cfg = norm_cfg\n        self.stage = stage\n        self.fp16_enabled = False\n        block, stage_blocks = ResNet.arch_settings[depth]\n        stage_block = stage_blocks[stage]\n        planes = 64 * 2**stage\n        inplanes = 64 * 2**(stage - 1) * block.expansion\n\n        res_layer = make_res_layer(\n            block,\n            inplanes,\n            planes,\n            stage_block,\n            stride=stride,\n            dilation=dilation,\n            style=style,\n            with_cp=with_cp,\n            norm_cfg=self.norm_cfg,\n            dcn=dcn)\n        self.add_module('layer{}'.format(stage + 1), res_layer)\n\n    def init_weights(self, pretrained=None):\n        if isinstance(pretrained, str):\n            logger = logging.getLogger()\n            load_checkpoint(self, pretrained, strict=False, logger=logger)\n        elif pretrained is None:\n            for m in self.modules():\n                if isinstance(m, nn.Conv2d):\n                    kaiming_init(m)\n                elif isinstance(m, nn.BatchNorm2d):\n                    constant_init(m, 1)\n        else:\n            raise TypeError('pretrained must be a str or None')\n\n    @auto_fp16()\n    def forward(self, x):\n        res_layer = getattr(self, 'layer{}'.format(self.stage + 1))\n        out = res_layer(x)\n        return out\n\n    def train(self, mode=True):\n        super(ResLayer, self).train(mode)\n        if self.norm_eval:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    m.eval()\n"""
mmdet/models/utils/__init__.py,0,"b""from .conv_ws import conv_ws_2d, ConvWS2d\nfrom .conv_module import build_conv_layer, ConvModule\nfrom .norm import build_norm_layer\nfrom .scale import Scale\nfrom .weight_init import (xavier_init, normal_init, uniform_init, kaiming_init,\n                          bias_init_with_prob)\n\n__all__ = [\n    'conv_ws_2d', 'ConvWS2d', 'build_conv_layer', 'ConvModule',\n    'build_norm_layer', 'xavier_init', 'normal_init', 'uniform_init',\n    'kaiming_init', 'bias_init_with_prob', 'Scale'\n]\n"""
mmdet/models/utils/conv_module.py,1,"b'import warnings\n\nimport torch.nn as nn\nfrom mmcv.cnn import kaiming_init, constant_init\n\nfrom .conv_ws import ConvWS2d\nfrom .norm import build_norm_layer\n\nconv_cfg = {\n    \'Conv\': nn.Conv2d,\n    \'ConvWS\': ConvWS2d,\n    # TODO: octave conv\n}\n\n\ndef build_conv_layer(cfg, *args, **kwargs):\n    """""" Build convolution layer\n\n    Args:\n        cfg (None or dict): cfg should contain:\n            type (str): identify conv layer type.\n            layer args: args needed to instantiate a conv layer.\n\n    Returns:\n        layer (nn.Module): created conv layer\n    """"""\n    if cfg is None:\n        cfg_ = dict(type=\'Conv\')\n    else:\n        assert isinstance(cfg, dict) and \'type\' in cfg\n        cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in conv_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        conv_layer = conv_cfg[layer_type]\n\n    layer = conv_layer(*args, **kwargs, **cfg_)\n\n    return layer\n\n\nclass ConvModule(nn.Module):\n    """"""Conv-Norm-Activation block.\n\n    Args:\n        in_channels (int): Same as nn.Conv2d.\n        out_channels (int): Same as nn.Conv2d.\n        kernel_size (int or tuple[int]): Same as nn.Conv2d.\n        stride (int or tuple[int]): Same as nn.Conv2d.\n        padding (int or tuple[int]): Same as nn.Conv2d.\n        dilation (int or tuple[int]): Same as nn.Conv2d.\n        groups (int): Same as nn.Conv2d.\n        bias (bool or str): If specified as `auto`, it will be decided by the\n            norm_cfg. Bias will be set as True if norm_cfg is None, otherwise\n            False.\n        conv_cfg (dict): Config dict for convolution layer.\n        norm_cfg (dict): Config dict for normalization layer.\n        activation (str or None): Activation type, ""ReLU"" by default.\n        inplace (bool): Whether to use inplace mode for activation.\n        activate_last (bool): Whether to apply the activation layer in the\n            last. (Do not use this flag since the behavior and api may be\n            changed in the future.)\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=\'auto\',\n                 conv_cfg=None,\n                 norm_cfg=None,\n                 activation=\'relu\',\n                 inplace=True,\n                 activate_last=True):\n        super(ConvModule, self).__init__()\n        assert conv_cfg is None or isinstance(conv_cfg, dict)\n        assert norm_cfg is None or isinstance(norm_cfg, dict)\n        self.conv_cfg = conv_cfg\n        self.norm_cfg = norm_cfg\n        self.activation = activation\n        self.inplace = inplace\n        self.activate_last = activate_last\n\n        self.with_norm = norm_cfg is not None\n        self.with_activatation = activation is not None\n        # if the conv layer is before a norm layer, bias is unnecessary.\n        if bias == \'auto\':\n            bias = False if self.with_norm else True\n        self.with_bias = bias\n\n        if self.with_norm and self.with_bias:\n            warnings.warn(\'ConvModule has norm and bias at the same time\')\n\n        # build convolution layer\n        self.conv = build_conv_layer(\n            conv_cfg,\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        # export the attributes of self.conv to a higher level for convenience\n        self.in_channels = self.conv.in_channels\n        self.out_channels = self.conv.out_channels\n        self.kernel_size = self.conv.kernel_size\n        self.stride = self.conv.stride\n        self.padding = self.conv.padding\n        self.dilation = self.conv.dilation\n        self.transposed = self.conv.transposed\n        self.output_padding = self.conv.output_padding\n        self.groups = self.conv.groups\n\n        # build normalization layers\n        if self.with_norm:\n            norm_channels = out_channels if self.activate_last else in_channels\n            self.norm_name, norm = build_norm_layer(norm_cfg, norm_channels)\n            self.add_module(self.norm_name, norm)\n\n        # build activation layer\n        if self.with_activatation:\n            if self.activation not in [\'relu\']:\n                raise ValueError(\'{} is currently not supported.\'.format(\n                    self.activation))\n            if self.activation == \'relu\':\n                self.activate = nn.ReLU(inplace=inplace)\n\n        # Use msra init by default\n        self.init_weights()\n\n    @property\n    def norm(self):\n        return getattr(self, self.norm_name)\n\n    def init_weights(self):\n        nonlinearity = \'relu\' if self.activation is None else self.activation\n        kaiming_init(self.conv, nonlinearity=nonlinearity)\n        if self.with_norm:\n            constant_init(self.norm, 1, bias=0)\n\n    def forward(self, x, activate=True, norm=True):\n        if self.activate_last:\n            x = self.conv(x)\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n        else:\n            # WARN: this may be removed or modified\n            if norm and self.with_norm:\n                x = self.norm(x)\n            if activate and self.with_activatation:\n                x = self.activate(x)\n            x = self.conv(x)\n        return x\n'"
mmdet/models/utils/conv_ws.py,2,"b'import torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_ws_2d(input,\n               weight,\n               bias=None,\n               stride=1,\n               padding=0,\n               dilation=1,\n               groups=1,\n               eps=1e-5):\n    c_in = weight.size(0)\n    weight_flat = weight.view(c_in, -1)\n    mean = weight_flat.mean(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    std = weight_flat.std(dim=1, keepdim=True).view(c_in, 1, 1, 1)\n    weight = (weight - mean) / (std + eps)\n    return F.conv2d(input, weight, bias, stride, padding, dilation, groups)\n\n\nclass ConvWS2d(nn.Conv2d):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 dilation=1,\n                 groups=1,\n                 bias=True,\n                 eps=1e-5):\n        super(ConvWS2d, self).__init__(\n            in_channels,\n            out_channels,\n            kernel_size,\n            stride=stride,\n            padding=padding,\n            dilation=dilation,\n            groups=groups,\n            bias=bias)\n        self.eps = eps\n\n    def forward(self, x):\n        return conv_ws_2d(x, self.weight, self.bias, self.stride, self.padding,\n                          self.dilation, self.groups, self.eps)\n'"
mmdet/models/utils/norm.py,1,"b'import torch.nn as nn\n\nnorm_cfg = {\n    # format: layer_type: (abbreviation, module)\n    \'BN\': (\'bn\', nn.BatchNorm2d),\n    \'SyncBN\': (\'bn\', nn.SyncBatchNorm),\n    \'GN\': (\'gn\', nn.GroupNorm),\n    # and potentially \'SN\'\n}\n\n\ndef build_norm_layer(cfg, num_features, postfix=\'\'):\n    """""" Build normalization layer\n\n    Args:\n        cfg (dict): cfg should contain:\n            type (str): identify norm layer type.\n            layer args: args needed to instantiate a norm layer.\n            requires_grad (bool): [optional] whether stop gradient updates\n        num_features (int): number of channels from input.\n        postfix (int, str): appended into norm abbreviation to\n            create named layer.\n\n    Returns:\n        name (str): abbreviation + postfix\n        layer (nn.Module): created norm layer\n    """"""\n    assert isinstance(cfg, dict) and \'type\' in cfg\n    cfg_ = cfg.copy()\n\n    layer_type = cfg_.pop(\'type\')\n    if layer_type not in norm_cfg:\n        raise KeyError(\'Unrecognized norm type {}\'.format(layer_type))\n    else:\n        abbr, norm_layer = norm_cfg[layer_type]\n        if norm_layer is None:\n            raise NotImplementedError\n\n    assert isinstance(postfix, (int, str))\n    name = abbr + str(postfix)\n\n    requires_grad = cfg_.pop(\'requires_grad\', True)\n    cfg_.setdefault(\'eps\', 1e-5)\n    if layer_type != \'GN\':\n        layer = norm_layer(num_features, **cfg_)\n        if layer_type == \'SyncBN\':\n            layer._specify_ddp_gpu_num(1)\n    else:\n        assert \'num_groups\' in cfg_\n        layer = norm_layer(num_channels=num_features, **cfg_)\n\n    for param in layer.parameters():\n        param.requires_grad = requires_grad\n\n    return name, layer\n'"
mmdet/models/utils/scale.py,2,"b'import torch\nimport torch.nn as nn\n\n\nclass Scale(nn.Module):\n\n    def __init__(self, scale=1.0):\n        super(Scale, self).__init__()\n        self.scale = nn.Parameter(torch.tensor(scale, dtype=torch.float))\n\n    def forward(self, x):\n        return x * self.scale\n'"
mmdet/models/utils/weight_init.py,1,"b'import numpy as np\nimport torch.nn as nn\n\n\ndef xavier_init(module, gain=1, bias=0, distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.xavier_uniform_(module.weight, gain=gain)\n    else:\n        nn.init.xavier_normal_(module.weight, gain=gain)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef normal_init(module, mean=0, std=1, bias=0):\n    nn.init.normal_(module.weight, mean, std)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef uniform_init(module, a=0, b=1, bias=0):\n    nn.init.uniform_(module.weight, a, b)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef kaiming_init(module,\n                 mode=\'fan_out\',\n                 nonlinearity=\'relu\',\n                 bias=0,\n                 distribution=\'normal\'):\n    assert distribution in [\'uniform\', \'normal\']\n    if distribution == \'uniform\':\n        nn.init.kaiming_uniform_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    else:\n        nn.init.kaiming_normal_(\n            module.weight, mode=mode, nonlinearity=nonlinearity)\n    if hasattr(module, \'bias\'):\n        nn.init.constant_(module.bias, bias)\n\n\ndef bias_init_with_prob(prior_prob):\n    """""" initialize conv/fc bias value according to giving probablity""""""\n    bias_init = float(-np.log((1 - prior_prob) / prior_prob))\n    return bias_init\n'"
mmdet/core/bbox/assigners/__init__.py,0,"b""from .base_assigner import BaseAssigner\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom .approx_max_iou_assigner import ApproxMaxIoUAssigner\nfrom .assign_result import AssignResult\n\n__all__ = [\n    'BaseAssigner', 'MaxIoUAssigner', 'ApproxMaxIoUAssigner', 'AssignResult'\n]\n"""
mmdet/core/bbox/assigners/approx_max_iou_assigner.py,2,"b'import torch\n\nfrom .max_iou_assigner import MaxIoUAssigner\nfrom ..geometry import bbox_overlaps\n\n\nclass ApproxMaxIoUAssigner(MaxIoUAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `-1`, `0`, or a positive integer\n    indicating the ground truth index.\n\n    - -1: don\'t care\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n\n    def assign(self,\n               approxs,\n               squares,\n               approxs_per_octave,\n               gt_bboxes,\n               gt_bboxes_ignore=None,\n               gt_labels=None):\n        """"""Assign gt to approxs.\n\n        This method assign a gt bbox to each group of approxs (bboxes),\n        each group of approxs is represent by a base approx (bbox) and\n        will be assigned with -1, 0, or a positive number.\n        -1 means don\'t care, 0 means negative sample,\n        positive number is the index (1-based) of assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to -1\n        2. use the max IoU of each group of approxs to assign\n        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            approxs (Tensor): Bounding boxes to be assigned,\n        shape(approxs_per_octave*n, 4).\n            squares (Tensor): Base Bounding boxes to be assigned,\n        shape(n, 4).\n            approxs_per_octave (int): number of approxs per octave\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n\n        if squares.shape[0] == 0 or gt_bboxes.shape[0] == 0:\n            raise ValueError(\'No gt or approxs\')\n        num_squares = squares.size(0)\n        num_gts = gt_bboxes.size(0)\n        # re-organize anchors by approxs_per_octave x num_squares\n        approxs = torch.transpose(\n            approxs.view(num_squares, approxs_per_octave, 4), 0,\n            1).contiguous().view(-1, 4)\n        all_overlaps = bbox_overlaps(approxs, gt_bboxes)\n\n        overlaps, _ = all_overlaps.view(approxs_per_octave, num_squares,\n                                        num_gts).max(dim=0)\n        overlaps = torch.transpose(overlaps, 0, 1)\n\n        bboxes = squares[:, :4]\n\n        if (self.ignore_iof_thr > 0) and (gt_bboxes_ignore is not None) and (\n                gt_bboxes_ignore.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = bbox_overlaps(bboxes,\n                                                gt_bboxes_ignore,\n                                                mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = bbox_overlaps(gt_bboxes_ignore,\n                                                bboxes,\n                                                mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        return assign_result\n'"
mmdet/core/bbox/assigners/assign_result.py,5,"b'import torch\n\n\nclass AssignResult(object):\n\n    def __init__(self, num_gts, gt_inds, max_overlaps, labels=None):\n        self.num_gts = num_gts\n        self.gt_inds = gt_inds\n        self.max_overlaps = max_overlaps\n        self.labels = labels\n\n    def add_gt_(self, gt_labels):\n        self_inds = torch.arange(\n            1, len(gt_labels) + 1, dtype=torch.long, device=gt_labels.device)\n        self.gt_inds = torch.cat([self_inds, self.gt_inds])\n        self.max_overlaps = torch.cat(\n            [self.max_overlaps.new_ones(self.num_gts), self.max_overlaps])\n        if self.labels is not None:\n            self.labels = torch.cat([gt_labels, self.labels])\n'"
mmdet/core/bbox/assigners/base_assigner.py,0,"b'from abc import ABCMeta, abstractmethod\n\n\nclass BaseAssigner(metaclass=ABCMeta):\n\n    @abstractmethod\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        pass\n'"
mmdet/core/bbox/assigners/max_iou_assigner.py,2,"b'import torch\n\nfrom .base_assigner import BaseAssigner\nfrom .assign_result import AssignResult\nfrom ..geometry import bbox_overlaps\n\n\nclass MaxIoUAssigner(BaseAssigner):\n    """"""Assign a corresponding gt bbox or background to each bbox.\n\n    Each proposals will be assigned with `-1`, `0`, or a positive integer\n    indicating the ground truth index.\n\n    - -1: don\'t care\n    - 0: negative sample, no assigned gt\n    - positive integer: positive sample, index (1-based) of assigned gt\n\n    Args:\n        pos_iou_thr (float): IoU threshold for positive bboxes.\n        neg_iou_thr (float or tuple): IoU threshold for negative bboxes.\n        min_pos_iou (float): Minimum iou for a bbox to be considered as a\n            positive bbox. Positive samples can have smaller IoU than\n            pos_iou_thr due to the 4th step (assign max IoU sample to each gt).\n        gt_max_assign_all (bool): Whether to assign all bboxes with the same\n            highest overlap with some gt to that gt.\n        ignore_iof_thr (float): IoF threshold for ignoring bboxes (if\n            `gt_bboxes_ignore` is specified). Negative values mean not\n            ignoring any bboxes.\n        ignore_wrt_candidates (bool): Whether to compute the iof between\n            `bboxes` and `gt_bboxes_ignore`, or the contrary.\n    """"""\n\n    def __init__(self,\n                 pos_iou_thr,\n                 neg_iou_thr,\n                 min_pos_iou=.0,\n                 gt_max_assign_all=True,\n                 ignore_iof_thr=-1,\n                 ignore_wrt_candidates=True):\n        self.pos_iou_thr = pos_iou_thr\n        self.neg_iou_thr = neg_iou_thr\n        self.min_pos_iou = min_pos_iou\n        self.gt_max_assign_all = gt_max_assign_all\n        self.ignore_iof_thr = ignore_iof_thr\n        self.ignore_wrt_candidates = ignore_wrt_candidates\n\n    def assign(self, bboxes, gt_bboxes, gt_bboxes_ignore=None, gt_labels=None):\n        """"""Assign gt to bboxes.\n\n        This method assign a gt bbox to every bbox (proposal/anchor), each bbox\n        will be assigned with -1, 0, or a positive number. -1 means don\'t care,\n        0 means negative sample, positive number is the index (1-based) of\n        assigned gt.\n        The assignment is done in following steps, the order matters.\n\n        1. assign every bbox to -1\n        2. assign proposals whose iou with all gts < neg_iou_thr to 0\n        3. for each bbox, if the iou with its nearest gt >= pos_iou_thr,\n           assign it to that bbox\n        4. for each gt bbox, assign its nearest proposals (may be more than\n           one) to itself\n\n        Args:\n            bboxes (Tensor): Bounding boxes to be assigned, shape(n, 4).\n            gt_bboxes (Tensor): Groundtruth boxes, shape (k, 4).\n            gt_bboxes_ignore (Tensor, optional): Ground truth bboxes that are\n                labelled as `ignored`, e.g., crowd boxes in COCO.\n            gt_labels (Tensor, optional): Label of gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if bboxes.shape[0] == 0 or gt_bboxes.shape[0] == 0:\n            raise ValueError(\'No gt or bboxes\')\n        bboxes = bboxes[:, :4]\n        overlaps = bbox_overlaps(gt_bboxes, bboxes)\n\n        if (self.ignore_iof_thr > 0) and (gt_bboxes_ignore is not None) and (\n                gt_bboxes_ignore.numel() > 0):\n            if self.ignore_wrt_candidates:\n                ignore_overlaps = bbox_overlaps(\n                    bboxes, gt_bboxes_ignore, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=1)\n            else:\n                ignore_overlaps = bbox_overlaps(\n                    gt_bboxes_ignore, bboxes, mode=\'iof\')\n                ignore_max_overlaps, _ = ignore_overlaps.max(dim=0)\n            overlaps[:, ignore_max_overlaps > self.ignore_iof_thr] = -1\n\n        assign_result = self.assign_wrt_overlaps(overlaps, gt_labels)\n        return assign_result\n\n    def assign_wrt_overlaps(self, overlaps, gt_labels=None):\n        """"""Assign w.r.t. the overlaps of bboxes with gts.\n\n        Args:\n            overlaps (Tensor): Overlaps between k gt_bboxes and n bboxes,\n                shape(k, n).\n            gt_labels (Tensor, optional): Labels of k gt_bboxes, shape (k, ).\n\n        Returns:\n            :obj:`AssignResult`: The assign result.\n        """"""\n        if overlaps.numel() == 0:\n            raise ValueError(\'No gt or proposals\')\n\n        num_gts, num_bboxes = overlaps.size(0), overlaps.size(1)\n\n        # 1. assign -1 by default\n        assigned_gt_inds = overlaps.new_full(\n            (num_bboxes, ), -1, dtype=torch.long)\n\n        # for each anchor, which gt best overlaps with it\n        # for each anchor, the max iou of all gts\n        max_overlaps, argmax_overlaps = overlaps.max(dim=0)\n        # for each gt, which anchor best overlaps with it\n        # for each gt, the max iou of all proposals\n        gt_max_overlaps, gt_argmax_overlaps = overlaps.max(dim=1)\n\n        # 2. assign negative: below\n        if isinstance(self.neg_iou_thr, float):\n            assigned_gt_inds[(max_overlaps >= 0)\n                             & (max_overlaps < self.neg_iou_thr)] = 0\n        elif isinstance(self.neg_iou_thr, tuple):\n            assert len(self.neg_iou_thr) == 2\n            assigned_gt_inds[(max_overlaps >= self.neg_iou_thr[0])\n                             & (max_overlaps < self.neg_iou_thr[1])] = 0\n\n        # 3. assign positive: above positive IoU threshold\n        pos_inds = max_overlaps >= self.pos_iou_thr\n        assigned_gt_inds[pos_inds] = argmax_overlaps[pos_inds] + 1\n\n        # 4. assign fg: for each gt, proposals with highest IoU\n        for i in range(num_gts):\n            if gt_max_overlaps[i] >= self.min_pos_iou:\n                if self.gt_max_assign_all:\n                    max_iou_inds = overlaps[i, :] == gt_max_overlaps[i]\n                    assigned_gt_inds[max_iou_inds] = i + 1\n                else:\n                    assigned_gt_inds[gt_argmax_overlaps[i]] = i + 1\n\n        if gt_labels is not None:\n            assigned_labels = assigned_gt_inds.new_zeros((num_bboxes, ))\n            pos_inds = torch.nonzero(assigned_gt_inds > 0).squeeze()\n            if pos_inds.numel() > 0:\n                assigned_labels[pos_inds] = gt_labels[\n                    assigned_gt_inds[pos_inds] - 1]\n        else:\n            assigned_labels = None\n\n        return AssignResult(\n            num_gts, assigned_gt_inds, max_overlaps, labels=assigned_labels)\n'"
mmdet/core/bbox/samplers/__init__.py,0,"b""from .base_sampler import BaseSampler\nfrom .pseudo_sampler import PseudoSampler\nfrom .random_sampler import RandomSampler\nfrom .instance_balanced_pos_sampler import InstanceBalancedPosSampler\nfrom .iou_balanced_neg_sampler import IoUBalancedNegSampler\nfrom .combined_sampler import CombinedSampler\nfrom .ohem_sampler import OHEMSampler\nfrom .sampling_result import SamplingResult\n\n__all__ = [\n    'BaseSampler', 'PseudoSampler', 'RandomSampler',\n    'InstanceBalancedPosSampler', 'IoUBalancedNegSampler', 'CombinedSampler',\n    'OHEMSampler', 'SamplingResult'\n]\n"""
mmdet/core/bbox/samplers/base_sampler.py,4,"b'from abc import ABCMeta, abstractmethod\n\nimport torch\n\nfrom .sampling_result import SamplingResult\n\n\nclass BaseSampler(metaclass=ABCMeta):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        self.num = num\n        self.pos_fraction = pos_fraction\n        self.neg_pos_ub = neg_pos_ub\n        self.add_gt_as_proposals = add_gt_as_proposals\n        self.pos_sampler = self\n        self.neg_sampler = self\n\n    @abstractmethod\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pass\n\n    @abstractmethod\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        pass\n\n    def sample(self,\n               assign_result,\n               bboxes,\n               gt_bboxes,\n               gt_labels=None,\n               **kwargs):\n        """"""Sample positive and negative bboxes.\n\n        This is a simple implementation of bbox sampling given candidates,\n        assigning results and ground truth bboxes.\n\n        Args:\n            assign_result (:obj:`AssignResult`): Bbox assigning results.\n            bboxes (Tensor): Boxes to be sampled from.\n            gt_bboxes (Tensor): Ground truth bboxes.\n            gt_labels (Tensor, optional): Class labels of ground truth bboxes.\n\n        Returns:\n            :obj:`SamplingResult`: Sampling result.\n        """"""\n        bboxes = bboxes[:, :4]\n\n        gt_flags = bboxes.new_zeros((bboxes.shape[0], ), dtype=torch.uint8)\n        if self.add_gt_as_proposals:\n            bboxes = torch.cat([gt_bboxes, bboxes], dim=0)\n            assign_result.add_gt_(gt_labels)\n            gt_ones = bboxes.new_ones(gt_bboxes.shape[0], dtype=torch.uint8)\n            gt_flags = torch.cat([gt_ones, gt_flags])\n\n        num_expected_pos = int(self.num * self.pos_fraction)\n        pos_inds = self.pos_sampler._sample_pos(\n            assign_result, num_expected_pos, bboxes=bboxes, **kwargs)\n        # We found that sampled indices have duplicated items occasionally.\n        # (may be a bug of PyTorch)\n        pos_inds = pos_inds.unique()\n        num_sampled_pos = pos_inds.numel()\n        num_expected_neg = self.num - num_sampled_pos\n        if self.neg_pos_ub >= 0:\n            _pos = max(1, num_sampled_pos)\n            neg_upper_bound = int(self.neg_pos_ub * _pos)\n            if num_expected_neg > neg_upper_bound:\n                num_expected_neg = neg_upper_bound\n        neg_inds = self.neg_sampler._sample_neg(\n            assign_result, num_expected_neg, bboxes=bboxes, **kwargs)\n        neg_inds = neg_inds.unique()\n\n        return SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                              assign_result, gt_flags)\n'"
mmdet/core/bbox/samplers/combined_sampler.py,0,"b'from .base_sampler import BaseSampler\nfrom ..assign_sampling import build_sampler\n\n\nclass CombinedSampler(BaseSampler):\n\n    def __init__(self, pos_sampler, neg_sampler, **kwargs):\n        super(CombinedSampler, self).__init__(**kwargs)\n        self.pos_sampler = build_sampler(pos_sampler, **kwargs)\n        self.neg_sampler = build_sampler(neg_sampler, **kwargs)\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n'"
mmdet/core/bbox/samplers/instance_balanced_pos_sampler.py,5,"b'import numpy as np\nimport torch\n\nfrom .random_sampler import RandomSampler\n\n\nclass InstanceBalancedPosSampler(RandomSampler):\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            unique_gt_inds = assign_result.gt_inds[pos_inds].unique()\n            num_gts = len(unique_gt_inds)\n            num_per_gt = int(round(num_expected / float(num_gts)) + 1)\n            sampled_inds = []\n            for i in unique_gt_inds:\n                inds = torch.nonzero(assign_result.gt_inds == i.item())\n                if inds.numel() != 0:\n                    inds = inds.squeeze(1)\n                else:\n                    continue\n                if len(inds) > num_per_gt:\n                    inds = self.random_choice(inds, num_per_gt)\n                sampled_inds.append(inds)\n            sampled_inds = torch.cat(sampled_inds)\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(\n                    list(set(pos_inds.cpu()) - set(sampled_inds.cpu())))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                extra_inds = torch.from_numpy(extra_inds).to(\n                    assign_result.gt_inds.device).long()\n                sampled_inds = torch.cat([sampled_inds, extra_inds])\n            elif len(sampled_inds) > num_expected:\n                sampled_inds = self.random_choice(sampled_inds, num_expected)\n            return sampled_inds\n'"
mmdet/core/bbox/samplers/iou_balanced_neg_sampler.py,2,"b'import numpy as np\nimport torch\n\nfrom .random_sampler import RandomSampler\n\n\nclass IoUBalancedNegSampler(RandomSampler):\n    """"""IoU Balanced Sampling\n\n    arXiv: https://arxiv.org/pdf/1904.02701.pdf (CVPR 2019)\n\n    Sampling proposals according to their IoU. `floor_fraction` of needed RoIs\n    are sampled from proposals whose IoU are lower than `floor_thr` randomly.\n    The others are sampled from proposals whose IoU are higher than\n    `floor_thr`. These proposals are sampled from some bins evenly, which are\n    split by `num_bins` via IoU evenly.\n\n    Args:\n        num (int): number of proposals.\n        pos_fraction (float): fraction of positive proposals.\n        floor_thr (float): threshold (minimum) IoU for IoU balanced sampling,\n            set to -1 if all using IoU balanced sampling.\n        floor_fraction (float): sampling fraction of proposals under floor_thr.\n        num_bins (int): number of bins in IoU balanced sampling.\n    """"""\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 floor_thr=-1,\n                 floor_fraction=0,\n                 num_bins=3,\n                 **kwargs):\n        super(IoUBalancedNegSampler, self).__init__(num, pos_fraction,\n                                                    **kwargs)\n        assert floor_thr >= 0 or floor_thr == -1\n        assert 0 <= floor_fraction <= 1\n        assert num_bins >= 1\n\n        self.floor_thr = floor_thr\n        self.floor_fraction = floor_fraction\n        self.num_bins = num_bins\n\n    def sample_via_interval(self, max_overlaps, full_set, num_expected):\n        max_iou = max_overlaps.max()\n        iou_interval = (max_iou - self.floor_thr) / self.num_bins\n        per_num_expected = int(num_expected / self.num_bins)\n\n        sampled_inds = []\n        for i in range(self.num_bins):\n            start_iou = self.floor_thr + i * iou_interval\n            end_iou = self.floor_thr + (i + 1) * iou_interval\n            tmp_set = set(\n                np.where(\n                    np.logical_and(max_overlaps >= start_iou,\n                                   max_overlaps < end_iou))[0])\n            tmp_inds = list(tmp_set & full_set)\n            if len(tmp_inds) > per_num_expected:\n                tmp_sampled_set = self.random_choice(tmp_inds,\n                                                     per_num_expected)\n            else:\n                tmp_sampled_set = np.array(tmp_inds, dtype=np.int)\n            sampled_inds.append(tmp_sampled_set)\n\n        sampled_inds = np.concatenate(sampled_inds)\n        if len(sampled_inds) < num_expected:\n            num_extra = num_expected - len(sampled_inds)\n            extra_inds = np.array(list(full_set - set(sampled_inds)))\n            if len(extra_inds) > num_extra:\n                extra_inds = self.random_choice(extra_inds, num_extra)\n            sampled_inds = np.concatenate([sampled_inds, extra_inds])\n\n        return sampled_inds\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            max_overlaps = assign_result.max_overlaps.cpu().numpy()\n            # balance sampling for negative samples\n            neg_set = set(neg_inds.cpu().numpy())\n\n            if self.floor_thr > 0:\n                floor_set = set(\n                    np.where(\n                        np.logical_and(max_overlaps >= 0,\n                                       max_overlaps < self.floor_thr))[0])\n                iou_sampling_set = set(\n                    np.where(max_overlaps >= self.floor_thr)[0])\n            elif self.floor_thr == 0:\n                floor_set = set(np.where(max_overlaps == 0)[0])\n                iou_sampling_set = set(\n                    np.where(max_overlaps > self.floor_thr)[0])\n            else:\n                floor_set = set()\n                iou_sampling_set = set(\n                    np.where(max_overlaps > self.floor_thr)[0])\n\n            floor_neg_inds = list(floor_set & neg_set)\n            iou_sampling_neg_inds = list(iou_sampling_set & neg_set)\n            num_expected_iou_sampling = int(num_expected *\n                                            (1 - self.floor_fraction))\n            if len(iou_sampling_neg_inds) > num_expected_iou_sampling:\n                if self.num_bins >= 2:\n                    iou_sampled_inds = self.sample_via_interval(\n                        max_overlaps, set(iou_sampling_neg_inds),\n                        num_expected_iou_sampling)\n                else:\n                    iou_sampled_inds = self.random_choice(\n                        iou_sampling_neg_inds, num_expected_iou_sampling)\n            else:\n                iou_sampled_inds = np.array(\n                    iou_sampling_neg_inds, dtype=np.int)\n            num_expected_floor = num_expected - len(iou_sampled_inds)\n            if len(floor_neg_inds) > num_expected_floor:\n                sampled_floor_inds = self.random_choice(\n                    floor_neg_inds, num_expected_floor)\n            else:\n                sampled_floor_inds = np.array(floor_neg_inds, dtype=np.int)\n            sampled_inds = np.concatenate(\n                (sampled_floor_inds, iou_sampled_inds))\n            if len(sampled_inds) < num_expected:\n                num_extra = num_expected - len(sampled_inds)\n                extra_inds = np.array(list(neg_set - set(sampled_inds)))\n                if len(extra_inds) > num_extra:\n                    extra_inds = self.random_choice(extra_inds, num_extra)\n                sampled_inds = np.concatenate((sampled_inds, extra_inds))\n            sampled_inds = torch.from_numpy(sampled_inds).long().to(\n                assign_result.gt_inds.device)\n            return sampled_inds\n'"
mmdet/core/bbox/samplers/ohem_sampler.py,3,"b""import torch\n\nfrom .base_sampler import BaseSampler\nfrom ..transforms import bbox2roi\n\n\nclass OHEMSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 context,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(OHEMSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                          add_gt_as_proposals)\n        if not hasattr(context, 'num_stages'):\n            self.bbox_roi_extractor = context.bbox_roi_extractor\n            self.bbox_head = context.bbox_head\n        else:\n            self.bbox_roi_extractor = context.bbox_roi_extractor[\n                context.current_stage]\n            self.bbox_head = context.bbox_head[context.current_stage]\n\n    def hard_mining(self, inds, num_expected, bboxes, labels, feats):\n        with torch.no_grad():\n            rois = bbox2roi([bboxes])\n            bbox_feats = self.bbox_roi_extractor(\n                feats[:self.bbox_roi_extractor.num_inputs], rois)\n            cls_score, _ = self.bbox_head(bbox_feats)\n            loss = self.bbox_head.loss(\n                cls_score=cls_score,\n                bbox_pred=None,\n                labels=labels,\n                label_weights=cls_score.new_ones(cls_score.size(0)),\n                bbox_targets=None,\n                bbox_weights=None,\n                reduction_override='none')['loss_cls']\n            _, topk_loss_inds = loss.topk(num_expected)\n        return inds[topk_loss_inds]\n\n    def _sample_pos(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard positive samples\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.hard_mining(pos_inds, num_expected, bboxes[pos_inds],\n                                    assign_result.labels[pos_inds], feats)\n\n    def _sample_neg(self,\n                    assign_result,\n                    num_expected,\n                    bboxes=None,\n                    feats=None,\n                    **kwargs):\n        # Sample some hard negative samples\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.hard_mining(neg_inds, num_expected, bboxes[neg_inds],\n                                    assign_result.labels[neg_inds], feats)\n"""
mmdet/core/bbox/samplers/pseudo_sampler.py,3,"b'import torch\n\nfrom .base_sampler import BaseSampler\nfrom .sampling_result import SamplingResult\n\n\nclass PseudoSampler(BaseSampler):\n\n    def __init__(self, **kwargs):\n        pass\n\n    def _sample_pos(self, **kwargs):\n        raise NotImplementedError\n\n    def _sample_neg(self, **kwargs):\n        raise NotImplementedError\n\n    def sample(self, assign_result, bboxes, gt_bboxes, **kwargs):\n        pos_inds = torch.nonzero(\n            assign_result.gt_inds > 0).squeeze(-1).unique()\n        neg_inds = torch.nonzero(\n            assign_result.gt_inds == 0).squeeze(-1).unique()\n        gt_flags = bboxes.new_zeros(bboxes.shape[0], dtype=torch.uint8)\n        sampling_result = SamplingResult(pos_inds, neg_inds, bboxes, gt_bboxes,\n                                         assign_result, gt_flags)\n        return sampling_result\n'"
mmdet/core/bbox/samplers/random_sampler.py,3,"b'import numpy as np\nimport torch\n\nfrom .base_sampler import BaseSampler\n\n\nclass RandomSampler(BaseSampler):\n\n    def __init__(self,\n                 num,\n                 pos_fraction,\n                 neg_pos_ub=-1,\n                 add_gt_as_proposals=True,\n                 **kwargs):\n        super(RandomSampler, self).__init__(num, pos_fraction, neg_pos_ub,\n                                            add_gt_as_proposals)\n\n    @staticmethod\n    def random_choice(gallery, num):\n        """"""Random select some elements from the gallery.\n\n        It seems that Pytorch\'s implementation is slower than numpy so we use\n        numpy to randperm the indices.\n        """"""\n        assert len(gallery) >= num\n        if isinstance(gallery, list):\n            gallery = np.array(gallery)\n        cands = np.arange(len(gallery))\n        np.random.shuffle(cands)\n        rand_inds = cands[:num]\n        if not isinstance(gallery, np.ndarray):\n            rand_inds = torch.from_numpy(rand_inds).long().to(gallery.device)\n        return gallery[rand_inds]\n\n    def _sample_pos(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some positive samples.""""""\n        pos_inds = torch.nonzero(assign_result.gt_inds > 0)\n        if pos_inds.numel() != 0:\n            pos_inds = pos_inds.squeeze(1)\n        if pos_inds.numel() <= num_expected:\n            return pos_inds\n        else:\n            return self.random_choice(pos_inds, num_expected)\n\n    def _sample_neg(self, assign_result, num_expected, **kwargs):\n        """"""Randomly sample some negative samples.""""""\n        neg_inds = torch.nonzero(assign_result.gt_inds == 0)\n        if neg_inds.numel() != 0:\n            neg_inds = neg_inds.squeeze(1)\n        if len(neg_inds) <= num_expected:\n            return neg_inds\n        else:\n            return self.random_choice(neg_inds, num_expected)\n'"
mmdet/core/bbox/samplers/sampling_result.py,1,"b'import torch\n\n\nclass SamplingResult(object):\n\n    def __init__(self, pos_inds, neg_inds, bboxes, gt_bboxes, assign_result,\n                 gt_flags):\n        self.pos_inds = pos_inds\n        self.neg_inds = neg_inds\n        self.pos_bboxes = bboxes[pos_inds]\n        self.neg_bboxes = bboxes[neg_inds]\n        self.pos_is_gt = gt_flags[pos_inds]\n\n        self.num_gts = gt_bboxes.shape[0]\n        self.pos_assigned_gt_inds = assign_result.gt_inds[pos_inds] - 1\n        self.pos_gt_bboxes = gt_bboxes[self.pos_assigned_gt_inds, :]\n        if assign_result.labels is not None:\n            self.pos_gt_labels = assign_result.labels[pos_inds]\n        else:\n            self.pos_gt_labels = None\n\n    @property\n    def bboxes(self):\n        return torch.cat([self.pos_bboxes, self.neg_bboxes])\n'"
