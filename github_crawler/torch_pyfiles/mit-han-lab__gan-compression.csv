file_path,api_count,code
distill.py,0,"b""from trainer import Trainer\n\nif __name__ == '__main__':\n    trainer = Trainer('distill')\n    trainer.start()\n    print('Distillation finished!!!')\n"""
export.py,1,"b""import argparse\nimport os\n\nimport torch\nfrom torch import nn\n\nfrom configs import decode_config\nfrom models.modules.sync_batchnorm import SynchronizedBatchNorm2d\nfrom utils.util import load_network\n\n\ndef transfer_conv(mA, mB):\n    weight = mA.weight.data\n    a, b, c, d = mB.weight.data.shape\n    mB.weight.data = weight[:a, :b, :c, :d]\n    if mB.bias is not None:\n        a = mB.bias.data.shape[0]\n        mB.bias.data = mA.bias.data[:a]\n\n\ndef transfer_bn(mA, mB):\n    assert isinstance(mA, SynchronizedBatchNorm2d) and isinstance(mB, SynchronizedBatchNorm2d)\n    if mA.weight is not None:\n        assert mB.weight is not None\n        assert mA.weight.shape == mB.weight.shape\n        mB.weight.data = mA.weight.data\n    if mA.bias is not None:\n        assert mB.bias is not None\n        assert mA.bias.shape == mB.bias.shape\n        mB.bias.data = mA.bias.data\n    if mA.running_mean is not None:\n        assert mB.running_mean is not None\n        assert mA.running_mean.shape == mB.running_mean.shape\n        mB.running_mean.data = mA.running_mean.data\n    if mA.running_var is not None:\n        assert mB.running_var is not None\n        assert mA.running_var.shape == mB.running_var.shape\n        mB.running_var.data = mA.running_var.data\n    if mA.num_batches_tracked is not None:\n        assert mB.num_batches_tracked is not None\n        mB.num_batches_tracked.data = mA.num_batches_tracked.data\n\n\ndef transfer_weight(netA, netB):  # netA -> netB\n    for nA, mA in netA.named_modules():\n        if isinstance(mA, (nn.ConvTranspose2d, nn.Conv2d)):\n            for nB, mB in netB.named_modules():\n                if nA == nB:\n                    transfer_conv(mA, mB)\n        elif isinstance(mA, SynchronizedBatchNorm2d):\n            for nB, mB in netB.named_modules():\n                if nA == nB:\n                    transfer_bn(mA, mB)\n\n\ndef main(opt):\n    config = decode_config(opt.config_str)\n    if opt.model == 'mobile_resnet':\n        from models.modules.resnet_architecture.mobile_resnet_generator import MobileResnetGenerator as SuperModel\n        from models.modules.resnet_architecture.sub_mobile_resnet_generator import SubMobileResnetGenerator as SubModel\n        input_nc, output_nc = opt.input_nc, opt.output_nc\n        super_model = SuperModel(input_nc, output_nc, ngf=opt.ngf, norm_layer=nn.InstanceNorm2d, n_blocks=9)\n        sub_model = SubModel(input_nc, output_nc, config=config, norm_layer=nn.InstanceNorm2d, n_blocks=9)\n    elif opt.model == 'mobile_spade':\n        from models.modules.spade_architecture.mobile_spade_generator import MobileSPADEGenerator as SuperModel\n        from models.modules.spade_architecture.sub_mobile_spade_generator import SubMobileSPADEGenerator as SubModel\n        opt.norm_G = 'spadesyncbatch3x3'\n        opt.num_upsampling_layers = 'more'\n        opt.semantic_nc = opt.input_nc + (1 if opt.contain_dontcare_label else 0) + (0 if opt.no_instance else 1)\n        super_model = SuperModel(opt)\n        sub_model = SubModel(opt, config)\n    else:\n        raise NotImplementedError('Unknown architecture [%s]!' % opt.model)\n\n    load_network(super_model, opt.input_path)\n    transfer_weight(super_model, sub_model)\n\n    output_dir = os.path.dirname(opt.output_path)\n    os.makedirs(output_dir, exist_ok=True)\n    torch.save(sub_model.state_dict(), opt.output_path)\n    print('Successfully export the subnet at [%s].' % opt.output_path)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Export a specific subnet from a supernet')\n    parser.add_argument('--model', type=str, default='mobile_resnet', choices=['mobile_resnet', 'mobile_spade'],\n                        help='specify the model type you want to export')\n    parser.add_argument('--ngf', type=int, default=48, help='the base number of filters of the generator')\n    parser.add_argument('--input_path', type=str, required=True, help='the input model path')\n    parser.add_argument('--output_path', type=str, required=True, help='the path to the exported model')\n    parser.add_argument('--config_str', type=str, required=True,\n                        help='the configuration string for a specific subnet in the supernet')\n    parser.add_argument('--input_nc', type=int, default=3,\n                        help='# of input image channels: 3 for RGB and 1 for grayscale')\n    parser.add_argument('--output_nc', type=int, default=3,\n                        help='# of output image channels: 3 for RGB and 1 for grayscale')\n    parser.add_argument('--no_instance', action='store_true',\n                        help='if specified, do *not* add instance map as input')\n    parser.add_argument('--contain_dontcare_label', action='store_true',\n                        help='if the label map contains dontcare label (dontcare=255)')\n    parser.add_argument('--crop_size', type=int, default=512,\n                        help='then crop to this size')\n    parser.add_argument('--aspect_ratio', type=float, default=2.0,\n                        help='The ratio width/height. The final height of the load image will be crop_size/aspect_ratio')\n    opt = parser.parse_args()\n    main(opt)\n"""
get_real_stat.py,4,"b'import argparse\nimport warnings\n\nimport numpy as np\nimport torch\nimport tqdm\n\nfrom data import create_dataloader\nfrom metric.fid_score import _compute_statistics_of_ims\nfrom metric.inception import InceptionV3\nfrom utils import util\n\n\ndef main(opt):\n    dataloader = create_dataloader(opt)\n    device = torch.device(\'cuda:{}\'.format(opt.gpu_ids[0])) if opt.gpu_ids \\\n        else torch.device(\'cpu\')\n    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n    inception_model = InceptionV3([block_idx])\n    inception_model.to(device)\n    inception_model.eval()\n\n    tensors = []\n    for i, data_i in enumerate(tqdm.tqdm(dataloader)):\n        tensor = data_i[opt.direction[-1]]\n        tensors.append(tensor)\n    tensors = torch.cat(tensors, dim=0)\n    tensors = util.tensor2im(tensors).astype(float)\n    mu, sigma = _compute_statistics_of_ims(tensors, inception_model, 32, 2048, device, use_tqdm=True)\n    np.savez(opt.output_path, mu=mu, sigma=sigma)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser(description=\'Extract some statistical information of a dataset to compute FID\')\n    parser.add_argument(\'--input_nc\', type=int, default=3,\n                        help=\'# of input image channels: 3 for RGB and 1 for grayscale\')\n    parser.add_argument(\'--output_nc\', type=int, default=3,\n                        help=\'# of output image channels: 3 for RGB and 1 for grayscale\')\n    parser.add_argument(\'--dataroot\', required=True,\n                        help=\'path to images (should have subfolders trainA, trainB, valA, valB, train, val, etc)\')\n    parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\',\n                        help=\'chooses how datasets are loaded. [aligned | single]\')\n    parser.add_argument(\'--direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n    parser.add_argument(\'--load_size\', type=int, default=256, help=\'scale images to this size\')\n    parser.add_argument(\'--crop_size\', type=int, default=256, help=\'then crop to this size\')\n    parser.add_argument(\'--preprocess\', type=str, default=\'none\', help=\'scaling and cropping of images at load time \'\n                             \'[resize_and_crop | crop | scale_width | scale_width_and_crop | none]\')\n    parser.add_argument(\'--phase\', type=str, default=\'val\', help=\'train, val, test, etc\')\n    parser.add_argument(\'--output_path\', type=str, required=True,\n                        help=\'the path to save the statistical information.\')\n    parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n\n    opt = parser.parse_args()\n    opt.num_threads = 0\n    opt.batch_size = 1\n    opt.serial_batches = True\n    opt.no_flip = True\n    opt.max_dataset_size = -1\n    opt.load_in_memory = False\n    if opt.dataset_mode == \'single\' and opt.direction == \'AtoB\':\n        warnings.warn(\'Dataset mode [single] only supports direction BtoA. \'\n                      \'We will change the direction to BtoA.!\')\n        opt.direction = \'BtoA\'\n\n\n    def parse_gpu_ids(str_ids):\n        str_ids = str_ids.split(\',\')\n        gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                gpu_ids.append(id)\n        if len(gpu_ids) > 0:\n            torch.cuda.set_device(gpu_ids[0])\n        return gpu_ids\n\n\n    opt.gpu_ids = parse_gpu_ids(opt.gpu_ids)\n\n    if not opt.output_path.endswith(\'.npz\'):\n        warnings.warn(\'The output is a numpy npz file, but the output path does\\\'nt end with "".npz"".\')\n    if len(opt.gpu_ids) > 1:\n        warnings.warn(\'The code only supports single GPU. Only gpu [%d] will be used.\' % opt.gpu_ids[0])\n    main(opt)\n'"
get_test_opt.py,0,"b""import os\nimport pickle\n\nfrom options.test_options import TestOptions\n\nif __name__ == '__main__':\n    opt = TestOptions().parse(verbose=False)\n    os.makedirs('opts', exist_ok=True)\n    if 'full' in opt.restore_G_path:\n        output_path = 'opts/opt_full.pkl'\n    else:\n        assert 'compressed' in opt.restore_G_path\n        output_path = 'opts/opt_compressed.pkl'\n    with open(output_path, 'wb') as f:\n        pickle.dump(opt, f)\n    print('Save options at [%s].' % output_path)\n"""
latency.py,3,"b""import sys\nimport time\nimport warnings\n\nimport torch\nimport tqdm\nfrom torch.backends import cudnn\n\nfrom configs import decode_config\nfrom data import create_dataloader\nfrom models import create_model\nfrom options.test_options import TestOptions\n\n\ndef check(opt):\n    assert opt.serial_batches\n    assert opt.no_flip\n    assert opt.load_size == opt.crop_size\n    assert opt.preprocess == 'resize_and_crop'\n    assert opt.batch_size == 1\n\n    if not opt.no_fid:\n        assert opt.real_stat_path is not None\n    if opt.phase == 'train':\n        warnings.warn('You are using training set for inference.')\n\n\nif __name__ == '__main__':\n    cudnn.enabled = True\n    opt = TestOptions().parse()\n    print(' '.join(sys.argv))\n    if opt.config_str is not None:\n        assert 'super' in opt.netG or 'sub' in opt.netG\n        config = decode_config(opt.config_str)\n    else:\n        assert 'super' not in opt.model\n        config = None\n\n    dataloader = create_dataloader(opt)\n    model = create_model(opt)\n    model.setup(opt)\n\n    for data in dataloader:\n        model.set_input(data)\n        break\n\n    # Warm-up times\n    for i in tqdm.trange(opt.times):\n        model.test(config)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.synchronize()\n\n    start_time = time.time()\n    for i in tqdm.trange(opt.times):\n        model.test(config)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.synchronize()\n    cost_time = time.time() - start_time\n    print('Cost Time: %.2fs\\tLatency: %.4fs' % (cost_time, cost_time / opt.times))\n"""
search.py,4,"b""import ntpath\nimport os\nimport pickle\nimport random\nimport sys\nimport warnings\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom torch import nn\nfrom torch.backends import cudnn\n\nfrom configs import encode_config\nfrom data import create_dataloader\nfrom metric import get_fid, get_mIoU\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import create_model\nfrom options.search_options import SearchOptions\nfrom utils import util\n\n\ndef set_seed(seed):\n    cudnn.benchmark = False  # if benchmark=True, deterministic will be False\n    cudnn.deterministic = True\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef check(opt):\n    assert opt.serial_batches\n    assert opt.no_flip\n    assert opt.load_size == opt.crop_size\n    assert opt.preprocess == 'resize_and_crop'\n    assert opt.config_set is not None\n    if len(opt.gpu_ids) > 1:\n        warnings.warn('The code only supports single GPU. Only gpu [%d] will be used.' % opt.gpu_ids[0])\n    if opt.phase == 'train':\n        warnings.warn('You are using training set for evaluation.')\n\n\nif __name__ == '__main__':\n    opt = SearchOptions().parse()\n    print(' '.join(sys.argv), flush=True)\n    check(opt)\n    set_seed(opt.seed)\n\n    if 'resnet' in opt.netG:\n        from configs.resnet_configs import get_configs\n    elif 'spade' in opt.netG:\n        # TODO\n        raise NotImplementedError\n    else:\n        raise NotImplementedError\n    configs = get_configs(config_name=opt.config_set)\n    configs = list(configs.all_configs())\n\n    dataloader = create_dataloader(opt)\n    model = create_model(opt)\n    model.setup(opt)\n    device = model.device\n\n    if not opt.no_fid:\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        inception_model = InceptionV3([block_idx])\n        inception_model.to(device)\n        inception_model.eval()\n    if 'cityscapes' in opt.dataroot and opt.direction == 'BtoA':\n        drn_model = DRNSeg('drn_d_105', 19, pretrained=False)\n        util.load_network(drn_model, opt.drn_path, verbose=False)\n        if len(opt.gpu_ids) > 0:\n            drn_model = nn.DataParallel(drn_model, opt.gpu_ids)\n        drn_model.eval()\n\n    npz = np.load(opt.real_stat_path)\n    results = []\n\n    for data_i in dataloader:\n        model.set_input(data_i)\n        break\n\n    for config in tqdm.tqdm(configs):\n        qualified = True\n        macs, _ = model.profile(config)\n        if macs > opt.budget:\n            qualified = False\n        else:\n            qualified = True\n\n        fakes, names = [], []\n\n        if qualified:\n            for i, data_i in enumerate(dataloader):\n                model.set_input(data_i)\n                model.test(config)\n                fakes.append(model.fake_B.cpu())\n                for path in model.get_image_paths():\n                    short_path = ntpath.basename(path)\n                    name = os.path.splitext(short_path)[0]\n                    names.append(name)\n\n        result = {'config_str': encode_config(config), 'macs': macs}\n        if not opt.no_fid:\n            if qualified:\n                fid = get_fid(fakes, inception_model, npz, device, opt.batch_size, use_tqdm=False)\n                result['fid'] = fid\n            else:\n                result['fid'] = 1e9\n        if 'cityscapes' in opt.dataroot and opt.direction == 'BtoA':\n            if qualified:\n                mIoU = get_mIoU(fakes, names, drn_model, device,\n                              data_dir=opt.cityscapes_path,\n                              batch_size=opt.batch_size,\n                              num_workers=opt.num_threads,\n                              use_tqdm=False)\n                result['mIoU'] = mIoU\n            else:\n                result['mIoU'] = 0\n        print(result, flush=True)\n        results.append(result)\n\n    os.makedirs(os.path.dirname(opt.output_path), exist_ok=True)\n    with open(opt.output_path, 'wb') as f:\n        pickle.dump(results, f)\n    print('Successfully finish searching!!!', flush=True)\n"""
search_multi.py,4,"b'import copy\nimport ntpath\nimport os\nimport pickle\nimport random\nimport sys\nimport warnings\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom torch import multiprocessing as mp\nfrom torch import nn\nfrom torch.backends import cudnn\n\nfrom configs import encode_config\nfrom data import create_dataloader\nfrom metric import get_fid, get_mIoU\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import create_model\nfrom options.search_options import SearchOptions\nfrom utils import util\n\n\ndef set_seed(seed):\n    cudnn.benchmark = False  # if benchmark=True, deterministic will be False\n    cudnn.deterministic = True\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\ndef check(opt):\n    assert opt.serial_batches\n    assert opt.no_flip\n    assert opt.load_size == opt.crop_size\n    assert opt.preprocess == \'resize_and_crop\'\n    assert opt.config_set is not None\n    if len(opt.gpu_ids) == 0:\n        raise ValueError(""Multi-gpu searching doesn\'t support cpu. Please specify at least one gpu."")\n    if opt.phase == \'train\':\n        warnings.warn(\'You are using training set for evaluation.\')\n\n\ndef main(configs, opt, gpu_id, queue, verbose):\n    opt.gpu_ids = [gpu_id]\n    dataloader = create_dataloader(opt, verbose)\n    model = create_model(opt, verbose)\n    model.setup(opt, verbose)\n    device = model.device\n    if not opt.no_fid:\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        inception_model = InceptionV3([block_idx])\n        inception_model.to(device)\n        inception_model.eval()\n    if \'cityscapes\' in opt.dataroot and opt.direction == \'BtoA\':\n        drn_model = DRNSeg(\'drn_d_105\', 19, pretrained=False)\n        util.load_network(drn_model, opt.drn_path, verbose=False)\n        if len(opt.gpu_ids) > 0:\n            drn_model = nn.DataParallel(drn_model, opt.gpu_ids)\n        drn_model.eval()\n\n    npz = np.load(opt.real_stat_path)\n    results = []\n\n    for data_i in dataloader:\n        model.set_input(data_i)\n        break\n\n    for config in tqdm.tqdm(configs):\n        qualified = True\n        macs, _ = model.profile(config)\n        if macs > opt.budget:\n            qualified = False\n        else:\n            qualified = True\n\n        fakes, names = [], []\n\n        if qualified:\n            for i, data_i in enumerate(dataloader):\n                model.set_input(data_i)\n\n                model.test(config)\n                fakes.append(model.fake_B.cpu())\n                for path in model.get_image_paths():\n                    short_path = ntpath.basename(path)\n                    name = os.path.splitext(short_path)[0]\n                    names.append(name)\n\n        result = {\'config_str\': encode_config(config), \'macs\': macs}\n        if not opt.no_fid:\n            if qualified:\n                fid = get_fid(fakes, inception_model, npz, device, opt.batch_size, use_tqdm=False)\n                result[\'fid\'] = fid\n            else:\n                result[\'fid\'] = 1e9\n        if \'cityscapes\' in opt.dataroot and opt.direction == \'BtoA\':\n            if qualified:\n                mIoU = get_mIoU(fakes, names, drn_model, device,\n                                data_dir=opt.cityscapes_path,\n                                batch_size=opt.batch_size,\n                                num_workers=opt.num_threads, use_tqdm=False)\n                result[\'mIoU\'] = mIoU\n            else:\n                result[\'mIoU\'] = mIoU\n        print(result, flush=True)\n        results.append(result)\n    queue.put(results)\n\n\nif __name__ == \'__main__\':\n    mp.set_start_method(\'spawn\')\n    opt = SearchOptions().parse()\n    print(\' \'.join(sys.argv), flush=True)\n    check(opt)\n    set_seed(opt.seed)\n\n    if \'resnet\' in opt.netG:\n        from configs.resnet_configs import get_configs\n    elif \'spade\' in opt.netG:\n        # TODO\n        pass\n    else:\n        raise NotImplementedError\n    configs = get_configs(config_name=opt.config_set)\n    configs = list(configs.all_configs())\n    random.shuffle(configs)\n\n    chunk_size = (len(configs) + len(opt.gpu_ids) - 1) // len(opt.gpu_ids)\n\n    processes = []\n    queue = mp.Queue()\n\n    for i, gpu_id in enumerate(opt.gpu_ids):\n        start = min(i * chunk_size, len(configs))\n        end = min((i + 1) * chunk_size, len(configs))\n        p = mp.Process(target=main, args=(configs[start:end], copy.deepcopy(opt), gpu_id, queue, i == 0))\n        processes.append(p)\n        p.start()\n    for p in processes:\n        p.join()\n\n    results = []\n    for p in processes:\n        results += queue.get()\n\n    os.makedirs(os.path.dirname(opt.output_path), exist_ok=True)\n    with open(opt.output_path, \'wb\') as f:\n        pickle.dump(results, f)\n    print(\'Successfully finish searching!!!\', flush=True)\n'"
select_arch.py,0,"b""import argparse\nimport pickle\n\n\ndef takeMACs(item):\n    return item['macs']\n\n\ndef main(opt):\n    with open(opt.pkl_path, 'rb') as f:\n        results = pickle.load(f)\n    results.sort(key=takeMACs)\n\n    for item in results:\n        assert isinstance(item, dict)\n        qualified = True\n        if item['macs'] > opt.macs:\n            qualified = False\n        elif 'fid' in item and item['fid'] > opt.fid:\n            qualified = False\n        elif 'mIoU' in item and item['mIoU'] < opt.mIoU:\n            qualified = False\n        if qualified:\n            print(item)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='An auxiliary script to read the parse the output pickle and select the configurations you want.')\n    parser.add_argument('--pkl_path', type=str, required=True, help='the input .pkl file path')\n    parser.add_argument('--macs', type=float, default=5.68e9, help='the MACs threshold')\n    parser.add_argument('--fid', type=float, default=-1, help='the FID threshold')\n    parser.add_argument('--mIoU', type=float, default=1e18, help='the mIoU threshold')\n    opt = parser.parse_args()\n    main(opt)\n"""
test.py,0,"b""import ntpath\nimport os\nimport sys\nimport warnings\n\nimport numpy as np\nimport tqdm\nfrom torch import nn\n\nfrom configs import decode_config\nfrom data import create_dataloader\nfrom metric import get_mIoU, get_fid\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import create_model\nfrom options.test_options import TestOptions\nfrom utils import html, util\n\n\ndef save_images(webpage, visuals, image_path, opt):\n    def convert_visuals_to_numpy(visuals):\n        for key, t in visuals.items():\n            tile = opt.batch_size > 8\n            if key == 'labels':\n                t = util.tensor2label(t, opt.input_nc + 2, tile=tile)\n            else:\n                t = util.tensor2im(t, tile=tile)\n            visuals[key] = t\n        return visuals\n\n    visuals = convert_visuals_to_numpy(visuals)\n\n    image_dir = webpage.get_image_dir()\n    short_path = ntpath.basename(image_path[0])\n    name = os.path.splitext(short_path)[0]\n\n    webpage.add_header(name)\n    ims = []\n    txts = []\n    links = []\n\n    for label, image_numpy in visuals.items():\n        image_name = os.path.join(label, '%s.png' % (name))\n        save_path = os.path.join(image_dir, image_name)\n        util.save_image(image_numpy, save_path, create_dir=True)\n\n        ims.append(image_name)\n        txts.append(label)\n        links.append(image_name)\n    webpage.add_images(ims, txts, links, width=opt.display_winsize)\n\n\ndef check(opt):\n    assert opt.serial_batches\n    assert opt.no_flip\n    assert opt.load_size == opt.crop_size\n    assert opt.preprocess == 'resize_and_crop'\n    assert opt.batch_size == 1\n\n    if not opt.no_fid:\n        assert opt.real_stat_path is not None\n    if opt.phase == 'train':\n        warnings.warn('You are using training set for inference.')\n\n\nif __name__ == '__main__':\n    opt = TestOptions().parse()\n    print(' '.join(sys.argv))\n    if opt.config_str is not None:\n        assert 'super' in opt.netG or 'sub' in opt.netG\n        config = decode_config(opt.config_str)\n    else:\n        assert 'super' not in opt.model\n        config = None\n\n    dataloader = create_dataloader(opt)\n    model = create_model(opt)\n    model.setup(opt)\n\n    web_dir = opt.results_dir  # define the website directory\n    webpage = html.HTML(web_dir, 'restore_G_path: %s' % (opt.restore_G_path))\n    fakes, names = [], []\n    for i, data in enumerate(tqdm.tqdm(dataloader)):\n        model.set_input(data)  # unpack data from data loader\n        if i == 0 and opt.need_profile:\n            model.profile(config)\n        model.test(config)  # run inference\n        visuals = model.get_current_visuals()  # get image results\n        generated = visuals['fake_B'].cpu()\n        fakes.append(generated)\n        for path in model.get_image_paths():\n            short_path = ntpath.basename(path)\n            name = os.path.splitext(short_path)[0]\n            names.append(name)\n        if i < opt.num_test:\n            save_images(webpage, visuals, model.get_image_paths(), opt)\n    webpage.save()  # save the HTML\n    device = model.device\n\n    if 'cityscapes' in opt.dataroot and not opt.no_mIoU and opt.direction == 'BtoA':\n        drn_model = DRNSeg('drn_d_105', 19, pretrained=False)\n        util.load_network(drn_model, opt.drn_path, verbose=False)\n        if len(opt.gpu_ids) > 0:\n            drn_model = nn.DataParallel(drn_model, opt.gpu_ids)\n        drn_model.eval()\n        mIoU = get_mIoU(fakes, names, drn_model, device,\n                        data_dir=opt.cityscapes_path,\n                        batch_size=opt.batch_size,\n                        num_workers=opt.num_threads)\n        print('mIoU: %.2f' % mIoU)\n\n    if not opt.no_fid:\n        print('Calculating FID...', flush=True)\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        inception_model = InceptionV3([block_idx])\n        inception_model.to(device)\n        inception_model.eval()\n        npz = np.load(opt.real_stat_path)\n        fid = get_fid(fakes, inception_model, npz, device, opt.batch_size)\n        print('fid score: %.2f' % fid, flush=True)\n"""
train.py,0,"b""from trainer import Trainer\n\nif __name__ == '__main__':\n    trainer = Trainer('train')\n    trainer.start()\n    print('Training finished!!!')\n"""
train_supernet.py,0,"b""from trainer import Trainer\n\nif __name__ == '__main__':\n    trainer = Trainer('supernet')\n    trainer.start()\n    print('Supernet training finished!!!')\n"""
trainer.py,4,"b""import os\nimport random\nimport sys\nimport time\nimport warnings\n\nimport numpy as np\nimport torch\nfrom torch.backends import cudnn\n\nfrom data import create_dataloader\nfrom utils.logger import Logger\n\n\ndef set_seed(seed):\n    cudnn.benchmark = False  # if benchmark=True, deterministic will be False\n    cudnn.deterministic = True\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\n\nclass Trainer:\n    def __init__(self, task):\n        if task == 'train':\n            from options.train_options import TrainOptions as Options\n            from models import create_model as create_model\n        elif task == 'distill':\n            from options.distill_options import DistillOptions as Options\n            from distillers import create_distiller as create_model\n        elif task == 'supernet':\n            from options.supernet_options import SupernetOptions as Options\n            from supernets import create_supernet as create_model\n        else:\n            raise NotImplementedError('Unknown task [%s]!!!' % task)\n        opt = Options().parse()\n        opt.tensorboard_dir = opt.log_dir if opt.tensorboard_dir is None else opt.tensorboard_dir\n        print(' '.join(sys.argv))\n        if opt.phase != 'train':\n            warnings.warn('You are not using training set for %s!!!' % task)\n        with open(os.path.join(opt.log_dir, 'opt.txt'), 'a') as f:\n            f.write(' '.join(sys.argv) + '\\n')\n        set_seed(opt.seed)\n\n        dataloader = create_dataloader(opt)  # create a dataset given opt.dataset_mode and other options\n        dataset_size = len(dataloader.dataset)  # get the number of images in the dataset.\n        print('The number of training images = %d' % dataset_size)\n\n        model = create_model(opt)  # create a model given opt.model and other options\n        model.setup(opt)  # regular setup: load and print networks; create schedulers\n        logger = Logger(opt)\n\n        self.opt = opt\n        self.dataloader = dataloader\n        self.model = model\n        self.logger = logger\n\n    def evaluate(self, epoch, iter, message):\n        start_time = time.time()\n        metrics = self.model.evaluate_model(iter)\n        self.logger.print_current_metrics(epoch, iter, metrics, time.time() - start_time)\n        self.logger.plot(metrics, iter)\n        self.logger.print_info(message)\n        self.model.save_networks('latest')\n\n    def start(self):\n        opt = self.opt\n        dataset = self.dataloader\n        model = self.model\n        logger = self.logger\n\n        start_epoch = opt.epoch_base\n        end_epoch = opt.epoch_base + opt.nepochs + opt.nepochs_decay - 1\n        total_iter = opt.iter_base\n        for epoch in range(start_epoch, end_epoch + 1):\n            epoch_start_time = time.time()  # timer for entire epoch\n            for i, data_i in enumerate(dataset):\n                iter_start_time = time.time()\n                model.set_input(data_i)\n                model.optimize_parameters()\n\n                if total_iter % opt.print_freq == 0:\n                    losses = model.get_current_losses()\n                    logger.print_current_errors(epoch, total_iter, losses, time.time() - iter_start_time)\n                    logger.plot(losses, total_iter)\n\n                if total_iter % opt.save_latest_freq == 0 or total_iter == opt.iter_base:\n                    self.evaluate(epoch, total_iter,\n                                  'Saving the latest model (epoch %d, total_steps %d)' % (epoch, total_iter))\n                    if model.is_best:\n                        model.save_networks('iter%d' % total_iter)\n\n                total_iter += 1\n            logger.print_info(\n                'End of epoch %d / %d \\t Time Taken: %.2f sec' % (epoch, end_epoch, time.time() - epoch_start_time))\n            if epoch % opt.save_epoch_freq == 0 or epoch == end_epoch:\n                self.evaluate(epoch, total_iter,\n                              'Saving the model at the end of epoch %d, iters %d' % (epoch, total_iter))\n                model.save_networks(epoch)\n            model.update_learning_rate(logger)\n"""
configs/__init__.py,0,"b""def encode_config(config):\n    return '_'.join([str(c) for c in config['channels']])\n\n\ndef decode_config(config_str):\n    channels = config_str.split('_')\n    assert len(channels) == 6 or len(channels) == 8\n    channels = [int(c) for c in channels]\n    return {'channels': channels}\n"""
configs/resnet_configs.py,0,"b""import random\n\n\nclass ResnetConfigs:\n    def __init__(self, n_channels):\n        self.attributes = ['n_channels']\n        self.n_channels = n_channels\n\n    def sample(self):\n        ret = {}\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(random.choice(n_channel))\n        return ret\n\n    def largest(self):\n        ret = {}\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(max(n_channel))\n        return ret\n\n    def smallest(self):\n        ret = {}\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(min(n_channel))\n        return ret\n\n    def all_configs(self):\n\n        def yield_channels(i):\n            if i == len(self.n_channels):\n                yield []\n                return\n            for n in self.n_channels[i]:\n                for after_channels in yield_channels(i + 1):\n                    yield [n] + after_channels\n\n        for channels in yield_channels(0):\n            yield {'channels': channels}\n\n    def __call__(self, name):\n        assert name in ('largest', 'smallest')\n        if name == 'largest':\n            return self.largest()\n        elif name == 'smallest':\n            return self.smallest()\n        else:\n            raise NotImplementedError\n\n    def __str__(self):\n        ret = ''\n        for attr in self.attributes:\n            ret += 'attr: %s\\n' % str(getattr(self, attr))\n        return ret\n\n    def __len__(self):\n        ret = 1\n        for n_channel in self.n_channels:\n            ret *= len(n_channel)\n\n\ndef get_configs(config_name):\n    if config_name == 'channels-48':\n        return ResnetConfigs(n_channels=[[48, 32], [48, 32], [48, 40, 32],\n                                         [48, 40, 32], [48, 40, 32], [48, 40, 32],\n                                         [48, 32, 24, 16], [48, 32, 24, 16]])\n    elif config_name == 'channels-32':\n        return ResnetConfigs(n_channels=[[32, 24, 16], [32, 24, 16], [32, 24, 16],\n                                         [32, 24, 16], [32, 24, 16], [32, 24, 16],\n                                         [32, 24, 16], [32, 24, 16]])\n    elif config_name == 'debug':\n        return ResnetConfigs(n_channels=[[48, 32], [48, 32], [48, 40, 32],\n                                         [48, 40, 32], [48, 40, 32], [48, 40, 32],\n                                         [48], [48]])\n    elif config_name == 'test':\n        return ResnetConfigs(n_channels=[[8], [6, 8], [6, 8],\n                                         [8], [8], [8],\n                                         [8], [8]])\n    else:\n        raise NotImplementedError('Unknown configuration [%s]!!!' % config_name)\n"""
configs/single_configs.py,0,"b""class SingleConfigs:\n    def __init__(self, config):\n        self.attributes = ['n_channels']\n        self.configs = [config]\n\n    def sample(self):\n        return self.configs[0]\n\n    def largest(self):\n        return self.configs[0]\n\n    def smallest(self):\n        return self.configs[0]\n\n    def all_configs(self):\n        for config in self.configs:\n            yield config\n\n    def __call__(self, name):\n        assert name in ('largest', 'smallest')\n        if name == 'largest':\n            return self.largest()\n        elif name == 'smallest':\n            return self.smallest()\n        else:\n            raise NotImplementedError\n\n    def __str__(self):\n        ret = ''\n        for attr in self.attributes:\n            ret += 'attr: %s\\n' % str(getattr(self, attr))\n        return ret\n\n    def __len__(self):\n        return len(self.configs)\n"""
configs/spade_configs.py,0,"b""import random\n\n\nclass SPADEConfigs:\n    def __init__(self, n_channels):\n        self.attributes = ['n_channels']\n        self.n_channels = n_channels\n\n    def sample(self):\n        ret = {}\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(random.choice(n_channel))\n        return ret\n\n    def largest(self):\n        ret = {}\n\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(max(n_channel))\n        return ret\n\n    def smallest(self):\n        ret = {}\n\n        ret['channels'] = []\n        for n_channel in self.n_channels:\n            ret['channels'].append(min(n_channel))\n        return ret\n\n    def all_configs(self):\n\n        def yield_channels(i):\n            if i == len(self.n_channels):\n                yield []\n                return\n            for n in self.n_channels[i]:\n                for after_channels in yield_channels(i + 1):\n                    yield [n] + after_channels\n\n        for channels in yield_channels(0):\n            yield {'channels': channels}\n\n    def __call__(self, name):\n        assert name in ('largest', 'smallest')\n        if name == 'largest':\n            return self.largest()\n        elif name == 'smallest':\n            return self.smallest()\n        else:\n            raise NotImplementedError\n\n    def __str__(self):\n        ret = ''\n        for attr in self.attributes:\n            ret += 'attr: %s\\n' % str(getattr(self, attr))\n        return ret\n\n\ndef get_configs(config_name):\n    # TODO\n    raise NotImplementedError('Unknown configuration [%s]!!!' % config_name)\n"""
data/__init__.py,2,"b'import copy\nimport importlib\nimport os\n\nimport torch.utils.data\n\nfrom data.base_dataset import BaseDataset\n\n\ndef find_dataset_using_name(dataset_name):\n    dataset_filename = ""data."" + dataset_name + ""_dataset""\n    datasetlib = importlib.import_module(dataset_filename)\n\n    dataset = None\n    target_dataset_name = dataset_name.replace(\'_\', \'\') + \'dataset\'\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() \\\n                and issubclass(cls, BaseDataset):\n            dataset = cls\n\n    if dataset is None:\n        raise NotImplementedError(\n            ""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase."" % (\n                dataset_filename, target_dataset_name))\n\n    return dataset\n\n\ndef get_option_setter(dataset_name):\n    """"""Return the static method <modify_commandline_options> of the dataset class.""""""\n    dataset_class = find_dataset_using_name(dataset_name)\n    return dataset_class.modify_commandline_options\n\n\ndef create_dataloader(opt, verbose=True):\n    """"""Create a dataset given the option.\n\n    This function wraps the class CustomDatasetDataLoader.\n        This is the main interface between this package and \'train.py\'/\'test.py\'\n\n    Example:\n        >>> from data import create_dataloader\n        >>> dataset = create_dataloader(opt)\n    """"""\n    dataloader = CustomDatasetDataLoader(opt, verbose)\n    dataloader = dataloader.load_data()\n    return dataloader\n\n\ndef create_eval_dataloader(opt, direction=None):\n    opt = copy.deepcopy(opt)\n    # Set some evaluation options\n    # opt.prepocess = \'resize_and_crop\'\n    opt.load_size = opt.crop_size\n    opt.no_flip = True\n    opt.serial_batches = True\n    opt.batch_size = opt.eval_batch_size\n    opt.phase = \'val\'\n    if opt.dataset_mode == \'unaligned\':\n        assert direction is not None\n        opt.dataset_mode = \'single\'\n        opt.dataroot = os.path.join(opt.dataroot, \'val%s\' % (direction[0]))\n    dataloader = CustomDatasetDataLoader(opt)\n    dataloader = dataloader.load_data()\n    return dataloader\n\n\nclass CustomDatasetDataLoader():\n    """"""Wrapper class of Dataset class that performs multi-threaded data loading""""""\n\n    def __init__(self, opt, verbose=True):\n        """"""Initialize this class\n\n        Step 1: create a dataset instance given the name [dataset_mode]\n        Step 2: create a multi-threaded data loader.\n        """"""\n        self.opt = opt\n        dataset_class = find_dataset_using_name(opt.dataset_mode)\n        self.dataset = dataset_class(opt)\n        # print(len(self.dataset))\n        if verbose:\n            print(""dataset [%s] was created"" % type(self.dataset).__name__)\n        self.dataloader = torch.utils.data.DataLoader(\n            self.dataset,\n            batch_size=opt.batch_size,\n            shuffle=not opt.serial_batches,\n            num_workers=int(opt.num_threads))\n\n    def load_data(self):\n        return self\n\n    def __len__(self):\n        """"""Return the number of data in the dataset""""""\n        return (len(self.dataset) + self.opt.batch_size - 1) // self.opt.batch_size\n\n    def __iter__(self):\n        """"""Return a batch of data""""""\n        # print(len(self.dataloader))\n        for i, data in enumerate(self.dataloader):\n            yield data\n'"
data/aligned_dataset.py,0,"b'import os.path\n\nfrom PIL import Image\n\nfrom data.base_dataset import BaseDataset, get_params, get_transform\nfrom data.image_folder import make_dataset\n\n\nclass AlignedDataset(BaseDataset):\n    """"""A dataset class for paired image dataset.\n\n    It assumes that the directory \'/path/to/data/train\' contains image pairs in the form of {A,B}.\n    During test time, you need to prepare a directory \'/path/to/data/test\'.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        BaseDataset.__init__(self, opt)\n        self.dir_AB = os.path.join(opt.dataroot, opt.phase)  # get the image directory\n        self.AB_paths = sorted(make_dataset(self.dir_AB))  # get image paths\n        # print(self.AB_paths)\n\n        assert (self.opt.load_size >= self.opt.crop_size)  # crop_size should be smaller than the size of loaded image\n        self.input_nc = self.opt.output_nc if self.opt.direction == \'BtoA\' else self.opt.input_nc\n        self.output_nc = self.opt.input_nc if self.opt.direction == \'BtoA\' else self.opt.output_nc\n        self.cache = {}\n\n    def __getitem__(self, index):\n        AB_path = self.AB_paths[index]\n        if not self.opt.load_in_memory or self.cache.get(index) is None:\n            AB = Image.open(AB_path).convert(\'RGB\')\n            if self.opt.load_in_memory:\n                self.cache[index] = AB\n        else:\n            AB = self.cache[index]\n\n        # split AB image into A and B\n        w, h = AB.size\n        w2 = int(w / 2)\n        A = AB.crop((0, 0, w2, h))\n        B = AB.crop((w2, 0, w, h))\n\n        # apply the same transform to both A and B\n        transform_params = get_params(self.opt, A.size)\n        A_transform = get_transform(self.opt, transform_params, grayscale=(self.input_nc == 1))\n        B_transform = get_transform(self.opt, transform_params, grayscale=(self.output_nc == 1))\n\n        A = A_transform(A)\n        B = B_transform(B)\n        return {\'A\': A, \'B\': B, \'A_paths\': AB_path, \'B_paths\': AB_path}\n\n    def __len__(self):\n        """"""Return the total number of images in the dataset.""""""\n        if self.opt.max_dataset_size == -1:\n            return len(self.AB_paths)\n        else:\n            return self.opt.max_dataset_size\n'"
data/base_dataset.py,1,"b'""""""This module implements an abstract base class (ABC) \'BaseDataset\' for datasets.\n\nIt also includes common transformation functions (e.g., get_transform, __scale_width), which can be later used in subclasses.\n""""""\nimport random\nfrom abc import ABC, abstractmethod\n\nimport numpy as np\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom PIL import Image\n\n\nclass BaseDataset(data.Dataset, ABC):\n    """"""This class is an abstract base class (ABC) for datasets.\n\n    To create a subclass, you need to implement the following four functions:\n    -- <__init__>:                      initialize the class, first call BaseDataset.__init__(self, opt).\n    -- <__len__>:                       return the size of dataset.\n    -- <__getitem__>:                   get a data point.\n    -- <modify_commandline_options>:    (optionally) add dataset-specific options and set default options.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize the class; save the options in the class\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        self.opt = opt\n        self.root = opt.dataroot\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        """"""Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        """"""\n        return parser\n\n    @abstractmethod\n    def __len__(self):\n        """"""Return the total number of images in the dataset.""""""\n        return 0\n\n    @abstractmethod\n    def __getitem__(self, index):\n        """"""Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns:\n            a dictionary of data with their names. It ususally contains the data itself and its metadata information.\n        """"""\n        pass\n\n\ndef get_params(opt, size):\n    w, h = size\n    new_h = h\n    new_w = w\n    if opt.preprocess == \'resize_and_crop\':\n        new_h = new_w = opt.load_size\n    elif opt.preprocess == \'scale_width_and_crop\':\n        new_w = opt.load_size\n        new_h = opt.load_size * h // w\n\n    crop_w = opt.crop_size\n    crop_h = opt.crop_size * h // w\n\n    x = random.randint(0, np.maximum(0, new_w - crop_w))\n    y = random.randint(0, np.maximum(0, new_h - crop_h))\n\n    flip = random.random() > 0.5\n\n    return {\'crop_pos\': (x, y), \'flip\': flip, \'crop_size\': (crop_w, crop_h)}\n\n\ndef get_transform(opt, params=None, grayscale=False, method=Image.BICUBIC, toTensor=True, normalized=True):\n    transform_list = []\n    if grayscale:\n        transform_list.append(transforms.Grayscale(1))\n    if \'resize\' in opt.preprocess:\n        osize = [opt.load_size, opt.load_size]\n        transform_list.append(transforms.Resize(osize, method))\n    elif \'scale_width\' in opt.preprocess:\n        transform_list.append(transforms.Lambda(lambda img: __scale_width(img, opt.load_size, method)))\n    if \'crop\' in opt.preprocess:\n        if params is None:\n            transform_list.append(transforms.RandomCrop(opt.crop_size))\n        else:\n            transform_list.append(transforms.Lambda(lambda img: __crop(img, params[\'crop_pos\'], params[\'crop_size\'])))\n    if opt.preprocess == \'none\':\n        transform_list.append(transforms.Lambda(lambda img: __make_power_2(img, base=4, method=method)))\n    if not opt.no_flip:\n        if params is None:\n            transform_list.append(transforms.RandomHorizontalFlip())\n        elif params[\'flip\']:\n            transform_list.append(transforms.Lambda(lambda img: __flip(img, params[\'flip\'])))\n\n    if toTensor:\n        transform_list += [transforms.ToTensor()]\n\n    if normalized:\n        if grayscale:\n            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n        else:\n            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n    return transforms.Compose(transform_list)\n\n\ndef __resize(img, w, h, method=Image.BICUBIC):\n    return img.resize((w, h), method)\n\n\ndef __make_power_2(img, base, method=Image.BICUBIC):\n    ow, oh = img.size\n    h = int(round(oh / base) * base)\n    w = int(round(ow / base) * base)\n    if (h == oh) and (w == ow):\n        return img\n\n    __print_size_warning(ow, oh, w, h)\n    return img.resize((w, h), method)\n\n\ndef __scale_width(img, target_width, method=Image.BICUBIC):\n    ow, oh = img.size\n    if (ow == target_width):\n        return img\n    w = target_width\n    h = int(target_width * oh / ow)\n    return img.resize((w, h), method)\n\n\ndef __crop(img, pos, size):\n    ow, oh = img.size\n    x1, y1 = pos\n    if isinstance(size, int):\n        tw = th = size\n    else:\n        assert len(size) == 2\n        tw, th = size\n    if (ow > tw or oh > th):\n        return img.crop((x1, y1, x1 + tw, y1 + th))\n    return img\n\n\ndef __flip(img, flip):\n    if flip:\n        return img.transpose(Image.FLIP_LEFT_RIGHT)\n    return img\n\n\ndef __print_size_warning(ow, oh, w, h):\n    """"""Print warning information about image size(only print once)""""""\n    if not hasattr(__print_size_warning, \'has_printed\'):\n        print(""The image size needs to be a multiple of 4. ""\n              ""The loaded image size was (%d, %d), so it was adjusted to ""\n              ""(%d, %d). This adjustment will be done to all images ""\n              ""whose sizes are not multiples of 4"" % (ow, oh, w, h))\n        __print_size_warning.has_printed = True\n'"
data/cityscapes_dataset.py,0,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport argparse\nimport os.path\n\nfrom PIL import Image\n\nfrom utils import util\nfrom .base_dataset import BaseDataset, get_params, get_transform\nfrom .image_folder import make_dataset\n\n\nclass CityscapesDataset(BaseDataset):\n\n    def __init__(self, opt):\n        super(CityscapesDataset, self).__init__(opt)\n        self.initialize(opt)\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser = BaseDataset.modify_commandline_options(parser, is_train)\n        assert isinstance(parser, argparse.ArgumentParser)\n        parser.add_argument(\'--no_pairing_check\', action=\'store_true\',\n                            help=\'If specified, skip sanity check of correct label-image file pairing\')\n        parser.add_argument(\'--no_instance\', action=\'store_true\',\n                            help=\'if specified, do *not* add instance map as input\')\n        parser.add_argument(\'--contain_dontcare_label\', action=\'store_true\',\n                            help=\'if the label map contains dontcare label (dontcare=255)\')\n        parser.set_defaults(preprocess=\'scale_width\', no_flip=True, aspect_ratio=2,\n                            load_size=512, crop_size=512, direction=\'BtoA\',\n                            display_winsize=512, input_nc=35, num_threads=0)\n        return parser\n\n    def get_paths(self, opt):\n        root = opt.dataroot\n        phase = opt.phase\n\n        label_dir = os.path.join(root, \'gtFine\', phase)\n        label_paths_all = make_dataset(label_dir, recursive=True)\n        label_paths = [p for p in label_paths_all if p.endswith(\'_labelIds.png\')]\n\n        image_dir = os.path.join(root, \'leftImg8bit\', phase)\n        image_paths = make_dataset(image_dir, recursive=True)\n\n        if not opt.no_instance:\n            instance_paths = [p for p in label_paths_all if p.endswith(\'_instanceIds.png\')]\n        else:\n            instance_paths = []\n\n        return label_paths, image_paths, instance_paths\n\n    def paths_match(self, path1, path2):\n        name1 = os.path.basename(path1)\n        name2 = os.path.basename(path2)\n        # compare the first 3 components, [city]_[id1]_[id2]\n        return \'_\'.join(name1.split(\'_\')[:3]) == \'_\'.join(name2.split(\'_\')[:3])\n\n    def __getitem__(self, index):\n        # Label Image\n        label_path = self.label_paths[index]\n        if not self.opt.load_in_memory or self.label_cache.get(index) is None:\n            label = Image.open(label_path)\n            if self.opt.load_in_memory:\n                self.label_cache[index] = label\n        else:\n            label = self.label_cache[index]\n        params = get_params(self.opt, label.size)\n        transform_label = get_transform(self.opt, params, method=Image.NEAREST, normalized=False)\n        label_tensor = transform_label(label) * 255.0\n        label_tensor[label_tensor == 255] = self.opt.input_nc  # \'unknown\' is opt.input_nc\n\n        # input image (real images)\n        image_path = self.image_paths[index]\n        assert self.paths_match(label_path, image_path), \\\n            ""The label_path %s and image_path %s don\'t match."" % \\\n            (label_path, image_path)\n        if not self.opt.load_in_memory or self.image_cache.get(index) is None:\n            image = Image.open(image_path)\n            if self.opt.load_in_memory:\n                self.image_cache[index] = image\n        else:\n            image = self.image_cache[index]\n        image = image.convert(\'RGB\')\n\n        transform_image = get_transform(self.opt, params)\n        image_tensor = transform_image(image)\n\n        # if using instance maps\n        if self.opt.no_instance:\n            instance_tensor = 0\n        else:\n            instance_path = self.instance_paths[index]\n            if not self.opt.load_in_memory or self.instance_cache.get(index) is None:\n                instance = Image.open(instance_path)\n                if self.opt.load_in_memory:\n                    self.instance_cache[index] = instance\n            else:\n                instance = self.instance_cache[index]\n            if instance.mode == \'L\':\n                instance_tensor = transform_label(instance) * 255\n                instance_tensor = instance_tensor.long()\n            else:\n                instance_tensor = transform_label(instance)\n\n        input_dict = {\n            \'label\': label_tensor,\n            \'instance\': instance_tensor,\n            \'image\': image_tensor,\n            \'path\': image_path,\n        }\n\n        # Give subclasses a chance to modify the final output\n        self.postprocess(input_dict)\n\n        return input_dict\n\n    def initialize(self, opt):\n        self.opt = opt\n\n        label_paths, image_paths, instance_paths = self.get_paths(opt)\n\n        util.natural_sort(label_paths)\n        util.natural_sort(image_paths)\n        if not opt.no_instance:\n            util.natural_sort(instance_paths)\n\n        if opt.max_dataset_size > 0:\n            label_paths = label_paths[:opt.max_dataset_size]\n            image_paths = image_paths[:opt.max_dataset_size]\n            instance_paths = instance_paths[:opt.max_dataset_size]\n\n        if not opt.no_pairing_check:\n            for path1, path2 in zip(label_paths, image_paths):\n                assert self.paths_match(path1, path2), \\\n                    ""The label-image pair (%s, %s) do not look like the right pair because the filenames are quite different. Are you sure about the pairing? Please see data/pix2pix_dataset.py to see what is going on, and use --no_pairing_check to bypass this."" % (\n                        path1, path2)\n\n        self.label_paths = label_paths\n        self.image_paths = image_paths\n        self.instance_paths = instance_paths\n\n        size = len(self.label_paths)\n        self.dataset_size = size\n        self.label_cache = {}\n        self.image_cache = {}\n        self.instance_cache = {}\n\n    def postprocess(self, input_dict):\n        return input_dict\n\n    def __len__(self):\n        return self.dataset_size\n'"
data/image_folder.py,0,"b'""""""A modified image folder class\n\nWe modify the official PyTorch image folder (https://github.com/pytorch/vision/blob/master/torchvision/datasets/folder.py)\nso that this class can load images from both current directory and its subdirectories.\n""""""\n\nimport os\nimport os.path\n\nfrom PIL import Image\n\nIMG_EXTENSIONS = [\n    \'.jpg\', \'.JPG\', \'.jpeg\', \'.JPEG\',\n    \'.png\', \'.PNG\', \'.ppm\', \'.PPM\', \'.bmp\', \'.BMP\',\n]\n\n\ndef is_image_file(filename):\n    return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\n\ndef make_dataset_rec(dir, images):\n    assert os.path.isdir(dir), \'%s is not a valid directory\' % dir\n\n    for root, dnames, fnames in sorted(os.walk(dir, followlinks=True)):\n        for fname in fnames:\n            if is_image_file(fname):\n                path = os.path.join(root, fname)\n                images.append(path)\n\n\ndef make_dataset(dir, max_dataset_size=float(""inf""), recursive=False, read_cache=False, write_cache=False):\n    images = []\n\n    if read_cache:\n        possible_filelist = os.path.join(dir, \'files.list\')\n        if os.path.isfile(possible_filelist):\n            with open(possible_filelist, \'r\') as f:\n                images = f.read().splitlines()\n                return images\n\n    if recursive:\n        make_dataset_rec(dir, images)\n    else:\n        assert os.path.isdir(dir) or os.path.islink(dir), \'%s is not a valid directory\' % dir\n\n        for root, dnames, fnames in sorted(os.walk(dir)):\n            for fname in fnames:\n                if is_image_file(fname):\n                    path = os.path.join(root, fname)\n                    images.append(path)\n\n    if write_cache:\n        filelist_cache = os.path.join(dir, \'files.list\')\n        with open(filelist_cache, \'w\') as f:\n            for path in images:\n                f.write(""%s\\n"" % path)\n            print(\'wrote filelist cache at %s\' % filelist_cache)\n    return images[:min(max_dataset_size, len(images))]\n\n\ndef default_loader(path):\n    return Image.open(path).convert(\'RGB\')\n'"
data/single_dataset.py,0,"b'from PIL import Image\n\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\n\n\nclass SingleDataset(BaseDataset):\n    """"""This dataset class can load a set of images specified by the path --dataroot /path/to/data.\n\n    It can be used for generating CycleGAN results only for one side with the model option \'-model test\'.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        BaseDataset.__init__(self, opt)\n        self.A_paths = sorted(make_dataset(opt.dataroot, opt.max_dataset_size))\n        input_nc = self.opt.output_nc if self.opt.direction == \'BtoA\' else self.opt.input_nc\n        self.transform = get_transform(opt, grayscale=(input_nc == 1))\n\n    def __getitem__(self, index):\n        """"""Return a data point and its metadata information.\n\n        Parameters:\n            index - - a random integer for data indexing\n\n        Returns a dictionary that contains A and A_paths\n            A(tensor) - - an image in one domain\n            A_paths(str) - - the path of the image\n        """"""\n        A_path = self.A_paths[index]\n        A_img = Image.open(A_path).convert(\'RGB\')\n        A = self.transform(A_img)\n        return {\'A\': A, \'A_paths\': A_path}\n\n    def __len__(self):\n        """"""Return the total number of images in the dataset.""""""\n        if self.opt.max_dataset_size == -1:\n            return len(self.A_paths)\n        else:\n            return self.opt.max_dataset_size\n'"
data/template_dataset.py,0,"b'""""""Dataset class template\n\nThis module provides a template for users to implement custom datasets.\nYou can specify \'--dataset_mode template\' to use this dataset.\nThe class name should be consistent with both the filename and its dataset_mode option.\nThe filename should be <dataset_mode>_dataset.py\nThe class name should be <Dataset_mode>Dataset.py\nYou need to implement the following functions:\n    -- <modify_commandline_options>:\xe3\x80\x80Add dataset-specific options and rewrite default values for existing options.\n    -- <__init__>: Initialize this dataset class.\n    -- <__getitem__>: Return a data point and its metadata information.\n    -- <__len__>: Return the number of images.\n""""""\nfrom data.base_dataset import BaseDataset, get_transform\n\n\n# from data.image_folder import make_dataset\n# from PIL import Image\n\n\nclass TemplateDataset(BaseDataset):\n    """"""A template dataset class for you to implement custom datasets.""""""\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        """"""Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        """"""\n        parser.add_argument(\'--new_dataset_option\', type=float, default=1.0, help=\'new dataset option\')\n        parser.set_defaults(max_dataset_size=10, new_dataset_option=2.0)  # specify dataset-specific default values\n        return parser\n\n    def __init__(self, opt):\n        """"""Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n\n        A few things can be done here.\n        - save the options (have been done in BaseDataset)\n        - get image paths and meta information of the dataset.\n        - define the image transformation.\n        """"""\n        # save the option and dataset root\n        BaseDataset.__init__(self, opt)\n        # get the image paths of your dataset;\n        self.image_paths = []  # You can call sorted(make_dataset(self.root, opt.max_dataset_size)) to get all the image paths under the directory self.root\n        # define the default transform function. You can use <base_dataset.get_transform>; You can also define your custom transform function\n        self.transform = get_transform(opt)\n\n    def __getitem__(self, index):\n        """"""Return a data point and its metadata information.\n\n        Parameters:\n            index -- a random integer for data indexing\n\n        Returns:\n            a dictionary of data with their names. It usually contains the data itself and its metadata information.\n\n        Step 1: get a random image path: e.g., path = self.image_paths[index]\n        Step 2: load your data from the disk: e.g., image = Image.open(path).convert(\'RGB\').\n        Step 3: convert your data to a PyTorch tensor. You can use helpder functions such as self.transform. e.g., data = self.transform(image)\n        Step 4: return a data point as a dictionary.\n        """"""\n        path = \'temp\'  # needs to be a string\n        data_A = None  # needs to be a tensor\n        data_B = None  # needs to be a tensor\n        return {\'data_A\': data_A, \'data_B\': data_B, \'path\': path}\n\n    def __len__(self):\n        """"""Return the total number of images.""""""\n        return len(self.image_paths)\n'"
data/unaligned_dataset.py,0,"b'import os.path\nimport random\n\nfrom PIL import Image\n\nfrom data.base_dataset import BaseDataset, get_transform\nfrom data.image_folder import make_dataset\n\n\nclass UnalignedDataset(BaseDataset):\n    """"""\n    This dataset class can load unaligned/unpaired datasets.\n\n    It requires two directories to host training images from domain A \'/path/to/data/trainA\'\n    and from domain B \'/path/to/data/trainB\' respectively.\n    You can train the model with the dataset flag \'--dataroot /path/to/data\'.\n    Similarly, you need to prepare two directories:\n    \'/path/to/data/testA\' and \'/path/to/data/testB\' during test time.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize this dataset class.\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        BaseDataset.__init__(self, opt)\n\n        self.dir_A = os.path.join(opt.dataroot, opt.phase + \'A\')  # create a path \'/path/to/data/trainA\'\n        self.dir_B = os.path.join(opt.dataroot, opt.phase + \'B\')  # create a path \'/path/to/data/trainB\'\n\n        self.A_paths = sorted(make_dataset(self.dir_A, opt.max_dataset_size))  # load images from \'/path/to/data/trainA\'\n        self.B_paths = sorted(make_dataset(self.dir_B, opt.max_dataset_size))  # load images from \'/path/to/data/trainB\'\n        self.A_size = len(self.A_paths)  # get the size of dataset A\n        self.B_size = len(self.B_paths)  # get the size of dataset B\n        btoA = self.opt.direction == \'BtoA\'\n        input_nc = self.opt.output_nc if btoA else self.opt.input_nc  # get the number of channels of input image\n        output_nc = self.opt.input_nc if btoA else self.opt.output_nc  # get the number of channels of output image\n        self.transform_A = get_transform(self.opt, grayscale=(input_nc == 1))\n        self.transform_B = get_transform(self.opt, grayscale=(output_nc == 1))\n\n    def __getitem__(self, index):\n        """"""Return a data point and its metadata information.\n\n        Parameters:\n            index (int)      -- a random integer for data indexing\n\n        Returns a dictionary that contains A, B, A_paths and B_paths\n            A (tensor)       -- an image in the input domain\n            B (tensor)       -- its corresponding image in the target domain\n            A_paths (str)    -- image paths\n            B_paths (str)    -- image paths\n        """"""\n        A_path = self.A_paths[index % self.A_size]  # make sure index is within then range\n        if self.opt.serial_batches:  # make sure index is within then range\n            index_B = index % self.B_size\n        else:  # randomize the index for domain B to avoid fixed pairs.\n            index_B = random.randint(0, self.B_size - 1)\n        B_path = self.B_paths[index_B]\n        A_img = Image.open(A_path).convert(\'RGB\')\n        B_img = Image.open(B_path).convert(\'RGB\')\n        # apply image transformation\n        A = self.transform_A(A_img)\n        B = self.transform_B(B_img)\n\n        return {\'A\': A, \'B\': B, \'A_paths\': A_path, \'B_paths\': B_path}\n\n    def __len__(self):\n        """"""Return the total number of images in the dataset.\n\n        As we have two datasets with potentially different number of images,\n        we take a maximum of\n        """"""\n        return max(self.A_size, self.B_size)\n'"
datasets/combine_A_and_B.py,0,"b""import argparse\nimport os\n\nimport cv2\nimport numpy as np\n\nparser = argparse.ArgumentParser('create image pairs')\nparser.add_argument('--fold_A', dest='fold_A', help='input directory for image A', type=str,\n                    default='../dataset/50kshoes_edges')\nparser.add_argument('--fold_B', dest='fold_B', help='input directory for image B', type=str,\n                    default='../dataset/50kshoes_jpg')\nparser.add_argument('--fold_AB', dest='fold_AB', help='output directory', type=str, default='../dataset/test_AB')\nparser.add_argument('--num_imgs', dest='num_imgs', help='number of images', type=int, default=1000000)\nparser.add_argument('--use_AB', dest='use_AB', help='if true: (0001_A, 0001_B) to (0001_AB)', action='store_true')\nargs = parser.parse_args()\n\nfor arg in vars(args):\n    print('[%s] = ' % arg, getattr(args, arg))\n\nsplits = os.listdir(args.fold_A)\n\nfor sp in splits:\n    img_fold_A = os.path.join(args.fold_A, sp)\n    img_fold_B = os.path.join(args.fold_B, sp)\n    img_list = os.listdir(img_fold_A)\n    if args.use_AB:\n        img_list = [img_path for img_path in img_list if '_A.' in img_path]\n\n    num_imgs = min(args.num_imgs, len(img_list))\n    print('split = %s, use %d/%d images' % (sp, num_imgs, len(img_list)))\n    img_fold_AB = os.path.join(args.fold_AB, sp)\n    if not os.path.isdir(img_fold_AB):\n        os.makedirs(img_fold_AB)\n    print('split = %s, number of images = %d' % (sp, num_imgs))\n    for n in range(num_imgs):\n        name_A = img_list[n]\n        path_A = os.path.join(img_fold_A, name_A)\n        if args.use_AB:\n            name_B = name_A.replace('_A.', '_B.')\n        else:\n            name_B = name_A\n        path_B = os.path.join(img_fold_B, name_B)\n        if os.path.isfile(path_A) and os.path.isfile(path_B):\n            name_AB = name_A\n            if args.use_AB:\n                name_AB = name_AB.replace('_A.', '.')  # remove _A\n            path_AB = os.path.join(img_fold_AB, name_AB)\n            im_A = cv2.imread(path_A, 1)  # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n            im_B = cv2.imread(path_B, 1)  # python2: cv2.CV_LOAD_IMAGE_COLOR; python3: cv2.IMREAD_COLOR\n            im_AB = np.concatenate([im_A, im_B], 1)\n            cv2.imwrite(path_AB, im_AB)\n"""
datasets/get_trainIds.py,0,"b""# Copy from https://github.com/fyu/drn/blob/master/datasets/cityscapes/prepare_data.py\nimport os\nimport sys\nfrom collections import namedtuple\nfrom os.path import join, split, exists\n\nimport numpy as np\nfrom PIL import Image\n\n# a label and all meta information\nLabel = namedtuple('Label', [\n\n    'name',  # The identifier of this label, e.g. 'car', 'person', ... .\n    # We use them to uniquely name a class\n\n    'id',  # An integer ID that is associated with this label.\n    # The IDs are used to represent the label in ground truth images\n    # An ID of -1 means that this label does not have an ID and thus\n    # is ignored when creating ground truth images (e.g. license plate).\n    # Do not modify these IDs, since exactly these IDs are expected by the\n    # evaluation server.\n\n    'trainId',  # Feel free to modify these IDs as suitable for your method. Then create\n    # ground truth images with train IDs, using the tools provided in the\n    # 'preparation' folder. However, make sure to validate or submit results\n    # to our evaluation server using the regular IDs above!\n    # For trainIds, multiple labels might have the same ID. Then, these labels\n    # are mapped to the same class in the ground truth images. For the inverse\n    # mapping, we use the label that is defined first in the list below.\n    # For example, mapping all void-type classes to the same ID in training,\n    # might make sense for some approaches.\n    # Max value is 255!\n\n    'category',  # The name of the category that this label belongs to\n\n    'categoryId',  # The ID of this category. Used to create ground truth images\n    # on category level.\n\n    'hasInstances',  # Whether this label distinguishes between single instances or not\n\n    'ignoreInEval',  # Whether pixels having this class as ground truth label are ignored\n    # during evaluations or not\n\n    'color',  # The color of this label\n])\n\nlabels = [\n    #       name                     id    trainId   category            catId     hasInstances   ignoreInEval   color\n    Label('unlabeled', 0, 255, 'void', 0, False, True, (0, 0, 0)),\n    Label('ego vehicle', 1, 255, 'void', 0, False, True, (0, 0, 0)),\n    Label('rectification border', 2, 255, 'void', 0, False, True, (0, 0, 0)),\n    Label('out of roi', 3, 255, 'void', 0, False, True, (0, 0, 0)),\n    Label('static', 4, 255, 'void', 0, False, True, (0, 0, 0)),\n    Label('dynamic', 5, 255, 'void', 0, False, True, (111, 74, 0)),\n    Label('ground', 6, 255, 'void', 0, False, True, (81, 0, 81)),\n    Label('road', 7, 0, 'flat', 1, False, False, (128, 64, 128)),\n    Label('sidewalk', 8, 1, 'flat', 1, False, False, (244, 35, 232)),\n    Label('parking', 9, 255, 'flat', 1, False, True, (250, 170, 160)),\n    Label('rail track', 10, 255, 'flat', 1, False, True, (230, 150, 140)),\n    Label('building', 11, 2, 'construction', 2, False, False, (70, 70, 70)),\n    Label('wall', 12, 3, 'construction', 2, False, False, (102, 102, 156)),\n    Label('fence', 13, 4, 'construction', 2, False, False, (190, 153, 153)),\n    Label('guard rail', 14, 255, 'construction', 2, False, True, (180, 165, 180)),\n    Label('bridge', 15, 255, 'construction', 2, False, True, (150, 100, 100)),\n    Label('tunnel', 16, 255, 'construction', 2, False, True, (150, 120, 90)),\n    Label('pole', 17, 5, 'object', 3, False, False, (153, 153, 153)),\n    Label('polegroup', 18, 255, 'object', 3, False, True, (153, 153, 153)),\n    Label('traffic light', 19, 6, 'object', 3, False, False, (250, 170, 30)),\n    Label('traffic sign', 20, 7, 'object', 3, False, False, (220, 220, 0)),\n    Label('vegetation', 21, 8, 'nature', 4, False, False, (107, 142, 35)),\n    Label('terrain', 22, 9, 'nature', 4, False, False, (152, 251, 152)),\n    Label('sky', 23, 10, 'sky', 5, False, False, (70, 130, 180)),\n    Label('person', 24, 11, 'human', 6, True, False, (220, 20, 60)),\n    Label('rider', 25, 12, 'human', 6, True, False, (255, 0, 0)),\n    Label('car', 26, 13, 'vehicle', 7, True, False, (0, 0, 142)),\n    Label('truck', 27, 14, 'vehicle', 7, True, False, (0, 0, 70)),\n    Label('bus', 28, 15, 'vehicle', 7, True, False, (0, 60, 100)),\n    Label('caravan', 29, 255, 'vehicle', 7, True, True, (0, 0, 90)),\n    Label('trailer', 30, 255, 'vehicle', 7, True, True, (0, 0, 110)),\n    Label('train', 31, 16, 'vehicle', 7, True, False, (0, 80, 100)),\n    Label('motorcycle', 32, 17, 'vehicle', 7, True, False, (0, 0, 230)),\n    Label('bicycle', 33, 18, 'vehicle', 7, True, False, (119, 11, 32)),\n    Label('license plate', -1, -1, 'vehicle', 7, False, True, (0, 0, 142)),\n]\n\n\ndef label2id(image):\n    array = np.array(image)\n    out_array = np.empty(array.shape, dtype=array.dtype)\n    for l in labels:\n        if 0 <= l.trainId < 255:\n            out_array[array == l.trainId] = l.id\n    return Image.fromarray(out_array)\n\n\ndef id2label(image):\n    array = np.array(image)\n    out_array = np.empty(array.shape, dtype=array.dtype)\n    for l in labels:\n        out_array[array == l.id] = l.trainId\n    return Image.fromarray(out_array)\n\n\ndef prepare_cityscape_submission(in_dir):\n    our_dir = in_dir + '_id'\n    for root, dirs, filenames in os.walk(in_dir):\n        for name in filenames:\n            in_path = join(root, name)\n            out_path = join(root.replace(in_dir, our_dir), name)\n            file_dir = split(out_path)[0]\n            if not exists(file_dir):\n                os.makedirs(file_dir)\n            image = Image.open(in_path)\n            id_map = label2id(image)\n            print('Writing', out_path)\n            id_map.save(out_path)\n\n\ndef prepare_cityscape_training(in_dir):\n    for root, dirs, filenames in os.walk(in_dir):\n        for name in filenames:\n            parts = name.split('_')\n            if parts[-1] != 'labelIds.png':\n                continue\n            parts[-1] = 'trainIds.png'\n            out_name = '_'.join(parts)\n            in_path = join(root, name)\n            out_path = join(root, out_name)\n            image = Image.open(in_path)\n            id_map = id2label(image)\n            print('Writing', out_path)\n            id_map.save(out_path)\n\n\nif __name__ == '__main__':\n    prepare_cityscape_training(sys.argv[1])\n"""
datasets/make_dataset_aligned.py,0,"b'import os\n\nfrom PIL import Image\n\n\ndef get_file_paths(folder):\n    image_file_paths = []\n    for root, dirs, filenames in os.walk(folder):\n        filenames = sorted(filenames)\n        for filename in filenames:\n            input_path = os.path.abspath(root)\n            file_path = os.path.join(input_path, filename)\n            if filename.endswith(\'.png\') or filename.endswith(\'.jpg\'):\n                image_file_paths.append(file_path)\n\n        break  # prevent descending into subfolders\n    return image_file_paths\n\n\ndef align_images(a_file_paths, b_file_paths, target_path):\n    if not os.path.exists(target_path):\n        os.makedirs(target_path)\n\n    for i in range(len(a_file_paths)):\n        img_a = Image.open(a_file_paths[i])\n        img_b = Image.open(b_file_paths[i])\n        assert(img_a.size == img_b.size)\n\n        aligned_image = Image.new(""RGB"", (img_a.size[0] * 2, img_a.size[1]))\n        aligned_image.paste(img_a, (0, 0))\n        aligned_image.paste(img_b, (img_a.size[0], 0))\n        aligned_image.save(os.path.join(target_path, \'{:04d}.jpg\'.format(i)))\n\n\nif __name__ == \'__main__\':\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--dataset-path\',\n        dest=\'dataset_path\',\n        help=\'Which folder to process (it should have subfolders testA, testB, trainA and trainB\'\n    )\n    args = parser.parse_args()\n\n    dataset_folder = args.dataset_path\n    print(dataset_folder)\n\n    test_a_path = os.path.join(dataset_folder, \'testA\')\n    test_b_path = os.path.join(dataset_folder, \'testB\')\n    test_a_file_paths = get_file_paths(test_a_path)\n    test_b_file_paths = get_file_paths(test_b_path)\n    assert(len(test_a_file_paths) == len(test_b_file_paths))\n    test_path = os.path.join(dataset_folder, \'test\')\n\n    train_a_path = os.path.join(dataset_folder, \'trainA\')\n    train_b_path = os.path.join(dataset_folder, \'trainB\')\n    train_a_file_paths = get_file_paths(train_a_path)\n    train_b_file_paths = get_file_paths(train_b_path)\n    assert(len(train_a_file_paths) == len(train_b_file_paths))\n    train_path = os.path.join(dataset_folder, \'train\')\n\n    align_images(test_a_file_paths, test_b_file_paths, test_path)\n    align_images(train_a_file_paths, train_b_file_paths, train_path)\n'"
datasets/prepare_cityscapes_dataset.py,0,"b'import glob\nimport os\n\nfrom PIL import Image\nfrom tqdm import tqdm\n\nhelp_msg = """"""\nThe dataset can be downloaded from https://cityscapes-dataset.com.\nPlease download the datasets [gtFine_trainvaltest.zip] and [leftImg8bit_trainvaltest.zip] and unzip them.\ngtFine contains the semantics segmentations. Use --gtFine_dir to specify the path to the unzipped gtFine_trainvaltest directory.\nleftImg8bit contains the dashcam photographs. Use --leftImg8bit_dir to specify the path to the unzipped leftImg8bit_trainvaltest directory.\nThe processed images will be placed at --output_dir.\n\nExample usage:\n\npython prepare_cityscapes_dataset.py --gitFine_dir ./gtFine/ --leftImg8bit_dir ./leftImg8bit --output_dir ./datasets/cityscapes/\n""""""\n\n\ndef load_resized_img(path):\n    return Image.open(path).convert(\'RGB\').resize((256, 256))\n\n\ndef check_matching_pair(segmap_path, photo_path):\n    segmap_identifier = os.path.basename(segmap_path).replace(\'_gtFine_color\', \'\')\n    photo_identifier = os.path.basename(photo_path).replace(\'_leftImg8bit\', \'\')\n\n    assert segmap_identifier == photo_identifier, \\\n        ""[%s] and [%s] don\'t seem to be matching. Aborting."" % (segmap_path, photo_path)\n\n\ndef process_cityscapes(gtFine_dir, leftImg8bit_dir, output_dir, phase, table_path=None):\n    save_phase = phase\n    savedir = os.path.join(output_dir, save_phase)\n    os.makedirs(savedir, exist_ok=True)\n    os.makedirs(savedir + \'A\', exist_ok=True)\n    os.makedirs(savedir + \'B\', exist_ok=True)\n    print(""Directory structure prepared at %s"" % output_dir)\n\n    segmap_expr = os.path.join(gtFine_dir, phase) + ""/*/*_color.png""\n    segmap_paths = glob.glob(segmap_expr)\n    segmap_paths = sorted(segmap_paths)\n\n    photo_expr = os.path.join(leftImg8bit_dir, phase) + ""/*/*_leftImg8bit.png""\n    photo_paths = glob.glob(photo_expr)\n    photo_paths = sorted(photo_paths)\n\n    assert len(segmap_paths) == len(photo_paths), \\\n        ""%d images that match [%s], and %d images that match [%s]. Aborting."" % (\n            len(segmap_paths), segmap_expr, len(photo_paths), photo_expr)\n\n    if table_path is not None:\n        f = open(table_path, \'w\')\n    else:\n        f = None\n\n    for i, (segmap_path, photo_path) in enumerate(tqdm(zip(segmap_paths, photo_paths))):\n        check_matching_pair(segmap_path, photo_path)\n        segmap = load_resized_img(segmap_path)\n        photo = load_resized_img(photo_path)\n\n        # data for pix2pix where the two images are placed side-by-side\n        sidebyside = Image.new(\'RGB\', (512, 256))\n        sidebyside.paste(segmap, (256, 0))\n        sidebyside.paste(photo, (0, 0))\n        savepath = os.path.join(savedir, ""%d.jpg"" % i)\n        sidebyside.save(savepath, format=\'JPEG\', subsampling=0, quality=100)\n\n        # data for cycle_gan where the two images are stored at two distinct directories\n        savepath = os.path.join(savedir + \'A\', ""%d_A.jpg"" % i)\n        photo.save(savepath, format=\'JPEG\', subsampling=0, quality=100)\n        savepath = os.path.join(savedir + \'B\', ""%d_B.jpg"" % i)\n        segmap.save(savepath, format=\'JPEG\', subsampling=0, quality=100)\n\n        if f is not None:\n            rel_segmap_path = os.path.relpath(segmap_path, os.path.dirname(os.path.abspath(gtFine_dir)))\n            rel_photo_path = os.path.relpath(photo_path, os.path.dirname(os.path.abspath(leftImg8bit_dir)))\n            f.write(\'%d %s %s\\n\' % (i, rel_segmap_path.replace(\'_color\', \'_trainIds\'), rel_photo_path))\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gtFine_dir\', type=str, required=True,\n                        help=\'Path to the Cityscapes gtFine directory.\')\n    parser.add_argument(\'--leftImg8bit_dir\', type=str, required=True,\n                        help=\'Path to the Cityscapes leftImg8bit_trainvaltest directory.\')\n    parser.add_argument(\'--output_dir\', type=str, required=True,\n                        default=\'database/cityscapes-origin\',\n                        help=\'Directory the output images will be written to.\')\n    parser.add_argument(\'--table_path\', type=str, default=\'datasets/table.txt\',\n                        help=\'Generate a mapping table to map the generated images to the original images. \'\n                             \'The table will be used for mAP computation.\')\n    opt = parser.parse_args()\n\n    print(help_msg)\n\n    print(\'Preparing Cityscapes Dataset for val phase\')\n    process_cityscapes(opt.gtFine_dir, opt.leftImg8bit_dir,\n                       opt.output_dir, ""val"", table_path=opt.table_path)\n    print(\'Preparing Cityscapes Dataset for train phase\')\n    process_cityscapes(opt.gtFine_dir, opt.leftImg8bit_dir,\n                       opt.output_dir, ""train"")\n\n    print(\'Done\')\n'"
distillers/__init__.py,0,"b'import importlib\n\n\ndef find_distiller_using_name(distiller_name):\n    distiller_filename = ""distillers."" + distiller_name + \'_distiller\'\n    # print(distiller_filename)\n    modellib = importlib.import_module(distiller_filename)\n    distiller = None\n    target_distiller_name = distiller_name.replace(\'_\', \'\') + \'distiller\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_distiller_name.lower():\n            distiller = cls\n\n    if distiller is None:\n        print(""In %s.py, there should be a class of Distiller with class name that matches %s in lowercase."" %\n              (distiller_filename, target_distiller_name))\n        exit(0)\n\n    return distiller\n\n\ndef get_option_setter(distiller_name):\n    distiller_class = find_distiller_using_name(distiller_name)\n    return distiller_class.modify_commandline_options\n\n\ndef create_distiller(opt, verbose=True):\n    distiller = find_distiller_using_name(opt.distiller)\n    instance = distiller(opt)\n    if verbose:\n        print(""distiller [%s] was created"" % type(instance).__name__)\n    return instance\n'"
distillers/base_resnet_distiller.py,18,"b""import itertools\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\n\nimport models.modules.loss\nfrom data import create_eval_dataloader\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import networks\nfrom models.base_model import BaseModel\nfrom models.modules.super_modules import SuperConv2d\nfrom utils import util\n\n\nclass BaseResnetDistiller(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert is_train\n        parser = super(BaseResnetDistiller, BaseResnetDistiller).modify_commandline_options(parser, is_train)\n        parser.add_argument('--teacher_netG', type=str, default='mobile_resnet_9blocks',\n                            help='specify teacher generator architecture '\n                                 '[resnet_9blocks | mobile_resnet_9blocks | super_mobile_resnet_9blocks]')\n        parser.add_argument('--student_netG', type=str, default='mobile_resnet_9blocks',\n                            help='specify student generator architecture '\n                                 '[resnet_9blocks | mobile_resnet_9blocks | super_mobile_resnet_9blocks]')\n        parser.add_argument('--teacher_ngf', type=int, default=64,\n                            help='the base number of filters of the teacher generator')\n        parser.add_argument('--student_ngf', type=int, default=48,\n                            help='the base number of filters of the student generator')\n        parser.add_argument('--restore_teacher_G_path', type=str, required=True,\n                            help='the path to restore the teacher generator')\n        parser.add_argument('--restore_student_G_path', type=str, default=None,\n                            help='the path to restore the student generator')\n        parser.add_argument('--restore_A_path', type=str, default=None,\n                            help='the path to restore the adaptors for distillation')\n        parser.add_argument('--restore_D_path', type=str, default=None,\n                            help='the path to restore the discriminator')\n        parser.add_argument('--restore_O_path', type=str, default=None,\n                            help='the path to restore the optimizer')\n        parser.add_argument('--recon_loss_type', type=str, default='l1',\n                            choices=['l1', 'l2', 'smooth_l1', 'vgg'],\n                            help='the type of the reconstruction loss')\n        parser.add_argument('--lambda_distill', type=float, default=1,\n                            help='weights for the intermediate activation distillation loss')\n        parser.add_argument('--lambda_recon', type=float, default=100,\n                            help='weights for the reconstruction loss.')\n        parser.add_argument('--lambda_gan', type=float, default=1,\n                            help='weight for gan loss')\n        parser.add_argument('--teacher_dropout_rate', type=float, default=0)\n        parser.add_argument('--student_dropout_rate', type=float, default=0)\n        parser.add_argument('--gan_mode', type=str, default='hinge', choices=['lsgan', 'vanilla', 'hinge'],\n                            help='the type of GAN objective. [vanilla| lsgan | hinge]. '\n                                 'vanilla GAN loss is the cross-entropy objective used in the original GAN paper.')\n        return parser\n\n    def __init__(self, opt):\n        assert opt.isTrain\n        super(BaseResnetDistiller, self).__init__(opt)\n        self.loss_names = ['G_gan', 'G_distill', 'G_recon', 'D_fake', 'D_real']\n        self.optimizers = []\n        self.image_paths = []\n        self.visual_names = ['real_A', 'Sfake_B', 'Tfake_B', 'real_B']\n        self.model_names = ['netG_student', 'netG_teacher', 'netD']\n        self.netG_teacher = networks.define_G(opt.input_nc, opt.output_nc, opt.teacher_ngf,\n                                              opt.teacher_netG, opt.norm, opt.teacher_dropout_rate,\n                                              opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n        self.netG_student = networks.define_G(opt.input_nc, opt.output_nc, opt.student_ngf,\n                                              opt.student_netG, opt.norm, opt.student_dropout_rate,\n                                              opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n        if hasattr(opt, 'distiller'):\n            self.netG_pretrained = networks.define_G(opt.input_nc, opt.output_nc, opt.pretrained_ngf,\n                                                     opt.pretrained_netG, opt.norm, 0,\n                                                     opt.init_type, opt.init_gain, self.gpu_ids, opt=opt)\n\n        if opt.dataset_mode == 'aligned':\n            self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n        elif opt.dataset_mode == 'unaligned':\n            self.netD = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n                                          opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n        else:\n            raise NotImplementedError('Unknown dataset mode [%s]!!!' % opt.dataset_mode)\n\n        self.netG_teacher.eval()\n        self.criterionGAN = models.modules.loss.GANLoss(opt.gan_mode).to(self.device)\n        if opt.recon_loss_type == 'l1':\n            self.criterionRecon = torch.nn.L1Loss()\n        elif opt.recon_loss_type == 'l2':\n            self.criterionRecon = torch.nn.MSELoss()\n        elif opt.recon_loss_type == 'smooth_l1':\n            self.criterionRecon = torch.nn.SmoothL1Loss()\n        elif opt.recon_loss_type == 'vgg':\n            self.criterionRecon = models.modules.loss.VGGLoss(self.device)\n        else:\n            raise NotImplementedError('Unknown reconstruction loss type [%s]!' % opt.loss_type)\n\n        self.mapping_layers = ['module.model.%d' % i for i in range(9, 21, 3)]\n\n        self.netAs = []\n        self.Tacts, self.Sacts = {}, {}\n\n        G_params = [self.netG_student.parameters()]\n        for i, n in enumerate(self.mapping_layers):\n            ft, fs = self.opt.teacher_ngf, self.opt.student_ngf\n            if hasattr(opt, 'distiller'):\n                netA = nn.Conv2d(in_channels=fs * 4, out_channels=ft * 4, kernel_size=1). \\\n                    to(self.device)\n            else:\n                netA = SuperConv2d(in_channels=fs * 4, out_channels=ft * 4, kernel_size=1). \\\n                    to(self.device)\n            networks.init_net(netA)\n            G_params.append(netA.parameters())\n            self.netAs.append(netA)\n            self.loss_names.append('G_distill%d' % i)\n\n        self.optimizer_G = torch.optim.Adam(itertools.chain(*G_params), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizers.append(self.optimizer_G)\n        self.optimizers.append(self.optimizer_D)\n\n        self.eval_dataloader = create_eval_dataloader(self.opt, direction=opt.direction)\n\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        self.inception_model = InceptionV3([block_idx])\n        self.inception_model.to(self.device)\n        self.inception_model.eval()\n\n        if 'cityscapes' in opt.dataroot:\n            self.drn_model = DRNSeg('drn_d_105', 19, pretrained=False)\n            util.load_network(self.drn_model, opt.drn_path, verbose=False)\n            if len(opt.gpu_ids) > 0:\n                self.drn_model = nn.DataParallel(self.drn_model, opt.gpu_ids)\n            self.drn_model.eval()\n\n        self.npz = np.load(opt.real_stat_path)\n        self.is_best = False\n\n    def setup(self, opt, verbose=True):\n        self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n        self.load_networks(verbose)\n        if verbose:\n            self.print_networks()\n        if self.opt.lambda_distill > 0:\n            def get_activation(mem, name):\n                def get_output_hook(module, input, output):\n                    mem[name] = output\n\n                return get_output_hook\n\n            def add_hook(net, mem, mapping_layers):\n                for n, m in net.named_modules():\n                    if n in mapping_layers:\n                        m.register_forward_hook(get_activation(mem, n))\n\n            add_hook(self.netG_teacher, self.Tacts, self.mapping_layers)\n            add_hook(self.netG_student, self.Sacts, self.mapping_layers)\n\n    def set_input(self, input):\n        AtoB = self.opt.direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def set_single_input(self, input):\n        self.real_A = input['A'].to(self.device)\n        self.image_paths = input['A_paths']\n\n    def forward(self):\n        raise NotImplementedError\n\n    def backward_D(self):\n        if self.opt.dataset_mode == 'aligned':\n            fake = torch.cat((self.real_A, self.Sfake_B), 1).detach()\n            real = torch.cat((self.real_A, self.real_B), 1).detach()\n        else:\n            fake = self.Sfake_B.detach()\n            real = self.real_B.detach()\n\n        pred_fake = self.netD(fake)\n        self.loss_D_fake = self.criterionGAN(pred_fake, False, for_discriminator=True)\n\n        pred_real = self.netD(real)\n        self.loss_D_real = self.criterionGAN(pred_real, True, for_discriminator=True)\n\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n\n    def calc_distill_loss(self):\n        raise NotImplementedError\n\n    def backward_G(self):\n        raise NotImplementedError\n\n    def optimize_parameters(self):\n        raise NotImplementedError\n\n    def print_networks(self):\n        print('---------- Networks initialized -------------')\n        for name in self.model_names:\n            if hasattr(self, name):\n                net = getattr(self, name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                print(net)\n                print('[Network %s] Total number of parameters : %.3f M' % (name, num_params / 1e6))\n                with open(os.path.join(self.opt.log_dir, name + '.txt'), 'w') as f:\n                    f.write(str(net) + '\\n')\n                    f.write('[Network %s] Total number of parameters : %.3f M\\n' % (name, num_params / 1e6))\n        print('-----------------------------------------------')\n\n    def load_networks(self, verbose=True):\n        util.load_network(self.netG_teacher, self.opt.restore_teacher_G_path, verbose)\n        if self.opt.restore_student_G_path is not None:\n            util.load_network(self.netG_student, self.opt.restore_student_G_path, verbose)\n        if self.opt.restore_D_path is not None:\n            util.load_network(self.netD, self.opt.restore_D_path, verbose)\n        if self.opt.restore_A_path is not None:\n            for i, netA in enumerate(self.netAs):\n                path = '%s-%d.pth' % (self.opt.restore_A_path, i)\n                util.load_network(netA, path, verbose)\n        if self.opt.restore_O_path is not None:\n            for i, optimizer in enumerate(self.optimizers):\n                path = '%s-%d.pth' % (self.opt.restore_O_path, i)\n                util.load_optimizer(optimizer, path, verbose)\n                for param_group in optimizer.param_groups:\n                    param_group['lr'] = self.opt.lr\n\n    def save_networks(self, epoch):\n        save_filename = '%s_net_%s.pth' % (epoch, 'G')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s_student' % 'G')\n        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n            torch.save(net.module.cpu().state_dict(), save_path)\n            net.cuda(self.gpu_ids[0])\n        else:\n            torch.save(net.cpu().state_dict(), save_path)\n\n        save_filename = '%s_net_%s.pth' % (epoch, 'D')\n        save_path = os.path.join(self.save_dir, save_filename)\n        net = getattr(self, 'net%s' % 'D')\n        if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n            torch.save(net.module.cpu().state_dict(), save_path)\n            net.cuda(self.gpu_ids[0])\n        else:\n            torch.save(net.cpu().state_dict(), save_path)\n\n        for i, net in enumerate(self.netAs):\n            save_filename = '%s_net_%s-%d.pth' % (epoch, 'A', i)\n            save_path = os.path.join(self.save_dir, save_filename)\n            if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                torch.save(net.cpu().state_dict(), save_path)\n                net.cuda(self.gpu_ids[0])\n            else:\n                torch.save(net.cpu().state_dict(), save_path)\n\n        for i, optimizer in enumerate(self.optimizers):\n            save_filename = '%s_optim-%d.pth' % (epoch, i)\n            save_path = os.path.join(self.save_dir, save_filename)\n            torch.save(optimizer.state_dict(), save_path)\n\n    def evaluate_model(self, step):\n        raise NotImplementedError\n\n    def test(self):\n        with torch.no_grad():\n            self.forward()\n"""
distillers/resnet_distiller.py,3,"b""import ntpath\nimport os\n\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom tqdm import tqdm\n\nfrom metric import get_fid, get_mIoU\nfrom utils import util\nfrom utils.weight_transfer import load_pretrained_weight\nfrom .base_resnet_distiller import BaseResnetDistiller\n\n\nclass ResnetDistiller(BaseResnetDistiller):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert is_train\n        parser = super(ResnetDistiller, ResnetDistiller).modify_commandline_options(parser, is_train)\n        parser.add_argument('--restore_pretrained_G_path', type=str, default=None,\n                            help='the path to restore pretrained G')\n        parser.add_argument('--pretrained_netG', type=str, default='mobile_resnet_9blocks')\n        parser.add_argument('--pretrained_ngf', type=int, default=64)\n        parser.set_defaults(norm='instance', dataset_mode='aligned', log_dir='logs/supernet')\n        return parser\n\n    def __init__(self, opt):\n        assert opt.isTrain\n        super(ResnetDistiller, self).__init__(opt)\n        self.best_fid = 1e9\n        self.best_mIoU = -1e9\n        self.fids, self.mIoUs = [], []\n        self.npz = np.load(opt.real_stat_path)\n\n    def forward(self):\n        with torch.no_grad():\n            self.Tfake_B = self.netG_teacher(self.real_A)\n        self.Sfake_B = self.netG_student(self.real_A)\n\n    def calc_distill_loss(self):\n        losses = []\n        for i, netA in enumerate(self.netAs):\n            assert isinstance(netA, nn.Conv2d)\n            n = self.mapping_layers[i]\n            Tact = self.Tacts[n]\n            Sact = self.Sacts[n]\n            Sact = netA(Sact)\n            loss = F.mse_loss(Sact, Tact)\n            setattr(self, 'loss_G_distill%d' % i, loss)\n            losses.append(loss)\n        return sum(losses)\n\n    def backward_G(self):\n        if self.opt.dataset_mode == 'aligned':\n            self.loss_G_recon = self.criterionRecon(self.Sfake_B, self.real_B) * self.opt.lambda_recon\n            fake = torch.cat((self.real_A, self.Sfake_B), 1)\n        else:\n            self.loss_G_recon = self.criterionRecon(self.Sfake_B, self.Tfake_B) * self.opt.lambda_recon\n            fake = self.Sfake_B\n        pred_fake = self.netD(fake)\n        self.loss_G_gan = self.criterionGAN(pred_fake, True, for_discriminator=False) * self.opt.lambda_gan\n        if self.opt.lambda_distill > 0:\n            self.loss_G_distill = self.calc_distill_loss() * self.opt.lambda_distill\n        else:\n            self.loss_G_distill = 0\n        self.loss_G = self.loss_G_gan + self.loss_G_recon + self.loss_G_distill\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.optimizer_D.zero_grad()\n        self.optimizer_G.zero_grad()\n        self.forward()\n        util.set_requires_grad(self.netD, True)\n        self.backward_D()\n        util.set_requires_grad(self.netD, False)\n        self.backward_G()\n        self.optimizer_D.step()\n        self.optimizer_G.step()\n\n    def load_networks(self, verbose=True):\n        if self.opt.restore_pretrained_G_path is not None:\n            util.load_network(self.netG_pretrained, self.opt.restore_pretrained_G_path, verbose)\n            load_pretrained_weight(self.opt.pretrained_netG, self.opt.student_netG,\n                                   self.netG_pretrained, self.netG_student,\n                                   self.opt.pretrained_ngf, self.opt.student_ngf)\n            del self.netG_pretrained\n        super(ResnetDistiller, self).load_networks()\n\n    def evaluate_model(self, step):\n        self.is_best = False\n        save_dir = os.path.join(self.opt.log_dir, 'eval', str(step))\n        os.makedirs(save_dir, exist_ok=True)\n        self.netG_student.eval()\n        fakes, names = [], []\n        cnt = 0\n        for i, data_i in enumerate(tqdm(self.eval_dataloader)):\n            if self.opt.dataset_mode == 'aligned':\n                self.set_input(data_i)\n            else:\n                self.set_single_input(data_i)\n            self.test()\n            fakes.append(self.Sfake_B.cpu())\n            for j in range(len(self.image_paths)):\n                short_path = ntpath.basename(self.image_paths[j])\n                name = os.path.splitext(short_path)[0]\n                names.append(name)\n                if cnt < 10:\n                    input_im = util.tensor2im(self.real_A[j])\n                    Sfake_im = util.tensor2im(self.Sfake_B[j])\n                    Tfake_im = util.tensor2im(self.Tfake_B[j])\n                    util.save_image(input_im, os.path.join(save_dir, 'input', '%s.png') % name, create_dir=True)\n                    util.save_image(Sfake_im, os.path.join(save_dir, 'Sfake', '%s.png' % name), create_dir=True)\n                    util.save_image(Tfake_im, os.path.join(save_dir, 'Tfake', '%s.png' % name), create_dir=True)\n                    if self.opt.dataset_mode == 'aligned':\n                        real_im = util.tensor2im(self.real_B[j])\n                        util.save_image(real_im, os.path.join(save_dir, 'real', '%s.png' % name), create_dir=True)\n                cnt += 1\n\n        fid = get_fid(fakes, self.inception_model, self.npz,\n                      device=self.device, batch_size=self.opt.eval_batch_size)\n        if fid < self.best_fid:\n            self.is_best = True\n            self.best_fid = fid\n        self.fids.append(fid)\n        if len(self.fids) > 3:\n            self.fids.pop(0)\n        ret = {'metric/fid': fid, 'metric/fid-mean': sum(self.fids) / len(self.fids), 'metric/fid-best': self.best_fid}\n        if 'cityscapes' in self.opt.dataroot and self.opt.direction == 'BtoA':\n            mIoU = get_mIoU(fakes, names, self.drn_model, self.device,\n                           table_path=self.opt.table_path,\n                           data_dir=self.opt.cityscapes_path,\n                           batch_size=self.opt.eval_batch_size,\n                           num_workers=self.opt.num_threads)\n            if mIoU > self.best_mIoU:\n                self.is_best = True\n                self.best_mIoU = mIoU\n            self.mIoUs.append(mIoU)\n            if len(self.mIoUs) > 3:\n                self.mIoUs = self.mIoUs[1:]\n            ret['metric/mIoU'] = mIoU\n            ret['metric/mIoU-mean'] = sum(self.mIoUs) / len(self.mIoUs)\n            ret['metric/mIoU-best'] = self.best_mIoU\n        self.netG_student.train()\n        return ret\n"""
metric/__init__.py,2,"b""import torch\n\nfrom metric.fid_score import _compute_statistics_of_ims, calculate_frechet_distance\nfrom utils import util\nfrom .mIoU_score import test\n\n\ndef get_fid(fakes, model, npz, device, batch_size=1, use_tqdm=True):\n    m1, s1 = npz['mu'], npz['sigma']\n    fakes = torch.cat(fakes, dim=0)\n    fakes = util.tensor2im(fakes).astype(float)\n    m2, s2 = _compute_statistics_of_ims(fakes, model, batch_size, 2048,\n                                        device, use_tqdm=use_tqdm)\n    return float(calculate_frechet_distance(m1, s1, m2, s2))\n\n\ndef get_mIoU(fakes, names, model, device,\n             table_path='datasets/table.txt',\n             data_dir='database/cityscapes',\n             batch_size=1, num_workers=8, num_classes=19,\n             use_tqdm=True):\n    fakes = torch.cat(fakes, dim=0)\n    fakes = util.tensor2im(fakes)\n    mAP = test(fakes, names, model, device, table_path=table_path, data_dir=data_dir,\n               batch_size=batch_size, num_workers=num_workers, num_classes=num_classes, use_tqdm=use_tqdm)\n    return float(mAP)\n"""
metric/drn.py,3,"b""import math\n\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\nBatchNorm = nn.BatchNorm2d\n\nwebroot = 'http://dl.yf.io/drn/'\n\nmodel_urls = {\n    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n    'drn-c-26': webroot + 'drn_c_26-ddedf421.pth',\n    'drn-c-42': webroot + 'drn_c_42-9d336e8c.pth',\n    'drn-c-58': webroot + 'drn_c_58-0a53a92c.pth',\n    'drn-d-22': webroot + 'drn_d_22-4bd2f8ea.pth',\n    'drn-d-38': webroot + 'drn_d_38-eebb45f0.pth',\n    'drn-d-54': webroot + 'drn_d_54-0e0534ff.pth',\n    'drn-d-105': webroot + 'drn_d_105-12b40979.pth'\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=padding, bias=False, dilation=dilation)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride,\n                             padding=dilation[0], dilation=dilation[0])\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes,\n                             padding=dilation[1], dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000,\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 out_map=False, out_middle=False, pool_size=28, arch='D'):\n        super(DRN, self).__init__()\n        self.inplanes = channels[0]\n        self.out_map = out_map\n        self.out_dim = channels[-1]\n        self.out_middle = out_middle\n        self.arch = arch\n\n        if arch == 'C':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                                   padding=3, bias=False)\n            self.bn1 = BatchNorm(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(\n                BasicBlock, channels[0], layers[0], stride=1)\n            self.layer2 = self._make_layer(\n                BasicBlock, channels[1], layers[1], stride=2)\n        elif arch == 'D':\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3,\n                          bias=False),\n                BatchNorm(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(\n                channels[0], layers[0], stride=1)\n            self.layer2 = self._make_conv_layers(\n                channels[1], layers[1], stride=2)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n        self.layer5 = self._make_layer(block, channels[4], layers[4],\n                                       dilation=2, new_level=False)\n        self.layer6 = None if layers[5] == 0 else \\\n            self._make_layer(block, channels[5], layers[5], dilation=4,\n                             new_level=False)\n\n        if arch == 'C':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_layer(BasicBlock, channels[6], layers[6], dilation=2,\n                                 new_level=False, residual=False)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_layer(BasicBlock, channels[7], layers[7], dilation=1,\n                                 new_level=False, residual=False)\n        elif arch == 'D':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_conv_layers(channels[6], layers[6], dilation=2)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_conv_layers(channels[7], layers[7], dilation=1)\n\n        if num_classes > 0:\n            self.avgpool = nn.AvgPool2d(pool_size)\n            self.fc = nn.Conv2d(self.out_dim, num_classes, kernel_size=1,\n                                stride=1, padding=0, bias=True)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        y = list()\n\n        if self.arch == 'C':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == 'D':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        y.append(x)\n        x = self.layer2(x)\n        y.append(x)\n\n        x = self.layer3(x)\n        y.append(x)\n\n        x = self.layer4(x)\n        y.append(x)\n\n        x = self.layer5(x)\n        y.append(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n            y.append(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n            y.append(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n            y.append(x)\n\n        if self.out_map:\n            x = self.fc(x)\n        else:\n            x = self.avgpool(x)\n            x = self.fc(x)\n            x = x.view(x.size(0), -1)\n\n        if self.out_middle:\n            return x, y\n        else:\n            return x\n\n\nclass DRN_A(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                       dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                       dilation=4)\n        self.avgpool = nn.AvgPool2d(28, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef drn_a_50(pretrained=False, **kwargs):\n    model = DRN_A(Bottleneck, [3, 4, 6, 3], **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['resnet50']))\n    return model\n\n\ndef drn_c_26(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-c-26']))\n    return model\n\n\ndef drn_c_42(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='C', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-c-42']))\n    return model\n\n\ndef drn_c_58(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='C', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-c-58']))\n    return model\n\n\ndef drn_d_22(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-22']))\n    return model\n\n\ndef drn_d_24(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 2, 2], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-24']))\n    return model\n\n\ndef drn_d_38(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 1, 1], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-38']))\n    return model\n\n\ndef drn_d_40(pretrained=False, **kwargs):\n    model = DRN(BasicBlock, [1, 1, 3, 4, 6, 3, 2, 2], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-40']))\n    return model\n\n\ndef drn_d_54(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 1, 1], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-54']))\n    return model\n\n\ndef drn_d_56(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 6, 3, 2, 2], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-56']))\n    return model\n\n\ndef drn_d_105(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 1, 1], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-105']))\n    return model\n\n\ndef drn_d_107(pretrained=False, **kwargs):\n    model = DRN(Bottleneck, [1, 1, 3, 4, 23, 3, 2, 2], arch='D', **kwargs)\n    if pretrained:\n        model.load_state_dict(model_zoo.load_url(model_urls['drn-d-107']))\n    return model\n"""
metric/fid_score.py,5,"b'#!/usr/bin/env python3\n""""""Calculates the Frechet Inception Distance (FID) to evalulate GANs\n\nThe FID metric calculates the distance between two distributions of images.\nTypically, we have summary statistics (mean & covariance matrix) of one\nof these distributions, while the 2nd distribution is given by a GAN.\n\nWhen run as a stand-alone program, it compares the distribution of\nimages that are stored as PNG/JPEG at a specified location with a\ndistribution given by summary statistics (in pickle format).\n\nThe FID is calculated by assuming that X_1 and X_2 are the activations of\nthe pool_3 layer of the inception net for generated samples and real world\nsamples respectively.\n\nSee --help to see further details.\n\nCode apapted from https://github.com/bioinf-jku/TTUR to use PyTorch instead\nof Tensorflow\n\nCopyright 2018 Institute of Bioinformatics, JKU Linz\n\nLicensed under the Apache License, Version 2.0 (the ""License"");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an ""AS IS"" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n""""""\nimport os\nimport pathlib\nfrom argparse import ArgumentParser, ArgumentDefaultsHelpFormatter\n\nimport cv2\nimport numpy as np\nimport torch\nfrom cv2 import imread\nfrom scipy import linalg\nfrom torch.nn.functional import adaptive_avg_pool2d\n\nfrom .inception import InceptionV3\n\n# try:\n#     from tqdm import tqdm\n# except ImportError:\n#     # If not tqdm is not available, provide a mock version of it\n#     def tqdm(x):\n#         return x\n\nparser = ArgumentParser(formatter_class=ArgumentDefaultsHelpFormatter)\nparser.add_argument(\'path\', type=str, nargs=2,\n                    help=(\'Path to the generated images or \'\n                          \'to .npz statistic files\'))\nparser.add_argument(\'--batch-size\', type=int, default=50,\n                    help=\'Batch size to use\')\nparser.add_argument(\'--dims\', type=int, default=2048,\n                    choices=list(InceptionV3.BLOCK_INDEX_BY_DIM),\n                    help=(\'Dimensionality of Inception features to use. \'\n                          \'By default, uses pool3 features\'))\nparser.add_argument(\'-c\', \'--gpu\', default=\'\', type=str,\n                    help=\'GPU to use (leave blank for CPU only)\')\n\n\ndef get_activations(files, model, batch_size=50, dims=2048,\n                    cuda=False, verbose=False, use_tqdm=False):\n    """"""Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- files       : List of image files paths\n    -- model       : Instance of inception model\n    -- batch_size  : Batch size of images for the model to process at once.\n                     Make sure that the number of samples is a multiple of\n                     the batch size, otherwise some samples are ignored. This\n                     behavior is retained to match the original FID score\n                     implementation.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    """"""\n    model.eval()\n\n    if len(files) % batch_size != 0:\n        print((\'Warning: number of images is not a multiple of the \'\n               \'batch size. Some samples are going to be ignored.\'))\n    if batch_size > len(files):\n        print((\'Warning: batch size is bigger than the datasets size. \'\n               \'Setting batch size to datasets size\'))\n        batch_size = len(files)\n    # print(len(files), batch_size)\n\n    n_batches = len(files) // batch_size\n    n_used_imgs = n_batches * batch_size\n\n    pred_arr = np.empty((n_used_imgs, dims))\n\n    if use_tqdm:\n        from tqdm import tqdm\n    else:\n        def tqdm(x):\n            return x\n    for i in tqdm(range(n_batches)):\n        if verbose:\n            print(\'\\rPropagating batch %d/%d\' % (i + 1, n_batches),\n                  end=\'\', flush=True)\n        start = i * batch_size\n        end = start + batch_size\n        images = np.array([imread(str(f)).astype(np.float32)\n                           for f in files[start:end]])\n        if len(images.shape) != 4:\n            images = imread(str(files[start]))\n            images = cv2.cvtColor(images, cv2.COLOR_BGR2GRAY)\n            images = np.array([images.astype(np.float32)])\n        # Reshape to (n_images, 3, height, width)\n        images = images.transpose((0, 3, 1, 2))\n        images /= 255\n\n        batch = torch.from_numpy(images).type(torch.FloatTensor)\n        if cuda:\n            batch = batch.cuda()\n        with torch.no_grad():\n            pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(end - start, -1)\n\n    if verbose:\n        print(\' done\')\n\n    return pred_arr\n\n\ndef get_activations_from_ims(ims, model, batch_size=50, dims=2048,\n                             device=None, verbose=False, use_tqdm=True):\n    """"""Calculates the activations of the pool_3 layer for all images.\n\n    Params:\n    -- files       : List of image files paths\n    -- model       : Instance of inception model\n    -- batch_size  : Batch size of images for the model to process at once.\n                     Make sure that the number of samples is a multiple of\n                     the batch size, otherwise some samples are ignored. This\n                     behavior is retained to match the original FID score\n                     implementation.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the number\n                     of calculated batches is reported.\n    Returns:\n    -- A numpy array of dimension (num images, dims) that contains the\n       activations of the given tensor when feeding inception with the\n       query tensor.\n    """"""\n    model.eval()\n\n    # if len(ims) % batch_size != 0:\n    #     print((\'Warning: number of images is not a multiple of the \'\n    #            \'batch size. Some samples are going to be ignored.\'))\n    # if batch_size > len(ims):\n    #     print((\'Warning: batch size is bigger than the datasets size. \'\n    #            \'Setting batch size to datasets size\'))\n    #     batch_size = len(ims)\n    # print(len(files), batch_size)\n\n    n_batches = (len(ims) + batch_size - 1) // batch_size\n    n_used_imgs = len(ims)\n\n    pred_arr = np.empty((n_used_imgs, dims))\n\n    if use_tqdm:\n        from tqdm import tqdm\n    else:\n        def tqdm(x):\n            return x\n    for i in tqdm(range(n_batches)):\n        start = i * batch_size\n        end = start + batch_size\n        if end > len(ims):\n            end = len(ims)\n        images = ims[start:end]\n        if images.shape[1] != 3:\n            images = images.transpose((0, 3, 1, 2))\n        images /= 255\n\n        batch = torch.from_numpy(images).type(torch.FloatTensor).to(device)\n        # if cuda:\n        #     batch = batch.cuda()\n        with torch.no_grad():\n            pred = model(batch)[0]\n\n        # If model output is not scalar, apply global spatial average pooling.\n        # This happens if you choose a dimensionality not equal 2048.\n        # print(pred.shape)\n        if pred.shape[2] != 1 or pred.shape[3] != 1:\n            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n\n        pred_arr[start:end] = pred.cpu().data.numpy().reshape(end - start, -1)\n\n    if verbose:\n        print(\' done\')\n\n    return pred_arr\n\n\ndef calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n    """"""Numpy implementation of the Frechet Distance.\n    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n    and X_2 ~ N(mu_2, C_2) is\n            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n\n    Stable version by Dougal J. Sutherland.\n\n    Params:\n    -- mu1   : Numpy array containing the activations of a layer of the\n               inception net (like returned by the function \'get_predictions\')\n               for generated samples.\n    -- mu2   : The sample mean over activations, precalculated on an\n               representative datasets set.\n    -- sigma1: The covariance matrix over activations for generated samples.\n    -- sigma2: The covariance matrix over activations, precalculated on an\n               representative datasets set.\n\n    Returns:\n    --   : The Frechet Distance.\n    """"""\n\n    mu1 = np.atleast_1d(mu1)\n    mu2 = np.atleast_1d(mu2)\n\n    sigma1 = np.atleast_2d(sigma1)\n    sigma2 = np.atleast_2d(sigma2)\n\n    assert mu1.shape == mu2.shape, \\\n        \'Training and test mean vectors have different lengths\'\n    assert sigma1.shape == sigma2.shape, \\\n        \'Training and test covariances have different dimensions\'\n\n    diff = mu1 - mu2\n\n    # print(t.max())\n    # print(t.min())\n    # print(abs(t).mean())\n    # print(t.mean())\n\n    # Product might be almost singular\n    t = sigma1.dot(sigma2)\n    for i in range(30):\n        # print(i)\n        flag = True\n        covmean, _ = linalg.sqrtm(t, disp=False)\n        if not np.isfinite(covmean).all():\n            msg = (\'fid calculation produces singular product; \'\n                   \'adding %s to diagonal of cov estimates\') % eps\n            print(msg)\n            offset = np.eye(sigma1.shape[0]) * eps\n            covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n\n        # Numerical error might give slight imaginary component\n        if np.iscomplexobj(covmean):\n            if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n                # raise ValueError(\'Imaginary component {}\'.format(m))\n                flag = False\n            covmean = covmean.real\n        if flag:\n            break\n    if not flag:\n        print(\'Warning: the fid may be incorrect!\')\n    tr_covmean = np.trace(covmean)\n\n    return (diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean)\n\n\ndef calculate_activation_statistics(files, model, batch_size=50,\n                                    dims=2048, cuda=False, verbose=False, use_tqdm=False):\n    """"""Calculation of the statistics used by the FID.\n    Params:\n    -- files       : List of image files paths\n    -- model       : Instance of inception model\n    -- batch_size  : The images numpy array is split into batches with\n                     batch size batch_size. A reasonable batch size\n                     depends on the hardware.\n    -- dims        : Dimensionality of features returned by Inception\n    -- cuda        : If set to True, use GPU\n    -- verbose     : If set to True and parameter out_step is given, the\n                     number of calculated batches is reported.\n    Returns:\n    -- mu    : The mean over samples of the activations of the pool_3 layer of\n               the inception model.\n    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n               the inception model.\n    """"""\n    act = get_activations(files, model, batch_size, dims, cuda, verbose, use_tqdm=use_tqdm)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n\n\ndef _compute_statistics_of_path(path, model, batch_size, dims, cuda, use_tqdm=False):\n    if path.endswith(\'.npz\'):\n        f = np.load(path)\n        m, s = f[\'mu\'][:], f[\'sigma\'][:]\n        f.close()\n    else:\n        path = pathlib.Path(path)\n        files = list(path.glob(\'*.jpg\')) + list(path.glob(\'*.png\'))\n        m, s = calculate_activation_statistics(files, model, batch_size,\n                                               dims, cuda, use_tqdm=use_tqdm)\n    return m, s\n\n\ndef _compute_statistics_of_ims(ims, model, batch_size, dims, device, use_tqdm=True):\n    act = get_activations_from_ims(ims, model, batch_size, dims, device, verbose=False, use_tqdm=use_tqdm)\n    mu = np.mean(act, axis=0)\n    sigma = np.cov(act, rowvar=False)\n    return mu, sigma\n\n\ndef calculate_fid_given_ims(ims_fake, ims_real, batch_size, cuda, dims, model=None, use_tqdm=False):\n    if model is None:\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n        model = InceptionV3([block_idx])\n        if cuda:\n            model.cuda()\n\n    m1, s1 = _compute_statistics_of_ims(ims_fake, model, batch_size,\n                                        dims, cuda, use_tqdm=use_tqdm)\n    m2, s2 = _compute_statistics_of_ims(ims_real, model, batch_size,\n                                        dims, cuda, use_tqdm=use_tqdm)\n\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n\n    return fid_value\n\n\ndef calculate_fid_given_paths(paths, batch_size, cuda, dims, model=None, use_tqdm=False):\n    """"""Calculates the FID of two paths""""""\n    for p in paths:\n        if not os.path.exists(p):\n            raise RuntimeError(\'Invalid path: %s\' % p)\n    if model is None:\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n\n        model = InceptionV3([block_idx])\n        if cuda:\n            model.cuda()\n    else:\n        pass\n\n    m1, s1 = _compute_statistics_of_path(paths[0], model, batch_size,\n                                         dims, cuda, use_tqdm=use_tqdm)\n    m2, s2 = _compute_statistics_of_path(paths[1], model, batch_size,\n                                         dims, cuda, use_tqdm=use_tqdm)\n    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n\n    return fid_value\n\n# def main():\n#     fid_value = calculate_fid_given_paths((\'datasets/coco_stuff/val_img\', \'datasets/coco_stuff/val_img\'),\n#                                           1, True, 2048)\n#     print(\'FID: \', fid_value)\n'"
metric/inception.py,13,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\ntry:\n    from torchvision.models.utils import load_state_dict_from_url\nexcept ImportError:\n    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n\n# Inception weights ported to Pytorch from\n# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\nFID_WEIGHTS_URL = \'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\'\n\n\nclass InceptionV3(nn.Module):\n    """"""Pretrained InceptionV3 network returning feature maps""""""\n\n    # Index of default block of inception to return,\n    # corresponds to output of final average pooling\n    DEFAULT_BLOCK_INDEX = 3\n\n    # Maps feature dimensionality to their output blocks indices\n    BLOCK_INDEX_BY_DIM = {\n        64: 0,  # First max pooling features\n        192: 1,  # Second max pooling featurs\n        768: 2,  # Pre-aux classifier features\n        2048: 3  # Final average pooling features\n    }\n\n    def __init__(self,\n                 output_blocks=[DEFAULT_BLOCK_INDEX],\n                 resize_input=True,\n                 normalize_input=True,\n                 requires_grad=False,\n                 use_fid_inception=True):\n        """"""Build pretrained InceptionV3\n\n        Parameters\n        ----------\n        output_blocks : list of int\n            Indices of blocks to return features of. Possible values are:\n                - 0: corresponds to output of first max pooling\n                - 1: corresponds to output of second max pooling\n                - 2: corresponds to output which is fed to aux classifier\n                - 3: corresponds to output of final average pooling\n        resize_input : bool\n            If true, bilinearly resizes input to width and height 299 before\n            feeding input to model. As the network without fully connected\n            layers is fully convolutional, it should be able to handle inputs\n            of arbitrary size, so resizing might not be strictly needed\n        normalize_input : bool\n            If true, scales the input from range (0, 1) to the range the\n            pretrained Inception network expects, namely (-1, 1)\n        requires_grad : bool\n            If true, parameters of the model require gradients. Possibly useful\n            for finetuning the network\n        use_fid_inception : bool\n            If true, uses the pretrained Inception model used in Tensorflow\'s\n            FID implementation. If false, uses the pretrained Inception model\n            available in torchvision. The FID Inception model has different\n            weights and a slightly different structure from torchvision\'s\n            Inception model. If you want to compute FID scores, you are\n            strongly advised to set this parameter to true to get comparable\n            results.\n        """"""\n        super(InceptionV3, self).__init__()\n\n        self.resize_input = resize_input\n        self.normalize_input = normalize_input\n        self.output_blocks = sorted(output_blocks)\n        self.last_needed_block = max(output_blocks)\n\n        assert self.last_needed_block <= 3, \\\n            \'Last possible output block index is 3\'\n\n        self.blocks = nn.ModuleList()\n\n        if use_fid_inception:\n            inception = fid_inception_v3()\n        else:\n            inception = models.inception_v3(pretrained=True)\n\n        # Block 0: input to maxpool1\n        block0 = [\n            inception.Conv2d_1a_3x3,\n            inception.Conv2d_2a_3x3,\n            inception.Conv2d_2b_3x3,\n            nn.MaxPool2d(kernel_size=3, stride=2)\n        ]\n        self.blocks.append(nn.Sequential(*block0))\n\n        # Block 1: maxpool1 to maxpool2\n        if self.last_needed_block >= 1:\n            block1 = [\n                inception.Conv2d_3b_1x1,\n                inception.Conv2d_4a_3x3,\n                nn.MaxPool2d(kernel_size=3, stride=2)\n            ]\n            self.blocks.append(nn.Sequential(*block1))\n\n        # Block 2: maxpool2 to aux classifier\n        if self.last_needed_block >= 2:\n            block2 = [\n                inception.Mixed_5b,\n                inception.Mixed_5c,\n                inception.Mixed_5d,\n                inception.Mixed_6a,\n                inception.Mixed_6b,\n                inception.Mixed_6c,\n                inception.Mixed_6d,\n                inception.Mixed_6e,\n            ]\n            self.blocks.append(nn.Sequential(*block2))\n\n        # Block 3: aux classifier to final avgpool\n        if self.last_needed_block >= 3:\n            block3 = [\n                inception.Mixed_7a,\n                inception.Mixed_7b,\n                inception.Mixed_7c,\n                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n            ]\n            self.blocks.append(nn.Sequential(*block3))\n\n        for param in self.parameters():\n            param.requires_grad = requires_grad\n\n    def forward(self, inp):\n        """"""Get Inception feature maps\n\n        Parameters\n        ----------\n        inp : torch.autograd.Variable\n            Input tensor of shape Bx3xHxW. Values are expected to be in\n            range (0, 1)\n\n        Returns\n        -------\n        List of torch.autograd.Variable, corresponding to the selected output\n        block, sorted ascending by index\n        """"""\n        outp = []\n        x = inp\n\n        if self.resize_input:\n            x = F.interpolate(x,\n                              size=(299, 299),\n                              mode=\'bilinear\',\n                              align_corners=False)\n\n        if self.normalize_input:\n            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n\n        for idx, block in enumerate(self.blocks):\n            x = block(x)\n            if idx in self.output_blocks:\n                outp.append(x)\n\n            if idx == self.last_needed_block:\n                break\n\n        return outp\n\n\ndef fid_inception_v3():\n    """"""Build pretrained Inception model for FID computation\n\n    The Inception model for FID computation uses a different set of weights\n    and has a slightly different structure than torchvision\'s Inception.\n\n    This method first constructs torchvision\'s Inception and then patches the\n    necessary parts that are different in the FID Inception model.\n    """"""\n    inception = models.inception_v3(num_classes=1008,\n                                    aux_logits=False,\n                                    pretrained=False)\n    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n    inception.Mixed_7b = FIDInceptionE_1(1280)\n    inception.Mixed_7c = FIDInceptionE_2(2048)\n\n    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n    inception.load_state_dict(state_dict)\n    return inception\n\n\nclass FIDInceptionA(models.inception.InceptionA):\n    """"""InceptionA block patched for FID computation""""""\n\n    def __init__(self, in_channels, pool_features):\n        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch5x5 = self.branch5x5_1(x)\n        branch5x5 = self.branch5x5_2(branch5x5)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n\n        # Patch: Tensorflow\'s average pool does not use the padded zero\'s in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass FIDInceptionC(models.inception.InceptionC):\n    """"""InceptionC block patched for FID computation""""""\n\n    def __init__(self, in_channels, channels_7x7):\n        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch7x7 = self.branch7x7_1(x)\n        branch7x7 = self.branch7x7_2(branch7x7)\n        branch7x7 = self.branch7x7_3(branch7x7)\n\n        branch7x7dbl = self.branch7x7dbl_1(x)\n        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n\n        # Patch: Tensorflow\'s average pool does not use the padded zero\'s in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass FIDInceptionE_1(models.inception.InceptionE):\n    """"""First InceptionE block patched for FID computation""""""\n\n    def __init__(self, in_channels):\n        super(FIDInceptionE_1, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: Tensorflow\'s average pool does not use the padded zero\'s in\n        # its average calculation\n        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n                                   count_include_pad=False)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n\n\nclass FIDInceptionE_2(models.inception.InceptionE):\n    """"""Second InceptionE block patched for FID computation""""""\n\n    def __init__(self, in_channels):\n        super(FIDInceptionE_2, self).__init__(in_channels)\n\n    def forward(self, x):\n        branch1x1 = self.branch1x1(x)\n\n        branch3x3 = self.branch3x3_1(x)\n        branch3x3 = [\n            self.branch3x3_2a(branch3x3),\n            self.branch3x3_2b(branch3x3),\n        ]\n        branch3x3 = torch.cat(branch3x3, 1)\n\n        branch3x3dbl = self.branch3x3dbl_1(x)\n        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n        branch3x3dbl = [\n            self.branch3x3dbl_3a(branch3x3dbl),\n            self.branch3x3dbl_3b(branch3x3dbl),\n        ]\n        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n\n        # Patch: The FID Inception model uses max pooling instead of average\n        # pooling. This is likely an error in this specific Inception\n        # implementation, as other Inception models use average pooling here\n        # (which matches the description in the paper).\n        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n        branch_pool = self.branch_pool(branch_pool)\n\n        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n        return torch.cat(outputs, 1)\n'"
metric/mIoU_score.py,9,"b'import math\nimport os\nimport threading\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom . import drn\n\n\nclass FromArray(object):\n    def __init__(self, size):\n        self.size = size\n\n    def __call__(self, image_numpy, label):\n        image = Image.fromarray(image_numpy)\n        return image, label\n\n\nclass Normalize(object):\n    """"""Given mean: (R, G, B) and std: (R, G, B),\n    will normalize each channel of the torch.*Tensor, i.e.\n    channel = (channel - mean) / std\n    """"""\n\n    def __init__(self, mean, std):\n        self.mean = torch.FloatTensor(mean)\n        self.std = torch.FloatTensor(std)\n\n    def __call__(self, image, label):\n        for t, m, s in zip(image, self.mean, self.std):\n            t.sub_(m).div_(s)\n        return image, label\n\n\nclass ToTensor(object):\n    """"""Converts a PIL.Image or numpy.ndarray (H x W x C) in the range\n    [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n    """"""\n\n    def __call__(self, pic, label):\n        img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n        # PIL image mode: 1, L, P, I, F, RGB, YCbCr, RGBA, CMYK\n        if pic.mode == \'YCbCr\':\n            nchannel = 3\n        else:\n            nchannel = len(pic.mode)\n        img = img.view(pic.size[1], pic.size[0], nchannel)\n        img = img.transpose(0, 1).transpose(0, 2).contiguous()\n        img = img.float() / 255\n        return img, torch.from_numpy(np.array(label, dtype=np.int))\n\n\nclass Compose(object):\n    """"""Composes several transforms together.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, *args):\n        for t in self.transforms:\n            args = t(*args)\n        return args\n\n\nclass SegList(Dataset):\n\n    def __init__(self, images, names, table_path, data_dir):\n        self.images = images\n        self.names = names\n        self.table_path = table_path\n        self.data_dir = data_dir\n        self.transforms = Compose([\n            FromArray([2048, 1024]),\n            ToTensor(),\n            Normalize(mean=[0.29010095242892997, 0.32808144844279574, 0.28696394422942517],\n                      std=[0.1829540508368939, 0.18656561047509476, 0.18447508988480435])\n        ])\n        self.read_lists()\n\n    def __getitem__(self, index):\n        data = [self.images[index], Image.open(os.path.join(self.data_dir, self.label_list[index]))]\n        data = list(self.transforms(*data))\n        return tuple(data)\n\n    def __len__(self):\n        return len(self.names)\n\n    def read_lists(self):\n        self.label_list = []\n        table = []\n        with open(self.table_path, \'r\') as f:\n            lines = f.readlines()\n            for line in lines:\n                table.append(line.strip().split(\' \'))\n        for name in self.names:\n            for item in table:\n                if item[0] == name or item[2][:-len(\'.png\')].endswith(name):\n                    self.label_list.append(item[1])\n                    break\n        assert len(self.label_list) == len(self.names)\n\n\ndef per_class_iu(hist):\n    return np.diag(hist) / (hist.sum(1) + hist.sum(0) - np.diag(hist))\n\n\ndef fill_up_weights(up):\n    w = up.weight.data\n    f = math.ceil(w.size(2) / 2)\n    c = (2 * f - 1 - f % 2) / (2. * f)\n    for i in range(w.size(2)):\n        for j in range(w.size(3)):\n            w[0, 0, i, j] = \\\n                (1 - math.fabs(i / f - c)) * (1 - math.fabs(j / f - c))\n    for c in range(1, w.size(0)):\n        w[c, 0, :, :] = w[0, 0, :, :]\n\n\nclass DRNSeg(nn.Module):\n    def __init__(self, model_name, classes, pretrained_model=None,\n                 pretrained=True, use_torch_up=False):\n        super(DRNSeg, self).__init__()\n        model = drn.__dict__.get(model_name)(\n            pretrained=pretrained, num_classes=1000)\n\n        pmodel = nn.DataParallel(model)\n        if pretrained_model is not None:\n            pmodel.load_state_dict(pretrained_model)\n        self.base = nn.Sequential(*list(model.children())[:-2])\n\n        self.seg = nn.Conv2d(model.out_dim, classes,\n                             kernel_size=1, bias=True)\n        self.softmax = nn.LogSoftmax()\n        m = self.seg\n        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n        m.weight.data.normal_(0, math.sqrt(2. / n))\n        m.bias.data.zero_()\n        if use_torch_up:\n            self.up = nn.UpsamplingBilinear2d(scale_factor=8)\n        else:\n            up = nn.ConvTranspose2d(classes, classes, 16, stride=8, padding=4,\n                                    output_padding=0, groups=classes,\n                                    bias=False)\n            fill_up_weights(up)\n            up.weight.requires_grad = False\n            self.up = up\n\n    def forward(self, x):\n        x = self.base(x)\n        x = self.seg(x)\n        y = self.up(x)\n        return self.softmax(y), x\n\n    def optim_parameters(self, memo=None):\n        raise NotImplementedError(\'This code is just for evaluation!!!\')\n\n\ndef fast_hist(pred, label, n):\n    k = (label >= 0) & (label < n)\n    return np.bincount(\n        n * label[k].astype(int) + pred[k], minlength=n ** 2).reshape(n, n)\n\n\ndef resize_4d_tensor(tensor, width, height):\n    """"""\n    tensor: the semantic label tensor of shape [B, C, H, W]\n    width: target width\n    height: target height\n    """"""\n    tensor_cpu = tensor.cpu().numpy()\n    if tensor.size(2) == height and tensor.size(3) == width:\n        return tensor_cpu\n    out_size = (tensor.size(0), tensor.size(1), height, width)\n    out = np.empty(out_size, dtype=np.float32)\n\n    def resize_channel(j):\n        for i in range(tensor.size(0)):\n            out[i, j] = np.array(\n                Image.fromarray(tensor_cpu[i, j]).resize(\n                    (width, height), Image.BILINEAR))\n\n    workers = [threading.Thread(target=resize_channel, args=(j,))\n               for j in range(tensor.size(1))]\n    for w in workers:\n        w.start()\n    for w in workers:\n        w.join()\n    return out\n\n\ndef test(fakes, names, model, device, table_path=\'datasets/table.txt\', data_dir=\'database/cityscapes\',\n         batch_size=1, num_workers=8, num_classes=19, use_tqdm=True):\n    dataset = SegList(fakes, names, table_path, data_dir)\n    eval_dataloader = DataLoader(dataset, batch_size=batch_size,\n                                 shuffle=False, num_workers=num_workers)\n    model.eval()\n    hist = np.zeros((num_classes, num_classes))\n    if use_tqdm:\n        from tqdm import tqdm\n    else:\n        def tqdm(x):\n            return x\n    with torch.no_grad():\n        for iter, (image, label) in enumerate(tqdm(eval_dataloader)):\n            image = image.to(device)\n            final = model(image)[0]\n            final = resize_4d_tensor(final, 2048, 1024)\n            pred = final.argmax(axis=1)\n            label = label.numpy()\n            hist += fast_hist(pred.flatten(), label.flatten(), num_classes)\n\n    ious = per_class_iu(hist) * 100\n    return round(np.nanmean(ious), 2)\n'"
models/__init__.py,0,"b'import importlib\n\nfrom models.base_model import BaseModel\n\n\ndef find_model_using_name(model_name):\n    """"""Import the module ""models/[model_name]_model.py"".\n\n    In the file, the class called DatasetNameModel() will\n    be instantiated. It has to be a subclass of BaseModel,\n    and it is case-insensitive.\n    """"""\n    model_filename = ""models."" + model_name + \'_model\'\n    modellib = importlib.import_module(model_filename)\n    model = None\n    target_model_name = model_name.replace(\'_\', \'\') + \'model\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_model_name.lower() \\\n                and issubclass(cls, BaseModel):\n            model = cls\n\n    if model is None:\n        print(""In %s.py, there should be a subclass of BaseModel with class name that matches %s in lowercase."" % (\n            model_filename, target_model_name))\n        exit(0)\n\n    return model\n\n\ndef get_option_setter(model_name):\n    """"""Return the static method <modify_commandline_options> of the model class.""""""\n    model_class = find_model_using_name(model_name)\n    return model_class.modify_commandline_options\n\n\ndef create_model(opt, verbose=True):\n    """"""Create a model given the option.\n\n    This function warps the class CustomDatasetDataLoader.\n    This is the main interface between this package and \'train.py\'/\'test.py\'\n\n    Example:\n        >>> from models import create_model\n        >>> model = create_model(opt)\n    """"""\n    model = find_model_using_name(opt.model)\n    instance = model(opt)\n    if verbose:\n        print(""model [%s] was created"" % type(instance).__name__)\n    return instance\n'"
models/base_model.py,7,"b'import os\nfrom abc import ABC, abstractmethod\nfrom collections import OrderedDict\n\nimport torch\n\nfrom utils import util\nfrom . import networks\n\n\nclass BaseModel(ABC):\n    """"""This class is an abstract base class (ABC) for models.\n    To create a subclass, you need to implement the following five functions:\n        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n        -- <set_input>:                     unpack data from dataset and apply preprocessing.\n        -- <forward>:                       produce intermediate results.\n        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n        -- <modify_commandline_options>:    (optionally) add model-specific options and set default options.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize the BaseModel class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n\n        When creating your custom class, you need to implement your own initialization.\n        In this fucntion, you should first call <BaseModel.__init__(self, opt)>\n        Then, you need to define four lists:\n            -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n            -- self.model_names (str list):         specify the images that you want to display and save.\n            -- self.visual_names (str list):        define networks used in our training.\n            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n        """"""\n        self.opt = opt\n        self.gpu_ids = opt.gpu_ids\n        self.isTrain = opt.isTrain\n        self.device = torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids \\\n            else torch.device(\'cpu\')  # get device name: CPU or GPU\n        if opt.isTrain:\n            self.save_dir = os.path.join(opt.log_dir, \'checkpoints\')  # save all the checkpoints to save_dir\n            os.makedirs(self.save_dir, exist_ok=True)\n        if opt.preprocess != \'scale_width\':  # with [scale_width], input images might have different sizes, which hurts the performance of cudnn.benchmark.\n            torch.backends.cudnn.benchmark = True\n        self.model_names = []\n        self.visual_names = []\n        self.image_paths = []\n        self.metric = 0  # used for learning rate policy \'plateau\'\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        """"""Add new model-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n        """"""\n        return parser\n\n    @abstractmethod\n    def set_input(self, input):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): includes the data itself and its metadata information.\n        """"""\n        pass\n\n    @abstractmethod\n    def forward(self):\n        """"""Run forward pass; called by both functions <optimize_parameters> and <test>.""""""\n        pass\n\n    @abstractmethod\n    def optimize_parameters(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        pass\n\n    def setup(self, opt, verbose=True):\n        """"""Load and print networks; create schedulers\n\n        Parameters:\n            opt (Option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        if self.isTrain:\n            self.schedulers = [networks.get_scheduler(optimizer, opt) for optimizer in self.optimizers]\n        self.load_networks(verbose=verbose)\n        if verbose:\n            self.print_networks()\n\n    def print_networks(self):\n        print(\'---------- Networks initialized -------------\')\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, \'net\' + name)\n                num_params = 0\n                for param in net.parameters():\n                    num_params += param.numel()\n                print(net)\n                print(\'[Network %s] Total number of parameters : %.3f M\' % (name, num_params / 1e6))\n                if hasattr(self.opt, \'log_dir\'):\n                    with open(os.path.join(self.opt.log_dir, \'net\' + name + \'.txt\'), \'w\') as f:\n                        f.write(str(net) + \'\\n\')\n                        f.write(\'[Network %s] Total number of parameters : %.3f M\\n\' % (name, num_params / 1e6))\n        print(\'-----------------------------------------------\')\n\n    def eval(self):\n        """"""Make models eval mode during test time""""""\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, \'net\' + name)\n                net.eval()\n\n    def train(self):\n        """"""Make models eval mode during test time""""""\n        for name in self.model_names:\n            if isinstance(name, str):\n                net = getattr(self, \'net\' + name)\n                net.train()\n\n    def test(self):\n        """"""Forward function used in test time.\n\n        This function wraps <forward> function in no_grad() so we don\'t save intermediate steps for backprop\n        It also calls <compute_visuals> to produce additional visualization results\n        """"""\n        with torch.no_grad():\n            self.forward()\n\n    def get_image_paths(self):\n        """""" Return image paths that are used to load current data""""""\n        return self.image_paths\n\n    def update_learning_rate(self, logger=None):\n        for scheduler in self.schedulers:\n            if self.opt.lr_policy == \'plateau\':\n                scheduler.step(self.opt.metric)\n            else:\n                scheduler.step()\n        lr = self.optimizers[0].param_groups[0][\'lr\']\n        if logger is not None:\n            logger.print_info(\'learning rate = %.7f\\n\' % lr)\n        else:\n            print(\'learning rate = %.7f\' % lr)\n\n    def get_current_visuals(self):\n        """"""Return visualization images. train.py will display these images with visdom, and save the images to a HTML""""""\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str) and hasattr(self, name):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    def get_current_losses(self):\n        errors_set = OrderedDict()\n        for name in self.loss_names:\n            if not hasattr(self, \'loss_\' + name):\n                continue\n            key = name\n\n            def has_number(s):\n                for i in range(10):\n                    if str(i) in s:\n                        return True\n                return False\n\n            if has_number(key):\n                key = \'Specific_loss/\' + key\n            elif key.startswith(\'D_\'):\n                key = \'D_loss/\' + key\n            elif key.startswith(\'G_\'):\n                key = \'G_loss/\' + key\n            else:\n                assert False\n            errors_set[key] = float(getattr(self, \'loss_\' + name))\n        return errors_set\n\n    def load_networks(self, verbose=True):\n        for name in self.model_names:\n            net = getattr(self, \'net\' + name, None)\n            path = getattr(self.opt, \'restore_%s_path\' % name, None)\n            if path is not None:\n                util.load_network(net, path, verbose)\n\n    def save_networks(self, epoch):\n        for name in self.model_names:\n            if isinstance(name, str):\n                save_filename = \'%s_net_%s.pth\' % (epoch, name)\n                save_path = os.path.join(self.save_dir, save_filename)\n                net = getattr(self, \'net\' + name)\n                if len(self.gpu_ids) > 0 and torch.cuda.is_available():\n                    torch.save(net.module.cpu().state_dict(), save_path)\n                    net.cuda(self.gpu_ids[0])\n                else:\n                    torch.save(net.cpu().state_dict(), save_path)\n\n    def set_requires_grad(self, nets, requires_grad=False):\n        """"""Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n        Parameters:\n            nets (network list)   -- a list of networks\n            requires_grad (bool)  -- whether the networks require gradients or not\n        """"""\n        if not isinstance(nets, list):\n            nets = [nets]\n        for net in nets:\n            if net is not None:\n                for param in net.parameters():\n                    param.requires_grad = requires_grad\n\n    def evaluate_model(self, step):\n        raise NotImplementedError\n\n    def profile(self):\n        raise NotImplementedError\n'"
models/cycle_gan_model.py,5,"b'import itertools\nimport ntpath\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\n\nimport models.modules.loss\nfrom data import create_eval_dataloader\nfrom metric import get_fid, get_mIoU\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import networks\nfrom models.base_model import BaseModel\nfrom utils import util\nfrom utils.image_pool import ImagePool\n\n\nclass CycleGANModel(BaseModel):\n    """"""\n    This class implements the CycleGAN model, for learning image-to-image translation without paired data.\n\n    The model training requires \'--dataset_mode unaligned\' dataset.\n    By default, it uses a \'--netG resnet_9blocks\' ResNet generator,\n    a \'--netD basic\' discriminator (PatchGAN introduced by pix2pix),\n    and a least-square GANs objective (\'--gan_mode lsgan\').\n\n    CycleGAN paper: https://arxiv.org/pdf/1703.10593.pdf\n    """"""\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        """"""Add new dataset-specific options, and rewrite default values for existing options.\n\n        Parameters:\n            parser          -- original option parser\n            is_train (bool) -- whether training phase or test phase. You can use this flag to add training-specific or test-specific options.\n\n        Returns:\n            the modified parser.\n\n        For CycleGAN, in addition to GAN losses, we introduce lambda_A, lambda_B, and lambda_identity for the following losses.\n        A (source domain), B (target domain).\n        Generators: G_A: A -> B; G_B: B -> A.\n        Discriminators: D_A: G_A(A) vs. B; D_B: G_B(B) vs. A.\n        Forward cycle loss:  lambda_A * ||G_B(G_A(A)) - A|| (Eqn. (2) in the paper)\n        Backward cycle loss: lambda_B * ||G_A(G_B(B)) - B|| (Eqn. (2) in the paper)\n        Identity loss (optional): lambda_identity * (||G_A(B) - B|| * lambda_B + ||G_B(A) - A|| * lambda_A) (Sec 5.2 ""Photo generation from paintings"" in the paper)\n        Dropout is not used in the original CycleGAN paper.\n        """"""\n        assert is_train\n        parser = super(CycleGANModel, CycleGANModel).modify_commandline_options(parser, is_train)\n        parser.add_argument(\'--restore_G_A_path\', type=str, default=None,\n                            help=\'the path to restore the generator G_A\')\n        parser.add_argument(\'--restore_D_A_path\', type=str, default=None,\n                            help=\'the path to restore the discriminator D_A\')\n        parser.add_argument(\'--restore_G_B_path\', type=str, default=None,\n                            help=\'the path to restore the generator G_B\')\n        parser.add_argument(\'--restore_D_B_path\', type=str, default=None,\n                            help=\'the path to restore the discriminator D_B\')\n        parser.add_argument(\'--lambda_A\', type=float, default=10.0,\n                            help=\'weight for cycle loss (A -> B -> A)\')\n        parser.add_argument(\'--lambda_B\', type=float, default=10.0,\n                            help=\'weight for cycle loss (B -> A -> B)\')\n        parser.add_argument(\'--lambda_identity\', type=float, default=0.5,\n                            help=\'use identity mapping. \'\n                                 \'Setting lambda_identity other than 0 has an effect of scaling the weight of the identity mapping loss. \'\n                                 \'For example, if the weight of the identity loss should be 10 times smaller than the weight of the reconstruction loss, please set lambda_identity = 0.1\')\n        parser.add_argument(\'--real_stat_A_path\', type=str, required=True,\n                            help=\'the path to load the ground-truth A images information to compute FID.\')\n        parser.add_argument(\'--real_stat_B_path\', type=str, required=True,\n                            help=\'the path to load the ground-truth B images information to compute FID.\')\n        parser.set_defaults(norm=\'instance\', dataset_mode=\'unaligned\',\n                            batch_size=1, ndf=64, gan_mode=\'lsgan\',\n                            nepochs=100, nepochs_decay=100, save_epoch_freq=20)\n        return parser\n\n    def __init__(self, opt):\n        """"""Initialize the CycleGAN class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        assert opt.isTrain\n        assert opt.direction == \'AtoB\'\n        assert opt.dataset_mode == \'unaligned\'\n        BaseModel.__init__(self, opt)\n        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n        self.loss_names = [\'D_A\', \'G_A\', \'G_cycle_A\', \'G_idt_A\',\n                           \'D_B\', \'G_B\', \'G_cycle_B\', \'G_idt_B\']\n        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n        visual_names_A = [\'real_A\', \'fake_B\', \'rec_A\']\n        visual_names_B = [\'real_B\', \'fake_A\', \'rec_B\']\n        if self.opt.lambda_identity > 0.0:  # if identity loss is used, we also visualize idt_B=G_A(B) ad idt_A=G_A(B)\n            visual_names_A.append(\'idt_B\')\n            visual_names_B.append(\'idt_A\')\n\n        self.visual_names = visual_names_A + visual_names_B  # combine visualizations for A and B\n        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>.\n        self.model_names = [\'G_A\', \'G_B\', \'D_A\', \'D_B\']\n\n        # define networks (both Generators and discriminators)\n        # The naming is different from those used in the paper.\n        # Code (vs. paper): G_A (G), G_B (F), D_A (D_Y), D_B (D_X)\n        self.netG_A = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG, opt.norm,\n                                        opt.dropout_rate, opt.init_type, opt.init_gain, self.gpu_ids)\n        self.netG_B = networks.define_G(opt.output_nc, opt.input_nc, opt.ngf, opt.netG, opt.norm,\n                                        opt.dropout_rate, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        self.netD_A = networks.define_D(opt.output_nc, opt.ndf, opt.netD,\n                                        opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n        self.netD_B = networks.define_D(opt.input_nc, opt.ndf, opt.netD,\n                                        opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        if opt.lambda_identity > 0.0:  # only works when input and output images have the same number of channels\n            assert (opt.input_nc == opt.output_nc)\n        self.fake_A_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n        self.fake_B_pool = ImagePool(opt.pool_size)  # create image buffer to store previously generated images\n\n        # define loss functions\n        self.criterionGAN = models.modules.loss.GANLoss(opt.gan_mode).to(self.device)  # define GAN loss.\n        self.criterionCycle = torch.nn.L1Loss()\n        self.criterionIdt = torch.nn.L1Loss()\n\n        # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n        self.optimizer_G = torch.optim.Adam(itertools.chain(self.netG_A.parameters(), self.netG_B.parameters()),\n                                            lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_D = torch.optim.Adam(itertools.chain(self.netD_A.parameters(), self.netD_B.parameters()),\n                                            lr=opt.lr, betas=(opt.beta1, 0.999))\n\n        self.optimizers = []\n        self.optimizers.append(self.optimizer_G)\n        self.optimizers.append(self.optimizer_D)\n\n        self.eval_dataloader_AtoB = create_eval_dataloader(self.opt, direction=\'AtoB\')\n        self.eval_dataloader_BtoA = create_eval_dataloader(self.opt, direction=\'BtoA\')\n\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        self.inception_model = InceptionV3([block_idx])\n        self.inception_model.to(self.device)\n        self.inception_model.eval()\n\n        if \'cityscapes\' in opt.dataroot:\n            self.drn_model = DRNSeg(\'drn_d_105\', 19, pretrained=False)\n            util.load_network(self.drn_model, opt.drn_path, verbose=False)\n            if len(opt.gpu_ids) > 0:\n                self.drn_model = nn.DataParallel(self.drn_model, opt.gpu_ids)\n            self.drn_model.eval()\n\n        self.best_fid_A, self.best_fid_B = 1e9, 1e9\n        self.best_mIoU = -1e9\n        self.fids_A, self.fids_B = [], []\n        self.mIoUs = []\n        self.is_best = False\n        self.npz_A = np.load(opt.real_stat_A_path)\n        self.npz_B = np.load(opt.real_stat_B_path)\n\n    def set_input(self, input):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option \'direction\' can be used to swap domain A and domain B.\n        """"""\n        # Since it is a cycle.\n        self.real_A = input[\'A\'].to(self.device)\n        self.real_B = input[\'B\'].to(self.device)\n\n    def set_single_input(self, input):\n        self.real_A = input[\'A\'].to(self.device)\n        self.image_paths = input[\'A_paths\']\n\n    def forward(self):\n        """"""Run forward pass; called by both functions <optimize_parameters> and <test>.""""""\n        self.fake_B = self.netG_A(self.real_A)  # G_A(A)\n        self.rec_A = self.netG_B(self.fake_B)  # G_B(G_A(A))\n        self.fake_A = self.netG_B(self.real_B)  # G_B(B)\n        self.rec_B = self.netG_A(self.fake_A)  # G_A(G_B(B))\n\n    def backward_D_basic(self, netD, real, fake):\n        """"""Calculate GAN loss for the discriminator\n\n        Parameters:\n            netD (network)      -- the discriminator D\n            real (tensor array) -- real images\n            fake (tensor array) -- images generated by a generator\n\n        Return the discriminator loss.\n        We also call loss_D.backward() to calculate the gradients.\n        """"""\n        # Real\n        pred_real = netD(real)\n        loss_D_real = self.criterionGAN(pred_real, True)\n        # Fake\n        pred_fake = netD(fake.detach())\n        loss_D_fake = self.criterionGAN(pred_fake, False)\n        # Combined loss and calculate gradients\n        loss_D = (loss_D_real + loss_D_fake) * 0.5\n        loss_D.backward()\n        return loss_D\n\n    def backward_D_A(self):\n        """"""Calculate GAN loss for discriminator D_A""""""\n        fake_B = self.fake_B_pool.query(self.fake_B)\n        self.loss_D_A = self.backward_D_basic(self.netD_A, self.real_B, fake_B)\n\n    def backward_D_B(self):\n        """"""Calculate GAN loss for discriminator D_B""""""\n        fake_A = self.fake_A_pool.query(self.fake_A)\n        self.loss_D_B = self.backward_D_basic(self.netD_B, self.real_A, fake_A)\n\n    def backward_G(self):\n        """"""Calculate the loss for generators G_A and G_B""""""\n        lambda_idt = self.opt.lambda_identity\n        lambda_A = self.opt.lambda_A\n        lambda_B = self.opt.lambda_B\n        # Identity loss\n        if lambda_idt > 0:\n            # G_A should be identity if real_B is fed: ||G_A(B) - B||\n            self.idt_A = self.netG_A(self.real_B)\n            self.loss_G_idt_A = self.criterionIdt(self.idt_A, self.real_B) * lambda_B * lambda_idt\n            # G_B should be identity if real_A is fed: ||G_B(A) - A||\n            self.idt_B = self.netG_B(self.real_A)\n            self.loss_G_idt_B = self.criterionIdt(self.idt_B, self.real_A) * lambda_A * lambda_idt\n        else:\n            self.loss_G_idt_A = 0\n            self.loss_G_idt_B = 0\n\n        # GAN loss D_A(G_A(A))\n        self.loss_G_A = self.criterionGAN(self.netD_A(self.fake_B), True)\n        # GAN loss D_B(G_B(B))\n        self.loss_G_B = self.criterionGAN(self.netD_B(self.fake_A), True)\n        # Forward cycle loss || G_B(G_A(A)) - A||\n        self.loss_G_cycle_A = self.criterionCycle(self.rec_A, self.real_A) * lambda_A\n        # Backward cycle loss || G_A(G_B(B)) - B||\n        self.loss_G_cycle_B = self.criterionCycle(self.rec_B, self.real_B) * lambda_B\n        # combined loss and calculate gradients\n        self.loss_G = self.loss_G_A + self.loss_G_B + self.loss_G_cycle_A + self.loss_G_cycle_B + self.loss_G_idt_A + self.loss_G_idt_B\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # forward\n        self.forward()  # compute fake images and reconstruction images.\n        # G_A and G_B\n        self.set_requires_grad([self.netD_A, self.netD_B], False)  # Ds require no gradients when optimizing Gs\n        self.optimizer_G.zero_grad()  # set G_A and G_B\'s gradients to zero\n        self.backward_G()  # calculate gradients for G_A and G_B\n        self.optimizer_G.step()  # update G_A and G_B\'s weights\n        # D_A and D_B\n        self.set_requires_grad([self.netD_A, self.netD_B], True)\n        self.optimizer_D.zero_grad()  # set D_A and D_B\'s gradients to zero\n        self.backward_D_A()  # calculate gradients for D_A\n        self.backward_D_B()  # calculate graidents for D_B\n        self.optimizer_D.step()  # update D_A and D_B\'s weights\n\n    def test_single_side(self, direction):\n        generator = getattr(self, \'netG_%s\' % direction[0])\n        with torch.no_grad():\n            self.fake_B = generator(self.real_A)\n\n    def evaluate_model(self, step):\n        ret = {}\n        self.is_best = False\n        save_dir = os.path.join(self.opt.log_dir, \'eval\', str(step))\n        os.makedirs(save_dir, exist_ok=True)\n        self.netG_A.eval()\n        self.netG_B.eval()\n        for direction in [\'AtoB\', \'BtoA\']:\n            eval_dataloader = getattr(self, \'eval_dataloader_\' + direction)\n            fakes, names = [], []\n            cnt = 0\n            # print(len(eval_dataset))\n            for i, data_i in enumerate(tqdm(eval_dataloader)):\n                self.set_single_input(data_i)\n                self.test_single_side(direction)\n                # print(self.image_paths)\n                fakes.append(self.fake_B.cpu())\n                for j in range(len(self.image_paths)):\n                    short_path = ntpath.basename(self.image_paths[j])\n                    name = os.path.splitext(short_path)[0]\n                    names.append(name)\n                    if cnt < 10:\n                        input_im = util.tensor2im(self.real_A[j])\n                        fake_im = util.tensor2im(self.fake_B[j])\n                        util.save_image(input_im, os.path.join(save_dir, direction, \'input\', \'%s.png\' % name),\n                                        create_dir=True)\n                        util.save_image(fake_im, os.path.join(save_dir, direction, \'fake\', \'%s.png\' % name),\n                                        create_dir=True)\n                    cnt += 1\n\n            suffix = direction[-1]\n            fid = get_fid(fakes, self.inception_model, getattr(self, \'npz_%s\' % direction[-1]),\n                          device=self.device, batch_size=self.opt.eval_batch_size)\n            if fid < getattr(self, \'best_fid_%s\' % suffix):\n                self.is_best = True\n                setattr(self, \'best_fid_%s\' % suffix, fid)\n            fids = getattr(self, \'fids_%s\' % suffix)\n            fids.append(fid)\n            if len(fids) > 3:\n                fids.pop(0)\n            ret[\'metric/fid_%s\' % suffix] = fid\n            ret[\'metric/fid_%s-mean\' % suffix] = sum(getattr(self, \'fids_%s\' % suffix)) / len(\n                getattr(self, \'fids_%s\' % suffix))\n            ret[\'metric/fid_%s-best\' % suffix] = getattr(self, \'best_fid_%s\' % suffix)\n\n            if \'cityscapes\' in self.opt.dataroot and direction == \'BtoA\':\n                mIoU = get_mIoU(fakes, names, self.drn_model, self.device,\n                               table_path=self.opt.table_path,\n                               data_dir=self.opt.cityscapes_path,\n                               batch_size=self.opt.eval_batch_size,\n                               num_workers=self.opt.num_threads)\n                if mIoU > self.best_mIoU:\n                    self.is_best = True\n                    self.best_mIoU = mIoU\n                self.mIoUs.append(mIoU)\n                if len(self.mIoUs) > 3:\n                    self.mIoUs = self.mIoUs[1:]\n                ret[\'metric/mIoU\'] = mIoU\n                ret[\'metric/mIoU-mean\'] = sum(self.mIoUs) / len(self.mIoUs)\n                ret[\'metric/mIoU-best\'] = self.best_mIoU\n\n        self.netG_A.train()\n        self.netG_B.train()\n        return ret\n'"
models/networks.py,6,"b'import functools\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import init\nfrom torch.optim import lr_scheduler\n\nfrom configs import decode_config\n\n\n###############################################################################\n# Helper Functions\n###############################################################################\n\nclass BaseNetwork(nn.Module):\n    def __init__(self):\n        super(BaseNetwork, self).__init__()\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        return parser\n\n\nclass Identity(nn.Module):\n    def forward(self, x):\n        return x\n\n\ndef get_norm_layer(norm_type=\'instance\'):\n    """"""Return a normalization layer\n\n    Parameters:\n        norm_type (str) -- the name of the normalization layer: batch | instance | none\n\n    For BatchNorm, we use learnable affine parameters and track running statistics (mean/stddev).\n    For InstanceNorm, we do not use learnable affine parameters. We do not track running statistics.\n    """"""\n    if norm_type == \'batch\':\n        norm_layer = functools.partial(nn.BatchNorm2d, affine=True, track_running_stats=True)\n    elif norm_type == \'instance\':\n        norm_layer = functools.partial(nn.InstanceNorm2d, affine=False, track_running_stats=False)\n    elif norm_type == \'none\':\n        def norm_layer(x):\n            return Identity()\n    else:\n        raise NotImplementedError(\'normalization layer [%s] is not found\' % norm_type)\n    return norm_layer\n\n\ndef get_scheduler(optimizer, opt):\n    """"""Return a learning rate scheduler\n\n    Parameters:\n        optimizer          -- the optimizer of the network\n        opt (option class) -- stores all the experiment flags; needs to be a subclass of BaseOptions\xef\xbc\x8e\xe3\x80\x80\n                              opt.lr_policy is the name of learning rate policy: linear | step | plateau | cosine\n\n    For \'linear\', we keep the same learning rate for the first <opt.niter> epochs\n    and linearly decay the rate to zero over the next <opt.niter_decay> epochs.\n    For other schedulers (step, plateau, and cosine), we use the default PyTorch schedulers.\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    """"""\n    if opt.lr_policy == \'linear\':\n        def lambda_rule(epoch):\n            lr_l = 1.0 - max(0, epoch + 1 - opt.nepochs) / float(opt.nepochs_decay + 1)\n            return lr_l\n\n        scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda_rule)\n    elif opt.lr_policy == \'step\':\n        scheduler = lr_scheduler.StepLR(optimizer, step_size=opt.lr_decay_iters, gamma=0.1)\n    elif opt.lr_policy == \'plateau\':\n        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\'min\', factor=0.2, threshold=0.01, patience=5)\n    elif opt.lr_policy == \'cosine\':\n        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=opt.niter, eta_min=0)\n    else:\n        return NotImplementedError(\'learning rate policy [%s] is not implemented\', opt.lr_policy)\n    return scheduler\n\n\ndef init_weights(net, init_type=\'normal\', init_gain=0.02):\n    """"""Initialize network weights.\n\n    Parameters:\n        net (network)   -- network to be initialized\n        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n\n    We use \'normal\' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n    work better for some applications. Feel free to try yourself.\n    """"""\n\n    def init_func(m):  # define the initialization function\n        classname = m.__class__.__name__\n        if hasattr(m, \'weight\') and (classname.find(\'Conv\') != -1 or classname.find(\'Linear\') != -1) \\\n                and classname.find(\'SCC\') == -1:\n            if init_type == \'normal\':\n                init.normal_(m.weight.data, 0.0, init_gain)\n            elif init_type == \'xavier\':\n                init.xavier_normal_(m.weight.data, gain=init_gain)\n            elif init_type == \'kaiming\':\n                init.kaiming_normal_(m.weight.data, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                init.orthogonal_(m.weight.data, gain=init_gain)\n            else:\n                raise NotImplementedError(\'initialization method [%s] is not implemented\' % init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(\n                \'BatchNorm2d\') != -1:  # BatchNorm Layer\'s weight is not a matrix; only normal distribution applies.\n            if hasattr(m, \'weight\') and m.weight is not None:\n                init.normal_(m.weight.data, 1.0, init_gain)\n            if hasattr(m, \'bias\') and m.weight is not None:\n                init.constant_(m.bias.data, 0.0)\n\n    print(\'initialize network with %s\' % init_type)\n    net.apply(init_func)  # apply the initialization function <init_func>\n\n\ndef init_net(net, init_type=\'normal\', init_gain=0.02, gpu_ids=[]):\n    """"""Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n    Parameters:\n        net (network)      -- the network to be initialized\n        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Return an initialized network.\n    """"""\n    if len(gpu_ids) > 0:\n        assert (torch.cuda.is_available())\n        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n    init_weights(net, init_type, init_gain=init_gain)\n    return net\n\n\ndef define_G(input_nc, output_nc, ngf, netG, norm=\'batch\', dropout_rate=0,\n             init_type=\'normal\', init_gain=0.02, gpu_ids=[], opt=None):\n    norm_layer = get_norm_layer(norm_type=norm)\n    if netG == \'resnet_9blocks\':\n        from .modules.resnet_architecture.resnet_generator import ResnetGenerator\n        net = ResnetGenerator(input_nc, output_nc, ngf, norm_layer=norm_layer,\n                              dropout_rate=dropout_rate, n_blocks=9)\n    elif netG == \'mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.mobile_resnet_generator import MobileResnetGenerator\n        net = MobileResnetGenerator(input_nc, output_nc, ngf=ngf, norm_layer=norm_layer,\n                                    dropout_rate=dropout_rate, n_blocks=9)\n    elif netG == \'super_mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.super_mobile_resnet_generator import SuperMobileResnetGenerator\n        net = SuperMobileResnetGenerator(input_nc, output_nc, ngf=ngf, norm_layer=norm_layer,\n                                         dropout_rate=dropout_rate, n_blocks=9)\n    elif netG == \'sub_mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.sub_mobile_resnet_generator import SubMobileResnetGenerator\n        assert opt.config_str is not None\n        config = decode_config(opt.config_str)\n        net = SubMobileResnetGenerator(input_nc, output_nc, config, norm_layer=norm_layer,\n                                       dropout_rate=dropout_rate, n_blocks=9)\n    elif netG == \'spade\':\n        from .modules.spade_architecture.spade_generator import SPADEGenerator\n        net = SPADEGenerator(opt)\n    elif netG == \'mobile_spade\':\n        from .modules.spade_architecture.mobile_spade_generator import MobileSPADEGenerator\n        net = MobileSPADEGenerator(opt)\n    elif netG == \'sub_mobile_spade\':\n        from .modules.spade_architecture.sub_mobile_spade_generator import SubMobileSPADEGenerator\n        assert opt.config_str is not None\n        config = decode_config((opt.config_str))\n        net = SubMobileSPADEGenerator(opt, config)\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % netG)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\ndef define_D(input_nc, ndf, netD, n_layers_D=3, norm=\'batch\', init_type=\'normal\', init_gain=0.02, gpu_ids=[], opt=None):\n    """"""Create a discriminator\n\n    Parameters:\n        input_nc (int)     -- the number of channels in input images\n        ndf (int)          -- the number of filters in the first conv layer\n        netD (str)         -- the architecture\'s name: basic | n_layers | pixel\n        n_layers_D (int)   -- the number of conv layers in the discriminator; effective when netD==\'n_layers\'\n        norm (str)         -- the type of normalization layers used in the network.\n        init_type (str)    -- the name of the initialization method.\n        init_gain (float)  -- scaling factor for normal, xavier and orthogonal.\n        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n\n    Returns a discriminator\n\n    Our current implementation provides three types of discriminators:\n        [basic]: \'PatchGAN\' classifier described in the original pix2pix paper.\n        It can classify whether 70\xc3\x9770 overlapping patches are real or fake.\n        Such a patch-level discriminator architecture has fewer parameters\n        than a full-image discriminator and can work on arbitrarily-sized images\n        in a fully convolutional fashion.\n\n        [n_layers]: With this mode, you cna specify the number of conv layers in the discriminator\n        with the parameter <n_layers_D> (default=3 as used in [basic] (PatchGAN).)\n\n        [pixel]: 1x1 PixelGAN discriminator can classify whether a pixel is real or not.\n        It encourages greater color diversity but has no effect on spatial statistics.\n\n    The discriminator has been initialized by <init_net>. It uses Leakly RELU for non-linearity.\n    """"""\n    norm_layer = get_norm_layer(norm_type=norm)\n    if netD == \'n_layers\':\n        from .modules.discriminators import NLayerDiscriminator\n        net = NLayerDiscriminator(input_nc, ndf, n_layers_D, norm_layer=norm_layer)\n    elif netD == \'pixel\':  # classify if each pixel is real or fake\n        from .modules.discriminators import PixelDiscriminator\n        net = PixelDiscriminator(input_nc, ndf, norm_layer=norm_layer)\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' % netD)\n    return init_net(net, init_type, init_gain, gpu_ids)\n\n\ndef get_netG_cls(netG):\n    if netG == \'resnet_9blocks\':\n        from .modules.resnet_architecture.resnet_generator import ResnetGenerator\n        return ResnetGenerator\n    elif netG == \'mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.mobile_resnet_generator import MobileResnetGenerator\n        return MobileResnetGenerator\n    elif netG == \'super_mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.super_mobile_resnet_generator import SuperMobileResnetGenerator\n        return SuperMobileResnetGenerator\n    elif netG == \'sub_mobile_resnet_9blocks\':\n        from .modules.resnet_architecture.sub_mobile_resnet_generator import SubMobileResnetGenerator\n        return SubMobileResnetGenerator\n    elif netG == \'spade\':\n        from .modules.spade_architecture.spade_generator import SPADEGenerator\n        return SPADEGenerator\n    elif netG == \'mobile_spade\':\n        from .modules.spade_architecture.mobile_spade_generator import MobileSPADEGenerator\n        return MobileSPADEGenerator\n    elif netG == \'sub_mobile_spade\':\n        from .modules.spade_architecture.sub_mobile_spade_generator import SubMobileSPADEGenerator\n        return SubMobileSPADEGenerator\n    else:\n        raise NotImplementedError(\'Generator model name [%s] is not recognized\' % netG)\n\n\ndef get_netD_cls(netD):\n    if netD == \'n_layers\':\n        from .modules.discriminators import NLayerDiscriminator\n        return NLayerDiscriminator\n    elif netD == \'pixel\':  # classify if each pixel is real or fake\n        from .modules.discriminators import PixelDiscriminator\n        return PixelDiscriminator\n    elif netD == \'Multiscale\':\n        from .modules.discriminators import MultiscaleDiscriminator\n        return MultiscaleDiscriminator\n    else:\n        raise NotImplementedError(\'Discriminator model name [%s] is not recognized\' % netD)\n\n\ndef modify_commandline_options(parser, is_train):\n    opt, _ = parser.parse_known_args()\n\n    netG_cls = get_netG_cls(opt.netG)\n    parser = netG_cls.modify_commandline_options(parser, is_train)\n    if is_train:\n        netD_cls = get_netD_cls(opt.netD)\n        parser = netD_cls.modify_commandline_options(parser, is_train)\n\n    return parser\n'"
models/pix2pix_model.py,8,"b'import ntpath\nimport os\n\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom tqdm import tqdm\n\nimport models.modules.loss\nfrom data import create_eval_dataloader\nfrom metric import get_fid, get_mIoU\nfrom metric.inception import InceptionV3\nfrom metric.mIoU_score import DRNSeg\nfrom models import networks\nfrom models.base_model import BaseModel\nfrom utils import util\n\n\nclass Pix2PixModel(BaseModel):\n\n    @staticmethod\n    def modify_commandline_options(parser, is_train=True):\n        assert is_train\n        parser = super(Pix2PixModel, Pix2PixModel).modify_commandline_options(parser, is_train)\n        parser.add_argument(\'--restore_G_path\', type=str, default=None,\n                            help=\'the path to restore the generator\')\n        parser.add_argument(\'--restore_D_path\', type=str, default=None,\n                            help=\'the path to restore the discriminator\')\n        parser.add_argument(\'--recon_loss_type\', type=str, default=\'l1\',\n                            choices=[\'l1\', \'l2\', \'smooth_l1\'],\n                            help=\'the type of the reconstruction loss\')\n        parser.add_argument(\'--lambda_recon\', type=float, default=100,\n                            help=\'weight for reconstruction loss\')\n        parser.add_argument(\'--lambda_gan\', type=float, default=1,\n                            help=\'weight for gan loss\')\n        parser.add_argument(\'--real_stat_path\', type=str, required=True,\n                            help=\'the path to load the groud-truth images information to compute FID.\')\n        parser.set_defaults(norm=\'instance\', netG=\'mobile_resnet_9blocks\', batch_size=4,\n                            dataset_mode=\'aligned\', log_dir=\'logs/train/pix2pix\',\n                            pool_size=0, gan_mode=\'hinge\')\n        return parser\n\n    def __init__(self, opt):\n        """"""Initialize the pix2pix class.\n\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        """"""\n        assert opt.isTrain\n        BaseModel.__init__(self, opt)\n        # specify the training losses you want to print out. The training/test scripts will call <BaseModel.get_current_losses>\n        self.loss_names = [\'G_gan\', \'G_recon\', \'D_real\', \'D_fake\']\n        # specify the images you want to save/display. The training/test scripts will call <BaseModel.get_current_visuals>\n        self.visual_names = [\'real_A\', \'fake_B\', \'real_B\']\n        # specify the models you want to save to the disk. The training/test scripts will call <BaseModel.save_networks> and <BaseModel.load_networks>\n        self.model_names = [\'G\', \'D\']\n        # define networks (both generator and discriminator)\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG,\n                                      opt.norm, opt.dropout_rate, opt.init_type,\n                                      opt.init_gain, self.gpu_ids, opt=opt)\n\n        self.netD = networks.define_D(opt.input_nc + opt.output_nc, opt.ndf, opt.netD,\n                                      opt.n_layers_D, opt.norm, opt.init_type, opt.init_gain, self.gpu_ids)\n\n        # define loss functions\n        self.criterionGAN = models.modules.loss.GANLoss(opt.gan_mode).to(self.device)\n        if opt.recon_loss_type == \'l1\':\n            self.criterionRecon = torch.nn.L1Loss()\n        elif opt.recon_loss_type == \'l2\':\n            self.criterionRecon = torch.nn.MSELoss()\n        elif opt.recon_loss_type == \'smooth_l1\':\n            self.criterionRecon = torch.nn.SmoothL1Loss()\n        else:\n            raise NotImplementedError(\'Unknown reconstruction loss type [%s]!\' % opt.loss_type)\n        # initialize optimizers; schedulers will be automatically created by function <BaseModel.setup>.\n        self.optimizer_G = torch.optim.Adam(self.netG.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizer_D = torch.optim.Adam(self.netD.parameters(), lr=opt.lr, betas=(opt.beta1, 0.999))\n        self.optimizers = []\n        self.optimizers.append(self.optimizer_G)\n        self.optimizers.append(self.optimizer_D)\n\n        self.eval_dataloader = create_eval_dataloader(self.opt)\n\n        block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n        self.inception_model = InceptionV3([block_idx])\n        self.inception_model.to(self.device)\n        self.inception_model.eval()\n\n        if \'cityscapes\' in opt.dataroot:\n            self.drn_model = DRNSeg(\'drn_d_105\', 19, pretrained=False)\n            util.load_network(self.drn_model, opt.drn_path, verbose=False)\n            if len(opt.gpu_ids) > 0:\n                self.drn_model = nn.DataParallel(self.drn_model, opt.gpu_ids)\n            self.drn_model.eval()\n\n        self.best_fid = 1e9\n        self.best_mIoU = -1e9\n        self.fids, self.mIoUs = [], []\n        self.is_best = False\n        self.Tacts, self.Sacts = {}, {}\n        self.npz = np.load(opt.real_stat_path)\n\n    def set_input(self, input):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters:\n            input (dict): include the data itself and its metadata information.\n\n        The option \'direction\' can be used to swap images in domain A and domain B.\n        """"""\n        AtoB = self.opt.direction == \'AtoB\'\n        self.real_A = input[\'A\' if AtoB else \'B\'].to(self.device)\n        self.real_B = input[\'B\' if AtoB else \'A\'].to(self.device)\n        self.image_paths = input[\'A_paths\' if AtoB else \'B_paths\']\n\n    def forward(self):\n        """"""Run forward pass; called by both functions <optimize_parameters> and <test>.""""""\n        self.fake_B = self.netG(self.real_A)  # G(A)\n\n    def backward_D(self):\n        """"""Calculate GAN loss for the discriminator""""""\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1).detach()\n        real_AB = torch.cat((self.real_A, self.real_B), 1).detach()\n        pred_fake = self.netD(fake_AB)\n        self.loss_D_fake = self.criterionGAN(pred_fake, False, for_discriminator=True)\n\n        pred_real = self.netD(real_AB)\n        self.loss_D_real = self.criterionGAN(pred_real, True, for_discriminator=True)\n\n        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n        self.loss_D.backward()\n\n    def backward_G(self):\n        """"""Calculate GAN and L1 loss for the generator""""""\n        # First, G(A) should fake the discriminator\n        fake_AB = torch.cat((self.real_A, self.fake_B), 1)\n        pred_fake = self.netD(fake_AB)\n        self.loss_G_gan = self.criterionGAN(pred_fake, True, for_discriminator=False) * self.opt.lambda_gan\n        # Second, G(A) = B\n        self.loss_G_recon = self.criterionRecon(self.fake_B, self.real_B) * self.opt.lambda_recon\n        # combine loss and calculate gradients\n\n        self.loss_G = self.loss_G_gan + self.loss_G_recon\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.forward()  # compute fake images: G(A)\n        # update D\n        self.set_requires_grad(self.netD, True)  # enable backprop for D\n        self.optimizer_D.zero_grad()  # set D\'s gradients to zero\n        self.backward_D()  # calculate gradients for D\n        self.optimizer_D.step()  # update D\'s weights\n        # update G\n        self.set_requires_grad(self.netD, False)  # D requires no gradients when optimizing G\n        self.optimizer_G.zero_grad()  # set G\'s gradients to zero\n        self.backward_G()  # calculate graidents for G\n        self.optimizer_G.step()  # udpate G\'s weights\n\n    def evaluate_model(self, step):\n        self.is_best = False\n\n        save_dir = os.path.join(self.opt.log_dir, \'eval\', str(step))\n        os.makedirs(save_dir, exist_ok=True)\n        self.netG.eval()\n\n        fakes, names = [], []\n        cnt = 0\n        for i, data_i in enumerate(tqdm(self.eval_dataloader)):\n            self.set_input(data_i)\n            self.test()\n            fakes.append(self.fake_B.cpu())\n            for j in range(len(self.image_paths)):\n                short_path = ntpath.basename(self.image_paths[j])\n                name = os.path.splitext(short_path)[0]\n                names.append(name)\n                if cnt < 10:\n                    input_im = util.tensor2im(self.real_A[j])\n                    real_im = util.tensor2im(self.real_B[j])\n                    fake_im = util.tensor2im(self.fake_B[j])\n                    util.save_image(input_im, os.path.join(save_dir, \'input\', \'%s.png\' % name), create_dir=True)\n                    util.save_image(real_im, os.path.join(save_dir, \'real\', \'%s.png\' % name), create_dir=True)\n                    util.save_image(fake_im, os.path.join(save_dir, \'fake\', \'%s.png\' % name), create_dir=True)\n                cnt += 1\n\n        fid = get_fid(fakes, self.inception_model, self.npz,\n                      device=self.device, batch_size=self.opt.eval_batch_size)\n        if fid < self.best_fid:\n            self.is_best = True\n            self.best_fid = fid\n        self.fids.append(fid)\n        if len(self.fids) > 3:\n            self.fids.pop(0)\n\n        ret = {\'metric/fid\': fid, \'metric/fid-mean\': sum(self.fids) / len(self.fids), \'metric/fid-best\': self.best_fid}\n        if \'cityscapes\' in self.opt.dataroot:\n            mIoU = get_mIoU(fakes, names, self.drn_model, self.device,\n                           table_path=self.opt.table_path,\n                           data_dir=self.opt.cityscapes_path,\n                           batch_size=self.opt.eval_batch_size,\n                           num_workers=self.opt.num_threads)\n            if mIoU > self.best_mIoU:\n                self.is_best = True\n                self.best_mIoU = mIoU\n            self.mIoUs.append(mIoU)\n            if len(self.mIoUs) > 3:\n                self.mIoUs = self.mIoUs[1:]\n            ret[\'metric/mIoU\'] = mIoU\n            ret[\'metric/mIoU-mean\'] = sum(self.mIoUs) / len(self.mIoUs)\n            ret[\'metric/mIoU-best\'] = self.best_mIoU\n\n        self.netG.train()\n        return ret\n'"
models/spade_model.py,5,"b'import argparse\n\nimport torch\nfrom torch import nn\nfrom torchprofile import profile_macs\n\nfrom models import networks\nfrom .base_model import BaseModel\n\n\nclass SPADEModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert isinstance(parser, argparse.ArgumentParser)\n        parser.add_argument(\'--z_dim\', type=int, default=256,\n                            help=""dimension of the latent z vector"")\n        parser.set_defaults(netG=\'sub_mobile_spade\')\n        parser = networks.modify_commandline_options(parser, is_train)\n        return parser\n\n    def __init__(self, opt):\n        super(SPADEModel, self).__init__(opt)\n        self.model_names = [\'G\']\n        self.visual_names = [\'labels\', \'fake_B\', \'real_B\']\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG,\n                                      opt.norm, opt.dropout_rate, opt.init_type,\n                                      opt.init_gain, self.gpu_ids, opt=opt)\n        if opt.isTrain:\n            raise NotImplementedError(""Training mode of SPADE is currently not supported!!!"")\n        else:\n            self.netG.eval()\n\n    def set_input(self, input):\n        self.data = input\n        self.image_paths = input[\'path\']\n        self.labels = input[\'label\'].to(self.device)\n        self.input_semantics, self.real_B = self.preprocess_input(input)\n\n    def test(self, config=None):\n        with torch.no_grad():\n            self.forward()\n\n    def preprocess_input(self, data):\n        # move to GPU and change data types\n        data[\'label\'] = data[\'label\'].long()\n        # if self.use_gpu():\n        data[\'label\'] = data[\'label\'].to(self.device)\n        data[\'instance\'] = data[\'instance\'].to(self.device)\n        data[\'image\'] = data[\'image\'].to(self.device)\n\n        # create one-hot label map\n        label_map = data[\'label\']\n        bs, _, h, w = label_map.size()\n        nc = self.opt.input_nc + 1 if self.opt.contain_dontcare_label \\\n            else self.opt.input_nc\n        input_label = torch.zeros([bs, nc, h, w], device=self.device)\n        input_semantics = input_label.scatter_(1, label_map, 1.0)\n\n        # concatenate instance map if it exists\n        if not self.opt.no_instance:\n            inst_map = data[\'instance\']\n            instance_edge_map = self.get_edges(inst_map)\n            input_semantics = torch.cat((input_semantics, instance_edge_map), dim=1)\n\n        return input_semantics, data[\'image\']\n\n    def forward(self):\n        self.fake_B = self.netG(self.input_semantics)\n\n    def get_edges(self, t):\n        edge = torch.zeros(t.size(), dtype=torch.uint8, device=self.device)\n        edge[:, :, :, 1:] = edge[:, :, :, 1:] | ((t[:, :, :, 1:] != t[:, :, :, :-1]).byte())\n        edge[:, :, :, :-1] = edge[:, :, :, :-1] | ((t[:, :, :, 1:] != t[:, :, :, :-1]).byte())\n        edge[:, :, 1:, :] = edge[:, :, 1:, :] | ((t[:, :, 1:, :] != t[:, :, :-1, :]).byte())\n        edge[:, :, :-1, :] = edge[:, :, :-1, :] | ((t[:, :, 1:, :] != t[:, :, :-1, :]).byte())\n        return edge.float()\n\n    def profile(self, config=None, verbose=True):\n        netG = self.netG\n        if isinstance(netG, nn.DataParallel):\n            netG = netG.module\n        if config is not None:\n            netG.configs = config\n        with torch.no_grad():\n            macs = profile_macs(netG, (self.input_semantics,))\n        params = 0\n        for p in netG.parameters():\n            params += p.numel()\n        if verbose:\n            print(\'MACs: %.3fG\\tParams: %.3fM\' % (macs / 1e9, params / 1e6), flush=True)\n        return macs, params\n\n    def optimize_parameters(self):\n        raise NotImplementedError\n'"
models/test_model.py,2,"b""import torch\nfrom torch import nn\nfrom torchprofile import profile_macs\n\nfrom models import networks\nfrom .base_model import BaseModel\n\n\nclass TestModel(BaseModel):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert not is_train\n        return parser\n\n    def __init__(self, opt):\n        super(TestModel, self).__init__(opt)\n        self.visual_names = ['real_A', 'fake_B', 'real_B']\n        self.model_names = ['G']\n        self.netG = networks.define_G(opt.input_nc, opt.output_nc, opt.ngf, opt.netG,\n                                      opt.norm, opt.dropout_rate, opt.init_type,\n                                      opt.init_gain, self.gpu_ids, opt=opt)\n        self.netG.eval()\n\n    def set_input(self, input):\n        AtoB = self.opt.direction == 'AtoB'\n        self.real_A = input['A' if AtoB else 'B'].to(self.device)\n        if self.opt.dataset_mode != 'single':\n            self.real_B = input['B' if AtoB else 'A'].to(self.device)\n        self.image_paths = input['A_paths' if AtoB else 'B_paths']\n\n    def forward(self, config=None):\n        if config is not None:\n            self.netG.configs = config\n        self.fake_B = self.netG(self.real_A)\n\n    def optimize_parameters(self):\n        assert False, 'This model is only for testing, you cannot optimize the parameters!!!'\n\n    def save_networks(self, epoch):\n        assert False, 'This model is only for testing!!!'\n\n    def profile(self, config=None, verbose=True):\n        netG = self.netG\n        if isinstance(netG, nn.DataParallel):\n            netG = netG.module\n        if config is not None:\n            netG.configs = config\n        with torch.no_grad():\n            macs = profile_macs(netG, (self.real_A[:1],))\n        params = 0\n        for p in netG.parameters():\n            params += p.numel()\n        if verbose:\n            print('MACs: %.3fG\\tParams: %.3fM' % (macs / 1e9, params / 1e6), flush=True)\n        return macs, params\n\n    def test(self, config=None):\n        with torch.no_grad():\n            self.forward(config)\n\n    def get_current_losses(self):\n        assert False, 'This model is only for testing!!!'\n\n    def update_learning_rate(self, f=None):\n        assert False, 'This model is only for testing!!!'\n"""
options/__init__.py,0,"b'""""""This package options includes option modules: training options, test options, and basic options (used in both training and test).""""""\n'"
options/base_options.py,1,"b'import argparse\nimport os\nimport pickle\n\nimport torch\n\nimport data\nimport models\n\n\nclass BaseOptions:\n    """"""This class defines options used during both training and test time.\n\n    It also implements several helper functions such as parsing, printing, and saving the options.\n    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.\n    """"""\n\n    def __init__(self):\n        """"""Reset the class; indicates the class hasn\'t been initailized""""""\n        self.isTrain = True\n\n    def initialize(self, parser):\n        """"""Define the common options that are used in both training and test.""""""\n        # basic parameters\n        parser.add_argument(\'--dataroot\', required=True,\n                            help=\'path to images (should have subfolders trainA, trainB, valA, valB, train, val, etc)\')\n        parser.add_argument(\'--gpu_ids\', type=str, default=\'0\', help=\'gpu ids: e.g. 0  0,1,2, 0,2. use -1 for CPU\')\n        parser.add_argument(\'--seed\', type=int, default=233, help=\'random seed\')\n\n        # model parameters\n        parser.add_argument(\'--input_nc\', type=int, default=3,\n                            help=\'# of input image channels: 3 for RGB and 1 for grayscale\')\n        parser.add_argument(\'--output_nc\', type=int, default=3,\n                            help=\'# of output image channels: 3 for RGB and 1 for grayscale\')\n        parser.add_argument(\'--norm\', type=str, default=\'instance\',\n                            help=\'instance normalization or batch normalization [instance | batch | none]\')\n        parser.add_argument(\'--init_type\', type=str, default=\'normal\',\n                            help=\'network initialization [normal | xavier | kaiming | orthogonal]\')\n        parser.add_argument(\'--init_gain\', type=float, default=0.02,\n                            help=\'scaling factor for normal, xavier and orthogonal.\')\n\n        # dataset parameters\n        parser.add_argument(\'--dataset_mode\', type=str, default=\'aligned\',\n                            help=\'chooses how datasets are loaded. [unaligned | aligned | single]\')\n        parser.add_argument(\'--direction\', type=str, default=\'AtoB\', help=\'AtoB or BtoA\')\n        parser.add_argument(\'--serial_batches\', action=\'store_true\',\n                            help=\'if true, takes images in order to make batches, otherwise takes them randomly\')\n        parser.add_argument(\'--num_threads\', default=4, type=int, help=\'# threads for loading data\')\n        parser.add_argument(\'--batch_size\', type=int, default=1, help=\'input batch size\')\n        parser.add_argument(\'--load_size\', type=int, default=286, help=\'scale images to this size\')\n        parser.add_argument(\'--crop_size\', type=int, default=256, help=\'then crop to this size\')\n        parser.add_argument(\'--aspect_ratio\', type=float, default=1.0,\n                            help=\'The ratio width/height. The final height of the load image will be crop_size/aspect_ratio\')\n        parser.add_argument(\'--max_dataset_size\', type=int, default=-1,\n                            help=\'Maximum number of samples allowed per dataset. If the dataset directory contains more than max_dataset_size, only a subset is loaded.\')\n        parser.add_argument(\'--preprocess\', type=str, default=\'resize_and_crop\',\n                            help=\'scaling and cropping of images at load time [resize_and_crop | crop | scale_width | scale_width_and_crop | none]\')\n        parser.add_argument(\'--no_flip\', action=\'store_true\',\n                            help=\'if specified, do not flip the images for data augmentation\')\n        parser.add_argument(\'--display_winsize\', type=int, default=256,\n                            help=\'display window size for both visdom and HTML\')\n        parser.add_argument(\'--load_in_memory\', action=\'store_true\',\n                            help=\'whether you will load the data into the memory to bypass the IO.\')\n        parser.add_argument(\'--phase\', type=str, default=\'train\', help=\'train, val, test, etc\')\n        parser.add_argument(\'--config_set\', type=str, default=None,\n                            help=\'the name of the configuration set for the set of subnet configurations.\')\n        parser.add_argument(\'--config_str\', type=str, default=None,\n                            help=\'the configuration string for a specific subnet in the supernet\')\n\n        # evaluation parameters\n        parser.add_argument(\'--drn_path\', type=str, default=\'drn-d-105_ms_cityscapes.pth\',\n                            help=\'the path to the pretrained drn path to compute mIoU\')\n        parser.add_argument(\'--cityscapes_path\', type=str, default=\'database/cityscapes-origin\',\n                            help=\'the original cityscapes dataset path (not the pix2pix preprocessed one)\')\n        parser.add_argument(\'--table_path\', type=str, default=\'datasets/table.txt\',\n                            help=\'the path to the mapping table (generated by datasets/prepare_cityscapes_dataset.py)\')\n        return parser\n\n    def gather_options(self):\n        # """"""Initialize our parser with basic options(only once).\n        # Add additional model-specific and dataset-specific options.\n        # These options are defined in the <modify_commandline_options> function\n        # in model and dataset classes.\n        # """"""\n        parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser = self.initialize(parser)\n\n        # get the basic options\n        opt, _ = parser.parse_known_args()\n\n        # modify model-related parser options\n        model_name = opt.model\n        model_option_setter = models.get_option_setter(model_name)\n        parser = model_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()  # parse again with new defaults\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        # save and return the parser\n        self.parser = parser\n        return parser.parse_args()\n\n    def print_options(self, opt):\n        message = \'\'\n        message += \'----------------- Options ---------------\\n\'\n        for k, v in sorted(vars(opt).items()):\n            comment = \'\'\n            default = self.parser.get_default(k)\n            if v != default:\n                comment = \'\\t[default: %s]\' % str(default)\n            message += \'{:>25}: {:<30}{}\\n\'.format(str(k), str(v), comment)\n        message += \'----------------- End -------------------\'\n        print(message)\n\n        # save to the disk\n        if self.isTrain:\n            expr_dir = os.path.join(opt.log_dir)\n            os.makedirs(expr_dir, exist_ok=True)\n            file_name = os.path.join(expr_dir, \'opt.txt\')\n            with open(file_name, \'wt\') as opt_file:\n                opt_file.write(message)\n                opt_file.write(\'\\n\')\n            file_name = os.path.join(expr_dir, \'opt.pkl\')\n            with open(file_name, \'wb\') as opt_file:\n                pickle.dump(opt, opt_file)\n\n    def parse(self, verbose=True):\n        opt = self.gather_options()\n        opt.isTrain = self.isTrain  # train or test\n\n        if hasattr(opt, \'contain_dontcare_label\') and hasattr(opt, \'no_instance\'):\n            opt.semantic_nc = opt.input_nc + (1 if opt.contain_dontcare_label else 0) + (0 if opt.no_instance else 1)\n\n        if verbose:\n            self.print_options(opt)\n\n        # set gpu ids\n        str_ids = opt.gpu_ids.split(\',\')\n        opt.gpu_ids = []\n        for str_id in str_ids:\n            id = int(str_id)\n            if id >= 0:\n                opt.gpu_ids.append(id)\n        if len(opt.gpu_ids) > 0:\n            torch.cuda.set_device(opt.gpu_ids[0])\n\n        self.opt = opt\n        return self.opt\n'"
options/distill_options.py,0,"b'import argparse\n\nimport data\nimport distillers\nfrom .base_options import BaseOptions\n\n\nclass DistillOptions(BaseOptions):\n    """"""This class defines options used during both training and test time.\n\n    It also implements several helper functions such as parsing, printing, and saving the options.\n    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.\n    """"""\n\n    def __init__(self, isTrain=True):\n        """"""Reset the class; indicates the class hasn\'t been initailized""""""\n        super(DistillOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        # log parameters\n        parser.add_argument(\'--log_dir\', type=str, default=\'logs/distill\',\n                            help=\'specify an experiment directory\')\n        parser.add_argument(\'--tensorboard_dir\', type=str, default=None,\n                            help=\'tensorboard is saved here\')\n        parser.add_argument(\'--print_freq\', type=int, default=100,\n                            help=\'frequency of showing training results on console\')\n        parser.add_argument(\'--save_latest_freq\', type=int, default=20000,\n                            help=\'frequency of evaluating and save the latest model\')\n        parser.add_argument(\'--save_epoch_freq\', type=int, default=5,\n                            help=\'frequency of saving checkpoints at the end of epoch\')\n        parser.add_argument(\'--epoch_base\', type=int, default=1,\n                            help=\'the epoch base of the training (used for resuming)\')\n        parser.add_argument(\'--iter_base\', type=int, default=1,\n                            help=\'the iteration base of the training (used for resuming)\')\n\n        # model parameters\n        parser.add_argument(\'--distiller\', type=str, default=\'resnet\',\n                            help=\'specify which distiller you want to use [resnet | spade]\')\n        parser.add_argument(\'--netD\', type=str, default=\'n_layers\',\n                            help=\'specify discriminator architecture [n_layers | pixel]. \'\n                                 \'The basic model is a 70x70 PatchGAN. \'\n                                 \'n_layers allows you to specify the layers in the discriminator\')\n        parser.add_argument(\'--ndf\', type=int, default=128, help=\'the base number of discriminator filters\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if netD==n_layers\')\n\n        # training parameters\n        parser.add_argument(\'--nepochs\', type=int, default=5,\n                            help=\'number of epochs with the initial learning rate\')\n        parser.add_argument(\'--nepochs_decay\', type=int, default=15,\n                            help=\'number of epochs to linearly decay learning rate to zero\')\n        parser.add_argument(\'--beta1\', type=float, default=0.5, help=\'momentum term of adam\')\n        parser.add_argument(\'--lr\', type=float, default=0.0002, help=\'initial learning rate for adam\')\n        parser.add_argument(\'--lr_policy\', type=str, default=\'linear\',\n                            help=\'learning rate policy. [linear | step | plateau | cosine]\')\n        parser.add_argument(\'--lr_decay_iters\', type=int, default=50,\n                            help=\'multiply by a gamma every lr_decay_iters iterations\')\n\n        parser.add_argument(\'--eval_batch_size\', type=int, default=1, help=\'the evaluation batch size\')\n        parser.add_argument(\'--real_stat_path\', type=str, required=True,\n                            help=\'the path to load the ground-truth images information to compute FID.\')\n\n        return parser\n\n    def gather_options(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser = self.initialize(parser)\n\n        opt, _ = parser.parse_known_args()\n\n        distiller_name = opt.distiller\n        distiller_option_setter = distillers.get_option_setter(distiller_name)\n        parser = distiller_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        # save and return the parser\n        self.parser = parser\n        return parser.parse_args()\n'"
options/search_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass SearchOptions(BaseOptions):\n    """"""This class includes test options.\n\n    It also includes shared options defined in BaseOptions.\n    """"""\n\n    def __init__(self, isTrain=False):\n        super(SearchOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)  # define shared options\n        parser.add_argument(\'--output_path\', type=str, default=None, required=True,\n                            help=\'the path to save the evaluation result.\')\n        parser.add_argument(\'--num_test\', type=int, default=float(\'inf\'), help=\'how many test images to run\')\n        parser.add_argument(\'--model\', type=str, default=\'test\', help=\'which model do you want test\')\n        parser.add_argument(\'--no_fid\', action=\'store_true\',\n                            help=\'whether you want to compute FID. \'\n                                 \'you can only activate this when the dataset is cityscapes!!!\')\n        parser.add_argument(\'--restore_G_path\', type=str, required=True, help=\'the path to restore the generator\')\n        parser.add_argument(\'--netG\', type=str, default=\'super_mobile_resnet_9blocks\',\n                            choices=[\'super_mobile_resnet_9blocks\'],\n                            help=\'specify generator architecture\')\n        parser.add_argument(\'--ngf\', type=int, default=48, help=\'the base number of filters of the student generator\')\n        parser.add_argument(\'--dropout_rate\', type=float, default=0, help=\'the dropout rate of the generator\')\n        parser.add_argument(\'--budget\', type=float, default=1e18, help=\'the MAC budget\')\n        parser.add_argument(\'--real_stat_path\', type=str, required=True,\n                            help=\'the path to load the ground-truth images information to compute FID.\')\n\n        # rewrite devalue values\n        parser.set_defaults(phase=\'val\', serial_batches=True, no_flip=True,\n                            load_size=parser.get_default(\'crop_size\'), load_in_memory=True)\n\n        return parser\n'"
options/supernet_options.py,0,"b'import argparse\n\nimport data\nimport supernets\nfrom .base_options import BaseOptions\n\n\nclass SupernetOptions(BaseOptions):\n    """"""This class defines options used during both training and test time.\n\n    It also implements several helper functions such as parsing, printing, and saving the options.\n    It also gathers additional options defined in <modify_commandline_options> functions in both dataset class and model class.\n    """"""\n\n    def __init__(self, isTrain=True):\n        """"""Reset the class; indicates the class hasn\'t been initailized""""""\n        super(SupernetOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        """"""Define the common options that are used in both training and test.""""""\n        parser = BaseOptions.initialize(self, parser)\n        # log parameters\n        parser.add_argument(\'--log_dir\', type=str, default=\'logs/distill\',\n                            help=\'specify an experiment directory\')\n        parser.add_argument(\'--tensorboard_dir\', type=str, default=None,\n                            help=\'tensorboard is saved here\')\n        parser.add_argument(\'--print_freq\', type=int, default=100,\n                            help=\'frequency of showing training results on console\')\n        parser.add_argument(\'--save_latest_freq\', type=int, default=20000,\n                            help=\'frequency of evaluating and save the latest model\')\n        parser.add_argument(\'--save_epoch_freq\', type=int, default=5,\n                            help=\'frequency of saving checkpoints at the end of epoch\')\n        parser.add_argument(\'--epoch_base\', type=int, default=1,\n                            help=\'the epoch base of the training (used for resuming)\')\n        parser.add_argument(\'--iter_base\', type=int, default=1,\n                            help=\'the iteration base of the training (used for resuming)\')\n\n        # model parameters\n        parser.add_argument(\'--supernet\', type=str, default=\'resnet\',\n                            help=\'specify which supernet you want to use [resnet | spade]\')\n        parser.add_argument(\'--netD\', type=str, default=\'n_layers\',\n                            help=\'specify discriminator architecture [n_layers | pixel]. \'\n                                 \'The basic model is a 70x70 PatchGAN. \'\n                                 \'n_layers allows you to specify the layers in the discriminator\')\n        parser.add_argument(\'--ndf\', type=int, default=128, help=\'the base number of discriminator filters\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if netD==n_layers\')\n\n        # training parameters\n        parser.add_argument(\'--nepochs\', type=int, default=10,\n                            help=\'number of epochs with the initial learning rate\')\n        parser.add_argument(\'--nepochs_decay\', type=int, default=30,\n                            help=\'number of epochs to linearly decay learning rate to zero\')\n        parser.add_argument(\'--beta1\', type=float, default=0.5, help=\'momentum term of adam\')\n        parser.add_argument(\'--lr\', type=float, default=0.0002, help=\'initial learning rate for adam\')\n        parser.add_argument(\'--lr_policy\', type=str, default=\'linear\',\n                            help=\'learning rate policy. [linear | step | plateau | cosine]\')\n        parser.add_argument(\'--lr_decay_iters\', type=int, default=50,\n                            help=\'multiply by a gamma every lr_decay_iters iterations\')\n\n        parser.add_argument(\'--eval_batch_size\', type=int, default=1, help=\'the evaluation batch size\')\n        parser.add_argument(\'--real_stat_path\', type=str, required=True,\n                            help=\'the path to load the ground-truth images information to compute FID.\')\n        return parser\n\n    def gather_options(self):\n        parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n        parser = self.initialize(parser)\n\n        opt, _ = parser.parse_known_args()\n\n        supernet_name = opt.supernet\n        supernet_option_setter = supernets.get_option_setter(supernet_name)\n        parser = supernet_option_setter(parser, self.isTrain)\n        opt, _ = parser.parse_known_args()\n\n        # modify dataset-related parser options\n        dataset_name = opt.dataset_mode\n        dataset_option_setter = data.get_option_setter(dataset_name)\n        parser = dataset_option_setter(parser, self.isTrain)\n\n        # save and return the parser\n        self.parser = parser\n        return parser.parse_args()\n'"
options/test_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TestOptions(BaseOptions):\n    """"""This class includes test options.\n\n    It also includes shared options defined in BaseOptions.\n    """"""\n\n    def __init__(self, isTrain=False):\n        super(TestOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)  # define shared options\n        parser.add_argument(\'--results_dir\', type=str, default=None, required=True, help=\'saves results here.\')\n        parser.add_argument(\'--need_profile\', action=\'store_true\')\n        parser.add_argument(\'--num_test\', type=int, default=float(\'inf\'), help=\'how many test images to run\')\n        parser.add_argument(\'--model\', type=str, default=\'test\', help=\'which model do you want test\')\n        parser.add_argument(\'--restore_G_path\', type=str, required=True, help=\'the path to restore the generator\')\n        parser.add_argument(\'--netG\', type=str, default=\'sub_mobile_resnet_9blocks\',\n                            choices=[\'resnet_9blocks\',\n                                     \'mobile_resnet_9blocks\',\n                                     \'super_mobile_resnet_9blocks\',\n                                     \'sub_mobile_resnet_9blocks\',\n                                     \'spade\', \'sub_mobile_spade\'],\n                            help=\'specify generator architecture\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'the base number of filters of the student generator\')\n        parser.add_argument(\'--dropout_rate\', type=float, default=0, help=\'the dropout rate of the generator\')\n        # rewrite devalue values\n        parser.add_argument(\'--no_fid\', action=\'store_true\')\n        parser.add_argument(\'--real_stat_path\', type=str, required=None,\n                            help=\'the path to load the groud-truth images information to compute FID.\')\n        parser.add_argument(\'--no_mIoU\', action=\'store_true\')\n        parser.add_argument(\'--times\', type=int, default=100,\n                            help=\'times of forwarding the data to test the latency\')\n        parser.set_defaults(phase=\'val\', serial_batches=True, no_flip=True,\n                            load_size=parser.get_default(\'crop_size\'), batch_size=1)\n        return parser\n'"
options/train_options.py,0,"b'from .base_options import BaseOptions\n\n\nclass TrainOptions(BaseOptions):\n    """"""This class includes training options.\n\n    It also includes shared options defined in BaseOptions.\n    """"""\n\n    def __init__(self, isTrain=True):\n        super(TrainOptions, self).__init__()\n        self.isTrain = isTrain\n\n    def initialize(self, parser):\n        parser = BaseOptions.initialize(self, parser)\n        # log parameters\n        parser.add_argument(\'--log_dir\', type=str, default=\'logs\',\n                            help=\'training logs are saved here\')\n        parser.add_argument(\'--tensorboard_dir\', type=str, default=None,\n                            help=\'tensorboard is saved here\')\n        parser.add_argument(\'--print_freq\', type=int, default=100,\n                            help=\'frequency of showing training results on console\')\n        parser.add_argument(\'--save_latest_freq\', type=int, default=20000,\n                            help=\'frequency of evaluating and save the latest model\')\n        parser.add_argument(\'--save_epoch_freq\', type=int, default=5,\n                            help=\'frequency of saving checkpoints at the end of epoch\')\n        parser.add_argument(\'--epoch_base\', type=int, default=1,\n                            help=\'the epoch base of the training (used for resuming)\')\n        parser.add_argument(\'--iter_base\', type=int, default=1,\n                            help=\'the iteration base of the training (used for resuming)\')\n\n        # model parameters\n        parser.add_argument(\'--model\', type=str, default=\'pix2pix\', help=\'choose which model to use\')\n        parser.add_argument(\'--netD\', type=str, default=\'n_layers\',\n                            help=\'specify discriminator architecture [n_layers | pixel]. \'\n                                 \'The basic model is a 70x70 PatchGAN. \'\n                                 \'n_layers allows you to specify the layers in the discriminator\')\n        parser.add_argument(\'--netG\', type=str, default=\'mobile_resnet_9blocks\',\n                            help=\'specify generator architecture \'\n                                 \'[resnet_9blocks | mobile_resnet_9blocks | super_mobile_resnet_9blocks]\')\n        parser.add_argument(\'--ngf\', type=int, default=64, help=\'the base number of generator filters\')\n        parser.add_argument(\'--ndf\', type=int, default=128, help=\'the base number of discriminator filters\')\n        parser.add_argument(\'--n_layers_D\', type=int, default=3, help=\'only used if netD==n_layers\')\n        parser.add_argument(\'--dropout_rate\', type=float, default=0, help=\'the dropout rate of the generator\')\n\n        # training parameters\n        parser.add_argument(\'--nepochs\', type=int, default=5,\n                            help=\'number of epochs with the initial learning rate\')\n        parser.add_argument(\'--nepochs_decay\', type=int, default=15,\n                            help=\'number of epochs to linearly decay learning rate to zero\')\n        parser.add_argument(\'--beta1\', type=float, default=0.5, help=\'momentum term of adam\')\n        parser.add_argument(\'--lr\', type=float, default=0.0002, help=\'initial learning rate for adam\')\n        parser.add_argument(\'--gan_mode\', type=str, default=\'hinge\',\n                            help=\'the type of GAN objective. [vanilla| lsgan | wgangp | hinge]. \'\n                                 \'vanilla GAN loss is the cross-entropy objective used in the original GAN paper.\')\n        parser.add_argument(\'--pool_size\', type=int, default=50,\n                            help=\'the size of image buffer that stores previously generated images\')\n        parser.add_argument(\'--lr_policy\', type=str, default=\'linear\',\n                            help=\'learning rate policy. [linear | step | plateau | cosine]\')\n        parser.add_argument(\'--lr_decay_iters\', type=int, default=50,\n                            help=\'multiply by a gamma every lr_decay_iters iterations\')\n\n        parser.add_argument(\'--eval_batch_size\', type=int, default=1, help=\'the evaluation batch size\')\n        self.isTrain = True\n        return parser\n'"
scripts/download_model.py,0,"b""import argparse\nimport os\n\nimport wget\n\n\ndef check(opt):\n    if opt.model == 'pix2pix':\n        assert opt.task in ['edges2shoes-r', 'map2sat', 'cityscapes']\n    elif opt.model == 'cycle_gan':\n        assert opt.task in ['horse2zebra']\n    elif opt.model == 'gaugan':\n        assert opt.task in ['cityscapes']\n        assert opt.stage in ['compressed', 'full']\n    else:\n        raise NotImplementedError('Unsupported model [%s]!' % opt.model)\n\n\ndef download(path):\n    url = 'https://hanlab.mit.edu/files/gan_compression/' + path\n    dir = os.path.dirname(path)\n    os.makedirs(dir, exist_ok=True)\n    wget.download(url, path)\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Download a pretrained model.')\n    parser.add_argument('--stage', type=str, default='compressed',\n                        choices=['full', 'mobile', 'distill', 'supernet', 'finetune', 'compressed'],\n                        help='specify the stage you want to download')\n    parser.add_argument('--model', type=str, default='pix2pix',\n                        choices=['pix2pix', 'cycle_gan', 'gaugan'],\n                        help='specify the model you want to download')\n    parser.add_argument('--task', type=str, default='horse2zebra',\n                        choices=['horse2zebra', 'edges2shoes-r', 'map2sat', 'cityscapes'],\n                        help='the base number of filters of the generator')\n    opt = parser.parse_args()\n    check(opt)\n    path = os.path.join('pretrained', opt.model, opt.task, opt.stage, 'latest_net_G.pth')\n    download(path)\n    if opt.stage != 'compressed':\n        path = os.path.join('pretrained', opt.model, opt.task, opt.stage, 'latest_net_D.pth')\n        download(path)\n"""
scripts/test_before_push.py,0,"b""import os\n\n\ndef run(command):\n    print(command)\n    exit_status = os.system(command)\n    if exit_status > 0:\n        exit(1)\n\n\nif __name__ == '__main__':\n    # Download mini datasets\n    if not os.path.exists('database/mini'):\n        run('bash ./datasets/download_cyclegan_dataset.sh mini')\n    if not os.path.exists('./database/mini_pix2pix'):\n        run('bash ./datasets/download_cyclegan_dataset.sh mini_pix2pix')\n\n    # Test get_real_stat.py\n    if not os.path.exists('tmp/real_stat/mini.npz'):\n        run('python get_real_stat.py --dataset_mode single '\n            '--data_path database/mimi/valB '\n            '--output_path tmp/real_stat/mini.npz')\n    elif not os.path.exists('tmp/real_stat/mini_pix2pix.npz'):\n        run('python get_real_stat.py '\n            '--data_path database/mimi_pix2pix '\n            '--output_path tmp/real_stat/mini_pix2pix.npz')\n\n    # Test pretrained cycleGAN models\n    if not os.path.exists('pretrained/cycle_gan/horse2zebra/full/latest_net_G.pth'):\n        run('python scripts/download_model.py --stage full --model cycle_gan --task horse2zebra')\n    run('python test.py --dataroot database/mini/valA '\n        '--dataset_mode single '\n        '--results_dir tmp/results '\n        '--ngf 64 --netG resnet_9blocks '\n        '--restore_G_path pretrained/cycle_gan/horse2zebra/full/latest_net_G.pth '\n        '--need_profile --real_stat_path tmp/real_stat/mini.npz')\n\n    if not os.path.exists('pretrained/cycle_gan/horse2zebra/distill/latest_net_G.pth'):\n        run('python scripts/download_model.py --stage distill --model cycle_gan --task horse2zebra')\n    run('python test.py --dataroot database/mini/valA '\n        '--dataset_mode single '\n        '--results_dir tmp/results '\n        '--ngf 32 --netG mobile_resnet_9blocks '\n        '--restore_G_path pretrained/cycle_gan/horse2zebra/distill/latest_net_G.pth '\n        '--need_profile --real_stat_path tmp/real_stat/mini.npz')\n\n    if not os.path.exists('pretrained/cycle_gan/horse2zebra/supernet/latest_net_G.pth'):\n        run('python scripts/download_model.py --stage supernet --model cycle_gan --task horse2zebra')\n    run('python test.py --dataroot database/mini/valA '\n        '--dataset_mode single '\n        '--results_dir tmp/results '\n        '--ngf 32 --netG super_mobile_resnet_9blocks '\n        '--config_str 16_16_32_16_32_32_16_16 '\n        '--restore_G_path pretrained/cycle_gan/horse2zebra/supernet/latest_net_G.pth '\n        '--need_profile --real_stat_path tmp/real_stat/mini.npz')\n\n    if not os.path.exists('pretrained/cycle_gan/horse2zebra/compressed/latest_net_G.pth'):\n        run('python scripts/download_model.py --stage compressed --model cycle_gan --task horse2zebra')\n    run('python test.py --dataroot database/mini/valA '\n        '--dataset_mode single '\n        '--results_dir tmp/results '\n        '--netG sub_mobile_resnet_9blocks '\n        '--config_str 16_16_32_16_32_32_16_16 '\n        '--restore_G_path pretrained/cycle_gan/horse2zebra/compressed/latest_net_G.pth '\n        '--need_profile --real_stat_path tmp/real_stat/mini.npz')\n\n    # Test pretrained pix2pix models\n    if not os.path.exists('pretrained/pix2pix/edges2shoes-r/compressed/latest_net_G.pth'):\n        run('python scripts/download_model.py --stage compressed --model pix2pix --task edges2shoes-r')\n    run('python test.py --dataroot database/mini_pix2pix '\n        '--dataset_mode aligned '\n        '--results_dir tmp/results '\n        '--netG sub_mobile_resnet_9blocks '\n        '--config_str 32_32_48_32_48_48_16_16 '\n        '--restore_G_path pretrained/pix2pix/edges2shoes-r/compressed/latest_net_G.pth '\n        '--need_profile --real_stat_path tmp/real_stat/mini_pix2pix.npz')\n\n    # Test cycleGAN train\n    run('python train.py '\n        '--dataroot database/mini '\n        '--model cycle_gan --ngf 16 --ndf 16 --netG resnet_9blocks '\n        '--log_dir tmp/logs/cycle_gan/train '\n        '--real_stat_A_path tmp/real_stat/mini.npz '\n        '--real_stat_B_path tmp/real_stat/mini.npz '\n        '--nepochs 1 --nepochs_decay 0 '\n        '--print_freq 1')\n\n    # Test pix2pix train\n    run('python train.py '\n        '--dataroot database/mini_pix2pix '\n        '--model pix2pix --ngf 16 --ndf 16 '\n        '--log_dir tmp/logs/pix2pix/train '\n        '--real_stat_path tmp/real_stat/mini_pix2pix.npz '\n        '--nepochs 1 --nepochs_decay 0 '\n        '--print_freq 1')\n\n    # Test distillation\n    run('python distill.py --dataroot database/mini_pix2pix '\n        '--distiller resnet '\n        '--log_dir tmp/logs/pix2pix/distill '\n        '--student_ngf 8 --teacher_ngf 16 --ndf 16 '\n        '--restore_teacher_G_path tmp/logs/pix2pix/train/checkpoints/latest_net_G.pth '\n        '--restore_pretrained_G_path tmp/logs/pix2pix/train/checkpoints/latest_net_G.pth '\n        '--restore_D_path tmp/logs/pix2pix/train/checkpoints/latest_net_D.pth '\n        '--real_stat_path tmp/real_stat/mini_pix2pix.npz '\n        '--nepochs 1 --nepochs_decay 0 --save_epoch_freq 20 '\n        '--print_freq 1')\n\n    # Test supernet training\n    run('python train_supernet.py --dataroot database/mini_pix2pix '\n        '--distiller resnet --config_set test '\n        '--log_dir tmp/logs/pix2pix/supernet '\n        '--student_ngf 8 --teacher_ngf 16 --ndf 16 '\n        '--restore_teacher_G_path tmp/logs/pix2pix/train/checkpoints/latest_net_G.pth '\n        '--restore_student_G_path tmp/logs/pix2pix/distill/checkpoints/latest_net_G.pth '\n        '--restore_D_path tmp/logs/pix2pix/distill/checkpoints/latest_net_D.pth '\n        '--real_stat_path tmp/real_stat/mini_pix2pix.npz '\n        '--nepochs 1 --nepochs_decay 0 --save_epoch_freq 20 '\n        '--print_freq 1')\n\n    # Test fine-tuning\n    run('python distill.py --dataroot database/mini_pix2pix '\n        '--distiller resnet --config_str 8_6_6_8_8_8_8_8 '\n        '--log_dir tmp/logs/pix2pix/finetune '\n        '--student_ngf 8 --teacher_ngf 16 --ndf 16 '\n        '--restore_teacher_G_path tmp/logs/pix2pix/train/checkpoints/latest_net_G.pth '\n        '--restore_student_G_path tmp/logs/pix2pix/supernet/checkpoints/latest_net_G.pth '\n        '--restore_D_path tmp/logs/pix2pix/supernet/checkpoints/latest_net_D.pth '\n        '--real_stat_path tmp/real_stat/mini_pix2pix.npz '\n        '--nepochs 1 --nepochs_decay 0 --save_epoch_freq 20 '\n        '--print_freq 1')\n\n    # Test export\n    run('python export.py '\n        '--input_path tmp/logs/pix2pix/finetune/checkpoints/latest_net_G.pth '\n        '--output_path tmp/logs/pix2pix/compressed/checkpoints/compressed_net_G.pth '\n        '--config_str 8_6_6_8_8_8_8_8')\n"""
supernets/__init__.py,0,"b'import importlib\n\n\ndef find_supernet_using_name(supernet_name):\n    supernet_filename = ""supernets."" + supernet_name + \'_supernet\'\n    modellib = importlib.import_module(supernet_filename)\n    supernet = None\n    target_supernet_name = supernet_name.replace(\'_\', \'\') + \'supernet\'\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == target_supernet_name.lower():\n            supernet = cls\n\n    if supernet is None:\n        print(""In %s.py, there should be a class of supernet with class name that matches %s in lowercase."" %\n              (supernet_filename, target_supernet_name))\n        exit(0)\n\n    return supernet\n\n\ndef get_option_setter(supernet_name):\n    supernet_class = find_supernet_using_name(supernet_name)\n    return supernet_class.modify_commandline_options\n\n\ndef create_supernet(opt, verbose=True):\n    supernet = find_supernet_using_name(opt.supernet)\n    instance = supernet(opt)\n    if verbose:\n        print(""supernet [%s] was created"" % type(instance).__name__)\n    return instance\n'"
supernets/resnet_supernet.py,4,"b""import ntpath\nimport os\n\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn\nfrom tqdm import tqdm\n\nfrom configs import decode_config\nfrom configs.resnet_configs import get_configs\nfrom configs.single_configs import SingleConfigs\nfrom distillers.base_resnet_distiller import BaseResnetDistiller\nfrom metric import get_fid, get_mIoU\nfrom models.modules.super_modules import SuperConv2d\nfrom utils import util\n\n\nclass ResnetSupernet(BaseResnetDistiller):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert is_train\n        parser = super(ResnetSupernet, ResnetSupernet).modify_commandline_options(parser, is_train)\n        parser.set_defaults(norm='instance', student_netG='super_mobile_resnet_9blocks',\n                            dataset_mode='aligned', log_dir='logs/supernet')\n        return parser\n\n    def __init__(self, opt):\n        assert opt.isTrain\n        assert 'super' in opt.student_netG\n        super(ResnetSupernet, self).__init__(opt)\n        self.best_fid_largest = 1e9\n        self.best_fid_smallest = 1e9\n        self.best_mIoU_largest = -1e9\n        self.best_mIoU_smallest = -1e9\n        self.fids_largest, self.fids_smallest = [], []\n        self.mIoUs_largest, self.mIoUs_smallest = [], []\n        if opt.config_set is not None:\n            assert opt.config_str is None\n            self.configs = get_configs(opt.config_set)\n            self.opt.eval_mode = 'both'\n        else:\n            assert opt.config_str is not None\n            self.configs = SingleConfigs(decode_config(opt.config_str))\n            self.opt.eval_mode = 'largest'\n\n    def forward(self, config):\n        with torch.no_grad():\n            self.Tfake_B = self.netG_teacher(self.real_A)\n        if isinstance(self.netG_student, nn.DataParallel):\n            self.netG_student.module.configs = config\n        else:\n            self.netG_student.configs = config\n        self.Sfake_B = self.netG_student(self.real_A)\n\n    def calc_distill_loss(self):\n        losses = []\n        for i, netA in enumerate(self.netAs):\n            assert isinstance(netA, SuperConv2d)\n            n = self.mapping_layers[i]\n            Tact = self.Tacts[n]\n            Sact = self.Sacts[n]\n            Sact = netA(Sact, {'channel': netA.out_channels})\n            loss = F.mse_loss(Sact, Tact)\n            setattr(self, 'loss_G_distill%d' % i, loss)\n            losses.append(loss)\n        return sum(losses)\n\n    def backward_G(self):\n        if self.opt.dataset_mode == 'aligned':\n            self.loss_G_recon = self.criterionRecon(self.Sfake_B, self.real_B) * self.opt.lambda_recon\n            fake = torch.cat((self.real_A, self.Sfake_B), 1)\n        else:\n            self.loss_G_recon = self.criterionRecon(self.Sfake_B, self.Tfake_B) * self.opt.lambda_recon\n            fake = self.Sfake_B\n        pred_fake = self.netD(fake)\n        self.loss_G_gan = self.criterionGAN(pred_fake, True, for_discriminator=False) * self.opt.lambda_gan\n        if self.opt.lambda_distill > 0:\n            self.loss_G_distill = self.calc_distill_loss() * self.opt.lambda_distill\n        else:\n            self.loss_G_distill = 0\n        self.loss_G = self.loss_G_gan + self.loss_G_recon + self.loss_G_distill\n        self.loss_G.backward()\n\n    def optimize_parameters(self):\n        self.optimizer_D.zero_grad()\n        self.optimizer_G.zero_grad()\n        config = self.configs.sample()\n        self.forward(config=config)\n        util.set_requires_grad(self.netD, True)\n        self.backward_D()\n        util.set_requires_grad(self.netD, False)\n        self.backward_G()\n        self.optimizer_D.step()\n        self.optimizer_G.step()\n\n    def evaluate_model(self, step):\n        ret = {}\n        self.is_best = False\n        save_dir = os.path.join(self.opt.log_dir, 'eval', str(step))\n        os.makedirs(save_dir, exist_ok=True)\n        self.netG_student.eval()\n        if self.opt.eval_mode == 'both':\n            setting = ('largest', 'smallest')\n        else:\n            setting = (self.opt.eval_mode,)\n        for config_name in setting:\n            config = self.configs(config_name)\n            fakes, names = [], []\n            cnt = 0\n            for i, data_i in enumerate(tqdm(self.eval_dataloader)):\n                if self.opt.dataset_mode == 'aligned':\n                    self.set_input(data_i)\n                else:\n                    self.set_single_input(data_i)\n                self.test(config)\n                fakes.append(self.Sfake_B.cpu())\n                for j in range(len(self.image_paths)):\n                    short_path = ntpath.basename(self.image_paths[j])\n                    name = os.path.splitext(short_path)[0]\n                    names.append(name)\n                    if i < 10:\n                        Sfake_im = util.tensor2im(self.Sfake_B[j])\n                        real_im = util.tensor2im(self.real_B[j])\n                        Tfake_im = util.tensor2im(self.Tfake_B[j])\n                        util.save_image(real_im, os.path.join(save_dir, 'real', '%s.png' % name), create_dir=True)\n                        util.save_image(Sfake_im, os.path.join(save_dir, 'Sfake_%s' % config_name, '%s.png' % name),\n                                        create_dir=True)\n                        util.save_image(Tfake_im, os.path.join(save_dir, 'Tfake', '%s.png' % name), create_dir=True)\n                        if self.opt.dataset_mode == 'aligned':\n                            input_im = util.tensor2im(self.real_A[j])\n                            util.save_image(input_im, os.path.join(save_dir, 'input', '%s.png') % name, create_dir=True)\n                    cnt += 1\n\n            fid = get_fid(fakes, self.inception_model, self.npz,\n                          device=self.device, batch_size=self.opt.eval_batch_size)\n            if fid < getattr(self, 'best_fid_%s' % config_name):\n                self.is_best = True\n                setattr(self, 'best_fid_%s' % config_name, fid)\n            fids = getattr(self, 'fids_%s' % config_name)\n            fids.append(fid)\n            if len(fids) > 3:\n                fids.pop(0)\n\n            ret['metric/fid_%s' % config_name] = fid\n            ret['metric/fid_%s-mean' % config_name] = sum(getattr(self, 'fids_%s' % config_name)) / len(\n                getattr(self, 'fids_%s' % config_name))\n            ret['metric/fid_%s-best' % config_name] = getattr(self, 'best_fid_%s' % config_name)\n\n            if 'cityscapes' in self.opt.dataroot:\n                mIoU = get_mIoU(fakes, names, self.drn_model, self.device,\n                               table_path=self.opt.table_path,\n                               data_dir=self.opt.cityscapes_path,\n                               batch_size=self.opt.eval_batch_size,\n                               num_workers=self.opt.num_threads)\n                if mIoU > getattr(self, 'best_mIoU_%s' % config_name):\n                    self.is_best = True\n                    setattr(self, 'best_mIoU_%s' % config_name, mIoU)\n                mIoUs = getattr(self, 'mIoUs_%s' % config_name)\n                mIoUs.append(mIoU)\n                if len(mIoUs) > 3:\n                    mIoUs.pop(0)\n                ret['metric/mIoU_%s' % config_name] = mIoU\n                ret['metric/mIoU_%s-mean' % config_name] = sum(getattr(self, 'mIoUs_%s' % config_name)) / len(\n                    getattr(self, 'mIoUs_%s' % config_name))\n                ret['metric/mIoU_%s-best' % config_name] = getattr(self, 'best_mIoU_%s' % config_name)\n\n        self.netG_student.train()\n        return ret\n\n    def test(self, config):\n        with torch.no_grad():\n            self.forward(config)\n"""
utils/__init__.py,0,"b'""""""This package includes a miscellaneous collection of useful helper functions.""""""\n'"
utils/html.py,0,"b'import dominate\nfrom dominate.tags import meta, h3, table, tr, td, p, a, img, br\nimport os\n\n\nclass HTML:\n    """"""This HTML class allows us to save images and write texts into a single HTML file.\n\n     It consists of functions such as <add_header> (add a text header to the HTML file),\n     <add_images> (add a row of images to the HTML file), and <save> (save the HTML to the disk).\n     It is based on Python library \'dominate\', a Python library for creating and manipulating HTML documents using a DOM API.\n    """"""\n\n    def __init__(self, web_dir, title, refresh=0):\n        """"""Initialize the HTML classes\n\n        Parameters:\n            web_dir (str) -- a directory that stores the webpage. HTML file will be created at <web_dir>/index.html; images will be saved at <web_dir/images/\n            title (str)   -- the webpage name\n            refresh (int) -- how often the website refresh itself; if 0; no refreshing\n        """"""\n        self.title = title\n        self.web_dir = web_dir\n        self.img_dir = os.path.join(self.web_dir, \'images\')\n        if not os.path.exists(self.web_dir):\n            os.makedirs(self.web_dir)\n        if not os.path.exists(self.img_dir):\n            os.makedirs(self.img_dir)\n\n        self.doc = dominate.document(title=title)\n        if refresh > 0:\n            with self.doc.head:\n                meta(http_equiv=""refresh"", content=str(refresh))\n\n    def get_image_dir(self):\n        """"""Return the directory that stores images""""""\n        return self.img_dir\n\n    def add_header(self, text):\n        """"""Insert a header to the HTML file\n\n        Parameters:\n            text (str) -- the header text\n        """"""\n        with self.doc:\n            h3(text)\n\n    def add_images(self, ims, txts, links, width=400):\n        """"""add images to the HTML file\n\n        Parameters:\n            ims (str list)   -- a list of image paths\n            txts (str list)  -- a list of image names shown on the website\n            links (str list) --  a list of hyperref links; when you click an image, it will redirect you to a new page\n        """"""\n        self.t = table(border=1, style=""table-layout: fixed;"")  # Insert a table\n        self.doc.add(self.t)\n        with self.t:\n            with tr():\n                for im, txt, link in zip(ims, txts, links):\n                    with td(style=""word-wrap: break-word;"", halign=""center"", valign=""top""):\n                        with p():\n                            with a(href=os.path.join(\'images\', link)):\n                                img(style=""width:%dpx"" % width, src=os.path.join(\'images\', im))\n                            br()\n                            p(txt)\n\n    def save(self):\n        """"""save the current content to the HMTL file""""""\n        html_file = \'%s/index.html\' % self.web_dir\n        f = open(html_file, \'wt\')\n        f.write(self.doc.render())\n        f.close()\n\n\nif __name__ == \'__main__\':  # we show an example usage here.\n    html = HTML(\'web/\', \'test_html\')\n    html.add_header(\'hello world\')\n\n    ims, txts, links = [], [], []\n    for n in range(4):\n        ims.append(\'image_%d.png\' % n)\n        txts.append(\'text_%d\' % n)\n        links.append(\'image_%d.png\' % n)\n    html.add_images(ims, txts, links)\n    html.save()\n'"
utils/image_pool.py,2,"b'import random\nimport torch\n\n\nclass ImagePool():\n    """"""This class implements an image buffer that stores previously generated images.\n\n    This buffer enables us to update discriminators using a history of generated images\n    rather than the ones produced by the latest generators.\n    """"""\n\n    def __init__(self, pool_size):\n        """"""Initialize the ImagePool class\n\n        Parameters:\n            pool_size (int) -- the size of image buffer, if pool_size=0, no buffer will be created\n        """"""\n        self.pool_size = pool_size\n        if self.pool_size > 0:  # create an empty pool\n            self.num_imgs = 0\n            self.images = []\n\n    def query(self, images):\n        """"""Return an image from the pool.\n\n        Parameters:\n            images: the latest generated images from the generator\n\n        Returns images from the buffer.\n\n        By 50/100, the buffer will return input images.\n        By 50/100, the buffer will return images previously stored in the buffer,\n        and insert the current images to the buffer.\n        """"""\n        if self.pool_size == 0:  # if the buffer size is 0, do nothing\n            return images\n        return_images = []\n        for image in images:\n            image = torch.unsqueeze(image.data, 0)\n            if self.num_imgs < self.pool_size:   # if the buffer is not full; keep inserting current images to the buffer\n                self.num_imgs = self.num_imgs + 1\n                self.images.append(image)\n                return_images.append(image)\n            else:\n                p = random.uniform(0, 1)\n                if p > 0.5:  # by 50% chance, the buffer will return a previously stored image, and insert the current image into the buffer\n                    random_id = random.randint(0, self.pool_size - 1)  # randint is inclusive\n                    tmp = self.images[random_id].clone()\n                    self.images[random_id] = image\n                    return_images.append(tmp)\n                else:       # by another 50% chance, the buffer will return the current image\n                    return_images.append(image)\n        return_images = torch.cat(return_images, 0)   # collect all the images and return\n        return return_images\n'"
utils/logger.py,0,"b""import os\nimport time\n\nfrom tensorboardX import SummaryWriter\n\n\nclass Logger:\n    def __init__(self, opt):\n        self.opt = opt\n        self.log_file = open(os.path.join(opt.log_dir, 'log.txt'), 'a')\n        os.makedirs(opt.tensorboard_dir, exist_ok=True)\n        self.writer = SummaryWriter(opt.tensorboard_dir)\n        now = time.strftime('%c')\n        self.log_file.write('================ (%s) ================\\n' % now)\n        self.log_file.flush()\n\n    def plot(self, items, step):\n        if len(items) == 0:\n            return\n        for k, v in items.items():\n            self.writer.add_scalar(k, v, global_step=step)\n        self.writer.flush()\n\n    def print_current_errors(self, epoch, i, errors, t):\n        message = '(epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n\n        for k, v in errors.items():\n            if 'Specific' in k:\n                continue\n            kk = k.split('/')[-1]\n            message += '%s: %.3f ' % (kk, v)\n\n        print(message, flush=True)\n        self.log_file.write('%s\\n' % message)\n        self.log_file.flush()\n\n    def print_current_metrics(self, epoch, i, metrics, t):\n        message = '###(Evaluate epoch: %d, iters: %d, time: %.3f) ' % (epoch, i, t)\n\n        for k, v in metrics.items():\n            kk = k.split('/')[-1]\n            message += '%s: %.3f ' % (kk, v)\n\n        print(message, flush=True)\n        self.log_file.write('%s\\n' % message)\n\n    def print_info(self, message):\n        print(message, flush=True)\n        self.log_file.write(message + '\\n')\n"""
utils/util.py,5,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport importlib\nimport os\nimport re\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch import nn\n\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\n\ndef natural_keys(text):\n    \'\'\'\n    alist.sort(key=natural_keys) sorts in human order\n    http://nedbatchelder.com/blog/200712/human_sorting.html\n    (See Toothy\'s implementation in the comments)\n    \'\'\'\n    return [atoi(c) for c in re.split(r\'(\\d+)\', text)]\n\n\ndef natural_sort(items):\n    items.sort(key=natural_keys)\n\n\ndef tile_images(imgs, picturesPerRow=4):\n    """""" Code borrowed from\n    https://stackoverflow.com/questions/26521365/cleanly-tile-numpy-array-of-images-stored-in-a-flattened-1d-format/26521997\n    """"""\n\n    # Padding\n    if imgs.shape[0] % picturesPerRow == 0:\n        rowPadding = 0\n    else:\n        rowPadding = picturesPerRow - imgs.shape[0] % picturesPerRow\n    if rowPadding > 0:\n        imgs = np.concatenate([imgs, np.zeros((rowPadding, *imgs.shape[1:]), dtype=imgs.dtype)], axis=0)\n\n    # Tiling Loop (The conditionals are not necessary anymore)\n    tiled = []\n    for i in range(0, imgs.shape[0], picturesPerRow):\n        tiled.append(np.concatenate([imgs[j] for j in range(i, i + picturesPerRow)], axis=1))\n\n    tiled = np.concatenate(tiled, axis=0)\n    return tiled\n\n\n# Converts a Tensor into a Numpy array\n# |imtype|: the desired type of the converted numpy array\ndef tensor2im(image_tensor, imtype=np.uint8, normalize=True, tile=False):\n    if isinstance(image_tensor, list):\n        image_numpy = []\n        for i in range(len(image_tensor)):\n            image_numpy.append(tensor2im(image_tensor[i], imtype, normalize))\n        return image_numpy\n\n    if image_tensor.dim() == 4:\n        # transform each image in the batch\n        images_np = []\n        for b in range(image_tensor.size(0)):\n            one_image = image_tensor[b]\n            one_image_np = tensor2im(one_image)\n            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n        images_np = np.concatenate(images_np, axis=0)\n        if tile:\n            images_tiled = tile_images(images_np)\n            return images_tiled\n        else:\n            return images_np\n\n    if image_tensor.dim() == 2:\n        image_tensor = image_tensor.unsqueeze(0)\n    image_numpy = image_tensor.detach().cpu().float().numpy()\n    if normalize:\n        image_numpy = (np.transpose(image_numpy, (1, 2, 0)) + 1) / 2.0 * 255.0\n    else:\n        image_numpy = np.transpose(image_numpy, (1, 2, 0)) * 255.0\n    image_numpy = np.clip(image_numpy, 0, 255)\n    if image_numpy.shape[2] == 1:\n        image_numpy = image_numpy[:, :, 0]\n    return image_numpy.astype(imtype)\n\n\n# Converts a one-hot tensor into a colorful label map\ndef tensor2label(label_tensor, n_label, imtype=np.uint8, tile=False):\n    if label_tensor.dim() == 4:\n        # transform each image in the batch\n        images_np = []\n        for b in range(label_tensor.size(0)):\n            one_image = label_tensor[b]\n            one_image_np = tensor2label(one_image, n_label, imtype)\n            images_np.append(one_image_np.reshape(1, *one_image_np.shape))\n        images_np = np.concatenate(images_np, axis=0)\n        if tile:\n            images_tiled = tile_images(images_np)\n            return images_tiled\n        else:\n            images_np = images_np[0]\n            return images_np\n\n    if label_tensor.dim() == 1:\n        return np.zeros((64, 64, 3), dtype=np.uint8)\n    if n_label == 0:\n        return tensor2im(label_tensor, imtype)\n    label_tensor = label_tensor.cpu().float()\n    if label_tensor.size()[0] > 1:\n        label_tensor = label_tensor.max(0, keepdim=True)[1]\n    label_tensor = Colorize(n_label)(label_tensor)\n    label_numpy = np.transpose(label_tensor.numpy(), (1, 2, 0))\n    result = label_numpy.astype(imtype)\n    return result\n\n\ndef save_image(image_numpy, image_path, create_dir=False):\n    if create_dir:\n        os.makedirs(os.path.dirname(image_path), exist_ok=True)\n    if len(image_numpy.shape) == 4:\n        image_numpy = image_numpy[0]\n    if len(image_numpy.shape) == 2:\n        image_numpy = np.expand_dims(image_numpy, axis=2)\n    if image_numpy.shape[2] == 1:\n        image_numpy = np.repeat(image_numpy, 3, 2)\n    image_pil = Image.fromarray(image_numpy)\n\n    # save to png\n    image_pil.save(image_path.replace(\'.jpg\', \'.png\'))\n\n\ndef save_network(net, label, epoch, opt):\n    save_filename = \'%s_net_%s.pth\' % (epoch, label)\n    os.makedirs(os.path.join(opt.log_dir, \'checkpoints\'), exist_ok=True)\n    save_path = os.path.join(opt.log_dir, \'checkpoints\', save_filename)\n    torch.save(net.cpu().state_dict(), save_path)\n    if len(opt.gpu_ids) and torch.cuda.is_available():\n        net.cuda()\n\n\ndef load_network(net, load_path, verbose=True):\n    if verbose:\n        print(\'Load network at %s\' % load_path)\n    weights = torch.load(load_path)\n    if isinstance(net, nn.DataParallel):\n        net = net.module\n    net.load_state_dict(weights)\n    return net\n\n\n###############################################################################\n# Code from\n# https://github.com/ycszen/pytorch-seg/blob/master/transform.py\n# Modified so it complies with the Cityscape label map colors\n###############################################################################\ndef uint82bin(n, count=8):\n    """"""returns the binary of integer n, count refers to amount of bits""""""\n    return \'\'.join([str((n >> y) & 1) for y in range(count - 1, -1, -1)])\n\n\ndef labelcolormap(N):\n    if N == 35:  # cityscape\n        cmap = np.array([(0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (0, 0, 0), (111, 74, 0), (81, 0, 81),\n                         (128, 64, 128), (244, 35, 232), (250, 170, 160), (230, 150, 140), (70, 70, 70),\n                         (102, 102, 156), (190, 153, 153),\n                         (180, 165, 180), (150, 100, 100), (150, 120, 90), (153, 153, 153), (153, 153, 153),\n                         (250, 170, 30), (220, 220, 0),\n                         (107, 142, 35), (152, 251, 152), (70, 130, 180), (220, 20, 60), (255, 0, 0), (0, 0, 142),\n                         (0, 0, 70),\n                         (0, 60, 100), (0, 0, 90), (0, 0, 110), (0, 80, 100), (0, 0, 230), (119, 11, 32), (0, 0, 142)],\n                        dtype=np.uint8)\n    else:\n        cmap = np.zeros((N, 3), dtype=np.uint8)\n        for i in range(N):\n            r, g, b = 0, 0, 0\n            id = i + 1  # let\'s give 0 a color\n            for j in range(7):\n                str_id = uint82bin(id)\n                r = r ^ (np.uint8(str_id[-1]) << (7 - j))\n                g = g ^ (np.uint8(str_id[-2]) << (7 - j))\n                b = b ^ (np.uint8(str_id[-3]) << (7 - j))\n                id = id >> 3\n            cmap[i, 0] = r\n            cmap[i, 1] = g\n            cmap[i, 2] = b\n\n    return cmap\n\n\nclass Colorize(object):\n    def __init__(self, n=35):\n        self.cmap = labelcolormap(n)\n        self.cmap = torch.from_numpy(self.cmap[:n])\n\n    def __call__(self, gray_image):\n        size = gray_image.size()\n        color_image = torch.ByteTensor(3, size[1], size[2]).fill_(0)\n\n        for label in range(0, len(self.cmap)):\n            mask = (label == gray_image[0]).cpu()\n            color_image[0][mask] = self.cmap[label][0]\n            color_image[1][mask] = self.cmap[label][1]\n            color_image[2][mask] = self.cmap[label][2]\n\n        return color_image\n\n\ndef set_requires_grad(nets, requires_grad=False):\n    """"""Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n    Parameters:\n        nets (network list)   -- a list of networks\n        requires_grad (bool)  -- whether the networks require gradients or not\n    """"""\n    if not isinstance(nets, list):\n        nets = [nets]\n    for net in nets:\n        if net is not None:\n            for param in net.parameters():\n                param.requires_grad = requires_grad\n\n\ndef find_class_in_module(target_cls_name, module):\n    target_cls_name = target_cls_name.replace(\'_\', \'\').lower()\n    clslib = importlib.import_module(module)\n    cls = None\n    for name, clsobj in clslib.__dict__.items():\n        if name.lower() == target_cls_name:\n            cls = clsobj\n\n    if cls is None:\n        print(""In %s, there should be a class whose name matches %s in lowercase without underscore(_)"" % (\n            module, target_cls_name))\n        exit(0)\n\n    return cls\n'"
utils/weight_transfer.py,0,"b""from torch import nn\n\nfrom models.modules.resnet_architecture.mobile_resnet_generator import MobileResnetBlock\nfrom models.modules.mobile_modules import SeparableConv2d\nfrom models.modules.resnet_architecture.resnet_generator import ResnetBlock\n\n\ndef transfer_Conv2d(m1, m2, input_index=None, output_index=None):\n    assert isinstance(m1, nn.Conv2d) and isinstance(m2, nn.Conv2d)\n    if m1.out_channels == 3:  # If this is the last convolution\n        assert input_index is not None\n        m2.weight.data = m1.weight.data[:, input_index].clone()\n        if m2.bias is not None:\n            m2.bias.data = m1.bias.data.clone()\n        return None\n    else:\n        if m1.in_channels == 3:  # If this is the first convolution\n            assert input_index is None\n            input_index = [0, 1, 2]\n        p = m1.weight.data\n\n        if input_index is None:\n            q = p.abs().sum([0, 2, 3])\n            _, idxs = q.topk(m2.in_channels, largest=True)\n            p = p[:, idxs]\n        else:\n            p = p[:, input_index]\n\n        if output_index is None:\n            q = p.abs().sum([1, 2, 3])\n            _, idxs = q.topk(m2.out_channels, largest=True)\n        else:\n            idxs = output_index\n\n        m2.weight.data = p[idxs].clone()\n        if m2.bias is not None:\n            m2.bias.data = m1.bias.data[idxs].clone()\n\n        return idxs\n\n\ndef transfer_ConvTranspose2d(m1, m2, input_index=None, output_index=None):\n    assert isinstance(m1, nn.ConvTranspose2d) and isinstance(m2, nn.ConvTranspose2d)\n    assert output_index is None\n    p = m1.weight.data\n    if input_index is not None:\n        q = p.abs().sum([1, 2, 3])\n        _, idxs = q.topk(m2.in_channels, largest=True)\n        p = p[idxs]\n    else:\n        p = p[input_index]\n    q = p.abs().sum([0, 2, 3])\n    _, idxs = q.topk(m2.out_channels, largest=True)\n    m2.weight.data = p[:, idxs].clone()\n    if m2.bias is not None:\n        m2.bias.data = m1.bias.data[idxs].clone()\n    return idxs\n\n\ndef transfer_SeparableConv2d(m1, m2, input_index=None, output_index=None):\n    assert isinstance(m1, SeparableConv2d) and isinstance(m2, SeparableConv2d)\n    dw1, pw1 = m1.conv[0], m1.conv[2]\n    dw2, pw2 = m2.conv[0], m2.conv[2]\n\n    if input_index is None:\n        p = dw1.weight.data\n        q = p.abs().sum([1, 2, 3])\n        _, input_index = q.topk(dw2.out_channels, largest=True)\n    dw2.weight.data = dw1.weight.data[input_index].clone()\n    if dw2.bias is not None:\n        dw2.bias.data = dw1.bias.data[input_index].clone()\n\n    idxs = transfer_Conv2d(pw1, pw2, input_index, output_index)\n    return idxs\n\n\ndef transfer_MobileResnetBlock(m1, m2, input_index=None, output_index=None):\n    assert isinstance(m1, MobileResnetBlock) and isinstance(m2, MobileResnetBlock)\n    assert output_index is None\n    idxs = transfer_SeparableConv2d(m1.conv_block[1], m2.conv_block[1], input_index=input_index)\n    idxs = transfer_SeparableConv2d(m1.conv_block[6], m2.conv_block[6], input_index=idxs, output_index=input_index)\n    return idxs\n\n\ndef transfer_ResnetBlock(m1, m2, input_index=None, output_index=None):\n    assert isinstance(m1, ResnetBlock) and isinstance(m2, ResnetBlock)\n    assert output_index is None\n    idxs = transfer_Conv2d(m1.conv_block[1], m2.conv_block[1], input_index=input_index)\n    idxs = transfer_Conv2d(m1.conv_block[6], m2.conv_block[6], input_index=idxs, output_index=input_index)\n    return idxs\n\n\ndef transfer(m1, m2, input_index=None, output_index=None):\n    assert type(m1) == type(m2)\n    if isinstance(m1, nn.Conv2d):\n        return transfer_Conv2d(m1, m2, input_index, output_index)\n    elif isinstance(m1, nn.ConvTranspose2d):\n        return transfer_ConvTranspose2d(m1, m2, input_index, output_index)\n    elif isinstance(m1, ResnetBlock):\n        return transfer_Conv2d(m1, m2, input_index, output_index)\n    elif isinstance(m1, MobileResnetBlock):\n        return transfer_MobileResnetBlock(m1, m2, input_index, output_index)\n    else:\n        raise NotImplementedError('Unknown module [%s]!' % type(m1))\n\n\ndef load_pretrained_weight(model1, model2, netA, netB, ngf1, ngf2):\n    assert model1 == model2\n    assert ngf1 >= ngf2\n\n    if isinstance(netA, nn.DataParallel):\n        net1 = netA.module\n    else:\n        net1 = netA\n    if isinstance(netB, nn.DataParallel):\n        net2 = netB.module\n    else:\n        net2 = netB\n\n    index = None\n    if model1 == 'mobile_resnet_9blocks':\n        assert len(net1.model) == len(net2.model)\n        for i in range(28):\n            m1, m2 = net1.model[i], net2.model[i]\n            assert type(m1) == type(m2)\n            if isinstance(m1, (nn.Conv2d, nn.ConvTranspose2d, MobileResnetBlock)):\n                index = transfer(m1, m2, index)\n    elif model1 == 'resnet_9blocks':\n        assert len(net1.model) == len(net2.model)\n        for i in range(28):\n            m1, m2 = net1.model[i], net2.model[i]\n            assert type(m1) == type(m2)\n            if isinstance(m1, (nn.Conv2d, nn.ConvTranspose2d, ResnetBlock)):\n                index = transfer(m1, m2, index)\n    else:\n        raise NotImplementedError('Unknown model [%s]!' % model1)\n"""
models/modules/__init__.py,0,b''
models/modules/discriminators.py,1,"b'import argparse\nimport functools\n\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom models.networks import BaseNetwork\nfrom utils import util\n\n\nclass NLayerDiscriminator(BaseNetwork):\n    """"""Defines a PatchGAN discriminator""""""\n\n    def __init__(self, input_nc, ndf=64, n_layers=3, norm_layer=nn.BatchNorm2d):\n        """"""Construct a PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            n_layers (int)  -- the number of conv layers in the discriminator\n            norm_layer      -- normalization layer\n        """"""\n        super(NLayerDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        kw = 4\n        padw = 1\n        sequence = [nn.Conv2d(input_nc, ndf, kernel_size=kw, stride=2, padding=padw), nn.LeakyReLU(0.2, True)]\n        nf_mult = 1\n        nf_mult_prev = 1\n        for n in range(1, n_layers):  # gradually increase the number of filters\n            nf_mult_prev = nf_mult\n            nf_mult = min(2 ** n, 8)\n            sequence += [\n                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=2, padding=padw, bias=use_bias),\n                norm_layer(ndf * nf_mult),\n                nn.LeakyReLU(0.2, True)\n            ]\n\n        nf_mult_prev = nf_mult\n        nf_mult = min(2 ** n_layers, 8)\n        sequence += [\n            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult, kernel_size=kw, stride=1, padding=padw, bias=use_bias),\n            norm_layer(ndf * nf_mult),\n            nn.LeakyReLU(0.2, True)\n        ]\n\n        sequence += [\n            nn.Conv2d(ndf * nf_mult, 1, kernel_size=kw, stride=1, padding=padw)]  # output 1 channel prediction map\n        self.model = nn.Sequential(*sequence)\n\n    def forward(self, input):\n        """"""Standard forward.""""""\n        return self.model(input)\n\n\nclass PixelDiscriminator(BaseNetwork):\n    """"""Defines a 1x1 PatchGAN discriminator (pixelGAN)""""""\n\n    def __init__(self, input_nc, ndf=64, norm_layer=nn.BatchNorm2d):\n        """"""Construct a 1x1 PatchGAN discriminator\n\n        Parameters:\n            input_nc (int)  -- the number of channels in input images\n            ndf (int)       -- the number of filters in the last conv layer\n            norm_layer      -- normalization layer\n        """"""\n        super(PixelDiscriminator, self).__init__()\n        if type(norm_layer) == functools.partial:  # no need to use bias as BatchNorm2d has affine parameters\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        self.net = [\n            nn.Conv2d(input_nc, ndf, kernel_size=1, stride=1, padding=0),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf, ndf * 2, kernel_size=1, stride=1, padding=0, bias=use_bias),\n            norm_layer(ndf * 2),\n            nn.LeakyReLU(0.2, True),\n            nn.Conv2d(ndf * 2, 1, kernel_size=1, stride=1, padding=0, bias=use_bias)]\n\n        self.net = nn.Sequential(*self.net)\n\n    def forward(self, input):\n        """"""Standard forward.""""""\n        return self.net(input)\n\n\nclass MultiscaleDiscriminator(nn.Module):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        assert isinstance(parser, argparse.ArgumentParser)\n        parser.add_argument(\'--netD_subarch\', type=str, default=\'n_layer\',\n                            help=\'architecture of each discriminator\')\n        parser.add_argument(\'--num_D\', type=int, default=2,\n                            help=\'number of discriminators to be used in multiscale\')\n\n        opt, _ = parser.parse_known_args()\n\n        # define properties of each discriminator of the multiscale discriminator\n        subnetD = util.find_class_in_module(opt.netD_subarch + \'discriminator\',\n                                            \'models.modules.discriminators\')\n        subnetD.modify_commandline_options(parser, is_train)\n        parser.set_defaults(n_layers_D=4)\n\n        return parser\n\n    def __init__(self, opt):\n        super().__init__()\n        self.opt = opt\n\n        for i in range(opt.num_D):\n            subnetD = self.create_single_discriminator(opt)\n            self.add_module(\'discriminator_%d\' % i, subnetD)\n\n    def create_single_discriminator(self, opt):\n        subarch = opt.netD_subarch\n        if subarch == \'n_layer\':\n            netD = NLayerDiscriminator(opt)\n        else:\n            raise ValueError(\'unrecognized discriminator subarchitecture %s\' % subarch)\n        return netD\n\n    def downsample(self, input):\n        return F.avg_pool2d(input, kernel_size=3,\n                            stride=2, padding=[1, 1],\n                            count_include_pad=False)\n\n    # Returns list of lists of discriminator outputs.\n    # The final result is of size opt.num_D x opt.n_layers_D\n    def forward(self, input):\n        result = []\n        get_intermediate_features = not self.opt.no_ganFeat_loss\n        for name, D in self.named_children():\n            out = D(input)\n            if not get_intermediate_features:\n                out = [out]\n            result.append(out)\n            input = self.downsample(input)\n\n        return result\n'"
models/modules/loss.py,19,"b'import torch\nimport torchvision\nfrom torch import nn as nn\n\nfrom utils import util\n\n\nclass GANLoss(nn.Module):\n    """"""Define different GAN objectives.\n\n    The GANLoss class abstracts away the need to create the target label tensor\n    that has the same size as the input.\n    """"""\n\n    def __init__(self, gan_mode, target_real_label=1.0, target_fake_label=0.0):\n        """""" Initialize the GANLoss class.\n\n        Parameters:\n            gan_mode (str) - - the type of GAN objective. It currently supports vanilla, lsgan, and wgangp.\n            target_real_label (bool) - - label for a real image\n            target_fake_label (bool) - - label of a fake image\n\n        Note: Do not use sigmoid as the last layer of Discriminator.\n        LSGAN needs no sigmoid. vanilla GANs will handle it with BCEWithLogitsLoss.\n        """"""\n        super(GANLoss, self).__init__()\n        self.register_buffer(\'real_label\', torch.tensor(target_real_label))\n        self.register_buffer(\'fake_label\', torch.tensor(target_fake_label))\n        self.register_buffer(\'zero_tensor\', torch.tensor(0.))\n        self.zero_tensor.requires_grad_(False)\n        self.gan_mode = gan_mode\n        if gan_mode == \'lsgan\':\n            self.loss = nn.MSELoss()\n        elif gan_mode == \'vanilla\':\n            self.loss = nn.BCEWithLogitsLoss()\n        elif gan_mode == \'wgangp\':\n            self.loss = None\n        elif gan_mode == \'hinge\':\n            self.loss = None\n        else:\n            raise NotImplementedError(\'gan mode %s not implemented\' % gan_mode)\n\n    def get_target_tensor(self, prediction, target_is_real):\n        if target_is_real:\n            target_tensor = self.real_label\n        else:\n            target_tensor = self.fake_label\n        return target_tensor.expand_as(prediction)\n\n    def get_zero_tensor(self, prediction):\n        return self.zero_tensor.expand_as(prediction)\n\n    def __call__(self, prediction, target_is_real, for_discriminator=True):\n        """"""Calculate loss given Discriminator\'s output and grount truth labels.\n\n        Parameters:\n            prediction (tensor) - - tpyically the prediction output from a discriminator\n            target_is_real (bool) - - if the ground truth label is for real images or fake images\n\n        Returns:\n            the calculated loss.\n        """"""\n        if self.gan_mode in [\'lsgan\', \'vanilla\']:\n            target_tensor = self.get_target_tensor(prediction, target_is_real)\n            loss = self.loss(prediction, target_tensor)\n        elif self.gan_mode == \'wgangp\':\n            if target_is_real:\n                loss = -prediction.mean()\n            else:\n                loss = prediction.mean()\n        elif self.gan_mode == \'hinge\':\n            if for_discriminator:\n                if target_is_real:\n                    minval = torch.min(prediction - 1, self.get_zero_tensor(prediction))\n                    loss = -torch.mean(minval)\n                else:\n                    minval = torch.min(-prediction - 1, self.get_zero_tensor(prediction))\n                    loss = -torch.mean(minval)\n            else:\n                assert target_is_real\n                loss = -torch.mean(prediction)\n        else:\n            raise NotImplementedError(\'gan mode %s not implemented\' % self.gan_mode)\n        return loss\n\n\ndef cal_gradient_penalty(netD, real_data, fake_data, device, type=\'mixed\', constant=1.0, lambda_gp=10.0):\n    """"""Calculate the gradient penalty loss, used in WGAN-GP paper https://arxiv.org/abs/1704.00028\n\n    Arguments:\n        netD (network)              -- discriminator network\n        real_data (tensor array)    -- real images\n        fake_data (tensor array)    -- generated images from the generator\n        device (str)                -- GPU / CPU: from torch.device(\'cuda:{}\'.format(self.gpu_ids[0])) if self.gpu_ids else torch.device(\'cpu\')\n        type (str)                  -- if we mix real and fake data or not [real | fake | mixed].\n        constant (float)            -- the constant used in formula ( | |gradient||_2 - constant)^2\n        lambda_gp (float)           -- weight for this loss\n\n    Returns the gradient penalty loss\n    """"""\n    if lambda_gp > 0.0:\n        if type == \'real\':  # either use real images, fake images, or a linear interpolation of two.\n            interpolatesv = real_data\n        elif type == \'fake\':\n            interpolatesv = fake_data\n        elif type == \'mixed\':\n            alpha = torch.rand(real_data.shape[0], 1, device=device)\n            alpha = alpha.expand(real_data.shape[0], real_data.nelement() // real_data.shape[0]). \\\n                contiguous().view(*real_data.shape)\n            interpolatesv = alpha * real_data + ((1 - alpha) * fake_data)\n        else:\n            raise NotImplementedError(\'{} not implemented\'.format(type))\n        interpolatesv.requires_grad_(True)\n        disc_interpolates = netD(interpolatesv)\n        gradients = torch.autograd.grad(outputs=disc_interpolates, inputs=interpolatesv,\n                                        grad_outputs=torch.ones(disc_interpolates.size()).to(device),\n                                        create_graph=True, retain_graph=True, only_inputs=True)\n        gradients = gradients[0].view(real_data.size(0), -1)  # flat the data\n        gradient_penalty = (((gradients + 1e-16).norm(2, dim=1) - constant) ** 2).mean() * lambda_gp  # added eps\n        return gradient_penalty, gradients\n    else:\n        return 0.0, None\n\n\nclass VGG19(torch.nn.Module):\n    def __init__(self, requires_grad=False):\n        super().__init__()\n        vgg_pretrained_features = torchvision.models.vgg19(pretrained=True).features\n        self.slice1 = torch.nn.Sequential()\n        self.slice2 = torch.nn.Sequential()\n        self.slice3 = torch.nn.Sequential()\n        self.slice4 = torch.nn.Sequential()\n        self.slice5 = torch.nn.Sequential()\n        for x in range(2):\n            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(2, 7):\n            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(7, 12):\n            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(12, 21):\n            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n        for x in range(21, 30):\n            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n        if not requires_grad:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, X):\n        h_relu1 = self.slice1(X)\n        h_relu2 = self.slice2(h_relu1)\n        h_relu3 = self.slice3(h_relu2)\n        h_relu4 = self.slice4(h_relu3)\n        h_relu5 = self.slice5(h_relu4)\n        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n        return out\n\n\nclass VGGLoss(nn.Module):\n    def __init__(self, device):\n        super(VGGLoss, self).__init__()\n        self.vgg = VGG19().to(device)\n        self.vgg.eval()\n        util.set_requires_grad(self.vgg, False)\n        self.criterion = nn.L1Loss()\n        self.weights = [1.0 / 32, 1.0 / 16, 1.0 / 8, 1.0 / 4, 1.0]\n\n    def forward(self, x, y):\n        # x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n        # loss = 0\n        loss = 0\n        x_vgg = self.vgg(x)\n        with torch.no_grad():\n            y_vgg = self.vgg(y)\n\n        for i in range(len(x_vgg)):\n            loss += self.weights[i] * self.criterion(x_vgg[i], y_vgg[i].detach())\n        return loss\n'"
models/modules/mobile_modules.py,0,"b'from torch import nn\n\n\nclass SeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, norm_layer=nn.InstanceNorm2d,\n                 use_bias=True, scale_factor=1):\n        super(SeparableConv2d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=in_channels * scale_factor, kernel_size=kernel_size,\n                      stride=stride, padding=padding, groups=in_channels, bias=use_bias),\n            norm_layer(in_channels * scale_factor),\n            nn.Conv2d(in_channels=in_channels * scale_factor, out_channels=out_channels,\n                      kernel_size=1, stride=1, bias=use_bias),\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n'"
models/modules/super_modules.py,3,"b""import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .sync_batchnorm import SynchronizedBatchNorm2d\nfrom .sync_batchnorm.batchnorm import _ChildMessage, _sum_ft, _unsqueeze_ft\n\n\nclass SuperConv2d(nn.Conv2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros'):\n        super(SuperConv2d, self).__init__(in_channels, out_channels, kernel_size,\n                                          stride, padding, dilation, groups, bias, padding_mode)\n\n    def forward(self, x, config):\n        in_nc = x.size(1)\n        out_nc = config['channel']\n        weight = self.weight[:out_nc, :in_nc]  # [oc, ic, H, W]\n        if self.bias is not None:\n            bias = self.bias[:out_nc]\n        else:\n            bias = None\n        return F.conv2d(x, weight, bias, self.stride, self.padding, self.dilation, self.groups)\n\n\nclass SuperConvTranspose2d(nn.ConvTranspose2d):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, output_padding=0, groups=1, bias=True,\n                 dilation=1, padding_mode='zeros'):\n        super(SuperConvTranspose2d, self).__init__(in_channels, out_channels,\n                                                   kernel_size, stride, padding,\n                                                   output_padding, groups, bias,\n                                                   dilation, padding_mode)\n\n    def forward(self, x, config, output_size=None):\n        output_padding = self._output_padding(x, output_size, self.stride, self.padding, self.kernel_size)\n        in_nc = x.size(1)\n        out_nc = config['channel']\n        weight = self.weight[:in_nc, :out_nc]  # [ic, oc, H, W]\n        if self.bias is not None:\n            bias = self.bias[:out_nc]\n        else:\n            bias = None\n        return F.conv_transpose2d(x, weight, bias, self.stride, self.padding,\n                                  output_padding, self.groups, self.dilation)\n\n\nclass SuperSeparableConv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, norm_layer=nn.InstanceNorm2d,\n                 use_bias=True, scale_factor=1):\n        super(SuperSeparableConv2d, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels=in_channels, out_channels=in_channels * scale_factor, kernel_size=kernel_size,\n                      stride=stride, padding=padding, groups=in_channels, bias=use_bias),\n            norm_layer(in_channels * scale_factor),\n            nn.Conv2d(in_channels=in_channels * scale_factor, out_channels=out_channels,\n                      kernel_size=1, stride=1, bias=use_bias),\n        )\n\n    def forward(self, x, config):\n        in_nc = x.size(1)\n        out_nc = config['channel']\n\n        conv = self.conv[0]\n        assert isinstance(conv, nn.Conv2d)\n        weight = conv.weight[:in_nc]  # [oc, 1, H, W]\n        # print(weight.shape)\n        if conv.bias is not None:\n            bias = conv.bias[:in_nc]\n        else:\n            bias = None\n        x = F.conv2d(x, weight, bias, conv.stride, conv.padding, conv.dilation, in_nc)\n\n        x = self.conv[1](x)\n\n        conv = self.conv[2]\n        assert isinstance(conv, nn.Conv2d)\n        weight = conv.weight[:out_nc, :in_nc]  # [oc, ic, H, W]\n        # print(weight.shape)\n        if conv.bias is not None:\n            bias = conv.bias[:out_nc]\n        else:\n            bias = None\n        x = F.conv2d(x, weight, bias, conv.stride, conv.padding, conv.dilation, conv.groups)\n        return x\n\n\nclass SuperSynchronizedBatchNorm2d(SynchronizedBatchNorm2d):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        super(SuperSynchronizedBatchNorm2d, self).__init__(num_features, eps, momentum, affine)\n\n    def forward(self, x, config={'calibrate_bn': False}):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch's implementation.\n        # return input\n        input = x\n        if x.shape[1] != self.num_features:\n            padding = torch.zeros([x.shape[0], self.num_features - x.shape[1], x.shape[2], x.shape[3]], device=x.device)\n            input = torch.cat([input, padding], dim=1)\n        calibrate_bn = config['calibrate_bn']\n        if not (self._is_parallel and self.training):\n            if calibrate_bn:\n                ret = F.batch_norm(\n                    input, self.running_mean, self.running_var, self.weight, self.bias,\n                    self.training, 1, self.eps)\n            else:\n                ret = F.batch_norm(\n                    input, self.running_mean, self.running_var, self.weight, self.bias,\n                    self.training, self.momentum, self.eps)\n            return ret[:, :x.shape[1]]\n\n        momentum = self.momentum\n        if calibrate_bn:\n            self.momentum = 1\n        # print('another route')\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        if calibrate_bn:\n            self.momentum = momentum\n        output = output.view(input_shape)\n        return output[:, :x.shape[1]]\n"""
models/modules/resnet_architecture/__init__.py,0,b''
models/modules/resnet_architecture/mobile_resnet_generator.py,0,"b'import functools\n\nfrom torch import nn\n\nfrom models.modules.mobile_modules import SeparableConv2d\nfrom models.networks import BaseNetwork\n\n\nclass MobileResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        super(MobileResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, dropout_rate, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [\n            SeparableConv2d(in_channels=dim, out_channels=dim,\n                            kernel_size=3, padding=p, stride=1),\n            norm_layer(dim), nn.ReLU(True)\n        ]\n        conv_block += [nn.Dropout(dropout_rate)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [\n            SeparableConv2d(in_channels=dim, out_channels=dim,\n                            kernel_size=3, padding=p, stride=1),\n            norm_layer(dim)\n        ]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass MobileResnetGenerator(BaseNetwork):\n    def __init__(self, input_nc, output_nc, ngf, norm_layer=nn.InstanceNorm2d,\n                 dropout_rate=0, n_blocks=9, padding_type=\'reflect\'):\n        assert (n_blocks >= 0)\n        super(MobileResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n\n        n_blocks1 = n_blocks // 3\n        n_blocks2 = n_blocks1\n        n_blocks3 = n_blocks - n_blocks1 - n_blocks2\n\n        for i in range(n_blocks1):\n            model += [MobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                        dropout_rate=dropout_rate,\n                                        use_bias=use_bias)]\n\n        for i in range(n_blocks2):\n            model += [MobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                        dropout_rate=dropout_rate,\n                                        use_bias=use_bias)]\n\n        for i in range(n_blocks3):\n            model += [MobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                        dropout_rate=dropout_rate,\n                                        use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        # input = input.clamp(-1, 1)\n        # for i, module in enumerate(self.model):\n        #     print(i, input.size())\n        #     print(module)\n        #     if isinstance(module, nn.Conv2d):\n        #         print(module.stride)\n        #     input = module(input)\n        # return input\n        return self.model(input)\n'"
models/modules/resnet_architecture/resnet_generator.py,0,"b'import functools\n\nfrom torch import nn\n\nfrom models.networks import BaseNetwork\n\n\nclass ResnetBlock(nn.Module):\n    """"""Define a mobile-version Resnet block""""""\n\n    def __init__(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        super(ResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, dropout_rate, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        """"""Construct a convolutional block.\n\n        Parameters:\n            dim (int)           -- the number of channels in the conv layer.\n            padding_type (str)  -- the name of padding layer: reflect | replicate | zero\n            norm_layer          -- normalization layer\n            use_dropout (bool)  -- if use dropout layers.\n            use_bias (bool)     -- if the conv layer uses bias or not\n\n        Returns a conv block (with a conv layer, a normalization layer, and a non-linearity layer (ReLU))\n        """"""\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim), nn.ReLU(True)]\n\n        conv_block += [nn.Dropout(dropout_rate)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=p, bias=use_bias), norm_layer(dim)]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        """"""Forward function (with skip connections)""""""\n        out = x + self.conv_block(x)  # add skip connections\n        return out\n\n\nclass ResnetGenerator(BaseNetwork):\n    def __init__(self, input_nc, output_nc, ngf=64, norm_layer=nn.BatchNorm2d, dropout_rate=0, n_blocks=6,\n                 padding_type=\'reflect\'):\n        assert (n_blocks >= 0)\n        super(ResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n        for i in range(n_blocks):  # add ResNet blocks\n            model += [\n                ResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer, dropout_rate=dropout_rate,\n                            use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        input = input.clamp(-1, 1)\n        return self.model(input)\n'"
models/modules/resnet_architecture/sub_mobile_resnet_generator.py,0,"b""import functools\n\nfrom torch import nn\n\nfrom models.modules.mobile_modules import SeparableConv2d\nfrom models.networks import BaseNetwork\n\n\nclass MobileResnetBlock(nn.Module):\n    def __init__(self, ic, oc, padding_type, norm_layer, dropout_rate, use_bias):\n        super(MobileResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(ic, oc, padding_type, norm_layer, dropout_rate, use_bias)\n\n    def build_conv_block(self, ic, oc, padding_type, norm_layer, dropout_rate, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [\n            SeparableConv2d(in_channels=ic, out_channels=oc,\n                            kernel_size=3, padding=p, stride=1),\n            norm_layer(oc),\n            nn.ReLU(True)\n        ]\n        conv_block += [nn.Dropout(dropout_rate)]\n\n        p = 0\n        if padding_type == 'reflect':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == 'replicate':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == 'zero':\n            p = 1\n        else:\n            raise NotImplementedError('padding [%s] is not implemented' % padding_type)\n\n        conv_block += [\n            SeparableConv2d(in_channels=oc, out_channels=ic,\n                            kernel_size=3, padding=p, stride=1),\n            norm_layer(oc)\n        ]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        out = x + self.conv_block(x)\n        return out\n\n\nclass SubMobileResnetGenerator(BaseNetwork):\n    def __init__(self, input_nc, output_nc, config, norm_layer=nn.BatchNorm2d,\n                 dropout_rate=0, n_blocks=9, padding_type='reflect'):\n        assert n_blocks >= 0\n        super(SubMobileResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, config['channels'][0], kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(config['channels'][0]),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n\n            mult = 2 ** i\n            ic = config['channels'][i]\n            oc = config['channels'][i + 1]\n            model += [nn.Conv2d(ic * mult, oc * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ic * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n\n        ic = config['channels'][2]\n        # oc = config['channels'][3]\n        # for i in range(n_blocks):\n        #     oc = config['channels'][i + 3]\n        #     model += [MobileResnetBlock(ic * mult, oc * mult, padding_type=padding_type, norm_layer=norm_layer,\n        #                                 dropout_rate=dropout_rate, use_bias=use_bias)]\n        for i in range(n_blocks):\n            if len(config['channels']) == 6:\n                offset = 0\n            else:\n                offset = i // 3\n            oc = config['channels'][offset + 3]\n            model += [MobileResnetBlock(ic * mult, oc * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                        dropout_rate=dropout_rate, use_bias=use_bias)]\n\n        if len(config['channels']) == 6:\n            offset = 4\n        else:\n            offset = 6\n        for i in range(n_downsampling):  # add upsampling layers\n            oc = config['channels'][offset + i]\n            # print(oc)\n            mult = 2 ** (n_downsampling - i)\n            model += [nn.ConvTranspose2d(ic * mult, int(oc * mult / 2),\n                                         kernel_size=3, stride=2,\n                                         padding=1, output_padding=1,\n                                         bias=use_bias),\n                      norm_layer(int(oc * mult / 2)),\n                      nn.ReLU(True)]\n            ic = oc\n        model += [nn.ReflectionPad2d(3)]\n        model += [nn.Conv2d(ic, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        input = input.clamp(-1, 1)\n        return self.model(input)\n"""
models/modules/resnet_architecture/super_mobile_resnet_generator.py,0,"b'import functools\n\nfrom torch import nn\n\nfrom models.modules.super_modules import SuperConvTranspose2d, SuperConv2d, SuperSeparableConv2d\nfrom models.networks import BaseNetwork\n\n\nclass SuperMobileResnetBlock(nn.Module):\n    def __init__(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        super(SuperMobileResnetBlock, self).__init__()\n        self.conv_block = self.build_conv_block(dim, padding_type, norm_layer, dropout_rate, use_bias)\n\n    def build_conv_block(self, dim, padding_type, norm_layer, dropout_rate, use_bias):\n        conv_block = []\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [\n            SuperSeparableConv2d(in_channels=dim, out_channels=dim,\n                                 kernel_size=3, padding=p, stride=1),\n            norm_layer(dim),\n            nn.ReLU(True)\n        ]\n        conv_block += [nn.Dropout(dropout_rate)]\n\n        p = 0\n        if padding_type == \'reflect\':\n            conv_block += [nn.ReflectionPad2d(1)]\n        elif padding_type == \'replicate\':\n            conv_block += [nn.ReplicationPad2d(1)]\n        elif padding_type == \'zero\':\n            p = 1\n        else:\n            raise NotImplementedError(\'padding [%s] is not implemented\' % padding_type)\n\n        conv_block += [\n            SuperSeparableConv2d(in_channels=dim, out_channels=dim,\n                                 kernel_size=3, padding=p, stride=1),\n            norm_layer(dim)\n        ]\n\n        return nn.Sequential(*conv_block)\n\n    def forward(self, input, config):\n        x = input\n        cnt = 0\n        for module in self.conv_block:\n            if isinstance(module, SuperSeparableConv2d):\n                if cnt == 1:\n                    config[\'channel\'] = input.size(1)\n                x = module(x, config)\n                cnt += 1\n            else:\n                x = module(x)\n        out = input + x\n        return out\n\n\nclass SuperMobileResnetGenerator(BaseNetwork):\n    def __init__(self, input_nc, output_nc, ngf, norm_layer=nn.BatchNorm2d, dropout_rate=0, n_blocks=6,\n                 padding_type=\'reflect\'):\n        assert n_blocks >= 0\n        super(SuperMobileResnetGenerator, self).__init__()\n        if type(norm_layer) == functools.partial:\n            use_bias = norm_layer.func == nn.InstanceNorm2d\n        else:\n            use_bias = norm_layer == nn.InstanceNorm2d\n\n        model = [nn.ReflectionPad2d(3),\n                 SuperConv2d(input_nc, ngf, kernel_size=7, padding=0, bias=use_bias),\n                 norm_layer(ngf),\n                 nn.ReLU(True)]\n\n        n_downsampling = 2\n        for i in range(n_downsampling):  # add downsampling layers\n            mult = 2 ** i\n            model += [SuperConv2d(ngf * mult, ngf * mult * 2, kernel_size=3, stride=2, padding=1, bias=use_bias),\n                      norm_layer(ngf * mult * 2),\n                      nn.ReLU(True)]\n\n        mult = 2 ** n_downsampling\n\n        n_blocks1 = n_blocks // 3\n        n_blocks2 = n_blocks1\n        n_blocks3 = n_blocks - n_blocks1 - n_blocks2\n\n        for i in range(n_blocks1):\n            model += [SuperMobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                             dropout_rate=dropout_rate,\n                                             use_bias=use_bias)]\n\n        for i in range(n_blocks2):\n            model += [SuperMobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                             dropout_rate=dropout_rate,\n                                             use_bias=use_bias)]\n\n        for i in range(n_blocks3):\n            model += [SuperMobileResnetBlock(ngf * mult, padding_type=padding_type, norm_layer=norm_layer,\n                                             dropout_rate=dropout_rate,\n                                             use_bias=use_bias)]\n\n        for i in range(n_downsampling):  # add upsampling layers\n            mult = 2 ** (n_downsampling - i)\n            model += [SuperConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n                                           kernel_size=3, stride=2,\n                                           padding=1, output_padding=1,\n                                           bias=use_bias),\n                      norm_layer(int(ngf * mult / 2)),\n                      nn.ReLU(True)]\n        model += [nn.ReflectionPad2d(3)]\n        model += [SuperConv2d(ngf, output_nc, kernel_size=7, padding=0)]\n        model += [nn.Tanh()]\n        self.model = nn.Sequential(*model)\n\n    def forward(self, input):\n        """"""Standard forward""""""\n        configs = self.configs\n        input = input.clamp(-1, 1)\n        x = input\n        cnt = 0\n        for i in range(0, 10):\n            module = self.model[i]\n            if isinstance(module, SuperConv2d):\n                channel = configs[\'channels\'][cnt] * (2 ** cnt)\n                config = {\'channel\': channel}\n                x = module(x, config)\n                cnt += 1\n            else:\n                x = module(x)\n        for i in range(3):\n            for j in range(10 + i * 3, 13 + i * 3):\n                if len(configs[\'channels\']) == 6:\n                    channel = configs[\'channels\'][3] * 4\n                else:\n                    channel = configs[\'channels\'][i + 3] * 4\n                config = {\'channel\': channel}\n                module = self.model[j]\n                x = module(x, config)\n        cnt = 2\n        for i in range(19, 28):\n            module = self.model[i]\n            if isinstance(module, SuperConvTranspose2d):\n                cnt -= 1\n                if len(configs[\'channels\']) == 6:\n                    channel = configs[\'channels\'][5 - cnt] * (2 ** cnt)\n                else:\n                    channel = configs[\'channels\'][7 - cnt] * (2 ** cnt)\n                config = {\'channel\': channel}\n                x = module(x, config)\n            elif isinstance(module, SuperConv2d):\n                config = {\'channel\': module.out_channels}\n                x = module(x, config)\n            else:\n                x = module(x)\n        return x\n'"
models/modules/spade_architecture/__init__.py,0,b''
models/modules/spade_architecture/mobile_spade_generator.py,3,"b'import torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom models.networks import BaseNetwork\nfrom .normalization import MobileSPADE\n\n\nclass MobileSPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, opt):\n        super(MobileSPADEResnetBlock, self).__init__()\n        # Attributes\n        self.learned_shortcut = (fin != fout)\n        fmiddle = min(fin, fout)\n\n        # create conv layers\n        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n\n        # apply spectral norm if specified\n\n        # define normalization layers\n        spade_config_str = opt.norm_G\n        self.norm_0 = MobileSPADE(spade_config_str, fin, opt.semantic_nc, nhidden=opt.ngf * 2)\n        self.norm_1 = MobileSPADE(spade_config_str, fmiddle, opt.semantic_nc, nhidden=opt.ngf * 2)\n        if self.learned_shortcut:\n            self.norm_s = MobileSPADE(spade_config_str, fin, opt.semantic_nc, nhidden=opt.ngf * 2)\n\n    # note the resnet block with SPADE also takes in |seg|,\n    # the semantic segmentation map as input\n    def forward(self, x, seg):\n        x_s = self.shortcut(x, seg)\n\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n\n        out = x_s + dx\n\n        return out\n\n    def shortcut(self, x, seg):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg))\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\nclass MobileSPADEGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--norm_G\', type=str, default=\'spadesyncbatch3x3\',\n                            help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--num_upsampling_layers\',\n                            choices=(\'normal\', \'more\', \'most\'), default=\'more\',\n                            help=""If \'more\', adds upsampling layer between the two middle resnet blocks. ""\n                                 ""If \'most\', also add one more upsampling + resnet layer at the end of the generator"")\n\n        return parser\n\n    def __init__(self, opt):\n        super(MobileSPADEGenerator, self).__init__()\n        self.opt = opt\n        nf = opt.ngf\n\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        # downsampled segmentation map instead of random z\n        self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n\n        self.head_0 = MobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.G_middle_0 = MobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_1 = MobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.up_0 = MobileSPADEResnetBlock(16 * nf, 8 * nf, opt)\n        self.up_1 = MobileSPADEResnetBlock(8 * nf, 4 * nf, opt)\n        self.up_2 = MobileSPADEResnetBlock(4 * nf, 2 * nf, opt)\n        self.up_3 = MobileSPADEResnetBlock(2 * nf, 1 * nf, opt)\n\n        final_nc = nf\n\n        if opt.num_upsampling_layers == \'most\':\n            self.up_4 = MobileSPADEResnetBlock(1 * nf, nf // 2, opt)\n            final_nc = nf // 2\n\n        self.conv_img = nn.Conv2d(final_nc, 3, 3, padding=1)\n\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        if opt.num_upsampling_layers == \'normal\':\n            num_up_layers = 5\n        elif opt.num_upsampling_layers == \'more\':\n            num_up_layers = 6\n        elif opt.num_upsampling_layers == \'most\':\n            num_up_layers = 7\n        else:\n            raise ValueError(\'opt.num_upsampling_layers [%s] not recognized\' %\n                             opt.num_upsampling_layers)\n\n        sw = opt.crop_size // (2 ** num_up_layers)\n        sh = round(sw / opt.aspect_ratio)\n\n        return sw, sh\n\n    def forward(self, input, z=None):\n        seg = input\n\n        if self.opt.use_vae:\n            # we sample z from unit normal and reshape the tensor\n            if z is None:\n                z = torch.randn(input.size(0), self.opt.z_dim,\n                                dtype=torch.float32, device=input.get_device())\n            x = self.fc(z)\n            x = x.view(-1, 16 * self.opt.ngf, self.sh, self.sw)\n        else:\n            # we downsample segmap and run convolution\n            x = F.interpolate(seg, size=(self.sh, self.sw))\n            x = self.fc(x)\n\n        x = self.head_0(x, seg)\n\n        x = self.up(x)\n        x = self.G_middle_0(x, seg)\n\n        if self.opt.num_upsampling_layers == \'more\' or \\\n                self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n\n        x = self.G_middle_1(x, seg)\n\n        x = self.up(x)\n        x = self.up_0(x, seg)\n        x = self.up(x)\n        x = self.up_1(x, seg)\n        x = self.up(x)\n        x = self.up_2(x, seg)\n        x = self.up(x)\n        x = self.up_3(x, seg)\n\n        if self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n            x = self.up_4(x, seg)\n\n        x = self.conv_img(F.leaky_relu(x, 2e-1))\n        x = F.tanh(x)\n\n        return x\n'"
models/modules/spade_architecture/normalization.py,3,"b'""""""\nCopyright (C) 2019 NVIDIA Corporation.  All rights reserved.\nLicensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode).\n""""""\n\nimport re\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils.spectral_norm as spectral_norm\n\nfrom models.modules.mobile_modules import SeparableConv2d\nfrom models.modules.super_modules import SuperConv2d, SuperSeparableConv2d, SuperSynchronizedBatchNorm2d\nfrom models.modules.sync_batchnorm import SynchronizedBatchNorm2d\n\n\n# Returns a function that creates a normalization function\n# that does not condition on semantic map\ndef get_nonspade_norm_layer(opt, norm_type=\'instance\'):\n    # helper function to get # output channels of the previous layer\n    def get_out_channel(layer):\n        if hasattr(layer, \'out_channels\'):\n            return getattr(layer, \'out_channels\')\n        return layer.weight.size(0)\n\n    # this function will be returned\n    def add_norm_layer(layer):\n        nonlocal norm_type\n        if norm_type.startswith(\'spectral\'):\n            layer = spectral_norm(layer)\n            subnorm_type = norm_type[len(\'spectral\'):]\n\n        if subnorm_type == \'none\' or len(subnorm_type) == 0:\n            return layer\n\n        # remove bias in the previous layer, which is meaningless\n        # since it has no effect after normalization\n        if getattr(layer, \'bias\', None) is not None:\n            delattr(layer, \'bias\')\n            layer.register_parameter(\'bias\', None)\n\n        if subnorm_type == \'batch\':\n            norm_layer = nn.BatchNorm2d(get_out_channel(layer), affine=True)\n        elif subnorm_type == \'sync_batch\':\n            norm_layer = SynchronizedBatchNorm2d(get_out_channel(layer), affine=True)\n        elif subnorm_type == \'instance\':\n            norm_layer = nn.InstanceNorm2d(get_out_channel(layer), affine=False)\n        else:\n            raise ValueError(\'normalization layer %s is not recognized\' % subnorm_type)\n\n        return nn.Sequential(layer, norm_layer)\n\n    return add_norm_layer\n\n\n# Creates SPADE normalization layer based on the given configuration\n# SPADE consists of two steps. First, it normalizes the activations using\n# your favorite normalization method, such as Batch Norm or Instance Norm.\n# Second, it applies scale and bias to the normalized output, conditioned on\n# the segmentation map.\n# The format of |config_text| is spade(norm)(ks), where\n# (norm) specifies the type of parameter-free normalization.\n#       (e.g. syncbatch, batch, instance)\n# (ks) specifies the size of kernel in the SPADE module (e.g. 3x3)\n# Example |config_text| will be spadesyncbatch3x3, or spadeinstance5x5.\n# Also, the other arguments are\n# |norm_nc|: the #channels of the normalized activations, hence the output dim of SPADE\n# |label_nc|: the #channels of the input semantic map, hence the input dim of SPADE\nclass MobileSPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc, nhidden=128):\n        super(MobileSPADE, self).__init__()\n\n        assert config_text.startswith(\'spade\')\n        parsed = re.search(r\'spade(\\D+)(\\d)x\\d\', config_text)\n        param_free_norm_type = str(parsed.group(1))\n        ks = int(parsed.group(2))\n\n        if param_free_norm_type == \'instance\':\n            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'syncbatch\':\n            self.param_free_norm = SynchronizedBatchNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'batch\':\n            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n        else:\n            raise ValueError(\'%s is not a recognized param-free norm type in SPADE\'\n                             % param_free_norm_type)\n\n        # The dimension of the intermediate embedding space. Yes, hardcoded.\n\n        pw = ks // 2\n        self.mlp_shared = nn.Sequential(\n            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = SeparableConv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n        self.mlp_beta = SeparableConv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap):\n\n        # Part 1. generate parameter-free normalized activations\n        normalized = self.param_free_norm(x)\n\n        # Part 2. produce scaling and bias conditioned on semantic map\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode=\'nearest\')\n        actv = self.mlp_shared(segmap)\n        gamma = self.mlp_gamma(actv)\n        beta = self.mlp_beta(actv)\n\n        # apply scale and bias\n        out = normalized * (1 + gamma) + beta\n\n        return out\n\n\nclass SPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc, nhidden=128):\n        super(SPADE, self).__init__()\n\n        assert config_text.startswith(\'spade\')\n        parsed = re.search(r\'spade(\\D+)(\\d)x\\d\', config_text)\n        param_free_norm_type = str(parsed.group(1))\n        ks = int(parsed.group(2))\n\n        if param_free_norm_type == \'instance\':\n            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'syncbatch\':\n            self.param_free_norm = SynchronizedBatchNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'batch\':\n            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n        else:\n            raise ValueError(\'%s is not a recognized param-free norm type in SPADE\'\n                             % param_free_norm_type)\n\n        # The dimension of the intermediate embedding space. Yes, hardcoded.\n\n        pw = ks // 2\n        self.mlp_shared = nn.Sequential(\n            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n        self.mlp_beta = nn.Conv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap):\n\n        # Part 1. generate parameter-free normalized activations\n        normalized = self.param_free_norm(x)\n\n        # Part 2. produce scaling and bias conditioned on semantic map\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode=\'nearest\')\n        actv = self.mlp_shared(segmap)\n        gamma = self.mlp_gamma(actv)\n        beta = self.mlp_beta(actv)\n\n        # apply scale and bias\n        out = normalized * (1 + gamma) + beta\n\n        return out\n\n\nclass SuperMobileSPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc, nhidden=128):\n        super(SuperMobileSPADE, self).__init__()\n\n        assert config_text.startswith(\'spade\')\n        parsed = re.search(r\'spade(\\D+)(\\d)x\\d\', config_text)\n        param_free_norm_type = str(parsed.group(1))\n        ks = int(parsed.group(2))\n\n        if param_free_norm_type == \'instance\':\n            self.param_free_norm = nn.InstanceNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'syncbatch\':\n            self.param_free_norm = SuperSynchronizedBatchNorm2d(norm_nc, affine=False)\n        elif param_free_norm_type == \'batch\':\n            self.param_free_norm = nn.BatchNorm2d(norm_nc, affine=False)\n        else:\n            raise ValueError(\'%s is not a recognized param-free norm type in SPADE\'\n                             % param_free_norm_type)\n\n        # The dimension of the intermediate embedding space. Yes, hardcoded.\n\n        pw = ks // 2\n        self.mlp_shared = nn.Sequential(\n            SuperConv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = SuperSeparableConv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n        self.mlp_beta = SuperSeparableConv2d(nhidden, norm_nc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap, config, verbose=False):\n        # print(\'###\', config, self.param_free_norm)\n        # Part 1. generate parameter-free normalized activations\n        normalized = self.param_free_norm(x, config)\n\n        # Part 2. produce scaling and bias conditioned on semantic map\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode=\'nearest\')\n        channel = config[\'hidden\']\n        actv = self.mlp_shared[0](segmap, {\'channel\': channel})\n        actv = self.mlp_shared[1](actv)\n        gamma = self.mlp_gamma(actv, {\'channel\': x.shape[1]})\n        beta = self.mlp_beta(actv, {\'channel\': x.shape[1]})\n\n        # apply scale and bias\n        out = normalized * (1 + gamma) + beta\n\n        return out\n\n\nclass SubMobileSPADE(nn.Module):\n    def __init__(self, config_text, norm_nc, label_nc, nhidden, oc):\n        super(SubMobileSPADE, self).__init__()\n\n        assert config_text.startswith(\'spade\')\n        parsed = re.search(r\'spade(\\D+)(\\d)x\\d\', config_text)\n        param_free_norm_type = str(parsed.group(1))\n        ks = int(parsed.group(2))\n\n        if param_free_norm_type == \'syncbatch\':\n            self.param_free_norm = SuperSynchronizedBatchNorm2d(norm_nc, affine=False)\n        else:\n            raise ValueError(\'%s is not a recognized param-free norm type in SPADE\'\n                             % param_free_norm_type)\n\n        # The dimension of the intermediate embedding space. Yes, hardcoded.\n\n        pw = ks // 2\n        self.mlp_shared = nn.Sequential(\n            nn.Conv2d(label_nc, nhidden, kernel_size=ks, padding=pw),\n            nn.ReLU()\n        )\n        self.mlp_gamma = SeparableConv2d(nhidden, oc, kernel_size=ks, padding=pw)\n        self.mlp_beta = SeparableConv2d(nhidden, oc, kernel_size=ks, padding=pw)\n\n    def forward(self, x, segmap):\n\n        # Part 1. generate parameter-free normalized activations\n        normalized = self.param_free_norm(x)\n\n        # Part 2. produce scaling and bias conditioned on semantic map\n        segmap = F.interpolate(segmap, size=x.size()[2:], mode=\'nearest\')\n        actv = self.mlp_shared(segmap)\n        gamma = self.mlp_gamma(actv)\n        beta = self.mlp_beta(actv)\n\n        # apply scale and bias\n        out = normalized * (1 + gamma) + beta\n\n        return out\n'"
models/modules/spade_architecture/spade_generator.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\nfrom models.networks import BaseNetwork\nfrom .normalization import SPADE\n\n\nclass SPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, opt):\n        super().__init__()\n        # Attributes\n        self.learned_shortcut = (fin != fout)\n        fmiddle = min(fin, fout)\n\n        # create conv layers\n        self.conv_0 = nn.Conv2d(fin, fmiddle, kernel_size=3, padding=1)\n        self.conv_1 = nn.Conv2d(fmiddle, fout, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_s = nn.Conv2d(fin, fout, kernel_size=1, bias=False)\n\n        # define normalization layers\n        spade_config_str = opt.norm_G\n        self.norm_0 = SPADE(spade_config_str, fin, opt.semantic_nc)\n        self.norm_1 = SPADE(spade_config_str, fmiddle, opt.semantic_nc)\n        if self.learned_shortcut:\n            self.norm_s = SPADE(spade_config_str, fin, opt.semantic_nc)\n\n    # note the resnet block with SPADE also takes in |seg|,\n    # the semantic segmentation map as input\n    def forward(self, x, seg):\n        x_s = self.shortcut(x, seg)\n\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n\n        out = x_s + dx\n\n        return out\n\n    def shortcut(self, x, seg):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg))\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\nclass SPADEGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--norm_G\', type=str, default=\'spadesyncbatch3x3\',\n                            help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--num_upsampling_layers\',\n                            choices=(\'normal\', \'more\', \'most\'), default=\'more\',\n                            help=""If \'more\', adds upsampling layer between the two middle resnet blocks. ""\n                                 ""If \'most\', also add one more upsampling + resnet layer at the end of the generator"")\n        return parser\n\n    def __init__(self, opt):\n        super(SPADEGenerator, self).__init__()\n        self.opt = opt\n        nf = opt.ngf\n\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n\n        self.head_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.G_middle_0 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_1 = SPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.up_0 = SPADEResnetBlock(16 * nf, 8 * nf, opt)\n        self.up_1 = SPADEResnetBlock(8 * nf, 4 * nf, opt)\n        self.up_2 = SPADEResnetBlock(4 * nf, 2 * nf, opt)\n        self.up_3 = SPADEResnetBlock(2 * nf, 1 * nf, opt)\n\n        final_nc = nf\n\n        if opt.num_upsampling_layers == \'most\':\n            self.up_4 = SPADEResnetBlock(1 * nf, nf // 2, opt)\n            final_nc = nf // 2\n\n        self.conv_img = nn.Conv2d(final_nc, 3, 3, padding=1)\n\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        if opt.num_upsampling_layers == \'normal\':\n            num_up_layers = 5\n        elif opt.num_upsampling_layers == \'more\':\n            num_up_layers = 6\n        elif opt.num_upsampling_layers == \'most\':\n            num_up_layers = 7\n        else:\n            raise ValueError(\'opt.num_upsampling_layers [%s] not recognized\' %\n                             opt.num_upsampling_layers)\n\n        sw = opt.crop_size // (2 ** num_up_layers)\n        sh = round(sw / opt.aspect_ratio)\n\n        return sw, sh\n\n    def forward(self, input, z=None):\n        seg = input\n\n        # we downsample segmap and run convolution\n        x = F.interpolate(seg, size=(self.sh, self.sw))\n        x = self.fc(x)\n\n        x = self.head_0(x, seg)\n\n        x = self.up(x)\n        x = self.G_middle_0(x, seg)\n\n        if self.opt.num_upsampling_layers == \'more\' or \\\n                self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n\n        x = self.G_middle_1(x, seg)\n\n        x = self.up(x)\n        x = self.up_0(x, seg)\n        x = self.up(x)\n        x = self.up_1(x, seg)\n        x = self.up(x)\n        x = self.up_2(x, seg)\n        x = self.up(x)\n        x = self.up_3(x, seg)\n\n        if self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n            x = self.up_4(x, seg)\n\n        x = self.conv_img(F.leaky_relu(x, 2e-1))\n        x = F.tanh(x)\n\n        return x\n'"
models/modules/spade_architecture/sub_mobile_spade_generator.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\nfrom models.networks import BaseNetwork\nfrom .normalization import SubMobileSPADE\n\n\nclass SubMobileSPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, ic, opt, config):\n        super(SubMobileSPADEResnetBlock, self).__init__()\n        # Attributes\n        self.learned_shortcut = (fin != fout)\n        self.ic = ic\n        self.config = config\n        channel, hidden = config[\'channel\'], config[\'hidden\']\n\n        fmiddle = min(fin, fout)\n\n        # create conv layers\n        self.conv_0 = nn.Conv2d(ic, channel, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_1 = nn.Conv2d(channel, channel, kernel_size=3, padding=1)\n        else:\n            self.conv_1 = nn.Conv2d(channel, ic, kernel_size=3, padding=1)\n\n        if self.learned_shortcut:\n            self.conv_s = nn.Conv2d(ic, channel, kernel_size=1, bias=False)\n\n        # apply spectral norm if specified\n\n        # define normalization layers\n        spade_config_str = opt.norm_G\n        self.norm_0 = SubMobileSPADE(spade_config_str, fin, opt.semantic_nc,\n                                     nhidden=hidden, oc=ic)\n        self.norm_1 = SubMobileSPADE(spade_config_str, fmiddle, opt.semantic_nc,\n                                     nhidden=hidden, oc=channel)\n        if self.learned_shortcut:\n            self.norm_s = SubMobileSPADE(spade_config_str, fin, opt.semantic_nc,\n                                         nhidden=hidden, oc=ic)\n\n    # note the resnet block with SPADE also takes in |seg|,\n    # the semantic segmentation map as input\n    def forward(self, x, seg):\n        x_s = self.shortcut(x, seg)\n\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg)))\n        dx = self.conv_1(self.actvn(self.norm_1(dx, seg)))\n\n        out = x_s + dx\n\n        return out\n\n    def shortcut(self, x, seg):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg))\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\nclass SubMobileSPADEGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--norm_G\', type=str, default=\'spadesyncbatch3x3\',\n                            help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--num_upsampling_layers\',\n                            choices=(\'normal\', \'more\', \'most\'), default=\'more\',\n                            help=""If \'more\', adds upsampling layer between the two middle resnet blocks. ""\n                                 ""If \'most\', also add one more upsampling + resnet layer at the end of the generator"")\n\n        return parser\n\n    def __init__(self, opt, config):\n        super(SubMobileSPADEGenerator, self).__init__()\n        self.opt = opt\n        self.config = config\n        nf = opt.ngf\n\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        # downsampled segmentation map instead of random z\n        channel = config[\'channels\'][0]\n        self.fc = nn.Conv2d(self.opt.semantic_nc, 16 * channel, 3, padding=1)\n\n        ic = channel * 16\n        channel = config[\'channels\'][1]\n        self.head_0 = SubMobileSPADEResnetBlock(16 * nf, 16 * nf, ic, opt,\n                                                {\'channel\': channel * 16,\n                                                 \'hidden\': channel * 2})\n\n        channel = config[\'channels\'][2]\n        self.G_middle_0 = SubMobileSPADEResnetBlock(16 * nf, 16 * nf, ic, opt,\n                                                    {\'channel\': channel * 16,\n                                                     \'hidden\': channel * 2})\n\n        channel = config[\'channels\'][3]\n        self.G_middle_1 = SubMobileSPADEResnetBlock(16 * nf, 16 * nf, ic, opt,\n                                                    {\'channel\': channel * 16,\n                                                     \'hidden\': channel * 2})\n\n        channel = config[\'channels\'][4]\n        self.up_0 = SubMobileSPADEResnetBlock(16 * nf, 8 * nf, ic, opt,\n                                              {\'channel\': channel * 8,\n                                               \'hidden\': channel * 2})\n\n        ic = channel * 8\n        channel = config[\'channels\'][5]\n        self.up_1 = SubMobileSPADEResnetBlock(8 * nf, 4 * nf, ic, opt,\n                                              {\'channel\': channel * 4,\n                                               \'hidden\': channel * 2})\n        ic = channel * 4\n        channel = config[\'channels\'][6]\n        self.up_2 = SubMobileSPADEResnetBlock(4 * nf, 2 * nf, ic, opt,\n                                              {\'channel\': channel * 2,\n                                               \'hidden\': channel * 2})\n        ic = channel * 2\n        channel = config[\'channels\'][7]\n        self.up_3 = SubMobileSPADEResnetBlock(2 * nf, 1 * nf, ic, opt,\n                                              {\'channel\': channel,\n                                               \'hidden\': channel * 2})\n\n        final_nc = channel\n\n        if opt.num_upsampling_layers == \'most\':\n            raise NotImplementedError\n        self.conv_img = nn.Conv2d(final_nc, 3, 3, padding=1)\n\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        if opt.num_upsampling_layers == \'normal\':\n            num_up_layers = 5\n        elif opt.num_upsampling_layers == \'more\':\n            num_up_layers = 6\n        elif opt.num_upsampling_layers == \'most\':\n            num_up_layers = 7\n        else:\n            raise ValueError(\'opt.num_upsampling_layers [%s] not recognized\' %\n                             opt.num_upsampling_layers)\n\n        sw = opt.crop_size // (2 ** num_up_layers)\n        sh = round(sw / opt.aspect_ratio)\n\n        return sw, sh\n\n    def forward(self, input, z=None):\n        seg = input\n\n        # we downsample segmap and run convolution\n        x = F.interpolate(seg, size=(self.sh, self.sw))\n        x = self.fc(x)\n\n        x = self.head_0(x, seg)\n\n        x = self.up(x)\n        x = self.G_middle_0(x, seg)\n\n        if self.opt.num_upsampling_layers == \'more\' or \\\n                self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n\n        x = self.G_middle_1(x, seg)\n\n        x = self.up(x)\n        x = self.up_0(x, seg)\n        x = self.up(x)\n        x = self.up_1(x, seg)\n        x = self.up(x)\n        x = self.up_2(x, seg)\n        x = self.up(x)\n        x = self.up_3(x, seg)\n\n        if self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n            x = self.up_4(x, seg)\n\n        x = self.conv_img(F.leaky_relu(x, 2e-1))\n        x = F.tanh(x)\n\n        return x\n'"
models/modules/spade_architecture/super_mobile_spade_generator.py,1,"b'from torch import nn\nfrom torch.nn import functional as F\n\nfrom models.modules.super_modules import SuperConv2d\nfrom models.networks import BaseNetwork\nfrom .normalization import SuperMobileSPADE\n\n\nclass SuperMobileSPADEResnetBlock(nn.Module):\n    def __init__(self, fin, fout, opt):\n        super(SuperMobileSPADEResnetBlock, self).__init__()\n        # Attributes\n        self.learned_shortcut = (fin != fout)\n        fmiddle = min(fin, fout)\n\n        # create conv layers\n        self.conv_0 = SuperConv2d(fin, fmiddle, kernel_size=3, padding=1)\n        self.conv_1 = SuperConv2d(fmiddle, fout, kernel_size=3, padding=1)\n        if self.learned_shortcut:\n            self.conv_s = SuperConv2d(fin, fout, kernel_size=1, bias=False)\n\n        # define normalization layers\n        spade_config_str = opt.norm_G\n        self.norm_0 = SuperMobileSPADE(spade_config_str, fin, opt.semantic_nc, nhidden=opt.ngf * 2)\n        self.norm_1 = SuperMobileSPADE(spade_config_str, fmiddle, opt.semantic_nc, nhidden=opt.ngf * 2)\n        if self.learned_shortcut:\n            self.norm_s = SuperMobileSPADE(spade_config_str, fin, opt.semantic_nc, nhidden=opt.ngf * 2)\n\n    # note the resnet block with SPADE also takes in |seg|,\n    # the semantic segmentation map as input\n    def forward(self, x, seg, config, verbose=False):\n        # if verbose:\n        #     print(config)\n        #     print(self.learned_shortcut)\n        #     print(x.shape)\n        x_s = self.shortcut(x, seg, config)\n\n        dx = self.conv_0(self.actvn(self.norm_0(x, seg, config, verbose=verbose)), config)\n        if self.learned_shortcut:\n            dx = self.conv_1(self.actvn(self.norm_1(dx, seg, config)), config)\n        else:\n            dx = self.conv_1(self.actvn(self.norm_1(dx, seg, config)), {\'channel\': x.shape[1]})\n\n        out = x_s + dx\n\n        return out\n\n    def shortcut(self, x, seg, config):\n        if self.learned_shortcut:\n            x_s = self.conv_s(self.norm_s(x, seg, config), config)\n        else:\n            x_s = x\n        return x_s\n\n    def actvn(self, x):\n        return F.leaky_relu(x, 2e-1)\n\n\nclass SuperMobileSPADEGenerator(BaseNetwork):\n    @staticmethod\n    def modify_commandline_options(parser, is_train):\n        parser.add_argument(\'--norm_G\', type=str, default=\'spadesyncbatch3x3\',\n                            help=\'instance normalization or batch normalization\')\n        parser.add_argument(\'--num_upsampling_layers\',\n                            choices=(\'normal\', \'more\', \'most\'), default=\'more\',\n                            help=""If \'more\', adds upsampling layer between the two middle resnet blocks. ""\n                                 ""If \'most\', also add one more upsampling + resnet layer at the end of the generator"")\n\n        return parser\n\n    def __init__(self, opt):\n        super(SuperMobileSPADEGenerator, self).__init__()\n        self.opt = opt\n        nf = opt.ngf\n\n        self.sw, self.sh = self.compute_latent_vector_size(opt)\n\n        # downsampled segmentation map instead of random z\n        self.fc = SuperConv2d(self.opt.semantic_nc, 16 * nf, 3, padding=1)\n\n        self.head_0 = SuperMobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.G_middle_0 = SuperMobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n        self.G_middle_1 = SuperMobileSPADEResnetBlock(16 * nf, 16 * nf, opt)\n\n        self.up_0 = SuperMobileSPADEResnetBlock(16 * nf, 8 * nf, opt)\n        self.up_1 = SuperMobileSPADEResnetBlock(8 * nf, 4 * nf, opt)\n        self.up_2 = SuperMobileSPADEResnetBlock(4 * nf, 2 * nf, opt)\n        self.up_3 = SuperMobileSPADEResnetBlock(2 * nf, 1 * nf, opt)\n\n        final_nc = nf\n\n        if opt.num_upsampling_layers == \'most\':\n            self.up_4 = SuperMobileSPADEResnetBlock(1 * nf, nf // 2, opt)\n            final_nc = nf // 2\n\n        self.conv_img = SuperConv2d(final_nc, 3, 3, padding=1)\n\n        self.up = nn.Upsample(scale_factor=2)\n\n    def compute_latent_vector_size(self, opt):\n        if opt.num_upsampling_layers == \'normal\':\n            num_up_layers = 5\n        elif opt.num_upsampling_layers == \'more\':\n            num_up_layers = 6\n        elif opt.num_upsampling_layers == \'most\':\n            num_up_layers = 7\n        else:\n            raise ValueError(\'opt.num_upsampling_layers [%s] not recognized\' %\n                             opt.num_upsampling_layers)\n\n        sw = opt.crop_size // (2 ** num_up_layers)\n        sh = round(sw / opt.aspect_ratio)\n\n        return sw, sh\n\n    def forward(self, input, acts=None, z=None):\n        seg = input\n        if acts is None:\n            acts = []\n        ret_acts = {}\n        # we downsample segmap and run convolution\n        x = F.interpolate(seg, size=(self.sh, self.sw))\n        channel = self.config[\'channels\'][0]\n        x = self.fc(x, {\'channel\': channel * 16})\n        if \'fc\' in acts:\n            ret_acts[\'fc\'] = x\n\n        channel = self.config[\'channels\'][1]\n        x = self.head_0(x, seg, {\'channel\': channel * 16, \'hidden\': channel * 2,\n                                 \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'head_0\' in acts:\n            ret_acts[\'head_0\'] = x\n\n        x = self.up(x)\n        channel = self.config[\'channels\'][2]\n        x = self.G_middle_0(x, seg, {\'channel\': channel * 16, \'hidden\': channel * 2,\n                                     \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'G_middle_0\' in acts:\n            ret_acts[\'G_middle_0\'] = x\n        # print(x.shape)\n        if self.opt.num_upsampling_layers == \'more\' or \\\n                self.opt.num_upsampling_layers == \'most\':\n            x = self.up(x)\n\n        channel = self.config[\'channels\'][3]\n        x = self.G_middle_1(x, seg, {\'channel\': channel * 16, \'hidden\': channel * 2,\n                                     \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'G_middle_1\' in acts:\n            ret_acts[\'G_middle_1\'] = x\n        x = self.up(x)\n        channel = self.config[\'channels\'][4]\n        x = self.up_0(x, seg, {\'channel\': channel * 8, \'hidden\': channel * 2,\n                               \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'up_0\' in acts:\n            ret_acts[\'up_0\'] = x\n        x = self.up(x)\n        channel = self.config[\'channels\'][5]\n        x = self.up_1(x, seg, {\'channel\': channel * 4, \'hidden\': channel * 2,\n                               \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'up_1\' in acts:\n            ret_acts[\'up_1\'] = x\n        x = self.up(x)\n        channel = self.config[\'channels\'][6]\n        x = self.up_2(x, seg, {\'channel\': channel * 2, \'hidden\': channel * 2,\n                               \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'up_2\' in acts:\n            ret_acts[\'up_2\'] = x\n        x = self.up(x)\n        channel = self.config[\'channels\'][7]\n        x = self.up_3(x, seg, {\'channel\': channel, \'hidden\': channel * 2,\n                               \'calibrate_bn\': self.config.get(\'calibrate_bn\', False)})\n        if \'up_3\' in acts:\n            ret_acts[\'up_3\'] = x\n        if self.opt.num_upsampling_layers == \'most\':\n            raise NotImplementedError\n        x = self.conv_img(F.leaky_relu(x, 2e-1), {\'channel\': self.conv_img.out_channels})\n        x = F.tanh(x)\n\n        if len(acts) == 0:\n            return x\n        else:\n            return x, ret_acts\n'"
models/modules/sync_batchnorm/__init__.py,0,"b'# -*- coding: utf-8 -*-\n# File   : __init__.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nfrom .batchnorm import SynchronizedBatchNorm1d, SynchronizedBatchNorm2d, SynchronizedBatchNorm3d\nfrom .batchnorm import patch_sync_batchnorm, convert_model\nfrom .replicate import DataParallelWithCallback, patch_replication_callback\n'"
models/modules/sync_batchnorm/batchnorm.py,16,"b'# -*- coding: utf-8 -*-\n# File   : batchnorm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\nimport contextlib\n\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\ntry:\n    from torch.nn.parallel._functions import ReduceAddCoalesced, Broadcast\nexcept ImportError:\n    ReduceAddCoalesced = Broadcast = None\n\ntry:\n    from jactorch.parallel.comm import SyncMaster\n    from jactorch.parallel.data_parallel import JacDataParallel as DataParallelWithCallback\nexcept ImportError:\n    from .comm import SyncMaster\n    from .replicate import DataParallelWithCallback\n\n__all__ = [\n    \'SynchronizedBatchNorm1d\', \'SynchronizedBatchNorm2d\', \'SynchronizedBatchNorm3d\',\n    \'patch_sync_batchnorm\', \'convert_model\'\n]\n\n\ndef _sum_ft(tensor):\n    """"""sum over the first and last dimention""""""\n    return tensor.sum(dim=0).sum(dim=-1)\n\n\ndef _unsqueeze_ft(tensor):\n    """"""add new dimensions at the front and the tail""""""\n    return tensor.unsqueeze(0).unsqueeze(-1)\n\n\n_ChildMessage = collections.namedtuple(\'_ChildMessage\', [\'sum\', \'ssum\', \'sum_size\'])\n_MasterMessage = collections.namedtuple(\'_MasterMessage\', [\'sum\', \'inv_std\'])\n\n\nclass _SynchronizedBatchNorm(_BatchNorm):\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n        assert ReduceAddCoalesced is not None, \'Can not use Synchronized Batch Normalization without CUDA support.\'\n\n        super(_SynchronizedBatchNorm, self).__init__(num_features, eps=eps, momentum=momentum, affine=affine)\n\n        self._sync_master = SyncMaster(self._data_parallel_master)\n\n        self._is_parallel = False\n        self._parallel_id = None\n        self._slave_pipe = None\n\n    def forward(self, input):\n        # If it is not parallel computation or is in evaluation mode, use PyTorch\'s implementation.\n        if not (self._is_parallel and self.training):\n            return F.batch_norm(\n                input, self.running_mean, self.running_var, self.weight, self.bias,\n                self.training, self.momentum, self.eps)\n\n        # Resize the input to (B, C, -1).\n        input_shape = input.size()\n        input = input.view(input.size(0), self.num_features, -1)\n\n        # Compute the sum and square-sum.\n        sum_size = input.size(0) * input.size(2)\n        input_sum = _sum_ft(input)\n        input_ssum = _sum_ft(input ** 2)\n\n        # Reduce-and-broadcast the statistics.\n        if self._parallel_id == 0:\n            mean, inv_std = self._sync_master.run_master(_ChildMessage(input_sum, input_ssum, sum_size))\n        else:\n            mean, inv_std = self._slave_pipe.run_slave(_ChildMessage(input_sum, input_ssum, sum_size))\n\n        # Compute the output.\n        if self.affine:\n            # MJY:: Fuse the multiplication for speed.\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std * self.weight) + _unsqueeze_ft(self.bias)\n        else:\n            output = (input - _unsqueeze_ft(mean)) * _unsqueeze_ft(inv_std)\n\n        # Reshape it.\n        return output.view(input_shape)\n\n    def __data_parallel_replicate__(self, ctx, copy_id):\n        self._is_parallel = True\n        self._parallel_id = copy_id\n\n        # parallel_id == 0 means master device.\n        if self._parallel_id == 0:\n            ctx.sync_master = self._sync_master\n        else:\n            self._slave_pipe = ctx.sync_master.register_slave(copy_id)\n\n    def _data_parallel_master(self, intermediates):\n        """"""Reduce the sum and square-sum, compute the statistics, and broadcast it.""""""\n\n        # Always using same ""device order"" makes the ReduceAdd operation faster.\n        # Thanks to:: Tete Xiao (http://tetexiao.com/)\n        intermediates = sorted(intermediates, key=lambda i: i[1].sum.get_device())\n\n        to_reduce = [i[1][:2] for i in intermediates]\n        to_reduce = [j for i in to_reduce for j in i]  # flatten\n        target_gpus = [i[1].sum.get_device() for i in intermediates]\n\n        sum_size = sum([i[1].sum_size for i in intermediates])\n        sum_, ssum = ReduceAddCoalesced.apply(target_gpus[0], 2, *to_reduce)\n        mean, inv_std = self._compute_mean_std(sum_, ssum, sum_size)\n\n        broadcasted = Broadcast.apply(target_gpus, mean, inv_std)\n\n        outputs = []\n        for i, rec in enumerate(intermediates):\n            outputs.append((rec[0], _MasterMessage(*broadcasted[i * 2:i * 2 + 2])))\n\n        return outputs\n\n    def _compute_mean_std(self, sum_, ssum, size):\n        """"""Compute the mean and standard-deviation with sum and square-sum. This method\n        also maintains the moving average on the master device.""""""\n        assert size > 1, \'BatchNorm computes unbiased standard-deviation, which requires size > 1.\'\n        mean = sum_ / size\n        sumvar = ssum - sum_ * mean\n        unbias_var = sumvar / (size - 1)\n        bias_var = sumvar / size\n\n        if hasattr(torch, \'no_grad\'):\n            with torch.no_grad():\n                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n                self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n        else:\n            self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.data\n            self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.data\n\n        return mean, bias_var.clamp(self.eps) ** -0.5\n\n\nclass SynchronizedBatchNorm1d(_SynchronizedBatchNorm):\n    r""""""Applies Synchronized Batch Normalization over a 2d or 3d input that is seen as a\n    mini-batch.\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm1d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, L)` slices, it\'s common terminology to call this Temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of size\n            `batch_size x num_features [x width]`\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C)` or :math:`(N, C, L)`\n        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm1d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 2 and input.dim() != 3:\n            raise ValueError(\'expected 2D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm1d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm2d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 4d input that is seen as a mini-batch\n    of 3d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm2d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, H, W)` slices, it\'s common terminology to call this Spatial BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, H, W)`\n        - Output: :math:`(N, C, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm2d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 4:\n            raise ValueError(\'expected 4D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm2d, self)._check_input_dim(input)\n\n\nclass SynchronizedBatchNorm3d(_SynchronizedBatchNorm):\n    r""""""Applies Batch Normalization over a 5d input that is seen as a mini-batch\n    of 4d inputs\n\n    .. math::\n\n        y = \\frac{x - mean[x]}{ \\sqrt{Var[x] + \\epsilon}} * gamma + beta\n\n    This module differs from the built-in PyTorch BatchNorm3d as the mean and\n    standard-deviation are reduced across all devices during training.\n\n    For example, when one uses `nn.DataParallel` to wrap the network during\n    training, PyTorch\'s implementation normalize the tensor on each device using\n    the statistics only on that device, which accelerated the computation and\n    is also easy to implement, but the statistics might be inaccurate.\n    Instead, in this synchronized version, the statistics will be computed\n    over all training samples distributed on multiple devices.\n\n    Note that, for one-GPU or CPU-only case, this module behaves exactly same\n    as the built-in PyTorch implementation.\n\n    The mean and standard-deviation are calculated per-dimension over\n    the mini-batches and gamma and beta are learnable parameter vectors\n    of size C (where C is the input size).\n\n    During training, this layer keeps a running estimate of its computed mean\n    and variance. The running sum is kept with a default momentum of 0.1.\n\n    During evaluation, this running mean/variance is used for normalization.\n\n    Because the BatchNorm is done over the `C` dimension, computing statistics\n    on `(N, D, H, W)` slices, it\'s common terminology to call this Volumetric BatchNorm\n    or Spatio-temporal BatchNorm\n\n    Args:\n        num_features: num_features from an expected input of\n            size batch_size x num_features x depth x height x width\n        eps: a value added to the denominator for numerical stability.\n            Default: 1e-5\n        momentum: the value used for the running_mean and running_var\n            computation. Default: 0.1\n        affine: a boolean value that when set to ``True``, gives the layer learnable\n            affine parameters. Default: ``True``\n\n    Shape::\n        - Input: :math:`(N, C, D, H, W)`\n        - Output: :math:`(N, C, D, H, W)` (same shape as input)\n\n    Examples:\n        >>> # With Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100)\n        >>> # Without Learnable Parameters\n        >>> m = SynchronizedBatchNorm3d(100, affine=False)\n        >>> input = torch.autograd.Variable(torch.randn(20, 100, 35, 45, 10))\n        >>> output = m(input)\n    """"""\n\n    def _check_input_dim(self, input):\n        if input.dim() != 5:\n            raise ValueError(\'expected 5D input (got {}D input)\'\n                             .format(input.dim()))\n        super(SynchronizedBatchNorm3d, self)._check_input_dim(input)\n\n\n@contextlib.contextmanager\ndef patch_sync_batchnorm():\n    import torch.nn as nn\n\n    backup = nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d\n\n    nn.BatchNorm1d = SynchronizedBatchNorm1d\n    nn.BatchNorm2d = SynchronizedBatchNorm2d\n    nn.BatchNorm3d = SynchronizedBatchNorm3d\n\n    yield\n\n    nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d = backup\n\n\ndef convert_model(module):\n    """"""Traverse the input module and its child recursively\n       and replace all instance of torch.nn.modules.batchnorm.BatchNorm*N*d\n       to SynchronizedBatchNorm*N*d\n\n    Args:\n        module: the input module needs to be convert to SyncBN model\n\n    Examples:\n        >>> import torch.nn as nn\n        >>> import torchvision\n        >>> # m is a standard pytorch model\n        >>> m = torchvision.models.resnet18(True)\n        >>> m = nn.DataParallel(m)\n        >>> # after convert, m is using SyncBN\n        >>> m = convert_model(m)\n    """"""\n    if isinstance(module, torch.nn.DataParallel):\n        mod = module.module\n        mod = convert_model(mod)\n        mod = DataParallelWithCallback(mod)\n        return mod\n\n    mod = module\n    for pth_module, sync_module in zip([torch.nn.modules.batchnorm.BatchNorm1d,\n                                        torch.nn.modules.batchnorm.BatchNorm2d,\n                                        torch.nn.modules.batchnorm.BatchNorm3d],\n                                       [SynchronizedBatchNorm1d,\n                                        SynchronizedBatchNorm2d,\n                                        SynchronizedBatchNorm3d]):\n        if isinstance(module, pth_module):\n            mod = sync_module(module.num_features, module.eps, module.momentum, module.affine)\n            mod.running_mean = module.running_mean\n            mod.running_var = module.running_var\n            if module.affine:\n                mod.weight.data = module.weight.data.clone().detach()\n                mod.bias.data = module.bias.data.clone().detach()\n\n    for name, child in module.named_children():\n        mod.add_module(name, convert_model(child))\n\n    return mod\n'"
models/modules/sync_batchnorm/batchnorm_reimpl.py,6,"b'#! /usr/bin/env python3\n# -*- coding: utf-8 -*-\n# File   : batchnorm_reimpl.py\n# Author : acgtyrant\n# Date   : 11/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n__all__ = [\'BatchNorm2dReimpl\']\n\n\nclass BatchNorm2dReimpl(nn.Module):\n    """"""\n    A re-implementation of batch normalization, used for testing the numerical\n    stability.\n\n    Author: acgtyrant\n    See also:\n    https://github.com/vacancy/Synchronized-BatchNorm-PyTorch/issues/14\n    """"""\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n        super().__init__()\n\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.weight = nn.Parameter(torch.empty(num_features))\n        self.bias = nn.Parameter(torch.empty(num_features))\n        self.register_buffer(\'running_mean\', torch.zeros(num_features))\n        self.register_buffer(\'running_var\', torch.ones(num_features))\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        self.running_mean.zero_()\n        self.running_var.fill_(1)\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        init.uniform_(self.weight)\n        init.zeros_(self.bias)\n\n    def forward(self, input_):\n        batchsize, channels, height, width = input_.size()\n        numel = batchsize * height * width\n        input_ = input_.permute(1, 0, 2, 3).contiguous().view(channels, numel)\n        sum_ = input_.sum(1)\n        sum_of_square = input_.pow(2).sum(1)\n        mean = sum_ / numel\n        sumvar = sum_of_square - sum_ * mean\n\n        self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * mean.detach()\n        unbias_var = sumvar / (numel - 1)\n        self.running_var = (1 - self.momentum) * self.running_var + self.momentum * unbias_var.detach()\n\n        bias_var = sumvar / numel\n        inv_std = 1 / (bias_var + self.eps).pow(0.5)\n        output = (input_ - mean.unsqueeze(1)) * inv_std.unsqueeze(1) * self.weight.unsqueeze(1) + self.bias.unsqueeze(1)\n\n        return output.view(channels, batchsize, height, width).permute(1, 0, 2, 3).contiguous()\n'"
models/modules/sync_batchnorm/comm.py,0,"b'# -*- coding: utf-8 -*-\n# File   : comm.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport collections\nimport queue\nimport threading\n\n__all__ = [\'FutureResult\', \'SlavePipe\', \'SyncMaster\']\n\n\nclass FutureResult(object):\n    """"""A thread-safe future implementation. Used only as one-to-one pipe.""""""\n\n    def __init__(self):\n        self._result = None\n        self._lock = threading.Lock()\n        self._cond = threading.Condition(self._lock)\n\n    def put(self, result):\n        with self._lock:\n            assert self._result is None, \'Previous result has\\\'t been fetched.\'\n            self._result = result\n            self._cond.notify()\n\n    def get(self):\n        with self._lock:\n            if self._result is None:\n                self._cond.wait()\n\n            res = self._result\n            self._result = None\n            return res\n\n\n_MasterRegistry = collections.namedtuple(\'MasterRegistry\', [\'result\'])\n_SlavePipeBase = collections.namedtuple(\'_SlavePipeBase\', [\'identifier\', \'queue\', \'result\'])\n\n\nclass SlavePipe(_SlavePipeBase):\n    """"""Pipe for master-slave communication.""""""\n\n    def run_slave(self, msg):\n        self.queue.put((self.identifier, msg))\n        ret = self.result.get()\n        self.queue.put(True)\n        return ret\n\n\nclass SyncMaster(object):\n    """"""An abstract `SyncMaster` object.\n\n    - During the replication, as the data parallel will trigger an callback of each module, all slave devices should\n    call `register(id)` and obtain an `SlavePipe` to communicate with the master.\n    - During the forward pass, master device invokes `run_master`, all messages from slave devices will be collected,\n    and passed to a registered callback.\n    - After receiving the messages, the master device should gather the information and determine to message passed\n    back to each slave devices.\n    """"""\n\n    def __init__(self, master_callback):\n        """"""\n\n        Args:\n            master_callback: a callback to be invoked after having collected messages from slave devices.\n        """"""\n        self._master_callback = master_callback\n        self._queue = queue.Queue()\n        self._registry = collections.OrderedDict()\n        self._activated = False\n\n    def __getstate__(self):\n        return {\'master_callback\': self._master_callback}\n\n    def __setstate__(self, state):\n        self.__init__(state[\'master_callback\'])\n\n    def register_slave(self, identifier):\n        """"""\n        Register an slave device.\n\n        Args:\n            identifier: an identifier, usually is the device id.\n\n        Returns: a `SlavePipe` object which can be used to communicate with the master device.\n\n        """"""\n        if self._activated:\n            assert self._queue.empty(), \'Queue is not clean before next initialization.\'\n            self._activated = False\n            self._registry.clear()\n        future = FutureResult()\n        self._registry[identifier] = _MasterRegistry(future)\n        return SlavePipe(identifier, self._queue, future)\n\n    def run_master(self, master_msg):\n        """"""\n        Main entry for the master device in each forward pass.\n        The messages were first collected from each devices (including the master device), and then\n        an callback will be invoked to compute the message to be sent back to each devices\n        (including the master device).\n\n        Args:\n            master_msg: the message that the master want to send to itself. This will be placed as the first\n            message when calling `master_callback`. For detailed usage, see `_SynchronizedBatchNorm` for an example.\n\n        Returns: the message to be sent back to the master device.\n\n        """"""\n        self._activated = True\n\n        intermediates = [(0, master_msg)]\n        for i in range(self.nr_slaves):\n            intermediates.append(self._queue.get())\n\n        results = self._master_callback(intermediates)\n        assert results[0][0] == 0, \'The first result should belongs to the master.\'\n\n        for i, res in results:\n            if i == 0:\n                continue\n            self._registry[i].result.put(res)\n\n        for i in range(self.nr_slaves):\n            assert self._queue.get() is True\n\n        return results[0][1]\n\n    @property\n    def nr_slaves(self):\n        return len(self._registry)\n'"
models/modules/sync_batchnorm/replicate.py,1,"b'# -*- coding: utf-8 -*-\n# File   : replicate.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport functools\n\nfrom torch.nn.parallel.data_parallel import DataParallel\n\n__all__ = [\n    \'CallbackContext\',\n    \'execute_replication_callbacks\',\n    \'DataParallelWithCallback\',\n    \'patch_replication_callback\'\n]\n\n\nclass CallbackContext(object):\n    pass\n\n\ndef execute_replication_callbacks(modules):\n    """"""\n    Execute an replication callback `__data_parallel_replicate__` on each module created by original replication.\n\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Note that, as all modules are isomorphism, we assign each sub-module with a context\n    (shared among multiple copies of this module on different devices).\n    Through this context, different copies can share some information.\n\n    We guarantee that the callback on the master copy (the first copy) will be called ahead of calling the callback\n    of any slave copies.\n    """"""\n    master_copy = modules[0]\n    nr_modules = len(list(master_copy.modules()))\n    ctxs = [CallbackContext() for _ in range(nr_modules)]\n\n    for i, module in enumerate(modules):\n        for j, m in enumerate(module.modules()):\n            if hasattr(m, \'__data_parallel_replicate__\'):\n                m.__data_parallel_replicate__(ctxs[j], i)\n\n\nclass DataParallelWithCallback(DataParallel):\n    """"""\n    Data Parallel with a replication callback.\n\n    An replication callback `__data_parallel_replicate__` of each module will be invoked after being created by\n    original `replicate` function.\n    The callback will be invoked with arguments `__data_parallel_replicate__(ctx, copy_id)`\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n        # sync_bn.__data_parallel_replicate__ will be invoked.\n    """"""\n\n    def replicate(self, module, device_ids):\n        modules = super(DataParallelWithCallback, self).replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n\ndef patch_replication_callback(data_parallel):\n    """"""\n    Monkey-patch an existing `DataParallel` object. Add the replication callback.\n    Useful when you have customized `DataParallel` implementation.\n\n    Examples:\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallel(sync_bn, device_ids=[0, 1])\n        > patch_replication_callback(sync_bn)\n        # this is equivalent to\n        > sync_bn = SynchronizedBatchNorm1d(10, eps=1e-5, affine=False)\n        > sync_bn = DataParallelWithCallback(sync_bn, device_ids=[0, 1])\n    """"""\n\n    assert isinstance(data_parallel, DataParallel)\n\n    old_replicate = data_parallel.replicate\n\n    @functools.wraps(old_replicate)\n    def new_replicate(module, device_ids):\n        modules = old_replicate(module, device_ids)\n        execute_replication_callbacks(modules)\n        return modules\n\n    data_parallel.replicate = new_replicate\n'"
models/modules/sync_batchnorm/unittest.py,1,"b""# -*- coding: utf-8 -*-\n# File   : unittest.py\n# Author : Jiayuan Mao\n# Email  : maojiayuan@gmail.com\n# Date   : 27/01/2018\n#\n# This file is part of Synchronized-BatchNorm-PyTorch.\n# https://github.com/vacancy/Synchronized-BatchNorm-PyTorch\n# Distributed under MIT License.\n\nimport unittest\n\nimport torch\n\n\nclass TorchTestCase(unittest.TestCase):\n    def assertTensorClose(self, x, y):\n        adiff = float((x - y).abs().max())\n        if (y == 0).all():\n            rdiff = 'NaN'\n        else:\n            rdiff = float((adiff / y).abs().max())\n\n        message = (\n            'Tensor close check failed\\n'\n            'adiff={}\\n'\n            'rdiff={}\\n'\n        ).format(adiff, rdiff)\n        self.assertTrue(torch.allclose(x, y), message)\n"""
