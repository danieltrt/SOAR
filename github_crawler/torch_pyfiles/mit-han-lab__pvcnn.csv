file_path,api_count,code
train.py,8,"b'import argparse\nimport os\nimport random\nimport shutil\n\n\ndef prepare():\n    from utils.common import get_save_path\n    from utils.config import configs\n    from utils.device import set_cuda_visible_devices\n\n    # since PyTorch jams device selection, we have to parse args before import torch (issue #26790)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'configs\', nargs=\'+\')\n    parser.add_argument(\'--devices\', default=None)\n    parser.add_argument(\'--evaluate\', default=False, action=\'store_true\')\n    args, opts = parser.parse_known_args()\n    if args.devices is not None and args.devices != \'cpu\':\n        gpus = set_cuda_visible_devices(args.devices)\n    else:\n        gpus = []\n\n    print(f\'==> loading configs from {args.configs}\')\n    configs.update_from_modules(*args.configs)\n    # define save path\n    configs.train.save_path = get_save_path(*args.configs, prefix=\'runs\')\n\n    # override configs with args\n    configs.update_from_arguments(*opts)\n    if len(gpus) == 0:\n        configs.device = \'cpu\'\n        configs.device_ids = []\n    else:\n        configs.device = \'cuda\'\n        configs.device_ids = gpus\n    if args.evaluate and configs.evaluate.fn is not None:\n        if \'dataset\' in configs.evaluate:\n            for k, v in configs.evaluate.dataset.items():\n                configs.dataset[k] = v\n    else:\n        configs.evaluate = None\n\n    if configs.evaluate is None:\n        metrics = []\n        if \'metric\' in configs.train and configs.train.metric is not None:\n            metrics.append(configs.train.metric)\n        if \'metrics\' in configs.train and configs.train.metrics is not None:\n            for m in configs.train.metrics:\n                if m not in metrics:\n                    metrics.append(m)\n        configs.train.metrics = metrics\n        configs.train.metric = None if len(metrics) == 0 else metrics[0]\n\n        save_path = configs.train.save_path\n        configs.train.checkpoint_path = os.path.join(save_path, \'latest.pth.tar\')\n        configs.train.checkpoints_path = os.path.join(save_path, \'latest\', \'e{}.pth.tar\')\n        configs.train.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n        best_checkpoints_dir = os.path.join(save_path, \'best\')\n        configs.train.best_checkpoint_paths = {\n            m: os.path.join(best_checkpoints_dir, \'best.{}.pth.tar\'.format(m.replace(\'/\', \'.\')))\n            for m in configs.train.metrics\n        }\n        os.makedirs(os.path.dirname(configs.train.checkpoints_path), exist_ok=True)\n        os.makedirs(best_checkpoints_dir, exist_ok=True)\n    else:\n        if \'best_checkpoint_path\' not in configs.evaluate or configs.evaluate.best_checkpoint_path is None:\n            if \'best_checkpoint_path\' in configs.train and configs.train.best_checkpoint_path is not None:\n                configs.evaluate.best_checkpoint_path = configs.train.best_checkpoint_path\n            else:\n                configs.evaluate.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n        assert configs.evaluate.best_checkpoint_path.endswith(\'.pth.tar\')\n        configs.evaluate.predictions_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.predictions\')\n        configs.evaluate.stats_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.eval.npy\')\n\n    return configs\n\n\ndef main():\n    configs = prepare()\n    if configs.evaluate is not None:\n        configs.evaluate.fn(configs)\n        return\n\n    import numpy as np\n    import tensorboardX\n    import torch\n    import torch.backends.cudnn as cudnn\n    from torch.utils.data import DataLoader\n    from tqdm import tqdm\n\n    ################################\n    # Train / Eval Kernel Function #\n    ################################\n\n    # train kernel\n    def train(model, loader, criterion, optimizer, scheduler, current_step, writer):\n        model.train()\n        for inputs, targets in tqdm(loader, desc=\'train\', ncols=0):\n            if isinstance(inputs, dict):\n                for k, v in inputs.items():\n                    batch_size = v.size(0)\n                    inputs[k] = v.to(configs.device, non_blocking=True)\n            else:\n                batch_size = inputs.size(0)\n                inputs = inputs.to(configs.device, non_blocking=True)\n            if isinstance(targets, dict):\n                for k, v in targets.items():\n                    targets[k] = v.to(configs.device, non_blocking=True)\n            else:\n                targets = targets.to(configs.device, non_blocking=True)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            writer.add_scalar(\'loss/train\', loss.item(), current_step)\n            current_step += batch_size\n            loss.backward()\n            optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n    # evaluate kernel\n    def evaluate(model, loader, split=\'test\'):\n        meters = {}\n        for k, meter in configs.train.meters.items():\n            meters[k.format(split)] = meter()\n        model.eval()\n        with torch.no_grad():\n            for inputs, targets in tqdm(loader, desc=split, ncols=0):\n                if isinstance(inputs, dict):\n                    for k, v in inputs.items():\n                        inputs[k] = v.to(configs.device, non_blocking=True)\n                else:\n                    inputs = inputs.to(configs.device, non_blocking=True)\n                if isinstance(targets, dict):\n                    for k, v in targets.items():\n                        targets[k] = v.to(configs.device, non_blocking=True)\n                else:\n                    targets = targets.to(configs.device, non_blocking=True)\n                outputs = model(inputs)\n                for meter in meters.values():\n                    meter.update(outputs, targets)\n        for k, meter in meters.items():\n            meters[k] = meter.compute()\n        return meters\n\n    ###########\n    # Prepare #\n    ###########\n\n    if configs.device == \'cuda\':\n        cudnn.benchmark = True\n        if configs.get(\'deterministic\', False):\n            cudnn.deterministic = True\n            cudnn.benchmark = False\n    if (\'seed\' not in configs) or (configs.seed is None):\n        configs.seed = torch.initial_seed() % (2 ** 32 - 1)\n    seed = configs.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    print(configs)\n\n    #####################################################################\n    # Initialize DataLoaders, Model, Criterion, LRScheduler & Optimizer #\n    #####################################################################\n\n    print(f\'\\n==> loading dataset ""{configs.dataset}""\')\n    dataset = configs.dataset()\n    loaders = {}\n    for split in dataset:\n        loaders[split] = DataLoader(\n            dataset[split], shuffle=(split == \'train\'), batch_size=configs.train.batch_size,\n            num_workers=configs.data.num_workers, pin_memory=True,\n            worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n        )\n\n    print(f\'\\n==> creating model ""{configs.model}""\')\n    model = configs.model()\n    if configs.device == \'cuda\':\n        model = torch.nn.DataParallel(model)\n    model = model.to(configs.device)\n    criterion = configs.train.criterion().to(configs.device)\n    optimizer = configs.train.optimizer(model.parameters())\n\n    last_epoch, best_metrics = -1, {m: None for m in configs.train.metrics}\n    if os.path.exists(configs.train.checkpoint_path):\n        print(f\'==> loading checkpoint ""{configs.train.checkpoint_path}""\')\n        checkpoint = torch.load(configs.train.checkpoint_path)\n        print(\' => loading model\')\n        model.load_state_dict(checkpoint.pop(\'model\'))\n        if \'optimizer\' in checkpoint and checkpoint[\'optimizer\'] is not None:\n            print(\' => loading optimizer\')\n            optimizer.load_state_dict(checkpoint.pop(\'optimizer\'))\n        last_epoch = checkpoint.get(\'epoch\', last_epoch)\n        meters = checkpoint.get(\'meters\', {})\n        for m in configs.train.metrics:\n            best_metrics[m] = meters.get(m + \'_best\', best_metrics[m])\n        del checkpoint\n\n    if \'scheduler\' in configs.train and configs.train.scheduler is not None:\n        configs.train.scheduler.last_epoch = last_epoch\n        print(f\'==> creating scheduler ""{configs.train.scheduler}""\')\n        scheduler = configs.train.scheduler(optimizer)\n    else:\n        scheduler = None\n\n    ############\n    # Training #\n    ############\n\n    if last_epoch >= configs.train.num_epochs:\n        meters = dict()\n        for split, loader in loaders.items():\n            if split != \'train\':\n                meters.update(evaluate(model, loader=loader, split=split))\n        for k, meter in meters.items():\n            print(f\'[{k}] = {meter:2f}\')\n        return\n\n    with tensorboardX.SummaryWriter(configs.train.save_path) as writer:\n        for current_epoch in range(last_epoch + 1, configs.train.num_epochs):\n            current_step = current_epoch * len(dataset[\'train\'])\n\n            # train\n            print(f\'\\n==> training epoch {current_epoch}/{configs.train.num_epochs}\')\n            train(model, loader=loaders[\'train\'], criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n                  current_step=current_step, writer=writer)\n            current_step += len(dataset[\'train\'])\n\n            # evaluate\n            meters = dict()\n            for split, loader in loaders.items():\n                if split != \'train\':\n                    meters.update(evaluate(model, loader=loader, split=split))\n\n            # check whether it is the best\n            best = {m: False for m in configs.train.metrics}\n            for m in configs.train.metrics:\n                if best_metrics[m] is None or best_metrics[m] < meters[m]:\n                    best_metrics[m], best[m] = meters[m], True\n                meters[m + \'_best\'] = best_metrics[m]\n            # log in tensorboard\n            for k, meter in meters.items():\n                print(f\'[{k}] = {meter:2f}\')\n                writer.add_scalar(k, meter, current_step)\n\n            # save checkpoint\n            torch.save({\n                \'epoch\': current_epoch,\n                \'model\': model.state_dict(),\n                \'optimizer\': optimizer.state_dict(),\n                \'meters\': meters,\n                \'configs\': configs,\n            }, configs.train.checkpoint_path)\n            shutil.copyfile(configs.train.checkpoint_path, configs.train.checkpoints_path.format(current_epoch))\n            for m in configs.train.metrics:\n                if best[m]:\n                    shutil.copyfile(configs.train.checkpoint_path, configs.train.best_checkpoint_paths[m])\n            if best.get(configs.train.metric, False):\n                shutil.copyfile(configs.train.checkpoint_path, configs.train.best_checkpoint_path)\n            print(f\'[save_path] = {configs.train.save_path}\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
train_dml.py,9,"b'import argparse\nimport os\nimport random\nimport shutil\n\n\ndef prepare():\n    from utils.common import get_save_path\n    from utils.config import configs\n    from utils.device import set_cuda_visible_devices\n\n    # since PyTorch jams device selection, we have to parse args before import torch (issue #26790)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'configs\', nargs=\'+\')\n    parser.add_argument(\'--devices\', default=None)\n    parser.add_argument(\'--evaluate\', default=False, action=\'store_true\')\n    args, opts = parser.parse_known_args()\n    if args.devices is not None and args.devices != \'cpu\':\n        gpus = set_cuda_visible_devices(args.devices)\n    else:\n        gpus = []\n\n    print(f\'==> loading configs from {args.configs}\')\n    configs.update_from_modules(*args.configs)\n    # define save path\n    configs.train.save_path = get_save_path(*args.configs, prefix=\'runs\')\n\n    # override configs with args\n    configs.update_from_arguments(*opts)\n    if len(gpus) == 0:\n        configs.device = \'cpu\'\n        configs.device_ids = []\n    else:\n        configs.device = \'cuda\'\n        configs.device_ids = gpus\n    if args.evaluate and configs.evaluate.fn is not None:\n        if \'dataset\' in configs.evaluate:\n            for k, v in configs.evaluate.dataset.items():\n                configs.dataset[k] = v\n    else:\n        configs.evaluate = None\n\n    if configs.evaluate is None:\n        configs.train.deep_mutual_learning = configs.train.get(\'deep_mutual_learning\', True)\n        metrics = []\n        if \'metric\' in configs.train and configs.train.metric is not None:\n            metrics.append(configs.train.metric)\n        if \'metrics\' in configs.train and configs.train.metrics is not None:\n            for m in configs.train.metrics:\n                if m not in metrics:\n                    metrics.append(m)\n        configs.train.metrics = metrics\n        configs.train.metric = None if len(metrics) == 0 else metrics[0]\n\n        save_path = configs.train.save_path\n        configs.train.checkpoint_path = os.path.join(save_path, \'latest.pth.tar\')\n        configs.train.checkpoints_path = os.path.join(save_path, \'latest\', \'e{}.pth.tar\')\n        configs.train.best_checkpoint_path = os.path.join(save_path, \'best.pth.tar\')\n        best_checkpoints_dir = os.path.join(save_path, \'best\')\n        configs.train.best_checkpoint_paths = {\n            m: os.path.join(best_checkpoints_dir, \'best.{}.pth.tar\'.format(m.replace(\'/\', \'.\')))\n            for m in configs.train.metrics\n        }\n        os.makedirs(os.path.dirname(configs.train.checkpoints_path), exist_ok=True)\n        os.makedirs(best_checkpoints_dir, exist_ok=True)\n        if configs.train.deep_mutual_learning:\n            configs.train.best_student_checkpoint_path = os.path.join(save_path, \'best_student.pth.tar\')\n            best_student_checkpoints_dir = os.path.join(save_path, \'best_student\')\n            configs.train.best_student_checkpoint_paths = {\n                m: os.path.join(best_student_checkpoints_dir, \'best.{}.pth.tar\'.format(m.replace(\'/\', \'.\')))\n                for m in configs.train.metrics\n            }\n            os.makedirs(best_student_checkpoints_dir, exist_ok=True)\n    else:\n        if \'best_checkpoint_path\' not in configs.evaluate or configs.evaluate.best_checkpoint_path is None:\n            if \'best_checkpoint_path\' in configs.train and configs.train.best_checkpoint_path is not None:\n                configs.evaluate.best_checkpoint_path = configs.train.best_checkpoint_path\n            else:\n                configs.evaluate.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n        assert configs.evaluate.best_checkpoint_path.endswith(\'.pth.tar\')\n        configs.evaluate.predictions_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.predictions\')\n        configs.evaluate.stats_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.eval.npy\')\n\n    return configs\n\n\ndef main():\n    configs = prepare()\n    if configs.evaluate is not None:\n        configs.evaluate.fn(configs)\n        return\n\n    import numpy as np\n    import tensorboardX\n    import torch\n    import torch.backends.cudnn as cudnn\n    from torch.utils.data import DataLoader\n    from tqdm import tqdm\n\n    from modules import KLLoss\n\n    ################################\n    # Train / Eval Kernel Function #\n    ################################\n\n    # train kernel\n    def train(model, loader, criterion, optimizer, scheduler, current_step, writer, schedule_per_epoch=True,\n              model_student=None, criterion_dml=None, optimizer_student=None):\n        model.train()\n        if model_student is not None:\n            model_student.train()\n        for inputs, targets in tqdm(loader, desc=\'train\', ncols=0):\n            inputs = inputs.to(configs.device, non_blocking=True)\n            targets = targets.to(configs.device, non_blocking=True)\n            if model_student is None:\n                optimizer.zero_grad()\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                writer.add_scalar(\'loss/train\', loss.item(), current_step)\n                current_step += targets.size(0)\n                loss.backward()\n                optimizer.step()\n            else:\n                # deep mutual learning\n                optimizer.zero_grad()\n                optimizer_student.zero_grad()\n                outputs = model(inputs)\n                outputs_student = model_student(inputs)\n                loss = criterion(outputs, targets) + criterion_dml(outputs_student, outputs)\n                loss_student = criterion(outputs_student, targets) + criterion_dml(outputs, outputs_student)\n                writer.add_scalar(\'loss/train\', loss.item(), current_step)\n                writer.add_scalar(\'loss/train_student\', loss_student.item(), current_step)\n                current_step += targets.size(0)\n                loss.backward()\n                optimizer.step()\n                loss_student.backward()\n                optimizer_student.step()\n            if not schedule_per_epoch and scheduler is not None:\n                scheduler.step()\n        if schedule_per_epoch and scheduler is not None:\n            scheduler.step()\n\n    # evaluate kernel\n    def evaluate(model, loader, split=\'test\'):\n        meters = {}\n        for k, meter in configs.train.meters.items():\n            meters[k.format(split)] = meter()\n        model.eval()\n        with torch.no_grad():\n            for inputs, targets in tqdm(loader, desc=split, ncols=0):\n                inputs = inputs.to(configs.device, non_blocking=True)\n                targets = targets.to(configs.device, non_blocking=True)\n                outputs = model(inputs)\n                for meter in meters.values():\n                    meter.update(outputs, targets)\n        for k, meter in meters.items():\n            meters[k] = meter.compute()\n        return meters\n\n    ###########\n    # Prepare #\n    ###########\n\n    if configs.device == \'cuda\':\n        cudnn.benchmark = True\n        if configs.get(\'deterministic\', False):\n            cudnn.deterministic = True\n            cudnn.benchmark = False\n    if (\'seed\' not in configs) or (configs.seed is None):\n        configs.seed = torch.initial_seed() % (2 ** 32 - 1)\n    seed = configs.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    print(configs)\n\n    #####################################################################\n    # Initialize DataLoaders, Model, Criterion, LRScheduler & Optimizer #\n    #####################################################################\n\n    print(f\'\\n==> loading dataset ""{configs.dataset}""\')\n    dataset = configs.dataset()\n    loaders = {}\n    for split in dataset:\n        loaders[split] = DataLoader(\n            dataset[split], shuffle=(split == \'train\'), batch_size=configs.train.batch_size,\n            num_workers=configs.data.num_workers, pin_memory=True,\n            worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n        )\n\n    print(f\'\\n==> creating model ""{configs.model}""\')\n    model = configs.model()\n    if configs.device == \'cuda\':\n        model = torch.nn.DataParallel(model)\n    model = model.to(configs.device)\n    criterion = configs.train.criterion().to(configs.device)\n    optimizer = configs.train.optimizer(model.parameters())\n    if configs.train.deep_mutual_learning:\n        model_student = configs.model()\n        if configs.device == \'cuda\':\n            model_student = torch.nn.DataParallel(model_student)\n        model_student = model_student.to(configs.device)\n        optimizer_student = configs.train.optimizer(model_student.parameters())\n        criterion_dml = KLLoss()\n    else:\n        model_student, optimizer_student, criterion_dml = None, None, None\n\n    last_epoch = -1\n    best_metrics = {m: None for m in configs.train.metrics}\n    best_metrics_student = {m: None for m in configs.train.metrics}\n    if os.path.exists(configs.train.checkpoint_path):\n        print(f\'==> loading checkpoint ""{configs.train.checkpoint_path}""\')\n        checkpoint = torch.load(configs.train.checkpoint_path)\n        print(\' => loading model\')\n        model.load_state_dict(checkpoint.pop(\'model\'))\n        if \'optimizer\' in checkpoint and checkpoint[\'optimizer\'] is not None:\n            print(\' => loading optimizer\')\n            optimizer.load_state_dict(checkpoint.pop(\'optimizer\'))\n        last_epoch = checkpoint.get(\'epoch\', last_epoch)\n        meters = checkpoint.get(\'meters\', {})\n        for m in configs.train.metrics:\n            best_metrics[m] = meters.get(m + \'_best\', best_metrics[m])\n\n        if configs.train.deep_mutual_learning:\n            if \'model_student\' in checkpoint and checkpoint[\'model_student\'] is not None:\n                print(\' => loading model_student\')\n                model_student.load_state_dict(checkpoint.pop(\'model_student\'))\n            if \'optimizer_student\' in checkpoint and checkpoint[\'optimizer_student\'] is not None:\n                print(\' => loading optimizer_student\')\n                optimizer_student.load_state_dict(checkpoint.pop(\'optimizer_student\'))\n            meters_student = checkpoint.get(\'meters_student\', {})\n            for m in configs.train.metrics:\n                best_metrics_student[m] = meters_student.get(m + \'_best\', best_metrics_student[m])\n\n    if \'scheduler\' in configs.train and configs.train.scheduler is not None:\n        scheduler_unit = configs.train.get(\'scheduler_unit\', \'epoch\')\n        schedule_per_epoch = (scheduler_unit is None) or (scheduler_unit == \'epoch\')\n        configs.train.scheduler_unit = \'epoch\' if schedule_per_epoch else \'iter\'\n        last_unit = last_epoch\n        if not schedule_per_epoch:\n            last_unit = (last_epoch + 1) * len(loaders[\'train\']) - 1\n        if \'T_max\' in configs.train.scheduler:\n            configs.train.scheduler.T_max = configs.train.num_epochs\n            if not schedule_per_epoch:\n                configs.train.scheduler.T_max *= len(loaders[\'train\'])\n            print(f\'==> modify scheduler T_max to {configs.train.scheduler.T_max} {configs.train.scheduler_unit}s\')\n        print(f\'==> creating scheduler ""{configs.train.scheduler}"" from {last_unit} {configs.train.scheduler_unit}\')\n        scheduler = configs.train.scheduler(optimizer, last_epoch=last_unit)\n    else:\n        scheduler = None\n        schedule_per_epoch = True\n\n    ############\n    # Training #\n    ############\n\n    if last_epoch >= configs.train.num_epochs:\n        meters, meters_student = dict(), dict()\n        for split, loader in loaders.items():\n            if split != \'train\':\n                meters.update(evaluate(model, loader=loader, split=split))\n                if configs.train.deep_mutual_learning:\n                    meters_student.update(evaluate(model_student, loader=loader, split=split))\n        for k, meter in meters.items():\n            print(f\'[{k}] = {meter:2f}\')\n        for k, meter in meters_student.items():\n            print(f\'[{k}_student] = {meter:2f}\')\n        return\n\n    with tensorboardX.SummaryWriter(configs.train.save_path) as writer:\n        for current_epoch in range(last_epoch + 1, configs.train.num_epochs):\n            current_step = current_epoch * len(dataset[\'train\'])\n\n            # train\n            print(f\'\\n==> training epoch {current_epoch}/{configs.train.num_epochs}\')\n            train(model, loader=loaders[\'train\'], criterion=criterion, optimizer=optimizer, scheduler=scheduler,\n                  current_step=current_step, writer=writer, schedule_per_epoch=schedule_per_epoch,\n                  model_student=model_student, criterion_dml=criterion_dml, optimizer_student=optimizer_student)\n            current_step += len(dataset[\'train\'])\n\n            # evaluate\n            meters, meters_student = dict(), dict()\n            for split, loader in loaders.items():\n                if split != \'train\':\n                    meters.update(evaluate(model, loader=loader, split=split))\n                    if configs.train.deep_mutual_learning:\n                        meters_student.update(evaluate(model_student, loader=loader, split=split))\n\n            # check whether it is the best\n            best, best_student = {m: False for m in configs.train.metrics}, {m: False for m in configs.train.metrics}\n            for m in configs.train.metrics:\n                if best_metrics[m] is None or best_metrics[m] < meters[m]:\n                    best_metrics[m], best[m] = meters[m], True\n                meters[m + \'_best\'] = best_metrics[m]\n                if configs.train.deep_mutual_learning:\n                    if best_metrics_student[m] is None or best_metrics_student[m] < meters_student[m]:\n                        best_metrics_student[m], best_student[m] = meters_student[m], True\n                    meters_student[m + \'_best\'] = best_metrics_student[m]\n            # log in tensorboard\n            for k, meter in meters.items():\n                print(f\'[{k}] = {meter:2f}\')\n                writer.add_scalar(k, meter, current_step)\n            for k, meter in meters_student.items():\n                print(f\'[{k}_student] = {meter:2f}\')\n                writer.add_scalar(k + \'_student\', meter, current_step)\n\n            # save checkpoint\n            torch.save({\n                \'epoch\': current_epoch,\n                \'model\': model.state_dict(),\n                \'model_student\': model_student.state_dict() if configs.train.deep_mutual_learning else None,\n                \'optimizer\': optimizer.state_dict(),\n                \'optimizer_student\': optimizer.state_dict() if configs.train.deep_mutual_learning else None,\n                \'meters\': meters,\n                \'meters_student\': meters_student,\n                \'configs\': configs,\n            }, configs.train.checkpoint_path)\n            shutil.copyfile(configs.train.checkpoint_path, configs.train.checkpoints_path.format(current_epoch))\n            for m in configs.train.metrics:\n                if best[m]:\n                    shutil.copyfile(configs.train.checkpoint_path, configs.train.best_checkpoint_paths[m])\n                if best_student[m]:\n                    shutil.copyfile(configs.train.checkpoint_path, configs.train.best_student_checkpoint_paths[m])\n            if best.get(configs.train.metric, False):\n                shutil.copyfile(configs.train.checkpoint_path, configs.train.best_checkpoint_path)\n            if best_student.get(configs.train.metric, False):\n                shutil.copyfile(configs.train.checkpoint_path, configs.train.best_student_checkpoint_path)\n            print(f\'[save_path] = {configs.train.save_path}\')\n\n\nif __name__ == \'__main__\':\n    main()\n'"
configs/__init__.py,0,"b'from utils.config import Config, configs\n\nconfigs.seed = 1588147245\nconfigs.deterministic = False\n\n# data configs\nconfigs.data = Config()\nconfigs.data.num_workers = 16\n'"
datasets/__init__.py,0,b'from datasets.s3dis import S3DIS\n'
datasets/s3dis.py,1,"b'import os\n\nimport h5py\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n__all__ = [\'S3DIS\']\n\n\nclass _S3DISDataset(Dataset):\n    def __init__(self, root, num_points, split=\'train\', with_normalized_coords=True, holdout_area=5):\n        """"""\n        :param root: directory path to the s3dis dataset\n        :param num_points: number of points to process for each scene\n        :param split: \'train\' or \'test\'\n        :param with_normalized_coords: whether include the normalized coords in features (default: True)\n        :param holdout_area: which area to holdout (default: 5)\n        """"""\n        assert split in [\'train\', \'test\']\n        self.root = root\n        self.split = split\n        self.num_points = num_points\n        self.holdout_area = None if holdout_area is None else int(holdout_area)\n        self.with_normalized_coords = with_normalized_coords\n        # keep at most 20/30 files in memory\n        self.cache_size = 20 if split == \'train\' else 30\n        self.cache = {}\n\n        # mapping batch index to corresponding file\n        areas = []\n        if self.split == \'train\':\n            for a in range(1, 7):\n                if a != self.holdout_area:\n                    areas.append(os.path.join(self.root, f\'Area_{a}\'))\n        else:\n            areas.append(os.path.join(self.root, f\'Area_{self.holdout_area}\'))\n\n        self.num_scene_windows, self.max_num_points = 0, 0\n        index_to_filename, scene_list = [], {}\n        filename_to_start_index = {}\n        for area in areas:\n            area_scenes = os.listdir(area)\n            area_scenes.sort()\n            for scene in area_scenes:\n                current_scene = os.path.join(area, scene)\n                scene_list[current_scene] = []\n                for split in [\'zero\', \'half\']:\n                    current_file = os.path.join(current_scene, f\'{split}_0.h5\')\n                    filename_to_start_index[current_file] = self.num_scene_windows\n                    h5f = h5py.File(current_file, \'r\')\n                    num_windows = h5f[\'data\'].shape[0]\n                    self.num_scene_windows += num_windows\n                    for i in range(num_windows):\n                        index_to_filename.append(current_file)\n                    scene_list[current_scene].append(current_file)\n        self.index_to_filename = index_to_filename\n        self.filename_to_start_index = filename_to_start_index\n        self.scene_list = scene_list\n\n    def __len__(self):\n        return self.num_scene_windows\n\n    def __getitem__(self, index):\n        filename = self.index_to_filename[index]\n        if filename not in self.cache.keys():\n            h5f = h5py.File(filename, \'r\')\n            scene_data = h5f[\'data\']\n            scene_label = h5f[\'label_seg\']\n            scene_num_points = h5f[\'data_num\']\n            if len(self.cache.keys()) < self.cache_size:\n                self.cache[filename] = (scene_data, scene_label, scene_num_points)\n            else:\n                victim_idx = np.random.randint(0, self.cache_size)\n                cache_keys = list(self.cache.keys())\n                cache_keys.sort()\n                self.cache.pop(cache_keys[victim_idx])\n                self.cache[filename] = (scene_data, scene_label, scene_num_points)\n        else:\n            scene_data, scene_label, scene_num_points = self.cache[filename]\n\n        internal_pos = index - self.filename_to_start_index[filename]\n        current_window_data = np.array(scene_data[internal_pos]).astype(np.float32)\n        current_window_label = np.array(scene_label[internal_pos]).astype(np.int64)\n        current_window_num_points = scene_num_points[internal_pos]\n\n        choices = np.random.choice(current_window_num_points, self.num_points,\n                                   replace=(current_window_num_points < self.num_points))\n        data = current_window_data[choices, ...].transpose()\n        label = current_window_label[choices]\n        # data[9, num_points] = [x_in_block, y_in_block, z_in_block, r, g, b, x / x_room, y / y_room, z / z_room]\n        if self.with_normalized_coords:\n            return data, label\n        else:\n            return data[:-3, :], label\n\n\nclass S3DIS(dict):\n    def __init__(self, root, num_points, split=None, with_normalized_coords=True, holdout_area=5):\n        super().__init__()\n        if split is None:\n            split = [\'train\', \'test\']\n        elif not isinstance(split, (list, tuple)):\n            split = [split]\n        for s in split:\n            self[s] = _S3DISDataset(root=root, num_points=num_points, split=s,\n                                    with_normalized_coords=with_normalized_coords, holdout_area=holdout_area)\n'"
datasets/shapenet.py,1,"b'import json\nimport os\n\nimport numpy as np\nfrom torch.utils.data import Dataset\n\n__all__ = [\'ShapeNet\']\n\n\nclass _ShapeNetDataset(Dataset):\n    def __init__(self, root, num_points, split=\'train\', with_normal=True, with_one_hot_shape_id=True,\n                 normalize=True, jitter=True):\n        assert split in [\'train\', \'test\']\n        self.root = root\n        self.num_points = num_points\n        self.split = split\n        self.with_normal = with_normal\n        self.with_one_hot_shape_id = with_one_hot_shape_id\n        self.normalize = normalize\n        self.jitter = jitter\n\n        shape_dir_to_shape_id = {}\n        with open(os.path.join(self.root, \'synsetoffset2category.txt\'), \'r\') as f:\n            for shape_id, line in enumerate(f):\n                shape_name, shape_dir = line.strip().split()\n                shape_dir_to_shape_id[shape_dir] = shape_id\n        file_paths = []\n        if self.split == \'train\':\n            split = [\'train\', \'val\']\n        else:\n            split = [\'test\']\n        for s in split:\n            with open(os.path.join(self.root, \'train_test_split\', f\'shuffled_{s}_file_list.json\'), \'r\') as f:\n                file_list = json.load(f)\n                for file_path in file_list:\n                    _, shape_dir, filename = file_path.split(\'/\')\n                    file_paths.append(\n                        (os.path.join(self.root, shape_dir, filename + \'.txt\'),\n                         shape_dir_to_shape_id[shape_dir])\n                    )\n        self.file_paths = file_paths\n        self.num_shapes = 16\n        self.num_classes = 50\n\n        self.cache = {}  # from index to (point_set, cls, seg) tuple\n        self.cache_size = 20000\n\n    def __getitem__(self, index):\n        if index in self.cache:\n            coords, normal, label, shape_id = self.cache[index]\n        else:\n            file_path, shape_id = self.file_paths[index]\n            data = np.loadtxt(file_path).astype(np.float32)\n            coords = data[:, :3]\n            if self.normalize:\n                coords = self.normalize_point_cloud(coords)\n            normal = data[:, 3:6]\n            label = data[:, -1].astype(np.int64)\n            if len(self.cache) < self.cache_size:\n                self.cache[index] = (coords, normal, label, shape_id)\n\n        choice = np.random.choice(label.shape[0], self.num_points, replace=True)\n        coords = coords[choice, :].transpose()\n        if self.jitter:\n            coords = self.jitter_point_cloud(coords)\n        if self.with_normal:\n            normal = normal[choice, :].transpose()\n            if self.with_one_hot_shape_id:\n                shape_one_hot = np.zeros((self.num_shapes, self.num_points), dtype=np.float32)\n                shape_one_hot[shape_id, :] = 1.0\n                point_set = np.concatenate([coords, normal, shape_one_hot])\n            else:\n                point_set = np.concatenate([coords, normal])\n        else:\n            if self.with_one_hot_shape_id:\n                shape_one_hot = np.zeros((self.num_shapes, self.num_points), dtype=np.float32)\n                shape_one_hot[shape_id, :] = 1.0\n                point_set = np.concatenate([coords, shape_one_hot])\n            else:\n                point_set = coords\n        return point_set, label[choice].transpose()\n\n    def __len__(self):\n        return len(self.file_paths)\n\n    @staticmethod\n    def normalize_point_cloud(points):\n        centroid = np.mean(points, axis=0)\n        points = points - centroid\n        return points / np.max(np.linalg.norm(points, axis=1))\n\n    @staticmethod\n    def jitter_point_cloud(points, sigma=0.01, clip=0.05):\n        """""" Randomly jitter points. jittering is per point.\n            Input:\n              3xN array, original batch of point clouds\n            Return:\n              3xN array, jittered batch of point clouds\n        """"""\n        assert (clip > 0)\n        return np.clip(sigma * np.random.randn(*points.shape), -1 * clip, clip).astype(np.float32) + points\n\n\nclass ShapeNet(dict):\n    def __init__(self, root, num_points, split=None, with_normal=True, with_one_hot_shape_id=True,\n                 normalize=True, jitter=True):\n        super().__init__()\n        if split is None:\n            split = [\'train\', \'test\']\n        elif not isinstance(split, (list, tuple)):\n            split = [split]\n        for s in split:\n            self[s] = _ShapeNetDataset(root=root, num_points=num_points, split=s,\n                                       with_normal=with_normal, with_one_hot_shape_id=with_one_hot_shape_id,\n                                       normalize=normalize, jitter=jitter if s == \'train\' else False)\n'"
evaluate/__init__.py,0,b''
meters/__init__.py,0,b'from meters.s3dis import MeterS3DIS\n'
meters/s3dis.py,5,"b""import torch\n\n__all__ = ['MeterS3DIS']\n\n\nclass MeterS3DIS:\n    def __init__(self, metric='iou', num_classes=13):\n        super().__init__()\n        assert metric in ['overall', 'class', 'iou']\n        self.metric = metric\n        self.num_classes = num_classes\n        self.reset()\n\n    def reset(self):\n        self.total_seen = [0] * self.num_classes\n        self.total_correct = [0] * self.num_classes\n        self.total_positive = [0] * self.num_classes\n        self.total_seen_num = 0\n        self.total_correct_num = 0\n\n    def update(self, outputs: torch.Tensor, targets: torch.Tensor):\n        # outputs: B x 13 x num_points, targets: B x num_points\n        predictions = outputs.argmax(1)\n        if self.metric == 'overall':\n            self.total_seen_num += targets.numel()\n            self.total_correct_num += torch.sum(targets == predictions).item()\n        else:\n            # self.metric == 'class' or self.metric == 'iou':\n            for i in range(self.num_classes):\n                itargets = (targets == i)\n                ipredictions = (predictions == i)\n                self.total_seen[i] += torch.sum(itargets).item()\n                self.total_positive[i] += torch.sum(ipredictions).item()\n                self.total_correct[i] += torch.sum(itargets & ipredictions).item()\n\n    def compute(self):\n        if self.metric == 'class':\n            accuracy = 0\n            for i in range(self.num_classes):\n                total_seen = self.total_seen[i]\n                if total_seen == 0:\n                    accuracy += 1\n                else:\n                    accuracy += self.total_correct[i] / total_seen\n            return accuracy / self.num_classes\n        elif self.metric == 'iou':\n            iou = 0\n            for i in range(self.num_classes):\n                total_seen = self.total_seen[i]\n                if total_seen == 0:\n                    iou += 1\n                else:\n                    total_correct = self.total_correct[i]\n                    iou += total_correct / (total_seen + self.total_positive[i] - total_correct)\n            return iou / self.num_classes\n        else:\n            return self.total_correct_num / self.total_seen_num\n"""
meters/shapenet.py,4,"b""import torch\n\n__all__ = ['MeterShapeNet']\n\n\ndefault_shape_name_to_part_classes = {\n    'Airplane': [0, 1, 2, 3],\n    'Bag': [4, 5],\n    'Cap': [6, 7],\n    'Car': [8, 9, 10, 11],\n    'Chair': [12, 13, 14, 15],\n    'Earphone': [16, 17, 18],\n    'Guitar': [19, 20, 21],\n    'Knife': [22, 23],\n    'Lamp': [24, 25, 26, 27],\n    'Laptop': [28, 29],\n    'Motorbike': [30, 31, 32, 33, 34, 35],\n    'Mug': [36, 37],\n    'Pistol': [38, 39, 40],\n    'Rocket': [41, 42, 43],\n    'Skateboard': [44, 45, 46],\n    'Table': [47, 48, 49],\n}\n\n\nclass MeterShapeNet:\n    def __init__(self, num_classes=50, num_shapes=16, shape_name_to_part_classes=None):\n        super().__init__()\n        self.num_classes = num_classes\n        self.num_shapes = num_shapes\n\n        self.shape_name_to_part_classes = default_shape_name_to_part_classes if shape_name_to_part_classes is None \\\n            else shape_name_to_part_classes\n        part_class_to_shape_part_classes = []\n        for shape_name, shape_part_classes in self.shape_name_to_part_classes.items():\n            start_class, end_class = shape_part_classes[0], shape_part_classes[-1] + 1\n            for _ in range(start_class, end_class):\n                part_class_to_shape_part_classes.append((start_class, end_class))\n        self.part_class_to_shape_part_classes = part_class_to_shape_part_classes\n        self.reset()\n\n    def reset(self):\n        self.iou_sum = 0\n        self.shape_count = 0\n\n    def update(self, outputs: torch.Tensor, targets: torch.Tensor):\n        # outputs: B x num_classes x num_points, targets: B x num_points\n        for b in range(outputs.size(0)):\n            start_class, end_class = self.part_class_to_shape_part_classes[targets[b, 0].item()]\n            prediction = torch.argmax(outputs[b, start_class:end_class, :], dim=0) + start_class\n            target = targets[b, :]\n            iou = 0.0\n            for i in range(start_class, end_class):\n                itarget = (target == i)\n                iprediction = (prediction == i)\n                union = torch.sum(itarget | iprediction).item()\n                intersection = torch.sum(itarget & iprediction).item()\n                if union == 0:\n                    iou += 1.0\n                else:\n                    iou += intersection / union\n            iou /= (end_class - start_class)\n            self.iou_sum += iou\n            self.shape_count += 1\n\n    def compute(self):\n        return self.iou_sum / self.shape_count\n"""
models/__init__.py,0,b''
models/utils.py,1,"b""import functools\n\nimport torch.nn as nn\n\nfrom modules import SharedMLP, PVConv, PointNetSAModule, PointNetAModule, PointNetFPModule\n\n__all__ = ['create_mlp_components', 'create_pointnet_components',\n           'create_pointnet2_sa_components', 'create_pointnet2_fp_modules']\n\n\ndef _linear_bn_relu(in_channels, out_channels):\n    return nn.Sequential(nn.Linear(in_channels, out_channels), nn.BatchNorm1d(out_channels), nn.ReLU(True))\n\n\ndef create_mlp_components(in_channels, out_channels, classifier=False, dim=2, width_multiplier=1):\n    r = width_multiplier\n\n    if dim == 1:\n        block = _linear_bn_relu\n    else:\n        block = SharedMLP\n    if not isinstance(out_channels, (list, tuple)):\n        out_channels = [out_channels]\n    if len(out_channels) == 0 or (len(out_channels) == 1 and out_channels[0] is None):\n        return nn.Sequential(), in_channels, in_channels\n\n    layers = []\n    for oc in out_channels[:-1]:\n        if oc < 1:\n            layers.append(nn.Dropout(oc))\n        else:\n            oc = int(r * oc)\n            layers.append(block(in_channels, oc))\n            in_channels = oc\n    if dim == 1:\n        if classifier:\n            layers.append(nn.Linear(in_channels, out_channels[-1]))\n        else:\n            layers.append(_linear_bn_relu(in_channels, int(r * out_channels[-1])))\n    else:\n        if classifier:\n            layers.append(nn.Conv1d(in_channels, out_channels[-1], 1))\n        else:\n            layers.append(SharedMLP(in_channels, int(r * out_channels[-1])))\n    return layers, out_channels[-1] if classifier else int(r * out_channels[-1])\n\n\ndef create_pointnet_components(blocks, in_channels, with_se=False, normalize=True, eps=0,\n                               width_multiplier=1, voxel_resolution_multiplier=1):\n    r, vr = width_multiplier, voxel_resolution_multiplier\n\n    layers, concat_channels = [], 0\n    for out_channels, num_blocks, voxel_resolution in blocks:\n        out_channels = int(r * out_channels)\n        if voxel_resolution is None:\n            block = SharedMLP\n        else:\n            block = functools.partial(PVConv, kernel_size=3, resolution=int(vr * voxel_resolution),\n                                      with_se=with_se, normalize=normalize, eps=eps)\n        for _ in range(num_blocks):\n            layers.append(block(in_channels, out_channels))\n            in_channels = out_channels\n            concat_channels += out_channels\n    return layers, in_channels, concat_channels\n\n\ndef create_pointnet2_sa_components(sa_blocks, extra_feature_channels, with_se=False, normalize=True, eps=0,\n                                   width_multiplier=1, voxel_resolution_multiplier=1):\n    r, vr = width_multiplier, voxel_resolution_multiplier\n    in_channels = extra_feature_channels + 3\n\n    sa_layers, sa_in_channels = [], []\n    for conv_configs, sa_configs in sa_blocks:\n        sa_in_channels.append(in_channels)\n        sa_blocks = []\n        if conv_configs is not None:\n            out_channels, num_blocks, voxel_resolution = conv_configs\n            out_channels = int(r * out_channels)\n            if voxel_resolution is None:\n                block = SharedMLP\n            else:\n                block = functools.partial(PVConv, kernel_size=3, resolution=int(vr * voxel_resolution),\n                                          with_se=with_se, normalize=normalize, eps=eps)\n            for _ in range(num_blocks):\n                sa_blocks.append(block(in_channels, out_channels))\n                in_channels = out_channels\n            extra_feature_channels = in_channels\n        num_centers, radius, num_neighbors, out_channels = sa_configs\n        _out_channels = []\n        for oc in out_channels:\n            if isinstance(oc, (list, tuple)):\n                _out_channels.append([int(r * _oc) for _oc in oc])\n            else:\n                _out_channels.append(int(r * oc))\n        out_channels = _out_channels\n        if num_centers is None:\n            block = PointNetAModule\n        else:\n            block = functools.partial(PointNetSAModule, num_centers=num_centers, radius=radius,\n                                      num_neighbors=num_neighbors)\n        sa_blocks.append(block(in_channels=extra_feature_channels, out_channels=out_channels,\n                               include_coordinates=True))\n        in_channels = extra_feature_channels = sa_blocks[-1].out_channels\n        if len(sa_blocks) == 1:\n            sa_layers.append(sa_blocks[0])\n        else:\n            sa_layers.append(nn.Sequential(*sa_blocks))\n\n    return sa_layers, sa_in_channels, in_channels, 1 if num_centers is None else num_centers\n\n\ndef create_pointnet2_fp_modules(fp_blocks, in_channels, sa_in_channels, with_se=False, normalize=True, eps=0,\n                                width_multiplier=1, voxel_resolution_multiplier=1):\n    r, vr = width_multiplier, voxel_resolution_multiplier\n\n    fp_layers = []\n    for fp_idx, (fp_configs, conv_configs) in enumerate(fp_blocks):\n        fp_blocks = []\n        out_channels = tuple(int(r * oc) for oc in fp_configs)\n        fp_blocks.append(\n            PointNetFPModule(in_channels=in_channels + sa_in_channels[-1 - fp_idx], out_channels=out_channels)\n        )\n        in_channels = out_channels[-1]\n        if conv_configs is not None:\n            out_channels, num_blocks, voxel_resolution = conv_configs\n            out_channels = int(r * out_channels)\n            if voxel_resolution is None:\n                block = SharedMLP\n            else:\n                block = functools.partial(PVConv, kernel_size=3, resolution=int(vr * voxel_resolution),\n                                          with_se=with_se, normalize=normalize, eps=eps)\n            for _ in range(num_blocks):\n                fp_blocks.append(block(in_channels, out_channels))\n                in_channels = out_channels\n        if len(fp_blocks) == 1:\n            fp_layers.append(fp_blocks[0])\n        else:\n            fp_layers.append(nn.Sequential(*fp_blocks))\n\n    return fp_layers, in_channels\n"""
modules/__init__.py,0,"b'from modules.ball_query import BallQuery\nfrom modules.frustum import FrustumPointNetLoss\nfrom modules.loss import KLLoss\nfrom modules.pointnet import PointNetAModule, PointNetSAModule, PointNetFPModule\nfrom modules.pvconv import PVConv\nfrom modules.se import SE3d\nfrom modules.shared_mlp import SharedMLP\nfrom modules.voxelization import Voxelization\n'"
modules/ball_query.py,2,"b""import torch\nimport torch.nn as nn\n\nimport modules.functional as F\n\n__all__ = ['BallQuery']\n\n\nclass BallQuery(nn.Module):\n    def __init__(self, radius, num_neighbors, include_coordinates=True):\n        super().__init__()\n        self.radius = radius\n        self.num_neighbors = num_neighbors\n        self.include_coordinates = include_coordinates\n\n    def forward(self, points_coords, centers_coords, points_features=None):\n        points_coords = points_coords.contiguous()\n        centers_coords = centers_coords.contiguous()\n        neighbor_indices = F.ball_query(centers_coords, points_coords, self.radius, self.num_neighbors)\n        neighbor_coordinates = F.grouping(points_coords, neighbor_indices)\n        neighbor_coordinates = neighbor_coordinates - centers_coords.unsqueeze(-1)\n\n        if points_features is None:\n            assert self.include_coordinates, 'No Features For Grouping'\n            neighbor_features = neighbor_coordinates\n        else:\n            neighbor_features = F.grouping(points_features, neighbor_indices)\n            if self.include_coordinates:\n                neighbor_features = torch.cat([neighbor_coordinates, neighbor_features], dim=1)\n        return neighbor_features\n\n    def extra_repr(self):\n        return 'radius={}, num_neighbors={}{}'.format(\n            self.radius, self.num_neighbors, ', include coordinates' if self.include_coordinates else '')\n"""
modules/frustum.py,29,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nimport modules.functional as PF\n\n__all__ = [\'FrustumPointNetLoss\', \'get_box_corners_3d\']\n\n\nclass FrustumPointNetLoss(nn.Module):\n    def __init__(self, num_heading_angle_bins, num_size_templates, size_templates, box_loss_weight=1.0,\n                 corners_loss_weight=10.0, heading_residual_loss_weight=20.0, size_residual_loss_weight=20.0):\n        super().__init__()\n        self.box_loss_weight = box_loss_weight\n        self.corners_loss_weight = corners_loss_weight\n        self.heading_residual_loss_weight = heading_residual_loss_weight\n        self.size_residual_loss_weight = size_residual_loss_weight\n\n        self.num_heading_angle_bins = num_heading_angle_bins\n        self.num_size_templates = num_size_templates\n        self.register_buffer(\'size_templates\', size_templates.view(self.num_size_templates, 3))\n        self.register_buffer(\n            \'heading_angle_bin_centers\', torch.arange(0, 2 * np.pi, 2 * np.pi / self.num_heading_angle_bins)\n        )\n\n    def forward(self, inputs, targets):\n        mask_logits = inputs[\'mask_logits\']  # (B, 2, N)\n        center_reg = inputs[\'center_reg\']  # (B, 3)\n        center = inputs[\'center\']  # (B, 3)\n        heading_scores = inputs[\'heading_scores\']  # (B, NH)\n        heading_residuals_normalized = inputs[\'heading_residuals_normalized\']  # (B, NH)\n        heading_residuals = inputs[\'heading_residuals\']  # (B, NH)\n        size_scores = inputs[\'size_scores\']  # (B, NS)\n        size_residuals_normalized = inputs[\'size_residuals_normalized\']  # (B, NS, 3)\n        size_residuals = inputs[\'size_residuals\']  # (B, NS, 3)\n\n        mask_logits_target = targets[\'mask_logits\']  # (B, N)\n        center_target = targets[\'center\']  # (B, 3)\n        heading_bin_id_target = targets[\'heading_bin_id\']  # (B, )\n        heading_residual_target = targets[\'heading_residual\']  # (B, )\n        size_template_id_target = targets[\'size_template_id\']  # (B, )\n        size_residual_target = targets[\'size_residual\']  # (B, 3)\n\n        batch_size = center.size(0)\n        batch_id = torch.arange(batch_size, device=center.device)\n\n        # Basic Classification and Regression losses\n        mask_loss = F.cross_entropy(mask_logits, mask_logits_target)\n        heading_loss = F.cross_entropy(heading_scores, heading_bin_id_target)\n        size_loss = F.cross_entropy(size_scores, size_template_id_target)\n        center_loss = PF.huber_loss(torch.norm(center_target - center, dim=-1), delta=2.0)\n        center_reg_loss = PF.huber_loss(torch.norm(center_target - center_reg, dim=-1), delta=1.0)\n\n        # Refinement losses for size/heading\n        heading_residuals_normalized = heading_residuals_normalized[batch_id, heading_bin_id_target]  # (B, )\n        heading_residual_normalized_target = heading_residual_target / (np.pi / self.num_heading_angle_bins)\n        heading_residual_normalized_loss = PF.huber_loss(\n            heading_residuals_normalized - heading_residual_normalized_target, delta=1.0\n        )\n        size_residuals_normalized = size_residuals_normalized[batch_id, size_template_id_target]  # (B, 3)\n        size_residual_normalized_target = size_residual_target / self.size_templates[size_template_id_target]\n        size_residual_normalized_loss = PF.huber_loss(\n            torch.norm(size_residual_normalized_target - size_residuals_normalized, dim=-1), delta=1.0\n        )\n\n        # Bounding box losses\n        heading = (heading_residuals[batch_id, heading_bin_id_target]\n                   + self.heading_angle_bin_centers[heading_bin_id_target])  # (B, )\n        # Warning: in origin code, size_residuals are added twice (issue #43 and #49 in charlesq34/frustum-pointnets)\n        size = (size_residuals[batch_id, size_template_id_target]\n                + self.size_templates[size_template_id_target])  # (B, 3)\n        corners = get_box_corners_3d(centers=center, headings=heading, sizes=size, with_flip=False)  # (B, 3, 8)\n        heading_target = self.heading_angle_bin_centers[heading_bin_id_target] + heading_residual_target  # (B, )\n        size_target = self.size_templates[size_template_id_target] + size_residual_target  # (B, 3)\n        corners_target, corners_target_flip = get_box_corners_3d(centers=center_target, headings=heading_target,\n                                                                 sizes=size_target, with_flip=True)  # (B, 3, 8)\n        corners_loss = PF.huber_loss(torch.min(\n            torch.norm(corners - corners_target, dim=1), torch.norm(corners - corners_target_flip, dim=1)\n        ), delta=1.0)\n        # Summing up\n        loss = mask_loss + self.box_loss_weight * (\n                center_loss + center_reg_loss + heading_loss + size_loss\n                + self.heading_residual_loss_weight * heading_residual_normalized_loss\n                + self.size_residual_loss_weight * size_residual_normalized_loss\n                + self.corners_loss_weight * corners_loss\n        )\n\n        return loss\n\n\ndef get_box_corners_3d(centers, headings, sizes, with_flip=False):\n    """"""\n    :param centers: coords of box centers, FloatTensor[N, 3]\n    :param headings: heading angles, FloatTensor[N, ]\n    :param sizes: box sizes, FloatTensor[N, 3]\n    :param with_flip: bool, whether to return flipped box (headings + np.pi)\n    :return:\n        coords of box corners, FloatTensor[N, 3, 8]\n        NOTE: corner points are in counter clockwise order, e.g.,\n          2--1\n        3--0 5\n        7--4\n    """"""\n    l = sizes[:, 0]  # (N,)\n    w = sizes[:, 1]  # (N,)\n    h = sizes[:, 2]  # (N,)\n    x_corners = torch.stack([l/2, l/2, -l/2, -l/2, l/2, l/2, -l/2, -l/2], dim=1)  # (N, 8)\n    y_corners = torch.stack([h/2, h/2, h/2, h/2, -h/2, -h/2, -h/2, -h/2], dim=1)  # (N, 8)\n    z_corners = torch.stack([w/2, -w/2, -w/2, w/2, w/2, -w/2, -w/2, w/2], dim=1)  # (N, 8)\n\n    c = torch.cos(headings)  # (N,)\n    s = torch.sin(headings)  # (N,)\n    o = torch.ones_like(headings)  # (N,)\n    z = torch.zeros_like(headings)  # (N,)\n\n    centers = centers.unsqueeze(-1)  # (B, 3, 1)\n    corners = torch.stack([x_corners, y_corners, z_corners], dim=1)  # (N, 3, 8)\n    R = torch.stack([c, z, s, z, o, z, -s, z, c], dim=1).view(-1, 3, 3)  # roty matrix: (N, 3, 3)\n    if with_flip:\n        R_flip = torch.stack([-c, z, -s, z, o, z, s, z, -c], dim=1).view(-1, 3, 3)\n        return torch.matmul(R, corners) + centers, torch.matmul(R_flip, corners) + centers\n    else:\n        return torch.matmul(R, corners) + centers\n\n    # centers = centers.unsqueeze(1)  # (B, 1, 3)\n    # corners = torch.stack([x_corners, y_corners, z_corners], dim=-1)  # (N, 8, 3)\n    # RT = torch.stack([c, z, -s, z, o, z, s, z, c], dim=1).view(-1, 3, 3)  # (N, 3, 3)\n    # if with_flip:\n    #     RT_flip = torch.stack([-c, z, s, z, o, z, -s, z, -c], dim=1).view(-1, 3, 3)  # (N, 3, 3)\n    #     return torch.matmul(corners, RT) + centers, torch.matmul(corners, RT_flip) + centers  # (N, 8, 3)\n    # else:\n    #     return torch.matmul(corners, RT) + centers  # (N, 8, 3)\n\n    # corners = torch.stack([x_corners, y_corners, z_corners], dim=1)  # (N, 3, 8)\n    # R = torch.stack([c, z, s, z, o, z, -s, z, c], dim=1).view(-1, 3, 3)  # (N, 3, 3)\n    # corners = torch.matmul(R, corners) + centers.unsqueeze(2)  # (N, 3, 8)\n    # corners = corners.transpose(1, 2)  # (N, 8, 3)\n'"
modules/loss.py,1,"b""import torch.nn as nn\n\nimport modules.functional as F\n\n__all__ = ['KLLoss']\n\n\nclass KLLoss(nn.Module):\n    def forward(self, x, y):\n        return F.kl_loss(x, y)\n"""
modules/pointnet.py,6,"b""import torch\nimport torch.nn as nn\n\nimport modules.functional as F\nfrom modules.ball_query import BallQuery\nfrom modules.shared_mlp import SharedMLP\n\n__all__ = ['PointNetAModule', 'PointNetSAModule', 'PointNetFPModule']\n\n\nclass PointNetAModule(nn.Module):\n    def __init__(self, in_channels, out_channels, include_coordinates=True):\n        super().__init__()\n        if not isinstance(out_channels, (list, tuple)):\n            out_channels = [[out_channels]]\n        elif not isinstance(out_channels[0], (list, tuple)):\n            out_channels = [out_channels]\n\n        mlps = []\n        total_out_channels = 0\n        for _out_channels in out_channels:\n            mlps.append(\n                SharedMLP(in_channels=in_channels + (3 if include_coordinates else 0),\n                          out_channels=_out_channels, dim=1)\n            )\n            total_out_channels += _out_channels[-1]\n\n        self.include_coordinates = include_coordinates\n        self.out_channels = total_out_channels\n        self.mlps = nn.ModuleList(mlps)\n\n    def forward(self, inputs):\n        features, coords = inputs\n        if self.include_coordinates:\n            features = torch.cat([features, coords], dim=1)\n        coords = torch.zeros((coords.size(0), 3, 1), device=coords.device)\n        if len(self.mlps) > 1:\n            features_list = []\n            for mlp in self.mlps:\n                features_list.append(mlp(features).max(dim=-1, keepdim=True).values)\n            return torch.cat(features_list, dim=1), coords\n        else:\n            return self.mlps[0](features).max(dim=-1, keepdim=True).values, coords\n\n    def extra_repr(self):\n        return f'out_channels={self.out_channels}, include_coordinates={self.include_coordinates}'\n\n\nclass PointNetSAModule(nn.Module):\n    def __init__(self, num_centers, radius, num_neighbors, in_channels, out_channels, include_coordinates=True):\n        super().__init__()\n        if not isinstance(radius, (list, tuple)):\n            radius = [radius]\n        if not isinstance(num_neighbors, (list, tuple)):\n            num_neighbors = [num_neighbors] * len(radius)\n        assert len(radius) == len(num_neighbors)\n        if not isinstance(out_channels, (list, tuple)):\n            out_channels = [[out_channels]] * len(radius)\n        elif not isinstance(out_channels[0], (list, tuple)):\n            out_channels = [out_channels] * len(radius)\n        assert len(radius) == len(out_channels)\n\n        groupers, mlps = [], []\n        total_out_channels = 0\n        for _radius, _out_channels, _num_neighbors in zip(radius, out_channels, num_neighbors):\n            groupers.append(\n                BallQuery(radius=_radius, num_neighbors=_num_neighbors, include_coordinates=include_coordinates)\n            )\n            mlps.append(\n                SharedMLP(in_channels=in_channels + (3 if include_coordinates else 0),\n                          out_channels=_out_channels, dim=2)\n            )\n            total_out_channels += _out_channels[-1]\n\n        self.num_centers = num_centers\n        self.out_channels = total_out_channels\n        self.groupers = nn.ModuleList(groupers)\n        self.mlps = nn.ModuleList(mlps)\n\n    def forward(self, inputs):\n        features, coords = inputs\n        centers_coords = F.furthest_point_sample(coords, self.num_centers)\n        features_list = []\n        for grouper, mlp in zip(self.groupers, self.mlps):\n            features_list.append(mlp(grouper(coords, centers_coords, features)).max(dim=-1).values)\n        if len(features_list) > 1:\n            return torch.cat(features_list, dim=1), centers_coords\n        else:\n            return features_list[0], centers_coords\n\n    def extra_repr(self):\n        return f'num_centers={self.num_centers}, out_channels={self.out_channels}'\n\n\nclass PointNetFPModule(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.mlp = SharedMLP(in_channels=in_channels, out_channels=out_channels, dim=1)\n\n    def forward(self, inputs):\n        if len(inputs) == 3:\n            points_coords, centers_coords, centers_features = inputs\n            points_features = None\n        else:\n            points_coords, centers_coords, centers_features, points_features = inputs\n        interpolated_features = F.nearest_neighbor_interpolate(points_coords, centers_coords, centers_features)\n        if points_features is not None:\n            interpolated_features = torch.cat(\n                [interpolated_features, points_features], dim=1\n            )\n        return self.mlp(interpolated_features), points_coords\n"""
modules/pvconv.py,1,"b""import torch.nn as nn\n\nimport modules.functional as F\nfrom modules.voxelization import Voxelization\nfrom modules.shared_mlp import SharedMLP\nfrom modules.se import SE3d\n\n__all__ = ['PVConv']\n\n\nclass PVConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, resolution, with_se=False, normalize=True, eps=0):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.resolution = resolution\n\n        self.voxelization = Voxelization(resolution, normalize=normalize, eps=eps)\n        voxel_layers = [\n            nn.Conv3d(in_channels, out_channels, kernel_size, stride=1, padding=kernel_size // 2),\n            nn.BatchNorm3d(out_channels, eps=1e-4),\n            nn.LeakyReLU(0.1, True),\n            nn.Conv3d(out_channels, out_channels, kernel_size, stride=1, padding=kernel_size // 2),\n            nn.BatchNorm3d(out_channels, eps=1e-4),\n            nn.LeakyReLU(0.1, True),\n         ]\n        if with_se:\n            voxel_layers.append(SE3d(out_channels))\n        self.voxel_layers = nn.Sequential(*voxel_layers)\n        self.point_features = SharedMLP(in_channels, out_channels)\n\n    def forward(self, inputs):\n        features, coords = inputs\n        voxel_features, voxel_coords = self.voxelization(features, coords)\n        voxel_features = self.voxel_layers(voxel_features)\n        voxel_features = F.trilinear_devoxelize(voxel_features, voxel_coords, self.resolution, self.training)\n        fused_features = voxel_features + self.point_features(features)\n        return fused_features, coords\n"""
modules/se.py,1,"b""import torch.nn as nn\n\n__all__ = ['SE3d']\n\n\nclass SE3d(nn.Module):\n    def __init__(self, channel, reduction=8):\n        super().__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, inputs):\n        return inputs * self.fc(inputs.mean(-1).mean(-1).mean(-1)).view(inputs.shape[0], inputs.shape[1], 1, 1, 1)\n"""
modules/shared_mlp.py,1,"b""import torch.nn as nn\n\n__all__ = ['SharedMLP']\n\n\nclass SharedMLP(nn.Module):\n    def __init__(self, in_channels, out_channels, dim=1):\n        super().__init__()\n        if dim == 1:\n            conv = nn.Conv1d\n            bn = nn.BatchNorm1d\n        elif dim == 2:\n            conv = nn.Conv2d\n            bn = nn.BatchNorm2d\n        else:\n            raise ValueError\n        if not isinstance(out_channels, (list, tuple)):\n            out_channels = [out_channels]\n        layers = []\n        for oc in out_channels:\n            layers.extend([\n                conv(in_channels, oc, 1),\n                bn(oc),\n                nn.ReLU(True),\n            ])\n            in_channels = oc\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        if isinstance(inputs, (list, tuple)):\n            return (self.layers(inputs[0]), *inputs[1:])\n        else:\n            return self.layers(inputs)\n"""
modules/voxelization.py,3,"b""import torch\nimport torch.nn as nn\n\nimport modules.functional as F\n\n__all__ = ['Voxelization']\n\n\nclass Voxelization(nn.Module):\n    def __init__(self, resolution, normalize=True, eps=0):\n        super().__init__()\n        self.r = int(resolution)\n        self.normalize = normalize\n        self.eps = eps\n\n    def forward(self, features, coords):\n        coords = coords.detach()\n        norm_coords = coords - coords.mean(2, keepdim=True)\n        if self.normalize:\n            norm_coords = norm_coords / (norm_coords.norm(dim=1, keepdim=True).max(dim=2, keepdim=True).values * 2.0 + self.eps) + 0.5\n        else:\n            norm_coords = (norm_coords + 1) / 2.0\n        norm_coords = torch.clamp(norm_coords * self.r, 0, self.r - 1)\n        vox_coords = torch.round(norm_coords).to(torch.int32)\n        return F.avg_voxelize(features, vox_coords, self.r), norm_coords\n\n    def extra_repr(self):\n        return 'resolution={}{}'.format(self.r, ', normalized eps = {}'.format(self.eps) if self.normalize else '')\n"""
utils/__init__.py,0,b''
utils/common.py,0,"b""import os\n\n__all__ = ['get_save_path']\n\n\ndef get_save_path(*configs, prefix='runs'):\n    memo = dict()\n    for c in configs:\n        cmemo = memo\n        c = c.replace('configs/', '').replace('.py', '').split('/')\n        for m in c:\n            if m not in cmemo:\n                cmemo[m] = dict()\n            cmemo = cmemo[m]\n\n    def get_str(m, p):\n        n = len(m)\n        if n > 1:\n            p += '['\n        for i, (k, v) in enumerate(m.items()):\n            p += k\n            if len(v) > 0:\n                p += '.'\n            p = get_str(v, p)\n            if n > 1 and i < n - 1:\n                p += '+'\n        if n > 1:\n            p += ']'\n        return p\n\n    return os.path.join(prefix, get_str(memo, ''))\n"""
utils/config.py,0,"b'import collections\nimport importlib.util\nimport os\n\nimport six\n\nfrom utils.container import G\n\n__all__ = [\'Config\', \'configs\', \'update_configs_from_module\', \'update_configs_from_arguments\']\n\n\nclass Config(G):\n    def __init__(self, func=None, args=None, keys=None, detach=False, **kwargs):\n        super().__init__(**kwargs)\n\n        if func is not None and not callable(func):\n            raise TypeError(\'func ""{}"" is not a callable function or class\'.format(repr(func)))\n        if args is not None and not isinstance(args, (collections.Sequence, collections.UserList)):\n            raise TypeError(\'args ""{}"" is not an iterable tuple or list\'.format(repr(args)))\n        if keys is not None and not isinstance(keys, (collections.Sequence, collections.UserList)):\n            raise TypeError(\'keys ""{}"" is not an iterable tuple or list\'.format(repr(keys)))\n        self.__dict__[\'_func_\'] = func\n        self.__dict__[\'_args_\'] = args\n        self.__dict__[\'_detach_\'] = detach\n        self.__dict__[\'_keys_\'] = keys\n\n    def __call__(self, *args, **kwargs):\n        if self._func_ is None:\n            return self\n\n        # override args\n        if args:\n            args = list(args)\n        elif self._args_:\n            args = list(self._args_)\n\n        # override kwargs\n        for k, v in self.items():\n            if self._keys_ is None or k in self._keys_:\n                kwargs.setdefault(k, v)\n\n        # recursively call non-detached funcs\n        queue = collections.deque([args, kwargs])\n        while queue:\n            x = queue.popleft()\n\n            if isinstance(x, (collections.Sequence, collections.UserList)) and not isinstance(x, six.string_types):\n                items = enumerate(x)\n            elif isinstance(x, (collections.Mapping, collections.UserDict)):\n                items = x.items()\n            else:\n                items = []\n\n            for k, v in items:\n                if isinstance(v, tuple):\n                    v = x[k] = list(v)\n                elif isinstance(v, Config):\n                    if v._detach_:\n                        continue\n                    v = x[k] = v()\n                queue.append(v)\n\n        return self._func_(*args, **kwargs)\n\n    def __str__(self, indent=0):\n        text = \'\'\n        if self._func_ is not None:\n            text += \' \' * indent + \'[func] = \' + str(self._func_)\n            extra = False\n            if self._detach_:\n                text += \' (detach=\' + str(self._detach_)\n                extra = True\n            if self._keys_:\n                text += \', \' if extra else \' (\'\n                text += \'keys=\' + str(self._keys_)\n                extra = True\n            text += \')\\n\' if extra else \'\\n\'\n            if self._args_:\n                for k, v in enumerate(self._args_):\n                    text += \' \' * indent + \'[args:\' + str(k) + \'] = \' + str(v) + \'\\n\'\n\n        for k, v in self.items():\n            text += \' \' * indent + \'[\' + str(k) + \']\'\n            if isinstance(v, Config):\n                text += \'\\n\' + v.__str__(indent + 2)\n            else:\n                text += \' = \' + str(v)\n            text += \'\\n\'\n\n        while text and text[-1] == \'\\n\':\n            text = text[:-1]\n        return text\n\n    def __repr__(self):\n        text = \'\'\n        if self._func_ is not None:\n            text += repr(self._func_)\n\n        items = []\n        if self._func_ is not None and self._args_:\n            items += [repr(v) for v in self._args_]\n        items += [str(k) + \'=\' + repr(v) for k, v in self.items()]\n        if self._func_ is not None and self._detach_:\n            items += [\'detach=\' + repr(self._detach_)]\n\n        text += \'(\' + \', \'.join(items) + \')\'\n        return text\n\n    @staticmethod\n    def update_from_modules(*modules):\n        for module in modules:\n            module = module.replace(\'.py\', \'\').replace(\'/\', \'.\')\n            importlib.import_module(module)\n\n    @staticmethod\n    def update_from_arguments(*args):\n        update_configs_from_arguments(args)\n\n\nconfigs = Config()\n\n\ndef update_configs_from_module(*mods):\n    imported_modules = set()\n\n    # from https://stackoverflow.com/questions/67631/how-to-import-a-module-given-the-full-path\n    def exec_module_once(module):\n        if module in imported_modules:\n            return\n        imported_modules.add(module)\n        spec = importlib.util.spec_from_file_location(os.path.basename(module), module)\n        foo = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(foo)\n\n    for mod in mods:\n        mod = os.path.normpath(mod)\n        for index, char in enumerate(mod):\n            if index == 0 or char == os.sep:\n                submod = os.path.join(mod[:index], \'__init__.py\')\n                if os.path.exists(submod):\n                    exec_module_once(submod)\n        exec_module_once(mod)\n\n\ndef update_configs_from_arguments(args):\n    index = 0\n\n    while index < len(args):\n        arg = args[index]\n\n        if arg.startswith(\'--configs.\'):\n            arg = arg.replace(\'--configs.\', \'\')\n        else:\n            raise Exception(\'unrecognized argument ""{}""\'.format(arg))\n\n        if \'=\' in arg:\n            index, keys, val = index + 1, arg[:arg.index(\'=\')].split(\'.\'), arg[arg.index(\'=\') + 1:]\n        else:\n            index, keys, val = index + 2, arg.split(\'.\'), args[index + 1]\n\n        config = configs\n        for k in keys[:-1]:\n            if k not in config:\n                config[k] = Config()\n            config = config[k]\n\n        def parse(x):\n            if (x[0] == \'\\\'\' and x[-1] == \'\\\'\') or (x[0] == \'\\""\' and x[-1] == \'\\""\'):\n                return x[1:-1]\n            try:\n                x = eval(x)\n            except:\n                pass\n            return x\n\n        config[keys[-1]] = parse(val)\n'"
utils/container.py,0,"b""__all__ = ['G']\n\n\n# from https://github.com/vacancy/Jacinle/blob/master/jacinle/utils/container.py\nclass G(dict):\n    def __getattr__(self, k):\n        if k not in self:\n            raise AttributeError(k)\n        return self[k]\n\n    def __setattr__(self, k, v):\n        self[k] = v\n\n    def __delattr__(self, k):\n        del self[k]\n"""
utils/device.py,0,"b""import os\n\n__all__ = ['set_cuda_visible_devices']\n\n\ndef set_cuda_visible_devices(devs):\n    gpus = []\n    for dev in devs.split(','):\n        dev = dev.strip().lower()\n        if dev == 'cpu':\n            continue\n        if dev.startswith('gpu'):\n            dev = dev[3:]\n        if '-' in dev:\n            l, r = map(int, dev.split('-'))\n            gpus.extend(range(l, r + 1))\n        else:\n            gpus.append(int(dev))\n\n    os.environ['CUDA_VISIBLE_DEVICES'] = ','.join([str(gpu) for gpu in gpus])\n    return gpus\n"""
configs/kitti/__init__.py,0,"b""from utils.config import Config, configs\n\n# data configs\nconfigs.data.classes = ('Car', 'Pedestrian', 'Cyclist')\nconfigs.data.num_classes = len(configs.data.classes)\n\n# evaluate configs\nconfigs.evaluate = Config()\nconfigs.evaluate.num_tests = 20\nconfigs.evaluate.ground_truth_path = 'data/kitti/ground_truth'\nconfigs.evaluate.image_id_file_path = 'data/kitti/image_sets/val.txt'\n"""
configs/s3dis/__init__.py,2,"b""import torch.nn as nn\nimport torch.optim as optim\n\nfrom datasets.s3dis import S3DIS\nfrom meters.s3dis import MeterS3DIS\nfrom evaluate.s3dis.eval import evaluate\nfrom utils.config import Config, configs\n\nconfigs.data.num_classes = 13\n\n# dataset configs\nconfigs.dataset = Config(S3DIS)\nconfigs.dataset.root = 'data/s3dis/pointcnn'\nconfigs.dataset.with_normalized_coords = True\n# configs.dataset.num_points = 2048\n# configs.dataset.holdout_area = 5\n\n# evaluate configs\nconfigs.evaluate = Config()\nconfigs.evaluate.fn = evaluate\nconfigs.evaluate.num_votes = 1\nconfigs.evaluate.batch_size = 10\nconfigs.evaluate.dataset = Config(split='test')\n\n# train configs\nconfigs.train = Config()\nconfigs.train.num_epochs = 50\nconfigs.train.batch_size = 32\n\n# train: meters\nconfigs.train.meters = Config()\nconfigs.train.meters['acc/iou_{}'] = Config(MeterS3DIS, metric='iou', num_classes=configs.data.num_classes)\nconfigs.train.meters['acc/acc_{}'] = Config(MeterS3DIS, metric='overall', num_classes=configs.data.num_classes)\n\n# train: metric for save best checkpoint\nconfigs.train.metric = 'acc/iou_test'\n\n# train: criterion\nconfigs.train.criterion = Config(nn.CrossEntropyLoss)\n\n# train: optimizer\nconfigs.train.optimizer = Config(optim.Adam)\nconfigs.train.optimizer.lr = 1e-3\n"""
configs/shapenet/__init__.py,2,"b""import torch.nn as nn\nimport torch.optim as optim\n\nfrom datasets.shapenet import ShapeNet\nfrom meters.shapenet import MeterShapeNet\nfrom evaluate.shapenet.eval import evaluate\nfrom utils.config import Config, configs\n\nconfigs.data.num_classes = 50\nconfigs.data.num_shapes = 16\n\n# dataset configs\nconfigs.dataset = Config(ShapeNet)\nconfigs.dataset.root = 'data/shapenet/shapenetcore_partanno_segmentation_benchmark_v0_normal'\nconfigs.dataset.with_normal = True\nconfigs.dataset.with_one_hot_shape_id = True\nconfigs.dataset.normalize = True\nconfigs.dataset.jitter = True\nconfigs.dataset.num_points = 2048\n\n# evaluate configs\nconfigs.evaluate = Config()\nconfigs.evaluate.fn = evaluate\nconfigs.evaluate.num_votes = 10\nconfigs.evaluate.dataset = Config(split='test')\n\n# train configs\nconfigs.train = Config()\nconfigs.train.num_epochs = 200\nconfigs.train.batch_size = 32\n\n# train: meters\nconfigs.train.meters = Config()\nconfigs.train.meters['acc/iou_{}'] = Config(MeterShapeNet, num_classes=configs.data.num_classes)\n\n# train: metric for save best checkpoint\nconfigs.train.metric = 'acc/iou_test'\n\n# train: criterion\nconfigs.train.criterion = Config(nn.CrossEntropyLoss)\n\n# train: optimizer\nconfigs.train.optimizer = Config(optim.Adam)\nconfigs.train.optimizer.lr = 1e-3\n"""
configs/shapenet/pointnet.py,1,"b'import torch.optim as optim\n\nfrom models.shapenet import PointNet\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PointNet)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_shapes = configs.data.num_shapes\nconfigs.model.extra_feature_channels = 0\n\nconfigs.dataset.with_normal = False\nconfigs.train.scheduler = Config(optim.lr_scheduler.StepLR)\nconfigs.train.scheduler.step_size = 20\nconfigs.train.scheduler.gamma = 0.5\n'"
configs/shapenet/pointnet2msg.py,1,"b'import torch.optim as optim\n\nfrom models.shapenet import PointNet2MSG\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PointNet2MSG)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_shapes = configs.data.num_shapes\nconfigs.model.extra_feature_channels = 3\n\nconfigs.train.scheduler = Config(optim.lr_scheduler.StepLR)\nconfigs.train.scheduler.step_size = 20\nconfigs.train.scheduler.gamma = 0.5\n'"
configs/shapenet/pointnet2ssg.py,1,"b'import torch.optim as optim\n\nfrom models.shapenet import PointNet2SSG\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PointNet2SSG)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_shapes = configs.data.num_shapes\nconfigs.model.extra_feature_channels = 3\n\nconfigs.dataset.with_one_hot_shape_id = False\nconfigs.train.scheduler = Config(optim.lr_scheduler.StepLR)\nconfigs.train.scheduler.step_size = 20\nconfigs.train.scheduler.gamma = 0.5\n'"
data/s3dis/prepare_data.py,0,"b""import argparse\nimport math\nimport os\nfrom datetime import datetime\n\nimport h5py\nimport numpy as np\nimport plyfile\nfrom matplotlib import cm\n\n\ndef prepare_label(data_dir, output_dir):\n    object_dict = {\n        'clutter': 0,\n        'ceiling': 1,\n        'floor': 2,\n        'wall': 3,\n        'beam': 4,\n        'column': 5,\n        'door': 6,\n        'window': 7,\n        'table': 8,\n        'chair': 9,\n        'sofa': 10,\n        'bookcase': 11,\n        'board': 12\n    }\n\n    for area in os.listdir(data_dir):\n        path_area = os.path.join(data_dir, area)\n        if not os.path.isdir(path_area):\n            continue\n\n        path_dir_rooms = os.listdir(path_area)\n        for room in path_dir_rooms:\n            path_annotations = os.path.join(data_dir, area, room, 'Annotations')\n            if not os.path.isdir(path_annotations):\n                continue\n\n            print(path_annotations)\n            path_prepare_label = os.path.join(output_dir, area, room)\n            if os.path.exists(os.path.join(path_prepare_label, '.labels')):\n                print(f'{path_prepare_label} already processed, skipping')\n                continue\n\n            xyz_room = np.zeros((1, 6))\n            label_room = np.zeros((1, 1))\n            # make store directories\n            if not os.path.exists(path_prepare_label):\n                os.makedirs(path_prepare_label)\n\n            path_objects = os.listdir(path_annotations)\n            for obj in path_objects:\n                object_key = obj.split('_', 1)[0]\n                try:\n                    val = object_dict[object_key]\n                except KeyError:\n                    continue\n                print(f'{room}/{obj[:-4]}')\n                xyz_object_path = os.path.join(path_annotations, obj)\n                try:\n                    xyz_object = np.loadtxt(xyz_object_path)[:, :]  # (N,6)\n                except ValueError as e:\n                    print(f'ERROR: cannot load {xyz_object_path}: {e}')\n                    continue\n                label_object = np.tile(val, (xyz_object.shape[0], 1))  # (N,1)\n                xyz_room = np.vstack((xyz_room, xyz_object))\n                label_room = np.vstack((label_room, label_object))\n\n            xyz_room = np.delete(xyz_room, [0], 0)\n            label_room = np.delete(label_room, [0], 0)\n\n            np.save(path_prepare_label + '/xyzrgb.npy', xyz_room)\n            np.save(path_prepare_label + '/label.npy', label_room)\n\n            # Marker indicating we've processed this room\n            open(os.path.join(path_prepare_label, '.labels'), 'w').close()\n\n\ndef main():\n    default_data_dir = 'data/s3dis/Stanford3dDataset_v1.2_Aligned_Version'\n    default_output_dir = 'data/s3dis/pointcnn'\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-d', '--data', dest='data_dir', default=default_data_dir,\n                        help=f'Path to S3DIS data (default is {default_data_dir})')\n    parser.add_argument('-f', '--folder', dest='output_dir', default=default_output_dir,\n                        help=f'Folder to write labels (default is {default_output_dir})')\n    parser.add_argument('--max_num_points', '-m', help='Max point number of each sample', type=int, default=8192)\n    parser.add_argument('--block_size', '-b', help='Block size', type=float, default=1.5)\n    parser.add_argument('--grid_size', '-g', help='Grid size', type=float, default=0.03)\n    parser.add_argument('--save_ply', '-s', help='Convert .pts to .ply', action='store_true')\n\n    args = parser.parse_args()\n    print(args)\n\n    prepare_label(data_dir=args.data_dir, output_dir=args.output_dir)\n\n    root = args.output_dir\n    max_num_points = args.max_num_points\n\n    batch_size = 2048\n    data = np.zeros((batch_size, max_num_points, 9))\n    data_num = np.zeros(batch_size, dtype=np.int32)\n    label = np.zeros(batch_size, dtype=np.int32)\n    label_seg = np.zeros((batch_size, max_num_points), dtype=np.int32)\n    indices_split_to_full = np.zeros((batch_size, max_num_points), dtype=np.int32)\n\n    for area_idx in range(1, 7):\n        folder = os.path.join(root, 'Area_%d' % area_idx)\n        datasets = [dataset for dataset in os.listdir(folder)]\n        for dataset_idx, dataset in enumerate(datasets):\n            dataset_marker = os.path.join(folder, dataset, '.dataset')\n            if os.path.exists(dataset_marker):\n                print(f'{datetime.now()}-{folder}/{dataset} already processed, skipping')\n                continue\n            filename_data = os.path.join(folder, dataset, 'xyzrgb.npy')\n            print(f'{datetime.now()}-Loading {filename_data}...')\n            # Modified according to PointNet convensions.\n            xyzrgb = np.load(filename_data)\n            xyzrgb[:, 0:3] -= np.amin(xyzrgb, axis=0)[0:3]\n\n            filename_labels = os.path.join(folder, dataset, 'label.npy')\n            print(f'{datetime.now()}-Loading {filename_labels}...')\n            labels = np.load(filename_labels).astype(int).flatten()\n\n            xyz, rgb = np.split(xyzrgb, [3], axis=-1)\n            xyz_min = np.amin(xyz, axis=0, keepdims=True)\n            xyz_max = np.amax(xyz, axis=0, keepdims=True)\n            xyz_center = (xyz_min + xyz_max) / 2\n            xyz_center[0][-1] = xyz_min[0][-1]\n            # Remark: Don't do global alignment.\n            # xyz = xyz - xyz_center\n            rgb = rgb / 255.0\n            max_room_x = np.max(xyz[:, 0])\n            max_room_y = np.max(xyz[:, 1])\n            max_room_z = np.max(xyz[:, 2])\n\n            offsets = [('zero', 0.0), ('half', args.block_size / 2)]\n            for offset_name, offset in offsets:\n                idx_h5 = 0\n                idx = 0\n\n                print(f'{datetime.now()}-Computing block id of {xyzrgb.shape[0]} points...')\n                xyz_min = np.amin(xyz, axis=0, keepdims=True) - offset\n                xyz_max = np.amax(xyz, axis=0, keepdims=True)\n                block_size = (args.block_size, args.block_size, 2 * (xyz_max[0, -1] - xyz_min[0, -1]))\n                # Note: Don't split over z axis.\n                xyz_blocks = np.floor((xyz - xyz_min) / block_size).astype(np.int)\n\n                print(f'{datetime.now()}-Collecting points belong to each block...')\n                blocks, point_block_indices, block_point_counts = np.unique(xyz_blocks, return_inverse=True,\n                                                                            return_counts=True, axis=0)\n                block_point_indices = np.split(np.argsort(point_block_indices), np.cumsum(block_point_counts[:-1]))\n                print(f'{datetime.now()}-{dataset} is split into {blocks.shape[0]} blocks.')\n\n                block_to_block_idx_map = dict()\n                for block_idx in range(blocks.shape[0]):\n                    block = (blocks[block_idx][0], blocks[block_idx][1])\n                    block_to_block_idx_map[(block[0], block[1])] = block_idx\n\n                # merge small blocks into one of their big neighbors\n                block_point_count_threshold = max_num_points/10\n                nbr_block_offsets = [(0, 1), (1, 0), (0, -1), (-1, 0), (-1, 1), (1, 1), (1, -1), (-1, -1)]\n                block_merge_count = 0\n                for block_idx in range(blocks.shape[0]):\n                    if block_point_counts[block_idx] >= block_point_count_threshold:\n                        continue\n\n                    block = (blocks[block_idx][0], blocks[block_idx][1])\n                    for x, y in nbr_block_offsets:\n                        nbr_block = (block[0] + x, block[1] + y)\n                        if nbr_block not in block_to_block_idx_map:\n                            continue\n\n                        nbr_block_idx = block_to_block_idx_map[nbr_block]\n                        if block_point_counts[nbr_block_idx] < block_point_count_threshold:\n                            continue\n\n                        block_point_indices[nbr_block_idx] = np.concatenate(\n                            [block_point_indices[nbr_block_idx], block_point_indices[block_idx]], axis=-1)\n                        block_point_indices[block_idx] = np.array([], dtype=np.int)\n                        block_merge_count = block_merge_count + 1\n                        break\n                print(f'{datetime.now()}-{block_merge_count} of {blocks.shape[0]} blocks are merged.')\n\n                idx_last_non_empty_block = 0\n                for block_idx in reversed(range(blocks.shape[0])):\n                    if block_point_indices[block_idx].shape[0] != 0:\n                        idx_last_non_empty_block = block_idx\n                        break\n\n                # uniformly sample each block\n                for block_idx in range(idx_last_non_empty_block + 1):\n                    point_indices = block_point_indices[block_idx]\n                    if point_indices.shape[0] == 0:\n                        continue\n                    block_points = xyz[point_indices]\n                    block_min = np.amin(block_points, axis=0, keepdims=True)\n                    xyz_grids = np.floor((block_points - block_min) / args.grid_size).astype(np.int)\n                    grids, point_grid_indices, grid_point_counts = np.unique(xyz_grids, return_inverse=True,\n                                                                             return_counts=True, axis=0)\n                    grid_point_indices = np.split(np.argsort(point_grid_indices), np.cumsum(grid_point_counts[:-1]))\n                    grid_point_count_avg = int(np.average(grid_point_counts))\n                    point_indices_repeated = []\n                    for grid_idx in range(grids.shape[0]):\n                        point_indices_in_block = grid_point_indices[grid_idx]\n                        repeat_num = math.ceil(grid_point_count_avg / point_indices_in_block.shape[0])\n                        if repeat_num > 1:\n                            point_indices_in_block = np.repeat(point_indices_in_block, repeat_num)\n                            np.random.shuffle(point_indices_in_block)\n                            point_indices_in_block = point_indices_in_block[:grid_point_count_avg]\n                        point_indices_repeated.extend(list(point_indices[point_indices_in_block]))\n                    block_point_indices[block_idx] = np.array(point_indices_repeated)\n                    block_point_counts[block_idx] = len(point_indices_repeated)\n\n                for block_idx in range(idx_last_non_empty_block + 1):\n                    point_indices = block_point_indices[block_idx]\n                    if point_indices.shape[0] == 0:\n                        continue\n\n                    block_point_num = point_indices.shape[0]\n                    block_split_num = int(math.ceil(block_point_num * 1.0 / max_num_points))\n                    point_num_avg = int(math.ceil(block_point_num * 1.0 / block_split_num))\n                    point_nums = [point_num_avg] * block_split_num\n                    point_nums[-1] = block_point_num - (point_num_avg * (block_split_num - 1))\n                    starts = [0] + list(np.cumsum(point_nums))\n\n                    # Modified following convensions of PointNet.\n                    np.random.shuffle(point_indices)\n                    block_points = xyz[point_indices]\n                    block_rgb = rgb[point_indices]\n                    block_labels = labels[point_indices]\n                    x, y, z = np.split(block_points, (1, 2), axis=-1)\n                    norm_x = x / max_room_x\n                    norm_y = y / max_room_y\n                    norm_z = z / max_room_z\n                    \n                    minx = np.min(x)\n                    miny = np.min(y)\n                    x = x - (minx + args.block_size / 2)\n                    y = y - (miny + args.block_size / 2)\n                    \n                    block_xyzrgb = np.concatenate([x, y, z, block_rgb, norm_x, norm_y, norm_z], axis=-1)\n                    for block_split_idx in range(block_split_num):\n                        start = starts[block_split_idx]\n                        point_num = point_nums[block_split_idx]\n                        end = start + point_num\n                        idx_in_batch = idx % batch_size\n                        data[idx_in_batch, 0:point_num, ...] = block_xyzrgb[start:end, :]\n                        data_num[idx_in_batch] = point_num\n                        label[idx_in_batch] = dataset_idx  # won't be used...\n                        label_seg[idx_in_batch, 0:point_num] = block_labels[start:end]\n                        indices_split_to_full[idx_in_batch, 0:point_num] = point_indices[start:end]\n\n                        if ((idx + 1) % batch_size == 0) or \\\n                                (block_idx == idx_last_non_empty_block and block_split_idx == block_split_num - 1):\n                            item_num = idx_in_batch + 1\n                            filename_h5 = os.path.join(folder, dataset, f'{offset_name}_{idx_h5:d}.h5')\n                            print(f'{datetime.now()}-Saving {filename_h5}...')\n\n                            file = h5py.File(filename_h5, 'w')\n                            file.create_dataset('data', data=data[0:item_num, ...])\n                            file.create_dataset('data_num', data=data_num[0:item_num, ...])\n                            file.create_dataset('label', data=label[0:item_num, ...])\n                            file.create_dataset('label_seg', data=label_seg[0:item_num, ...])\n                            file.create_dataset('indices_split_to_full', data=indices_split_to_full[0:item_num, ...])\n                            file.close()\n\n                            if args.save_ply:\n                                print(f'{datetime.now()}-Saving ply of {filename_h5}...')\n                                filepath_label_ply = os.path.join(folder, dataset, 'ply_label',\n                                                                  f'label_{offset_name}_{idx_h5:d}')\n                                save_ply_property_batch(data[0:item_num, :, 0:3], label_seg[0:item_num, ...],\n                                                        filepath_label_ply, data_num[0:item_num, ...], 14)\n\n                                filepath_rgb_ply = os.path.join(folder, dataset, 'ply_rgb',\n                                                                f'rgb_{offset_name}_{idx_h5:d}')\n                                save_ply_color_batch(data[0:item_num, :, 0:3], data[0:item_num, :, 3:6] * 255,\n                                                     filepath_rgb_ply, data_num[0:item_num, ...])\n\n                            idx_h5 = idx_h5 + 1\n                        idx = idx + 1\n\n            # Marker indicating we've processed this dataset\n            open(dataset_marker, 'w').close()\n    print(f'{datetime.now()}-Done.')\n\n\ndef save_ply(points, filename, colors=None, normals=None):\n    vertex = np.core.records.fromarrays(points.transpose(), names='x, y, z', formats='f4, f4, f4')\n    n = len(vertex)\n    desc = vertex.dtype.descr\n\n    if normals is not None:\n        vertex_normal = np.core.records.fromarrays(normals.transpose(), names='nx, ny, nz', formats='f4, f4, f4')\n        assert len(vertex_normal) == n\n        desc = desc + vertex_normal.dtype.descr\n\n    if colors is not None:\n        vertex_color = np.core.records.fromarrays(colors.transpose() * 255, names='red, green, blue',\n                                                  formats='u1, u1, u1')\n        assert len(vertex_color) == n\n        desc = desc + vertex_color.dtype.descr\n\n    vertex_all = np.empty(n, dtype=desc)\n\n    for prop in vertex.dtype.names:\n        vertex_all[prop] = vertex[prop]\n\n    if normals is not None:\n        for prop in vertex_normal.dtype.names:\n            vertex_all[prop] = vertex_normal[prop]\n\n    if colors is not None:\n        for prop in vertex_color.dtype.names:\n            vertex_all[prop] = vertex_color[prop]\n\n    ply = plyfile.PlyData([plyfile.PlyElement.describe(vertex_all, 'vertex')], text=False)\n    if not os.path.exists(os.path.dirname(filename)):\n        os.makedirs(os.path.dirname(filename))\n    ply.write(filename)\n\n\ndef save_ply_property(points, property, property_max, filename, cmap_name='tab20'):\n    point_num = points.shape[0]\n    colors = np.full(points.shape, 0.5)\n    cmap = cm.get_cmap(cmap_name)\n    for point_idx in range(point_num):\n        if property[point_idx] == 0:\n            colors[point_idx] = np.array([0, 0, 0])\n        else:\n            colors[point_idx] = cmap(property[point_idx] / property_max)[:3]\n    save_ply(points, filename, colors)\n\n\ndef save_ply_color_batch(points_batch, colors_batch, file_path, points_num=None):\n    batch_size = points_batch.shape[0]\n    if not isinstance(file_path, (list, tuple)):\n        basename = os.path.splitext(file_path)[0]\n        ext = '.ply'\n    for batch_idx in range(batch_size):\n        point_num = points_batch.shape[1] if points_num is None else points_num[batch_idx]\n        if isinstance(file_path, (list, tuple)):\n            save_ply(points_batch[batch_idx][:point_num], file_path[batch_idx], colors_batch[batch_idx][:point_num])\n        else:\n            save_ply(points_batch[batch_idx][:point_num], f'{basename}_{batch_idx:04d}{ext}',\n                     colors_batch[batch_idx][:point_num])\n\n\ndef save_ply_property_batch(points_batch, property_batch, file_path, points_num=None, property_max=None,\n                            cmap_name='tab20'):\n    batch_size = points_batch.shape[0]\n    if not isinstance(file_path, (list, tuple)):\n        basename = os.path.splitext(file_path)[0]\n        ext = '.ply'\n    property_max = np.max(property_batch) if property_max is None else property_max\n    for batch_idx in range(batch_size):\n        point_num = points_batch.shape[1] if points_num is None else points_num[batch_idx]\n        if isinstance(file_path, (list, tuple)):\n            save_ply_property(points_batch[batch_idx][:point_num], property_batch[batch_idx][:point_num],\n                              property_max, file_path[batch_idx], cmap_name)\n        else:\n            save_ply_property(points_batch[batch_idx][:point_num], property_batch[batch_idx][:point_num],\n                              property_max, f'{basename}_{batch_idx:04d}{ext}', cmap_name)\n\n\nif __name__ == '__main__':\n    main()\n"""
datasets/kitti/__init__.py,0,b'from datasets.kitti.frustum import FrustumKitti\n'
datasets/kitti/attributes.py,0,"b""import numpy as np\n\nfrom utils.container import G\n\n__all__ = ['kitti_attributes']\n\n\nkitti_attributes = G()\nkitti_attributes.class_names = ('Car', 'Van', 'Truck', 'Pedestrian', 'Person_sitting', 'Cyclist', 'Tram', 'Misc')\n# ref: https://github.com/charlesq34/frustum-pointnets/blob/master/models/model_util.py\nkitti_attributes.class_name_to_size_template = {\n    'Car': np.array([3.88311640418, 1.62856739989, 1.52563191462]),\n    'Van': np.array([5.06763659, 1.9007158, 2.20532825]),\n    'Truck': np.array([10.13586957, 2.58549199, 3.2520595]),\n    'Pedestrian': np.array([0.84422524, 0.66068622, 1.76255119]),\n    'Person_sitting': np.array([0.80057803, 0.5983815, 1.27450867]),\n    'Cyclist': np.array([1.76282397, 0.59706367, 1.73698127]),\n    'Tram': np.array([16.17150617, 2.53246914, 3.53079012]),\n    'Misc': np.array([3.64300781, 1.54298177, 1.92320313])\n}\n"""
datasets/kitti/frustum.py,1,"b'import os\nimport pickle\n\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom datasets.kitti.attributes import kitti_attributes as kitti\nfrom utils.container import G\n\n\nclass FrustumKitti(dict):\n    def __init__(self, root, num_points, split=None, classes=(\'Car\', \'Pedestrian\', \'Cyclist\'),\n                 num_heading_angle_bins=12, class_name_to_size_template_id=None,\n                 from_rgb_detection=False, random_flip=False, random_shift=False, frustum_rotate=False):\n        super().__init__()\n        if class_name_to_size_template_id is None:\n            class_name_to_size_template_id = {cat: cls for cls, cat in enumerate(kitti.class_names)}\n        if not isinstance(split, (list, tuple)):\n            if split is None:\n                split = [\'train\', \'val\']\n            else:\n                split = [split]\n        if \'train\' in split:\n            self[\'train\'] = _FrustumKittiDataset(\n                root=root, num_points=num_points, split=\'train\', classes=classes,\n                num_heading_angle_bins=num_heading_angle_bins,\n                class_name_to_size_template_id=class_name_to_size_template_id,\n                random_flip=random_flip, random_shift=random_shift, frustum_rotate=frustum_rotate)\n        if \'val\' in split:\n            self[\'val\'] = _FrustumKittiDataset(\n                root=root, num_points=num_points, split=\'val\', classes=classes,\n                num_heading_angle_bins=num_heading_angle_bins,\n                class_name_to_size_template_id=class_name_to_size_template_id,\n                random_flip=False, random_shift=False, frustum_rotate=frustum_rotate,\n                from_rgb_detection=from_rgb_detection)\n\n\nclass _FrustumKittiDataset(Dataset):\n    def __init__(self, root, num_points, split, classes, num_heading_angle_bins, class_name_to_size_template_id,\n                 from_rgb_detection=False, random_flip=False, random_shift=False, frustum_rotate=False):\n        """"""\n        Frustum Kitti Dataset\n        :param root: directory path to kitti prepared dataset\n        :param num_points: number of points to process for each scene\n        :param split: \'train\' or \'test\'\n        :param classes: tuple of classes names\n        :param num_heading_angle_bins: #heading angle bins, int\n        :param class_name_to_size_template_id: dict\n        :param from_rgb_detection: bool, if True we assume we do not have groundtruth, just return data elements.\n        :param random_flip: bool, in 50% randomly flip the point cloud in left and right (after the frustum rotation)\n        :param random_shift: bool, if True randomly shift the point cloud back and forth by a random distance\n        :param frustum_rotate: bool, whether to do frustum rotation\n        """"""\n        assert split in [\'train\', \'val\']\n        self.root = root\n        self.split = split\n        self.classes = classes\n        self.num_classes = len(classes)\n        self.class_name_to_class_id = {cat: cls for cls, cat in enumerate(self.classes)}\n        self.num_heading_angle_bins = num_heading_angle_bins\n        self.class_name_to_size_template_id = class_name_to_size_template_id\n\n        self.num_points = num_points\n        self.random_flip = random_flip\n        self.random_shift = random_shift\n        self.frustum_rotate = frustum_rotate\n        self.from_rgb_detection = from_rgb_detection\n        self.data = G()\n\n        if self.from_rgb_detection:\n            with open(os.path.join(self.root, f\'frustum_carpedcyc_{split}_rgb_detection.pickle\'), \'rb\') as fp:\n                self.data.ids = pickle.load(fp)\n                self.data.boxes_2d = pickle.load(fp, encoding=\'latin1\')\n                self.data.point_clouds = pickle.load(fp, encoding=\'latin1\')\n                self.data.class_names = pickle.load(fp, encoding=\'latin1\')\n                # frustum_angle is clockwise angle from positive x-axis\n                self.data.frustum_rotation_angles = pickle.load(fp, encoding=\'latin1\')\n                self.data.probs = pickle.load(fp, encoding=\'latin1\')\n        else:\n            with open(os.path.join(self.root, f\'frustum_carpedcyc_{split}.pickle\'), \'rb\') as fp:\n                self.data.ids = pickle.load(fp)\n                self.data.boxes_2d = pickle.load(fp, encoding=\'latin1\')\n                self.data.boxes_3d = pickle.load(fp, encoding=\'latin1\')\n                self.data.point_clouds = pickle.load(fp, encoding=\'latin1\')\n                self.data.mask_logits = pickle.load(fp, encoding=\'latin1\')\n                self.data.class_names = pickle.load(fp, encoding=\'latin1\')\n                self.data.heading_angles = pickle.load(fp, encoding=\'latin1\')\n                self.data.sizes = pickle.load(fp, encoding=\'latin1\')\n                # frustum_angle is clockwise angle from positive x-axis\n                self.data.frustum_rotation_angles = pickle.load(fp, encoding=\'latin1\')\n\n    def __len__(self):\n        return len(self.data.point_clouds)\n\n    def __getitem__(self, index):\n        # frustum rotation angle is from x clockwise to z\n        # rotation angle is from z clockwise to x\n        # frustum rotation angle shifted by pi/2 so that it can be directly used to adjust ground truth heading angle\n        rotation_angle = np.pi / 2.0 + self.data.frustum_rotation_angles[index]\n\n        # Compute one hot vector\n        class_name = self.data.class_names[index]\n        one_hot_vector = np.zeros(self.num_classes)\n        one_hot_vector[self.class_name_to_class_id[class_name]] = 1\n        one_hot_vector = one_hot_vector.astype(np.float32)\n\n        # Get point cloud\n        point_cloud = self.data.point_clouds[index]\n        if self.frustum_rotate:\n            # Use np.copy to avoid corrupting original data\n            point_cloud = self.rotate_points_along_y(np.copy(point_cloud), rotation_angle)\n        choice = np.random.choice(point_cloud.shape[0], self.num_points, replace=True)\n        point_cloud = point_cloud[choice, :]\n\n        if self.from_rgb_detection:\n            return {\'features\': point_cloud.astype(np.float32).T, \'one_hot_vectors\': one_hot_vector}, \\\n                   {\'rotation_angle\': rotation_angle.astype(np.float32), \'rgb_score\': self.data.probs[index]}\n\n        mask_logits = self.data.mask_logits[index][choice]\n        center = (self.data.boxes_3d[index][0, :] + self.data.boxes_3d[index][6, :]) / 2.0\n        heading_angle = self.data.heading_angles[index]\n        size_template_id = self.class_name_to_size_template_id[class_name]\n        size_residual = self.data.sizes[index] - kitti.class_name_to_size_template[class_name]\n        if self.frustum_rotate:\n            center = self.rotate_points_along_y(np.expand_dims(center, 0), rotation_angle).squeeze()\n            heading_angle -= rotation_angle\n\n        # Data Augmentation\n        if self.random_flip:\n            # note: rotation_angle won\'t be correct if we have random_flip so do not use it in case of random flipping\n            if np.random.random() > 0.5:  # 50% chance flipping\n                point_cloud[:, 0] = -point_cloud[:, 0]\n                center[0] = -center[0]\n                heading_angle = np.pi - heading_angle\n        if self.random_shift:\n            dist = np.sqrt(np.sum(center[0] ** 2 + center[1] ** 2))\n            shift = np.clip(np.random.randn() * dist * 0.05, dist * 0.8, dist * 1.2)\n            point_cloud[:, 2] += shift\n            center[2] += shift\n\n        heading_bin_id, heading_residual = self.angle_to_bin_id(heading_angle, self.num_heading_angle_bins)\n\n        return {\'features\': point_cloud.astype(np.float32).T, \'one_hot_vectors\': one_hot_vector},\\\n               {\'mask_logits\': mask_logits.astype(np.int64), \'center\': center.astype(np.float32),\n                \'heading_bin_id\': heading_bin_id,  \'heading_residual\': np.array(heading_residual, dtype=np.float32),\n                \'size_template_id\': size_template_id, \'size_residual\': size_residual.astype(np.float32),\n                \'class_id\': self.class_name_to_class_id[class_name]}\n\n    @staticmethod\n    def rotate_points_along_y(features, rotation_angle):\n        """"""\n        (https://github.com/charlesq34/frustum-pointnets/blob/master/sunrgbd/sunrgbd_detection/roi_seg_box3d_dataset.py)\n        :param features: numpy array (N,C), first 3 channels are XYZ-coords\n                         z is facing forward, x is left ward, y is downward\n        :param rotation_angle: float, from z to x axis, unit: rad (rotate axis from z to x = rotate coords from x to z)\n        :return:\n            features: numpy array (N, C) with [0, 2] rotated\n        """"""\n        v_cos = np.cos(rotation_angle)\n        v_sin = np.sin(rotation_angle)\n        # rotation_matrix = np.array([[v_cos, -v_sin], [v_sin, v_cos]])\n        rotation_matrix_transpose = [[v_cos, v_sin], [-v_sin, v_cos]]\n        features[:, [0, 2]] = np.dot(features[:, [0, 2]], rotation_matrix_transpose)\n        return features\n\n    @staticmethod\n    def angle_to_bin_id(angle, num_angle_bins):\n        """"""\n        (https://github.com/charlesq34/frustum-pointnets/blob/master/sunrgbd/sunrgbd_detection/roi_seg_box3d_dataset.py)\n        Convert continuous angle to discrete bin and residual.\n        :param angle: float, unit: rad\n        :param num_angle_bins: int, #angle bins\n        :return:\n            bin_id: int, bin id\n            angle_residual: float, bin_id * (2pi/N) + angle_residual = angle\n        """"""\n        angle = angle % (2 * np.pi)\n        assert 0 <= angle <= 2 * np.pi\n        angle_per_bin = 2 * np.pi / float(num_angle_bins)\n        shifted_angle = (angle + angle_per_bin / 2) % (2 * np.pi)\n        bin_id = int(shifted_angle / angle_per_bin)\n        angle_residual = shifted_angle - (bin_id * angle_per_bin + angle_per_bin / 2)\n        return bin_id, angle_residual\n'"
evaluate/kitti/__init__.py,0,b''
evaluate/s3dis/__init__.py,0,b'\n'
evaluate/s3dis/eval.py,8,"b'import argparse\nimport os\nimport random\nimport sys\n\nimport numba\nimport numpy as np\n\nsys.path.append(os.getcwd())\n\n__all__ = [\'evaluate\']\n\n\ndef prepare():\n    from utils.common import get_save_path\n    from utils.config import configs\n    from utils.device import set_cuda_visible_devices\n\n    # since PyTorch jams device selection, we have to parse args before import torch (issue #26790)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'configs\', nargs=\'+\')\n    parser.add_argument(\'--devices\', default=None)\n    args, opts = parser.parse_known_args()\n    if args.devices is not None and args.devices != \'cpu\':\n        gpus = set_cuda_visible_devices(args.devices)\n    else:\n        gpus = []\n\n    print(f\'==> loading configs from {args.configs}\')\n    configs.update_from_modules(*args.configs)\n    # define save path\n    save_path = get_save_path(*args.configs, prefix=\'runs\')\n    os.makedirs(save_path, exist_ok=True)\n    configs.train.save_path = save_path\n\n    # override configs with args\n    configs.update_from_arguments(*opts)\n    if len(gpus) == 0:\n        configs.device = \'cpu\'\n        configs.device_ids = []\n    else:\n        configs.device = \'cuda\'\n        configs.device_ids = gpus\n    configs.dataset.split = configs.evaluate.dataset.split\n    if \'best_checkpoint_path\' not in configs.evaluate or configs.evaluate.best_checkpoint_path is None:\n        if \'best_checkpoint_path\' in configs.train and configs.train.best_checkpoint_path is not None:\n            configs.evaluate.best_checkpoint_path = configs.train.best_checkpoint_path\n        else:\n            configs.evaluate.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n    assert configs.evaluate.best_checkpoint_path.endswith(\'.pth.tar\')\n    configs.evaluate.stats_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.eval.npy\')\n\n    return configs\n\n\ndef evaluate(configs=None):\n    configs = prepare() if configs is None else configs\n\n    import h5py\n    import math\n    import torch\n    import torch.backends.cudnn as cudnn\n    import torch.nn.functional as F\n    from tqdm import tqdm\n\n    #####################\n    # Kernel Definition #\n    #####################\n\n    def print_stats(stats):\n        stats = stats.sum(axis=-1)\n        iou = stats[2] / (stats[0] + stats[1] - stats[2])\n        print(\'classes: {}\'.format(\'  \'.join(map(\'{:>8d}\'.format, stats[0].astype(np.int64)))))\n        print(\'positiv: {}\'.format(\'  \'.join(map(\'{:>8d}\'.format, stats[1].astype(np.int64)))))\n        print(\'truepos: {}\'.format(\'  \'.join(map(\'{:>8d}\'.format, stats[2].astype(np.int64)))))\n        print(\'clssiou: {}\'.format(\'  \'.join(map(\'{:>8.2f}\'.format, iou * 100))))\n        print(\'meanAcc: {:4.2f}\'.format(stats[2].sum() / stats[1].sum() * 100))\n        print(\'meanIoU: {:4.2f}\'.format(iou.mean() * 100))\n\n    ###########\n    # Prepare #\n    ###########\n\n    if configs.device == \'cuda\':\n        cudnn.benchmark = True\n        if configs.get(\'deterministic\', False):\n            cudnn.deterministic = True\n            cudnn.benchmark = False\n    if (\'seed\' not in configs) or (configs.seed is None):\n        configs.seed = torch.initial_seed() % (2 ** 32 - 1)\n    seed = configs.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    print(configs)\n\n    if os.path.exists(configs.evaluate.stats_path):\n        stats = np.load(configs.evaluate.stats_path)\n        print_stats(stats)\n        return\n\n    #################################\n    # Initialize DataLoaders, Model #\n    #################################\n\n    print(f\'\\n==> loading dataset ""{configs.dataset}""\')\n    dataset = configs.dataset()[configs.dataset.split]\n\n    print(f\'\\n==> creating model ""{configs.model}""\')\n    model = configs.model()\n    if configs.device == \'cuda\':\n        model = torch.nn.DataParallel(model)\n    model = model.to(configs.device)\n\n    if os.path.exists(configs.evaluate.best_checkpoint_path):\n        print(f\'==> loading checkpoint ""{configs.evaluate.best_checkpoint_path}""\')\n        checkpoint = torch.load(configs.evaluate.best_checkpoint_path)\n        model.load_state_dict(checkpoint.pop(\'model\'))\n        del checkpoint\n    else:\n        return\n\n    model.eval()\n\n    ##############\n    # Evaluation #\n    ##############\n\n    total_num_scenes = len(dataset.scene_list)\n    stats = np.zeros((3, configs.data.num_classes, total_num_scenes))\n\n    for scene_index, (scene, scene_files) in enumerate(tqdm(dataset.scene_list.items(), desc=\'eval\', ncols=0)):\n        ground_truth = np.load(os.path.join(scene, \'label.npy\')).reshape(-1)\n        total_num_points_in_scene = ground_truth.shape[0]\n        confidences = np.zeros(total_num_points_in_scene, dtype=np.float32)\n        predictions = np.full(total_num_points_in_scene, -1, dtype=np.int64)\n\n        for filename in scene_files:\n            h5f = h5py.File(filename, \'r\')\n            scene_data = h5f[\'data\'][...].astype(np.float32)\n            scene_num_points = h5f[\'data_num\'][...].astype(np.int64)\n            window_to_scene_mapping = h5f[\'indices_split_to_full\'][...].astype(np.int64)\n\n            num_windows, max_num_points_per_window, num_channels = scene_data.shape\n            extra_batch_size = configs.evaluate.num_votes * math.ceil(max_num_points_per_window / dataset.num_points)\n            total_num_voted_points = extra_batch_size * dataset.num_points\n\n            for min_window_index in range(0, num_windows, configs.evaluate.batch_size):\n                max_window_index = min(min_window_index + configs.evaluate.batch_size, num_windows)\n                batch_size = max_window_index - min_window_index\n                window_data = scene_data[np.arange(min_window_index, max_window_index)]\n                window_data = window_data.reshape(batch_size, -1, num_channels)\n\n                # repeat, shuffle and tile\n                # TODO: speedup here\n                batched_inputs = np.zeros((batch_size, total_num_voted_points, num_channels), dtype=np.float32)\n                batched_shuffled_point_indices = np.zeros((batch_size, total_num_voted_points), dtype=np.int64)\n                for relative_window_index in range(batch_size):\n                    num_points_in_window = scene_num_points[relative_window_index + min_window_index]\n                    num_repeats = math.ceil(total_num_voted_points / num_points_in_window)\n                    shuffled_point_indices = np.tile(np.arange(num_points_in_window), num_repeats)\n                    shuffled_point_indices = shuffled_point_indices[:total_num_voted_points]\n                    np.random.shuffle(shuffled_point_indices)\n                    batched_shuffled_point_indices[relative_window_index] = shuffled_point_indices\n                    batched_inputs[relative_window_index] = window_data[relative_window_index][shuffled_point_indices]\n\n                # model inference\n                inputs = torch.from_numpy(\n                    batched_inputs.reshape((batch_size * extra_batch_size, dataset.num_points, -1)).transpose(0, 2, 1)\n                ).float().to(configs.device)\n                with torch.no_grad():\n                    batched_confidences, batched_predictions = F.softmax(model(inputs), dim=1).max(dim=1)\n                    batched_confidences = batched_confidences.view(batch_size, total_num_voted_points).cpu().numpy()\n                    batched_predictions = batched_predictions.view(batch_size, total_num_voted_points).cpu().numpy()\n\n                update_scene_predictions(batched_confidences, batched_predictions, batched_shuffled_point_indices,\n                                         confidences, predictions, window_to_scene_mapping,\n                                         total_num_voted_points, batch_size, min_window_index)\n\n        # update stats\n        update_stats(stats, ground_truth, predictions, scene_index, total_num_points_in_scene)\n\n    np.save(configs.evaluate.stats_path, stats)\n    print_stats(stats)\n\n\n@numba.jit()\ndef update_scene_predictions(batched_confidences, batched_predictions, batched_shuffled_point_indices,\n                             scene_confidences, scene_predictions, window_to_scene_mapping, total_num_voted_points,\n                             batch_size, min_window_index):\n    for b in range(batch_size):\n        window_index = min_window_index + b\n        current_window_mapping = window_to_scene_mapping[window_index]\n        current_shuffled_point_indices = batched_shuffled_point_indices[b]\n        current_confidences = batched_confidences[b]\n        current_predictions = batched_predictions[b]\n        for p in range(total_num_voted_points):\n            point_index = current_window_mapping[current_shuffled_point_indices[p]]\n            current_confidence = current_confidences[p]\n            if current_confidence > scene_confidences[point_index]:\n                scene_confidences[point_index] = current_confidence\n                scene_predictions[point_index] = current_predictions[p]\n\n\n@numba.jit()\ndef update_stats(stats, ground_truth, predictions, scene_index, total_num_points_in_scene):\n    for p in range(total_num_points_in_scene):\n        gt = int(ground_truth[p])\n        pd = int(predictions[p])\n        stats[0, gt, scene_index] += 1\n        stats[1, pd, scene_index] += 1\n        if gt == pd:\n            stats[2, gt, scene_index] += 1\n\n\nif __name__ == \'__main__\':\n    evaluate()\n'"
evaluate/shapenet/__init__.py,0,b''
evaluate/shapenet/eval.py,8,"b'import argparse\nimport os\nimport random\nimport sys\n\nimport numba\nimport numpy as np\n\nsys.path.append(os.getcwd())\n\n__all__ = [\'evaluate\']\n\n\ndef prepare():\n    from utils.common import get_save_path\n    from utils.config import configs\n    from utils.device import set_cuda_visible_devices\n\n    # since PyTorch jams device selection, we have to parse args before import torch (issue #26790)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'configs\', nargs=\'+\')\n    parser.add_argument(\'--devices\', default=None)\n    args, opts = parser.parse_known_args()\n    if args.devices is not None and args.devices != \'cpu\':\n        gpus = set_cuda_visible_devices(args.devices)\n    else:\n        gpus = []\n\n    print(f\'==> loading configs from {args.configs}\')\n    configs.update_from_modules(*args.configs)\n    # define save path\n    save_path = get_save_path(*args.configs, prefix=\'runs\')\n    os.makedirs(save_path, exist_ok=True)\n    configs.train.save_path = save_path\n    configs.train.checkpoint_path = os.path.join(save_path, \'latest.pth.tar\')\n    configs.train.best_checkpoint_path = os.path.join(save_path, \'best.pth.tar\')\n\n    # override configs with args\n    configs.update_from_arguments(*opts)\n    if len(gpus) == 0:\n        configs.device = \'cpu\'\n        configs.device_ids = []\n    else:\n        configs.device = \'cuda\'\n        configs.device_ids = gpus\n    configs.dataset.split = configs.evaluate.dataset.split\n    if \'best_checkpoint_path\' not in configs.evaluate or configs.evaluate.best_checkpoint_path is None:\n        if \'best_checkpoint_path\' in configs.train and configs.train.best_checkpoint_path is not None:\n            configs.evaluate.best_checkpoint_path = configs.train.best_checkpoint_path\n        else:\n            configs.evaluate.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n    assert configs.evaluate.best_checkpoint_path.endswith(\'.pth.tar\')\n    configs.evaluate.stats_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.eval.npy\')\n\n    return configs\n\n\ndef evaluate(configs=None):\n    configs = prepare() if configs is None else configs\n\n    import math\n    import torch\n    import torch.backends.cudnn as cudnn\n    import torch.nn.functional as F\n    from tqdm import tqdm\n\n    from meters.shapenet import MeterShapeNet\n\n    ###########\n    # Prepare #\n    ###########\n\n    if configs.device == \'cuda\':\n        cudnn.benchmark = True\n        if configs.get(\'deterministic\', False):\n            cudnn.deterministic = True\n            cudnn.benchmark = False\n    if (\'seed\' not in configs) or (configs.seed is None):\n        configs.seed = torch.initial_seed() % (2 ** 32 - 1)\n    seed = configs.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n\n    print(configs)\n\n    if os.path.exists(configs.evaluate.stats_path):\n        stats = np.load(configs.evaluate.stats_path)\n        print(\'clssIoU: {}\'.format(\'  \'.join(map(\'{:>8.2f}\'.format, stats[:, 0] / stats[:, 1] * 100))))\n        print(\'meanIoU: {:4.2f}\'.format(stats[:, 0].sum() / stats[:, 1].sum() * 100))\n        return\n\n    #################################\n    # Initialize DataLoaders, Model #\n    #################################\n\n    print(f\'\\n==> loading dataset ""{configs.dataset}""\')\n    dataset = configs.dataset()[configs.dataset.split]\n    meter = MeterShapeNet()\n\n    print(f\'\\n==> creating model ""{configs.model}""\')\n    model = configs.model()\n    if configs.device == \'cuda\':\n        model = torch.nn.DataParallel(model)\n    model = model.to(configs.device)\n\n    if os.path.exists(configs.evaluate.best_checkpoint_path):\n        print(f\'==> loading checkpoint ""{configs.evaluate.best_checkpoint_path}""\')\n        checkpoint = torch.load(configs.evaluate.best_checkpoint_path)\n        model.load_state_dict(checkpoint.pop(\'model\'))\n        del checkpoint\n    else:\n        return\n\n    model.eval()\n\n    ##############\n    # Evaluation #\n    ##############\n\n    stats = np.zeros((configs.data.num_shapes, 2))\n\n    for shape_index, (file_path, shape_id) in enumerate(tqdm(dataset.file_paths, desc=\'eval\', ncols=0)):\n        data = np.loadtxt(file_path).astype(np.float32)\n        total_num_points_in_shape = data.shape[0]\n        confidences = np.zeros(total_num_points_in_shape, dtype=np.float32)\n        predictions = np.full(total_num_points_in_shape, -1, dtype=np.int64)\n\n        coords = data[:, :3]\n        if dataset.normalize:\n            coords = dataset.normalize_point_cloud(coords)\n        coords = coords.transpose()\n        ground_truth = data[:, -1].astype(np.int64)\n        if dataset.with_normal:\n            normal = data[:, 3:6].transpose()\n            if dataset.with_one_hot_shape_id:\n                shape_one_hot = np.zeros((dataset.num_shapes, coords.shape[-1]), dtype=np.float32)\n                shape_one_hot[shape_id, :] = 1.0\n                point_set = np.concatenate([coords, normal, shape_one_hot])\n            else:\n                point_set = np.concatenate([coords, normal])\n        else:\n            if dataset.with_one_hot_shape_id:\n                shape_one_hot = np.zeros((dataset.num_shapes, coords.shape[-1]), dtype=np.float32)\n                shape_one_hot[shape_id, :] = 1.0\n                point_set = np.concatenate([coords, shape_one_hot])\n            else:\n                point_set = coords\n        extra_batch_size = configs.evaluate.num_votes * math.ceil(total_num_points_in_shape / dataset.num_points)\n        total_num_voted_points = extra_batch_size * dataset.num_points\n        num_repeats = math.ceil(total_num_voted_points / total_num_points_in_shape)\n        shuffled_point_indices = np.tile(np.arange(total_num_points_in_shape), num_repeats)\n        shuffled_point_indices = shuffled_point_indices[:total_num_voted_points]\n        np.random.shuffle(shuffled_point_indices)\n        start_class, end_class = meter.part_class_to_shape_part_classes[ground_truth[0]]\n\n        # model inference\n        inputs = torch.from_numpy(\n            point_set[:, shuffled_point_indices].reshape(-1, extra_batch_size, dataset.num_points).transpose(1, 0, 2)\n        ).float().to(configs.device)\n        with torch.no_grad():\n            vote_confidences = F.softmax(model(inputs), dim=1)\n            vote_confidences, vote_predictions = vote_confidences[:, start_class:end_class, :].max(dim=1)\n            vote_confidences = vote_confidences.view(total_num_voted_points).cpu().numpy()\n            vote_predictions = (vote_predictions + start_class).view(total_num_voted_points).cpu().numpy()\n\n        update_shape_predictions(vote_confidences, vote_predictions, shuffled_point_indices,\n                                 confidences, predictions, total_num_voted_points)\n        update_stats(stats, ground_truth, predictions, shape_id, start_class, end_class)\n\n    np.save(configs.evaluate.stats_path, stats)\n    print(\'clssIoU: {}\'.format(\'  \'.join(map(\'{:>8.2f}\'.format, stats[:, 0] / stats[:, 1] * 100))))\n    print(\'meanIoU: {:4.2f}\'.format(stats[:, 0].sum() / stats[:, 1].sum() * 100))\n\n\n@numba.jit()\ndef update_shape_predictions(vote_confidences, vote_predictions, shuffled_point_indices,\n                             shape_confidences, shape_predictions, total_num_voted_points):\n    for p in range(total_num_voted_points):\n        point_index = shuffled_point_indices[p]\n        current_confidence = vote_confidences[p]\n        if current_confidence > shape_confidences[point_index]:\n            shape_confidences[point_index] = current_confidence\n            shape_predictions[point_index] = vote_predictions[p]\n\n\n@numba.jit()\ndef update_stats(stats, ground_truth, predictions, shape_id, start_class, end_class):\n    iou = 0.0\n    for i in range(start_class, end_class):\n        igt = (ground_truth == i)\n        ipd = (predictions == i)\n        union = np.sum(igt | ipd)\n        intersection = np.sum(igt & ipd)\n        if union == 0:\n            iou += 1\n        else:\n            iou += intersection / union\n    iou /= (end_class - start_class)\n    stats[shape_id][0] += iou\n    stats[shape_id][1] += 1\n\n\nif __name__ == \'__main__\':\n    evaluate()\n'"
meters/kitti/__init__.py,0,b'from meters.kitti.frustum import MeterFrustumKitti\n'
meters/kitti/frustum.py,5,"b""import numpy as np\nimport torch\n\nfrom modules.frustum import get_box_corners_3d\nfrom meters.kitti.utils import get_box_iou_3d\n\n__all__ = ['MeterFrustumKitti']\n\n\nclass MeterFrustumKitti:\n    def __init__(self, num_heading_angle_bins, num_size_templates, size_templates, class_name_to_class_id,\n                 metric='iou_3d'):\n        super().__init__()\n        assert metric in ['iou_2d', 'iou_3d', 'accuracy', 'iou_3d_accuracy', 'iou_3d_class_accuracy']\n        self.metric = metric\n        self.num_heading_angle_bins = num_heading_angle_bins\n        self.num_size_templates = num_size_templates\n        self.size_templates = size_templates.view(self.num_size_templates, 3)\n        self.heading_angle_bin_centers = torch.arange(0, 2 * np.pi, 2 * np.pi / self.num_heading_angle_bins)\n        self.class_name_to_class_id = class_name_to_class_id\n        self.reset()\n\n    def reset(self):\n        self.total_seen_num = 0\n        self.total_correct_num = 0\n        self.iou_3d_corrent_num = 0\n        self.iou_2d_sum = 0\n        self.iou_3d_sum = 0\n        self.iou_3d_corrent_num_per_class = {cls: 0 for cls in self.class_name_to_class_id.keys()}\n        self.total_seen_num_per_class = {cls: 0 for cls in self.class_name_to_class_id.keys()}\n\n    def update(self, outputs, targets):\n        if self.metric == 'accuracy':\n            mask_logits = outputs['mask_logits']\n            mask_logits_target = targets['mask_logits']\n            self.total_seen_num += mask_logits_target.numel()\n            self.total_correct_num += torch.sum(mask_logits.argmax(dim=1) == mask_logits_target).item()\n        else:\n            center = outputs['center']  # (B, 3)\n            heading_scores = outputs['heading_scores']  # (B, NH)\n            heading_residuals = outputs['heading_residuals']  # (B, NH)\n            size_scores = outputs['size_scores']  # (B, NS)\n            size_residuals = outputs['size_residuals']  # (B, NS, 3)\n\n            center_target = targets['center']  # (B, 3)\n            heading_bin_id_target = targets['heading_bin_id']  # (B, )\n            heading_residual_target = targets['heading_residual']  # (B, )\n            size_template_id_target = targets['size_template_id']  # (B, )\n            size_residual_target = targets['size_residual']  # (B, 3)\n            class_id_target = targets['class_id'].cpu().numpy()  # (B, )\n\n            batch_size = center.size(0)\n            batch_id = torch.arange(batch_size, device=center.device)\n            self.size_templates = self.size_templates.to(center.device)\n            self.heading_angle_bin_centers = self.heading_angle_bin_centers.to(center.device)\n\n            heading_bin_id = torch.argmax(heading_scores, dim=1)\n            heading = self.heading_angle_bin_centers[heading_bin_id] + heading_residuals[batch_id, heading_bin_id]\n            size_template_id = torch.argmax(size_scores, dim=1)\n            size = self.size_templates[size_template_id] + size_residuals[batch_id, size_template_id]  # (B, 3)\n            corners = get_box_corners_3d(centers=center, headings=heading, sizes=size, with_flip=False)  # (B, 8, 3)\n            heading_target = self.heading_angle_bin_centers[heading_bin_id_target] + heading_residual_target  # (B, )\n            size_target = self.size_templates[size_template_id_target] + size_residual_target  # (B, 3)\n            corners_target = get_box_corners_3d(centers=center_target, headings=heading_target,\n                                                sizes=size_target, with_flip=False)  # (B, 8, 3)\n            iou_3d, iou_2d = get_box_iou_3d(corners.cpu().numpy(), corners_target.cpu().numpy())\n            self.iou_2d_sum += iou_2d.sum()\n            self.iou_3d_sum += iou_3d.sum()\n            self.iou_3d_corrent_num += np.sum(iou_3d >= 0.7)\n            self.total_seen_num += batch_size\n            for cls, cls_id in self.class_name_to_class_id.items():\n                mask = (class_id_target == cls_id)\n                self.iou_3d_corrent_num_per_class[cls] += np.sum(iou_3d[mask] >= (0.7 if cls == 'Car' else 0.5))\n                self.total_seen_num_per_class[cls] += np.sum(mask)\n\n    def compute(self):\n        if self.metric == 'iou_3d':\n            return self.iou_3d_sum / self.total_seen_num\n        elif self.metric == 'iou_2d':\n            return self.iou_2d_sum / self.total_seen_num\n        elif self.metric == 'accuracy':\n            return self.total_correct_num / self.total_seen_num\n        elif self.metric == 'iou_3d_accuracy':\n            return self.iou_3d_corrent_num / self.total_seen_num\n        elif self.metric == 'iou_3d_class_accuracy':\n            return sum(self.iou_3d_corrent_num_per_class[cls] / max(self.total_seen_num_per_class[cls], 1)\n                       for cls in self.class_name_to_class_id.keys()) / len(self.class_name_to_class_id)\n        else:\n            raise KeyError\n"""
meters/kitti/utils.py,0,"b'# Copyright 2018 Charles R. Qi from Stanford University and\n# Wei Liu from Nuro Inc.\n#\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an ""AS IS"" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport numba\nimport numpy as np\nfrom scipy.spatial import ConvexHull\n\n__all__ = [\'get_box_iou_3d\']\n\n\n@numba.njit()\ndef poly_area(coords):\n    """"""\n    calculate area of polygon given x-y coordinates\n    (ref: http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates)\n    :param coords: FloatTensor[4, 2]\n    """"""\n    x = coords[:, 0]\n    y = coords[:, 1]\n    return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))\n\n\ndef polygon_clip(subject_polygon, clip_polygon):\n    """"""\n    clip a polygon with another polygon\n    (ref: https://rosettacode.org/wiki/Sutherland-Hodgman_polygon_clipping#Python)\n    :param subject_polygon: a list of (x,y) 2d points, any polygon\n    :param clip_polygon: a list of (x,y) 2d points, has to be *convex*\n    :return:\n        a list of (x,y) vertex point for the intersection polygon\n    """"""\n\n    def inside(p):\n        return (cp2[0] - cp1[0]) * (p[1] - cp1[1]) > (cp2[1] - cp1[1]) * (p[0] - cp1[0])\n\n    def compute_intersection():\n        dc = [cp1[0] - cp2[0], cp1[1] - cp2[1]]\n        dp = [s[0] - e[0], s[1] - e[1]]\n        n1 = cp1[0] * cp2[1] - cp1[1] * cp2[0]\n        n2 = s[0] * e[1] - s[1] * e[0]\n        n3 = 1.0 / (dc[0] * dp[1] - dc[1] * dp[0])\n        return [(n1 * dp[0] - n2 * dc[0]) * n3, (n1 * dp[1] - n2 * dc[1]) * n3]\n\n    output_list = subject_polygon\n    cp1 = clip_polygon[-1]\n\n    for clip_vertex in clip_polygon:\n        cp2 = clip_vertex\n        input_list = output_list\n        output_list = []\n        s = input_list[-1]\n\n        for subject_vertex in input_list:\n            e = subject_vertex\n            if inside(e):\n                if not inside(s):\n                    output_list.append(compute_intersection())\n                output_list.append(e)\n            elif inside(s):\n                output_list.append(compute_intersection())\n            s = e\n        cp1 = cp2\n        if len(output_list) == 0:\n            return None\n    return output_list\n\n\ndef convex_hull_intersection(p1, pt):\n    """"""\n    compute area of two convex hull\'s intersection area\n    :param p1: a list of (x,y) tuples of hull vertices\n    :param pt: a list of (x,y) tuples of hull vertices\n    :return:\n        a list of (x,y) for the intersection and its volume\n    """"""\n    inter_p = polygon_clip(p1, pt)\n    if inter_p is not None:\n        hull_inter = ConvexHull(inter_p)\n        return inter_p, hull_inter.volume\n    else:\n        return None, 0.0\n\n\n@numba.njit()\ndef box_volume_3d(corners):\n    a = np.sqrt(np.sum((corners[:, 0] - corners[:, 1]) ** 2))\n    b = np.sqrt(np.sum((corners[:, 1] - corners[:, 2]) ** 2))\n    c = np.sqrt(np.sum((corners[:, 0] - corners[:, 4]) ** 2))\n    return a * b * c\n\n\ndef get_box_iou_3d(corners_1, corners_t):\n    """"""\n    calculate iou of 3d box\n    :param corners_1: FloatTensor[B, 3, 8], assume up direction is negative Y\n    :param corners_t: FloatTensor[B, 3, 8], assume up direction is negative Y\n        NOTE: corner points are in counter clockwise order, e.g.,\n          2--1\n        3--0 5\n        7--4\n    :return:\n        iou_3d: 3D bounding box IoU, FloatTensor[B]\n        iou_2d: bird\'s eye view 2D bounding box IoU, FloatTensor[B]\n    """"""\n    if corners_1.ndim == 3:\n        batch_size = corners_1.shape[0]\n        iou_3d = np.zeros(batch_size)\n        iou_2d = np.zeros(batch_size)\n        for b in range(batch_size):\n            iou_3d[b], iou_2d[b] = get_box_iou_3d(corners_1[b], corners_t[b])\n        return iou_3d, iou_2d\n    else:\n        # corner points are in counter clockwise order\n        corners_1_upper_xz = [(corners_1[0, 3], corners_1[2, 3]), (corners_1[0, 2], corners_1[2, 2]),\n                              (corners_1[0, 1], corners_1[2, 1]), (corners_1[0, 0], corners_1[2, 0])]\n        corners_t_upper_xz = [(corners_t[0, 3], corners_t[2, 3]), (corners_t[0, 2], corners_t[2, 2]),\n                              (corners_t[0, 1], corners_t[2, 1]), (corners_t[0, 0], corners_t[2, 0])]\n        area_1 = poly_area(np.array(corners_1_upper_xz))\n        area_2 = poly_area(np.array(corners_t_upper_xz))\n        inter, inter_area = convex_hull_intersection(corners_1_upper_xz, corners_t_upper_xz)\n        iou_2d = inter_area / (area_1 + area_2 - inter_area)\n        y_max = min(corners_1[1, 0], corners_t[1, 0])\n        y_min = max(corners_1[1, 4], corners_t[1, 4])\n        inter_vol = inter_area * max(0.0, y_max - y_min)\n        vol1 = box_volume_3d(corners_1)\n        vol2 = box_volume_3d(corners_t)\n        iou_3d = inter_vol / (vol1 + vol2 - inter_vol)\n        return iou_3d, iou_2d\n'"
models/kitti/__init__.py,0,b'from models.kitti.frustum import FrustumPointNet\n'
models/s3dis/__init__.py,0,b'from models.s3dis.pointnet import PointNet\nfrom models.s3dis.pvcnn import PVCNN\nfrom models.s3dis.pvcnnpp import PVCNN2\n'
models/s3dis/pointnet.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet_components, create_mlp_components\n\n__all__ = ['PointNet']\n\n\nclass PointNet(nn.Module):\n    blocks = ((64, 3, None), (128, 1, None), (1024, 1, None))\n\n    def __init__(self, num_classes, extra_feature_channels=6, width_multiplier=1):\n        super().__init__()\n        self.in_channels = extra_feature_channels + 3\n\n        layers, channels_point, _ = create_pointnet_components(blocks=self.blocks, in_channels=self.in_channels,\n                                                               width_multiplier=width_multiplier)\n        self.point_features = nn.Sequential(*layers)\n\n        layers, channels_cloud = create_mlp_components(in_channels=channels_point, out_channels=[256, 128],\n                                                       classifier=False, dim=1, width_multiplier=width_multiplier)\n        self.cloud_features = nn.Sequential(*layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=(channels_point + channels_cloud), out_channels=[512, 256, 0.3, num_classes],\n            classifier=True, dim=2, width_multiplier=width_multiplier)\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        if isinstance(inputs, dict):\n            inputs = inputs['features']\n\n        point_features = self.point_features(inputs)\n        cloud_features = self.cloud_features(point_features.max(dim=-1, keepdim=False).values)\n        features = torch.cat([point_features, cloud_features.unsqueeze(-1).repeat([1, 1, inputs.size(-1)])], dim=1)\n        return self.classifier(features)\n"""
models/s3dis/pvcnn.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_mlp_components, create_pointnet_components\n\n__all__ = ['PVCNN']\n\n\nclass PVCNN(nn.Module):\n    blocks = ((64, 1, 32), (64, 2, 16), (128, 1, 16), (1024, 1, None))\n\n    def __init__(self, num_classes, extra_feature_channels=6, width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = extra_feature_channels + 3\n\n        layers, channels_point, concat_channels_point = create_pointnet_components(\n            blocks=self.blocks, in_channels=self.in_channels, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.point_features = nn.ModuleList(layers)\n\n        layers, channels_cloud = create_mlp_components(\n            in_channels=channels_point, out_channels=[256, 128],\n            classifier=False, dim=1, width_multiplier=width_multiplier)\n        self.cloud_features = nn.Sequential(*layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=(concat_channels_point + channels_cloud),\n            out_channels=[512, 0.3, 256, 0.3, num_classes],\n            classifier=True, dim=2, width_multiplier=width_multiplier\n        )\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        if isinstance(inputs, dict):\n            inputs = inputs['features']\n\n        coords = inputs[:, :3, :]\n        out_features_list = []\n        for i in range(len(self.point_features)):\n            inputs, _ = self.point_features[i]((inputs, coords))\n            out_features_list.append(inputs)\n        # inputs: num_batches * 1024 * num_points -> num_batches * 1024 -> num_batches * 128\n        inputs = self.cloud_features(inputs.max(dim=-1, keepdim=False).values)\n        out_features_list.append(inputs.unsqueeze(-1).repeat([1, 1, coords.size(-1)]))\n        return self.classifier(torch.cat(out_features_list, dim=1))\n"""
models/s3dis/pvcnnpp.py,1,"b""import torch.nn as nn\n\nfrom models.utils import create_pointnet2_sa_components, create_pointnet2_fp_modules, create_mlp_components\n\n__all__ = ['PVCNN2']\n\n\nclass PVCNN2(nn.Module):\n    sa_blocks = [\n        ((32, 2, 32), (1024, 0.1, 32, (32, 64))),\n        ((64, 3, 16), (256, 0.2, 32, (64, 128))),\n        ((128, 3, 8), (64, 0.4, 32, (128, 256))),\n        (None, (16, 0.8, 32, (256, 256, 512))),\n    ]\n    fp_blocks = [\n        ((256, 256), (256, 1, 8)),\n        ((256, 256), (256, 1, 8)),\n        ((256, 128), (128, 2, 16)),\n        ((128, 128, 64), (64, 1, 32)),\n    ]\n\n    def __init__(self, num_classes, extra_feature_channels=6, width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = extra_feature_channels + 3\n\n        sa_layers, sa_in_channels, channels_sa_features, _ = create_pointnet2_sa_components(\n            sa_blocks=self.sa_blocks, extra_feature_channels=extra_feature_channels, with_se=True,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.sa_layers = nn.ModuleList(sa_layers)\n\n        # only use extra features in the last fp module\n        sa_in_channels[0] = extra_feature_channels\n        fp_layers, channels_fp_features = create_pointnet2_fp_modules(\n            fp_blocks=self.fp_blocks, in_channels=channels_sa_features, sa_in_channels=sa_in_channels, with_se=True,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.fp_layers = nn.ModuleList(fp_layers)\n\n        layers, _ = create_mlp_components(in_channels=channels_fp_features, out_channels=[128, 0.5, num_classes],\n                                          classifier=True, dim=2, width_multiplier=width_multiplier)\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        if isinstance(inputs, dict):\n            inputs = inputs['features']\n\n        coords, features = inputs[:, :3, :].contiguous(), inputs\n        coords_list, in_features_list = [], []\n        for sa_blocks in self.sa_layers:\n            in_features_list.append(features)\n            coords_list.append(coords)\n            features, coords = sa_blocks((features, coords))\n        in_features_list[0] = inputs[:, 3:, :].contiguous()\n\n        for fp_idx, fp_blocks in enumerate(self.fp_layers):\n            features, coords = fp_blocks((coords_list[-1-fp_idx], coords, features, in_features_list[-1-fp_idx]))\n\n        return self.classifier(features)\n"""
models/shapenet/__init__.py,0,"b'from models.shapenet.pointnet import PointNet\nfrom models.shapenet.pointnetpp import PointNet2SSG, PointNet2MSG\nfrom models.shapenet.pvcnn import PVCNN\n'"
models/shapenet/pointnet.py,5,"b""import torch\nimport torch.nn as nn\n\nfrom modules import SharedMLP\n\n__all__ = ['PointNet']\n\n\nclass Transformer(nn.Module):\n    def __init__(self, channels):\n        super(Transformer, self).__init__()\n        self.channels = channels\n\n        self.features = nn.Sequential(\n            SharedMLP(self.channels, 64),\n            SharedMLP(64, 128),\n            SharedMLP(128, 1024),\n        )\n        self.tranformer = nn.Sequential(\n            nn.Linear(1024, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 256),\n            nn.BatchNorm1d(256),\n            nn.ReLU(inplace=True),\n            nn.Linear(256, self.channels * self.channels)\n        )\n\n    def forward(self, inputs):\n        transform_weight = self.tranformer(torch.max(self.features(inputs), dim=-1, keepdim=False).values)\n        transform_weight = transform_weight.view(-1, self.channels, self.channels)\n        transform_weight = transform_weight + torch.eye(self.channels, device=transform_weight.device)\n        outputs = torch.bmm(transform_weight, inputs)\n        return outputs\n\n\nclass PointNet(nn.Module):\n    blocks = ((True, 64, 1), (False, 128, 2), (True, 512, 1), (False, 2048, 1))\n\n    def __init__(self, num_classes, num_shapes, with_transformer=False, extra_feature_channels=0, width_multiplier=1):\n        super().__init__()\n        assert extra_feature_channels >= 0\n        r = width_multiplier\n        self.in_channels = in_channels = extra_feature_channels + 3\n        self.num_shapes = num_shapes\n        self.with_transformer = with_transformer\n\n        layers, concat_channels = [], 0\n        for with_transformer_before, out_channels, num_blocks in self.blocks:\n            with_transformer_before = with_transformer_before and with_transformer\n            out_channels = int(r * out_channels)\n            for block_index in range(num_blocks):\n                if with_transformer_before and block_index == 0:\n                    layers.append(nn.Sequential(Transformer(in_channels), SharedMLP(in_channels, out_channels)))\n                else:\n                    layers.append(SharedMLP(in_channels, out_channels))\n                in_channels = out_channels\n                concat_channels += out_channels\n        self.point_features = nn.ModuleList(layers)\n\n        self.classifier = nn.Sequential(\n            SharedMLP(in_channels=in_channels + concat_channels + num_shapes, out_channels=int(r * 256)),\n            nn.Dropout(0.2),\n            SharedMLP(in_channels=int(r * 256), out_channels=int(r * 256)),\n            nn.Dropout(0.2),\n            SharedMLP(in_channels=int(r * 256), out_channels=int(r * 128)),\n            nn.Conv1d(int(r * 128), num_classes, 1)\n        )\n\n    def forward(self, inputs):\n        # inputs: [B, in_channels + S, N]\n        assert inputs.size(1) == self.in_channels + self.num_shapes\n        features = inputs[:, :self.in_channels, :]\n        one_hot_vectors = inputs[:, -self.num_shapes:, :]\n        num_points = features.size(-1)\n\n        out_features_list = [one_hot_vectors]\n        for i in range(len(self.point_features)):\n            features = self.point_features[i](features)\n            out_features_list.append(features)\n        out_features_list.append(features.max(dim=-1, keepdim=True).values.repeat([1, 1, num_points]))\n        return self.classifier(torch.cat(out_features_list, dim=1))\n"""
models/shapenet/pointnetpp.py,1,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet2_sa_components, create_pointnet2_fp_modules, create_mlp_components\n\n__all__ = ['PointNet2SSG', 'PointNet2MSG']\n\n\nclass PointNet2(nn.Module):\n    def __init__(self, num_classes, num_shapes, sa_blocks, fp_blocks, with_one_hot_shape_id=True,\n                 extra_feature_channels=3, width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        assert extra_feature_channels >= 0\n\n        self.in_channels = extra_feature_channels + 3\n        self.num_shapes = num_shapes\n        self.with_one_hot_shape_id = with_one_hot_shape_id\n\n        sa_layers, sa_in_channels, channels_sa_features, _ = create_pointnet2_sa_components(\n            sa_blocks=sa_blocks, extra_feature_channels=extra_feature_channels, width_multiplier=width_multiplier\n        )\n        self.sa_layers = nn.ModuleList(sa_layers)\n\n        # use one hot vector in the last fp module\n        sa_in_channels[0] += num_shapes if with_one_hot_shape_id else 0\n        fp_layers, channels_fp_features = create_pointnet2_fp_modules(\n            fp_blocks=fp_blocks, in_channels=channels_sa_features, sa_in_channels=sa_in_channels,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.fp_layers = nn.ModuleList(fp_layers)\n\n        layers, _ = create_mlp_components(in_channels=channels_fp_features, out_channels=[128, 0.5, num_classes],\n                                       classifier=True, dim=2, width_multiplier=width_multiplier)\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        # inputs : [B, in_channels + S, N]\n        features = inputs[:, :self.in_channels, :]\n        if self.with_one_hot_shape_id:\n            assert inputs.size(1) == self.in_channels + self.num_shapes\n            features_with_one_hot_vectors = inputs\n        else:\n            features_with_one_hot_vectors = features\n\n        coords, features = features[:, :3, :].contiguous(), features[:, 3:, :].contiguous()\n        coords_list, in_features_list = [], []\n        for sa_module in self.sa_layers:\n            in_features_list.append(features)\n            coords_list.append(coords)\n            features, coords = sa_module((features, coords))\n        in_features_list[0] = features_with_one_hot_vectors.contiguous()\n\n        for fp_idx, fp_module in enumerate(self.fp_layers):\n            features, coords = fp_module((coords_list[-1-fp_idx], coords, features, in_features_list[-1-fp_idx]))\n\n        return self.classifier(features)\n\n\nclass PointNet2SSG(PointNet2):\n    sa_blocks = [\n        (None, (512, 0.2, 64, (64, 64, 128))),\n        (None, (128, 0.4, 64, (128, 128, 256))),\n        (None, (None, None, None, (256, 512, 1024))),\n    ]\n    fp_blocks = [((256, 256), None), ((256, 128), None), ((128, 128, 128), None)]\n\n    def __init__(self, num_classes, num_shapes, extra_feature_channels=3, width_multiplier=1,\n                 voxel_resolution_multiplier=1):\n        super().__init__(\n            num_classes=num_classes, num_shapes=num_shapes, sa_blocks=self.sa_blocks, fp_blocks=self.fp_blocks,\n            with_one_hot_shape_id=False, extra_feature_channels=extra_feature_channels,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n\n\nclass PointNet2MSG(PointNet2):\n    sa_blocks = [\n        (None, (512, [0.1, 0.2, 0.4], [32, 64, 128], [(32, 32, 64), (64, 64, 128), (64, 96, 128)])),\n        (None, (128, [0.4, 0.8], [64, 128], [(128, 128, 256), (128, 196, 256)])),\n        (None, (None, None, None, (256, 512, 1024))),\n    ]\n    fp_blocks = [((256, 256), None), ((256, 128), None), ((128, 128, 128), None)]\n\n    def __init__(self, num_classes, num_shapes, extra_feature_channels=3, width_multiplier=1,\n                 voxel_resolution_multiplier=1):\n        super().__init__(\n            num_classes=num_classes, num_shapes=num_shapes, sa_blocks=self.sa_blocks, fp_blocks=self.fp_blocks,\n            with_one_hot_shape_id=True, extra_feature_channels=extra_feature_channels,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n"""
models/shapenet/pvcnn.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet_components, create_mlp_components\n\n__all__ = ['PVCNN']\n\n\nclass PVCNN(nn.Module):\n    blocks = ((64, 1, 32), (128, 2, 16), (512, 1, None), (2048, 1, None))\n\n    def __init__(self, num_classes, num_shapes, extra_feature_channels=3,\n                 width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        assert extra_feature_channels >= 0\n        self.in_channels = extra_feature_channels + 3\n        self.num_shapes = num_shapes\n\n        layers, channels_point, concat_channels_point = create_pointnet_components(\n            blocks=self.blocks, in_channels=self.in_channels, with_se=True, normalize=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.point_features = nn.ModuleList(layers)\n\n        layers, _ = create_mlp_components(in_channels=(num_shapes + channels_point + concat_channels_point),\n                                          out_channels=[256, 0.2, 256, 0.2, 128, num_classes],\n                                          classifier=True, dim=2, width_multiplier=width_multiplier)\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        # inputs : [B, in_channels + S, N]\n        features = inputs[:, :self.in_channels, :]\n        one_hot_vectors = inputs[:, -self.num_shapes:, :]\n        num_points = features.size(-1)\n\n        coords = features[:, :3, :]\n        out_features_list = [one_hot_vectors]\n        for i in range(len(self.point_features)):\n            features, _ = self.point_features[i]((features, coords))\n            out_features_list.append(features)\n        out_features_list.append(features.max(dim=-1, keepdim=True).values.repeat([1, 1, num_points]))\n        return self.classifier(torch.cat(out_features_list, dim=1))\n"""
modules/functional/__init__.py,0,"b'from modules.functional.ball_query import ball_query\nfrom modules.functional.devoxelization import trilinear_devoxelize\nfrom modules.functional.grouping import grouping\nfrom modules.functional.interpolatation import nearest_neighbor_interpolate\nfrom modules.functional.loss import kl_loss, huber_loss\nfrom modules.functional.sampling import gather, furthest_point_sample, logits_mask\nfrom modules.functional.voxelization import avg_voxelize\n'"
modules/functional/backend.py,1,"b""import os\n\nfrom torch.utils.cpp_extension import load\n\n_src_path = os.path.dirname(os.path.abspath(__file__))\n_backend = load(name='_pvcnn_backend',\n                extra_cflags=['-O3', '-std=c++17'],\n                sources=[os.path.join(_src_path,'src', f) for f in [\n                    'ball_query/ball_query.cpp',\n                    'ball_query/ball_query.cu',\n                    'grouping/grouping.cpp',\n                    'grouping/grouping.cu',\n                    'interpolate/neighbor_interpolate.cpp',\n                    'interpolate/neighbor_interpolate.cu',\n                    'interpolate/trilinear_devox.cpp',\n                    'interpolate/trilinear_devox.cu',\n                    'sampling/sampling.cpp',\n                    'sampling/sampling.cu',\n                    'voxelization/vox.cpp',\n                    'voxelization/vox.cu',\n                    'bindings.cpp',\n                ]]\n                )\n\n__all__ = ['_backend']\n"""
modules/functional/ball_query.py,1,"b'from torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'ball_query\']\n\n\ndef ball_query(centers_coords, points_coords, radius, num_neighbors):\n        """"""\n        :param centers_coords: coordinates of centers, FloatTensor[B, 3, M]\n        :param points_coords: coordinates of points, FloatTensor[B, 3, N]\n        :param radius: float, radius of ball query\n        :param num_neighbors: int, maximum number of neighbors\n        :return:\n            neighbor_indices: indices of neighbors, IntTensor[B, M, U]\n        """"""\n        centers_coords = centers_coords.contiguous()\n        points_coords = points_coords.contiguous()\n        return _backend.ball_query(centers_coords, points_coords, radius, num_neighbors)\n'"
modules/functional/devoxelization.py,1,"b'from torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'trilinear_devoxelize\']\n\n\nclass TrilinearDevoxelization(Function):\n    @staticmethod\n    def forward(ctx, features, coords, resolution, is_training=True):\n        """"""\n        :param ctx:\n        :param coords: the coordinates of points, FloatTensor[B, 3, N]\n        :param features: FloatTensor[B, C, R, R, R]\n        :param resolution: int, the voxel resolution\n        :param is_training: bool, training mode\n        :return:\n            FloatTensor[B, C, N]\n        """"""\n        B, C = features.shape[:2]\n        features = features.contiguous().view(B, C, -1)\n        coords = coords.contiguous()\n        outs, inds, wgts = _backend.trilinear_devoxelize_forward(resolution, is_training, coords, features)\n        if is_training:\n            ctx.save_for_backward(inds, wgts)\n            ctx.r = resolution\n        return outs\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        """"""\n        :param ctx: \n        :param grad_output: gradient of outputs, FloatTensor[B, C, N]\n        :return:\n            gradient of inputs, FloatTensor[B, C, R, R, R]\n        """"""\n        inds, wgts = ctx.saved_tensors\n        grad_inputs = _backend.trilinear_devoxelize_backward(grad_output.contiguous(), inds, wgts, ctx.r)\n        return grad_inputs.view(grad_output.size(0), grad_output.size(1), ctx.r, ctx.r, ctx.r), None, None, None\n\n\ntrilinear_devoxelize = TrilinearDevoxelization.apply\n'"
modules/functional/grouping.py,1,"b'from torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'grouping\']\n\n\nclass Grouping(Function):\n    @staticmethod\n    def forward(ctx, features, indices):\n        """"""\n        :param ctx:\n        :param features: features of points, FloatTensor[B, C, N]\n        :param indices: neighbor indices of centers, IntTensor[B, M, U], M is #centers, U is #neighbors\n        :return:\n            grouped_features: grouped features, FloatTensor[B, C, M, U]\n        """"""\n        features = features.contiguous()\n        indices = indices.contiguous()\n        ctx.save_for_backward(indices)\n        ctx.num_points = features.size(-1)\n        return _backend.grouping_forward(features, indices)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        indices, = ctx.saved_tensors\n        grad_features = _backend.grouping_backward(grad_output.contiguous(), indices, ctx.num_points)\n        return grad_features, None\n\n\ngrouping = Grouping.apply\n'"
modules/functional/interpolatation.py,1,"b'from torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'nearest_neighbor_interpolate\']\n\n\nclass NeighborInterpolation(Function):\n    @staticmethod\n    def forward(ctx, points_coords, centers_coords, centers_features):\n        """"""\n        :param ctx:\n        :param points_coords: coordinates of points, FloatTensor[B, 3, N]\n        :param centers_coords: coordinates of centers, FloatTensor[B, 3, M]\n        :param centers_features: features of centers, FloatTensor[B, C, M]\n        :return:\n            points_features: features of points, FloatTensor[B, C, N]\n        """"""\n        centers_coords = centers_coords.contiguous()\n        points_coords = points_coords.contiguous()\n        centers_features = centers_features.contiguous()\n        points_features, indices, weights = _backend.three_nearest_neighbors_interpolate_forward(\n            points_coords, centers_coords, centers_features\n        )\n        ctx.save_for_backward(indices, weights)\n        ctx.num_centers = centers_coords.size(-1)\n        return points_features\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        indices, weights = ctx.saved_tensors\n        grad_centers_features = _backend.three_nearest_neighbors_interpolate_backward(\n            grad_output.contiguous(), indices, weights, ctx.num_centers\n        )\n        return None, None, grad_centers_features\n\n\nnearest_neighbor_interpolate = NeighborInterpolation.apply\n'"
modules/functional/loss.py,5,"b""import torch\nimport torch.nn.functional as F\n\n__all__ = ['kl_loss', 'huber_loss']\n\n\ndef kl_loss(x, y):\n    x = F.softmax(x.detach(), dim=1)\n    y = F.log_softmax(y, dim=1)\n    return torch.mean(torch.sum(x * (torch.log(x) - y), dim=1))\n\n\ndef huber_loss(error, delta):\n    abs_error = torch.abs(error)\n    quadratic = torch.min(abs_error, torch.full_like(abs_error, fill_value=delta))\n    losses = 0.5 * (quadratic ** 2) + delta * (abs_error - quadratic)\n    return torch.mean(losses)\n"""
modules/functional/sampling.py,6,"b'import numpy as np\nimport torch\nfrom torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'gather\', \'furthest_point_sample\', \'logits_mask\']\n\n\nclass Gather(Function):\n    @staticmethod\n    def forward(ctx, features, indices):\n        """"""\n        Gather\n        :param ctx:\n        :param features: features of points, FloatTensor[B, C, N]\n        :param indices: centers\' indices in points, IntTensor[b, m]\n        :return:\n            centers_coords: coordinates of sampled centers, FloatTensor[B, C, M]\n        """"""\n        features = features.contiguous()\n        indices = indices.int().contiguous()\n        ctx.save_for_backward(indices)\n        ctx.num_points = features.size(-1)\n        return _backend.gather_features_forward(features, indices)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        indices, = ctx.saved_tensors\n        grad_features = _backend.gather_features_backward(grad_output.contiguous(), indices, ctx.num_points)\n        return grad_features, None\n\n\ngather = Gather.apply\n\n\ndef furthest_point_sample(coords, num_samples):\n    """"""\n    Uses iterative furthest point sampling to select a set of npoint features that have the largest\n    minimum distance to the sampled point set\n    :param coords: coordinates of points, FloatTensor[B, 3, N]\n    :param num_samples: int, M\n    :return:\n       centers_coords: coordinates of sampled centers, FloatTensor[B, 3, M]\n    """"""\n    coords = coords.contiguous()\n    indices = _backend.furthest_point_sampling(coords, num_samples)\n    return gather(coords, indices)\n\n\ndef logits_mask(coords, logits, num_points_per_object):\n    """"""\n    Use logits to sample points\n    :param coords: coords of points, FloatTensor[B, 3, N]\n    :param logits: binary classification logits, FloatTensor[B, 2, N]\n    :param num_points_per_object: M, #points per object after masking, int\n    :return:\n        selected_coords: FloatTensor[B, 3, M]\n        masked_coords_mean: mean coords of selected points, FloatTensor[B, 3]\n        mask: mask to select points, BoolTensor[B, N]\n    """"""\n    batch_size, _, num_points = coords.shape\n    mask = torch.lt(logits[:, 0, :], logits[:, 1, :])   # [B, N]\n    num_candidates = torch.sum(mask, dim=-1, keepdim=True)  # [B, 1]\n    masked_coords = coords * mask.view(batch_size, 1, num_points)  # [B, C, N]\n    masked_coords_mean = torch.sum(masked_coords, dim=-1) / torch.max(num_candidates,\n                                                                      torch.ones_like(num_candidates)).float()  # [B, C]\n    selected_indices = torch.zeros((batch_size, num_points_per_object), device=coords.device, dtype=torch.int32)\n    for i in range(batch_size):\n        current_mask = mask[i]  # [N]\n        current_candidates = current_mask.nonzero().view(-1)\n        current_num_candidates = current_candidates.numel()\n        if current_num_candidates >= num_points_per_object:\n            choices = np.random.choice(current_num_candidates, num_points_per_object, replace=False)\n            selected_indices[i] = current_candidates[choices]\n        elif current_num_candidates > 0:\n            choices = np.concatenate([\n                np.arange(current_num_candidates).repeat(num_points_per_object // current_num_candidates),\n                np.random.choice(current_num_candidates, num_points_per_object % current_num_candidates, replace=False)\n            ])\n            np.random.shuffle(choices)\n            selected_indices[i] = current_candidates[choices]\n    selected_coords = gather(masked_coords - masked_coords_mean.view(batch_size, -1, 1), selected_indices)\n    return selected_coords, masked_coords_mean, mask\n'"
modules/functional/voxelization.py,1,"b'from torch.autograd import Function\n\nfrom modules.functional.backend import _backend\n\n__all__ = [\'avg_voxelize\']\n\n\nclass AvgVoxelization(Function):\n    @staticmethod\n    def forward(ctx, features, coords, resolution):\n        """"""\n        :param ctx:\n        :param features: Features of the point cloud, FloatTensor[B, C, N]\n        :param coords: Voxelized Coordinates of each point, IntTensor[B, 3, N]\n        :param resolution: Voxel resolution\n        :return:\n            Voxelized Features, FloatTensor[B, C, R, R, R]\n        """"""\n        features = features.contiguous()\n        coords = coords.int().contiguous()\n        b, c, _ = features.shape\n        out, indices, counts = _backend.avg_voxelize_forward(features, coords, resolution)\n        ctx.save_for_backward(indices, counts)\n        return out.view(b, c, resolution, resolution, resolution)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        """"""\n        :param ctx:\n        :param grad_output: gradient of output, FloatTensor[B, C, R, R, R]\n        :return:\n            gradient of inputs, FloatTensor[B, C, N]\n        """"""\n        b, c = grad_output.shape[:2]\n        indices, counts = ctx.saved_tensors\n        grad_features = _backend.avg_voxelize_backward(grad_output.contiguous().view(b, c, -1), indices, counts)\n        return grad_features, None, None\n\n\navg_voxelize = AvgVoxelization.apply\n'"
configs/kitti/frustum/__init__.py,2,"b""import numpy as np\nimport torch\nimport torch.optim as optim\n\nfrom datasets.kitti import FrustumKitti\nfrom datasets.kitti.attributes import kitti_attributes as kitti\nfrom meters.kitti import MeterFrustumKitti\nfrom modules.frustum import FrustumPointNetLoss\nfrom evaluate.kitti.frustum.eval import evaluate\nfrom utils.config import Config, configs\n\n# data configs\nconfigs.data.num_points_per_object = 512\nconfigs.data.num_heading_angle_bins = 12\nconfigs.data.size_template_names = kitti.class_names\nconfigs.data.num_size_templates = len(configs.data.size_template_names)\nconfigs.data.class_name_to_size_template_id = {\n    cat: cls for cls, cat in enumerate(configs.data.size_template_names)\n}\nconfigs.data.size_template_id_to_class_name = {\n    v: k for k, v in configs.data.class_name_to_size_template_id.items()\n}\nconfigs.data.size_templates = np.zeros((configs.data.num_size_templates, 3))\nfor i in range(configs.data.num_size_templates):\n    configs.data.size_templates[i, :] = kitti.class_name_to_size_template[\n        configs.data.size_template_id_to_class_name[i]]\nconfigs.data.size_templates = torch.from_numpy(configs.data.size_templates.astype(np.float32))\n\n# dataset configs\nconfigs.dataset = Config(FrustumKitti)\nconfigs.dataset.root = 'data/kitti/frustum/frustum_data'\nconfigs.dataset.num_points = 1024\nconfigs.dataset.classes = configs.data.classes\nconfigs.dataset.num_heading_angle_bins = configs.data.num_heading_angle_bins\nconfigs.dataset.class_name_to_size_template_id = configs.data.class_name_to_size_template_id\nconfigs.dataset.random_flip = True\nconfigs.dataset.random_shift = True\nconfigs.dataset.frustum_rotate = True\nconfigs.dataset.from_rgb_detection = False\n\n# evaluate configs\nconfigs.evaluate.fn = evaluate\nconfigs.evaluate.batch_size = 32\nconfigs.evaluate.dataset = Config(split='val', from_rgb_detection=True)\n\n# train configs\nconfigs.train = Config()\nconfigs.train.num_epochs = 209\nconfigs.train.batch_size = 32\n\n# train: meters\nconfigs.train.meters = Config()\nfor name, metric in [\n    ('acc/iou_3d_{}', 'iou_3d'), ('acc/acc_{}', 'accuracy'),\n    ('acc/iou_3d_acc_{}', 'iou_3d_accuracy'), ('acc/iou_3d_class_acc_{}', 'iou_3d_class_accuracy')\n]:\n    configs.train.meters[name] = Config(\n        MeterFrustumKitti, metric=metric, num_heading_angle_bins=configs.data.num_heading_angle_bins,\n        num_size_templates=configs.data.num_size_templates, size_templates=configs.data.size_templates,\n        class_name_to_class_id={cat: cls for cls, cat in enumerate(configs.data.classes)}\n    )\n\n# train: metric for save best checkpoint\nconfigs.train.metrics = ('acc/iou_3d_class_acc_val', 'acc/iou_3d_acc_val')\n\n# train: criterion\nconfigs.train.criterion = Config(FrustumPointNetLoss)\nconfigs.train.criterion.num_heading_angle_bins = configs.data.num_heading_angle_bins\nconfigs.train.criterion.num_size_templates = configs.data.num_size_templates\nconfigs.train.criterion.size_templates = configs.data.size_templates\nconfigs.train.criterion.box_loss_weight = 1.0\nconfigs.train.criterion.corners_loss_weight = 10.0\nconfigs.train.criterion.heading_residual_loss_weight = 20.0\nconfigs.train.criterion.size_residual_loss_weight = 20.0\n\n# train: optimizer\nconfigs.train.optimizer = Config(optim.Adam)\nconfigs.train.optimizer.lr = 1e-3\n"""
configs/kitti/frustum/pointnet.py,1,"b'import torch.optim as optim\n\nfrom models.kitti.frustum import FrustumPointNet\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(FrustumPointNet)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_heading_angle_bins = configs.data.num_heading_angle_bins\nconfigs.model.num_size_templates = configs.data.num_size_templates\nconfigs.model.num_points_per_object = configs.data.num_points_per_object\nconfigs.model.size_templates = configs.data.size_templates\nconfigs.model.extra_feature_channels = 1\n\n# train: scheduler\nconfigs.train.scheduler = Config(optim.lr_scheduler.StepLR)\nconfigs.train.scheduler.step_size = 25\nconfigs.train.scheduler.gamma = 0.5\n'"
configs/kitti/frustum/pointnet2.py,1,"b'import torch.optim as optim\n\nfrom models.kitti.frustum import FrustumPointNet2\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(FrustumPointNet2)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_heading_angle_bins = configs.data.num_heading_angle_bins\nconfigs.model.num_size_templates = configs.data.num_size_templates\nconfigs.model.num_points_per_object = configs.data.num_points_per_object\nconfigs.model.size_templates = configs.data.size_templates\nconfigs.model.extra_feature_channels = 1\n\nconfigs.train.batch_size = 24\n# train: scheduler\nconfigs.train.scheduler = Config(optim.lr_scheduler.CosineAnnealingLR)\nconfigs.train.scheduler.T_max = configs.train.num_epochs\n'"
configs/kitti/frustum/pvcnne.py,1,"b'import torch.optim as optim\n\nfrom models.kitti.frustum import FrustumPVCNNE\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(FrustumPVCNNE)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_heading_angle_bins = configs.data.num_heading_angle_bins\nconfigs.model.num_size_templates = configs.data.num_size_templates\nconfigs.model.num_points_per_object = configs.data.num_points_per_object\nconfigs.model.size_templates = configs.data.size_templates\nconfigs.model.extra_feature_channels = 1\n\n# train: scheduler\nconfigs.train.scheduler = Config(optim.lr_scheduler.CosineAnnealingLR)\nconfigs.train.scheduler.T_max = configs.train.num_epochs\n'"
configs/s3dis/pointnet/__init__.py,1,"b'import torch.optim as optim\n\nfrom models.s3dis import PointNet\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PointNet)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.extra_feature_channels = 6\nconfigs.dataset.num_points = 4096\n\n# configs.train.scheduler = Config(optim.lr_scheduler.StepLR)\n# configs.train.scheduler.step_size = 5  # learning rate clip = 1e-5\nconfigs.train.scheduler = Config(optim.lr_scheduler.MultiStepLR)\nconfigs.train.scheduler.milestones = [5, 10, 15, 20, 25, 30, 35]\nconfigs.train.scheduler.gamma = 0.5\n'"
configs/s3dis/pointnet/area5.py,0,b'from utils.config import configs\n\nconfigs.dataset.holdout_area = 5\n'
configs/s3dis/pvcnn/__init__.py,1,"b'import torch.optim as optim\n\nfrom models.s3dis import PVCNN\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PVCNN)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.extra_feature_channels = 6\nconfigs.dataset.num_points = 4096\n\nconfigs.train.optimizer.weight_decay = 1e-5\n# train: scheduler\nconfigs.train.scheduler = Config(optim.lr_scheduler.CosineAnnealingLR)\nconfigs.train.scheduler.T_max = configs.train.num_epochs\n'"
configs/s3dis/pvcnn2/__init__.py,1,"b'import torch.optim as optim\n\nfrom models.s3dis import PVCNN2\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PVCNN2)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.extra_feature_channels = 6\nconfigs.dataset.num_points = 8192\n\nconfigs.train.optimizer.weight_decay = 1e-5\n# train: scheduler\nconfigs.train.scheduler = Config(optim.lr_scheduler.CosineAnnealingLR)\nconfigs.train.scheduler.T_max = configs.train.num_epochs\n'"
configs/shapenet/pvcnn/__init__.py,1,"b'import torch.optim as optim\n\nfrom models.shapenet import PVCNN\nfrom utils.config import Config, configs\n\n# model\nconfigs.model = Config(PVCNN)\nconfigs.model.num_classes = configs.data.num_classes\nconfigs.model.num_shapes = configs.data.num_shapes\nconfigs.model.extra_feature_channels = 3\n\nconfigs.train.num_epochs = 250\nconfigs.train.scheduler = Config(optim.lr_scheduler.CosineAnnealingLR)\nconfigs.train.scheduler.T_max = configs.train.num_epochs\n'"
configs/shapenet/pvcnn/c0p25.py,0,b'from utils.config import configs\n\nconfigs.model.width_multiplier = 0.25\n'
configs/shapenet/pvcnn/c0p5.py,0,b'from utils.config import configs\n\nconfigs.model.width_multiplier = 0.5\n'
configs/shapenet/pvcnn/c1.py,0,b''
evaluate/kitti/frustum/__init__.py,0,b''
evaluate/kitti/frustum/eval.py,11,"b'import argparse\nimport os\nimport random\nimport shutil\nimport sys\n\nimport numba\nimport numpy as np\n\nsys.path.append(os.getcwd())\n\n__all__ = [\'evaluate\']\n\n\ndef prepare():\n    from utils.common import get_save_path\n    from utils.config import configs\n    from utils.device import set_cuda_visible_devices\n\n    # since PyTorch jams device selection, we have to parse args before import torch (issue #26790)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'configs\', nargs=\'+\')\n    parser.add_argument(\'--devices\', default=None)\n    args, opts = parser.parse_known_args()\n    if args.devices is not None and args.devices != \'cpu\':\n        gpus = set_cuda_visible_devices(args.devices)\n    else:\n        gpus = []\n\n    print(f\'==> loading configs from {args.configs}\')\n    configs.update_from_modules(*args.configs)\n    # define save path\n    save_path = get_save_path(*args.configs, prefix=\'runs\')\n    os.makedirs(save_path, exist_ok=True)\n    configs.train.save_path = save_path\n    configs.train.best_checkpoint_path = os.path.join(save_path, \'best.pth.tar\')\n\n    # override configs with args\n    configs.update_from_arguments(*opts)\n    if len(gpus) == 0:\n        configs.device = \'cpu\'\n        configs.device_ids = []\n    else:\n        configs.device = \'cuda\'\n        configs.device_ids = gpus\n    if \'dataset\' in configs.evaluate:\n        for k, v in configs.evaluate.dataset.items():\n            configs.dataset[k] = v\n    if \'best_checkpoint_path\' not in configs.evaluate or configs.evaluate.best_checkpoint_path is None:\n        if \'best_checkpoint_path\' in configs.train and configs.train.best_checkpoint_path is not None:\n            configs.evaluate.best_checkpoint_path = configs.train.best_checkpoint_path\n        else:\n            configs.evaluate.best_checkpoint_path = os.path.join(configs.train.save_path, \'best.pth.tar\')\n    assert configs.evaluate.best_checkpoint_path.endswith(\'.pth.tar\')\n    configs.evaluate.predictions_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.predictions\')\n    configs.evaluate.stats_path = configs.evaluate.best_checkpoint_path.replace(\'.pth.tar\', \'.eval.npy\')\n\n    return configs\n\n\ndef evaluate(configs=None):\n    configs = prepare() if configs is None else configs\n\n    import time\n\n    import torch\n    import torch.backends.cudnn as cudnn\n    from torch.utils.data import DataLoader\n    from tqdm import tqdm\n\n    from ..utils import eval_from_files\n\n    ###########\n    # Prepare #\n    ###########\n\n    if configs.device == \'cuda\':\n        cudnn.benchmark = True\n        if configs.get(\'deterministic\', False):\n            cudnn.deterministic = True\n            cudnn.benchmark = False\n    if (\'seed\' not in configs) or (configs.seed is None):\n        configs.seed = torch.initial_seed() % (2 ** 32 - 1)\n\n    if configs.evaluate.num_tests > 1:\n        results = dict()\n        stats_path = os.path.join(configs.evaluate.stats_path.replace(\'.npy\', \'.t\'), \'best.eval.t{}.npy\')\n        predictions_path = os.path.join(configs.evaluate.predictions_path + \'.t\', \'best.predictions.t{}\')\n        os.makedirs(os.path.dirname(stats_path), exist_ok=True)\n        os.makedirs(os.path.dirname(predictions_path), exist_ok=True)\n\n    #################################\n    # Initialize DataLoaders, Model #\n    #################################\n    print(f\'\\n==> loading dataset ""{configs.dataset}""\')\n    dataset = configs.dataset()[configs.dataset.split]\n\n    print(f\'\\n==> creating model ""{configs.model}""\')\n    model = configs.model()\n    if configs.device == \'cuda\':\n        model = torch.nn.DataParallel(model)\n    model = model.to(configs.device)\n\n    if os.path.exists(configs.evaluate.best_checkpoint_path):\n        print(f\'==> loading checkpoint ""{configs.evaluate.best_checkpoint_path}""\')\n        checkpoint = torch.load(configs.evaluate.best_checkpoint_path)\n        model.load_state_dict(checkpoint.pop(\'model\'))\n        del checkpoint\n    else:\n        return\n\n    model.eval()\n\n    for test_index in range(configs.evaluate.num_tests):\n        if test_index == 0:\n            print(configs)\n\n        seed = configs.seed\n        if test_index > 0:\n            seed = random.randint(1, int(time.time())) % (2 ** 32 - 1)\n            print(f\'\\n==> Test [{test_index:02d}/{configs.evaluate.num_tests:02d}] initial seed\\n[seed] = {seed}\')\n        random.seed(seed)\n        np.random.seed(seed)\n        torch.manual_seed(seed)\n\n        if configs.evaluate.num_tests > 1:\n            configs.evaluate.stats_path = stats_path.format(test_index)\n            configs.evaluate.predictions_path = predictions_path.format(test_index)\n\n        if os.path.exists(configs.evaluate.stats_path):\n            print(f\'==> hit {configs.evaluate.stats_path}\')\n            predictions = np.load(configs.evaluate.stats_path)\n            image_ids = write_predictions(configs.evaluate.predictions_path, ids=dataset.data.ids,\n                                          classes=dataset.data.class_names, boxes_2d=dataset.data.boxes_2d,\n                                          predictions=predictions,\n                                          image_id_file_path=configs.evaluate.image_id_file_path)\n            _, current_results = eval_from_files(prediction_folder=configs.evaluate.predictions_path,\n                                                 ground_truth_folder=configs.evaluate.ground_truth_path,\n                                                 image_ids=image_ids, verbose=True)\n            if configs.evaluate.num_tests == 1:\n                return\n            else:\n                for class_name, v in current_results.items():\n                    if class_name not in results:\n                        results[class_name] = dict()\n                    for kind, r in v.items():\n                        if kind not in results[class_name]:\n                            results[class_name][kind] = []\n                        results[class_name][kind].append(r)\n                continue\n\n        loader = DataLoader(\n            dataset, shuffle=False, batch_size=configs.evaluate.batch_size,\n            num_workers=configs.data.num_workers, pin_memory=True,\n            worker_init_fn=lambda worker_id: np.random.seed(seed + worker_id)\n        )\n\n        ##############\n        # Evaluation #\n        ##############\n\n        predictions = np.zeros((len(dataset), 8))\n        size_templates = configs.data.size_templates.to(configs.device)\n        heading_angle_bin_centers = torch.arange(\n            0, 2 * np.pi, 2 * np.pi / configs.data.num_heading_angle_bins).to(configs.device)\n        current_step = 0\n\n        with torch.no_grad():\n            for inputs, targets in tqdm(loader, desc=\'eval\', ncols=0):\n                for k, v in inputs.items():\n                    inputs[k] = v.to(configs.device, non_blocking=True)\n                outputs = model(inputs)\n\n                center = outputs[\'center\']  # (B, 3)\n                heading_scores = outputs[\'heading_scores\']  # (B, NH)\n                heading_residuals = outputs[\'heading_residuals\']  # (B, NH)\n                size_scores = outputs[\'size_scores\']  # (B, NS)\n                size_residuals = outputs[\'size_residuals\']  # (B, NS, 3)\n\n                batch_size = center.size(0)\n                batch_id = torch.arange(batch_size, device=center.device)\n                heading_bin_id = torch.argmax(heading_scores, dim=1)\n                heading = heading_angle_bin_centers[heading_bin_id] + heading_residuals[batch_id, heading_bin_id]  # (B, )\n                size_template_id = torch.argmax(size_scores, dim=1)\n                size = size_templates[size_template_id] + size_residuals[batch_id, size_template_id]  # (B, 3)\n\n                center = center.cpu().numpy()\n                heading = heading.cpu().numpy()\n                size = size.cpu().numpy()\n                rotation_angle = targets[\'rotation_angle\'].cpu().numpy()  # (B, )\n                rgb_score = targets[\'rgb_score\'].cpu().numpy()  # (B, )\n\n                update_predictions(predictions=predictions, center=center, heading=heading, size=size,\n                                   rotation_angle=rotation_angle, rgb_score=rgb_score,\n                                   current_step=current_step, batch_size=batch_size)\n                current_step += batch_size\n\n        np.save(configs.evaluate.stats_path, predictions)\n        image_ids = write_predictions(configs.evaluate.predictions_path, ids=dataset.data.ids,\n                                      classes=dataset.data.class_names, boxes_2d=dataset.data.boxes_2d,\n                                      predictions=predictions, image_id_file_path=configs.evaluate.image_id_file_path)\n        _, current_results = eval_from_files(prediction_folder=configs.evaluate.predictions_path,\n                                             ground_truth_folder=configs.evaluate.ground_truth_path,\n                                             image_ids=image_ids, verbose=True)\n        if configs.evaluate.num_tests == 1:\n            return\n        else:\n            for class_name, v in current_results.items():\n                if class_name not in results:\n                    results[class_name] = dict()\n                for kind, r in v.items():\n                    if kind not in results[class_name]:\n                        results[class_name][kind] = []\n                    results[class_name][kind].append(r)\n    for class_name, v in results.items():\n        print(f\'{class_name}  AP(Average Precision)\')\n        for kind, r in v.items():\n            r = np.asarray(r)\n            m = r.mean(axis=0)\n            s = r.std(axis=0)\n            u = r.max(axis=0)\n            rs = \', \'.join(f\'{mv:.2f} +/- {sv:.2f} ({uv:.2f})\' for mv, sv, uv in zip(m, s, u))\n            print(f\'{kind:<4} AP: {rs}\')\n\n\n@numba.jit()\ndef update_predictions(predictions, center, heading, size, rotation_angle, rgb_score, current_step, batch_size):\n    for b in range(batch_size):\n        l, w, h = size[b]\n        x, y, z = center[b]  # (3)\n        r = rotation_angle[b]\n        t = heading[b]\n        s = rgb_score[b]\n        v_cos = np.cos(r)\n        v_sin = np.sin(r)\n        cx = v_cos * x + v_sin * z  # it should be v_cos * x - v_sin * z, but the rotation angle = -r\n        cy = y + h / 2.0\n        cz = v_cos * z - v_sin * x  # it should be v_sin * x + v_cos * z, but the rotation angle = -r\n        r = r + t\n        while r > np.pi:\n            r = r - 2 * np.pi\n        while r < -np.pi:\n            r = r + 2 * np.pi\n        predictions[current_step + b] = [h, w, l, cx, cy, cz, r, s]\n\n\ndef write_predictions(prediction_path, ids, classes, boxes_2d, predictions, image_id_file_path=None):\n    import pathlib\n\n    # map from idx to list of strings, each string is a line (with \\n)\n    results = {}\n    for i in range(predictions.shape[0]):\n        idx = ids[i]\n        output_str = (\'{} -1 -1 -10 \'\n                      \'{:f} {:f} {:f} {:f} \'\n                      \'{:f} {:f} {:f} {:f} {:f} {:f} {:f} {:f}\\n\'.format(classes[i], *boxes_2d[i][:4], *predictions[i]))\n        if idx not in results:\n            results[idx] = []\n        results[idx].append(output_str)\n\n    # write txt files\n    if os.path.exists(prediction_path):\n        shutil.rmtree(prediction_path)\n    os.mkdir(prediction_path)\n    for k, v in results.items():\n        file_path = os.path.join(prediction_path, f\'{k:06d}.txt\')\n        with open(file_path, \'w\') as f:\n            f.writelines(v)\n\n    if image_id_file_path is not None and os.path.exists(image_id_file_path):\n        with open(image_id_file_path, \'r\') as f:\n            val_ids = f.readlines()\n        for idx in val_ids:\n            idx = idx.strip()\n            file_path = os.path.join(prediction_path, f\'{idx}.txt\')\n            if not os.path.exists(file_path):\n                # print(f\'warning: {file_path} doesn\\\'t exist as indicated in {image_id_file_path}\')\n                pathlib.Path(file_path).touch()\n        return image_id_file_path\n    else:\n        image_ids = sorted([k for k in results.keys()])\n        return image_ids\n\n\nif __name__ == \'__main__\':\n    evaluate()\n'"
evaluate/kitti/utils/__init__.py,0,b'from .common import eval_from_files\n'
evaluate/kitti/utils/common.py,0,"b""# ref: https://github.com/traveller59/kitti-object-eval-python/blob/master/kitti_common.py\n\nimport pathlib\nimport re\n\nimport numpy as np\n\nfrom .eval import get_official_eval_result\n\n__all__ = ['eval_from_files']\n\n\ndef get_label_annotation(label_path):\n    annotations = dict()\n    with open(label_path, 'r') as f:\n        lines = f.readlines()\n    content = [line.strip().split(' ') for line in lines]\n    annotations['name'] = np.array([x[0] for x in content])\n    annotations['truncated'] = np.array([float(x[1]) for x in content])\n    annotations['occluded'] = np.array([int(x[2]) for x in content])\n    annotations['alpha'] = np.array([float(x[3]) for x in content])\n    annotations['bbox'] = np.array([[float(info) for info in x[4:8]] for x in content]).reshape(-1, 4)\n    # dimensions will convert hwl format to standard lhw(camera) format.\n    annotations['dimensions'] = np.array(\n        [[float(info) for info in x[8:11]] for x in content]).reshape(-1, 3)[:, [2, 0, 1]]\n    annotations['location'] = np.array([[float(info) for info in x[11:14]] for x in content]).reshape(-1, 3)\n    annotations['rotation_y'] = np.array([float(x[14]) for x in content]).reshape(-1)\n    if len(content) != 0 and len(content[0]) == 16:  # have score\n        annotations['score'] = np.array([float(x[15]) for x in content])\n    else:\n        annotations['score'] = np.zeros([len(annotations['bbox'])])\n    return annotations\n\n\ndef get_label_annotations(label_folder, image_ids=None):\n    if image_ids is None:\n        file_paths = pathlib.Path(label_folder).glob('*.txt')\n        prog = re.compile(r'^\\d{6}.txt$')\n        file_paths = filter(lambda f: prog.match(f.name), file_paths)\n        image_ids = [int(p.stem) for p in file_paths]\n        image_ids = sorted(image_ids)\n    if not isinstance(image_ids, list):\n        image_ids = list(range(image_ids))\n    annotations = []\n    label_folder = pathlib.Path(label_folder)\n    for idx in image_ids:\n        image_idx = f'{idx:06d}'\n        label_filename = label_folder / (image_idx + '.txt')\n        annotations.append(get_label_annotation(label_filename))\n    return annotations\n\n\ndef eval_from_files(prediction_folder, ground_truth_folder, image_ids=None, verbose=False):\n    prediction_annotations = get_label_annotations(prediction_folder)\n    if isinstance(image_ids, str):\n        with open(image_ids, 'r') as f:\n            lines = f.readlines()\n        image_ids = [int(line) for line in lines]\n    ground_truth_annotations = get_label_annotations(ground_truth_folder, image_ids=image_ids)\n    metrics, results, results_str = get_official_eval_result(\n        gt_annos=ground_truth_annotations, dt_annos=prediction_annotations, current_classes=[0, 1, 2]\n    )\n    if verbose:\n        print(results_str)\n    return metrics, results\n"""
evaluate/kitti/utils/eval.py,0,"b'# ref: https://github.com/traveller59/kitti-object-eval-python/blob/master/eval.py\n\nimport io as sysio\n\nimport numba\nimport numpy as np\n\nfrom .iou import rotate_iou_gpu_eval\n\n\n__all__ = [\'get_official_eval_result\']\n\n\ndef get_map(prec):\n    sums = 0\n    for i in range(0, prec.shape[-1], 4):\n        sums = sums + prec[..., i]\n    return sums / 11 * 100\n\n\ndef get_split_parts(num, num_part):\n    same_part = num // num_part\n    remain_num = num % num_part\n    if remain_num == 0:\n        return [same_part] * num_part\n    else:\n        return [same_part] * num_part + [remain_num]\n\n\n@numba.jit(nopython=True)\ndef image_box_overlap(boxes, query_boxes, criterion=-1):\n    N = boxes.shape[0]\n    K = query_boxes.shape[0]\n    overlaps = np.zeros((N, K), dtype=boxes.dtype)\n    for k in range(K):\n        qbox_area = (query_boxes[k, 2] - query_boxes[k, 0]) * (query_boxes[k, 3] - query_boxes[k, 1])\n        for n in range(N):\n            iw = (min(boxes[n, 2], query_boxes[k, 2]) - max(boxes[n, 0], query_boxes[k, 0]))\n            if iw > 0:\n                ih = (min(boxes[n, 3], query_boxes[k, 3]) - max(boxes[n, 1], query_boxes[k, 1]))\n                if ih > 0:\n                    if criterion == -1:\n                        ua = (boxes[n, 2] - boxes[n, 0]) * (boxes[n, 3] - boxes[n, 1]) + qbox_area - iw * ih\n                    elif criterion == 0:\n                        ua = (boxes[n, 2] - boxes[n, 0]) * (boxes[n, 3] - boxes[n, 1])\n                    elif criterion == 1:\n                        ua = qbox_area\n                    else:\n                        ua = 1.0\n                    overlaps[n, k] = iw * ih / ua\n    return overlaps\n\n\ndef bev_box_overlap(boxes, qboxes, criterion=-1):\n    return rotate_iou_gpu_eval(boxes, qboxes, criterion)\n\n\n@numba.jit(nopython=True)\ndef d3_box_overlap_kernel(boxes, qboxes, rinc, criterion=-1, z_axis=1, z_center=1.0):\n    """"""\n    :param boxes:\n    :param qboxes:\n    :param rinc:\n    :param criterion:\n    :param z_axis: the z (height) axis\n    :param z_center: unified z (height) center of box\n    :return:\n    """"""\n    N, K = boxes.shape[0], qboxes.shape[0]\n    for i in range(N):\n        for j in range(K):\n            if rinc[i, j] > 0:\n                min_z = min(boxes[i, z_axis] + boxes[i, z_axis + 3] * (1 - z_center),\n                            qboxes[j, z_axis] + qboxes[j, z_axis + 3] * (1 - z_center))\n                max_z = max(boxes[i, z_axis] - boxes[i, z_axis + 3] * z_center,\n                            qboxes[j, z_axis] - qboxes[j, z_axis + 3] * z_center)\n                iw = min_z - max_z\n                if iw > 0:\n                    area1 = boxes[i, 3] * boxes[i, 4] * boxes[i, 5]\n                    area2 = qboxes[j, 3] * qboxes[j, 4] * qboxes[j, 5]\n                    inc = iw * rinc[i, j]\n                    if criterion == -1:\n                        ua = (area1 + area2 - inc)\n                    elif criterion == 0:\n                        ua = area1\n                    elif criterion == 1:\n                        ua = area2\n                    else:\n                        ua = 1.0\n                    rinc[i, j] = inc / ua\n                else:\n                    rinc[i, j] = 0.0\n\n\ndef d3_box_overlap(boxes, qboxes, criterion=-1, z_axis=1, z_center=1.0):\n    """"""\n    kitti camera format z_axis=1.\n    """"""\n    bev_axes = list(range(7))\n    bev_axes.pop(z_axis + 3)\n    bev_axes.pop(z_axis)\n    rinc = rotate_iou_gpu_eval(boxes[:, bev_axes], qboxes[:, bev_axes], 2)\n    d3_box_overlap_kernel(boxes, qboxes, rinc, criterion, z_axis, z_center)\n    return rinc\n\n\ndef calculate_iou_partly(gt_annos, dt_annos, metric, num_parts=50, z_axis=1, z_center=1.0):\n    """"""\n    fast iou algorithm. this function can be used independently to do result analysis.\n    :param gt_annos: must from get_label_annos() in kitti_common.py, dict\n    :param dt_annos: must from get_label_annos() in kitti_common.py, dict\n    :param metric: eval type, 0: bbox, 1: bev, 2: 3d\n    :param num_parts: a parameter for fast calculate algorithm, int\n    :param z_axis: height axis, kitti camera use 1, lidar use 2.\n    :param z_center:\n    :return:\n    """"""\n    assert len(gt_annos) == len(dt_annos)\n    total_dt_num = np.stack([len(a[\'name\']) for a in dt_annos], 0)\n    total_gt_num = np.stack([len(a[\'name\']) for a in gt_annos], 0)\n    num_examples = len(gt_annos)\n    split_parts = get_split_parts(num_examples, num_parts)\n    parted_overlaps = []\n    example_idx = 0\n    bev_axes = list(range(3))\n    bev_axes.pop(z_axis)\n    for num_part in split_parts:\n        gt_annos_part = gt_annos[example_idx:example_idx + num_part]\n        dt_annos_part = dt_annos[example_idx:example_idx + num_part]\n        if metric == 0:\n            gt_boxes = np.concatenate([a[\'bbox\'] for a in gt_annos_part], 0)\n            dt_boxes = np.concatenate([a[\'bbox\'] for a in dt_annos_part], 0)\n            overlap_part = image_box_overlap(gt_boxes, dt_boxes)\n        elif metric == 1:\n            loc = np.concatenate([a[\'location\'][:, bev_axes] for a in gt_annos_part], 0)\n            dims = np.concatenate([a[\'dimensions\'][:, bev_axes] for a in gt_annos_part], 0)\n            rots = np.concatenate([a[\'rotation_y\'] for a in gt_annos_part], 0)\n            gt_boxes = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1)\n            loc = np.concatenate([a[\'location\'][:, bev_axes] for a in dt_annos_part], 0)\n            dims = np.concatenate([a[\'dimensions\'][:, bev_axes] for a in dt_annos_part], 0)\n            rots = np.concatenate([a[\'rotation_y\'] for a in dt_annos_part], 0)\n            dt_boxes = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1)\n            overlap_part = bev_box_overlap(gt_boxes, dt_boxes).astype(np.float64)\n        elif metric == 2:\n            loc = np.concatenate([a[\'location\'] for a in gt_annos_part], 0)\n            dims = np.concatenate([a[\'dimensions\'] for a in gt_annos_part], 0)\n            rots = np.concatenate([a[\'rotation_y\'] for a in gt_annos_part], 0)\n            gt_boxes = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1)\n            loc = np.concatenate([a[\'location\'] for a in dt_annos_part], 0)\n            dims = np.concatenate([a[\'dimensions\'] for a in dt_annos_part], 0)\n            rots = np.concatenate([a[\'rotation_y\'] for a in dt_annos_part], 0)\n            dt_boxes = np.concatenate([loc, dims, rots[..., np.newaxis]], axis=1)\n            overlap_part = d3_box_overlap(gt_boxes, dt_boxes, z_axis=z_axis, z_center=z_center).astype(np.float64)\n        else:\n            raise ValueError(\'unknown metric\')\n        parted_overlaps.append(overlap_part)\n        example_idx += num_part\n    overlaps = []\n    example_idx = 0\n    for j, num_part in enumerate(split_parts):\n        gt_num_idx, dt_num_idx = 0, 0\n        for i in range(num_part):\n            gt_box_num = total_gt_num[example_idx + i]\n            dt_box_num = total_dt_num[example_idx + i]\n            overlaps.append(parted_overlaps[j][gt_num_idx:gt_num_idx + gt_box_num, dt_num_idx:dt_num_idx + dt_box_num])\n            gt_num_idx += gt_box_num\n            dt_num_idx += dt_box_num\n        example_idx += num_part\n\n    return overlaps, parted_overlaps, total_gt_num, total_dt_num\n\n\ndef clean_data(gt_anno, dt_anno, current_class, difficulty):\n    _class_names = [\'car\', \'pedestrian\', \'cyclist\', \'van\', \'person_sitting\', \'car\', \'tractor\', \'trailer\']\n    _min_height = [40, 25, 25]\n    _max_occlusion = [0, 1, 2]\n    _max_truncation = [0.15, 0.3, 0.5]\n    dc_bboxes, ignored_gt, ignored_dt = [], [], []\n    current_cls_name = _class_names[current_class].lower()\n    num_gt = len(gt_anno[\'name\'])\n    num_dt = len(dt_anno[\'name\'])\n    num_valid_gt = 0\n    for i in range(num_gt):\n        bbox = gt_anno[\'bbox\'][i]\n        gt_name = gt_anno[\'name\'][i].lower()\n        height = bbox[3] - bbox[1]\n        if gt_name == current_cls_name:\n            valid_class = 1\n        elif current_cls_name == \'Pedestrian\'.lower() and \'Person_sitting\'.lower() == gt_name:\n            valid_class = 0\n        elif current_cls_name == \'Car\'.lower() and \'Van\'.lower() == gt_name:\n            valid_class = 0\n        else:\n            valid_class = -1\n        ignore = False\n        if ((gt_anno[\'occluded\'][i] > _max_occlusion[difficulty])\n                or (gt_anno[\'truncated\'][i] > _max_truncation[difficulty])\n                or (height <= _min_height[difficulty])):\n            ignore = True\n        if valid_class == 1 and not ignore:\n            ignored_gt.append(0)\n            num_valid_gt += 1\n        elif valid_class == 0 or (ignore and (valid_class == 1)):\n            ignored_gt.append(1)\n        else:\n            ignored_gt.append(-1)\n        if gt_anno[\'name\'][i] == \'DontCare\':\n            dc_bboxes.append(gt_anno[\'bbox\'][i])\n    for i in range(num_dt):\n        if dt_anno[\'name\'][i].lower() == current_cls_name:\n            valid_class = 1\n        else:\n            valid_class = -1\n        height = abs(dt_anno[\'bbox\'][i, 3] - dt_anno[\'bbox\'][i, 1])\n        if height < _min_height[difficulty]:\n            ignored_dt.append(1)\n        elif valid_class == 1:\n            ignored_dt.append(0)\n        else:\n            ignored_dt.append(-1)\n\n    return num_valid_gt, ignored_gt, ignored_dt, dc_bboxes\n\n\ndef _prepare_data(gt_annos, dt_annos, current_class, difficulty):\n    gt_datas_list = []\n    dt_datas_list = []\n    total_dc_num = []\n    ignored_gts, ignored_dets, dontcares = [], [], []\n    total_num_valid_gt = 0\n    for i in range(len(gt_annos)):\n        rets = clean_data(gt_annos[i], dt_annos[i], current_class, difficulty)\n        num_valid_gt, ignored_gt, ignored_det, dc_bboxes = rets\n        ignored_gts.append(np.array(ignored_gt, dtype=np.int64))\n        ignored_dets.append(np.array(ignored_det, dtype=np.int64))\n        if len(dc_bboxes) == 0:\n            dc_bboxes = np.zeros((0, 4)).astype(np.float64)\n        else:\n            dc_bboxes = np.stack(dc_bboxes, 0).astype(np.float64)\n        total_dc_num.append(dc_bboxes.shape[0])\n        dontcares.append(dc_bboxes)\n        total_num_valid_gt += num_valid_gt\n        gt_datas = np.concatenate([gt_annos[i][\'bbox\'], gt_annos[i][\'alpha\'][..., np.newaxis]], 1)\n        dt_datas = np.concatenate([dt_annos[i][\'bbox\'], dt_annos[i][\'alpha\'][..., np.newaxis],\n                                   dt_annos[i][\'score\'][..., np.newaxis]], 1)\n        gt_datas_list.append(gt_datas)\n        dt_datas_list.append(dt_datas)\n    total_dc_num = np.stack(total_dc_num, axis=0)\n    return gt_datas_list, dt_datas_list, ignored_gts, ignored_dets, dontcares, total_dc_num, total_num_valid_gt\n\n\n@numba.jit(nopython=True)\ndef compute_statistics_jit(overlaps, gt_datas, dt_datas, ignored_gt, ignored_det, dc_bboxes, metric, min_overlap,\n                           thresh=0, compute_fp=False, compute_aos=False):\n    det_size = dt_datas.shape[0]\n    gt_size = gt_datas.shape[0]\n    dt_scores = dt_datas[:, -1]\n    dt_alphas = dt_datas[:, 4]\n    gt_alphas = gt_datas[:, 4]\n    dt_bboxes = dt_datas[:, :4]\n\n    assigned_detection = [False] * det_size\n    ignored_threshold = [False] * det_size\n    if compute_fp:\n        for i in range(det_size):\n            if dt_scores[i] < thresh:\n                ignored_threshold[i] = True\n    _no_detection = -10000000\n    tp, fp, fn, similarity = 0, 0, 0, 0\n    thresholds = np.zeros((gt_size, ))\n    thresh_idx = 0\n    delta = np.zeros((gt_size, ))\n    delta_idx = 0\n    for i in range(gt_size):\n        if ignored_gt[i] == -1:\n            continue\n        det_idx = -1\n        valid_detection = _no_detection\n        max_overlap = 0\n        assigned_ignored_det = False\n\n        for j in range(det_size):\n            if ignored_det[j] == -1:\n                continue\n            if assigned_detection[j]:\n                continue\n            if ignored_threshold[j]:\n                continue\n            overlap = overlaps[j, i]\n            dt_score = dt_scores[j]\n            if not compute_fp and (overlap > min_overlap) and dt_score > valid_detection:\n                det_idx = j\n                valid_detection = dt_score\n            elif (compute_fp and (overlap > min_overlap) and (overlap > max_overlap or assigned_ignored_det)\n                  and ignored_det[j] == 0):\n                max_overlap = overlap\n                det_idx = j\n                valid_detection = 1\n                assigned_ignored_det = False\n            elif compute_fp and (overlap > min_overlap) and (valid_detection == _no_detection) and ignored_det[j] == 1:\n                det_idx = j\n                valid_detection = 1\n                assigned_ignored_det = True\n\n        if (valid_detection == _no_detection) and ignored_gt[i] == 0:\n            fn += 1\n        elif (valid_detection != _no_detection) and (ignored_gt[i] == 1 or ignored_det[det_idx] == 1):\n            assigned_detection[det_idx] = True\n        elif valid_detection != _no_detection:\n            # only a tp add a threshold.\n            tp += 1\n            thresholds[thresh_idx] = dt_scores[det_idx]\n            thresh_idx += 1\n            if compute_aos:\n                delta[delta_idx] = gt_alphas[i] - dt_alphas[det_idx]\n                delta_idx += 1\n            assigned_detection[det_idx] = True\n\n    if compute_fp:\n        for i in range(det_size):\n            if not (assigned_detection[i] or ignored_det[i] == -1 or ignored_det[i] == 1 or ignored_threshold[i]):\n                fp += 1\n        nstuff = 0\n        if metric == 0:\n            overlaps_dt_dc = image_box_overlap(dt_bboxes, dc_bboxes, 0)\n            for i in range(dc_bboxes.shape[0]):\n                for j in range(det_size):\n                    if assigned_detection[j]:\n                        continue\n                    if ignored_det[j] == -1 or ignored_det[j] == 1:\n                        continue\n                    if ignored_threshold[j]:\n                        continue\n                    if overlaps_dt_dc[j, i] > min_overlap:\n                        assigned_detection[j] = True\n                        nstuff += 1\n        fp -= nstuff\n        if compute_aos:\n            tmp = np.zeros((fp + delta_idx, ))\n            for i in range(delta_idx):\n                tmp[i + fp] = (1.0 + np.cos(delta[i])) / 2.0\n            if tp > 0 or fp > 0:\n                similarity = np.sum(tmp)\n            else:\n                similarity = -1\n    return tp, fp, fn, similarity, thresholds[:thresh_idx]\n\n\n@numba.jit\ndef get_thresholds(scores: np.ndarray, num_gt, num_sample_pts=41):\n    scores.sort()\n    scores = scores[::-1]\n    current_recall = 0\n    thresholds = []\n    for i, score in enumerate(scores):\n        l_recall = (i + 1) / num_gt\n        if i < (len(scores) - 1):\n            r_recall = (i + 2) / num_gt\n        else:\n            r_recall = l_recall\n        if (((r_recall - current_recall) < (current_recall - l_recall))\n                and (i < (len(scores) - 1))):\n            continue\n        thresholds.append(score)\n        current_recall += 1 / (num_sample_pts - 1.0)\n    return thresholds\n\n\n@numba.jit(nopython=True)\ndef fused_compute_statistics(overlaps, pr, gt_nums, dt_nums, dc_nums, gt_datas, dt_datas, dontcares,\n                             ignored_gts, ignored_dets, metric, min_overlap, thresholds, compute_aos=False):\n    gt_num = 0\n    dt_num = 0\n    dc_num = 0\n    for i in range(gt_nums.shape[0]):\n        for t, thresh in enumerate(thresholds):\n            overlap = overlaps[dt_num:dt_num + dt_nums[i], gt_num:gt_num + gt_nums[i]]\n            gt_data = gt_datas[gt_num:gt_num + gt_nums[i]]\n            dt_data = dt_datas[dt_num:dt_num + dt_nums[i]]\n            ignored_gt = ignored_gts[gt_num:gt_num + gt_nums[i]]\n            ignored_det = ignored_dets[dt_num:dt_num + dt_nums[i]]\n            dontcare = dontcares[dc_num:dc_num + dc_nums[i]]\n            tp, fp, fn, similarity, _ = compute_statistics_jit(\n                overlap, gt_data, dt_data, ignored_gt, ignored_det, dontcare, metric,\n                min_overlap=min_overlap, thresh=thresh, compute_fp=True, compute_aos=compute_aos)\n            pr[t, 0] += tp\n            pr[t, 1] += fp\n            pr[t, 2] += fn\n            if similarity != -1:\n                pr[t, 3] += similarity\n        gt_num += gt_nums[i]\n        dt_num += dt_nums[i]\n        dc_num += dc_nums[i]\n\n\ndef eval_class(gt_annos, dt_annos, current_classes, difficulties, metric, min_overlaps, compute_aos=False, z_axis=1,\n               z_center=1.0, num_parts=50):\n    """"""\n    Kitti eval. support 2d/bev/3d/aos eval. support 0.5:0.05:0.95 coco AP.\n    :param gt_annos: must from get_label_annos() in kitti_common.py, dict\n    :param dt_annos: must from get_label_annos() in kitti_common.py, dict\n    :param current_classes: 0: car, 1: pedestrian, 2: cyclist, int\n    :param difficulties: eval difficulty, 0: easy, 1: normal, 2: hard, int\n    :param metric: eval type, 0: bbox, 1: bev, 2: 3d, int\n    :param min_overlaps: [[0.7, 0.5, 0.5], [0.7, 0.5, 0.5], [0.7, 0.5, 0.5]] format: [metric, class]\n                         choose one from matrix above, float\n    :param compute_aos:\n    :param z_axis:\n    :param z_center:\n    :param num_parts:\n    :return: dict of recall, precision and aos\n    """"""\n    assert len(gt_annos) == len(dt_annos)\n    num_examples = len(gt_annos)\n    split_parts = get_split_parts(num_examples, num_parts)\n\n    rets = calculate_iou_partly(dt_annos, gt_annos, metric, num_parts, z_axis=z_axis, z_center=z_center)\n    overlaps, parted_overlaps, total_dt_num, total_gt_num = rets\n    _n_sample_pts = 41\n    num_min_overlap = len(min_overlaps)\n    num_class = len(current_classes)\n    num_difficulty = len(difficulties)\n    precision = np.zeros([num_class, num_difficulty, num_min_overlap, _n_sample_pts])\n    aos = np.zeros([num_class, num_difficulty, num_min_overlap, _n_sample_pts])\n    all_thresholds = np.zeros([num_class, num_difficulty, num_min_overlap, _n_sample_pts])\n    for m, current_class in enumerate(current_classes):\n        for l, difficulty in enumerate(difficulties):\n            rets = _prepare_data(gt_annos, dt_annos, current_class, difficulty)\n            (gt_datas_list, dt_datas_list, ignored_gts, ignored_dets,\n             dontcares, total_dc_num, total_num_valid_gt) = rets\n            for k, min_overlap in enumerate(min_overlaps[:, metric, m]):\n                thresholdss = []\n                for i in range(len(gt_annos)):\n                    rets = compute_statistics_jit(overlaps[i], gt_datas_list[i], dt_datas_list[i], ignored_gts[i],\n                                                  ignored_dets[i], dontcares[i], metric, min_overlap=min_overlap,\n                                                  thresh=0, compute_fp=False)\n                    tp, fp, fn, similarity, thresholds = rets\n                    thresholdss += thresholds.tolist()\n                thresholdss = np.array(thresholdss)\n                thresholds = get_thresholds(thresholdss, total_num_valid_gt)\n                thresholds = np.array(thresholds)\n                all_thresholds[m, l, k, :len(thresholds)] = thresholds\n                pr = np.zeros([len(thresholds), 4])\n                idx = 0\n                for j, num_part in enumerate(split_parts):\n                    gt_datas_part = np.concatenate(gt_datas_list[idx:idx + num_part], 0)\n                    dt_datas_part = np.concatenate(dt_datas_list[idx:idx + num_part], 0)\n                    dc_datas_part = np.concatenate(dontcares[idx:idx + num_part], 0)\n                    ignored_dets_part = np.concatenate(ignored_dets[idx:idx + num_part], 0)\n                    ignored_gts_part = np.concatenate(ignored_gts[idx:idx + num_part], 0)\n                    fused_compute_statistics(parted_overlaps[j], pr, total_gt_num[idx:idx + num_part],\n                                             total_dt_num[idx:idx + num_part], total_dc_num[idx:idx + num_part],\n                                             gt_datas_part, dt_datas_part, dc_datas_part, ignored_gts_part,\n                                             ignored_dets_part, metric, min_overlap=min_overlap, thresholds=thresholds,\n                                             compute_aos=compute_aos)\n                    idx += num_part\n                for i in range(len(thresholds)):\n                    precision[m, l, k, i] = pr[i, 0] / (pr[i, 0] + pr[i, 1])\n                    if compute_aos:\n                        aos[m, l, k, i] = pr[i, 3] / (pr[i, 0] + pr[i, 1])\n                for i in range(len(thresholds)):\n                    precision[m, l, k, i] = np.max(\n                        precision[m, l, k, i:], axis=-1)\n                    if compute_aos:\n                        aos[m, l, k, i] = np.max(aos[m, l, k, i:], axis=-1)\n\n    ret_dict = {\'precision\': precision, \'orientation\': aos, \'thresholds\': all_thresholds, \'min_overlaps\': min_overlaps}\n    return ret_dict\n\n\ndef do_eval(gt_annos, dt_annos, current_classes, min_overlaps, compute_aos=False, difficulties=(0, 1, 2),\n            z_axis=1, z_center=1.0):\n    types = [\'bbox\', \'bev\', \'3d\']\n    metrics = {}\n    for i in range(3):\n        metrics[types[i]] = eval_class(gt_annos, dt_annos, current_classes, difficulties, i, min_overlaps, compute_aos,\n                                       z_axis=z_axis, z_center=z_center)\n    return metrics\n\n\ndef print_str(value, *arg, sstream=None):\n    if sstream is None:\n        sstream = sysio.StringIO()\n    sstream.truncate(0)\n    sstream.seek(0)\n    print(value, *arg, file=sstream)\n    return sstream.getvalue()\n\n\ndef get_official_eval_result(gt_annos, dt_annos, current_classes, difficulties=(0, 1, 2), z_axis=1, z_center=1.0):\n    """"""\n    :param gt_annos: must contains following keys: [bbox, location, dimensions, rotation_y, score]\n    :param dt_annos: must contains following keys: [bbox, location, dimensions, rotation_y, score]\n    :param current_classes:\n    :param difficulties:\n    :param z_axis:\n    :param z_center:\n    :return:\n    """"""\n    min_overlaps = np.array([[[0.7, 0.5, 0.5, 0.7, 0.5, 0.7, 0.7, 0.7],\n                              [0.7, 0.5, 0.5, 0.7, 0.5, 0.7, 0.7, 0.7],\n                              [0.7, 0.5, 0.5, 0.7, 0.5, 0.7, 0.7, 0.7]]])\n    class_to_name = {\n        0: \'Car\',\n        1: \'Pedestrian\',\n        2: \'Cyclist\',\n        3: \'Van\',\n        4: \'Person_sitting\',\n        5: \'car\',\n        6: \'tractor\',\n        7: \'trailer\',\n    }\n    name_to_class = {v: n for n, v in class_to_name.items()}\n    if not isinstance(current_classes, (list, tuple)):\n        current_classes = [current_classes]\n    current_classes_int = []\n    for cur_cls in current_classes:\n        if isinstance(cur_cls, str):\n            current_classes_int.append(name_to_class[cur_cls])\n        else:\n            current_classes_int.append(cur_cls)\n    current_classes = current_classes_int\n    min_overlaps = min_overlaps[:, :, current_classes]\n    # check whether alpha is valid\n    compute_aos = False\n    for anno in dt_annos:\n        if anno[\'alpha\'].shape[0] != 0:\n            if anno[\'alpha\'][0] != -10:\n                compute_aos = True\n            break\n    metrics = do_eval(gt_annos, dt_annos, current_classes, min_overlaps, compute_aos, difficulties,\n                      z_axis=z_axis, z_center=z_center)\n    results_str = \'\'\n    results = dict()\n    for j, cur_cls in enumerate(current_classes):\n        cur_cls_name = class_to_name[cur_cls]\n        # mAP threshold array: [num_min_overlap, metric, class]\n        # mAP result: [num_class, num_diff, num_min_overlap]\n        map_bbox = get_map(metrics[\'bbox\'][\'precision\'][j, :, 0])\n        map_bbox_str = \', \'.join(f\'{v:.2f}\' for v in map_bbox)\n        map_bev = get_map(metrics[\'bev\'][\'precision\'][j, :, 0])\n        map_bev_str = \', \'.join(f\'{v:.2f}\' for v in map_bev)\n        map_3d = get_map(metrics[\'3d\'][\'precision\'][j, :, 0])\n        map_3d_str = \', \'.join(f\'{v:.2f}\' for v in map_3d)\n        results_str += print_str((f\'{cur_cls_name}\'\n                                  \' AP(Average Precision)@{:.2f}, {:.2f}, {:.2f}:\'.format(*min_overlaps[0, :, j])))\n        results_str += print_str(f\'bbox AP:{map_bbox_str}\')\n        results_str += print_str(f\'bev  AP:{map_bev_str}\')\n        results_str += print_str(f\'3d   AP:{map_3d_str}\')\n        if compute_aos:\n            map_aos = get_map(metrics[\'bbox\'][\'orientation\'][j, :, 0])\n            map_aos = \', \'.join(f\'{v:.2f}\' for v in map_aos)\n            results_str += print_str(f\'aos  AP:{map_aos}\')\n        results[cur_cls_name] = {\'bbox\': map_bbox, \'bev\': map_bev, \'3d\': map_3d}\n\n    return metrics, results, results_str\n'"
evaluate/kitti/utils/iou.py,0,"b'# ref: https://github.com/traveller59/second.pytorch/blob/master/second/core/non_max_suppression/nms_gpu.py\n\nimport math\n\nimport numba\nimport numpy as np\nfrom numba import cuda\n\n__all__ = [\'rotate_iou_gpu_eval\']\n\n\n@numba.jit(nopython=True)\ndef div_up(m, n):\n    return m // n + (m % n > 0)\n\n\n@cuda.jit(device=True, inline=True)\ndef rbbox_to_corners(corners, rbbox):\n    # generate clockwise corners and rotate it clockwise\n    angle = rbbox[4]\n    a_cos = math.cos(angle)\n    a_sin = math.sin(angle)\n    center_x = rbbox[0]\n    center_y = rbbox[1]\n    x_d = rbbox[2]\n    y_d = rbbox[3]\n    corners_x = cuda.local.array((4,), dtype=numba.float32)\n    corners_y = cuda.local.array((4,), dtype=numba.float32)\n    corners_x[0] = -x_d / 2\n    corners_x[1] = -x_d / 2\n    corners_x[2] = x_d / 2\n    corners_x[3] = x_d / 2\n    corners_y[0] = -y_d / 2\n    corners_y[1] = y_d / 2\n    corners_y[2] = y_d / 2\n    corners_y[3] = -y_d / 2\n    for i in range(4):\n        corners[2 * i] = a_cos * corners_x[i] + a_sin * corners_y[i] + center_x\n        corners[2 * i + 1] = -a_sin * corners_x[i] + a_cos * corners_y[i] + center_y\n\n\n@cuda.jit(device=True, inline=True)\ndef point_in_quadrilateral(pt_x, pt_y, corners):\n    ab0 = corners[2] - corners[0]\n    ab1 = corners[3] - corners[1]\n    ad0 = corners[6] - corners[0]\n    ad1 = corners[7] - corners[1]\n    ap0 = pt_x - corners[0]\n    ap1 = pt_y - corners[1]\n\n    ab_ab = ab0 * ab0 + ab1 * ab1\n    ab_ap = ab0 * ap0 + ab1 * ap1\n    ad_ad = ad0 * ad0 + ad1 * ad1\n    ad_ap = ad0 * ap0 + ad1 * ap1\n\n    eps = -1e-6\n    return ab_ab - ab_ap >= eps and ab_ap >= eps and ad_ad - ad_ap >= eps and ad_ap >= eps\n\n\n@cuda.jit(device=True, inline=True)\ndef line_segment_intersection(pts1, pts2, i, j, temp_pts):\n    a = cuda.local.array((2,), dtype=numba.float32)\n    b = cuda.local.array((2,), dtype=numba.float32)\n    c = cuda.local.array((2,), dtype=numba.float32)\n    d = cuda.local.array((2,), dtype=numba.float32)\n\n    a[0] = pts1[2 * i]\n    a[1] = pts1[2 * i + 1]\n\n    b[0] = pts1[2 * ((i + 1) % 4)]\n    b[1] = pts1[2 * ((i + 1) % 4) + 1]\n\n    c[0] = pts2[2 * j]\n    c[1] = pts2[2 * j + 1]\n\n    d[0] = pts2[2 * ((j + 1) % 4)]\n    d[1] = pts2[2 * ((j + 1) % 4) + 1]\n    ba_0 = b[0] - a[0]\n    ba_1 = b[1] - a[1]\n    da_0 = d[0] - a[0]\n    ca_0 = c[0] - a[0]\n    da_1 = d[1] - a[1]\n    ca_1 = c[1] - a[1]\n    acd = da_1 * ca_0 > ca_1 * da_0\n    bcd = (d[1] - b[1]) * (c[0] - b[0]) > (c[1] - b[1]) * (d[0] - b[0])\n    if acd != bcd:\n        abc = ca_1 * ba_0 > ba_1 * ca_0\n        abd = da_1 * ba_0 > ba_1 * da_0\n        if abc != abd:\n            dc0 = d[0] - c[0]\n            dc1 = d[1] - c[1]\n            ab_ba = a[0] * b[1] - b[0] * a[1]\n            cd_dc = c[0] * d[1] - d[0] * c[1]\n            dh = ba_1 * dc0 - ba_0 * dc1\n            dx = ab_ba * dc0 - ba_0 * cd_dc\n            dy = ab_ba * dc1 - ba_1 * cd_dc\n            temp_pts[0] = dx / dh\n            temp_pts[1] = dy / dh\n            return True\n    return False\n\n\n@cuda.jit(device=True, inline=True)\ndef quadrilateral_intersection(pts1, pts2, int_pts):\n    num_of_inter = 0\n    for i in range(4):\n        if point_in_quadrilateral(pts1[2 * i], pts1[2 * i + 1], pts2):\n            int_pts[num_of_inter * 2] = pts1[2 * i]\n            int_pts[num_of_inter * 2 + 1] = pts1[2 * i + 1]\n            num_of_inter += 1\n        if point_in_quadrilateral(pts2[2 * i], pts2[2 * i + 1], pts1):\n            int_pts[num_of_inter * 2] = pts2[2 * i]\n            int_pts[num_of_inter * 2 + 1] = pts2[2 * i + 1]\n            num_of_inter += 1\n    temp_pts = cuda.local.array((2,), dtype=numba.float32)\n    for i in range(4):\n        for j in range(4):\n            has_pts = line_segment_intersection(pts1, pts2, i, j, temp_pts)\n            if has_pts:\n                int_pts[num_of_inter * 2] = temp_pts[0]\n                int_pts[num_of_inter * 2 + 1] = temp_pts[1]\n                num_of_inter += 1\n\n    return num_of_inter\n\n\n@cuda.jit(device=True, inline=True)\ndef sort_vertex_in_convex_polygon(int_pts, num_of_inter):\n    if num_of_inter > 0:\n        center = cuda.local.array((2,), dtype=numba.float32)\n        center[:] = 0.0\n        for i in range(num_of_inter):\n            center[0] += int_pts[2 * i]\n            center[1] += int_pts[2 * i + 1]\n        center[0] /= num_of_inter\n        center[1] /= num_of_inter\n        v = cuda.local.array((2,), dtype=numba.float32)\n        vs = cuda.local.array((16,), dtype=numba.float32)\n        for i in range(num_of_inter):\n            v[0] = int_pts[2 * i] - center[0]\n            v[1] = int_pts[2 * i + 1] - center[1]\n            d = math.sqrt(v[0] * v[0] + v[1] * v[1])\n            v[0] = v[0] / d\n            v[1] = v[1] / d\n            if v[1] < 0:\n                v[0] = -2 - v[0]\n            vs[i] = v[0]\n        for i in range(1, num_of_inter):\n            if vs[i - 1] > vs[i]:\n                temp = vs[i]\n                tx = int_pts[2 * i]\n                ty = int_pts[2 * i + 1]\n                j = i\n                while j > 0 and vs[j - 1] > temp:\n                    vs[j] = vs[j - 1]\n                    int_pts[j * 2] = int_pts[j * 2 - 2]\n                    int_pts[j * 2 + 1] = int_pts[j * 2 - 1]\n                    j -= 1\n                vs[j] = temp\n                int_pts[j * 2] = tx\n                int_pts[j * 2 + 1] = ty\n\n\n@cuda.jit(device=True, inline=True)\ndef triangle_area(a, b, c):\n    return ((a[0] - c[0]) * (b[1] - c[1]) - (a[1] - c[1]) * (b[0] - c[0])) / 2.0\n\n\n@cuda.jit(device=True, inline=True)\ndef area(int_pts, num_of_inter):\n    area_val = 0.0\n    for i in range(num_of_inter - 2):\n        area_val += abs(triangle_area(int_pts[:2], int_pts[2 * i + 2:2 * i + 4], int_pts[2 * i + 4:2 * i + 6]))\n    return area_val\n\n\n@cuda.jit(device=True, inline=True)\ndef inter(rbbox1, rbbox2):\n    corners1 = cuda.local.array((8,), dtype=numba.float32)\n    corners2 = cuda.local.array((8,), dtype=numba.float32)\n    intersection_corners = cuda.local.array((16,), dtype=numba.float32)\n\n    rbbox_to_corners(corners1, rbbox1)\n    rbbox_to_corners(corners2, rbbox2)\n\n    num_intersection = quadrilateral_intersection(corners1, corners2, intersection_corners)\n    sort_vertex_in_convex_polygon(intersection_corners, num_intersection)\n    return area(intersection_corners, num_intersection)\n\n\n@cuda.jit(\'(float32[:], float32[:], int32)\', device=True, inline=True)\ndef dev_rotate_iou_eval(rbox1, rbox2, criterion=-1):\n    area1 = rbox1[2] * rbox1[3]\n    area2 = rbox2[2] * rbox2[3]\n    area_inter = inter(rbox1, rbox2)\n    if criterion == -1:\n        return area_inter / (area1 + area2 - area_inter)\n    elif criterion == 0:\n        return area_inter / area1\n    elif criterion == 1:\n        return area_inter / area2\n    else:\n        return area_inter\n\n\n@cuda.jit(\'(int64, int64, float32[:], float32[:], float32[:], int32)\', fastmath=False)\ndef rotate_iou_kernel_eval(N, K, dev_boxes, dev_query_boxes, dev_iou, criterion=-1):\n    threads_per_block = 8 * 8\n    row_start = cuda.blockIdx.x\n    col_start = cuda.blockIdx.y\n    tx = cuda.threadIdx.x\n    row_size = min(N - row_start * threads_per_block, threads_per_block)\n    col_size = min(K - col_start * threads_per_block, threads_per_block)\n    block_boxes = cuda.shared.array(shape=(64 * 5,), dtype=numba.float32)\n    block_qboxes = cuda.shared.array(shape=(64 * 5,), dtype=numba.float32)\n\n    dev_query_box_idx = threads_per_block * col_start + tx\n    dev_box_idx = threads_per_block * row_start + tx\n    if tx < col_size:\n        block_qboxes[tx * 5 + 0] = dev_query_boxes[dev_query_box_idx * 5 + 0]\n        block_qboxes[tx * 5 + 1] = dev_query_boxes[dev_query_box_idx * 5 + 1]\n        block_qboxes[tx * 5 + 2] = dev_query_boxes[dev_query_box_idx * 5 + 2]\n        block_qboxes[tx * 5 + 3] = dev_query_boxes[dev_query_box_idx * 5 + 3]\n        block_qboxes[tx * 5 + 4] = dev_query_boxes[dev_query_box_idx * 5 + 4]\n    if tx < row_size:\n        block_boxes[tx * 5 + 0] = dev_boxes[dev_box_idx * 5 + 0]\n        block_boxes[tx * 5 + 1] = dev_boxes[dev_box_idx * 5 + 1]\n        block_boxes[tx * 5 + 2] = dev_boxes[dev_box_idx * 5 + 2]\n        block_boxes[tx * 5 + 3] = dev_boxes[dev_box_idx * 5 + 3]\n        block_boxes[tx * 5 + 4] = dev_boxes[dev_box_idx * 5 + 4]\n    cuda.syncthreads()\n    if tx < row_size:\n        for i in range(col_size):\n            offset = row_start * threads_per_block * K + col_start * threads_per_block + tx * K + i\n            dev_iou[offset] = dev_rotate_iou_eval(\n                block_qboxes[i * 5:i * 5 + 5], block_boxes[tx * 5:tx * 5 + 5], criterion\n            )\n\n\ndef rotate_iou_gpu_eval(boxes, query_boxes, criterion=-1, device_id=0):\n    """"""\n    rotated box iou running in gpu. 8x faster than cpu version (take 5ms in one example with numba.cuda code).\n    convert from [this project](https://github.com/hongzhenwang/RRPN-revise/tree/master/lib/rotation).\n    :param boxes: rbboxes, format: centers, dims, angles(clockwise when positive), FloatTensor[N, 5]\n    :param query_boxes: FloatTensor[K, 5]\n    :param criterion: optional, default: -1\n    :param device_id: int, optional, default: 0\n    :return:\n    """"""\n    boxes = boxes.astype(np.float32)\n    query_boxes = query_boxes.astype(np.float32)\n    N = boxes.shape[0]\n    K = query_boxes.shape[0]\n    iou = np.zeros((N, K), dtype=np.float32)\n    if N == 0 or K == 0:\n        return iou\n    threads_per_block = 8 * 8\n    cuda.select_device(device_id)\n    blocks_per_grid = (div_up(N, threads_per_block), div_up(K, threads_per_block))\n\n    stream = cuda.stream()\n    with stream.auto_synchronize():\n        boxes_dev = cuda.to_device(boxes.reshape([-1]), stream)\n        query_boxes_dev = cuda.to_device(query_boxes.reshape([-1]), stream)\n        iou_dev = cuda.to_device(iou.reshape([-1]), stream)\n        rotate_iou_kernel_eval[blocks_per_grid, threads_per_block, stream](N, K, boxes_dev, query_boxes_dev,\n                                                                           iou_dev, criterion)\n        iou_dev.copy_to_host(iou.reshape([-1]), stream=stream)\n    return iou.astype(boxes.dtype)\n'"
models/kitti/frustum/__init__.py,0,"b'from models.kitti.frustum.frustum_net import FrustumPointNet, FrustumPointNet2, FrustumPVCNNE\n'"
models/kitti/frustum/center_regression_net.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_mlp_components\n\n__all__ = ['CenterRegressionNet']\n\n\nclass CenterRegressionNet(nn.Module):\n    blocks = (128, 128, 256)\n\n    def __init__(self, num_classes=3, width_multiplier=1):\n        super().__init__()\n        self.in_channels = 3\n        self.num_classes = num_classes\n\n        layers, channels = create_mlp_components(in_channels=self.in_channels, out_channels=self.blocks,\n                                                 classifier=False, dim=2, width_multiplier=width_multiplier)\n        self.features = nn.Sequential(*layers)\n\n        layers, _ = create_mlp_components(in_channels=(channels + num_classes), out_channels=[256, 128, 3],\n                                          classifier=True, dim=1, width_multiplier=width_multiplier)\n        self.regression = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        coords = inputs['coords']\n        one_hot_vectors = inputs['one_hot_vectors']\n        assert one_hot_vectors.dim() == 2  # [B, C]\n\n        features = self.features(coords)\n        features = features.max(dim=-1, keepdim=False).values\n        return self.regression(torch.cat([features, one_hot_vectors], dim=1))\n"""
models/kitti/frustum/frustum_net.py,1,"b""import functools\n\nimport numpy as np\nimport torch.nn as nn\n\nimport modules.functional as F\nfrom models.kitti.frustum.box_estimation import *\nfrom models.kitti.frustum.segmentation import *\nfrom models.kitti.frustum.center_regression_net import CenterRegressionNet\n\n__all__ = ['FrustumPointNet', 'FrustumPointNet2', 'FrustumPVCNNE']\n\n\nclass FrustumNet(nn.Module):\n    def __init__(self, num_classes, instance_segmentation_net, box_estimation_net,\n                 num_heading_angle_bins, num_size_templates, num_points_per_object,\n                 size_templates, extra_feature_channels=1, width_multiplier=1):\n        super().__init__()\n        if not isinstance(width_multiplier, (list, tuple)):\n            width_multiplier = [width_multiplier] * 3\n        self.in_channels = 3 + extra_feature_channels\n        self.num_classes = num_classes\n        self.num_heading_angle_bins = num_heading_angle_bins\n        self.num_size_templates = num_size_templates\n        self.num_points_per_object = num_points_per_object\n\n        self.inst_seg_net = instance_segmentation_net(num_classes=num_classes,\n                                                      extra_feature_channels=extra_feature_channels,\n                                                      width_multiplier=width_multiplier[0])\n        self.center_reg_net = CenterRegressionNet(num_classes=num_classes, width_multiplier=width_multiplier[1])\n        self.box_est_net = box_estimation_net(num_classes=num_classes, num_heading_angle_bins=num_heading_angle_bins,\n                                              num_size_templates=num_size_templates,\n                                              width_multiplier=width_multiplier[2])\n        self.register_buffer('size_templates', size_templates.view(1, self.num_size_templates, 3))\n\n    def forward(self, inputs):\n        features = inputs['features']\n        one_hot_vectors = inputs['one_hot_vectors']\n        assert one_hot_vectors.dim() == 2\n\n        # foreground/background segmentation\n        mask_logits = self.inst_seg_net({'features': features, 'one_hot_vectors': one_hot_vectors})\n        # mask out Background points\n        foreground_coords, foreground_coords_mean, _ = F.logits_mask(\n            coords=features[:, :3, :], logits=mask_logits, num_points_per_object=self.num_points_per_object\n        )\n        # center regression\n        delta_coords = self.center_reg_net({'coords': foreground_coords, 'one_hot_vectors': one_hot_vectors})\n        foreground_coords = foreground_coords - delta_coords.unsqueeze(-1)\n        # box estimation\n        estimation = self.box_est_net({'coords': foreground_coords, 'one_hot_vectors': one_hot_vectors})\n        estimations = estimation.split([3, self.num_heading_angle_bins, self.num_heading_angle_bins,\n                                        self.num_size_templates, self.num_size_templates * 3], dim=-1)\n\n        # parse results\n        outputs = dict()\n        outputs['mask_logits'] = mask_logits\n        outputs['center_reg'] = foreground_coords_mean + delta_coords\n        outputs['center'] = estimations[0] + outputs['center_reg']\n        outputs['heading_scores'] = estimations[1]\n        outputs['heading_residuals_normalized'] = estimations[2]\n        outputs['heading_residuals'] = estimations[2] * (np.pi / self.num_heading_angle_bins)\n        outputs['size_scores'] = estimations[3]\n        size_residuals_normalized = estimations[4].view(-1, self.num_size_templates, 3)\n        outputs['size_residuals_normalized'] = size_residuals_normalized\n        outputs['size_residuals'] = size_residuals_normalized * self.size_templates\n\n        return outputs\n\n\nclass FrustumPointNet(FrustumNet):\n    def __init__(self, num_classes, num_heading_angle_bins, num_size_templates, num_points_per_object,\n                 size_templates, extra_feature_channels=1, width_multiplier=1):\n        super().__init__(num_classes=num_classes, instance_segmentation_net=InstanceSegmentationPointNet,\n                         box_estimation_net=BoxEstimationPointNet, num_heading_angle_bins=num_heading_angle_bins,\n                         num_size_templates=num_size_templates, num_points_per_object=num_points_per_object,\n                         size_templates=size_templates, extra_feature_channels=extra_feature_channels,\n                         width_multiplier=width_multiplier)\n\n\nclass FrustumPointNet2(FrustumNet):\n    def __init__(self, num_classes, num_heading_angle_bins, num_size_templates, num_points_per_object,\n                 size_templates, extra_feature_channels=1, width_multiplier=1):\n        super().__init__(num_classes=num_classes, instance_segmentation_net=InstanceSegmentationPointNet2,\n                         box_estimation_net=BoxEstimationPointNet2, num_heading_angle_bins=num_heading_angle_bins,\n                         num_size_templates=num_size_templates, num_points_per_object=num_points_per_object,\n                         size_templates=size_templates, extra_feature_channels=extra_feature_channels,\n                         width_multiplier=width_multiplier)\n\n\nclass FrustumPVCNNE(FrustumNet):\n    def __init__(self, num_classes, num_heading_angle_bins, num_size_templates, num_points_per_object,\n                 size_templates, extra_feature_channels=1, width_multiplier=1, voxel_resolution_multiplier=1):\n        instance_segmentation_net = functools.partial(InstanceSegmentationPVCNN,\n                                                      voxel_resolution_multiplier=voxel_resolution_multiplier)\n        super().__init__(num_classes=num_classes, instance_segmentation_net=instance_segmentation_net,\n                         box_estimation_net=BoxEstimationPointNet, num_heading_angle_bins=num_heading_angle_bins,\n                         num_size_templates=num_size_templates, num_points_per_object=num_points_per_object,\n                         size_templates=size_templates, extra_feature_channels=extra_feature_channels,\n                         width_multiplier=width_multiplier)\n"""
configs/s3dis/pvcnn/area5/__init__.py,0,b'from utils.config import configs\n\nconfigs.dataset.holdout_area = 5\n'
configs/s3dis/pvcnn/area5/c0p125.py,0,b'from utils.config import configs\n\nconfigs.model.width_multiplier = 0.125\n'
configs/s3dis/pvcnn/area5/c0p25.py,0,b'from utils.config import configs\n\nconfigs.model.width_multiplier = 0.25\n'
configs/s3dis/pvcnn/area5/c1.py,0,b''
configs/s3dis/pvcnn2/area5/__init__.py,0,b'from utils.config import configs\n\nconfigs.dataset.holdout_area = 5\n'
configs/s3dis/pvcnn2/area5/c0p5.py,0,b'from utils.config import configs\n\nconfigs.model.width_multiplier = 0.5\n'
configs/s3dis/pvcnn2/area5/c1.py,0,b''
models/kitti/frustum/box_estimation/__init__.py,0,b'from models.kitti.frustum.box_estimation.pointnet import BoxEstimationPointNet\nfrom models.kitti.frustum.box_estimation.pointnetpp import BoxEstimationPointNet2\n'
models/kitti/frustum/box_estimation/pointnet.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet_components, create_mlp_components\n\n__all__ = ['BoxEstimationPointNet']\n\n\nclass BoxEstimationNet(nn.Module):\n    def __init__(self, num_classes, blocks, num_heading_angle_bins, num_size_templates,\n                 width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = 3\n        self.num_classes = num_classes\n\n        layers, channels_point, _ = create_pointnet_components(\n            blocks=blocks, in_channels=self.in_channels, with_se=False, normalize=True, eps=1e-15,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.features = nn.Sequential(*layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=channels_point + num_classes,\n            out_channels=[512, 256, (3 + num_heading_angle_bins * 2 + num_size_templates * 4)],\n            classifier=True, dim=1, width_multiplier=width_multiplier\n        )\n        self.classifier = nn.Sequential(*layers)\n        # outputs are: center(x, y, z)\n        #              + num_heading_angle_bins * (score, delta angle)\n        #              + num_size_templates * (score, delta x, delta y, delta z)\n\n    def forward(self, inputs):\n        coords = inputs['coords']\n        one_hot_vectors = inputs['one_hot_vectors']\n        assert one_hot_vectors.dim() == 2  # [B, C]\n\n        features, _ = self.features((coords, coords))\n        features = features.max(dim=-1, keepdim=False).values\n        return self.classifier(torch.cat([features, one_hot_vectors], dim=1))\n\n\nclass BoxEstimationPointNet(BoxEstimationNet):\n    blocks = ((128, 2, None), (256, 1, None), (512, 1, None))\n\n    def __init__(self, num_classes=3, num_heading_angle_bins=12, num_size_templates=8, width_multiplier=1):\n        super().__init__(num_classes=num_classes, blocks=self.blocks, num_heading_angle_bins=num_heading_angle_bins,\n                         num_size_templates=num_size_templates, width_multiplier=width_multiplier)\n"""
models/kitti/frustum/box_estimation/pointnetpp.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet2_sa_components, create_mlp_components\n\n__all__ = ['BoxEstimationPointNet2']\n\n\nclass BoxEstimationNet2(nn.Module):\n    def __init__(self, num_classes, sa_blocks, num_heading_angle_bins, num_size_templates,\n                 width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = 3\n        self.num_classes = num_classes\n\n        sa_layers, _, channels_sa_features, num_centers = create_pointnet2_sa_components(\n            sa_blocks=sa_blocks, extra_feature_channels=0, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.features = nn.Sequential(*sa_layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=(channels_sa_features * num_centers + num_classes),\n            out_channels=[512, 256, (3 + num_heading_angle_bins * 2 + num_size_templates * 4)],\n            classifier=True, dim=1, width_multiplier=width_multiplier\n        )\n        self.classifier = nn.Sequential(*layers)\n        # outputs are: center(x, y, z)\n        #              + num_heading_angle_bins * (score, delta angle)\n        #              + num_size_templates * (score, delta x, delta y, delta z)\n\n    def forward(self, inputs):\n        coords = inputs['coords']\n        one_hot_vectors = inputs['one_hot_vectors']\n        assert one_hot_vectors.dim() == 2  # [B, C]\n\n        features, _ = self.features((None, coords))\n        features = features.view(features.size(0), -1)\n        return self.classifier(torch.cat([features, one_hot_vectors], dim=1))\n\n\nclass BoxEstimationPointNet2(BoxEstimationNet2):\n    sa_blocks = [\n        (None, (128, 0.2, 64, (64, 64, 128))),\n        (None, (32, 0.4, 64, (128, 128, 256))),\n        (None, (None, None, None, (256, 256, 512))),\n    ]\n\n    def __init__(self, num_classes=3, num_heading_angle_bins=12, num_size_templates=8, width_multiplier=1):\n        super().__init__(num_classes=num_classes, sa_blocks=self.sa_blocks,\n                         num_heading_angle_bins=num_heading_angle_bins, num_size_templates=num_size_templates,\n                         width_multiplier=width_multiplier)\n"""
models/kitti/frustum/segmentation/__init__.py,0,"b'from models.kitti.frustum.segmentation.pointnet import InstanceSegmentationPointNet, InstanceSegmentationPVCNN\nfrom models.kitti.frustum.segmentation.pointnetpp import InstanceSegmentationPointNet2\n'"
models/kitti/frustum/segmentation/pointnet.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet_components, create_mlp_components\n\n__all__ = ['InstanceSegmentationPointNet', 'InstanceSegmentationPVCNN']\n\n\nclass InstanceSegmentationNet(nn.Module):\n    def __init__(self, num_classes, point_blocks, cloud_blocks, extra_feature_channels,\n                 width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = extra_feature_channels + 3\n        self.num_classes = num_classes\n\n        layers, channels_point, _ = create_pointnet_components(\n            blocks=point_blocks, in_channels=self.in_channels, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.point_features = nn.Sequential(*layers)\n\n        layers, channels_cloud, _ = create_pointnet_components(\n            blocks=cloud_blocks, in_channels=channels_point, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.cloud_features = nn.Sequential(*layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=(channels_point + channels_cloud + num_classes), out_channels=[512, 256, 128, 128, 0.5, 2],\n            classifier=True, dim=2, width_multiplier=width_multiplier\n        )\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        features = inputs['features']\n        num_points = features.size(-1)\n        one_hot_vectors = inputs['one_hot_vectors'].unsqueeze(-1).repeat([1, 1, num_points])\n        assert one_hot_vectors.dim() == 3  # [B, C, N]\n\n        point_features, point_coords = self.point_features((features, features[:, :3, :]))\n        cloud_features, _ = self.cloud_features((point_features, point_coords))\n        cloud_features = cloud_features.max(dim=-1, keepdim=True).values.repeat([1, 1, num_points])\n        return self.classifier(torch.cat([one_hot_vectors, point_features, cloud_features], dim=1))\n\n\nclass InstanceSegmentationPointNet(InstanceSegmentationNet):\n    point_blocks = ((64, 3, None),)\n    cloud_blocks = ((128, 1, None), (1024, 1, None))\n\n    def __init__(self, num_classes=3, extra_feature_channels=1, width_multiplier=1):\n        super().__init__(\n            num_classes=num_classes, point_blocks=self.point_blocks, cloud_blocks=self.cloud_blocks,\n            extra_feature_channels=extra_feature_channels, width_multiplier=width_multiplier\n        )\n\n\nclass InstanceSegmentationPVCNN(InstanceSegmentationNet):\n    point_blocks = ((64, 2, 16), (64, 1, 12), (128, 1, 12), (1024, 1, None))\n    cloud_blocks = ()\n\n    def __init__(self, num_classes=3, extra_feature_channels=1, width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__(\n            num_classes=num_classes, point_blocks=self.point_blocks, cloud_blocks=self.cloud_blocks,\n            extra_feature_channels=extra_feature_channels, width_multiplier=width_multiplier,\n            voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n"""
models/kitti/frustum/segmentation/pointnetpp.py,2,"b""import torch\nimport torch.nn as nn\n\nfrom models.utils import create_pointnet2_sa_components, create_pointnet2_fp_modules, create_mlp_components\n\n__all__ = ['InstanceSegmentationPointNet2']\n\n\nclass InstanceSegmentationNet2(nn.Module):\n    def __init__(self, num_classes, sa_blocks, fp_blocks, extra_feature_channels,\n                 width_multiplier=1, voxel_resolution_multiplier=1):\n        super().__init__()\n        self.in_channels = extra_feature_channels + 3\n        self.num_classes = num_classes\n\n        sa_layers, sa_in_channels, channels_sa_features, _ = create_pointnet2_sa_components(\n            sa_blocks=sa_blocks, extra_feature_channels=extra_feature_channels, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.sa_layers = nn.ModuleList(sa_layers)\n\n        # use one hot vector in the first fp module\n        sa_in_channels[-1] += num_classes\n        fp_layers, channels_fp_features = create_pointnet2_fp_modules(\n            fp_blocks=fp_blocks, in_channels=channels_sa_features, sa_in_channels=sa_in_channels, with_se=False,\n            width_multiplier=width_multiplier, voxel_resolution_multiplier=voxel_resolution_multiplier\n        )\n        self.fp_layers = nn.ModuleList(fp_layers)\n\n        layers, _ = create_mlp_components(\n            in_channels=channels_fp_features, out_channels=[128, 0.3, 2],\n            classifier=True, dim=2, width_multiplier=width_multiplier\n        )\n        self.classifier = nn.Sequential(*layers)\n\n    def forward(self, inputs):\n        features = inputs['features']\n        one_hot_vectors = inputs['one_hot_vectors']\n        assert one_hot_vectors.dim() == 2  # [B, C]\n\n        coords, extra_features = features[:, :3, :].contiguous(), features[:, 3:, :].contiguous()\n        coords_list, in_features_list = [], []\n        for sa_module in self.sa_layers:\n            in_features_list.append(extra_features)\n            coords_list.append(coords)\n            extra_features, coords = sa_module((extra_features, coords))\n        in_features_list[0] = features.contiguous()\n\n        features = torch.cat(\n            [extra_features, one_hot_vectors.unsqueeze(-1).repeat([1, 1, extra_features.size(-1)])], dim=1\n        )\n        for fp_idx, fp_module in enumerate(self.fp_layers):\n            features, coords = fp_module(\n                (coords_list[-1 - fp_idx], coords, features, in_features_list[-1 - fp_idx])\n            )\n        return self.classifier(features)\n\n\nclass InstanceSegmentationPointNet2(InstanceSegmentationNet2):\n    sa_blocks = [\n        (None, (128, [0.2, 0.4, 0.8], [32, 64, 128], [(32, 32, 64), (64, 64, 128), (64, 96, 128)])),\n        (None, (32, [0.4, 0.8, 1.6], [64, 64, 128], [(64, 64, 128), (128, 128, 256), (128, 128, 256)])),\n        (None, (None, None, None, (128, 256, 1024))),\n    ]\n    fp_blocks = [((128, 128), None), ((128, 128), None), ((128, 128), None)]\n\n    def __init__(self, num_classes=3, extra_feature_channels=1, width_multiplier=1):\n        super().__init__(\n            num_classes=num_classes, sa_blocks=self.sa_blocks, fp_blocks=self.fp_blocks,\n            extra_feature_channels=extra_feature_channels, width_multiplier=width_multiplier\n        )\n"""
