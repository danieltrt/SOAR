file_path,api_count,code
refiner/__init__.py,0,b''
refiner/_init_paths.py,0,"b""import os.path as osp\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\nthis_dir = osp.dirname(__file__)\n\nlib_path = osp.join(this_dir, '..')\nadd_path(lib_path)"""
refiner/data.py,1,"b""import os\nfrom torch.utils.data import Dataset\nimport numpy as np\nimport pickle as pkl\nfrom lib.utils.prep_h36m import compute_similarity_transform\nimport logging\n\nMPII_NAMES = [''] * 15\nMPII_NAMES[0] = 'RFoot'\nMPII_NAMES[1] = 'RKnee'\nMPII_NAMES[2] = 'RHip'\nMPII_NAMES[3] = 'LHip'\nMPII_NAMES[4] = 'LKnee'\nMPII_NAMES[5] = 'LFoot'\n# MPII_NAMES[6]  = 'Hip'\nMPII_NAMES[6] = 'Thorax'\nMPII_NAMES[7] = 'Neck/Nose'\nMPII_NAMES[8] = 'Head'\nMPII_NAMES[9] = 'RWrist'\nMPII_NAMES[10] = 'RElbow'\nMPII_NAMES[11] = 'RShoulder'\nMPII_NAMES[12] = 'LShoulder'\nMPII_NAMES[13] = 'LElbow'\nMPII_NAMES[14] = 'LWrist'\n\nlogger = logging.getLogger(__name__)\n\n\nclass Human36M(Dataset):\n    def __init__(self, is_train):\n        fname = 'refiner/data/train.pkl' if is_train else 'refiner/data/valid.pkl'\n\n        self.is_train = is_train\n\n        self.data, self.labels, self.data_mean, \\\n        self.data_std, self.labels_mean, self.labels_std = self.get_db(fname)\n\n        logger.info('loaded %s samples from %s' % (len(self.data), fname))\n\n    def get_db(self, fname):\n        with open(fname, 'rb') as anno_file:\n            anno = pkl.load(anno_file)\n\n        data = np.asarray(anno['inp'], dtype=np.float32).reshape(len(anno['inp']), -1)\n        labels = np.asarray(anno['out'], dtype=np.float32).reshape(len(anno['out']), -1)\n\n        # Remove hip joint\n        data = np.delete(data, np.s_[18:21], axis=1)\n        labels = np.delete(labels, np.s_[18:21], axis=1)\n\n        if os.path.exists('refiner/data/norm.pkl'):\n            with open('refiner/data/norm.pkl', 'rb') as f:\n                data_mean, data_std, labels_mean, labels_std = pkl.load(f)\n        elif self.is_train:\n            data_mean, data_std = data.mean(axis=0), data.std(axis=0)\n            labels_mean, labels_std = labels.mean(axis=0), labels.std(axis=0)\n\n            with open('refiner/data/norm.pkl', 'wb') as f:\n                pkl.dump((data_mean, data_std, labels_mean, labels_std), f)\n\n        data = (data - data_mean) / data_std\n\n        if self.is_train:\n            labels = (labels - labels_mean) / labels_std\n\n        if self.is_train:\n            rnd = np.random.permutation(labels.shape[0])\n            data, labels = data[rnd], labels[rnd]\n\n        return data, labels, data_mean, data_std, labels_mean, labels_std\n\n    def __getitem__(self, index):\n        return self.data[index], self.labels[index]\n\n    def __len__(self):\n        return len(self.labels)\n\n    def evaluate(self, preds):\n        preds = preds * self.labels_std + self.labels_mean\n\n        preds = preds.reshape((preds.shape[0], -1, 3))\n\n        dist = []\n        dist_align = []\n        dist_14 = []\n        dist_14_align = []\n        dist_x = []\n        dist_y = []\n        dist_z = []\n        dist_per_joint = []\n\n        j14 = [0, 1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14]\n\n\n        for i in range(len(preds)):\n\n            pre_3d_kpt = preds[i]\n            gt_3d_kpt = self.labels[i].reshape((-1, 3))\n\n            joint_num = pre_3d_kpt.shape[0]\n\n            # align\n            _, Z, T, b, c = compute_similarity_transform(gt_3d_kpt, pre_3d_kpt, compute_optimal_scale=True)\n            pre_3d_kpt_align = (b * pre_3d_kpt.dot(T)) + c\n\n            diff = (gt_3d_kpt - pre_3d_kpt)\n            diff_align = (gt_3d_kpt - pre_3d_kpt_align)\n\n            e_jt = []\n            e_jt_align = []\n            e_jt_14 = []\n            e_jt_14_align = []\n            e_jt_x = []\n            e_jt_y = []\n            e_jt_z = []\n\n            for n_jt in range(0, joint_num):\n                e_jt.append(np.linalg.norm(diff[n_jt]))\n                e_jt_align.append(np.linalg.norm(diff_align[n_jt]))\n                e_jt_x.append(np.sqrt(diff[n_jt][0] ** 2))\n                e_jt_y.append(np.sqrt(diff[n_jt][1] ** 2))\n                e_jt_z.append(np.sqrt(diff[n_jt][2] ** 2))\n\n            for jt in j14:\n                e_jt_14.append(np.linalg.norm(diff[jt]))\n                e_jt_14_align.append(np.linalg.norm(diff_align[jt]))\n\n            dist.append(np.array(e_jt).mean())\n            dist_align.append(np.array(e_jt_align).mean())\n            dist_14.append(np.array(e_jt_14).mean())\n            dist_14_align.append(np.array(e_jt_14_align).mean())\n            dist_x.append(np.array(e_jt_x).mean())\n            dist_y.append(np.array(e_jt_y).mean())\n            dist_z.append(np.array(e_jt_z).mean())\n            dist_per_joint.append(np.array(e_jt))\n\n\n        per_joint_error = np.array(dist_per_joint).mean(axis=0).tolist()\n        joint_names = MPII_NAMES\n\n        logger.info('=== JOINTS ===')\n        for idx in range(len(joint_names)):\n            logger.info('%s : %s' % (joint_names[idx], per_joint_error[idx]))\n\n        results = {\n            'hm36_17j': np.asarray(dist).mean(),\n            'hm36_17j_align': np.array(dist_align).mean(),\n            'hm36_17j_14': np.asarray(dist_14).mean(),\n            'hm36_17j_14_al': np.array(dist_14_align).mean(),\n            'hm36_17j_x': np.array(dist_x).mean(),\n            'hm36_17j_y': np.array(dist_y).mean(),\n            'hm36_17j_z': np.array(dist_z).mean(),\n        }\n\n        logger.info('=== RESULTS ===')\n        for k, v in results.items():\n            logger.info('%s : %s' % (k, np.array(v).mean()))\n        logger.info('===============')\n\n        return np.asarray(dist).mean()"""
refiner/main.py,6,"b'import os\nimport time\nimport logging\nimport argparse\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.backends.cudnn as cudnn\n\nimport _init_paths\n\nfrom refiner.data import Human36M\nfrom refiner.model import get_model, weight_init\nfrom refiner.utils import lr_decay, AverageMeter, save_ckpt\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--exp\', type=str, default=\'test\', help=\'ID of experiment\')\n    parser.add_argument(\'--load\', type=str, default=None, help=\'path to load a pretrained checkpoint\')\n    parser.add_argument(\'--mode\', type=str, default=\'train\', help=\'mode: [train, test]\')\n    parser.add_argument(\'--num_epochs\', type=int, default=200, help=\'num epochs\')\n    parser.add_argument(\'--lr\', type=float, default=1e-3, help=\'learning rate\')\n    parser.add_argument(\'--lr_decay\', type=int, default=100000, help=\'# steps of lr decay\')\n    parser.add_argument(\'--lr_gamma\', type=float, default=0.96)\n    args = parser.parse_args()\n\n    return args\n\ndef train(model, train_dl, optimizer, glob_step, lr_now, criterion, args, logger):\n\n    losses = AverageMeter()\n\n    model.train()\n\n    start = time.time()\n\n    for i, (inp, tar) in enumerate(train_dl):\n        glob_step += 1\n\n        if glob_step % args.lr_decay == 0 or glob_step == 1:\n            lr_now = lr_decay(optimizer, glob_step, args.lr, args.lr_decay, args.lr_gamma)\n\n        inputs = inp.cuda()\n        targets = tar.cuda()\n\n        outputs = model(inputs)\n\n        optimizer.zero_grad()\n        loss = criterion(outputs[0], targets) + criterion(outputs[1], targets)\n\n        losses.update(loss.item(), targets.size(0))\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.)\n        optimizer.step()\n\n    logger.info(\'Avg Loss: %.5f\' % losses.avg)\n\n    return glob_step, lr_now\n\ndef test(model, test_dl):\n\n    model.eval()\n\n    preds = []\n    for i, (inp, tar) in enumerate(test_dl):\n        inputs = inp.cuda()\n\n        outputs = model(inputs)[-1]\n\n        out_arr = outputs.cpu().detach().numpy()\n\n        for j in range(out_arr.shape[0]):\n            preds.append(out_arr[j])\n\n    preds = np.asarray(preds)\n\n    error = test_dl.dataset.evaluate(preds)\n\n    return error\n\nif __name__ == \'__main__\':\n    args = parse_args()\n\n    err_best = 1000\n\n    log_dir = os.path.join(\'refiner/experiments\', args.exp)\n    time_str = time.strftime(\'%Y-%m-%d-%H-%M\')\n\n    if not os.path.exists(log_dir):\n        os.mkdir(log_dir)\n\n    head = \'%(asctime)-15s %(message)s\'\n    logging.basicConfig(filename=os.path.join(log_dir,\'%s_log_%s.log\'%(args.mode, time_str)),\n                        format=head)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    console = logging.StreamHandler()\n    logging.getLogger(\'\').addHandler(console)\n\n    model = get_model(weights=None)\n    model = model.cuda()\n    model.apply(weight_init)\n\n    criterion = nn.MSELoss(reduction=\'mean\').cuda()\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    if args.load:\n        logger.info("">>> loading ckpt from \'{}\'"".format(args.load))\n        ckpt = torch.load(args.load)\n        start_epoch = ckpt[\'epoch\']\n        err_best = ckpt[\'err\']\n        glob_step = ckpt[\'step\']\n        lr_now = ckpt[\'lr\']\n        model.load_state_dict(ckpt[\'state_dict\'])\n        optimizer.load_state_dict(ckpt[\'optimizer\'])\n        logger.info("">>> ckpt loaded (epoch: {} | err: {})"".format(start_epoch, err_best))\n\n    train_dl = torch.utils.data.DataLoader(\n        dataset=Human36M(is_train=True),\n        batch_size=64,\n        shuffle=True,\n        num_workers=8\n    )\n\n    test_dl = torch.utils.data.DataLoader(\n        dataset=Human36M(is_train=False),\n        batch_size=64,\n        shuffle=False,\n        num_workers=8\n    )\n\n    logger.info(""- done."")\n\n    cudnn.benchmark = True\n\n    if args.mode == \'train\':\n\n        logger.info(""Starting training for {} epoch(s)"".format(args.num_epochs))\n\n        glob_step = 0\n        lr_now = args.lr\n\n        for epoch in range(args.num_epochs):\n            logger.info(\'%s | %s | lr: %.6f\' % (epoch, args.num_epochs, lr_now))\n\n            glob_step, lr_now = train(model, train_dl, optimizer, glob_step, lr_now, criterion, args, logger)\n\n            logger.info(\'Evaluation\')\n\n            error = test(model, test_dl)\n\n            is_best = error < err_best\n            err_best = min(error, err_best)\n\n            save_ckpt({\'epoch\': epoch + 1,\n                       \'lr\': lr_now,\n                       \'step\': glob_step,\n                       \'err\': error,\n                       \'state_dict\': model.state_dict(),\n                       \'optimizer\': optimizer.state_dict()},\n                      ckpt_path=log_dir,\n                      is_best=is_best)\n            if is_best:\n                logger.info(\'Found new best, error: %s\' % error)\n\n    elif args.mode == \'test\':\n        test(model, test_dl)\n    else:\n        print(\'mode input error!\')'"
refiner/model.py,3,"b""from __future__ import absolute_import\nfrom __future__ import print_function\n\nimport torch.nn as nn\nimport torch\n\n\ndef weight_init(m):\n    if isinstance(m, nn.Linear):\n        nn.init.kaiming_normal_(m.weight)\n        # nn.init.constant_(m.weight, 0.)\n        # nn.init.constant_(m.bias, 0.)\n\n\nclass LinearPG(nn.Module):\n    def __init__(self, linear_size, p_dropout=0.5, bias=True, bn=True, leaky=False):\n        super(LinearPG, self).__init__()\n        self.l_size = linear_size\n        self.bn = bn\n        self.leaky = leaky\n\n        if self.leaky:\n            self.relu = nn.LeakyReLU(inplace=True)\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        self.dropout = nn.Dropout(p_dropout)\n\n        self.w1 = nn.Linear(self.l_size, self.l_size, bias=bias)\n        self.w2 = nn.Linear(self.l_size, self.l_size, bias=bias)\n        self.w3 = nn.Linear(self.l_size, self.l_size, bias=bias)\n        self.w4 = nn.Linear(self.l_size, self.l_size, bias=bias)\n\n        if self.bn:\n            self.batch_norm1 = nn.BatchNorm1d(self.l_size)\n            self.batch_norm2 = nn.BatchNorm1d(self.l_size)\n            self.batch_norm3 = nn.BatchNorm1d(self.l_size)\n            self.batch_norm4 = nn.BatchNorm1d(self.l_size)\n\n    def forward(self, x):\n        y = self.w1(x)\n        if self.bn:\n            y = self.batch_norm1(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        y = self.w2(y)\n        if self.bn:\n            y = self.batch_norm2(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        out = x + y\n\n        y = self.w3(out)\n        if self.bn:\n            y = self.batch_norm3(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        y = self.w4(y)\n        if self.bn:\n            y = self.batch_norm4(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        out = out + y\n\n        return out\n\n\nclass LinearModelPG(nn.Module):\n    def __init__(self,\n                 linear_size=1024,\n                 num_stage=2,\n                 p_dropout=0.5,\n                 input_size=15*3,\n                 output_size=15*3,\n                 bias=True,\n                 bn=True,\n                 leaky=False):\n        super(LinearModelPG, self).__init__()\n\n        self.linear_size = linear_size\n        self.bn = bn\n        self.leaky = leaky\n        self.p_dropout = p_dropout\n        self.num_stage = num_stage\n        # 2d joints\n        self.input_size =  input_size\n        # 3d joints\n        self.output_size = output_size\n\n        self.linear_stages = []\n        for l in range(num_stage):\n            self.linear_stages.append(LinearPG(self.linear_size, self.p_dropout, bias=bias, bn=self.bn,\n                                               leaky=self.leaky))\n        self.linear_stages = nn.ModuleList(self.linear_stages)\n\n        self.w1 = nn.Linear(self.input_size, self.linear_size, bias=bias)\n        self.w2 = nn.Linear(self.linear_size, self.output_size, bias=bias)\n        self.w3 = nn.Linear(self.output_size, self.linear_size, bias=bias)\n        self.w4 = nn.Linear(self.linear_size, self.output_size, bias=bias)\n\n        if self.leaky:\n            self.relu = nn.LeakyReLU(inplace=True)\n        else:\n            self.relu = nn.ReLU(inplace=True)\n\n        if self.bn:\n            self.batch_norm1 = nn.BatchNorm1d(self.linear_size)\n            self.batch_norm3 = nn.BatchNorm1d(self.linear_size)\n\n        self.dropout = nn.Dropout(self.p_dropout)\n\n\n    def forward(self, x):\n        # pre-processing\n        y = self.w1(x)\n        if self.bn:\n            y = self.batch_norm1(y)\n        y = self.relu(y)\n        inp = self.dropout(y)\n\n        s1 = self.linear_stages[0](inp)\n\n        p1 = self.w2(s1)\n\n        y = self.w3(p1)\n        if self.bn:\n            y = self.batch_norm3(y)\n        y = self.relu(y)\n        y = self.dropout(y)\n\n        y = s1 + y + inp\n\n        y = self.linear_stages[1](y)\n\n        y = inp + y\n\n        p2 = self.w4(y)\n\n        return p1, p2\n\n\ndef get_model(weights, **kwargs):\n    model = LinearModelPG(**kwargs)\n    if weights:\n        model.load_state_dict(torch.load(weights)['state_dict'])\n    return model\n\n\nif __name__ == '__main__':\n    model = LinearModelPG(input_size=16*3)\n\n    inp = torch.randn(64,48)\n\n    output = model(inp)\n\n    print(output[0].shape)\n"""
refiner/utils.py,2,"b""import os\nimport torch\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef lr_decay(optimizer, step, lr, decay_step, gamma):\n    lr = lr * gamma ** (step/decay_step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n\ndef step_decay(optimizer, step, lr, decay_step, gamma):\n    lr = lr * gamma ** (step / decay_step)\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    return lr\n\ndef save_ckpt(state, ckpt_path, is_best=True):\n    if is_best:\n        file_path = os.path.join(ckpt_path, 'best.pth.tar')\n        torch.save(state, file_path)\n    else:\n        file_path = os.path.join(ckpt_path, 'last.pth.tar')\n        torch.save(state, file_path)"""
scripts/_init_paths.py,0,"b""import os.path as osp\nimport sys\n\n\ndef add_path(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n\nthis_dir = osp.dirname(__file__)\n\nlib_path = osp.join(this_dir, '..')\nadd_path(lib_path)\n"""
scripts/train.py,13,"b""import argparse\nimport os\nimport pprint\nimport shutil\nimport _init_paths\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\n\nfrom lib.core.config import config\nfrom lib.core.config import update_config\nfrom lib.core.config import update_dir\nfrom lib.core.config import get_model_name\nfrom lib.core.function import train_integral\nfrom lib.core.function import validate_integral, eval_integral\nfrom lib.utils.utils import get_optimizer\nfrom lib.utils.utils import save_checkpoint\nfrom lib.utils.utils import create_logger\n\nimport lib.core.integral_loss as loss\nimport lib.dataset as dataset\nimport lib.models as models\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train keypoints network')\n    # general\n    parser.add_argument('--cfg',\n                        help='experiment configure file name',\n                        required=True,\n                        type=str)\n\n    args, rest = parser.parse_known_args()\n    # update config\n    update_config(args.cfg)\n\n    # training\n    parser.add_argument('--frequent',\n                        help='frequency of logging',\n                        default=config.PRINT_FREQ,\n                        type=int)\n    parser.add_argument('--gpus',\n                        help='gpus',\n                        type=str)\n    parser.add_argument('--workers',\n                        help='num of dataloader workers',\n                        type=int,\n                        default=8)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef reset_config(config, args):\n    if args.gpus:\n        config.GPUS = args.gpus\n    if args.workers:\n        config.WORKERS = args.workers\n\n\ndef main():\n    best_perf = 0.0\n\n    args = parse_args()\n    reset_config(config, args)\n\n    logger, final_output_dir = create_logger(\n        config, args.cfg, 'train')\n\n    logger.info(pprint.pformat(args))\n    logger.info(pprint.pformat(config))\n\n    # cudnn related setting\n    cudnn.benchmark = config.CUDNN.BENCHMARK\n    torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n    torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n\n    model = models.pose3d_resnet.get_pose_net(config, is_train=True)\n\n    # copy model file\n    this_dir = os.path.dirname(__file__)\n\n    shutil.copy2(\n        args.cfg,\n        final_output_dir\n    )\n\n    gpus = [int(i) for i in config.GPUS.split(',')]\n    model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n\n    # define loss function (criterion) and optimizer\n    loss_fn = eval('loss.'+config.LOSS.FN)\n    criterion = loss_fn(num_joints=config.MODEL.NUM_JOINTS, norm=config.LOSS.NORM).cuda()\n\n    # define training, validation and evaluation routines\n    train = train_integral\n    validate = validate_integral\n    evaluate = eval_integral\n\n    optimizer = get_optimizer(config, model)\n\n    lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n        optimizer, config.TRAIN.LR_STEP, config.TRAIN.LR_FACTOR\n    )\n\n    # Resume from a trained model\n    if not(config.MODEL.RESUME is ''):\n        checkpoint = torch.load(config.MODEL.RESUME)\n        if 'epoch' in checkpoint.keys():\n            config.TRAIN.BEGIN_EPOCH = checkpoint['epoch']\n            best_perf = checkpoint['perf']\n            model.load_state_dict(checkpoint['state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            logger.info('=> resume from pretrained model {}'.format(config.MODEL.RESUME))\n        else:\n            model.load_state_dict(checkpoint)\n            logger.info('=> resume from pretrained model {}'.format(config.MODEL.RESUME))\n\n    # Choose the dataset, either Human3.6M or mpii\n    ds = eval('dataset.'+config.DATASET.DATASET)\n\n    # Data loading code\n    train_dataset = ds(\n        cfg=config,\n        root=config.DATASET.ROOT,\n        image_set=config.DATASET.TRAIN_SET,\n        is_train=True\n    )\n    valid_dataset = ds(\n        cfg=config,\n        root=config.DATASET.ROOT,\n        image_set=config.DATASET.TEST_SET,\n        is_train=False\n    )\n\n    train_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN.BATCH_SIZE*len(gpus),\n        shuffle=config.TRAIN.SHUFFLE,\n        num_workers=config.WORKERS,\n        pin_memory=True\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.TEST.BATCH_SIZE*len(gpus),\n        shuffle=False,\n        num_workers=config.WORKERS,\n        pin_memory=True\n    )\n\n    best_model = False\n    for epoch in range(config.TRAIN.BEGIN_EPOCH, config.TRAIN.END_EPOCH):\n        lr_scheduler.step()\n\n        # train for one epoch\n        train(config, train_loader, model, criterion, optimizer, epoch)\n\n        # evaluate on validation set\n        preds_in_patch_with_score = validate(valid_loader, model)\n        acc = evaluate(epoch, preds_in_patch_with_score, valid_loader, final_output_dir, debug=config.DEBUG.DEBUG)\n\n        perf_indicator = 500. - acc if config.DATASET.DATASET == 'h36m' or 'mpii_3dhp' or 'jta' else acc\n\n        if perf_indicator > best_perf:\n            best_perf = perf_indicator\n            best_model = True\n        else:\n            best_model = False\n\n        logger.info('=> saving checkpoint to {}'.format(final_output_dir))\n        save_checkpoint({\n            'epoch': epoch + 1,\n            'model': get_model_name(config),\n            'state_dict': model.state_dict(),\n            'perf': perf_indicator,\n            'optimizer': optimizer.state_dict(),\n        }, best_model, final_output_dir)\n\n    final_model_state_file = os.path.join(final_output_dir,\n                                          'final_state.pth.tar')\n    logger.info('saving final model state to {}'.format(\n        final_model_state_file))\n    torch.save(model.module.state_dict(), final_model_state_file)\n\nif __name__ == '__main__':\n    main()\n"""
scripts/valid.py,10,"b""import argparse\nimport pprint\nimport _init_paths\n\nimport torch\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim\nimport torch.utils.data\nimport torch.utils.data.distributed\n\nfrom lib.core.config import config\nfrom lib.core.config import update_config\nfrom lib.core.config import update_dir\nfrom lib.core.function import validate_integral, eval_integral\nfrom lib.core.integral_loss import L1JointLocationLoss\nfrom lib.utils.utils import create_logger\n\nimport lib.dataset as dataset\nimport lib.models as models\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description='Train keypoints network')\n    # general\n    parser.add_argument('--cfg',\n                        help='experiment configure file name',\n                        required=True,\n                        type=str)\n\n    args, rest = parser.parse_known_args()\n    # update config\n    update_config(args.cfg)\n\n    # training\n    parser.add_argument('--frequent',\n                        help='frequency of logging',\n                        default=config.PRINT_FREQ,\n                        type=int)\n    parser.add_argument('--gpus',\n                        help='gpus',\n                        type=str)\n    parser.add_argument('--workers',\n                        help='num of dataloader workers',\n                        type=int)\n\n    args = parser.parse_args()\n\n    return args\n\n\ndef reset_config(config, args):\n    if args.gpus:\n        config.GPUS = args.gpus\n    if args.workers:\n        config.WORKERS = args.workers\n\n\ndef main():\n    args = parse_args()\n    reset_config(config, args)\n\n    logger, final_output_dir = create_logger(\n        config, args.cfg, 'valid')\n\n    logger.info(pprint.pformat(args))\n    logger.info(pprint.pformat(config))\n\n    # cudnn related setting\n    cudnn.benchmark = config.CUDNN.BENCHMARK\n    torch.backends.cudnn.deterministic = config.CUDNN.DETERMINISTIC\n    torch.backends.cudnn.enabled = config.CUDNN.ENABLED\n\n    model = models.pose3d_resnet.get_pose_net(config, is_train=False)\n\n    gpus = [int(i) for i in config.GPUS.split(',')]\n    model = torch.nn.DataParallel(model, device_ids=gpus).cuda()\n\n    # define loss function (criterion) and optimizer\n    validate = validate_integral\n    evaluate = eval_integral\n\n    # Start from a model\n    if not(config.MODEL.RESUME is ''):\n        checkpoint = torch.load(config.MODEL.RESUME)\n        if 'epoch' in checkpoint.keys():\n            config.TRAIN.BEGIN_EPOCH = checkpoint['epoch']\n            model.load_state_dict(checkpoint['state_dict'])\n            logger.info('=> resume from pretrained model {}'.format(config.MODEL.RESUME))\n        else:\n            model.load_state_dict(checkpoint)\n            logger.info('=> resume from pretrained model {}'.format(config.MODEL.RESUME))\n\n    ds = eval('dataset.' + config.DATASET.DATASET)\n\n    # Data loading code\n    valid_dataset = ds(\n        cfg=config,\n        root=config.DATASET.ROOT,\n        image_set=config.DATASET.TEST_SET,\n        is_train=False\n    )\n    valid_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.TEST.BATCH_SIZE * len(gpus),\n        shuffle=False,\n        num_workers=config.WORKERS,\n        pin_memory=True\n    )\n\n    # evaluate on validation set\n    preds_in_patch_with_score = validate(valid_loader, model)\n    acc = evaluate(0, preds_in_patch_with_score, valid_loader, final_output_dir, config.DEBUG.DEBUG)\n\n    print(acc)\n\n\nif __name__ == '__main__':\n    main()"""
lib/core/config.py,0,"b'import os\nimport yaml\n\nimport numpy as np\nfrom easydict import EasyDict as edict\n\n\nconfig = edict()\n\nconfig.OUTPUT_DIR = \'\'\nconfig.LOG_DIR = \'\'\nconfig.DATA_DIR = \'\'\nconfig.GPUS = \'0\'\nconfig.WORKERS = 8\nconfig.PRINT_FREQ = 20\nconfig.EXP_NAME = \'default\'\n\n# Cudnn related params\nconfig.CUDNN = edict()\nconfig.CUDNN.BENCHMARK = True\nconfig.CUDNN.DETERMINISTIC = False\nconfig.CUDNN.ENABLED = True\n\n# pose_resnet related params\nPOSE_RESNET = edict()\nPOSE_RESNET.NUM_LAYERS = 50\nPOSE_RESNET.DECONV_WITH_BIAS = False\nPOSE_RESNET.NUM_DECONV_LAYERS = 3\nPOSE_RESNET.NUM_DECONV_FILTERS = [256, 256, 256]\nPOSE_RESNET.NUM_DECONV_KERNELS = [4, 4, 4]\nPOSE_RESNET.FINAL_CONV_KERNEL = 1\nPOSE_RESNET.TARGET_TYPE = \'gaussian\'\nPOSE_RESNET.HEATMAP_SIZE = [64, 64]  # width * height, ex: 24 * 32\nPOSE_RESNET.SIGMA = 2\n\nMODEL_EXTRAS = {\n\t\'pose3d_resnet\': POSE_RESNET\n}\n\n# common params for NETWORK\nconfig.MODEL = edict()\nconfig.MODEL.NAME = \'pose3d_resnet\'\nconfig.MODEL.INIT_WEIGHTS = True\nconfig.MODEL.PRETRAINED = \'\'\nconfig.MODEL.RESUME = \'\'\nconfig.MODEL.NUM_JOINTS = 17\nconfig.MODEL.IMAGE_SIZE = [256, 256]  # width * height, ex: 192 * 256\nconfig.MODEL.DEPTH_RES = 64\nconfig.MODEL.VOLUME = True\nconfig.MODEL.EXTRA = MODEL_EXTRAS[config.MODEL.NAME]\n\n# Loss function params\nconfig.LOSS = edict()\nconfig.LOSS.USE_TARGET_WEIGHT = True\nconfig.LOSS.FN = \'L1JointLocationLoss\'\n# h36m specific training params\nconfig.LOSS.USE_SOFT = True\nconfig.LOSS.NORM = False\nconfig.LOSS.DEPTH_LAMBDA = 1.\n\n# DATASET related params\nconfig.DATASET = edict()\nconfig.DATASET.ROOT = \'\'\nconfig.DATASET.DATASET = \'mpii\'\nconfig.DATASET.TRAIN_SET = \'train\'\nconfig.DATASET.TEST_SET = \'valid\'\nconfig.DATASET.DATA_FORMAT = \'jpg\'\nconfig.DATASET.HYBRID_JOINTS_TYPE = \'\'\nconfig.DATASET.SELECT_DATA = False\nconfig.DATASET.TRI = False\nconfig.DATASET.MPII_ORDER = False\n\n# H36M related params\nconfig.DATASET.TRAIN_FRAME = 32\nconfig.DATASET.VAL_FRAME = 64\nconfig.DATASET.NUM_CAMS = 4\nconfig.DATASET.DEPTH_RANGE = 2000 # width, height of the area around the subject in mm\n\n# training data augmentation\nconfig.DATASET.FLIP = True\nconfig.DATASET.SCALE_FACTOR = 0.25\nconfig.DATASET.ROT_FACTOR = 30\nconfig.DATASET.OCCLUSION = False # Assign True if you want to use occlusion augmentation proposed by\n\t\t\t\t\t\t\t\t# Sarandi et al. in https://arxiv.org/abs/1808.09316\nconfig.DATASET.VOC = \'/media/muhammed/Other/RESEARCH/datasets/VOCdevkit/VOC2012\' # path to PASCAL VOC2012 dataset\nconfig.DATASET.BG_AUG = False\nconfig.DATASET.Z_WEIGHT = 1. # weighting parameter for z axis\n\n# train\nconfig.TRAIN = edict()\n\nconfig.TRAIN.LR_FACTOR = 0.1\nconfig.TRAIN.LR_STEP = [90, 110]\nconfig.TRAIN.LR = 0.001\n\nconfig.TRAIN.OPTIMIZER = \'adam\'\nconfig.TRAIN.MOMENTUM = 0.9\nconfig.TRAIN.WD = 0.0001\nconfig.TRAIN.NESTEROV = False\nconfig.TRAIN.GAMMA1 = 0.99\nconfig.TRAIN.GAMMA2 = 0.0\n\nconfig.TRAIN.BEGIN_EPOCH = 0\nconfig.TRAIN.END_EPOCH = 140\n\nconfig.TRAIN.RESUME = False\nconfig.TRAIN.CHECKPOINT = \'\'\n\nconfig.TRAIN.BATCH_SIZE = 32\nconfig.TRAIN.SHUFFLE = True\n\n# testing\nconfig.TEST = edict()\n\n# size of images for each device\nconfig.TEST.BATCH_SIZE = 32\n# Test Model Epoch\nconfig.TEST.FLIP_TEST = False\nconfig.TEST.POST_PROCESS = True\nconfig.TEST.SHIFT_HEATMAP = True\n\nconfig.TEST.USE_GT_BBOX = False\n# nms\nconfig.TEST.OKS_THRE = 0.5\nconfig.TEST.IN_VIS_THRE = 0.0\nconfig.TEST.COCO_BBOX_FILE = \'\'\nconfig.TEST.BBOX_THRE = 1.0\nconfig.TEST.MODEL_FILE = \'\'\nconfig.TEST.IMAGE_THRE = 0.0\nconfig.TEST.NMS_THRE = 1.0\n\n# debug\nconfig.DEBUG = edict()\nconfig.DEBUG.DEBUG = False\nconfig.DEBUG.SAVE_BATCH_IMAGES_GT = False\nconfig.DEBUG.SAVE_BATCH_IMAGES_PRED = False\nconfig.DEBUG.SAVE_HEATMAPS_GT = False\nconfig.DEBUG.SAVE_HEATMAPS_PRED = False\nconfig.DEBUG.SAVE_3D = False\n\n\ndef _update_dict(k, v):\n\tif k == \'DATASET\':\n\t\tif \'MEAN\' in v and v[\'MEAN\']:\n\t\t\tv[\'MEAN\'] = np.array([eval(x) if isinstance(x, str) else x\n\t\t\t\t\t\t\t\t  for x in v[\'MEAN\']])\n\t\tif \'STD\' in v and v[\'STD\']:\n\t\t\tv[\'STD\'] = np.array([eval(x) if isinstance(x, str) else x\n\t\t\t\t\t\t\t\t for x in v[\'STD\']])\n\tif k == \'MODEL\':\n\t\tif \'EXTRA\' in v and \'HEATMAP_SIZE\' in v[\'EXTRA\']:\n\t\t\tif isinstance(v[\'EXTRA\'][\'HEATMAP_SIZE\'], int):\n\t\t\t\tv[\'EXTRA\'][\'HEATMAP_SIZE\'] = np.array(\n\t\t\t\t\t[v[\'EXTRA\'][\'HEATMAP_SIZE\'], v[\'EXTRA\'][\'HEATMAP_SIZE\']])\n\t\t\telse:\n\t\t\t\tv[\'EXTRA\'][\'HEATMAP_SIZE\'] = np.array(\n\t\t\t\t\tv[\'EXTRA\'][\'HEATMAP_SIZE\'])\n\t\tif \'IMAGE_SIZE\' in v:\n\t\t\tif isinstance(v[\'IMAGE_SIZE\'], int):\n\t\t\t\tv[\'IMAGE_SIZE\'] = np.array([v[\'IMAGE_SIZE\'], v[\'IMAGE_SIZE\']])\n\t\t\telse:\n\t\t\t\tv[\'IMAGE_SIZE\'] = np.array(v[\'IMAGE_SIZE\'])\n\tfor vk, vv in v.items():\n\t\tif vk in config[k]:\n\t\t\tconfig[k][vk] = vv\n\t\telse:\n\t\t\traise ValueError(""{}.{} not exist in config.py"".format(k, vk))\n\n\ndef update_config(config_file):\n\texp_config = None\n\twith open(config_file) as f:\n\t\texp_config = edict(yaml.load(f))\n\t\tfor k, v in exp_config.items():\n\t\t\tif k in config:\n\t\t\t\tif isinstance(v, dict):\n\t\t\t\t\t_update_dict(k, v)\n\t\t\t\telse:\n\t\t\t\t\tif k == \'SCALES\':\n\t\t\t\t\t\tconfig[k][0] = (tuple(v))\n\t\t\t\t\telse:\n\t\t\t\t\t\tconfig[k] = v\n\t\t\telse:\n\t\t\t\traise ValueError(""{} not exist in config.py"".format(k))\n\n\ndef gen_config(config_file):\n\tcfg = dict(config)\n\tfor k, v in cfg.items():\n\t\tif isinstance(v, edict):\n\t\t\tcfg[k] = dict(v)\n\n\twith open(config_file, \'w\') as f:\n\t\tyaml.dump(dict(cfg), f, default_flow_style=False)\n\n\ndef update_dir(model_dir, log_dir, data_dir):\n\tif model_dir:\n\t\tconfig.OUTPUT_DIR = model_dir\n\n\tif log_dir:\n\t\tconfig.LOG_DIR = log_dir\n\n\tif data_dir:\n\t\tconfig.DATA_DIR = data_dir\n\n\tconfig.DATASET.ROOT = os.path.join(\n\t\t\tconfig.DATA_DIR, config.DATASET.ROOT)\n\n\tconfig.TEST.COCO_BBOX_FILE = os.path.join(\n\t\t\tconfig.DATA_DIR, config.TEST.COCO_BBOX_FILE)\n\n\tconfig.MODEL.PRETRAINED = os.path.join(\n\t\t\tconfig.DATA_DIR, config.MODEL.PRETRAINED)\n\n\ndef get_model_name(cfg):\n\tname = cfg.MODEL.NAME\n\tfull_name = cfg.MODEL.NAME\n\textra = cfg.MODEL.EXTRA\n\n\tif name == \'pose_resnet\':\n\t\tname = \'{model}_{num_layers}\'.format(\n\t\t\tmodel=name,\n\t\t\tnum_layers=extra.NUM_LAYERS)\n\t\tdeconv_suffix = \'\'.join(\n\t\t\t\'d{}\'.format(num_filters)\n\t\t\tfor num_filters in extra.NUM_DECONV_FILTERS)\n\t\tfull_name = \'{height}x{width}_{name}_{deconv_suffix}\'.format(\n\t\t\theight=cfg.MODEL.IMAGE_SIZE[1],\n\t\t\twidth=cfg.MODEL.IMAGE_SIZE[0],\n\t\t\tname=name,\n\t\t\tdeconv_suffix=deconv_suffix)\n\telif name == \'pose3d_resnet\':\n\t\tname = \'{model}_{num_layers}\'.format(\n\t\t\tmodel=name,\n\t\t\tnum_layers=extra.NUM_LAYERS)\n\t\tsuffix = \'DR%s_S%s_DL%s\'%(cfg.MODEL.DEPTH_RES,\n\t\t\t\t\t\t\t\t  int(cfg.LOSS.USE_SOFT),\n\t\t\t\t\t\t\t\t  int(cfg.LOSS.DEPTH_LAMBDA))\n\t\tfull_name = \'{height}x{width}_{name}_{suffix}\'.format(\n\t\t\theight=cfg.MODEL.IMAGE_SIZE[1],\n\t\t\twidth=cfg.MODEL.IMAGE_SIZE[0],\n\t\t\tname=name,\n\t\t\tsuffix=suffix)\n\telse:\n\t\traise ValueError(\'Unkown model: {}\'.format(cfg.MODEL))\n\n\tprint(name, full_name)\n\treturn name, full_name\n\n\nif __name__ == \'__main__\':\n\timport sys\n\tgen_config(sys.argv[1])\n'"
lib/core/evaluate.py,0,"b""import numpy as np\n\nfrom lib.core.inference import get_max_preds\n\n\ndef calc_dists(preds, target, normalize):\n    preds = preds.astype(np.float32)\n    target = target.astype(np.float32)\n    dists = np.zeros((preds.shape[1], preds.shape[0]))\n    for n in range(preds.shape[0]):\n        for c in range(preds.shape[1]):\n            if target[n, c, 0] > 1 and target[n, c, 1] > 1:\n                normed_preds = preds[n, c, :] / normalize[n]\n                normed_targets = target[n, c, :] / normalize[n]\n                dists[c, n] = np.linalg.norm(normed_preds - normed_targets)\n            else:\n                dists[c, n] = -1\n    return dists\n\n\ndef dist_acc(dists, thr=0.5):\n    ''' Return percentage below threshold while ignoring values with a -1 '''\n    dist_cal = np.not_equal(dists, -1)\n    num_dist_cal = dist_cal.sum()\n    if num_dist_cal > 0:\n        return np.less(dists[dist_cal], thr).sum() * 1.0 / num_dist_cal\n    else:\n        return -1\n\n\ndef accuracy(output, target, hm_type='gaussian', thr=0.5):\n    '''\n    Calculate accuracy according to PCK,\n    but uses ground truth heatmap rather than x,y locations\n    First value to be returned is average accuracy across 'idxs',\n    followed by individual accuracies\n    '''\n    idx = list(range(output.shape[1]))\n    norm = 1.0\n    if hm_type == 'gaussian':\n        pred, _ = get_max_preds(output)\n        target, _ = get_max_preds(target)\n        h = output.shape[2]\n        w = output.shape[3]\n        norm = np.ones((pred.shape[0], 2)) * np.array([h, w]) / 10\n    dists = calc_dists(pred, target, norm)\n\n    acc = np.zeros((len(idx) + 1))\n    avg_acc = 0\n    cnt = 0\n\n    for i in range(len(idx)):\n        acc[i + 1] = dist_acc(dists[idx[i]])\n        if acc[i + 1] >= 0:\n            avg_acc = avg_acc + acc[i + 1]\n            cnt += 1\n\n    avg_acc = avg_acc / cnt if cnt != 0 else 0\n    if cnt != 0:\n        acc[0] = avg_acc\n    return acc, avg_acc, cnt, pred\n"""
lib/core/function.py,1,"b'import logging\nimport time\n\nimport numpy as np\nimport torch\n\nfrom lib.utils.img_utils import trans_coords_from_patch_to_org_3d\nfrom lib.core.integral_loss import get_result_func\nfrom lib.utils.utils import AverageMeter\n\nlogger = logging.getLogger(__name__)\n\n\ndef train_integral(config, train_loader, model, criterion, optimizer, epoch):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n\n    # switch to train mode\n    model.train()\n    end = time.time()\n\n    for i, data in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n\n        batch_data, batch_label, batch_label_weight, meta = data\n\n        optimizer.zero_grad()\n\n        batch_data = batch_data.cuda()\n        batch_label = batch_label.cuda()\n        batch_label_weight = batch_label_weight.cuda()\n\n        batch_size = batch_data.size(0)\n        # compute output\n        preds = model(batch_data)\n\n        loss = criterion(preds, batch_label, batch_label_weight)\n        del batch_data, batch_label, batch_label_weight, preds\n\n        # compute gradient and do update step\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # record loss\n        losses.update(loss.item(), batch_size)\n        del loss\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n\n        if i % config.PRINT_FREQ == 0:\n            msg = \'Epoch: [{0}][{1}/{2}]\\t\' \\\n                  \'Time {batch_time.val:.3f}s ({batch_time.avg:.3f}s)\\t\' \\\n                  \'Speed {speed:.1f} samples/s\\t\' \\\n                  \'Data {data_time.val:.3f}s ({data_time.avg:.3f}s)\\t\' \\\n                  \'Loss {loss.val:.5f} ({loss.avg:.5f})\'.format(\n                      epoch, i, len(train_loader), batch_time=batch_time,\n                      speed=batch_size/batch_time.val,\n                      data_time=data_time, loss=losses)\n            logger.info(msg)\n\n\ndef validate_integral(val_loader, model):\n    print(""Validation stage"")\n    result_func = get_result_func()\n\n    # switch to evaluate mode\n    model.eval()\n\n    preds_in_patch_with_score = []\n    with torch.no_grad():\n        for i, data in enumerate(val_loader):\n            batch_data, batch_label, batch_label_weight, meta = data\n\n            batch_data = batch_data.cuda()\n            batch_label = batch_label.cuda()\n            batch_label_weight = batch_label_weight.cuda()\n\n            # compute output\n            preds = model(batch_data)\n            del batch_data, batch_label, batch_label_weight\n\n\n            preds_in_patch_with_score.append(result_func(256, 256, preds))\n            del preds\n\n        _p = np.asarray(preds_in_patch_with_score)\n\n        # Dirty solution for partial batches\n        if len(_p.shape) < 2:\n            tp = np.zeros(((_p.shape[0] - 1) * _p[0].shape[0] + _p[-1].shape[0], _p[0].shape[1], _p[0].shape[2]))\n\n            start = 0\n            end = _p[0].shape[0]\n\n            for t in _p:\n                tp[start:end] = t\n                start = end\n                end += t.shape[0]\n\n            _p = tp\n        else:\n            _p = _p.reshape((_p.shape[0] * _p.shape[1], _p.shape[2], _p.shape[3]))\n\n        preds_in_patch_with_score = _p[0: len(val_loader.dataset)]\n\n        return preds_in_patch_with_score\n\n\ndef eval_integral(epoch, preds_in_patch_with_score, val_loader, final_output_path, debug=False):\n    print(""Evaluation stage"")\n    # From patch to original image coordinate system\n    imdb_list = val_loader.dataset.db\n    imdb = val_loader.dataset\n\n    preds_in_img_with_score = []\n\n    for n_sample in range(len(val_loader.dataset)):\n        preds_in_img_with_score.append(\n            trans_coords_from_patch_to_org_3d(preds_in_patch_with_score[n_sample], imdb_list[n_sample][\'center_x\'],\n                                              imdb_list[n_sample][\'center_y\'], imdb_list[n_sample][\'width\'],\n                                              imdb_list[n_sample][\'height\'], 256, 256,\n                                              2000, 2000))\n\n    preds_in_img_with_score = np.asarray(preds_in_img_with_score)\n\n    # Evaluate\n    name_value, perf = imdb.evaluate(preds_in_img_with_score.copy(), final_output_path, debug=debug)\n    for name, value in name_value:\n        logger.info(\'Epoch[%d] Validation-%s %f\', epoch, name, value)\n\n    return perf\n'"
lib/core/inference.py,1,"b'import math\n\nimport numpy as np\nimport torch\nfrom torch.nn import functional as F\n\nfrom lib.utils.transforms import transform_preds\nfrom lib.utils.cameras import load_cameras\nfrom lib.utils.cameras import Camera\n\n\ndef get_max_preds(batch_heatmaps):\n    \'\'\'\n    get predictions from score maps\n    heatmaps: numpy.ndarray([batch_size, num_joints, height, width])\n    \'\'\'\n    assert isinstance(batch_heatmaps, np.ndarray), \\\n        \'batch_heatmaps should be numpy.ndarray\'\n    assert batch_heatmaps.ndim == 4, \'batch_images should be 4-ndim\'\n\n    batch_size = batch_heatmaps.shape[0]\n    num_joints = batch_heatmaps.shape[1]\n    width = batch_heatmaps.shape[3]\n    heatmaps_reshaped = batch_heatmaps.reshape((batch_size, num_joints, -1))\n    idx = np.argmax(heatmaps_reshaped, 2)\n    maxvals = np.amax(heatmaps_reshaped, 2)\n\n    maxvals = maxvals.reshape((batch_size, num_joints, 1))\n    idx = idx.reshape((batch_size, num_joints, 1))\n\n    preds = np.tile(idx, (1, 1, 2)).astype(np.float32)\n\n    preds[:, :, 0] = (preds[:, :, 0]) % width\n    preds[:, :, 1] = np.floor((preds[:, :, 1]) / width)\n\n    pred_mask = np.tile(np.greater(maxvals, 0.0), (1, 1, 2))\n    pred_mask = pred_mask.astype(np.float32)\n\n    preds *= pred_mask\n    return preds, maxvals\n\n\ndef get_final_preds(config, batch_heatmaps, center, scale):\n    coords, maxvals = get_max_preds(batch_heatmaps)\n\n    heatmap_height = batch_heatmaps.shape[2]\n    heatmap_width = batch_heatmaps.shape[3]\n\n    # post-processing\n    if config.TEST.POST_PROCESS:\n        for n in range(coords.shape[0]):\n            for p in range(coords.shape[1]):\n                hm = batch_heatmaps[n][p]\n                px = int(math.floor(coords[n][p][0] + 0.5))\n                py = int(math.floor(coords[n][p][1] + 0.5))\n                if 1 < px < heatmap_width-1 and 1 < py < heatmap_height-1:\n                    diff = np.array([hm[py][px+1] - hm[py][px-1],\n                                     hm[py+1][px]-hm[py-1][px]])\n                    coords[n][p] += np.sign(diff) * .25\n\n    preds = coords.copy()\n\n    # Transform back\n    for i in range(coords.shape[0]):\n        preds[i] = transform_preds(coords[i], center[i], scale[i],\n                                   [heatmap_width, heatmap_height])\n\n    return preds, maxvals\n\n\ndef get_per_joint_error(config, output_hm, output_depth, meta, cams, glob, centered=True):\n    """"""\n    :param config: configuration object\n    :param output_hm: predicted 2d heatmap, shape=(N x num_joints x output_shape x output_shape)\n    :param output_depth: predicted depth map, shape=(N x num_joints x depth_res)\n    :param meta: meta values\n    :param cams: cam dictionary\n    :return: per_joint_error: mean error in mm\n    """"""\n\n    batch_size = output_hm.shape[0]\n\n    c = meta[\'center\'].numpy()\n    s = meta[\'scale\'].numpy()\n    gt_joints = meta[\'joints_3d\'].numpy()\n    subjects = meta[\'subject\'].numpy()\n    cam_ids = meta[\'cam_id\'].numpy()\n\n    images_dir = meta[\'image\']\n\n    n_joints = config.MODEL.NUM_JOINTS\n\n    # Get predicted 2D joint coordinates\n    preds, _ = get_final_preds(config, output_hm, c, s)\n\n    # Get predicted depth coordinates in mm\n    preds_depth = get_depth_preds(config, output_depth)\n\n    gt_depth = meta[\'depth\'].numpy()\n    # gt_depth = get_depth_preds(config, gt_depth)\n\n\n    error = 0.\n    for i in range(batch_size):\n        gt_joints_3d = gt_joints[i]\n        file_name = images_dir[i]\n        cam_params = cams[(subjects[i], cam_ids[i])]\n        kps = preds[i]\n\n        cam = Camera(cam_params)\n\n        jj, dZ, _, _, _ = cam.project_point_radial(gt_joints_3d)\n\n        # Unproject points from image plane to camera frame\n        pred_joints_3d = cam.unproject_pts(kps, dZ[0] + preds_depth[i])\n        pred_joints_3d = cam.camera_to_world_frame(pred_joints_3d)\n\n        if centered:\n            # Make joints hip oriented\n            pred_joints_3d = pred_joints_3d - pred_joints_3d[0]\n            gt_joints_3d = gt_joints_3d - gt_joints_3d[0]\n\n        gt_joints_3d = gt_joints_3d.reshape((-1))\n        pred_joints_3d = pred_joints_3d.reshape((-1))\n\n        sqerr = (gt_joints_3d - pred_joints_3d) ** 2\n        dists = np.zeros((n_joints))\n\n        dist_idx = 0\n        for k in np.arange(0, n_joints * 3, 3):\n            # Sum across X,Y, and Z dimenstions to obtain L2 distance\n            dists[dist_idx] = np.sqrt(np.sum(sqerr[k:k + 3]))\n            dist_idx += 1\n        e = np.mean(dists)\n        error += e\n\n        gt_joints_3d = gt_joints_3d.reshape((17, -1))\n        pred_joints_3d = pred_joints_3d.reshape((17, -1))\n\n    per_joint_error = error / batch_size\n    return per_joint_error\n\n\ndef get_depth_preds(cfg, pred):\n    """"""\n    :param cfg: configuration object\n    :param pred_depth: predicted depth map, shape=(N x num_joints x depth_res)\n    :return: depth: distance from camera center in mm, shape=(N x num_joints)\n    """"""\n    batch_size = pred.shape[0]\n\n    num_joints = cfg.MODEL.NUM_JOINTS\n    depth_res = cfg.MODEL.DEPTH_RES\n    depth_range = cfg.DATASET.DEPTH_RANGE\n    depth = np.zeros((batch_size, num_joints))\n\n    pred = pred.reshape((batch_size, num_joints, depth_res))\n\n    for i in range(batch_size):\n        for j in range(num_joints):\n            d = softargmax1d(pred[i,j])\n            d = d / float(depth_res) - 0.5\n            d *= depth_range\n            depth[i,j] = d\n\n    return depth\n\ndef softargmax1d(inp):\n    exp = np.exp(inp)\n    i = np.arange(exp.shape[0])\n    return np.sum(i * (exp / exp.sum()))'"
lib/core/integral_loss.py,16,"b'import torch\nfrom torch.nn import functional as F\nimport numpy as np\nimport torch.nn as nn\nimport math\n\ndef weighted_mse_loss(input, target, weights, size_average, norm=False):\n\n    if norm:\n        input = input / torch.norm(input, 1)\n        target = target / torch.norm(target, 1)\n\n    out = (input - target) ** 2\n    out = out * weights\n    if size_average:\n        return out.sum() / len(input)\n    else:\n        return out.sum()\n\ndef weighted_l1_loss(input, target, weights, size_average, norm=False):\n\n    if norm:\n        input = input / torch.norm(input, 1)\n        target = target / torch.norm(target, 1)\n\n    out = torch.abs(input - target)\n    out = out * weights\n    if size_average:\n        return out.sum() / len(input)\n    else:\n        return out.sum()\n\ndef weighted_smooth_l1_loss(input, target, weights, size_average, norm=False):\n\n    if norm:\n        input = input / torch.norm(input, 1)\n        target = target / torch.norm(target, 1)\n\n    diff = input - target\n    abs = torch.abs(diff)\n    out = torch.where(abs<1., 0.5*diff**2, abs-0.5)\n\n    out = out * weights\n    if size_average:\n        return out.sum() / len(input)\n    else:\n        return out.sum()\n\ndef generate_3d_integral_preds_tensor(heatmaps, num_joints, x_dim, y_dim, z_dim):\n    assert isinstance(heatmaps, torch.Tensor)\n\n    heatmaps = heatmaps.reshape((heatmaps.shape[0], num_joints, z_dim, y_dim, x_dim))\n\n    accu_x = heatmaps.sum(dim=2)\n    accu_x = accu_x.sum(dim=2)\n    accu_y = heatmaps.sum(dim=2)\n    accu_y = accu_y.sum(dim=3)\n    accu_z = heatmaps.sum(dim=3)\n    accu_z = accu_z.sum(dim=3)\n\n    accu_x = accu_x * torch.cuda.comm.broadcast(torch.arange(float(x_dim)), devices=[accu_x.device.index])[0]\n    accu_y = accu_y * torch.cuda.comm.broadcast(torch.arange(float(y_dim)), devices=[accu_y.device.index])[0]\n    accu_z = accu_z * torch.cuda.comm.broadcast(torch.arange(float(z_dim)), devices=[accu_z.device.index])[0]\n\n    accu_x = accu_x.sum(dim=2, keepdim=True)\n    accu_y = accu_y.sum(dim=2, keepdim=True)\n    accu_z = accu_z.sum(dim=2, keepdim=True)\n\n    return accu_x, accu_y, accu_z\n\ndef softmax_integral_tensor(preds, num_joints, output_3d, hm_width, hm_height, hm_depth):\n    # global soft max\n    preds = preds.reshape((preds.shape[0], num_joints, -1))\n    preds = F.softmax(preds, 2)\n\n    # integrate heatmap into joint location\n    if output_3d:\n        x, y, z = generate_3d_integral_preds_tensor(preds, num_joints, hm_width, hm_height, hm_depth)\n    else:\n        assert 0, \'Not Implemented!\'\n    x = x / float(hm_width) - 0.5\n    y = y / float(hm_height) - 0.5\n    z = z / float(hm_depth) - 0.5\n    preds = torch.cat((x, y, z), dim=2)\n    preds = preds.reshape((preds.shape[0], num_joints * 3))\n    return preds\n\ndef _assert_no_grad(tensor):\n    assert not tensor.requires_grad, \\\n        ""nn criterions don\'t compute the gradient w.r.t. targets - please "" \\\n        ""mark these tensors as not requiring gradients""\n\nclass L2JointLocationLoss(nn.Module):\n    def __init__(self, num_joints,size_average=True, reduce=True, norm=False):\n        super(L2JointLocationLoss, self).__init__()\n        self.size_average = size_average\n        self.reduce = reduce\n        self.num_joints = num_joints\n        self.norm = norm\n\n    def forward(self, preds, *args):\n        gt_joints = args[0]\n        gt_joints_vis = args[1]\n\n        num_joints = int(gt_joints_vis.shape[1] / 3)\n        hm_width = preds.shape[-1]\n        hm_height = preds.shape[-2]\n        hm_depth = preds.shape[-3] // self.num_joints\n\n        print(num_joints)\n\n        pred_jts = softmax_integral_tensor(preds, self.num_joints, self.output_3d, hm_width, hm_height, hm_depth)\n\n        _assert_no_grad(gt_joints)\n        _assert_no_grad(gt_joints_vis)\n        return weighted_mse_loss(pred_jts, gt_joints, gt_joints_vis, self.size_average, self.norm)\n\nclass L1JointLocationLoss(nn.Module):\n    def __init__(self, num_joints, size_average=True, reduce=True, norm=False):\n        super(L1JointLocationLoss, self).__init__()\n        self.size_average = size_average\n        self.reduce = reduce\n        self.num_joints = num_joints\n        self.norm = norm\n\n    def forward(self, preds, *args):\n        gt_joints = args[0]\n        gt_joints_vis = args[1]\n\n        hm_width = preds.shape[-1]\n        hm_height = preds.shape[-2]\n        hm_depth = preds.shape[-3] // self.num_joints\n\n        pred_jts = softmax_integral_tensor(preds, self.num_joints, True, hm_width, hm_height, hm_depth)\n\n        _assert_no_grad(gt_joints)\n        _assert_no_grad(gt_joints_vis)\n        return weighted_l1_loss(pred_jts, gt_joints, gt_joints_vis, self.size_average, self.norm)\n\nclass SmoothL1JointLocationLoss(nn.Module):\n    def __init__(self, num_joints, size_average=True, reduce=True, norm=False):\n        super(SmoothL1JointLocationLoss, self).__init__()\n        self.size_average = size_average\n        self.reduce = reduce\n        self.num_joints = num_joints\n        self.norm = norm\n\n    def forward(self, preds, *args):\n        gt_joints = args[0]\n        gt_joints_vis = args[1]\n\n        hm_width = preds.shape[-1]\n        hm_height = preds.shape[-2]\n        hm_depth = preds.shape[-3] // self.num_joints\n\n        pred_jts = softmax_integral_tensor(preds, self.num_joints, True, hm_width, hm_height, hm_depth)\n\n        _assert_no_grad(gt_joints)\n        _assert_no_grad(gt_joints_vis)\n        return weighted_smooth_l1_loss(pred_jts, gt_joints, gt_joints_vis, self.size_average, self.norm)\n\ndef get_loss_func(config):\n    if config.loss_type == \'L1\':\n        return L1JointLocationLoss(config.output_3d)\n    elif config.loss_type == \'L2\':\n        return L2JointLocationLoss(config.output_3d)\n    else:\n        assert 0, \'Error. Unknown heatmap type {}\'.format(config.heatmap_type)\n\ndef generate_joint_location_label(patch_width, patch_height, joints, joints_vis):\n    joints[:, 0] = joints[:, 0] / patch_width - 0.5\n    joints[:, 1] = joints[:, 1] / patch_height - 0.5\n    joints[:, 2] = joints[:, 2] / patch_width\n\n    joints = joints.reshape((-1))\n    joints_vis = joints_vis.reshape((-1))\n    return joints, joints_vis\n\ndef reverse_joint_location_label(patch_width, patch_height, joints):\n    joints = joints.reshape((joints.shape[0] // 3, 3))\n\n    joints[:, 0] = (joints[:, 0] + 0.5) * patch_width\n    joints[:, 1] = (joints[:, 1] + 0.5) * patch_height\n    joints[:, 2] = joints[:, 2] * patch_width\n    return joints\n\ndef get_joint_location_result(patch_width, patch_height, preds):\n    hm_width = preds.shape[-1]\n    hm_height = preds.shape[-2]\n\n    hm_depth = hm_width\n    num_joints = preds.shape[1] // hm_depth\n\n    pred_jts = softmax_integral_tensor(preds, num_joints, True, hm_width, hm_height, hm_depth)\n    coords = pred_jts.detach().cpu().numpy()\n    coords = coords.astype(float)\n    coords = coords.reshape((coords.shape[0], int(coords.shape[1] / 3), 3))\n    # project to original image size\n    coords[:, :, 0] = (coords[:, :, 0] + 0.5) * patch_width\n    coords[:, :, 1] = (coords[:, :, 1] + 0.5) * patch_height\n    coords[:, :, 2] = coords[:, :, 2] * patch_width\n    scores = np.ones((coords.shape[0], coords.shape[1], 1), dtype=float)\n\n    # add score to last dimension\n    coords = np.concatenate((coords, scores), axis=2)\n\n    return coords\n\ndef get_label_func():\n    return generate_joint_location_label\n\ndef get_result_func():\n    return get_joint_location_result\n\ndef merge_flip_func(a, b, flip_pair):\n    # NOTE: flip test of integral is implemented in net_modules.py\n    return a\n\ndef get_merge_func(loss_config):\n    return merge_flip_func\n'"
lib/dataset/JointIntegralDataset.py,1,"b""import logging\nimport numpy as np\nfrom torch.utils.data import Dataset\n\nfrom lib.core.integral_loss import get_label_func\n\nfrom lib.utils.augmentation import load_occluders\n\nH36M_NAMES = ['']*17\nH36M_NAMES[0]  = 'Hip'\nH36M_NAMES[1]  = 'RHip'\nH36M_NAMES[2]  = 'RKnee'\nH36M_NAMES[3]  = 'RFoot'\nH36M_NAMES[4]  = 'LHip'\nH36M_NAMES[5]  = 'LKnee'\nH36M_NAMES[6]  = 'LFoot'\nH36M_NAMES[7] = 'Spine'\nH36M_NAMES[8] = 'Thorax'\nH36M_NAMES[9] = 'Neck/Nose'\nH36M_NAMES[10] = 'Head'\nH36M_NAMES[11] = 'LShoulder'\nH36M_NAMES[12] = 'LElbow'\nH36M_NAMES[13] = 'LWrist'\nH36M_NAMES[14] = 'RShoulder'\nH36M_NAMES[15] = 'RElbow'\nH36M_NAMES[16] = 'RWrist'\n\nMPII_NAMES = ['']*16\nMPII_NAMES[0]  = 'RFoot'\nMPII_NAMES[1]  = 'RKnee'\nMPII_NAMES[2]  = 'RHip'\nMPII_NAMES[3]  = 'LHip'\nMPII_NAMES[4]  = 'LKnee'\nMPII_NAMES[5]  = 'LFoot'\nMPII_NAMES[6]  = 'Hip'\nMPII_NAMES[7]  = 'Thorax'\nMPII_NAMES[8]  = 'Neck/Nose'\nMPII_NAMES[9]  = 'Head'\nMPII_NAMES[10] = 'RWrist'\nMPII_NAMES[11] = 'RElbow'\nMPII_NAMES[12] = 'RShoulder'\nMPII_NAMES[13] = 'LShoulder'\nMPII_NAMES[14] = 'LElbow'\nMPII_NAMES[15] = 'LWrist'\n\nlogger = logging.getLogger(__name__)\n\nH36M_TO_MPII_PERM = np.array([H36M_NAMES.index(h) for h in MPII_NAMES if h != '' and h in H36M_NAMES])\n\n\nclass JointsIntegralDataset(Dataset):\n    def __init__(self, cfg, root, image_set, is_train):\n        self.cfg = cfg\n        self.is_train = is_train\n\n        self.root = root\n        self.image_set = image_set\n\n        self.is_train = is_train\n\n        self.patch_width = cfg.MODEL.IMAGE_SIZE[0]\n        self.patch_height = cfg.MODEL.IMAGE_SIZE[1]\n\n        self.rect_3d_width = 2000.\n        self.rect_3d_height = 2000.\n\n        self.mean = np.array([123.675, 116.280, 103.530])\n        self.std = np.array([58.395, 57.120, 57.375])\n        self.num_cams = cfg.DATASET.NUM_CAMS\n\n        self.label_func = get_label_func()\n\n        self.occluders = load_occluders(cfg.DATASET.VOC) if cfg.DATASET.OCCLUSION and is_train else None\n\n        self.cam_config = []\n        self.parent_ids = None\n        self.db_length = 0\n\n        self.db = []\n\n\n    def __len__(self, ):\n        return self.db_length\n\n\n    def __getitem__(self, idx):\n        raise NotImplementedError\n\n\n    def evaluate(self, preds, save_path=None, debug=False):\n        raise NotImplementedError\n\n\n"""
lib/dataset/__init__.py,0,b'# ------------------------------------------------------------------------------\n# Copyright (c) Microsoft\n# Licensed under the MIT License.\n# Written by Bin Xiao (Bin.Xiao@microsoft.com)\n# ------------------------------------------------------------------------------\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom lib.dataset.h36m import H36M_Integral as h36m\nfrom lib.dataset.mpii_integral import MPIIDataset as mpii_integral\n'
lib/dataset/h36m.py,0,"b""import torch\nimport pickle as pkl\nimport random\nimport os\nimport logging\nimport numpy as np\nimport copy\n\nfrom lib.utils.prep_h36m import CamBackProj, compute_similarity_transform\nfrom lib.dataset.JointIntegralDataset import JointsIntegralDataset, H36M_NAMES, MPII_NAMES\nfrom lib.utils.data_utils import define_actions\nfrom lib.utils.img_utils import get_single_patch_sample\n\nlogger = logging.getLogger(__name__)\n\nH36M_TO_MPII_PERM = np.array([H36M_NAMES.index(h) for h in MPII_NAMES if h != '' and h in H36M_NAMES])\n\n\nclass H36M_Integral(JointsIntegralDataset):\n    def __init__(self, cfg, root, image_set, is_train):\n        super().__init__(cfg, root, image_set, is_train)\n\n        self.parent_ids = np.array([0, 0, 1, 2, 0, 4, 5, 0, 8, 8, 9, 8, 11, 12, 8, 14, 15], dtype=np.int)\n\n        self.cam_config = [[1, 2], [0, 3], [0, 3], [1, 2]] # Camera neighborhoods\n\n        self.db = self._get_train_db() if is_train else self._get_val_db()\n\n        logger.info('=> load {} samples'.format(self.db_length))\n\n\n    def __getitem__(self, idx):\n        if self.is_train and self.cfg.DATASET.TRI:\n            # select a random camera from db\n            cam_1 = np.random.randint(self.num_cams)\n            # select a neighboring camera\n            cam_2 = self.cam_config[cam_1][0] if random.random() <= 0.5 else self.cam_config[cam_1][1]\n\n            # get db records for each view\n            db_rec_1 = copy.deepcopy(self.db[cam_1][idx])\n            db_rec_2 = copy.deepcopy(self.db[cam_2][idx])\n\n            # there you go\n            bundle_1 = self.get_data(db_rec_1)\n            bundle_2 = self.get_data(db_rec_2)\n\n            return {'cam_1': bundle_1, 'cam_2': bundle_2}\n        else:\n            db_rec = copy.deepcopy(self.db[idx])\n            return self.get_data(db_rec)\n\n\n    def get_data(self, the_db):\n\n        image_file = os.path.join(self.root, the_db['image'])\n\n\n        cam = the_db['cam']\n\n        joints_vis = the_db['joints_3d_vis'].copy()\n        joints_vis[:,2] *= self.cfg.DATASET.Z_WEIGHT\n\n        img_patch, label, label_weight, scale, rot = get_single_patch_sample(image_file, the_db['center_x'],\n                                                                 the_db['center_y'], the_db['width'],\n                                                                 the_db['height'], the_db['joints_3d'].copy(),\n                                                                 joints_vis,\n                                                                 the_db['flip_pairs'].copy(), the_db['parent_ids'].copy(),\n                                                                 self.patch_width, self.patch_height,\n                                                                 self.rect_3d_width, self.rect_3d_height,\n                                                                 self.mean, self.std, self.is_train, self.label_func,\n                                                                 occluder=self.occluders, DEBUG=self.cfg.DEBUG.DEBUG)\n\n        meta = {\n            'image'   : image_file,\n            'center_x': the_db['center_x'],\n            'center_y': the_db['center_y'],\n            'width'   : the_db['width'],\n            'height'  : the_db['height'],\n            'scale'   : float(scale),\n            'rot'     : float(rot),\n            'R'       : cam.R,\n            'T'       : cam.T,\n            'f'       : cam.f,\n            'c'       : cam.c,\n            'projection_matrix': cam.projection_matrix\n        }\n\n        return img_patch.astype(np.float32), label.astype(np.float32), label_weight.astype(np.float32), meta\n\n\n    def _get_train_db(self):\n        # create train/val split\n        file_name = os.path.join(self.root,\n                                 'annot',\n                                 self.image_set + '.pkl')\n\n        with open(file_name, 'rb') as anno_file:\n            anno = pkl.load(anno_file)\n\n        if isinstance(anno, dict):\n            gt_db = [[] for i in range(self.num_cams)]  # for each cameras construct a database\n            rnd_subset = np.random.permutation(len(anno[1]))\n            for idx in rnd_subset:\n                for cid in range(self.num_cams):\n                    a = anno[cid+1][idx]\n                    gt_db[cid].append(a)\n\n            self.db_length = len(gt_db[0])\n\n            if not self.cfg.DATASET.TRI:\n                temp_db = []\n                for db in gt_db:\n                    temp_db += db\n                gt_db = temp_db\n\n                # Shuffle the dataset\n                random.shuffle(gt_db)\n\n                self.db_length = len(gt_db)\n        else:\n            gt_db = []\n\n            for idx in range(len(anno)):\n                a = anno[idx]\n                gt_db.append(a)\n\n            # Shuffle the dataset\n            random.shuffle(gt_db)\n            self.db_length = len(gt_db)\n\n        return gt_db\n\n\n    def _get_val_db(self):\n        # create train/val split\n        file_name = os.path.join(self.root,\n                                 'annot',\n                                 self.image_set + '.pkl')\n\n        with open(file_name, 'rb') as anno_file:\n            anno = pkl.load(anno_file)\n\n        if isinstance(anno, dict):\n            gt_db = [[] for i in range(self.num_cams)]  # for each cameras construct a database\n            rnd_subset = np.random.permutation(len(anno[1]))\n            for idx in rnd_subset:\n                for cid in range(self.num_cams):\n                    a = anno[cid + 1][idx]\n                    gt_db[cid].append(a)\n\n            temp_db = []\n            for db in gt_db:\n                temp_db += db\n            gt_db = temp_db\n\n        else:\n            gt_db = []\n\n            for idx in range(len(anno)):\n                a = anno[idx]\n                gt_db.append(a)\n\n        self.db_length = len(gt_db)\n\n        return gt_db\n\n\n    def evaluate(self, preds, save_path=None, debug=False, actionwise=False):\n        preds = preds[:, :, 0:3]\n\n        gt_poses = []\n        pred_poses = []\n\n        gt_poses_glob = []\n        pred_poses_glob = []\n        pred_2d_poses = []\n        all_images = []\n\n        gts = self.db\n\n        sample_num = preds.shape[0]\n        root = 6 if self.cfg.DATASET.MPII_ORDER else 0\n        pred_to_save = []\n\n        j14 = [0,1,2,3,4,5,6,7,10,11,12,13,14,15] if self.cfg.DATASET.MPII_ORDER else [0,1,2,4,5,6,7,8,9,10,11,12,14,15]\n\n        dist = []\n        dist_align = []\n        dist_norm = []\n        dist_14 = []\n        dist_14_align = []\n        dist_14_norm = []\n        dist_x = []\n        dist_y = []\n        dist_z = []\n        dist_per_joint = []\n        pck = []\n\n        if actionwise:\n            acts = define_actions('All')\n            dist_actions = {}\n            dist_actions_align = {}\n            for act in acts:\n                dist_actions[act] = []\n                dist_actions_align[act] = []\n\n        for n_sample in range(0, sample_num):\n            gt = gts[n_sample]\n            # Org image info\n            fl = gt['fl'][0:2]\n            c_p = gt['c_p'][0:2]\n\n            gt_3d_root = np.reshape(gt['pelvis'], (1, 3))\n            gt_2d_kpt = gt['joints_3d'].copy()\n            gt_vis = gt['joints_3d_vis'].copy()\n            # subj = gt['subject']\n            if actionwise:\n                action = gt['action']\n\n            # get camera depth from root joint\n            pre_2d_kpt = preds[n_sample].copy()\n\n            if self.cfg.DATASET.MPII_ORDER:\n                gt_2d_kpt = gt_2d_kpt[H36M_TO_MPII_PERM,:]\n\n            pre_2d_kpt[:, 2] = pre_2d_kpt[:, 2] + gt_3d_root[0, 2]\n            gt_2d_kpt[:, 2] = gt_2d_kpt[:, 2] + gt_3d_root[0, 2]\n\n            joint_num = pre_2d_kpt.shape[0]\n\n            # back project\n            pre_3d_kpt = np.zeros((joint_num, 3), dtype=np.float)\n            gt_3d_kpt = np.zeros((joint_num, 3), dtype=np.float)\n\n            for n_jt in range(0, joint_num):\n                pre_3d_kpt[n_jt, 0], pre_3d_kpt[n_jt, 1], pre_3d_kpt[n_jt, 2] = \\\n                    CamBackProj(pre_2d_kpt[n_jt, 0], pre_2d_kpt[n_jt, 1], pre_2d_kpt[n_jt, 2], fl[0], fl[1], c_p[0],\n                                c_p[1])\n                gt_3d_kpt[n_jt, 0], gt_3d_kpt[n_jt, 1], gt_3d_kpt[n_jt, 2] = \\\n                    CamBackProj(gt_2d_kpt[n_jt, 0], gt_2d_kpt[n_jt, 1], gt_2d_kpt[n_jt, 2], fl[0], fl[1], c_p[0],\n                                c_p[1])\n\n            # align\n            _, Z, T, b, c = compute_similarity_transform(gt_3d_kpt, pre_3d_kpt, compute_optimal_scale=True)\n            pre_3d_kpt_align = (b * pre_3d_kpt.dot(T)) + c\n            pre_3d_kpt_norm = b * pre_3d_kpt\n\n            # should align root, required by protocol #1\n            pre_3d_kpt = pre_3d_kpt - pre_3d_kpt[root]\n            gt_3d_kpt  = gt_3d_kpt - gt_3d_kpt[root]\n            pre_3d_kpt_align = pre_3d_kpt_align - pre_3d_kpt_align[root]\n            pre_3d_kpt_norm = pre_3d_kpt_norm - pre_3d_kpt_norm[root]\n\n            if self.cfg.DATASET.MPII_ORDER:\n                gt_poses.append(gt_3d_kpt)\n                pred_poses.append(pre_3d_kpt)\n            else:\n                gt_poses.append(gt_3d_kpt[H36M_TO_MPII_PERM,:])\n                pred_poses.append(pre_3d_kpt[H36M_TO_MPII_PERM,:])\n\n            diff = (gt_3d_kpt - pre_3d_kpt)\n            diff_align = (gt_3d_kpt - pre_3d_kpt_align)\n            diff_norm = (gt_3d_kpt - pre_3d_kpt_norm)\n\n            e_jt = []\n            e_jt_align = []\n            e_jt_norm = []\n            e_jt_14 = []\n            e_jt_14_align = []\n            e_jt_14_norm = []\n            e_jt_x = []\n            e_jt_y = []\n            e_jt_z = []\n\n            for n_jt in range(0, joint_num):\n                e_jt.append(np.linalg.norm(diff[n_jt]))\n                e_jt_align.append(np.linalg.norm(diff_align[n_jt]))\n                e_jt_norm.append(np.linalg.norm(diff_norm[n_jt]))\n                e_jt_x.append(np.sqrt(diff[n_jt][0]**2))\n                e_jt_y.append(np.sqrt(diff[n_jt][1]**2))\n                e_jt_z.append(np.sqrt(diff[n_jt][2]**2))\n\n                if np.linalg.norm(diff[n_jt]) >= 150:\n                    pck.append(0)\n                else:\n                    pck.append(1)\n\n            for jt in j14:\n                e_jt_14.append(np.linalg.norm(diff[jt]))\n                e_jt_14_align.append(np.linalg.norm(diff_align[jt]))\n                e_jt_14_norm.append(np.linalg.norm(diff_norm[jt]))\n\n            if self.cfg.DEBUG.DEBUG:\n                cam = gt['cam']\n                pred = cam.camera_to_world_frame(pre_3d_kpt)\n                gt_pt = cam.camera_to_world_frame(gt_3d_kpt)\n\n                gt_poses_glob.append(gt_pt)\n                pred_poses_glob.append(pred)\n                pred_2d_poses.append(pre_2d_kpt)\n                all_images.append(self.root + gts[n_sample]['image'])\n                import cv2\n                import matplotlib.pyplot as plt\n                from lib.utils.vis import drawskeleton, show3Dpose\n\n                if self.cfg.DATASET.MPII_ORDER:\n                    m = 1\n                else:\n                    m = 2\n\n                img = cv2.imread(self.root + gts[n_sample]['image'])\n\n                fig = plt.figure(figsize=(19.2, 10.8))\n\n                lc = (255, 0, 0), '#ff0000'\n                rc = (0, 0, 255), '#0000ff'\n                ax = fig.add_subplot('131')\n                drawskeleton(img, pre_2d_kpt, thickness=3, lcolor=lc[0], rcolor=rc[0], mpii=m)\n                drawskeleton(img, gt_2d_kpt, thickness=3, lcolor=(255, 0, 0), rcolor=(255, 0, 0), mpii=m)\n                ax.imshow(img[:,:,::-1])\n\n                ax = fig.add_subplot('132', projection='3d', aspect=1)\n                show3Dpose(gt_pt, ax, radius=750, lcolor=lc[1], rcolor=rc[1], mpii=m)\n\n                ax = fig.add_subplot('133', projection='3d', aspect=1)\n                show3Dpose(pred, ax, radius=750, lcolor=lc[1], rcolor=rc[1], mpii=m)\n                ax.set_title(str(n_sample) + ' %s' % np.array(e_jt).mean())\n\n                plt.show()\n\n            dist.append(np.array(e_jt).mean())\n            dist_align.append(np.array(e_jt_align).mean())\n            dist_norm.append(np.array(e_jt_norm).mean())\n            dist_14.append(np.array(e_jt_14).mean())\n            dist_14_align.append(np.array(e_jt_14_align).mean())\n            dist_14_norm.append(np.array(e_jt_14_norm).mean())\n            dist_x.append(np.array(e_jt_x).mean())\n            dist_y.append(np.array(e_jt_y).mean())\n            dist_z.append(np.array(e_jt_z).mean())\n            dist_per_joint.append(np.array(e_jt))\n\n            if actionwise:\n                dist_actions[action].append(np.array(e_jt).mean())\n                dist_actions_align[action].append(np.array(e_jt_align).mean())\n\n            pred_to_save.append({'pred': pre_3d_kpt,\n                                 'align_pred': pre_3d_kpt_align,\n                                 'gt': gt_3d_kpt})\n\n        per_joint_error = np.array(dist_per_joint).mean(axis=0).tolist()\n        joint_names = MPII_NAMES if self.cfg.DATASET.MPII_ORDER else H36M_NAMES\n\n        for idx in range(len(joint_names)):\n            print(joint_names[idx], per_joint_error[idx])\n        if actionwise:\n            print('========================')\n            for k, v in dist_actions.items():\n                print(k, np.array(v).mean())\n            print('========================')\n\n            print('========================')\n            for k, v in dist_actions_align.items():\n                print(k, np.array(v).mean())\n            print('========================')\n\n        name_value = [\n            ('hm36_17j      :', np.asarray(dist).mean()),\n            ('hm36_17j_align:', np.array(dist_align).mean()),\n            ('hm36_17j_norm:', np.array(dist_norm).mean()),\n            ('hm36_17j_14   :', np.asarray(dist_14).mean()),\n            ('hm36_17j_14_al:', np.array(dist_14_align).mean()),\n            ('hm36_17j_14_nm:', np.array(dist_14_norm).mean()),\n            ('hm36_17j_x    :', np.array(dist_x).mean()),\n            ('hm36_17j_y    :', np.array(dist_y).mean()),\n            ('hm36_17j_z    :', np.array(dist_z).mean()),\n        ]\n\n        return name_value, np.asarray(dist).mean()"""
lib/dataset/mpii_integral.py,0,"b""import logging\nimport os\nimport json\nimport numpy as np\nfrom scipy.io import loadmat, savemat\nimport copy\n\nfrom lib.utils.img_utils import get_single_patch_sample\nfrom lib.utils.utils import calc_kpt_bound\nfrom lib.dataset.JointIntegralDataset import JointsIntegralDataset\n\nlogger = logging.getLogger(__name__)\n\nclass MPIIDataset(JointsIntegralDataset):\n    def __init__(self, cfg, root, image_set, is_train):\n        super().__init__(cfg, root, image_set, is_train)\n\n\n        self.num_joints = 16\n        self.flip_pairs = [[0, 5], [1, 4], [2, 3], [10, 15], [11, 14], [12, 13]]\n        self.parent_ids = [1, 2, 6, 6, 3, 4, 6, 6, 7, 8, 11, 12, 7, 7, 13, 14]\n\n        self.db = self._get_db()\n        self.db_length = len(self.db)\n\n        logger.info('=> load {} samples'.format(len(self.db)))\n\n    def __getitem__(self, idx):\n        the_db = copy.deepcopy(self.db[idx])\n\n        img_patch, label, label_weight, _, _ = get_single_patch_sample(the_db['image'], the_db['center_x'],\n                                                                       the_db['center_y'], the_db['width'],\n                                                                       the_db['height'], the_db['joints_3d'].copy(),\n                                                                       the_db['joints_3d_vis'].copy(),\n                                                                       self.flip_pairs.copy(),\n                                                                       self.parent_ids.copy(),\n                                                                       self.patch_width, self.patch_height,\n                                                                       self.rect_3d_width, self.rect_3d_height,\n                                                                       self.mean, self.std, self.is_train, self.label_func,\n                                                                       occluder=self.occluders, DEBUG=False)\n\n        meta = {\n            'image': the_db['image'],\n        }\n\n        return img_patch.astype(np.float32), label.astype(np.float32), label_weight.astype(np.float32), meta\n\n    def _get_db(self):\n        # create train/val split\n        file_name = os.path.join(self.root,\n                                 'annot',\n                                 self.image_set+'.json')\n        with open(file_name) as anno_file:\n            anno = json.load(anno_file)\n\n        aspect_ratio = self.patch_width * 1.0 / self.patch_height\n\n        gt_db = []\n        for a in anno:\n            # joints and vis\n            jts_3d = np.zeros((self.num_joints, 3), dtype=np.float)\n            jts_3d_vis = np.zeros((self.num_joints, 3), dtype=np.float)\n            if self.image_set != 'test':\n                jts = np.array(a['joints'])\n                jts[:, 0:2] = jts[:, 0:2] - 1\n                jts_vis = np.array(a['joints_vis'])\n                assert len(jts) == self.num_joints, 'joint num diff: {} vs {}'.format(len(jts), self.num_joints)\n                jts_3d[:, 0:2] = jts[:, 0:2]\n                jts_3d_vis[:, 0] = jts_vis[:]\n                jts_3d_vis[:, 1] = jts_vis[:]\n\n            if np.sum(jts_3d_vis[:, 0]) < 2:  # only one joint visible, skip\n                continue\n\n            u, d, l, r = calc_kpt_bound(jts_3d, jts_3d_vis)\n            center = np.array([(l + r) * 0.5, (u + d) * 0.5], dtype=np.float32)\n            c_x = center[0]\n            c_y = center[1]\n            assert c_x >= 1\n\n            w = r - l\n            h = d - u\n\n            assert w > 0\n            assert h > 0\n\n            if w > aspect_ratio * h:\n                h = w * 1.0 / aspect_ratio\n            elif w < aspect_ratio * h:\n                w = h * aspect_ratio\n\n            width = w * 1.25\n            height = h * 1.25\n\n            img_path = os.path.join(self.root, 'images', a['image'])\n            gt_db.append({\n                'image': img_path,\n                'center_x': c_x,\n                'center_y': c_y,\n                'width': width,\n                'height': height,\n                'flip_pairs': self.flip_pairs,\n                'parent_ids': self.parent_ids,\n                'joints_3d': jts_3d,\n                'joints_3d_vis': jts_3d_vis,\n            })\n\n        return gt_db\n\n    def evaluate(self, preds, save_path=None, debug=False):\n        # convert 0-based index to 1-based index\n        preds = preds[:, :, 0:2] + 1.0\n\n        if save_path:\n            pred_file = os.path.join(save_path, 'pred.mat')\n            savemat(pred_file, mdict={'preds': preds})\n\n        if 'test' in self.cfg.DATASET.TEST_SET:\n            return {'Null': 0.0}, 0.0\n\n        SC_BIAS = 0.6\n        threshold = 0.5\n\n        gt_file = os.path.join(self.cfg.DATASET.ROOT,\n                               'annot',\n                               'gt_{}.mat'.format(self.cfg.DATASET.TEST_SET))\n        gt_dict = loadmat(gt_file)\n        dataset_joints = gt_dict['dataset_joints']\n        jnt_missing = gt_dict['jnt_missing']\n        pos_gt_src = gt_dict['pos_gt_src']\n        headboxes_src = gt_dict['headboxes_src']\n\n        pos_pred_src = np.transpose(preds, [1, 2, 0])\n\n        head = np.where(dataset_joints == 'head')[1][0]\n        lsho = np.where(dataset_joints == 'lsho')[1][0]\n        lelb = np.where(dataset_joints == 'lelb')[1][0]\n        lwri = np.where(dataset_joints == 'lwri')[1][0]\n        lhip = np.where(dataset_joints == 'lhip')[1][0]\n        lkne = np.where(dataset_joints == 'lkne')[1][0]\n        lank = np.where(dataset_joints == 'lank')[1][0]\n\n        rsho = np.where(dataset_joints == 'rsho')[1][0]\n        relb = np.where(dataset_joints == 'relb')[1][0]\n        rwri = np.where(dataset_joints == 'rwri')[1][0]\n        rkne = np.where(dataset_joints == 'rkne')[1][0]\n        rank = np.where(dataset_joints == 'rank')[1][0]\n        rhip = np.where(dataset_joints == 'rhip')[1][0]\n\n        jnt_visible = 1 - jnt_missing\n        uv_error = pos_pred_src - pos_gt_src\n        uv_err = np.linalg.norm(uv_error, axis=1)\n        headsizes = headboxes_src[1, :, :] - headboxes_src[0, :, :]\n        headsizes = np.linalg.norm(headsizes, axis=0)\n        headsizes *= SC_BIAS\n        scale = np.multiply(headsizes, np.ones((len(uv_err), 1)))\n        scaled_uv_err = np.divide(uv_err, scale)\n        scaled_uv_err = np.multiply(scaled_uv_err, jnt_visible)\n        jnt_count = np.sum(jnt_visible, axis=1)\n        less_than_threshold = np.multiply((scaled_uv_err <= threshold),\n                                          jnt_visible)\n        PCKh = np.divide(100.*np.sum(less_than_threshold, axis=1), jnt_count)\n\n        # save\n        rng = np.arange(0, 0.5+0.01, 0.01)\n        pckAll = np.zeros((len(rng), 16))\n\n        for r in range(len(rng)):\n            threshold = rng[r]\n            less_than_threshold = np.multiply(scaled_uv_err <= threshold,\n                                              jnt_visible)\n            pckAll[r, :] = np.divide(100.*np.sum(less_than_threshold, axis=1),\n                                     jnt_count)\n\n        PCKh = np.ma.array(PCKh, mask=False)\n        PCKh.mask[6:8] = True\n\n        jnt_count = np.ma.array(jnt_count, mask=False)\n        jnt_count.mask[6:8] = True\n        jnt_ratio = jnt_count / np.sum(jnt_count).astype(np.float64)\n\n        name_value = [\n            ('Head', PCKh[head]),\n            ('Shoulder', 0.5 * (PCKh[lsho] + PCKh[rsho])),\n            ('Elbow', 0.5 * (PCKh[lelb] + PCKh[relb])),\n            ('Wrist', 0.5 * (PCKh[lwri] + PCKh[rwri])),\n            ('Hip', 0.5 * (PCKh[lhip] + PCKh[rhip])),\n            ('Knee', 0.5 * (PCKh[lkne] + PCKh[rkne])),\n            ('Ankle', 0.5 * (PCKh[lank] + PCKh[rank])),\n            ('Mean', np.sum(PCKh * jnt_ratio)),\n            ('Mean@0.1', np.sum(pckAll[11, :] * jnt_ratio))\n        ]\n        # name_value = OrderedDict(name_value)\n\n        return name_value, np.sum(PCKh * jnt_ratio)\n"""
lib/models/__init__.py,0,b'import lib.models.pose3d_resnet'
lib/models/pose3d_resnet.py,3,"b'import os\nimport logging\n\nimport torch\nimport torch.nn as nn\n\n\nBN_MOMENTUM = 0.1\nlogger = logging.getLogger(__name__)\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes, momentum=BN_MOMENTUM)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion,\n                                  momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PoseResNet(nn.Module):\n\n    def __init__(self, block, layers, cfg, **kwargs):\n        self.inplanes = 64\n        extra = cfg.MODEL.EXTRA\n        self.deconv_with_bias = extra.DECONV_WITH_BIAS\n        self.volume = cfg.MODEL.VOLUME\n        super(PoseResNet, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64, momentum=BN_MOMENTUM)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        # used for deconv layers\n        self.deconv_layers = self._make_deconv_layer(\n            extra.NUM_DECONV_LAYERS,\n            extra.NUM_DECONV_FILTERS,\n            extra.NUM_DECONV_KERNELS,\n        )\n\n        self.final_layer = nn.Conv2d(\n            in_channels=extra.NUM_DECONV_FILTERS[-1],\n            out_channels=cfg.MODEL.NUM_JOINTS * cfg.MODEL.DEPTH_RES if self.volume else cfg.MODEL.NUM_JOINTS,\n            kernel_size=extra.FINAL_CONV_KERNEL,\n            stride=1,\n            padding=1 if extra.FINAL_CONV_KERNEL == 3 else 0\n        )\n\n        if not self.volume:\n            self.avgpool = nn.AvgPool2d(kernel_size=int(cfg.MODEL.IMAGE_SIZE[0] / 2**5), stride=1)\n            self.depth_fc = nn.Linear(2048, cfg.MODEL.NUM_JOINTS * cfg.MODEL.DEPTH_RES)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion, momentum=BN_MOMENTUM),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def _get_deconv_cfg(self, deconv_kernel, index):\n        if deconv_kernel == 4:\n            padding = 1\n            output_padding = 0\n        elif deconv_kernel == 3:\n            padding = 1\n            output_padding = 1\n        elif deconv_kernel == 2:\n            padding = 0\n            output_padding = 0\n\n        return deconv_kernel, padding, output_padding\n\n    def _make_deconv_layer(self, num_layers, num_filters, num_kernels):\n        assert num_layers == len(num_filters), \\\n            \'ERROR: num_deconv_layers is different len(num_deconv_filters)\'\n        assert num_layers == len(num_kernels), \\\n            \'ERROR: num_deconv_layers is different len(num_deconv_filters)\'\n\n        layers = []\n        for i in range(num_layers):\n            kernel, padding, output_padding = \\\n                self._get_deconv_cfg(num_kernels[i], i)\n\n            planes = num_filters[i]\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=self.inplanes,\n                    out_channels=planes,\n                    kernel_size=kernel,\n                    stride=2,\n                    padding=padding,\n                    output_padding=output_padding,\n                    bias=self.deconv_with_bias))\n            layers.append(nn.BatchNorm2d(planes, momentum=BN_MOMENTUM))\n            layers.append(nn.ReLU(inplace=True))\n            self.inplanes = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n\n        if self.volume:\n            x = self.deconv_layers(x)\n            x = self.final_layer(x)\n\n            return x\n        else:\n            y = x\n\n            x = self.deconv_layers(x)\n            x = self.final_layer(x)\n\n            y = self.avgpool(y)\n            y = y.view(y.size(0), -1)\n            y = self.depth_fc(y)\n\n            return x, y\n\n    def init_weights(self, pretrained=\'\'):\n        if os.path.isfile(pretrained):\n            logger.info(\'=> init deconv weights from normal distribution\')\n            for name, m in self.deconv_layers.named_modules():\n                if isinstance(m, nn.ConvTranspose2d):\n                    logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                    logger.info(\'=> init {}.bias as 0\'.format(name))\n                    nn.init.normal_(m.weight, std=0.001)\n                    if self.deconv_with_bias:\n                        nn.init.constant_(m.bias, 0)\n                elif isinstance(m, nn.BatchNorm2d):\n                    logger.info(\'=> init {}.weight as 1\'.format(name))\n                    logger.info(\'=> init {}.bias as 0\'.format(name))\n                    nn.init.constant_(m.weight, 1)\n                    nn.init.constant_(m.bias, 0)\n            logger.info(\'=> init final conv weights from normal distribution\')\n            for m in self.final_layer.modules():\n                if isinstance(m, nn.Conv2d):\n                    # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                    logger.info(\'=> init {}.weight as normal(0, 0.001)\'.format(name))\n                    logger.info(\'=> init {}.bias as 0\'.format(name))\n                    nn.init.normal_(m.weight, std=0.001)\n                    nn.init.constant_(m.bias, 0)\n\n            # If it is an mpii or coco pretrained model we have to resolve\n            # name conflictions to load all modules\n            if \'mpii\' in pretrained:\n                logger.info(\'=> loading pretrained MPII model {}\'.format(pretrained))\n                self.load_pretrained_pose_model(pretrained)\n            elif \'coco\' in pretrained:\n                logger.info(\'=> loading pretrained COCO model {}\'.format(pretrained))\n                self.load_pretrained_pose_model(pretrained)\n            elif \'imagenet\' in pretrained:\n                pretrained_state_dict = torch.load(pretrained)\n                logger.info(\'=> loading pretrained imagenet model {}\'.format(pretrained))\n                self.load_state_dict(pretrained_state_dict, strict=False)\n\n\n        else:\n            logger.error(\'=> imagenet pretrained model dose not exist\')\n            logger.error(\'=> please download it first\')\n            raise ValueError(\'imagenet pretrained model does not exist\')\n\n    def load_pretrained_pose_model(self, pretrained):\n\n        def removekey(d, key):\n            r = dict(d)\n            del r[key]\n            return r\n\n        pretrained_dict = torch.load(pretrained)\n        model_dict = self.state_dict()\n\n        # Dirty solution to \'module\' prefix in data parallel models\n        if len(pretrained_dict.keys()) == len([x for x in list(pretrained_dict.keys()) if \'module\' in x]):\n            from collections import OrderedDict\n            new_state_dict = OrderedDict()\n            for k, v in pretrained_dict.items():\n                name = k[7:]  # remove \'module\'\n                new_state_dict[name] = v\n            pretrained_dict = new_state_dict\n\n        # Remove keys that has the same name but different shapes\n        for k in pretrained_dict.keys():\n            if k in model_dict.keys():\n                if model_dict[k].shape != pretrained_dict[k].shape:\n                    logger.info(\'WARNING! There is a mismatch in => %s (%s, %s)\'%(k, model_dict[k].size(),\n                                                                            pretrained_dict[k].size()))\n                    pretrained_dict = removekey(pretrained_dict, k)\n            else:\n                logger.info(\'%s not in model_dict\'%k)\n\n        self.load_state_dict(pretrained_dict, strict=False)\n\nresnet_spec = {18: (BasicBlock, [2, 2, 2, 2]),\n               34: (BasicBlock, [3, 4, 6, 3]),\n               50: (Bottleneck, [3, 4, 6, 3]),\n               101: (Bottleneck, [3, 4, 23, 3]),\n               152: (Bottleneck, [3, 8, 36, 3])}\n\n\ndef get_pose_net(cfg, is_train, **kwargs):\n    num_layers = cfg.MODEL.EXTRA.NUM_LAYERS\n\n    block_class, layers = resnet_spec[num_layers]\n\n    model = PoseResNet(block_class, layers, cfg, **kwargs)\n\n    if is_train and cfg.MODEL.INIT_WEIGHTS:\n        model.init_weights(cfg.MODEL.PRETRAINED)\n\n    return model\n'"
lib/utils/__init__.py,0,b''
lib/utils/augmentation.py,0,"b'import os.path\nimport random\nimport xml.etree.ElementTree\nimport numpy as np\nimport cv2\nimport PIL.Image\n\ndef load_occluders(pascal_voc_root_path):\n    occluders = []\n    structuring_element = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))\n    \n    annotation_paths = list_filepaths(os.path.join(pascal_voc_root_path, \'Annotations\'))\n    for annotation_path in annotation_paths:\n        xml_root = xml.etree.ElementTree.parse(annotation_path).getroot()\n        is_segmented = (xml_root.find(\'segmented\').text != \'0\')\n\n        if not is_segmented:\n            continue\n\n        boxes = []\n        for i_obj, obj in enumerate(xml_root.findall(\'object\')):\n            is_person = (obj.find(\'name\').text == \'person\')\n            is_difficult = (obj.find(\'difficult\').text != \'0\')\n            is_truncated = (obj.find(\'truncated\').text != \'0\')\n            if not is_person and not is_difficult and not is_truncated:\n                bndbox = obj.find(\'bndbox\')\n                box = [int(bndbox.find(s).text) for s in [\'xmin\', \'ymin\', \'xmax\', \'ymax\']]\n                boxes.append((i_obj, box))\n\n        if not boxes:\n            continue\n\n        im_filename = xml_root.find(\'filename\').text\n        seg_filename = im_filename.replace(\'jpg\', \'png\')\n\n        im_path = os.path.join(pascal_voc_root_path, \'JPEGImages\', im_filename)\n        seg_path = os.path.join(pascal_voc_root_path,\'SegmentationObject\', seg_filename)\n\n        im = np.asarray(PIL.Image.open(im_path))\n        labels = np.asarray(PIL.Image.open(seg_path))\n\n        for i_obj, (xmin, ymin, xmax, ymax) in boxes:\n            object_mask = (labels[ymin:ymax, xmin:xmax] == i_obj + 1).astype(np.uint8)*255\n            object_image = im[ymin:ymax, xmin:xmax]\n            if cv2.countNonZero(object_mask) < 500:\n                # Ignore small objects\n                continue\n\n            # Reduce the opacity of the mask along the border for smoother blending\n            eroded = cv2.erode(object_mask, structuring_element)\n            object_mask[eroded < object_mask] = 192\n            object_with_mask = np.concatenate([object_image, object_mask[..., np.newaxis]], axis=-1)\n            \n            # Downscale for efficiency\n            object_with_mask = resize_by_factor(object_with_mask, 0.5)\n            occluders.append(object_with_mask)\n\n    return occluders\n\n\ndef occlude_with_objects(im, occluders):\n    """"""Returns an augmented version of `im`, containing some occluders from the Pascal VOC dataset.""""""\n\n    result = im.copy()\n    width_height = np.asarray([im.shape[1], im.shape[0]])\n    im_scale_factor = min(width_height) / 256\n    count = np.random.randint(1, 8)\n\n    for _ in range(count):\n        occluder = random.choice(occluders)\n\n        random_scale_factor = np.random.uniform(0.2, 1.0)\n        scale_factor = random_scale_factor * im_scale_factor\n        occluder = resize_by_factor(occluder, scale_factor)\n\n        center = np.random.uniform([0,0], width_height)\n        paste_over(im_src=occluder, im_dst=result, center=center)\n\n    return result\n\n\ndef paste_over(im_src, im_dst, center):\n    """"""Pastes `im_src` onto `im_dst` at a specified position, with alpha blending, in place.\n\n    Locations outside the bounds of `im_dst` are handled as expected (only a part or none of\n    `im_src` becomes visible).\n\n    Args:\n        im_src: The RGBA image to be pasted onto `im_dst`. Its size can be arbitrary.\n        im_dst: The target image.\n        alpha: A float (0.0-1.0) array of the same size as `im_src` controlling the alpha blending\n            at each pixel. Large values mean more visibility for `im_src`.\n        center: coordinates in `im_dst` where the center of `im_src` should be placed.\n    """"""\n\n    width_height_src = np.asarray([im_src.shape[1], im_src.shape[0]])\n    width_height_dst = np.asarray([im_dst.shape[1], im_dst.shape[0]])\n\n    center = np.round(center).astype(np.int32)\n    raw_start_dst = center - width_height_src // 2\n    raw_end_dst = raw_start_dst + width_height_src\n\n    start_dst = np.clip(raw_start_dst, 0, width_height_dst)\n    end_dst = np.clip(raw_end_dst, 0, width_height_dst)\n    region_dst = im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]]\n\n    start_src = start_dst - raw_start_dst\n    end_src = width_height_src + (end_dst - raw_end_dst)\n    region_src = im_src[start_src[1]:end_src[1], start_src[0]:end_src[0]]\n    color_src = region_src[..., 0:3]\n    alpha = region_src[..., 3:].astype(np.float32)/255\n\n    im_dst[start_dst[1]:end_dst[1], start_dst[0]:end_dst[0]] = (\n            alpha * color_src + (1 - alpha) * region_dst)\n\n\ndef resize_by_factor(im, factor):\n    """"""Returns a copy of `im` resized by `factor`, using bilinear interp for up and area interp\n    for downscaling.\n    """"""\n    new_size = tuple(np.round(np.array([im.shape[1], im.shape[0]]) * factor).astype(int))\n    interp = cv2.INTER_LINEAR if factor > 1.0 else cv2.INTER_AREA\n    return cv2.resize(im, new_size, fx=factor, fy=factor, interpolation=interp)\n\n\ndef list_filepaths(dirpath):\n    names = os.listdir(dirpath)\n    paths = [os.path.join(dirpath, name) for name in names]\n    return sorted(filter(os.path.isfile, paths))\n'"
lib/utils/cameras.py,0,"b'import h5py\nimport numpy as np\nimport cv2\n\nclass Camera():\n    def __init__(self, cam_params):\n        # R, T, f, c, k, p, name\n        self.cam_params = cam_params\n        self.R, self.T, self.f, self.c, self.k, self.p, self.name = cam_params\n\n        self.camera_matrix = self.get_intrinsic_matrix()\n        if self.k is not None or self.p is not None:\n            self.dist_coeffs = self.get_dist_coeffs()\n\n        self.tvec = self.get_tvec()\n        self.projection_matrix = self.get_projection_matrix()\n\n    def project_point_radial(self, P):\n        """"""\n        Project points from 3d to 2d using camera parameters\n        including radial and tangential distortion\n\n        Args\n        P: Nx3 points in world coordinates\n        R: 3x3 Camera rotation matrix\n        T: 3x1 Camera translation parameters\n        f: (scalar) Camera focal length\n        c: 2x1 Camera center\n        k: 3x1 Camera radial distortion coefficients\n        p: 2x1 Camera tangential distortion coefficients\n        Returns\n        Proj: Nx2 points in pixel space\n        D: 1xN depth of each point in camera space\n        radial: 1xN radial distortion per point\n        tan: 1xN tangential distortion per point\n        r2: 1xN squared radius of the projected points before distortion\n        """"""\n\n        # P is a matrix of 3-dimensional points\n        assert len(P.shape) == 2\n        assert P.shape[1] == 3\n\n        N = P.shape[0]\n        X = self.R.dot( P.T - self.T ) # rotate and translate\n        XX = X[:2,:] / X[2,:]\n        r2 = XX[0,:]**2 + XX[1,:]**2\n\n        radial = 1 + np.einsum( \'ij,ij->j\', np.tile(self.k,(1, N)), np.array([r2, r2**2, r2**3]) )\n        tan = self.p[0]*XX[1,:] + self.p[1]*XX[0,:]\n\n        XXX = XX * np.tile(radial+tan,(2,1)) + \\\n              np.outer(np.array([self.p[1], self.p[0]]).reshape(-1), r2 )\n\n        Proj = (self.f * XXX) + self.c\n        Proj = Proj.T\n\n        D = X[2,]\n\n        return Proj, D, radial, tan, r2\n\n    def unproject_pts(self, pts_uv, pts_d):\n        """"""\n        This function converts a set of 2D image coordinates to vectors in pinhole camera space.\n        Hereby the intrinsics of the camera are taken into account.\n        UV is converted to normalized image space (think frustum with image plane at z=1) then undistored\n        adding a z_coordinate of 1 yield vectors pointing from 0,0,0 to the undistored image pixel.\n        @return: ndarray with shape=(n, 3)\n\n        """"""\n        pts_uv = np.array(pts_uv)\n        num_pts = pts_uv.size / 2\n        pts_uv.shape = (int(num_pts), 1, 2)\n\n        pts_uv = cv2.undistortPoints(pts_uv, self.camera_matrix, self.dist_coeffs)\n\n        pts_3d = cv2.convertPointsToHomogeneous(np.float32(pts_uv))\n        pts_3d.shape = (int(num_pts),3)\n\n        pts_d.shape = (int(num_pts),1)\n\n        return pts_3d * pts_d\n\n    def world_to_camera_frame(self, P):\n        """"""\n        Convert points from world to camera coordinates\n\n        Args\n        P: Nx3 3d points in world coordinates\n        R: 3x3 Camera rotation matrix\n        T: 3x1 Camera translation parameters\n        Returns\n        X_cam: Nx3 3d points in camera coordinates\n        """"""\n\n        assert len(P.shape) == 2\n        assert P.shape[1] == 3\n\n        X_cam = self.R.dot( P.T - self.T ) # rotate and translate\n\n        return X_cam.T\n\n    def camera_to_world_frame(self, P):\n        """"""Inverse of world_to_camera_frame\n\n        Args\n        P: Nx3 points in camera coordinates\n        R: 3x3 Camera rotation matrix\n        T: 3x1 Camera translation parameters\n        Returns\n        X_cam: Nx3 points in world coordinates\n        """"""\n\n        assert len(P.shape) == 2\n        assert P.shape[1] == 3\n\n        X_cam = self.R.T.dot( P.T ) + self.T # rotate and translate\n\n        return X_cam.T\n\n    def get_intrinsic_matrix(self):\n        fx, fy = self.cam_params[2]\n        cx, cy = self.cam_params[3]\n        K = np.array([[fx, 0., cx],[0., fy, cy],[0., 0., 1.]]).astype(np.double)\n        return K\n\n    def get_projection_matrix(self):\n        K = self.get_intrinsic_matrix()\n        T = self.tvec\n        if len(T.shape) < 2:\n            T = np.expand_dims(T, axis=-1)\n        return np.dot(K, np.concatenate((self.R, T), axis=1))\n\n    def get_essential_matrix(self, fundamental_mat):\n        return np.dot(np.dot(self.camera_matrix.T, fundamental_mat), self.camera_matrix)\n\n    def get_fundamental_matrix(self, u1, u2):\n        u1 = np.int32(u1)\n        u2 = np.int32(u2)\n        F, mask = cv2.findFundamentalMat(u1, u2, cv2.FM_LMEDS)\n        # We select only inlier points\n        u1 = u1[mask.ravel() == 1]\n        u2 = u2[mask.ravel() == 1]\n        return F, (u1, u2)\n\n    def get_disp_matrix(self):\n        T = self.get_tvec()\n        return np.concatenate((self.R, T), axis=1)\n\n    def get_tvec(self):\n        return np.dot(self.R, np.negative(self.T))\n\n    def get_dist_coeffs(self):\n        return np.array([self.cam_params[4][0],\n                         self.cam_params[4][1],\n                         self.cam_params[5][0],\n                         self.cam_params[5][1],\n                         self.cam_params[4][2]])\n\n    def project_points(self, P):\n        kps, _ = cv2.projectPoints(objectPoints=P,\n                                   rvec=self.R,\n                                   tvec=self.tvec,\n                                   cameraMatrix=self.camera_matrix,\n                                   distCoeffs=self.dist_coeffs)\n        return kps\n\ndef load_camera_params( hf, path ):\n    """"""Load h36m camera parameters\n\n    Args\n    hf: hdf5 open file with h36m cameras data\n    path: path or key inside hf to the camera we are interested in\n    Returns\n    R: 3x3 Camera rotation matrix\n    T: 3x1 Camera translation parameters\n    f: (scalar) Camera focal length\n    c: 2x1 Camera center\n    k: 3x1 Camera radial distortion coefficients\n    p: 2x1 Camera tangential distortion coefficients\n    name: String with camera id\n    """"""\n\n    R = hf[ path.format(\'R\') ][:]\n    R = R.T\n\n    T = hf[ path.format(\'T\') ][:]\n    f = hf[ path.format(\'f\') ][:]\n    c = hf[ path.format(\'c\') ][:]\n    k = hf[ path.format(\'k\') ][:]\n    p = hf[ path.format(\'p\') ][:]\n\n    name = hf[ path.format(\'Name\') ][:]\n    name = """".join( [chr(item) for item in name] )\n\n    return R, T, f, c, k, p, name\n\ndef load_cameras(bpath=\'cameras.h5\', subjects=[1,5,6,7,8,9,11]):\n    """"""Loads the cameras of h36m\n\n    Args\n    bpath: path to hdf5 file with h36m camera data\n    subjects: List of ints representing the subject IDs for which cameras are requested\n    Returns\n    rcams: dictionary of 4 tuples per subject ID containing its camera parameters for the 4 h36m cams\n    """"""\n    rcams = {}\n\n    with h5py.File(bpath,\'r\') as hf:\n        for s in subjects:\n            for c in range(4): # There are 4 cameras in human3.6m\n                rcams[(s, c+1)] = load_camera_params(hf, \'subject%d/camera%d/{0}\' % (s,c+1) )\n\n    return rcams\n\n'"
lib/utils/data_utils.py,0,"b'import os\nimport numpy as np\nimport h5py\nimport glob\nimport copy\n\nfrom lib.utils.cameras import Camera\n\n# Human3.6m IDs for training and testing\nTRAIN_SUBJECTS = [1,5,6,7,8]\nTEST_SUBJECTS  = [9,11]\n\n# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\nH36M_NAMES = [\'\']*32\nH36M_NAMES[0]  = \'Hip\'\nH36M_NAMES[1]  = \'RHip\'\nH36M_NAMES[2]  = \'RKnee\'\nH36M_NAMES[3]  = \'RFoot\'\nH36M_NAMES[6]  = \'LHip\'\nH36M_NAMES[7]  = \'LKnee\'\nH36M_NAMES[8]  = \'LFoot\'\nH36M_NAMES[12] = \'Spine\'\nH36M_NAMES[13] = \'Thorax\'\nH36M_NAMES[14] = \'Neck/Nose\'\nH36M_NAMES[15] = \'Head\'\nH36M_NAMES[17] = \'LShoulder\'\nH36M_NAMES[18] = \'LElbow\'\nH36M_NAMES[19] = \'LWrist\'\nH36M_NAMES[25] = \'RShoulder\'\nH36M_NAMES[26] = \'RElbow\'\nH36M_NAMES[27] = \'RWrist\'\n\n# Stacked Hourglass produces 16 joints. These are the names.\nSH_NAMES = [\'\']*16\nSH_NAMES[0]  = \'RFoot\'\nSH_NAMES[1]  = \'RKnee\'\nSH_NAMES[2]  = \'RHip\'\nSH_NAMES[3]  = \'LHip\'\nSH_NAMES[4]  = \'LKnee\'\nSH_NAMES[5]  = \'LFoot\'\nSH_NAMES[6]  = \'Hip\'\nSH_NAMES[7]  = \'Spine\'\nSH_NAMES[8]  = \'Thorax\'\nSH_NAMES[9]  = \'Head\'\nSH_NAMES[10] = \'RWrist\'\nSH_NAMES[11] = \'RElbow\'\nSH_NAMES[12] = \'RShoulder\'\nSH_NAMES[13] = \'LShoulder\'\nSH_NAMES[14] = \'LElbow\'\nSH_NAMES[15] = \'LWrist\'\n\nverbose = False\nUSE_FINETUNED_PREDS = False\n\nSH_TO_GT_PERM = np.array([SH_NAMES.index( h ) for h in H36M_NAMES if h != \'\' and h in SH_NAMES])\nassert np.all(SH_TO_GT_PERM == np.array([6,2,1,0,3,4,5,7,8,9,13,14,15,12,11,10]))\n\nGT_TO_SH_PERM = np.array([H36M_NAMES.index( h ) for h in SH_NAMES if h != \'\' and h in H36M_NAMES])\n\ndef load_data( bpath, subjects, actions, dim=3 ):\n    """"""\n    Loads 2d ground truth from disk, and puts it in an easy-to-acess dictionary\n\n    Args\n    bpath: String. Path where to load the data from\n    subjects: List of integers. Subjects whose data will be loaded\n    actions: List of strings. The actions to load\n    dim: Integer={2,3}. Load 2 or 3-dimensional data\n    Returns:\n    data: Dictionary with keys k=(subject, action, seqname)\n        values v=(nx(32*2) matrix of 2d ground truth)\n        There will be 2 entries per subject/action if loading 3d data\n        There will be 8 entries per subject/action if loading 2d data\n    """"""\n\n    if not dim in [2,3]:\n        raise(ValueError, \'dim must be 2 or 3\')\n\n    data = {}\n\n    for subj in subjects:\n        for action in actions:\n\n            if verbose:\n                print(\'Reading subject {0}, action {1}\'.format(subj, action))\n\n            dpath = os.path.join(bpath, \'S{0}\'.format(subj), \'MyPoses/{0}D_positions\'.format(dim), \'{0}*.h5\'.format(action))\n\n            if verbose:\n                print( dpath )\n\n            fnames = glob.glob( dpath )\n\n            loaded_seqs = 0\n            for fname in fnames:\n                seqname = os.path.basename( fname )\n\n                # This rule makes sure SittingDown is not loaded when Sitting is requested\n                if action == ""Sitting"" and seqname.startswith( ""SittingDown"" ):\n                    continue\n\n                # This rule makes sure that WalkDog and WalkTogeter are not loaded when\n                # Walking is requested.\n                if seqname.startswith( action ):\n                    if verbose:\n                        print( fname )\n                    loaded_seqs = loaded_seqs + 1\n\n                    with h5py.File( fname, \'r\' ) as h5f:\n                        poses = h5f[\'{0}D_positions\'.format(dim)][:]\n\n                    poses = poses.T\n                    data[ (subj, action, seqname) ] = poses\n\n            if dim == 2:\n                assert loaded_seqs == 8, ""Expecting 8 sequences, found {0} instead"".format( loaded_seqs )\n            else:\n                assert loaded_seqs == 2, ""Expecting 2 sequences, found {0} instead"".format( loaded_seqs )\n\n    return data\n\n\ndef load_stacked_hourglass(data_dir, subjects, actions):\n    """"""\n    Load 2d detections from disk, and put it in an easy-to-acess dictionary.\n\n    Args\n    data_dir: string. Directory where to load the data from,\n    subjects: list of integers. Subjects whose data will be loaded.\n    actions: list of strings. The actions to load.\n    Returns\n    data: dictionary with keys k=(subject, action, seqname)\n            values v=(nx(32*2) matrix of 2d stacked hourglass detections)\n            There will be 2 entries per subject/action if loading 3d data\n            There will be 8 entries per subject/action if loading 2d data\n    """"""\n    # Permutation that goes from SH detections to H36M ordering.\n    SH_TO_GT_PERM = np.array([SH_NAMES.index( h ) for h in H36M_NAMES if h != \'\' and h in SH_NAMES])\n    assert np.all( SH_TO_GT_PERM == np.array([6,2,1,0,3,4,5,7,8,9,13,14,15,12,11,10]) )\n\n    data = {}\n\n    for subj in subjects:\n        for action in actions:\n\n            if verbose:\n                print(\'Reading subject {0}, action {1}\'.format(subj, action))\n\n            if USE_FINETUNED_PREDS:\n                dpath = os.path.join(data_dir, \'S{0}\'.format(subj), \'StackedHourglassFineTuned240/{0}*.h5\'.format(action))\n            else:\n                dpath = os.path.join( data_dir, \'S{0}\'.format(subj), \'StackedHourglass/{0}*.h5\'.format(action) )\n\n            if verbose:\n                print( dpath )\n\n            fnames = glob.glob( dpath )\n\n            loaded_seqs = 0\n            for fname in fnames:\n                seqname = os.path.basename( fname )\n                seqname = seqname.replace(\'_\',\' \')\n\n                # This rule makes sure SittingDown is not loaded when Sitting is requested\n                if action == ""Sitting"" and seqname.startswith( ""SittingDown"" ):\n                    continue\n\n                # This rule makes sure that WalkDog and WalkTogeter are not loaded when\n                # Walking is requested.\n                if seqname.startswith( action ):\n                    if verbose:\n                        print( fname )\n                    loaded_seqs = loaded_seqs + 1\n\n                    # Load the poses from the .h5 file\n                    with h5py.File(fname, \'r\') as h5f:\n                        poses = h5f[\'poses\'][:]\n\n                        # Permute the loaded data to make it compatible with H36M\n                        poses = poses[:,SH_TO_GT_PERM,:]\n\n                        # Reshape into n x (32*2) matrix\n                        poses = np.reshape(poses,[poses.shape[0], -1])\n                        poses_final = np.zeros([poses.shape[0], len(H36M_NAMES)*2])\n\n                        dim_to_use_x = np.where(np.array([x != \'\' and x != \'Neck/Nose\' for x in H36M_NAMES]))[0] * 2\n                        dim_to_use_y = dim_to_use_x+1\n\n                        dim_to_use = np.zeros(len(SH_NAMES)*2,dtype=np.int32)\n                        dim_to_use[0::2] = dim_to_use_x\n                        dim_to_use[1::2] = dim_to_use_y\n                        poses_final[:,dim_to_use] = poses\n                        seqname = seqname+\'-sh\'\n                        data[ (subj, action, seqname) ] = poses_final\n\n            # Make sure we loaded 8 sequences\n            if (subj == 11 and action == \'Directions\'): # <-- this video is damaged\n                assert loaded_seqs == 7, ""Expecting 7 sequences, found {0} instead. S:{1} {2}"".format(loaded_seqs, subj, action)\n            else:\n                assert loaded_seqs == 8, ""Expecting 8 sequences, found {0} instead. S:{1} {2}"".format(loaded_seqs, subj, action)\n\n    return data\n\n\ndef normalization_stats(complete_data, dim, predict_14=False):\n    """"""\n    Computes normalization statistics: mean and stdev, dimensions used and ignored\n\n    Args\n    complete_data: nxd np array with poses\n    dim. integer={2,3} dimensionality of the data\n    predict_14. boolean. Whether to use only 14 joints\n    Returns\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dimensions_to_ignore: list of dimensions not used in the model\n    dimensions_to_use: list of dimensions used in the model\n    """"""\n    if not dim in [2,3]:\n        raise(ValueError, \'dim must be 2 or 3\')\n\n    data_mean = np.mean(complete_data, axis=0)\n    data_std  = np.std(complete_data, axis=0)\n\n    # Encodes which 17 (or 14) 2d-3d pairs we are predicting\n    dimensions_to_ignore = []\n    if dim == 2:\n        dimensions_to_use    = np.where(np.array([x != \'\' and x != \'Neck/Nose\' for x in H36M_NAMES]))[0]\n        dimensions_to_use    = np.sort(np.hstack((dimensions_to_use*2, dimensions_to_use*2+1)))\n        dimensions_to_ignore = np.delete( np.arange(len(H36M_NAMES)*2), dimensions_to_use )\n    else: # dim == 3\n        dimensions_to_use = np.where(np.array([x != \'\' for x in H36M_NAMES]))[0]\n        dimensions_to_use = np.delete(dimensions_to_use, [0,7,9] if predict_14 else 0)\n\n        dimensions_to_use = np.sort(np.hstack((dimensions_to_use*3,\n                                               dimensions_to_use*3+1,\n                                               dimensions_to_use*3+2)))\n        dimensions_to_ignore = np.delete(np.arange(len(H36M_NAMES)*3), dimensions_to_use)\n\n    return data_mean, data_std, dimensions_to_ignore, dimensions_to_use\n\n\ndef transform_world_to_camera(poses_set, cams, ncams=4):\n    """"""\n    Project 3d poses from world coordinate to camera coordinate system\n    Args\n        poses_set: dictionary with 3d poses\n        cams: dictionary with cameras\n        ncams: number of cameras per subject\n    Return:\n        t3d_camera: dictionary with 3d poses in camera coordinate\n    """"""\n    t3d_camera = {}\n    for t3dk in sorted( poses_set.keys() ):\n\n        subj, action, seqname = t3dk\n        t3d_world = poses_set[ t3dk ]\n\n        for c in range( ncams ):\n            cam_param = cams[ (subj, c+1) ]\n            cam = Camera(cam_param)\n            camera_coord = cam.world_to_camera_frame(np.reshape(t3d_world, [-1, 3]))\n            camera_coord = np.reshape(camera_coord, [-1, len(H36M_NAMES)*3])\n\n            sname = seqname[:-3]+"".""+cam.name+"".h5"" # e.g.: Waiting 1.58860488.h5\n            t3d_camera[ (subj, action, sname) ] = camera_coord\n\n    return t3d_camera\n\n\ndef normalize_data(data, data_mean, data_std, dim_to_use):\n    """"""\n    Normalizes a dictionary of poses\n\n    Args\n    data: dictionary where values are\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dim_to_use: list of dimensions to keep in the data\n    Returns\n    data_out: dictionary with same keys as data, but values have been normalized\n    """"""\n    data_out = {}\n\n    for key in data.keys():\n        data[key] = data[key][:, dim_to_use]\n        mu = data_mean[dim_to_use]\n        stddev = data_std[dim_to_use]\n        data_out[key] = np.divide((data[key] - mu), stddev)\n\n    return data_out\n\n\ndef unNormalizeData(normalized_data, data_mean, data_std, dimensions_to_ignore):\n    """"""\n    Un-normalizes a matrix whose mean has been substracted and that has been divided by\n    standard deviation. Some dimensions might also be missing\n\n    Args\n    normalized_data: nxd matrix to unnormalize\n    data_mean: np vector with the mean of the data\n    data_std: np vector with the standard deviation of the data\n    dimensions_to_ignore: list of dimensions that were removed from the original data\n    Returns\n    orig_data: the input normalized_data, but unnormalized\n    """"""\n    T = normalized_data.shape[0] # Batch size\n    D = data_mean.shape[0] # Dimensionality\n\n    orig_data = np.zeros((T, D), dtype=np.float32)\n    dimensions_to_use = np.array([dim for dim in range(D)\n                                if dim not in dimensions_to_ignore])\n\n    orig_data[:, dimensions_to_use] = normalized_data\n\n    # Multiply times stdev and add the mean\n    stdMat = data_std.reshape((1, D))\n    stdMat = np.repeat(stdMat, T, axis=0)\n    meanMat = data_mean.reshape((1, D))\n    meanMat = np.repeat(meanMat, T, axis=0)\n    orig_data = np.multiply(orig_data, stdMat) + meanMat\n    return orig_data\n\n\ndef define_actions( action ):\n    """"""\n    Given an action string, returns a list of corresponding actions.\n\n    Args\n    action: String. either ""all"" or one of the h36m actions\n    Returns\n    actions: List of strings. Actions to use.\n    Raises\n    ValueError: if the action is not a valid action in Human 3.6M\n    """"""\n    actions = [""Directions"",""Discussion"",""Eating"",""Greeting"",\n             ""Phoning"",""Photo"",""Posing"",""Purchases"",\n             ""Sitting"",""SittingDown"",""Smoking"",""Waiting"",\n             ""WalkDog"",""Walking"",""WalkTogether""]\n\n    if action == ""All"" or action == ""all"":\n        return actions\n\n    if not action in actions:\n        raise( ValueError, ""Unrecognized action: %s"" % action )\n\n    return [action]\n\n\ndef project_to_cameras( poses_set, cams, ncams=4 ):\n    """"""\n    Project 3d poses using camera parameters\n\n    Args\n    poses_set: dictionary with 3d poses\n    cams: dictionary with camera parameters\n    ncams: number of cameras per subject\n    Returns\n    t2d: dictionary with 2d poses\n    """"""\n    t2d = {}\n\n    RELATIVE_DEPTH_DATA = []\n    for t3dk in sorted( poses_set.keys() ):\n        subj, a, seqname = t3dk\n        t3d = poses_set[ t3dk ]\n\n        for cam in range( ncams ):\n            cam_param = cams[ (subj, cam+1) ]\n            cam = Camera(cam_param)\n            pts2d, D, _, _, _ = cam.project_point_radial(np.reshape(t3d, [-1, 3]))\n\n            pts2d = np.reshape( pts2d, [-1, len(H36M_NAMES)*2] )\n\n            D = D.reshape([-1, len(H36M_NAMES)])\n            depth = np.abs(D[:,:] - D[:,0].reshape((-1,1)))\n\n            RELATIVE_DEPTH_DATA.append(depth)\n\n            sname = seqname[:-3]+"".""+cam.name+"".h5"" # e.g.: Waiting 1.58860488.h5\n            t2d[ (subj, a, sname) ] = pts2d\n\n    np.save(\'relative_depth\', np.asarray(RELATIVE_DEPTH_DATA))\n    return t2d\n\n\ndef read_2d_predictions( actions, data_dir ):\n    """"""\n    Loads 2d data from precomputed Stacked Hourglass detections\n\n    Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n    Returns\n    train_set: dictionary with loaded 2d stacked hourglass detections for training\n    test_set: dictionary with loaded 2d stacked hourglass detections for testing\n    data_mean: vector with the mean of the 2d training data\n    data_std: vector with the standard deviation of the 2d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n    """"""\n\n    train_set = load_stacked_hourglass( data_dir, TRAIN_SUBJECTS, actions)\n    test_set  = load_stacked_hourglass( data_dir, TEST_SUBJECTS,  actions)\n\n    complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n\n    data_mean, data_std,  dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=2 )\n    # print(complete_train.shape)\n    # print(dim_to_use, dim_to_ignore)\n\n    train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n    test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n    return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use\n\n\ndef create_2d_data( actions, data_dir, rcams ):\n    """"""\n    Creates 2d poses by projecting 3d poses with the corresponding camera\n    parameters. Also normalizes the 2d poses\n\n    Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n    rcams: dictionary with camera parameters\n    Returns\n    train_set: dictionary with projected 2d poses for training\n    test_set: dictionary with projected 2d poses for testing\n    data_mean: vector with the mean of the 2d training data\n    data_std: vector with the standard deviation of the 2d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n    """"""\n\n    # Load 3d data\n    train_set = load_data( data_dir, TRAIN_SUBJECTS, actions, dim=3 )\n    test_set  = load_data( data_dir, TEST_SUBJECTS,  actions, dim=3 )\n\n    train_set = project_to_cameras( train_set, rcams )\n    test_set  = project_to_cameras( test_set, rcams )\n\n    # Compute normalization statistics.\n    complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n    data_mean, data_std, dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=2 )\n\n    # Divide every dimension independently\n    train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n    test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n    return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use\n\n\ndef read_3d_data( actions, data_dir, camera_frame, rcams, predict_14=False ):\n    """"""\n    Loads 3d poses, zero-centres and normalizes them\n\n    Args\n    actions: list of strings. Actions to load\n    data_dir: string. Directory where the data can be loaded from\n    camera_frame: boolean. Whether to convert the data to camera coordinates\n    rcams: dictionary with camera parameters\n    predict_14: boolean. Whether to predict only 14 joints\n    Returns\n    train_set: dictionary with loaded 3d poses for training\n    test_set: dictionary with loaded 3d poses for testing\n    data_mean: vector with the mean of the 3d training data\n    data_std: vector with the standard deviation of the 3d training data\n    dim_to_ignore: list with the dimensions to not predict\n    dim_to_use: list with the dimensions to predict\n    train_root_positions: dictionary with the 3d positions of the root in train\n    test_root_positions: dictionary with the 3d positions of the root in test\n    """"""\n    # Load 3d data\n    train_set = load_data( data_dir, TRAIN_SUBJECTS, actions, dim=3 )\n    test_set  = load_data( data_dir, TEST_SUBJECTS,  actions, dim=3 )\n\n    if camera_frame:\n        train_set = transform_world_to_camera( train_set, rcams )\n        test_set  = transform_world_to_camera( test_set, rcams )\n\n    # Apply 3d post-processing (centering around root)\n    train_set, train_root_positions = postprocess_3d( train_set )\n    test_set,  test_root_positions  = postprocess_3d( test_set )\n\n    # Compute normalization statistics\n    complete_train = copy.deepcopy( np.vstack( train_set.values() ))\n    data_mean, data_std, dim_to_ignore, dim_to_use = normalization_stats( complete_train, dim=3, predict_14=predict_14 )\n\n    # Divide every dimension independently\n    train_set = normalize_data( train_set, data_mean, data_std, dim_to_use )\n    test_set  = normalize_data( test_set,  data_mean, data_std, dim_to_use )\n\n    return train_set, test_set, data_mean, data_std, dim_to_ignore, dim_to_use, train_root_positions, test_root_positions\n\n\ndef postprocess_3d( poses_set ):\n    """"""\n    Center 3d points around root\n\n    Args\n    poses_set: dictionary with 3d data\n    Returns\n    poses_set: dictionary with 3d data centred around root (center hip) joint\n    root_positions: dictionary with the original 3d position of each pose\n    """"""\n    root_positions = {}\n    for k in poses_set.keys():\n        # Keep track of the global position\n        root_positions[k] = copy.deepcopy(poses_set[k][:,:3])\n\n        # Remove the root from the 3d position\n        poses = poses_set[k]\n        poses = poses - np.tile( poses[:,:3], [1, len(H36M_NAMES)] )\n        poses_set[k] = poses\n\n    return poses_set, root_positions\n\n\ndef postprocess_2d( poses_set ):\n    """"""\n    Center 2d points around root\n\n    Args\n    poses_set: dictionary with 2d data\n    Returns\n    poses_set: dictionary with 2d data centred around root (center hip) joint\n    root_positions: dictionary with the original 2d position of each pose\n    """"""\n    root_positions = {}\n    for k in poses_set.keys():\n        # Keep track of the global position\n        root_positions[k] = copy.deepcopy(poses_set[k][:,:2])\n\n        # Remove the root from the 2d position\n        poses = poses_set[k]\n        poses = poses - np.tile( poses[:,:2], [1, len(H36M_NAMES)] )\n        poses_set[k] = poses\n\n    return poses_set, root_positions\n'"
lib/utils/img_utils.py,2,"b'import cv2\nimport torch\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D  # <-- Note the capitalization!\n\nfrom lib.utils.utils import calc_total_skeleton_length\nfrom lib.utils.transforms import get_affine_transform\nfrom lib.utils.augmentation import occlude_with_objects\nfrom lib.core.integral_loss import get_joint_location_result, generate_joint_location_label\nfrom lib.utils.triangulation import iterative_LS_triangulation\nfrom lib.utils.prep_h36m import from_worldjt_to_imagejt\nimport random\nimport numpy as np\nfrom easydict import EasyDict as edict\n\ndef get_default_augment_config():\n    config = edict()\n    config.scale_factor = 0.25\n    config.rot_factor = 30\n    config.color_factor = 0.2\n    config.do_flip_aug = False # True\n\n    config.rot_aug_rate = 0.6  #possibility to rot aug\n    config.flip_aug_rate = 0.5 #possibility to flip aug\n    return config\n\n\ndef do_augmentation():\n    aug_config = get_default_augment_config()\n\n    scale = np.clip(np.random.randn(), -1.0, 1.0) * aug_config.scale_factor + 1.0\n    rot = np.clip(np.random.randn(), -2.0,\n                  2.0) * aug_config.rot_factor if random.random() <= aug_config.rot_aug_rate else 0\n    do_flip = aug_config.do_flip_aug and random.random() <= aug_config.flip_aug_rate\n    c_up = 1.0 + aug_config.color_factor\n    c_low = 1.0 - aug_config.color_factor\n    color_scale = [random.uniform(c_low, c_up), random.uniform(c_low, c_up), random.uniform(c_low, c_up)]\n    return scale, rot, do_flip, color_scale\n\n\ndef fliplr_joints(_joints, _joints_vis, width, matched_parts):\n    """"""\n    flip coords\n    joints: numpy array, nJoints * dim, dim == 2 [x, y] or dim == 3  [x, y, z]\n    joints_vis: same as joints\n    width: image width\n    matched_parts: list of pairs\n    """"""\n    joints = _joints.copy()\n    joints_vis = _joints_vis.copy()\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints, joints_vis\n\n\ndef rotate_2d(pt_2d, rot_rad):\n    x = pt_2d[0]\n    y = pt_2d[1]\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    xx = x * cs - y * sn\n    yy = x * sn + y * cs\n    return np.array([xx, yy], dtype=np.float32)\n\n\ndef gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, scale, rot, inv=False):\n    # augment size with scale\n    src_w = src_width * scale\n    src_h = src_height * scale\n    src_center = np.zeros(2)\n    src_center[0] = c_x\n    src_center[1] = c_y # np.array([c_x, c_y], dtype=np.float32)\n    # augment rotation\n    rot_rad = np.pi * rot / 180\n    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n\n    dst_w = dst_width\n    dst_h = dst_height\n    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = src_center\n    src[1, :] = src_center + src_downdir\n    src[2, :] = src_center + src_rightdir\n\n    dst = np.zeros((3, 2), dtype=np.float32)\n    dst[0, :] = dst_center\n    dst[1, :] = dst_center + dst_downdir\n    dst[2, :] = dst_center + dst_rightdir\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef trans_point2d(pt_2d, trans):\n    src_pt = np.array([pt_2d[0], pt_2d[1], 1.]).T\n    dst_pt = np.dot(trans, src_pt)\n    return dst_pt[0:2]\n\n\ndef generate_patch_image_cv(cvimg, c_x, c_y, bb_width, bb_height, patch_width, patch_height, do_flip, scale, rot):\n    img = cvimg.copy()\n    # c = center.copy()\n    img_height, img_width, img_channels = img.shape\n\n    if do_flip:\n        img = img[:, ::-1, :]\n        c_x = img_width - c_x - 1\n\n    trans = gen_trans_from_patch_cv(c_x, c_y, bb_width, bb_height, patch_width, patch_height, scale, rot, inv=False)\n\n    img_patch = cv2.warpAffine(img, trans, (int(patch_width), int(patch_height)), flags=cv2.INTER_LINEAR)\n\n    return img_patch, trans\n\n\ndef convert_cvimg_to_tensor(cvimg, occlusion_aug=True):\n    # from h,w,c(OpenCV) to c,h,w\n    tensor = cvimg.copy()\n    tensor = np.transpose(tensor, (2, 0, 1))\n    # from BGR(OpenCV) to RGB\n    # tensor = tensor[::-1, :, :]\n    # from int to float\n    tensor = tensor.astype(np.float32)\n    return tensor\n\n\ndef trans_coords_from_patch_to_org(coords_in_patch, c_x, c_y, bb_width, bb_height, patch_width, patch_height,\n                                   scale=1.0, rot=0):\n    coords_in_org = coords_in_patch.copy()\n    trans = gen_trans_from_patch_cv(c_x, c_y, bb_width, bb_height, patch_width, patch_height, scale, rot, inv=True)\n    for p in range(coords_in_patch.shape[0]):\n        coords_in_org[p, 0:2] = trans_point2d(coords_in_patch[p, 0:2], trans)\n    return coords_in_org\n\n\ndef trans_coords_from_patch_to_org_3d(coords_in_patch, c_x, c_y, bb_width, bb_height, patch_width, patch_height\n                                      , rect_3d_width, rect_3d_height, scale=1.0, rot=0):\n    res_img = trans_coords_from_patch_to_org(coords_in_patch, c_x, c_y, bb_width, bb_height, patch_width, patch_height,\n                                             scale, rot)\n    res_img[:, 2] = coords_in_patch[:, 2] / patch_width * rect_3d_width\n    return res_img\n\n\ndef rescale_pose_from_patch_to_camera(preds_in_patch, target_bone_len, parent_ids):\n    preds_in_patch_base_pelvis = preds_in_patch - preds_in_patch[0]\n    skeleton_length = calc_total_skeleton_length(preds_in_patch_base_pelvis, parent_ids)\n    rescale_factor = 1.0 * target_bone_len / skeleton_length\n    preds_in_patch_base_pelvis = rescale_factor * preds_in_patch_base_pelvis\n    return preds_in_patch_base_pelvis\n\n\ndef self_supervision(preds, meta):\n    batch_size = preds.shape[0]\n\n    preds_in_patch_with_score = get_joint_location_result(256, 256, preds)\n\n    preds_in_img_with_score = []\n    for n_sample in range(batch_size):\n        kp = trans_coords_from_patch_to_org_3d(preds_in_patch_with_score[n_sample],\n                                               meta[\'center_x\'][n_sample],\n                                               meta[\'center_y\'][n_sample],\n                                               meta[\'width\'][n_sample],\n                                               meta[\'height\'][n_sample],\n                                               256, 256,\n                                               2000, 2000,\n                                               scale=meta[\'scale\'][n_sample],\n                                               rot=meta[\'rot\'][n_sample]\n                                               )\n        preds_in_img_with_score.append(kp)\n\n    preds_in_img_with_score = np.asarray(preds_in_img_with_score)\n    coords_3d_in_global_frame = triangulate(preds_in_img_with_score, meta)\n\n    batch_label, batch_label_weight = get_batch_labels_from_global_coords(coords_3d_in_global_frame, meta)\n\n    return batch_label, batch_label_weight\n\n\ndef triangulate(kps, meta):\n    half_batch_size = kps.shape[0] // 2\n\n    coords_3d_in_global_frame = []\n\n    for idx in range(half_batch_size):\n        u1, u2 = kps[idx, :, 0:2], kps[half_batch_size + idx, :, 0:2]\n\n        P1, P2 = meta[\'projection_matrix\'][idx].numpy(), \\\n                 meta[\'projection_matrix\'][half_batch_size + idx].numpy()\n\n        pt_3d, pt_vis = iterative_LS_triangulation(u1, P1, u2, P2)\n        coords_3d_in_global_frame.append(pt_3d)\n\n    coords_3d_in_global_frame = np.asarray(coords_3d_in_global_frame)\n    coords_3d_in_global_frame = np.vstack([coords_3d_in_global_frame, coords_3d_in_global_frame])\n    return coords_3d_in_global_frame\n\n\ndef get_batch_labels_from_global_coords(coords_3d_in_global_frame, meta):\n    batch_size = coords_3d_in_global_frame.shape[0]\n\n    batch_label = []\n    batch_label_weight = []\n\n    for i in range(batch_size):\n        scale = meta[\'scale\'][i]\n        aug_rot = meta[\'rot\'][i]\n        c_x = meta[\'center_x\'][i]\n        c_y = meta[\'center_y\'][i]\n        bb_width = meta[\'width\'][i]\n        bb_height = meta[\'height\'][i]\n\n        keypoints, transf, rot, fl, c_p = coords_3d_in_global_frame[i], meta[\'T\'][i].numpy(), meta[\'R\'][i].numpy(), \\\n                                         meta[\'f\'][i].numpy(), meta[\'c\'][i].numpy()\n\n        rect2d_l, rect2d_r, rect2d_t, rect2d_b, joints, pt_3d, joints_vis, pelvis3d = \\\n            from_worldjt_to_imagejt(keypoints.shape[0], rot, keypoints, transf, fl, c_p, 2000., 2000.)\n\n        trans = gen_trans_from_patch_cv(c_x, c_y, bb_width, bb_height, 256, 256, scale, aug_rot, inv=False)\n\n        for n_jt in range(len(joints)):\n            joints[n_jt, 0:2] = trans_point2d(joints[n_jt, 0:2], trans)\n            joints[n_jt, 2] = joints[n_jt, 2] / (2000. * scale) * 256.\n\n        label, label_weight = generate_joint_location_label(256., 256., joints, joints_vis)\n\n        batch_label.append(label)\n        batch_label_weight.append(label_weight)\n\n    return np.asarray(batch_label, dtype=np.float32), np.asarray(batch_label_weight, dtype=np.float32)\n\n\ndef get_single_patch_sample(img_path, center_x, center_y, width, height,\n                            joints, joints_vis, flip_pairs, parent_ids,\n                            patch_width, patch_height, rect_3d_width, rect_3d_height, mean, std,\n                            do_augment, label_func, depth_in_image=False, occluder=None, DEBUG=False):\n    # 1. load image\n    cvimg = cv2.imread(\n        img_path, cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n\n    if not isinstance(cvimg, np.ndarray):\n        raise IOError(""Fail to read %s"" % img_path)\n\n    img_height, img_width, img_channels = cvimg.shape\n\n    # 2. get augmentation params\n    if do_augment:\n        scale, rot, do_flip, color_scale = do_augmentation()\n    else:\n        scale, rot, do_flip, color_scale = 1.0, 0, False, [1.0, 1.0, 1.0]\n\n    # 3. generate image patch\n    img_patch_cv, trans = generate_patch_image_cv(cvimg, center_x, center_y, width, height, patch_width, patch_height,\n                                                  do_flip, scale, rot)\n    image = img_patch_cv.copy()\n    image = image[:, :, ::-1]\n\n    if occluder:\n        image = occlude_with_objects(image, occluder)\n\n    img_patch_cv = image.copy()\n    img_patch = convert_cvimg_to_tensor(image)\n\n    # apply normalization\n    for n_c in range(img_channels):\n        img_patch[n_c, :, :] = np.clip(img_patch[n_c, :, :] * color_scale[n_c], 0, 255)\n        if mean is not None and std is not None:\n            img_patch[n_c, :, :] = (img_patch[n_c, :, :] - mean[n_c]) / std[n_c]\n\n    # 4. generate patch joint ground truth\n    # flip joints and apply Affine Transform on joints\n    if do_flip:\n        joints, joints_vis = fliplr_joints(joints, joints_vis, img_width, flip_pairs)\n\n    for n_jt in range(len(joints)):\n        joints[n_jt, 0:2] = trans_point2d(joints[n_jt, 0:2], trans)\n        if depth_in_image:\n            joints[n_jt, 2] = joints[n_jt, 2] / (width * scale) * patch_width\n        else:\n            joints[n_jt, 2] = joints[n_jt, 2] / (rect_3d_width * scale) * patch_width\n\n    # 5. get label of some type according to certain need\n    label, label_weight = label_func(patch_width, patch_height, joints, joints_vis)\n\n    return img_patch, label, label_weight, scale, rot\n\n\ndef multi_meshgrid(*args):\n    """"""\n    Creates a meshgrid from possibly many\n    elements (instead of only 2).\n    Returns a nd tensor with as many dimensions\n    as there are arguments\n    """"""\n    args = list(args)\n    template = [1 for _ in args]\n    for i in range(len(args)):\n        n = args[i].shape[0]\n        template_copy = template.copy()\n        template_copy[i] = n\n        args[i] = args[i].view(*template_copy)\n        # there will be some broadcast magic going on\n    return tuple(args)\n\n\ndef flip(tensor, dims):\n    if not isinstance(dims, (tuple, list)):\n        dims = [dims]\n    indices = [torch.arange(tensor.shape[dim] - 1, -1, -1,\n                            dtype=torch.int64) for dim in dims]\n    multi_indices = multi_meshgrid(*indices)\n    final_indices = [slice(i) for i in tensor.shape]\n    for i, dim in enumerate(dims):\n        final_indices[dim] = multi_indices[i]\n    flipped = tensor[final_indices]\n    assert flipped.device == tensor.device\n    assert flipped.requires_grad == tensor.requires_grad\n    return flipped'"
lib/utils/prep_h36m.py,0,"b'import numpy as np\nimport pickle\n\n\n# Joints in H3.6M -- data has 32 joints, but only 17 that move; these are the indices.\nH36M_NAMES = [\'\']*17\nH36M_NAMES[0]  = \'Hip\'\nH36M_NAMES[1]  = \'RHip\'\nH36M_NAMES[2]  = \'RKnee\'\nH36M_NAMES[3]  = \'RFoot\'\nH36M_NAMES[4]  = \'LHip\'\nH36M_NAMES[5]  = \'LKnee\'\nH36M_NAMES[6]  = \'LFoot\'\nH36M_NAMES[7] = \'Spine\'\nH36M_NAMES[8] = \'Thorax\'\nH36M_NAMES[9] = \'Neck/Nose\'\nH36M_NAMES[10] = \'Head\'\nH36M_NAMES[11] = \'LShoulder\'\nH36M_NAMES[12] = \'LElbow\'\nH36M_NAMES[13] = \'LWrist\'\nH36M_NAMES[14] = \'RShoulder\'\nH36M_NAMES[15] = \'RElbow\'\nH36M_NAMES[16] = \'RWrist\'\n\n# Stacked Hourglass produces 16 joints. These are the names.\nMPII_NAMES = [\'\']*16\nMPII_NAMES[0]  = \'RFoot\'\nMPII_NAMES[1]  = \'RKnee\'\nMPII_NAMES[2]  = \'RHip\'\nMPII_NAMES[3]  = \'LHip\'\nMPII_NAMES[4]  = \'LKnee\'\nMPII_NAMES[5]  = \'LFoot\'\nMPII_NAMES[6]  = \'Hip\'\nMPII_NAMES[7]  = \'Thorax\'\nMPII_NAMES[8]  = \'Neck/Nose\'\nMPII_NAMES[9]  = \'Head\'\nMPII_NAMES[10] = \'RWrist\'\nMPII_NAMES[11] = \'RElbow\'\nMPII_NAMES[12] = \'RShoulder\'\nMPII_NAMES[13] = \'LShoulder\'\nMPII_NAMES[14] = \'LElbow\'\nMPII_NAMES[15] = \'LWrist\'\n\nH36M_TO_MPII_PERM = np.array([H36M_NAMES.index(h) for h in MPII_NAMES if h != \'\' and h in H36M_NAMES])\n\ncalc_jts = [0,1,2,3,4,5,6,7,10,11,12,13,14,15]\n\ndims_to_use = [0,1,2,3,6,7,8,12,13,14,15,17,18,19,25,26,27]\n\ns_hm36_2_mpii_jt = [3, 2, 1, 4, 5, 6, 0, 8, 9, 10, 16, 15, 14, 11, 12, 13]\n\nDIM_TO_USE_3D = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 18, 19, 20, 21, 22, 23, 24, 25, 26, 36, 37, 38, 39, 40, 41,\n                 42, 43, 44, 45, 46, 47, 51, 52, 53, 54, 55, 56, 57, 58, 59, 75, 76, 77, 78, 79, 80, 81, 82, 83]\n\nroot_dir = \'/media/muhammed/Other/RESEARCH/datasets/Human3.6M/ours\'\n\nACTIONS_ALL = [""Directions"",""Discussion"",""Eating"",""Greeting"",\n             ""Phoning"",""Photo"",""Posing"",""Purchases"",\n             ""Sitting"",""SittingDown"",""Smoking"",""Waiting"",\n             ""WalkDog"",""Walking"",""WalkTogether""]\n\nTRAIN_SUBJECTS = [1,5,6,7,8]\nTEST_SUBJECTS  = [9,11]\n\ns_36_bone_jts = np.array([[0, 7], [7, 8], [8, 9], [9, 10], [8, 11], [11, 12], [12, 13],\n                          [8, 14], [14, 15], [15, 16], [0, 1], [1, 2], [2, 3], [0, 4], [4, 5], [5, 6]])\n\ns_36_flip_pairs = np.array([[1, 4], [2, 5], [3, 6], [14, 11], [15, 12], [16, 13]], dtype=np.int)\ns_36_parent_ids = np.array([0, 0, 1, 2, 0, 4, 5, 0, 8, 8, 9, 8, 11, 12, 8, 14, 15], dtype=np.int)\n\nmpii_flip_pairs = [[0, 5], [1, 4], [2, 3], [10, 15], [11, 14], [12, 13]]\nmpii_parent_ids = [1, 2, 6, 6, 3, 4, 6, 6, 7, 8, 11, 12, 7, 7, 13, 14]\n\ndef save(object, filename):\n    with open(filename, \'wb\') as f:\n        pickle.dump(object, f, protocol=4)\n\ndef check_image(img):\n    if img.shape == (1002, 1000, 3):\n        img = img[:1000, :, :]\n    elif img.shape == (1002, 1000):\n        img = img[:1000, :]\n    return img\n\ndef CamBackProj(cam_x, cam_y, depth, fx, fy, u, v):\n    x = (cam_x - u) / fx * depth\n    y = (cam_y - v) / fy * depth\n    z = depth\n    return x, y, z\n\ndef rigid_transform_3D(A, B):\n    centroid_A = np.mean(A, axis = 0)\n    centroid_B = np.mean(B, axis = 0)\n    H = np.dot(np.transpose(A - centroid_A), B - centroid_B)\n    U, s, V = np.linalg.svd(H)\n    R = np.dot(np.transpose(V), np.transpose(U))\n    if np.linalg.det(R) < 0:\n        V[2] = -V[2]\n        R = np.dot(np.transpose(V), np.transpose(U))\n    t = -np.dot(R, np.transpose(centroid_A)) + np.transpose(centroid_B)\n    return R, t\n\ndef rigid_align(A, B):\n    R, t = rigid_transform_3D(A, B)\n    A2 = np.transpose(np.dot(R, np.transpose(A))) + t\n    return A2\n\ndef compute_similarity_transform(X, Y, compute_optimal_scale=False):\n    """"""\n    A port of MATLAB\'s `procrustes` function to Numpy.\n    Adapted from http://stackoverflow.com/a/18927641/1884420\n\n    Args\n    X: array NxM of targets, with N number of points and M point dimensionality\n    Y: array NxM of inputs\n    compute_optimal_scale: whether we compute optimal scale or force it to be 1\n\n    Returns:\n    d: squared error after transformation\n    Z: transformed Y\n    T: computed rotation\n    b: scaling\n    c: translation\n    """"""\n\n    muX = X.mean(0)\n    muY = Y.mean(0)\n\n    X0 = X - muX\n    Y0 = Y - muY\n\n    ssX = (X0**2.).sum()\n    ssY = (Y0**2.).sum()\n\n    # centred Frobenius norm\n    normX = np.sqrt(ssX)\n    normY = np.sqrt(ssY)\n\n    # scale to equal (unit) norm\n    X0 = X0 / normX\n    Y0 = Y0 / normY\n\n    # optimum rotation matrix of Y\n    A = np.dot(X0.T, Y0)\n    U,s,Vt = np.linalg.svd(A,full_matrices=False)\n    V = Vt.T\n    T = np.dot(V, U.T)\n\n    # Make sure we have a rotation\n    detT = np.linalg.det(T)\n    V[:,-1] *= np.sign( detT )\n    s[-1]   *= np.sign( detT )\n    T = np.dot(V, U.T)\n\n    traceTA = s.sum()\n\n    if compute_optimal_scale:  # Compute optimum scaling of Y.\n        b = traceTA * normX / normY\n        d = 1 - traceTA**2\n        Z = normX*traceTA*np.dot(Y0, T) + muX\n    else:  # If no scaling allowed\n        b = 1\n        d = 1 + ssY/ssX - 2 * traceTA * normY / normX\n        Z = normY*np.dot(Y0, T) + muX\n\n    c = muX - b*np.dot(muY, T)\n\n    return d, Z, T, b, c\n\ndef CamProj(x, y, z, fx, fy, u, v):\n    cam_x = x / z * fx\n    cam_x = cam_x + u\n    cam_y = y / z * fy\n    cam_y = cam_y + v\n    return cam_x, cam_y\n\ndef from_worldjt_to_imagejt(joint_num, rot, keypoints, trans, fl, c_p, rect_3d_width, rect_3d_height, mpii=False):\n    # project to image space\n    pt_3d = np.zeros((joint_num, 3), dtype=np.float)\n    pt_2d = np.zeros((joint_num, 3), dtype=np.float)\n\n    root_joint = 6 if mpii else 0\n\n    for n_jt in range(0, joint_num):\n\n        pt_3d[n_jt] = np.dot(rot, keypoints[n_jt] - trans.reshape(3))\n        pt_2d[n_jt, 0], pt_2d[n_jt, 1] = CamProj(pt_3d[n_jt, 0], pt_3d[n_jt, 1], pt_3d[n_jt, 2], fl[0], fl[1],\n                                                 c_p[0], c_p[1])\n        pt_2d[n_jt, 2] = pt_3d[n_jt, 2]\n\n    pelvis3d = pt_3d[root_joint]\n    # build 3D bounding box centered on pelvis, size 2000^2\n    rect3d_lt = pelvis3d - [rect_3d_width / 2, rect_3d_height / 2, 0]\n    rect3d_rb = pelvis3d + [rect_3d_width / 2, rect_3d_height / 2, 0]\n    # back-project 3D BBox to 2D image\n    rect2d_l, rect2d_t = CamProj(rect3d_lt[0], rect3d_lt[1], rect3d_lt[2], fl[0], fl[1], c_p[0], c_p[1])\n    rect2d_r, rect2d_b = CamProj(rect3d_rb[0], rect3d_rb[1], rect3d_rb[2], fl[0], fl[1], c_p[0], c_p[1])\n\n    # Subtract pelvis depth\n    pt_2d[:, 2] = pt_2d[:, 2] - pelvis3d[2]\n    pt_2d = pt_2d.reshape((joint_num, 3))\n    vis = np.ones((joint_num, 3), dtype=np.float)\n\n    return rect2d_l, rect2d_r, rect2d_t, rect2d_b, pt_2d, pt_3d, vis, pelvis3d\n\n\ndef save_annotations():\n    # will be released after proper testing and cleaning\n    raise NotImplementedError\n\ndef save_triangulations():\n    # will be released after proper testing and cleaning\n    raise NotImplementedError\n'"
lib/utils/transforms.py,0,"b'import numpy as np\nimport cv2\n\n\ndef flip_back(output_flipped, matched_parts):\n    \'\'\'\n    ouput_flipped: numpy.ndarray(batch_size, num_joints, height, width)\n    \'\'\'\n    assert output_flipped.ndim == 4,\\\n        \'output_flipped should be [batch_size, num_joints, height, width]\'\n\n    output_flipped = output_flipped[:, :, :, ::-1]\n\n    for pair in matched_parts:\n        tmp = output_flipped[:, pair[0], :, :].copy()\n        output_flipped[:, pair[0], :, :] = output_flipped[:, pair[1], :, :]\n        output_flipped[:, pair[1], :, :] = tmp\n\n    return output_flipped\n\n\ndef fliplr_joints(joints, joints_vis, width, matched_parts):\n    """"""\n    flip coords\n    """"""\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = \\\n            joints[pair[1], :], joints[pair[0], :].copy()\n        joints_vis[pair[0], :], joints_vis[pair[1], :] = \\\n            joints_vis[pair[1], :], joints_vis[pair[0], :].copy()\n\n    return joints*joints_vis, joints_vis\n\n\ndef transform_preds(coords, center, scale, output_size):\n    target_coords = np.zeros(coords.shape)\n    trans = get_affine_transform(center, scale, 0, output_size, inv=1)\n    for p in range(coords.shape[0]):\n        target_coords[p, 0:2] = affine_transform(coords[p, 0:2], trans)\n    return target_coords\n\n\ndef get_affine_transform(center,\n                         scale,\n                         rot,\n                         output_size,\n                         shift=np.array([0, 0], dtype=np.float32),\n                         inv=0):\n    if not isinstance(scale, np.ndarray) and not isinstance(scale, list):\n        print(scale)\n        scale = np.array([scale, scale])\n\n    scale_tmp = scale * 200.0\n    src_w = scale_tmp[0]\n    dst_w = output_size[0]\n    dst_h = output_size[1]\n\n    rot_rad = np.pi * rot / 180\n    src_dir = get_dir([0, src_w * -0.5], rot_rad)\n    dst_dir = np.array([0, dst_w * -0.5], np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    dst = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = center + scale_tmp * shift\n    src[1, :] = center + src_dir + scale_tmp * shift\n    dst[0, :] = [dst_w * 0.5, dst_h * 0.5]\n    dst[1, :] = np.array([dst_w * 0.5, dst_h * 0.5]) + dst_dir\n\n    src[2:, :] = get_3rd_point(src[0, :], src[1, :])\n    dst[2:, :] = get_3rd_point(dst[0, :], dst[1, :])\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\n\ndef affine_transform(pt, t):\n    new_pt = np.array([pt[0], pt[1], 1.]).T\n    new_pt = np.dot(t, new_pt)\n    return new_pt[:2]\n\n\ndef get_3rd_point(a, b):\n    direct = a - b\n    return b + np.array([-direct[1], direct[0]], dtype=np.float32)\n\n\ndef get_dir(src_point, rot_rad):\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n\n    src_result = [0, 0]\n    src_result[0] = src_point[0] * cs - src_point[1] * sn\n    src_result[1] = src_point[0] * sn + src_point[1] * cs\n\n    return src_result\n\n\ndef crop(img, center, scale, output_size, rot=0):\n    trans = get_affine_transform(center, scale, rot, output_size)\n\n    dst_img = cv2.warpAffine(img,\n                             trans,\n                             (int(output_size[0]), int(output_size[1])),\n                             flags=cv2.INTER_LINEAR)\n\n    return dst_img\n'"
lib/utils/triangulation.py,0,"b'import numpy as np\nimport cv2\n\n\'\'\'\nCode borrowed from: https://github.com/Eliasvan/Multiple-Quadrotor-SLAM\n\'\'\'\n\ndef linear_eigen_triangulation(u1, P1, u2, P2, max_coordinate_value=1.e16):\n\t""""""\n\tLinear Eigenvalue based (using SVD) triangulation.\n\tWrapper to OpenCV\'s ""triangulatePoints()"" function.\n\tRelative speed: 1.0\n\n\t(u1, P1) is the reference pair containing normalized image coordinates (x, y) and the corresponding camera matrix.\n\t(u2, P2) is the second pair.\n\t""max_coordinate_value"" is a threshold to decide whether points are at infinity\n\n\tu1 and u2 are matrices: amount of points equals #rows and should be equal for u1 and u2.\n\n\tThe status-vector is based on the assumption that all 3D points have finite coordinates.\n\t""""""\n\tx = cv2.triangulatePoints(P1[0:3, 0:4], P2[0:3, 0:4], u1.T, u2.T)  # OpenCV\'s Linear-Eigen triangl\n\n\tx[0:3, :] /= x[3:4, :]  # normalize coordinates\n\tx_status = (np.max(abs(x[0:3, :]), axis=0) <= max_coordinate_value)  # NaN or Inf will receive status False\n\n\treturn x[0:3, :].T.astype(output_dtype), x_status\n\n\n# Initialize consts to be used in linear_LS_triangulation()\nlinear_LS_triangulation_C = -np.eye(2, 3)\n\n\ndef linear_LS_triangulation(u1, P1, u2, P2):\n\t""""""\n\tLinear Least Squares based triangulation.\n\tRelative speed: 0.1\n\n\t(u1, P1) is the reference pair containing normalized image coordinates (x, y) and the corresponding camera matrix.\n\t(u2, P2) is the second pair.\n\n\tu1 and u2 are matrices: amount of points equals #rows and should be equal for u1 and u2.\n\n\tThe status-vector will be True for all points.\n\t""""""\n\tA = np.zeros((4, 3))\n\tb = np.zeros((4, 1))\n\n\t# Create array of triangulated points\n\tx = np.zeros((3, len(u1)))\n\n\t# Initialize C matrices\n\tC1 = np.array(linear_LS_triangulation_C)\n\tC2 = np.array(linear_LS_triangulation_C)\n\n\tfor i in range(len(u1)):\n\t\t# Derivation of matrices A and b:\n\t\t# for each camera following equations hold in case of perfect point matches:\n\t\t#     u.x * (P[2,:] * x)     =     P[0,:] * x\n\t\t#     u.y * (P[2,:] * x)     =     P[1,:] * x\n\t\t# and imposing the constraint:\n\t\t#     x = [x.x, x.y, x.z, 1]^T\n\t\t# yields:\n\t\t#     (u.x * P[2, 0:3] - P[0, 0:3]) * [x.x, x.y, x.z]^T     +     (u.x * P[2, 3] - P[0, 3]) * 1     =     0\n\t\t#     (u.y * P[2, 0:3] - P[1, 0:3]) * [x.x, x.y, x.z]^T     +     (u.y * P[2, 3] - P[1, 3]) * 1     =     0\n\t\t# and since we have to do this for 2 cameras, and since we imposed the constraint,\n\t\t# we have to solve 4 equations in 3 unknowns (in LS sense).\n\n\t\t# Build C matrices, to construct A and b in a concise way\n\t\tC1[:, 2] = u1[i, :]\n\t\tC2[:, 2] = u2[i, :]\n\n\t\t# Build A matrix:\n\t\t# [\n\t\t#     [ u1.x * P1[2,0] - P1[0,0],    u1.x * P1[2,1] - P1[0,1],    u1.x * P1[2,2] - P1[0,2] ],\n\t\t#     [ u1.y * P1[2,0] - P1[1,0],    u1.y * P1[2,1] - P1[1,1],    u1.y * P1[2,2] - P1[1,2] ],\n\t\t#     [ u2.x * P2[2,0] - P2[0,0],    u2.x * P2[2,1] - P2[0,1],    u2.x * P2[2,2] - P2[0,2] ],\n\t\t#     [ u2.y * P2[2,0] - P2[1,0],    u2.y * P2[2,1] - P2[1,1],    u2.y * P2[2,2] - P2[1,2] ]\n\t\t# ]\n\t\tA[0:2, :] = C1.dot(P1[0:3, 0:3])  # C1 * R1\n\t\tA[2:4, :] = C2.dot(P2[0:3, 0:3])  # C2 * R2\n\n\t\t# Build b vector:\n\t\t# [\n\t\t#     [ -(u1.x * P1[2,3] - P1[0,3]) ],\n\t\t#     [ -(u1.y * P1[2,3] - P1[1,3]) ],\n\t\t#     [ -(u2.x * P2[2,3] - P2[0,3]) ],\n\t\t#     [ -(u2.y * P2[2,3] - P2[1,3]) ]\n\t\t# ]\n\t\tb[0:2, :] = C1.dot(P1[0:3, 3:4])  # C1 * t1\n\t\tb[2:4, :] = C2.dot(P2[0:3, 3:4])  # C2 * t2\n\t\tb *= -1\n\n\t\t# Solve for x vector\n\t\tcv2.solve(A, b, x[:, i:i + 1], cv2.DECOMP_SVD)\n\n\treturn x.T.astype(output_dtype), np.ones(len(u1), dtype=bool)\n\n\n# Initialize consts to be used in iterative_LS_triangulation()\niterative_LS_triangulation_C = -np.eye(2, 3)\n\n\ndef iterative_LS_triangulation(u1, P1, u2, P2, tolerance=3.e-5):\n\t""""""\n\tIterative (Linear) Least Squares based triangulation.\n\tFrom ""Triangulation"", Hartley, R.I. and Sturm, P., Computer vision and image understanding, 1997.\n\tRelative speed: 0.025\n\n\t(u1, P1) is the reference pair containing normalized image coordinates (x, y) and the corresponding camera matrix.\n\t(u2, P2) is the second pair.\n\t""tolerance"" is the depth convergence tolerance.\n\n\tAdditionally returns a status-vector to indicate outliers:\n\t\t1: inlier, and in front of both cameras\n\t\t0: outlier, but in front of both cameras\n\t\t-1: only in front of second camera\n\t\t-2: only in front of first camera\n\t\t-3: not in front of any camera\n\tOutliers are selected based on non-convergence of depth, and on negativity of depths (=> behind camera(s)).\n\n\tu1 and u2 are matrices: amount of points equals #rows and should be equal for u1 and u2.\n\t""""""\n\tA = np.zeros((4, 3))\n\tb = np.zeros((4, 1))\n\n\t# Create array of triangulated points\n\tx = np.empty((4, len(u1)))\n\tx[3, :].fill(1)  # create empty array of homogenous 3D coordinates\n\tx_status = np.empty(len(u1), dtype=int)\n\n\t# Initialize C matrices\n\tC1 = np.array(iterative_LS_triangulation_C)\n\tC2 = np.array(iterative_LS_triangulation_C)\n\n\tfor xi in range(len(u1)):\n\t\t# Build C matrices, to construct A and b in a concise way\n\t\tC1[:, 2] = u1[xi, :]\n\t\tC2[:, 2] = u2[xi, :]\n\n\t\t# Build A matrix\n\t\tA[0:2, :] = C1.dot(P1[0:3, 0:3])  # C1 * R1\n\t\tA[2:4, :] = C2.dot(P2[0:3, 0:3])  # C2 * R2\n\n\t\t# Build b vector\n\t\tb[0:2, :] = C1.dot(P1[0:3, 3:4])  # C1 * t1\n\t\tb[2:4, :] = C2.dot(P2[0:3, 3:4])  # C2 * t2\n\t\tb *= -1\n\n\t\t# Init depths\n\t\td1 = d2 = 1.\n\n\t\tfor i in range(10):  # Hartley suggests 10 iterations at most\n\t\t\t# Solve for x vector\n\t\t\tcv2.solve(A, b, x[0:3, xi:xi + 1], cv2.DECOMP_SVD)\n\n\t\t\t# Calculate new depths\n\t\t\td1_new = P1[2, :].dot(x[:, xi])\n\t\t\td2_new = P2[2, :].dot(x[:, xi])\n\n\t\t\tif abs(d1_new - d1) <= tolerance and \\\n\t\t\t\t\t\t\tabs(d2_new - d2) <= tolerance:\n\t\t\t\tbreak\n\n\t\t\t# Re-weight A matrix and b vector with the new depths\n\t\t\tA[0:2, :] *= 1 / d1_new\n\t\t\tA[2:4, :] *= 1 / d2_new\n\t\t\tb[0:2, :] *= 1 / d1_new\n\t\t\tb[2:4, :] *= 1 / d2_new\n\n\t\t\t# Update depths\n\t\t\td1 = d1_new\n\t\t\td2 = d2_new\n\n\t\t# Set status\n\t\tx_status[xi] = (i < 10 and  # points should have converged by now\n\t\t                (d1_new > 0 and d2_new > 0))  # points should be in front of both cameras\n\t\tif d1_new <= 0: x_status[xi] -= 1\n\t\tif d2_new <= 0: x_status[xi] -= 2\n\n\treturn x[0:3, :].T.astype(output_dtype), x_status\n\n\ndef polynomial_triangulation(u1, P1, u2, P2):\n\t""""""\n\tPolynomial (Optimal) triangulation.\n\tUses Linear-Eigen for final triangulation.\n\tRelative speed: 0.1\n\n\t(u1, P1) is the reference pair containing normalized image coordinates (x, y) and the corresponding camera matrix.\n\t(u2, P2) is the second pair.\n\n\tu1 and u2 are matrices: amount of points equals #rows and should be equal for u1 and u2.\n\n\tThe status-vector is based on the assumption that all 3D points have finite coordinates.\n\t""""""\n\tP1_full = np.eye(4)\n\tP1_full[0:3, :] = P1[0:3, :]  # convert to 4x4\n\tP2_full = np.eye(4)\n\tP2_full[0:3, :] = P2[0:3, :]  # convert to 4x4\n\tP_canon = P2_full.dot(cv2.invert(P1_full)[1])  # find canonical P which satisfies P2 = P_canon * P1\n\n\t# ""F = [t]_cross * R"" [HZ 9.2.4]; transpose is needed for numpy\n\tF = np.cross(P_canon[0:3, 3], P_canon[0:3, 0:3], axisb=0).T\n\n\t# Other way of calculating ""F"" [HZ (9.2)]\n\t# op1 = (P2[0:3, 3:4] - P2[0:3, 0:3] .dot (cv2.invert(P1[0:3, 0:3])[1]) .dot (P1[0:3, 3:4]))\n\t# op2 = P2[0:3, 0:4] .dot (cv2.invert(P1_full)[1][0:4, 0:3])\n\t# F = np.cross(op1.reshape(-1), op2, axisb=0).T\n\n\t# Project 2D matches to closest pair of epipolar lines\n\tu1_new, u2_new = cv2.correctMatches(F, u1.reshape(1, len(u1), 2), u2.reshape(1, len(u1), 2))\n\n\t# For a purely sideways trajectory of 2nd cam, correctMatches() returns NaN for all possible points!\n\tif np.isnan(u1_new).all() or np.isnan(u2_new).all():\n\t\tF = cv2.findFundamentalMat(u1, u2, cv2.FM_8POINT)[0]  # so use a noisy version of the fund mat\n\t\tu1_new, u2_new = cv2.correctMatches(F, u1.reshape(1, len(u1), 2), u2.reshape(1, len(u1), 2))\n\n\t# Triangulate using the refined image points\n\treturn linear_eigen_triangulation(u1_new[0], P1, u2_new[0], P2)\n\n\noutput_dtype = float\n\n\ndef set_triangl_output_dtype(output_dtype_):\n\t""""""\n\tSet the datatype of the triangulated 3D point positions.\n\t(Default is set to ""float"")\n\t""""""\n\tglobal output_dtype\n\toutput_dtype = output_dtype_'"
lib/utils/utils.py,3,"b'import os\nimport logging\nimport time\nfrom pathlib import Path\nimport numpy as np\n\nimport torch\nimport torch.optim as optim\n\nfrom lib.core.config import get_model_name\n\n\ndef create_logger(cfg, cfg_name, phase=\'train\'):\n    root_output_dir = Path(cfg.OUTPUT_DIR)\n    # set up logger\n    if not root_output_dir.exists():\n        print(\'=> creating {}\'.format(root_output_dir))\n        root_output_dir.mkdir()\n\n    dataset = cfg.DATASET.DATASET + \'_\' + cfg.DATASET.HYBRID_JOINTS_TYPE \\\n        if cfg.DATASET.HYBRID_JOINTS_TYPE else cfg.DATASET.DATASET\n    dataset = dataset.replace(\':\', \'_\')\n    model, _ = get_model_name(cfg)\n    cfg_name = os.path.basename(cfg_name).split(\'.\')[0]\n\n    final_output_dir = root_output_dir / dataset / model / cfg.EXP_NAME # cfg_name\n\n    print(\'=> creating {}\'.format(final_output_dir))\n    final_output_dir.mkdir(parents=True, exist_ok=True)\n\n    time_str = time.strftime(\'%Y-%m-%d-%H-%M\')\n    log_file = \'{}_{}_{}.log\'.format(cfg_name, time_str, phase)\n    final_log_file = final_output_dir / log_file\n    head = \'%(asctime)-15s %(message)s\'\n    logging.basicConfig(filename=str(final_log_file),\n                        format=head)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    console = logging.StreamHandler()\n    logging.getLogger(\'\').addHandler(console)\n\n    return logger, str(final_output_dir)\n\n\ndef get_optimizer(cfg, model):\n    optimizer = None\n    if cfg.TRAIN.OPTIMIZER == \'sgd\':\n        optimizer = optim.SGD(\n            model.parameters(),\n            lr=cfg.TRAIN.LR,\n            momentum=cfg.TRAIN.MOMENTUM,\n            weight_decay=cfg.TRAIN.WD,\n            nesterov=cfg.TRAIN.NESTEROV\n        )\n    elif cfg.TRAIN.OPTIMIZER == \'adam\':\n        optimizer = optim.Adam(\n            model.parameters(),\n            lr=cfg.TRAIN.LR\n        )\n\n    return optimizer\n\n\ndef save_checkpoint(states, is_best, output_dir,\n                    filename=\'checkpoint.pth.tar\'):\n    torch.save(states, os.path.join(output_dir, filename))\n    if is_best and \'state_dict\' in states:\n        torch.save(states[\'state_dict\'],\n                   os.path.join(output_dir, \'model_best.pth.tar\'))\n\ndef calc_total_skeleton_length(joints, parent_ids):\n    \'\'\'\n    Based on s_36_parent_ids\n    \'\'\'\n    length = []\n    for j_id in range(len(joints)):\n        p_id = parent_ids[j_id]\n        l = np.linalg.norm(joints[p_id] - joints[j_id])\n        length.append(l)\n\n    return sum(length)\n\n\ndef calc_total_skeleton_length_bone(joints, bone_jts):\n    \'\'\'\n    Based on s_36_bone_jts\n    \'\'\'\n    length = []\n    for jt_a, jt_b in bone_jts:\n        l = np.linalg.norm(joints[jt_a] - joints[jt_b])\n        length.append(l)\n\n    return sum(length)\n\n\ndef calc_kpt_bound(kpts, kpts_vis):\n    MAX_COORD = 10000\n    x = kpts[:, 0]\n    y = kpts[:, 1]\n    z = kpts_vis[:, 0]\n    u = MAX_COORD\n    d = -1\n    l = MAX_COORD\n    r = -1\n    for idx, vis in enumerate(z):\n        if vis == 0:  # skip invisible joint\n            continue\n        u = min(u, y[idx])\n        d = max(d, y[idx])\n        l = min(l, x[idx])\n        r = max(r, x[idx])\n    return u, d, l, r\n\n\ndef calc_kpt_bound_pad(kpts, kpts_vis, aspect_ratio):\n    u, d, l, r = calc_kpt_bound(kpts, kpts_vis)\n\n    c_x = (l + r) * 0.5\n    c_y = (u + d) * 0.5\n    assert c_x >= 1\n\n    w = r - l\n    h = d - u\n    assert w > 0\n    assert h > 0\n\n    if w > aspect_ratio * h:\n        h = w * 1.0 / aspect_ratio\n    elif w < aspect_ratio * h:\n        w = h * aspect_ratio\n\n    w *= 1.25\n    h *= 1.25\n\n    return c_x, c_y, w, h\n\n\ndef compute_similarity_transform(X, Y, compute_optimal_scale=False):\n    """"""\n    A port of MATLAB\'s `procrustes` function to Numpy.\n    Adapted from http://stackoverflow.com/a/18927641/1884420\n\n    Args\n        X: array NxM of targets, with N number of points and M point dimensionality\n        Y: array NxM of inputs\n        compute_optimal_scale: whether we compute optimal scale or force it to be 1\n\n    Returns:\n        d: squared error after transformation\n        Z: transformed Y\n        T: computed rotation\n        b: scaling\n        c: translation\n    """"""\n    muX = X.mean(0)\n    muY = Y.mean(0)\n\n    X0 = X - muX\n    Y0 = Y - muY\n\n    ssX = (X0 ** 2.).sum()\n    ssY = (Y0 ** 2.).sum()\n\n    # centred Frobenius norm\n    normX = np.sqrt(ssX)\n    normY = np.sqrt(ssY)\n\n    # scale to equal (unit) norm\n    X0 = X0 / normX\n    Y0 = Y0 / normY\n\n    # optimum rotation matrix of Y\n    A = np.dot(X0.T, Y0)\n    U, s, Vt = np.linalg.svd(A, full_matrices=False)\n    V = Vt.T\n    T = np.dot(V, U.T)\n\n    # Make sure we have a rotation\n    detT = np.linalg.det(T)\n    V[:, -1] *= np.sign(detT)\n    s[-1] *= np.sign(detT)\n    T = np.dot(V, U.T)\n\n    traceTA = s.sum()\n\n    if compute_optimal_scale:  # Compute optimum scaling of Y.\n        b = traceTA * normX / normY\n        d = 1 - traceTA ** 2\n        Z = normX * traceTA * np.dot(Y0, T) + muX\n    else:  # If no scaling allowed\n        b = 1\n        d = 1 + ssY / ssX - 2 * traceTA * normY / normX\n        Z = normY * np.dot(Y0, T) + muX\n\n    c = muX - b * np.dot(muY, T)\n\n    return d, Z, T, b, c\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value""""""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count if self.count != 0 else 0'"
lib/utils/vis.py,0,"b'import numpy as np\nimport cv2\n\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n\ndef drawskeleton(img, kps, thickness=3, lcolor=(255,0,0), rcolor=(0,0,255), mpii=2):\n\n    if mpii == 0: # h36m with mpii joints\n        connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n                       [5, 6], [0, 8], [8, 9], [9, 10],\n                       [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n        LR = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=bool)\n    elif mpii == 1: # only mpii\n        connections = [[0, 1], [1, 2], [2, 6], [6, 3], [3, 4], [4, 5], [6, 7],\n                       [7, 8], [8, 9], [7, 12], [12, 11], [11, 10], [7, 13], [13, 14], [14, 15]]\n        LR = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0], dtype=bool)\n    else: # default h36m\n        connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n                       [5, 6], [0, 7], [7, 8], [8, 9], [9, 10],\n                       [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n\n        LR = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], dtype=bool)\n\n    for j,c in enumerate(connections):\n        start = map(int, kps[c[0]])\n        end = map(int, kps[c[1]])\n        start = list(start)\n        end = list(end)\n        cv2.line(img, (start[0], start[1]), (end[0], end[1]), lcolor if LR[j] else rcolor, thickness)\n\n\ndef show3Dpose(channels, ax, radius=40, mpii=2, lcolor=\'#ff0000\', rcolor=\'#0000ff\'):\n    vals = channels\n\n    if mpii == 0: # h36m with mpii joints\n        connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n                       [5, 6], [0, 8], [8, 9], [9, 10],\n                       [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n        LR = np.array([0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=bool)\n    elif mpii == 1: # only mpii\n        connections = [[0, 1], [1, 2], [2, 6], [6, 3], [3, 4], [4, 5], [6, 7],\n                       [7, 8], [8, 9], [7, 12], [12, 11], [11, 10], [7, 13], [13, 14], [14, 15]]\n        LR = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1], dtype=bool)\n    else: # default h36m\n        connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n                       [5, 6], [0, 7], [7, 8], [8, 9], [9, 10],\n                       [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n\n        LR = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], dtype=bool)\n\n    for ind, (i,j) in enumerate(connections):\n        x, y, z = [np.array([vals[i, c], vals[j, c]]) for c in range(3)]\n        ax.plot(x, y, z, lw=2, c=lcolor if LR[ind] else rcolor)\n\n    RADIUS = radius  # space around the subject\n    if mpii == 1:\n        xroot, yroot, zroot = vals[6, 0], vals[6, 1], vals[6, 2]\n    else:\n        xroot, yroot, zroot = vals[0, 0], vals[0, 1], vals[0, 2]\n    ax.set_xlim3d([-RADIUS + xroot, RADIUS + xroot])\n    ax.set_zlim3d([-RADIUS + zroot, RADIUS + zroot])\n    ax.set_ylim3d([-RADIUS + yroot, RADIUS + yroot])\n\n    ax.set_xlabel(""x"")\n    ax.set_ylabel(""y"")\n    ax.set_zlabel(""z"")\n'"
