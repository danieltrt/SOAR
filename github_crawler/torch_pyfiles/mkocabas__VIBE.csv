file_path,api_count,code
demo.py,10,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nos.environ[\'PYOPENGL_PLATFORM\'] = \'egl\'\n\nimport cv2\nimport time\nimport torch\nimport joblib\nimport shutil\nimport colorsys\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nfrom multi_person_tracker import MPT\nfrom torch.utils.data import DataLoader\n\nfrom lib.models.vibe import VIBE_Demo\nfrom lib.utils.renderer import Renderer\nfrom lib.dataset.inference import Inference\nfrom lib.data_utils.kp_utils import convert_kps\nfrom lib.utils.pose_tracker import run_posetracker\n\nfrom lib.utils.demo_utils import (\n    download_youtube_clip,\n    smplify_runner,\n    convert_crop_cam_to_orig_img,\n    prepare_rendering_results,\n    video_to_images,\n    images_to_video,\n    download_ckpt,\n)\n\nMIN_NUM_FRAMES = 25\n\ndef main(args):\n    device = torch.device(\'cuda\') if torch.cuda.is_available() else torch.device(\'cpu\')\n\n    video_file = args.vid_file\n\n    # ========= [Optional] download the youtube video ========= #\n    if video_file.startswith(\'https://www.youtube.com\'):\n        print(f\'Donwloading YouTube video \\""{video_file}\\""\')\n        video_file = download_youtube_clip(video_file, \'/tmp\')\n\n        if video_file is None:\n            exit(\'Youtube url is not valid!\')\n\n        print(f\'YouTube Video has been downloaded to {video_file}...\')\n\n    if not os.path.isfile(video_file):\n        exit(f\'Input video \\""{video_file}\\"" does not exist!\')\n\n    output_path = os.path.join(args.output_folder, os.path.basename(video_file).replace(\'.mp4\', \'\'))\n    os.makedirs(output_path, exist_ok=True)\n\n    image_folder, num_frames, img_shape = video_to_images(video_file, return_info=True)\n\n    print(f\'Input video number of frames {num_frames}\')\n    orig_height, orig_width = img_shape[:2]\n\n    total_time = time.time()\n\n    # ========= Run tracking ========= #\n    bbox_scale = 1.1\n    if args.tracking_method == \'pose\':\n        if not os.path.isabs(video_file):\n            video_file = os.path.join(os.getcwd(), video_file)\n        tracking_results = run_posetracker(video_file, staf_folder=args.staf_dir, display=args.display)\n    else:\n        # run multi object tracker\n        mot = MPT(\n            device=device,\n            batch_size=args.tracker_batch_size,\n            display=args.display,\n            detector_type=args.detector,\n            output_format=\'dict\',\n            yolo_img_size=args.yolo_img_size,\n        )\n        tracking_results = mot(image_folder)\n\n    # remove tracklets if num_frames is less than MIN_NUM_FRAMES\n    for person_id in list(tracking_results.keys()):\n        if tracking_results[person_id][\'frames\'].shape[0] < MIN_NUM_FRAMES:\n            del tracking_results[person_id]\n\n    # ========= Define VIBE model ========= #\n    model = VIBE_Demo(\n        seqlen=16,\n        n_layers=2,\n        hidden_size=1024,\n        add_linear=True,\n        use_residual=True,\n    ).to(device)\n\n    # ========= Load pretrained weights ========= #\n    pretrained_file = download_ckpt(use_3dpw=False)\n    ckpt = torch.load(pretrained_file)\n    print(f\'Performance of pretrained model on 3DPW: {ckpt[""performance""]}\')\n    ckpt = ckpt[\'gen_state_dict\']\n    model.load_state_dict(ckpt, strict=False)\n    model.eval()\n    print(f\'Loaded pretrained weights from \\""{pretrained_file}\\""\')\n\n    # ========= Run VIBE on each person ========= #\n    print(f\'Running VIBE on each tracklet...\')\n    vibe_time = time.time()\n    vibe_results = {}\n    for person_id in tqdm(list(tracking_results.keys())):\n        bboxes = joints2d = None\n\n        if args.tracking_method == \'bbox\':\n            bboxes = tracking_results[person_id][\'bbox\']\n        elif args.tracking_method == \'pose\':\n            joints2d = tracking_results[person_id][\'joints2d\']\n\n        frames = tracking_results[person_id][\'frames\']\n\n        dataset = Inference(\n            image_folder=image_folder,\n            frames=frames,\n            bboxes=bboxes,\n            joints2d=joints2d,\n            scale=bbox_scale,\n        )\n\n        bboxes = dataset.bboxes\n        frames = dataset.frames\n        has_keypoints = True if joints2d is not None else False\n\n        dataloader = DataLoader(dataset, batch_size=args.vibe_batch_size, num_workers=16)\n\n        with torch.no_grad():\n\n            pred_cam, pred_verts, pred_pose, pred_betas, pred_joints3d, norm_joints2d = [], [], [], [], [], []\n\n            for batch in dataloader:\n                if has_keypoints:\n                    batch, nj2d = batch\n                    norm_joints2d.append(nj2d.numpy().reshape(-1, 21, 3))\n\n                batch = batch.unsqueeze(0)\n                batch = batch.to(device)\n\n                batch_size, seqlen = batch.shape[:2]\n                output = model(batch)[-1]\n\n                pred_cam.append(output[\'theta\'][:, :, :3].reshape(batch_size * seqlen, -1))\n                pred_verts.append(output[\'verts\'].reshape(batch_size * seqlen, -1, 3))\n                pred_pose.append(output[\'theta\'][:,:,3:75].reshape(batch_size * seqlen, -1))\n                pred_betas.append(output[\'theta\'][:, :,75:].reshape(batch_size * seqlen, -1))\n                pred_joints3d.append(output[\'kp_3d\'].reshape(batch_size * seqlen, -1, 3))\n\n\n            pred_cam = torch.cat(pred_cam, dim=0)\n            pred_verts = torch.cat(pred_verts, dim=0)\n            pred_pose = torch.cat(pred_pose, dim=0)\n            pred_betas = torch.cat(pred_betas, dim=0)\n            pred_joints3d = torch.cat(pred_joints3d, dim=0)\n\n            del batch\n\n        # ========= [Optional] run Temporal SMPLify to refine the results ========= #\n        if args.run_smplify and args.tracking_method == \'pose\':\n            norm_joints2d = np.concatenate(norm_joints2d, axis=0)\n            norm_joints2d = convert_kps(norm_joints2d, src=\'staf\', dst=\'spin\')\n            norm_joints2d = torch.from_numpy(norm_joints2d).float().to(device)\n\n            # Run Temporal SMPLify\n            update, new_opt_vertices, new_opt_cam, new_opt_pose, new_opt_betas, \\\n            new_opt_joints3d, new_opt_joint_loss, opt_joint_loss = smplify_runner(\n                pred_rotmat=pred_pose,\n                pred_betas=pred_betas,\n                pred_cam=pred_cam,\n                j2d=norm_joints2d,\n                device=device,\n                batch_size=norm_joints2d.shape[0],\n                pose2aa=False,\n            )\n\n            # update the parameters after refinement\n            print(f\'Update ratio after Temporal SMPLify: {update.sum()} / {norm_joints2d.shape[0]}\')\n            pred_verts = pred_verts.cpu()\n            pred_cam = pred_cam.cpu()\n            pred_pose = pred_pose.cpu()\n            pred_betas = pred_betas.cpu()\n            pred_joints3d = pred_joints3d.cpu()\n            pred_verts[update] = new_opt_vertices[update]\n            pred_cam[update] = new_opt_cam[update]\n            pred_pose[update] = new_opt_pose[update]\n            pred_betas[update] = new_opt_betas[update]\n            pred_joints3d[update] = new_opt_joints3d[update]\n\n        elif args.run_smplify and args.tracking_method == \'bbox\':\n            print(\'[WARNING] You need to enable pose tracking to run Temporal SMPLify algorithm!\')\n            print(\'[WARNING] Continuing without running Temporal SMPLify!..\')\n\n        # ========= Save results to a pickle file ========= #\n        pred_cam = pred_cam.cpu().numpy()\n        pred_verts = pred_verts.cpu().numpy()\n        pred_pose = pred_pose.cpu().numpy()\n        pred_betas = pred_betas.cpu().numpy()\n        pred_joints3d = pred_joints3d.cpu().numpy()\n\n        orig_cam = convert_crop_cam_to_orig_img(\n            cam=pred_cam,\n            bbox=bboxes,\n            img_width=orig_width,\n            img_height=orig_height\n        )\n\n        output_dict = {\n            \'pred_cam\': pred_cam,\n            \'orig_cam\': orig_cam,\n            \'verts\': pred_verts,\n            \'pose\': pred_pose,\n            \'betas\': pred_betas,\n            \'joints3d\': pred_joints3d,\n            \'joints2d\': joints2d,\n            \'bboxes\': bboxes,\n            \'frame_ids\': frames,\n        }\n\n        vibe_results[person_id] = output_dict\n\n    del model\n\n    end = time.time()\n    fps = num_frames / (end - vibe_time)\n\n    print(f\'VIBE FPS: {fps:.2f}\')\n    total_time = time.time() - total_time\n    print(f\'Total time spent: {total_time:.2f} seconds (including model loading time).\')\n    print(f\'Total FPS (including model loading time): {num_frames / total_time:.2f}.\')\n\n    print(f\'Saving output results to \\""{os.path.join(output_path, ""vibe_output.pkl"")}\\"".\')\n\n    joblib.dump(vibe_results, os.path.join(output_path, ""vibe_output.pkl""))\n\n    if not args.no_render:\n        # ========= Render results as a single video ========= #\n        renderer = Renderer(resolution=(orig_width, orig_height), orig_img=True, wireframe=args.wireframe)\n\n        output_img_folder = f\'{image_folder}_output\'\n        os.makedirs(output_img_folder, exist_ok=True)\n\n        print(f\'Rendering output video, writing frames to {output_img_folder}\')\n\n        # prepare results for rendering\n        frame_results = prepare_rendering_results(vibe_results, num_frames)\n        mesh_color = {k: colorsys.hsv_to_rgb(np.random.rand(), 0.5, 1.0) for k in vibe_results.keys()}\n\n        image_file_names = sorted([\n            os.path.join(image_folder, x)\n            for x in os.listdir(image_folder)\n            if x.endswith(\'.png\') or x.endswith(\'.jpg\')\n        ])\n\n        for frame_idx in tqdm(range(len(image_file_names))):\n            img_fname = image_file_names[frame_idx]\n            img = cv2.imread(img_fname)\n\n            if args.sideview:\n                side_img = np.zeros_like(img)\n\n            for person_id, person_data in frame_results[frame_idx].items():\n                frame_verts = person_data[\'verts\']\n                frame_cam = person_data[\'cam\']\n\n                mc = mesh_color[person_id]\n\n                mesh_filename = None\n\n                if args.save_obj:\n                    mesh_folder = os.path.join(output_path, \'meshes\', f\'{person_id:04d}\')\n                    os.makedirs(mesh_folder, exist_ok=True)\n                    mesh_filename = os.path.join(mesh_folder, f\'{frame_idx:06d}.obj\')\n\n                img = renderer.render(\n                    img,\n                    frame_verts,\n                    cam=frame_cam,\n                    color=mc,\n                    mesh_filename=mesh_filename,\n                )\n\n                if args.sideview:\n                    side_img = renderer.render(\n                        side_img,\n                        frame_verts,\n                        cam=frame_cam,\n                        color=mc,\n                        angle=270,\n                        axis=[0,1,0],\n                    )\n\n            if args.sideview:\n                img = np.concatenate([img, side_img], axis=1)\n\n            cv2.imwrite(os.path.join(output_img_folder, f\'{frame_idx:06d}.png\'), img)\n\n            if args.display:\n                cv2.imshow(\'Video\', img)\n                if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n                    break\n\n        if args.display:\n            cv2.destroyAllWindows()\n\n        # ========= Save rendered video ========= #\n        vid_name = os.path.basename(video_file)\n        save_name = f\'{vid_name.replace("".mp4"", """")}_vibe_result.mp4\'\n        save_name = os.path.join(output_path, save_name)\n        print(f\'Saving result video to {save_name}\')\n        images_to_video(img_folder=output_img_folder, output_vid_file=save_name)\n        shutil.rmtree(output_img_folder)\n\n    shutil.rmtree(image_folder)\n    print(\'================= END =================\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\'--vid_file\', type=str,\n                        help=\'input video path or youtube link\')\n\n    parser.add_argument(\'--output_folder\', type=str,\n                        help=\'output folder to write results\')\n\n    parser.add_argument(\'--tracking_method\', type=str, default=\'bbox\', choices=[\'bbox\', \'pose\'],\n                        help=\'tracking method to calculate the tracklet of a subject from the input video\')\n\n    parser.add_argument(\'--detector\', type=str, default=\'yolo\', choices=[\'yolo\', \'maskrcnn\'],\n                        help=\'object detector to be used for bbox tracking\')\n\n    parser.add_argument(\'--yolo_img_size\', type=int, default=416,\n                        help=\'input image size for yolo detector\')\n\n    parser.add_argument(\'--tracker_batch_size\', type=int, default=12,\n                        help=\'batch size of object detector used for bbox tracking\')\n\n    parser.add_argument(\'--staf_dir\', type=str, default=\'/home/mkocabas/developments/openposetrack\',\n                        help=\'path to directory STAF pose tracking method installed.\')\n\n    parser.add_argument(\'--vibe_batch_size\', type=int, default=450,\n                        help=\'batch size of VIBE\')\n\n    parser.add_argument(\'--display\', action=\'store_true\',\n                        help=\'visualize the results of each step during demo\')\n\n    parser.add_argument(\'--run_smplify\', action=\'store_true\',\n                        help=\'run smplify for refining the results, you need pose tracking to enable it\')\n\n    parser.add_argument(\'--no_render\', action=\'store_true\',\n                        help=\'disable final rendering of output video.\')\n\n    parser.add_argument(\'--wireframe\', action=\'store_true\',\n                        help=\'render all meshes as wireframes.\')\n\n    parser.add_argument(\'--sideview\', action=\'store_true\',\n                        help=\'render meshes from alternate viewpoint.\')\n\n    parser.add_argument(\'--save_obj\', action=\'store_true\',\n                        help=\'save results as .obj files.\')\n\n    args = parser.parse_args()\n\n    main(args)\n'"
eval.py,2,"b""import os\nimport torch\n\nfrom lib.dataset import ThreeDPW\nfrom lib.models import VIBE\nfrom lib.core.evaluate import Evaluator\nfrom lib.core.config import parse_args\nfrom torch.utils.data import DataLoader\n\n\ndef main(cfg):\n    print('...Evaluating on 3DPW test set...')\n\n    model = VIBE(\n        n_layers=cfg.MODEL.TGRU.NUM_LAYERS,\n        batch_size=cfg.TRAIN.BATCH_SIZE,\n        seqlen=cfg.DATASET.SEQLEN,\n        hidden_size=cfg.MODEL.TGRU.HIDDEN_SIZE,\n        pretrained=cfg.TRAIN.PRETRAINED_REGRESSOR,\n        add_linear=cfg.MODEL.TGRU.ADD_LINEAR,\n        bidirectional=cfg.MODEL.TGRU.BIDIRECTIONAL,\n        use_residual=cfg.MODEL.TGRU.RESIDUAL,\n    ).to(cfg.DEVICE)\n\n    if cfg.TRAIN.PRETRAINED != '' and os.path.isfile(cfg.TRAIN.PRETRAINED):\n        checkpoint = torch.load(cfg.TRAIN.PRETRAINED)\n        best_performance = checkpoint['performance']\n        model.load_state_dict(checkpoint['gen_state_dict'])\n        print(f'==> Loaded pretrained model from {cfg.TRAIN.PRETRAINED}...')\n        print(f'Performance on 3DPW test set {best_performance}')\n    else:\n        print(f'{cfg.TRAIN.PRETRAINED} is not a pretrained model!!!!')\n        exit()\n\n    test_db = ThreeDPW(set='test', seqlen=cfg.DATASET.SEQLEN, debug=cfg.DEBUG)\n\n    test_loader = DataLoader(\n        dataset=test_db,\n        batch_size=cfg.TRAIN.BATCH_SIZE,\n        shuffle=False,\n        num_workers=cfg.NUM_WORKERS,\n    )\n\n    Evaluator(\n        model=model,\n        device=cfg.DEVICE,\n        test_loader=test_loader,\n    ).run()\n\n\nif __name__ == '__main__':\n    cfg, cfg_file = parse_args()\n\n    main(cfg)\n"""
train.py,10,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nos.environ[\'PYOPENGL_PLATFORM\'] = \'egl\'\n\nimport torch\nimport pprint\nimport random\nimport numpy as np\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom lib.core.loss import VIBELoss\nfrom lib.core.trainer import Trainer\nfrom lib.core.config import parse_args\nfrom lib.utils.utils import prepare_output_dir\nfrom lib.models import VIBE, MotionDiscriminator\nfrom lib.dataset.loaders import get_data_loaders\nfrom lib.utils.utils import create_logger, get_optimizer\n\n\ndef main(cfg):\n    if cfg.SEED_VALUE >= 0:\n        print(f\'Seed value for the experiment {cfg.SEED_VALUE}\')\n        os.environ[\'PYTHONHASHSEED\'] = str(cfg.SEED_VALUE)\n        random.seed(cfg.SEED_VALUE)\n        torch.manual_seed(cfg.SEED_VALUE)\n        np.random.seed(cfg.SEED_VALUE)\n\n    logger = create_logger(cfg.LOGDIR, phase=\'train\')\n\n    logger.info(f\'GPU name -> {torch.cuda.get_device_name()}\')\n    logger.info(f\'GPU feat -> {torch.cuda.get_device_properties(""cuda"")}\')\n\n    logger.info(pprint.pformat(cfg))\n\n    # cudnn related setting\n    cudnn.benchmark = cfg.CUDNN.BENCHMARK\n    torch.backends.cudnn.deterministic = cfg.CUDNN.DETERMINISTIC\n    torch.backends.cudnn.enabled = cfg.CUDNN.ENABLED\n\n    writer = SummaryWriter(log_dir=cfg.LOGDIR)\n    writer.add_text(\'config\', pprint.pformat(cfg), 0)\n\n    # ========= Dataloaders ========= #\n    data_loaders = get_data_loaders(cfg)\n\n    # ========= Compile Loss ========= #\n    loss = VIBELoss(\n        e_loss_weight=cfg.LOSS.KP_2D_W,\n        e_3d_loss_weight=cfg.LOSS.KP_3D_W,\n        e_pose_loss_weight=cfg.LOSS.POSE_W,\n        e_shape_loss_weight=cfg.LOSS.SHAPE_W,\n        d_motion_loss_weight=cfg.LOSS.D_MOTION_LOSS_W,\n    )\n\n    # ========= Initialize networks, optimizers and lr_schedulers ========= #\n    generator = VIBE(\n        n_layers=cfg.MODEL.TGRU.NUM_LAYERS,\n        batch_size=cfg.TRAIN.BATCH_SIZE,\n        seqlen=cfg.DATASET.SEQLEN,\n        hidden_size=cfg.MODEL.TGRU.HIDDEN_SIZE,\n        pretrained=cfg.TRAIN.PRETRAINED_REGRESSOR,\n        add_linear=cfg.MODEL.TGRU.ADD_LINEAR,\n        bidirectional=cfg.MODEL.TGRU.BIDIRECTIONAL,\n        use_residual=cfg.MODEL.TGRU.RESIDUAL,\n    ).to(cfg.DEVICE)\n\n    if cfg.TRAIN.PRETRAINED != \'\' and os.path.isfile(cfg.TRAIN.PRETRAINED):\n        checkpoint = torch.load(cfg.TRAIN.PRETRAINED)\n        best_performance = checkpoint[\'performance\']\n        generator.load_state_dict(checkpoint[\'gen_state_dict\'])\n        print(f\'==> Loaded pretrained model from {cfg.TRAIN.PRETRAINED}...\')\n        print(f\'Performance on 3DPW test set {best_performance}\')\n    else:\n        print(f\'{cfg.TRAIN.PRETRAINED} is not a pretrained model!!!!\')\n\n    gen_optimizer = get_optimizer(\n        model=generator,\n        optim_type=cfg.TRAIN.GEN_OPTIM,\n        lr=cfg.TRAIN.GEN_LR,\n        weight_decay=cfg.TRAIN.GEN_WD,\n        momentum=cfg.TRAIN.GEN_MOMENTUM,\n    )\n\n    motion_discriminator = MotionDiscriminator(\n        rnn_size=cfg.TRAIN.MOT_DISCR.HIDDEN_SIZE,\n        input_size=69,\n        num_layers=cfg.TRAIN.MOT_DISCR.NUM_LAYERS,\n        output_size=1,\n        feature_pool=cfg.TRAIN.MOT_DISCR.FEATURE_POOL,\n        attention_size=None if cfg.TRAIN.MOT_DISCR.FEATURE_POOL !=\'attention\' else cfg.TRAIN.MOT_DISCR.ATT.SIZE,\n        attention_layers=None if cfg.TRAIN.MOT_DISCR.FEATURE_POOL !=\'attention\' else cfg.TRAIN.MOT_DISCR.ATT.LAYERS,\n        attention_dropout=None if cfg.TRAIN.MOT_DISCR.FEATURE_POOL !=\'attention\' else cfg.TRAIN.MOT_DISCR.ATT.DROPOUT\n    ).to(cfg.DEVICE)\n\n    dis_motion_optimizer = get_optimizer(\n        model=motion_discriminator,\n        optim_type=cfg.TRAIN.MOT_DISCR.OPTIM,\n        lr=cfg.TRAIN.MOT_DISCR.LR,\n        weight_decay=cfg.TRAIN.MOT_DISCR.WD,\n        momentum=cfg.TRAIN.MOT_DISCR.MOMENTUM\n    )\n\n    motion_lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        dis_motion_optimizer,\n        mode=\'min\',\n        factor=0.1,\n        patience=cfg.TRAIN.LR_PATIENCE,\n        verbose=True,\n    )\n\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        gen_optimizer,\n        mode=\'min\',\n        factor=0.1,\n        patience=cfg.TRAIN.LR_PATIENCE,\n        verbose=True,\n    )\n\n    # ========= Start Training ========= #\n    Trainer(\n        data_loaders=data_loaders,\n        generator=generator,\n        motion_discriminator=motion_discriminator,\n        criterion=loss,\n        dis_motion_optimizer=dis_motion_optimizer,\n        dis_motion_update_steps=cfg.TRAIN.MOT_DISCR.UPDATE_STEPS,\n        gen_optimizer=gen_optimizer,\n        start_epoch=cfg.TRAIN.START_EPOCH,\n        end_epoch=cfg.TRAIN.END_EPOCH,\n        device=cfg.DEVICE,\n        writer=writer,\n        debug=cfg.DEBUG,\n        logdir=cfg.LOGDIR,\n        lr_scheduler=lr_scheduler,\n        motion_lr_scheduler=motion_lr_scheduler,\n        resume=cfg.TRAIN.RESUME,\n        num_iters_per_epoch=cfg.TRAIN.NUM_ITERS_PER_EPOCH,\n        debug_freq=cfg.DEBUG_FREQ,\n    ).fit()\n\n\nif __name__ == \'__main__\':\n    cfg, cfg_file = parse_args()\n    cfg = prepare_output_dir(cfg, cfg_file)\n\n    main(cfg)\n'"
tests/test_2d_datasets.py,2,"b""import sys\nsys.path.append('.')\n\nimport torch\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\n\nfrom lib.dataset import *\nfrom lib.utils.vis import batch_draw_skeleton, batch_visualize_preds\n\n\ndef debug_2d_data(dataset, DEBUG=True):\n    is_train = True\n    seqlen = 32\n    batch_size = 1\n    db = eval(dataset)(seqlen=seqlen, debug=DEBUG)\n\n    dataloader = DataLoader(\n        dataset=db,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=1,\n    )\n\n    for i, target in enumerate(dataloader):\n        for k, v in target.items():\n            print(k, v.shape)\n\n        if DEBUG:\n            if dataset is 'Insta':\n                input = torch.ones(batch_size, seqlen, 3, 224, 224)[0]\n            else:\n                input = target['video'][0]\n            single_target = {k: v[0] for k, v in target.items()}\n\n            dataset_name = 'spin'\n            plt.figure(figsize=(19.2,10.8))\n            images = batch_draw_skeleton(input, single_target, dataset=dataset_name, max_images=4)\n            plt.imshow(images)\n            plt.show()\n\n        if i == 20:\n            break\n\n\nif __name__ == '__main__':\n    debug_2d_data('Insta', DEBUG=True)\n"""
tests/test_3d_datasets.py,1,"b""import sys\nsys.path.append('.')\nimport time\nfrom lib.dataset import *\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nfrom lib.models.smpl import SMPL, SMPL_MODEL_DIR\nfrom lib.utils.vis import batch_draw_skeleton, batch_visualize_preds\n\ndataset = 'MPII3D'\nseqlen = 16\nDEBUG = True\n\ndb = eval(dataset)(set='val', seqlen=seqlen, debug=DEBUG)\n\ndataloader = DataLoader(\n    dataset=db,\n    batch_size=4,\n    shuffle=True,\n    num_workers=1,\n)\n\nsmpl = SMPL(SMPL_MODEL_DIR)\n\nstart = time.time()\nfor i, target in enumerate(dataloader):\n    data_time = time.time() - start\n    start = time.time()\n    print(f'Data loading time {data_time:.4f}')\n\n    for k, v in target.items():\n        print(k, v.shape)\n\n    if DEBUG:\n        input = target['video'][0]\n        single_target = {k: v[0] for k, v in target.items()}\n\n        if dataset == 'MPII3D':\n            images = batch_draw_skeleton(input, single_target, dataset='spin', max_images=4)\n            plt.imshow(images)\n            plt.show()\n        else:\n            theta = single_target['theta']\n            pose, shape = theta[:, 3:75], theta[:, 75:]\n\n            # verts, j3d, smpl_j3d = smpl(pose, shape)\n\n            pred_output = smpl(betas=shape, body_pose=pose[:, 3:], global_orient=pose[:, :3], pose2rot=True)\n\n            single_target['verts'] = pred_output.vertices\n\n            images = batch_visualize_preds(input, single_target, single_target, max_images=4, dataset='spin')\n            # images = batch_draw_skeleton(input, single_target, dataset='common', max_images=10)\n            plt.imshow(images)\n            plt.show()\n\n    if i == 100:\n        break"""
lib/core/__init__.py,0,b''
lib/core/config.py,0,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport argparse\nfrom yacs.config import CfgNode as CN\n\n# CONSTANTS\n# You may modify them at will\nVIBE_DB_DIR = \'data/vibe_db\'\nAMASS_DIR = \'data/amass\'\nINSTA_DIR = \'data/insta_variety\'\nMPII3D_DIR = \'data/mpi_inf_3dhp\'\nTHREEDPW_DIR = \'data/3dpw\'\nPENNACTION_DIR = \'data/penn_action\'\nPOSETRACK_DIR = \'data/posetrack\'\nVIBE_DATA_DIR = \'data/vibe_data\'\n\n# Configuration variables\ncfg = CN()\n\ncfg.OUTPUT_DIR = \'results\'\ncfg.EXP_NAME = \'default\'\ncfg.DEVICE = \'cuda\'\ncfg.DEBUG = True\ncfg.LOGDIR = \'\'\ncfg.NUM_WORKERS = 8\ncfg.DEBUG_FREQ = 1000\ncfg.SEED_VALUE = -1\n\ncfg.CUDNN = CN()\ncfg.CUDNN.BENCHMARK = True\ncfg.CUDNN.DETERMINISTIC = False\ncfg.CUDNN.ENABLED = True\n\ncfg.TRAIN = CN()\ncfg.TRAIN.DATASETS_2D = [\'Insta\']\ncfg.TRAIN.DATASETS_3D = [\'MPII3D\']\ncfg.TRAIN.DATASET_EVAL = \'ThreeDPW\'\ncfg.TRAIN.BATCH_SIZE = 32\ncfg.TRAIN.DATA_2D_RATIO = 0.5\ncfg.TRAIN.START_EPOCH = 0\ncfg.TRAIN.END_EPOCH = 5\ncfg.TRAIN.PRETRAINED_REGRESSOR = \'\'\ncfg.TRAIN.PRETRAINED = \'\'\ncfg.TRAIN.RESUME = \'\'\ncfg.TRAIN.NUM_ITERS_PER_EPOCH = 1000\ncfg.TRAIN.LR_PATIENCE = 5\n\n# <====== generator optimizer\ncfg.TRAIN.GEN_OPTIM = \'Adam\'\ncfg.TRAIN.GEN_LR = 1e-4\ncfg.TRAIN.GEN_WD = 1e-4\ncfg.TRAIN.GEN_MOMENTUM = 0.9\n\n# <====== motion discriminator optimizer\ncfg.TRAIN.MOT_DISCR = CN()\ncfg.TRAIN.MOT_DISCR.OPTIM = \'SGD\'\ncfg.TRAIN.MOT_DISCR.LR = 1e-2\ncfg.TRAIN.MOT_DISCR.WD = 1e-4\ncfg.TRAIN.MOT_DISCR.MOMENTUM = 0.9\ncfg.TRAIN.MOT_DISCR.UPDATE_STEPS = 1\ncfg.TRAIN.MOT_DISCR.FEATURE_POOL = \'concat\'\ncfg.TRAIN.MOT_DISCR.HIDDEN_SIZE = 1024\ncfg.TRAIN.MOT_DISCR.NUM_LAYERS = 1\ncfg.TRAIN.MOT_DISCR.ATT = CN()\ncfg.TRAIN.MOT_DISCR.ATT.SIZE = 1024\ncfg.TRAIN.MOT_DISCR.ATT.LAYERS = 1\ncfg.TRAIN.MOT_DISCR.ATT.DROPOUT = 0.1\n\ncfg.DATASET = CN()\ncfg.DATASET.SEQLEN = 20\ncfg.DATASET.OVERLAP = 0.5\n\ncfg.LOSS = CN()\ncfg.LOSS.KP_2D_W = 60.\ncfg.LOSS.KP_3D_W = 30.\ncfg.LOSS.SHAPE_W = 0.001\ncfg.LOSS.POSE_W = 1.0\ncfg.LOSS.D_MOTION_LOSS_W = 1.\n\ncfg.MODEL = CN()\n\ncfg.MODEL.TEMPORAL_TYPE = \'gru\'\n\n# GRU model hyperparams\ncfg.MODEL.TGRU = CN()\ncfg.MODEL.TGRU.NUM_LAYERS = 1\ncfg.MODEL.TGRU.ADD_LINEAR = False\ncfg.MODEL.TGRU.RESIDUAL = False\ncfg.MODEL.TGRU.HIDDEN_SIZE = 2048\ncfg.MODEL.TGRU.BIDIRECTIONAL = False\n\n\ndef get_cfg_defaults():\n    """"""Get a yacs CfgNode object with default values for my_project.""""""\n    # Return a clone so that the defaults will not be altered\n    # This is for the ""local variable"" use pattern\n    return cfg.clone()\n\n\ndef update_cfg(cfg_file):\n    cfg = get_cfg_defaults()\n    cfg.merge_from_file(cfg_file)\n    return cfg.clone()\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--cfg\', type=str, help=\'cfg file path\')\n\n    args = parser.parse_args()\n    print(args, end=\'\\n\\n\')\n\n    cfg_file = args.cfg\n    if args.cfg is not None:\n        cfg = update_cfg(args.cfg)\n    else:\n        cfg = get_cfg_defaults()\n\n    return cfg, cfg_file\n'"
lib/core/evaluate.py,7,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport time\nimport torch\nimport shutil\nimport logging\nimport numpy as np\nimport os.path as osp\nfrom progress.bar import Bar\n\nfrom lib.core.config import VIBE_DATA_DIR\nfrom lib.utils.utils import move_dict_to_device, AverageMeter\n\nfrom lib.utils.eval_utils import (\n    compute_accel,\n    compute_error_accel,\n    compute_error_verts,\n    batch_compute_similarity_transform_torch,\n)\n\nlogger = logging.getLogger(__name__)\n\nclass Evaluator():\n    def __init__(\n            self,\n            test_loader,\n            model,\n            device=None,\n    ):\n        self.test_loader = test_loader\n        self.model = model\n        self.device = device\n\n        self.evaluation_accumulators = dict.fromkeys(['pred_j3d', 'target_j3d', 'target_theta', 'pred_verts'])\n\n        if self.device is None:\n            self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n    def validate(self):\n        self.model.eval()\n\n        start = time.time()\n\n        summary_string = ''\n\n        bar = Bar('Validation', fill='#', max=len(self.test_loader))\n\n        if self.evaluation_accumulators is not None:\n            for k,v in self.evaluation_accumulators.items():\n                self.evaluation_accumulators[k] = []\n\n        J_regressor = torch.from_numpy(np.load(osp.join(VIBE_DATA_DIR, 'J_regressor_h36m.npy'))).float()\n\n        for i, target in enumerate(self.test_loader):\n\n            # video = video.to(self.device)\n            move_dict_to_device(target, self.device)\n\n            # <=============\n            with torch.no_grad():\n                inp = target['features']\n\n                preds = self.model(inp, J_regressor=J_regressor)\n\n                # convert to 14 keypoint format for evaluation\n                # if self.use_spin:\n                n_kp = preds[-1]['kp_3d'].shape[-2]\n                pred_j3d = preds[-1]['kp_3d'].view(-1, n_kp, 3).cpu().numpy()\n                target_j3d = target['kp_3d'].view(-1, n_kp, 3).cpu().numpy()\n                pred_verts = preds[-1]['verts'].view(-1, 6890, 3).cpu().numpy()\n                target_theta = target['theta'].view(-1, 85).cpu().numpy()\n\n\n                self.evaluation_accumulators['pred_verts'].append(pred_verts)\n                self.evaluation_accumulators['target_theta'].append(target_theta)\n\n                self.evaluation_accumulators['pred_j3d'].append(pred_j3d)\n                self.evaluation_accumulators['target_j3d'].append(target_j3d)\n            # =============>\n\n            batch_time = time.time() - start\n\n            summary_string = f'({i + 1}/{len(self.test_loader)}) | batch: {batch_time * 10.0:.4}ms | ' \\\n                             f'Total: {bar.elapsed_td} | ETA: {bar.eta_td:}'\n\n            bar.suffix = summary_string\n            bar.next()\n\n        bar.finish()\n\n        logger.info(summary_string)\n\n    def evaluate(self):\n\n        for k, v in self.evaluation_accumulators.items():\n            self.evaluation_accumulators[k] = np.vstack(v)\n\n        pred_j3ds = self.evaluation_accumulators['pred_j3d']\n        target_j3ds = self.evaluation_accumulators['target_j3d']\n\n        pred_j3ds = torch.from_numpy(pred_j3ds).float()\n        target_j3ds = torch.from_numpy(target_j3ds).float()\n\n        print(f'Evaluating on {pred_j3ds.shape[0]} number of poses...')\n        pred_pelvis = (pred_j3ds[:,[2],:] + pred_j3ds[:,[3],:]) / 2.0\n        target_pelvis = (target_j3ds[:,[2],:] + target_j3ds[:,[3],:]) / 2.0\n\n\n        pred_j3ds -= pred_pelvis\n        target_j3ds -= target_pelvis\n\n        # Absolute error (MPJPE)\n        errors = torch.sqrt(((pred_j3ds - target_j3ds) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n        S1_hat = batch_compute_similarity_transform_torch(pred_j3ds, target_j3ds)\n        errors_pa = torch.sqrt(((S1_hat - target_j3ds) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n        pred_verts = self.evaluation_accumulators['pred_verts']\n        target_theta = self.evaluation_accumulators['target_theta']\n\n        m2mm = 1000\n\n        pve = np.mean(compute_error_verts(target_theta=target_theta, pred_verts=pred_verts)) * m2mm\n        accel = np.mean(compute_accel(pred_j3ds)) * m2mm\n        accel_err = np.mean(compute_error_accel(joints_pred=pred_j3ds, joints_gt=target_j3ds)) * m2mm\n        mpjpe = np.mean(errors) * m2mm\n        pa_mpjpe = np.mean(errors_pa) * m2mm\n\n        eval_dict = {\n            'mpjpe': mpjpe,\n            'pa-mpjpe': pa_mpjpe,\n            'pve': pve,\n            'accel': accel,\n            'accel_err': accel_err\n        }\n\n        log_str = ' '.join([f'{k.upper()}: {v:.4f},'for k,v in eval_dict.items()])\n        print(log_str)\n\n    def run(self):\n        self.validate()\n        self.evaluate()"""
lib/core/loss.py,13,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport torch\nimport torch.nn as nn\n\nfrom lib.utils.geometry import batch_rodrigues\n\nclass VIBELoss(nn.Module):\n    def __init__(\n            self,\n            e_loss_weight=60.,\n            e_3d_loss_weight=30.,\n            e_pose_loss_weight=1.,\n            e_shape_loss_weight=0.001,\n            d_motion_loss_weight=1.,\n            device=\'cuda\',\n    ):\n        super(VIBELoss, self).__init__()\n        self.e_loss_weight = e_loss_weight\n        self.e_3d_loss_weight = e_3d_loss_weight\n        self.e_pose_loss_weight = e_pose_loss_weight\n        self.e_shape_loss_weight = e_shape_loss_weight\n        self.d_motion_loss_weight = d_motion_loss_weight\n\n        self.device = device\n        self.criterion_shape = nn.L1Loss().to(self.device)\n        self.criterion_keypoints = nn.MSELoss(reduction=\'none\').to(self.device)\n        self.criterion_regr = nn.MSELoss().to(self.device)\n\n        self.enc_loss = batch_encoder_disc_l2_loss\n        self.dec_loss = batch_adv_disc_l2_loss\n\n    def forward(\n            self,\n            generator_outputs,\n            data_2d,\n            data_3d,\n            data_body_mosh=None,\n            data_motion_mosh=None,\n            body_discriminator=None,\n            motion_discriminator=None,\n    ):\n        # to reduce time dimension\n        reduce = lambda x: x.reshape((x.shape[0] * x.shape[1],) + x.shape[2:])\n        # flatten for weight vectors\n        flatten = lambda x: x.reshape(-1)\n        # accumulate all predicted thetas from IEF\n        accumulate_thetas = lambda x: torch.cat([output[\'theta\'] for output in x],0)\n\n        if data_2d:\n            sample_2d_count = data_2d[\'kp_2d\'].shape[0]\n            real_2d = torch.cat((data_2d[\'kp_2d\'], data_3d[\'kp_2d\']), 0)\n        else:\n            sample_2d_count = 0\n            real_2d = data_3d[\'kp_2d\']\n\n        real_2d = reduce(real_2d)\n\n        real_3d = reduce(data_3d[\'kp_3d\'])\n        data_3d_theta = reduce(data_3d[\'theta\'])\n\n        w_3d = data_3d[\'w_3d\'].type(torch.bool)\n        w_smpl = data_3d[\'w_smpl\'].type(torch.bool)\n\n        total_predict_thetas = accumulate_thetas(generator_outputs)\n\n        preds = generator_outputs[-1]\n\n        pred_j3d = preds[\'kp_3d\'][sample_2d_count:]\n        pred_theta = preds[\'theta\'][sample_2d_count:]\n\n        theta_size = pred_theta.shape[:2]\n\n        pred_theta = reduce(pred_theta)\n        pred_j2d = reduce(preds[\'kp_2d\'])\n        pred_j3d = reduce(pred_j3d)\n\n        w_3d = flatten(w_3d)\n        w_smpl = flatten(w_smpl)\n\n        pred_theta = pred_theta[w_smpl]\n        pred_j3d = pred_j3d[w_3d]\n        data_3d_theta = data_3d_theta[w_smpl]\n        real_3d = real_3d[w_3d]\n\n        # <======== Generator Loss\n        loss_kp_2d =  self.keypoint_loss(pred_j2d, real_2d, openpose_weight=1., gt_weight=1.) * self.e_loss_weight\n\n        loss_kp_3d = self.keypoint_3d_loss(pred_j3d, real_3d)\n        loss_kp_3d = loss_kp_3d * self.e_3d_loss_weight\n\n        real_shape, pred_shape = data_3d_theta[:, 75:], pred_theta[:, 75:]\n        real_pose, pred_pose = data_3d_theta[:, 3:75], pred_theta[:, 3:75]\n\n        loss_dict = {\n            \'loss_kp_2d\': loss_kp_2d,\n            \'loss_kp_3d\': loss_kp_3d,\n        }\n        if pred_theta.shape[0] > 0:\n            loss_pose, loss_shape = self.smpl_losses(pred_pose, pred_shape, real_pose, real_shape)\n            loss_shape = loss_shape * self.e_shape_loss_weight\n            loss_pose = loss_pose * self.e_pose_loss_weight\n            loss_dict[\'loss_shape\'] = loss_shape\n            loss_dict[\'loss_pose\'] = loss_pose\n\n        gen_loss = torch.stack(list(loss_dict.values())).sum()\n\n        # <======== Motion Discriminator Loss\n        end_idx = 75\n        start_idx = 6\n        pred_motion = total_predict_thetas\n        e_motion_disc_loss = self.enc_loss(motion_discriminator(pred_motion[:, :, start_idx:end_idx]))\n        e_motion_disc_loss = e_motion_disc_loss * self.d_motion_loss_weight\n\n        fake_motion = pred_motion.detach()\n        real_motion = data_motion_mosh[\'theta\']\n        fake_disc_value = motion_discriminator(fake_motion[:, :, start_idx:end_idx])\n        real_disc_value = motion_discriminator(real_motion[:, :, start_idx:end_idx])\n        d_motion_disc_real, d_motion_disc_fake, d_motion_disc_loss = self.dec_loss(real_disc_value, fake_disc_value)\n\n        d_motion_disc_real = d_motion_disc_real * self.d_motion_loss_weight\n        d_motion_disc_fake = d_motion_disc_fake * self.d_motion_loss_weight\n        d_motion_disc_loss = d_motion_disc_loss * self.d_motion_loss_weight\n\n        loss_dict[\'e_m_disc_loss\'] = e_motion_disc_loss\n        loss_dict[\'d_m_disc_real\'] = d_motion_disc_real\n        loss_dict[\'d_m_disc_fake\'] = d_motion_disc_fake\n        loss_dict[\'d_m_disc_loss\'] = d_motion_disc_loss\n\n        gen_loss = gen_loss + e_motion_disc_loss\n        motion_dis_loss = d_motion_disc_loss\n\n        return gen_loss, motion_dis_loss, loss_dict\n\n    def keypoint_loss(self, pred_keypoints_2d, gt_keypoints_2d, openpose_weight, gt_weight):\n        """"""\n        Compute 2D reprojection loss on the keypoints.\n        The loss is weighted by the confidence.\n        The available keypoints are different for each dataset.\n        """"""\n        conf = gt_keypoints_2d[:, :, -1].unsqueeze(-1).clone()\n        conf[:, :25] *= openpose_weight\n        conf[:, 25:] *= gt_weight\n        loss = (conf * self.criterion_keypoints(pred_keypoints_2d, gt_keypoints_2d[:, :, :-1])).mean()\n        return loss\n\n    def keypoint_3d_loss(self, pred_keypoints_3d, gt_keypoints_3d):\n        """"""\n        Compute 3D keypoint loss for the examples that 3D keypoint annotations are available.\n        The loss is weighted by the confidence.\n        """"""\n        pred_keypoints_3d = pred_keypoints_3d[:, 25:39, :]\n        gt_keypoints_3d = gt_keypoints_3d[:, 25:39, :]\n\n        # conf = gt_keypoints_3d[:, :, -1].unsqueeze(-1).clone()\n        # gt_keypoints_3d = gt_keypoints_3d[:, :, :-1].clone()\n        # gt_keypoints_3d = gt_keypoints_3d\n        # conf = conf\n        pred_keypoints_3d = pred_keypoints_3d\n        if len(gt_keypoints_3d) > 0:\n            gt_pelvis = (gt_keypoints_3d[:, 2,:] + gt_keypoints_3d[:, 3,:]) / 2\n            gt_keypoints_3d = gt_keypoints_3d - gt_pelvis[:, None, :]\n            pred_pelvis = (pred_keypoints_3d[:, 2,:] + pred_keypoints_3d[:, 3,:]) / 2\n            pred_keypoints_3d = pred_keypoints_3d - pred_pelvis[:, None, :]\n            # print(conf.shape, pred_keypoints_3d.shape, gt_keypoints_3d.shape)\n            # return (conf * self.criterion_keypoints(pred_keypoints_3d, gt_keypoints_3d)).mean()\n            return self.criterion_keypoints(pred_keypoints_3d, gt_keypoints_3d).mean()\n        else:\n            return torch.FloatTensor(1).fill_(0.).to(self.device)\n\n    def smpl_losses(self, pred_rotmat, pred_betas, gt_pose, gt_betas):\n        pred_rotmat_valid = batch_rodrigues(pred_rotmat.reshape(-1,3)).reshape(-1, 24, 3, 3)\n        gt_rotmat_valid = batch_rodrigues(gt_pose.reshape(-1,3)).reshape(-1, 24, 3, 3)\n        pred_betas_valid = pred_betas\n        gt_betas_valid = gt_betas\n        if len(pred_rotmat_valid) > 0:\n            loss_regr_pose = self.criterion_regr(pred_rotmat_valid, gt_rotmat_valid)\n            loss_regr_betas = self.criterion_regr(pred_betas_valid, gt_betas_valid)\n        else:\n            loss_regr_pose = torch.FloatTensor(1).fill_(0.).to(self.device)\n            loss_regr_betas = torch.FloatTensor(1).fill_(0.).to(self.device)\n        return loss_regr_pose, loss_regr_betas\n\n\ndef batch_encoder_disc_l2_loss(disc_value):\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n    k = disc_value.shape[0]\n    return torch.sum((disc_value - 1.0) ** 2) * 1.0 / k\n\n\ndef batch_adv_disc_l2_loss(real_disc_value, fake_disc_value):\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n    ka = real_disc_value.shape[0]\n    kb = fake_disc_value.shape[0]\n    lb, la = torch.sum(fake_disc_value ** 2) / kb, torch.sum((real_disc_value - 1) ** 2) / ka\n    return la, lb, la + lb\n\n\ndef batch_encoder_disc_wasserstein_loss(disc_value):\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n    k = disc_value.shape[0]\n    return -1 * disc_value.sum() / k\n\n\ndef batch_adv_disc_wasserstein_loss(real_disc_value, fake_disc_value):\n    \'\'\'\n        Inputs:\n            disc_value: N x 25\n    \'\'\'\n\n    ka = real_disc_value.shape[0]\n    kb = fake_disc_value.shape[0]\n\n    la = -1 * real_disc_value.sum() / ka\n    lb = fake_disc_value.sum() / kb\n    return la, lb, la + lb\n\n\ndef batch_smooth_pose_loss(pred_theta):\n    pose = pred_theta[:,:,3:75]\n    pose_diff = pose[:,1:,:] - pose[:,:-1,:]\n    return torch.mean(pose_diff).abs()\n\n\ndef batch_smooth_shape_loss(pred_theta):\n    shape = pred_theta[:, :, 75:]\n    shape_diff = shape[:, 1:, :] - shape[:, :-1, :]\n    return torch.mean(shape_diff).abs()\n'"
lib/core/trainer.py,12,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport time\nimport torch\nimport shutil\nimport logging\nimport numpy as np\nimport os.path as osp\nfrom progress.bar import Bar\n\nfrom lib.core.config import VIBE_DATA_DIR\nfrom lib.utils.utils import move_dict_to_device, AverageMeter\n\nfrom lib.utils.eval_utils import (\n    compute_accel,\n    compute_error_accel,\n    compute_error_verts,\n    batch_compute_similarity_transform_torch,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass Trainer():\n    def __init__(\n            self,\n            data_loaders,\n            generator,\n            motion_discriminator,\n            gen_optimizer,\n            dis_motion_optimizer,\n            dis_motion_update_steps,\n            end_epoch,\n            criterion,\n            start_epoch=0,\n            lr_scheduler=None,\n            motion_lr_scheduler=None,\n            device=None,\n            writer=None,\n            debug=False,\n            debug_freq=1000,\n            logdir=\'output\',\n            resume=None,\n            performance_type=\'min\',\n            num_iters_per_epoch=1000,\n    ):\n\n        # Prepare dataloaders\n        self.train_2d_loader, self.train_3d_loader, self.disc_motion_loader, self.valid_loader = data_loaders\n\n        self.disc_motion_iter = iter(self.disc_motion_loader)\n\n        self.train_2d_iter = self.train_3d_iter = None\n\n        if self.train_2d_loader:\n            self.train_2d_iter = iter(self.train_2d_loader)\n\n        if self.train_3d_loader:\n            self.train_3d_iter = iter(self.train_3d_loader)\n\n        # Models and optimizers\n        self.generator = generator\n        self.gen_optimizer = gen_optimizer\n\n        self.motion_discriminator = motion_discriminator\n        self.dis_motion_optimizer = dis_motion_optimizer\n\n        # Training parameters\n        self.start_epoch = start_epoch\n        self.end_epoch = end_epoch\n        self.criterion = criterion\n        self.lr_scheduler = lr_scheduler\n        self.motion_lr_scheduler = motion_lr_scheduler\n        self.device = device\n        self.writer = writer\n        self.debug = debug\n        self.debug_freq = debug_freq\n        self.logdir = logdir\n\n        self.dis_motion_update_steps = dis_motion_update_steps\n\n        self.performance_type = performance_type\n        self.train_global_step = 0\n        self.valid_global_step = 0\n        self.epoch = 0\n        self.best_performance = float(\'inf\') if performance_type == \'min\' else -float(\'inf\')\n\n        self.evaluation_accumulators = dict.fromkeys([\'pred_j3d\', \'target_j3d\', \'target_theta\', \'pred_verts\'])\n\n        self.num_iters_per_epoch = num_iters_per_epoch\n\n        if self.writer is None:\n            from torch.utils.tensorboard import SummaryWriter\n            self.writer = SummaryWriter(log_dir=self.logdir)\n\n        if self.device is None:\n            self.device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n\n        # Resume from a pretrained model\n        if resume is not None:\n            self.resume_pretrained(resume)\n\n    def train(self):\n        # Single epoch training routine\n\n        losses = AverageMeter()\n\n        timer = {\n            \'data\': 0,\n            \'forward\': 0,\n            \'loss\': 0,\n            \'backward\': 0,\n            \'batch\': 0,\n        }\n\n        self.generator.train()\n        self.motion_discriminator.train()\n\n        start = time.time()\n\n        summary_string = \'\'\n\n        bar = Bar(f\'Epoch {self.epoch + 1}/{self.end_epoch}\', fill=\'#\', max=self.num_iters_per_epoch)\n\n        for i in range(self.num_iters_per_epoch):\n            # Dirty solution to reset an iterator\n            target_2d = target_3d = None\n            if self.train_2d_iter:\n                try:\n                    target_2d = next(self.train_2d_iter)\n                except StopIteration:\n                    self.train_2d_iter = iter(self.train_2d_loader)\n                    target_2d = next(self.train_2d_iter)\n\n                move_dict_to_device(target_2d, self.device)\n\n            if self.train_3d_iter:\n                try:\n                    target_3d = next(self.train_3d_iter)\n                except StopIteration:\n                    self.train_3d_iter = iter(self.train_3d_loader)\n                    target_3d = next(self.train_3d_iter)\n\n                move_dict_to_device(target_3d, self.device)\n\n            real_body_samples = real_motion_samples = None\n\n            try:\n                real_motion_samples = next(self.disc_motion_iter)\n            except StopIteration:\n                self.disc_motion_iter = iter(self.disc_motion_loader)\n                real_motion_samples = next(self.disc_motion_iter)\n\n            move_dict_to_device(real_motion_samples, self.device)\n\n            # <======= Feedforward generator and discriminator\n            if target_2d and target_3d:\n                inp = torch.cat((target_2d[\'features\'], target_3d[\'features\']), dim=0).to(self.device)\n            elif target_3d:\n                inp = target_3d[\'features\'].to(self.device)\n            else:\n                inp = target_2d[\'features\'].to(self.device)\n\n            timer[\'data\'] = time.time() - start\n            start = time.time()\n\n            preds = self.generator(inp)\n\n            timer[\'forward\'] = time.time() - start\n            start = time.time()\n\n            gen_loss, motion_dis_loss, loss_dict = self.criterion(\n                generator_outputs=preds,\n                data_2d=target_2d,\n                data_3d=target_3d,\n                data_body_mosh=real_body_samples,\n                data_motion_mosh=real_motion_samples,\n                motion_discriminator=self.motion_discriminator,\n            )\n            # =======>\n\n            timer[\'loss\'] = time.time() - start\n            start = time.time()\n\n            # <======= Backprop generator and discriminator\n            self.gen_optimizer.zero_grad()\n            gen_loss.backward()\n            self.gen_optimizer.step()\n\n            if self.train_global_step % self.dis_motion_update_steps == 0:\n                self.dis_motion_optimizer.zero_grad()\n                motion_dis_loss.backward()\n                self.dis_motion_optimizer.step()\n            # =======>\n\n            # <======= Log training info\n            total_loss = gen_loss + motion_dis_loss\n\n            losses.update(total_loss.item(), inp.size(0))\n\n            timer[\'backward\'] = time.time() - start\n            timer[\'batch\'] = timer[\'data\'] + timer[\'forward\'] + timer[\'loss\'] + timer[\'backward\']\n            start = time.time()\n\n            summary_string = f\'({i + 1}/{self.num_iters_per_epoch}) | Total: {bar.elapsed_td} | \' \\\n                             f\'ETA: {bar.eta_td:} | loss: {losses.avg:.4f}\'\n\n            for k, v in loss_dict.items():\n                summary_string += f\' | {k}: {v:.2f}\'\n                self.writer.add_scalar(\'train_loss/\'+k, v, global_step=self.train_global_step)\n\n            for k,v in timer.items():\n                summary_string += f\' | {k}: {v:.2f}\'\n\n            self.writer.add_scalar(\'train_loss/loss\', total_loss.item(), global_step=self.train_global_step)\n\n            if self.debug:\n                print(\'==== Visualize ====\')\n                from lib.utils.vis import batch_visualize_vid_preds\n                video = target_3d[\'video\']\n                dataset = \'spin\'\n                vid_tensor = batch_visualize_vid_preds(video, preds[-1], target_3d.copy(),\n                                                       vis_hmr=False, dataset=dataset)\n                self.writer.add_video(\'train-video\', vid_tensor, global_step=self.train_global_step, fps=10)\n\n            self.train_global_step += 1\n            bar.suffix = summary_string\n            bar.next()\n\n            if torch.isnan(total_loss):\n                exit(\'Nan value in loss, exiting!...\')\n            # =======>\n\n        bar.finish()\n\n        logger.info(summary_string)\n\n    def validate(self):\n        self.generator.eval()\n\n        start = time.time()\n\n        summary_string = \'\'\n\n        bar = Bar(\'Validation\', fill=\'#\', max=len(self.valid_loader))\n\n        if self.evaluation_accumulators is not None:\n            for k,v in self.evaluation_accumulators.items():\n                self.evaluation_accumulators[k] = []\n\n        J_regressor = torch.from_numpy(np.load(osp.join(VIBE_DATA_DIR, \'J_regressor_h36m.npy\'))).float()\n\n        for i, target in enumerate(self.valid_loader):\n\n            move_dict_to_device(target, self.device)\n\n            # <=============\n            with torch.no_grad():\n                inp = target[\'features\']\n\n                preds = self.generator(inp, J_regressor=J_regressor)\n\n                # convert to 14 keypoint format for evaluation\n                n_kp = preds[-1][\'kp_3d\'].shape[-2]\n                pred_j3d = preds[-1][\'kp_3d\'].view(-1, n_kp, 3).cpu().numpy()\n                target_j3d = target[\'kp_3d\'].view(-1, n_kp, 3).cpu().numpy()\n                pred_verts = preds[-1][\'verts\'].view(-1, 6890, 3).cpu().numpy()\n                target_theta = target[\'theta\'].view(-1, 85).cpu().numpy()\n\n\n                self.evaluation_accumulators[\'pred_verts\'].append(pred_verts)\n                self.evaluation_accumulators[\'target_theta\'].append(target_theta)\n\n                self.evaluation_accumulators[\'pred_j3d\'].append(pred_j3d)\n                self.evaluation_accumulators[\'target_j3d\'].append(target_j3d)\n            # =============>\n\n            # <============= DEBUG\n            if self.debug and self.valid_global_step % self.debug_freq == 0:\n                from lib.utils.vis import batch_visualize_vid_preds\n                video = target[\'video\']\n                dataset = \'common\'\n                vid_tensor = batch_visualize_vid_preds(video, preds[-1], target, vis_hmr=False, dataset=dataset)\n                self.writer.add_video(\'valid-video\', vid_tensor, global_step=self.valid_global_step, fps=10)\n            # =============>\n\n            batch_time = time.time() - start\n\n            summary_string = f\'({i + 1}/{len(self.valid_loader)}) | batch: {batch_time * 10.0:.4}ms | \' \\\n                             f\'Total: {bar.elapsed_td} | ETA: {bar.eta_td:}\'\n\n            self.valid_global_step += 1\n            bar.suffix = summary_string\n            bar.next()\n\n        bar.finish()\n\n        logger.info(summary_string)\n\n    def fit(self):\n\n        for epoch in range(self.start_epoch, self.end_epoch):\n            self.epoch = epoch\n            self.train()\n            self.validate()\n            performance = self.evaluate()\n\n            if self.lr_scheduler is not None:\n                self.lr_scheduler.step(performance)\n\n            if self.motion_lr_scheduler is not None:\n                self.motion_lr_scheduler.step(performance)\n\n            # log the learning rate\n            for param_group in self.gen_optimizer.param_groups:\n                print(f\'Learning rate {param_group[""lr""]}\')\n                self.writer.add_scalar(\'lr/gen_lr\', param_group[\'lr\'], global_step=self.epoch)\n\n            for param_group in self.dis_motion_optimizer.param_groups:\n                print(f\'Learning rate {param_group[""lr""]}\')\n                self.writer.add_scalar(\'lr/dis_lr\', param_group[\'lr\'], global_step=self.epoch)\n\n            logger.info(f\'Epoch {epoch+1} performance: {performance:.4f}\')\n\n            self.save_model(performance, epoch)\n\n            if performance > 80.0:\n                exit(f\'MPJPE error is {performance}, higher than 80.0. Exiting!...\')\n\n        self.writer.close()\n\n    def save_model(self, performance, epoch):\n        save_dict = {\n            \'epoch\': epoch,\n            \'gen_state_dict\': self.generator.state_dict(),\n            \'performance\': performance,\n            \'gen_optimizer\': self.gen_optimizer.state_dict(),\n            \'disc_motion_state_dict\': self.motion_discriminator.state_dict(),\n            \'disc_motion_optimizer\': self.dis_motion_optimizer.state_dict(),\n        }\n\n        filename = osp.join(self.logdir, \'checkpoint.pth.tar\')\n        torch.save(save_dict, filename)\n\n        if self.performance_type == \'min\':\n            is_best = performance < self.best_performance\n        else:\n            is_best = performance > self.best_performance\n\n        if is_best:\n            logger.info(\'Best performance achived, saving it!\')\n            self.best_performance = performance\n            shutil.copyfile(filename, osp.join(self.logdir, \'model_best.pth.tar\'))\n\n            with open(osp.join(self.logdir, \'best.txt\'), \'w\') as f:\n                f.write(str(float(performance)))\n\n    def resume_pretrained(self, model_path):\n        if osp.isfile(model_path):\n            checkpoint = torch.load(model_path)\n            self.start_epoch = checkpoint[\'epoch\']\n            self.generator.load_state_dict(checkpoint[\'gen_state_dict\'])\n            self.gen_optimizer.load_state_dict(checkpoint[\'gen_optimizer\'])\n            self.best_performance = checkpoint[\'performance\']\n\n            if \'disc_motion_optimizer\' in checkpoint.keys():\n                self.motion_discriminator.load_state_dict(checkpoint[\'disc_motion_state_dict\'])\n                self.dis_motion_optimizer.load_state_dict(checkpoint[\'disc_motion_optimizer\'])\n\n            logger.info(f""=> loaded checkpoint \'{model_path}\' ""\n                  f""(epoch {self.start_epoch}, performance {self.best_performance})"")\n        else:\n            logger.info(f""=> no checkpoint found at \'{model_path}\'"")\n\n    def evaluate(self):\n\n        for k, v in self.evaluation_accumulators.items():\n            self.evaluation_accumulators[k] = np.vstack(v)\n\n        pred_j3ds = self.evaluation_accumulators[\'pred_j3d\']\n        target_j3ds = self.evaluation_accumulators[\'target_j3d\']\n\n        pred_j3ds = torch.from_numpy(pred_j3ds).float()\n        target_j3ds = torch.from_numpy(target_j3ds).float()\n\n        print(f\'Evaluating on {pred_j3ds.shape[0]} number of poses...\')\n        pred_pelvis = (pred_j3ds[:,[2],:] + pred_j3ds[:,[3],:]) / 2.0\n        target_pelvis = (target_j3ds[:,[2],:] + target_j3ds[:,[3],:]) / 2.0\n\n\n        pred_j3ds -= pred_pelvis\n        target_j3ds -= target_pelvis\n        # Absolute error (MPJPE)\n        errors = torch.sqrt(((pred_j3ds - target_j3ds) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n        S1_hat = batch_compute_similarity_transform_torch(pred_j3ds, target_j3ds)\n        errors_pa = torch.sqrt(((S1_hat - target_j3ds) ** 2).sum(dim=-1)).mean(dim=-1).cpu().numpy()\n        pred_verts = self.evaluation_accumulators[\'pred_verts\']\n        target_theta = self.evaluation_accumulators[\'target_theta\']\n\n        m2mm = 1000\n\n        pve = np.mean(compute_error_verts(target_theta=target_theta, pred_verts=pred_verts)) * m2mm\n        accel = np.mean(compute_accel(pred_j3ds)) * m2mm\n        accel_err = np.mean(compute_error_accel(joints_pred=pred_j3ds, joints_gt=target_j3ds)) * m2mm\n        mpjpe = np.mean(errors) * m2mm\n        pa_mpjpe = np.mean(errors_pa) * m2mm\n\n        eval_dict = {\n            \'mpjpe\': mpjpe,\n            \'pa-mpjpe\': pa_mpjpe,\n            \'accel\': accel,\n            \'pve\': pve,\n            \'accel_err\': accel_err\n        }\n\n        log_str = f\'Epoch {self.epoch}, \'\n        log_str += \' \'.join([f\'{k.upper()}: {v:.4f},\'for k,v in eval_dict.items()])\n        logger.info(log_str)\n\n        for k,v in eval_dict.items():\n            self.writer.add_scalar(f\'error/{k}\', v, global_step=self.epoch)\n\n        return pa_mpjpe\n'"
lib/data_utils/amass_utils.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport joblib\nimport argparse\nimport numpy as np\nimport os.path as osp\nfrom tqdm import tqdm\n\nfrom lib.core.config import VIBE_DB_DIR\n\ndict_keys = ['betas', 'dmpls', 'gender', 'mocap_framerate', 'poses', 'trans']\n\n# extract SMPL joints from SMPL-H model\njoints_to_use = np.array([\n    0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,\n    11, 12, 13, 14, 15, 16, 17, 18, 19,\n    20, 21, 22, 37\n])\njoints_to_use = np.arange(0,156).reshape((-1,3))[joints_to_use].reshape(-1)\n\nall_sequences = [\n    'ACCAD',\n    'BioMotionLab_NTroje',\n    'CMU',\n    'EKUT',\n    'Eyes_Japan_Dataset',\n    'HumanEva',\n    'KIT',\n    'MPI_HDM05',\n    'MPI_Limits',\n    'MPI_mosh',\n    'SFU',\n    'SSM_synced',\n    'TCD_handMocap',\n    'TotalCapture',\n    'Transitions_mocap',\n]\n\ndef read_data(folder, sequences):\n    # sequences = [osp.join(folder, x) for x in sorted(os.listdir(folder)) if osp.isdir(osp.join(folder, x))]\n\n    if sequences == 'all':\n        sequences = all_sequences\n\n    db = {\n        'theta': [],\n        'vid_name': [],\n    }\n\n    for seq_name in sequences:\n        print(f'Reading {seq_name} sequence...')\n        seq_folder = osp.join(folder, seq_name)\n\n        thetas, vid_names = read_single_sequence(seq_folder, seq_name)\n        seq_name_list = np.array([seq_name]*thetas.shape[0])\n        print(seq_name, 'number of videos', thetas.shape[0])\n        db['theta'].append(thetas)\n        db['vid_name'].append(vid_names)\n\n    db['theta'] = np.concatenate(db['theta'], axis=0)\n    db['vid_name'] = np.concatenate(db['vid_name'], axis=0)\n\n    return db\n\n\n\ndef read_single_sequence(folder, seq_name):\n    subjects = os.listdir(folder)\n\n    thetas = []\n    vid_names = []\n\n    for subject in tqdm(subjects):\n        actions = [x for x in os.listdir(osp.join(folder, subject)) if x.endswith('.npz')]\n\n        for action in actions:\n            fname = osp.join(folder, subject, action)\n            \n            if fname.endswith('shape.npz'):\n                continue\n                \n            data = np.load(fname)\n                \n            pose = data['poses'][:, joints_to_use]\n\n            if pose.shape[0] < 60:\n                continue\n\n            shape = np.repeat(data['betas'][:10][np.newaxis], pose.shape[0], axis=0)\n            theta = np.concatenate([pose,shape], axis=1)\n            vid_name = np.array([f'{seq_name}_{subject}_{action[:-4]}']*pose.shape[0])\n\n            vid_names.append(vid_name)\n            thetas.append(theta)\n\n    return np.concatenate(thetas, axis=0), np.concatenate(vid_names, axis=0)\n\n\ndef read_seq_data(folder, nsubjects, fps):\n    subjects = os.listdir(folder)\n    sequences = {}\n\n    assert nsubjects < len(subjects), 'nsubjects should be less than len(subjects)'\n\n    for subject in subjects[:nsubjects]:\n        actions = os.listdir(osp.join(folder, subject))\n\n        for action in actions:\n            data = np.load(osp.join(folder, subject, action))\n            mocap_framerate = int(data['mocap_framerate'])\n            sampling_freq = mocap_framerate // fps\n            sequences[(subject, action)] = data['poses'][0::sampling_freq, joints_to_use]\n\n    train_set = {}\n    test_set = {}\n\n    for i, (k,v) in enumerate(sequences.items()):\n        if i < len(sequences.keys()) - len(sequences.keys()) // 4:\n            train_set[k] = v\n        else:\n            test_set[k] = v\n\n    return train_set, test_set\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dir', type=str, help='dataset directory', default='data/amass')\n    args = parser.parse_args()\n\n    db = read_data(args.dir, sequences=all_sequences)\n    db_file = osp.join(VIBE_DB_DIR, 'amass_db.pt')\n    print(f'Saving AMASS dataset to {db_file}')\n    joblib.dump(db, db_file)\n"""
lib/data_utils/feature_extractor.py,7,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport torch\nimport torchvision\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom lib.utils.vis import batch_visualize_preds\nfrom lib.data_utils.img_utils import get_single_image_crop, convert_cvimg_to_tensor\n\n\ndef extract_features(model, video, bbox, debug=False, batch_size=200, kp_2d=None, dataset=None, scale=1.3):\n    '''\n    :param model: pretrained HMR model, use lib/models/hmr.py:get_pretrained_hmr()\n    :param video: video filename, torch.Tensor in shape (num_frames,W,H,C)\n    :param bbox: bbox array in shape (T,4)\n    :param debug: boolean, true if you want to debug HMR predictions\n    :param batch_size: batch size for HMR input\n    :return: features: resnet50 features np.ndarray -> shape (num_frames, 4)\n    '''\n    device = 'cuda'\n\n    if isinstance(video, torch.Tensor) or isinstance(video, np.ndarray):\n        video = video\n    elif isinstance(video, str):\n        if os.path.isfile(video):\n            video, _, _ = torchvision.io.read_video(video)\n        else:\n            raise ValueError(f'{video} is not a valid file.')\n    else:\n        raise ValueError(f'Unknown type {type(video)} for video object')\n\n    # For debugging ground truth 2d keypoints\n    if debug and kp_2d is not None:\n        import cv2\n        if isinstance(video[0], np.str_):\n            print(video[0])\n            frame = cv2.cvtColor(cv2.imread(video[0]), cv2.COLOR_BGR2RGB)\n        elif isinstance(video[0], np.ndarray):\n            frame = video[0]\n        else:\n            frame = video[0].numpy()\n        for i in range(kp_2d.shape[1]):\n            frame = cv2.circle(\n                frame.copy(),\n                (int(kp_2d[0,i,0]), int(kp_2d[0,i,1])),\n                thickness=3,\n                color=(255,0,0),\n                radius=3,\n            )\n\n        plt.imshow(frame)\n        plt.show()\n\n    if dataset == 'insta':\n        video = torch.cat(\n            [convert_cvimg_to_tensor(image).unsqueeze(0) for image in video], dim=0\n        ).to(device)\n    else:\n        # crop bbox locations\n        video = torch.cat(\n            [get_single_image_crop(image, bbox, scale=scale).unsqueeze(0) for image, bbox in zip(video, bbox)], dim=0\n        ).to(device)\n\n    features = []\n\n    # split video into batches of frames\n    frames = torch.split(video, batch_size)\n\n    with torch.no_grad():\n        for images in frames:\n\n            if not debug:\n                pred = model.feature_extractor(images)\n                features.append(pred.cpu())\n                del pred, images\n            else:\n                preds = model(images)\n                dataset = 'spin' # dataset if dataset else 'common'\n                result_image = batch_visualize_preds(\n                    images,\n                    preds[-1],\n                    target_exists=False,\n                    max_images=4,\n                    dataset=dataset,\n                )\n\n                plt.figure(figsize=(19.2, 10.8))\n                plt.axis('off')\n                plt.imshow(result_image)\n                plt.show()\n\n                del preds, images\n                return 0\n\n        features = torch.cat(features, dim=0)\n\n    return features.numpy()\n"""
lib/data_utils/img_utils.py,3,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport cv2\nimport torch\n\nimport random\nimport numpy as np\nimport torchvision.transforms as transforms\nfrom skimage.util.shape import view_as_windows\n\ndef get_image(filename):\n    image = cv2.imread(filename)\n    return cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n\ndef do_augmentation(scale_factor=0.3, color_factor=0.2):\n    scale = random.uniform(1.2, 1.2+scale_factor)\n    # scale = np.clip(np.random.randn(), 0.0, 1.0) * scale_factor + 1.2\n    rot = 0 # np.clip(np.random.randn(), -2.0, 2.0) * aug_config.rot_factor if random.random() <= aug_config.rot_aug_rate else 0\n    do_flip = False # aug_config.do_flip_aug and random.random() <= aug_config.flip_aug_rate\n    c_up = 1.0 + color_factor\n    c_low = 1.0 - color_factor\n    color_scale = [random.uniform(c_low, c_up), random.uniform(c_low, c_up), random.uniform(c_low, c_up)]\n    return scale, rot, do_flip, color_scale\n\ndef trans_point2d(pt_2d, trans):\n    src_pt = np.array([pt_2d[0], pt_2d[1], 1.]).T\n    dst_pt = np.dot(trans, src_pt)\n    return dst_pt[0:2]\n\ndef rotate_2d(pt_2d, rot_rad):\n    x = pt_2d[0]\n    y = pt_2d[1]\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    xx = x * cs - y * sn\n    yy = x * sn + y * cs\n    return np.array([xx, yy], dtype=np.float32)\n\ndef gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, scale, rot, inv=False):\n    # augment size with scale\n    src_w = src_width * scale\n    src_h = src_height * scale\n    src_center = np.zeros(2)\n    src_center[0] = c_x\n    src_center[1] = c_y # np.array([c_x, c_y], dtype=np.float32)\n    # augment rotation\n    rot_rad = np.pi * rot / 180\n    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n\n    dst_w = dst_width\n    dst_h = dst_height\n    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = src_center\n    src[1, :] = src_center + src_downdir\n    src[2, :] = src_center + src_rightdir\n\n    dst = np.zeros((3, 2), dtype=np.float32)\n    dst[0, :] = dst_center\n    dst[1, :] = dst_center + dst_downdir\n    dst[2, :] = dst_center + dst_rightdir\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\ndef generate_patch_image_cv(cvimg, c_x, c_y, bb_width, bb_height, patch_width, patch_height, do_flip, scale, rot):\n    img = cvimg.copy()\n    img_height, img_width, img_channels = img.shape\n\n    if do_flip:\n        img = img[:, ::-1, :]\n        c_x = img_width - c_x - 1\n\n    trans = gen_trans_from_patch_cv(c_x, c_y, bb_width, bb_height, patch_width, patch_height, scale, rot, inv=False)\n\n    img_patch = cv2.warpAffine(img, trans, (int(patch_width), int(patch_height)),\n                               flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT)\n\n    return img_patch, trans\n\ndef crop_image(image, kp_2d, center_x, center_y, width, height, patch_width, patch_height, do_augment):\n\n    # get augmentation params\n    if do_augment:\n        scale, rot, do_flip, color_scale = do_augmentation()\n    else:\n        scale, rot, do_flip, color_scale = 1.3, 0, False, [1.0, 1.0, 1.0]\n\n    # generate image patch\n    image, trans = generate_patch_image_cv(\n        image,\n        center_x,\n        center_y,\n        width,\n        height,\n        patch_width,\n        patch_height,\n        do_flip,\n        scale,\n        rot\n    )\n\n    for n_jt in range(kp_2d.shape[0]):\n        kp_2d[n_jt] = trans_point2d(kp_2d[n_jt], trans)\n\n    return image, kp_2d, trans\n\ndef transfrom_keypoints(kp_2d, center_x, center_y, width, height, patch_width, patch_height, do_augment):\n\n    if do_augment:\n        scale, rot, do_flip, color_scale = do_augmentation()\n    else:\n        scale, rot, do_flip, color_scale = 1.2, 0, False, [1.0, 1.0, 1.0]\n\n    # generate transformation\n    trans = gen_trans_from_patch_cv(\n        center_x,\n        center_y,\n        width,\n        height,\n        patch_width,\n        patch_height,\n        scale,\n        rot,\n        inv=False,\n    )\n\n    for n_jt in range(kp_2d.shape[0]):\n        kp_2d[n_jt] = trans_point2d(kp_2d[n_jt], trans)\n\n    return kp_2d, trans\n\ndef get_image_crops(image_file, bboxes):\n    image = cv2.cvtColor(cv2.imread(image_file), cv2.COLOR_BGR2RGB)\n    crop_images = []\n    for bb in bboxes:\n        c_y, c_x = (bb[0]+bb[2]) // 2, (bb[1]+bb[3]) // 2\n        h, w = bb[2]-bb[0], bb[3]-bb[1]\n        w = h = np.where(w / h > 1, w, h)\n        crop_image, _ = generate_patch_image_cv(\n            cvimg=image.copy(),\n            c_x=c_x,\n            c_y=c_y,\n            bb_width=w,\n            bb_height=h,\n            patch_width=224,\n            patch_height=224,\n            do_flip=False,\n            scale=1.3,\n            rot=0,\n        )\n        crop_image = convert_cvimg_to_tensor(crop_image)\n        crop_images.append(crop_image)\n\n    batch_image = torch.cat([x.unsqueeze(0) for x in crop_images])\n    return batch_image\n\ndef get_single_image_crop(image, bbox, scale=1.3):\n    if isinstance(image, str):\n        if os.path.isfile(image):\n            image = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n        else:\n            print(image)\n            raise BaseException(image, 'is not a valid file!')\n    elif isinstance(image, torch.Tensor):\n        image = image.numpy()\n    elif not isinstance(image, np.ndarray):\n        raise('Unknown type for object', type(image))\n\n    crop_image, _ = generate_patch_image_cv(\n        cvimg=image.copy(),\n        c_x=bbox[0],\n        c_y=bbox[1],\n        bb_width=bbox[2],\n        bb_height=bbox[3],\n        patch_width=224,\n        patch_height=224,\n        do_flip=False,\n        scale=scale,\n        rot=0,\n    )\n\n    crop_image = convert_cvimg_to_tensor(crop_image)\n\n    return crop_image\n\ndef get_single_image_crop_demo(image, bbox, kp_2d, scale=1.2, crop_size=224):\n    if isinstance(image, str):\n        if os.path.isfile(image):\n            image = cv2.cvtColor(cv2.imread(image), cv2.COLOR_BGR2RGB)\n        else:\n            print(image)\n            raise BaseException(image, 'is not a valid file!')\n    elif isinstance(image, torch.Tensor):\n        image = image.numpy()\n    elif not isinstance(image, np.ndarray):\n        raise('Unknown type for object', type(image))\n\n    crop_image, trans = generate_patch_image_cv(\n        cvimg=image.copy(),\n        c_x=bbox[0],\n        c_y=bbox[1],\n        bb_width=bbox[2],\n        bb_height=bbox[3],\n        patch_width=crop_size,\n        patch_height=crop_size,\n        do_flip=False,\n        scale=scale,\n        rot=0,\n    )\n\n    if kp_2d is not None:\n        for n_jt in range(kp_2d.shape[0]):\n            kp_2d[n_jt, :2] = trans_point2d(kp_2d[n_jt], trans)\n\n    raw_image = crop_image.copy()\n\n    crop_image = convert_cvimg_to_tensor(crop_image)\n\n    return crop_image, raw_image, kp_2d\n\ndef read_image(filename):\n    image = cv2.cvtColor(cv2.imread(filename), cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (224,224))\n    return convert_cvimg_to_tensor(image)\n\ndef convert_cvimg_to_tensor(image):\n    transform = get_default_transform()\n    image = transform(image)\n    return image\n\ndef torch2numpy(image):\n    image = image.detach().cpu()\n    inv_normalize = transforms.Normalize(\n        mean=[-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.255],\n        std=[1 / 0.229, 1 / 0.224, 1 / 0.255]\n    )\n    image = inv_normalize(image)\n    image = image.clamp(0., 1.)\n    image = image.numpy() * 255.\n    image = np.transpose(image, (1, 2, 0))\n    return image.astype(np.uint8)\n\ndef torch_vid2numpy(video):\n    video = video.detach().cpu().numpy()\n    # video = np.transpose(video, (0, 2, 1, 3, 4)) # NCTHW->NTCHW\n    # Denormalize\n    mean = np.array([-0.485 / 0.229, -0.456 / 0.224, -0.406 / 0.255])\n    std = np.array([1 / 0.229, 1 / 0.224, 1 / 0.255])\n\n    mean = mean[np.newaxis, np.newaxis, ..., np.newaxis, np.newaxis]\n    std = std[np.newaxis, np.newaxis, ..., np.newaxis, np.newaxis]\n\n    video = (video - mean) / std # [:, :, i, :, :].sub_(mean[i]).div_(std[i]).clamp_(0., 1.).mul_(255.)\n    video = video.clip(0.,1.) * 255\n    video = video.astype(np.uint8)\n    return video\n\ndef get_bbox_from_kp2d(kp_2d):\n    # get bbox\n    if len(kp_2d.shape) > 2:\n        ul = np.array([kp_2d[:, :, 0].min(axis=1), kp_2d[:, :, 1].min(axis=1)])  # upper left\n        lr = np.array([kp_2d[:, :, 0].max(axis=1), kp_2d[:, :, 1].max(axis=1)])  # lower right\n    else:\n        ul = np.array([kp_2d[:, 0].min(), kp_2d[:, 1].min()])  # upper left\n        lr = np.array([kp_2d[:, 0].max(), kp_2d[:, 1].max()])  # lower right\n\n    # ul[1] -= (lr[1] - ul[1]) * 0.10  # prevent cutting the head\n    w = lr[0] - ul[0]\n    h = lr[1] - ul[1]\n    c_x, c_y = ul[0] + w / 2, ul[1] + h / 2\n    # to keep the aspect ratio\n    w = h = np.where(w / h > 1, w, h)\n    w = h = h * 1.1\n\n    bbox = np.array([c_x, c_y, w, h])  # shape = (4,N)\n    return bbox\n\ndef normalize_2d_kp(kp_2d, crop_size=224, inv=False):\n    # Normalize keypoints between -1, 1\n    if not inv:\n        ratio = 1.0 / crop_size\n        kp_2d = 2.0 * kp_2d * ratio - 1.0\n    else:\n        ratio = 1.0 / crop_size\n        kp_2d = (kp_2d + 1.0)/(2*ratio)\n\n    return kp_2d\n\ndef get_default_transform():\n    normalize = transforms.Normalize(\n        mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n    )\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        normalize,\n    ])\n    return transform\n\ndef split_into_chunks(vid_names, seqlen, stride):\n    video_start_end_indices = []\n\n    video_names, group = np.unique(vid_names, return_index=True)\n    perm = np.argsort(group)\n    video_names, group = video_names[perm], group[perm]\n\n    indices = np.split(np.arange(0, vid_names.shape[0]), group[1:])\n\n    for idx in range(len(video_names)):\n        indexes = indices[idx]\n        if indexes.shape[0] < seqlen:\n            continue\n        chunks = view_as_windows(indexes, (seqlen,), step=stride)\n        start_finish = chunks[:, (0, -1)].tolist()\n        video_start_end_indices += start_finish\n\n    return video_start_end_indices"""
lib/data_utils/insta_utils.py,0,"b'# Some functions are borrowed from\n# https://github.com/akanazawa/human_dynamics/blob/master/src/datasets/insta_variety_to_tfrecords.py\n# Adhere to their licence to use these functions\n\n""""""\nVisualizes tfrecords.\nSample usage:\npython -m src.datasets.visualize_tfrecords --data_rootdir /scratch3/kanazawa/hmmr_tfrecords_release_test/ --dataset penn_action --split test\n""""""\nimport os\nimport sys\nimport h5py\nsys.path.append(\'.\')\n\nimport argparse\nimport numpy as np\nimport os.path as osp\nfrom glob import glob\nfrom tqdm import tqdm\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom lib.models import spin\nfrom lib.utils.vis import draw_skeleton\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.feature_extractor import extract_features\n\nclass ImageCoder(object):\n    """"""\n    Helper class that provides TensorFlow image coding utilities.\n    Taken from\n    https://github.com/tensorflow/models/blob/master/inception/inception/data/\n        build_image_data.py\n    """"""\n\n    def __init__(self):\n        # Create a single Session to run all image coding calls.\n        gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.05)\n        self._sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n\n        # Initializes function that converts PNG to JPEG data.\n        self._png_data = tf.placeholder(dtype=tf.string)\n        image = tf.image.decode_png(self._png_data, channels=3)\n        self._png_to_jpeg = tf.image.encode_jpeg(\n            image, format=\'rgb\', quality=100)\n\n        # Initializes function that decodes RGB JPEG data.\n        self._decode_jpeg_data = tf.placeholder(dtype=tf.string)\n        self._decode_jpeg = tf.image.decode_jpeg(\n            self._decode_jpeg_data, channels=3)\n\n        self._encode_jpeg_data = tf.placeholder(dtype=tf.uint8)\n        self._encode_jpeg = tf.image.encode_jpeg(\n            self._encode_jpeg_data, format=\'rgb\')\n\n        self._decode_png_data = tf.placeholder(dtype=tf.string)\n        self._decode_png = tf.image.decode_png(\n            self._decode_png_data, channels=3)\n\n        self._encode_png_data = tf.placeholder(dtype=tf.uint8)\n        self._encode_png = tf.image.encode_png(self._encode_png_data)\n\n    def png_to_jpeg(self, image_data):\n        return self._sess.run(\n            self._png_to_jpeg, feed_dict={\n                self._png_data: image_data\n            })\n\n    def decode_jpeg(self, image_data):\n        image = self._sess.run(\n            self._decode_jpeg, feed_dict={\n                self._decode_jpeg_data: image_data\n            })\n        assert len(image.shape) == 3\n        assert image.shape[2] == 3\n        return image\n\n    def encode_jpeg(self, image):\n        image_data = self._sess.run(\n            self._encode_jpeg, feed_dict={\n                self._encode_jpeg_data: image\n            })\n        return image_data\n\n    def encode_png(self, image):\n        image_data = self._sess.run(\n            self._encode_png, feed_dict={\n                self._encode_png_data: image\n            })\n        return image_data\n\n    def decode_png(self, image_data):\n        image = self._sess.run(\n            self._decode_png, feed_dict={\n                self._decode_png_data: image_data\n            })\n        assert len(image.shape) == 3\n        assert image.shape[2] == 3\n        return image\n\n\ndef read_from_example(serialized_ex):\n    """"""\n    Returns data from an entry in test tfrecord.\n\n    Args:\n        serialized_ex (str).\n\n    Returns:\n        dict. Keys:\n            N (1).\n            centers (Nx2).\n            kps (Nx19x3).\n            gt3ds (Nx14x3).\n            images (Nx224x224x3).\n            im_shapes (Nx2).\n            im_paths (N).\n            poses (Nx24x3).\n            scales (N).\n            shape (10).\n            start_pts (Nx2).\n            time_pts (2).\n    """"""\n    coder = ImageCoder()\n    example = tf.train.Example()\n    example.ParseFromString(serialized_ex)\n    features = example.features.feature\n\n    # Load features from example.\n    N = features[\'meta/N\'].int64_list.value[0]\n    im_datas = features[\'image/encoded\'].bytes_list.value\n    centers = features[\'image/centers\'].int64_list.value\n    xys = features[\'image/xys\'].float_list.value\n    face_pts = features[\'image/face_pts\'].float_list.value\n    toe_pts = features[\'image/toe_pts\'].float_list.value\n    vis = features[\'image/visibilities\'].int64_list.value\n    scales = np.array(features[\'image/scale_factors\'].float_list.value)\n    gt3ds = features[\'mosh/gt3ds\'].float_list.value\n    poses = features[\'mosh/poses\'].float_list.value\n    shape = features[\'mosh/shape\'].float_list.value\n    time_pts = features[\'meta/time_pts\'].int64_list.value\n    start_pts = np.array(features[\'image/crop_pts\'].int64_list.value)\n    im_shapes = features[\'image/heightwidths\'].int64_list.value\n    im_paths = features[\'image/filenames\'].bytes_list.value\n\n    # Process and reshape features.\n    images = [coder.decode_jpeg(im_data) for im_data in im_datas]\n    centers = np.array(centers).reshape((N, 2))\n    gt3ds = np.array(gt3ds).reshape((N, -1, 3))\n    gt3ds = gt3ds[:, :14]  # Don\'t want toes_pts or face_pts\n    xys = np.array(xys).reshape((N, 2, 14))\n    vis = np.array(vis, dtype=np.float).reshape((N, 1, 14))\n    face_pts = np.array(face_pts).reshape((N, 3, 5))\n    toe_pts = np.array(toe_pts).reshape((N, 3, 6))\n    kps = np.dstack((\n        np.hstack((xys, vis)),\n        face_pts,\n        toe_pts,\n    ))\n    kps = np.transpose(kps, axes=[0, 2, 1])\n    poses = np.array(poses).reshape((N, 24, 3))\n    shape = np.array(shape)\n    start_pts = np.array(start_pts).reshape((N, 2))\n    im_shapes = np.array(im_shapes).reshape((N, 2))\n\n    return {\n        \'N\': N,\n        \'centers\': centers,\n        \'kps\': kps,\n        \'gt3ds\': gt3ds,\n        \'images\': images,\n        \'im_shapes\': im_shapes,\n        \'im_paths\': im_paths,\n        \'poses\': poses,\n        \'scales\': scales,\n        \'shape\': shape,\n        \'start_pts\': start_pts,\n        \'time_pts\': time_pts,\n    }\n\n\ndef visualize_tfrecords(fpaths):\n    sess = tf.Session()\n    for fname in fpaths:\n        print(fname)\n        for serialized_ex in tf.python_io.tf_record_iterator(fname):\n            example = tf.train.Example()\n            example.ParseFromString(serialized_ex)\n            # import ipdb; ipdb.set_trace()\n            # Now these are sequences.\n            N = int(example.features.feature[\'meta/N\'].int64_list.value[0])\n            print(N)\n            # This is a list of length N\n            images_data = example.features.feature[\n                \'image/encoded\'].bytes_list.value\n\n            xys = example.features.feature[\'image/xys\'].float_list.value\n            xys = np.array(xys).reshape(-1, 2, 14)\n\n            face_pts = example.features.feature[\n                \'image/face_pts\'].float_list.value\n            face_pts = np.array(face_pts).reshape(-1, 3, 5)\n\n            toe_pts = example.features.feature[\n                \'image/toe_pts\'].float_list.value\n\n            if len(toe_pts) == 0:\n                toe_pts = np.zeros(xys.shape[0], 3, 6)\n\n            toe_pts = np.array(toe_pts).reshape(-1, 3, 6)\n\n            visibles = example.features.feature[\n                \'image/visibilities\'].int64_list.value\n            visibles = np.array(visibles).reshape(-1, 1, 14)\n            centers = example.features.feature[\n                \'image/centers\'].int64_list.value\n            centers = np.array(centers).reshape(-1, 2)\n\n            if \'image/phis\' in example.features.feature.keys():\n                phis = example.features.feature[\'image/phis\'].float_list.value\n                phis = np.array(phis)\n\n            for i in range(N):\n                image = sess.run(tf.image.decode_jpeg(images_data[i], channels=3))\n                kp = np.vstack((xys[i], visibles[i]))\n                faces = face_pts[i]\n\n                toes = toe_pts[i]\n                kp = np.hstack((kp, faces, toes))\n                if \'image/phis\' in example.features.feature.keys():\n                    # Preprocessed, so kps are in [-1, 1]\n                    img_shape = image.shape[0]\n                    vis = kp[2, :]\n                    kp = ((kp[:2, :] + 1) * 0.5) * img_shape\n                    kp = np.vstack((kp, vis))\n\n                plt.ion()\n                plt.clf()\n                plt.figure(1)\n\n                skel_img = draw_skeleton(image, kp.T, dataset=\'insta\', unnormalize=False)\n                plt.imshow(skel_img)\n                plt.title(f\'{i}\')\n\n                plt.axis(\'off\')\n                plt.pause(0.5)\n\n\ndef read_single_record(fname):\n    dataset = {\n        \'vid_name\': [],\n        \'frame_id\': [],\n        \'joints2D\': [], # should contain openpose keypoints only\n        \'features\': [],\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    sess = tf.Session()\n\n    for vid_idx, serialized_ex in tqdm(enumerate(tf.python_io.tf_record_iterator(fname))):\n        example = tf.train.Example()\n        example.ParseFromString(serialized_ex)\n\n        N = int(example.features.feature[\'meta/N\'].int64_list.value[0])\n\n        # print(fname, vid_idx, N)\n        # This is a list of length N\n        images_data = example.features.feature[\n            \'image/encoded\'].bytes_list.value\n\n        xys = example.features.feature[\'image/xys\'].float_list.value\n        xys = np.array(xys).reshape(-1, 2, 14)\n\n        face_pts = example.features.feature[\n            \'image/face_pts\'].float_list.value\n        face_pts = np.array(face_pts).reshape(-1, 3, 5)\n\n        toe_pts = example.features.feature[\n            \'image/toe_pts\'].float_list.value\n\n        if len(toe_pts) == 0:\n            toe_pts = np.zeros(xys.shape[0], 3, 6)\n\n        toe_pts = np.array(toe_pts).reshape(-1, 3, 6)\n\n        visibles = example.features.feature[\n            \'image/visibilities\'].int64_list.value\n        visibles = np.array(visibles).reshape(-1, 1, 14)\n\n        video = []\n        kp_2d = []\n        for i in range(N):\n            image = np.expand_dims(sess.run(tf.image.decode_jpeg(images_data[i], channels=3)), axis=0)\n            video.append(image)\n\n            kp = np.vstack((xys[i], visibles[i]))\n            faces = face_pts[i]\n            toes = toe_pts[i]\n\n            kp = np.hstack((kp, faces, toes))\n\n            if \'image/phis\' in example.features.feature.keys():\n                # Preprocessed, so kps are in [-1, 1]\n                img_shape = 224  # image.shape[0]\n                vis = kp[2, :]\n                kp = ((kp[:2, :] + 1) * 0.5) * img_shape\n                kp = np.vstack((kp, vis))\n\n            kp_2d.append(np.expand_dims(kp.T, axis=0))\n\n        video = np.concatenate(video, axis=0)\n        kp_2d = np.concatenate(kp_2d, axis=0)\n\n        vid_name = f\'{fname}-{vid_idx}\'\n        frame_id = np.arange(N)\n        joints2D = kp_2d\n\n        dataset[\'vid_name\'].append(np.array([vid_name] * N))\n        dataset[\'frame_id\'].append(frame_id)\n        dataset[\'joints2D\'].append(joints2D)\n\n        features = extract_features(model, video, bbox=None, kp_2d=kp_2d, dataset=\'insta\', debug=False)\n        dataset[\'features\'].append(features)\n\n        print(features.shape)\n        assert features.shape[0] == N\n\n    for k in dataset.keys():\n        dataset[k] = np.concatenate(dataset[k])\n\n    for k,v in dataset.items():\n        print(k, len(v))\n\n    return dataset\n\n\ndef save_hdf5(filename, db):\n    with h5py.File(filename, \'w\') as f:\n        for k, v in db.items():\n            if k == \'vid_name\':\n                v = np.array(v, dtype=np.string_)\n            f.create_dataset(k, data=v)\n\n\ndef concatenate_annotations():\n    ds = {\n        \'vid_name\': [],\n        \'frame_id\': [],\n        \'joints2D\': [],\n        \'features\': [],\n    }\n\n    for i in range(273):\n        filename = osp.join(VIBE_DB_DIR, \'insta_parts\', f\'insta_train_part_{i}.h5\')\n        print(filename)\n        with h5py.File(filename, \'r\') as f:\n            for k in ds.keys():\n                ds[k].append(f[k].value)\n\n    for k in ds.keys():\n        ds[k] = np.concatenate(ds[k])\n\n    print(\'Saving Insta Variety dataset!..\')\n    db_file = osp.join(VIBE_DB_DIR, \'insta_train_db.h5\')\n    save_hdf5(db_file, ds)\n    print(\'Saved Insta Variety dataset!...\')\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir\', type=str, help=\'dataset directory\', default=\'data/insta_variety\')\n    args = parser.parse_args()\n\n    split = \'train\'\n    fpaths = glob(f\'{args.dir}/{split}/*.tfrecord\')\n    fpaths = sorted(fpaths)\n\n    os.makedirs(osp.join(VIBE_DB_DIR, \'insta_parts\'), exist_ok=True)\n\n    for idx, fp in enumerate(fpaths):\n        dataset = read_single_record(fp)\n\n        db_file = osp.join(VIBE_DB_DIR, \'insta_parts\', f\'insta_train_part_{idx}.h5\')\n\n        save_hdf5(db_file, dataset)\n\n    concatenate_annotations()\n\n\n\n\n\n'"
lib/data_utils/kp_utils.py,0,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport numpy as np\n\ndef keypoint_hflip(kp, img_width):\n    # Flip a keypoint horizontally around the y-axis\n    # kp N,2\n    if len(kp.shape) == 2:\n        kp[:,0] = (img_width - 1.) - kp[:,0]\n    elif len(kp.shape) == 3:\n        kp[:, :, 0] = (img_width - 1.) - kp[:, :, 0]\n    return kp\n\ndef convert_kps(joints2d, src, dst):\n    src_names = eval(f\'get_{src}_joint_names\')()\n    dst_names = eval(f\'get_{dst}_joint_names\')()\n\n    out_joints2d = np.zeros((joints2d.shape[0], len(dst_names), 3))\n\n    for idx, jn in enumerate(dst_names):\n        if jn in src_names:\n            out_joints2d[:, idx] = joints2d[:, src_names.index(jn)]\n\n    return out_joints2d\n\ndef get_perm_idxs(src, dst):\n    src_names = eval(f\'get_{src}_joint_names\')()\n    dst_names = eval(f\'get_{dst}_joint_names\')()\n    idxs = [src_names.index(h) for h in dst_names if h in src_names]\n    return idxs\n\ndef get_mpii3d_test_joint_names():\n    return [\n        \'headtop\', # \'head_top\',\n        \'neck\',\n        \'rshoulder\',# \'right_shoulder\',\n        \'relbow\',# \'right_elbow\',\n        \'rwrist\',# \'right_wrist\',\n        \'lshoulder\',# \'left_shoulder\',\n        \'lelbow\', # \'left_elbow\',\n        \'lwrist\', # \'left_wrist\',\n        \'rhip\', # \'right_hip\',\n        \'rknee\', # \'right_knee\',\n        \'rankle\',# \'right_ankle\',\n        \'lhip\',# \'left_hip\',\n        \'lknee\',# \'left_knee\',\n        \'lankle\',# \'left_ankle\'\n        \'hip\',# \'pelvis\',\n        \'Spine (H36M)\',# \'spine\',\n        \'Head (H36M)\',# \'head\'\n    ]\n\ndef get_mpii3d_joint_names():\n    return [\n        \'spine3\', # 0,\n        \'spine4\', # 1,\n        \'spine2\', # 2,\n        \'Spine (H36M)\', #\'spine\', # 3,\n        \'hip\', # \'pelvis\', # 4,\n        \'neck\', # 5,\n        \'Head (H36M)\', # \'head\', # 6,\n        ""headtop"", # \'head_top\', # 7,\n        \'left_clavicle\', # 8,\n        ""lshoulder"", # \'left_shoulder\', # 9,\n        ""lelbow"", # \'left_elbow\',# 10,\n        ""lwrist"", # \'left_wrist\',# 11,\n        \'left_hand\',# 12,\n        \'right_clavicle\',# 13,\n        \'rshoulder\',# \'right_shoulder\',# 14,\n        \'relbow\',# \'right_elbow\',# 15,\n        \'rwrist\',# \'right_wrist\',# 16,\n        \'right_hand\',# 17,\n        \'lhip\', # left_hip\',# 18,\n        \'lknee\', # \'left_knee\',# 19,\n        \'lankle\', #left ankle # 20\n        \'left_foot\', # 21\n        \'left_toe\', # 22\n        ""rhip"", # \'right_hip\',# 23\n        ""rknee"", # \'right_knee\',# 24\n        ""rankle"", #\'right_ankle\', # 25\n        \'right_foot\',# 26\n        \'right_toe\' # 27\n    ]\n\ndef get_insta_joint_names():\n    return [\n        \'OP RHeel\',\n        \'OP RKnee\',\n        \'OP RHip\',\n        \'OP LHip\',\n        \'OP LKnee\',\n        \'OP LHeel\',\n        \'OP RWrist\',\n        \'OP RElbow\',\n        \'OP RShoulder\',\n        \'OP LShoulder\',\n        \'OP LElbow\',\n        \'OP LWrist\',\n        \'OP Neck\',\n        \'headtop\',\n        \'OP Nose\',\n        \'OP LEye\',\n        \'OP REye\',\n        \'OP LEar\',\n        \'OP REar\',\n        \'OP LBigToe\',\n        \'OP RBigToe\',\n        \'OP LSmallToe\',\n        \'OP RSmallToe\',\n        \'OP LAnkle\',\n        \'OP RAnkle\',\n    ]\n\ndef get_insta_skeleton():\n    return np.array(\n        [\n            [0 , 1],\n            [1 , 2],\n            [2 , 3],\n            [3 , 4],\n            [4 , 5],\n            [6 , 7],\n            [7 , 8],\n            [8 , 9],\n            [9 ,10],\n            [2 , 8],\n            [3 , 9],\n            [10,11],\n            [8 ,12],\n            [9 ,12],\n            [12,13],\n            [12,14],\n            [14,15],\n            [14,16],\n            [15,17],\n            [16,18],\n            [0 ,20],\n            [20,22],\n            [5 ,19],\n            [19,21],\n            [5 ,23],\n            [0 ,24],\n        ])\n\ndef get_staf_skeleton():\n    return np.array(\n        [\n            [0, 1],\n            [1, 2],\n            [2, 3],\n            [3, 4],\n            [1, 5],\n            [5, 6],\n            [6, 7],\n            [1, 8],\n            [8, 9],\n            [9, 10],\n            [10, 11],\n            [8, 12],\n            [12, 13],\n            [13, 14],\n            [0, 15],\n            [0, 16],\n            [15, 17],\n            [16, 18],\n            [2, 9],\n            [5, 12],\n            [1, 19],\n            [20, 19],\n        ]\n    )\n\ndef get_staf_joint_names():\n    return [\n        \'OP Nose\', # 0,\n        \'OP Neck\', # 1,\n        \'OP RShoulder\', # 2,\n        \'OP RElbow\', # 3,\n        \'OP RWrist\', # 4,\n        \'OP LShoulder\', # 5,\n        \'OP LElbow\', # 6,\n        \'OP LWrist\', # 7,\n        \'OP MidHip\', # 8,\n        \'OP RHip\', # 9,\n        \'OP RKnee\', # 10,\n        \'OP RAnkle\', # 11,\n        \'OP LHip\', # 12,\n        \'OP LKnee\', # 13,\n        \'OP LAnkle\', # 14,\n        \'OP REye\', # 15,\n        \'OP LEye\', # 16,\n        \'OP REar\', # 17,\n        \'OP LEar\', # 18,\n        \'Neck (LSP)\', # 19,\n        \'Top of Head (LSP)\', # 20,\n    ]\n\ndef get_spin_joint_names():\n    return [\n        \'OP Nose\',        # 0\n        \'OP Neck\',        # 1\n        \'OP RShoulder\',   # 2\n        \'OP RElbow\',      # 3\n        \'OP RWrist\',      # 4\n        \'OP LShoulder\',   # 5\n        \'OP LElbow\',      # 6\n        \'OP LWrist\',      # 7\n        \'OP MidHip\',      # 8\n        \'OP RHip\',        # 9\n        \'OP RKnee\',       # 10\n        \'OP RAnkle\',      # 11\n        \'OP LHip\',        # 12\n        \'OP LKnee\',       # 13\n        \'OP LAnkle\',      # 14\n        \'OP REye\',        # 15\n        \'OP LEye\',        # 16\n        \'OP REar\',        # 17\n        \'OP LEar\',        # 18\n        \'OP LBigToe\',     # 19\n        \'OP LSmallToe\',   # 20\n        \'OP LHeel\',       # 21\n        \'OP RBigToe\',     # 22\n        \'OP RSmallToe\',   # 23\n        \'OP RHeel\',       # 24\n        \'rankle\',         # 25\n        \'rknee\',          # 26\n        \'rhip\',           # 27\n        \'lhip\',           # 28\n        \'lknee\',          # 29\n        \'lankle\',         # 30\n        \'rwrist\',         # 31\n        \'relbow\',         # 32\n        \'rshoulder\',      # 33\n        \'lshoulder\',      # 34\n        \'lelbow\',         # 35\n        \'lwrist\',         # 36\n        \'neck\',           # 37\n        \'headtop\',        # 38\n        \'hip\',            # 39 \'Pelvis (MPII)\', # 39\n        \'thorax\',         # 40 \'Thorax (MPII)\', # 40\n        \'Spine (H36M)\',   # 41\n        \'Jaw (H36M)\',     # 42\n        \'Head (H36M)\',    # 43\n        \'nose\',           # 44\n        \'leye\',           # 45 \'Left Eye\', # 45\n        \'reye\',           # 46 \'Right Eye\', # 46\n        \'lear\',           # 47 \'Left Ear\', # 47\n        \'rear\',           # 48 \'Right Ear\', # 48\n    ]\n\ndef get_h36m_joint_names():\n    return [\n        \'hip\',  # 0\n        \'lhip\',  # 1\n        \'lknee\',  # 2\n        \'lankle\',  # 3\n        \'rhip\',  # 4\n        \'rknee\',  # 5\n        \'rankle\',  # 6\n        \'Spine (H36M)\',  # 7\n        \'neck\',  # 8\n        \'Head (H36M)\',  # 9\n        \'headtop\',  # 10\n        \'lshoulder\',  # 11\n        \'lelbow\',  # 12\n        \'lwrist\',  # 13\n        \'rshoulder\',  # 14\n        \'relbow\',  # 15\n        \'rwrist\',  # 16\n    ]\n\ndef get_spin_skeleton():\n    return np.array(\n        [\n            [0 , 1],\n            [1 , 2],\n            [2 , 3],\n            [3 , 4],\n            [1 , 5],\n            [5 , 6],\n            [6 , 7],\n            [1 , 8],\n            [8 , 9],\n            [9 ,10],\n            [10,11],\n            [8 ,12],\n            [12,13],\n            [13,14],\n            [0 ,15],\n            [0 ,16],\n            [15,17],\n            [16,18],\n            [21,19],\n            [19,20],\n            [14,21],\n            [11,24],\n            [24,22],\n            [22,23],\n            [0 ,38],\n        ]\n    )\n\ndef get_posetrack_joint_names():\n    return [\n        ""nose"",\n        ""neck"",\n        ""headtop"",\n        ""lear"",\n        ""rear"",\n        ""lshoulder"",\n        ""rshoulder"",\n        ""lelbow"",\n        ""relbow"",\n        ""lwrist"",\n        ""rwrist"",\n        ""lhip"",\n        ""rhip"",\n        ""lknee"",\n        ""rknee"",\n        ""lankle"",\n        ""rankle""\n    ]\n\ndef get_posetrack_original_kp_names():\n    return [\n        \'nose\',\n        \'head_bottom\',\n        \'head_top\',\n        \'left_ear\',\n        \'right_ear\',\n        \'left_shoulder\',\n        \'right_shoulder\',\n        \'left_elbow\',\n        \'right_elbow\',\n        \'left_wrist\',\n        \'right_wrist\',\n        \'left_hip\',\n        \'right_hip\',\n        \'left_knee\',\n        \'right_knee\',\n        \'left_ankle\',\n        \'right_ankle\'\n    ]\n\ndef get_pennaction_joint_names():\n   return [\n       ""headtop"",   # 0\n       ""lshoulder"", # 1\n       ""rshoulder"", # 2\n       ""lelbow"",    # 3\n       ""relbow"",    # 4\n       ""lwrist"",    # 5\n       ""rwrist"",    # 6\n       ""lhip"" ,     # 7\n       ""rhip"" ,     # 8\n       ""lknee"",     # 9\n       ""rknee"" ,    # 10\n       ""lankle"",    # 11\n       ""rankle""     # 12\n   ]\n\ndef get_common_joint_names():\n    return [\n        ""rankle"",    # 0  ""lankle"",    # 0\n        ""rknee"",     # 1  ""lknee"",     # 1\n        ""rhip"",      # 2  ""lhip"",      # 2\n        ""lhip"",      # 3  ""rhip"",      # 3\n        ""lknee"",     # 4  ""rknee"",     # 4\n        ""lankle"",    # 5  ""rankle"",    # 5\n        ""rwrist"",    # 6  ""lwrist"",    # 6\n        ""relbow"",    # 7  ""lelbow"",    # 7\n        ""rshoulder"", # 8  ""lshoulder"", # 8\n        ""lshoulder"", # 9  ""rshoulder"", # 9\n        ""lelbow"",    # 10  ""relbow"",    # 10\n        ""lwrist"",    # 11  ""rwrist"",    # 11\n        ""neck"",      # 12  ""neck"",      # 12\n        ""headtop"",   # 13  ""headtop"",   # 13\n    ]\n\ndef get_common_skeleton():\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 1, 2 ],\n            [ 3, 4 ],\n            [ 4, 5 ],\n            [ 6, 7 ],\n            [ 7, 8 ],\n            [ 8, 2 ],\n            [ 8, 9 ],\n            [ 9, 3 ],\n            [ 2, 3 ],\n            [ 8, 12],\n            [ 9, 10],\n            [12, 9 ],\n            [10, 11],\n            [12, 13],\n        ]\n    )\n\ndef get_coco_joint_names():\n    return [\n        ""nose"",      # 0\n        ""leye"",      # 1\n        ""reye"",      # 2\n        ""lear"",      # 3\n        ""rear"",      # 4\n        ""lshoulder"", # 5\n        ""rshoulder"", # 6\n        ""lelbow"",    # 7\n        ""relbow"",    # 8\n        ""lwrist"",    # 9\n        ""rwrist"",    # 10\n        ""lhip"",      # 11\n        ""rhip"",      # 12\n        ""lknee"",     # 13\n        ""rknee"",     # 14\n        ""lankle"",    # 15\n        ""rankle"",    # 16\n    ]\n\ndef get_coco_skeleton():\n    # 0  - nose,\n    # 1  - leye,\n    # 2  - reye,\n    # 3  - lear,\n    # 4  - rear,\n    # 5  - lshoulder,\n    # 6  - rshoulder,\n    # 7  - lelbow,\n    # 8  - relbow,\n    # 9  - lwrist,\n    # 10 - rwrist,\n    # 11 - lhip,\n    # 12 - rhip,\n    # 13 - lknee,\n    # 14 - rknee,\n    # 15 - lankle,\n    # 16 - rankle,\n    return np.array(\n        [\n            [15, 13],\n            [13, 11],\n            [16, 14],\n            [14, 12],\n            [11, 12],\n            [ 5, 11],\n            [ 6, 12],\n            [ 5, 6 ],\n            [ 5, 7 ],\n            [ 6, 8 ],\n            [ 7, 9 ],\n            [ 8, 10],\n            [ 1, 2 ],\n            [ 0, 1 ],\n            [ 0, 2 ],\n            [ 1, 3 ],\n            [ 2, 4 ],\n            [ 3, 5 ],\n            [ 4, 6 ]\n        ]\n    )\n\ndef get_mpii_joint_names():\n    return [\n        ""rankle"",    # 0\n        ""rknee"",     # 1\n        ""rhip"",      # 2\n        ""lhip"",      # 3\n        ""lknee"",     # 4\n        ""lankle"",    # 5\n        ""hip"",       # 6\n        ""thorax"",    # 7\n        ""neck"",      # 8\n        ""headtop"",   # 9\n        ""rwrist"",    # 10\n        ""relbow"",    # 11\n        ""rshoulder"", # 12\n        ""lshoulder"", # 13\n        ""lelbow"",    # 14\n        ""lwrist"",    # 15\n    ]\n\ndef get_mpii_skeleton():\n    # 0  - rankle,\n    # 1  - rknee,\n    # 2  - rhip,\n    # 3  - lhip,\n    # 4  - lknee,\n    # 5  - lankle,\n    # 6  - hip,\n    # 7  - thorax,\n    # 8  - neck,\n    # 9  - headtop,\n    # 10 - rwrist,\n    # 11 - relbow,\n    # 12 - rshoulder,\n    # 13 - lshoulder,\n    # 14 - lelbow,\n    # 15 - lwrist,\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 1, 2 ],\n            [ 2, 6 ],\n            [ 6, 3 ],\n            [ 3, 4 ],\n            [ 4, 5 ],\n            [ 6, 7 ],\n            [ 7, 8 ],\n            [ 8, 9 ],\n            [ 7, 12],\n            [12, 11],\n            [11, 10],\n            [ 7, 13],\n            [13, 14],\n            [14, 15]\n        ]\n    )\n\ndef get_aich_joint_names():\n    return [\n        ""rshoulder"", # 0\n        ""relbow"",    # 1\n        ""rwrist"",    # 2\n        ""lshoulder"", # 3\n        ""lelbow"",    # 4\n        ""lwrist"",    # 5\n        ""rhip"",      # 6\n        ""rknee"",     # 7\n        ""rankle"",    # 8\n        ""lhip"",      # 9\n        ""lknee"",     # 10\n        ""lankle"",    # 11\n        ""headtop"",   # 12\n        ""neck"",      # 13\n    ]\n\ndef get_aich_skeleton():\n    # 0  - rshoulder,\n    # 1  - relbow,\n    # 2  - rwrist,\n    # 3  - lshoulder,\n    # 4  - lelbow,\n    # 5  - lwrist,\n    # 6  - rhip,\n    # 7  - rknee,\n    # 8  - rankle,\n    # 9  - lhip,\n    # 10 - lknee,\n    # 11 - lankle,\n    # 12 - headtop,\n    # 13 - neck,\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 1, 2 ],\n            [ 3, 4 ],\n            [ 4, 5 ],\n            [ 6, 7 ],\n            [ 7, 8 ],\n            [ 9, 10],\n            [10, 11],\n            [12, 13],\n            [13, 0 ],\n            [13, 3 ],\n            [ 0, 6 ],\n            [ 3, 9 ]\n        ]\n    )\n\ndef get_3dpw_joint_names():\n    return [\n        ""nose"",      # 0\n        ""thorax"",    # 1\n        ""rshoulder"", # 2\n        ""relbow"",    # 3\n        ""rwrist"",    # 4\n        ""lshoulder"", # 5\n        ""lelbow"",    # 6\n        ""lwrist"",    # 7\n        ""rhip"",      # 8\n        ""rknee"",     # 9\n        ""rankle"",    # 10\n        ""lhip"",      # 11\n        ""lknee"",     # 12\n        ""lankle"",    # 13\n    ]\n\ndef get_3dpw_skeleton():\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 1, 2 ],\n            [ 2, 3 ],\n            [ 3, 4 ],\n            [ 1, 5 ],\n            [ 5, 6 ],\n            [ 6, 7 ],\n            [ 2, 8 ],\n            [ 5, 11],\n            [ 8, 11],\n            [ 8, 9 ],\n            [ 9, 10],\n            [11, 12],\n            [12, 13]\n        ]\n    )\n\ndef get_smplcoco_joint_names():\n    return [\n        ""rankle"",    # 0\n        ""rknee"",     # 1\n        ""rhip"",      # 2\n        ""lhip"",      # 3\n        ""lknee"",     # 4\n        ""lankle"",    # 5\n        ""rwrist"",    # 6\n        ""relbow"",    # 7\n        ""rshoulder"", # 8\n        ""lshoulder"", # 9\n        ""lelbow"",    # 10\n        ""lwrist"",    # 11\n        ""neck"",      # 12\n        ""headtop"",   # 13\n        ""nose"",      # 14\n        ""leye"",      # 15\n        ""reye"",      # 16\n        ""lear"",      # 17\n        ""rear"",      # 18\n    ]\n\ndef get_smplcoco_skeleton():\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 1, 2 ],\n            [ 3, 4 ],\n            [ 4, 5 ],\n            [ 6, 7 ],\n            [ 7, 8 ],\n            [ 8, 12],\n            [12, 9 ],\n            [ 9, 10],\n            [10, 11],\n            [12, 13],\n            [14, 15],\n            [15, 17],\n            [16, 18],\n            [14, 16],\n            [ 8, 2 ],\n            [ 9, 3 ],\n            [ 2, 3 ],\n        ]\n    )\n\ndef get_smpl_joint_names():\n    return [\n        \'hips\',            # 0\n        \'leftUpLeg\',       # 1\n        \'rightUpLeg\',      # 2\n        \'spine\',           # 3\n        \'leftLeg\',         # 4\n        \'rightLeg\',        # 5\n        \'spine1\',          # 6\n        \'leftFoot\',        # 7\n        \'rightFoot\',       # 8\n        \'spine2\',          # 9\n        \'leftToeBase\',     # 10\n        \'rightToeBase\',    # 11\n        \'neck\',            # 12\n        \'leftShoulder\',    # 13\n        \'rightShoulder\',   # 14\n        \'head\',            # 15\n        \'leftArm\',         # 16\n        \'rightArm\',        # 17\n        \'leftForeArm\',     # 18\n        \'rightForeArm\',    # 19\n        \'leftHand\',        # 20\n        \'rightHand\',       # 21\n        \'leftHandIndex1\',  # 22\n        \'rightHandIndex1\', # 23\n    ]\n\ndef get_smpl_skeleton():\n    return np.array(\n        [\n            [ 0, 1 ],\n            [ 0, 2 ],\n            [ 0, 3 ],\n            [ 1, 4 ],\n            [ 2, 5 ],\n            [ 3, 6 ],\n            [ 4, 7 ],\n            [ 5, 8 ],\n            [ 6, 9 ],\n            [ 7, 10],\n            [ 8, 11],\n            [ 9, 12],\n            [ 9, 13],\n            [ 9, 14],\n            [12, 15],\n            [13, 16],\n            [14, 17],\n            [16, 18],\n            [17, 19],\n            [18, 20],\n            [19, 21],\n            [20, 22],\n            [21, 23],\n        ]\n    )'"
lib/data_utils/mpii3d_utils.py,0,"b'import os\nimport cv2\nimport glob\nimport h5py\nimport json\nimport joblib\nimport argparse\nimport numpy as np\nfrom tqdm import tqdm\nimport os.path as osp\nimport scipy.io as sio\n\nfrom lib.models import spin\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.utils.utils import tqdm_enumerate\nfrom lib.data_utils.kp_utils import convert_kps\nfrom lib.data_utils.img_utils import get_bbox_from_kp2d\nfrom lib.data_utils.feature_extractor import extract_features\n\n\ndef read_openpose(json_file, gt_part, dataset):\n    # get only the arms/legs joints\n    op_to_12 = [11, 10, 9, 12, 13, 14, 4, 3, 2, 5, 6, 7]\n    # read the openpose detection\n    json_data = json.load(open(json_file, \'r\'))\n    people = json_data[\'people\']\n    if len(people) == 0:\n        # no openpose detection\n        keyp25 = np.zeros([25,3])\n    else:\n        # size of person in pixels\n        scale = max(max(gt_part[:,0])-min(gt_part[:,0]),max(gt_part[:,1])-min(gt_part[:,1]))\n        # go through all people and find a match\n        dist_conf = np.inf*np.ones(len(people))\n        for i, person in enumerate(people):\n            # openpose keypoints\n            op_keyp25 = np.reshape(person[\'pose_keypoints_2d\'], [25,3])\n            op_keyp12 = op_keyp25[op_to_12, :2]\n            op_conf12 = op_keyp25[op_to_12, 2:3] > 0\n            # all the relevant joints should be detected\n            if min(op_conf12) > 0:\n                # weighted distance of keypoints\n                dist_conf[i] = np.mean(np.sqrt(np.sum(op_conf12*(op_keyp12 - gt_part[:12, :2])**2, axis=1)))\n        # closest match\n        p_sel = np.argmin(dist_conf)\n        # the exact threshold is not super important but these are the values we used\n        if dataset == \'mpii\':\n            thresh = 30\n        elif dataset == \'coco\':\n            thresh = 10\n        else:\n            thresh = 0\n        # dataset-specific thresholding based on pixel size of person\n        if min(dist_conf)/scale > 0.1 and min(dist_conf) < thresh:\n            keyp25 = np.zeros([25,3])\n        else:\n            keyp25 = np.reshape(people[p_sel][\'pose_keypoints_2d\'], [25,3])\n    return keyp25\n\n\ndef read_calibration(calib_file, vid_list):\n    Ks, Rs, Ts = [], [], []\n    file = open(calib_file, \'r\')\n    content = file.readlines()\n    for vid_i in vid_list:\n        K = np.array([float(s) for s in content[vid_i * 7 + 5][11:-2].split()])\n        K = np.reshape(K, (4, 4))\n        RT = np.array([float(s) for s in content[vid_i * 7 + 6][11:-2].split()])\n        RT = np.reshape(RT, (4, 4))\n        R = RT[:3, :3]\n        T = RT[:3, 3] / 1000\n        Ks.append(K)\n        Rs.append(R)\n        Ts.append(T)\n    return Ks, Rs, Ts\n\n\ndef read_data_train(dataset_path, debug=False):\n    h, w = 2048, 2048\n    dataset = {\n        \'vid_name\': [],\n        \'frame_id\': [],\n        \'joints3D\': [],\n        \'joints2D\': [],\n        \'bbox\': [],\n        \'img_name\': [],\n        \'features\': [],\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    # training data\n    user_list = range(1, 9)\n    seq_list = range(1, 3)\n    vid_list = list(range(3)) + list(range(4, 9))\n\n    # product = product(user_list, seq_list, vid_list)\n    # user_i, seq_i, vid_i = product[process_id]\n\n    for user_i in user_list:\n        for seq_i in seq_list:\n            seq_path = os.path.join(dataset_path,\n                                    \'S\' + str(user_i),\n                                    \'Seq\' + str(seq_i))\n            # mat file with annotations\n            annot_file = os.path.join(seq_path, \'annot.mat\')\n            annot2 = sio.loadmat(annot_file)[\'annot2\']\n            annot3 = sio.loadmat(annot_file)[\'annot3\']\n            # calibration file and camera parameters\n            for j, vid_i in enumerate(vid_list):\n                # image folder\n                imgs_path = os.path.join(seq_path,\n                                         \'video_\' + str(vid_i))\n                # per frame\n                pattern = os.path.join(imgs_path, \'*.jpg\')\n                img_list = sorted(glob.glob(pattern))\n                vid_used_frames = []\n                vid_used_joints = []\n                vid_used_bbox = []\n                vid_segments = []\n                vid_uniq_id = ""subj"" + str(user_i) + \'_seq\' + str(seq_i) + ""_vid"" + str(vid_i) + ""_seg0""\n                for i, img_i in tqdm_enumerate(img_list):\n\n                    # for each image we store the relevant annotations\n                    img_name = img_i.split(\'/\')[-1]\n                    joints_2d_raw = np.reshape(annot2[vid_i][0][i], (1, 28, 2))\n                    joints_2d_raw= np.append(joints_2d_raw, np.ones((1,28,1)), axis=2)\n                    joints_2d = convert_kps(joints_2d_raw, ""mpii3d"",  ""spin"").reshape((-1,3))\n\n                    # visualize = True\n                    # if visualize == True and i == 500:\n                    #     import matplotlib.pyplot as plt\n                    #\n                    #     frame = cv2.cvtColor(cv2.imread(img_i), cv2.COLOR_BGR2RGB)\n                    #\n                    #     for k in range(49):\n                    #         kp = joints_2d[k]\n                    #\n                    #         frame = cv2.circle(\n                    #             frame.copy(),\n                    #             (int(kp[0]), int(kp[1])),\n                    #             thickness=3,\n                    #             color=(255, 0, 0),\n                    #             radius=5,\n                    #         )\n                    #\n                    #         cv2.putText(frame, f\'{k}\', (int(kp[0]), int(kp[1]) + 1), cv2.FONT_HERSHEY_SIMPLEX, 1.5,\n                    #                     (0, 255, 0),\n                    #                     thickness=3)\n                    #\n                    #     plt.imshow(frame)\n                    #     plt.show()\n\n                    joints_3d_raw = np.reshape(annot3[vid_i][0][i], (1, 28, 3)) / 1000\n                    joints_3d = convert_kps(joints_3d_raw, ""mpii3d"", ""spin"").reshape((-1,3))\n\n                    bbox = get_bbox_from_kp2d(joints_2d[~np.all(joints_2d == 0, axis=1)]).reshape(4)\n\n                    joints_3d = joints_3d - joints_3d[39]  # 4 is the root\n\n                    # check that all joints are visible\n                    x_in = np.logical_and(joints_2d[:, 0] < w, joints_2d[:, 0] >= 0)\n                    y_in = np.logical_and(joints_2d[:, 1] < h, joints_2d[:, 1] >= 0)\n                    ok_pts = np.logical_and(x_in, y_in)\n                    if np.sum(ok_pts) < joints_2d.shape[0]:\n                        vid_uniq_id = ""_"".join(vid_uniq_id.split(""_"")[:-1])+ ""_seg"" +\\\n                                          str(int(dataset[\'vid_name\'][-1].split(""_"")[-1][3:])+1)\n                        continue\n\n                    dataset[\'vid_name\'].append(vid_uniq_id)\n                    dataset[\'frame_id\'].append(img_name.split(""."")[0])\n                    dataset[\'img_name\'].append(img_i)\n                    dataset[\'joints2D\'].append(joints_2d)\n                    dataset[\'joints3D\'].append(joints_3d)\n                    dataset[\'bbox\'].append(bbox)\n                    vid_segments.append(vid_uniq_id)\n                    vid_used_frames.append(img_i)\n                    vid_used_joints.append(joints_2d)\n                    vid_used_bbox.append(bbox)\n\n                vid_segments= np.array(vid_segments)\n                ids = np.zeros((len(set(vid_segments))+1))\n                ids[-1] = len(vid_used_frames) + 1\n                if (np.where(vid_segments[:-1] != vid_segments[1:])[0]).size != 0:\n                    ids[1:-1] = (np.where(vid_segments[:-1] != vid_segments[1:])[0]) + 1\n\n                for i in tqdm(range(len(set(vid_segments)))):\n                    features = extract_features(model, np.array(vid_used_frames)[int(ids[i]):int(ids[i+1])],\n                                                vid_used_bbox[int(ids[i]):int((ids[i+1]))],\n                                                kp_2d=np.array(vid_used_joints)[int(ids[i]):int(ids[i+1])],\n                                                dataset=\'spin\', debug=False)\n                    dataset[\'features\'].append(features)\n\n    for k in dataset.keys():\n        dataset[k] = np.array(dataset[k])\n    dataset[\'features\'] = np.concatenate(dataset[\'features\'])\n\n    return dataset\n\n\ndef read_test_data(dataset_path):\n\n    dataset = {\n        \'vid_name\': [],\n        \'frame_id\': [],\n        \'joints3D\': [],\n        \'joints2D\': [],\n        \'bbox\': [],\n        \'img_name\': [],\n        \'features\': [],\n        ""valid_i"": []\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    user_list = range(1, 7)\n\n    for user_i in user_list:\n        print(\'Subject\', user_i)\n        seq_path = os.path.join(dataset_path,\n                                \'mpi_inf_3dhp_test_set\',\n                                \'TS\' + str(user_i))\n        # mat file with annotations\n        annot_file = os.path.join(seq_path, \'annot_data.mat\')\n        mat_as_h5 = h5py.File(annot_file, \'r\')\n        annot2 = np.array(mat_as_h5[\'annot2\'])\n        annot3 = np.array(mat_as_h5[\'univ_annot3\'])\n        valid = np.array(mat_as_h5[\'valid_frame\'])\n\n        vid_used_frames = []\n        vid_used_joints = []\n        vid_used_bbox = []\n        vid_segments = []\n        vid_uniq_id = ""subj"" + str(user_i) + ""_seg0""\n\n\n        for frame_i, valid_i in tqdm(enumerate(valid)):\n\n            img_i = os.path.join(\'mpi_inf_3dhp_test_set\',\n                                    \'TS\' + str(user_i),\n                                    \'imageSequence\',\n                                    \'img_\' + str(frame_i + 1).zfill(6) + \'.jpg\')\n\n            joints_2d_raw = np.expand_dims(annot2[frame_i, 0, :, :], axis = 0)\n            joints_2d_raw = np.append(joints_2d_raw, np.ones((1, 17, 1)), axis=2)\n\n\n            joints_2d = convert_kps(joints_2d_raw, src=""mpii3d_test"", dst=""spin"").reshape((-1, 3))\n\n            # visualize = True\n            # if visualize == True:\n            #     import matplotlib.pyplot as plt\n            #\n            #     frame = cv2.cvtColor(cv2.imread(os.path.join(dataset_path, img_i)), cv2.COLOR_BGR2RGB)\n            #\n            #     for k in range(49):\n            #         kp = joints_2d[k]\n            #\n            #         frame = cv2.circle(\n            #             frame.copy(),\n            #             (int(kp[0]), int(kp[1])),\n            #             thickness=3,\n            #             color=(255, 0, 0),\n            #             radius=5,\n            #         )\n            #\n            #         cv2.putText(frame, f\'{k}\', (int(kp[0]), int(kp[1]) + 1), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0),\n            #                     thickness=3)\n            #\n            #     plt.imshow(frame)\n            #     plt.show()\n\n            joints_3d_raw = np.reshape(annot3[frame_i, 0, :, :], (1, 17, 3)) / 1000\n            joints_3d = convert_kps(joints_3d_raw, ""mpii3d_test"", ""spin"").reshape((-1, 3))\n            joints_3d = joints_3d - joints_3d[39] # substract pelvis zero is the root for test\n\n            bbox = get_bbox_from_kp2d(joints_2d[~np.all(joints_2d == 0, axis=1)]).reshape(4)\n\n\n            # check that all joints are visible\n            img_file = os.path.join(dataset_path, img_i)\n            I = cv2.imread(img_file)\n            h, w, _ = I.shape\n            x_in = np.logical_and(joints_2d[:, 0] < w, joints_2d[:, 0] >= 0)\n            y_in = np.logical_and(joints_2d[:, 1] < h, joints_2d[:, 1] >= 0)\n            ok_pts = np.logical_and(x_in, y_in)\n\n            if np.sum(ok_pts) < joints_2d.shape[0]:\n                vid_uniq_id = ""_"".join(vid_uniq_id.split(""_"")[:-1]) + ""_seg"" + \\\n                              str(int(dataset[\'vid_name\'][-1].split(""_"")[-1][3:]) + 1)\n                continue\n\n\n            dataset[\'vid_name\'].append(vid_uniq_id)\n            dataset[\'frame_id\'].append(img_file.split(""/"")[-1].split(""."")[0])\n            dataset[\'img_name\'].append(img_file)\n            dataset[\'joints2D\'].append(joints_2d)\n            dataset[\'joints3D\'].append(joints_3d)\n            dataset[\'bbox\'].append(bbox)\n            dataset[\'valid_i\'].append(valid_i)\n\n            vid_segments.append(vid_uniq_id)\n            vid_used_frames.append(img_file)\n            vid_used_joints.append(joints_2d)\n            vid_used_bbox.append(bbox)\n\n        vid_segments = np.array(vid_segments)\n        ids = np.zeros((len(set(vid_segments)) + 1))\n        ids[-1] = len(vid_used_frames) + 1\n        if (np.where(vid_segments[:-1] != vid_segments[1:])[0]).size != 0:\n            ids[1:-1] = (np.where(vid_segments[:-1] != vid_segments[1:])[0]) + 1\n\n        for i in tqdm(range(len(set(vid_segments)))):\n            features = extract_features(model, np.array(vid_used_frames)[int(ids[i]):int(ids[i + 1])],\n                                        vid_used_bbox[int(ids[i]):int(ids[i + 1])],\n                                        kp_2d=np.array(vid_used_joints)[int(ids[i]):int(ids[i + 1])],\n                                        dataset=\'spin\', debug=False)\n            dataset[\'features\'].append(features)\n\n    for k in dataset.keys():\n        dataset[k] = np.array(dataset[k])\n    dataset[\'features\'] = np.concatenate(dataset[\'features\'])\n\n    return dataset\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--dir\', type=str, help=\'dataset directory\', default=\'data/mpii_3d\')\n    args = parser.parse_args()\n\n    dataset = read_test_data(args.dir)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, \'mpii3d_val_db.pt\'))\n\n    dataset = read_data_train(args.dir)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, \'mpii3d_train_db.pt\'))\n\n\n\n'"
lib/data_utils/penn_action_utils.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport sys\nsys.path.append('.')\n\nimport glob\nimport torch\nimport joblib\nimport argparse\nfrom tqdm import tqdm\nimport os.path as osp\nfrom skimage import io\nfrom scipy.io import loadmat\n\nfrom lib.models import spin\nfrom lib.data_utils.kp_utils import *\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.img_utils import get_bbox_from_kp2d\nfrom lib.data_utils.feature_extractor import extract_features\n\n\ndef calc_kpt_bound(kp_2d):\n    MAX_COORD = 10000\n    x = kp_2d[:, 0]\n    y = kp_2d[:, 1]\n    z = kp_2d[:, 2]\n    u = MAX_COORD\n    d = -1\n    l = MAX_COORD\n    r = -1\n    for idx, vis in enumerate(z):\n        if vis == 0:  # skip invisible joint\n            continue\n        u = min(u, y[idx])\n        d = max(d, y[idx])\n        l = min(l, x[idx])\n        r = max(r, x[idx])\n    return u, d, l, r\n\n\ndef load_mat(path):\n    mat = loadmat(path)\n    del mat['pose'], mat['__header__'], mat['__globals__'], mat['__version__'], mat['train'], mat['action']\n    mat['nframes'] = mat['nframes'][0][0]\n\n    return mat\n\n\ndef read_data(folder):\n    dataset = {\n        'img_name' : [],\n        'joints2D': [],\n        'bbox': [],\n        'vid_name': [],\n        'features': [],\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    file_names = sorted(glob.glob(folder + '/labels/'+'*.mat'))\n\n    for fname in tqdm(file_names):\n        vid_dict=load_mat(fname)\n        imgs = sorted(glob.glob(folder + '/frames/'+ fname.strip().split('/')[-1].split('.')[0]+'/*.jpg'))\n        kp_2d = np.zeros((vid_dict['nframes'], 13, 3))\n        perm_idxs = get_perm_idxs('pennaction', 'common')\n\n        kp_2d[:, :, 0] = vid_dict['x']\n        kp_2d[:, :, 1] = vid_dict['y']\n        kp_2d[:, :, 2] = vid_dict['visibility']\n        kp_2d = kp_2d[:, perm_idxs, :]\n\n        # fix inconsistency\n        n_kp_2d = np.zeros((kp_2d.shape[0], 14, 3))\n        n_kp_2d[:, :12, :] = kp_2d[:, :-1, :]\n        n_kp_2d[:, 13, :] = kp_2d[:, 12, :]\n        kp_2d = n_kp_2d\n\n        bbox = np.zeros((vid_dict['nframes'], 4))\n\n        for fr_id, fr in enumerate(kp_2d):\n            u, d, l, r = calc_kpt_bound(fr)\n            center = np.array([(l + r) * 0.5, (u + d) * 0.5], dtype=np.float32)\n            c_x, c_y = center[0], center[1]\n            w, h = r - l, d - u\n            w = h = np.where(w / h > 1, w, h)\n\n            bbox[fr_id,:] = np.array([c_x, c_y, w, h])\n\n        dataset['vid_name'].append(np.array([f'{fname}']* vid_dict['nframes']))\n        dataset['img_name'].append(np.array(imgs))\n        dataset['joints2D'].append(kp_2d)\n        dataset['bbox'].append(bbox)\n\n        features = extract_features(model, np.array(imgs) , bbox, dataset='pennaction', debug=False)\n        dataset['features'].append(features)\n\n    for k in dataset.keys():\n        dataset[k] = np.array(dataset[k])\n    for k in dataset.keys():\n        dataset[k] = np.concatenate(dataset[k])\n\n    return dataset\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dir', type=str, help='dataset directory', default='data/pennaction')\n    args = parser.parse_args()\n\n    dataset = read_data(args.dir)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, 'pennaction_train_db.pt'))\n\n"""
lib/data_utils/posetrack_utils.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport sys\nsys.path.append('.')\n\nimport glob\nimport joblib\nimport argparse\nimport numpy as np\nimport json\nimport os.path as osp\n\nfrom lib.models import spin\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.utils.utils import tqdm_enumerate\nfrom lib.data_utils.feature_extractor import extract_features\nfrom lib.data_utils.kp_utils import get_posetrack_original_kp_names, convert_kps\n\ndef read_data(folder, set):\n    dataset = {\n        'img_name' : [] ,\n        'joints2D': [],\n        'bbox': [],\n        'vid_name': [],\n        'features': [],\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    file_names = glob.glob(osp.join(folder, 'posetrack_data/annotations/', f'{set}/*.json'))\n    file_names = sorted(file_names)\n    nn_corrupted = 0\n    tot_frames = 0\n    min_frame_number = 8\n\n    for fid,fname in tqdm_enumerate(file_names):\n        if fname == osp.join(folder, 'annotations/train/021133_mpii_train.json'):\n            continue\n\n        with open(fname, 'r') as entry:\n            anns = json.load(entry)\n        # num_frames = anns['images'][0]['nframes']\n        anns['images'] = [item for item in anns['images'] if item['is_labeled'] ]\n        num_frames = len(anns['images'])\n        frame2imgname = dict()\n        for el in anns['images']:\n            frame2imgname[el['frame_id']] = el['file_name']\n\n        num_people = -1\n        for x in anns['annotations']:\n            if num_people < x['track_id']:\n                num_people = x['track_id']\n        num_people += 1\n        posetrack_joints = get_posetrack_original_kp_names()\n        idxs = [anns['categories'][0]['keypoints'].index(h) for h in posetrack_joints if h in anns['categories'][0]['keypoints']]\n        for x in anns['annotations']:\n            kps = np.array(x['keypoints']).reshape((17,3))\n            kps = kps[idxs,:]\n            x['keypoints'] = list(kps.flatten())\n\n        tot_frames += num_people * num_frames\n        for p_id in range(num_people):\n\n            annot_pid = [(item['keypoints'], item['bbox'], item['image_id'])\n                         for item in anns['annotations']\n                         if item['track_id'] == p_id and not(np.count_nonzero(item['keypoints']) == 0)  ]\n\n            if len(annot_pid) < min_frame_number:\n                nn_corrupted += len(annot_pid)\n                continue\n\n            bbox = np.zeros((len(annot_pid),4))\n            # perm_idxs = get_perm_idxs('posetrack', 'common')\n            kp_2d = np.zeros((len(annot_pid), len(annot_pid[0][0])//3 ,3))\n            img_paths = np.zeros((len(annot_pid)))\n\n            for i, (key2djnts, bbox_p, image_id) in enumerate(annot_pid):\n\n                if (bbox_p[2]==0 or bbox_p[3]==0) :\n                    nn_corrupted +=1\n                    continue\n\n                img_paths[i] = image_id\n                key2djnts[2::3] = len(key2djnts[2::3])*[1]\n\n                kp_2d[i,:] = np.array(key2djnts).reshape(int(len(key2djnts)/3),3) # [perm_idxs, :]\n                for kp_loc in kp_2d[i,:]:\n                    if kp_loc[0] == 0 and kp_loc[1] == 0:\n                        kp_loc[2] = 0\n\n\n                x_tl = bbox_p[0]\n                y_tl = bbox_p[1]\n                w = bbox_p[2]\n                h = bbox_p[3]\n                bbox_p[0] = x_tl + w / 2\n                bbox_p[1] = y_tl + h / 2\n                #\n\n                w = h = np.where(w / h > 1, w, h)\n                w = h = h * 0.8\n                bbox_p[2] = w\n                bbox_p[3] = h\n                bbox[i, :] = bbox_p\n\n            img_paths = list(img_paths)\n            img_paths = [osp.join(folder, frame2imgname[item]) if item != 0 else 0 for item in img_paths ]\n\n            bbx_idxs = []\n            for bbx_id, bbx in enumerate(bbox):\n                if np.count_nonzero(bbx) == 0:\n                    bbx_idxs += [bbx_id]\n\n            kp_2d = np.delete(kp_2d, bbx_idxs, 0)\n            img_paths = np.delete(np.array(img_paths), bbx_idxs, 0)\n            bbox = np.delete(bbox, np.where(~bbox.any(axis=1))[0], axis=0)\n\n            # Convert to common 2d keypoint format\n            if bbox.size == 0 or bbox.shape[0] < min_frame_number:\n                nn_corrupted += 1\n                continue\n\n            kp_2d = convert_kps(kp_2d, src='posetrack', dst='spin')\n\n            dataset['vid_name'].append(np.array([f'{fname}_{p_id}']*img_paths.shape[0]))\n            dataset['img_name'].append(np.array(img_paths))\n            dataset['joints2D'].append(kp_2d)\n            dataset['bbox'].append(np.array(bbox))\n\n            # compute_features\n            features = extract_features(\n                model,\n                np.array(img_paths),\n                bbox,\n                kp_2d=kp_2d,\n                dataset='spin',\n                debug=False,\n            )\n\n            assert kp_2d.shape[0] == img_paths.shape[0] == bbox.shape[0]\n\n            dataset['features'].append(features)\n\n\n    print(nn_corrupted, tot_frames)\n    for k in dataset.keys():\n        dataset[k] = np.array(dataset[k])\n\n    for k in dataset.keys():\n        dataset[k] = np.concatenate(dataset[k])\n\n    for k,v in dataset.items():\n        print(k, v.shape)\n\n    return dataset\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dir', type=str, help='dataset directory', default='data/posetrack')\n    args = parser.parse_args()\n\n    dataset_train = read_data(args.dir, 'train')\n    joblib.dump(dataset_train, osp.join(VIBE_DB_DIR, 'posetrack_train_db.pt'))\n"""
lib/data_utils/threedpw_utils.py,7,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport sys\nsys.path.append('.')\n\nimport os\nimport cv2\nimport torch\nimport joblib\nimport argparse\nimport numpy as np\nimport pickle as pkl\nimport os.path as osp\nfrom tqdm import tqdm\n\nfrom lib.models import spin\nfrom lib.data_utils.kp_utils import *\nfrom lib.core.config import VIBE_DB_DIR, VIBE_DATA_DIR\nfrom lib.utils.smooth_bbox import get_smooth_bbox_params\nfrom lib.models.smpl import SMPL, SMPL_MODEL_DIR, H36M_TO_J14\nfrom lib.data_utils.feature_extractor import extract_features\nfrom lib.utils.geometry import batch_rodrigues, rotation_matrix_to_angle_axis\n\nNUM_JOINTS = 24\nVIS_THRESH = 0.3\nMIN_KP = 6\n\ndef read_data(folder, set, debug=False):\n\n    dataset = {\n        'vid_name': [],\n        'frame_id': [],\n        'joints3D': [],\n        'joints2D': [],\n        'shape': [],\n        'pose': [],\n        'bbox': [],\n        'img_name': [],\n        'features': [],\n        'valid': [],\n    }\n\n    model = spin.get_pretrained_hmr()\n\n    sequences = [x.split('.')[0] for x in os.listdir(osp.join(folder, 'sequenceFiles', set))]\n\n    J_regressor = None\n\n    smpl = SMPL(SMPL_MODEL_DIR, batch_size=1, create_transl=False)\n    if set == 'test' or set == 'validation':\n        J_regressor = torch.from_numpy(np.load(osp.join(VIBE_DATA_DIR, 'J_regressor_h36m.npy'))).float()\n\n    for i, seq in tqdm(enumerate(sequences)):\n\n        data_file = osp.join(folder, 'sequenceFiles', set, seq + '.pkl')\n\n        data = pkl.load(open(data_file, 'rb'), encoding='latin1')\n\n        img_dir = osp.join(folder, 'imageFiles', seq)\n\n        num_people = len(data['poses'])\n        num_frames = len(data['img_frame_ids'])\n        assert (data['poses2d'][0].shape[0] == num_frames)\n\n        for p_id in range(num_people):\n            pose = torch.from_numpy(data['poses'][p_id]).float()\n            shape = torch.from_numpy(data['betas'][p_id][:10]).float().repeat(pose.size(0), 1)\n            trans = torch.from_numpy(data['trans'][p_id]).float()\n            j2d = data['poses2d'][p_id].transpose(0,2,1)\n            cam_pose = data['cam_poses']\n            campose_valid = data['campose_valid'][p_id]\n\n            # ======== Align the mesh params ======== #\n            rot = pose[:, :3]\n            rot_mat = batch_rodrigues(rot)\n\n            Rc = torch.from_numpy(cam_pose[:, :3, :3]).float()\n            Rs = torch.bmm(Rc, rot_mat.reshape(-1, 3, 3))\n            rot = rotation_matrix_to_angle_axis(Rs)\n            pose[:, :3] = rot\n            # ======== Align the mesh params ======== #\n\n            output = smpl(betas=shape, body_pose=pose[:,3:], global_orient=pose[:,:3], transl=trans)\n            # verts = output.vertices\n            j3d = output.joints\n\n            if J_regressor is not None:\n                vertices = output.vertices\n                J_regressor_batch = J_regressor[None, :].expand(vertices.shape[0], -1, -1).to(vertices.device)\n                j3d = torch.matmul(J_regressor_batch, vertices)\n                j3d = j3d[:, H36M_TO_J14, :]\n\n            img_paths = []\n            for i_frame in range(num_frames):\n                img_path = os.path.join(img_dir + '/image_{:05d}.jpg'.format(i_frame))\n                img_paths.append(img_path)\n\n            bbox_params, time_pt1, time_pt2 = get_smooth_bbox_params(j2d, vis_thresh=VIS_THRESH, sigma=8)\n\n            # process bbox_params\n            c_x = bbox_params[:,0]\n            c_y = bbox_params[:,1]\n            scale = bbox_params[:,2]\n            w = h = 150. / scale\n            w = h = h * 1.1\n            bbox = np.vstack([c_x,c_y,w,h]).T\n\n            # process keypoints\n            j2d[:, :, 2] = j2d[:, :, 2] > 0.3  # set the visibility flags\n            # Convert to common 2d keypoint format\n            perm_idxs = get_perm_idxs('3dpw', 'common')\n            perm_idxs += [0, 0]  # no neck, top head\n            j2d = j2d[:, perm_idxs]\n            j2d[:, 12:, 2] = 0.0\n\n            # print('j2d', j2d[time_pt1:time_pt2].shape)\n            # print('campose', campose_valid[time_pt1:time_pt2].shape)\n\n            img_paths_array = np.array(img_paths)[time_pt1:time_pt2]\n            dataset['vid_name'].append(np.array([f'{seq}_{p_id}']*num_frames)[time_pt1:time_pt2])\n            dataset['frame_id'].append(np.arange(0, num_frames)[time_pt1:time_pt2])\n            dataset['img_name'].append(img_paths_array)\n            dataset['joints3D'].append(j3d.numpy()[time_pt1:time_pt2])\n            dataset['joints2D'].append(j2d[time_pt1:time_pt2])\n            dataset['shape'].append(shape.numpy()[time_pt1:time_pt2])\n            dataset['pose'].append(pose.numpy()[time_pt1:time_pt2])\n            dataset['bbox'].append(bbox)\n            dataset['valid'].append(campose_valid[time_pt1:time_pt2])\n\n            features = extract_features(model, img_paths_array, bbox,\n                                        kp_2d=j2d[time_pt1:time_pt2], debug=debug, dataset='3dpw', scale=1.2)\n            dataset['features'].append(features)\n\n    for k in dataset.keys():\n        dataset[k] = np.concatenate(dataset[k])\n        print(k, dataset[k].shape)\n\n    # Filter out keypoints\n    indices_to_use = np.where((dataset['joints2D'][:, :, 2] > VIS_THRESH).sum(-1) > MIN_KP)[0]\n    for k in dataset.keys():\n        dataset[k] = dataset[k][indices_to_use]\n\n    return dataset\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--dir', type=str, help='dataset directory', default='data/3dpw')\n    args = parser.parse_args()\n\n    debug = False\n\n    dataset = read_data(args.dir, 'validation', debug=debug)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, '3dpw_val_db.pt'))\n\n    dataset = read_data(args.dir, 'train', debug=debug)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, '3dpw_train_db.pt'))\n\n    dataset = read_data(args.dir, 'test', debug=debug)\n    joblib.dump(dataset, osp.join(VIBE_DB_DIR, '3dpw_test_db.pt'))\n"""
lib/dataset/__init__.py,0,b'from .dataset_2d import Dataset2D\nfrom .dataset_3d import Dataset3D\n\nfrom .insta import Insta\nfrom .amass import AMASS\nfrom .mpii3d import MPII3D\nfrom .threedpw import ThreeDPW\nfrom .posetrack import PoseTrack\nfrom .penn_action import PennAction\n\n'
lib/dataset/amass.py,2,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport torch\nimport joblib\nimport numpy as np\nimport os.path as osp\nfrom torch.utils.data import Dataset\n\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.img_utils import split_into_chunks\n\nclass AMASS(Dataset):\n    def __init__(self, seqlen):\n        self.seqlen = seqlen\n\n        self.stride = seqlen\n\n        self.db = self.load_db()\n        self.vid_indices = split_into_chunks(self.db['vid_name'], self.seqlen, self.stride)\n        del self.db['vid_name']\n        print(f'AMASS dataset number of videos: {len(self.vid_indices)}')\n\n    def __len__(self):\n        return len(self.vid_indices)\n\n    def __getitem__(self, index):\n        return self.get_single_item(index)\n\n    def load_db(self):\n        db_file = osp.join(VIBE_DB_DIR, 'amass_db.pt')\n        db = joblib.load(db_file)\n        return db\n\n    def get_single_item(self, index):\n        start_index, end_index = self.vid_indices[index]\n        thetas = self.db['theta'][start_index:end_index+1]\n\n        cam = np.array([1., 0., 0.])[None, ...]\n        cam = np.repeat(cam, thetas.shape[0], axis=0)\n        theta = np.concatenate([cam, thetas], axis=-1)\n\n        target = {\n            'theta': torch.from_numpy(theta).float(),  # cam, pose and shape\n        }\n        return target\n\n\n\n"""
lib/dataset/dataset_2d.py,4,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport os.path as osp\nimport joblib\n\nfrom torch.utils.data import Dataset\n\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.kp_utils import convert_kps\nfrom lib.data_utils.img_utils import normalize_2d_kp, transfrom_keypoints, split_into_chunks\n\nlogger = logging.getLogger(__name__)\n\nclass Dataset2D(Dataset):\n    def __init__(self, seqlen, overlap=0.,\n                 folder=None, dataset_name=None, debug=False):\n\n        self.folder = folder\n        self.dataset_name = dataset_name\n        self.seqlen = seqlen\n        self.stride = int(seqlen * (1-overlap))\n        self.debug = debug\n        self.db = self.load_db()\n        self.vid_indices = split_into_chunks(self.db[\'vid_name\'], self.seqlen, self.stride)\n\n\n    def __len__(self):\n        return len(self.vid_indices)\n\n    def __getitem__(self, index):\n        return self.get_single_item(index)\n\n    def load_db(self):\n        set = \'train\'\n\n        db_file = osp.join(VIBE_DB_DIR, f\'{self.dataset_name}_{set}_db.pt\')\n\n        if osp.isfile(db_file):\n            db = joblib.load(db_file)\n        else:\n            raise ValueError(f\'{db_file} do not exists\')\n\n        print(f\'Loaded {self.dataset_name} dataset from {db_file}\')\n        return db\n\n    def get_single_item(self, index):\n        start_index, end_index = self.vid_indices[index]\n\n        kp_2d = self.db[\'joints2D\'][start_index:end_index+1]\n        if self.dataset_name != \'posetrack\':\n            kp_2d = convert_kps(kp_2d, src=self.dataset_name, dst=\'spin\')\n        kp_2d_tensor = np.ones((self.seqlen, 49, 3), dtype=np.float16)\n\n        bbox  = self.db[\'bbox\'][start_index:end_index+1]\n\n        input = torch.from_numpy(self.db[\'features\'][start_index:end_index+1]).float()\n\n\n        for idx in range(self.seqlen):\n            # crop image and transform 2d keypoints\n            kp_2d[idx,:,:2], trans = transfrom_keypoints(\n                kp_2d=kp_2d[idx,:,:2],\n                center_x=bbox[idx,0],\n                center_y=bbox[idx,1],\n                width=bbox[idx,2],\n                height=bbox[idx,3],\n                patch_width=224,\n                patch_height=224,\n                do_augment=False,\n            )\n\n            kp_2d[idx,:,:2] = normalize_2d_kp(kp_2d[idx,:,:2], 224)\n            kp_2d_tensor[idx] = kp_2d[idx]\n\n        vid_name = self.db[\'vid_name\'][start_index:end_index+1]\n        frame_id = self.db[\'img_name\'][start_index:end_index+1].astype(str)\n        instance_id = np.array([v+f for v,f in zip(vid_name, frame_id)])\n\n        target = {\n            \'features\': input,\n            \'kp_2d\': torch.from_numpy(kp_2d_tensor).float(), # 2D keypoints transformed according to bbox cropping\n            # \'instance_id\': instance_id,\n        }\n\n        if self.debug:\n            from lib.data_utils.img_utils import get_single_image_crop\n\n            vid_name = self.db[\'vid_name\'][start_index]\n\n            if self.dataset_name == \'pennaction\':\n                vid_folder = ""frames""\n                vid_name = vid_name.split(\'/\')[-1].split(\'.\')[0]\n                img_id = ""img_name""\n            elif self.dataset_name == \'posetrack\':\n                vid_folder = osp.join(\'images\', vid_name.split(\'/\')[-2])\n                vid_name = vid_name.split(\'/\')[-1].split(\'.\')[0]\n                img_id = ""img_name""\n            else:\n                vid_name = \'_\'.join(vid_name.split(\'_\')[:-1])\n                vid_folder = \'imageFiles\'\n                img_id= \'frame_id\'\n            f = osp.join(self.folder, vid_folder, vid_name)\n            video_file_list = [osp.join(f, x) for x in sorted(os.listdir(f)) if x.endswith(\'.jpg\')]\n            frame_idxs = self.db[img_id][start_index:end_index + 1]\n            if self.dataset_name == \'pennaction\' or self.dataset_name == \'posetrack\':\n                video = frame_idxs\n            else:\n                video = [video_file_list[i] for i in frame_idxs]\n\n            video = torch.cat(\n                [get_single_image_crop(image, bbox).unsqueeze(0) for image, bbox in zip(video, bbox)], dim=0\n            )\n\n            target[\'video\'] = video\n\n        return target\n\n\n'"
lib/dataset/dataset_3d.py,15,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport torch\nimport random\nimport logging\nimport numpy as np\nimport os.path as osp\nimport joblib\n\nfrom torch.utils.data import Dataset\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.kp_utils import convert_kps\nfrom lib.data_utils.img_utils import normalize_2d_kp, transfrom_keypoints, split_into_chunks\n\nlogger = logging.getLogger(__name__)\n\nclass Dataset3D(Dataset):\n    def __init__(self, set, seqlen, overlap=0., folder=None, dataset_name=None, debug=False):\n\n        self.folder = folder\n        self.set = set\n        self.dataset_name = dataset_name\n        self.seqlen = seqlen\n        self.stride = int(seqlen * (1-overlap))\n        self.debug = debug\n        self.db = self.load_db()\n        self.vid_indices = split_into_chunks(self.db['vid_name'], self.seqlen, self.stride)\n\n    def __len__(self):\n        return len(self.vid_indices)\n\n    def __getitem__(self, index):\n        return self.get_single_item(index)\n\n    def load_db(self):\n        db_file = osp.join(VIBE_DB_DIR, f'{self.dataset_name}_{self.set}_db.pt')\n\n        if osp.isfile(db_file):\n            db = joblib.load(db_file)\n        else:\n            raise ValueError(f'{db_file} do not exists')\n\n        print(f'Loaded {self.dataset_name} dataset from {db_file}')\n        return db\n\n    def get_single_item(self, index):\n        start_index, end_index = self.vid_indices[index]\n\n        is_train = self.set == 'train'\n\n        if self.dataset_name == '3dpw':\n            kp_2d = convert_kps(self.db['joints2D'][start_index:end_index + 1], src='common', dst='spin')\n            kp_3d = self.db['joints3D'][start_index:end_index + 1]\n        elif self.dataset_name == 'mpii3d':\n            kp_2d = self.db['joints2D'][start_index:end_index + 1]\n            if is_train:\n                kp_3d = self.db['joints3D'][start_index:end_index + 1]\n            else:\n                kp_3d = convert_kps(self.db['joints3D'][start_index:end_index + 1], src='spin', dst='common')\n        elif self.dataset_name == 'h36m':\n            kp_2d = self.db['joints2D'][start_index:end_index + 1]\n            if is_train:\n                kp_3d = self.db['joints3D'][start_index:end_index + 1]\n            else:\n                kp_3d = convert_kps(self.db['joints3D'][start_index:end_index + 1], src='spin', dst='common')\n\n        kp_2d_tensor = np.ones((self.seqlen, 49, 3), dtype=np.float16)\n        nj = 14 if not is_train else 49\n        kp_3d_tensor = np.zeros((self.seqlen, nj, 3), dtype=np.float16)\n\n\n        if self.dataset_name == '3dpw':\n            pose  = self.db['pose'][start_index:end_index+1]\n            shape = self.db['shape'][start_index:end_index+1]\n            w_smpl = torch.ones(self.seqlen).float()\n            w_3d = torch.ones(self.seqlen).float()\n        elif self.dataset_name == 'h36m':\n            if not is_train:\n                pose = np.zeros((kp_2d.shape[0], 72))\n                shape = np.zeros((kp_2d.shape[0], 10))\n                w_smpl = torch.zeros(self.seqlen).float()\n                w_3d = torch.ones(self.seqlen).float()\n            else:\n                pose = self.db['pose'][start_index:end_index + 1]\n                shape = self.db['shape'][start_index:end_index + 1]\n                w_smpl = torch.ones(self.seqlen).float()\n                w_3d = torch.ones(self.seqlen).float()\n        elif self.dataset_name == 'mpii3d':\n            pose = np.zeros((kp_2d.shape[0], 72))\n            shape = np.zeros((kp_2d.shape[0], 10))\n            w_smpl = torch.zeros(self.seqlen).float()\n            w_3d = torch.ones(self.seqlen).float()\n\n        bbox = self.db['bbox'][start_index:end_index + 1]\n        input = torch.from_numpy(self.db['features'][start_index:end_index+1]).float()\n\n        theta_tensor = np.zeros((self.seqlen, 85), dtype=np.float16)\n\n        for idx in range(self.seqlen):\n            # crop image and transform 2d keypoints\n            kp_2d[idx,:,:2], trans = transfrom_keypoints(\n                kp_2d=kp_2d[idx,:,:2],\n                center_x=bbox[idx,0],\n                center_y=bbox[idx,1],\n                width=bbox[idx,2],\n                height=bbox[idx,3],\n                patch_width=224,\n                patch_height=224,\n                do_augment=False,\n            )\n\n            kp_2d[idx,:,:2] = normalize_2d_kp(kp_2d[idx,:,:2], 224)\n\n            # theta shape (85,)\n            theta = np.concatenate((np.array([1., 0., 0.]), pose[idx], shape[idx]), axis=0)\n\n            kp_2d_tensor[idx] = kp_2d[idx]\n            theta_tensor[idx] = theta\n            kp_3d_tensor[idx] = kp_3d[idx]\n\n        target = {\n            'features': input,\n            'theta': torch.from_numpy(theta_tensor).float(), # camera, pose and shape\n            'kp_2d': torch.from_numpy(kp_2d_tensor).float(), # 2D keypoints transformed according to bbox cropping\n            'kp_3d': torch.from_numpy(kp_3d_tensor).float(), # 3D keypoints\n            'w_smpl': w_smpl,\n            'w_3d': w_3d,\n        }\n\n        if self.dataset_name == 'mpii3d' and not is_train:\n            target['valid'] = self.db['valid_i'][start_index:end_index+1]\n\n        if self.dataset_name == '3dpw' and not is_train:\n            vn = self.db['vid_name'][start_index:end_index + 1]\n            fi = self.db['frame_id'][start_index:end_index + 1]\n            target['instance_id'] = [f'{v}/{f}'for v,f in zip(vn,fi)]\n\n\n\n        # if self.dataset_name == '3dpw' and not self.is_train:\n            # target['imgname'] = self.db['img_name'][start_index:end_index+1].tolist()\n            # target['imgname'] = np.array(target['imgname'])\n            # print(target['imgname'].dtype)\n            # target['center'] = self.db['bbox'][start_index:end_index+1, :2]\n            # target['valid'] = torch.from_numpy(self.db['valid'][start_index:end_index+1])\n\n        if self.debug:\n            from lib.data_utils.img_utils import get_single_image_crop\n\n            if self.dataset_name == 'mpii3d':\n                video = self.db['img_name'][start_index:end_index+1]\n                # print(video)\n            elif self.dataset_name == 'h36m':\n                video = self.db['img_name'][start_index:end_index + 1]\n            else:\n                vid_name = self.db['vid_name'][start_index]\n                vid_name = '_'.join(vid_name.split('_')[:-1])\n                f = osp.join(self.folder, 'imageFiles', vid_name)\n                video_file_list = [osp.join(f, x) for x in sorted(os.listdir(f)) if x.endswith('.jpg')]\n                frame_idxs = self.db['frame_id'][start_index:end_index + 1]\n                # print(f, frame_idxs)\n                video = [video_file_list[i] for i in frame_idxs]\n\n            video = torch.cat(\n                [get_single_image_crop(image, bbox).unsqueeze(0) for image, bbox in zip(video, bbox)], dim=0\n            )\n\n            target['video'] = video\n\n        return target\n\n\n\n\n\n"""
lib/dataset/inference.py,1,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport cv2\nimport numpy as np\nimport os.path as osp\nfrom torch.utils.data import Dataset\nfrom torchvision.transforms.functional import to_tensor\n\nfrom lib.utils.smooth_bbox import get_all_bbox_params\nfrom lib.data_utils.img_utils import get_single_image_crop_demo\n\n\nclass Inference(Dataset):\n    def __init__(self, image_folder, frames, bboxes=None, joints2d=None, scale=1.0, crop_size=224):\n        self.image_file_names = [\n            osp.join(image_folder, x)\n            for x in os.listdir(image_folder)\n            if x.endswith('.png') or x.endswith('.jpg')\n        ]\n        self.image_file_names = sorted(self.image_file_names)\n        self.image_file_names = np.array(self.image_file_names)[frames]\n        self.bboxes = bboxes\n        self.joints2d = joints2d\n        self.scale = scale\n        self.crop_size = crop_size\n        self.frames = frames\n        self.has_keypoints = True if joints2d is not None else False\n\n        self.norm_joints2d = np.zeros_like(self.joints2d)\n\n        if self.has_keypoints:\n            bboxes, time_pt1, time_pt2 = get_all_bbox_params(joints2d, vis_thresh=0.3)\n            bboxes[:, 2:] = 150. / bboxes[:, 2:]\n            self.bboxes = np.stack([bboxes[:, 0], bboxes[:, 1], bboxes[:, 2], bboxes[:, 2]]).T\n\n            self.image_file_names = self.image_file_names[time_pt1:time_pt2]\n            self.joints2d = joints2d[time_pt1:time_pt2]\n            self.frames = frames[time_pt1:time_pt2]\n\n    def __len__(self):\n        return len(self.image_file_names)\n\n    def __getitem__(self, idx):\n        img = cv2.cvtColor(cv2.imread(self.image_file_names[idx]), cv2.COLOR_BGR2RGB)\n\n        bbox = self.bboxes[idx]\n\n        j2d = self.joints2d[idx] if self.has_keypoints else None\n\n        norm_img, raw_img, kp_2d = get_single_image_crop_demo(\n            img,\n            bbox,\n            kp_2d=j2d,\n            scale=self.scale,\n            crop_size=self.crop_size)\n        if self.has_keypoints:\n            return norm_img, kp_2d\n        else:\n            return norm_img\n\n\nclass ImageFolder(Dataset):\n    def __init__(self, image_folder):\n        self.image_file_names = [\n            osp.join(image_folder, x)\n            for x in os.listdir(image_folder)\n            if x.endswith('.png') or x.endswith('.jpg')\n        ]\n        self.image_file_names = sorted(self.image_file_names)\n\n    def __len__(self):\n        return len(self.image_file_names)\n\n    def __getitem__(self, idx):\n        img = cv2.cvtColor(cv2.imread(self.image_file_names[idx]), cv2.COLOR_BGR2RGB)\n        return to_tensor(img)\n"""
lib/dataset/insta.py,3,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport h5py\nimport torch\nimport logging\nimport numpy as np\nimport os.path as osp\n\nfrom torch.utils.data import Dataset\nfrom lib.core.config import VIBE_DB_DIR\nfrom lib.data_utils.kp_utils import convert_kps\nfrom lib.data_utils.img_utils import normalize_2d_kp, split_into_chunks\n\nlogger = logging.getLogger(__name__)\n\nclass Insta(Dataset):\n    def __init__(self, seqlen, overlap=0., debug=False):\n        self.seqlen = seqlen\n        self.stride = int(seqlen * (1-overlap))\n\n        self.h5_file = osp.join(VIBE_DB_DIR, 'insta_train_db.h5')\n\n        with h5py.File(self.h5_file, 'r') as db:\n            self.db = db\n            self.vid_indices = split_into_chunks(self.db['vid_name'], self.seqlen, self.stride)\n\n        print(f'InstaVariety number of dataset objects {self.__len__()}')\n\n    def __len__(self):\n        return len(self.vid_indices)\n\n    def __getitem__(self, index):\n        return self.get_single_item(index)\n\n    def get_single_item(self, index):\n        start_index, end_index = self.vid_indices[index]\n\n        with h5py.File(self.h5_file, 'r') as db:\n            self.db = db\n\n            kp_2d = self.db['joints2D'][start_index:end_index + 1]\n            kp_2d = convert_kps(kp_2d, src='insta', dst='spin')\n            kp_2d_tensor = np.ones((self.seqlen, 49, 3), dtype=np.float16)\n\n\n            input = torch.from_numpy(self.db['features'][start_index:end_index+1]).float()\n\n            vid_name = self.db['vid_name'][start_index:end_index + 1]\n            frame_id = self.db['frame_id'][start_index:end_index + 1].astype(str)\n            instance_id = np.array([v.decode('ascii') + f for v, f in zip(vid_name, frame_id)])\n\n        for idx in range(self.seqlen):\n            kp_2d[idx,:,:2] = normalize_2d_kp(kp_2d[idx,:,:2], 224)\n            kp_2d_tensor[idx] = kp_2d[idx]\n\n        target = {\n            'features': input,\n            'kp_2d': torch.from_numpy(kp_2d_tensor).float(), # 2D keypoints transformed according to bbox cropping\n            # 'instance_id': instance_id\n        }\n\n        return target"""
lib/dataset/loaders.py,1,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom torch.utils.data import ConcatDataset, DataLoader\n\nfrom lib.dataset import *\n\n\ndef get_data_loaders(cfg):\n    def get_2d_datasets(dataset_names):\n        datasets = []\n        for dataset_name in dataset_names:\n            db = eval(dataset_name)(seqlen=cfg.DATASET.SEQLEN, debug=cfg.DEBUG)\n            datasets.append(db)\n        return ConcatDataset(datasets)\n\n    def get_3d_datasets(dataset_names):\n        datasets = []\n        for dataset_name in dataset_names:\n            db = eval(dataset_name)(set='train', seqlen=cfg.DATASET.SEQLEN, debug=cfg.DEBUG)\n            datasets.append(db)\n        return ConcatDataset(datasets)\n\n    # ===== 2D keypoint datasets =====\n    train_2d_dataset_names = cfg.TRAIN.DATASETS_2D\n    train_2d_db = get_2d_datasets(train_2d_dataset_names)\n\n    data_2d_batch_size = int(cfg.TRAIN.BATCH_SIZE * cfg.TRAIN.DATA_2D_RATIO)\n    data_3d_batch_size = cfg.TRAIN.BATCH_SIZE - data_2d_batch_size\n\n    train_2d_loader = DataLoader(\n        dataset=train_2d_db,\n        batch_size=data_2d_batch_size,\n        shuffle=True,\n        num_workers=cfg.NUM_WORKERS,\n    )\n\n    # ===== 3D keypoint datasets =====\n    train_3d_dataset_names = cfg.TRAIN.DATASETS_3D\n    train_3d_db = get_3d_datasets(train_3d_dataset_names)\n\n    train_3d_loader = DataLoader(\n        dataset=train_3d_db,\n        batch_size=data_3d_batch_size,\n        shuffle=True,\n        num_workers=cfg.NUM_WORKERS,\n    )\n\n    # ===== Motion Discriminator dataset =====\n    motion_disc_db = AMASS(seqlen=cfg.DATASET.SEQLEN)\n\n    motion_disc_loader = DataLoader(\n        dataset=motion_disc_db,\n        batch_size=cfg.TRAIN.BATCH_SIZE,\n        shuffle=True,\n        num_workers=cfg.NUM_WORKERS,\n    )\n\n    # ===== Evaluation dataset =====\n    valid_db = eval(cfg.TRAIN.DATASET_EVAL)(set='val', seqlen=cfg.DATASET.SEQLEN, debug=cfg.DEBUG)\n\n    valid_loader = DataLoader(\n        dataset=valid_db,\n        batch_size=cfg.TRAIN.BATCH_SIZE,\n        shuffle=False,\n        num_workers=cfg.NUM_WORKERS,\n    )\n\n    return train_2d_loader, train_3d_loader, motion_disc_loader, valid_loader"""
lib/dataset/mpii3d.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom lib.dataset import Dataset3D\nfrom lib.core.config import MPII3D_DIR\n\n\nclass MPII3D(Dataset3D):\n    def __init__(self, set, seqlen, overlap=0, debug=False):\n        db_name = 'mpii3d'\n\n        # during testing we don't need data augmentation\n        # but we can use it as an ensemble\n        is_train = set == 'train'\n        overlap = overlap if is_train else 0.\n        print('MPII3D Dataset overlap ratio: ', overlap)\n        super(MPII3D, self).__init__(\n            set = set,\n            folder=MPII3D_DIR,\n            seqlen=seqlen,\n            overlap=overlap,\n            dataset_name=db_name,\n            debug=debug,\n        )\n        print(f'{db_name} - number of dataset objects {self.__len__()}')"""
lib/dataset/penn_action.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom lib.dataset import Dataset2D\nfrom lib.core.config import PENNACTION_DIR\n\n\nclass PennAction(Dataset2D):\n    def __init__(self, seqlen, overlap=0.75, debug=False):\n        db_name = 'pennaction'\n\n        super(PennAction, self).__init__(\n            seqlen = seqlen,\n            folder=PENNACTION_DIR,\n            dataset_name=db_name,\n            debug=debug,\n            overlap=overlap,\n        )\n        print(f'{db_name} - number of dataset objects {self.__len__()}')\n"""
lib/dataset/posetrack.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom lib.dataset import Dataset2D\nfrom lib.core.config import POSETRACK_DIR\n\n\nclass PoseTrack(Dataset2D):\n    def __init__(self, seqlen, overlap=0.75, folder=None, debug=False):\n        db_name = 'posetrack'\n        super(PoseTrack, self).__init__(\n            seqlen = seqlen,\n            folder=POSETRACK_DIR,\n            dataset_name=db_name,\n            debug=debug,\n            overlap=overlap,\n        )\n        print(f'{db_name} - number of dataset objects {self.__len__()}')\n"""
lib/dataset/threedpw.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom lib.dataset import Dataset3D\nfrom lib.core.config import THREEDPW_DIR\n\nclass ThreeDPW(Dataset3D):\n    def __init__(self, set, seqlen, overlap=0.75, debug=False):\n        db_name = '3dpw'\n\n        # during testing we don't need data augmentation\n        # but we can use it as an ensemble\n        is_train = set == 'train'\n        overlap = overlap if is_train else 0.\n        print('3DPW Dataset overlap ratio: ', overlap)\n        super(ThreeDPW, self).__init__(\n            set=set,\n            folder=THREEDPW_DIR,\n            seqlen=seqlen,\n            overlap=overlap,\n            dataset_name=db_name,\n            debug=debug,\n        )\n        print(f'{db_name} - number of dataset objects {self.__len__()}')"""
lib/models/__init__.py,0,b'from .vibe import VIBE\nfrom .motion_discriminator import MotionDiscriminator\n'
lib/models/attention.py,2,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport torch\nfrom torch import nn\n\ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.uniform_(m.weight, -0.1, 0.1)\n        m.bias.data.fill_(0.01)\n\nclass SelfAttention(nn.Module):\n    def __init__(self, attention_size,\n                 batch_first=False,\n                 layers=1,\n                 dropout=.0,\n                 non_linearity=""tanh""):\n        super(SelfAttention, self).__init__()\n\n        self.batch_first = batch_first\n\n        if non_linearity == ""relu"":\n            activation = nn.ReLU()\n        else:\n            activation = nn.Tanh()\n\n        modules = []\n        for i in range(layers - 1):\n            modules.append(nn.Linear(attention_size, attention_size))\n            modules.append(activation)\n            modules.append(nn.Dropout(dropout))\n\n        # last attention layer must output 1\n        modules.append(nn.Linear(attention_size, 1))\n        modules.append(activation)\n        modules.append(nn.Dropout(dropout))\n\n        self.attention = nn.Sequential(*modules)\n        self.attention.apply(init_weights) \n        self.softmax = nn.Softmax(dim=-1)\n\n\n    def forward(self, inputs):\n\n        ##################################################################\n        # STEP 1 - perform dot product\n        # of the attention vector and each hidden state\n        ##################################################################\n\n        # inputs is a 3D Tensor: batch, len, hidden_size\n        # scores is a 2D Tensor: batch, len\n        scores = self.attention(inputs).squeeze()\n        scores = self.softmax(scores)\n\n        ##################################################################\n        # Step 2 - Weighted sum of hidden states, by the attention scores\n        ##################################################################\n\n        # multiply each hidden state with the attention weights\n        weighted = torch.mul(inputs, scores.unsqueeze(-1).expand_as(inputs))\n\n        # sum the hidden states\n        # representations = weighted.sum(1).squeeze()\n        representations = weighted.sum(1).squeeze()\n        return representations, scores\n\n'"
lib/models/motion_discriminator.py,5,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils import spectral_norm\nfrom lib.models.attention import SelfAttention\n\nclass MotionDiscriminator(nn.Module):\n\n    def __init__(self,\n                 rnn_size,\n                 input_size,\n                 num_layers,\n                 output_size=2,\n                 feature_pool=""concat"",\n                 use_spectral_norm=False,\n                 attention_size=1024,\n                 attention_layers=1,\n                 attention_dropout=0.5):\n\n        super(MotionDiscriminator, self).__init__()\n        self.input_size = input_size\n        self.rnn_size = rnn_size\n        self.feature_pool = feature_pool\n        self.num_layers = num_layers\n        self.attention_size = attention_size\n        self.attention_layers = attention_layers\n        self.attention_dropout = attention_dropout\n\n        self.gru = nn.GRU(self.input_size, self.rnn_size, num_layers=num_layers)\n\n        linear_size = self.rnn_size if not feature_pool == ""concat"" else self.rnn_size * 2\n\n        if feature_pool == ""attention"" :\n            self.attention = SelfAttention(attention_size=self.attention_size,\n                                       layers=self.attention_layers,\n                                       dropout=self.attention_dropout)\n        if use_spectral_norm:\n            self.fc = spectral_norm(nn.Linear(linear_size, output_size))\n        else:\n            self.fc = nn.Linear(linear_size, output_size)\n\n    def forward(self, sequence):\n        """"""\n        sequence: of shape [batch_size, seq_len, input_size]\n        """"""\n        batchsize, seqlen, input_size = sequence.shape\n        sequence = torch.transpose(sequence, 0, 1)\n\n        outputs, state = self.gru(sequence)\n\n        if self.feature_pool == ""concat"":\n            outputs = F.relu(outputs)\n            avg_pool = F.adaptive_avg_pool1d(outputs.permute(1, 2, 0), 1).view(batchsize, -1)\n            max_pool = F.adaptive_max_pool1d(outputs.permute(1, 2, 0), 1).view(batchsize, -1)\n            output = self.fc(torch.cat([avg_pool, max_pool], dim=1))\n        elif self.feature_pool == ""attention"":\n            outputs = outputs.permute(1, 0, 2)\n            y, attentions = self.attention(outputs)\n            output = self.fc(y)\n        else:\n            output = self.fc(outputs[-1])\n\n        return output\n'"
lib/models/resnet.py,11,"b'# This script is borrowed from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n# Adhere to their licence to use this script.\n\nimport torch\nimport torch.nn as nn\nfrom torchvision.models.utils import load_state_dict_from_url\n\n\n__all__ = [\'ResNet\', \'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\',\n           \'resnet152\', \'resnext50_32x4d\', \'resnext101_32x8d\',\n           \'wide_resnet50_2\', \'wide_resnet101_2\']\n\n\nmodel_urls = {\n    \'resnet18\': \'https://download.pytorch.org/models/resnet18-5c106cde.pth\',\n    \'resnet34\': \'https://download.pytorch.org/models/resnet34-333f7ec4.pth\',\n    \'resnet50\': \'https://download.pytorch.org/models/resnet50-19c8e357.pth\',\n    \'resnet101\': \'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\',\n    \'resnet152\': \'https://download.pytorch.org/models/resnet152-b121ed2d.pth\',\n    \'resnext50_32x4d\': \'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth\',\n    \'resnext101_32x8d\': \'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\',\n    \'wide_resnet50_2\': \'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth\',\n    \'wide_resnet101_2\': \'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth\',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n    """"""3x3 convolution with padding""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n\n\ndef conv1x1(in_planes, out_planes, stride=1):\n    """"""1x1 convolution""""""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(BasicBlock, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        if groups != 1 or base_width != 64:\n            raise ValueError(\'BasicBlock only supports groups=1 and base_width=64\')\n        if dilation > 1:\n            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")\n        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = norm_layer(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = norm_layer(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None):\n        super(Bottleneck, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        width = int(planes * (base_width / 64.)) * groups\n        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n        self.conv1 = conv1x1(inplanes, width)\n        self.bn1 = norm_layer(width)\n        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n        self.bn2 = norm_layer(width)\n        self.conv3 = conv1x1(width, planes * self.expansion)\n        self.bn3 = norm_layer(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out\n\n\nclass ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n                 groups=1, width_per_group=64, replace_stride_with_dilation=None,\n                 norm_layer=None):\n        super(ResNet, self).__init__()\n        if norm_layer is None:\n            norm_layer = nn.BatchNorm2d\n        self._norm_layer = norm_layer\n\n        self.inplanes = 64\n        self.dilation = 1\n        if replace_stride_with_dilation is None:\n            # each element in the tuple indicates if we should replace\n            # the 2x2 stride with a dilated convolution instead\n            replace_stride_with_dilation = [False, False, False]\n        if len(replace_stride_with_dilation) != 3:\n            raise ValueError(""replace_stride_with_dilation should be None ""\n                             ""or a 3-element tuple, got {}"".format(replace_stride_with_dilation))\n        self.groups = groups\n        self.base_width = width_per_group\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = norm_layer(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2,\n                                       dilate=replace_stride_with_dilation[0])\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2,\n                                       dilate=replace_stride_with_dilation[1])\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2,\n                                       dilate=replace_stride_with_dilation[2])\n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        # self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n        norm_layer = self._norm_layer\n        downsample = None\n        previous_dilation = self.dilation\n        if dilate:\n            self.dilation *= stride\n            stride = 1\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                norm_layer(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, self.groups,\n                            self.base_width, previous_dilation, norm_layer))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups=self.groups,\n                                base_width=self.base_width, dilation=self.dilation,\n                                norm_layer=norm_layer))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = torch.flatten(x, 1)\n        # x = self.fc(x)\n\n        return x\n\n\ndef _resnet(arch, block, layers, pretrained, progress, **kwargs):\n    model = ResNet(block, layers, **kwargs)\n    if pretrained:\n        state_dict = load_state_dict_from_url(model_urls[arch],\n                                              progress=progress)\n        model.load_state_dict(state_dict, strict=False)\n    return model\n\n\ndef resnet18(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-18 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet18\', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet34(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-34 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet34\', BasicBlock, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet50(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-50 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet50\', Bottleneck, [3, 4, 6, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet101(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-101 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet101\', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnet152(pretrained=False, progress=True, **kwargs):\n    r""""""ResNet-152 model from\n    `""Deep Residual Learning for Image Recognition"" <https://arxiv.org/pdf/1512.03385.pdf>\'_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    return _resnet(\'resnet152\', Bottleneck, [3, 8, 36, 3], pretrained, progress,\n                   **kwargs)\n\n\ndef resnext50_32x4d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-50 32x4d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 4\n    return _resnet(\'resnext50_32x4d\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef resnext101_32x8d(pretrained=False, progress=True, **kwargs):\n    r""""""ResNeXt-101 32x8d model from\n    `""Aggregated Residual Transformation for Deep Neural Networks"" <https://arxiv.org/pdf/1611.05431.pdf>`_\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'groups\'] = 32\n    kwargs[\'width_per_group\'] = 8\n    return _resnet(\'resnext101_32x8d\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet50_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-50-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet50_2\', Bottleneck, [3, 4, 6, 3],\n                   pretrained, progress, **kwargs)\n\n\ndef wide_resnet101_2(pretrained=False, progress=True, **kwargs):\n    r""""""Wide ResNet-101-2 model from\n    `""Wide Residual Networks"" <https://arxiv.org/pdf/1605.07146.pdf>`_\n\n    The model is the same as ResNet except for the bottleneck number of channels\n    which is twice larger in every block. The number of channels in outer 1x1\n    convolutions is the same, e.g. last block in ResNet-50 has 2048-512-2048\n    channels, and in Wide ResNet-50-2 has 2048-1024-2048.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        progress (bool): If True, displays a progress bar of the download to stderr\n    """"""\n    kwargs[\'width_per_group\'] = 64 * 2\n    return _resnet(\'wide_resnet101_2\', Bottleneck, [3, 4, 23, 3],\n                   pretrained, progress, **kwargs)\n'"
lib/models/smpl.py,3,"b'# This script is borrowed and extended from https://github.com/nkolot/SPIN/blob/master/models/hmr.py\n# Adhere to their licence to use this script\n\nimport torch\nimport numpy as np\nimport os.path as osp\nfrom smplx import SMPL as _SMPL\nfrom smplx.body_models import ModelOutput\nfrom smplx.lbs import vertices2joints\n\nfrom lib.core.config import VIBE_DATA_DIR\n\n# Map joints to SMPL joints\nJOINT_MAP = {\n    \'OP Nose\': 24, \'OP Neck\': 12, \'OP RShoulder\': 17,\n    \'OP RElbow\': 19, \'OP RWrist\': 21, \'OP LShoulder\': 16,\n    \'OP LElbow\': 18, \'OP LWrist\': 20, \'OP MidHip\': 0,\n    \'OP RHip\': 2, \'OP RKnee\': 5, \'OP RAnkle\': 8,\n    \'OP LHip\': 1, \'OP LKnee\': 4, \'OP LAnkle\': 7,\n    \'OP REye\': 25, \'OP LEye\': 26, \'OP REar\': 27,\n    \'OP LEar\': 28, \'OP LBigToe\': 29, \'OP LSmallToe\': 30,\n    \'OP LHeel\': 31, \'OP RBigToe\': 32, \'OP RSmallToe\': 33, \'OP RHeel\': 34,\n    \'Right Ankle\': 8, \'Right Knee\': 5, \'Right Hip\': 45,\n    \'Left Hip\': 46, \'Left Knee\': 4, \'Left Ankle\': 7,\n    \'Right Wrist\': 21, \'Right Elbow\': 19, \'Right Shoulder\': 17,\n    \'Left Shoulder\': 16, \'Left Elbow\': 18, \'Left Wrist\': 20,\n    \'Neck (LSP)\': 47, \'Top of Head (LSP)\': 48,\n    \'Pelvis (MPII)\': 49, \'Thorax (MPII)\': 50,\n    \'Spine (H36M)\': 51, \'Jaw (H36M)\': 52,\n    \'Head (H36M)\': 53, \'Nose\': 24, \'Left Eye\': 26,\n    \'Right Eye\': 25, \'Left Ear\': 28, \'Right Ear\': 27\n}\nJOINT_NAMES = [\n    \'OP Nose\', \'OP Neck\', \'OP RShoulder\',\n    \'OP RElbow\', \'OP RWrist\', \'OP LShoulder\',\n    \'OP LElbow\', \'OP LWrist\', \'OP MidHip\',\n    \'OP RHip\', \'OP RKnee\', \'OP RAnkle\',\n    \'OP LHip\', \'OP LKnee\', \'OP LAnkle\',\n    \'OP REye\', \'OP LEye\', \'OP REar\',\n    \'OP LEar\', \'OP LBigToe\', \'OP LSmallToe\',\n    \'OP LHeel\', \'OP RBigToe\', \'OP RSmallToe\', \'OP RHeel\',\n    \'Right Ankle\', \'Right Knee\', \'Right Hip\',\n    \'Left Hip\', \'Left Knee\', \'Left Ankle\',\n    \'Right Wrist\', \'Right Elbow\', \'Right Shoulder\',\n    \'Left Shoulder\', \'Left Elbow\', \'Left Wrist\',\n    \'Neck (LSP)\', \'Top of Head (LSP)\',\n    \'Pelvis (MPII)\', \'Thorax (MPII)\',\n    \'Spine (H36M)\', \'Jaw (H36M)\',\n    \'Head (H36M)\', \'Nose\', \'Left Eye\',\n    \'Right Eye\', \'Left Ear\', \'Right Ear\'\n]\n\nJOINT_IDS = {JOINT_NAMES[i]: i for i in range(len(JOINT_NAMES))}\nJOINT_REGRESSOR_TRAIN_EXTRA = osp.join(VIBE_DATA_DIR, \'J_regressor_extra.npy\')\nSMPL_MEAN_PARAMS = osp.join(VIBE_DATA_DIR, \'smpl_mean_params.npz\')\nSMPL_MODEL_DIR = VIBE_DATA_DIR\nH36M_TO_J17 = [6, 5, 4, 1, 2, 3, 16, 15, 14, 11, 12, 13, 8, 10, 0, 7, 9]\nH36M_TO_J14 = H36M_TO_J17[:14]\n\n\nclass SMPL(_SMPL):\n    """""" Extension of the official SMPL implementation to support more joints """"""\n\n    def __init__(self, *args, **kwargs):\n        super(SMPL, self).__init__(*args, **kwargs)\n        joints = [JOINT_MAP[i] for i in JOINT_NAMES]\n        J_regressor_extra = np.load(JOINT_REGRESSOR_TRAIN_EXTRA)\n        self.register_buffer(\'J_regressor_extra\', torch.tensor(J_regressor_extra, dtype=torch.float32))\n        self.joint_map = torch.tensor(joints, dtype=torch.long)\n\n    def forward(self, *args, **kwargs):\n        kwargs[\'get_skin\'] = True\n        smpl_output = super(SMPL, self).forward(*args, **kwargs)\n        extra_joints = vertices2joints(self.J_regressor_extra, smpl_output.vertices)\n        joints = torch.cat([smpl_output.joints, extra_joints], dim=1)\n        joints = joints[:, self.joint_map, :]\n        output = ModelOutput(vertices=smpl_output.vertices,\n                             global_orient=smpl_output.global_orient,\n                             body_pose=smpl_output.body_pose,\n                             joints=joints,\n                             betas=smpl_output.betas,\n                             full_pose=smpl_output.full_pose)\n        return output\n\n\ndef get_smpl_faces():\n    smpl = SMPL(SMPL_MODEL_DIR, batch_size=1, create_transl=False)\n    return smpl.faces'"
lib/models/spin.py,19,"b'# This script is borrowed and extended from https://github.com/nkolot/SPIN/blob/master/models/hmr.py\n# Adhere to their licence to use this script\n\nimport math\nimport torch\nimport numpy as np\nimport os.path as osp\nimport torch.nn as nn\nimport torchvision.models.resnet as resnet\n\nfrom lib.core.config import VIBE_DATA_DIR\nfrom lib.utils.geometry import rotation_matrix_to_angle_axis, rot6d_to_rotmat\nfrom lib.models.smpl import SMPL, SMPL_MODEL_DIR, H36M_TO_J14, SMPL_MEAN_PARAMS\n\n\nclass Bottleneck(nn.Module):\n    """"""\n    Redefinition of Bottleneck residual block\n    Adapted from the official PyTorch implementation\n    """"""\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass HMR(nn.Module):\n    """"""\n    SMPL Iterative Regressor with ResNet50 backbone\n    """"""\n    def __init__(self, block, layers, smpl_mean_params):\n        self.inplanes = 64\n        super(HMR, self).__init__()\n        npose = 24 * 6\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.avgpool = nn.AvgPool2d(7, stride=1)\n        self.fc1 = nn.Linear(512 * block.expansion + npose + 13, 1024)\n        self.drop1 = nn.Dropout()\n        self.fc2 = nn.Linear(1024, 1024)\n        self.drop2 = nn.Dropout()\n        self.decpose = nn.Linear(1024, npose)\n        self.decshape = nn.Linear(1024, 10)\n        self.deccam = nn.Linear(1024, 3)\n        nn.init.xavier_uniform_(self.decpose.weight, gain=0.01)\n        nn.init.xavier_uniform_(self.decshape.weight, gain=0.01)\n        nn.init.xavier_uniform_(self.deccam.weight, gain=0.01)\n\n        self.smpl = SMPL(\n            SMPL_MODEL_DIR,\n            batch_size=64,\n            create_transl=False\n        ).to(\'cpu\')\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        mean_params = np.load(smpl_mean_params)\n        init_pose = torch.from_numpy(mean_params[\'pose\'][:]).unsqueeze(0)\n        init_shape = torch.from_numpy(mean_params[\'shape\'][:].astype(\'float32\')).unsqueeze(0)\n        init_cam = torch.from_numpy(mean_params[\'cam\']).unsqueeze(0)\n        self.register_buffer(\'init_pose\', init_pose)\n        self.register_buffer(\'init_shape\', init_shape)\n        self.register_buffer(\'init_cam\', init_cam)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def feature_extractor(self, x):\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        xf = self.avgpool(x4)\n        xf = xf.view(xf.size(0), -1)\n        return xf\n\n    def forward(self, x, init_pose=None, init_shape=None, init_cam=None, n_iter=3, return_features=False):\n\n        batch_size = x.shape[0]\n\n        if init_pose is None:\n            init_pose = self.init_pose.expand(batch_size, -1)\n        if init_shape is None:\n            init_shape = self.init_shape.expand(batch_size, -1)\n        if init_cam is None:\n            init_cam = self.init_cam.expand(batch_size, -1)\n\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x1 = self.layer1(x)\n        x2 = self.layer2(x1)\n        x3 = self.layer3(x2)\n        x4 = self.layer4(x3)\n\n        xf = self.avgpool(x4)\n        xf = xf.view(xf.size(0), -1)\n\n        pred_pose = init_pose\n        pred_shape = init_shape\n        pred_cam = init_cam\n        for i in range(n_iter):\n            xc = torch.cat([xf, pred_pose, pred_shape, pred_cam], 1)\n            xc = self.fc1(xc)\n            xc = self.drop1(xc)\n            xc = self.fc2(xc)\n            xc = self.drop2(xc)\n            pred_pose = self.decpose(xc) + pred_pose\n            pred_shape = self.decshape(xc) + pred_shape\n            pred_cam = self.deccam(xc) + pred_cam\n\n        pred_rotmat = rot6d_to_rotmat(pred_pose).view(batch_size, 24, 3, 3)\n\n        pred_output = self.smpl(\n            betas=pred_shape,\n            body_pose=pred_rotmat[:, 1:],\n            global_orient=pred_rotmat[:, 0].unsqueeze(1),\n            pose2rot=False\n        )\n\n        pred_vertices = pred_output.vertices\n        pred_joints = pred_output.joints\n\n        pred_keypoints_2d = projection(pred_joints, pred_cam)\n\n        pose = rotation_matrix_to_angle_axis(pred_rotmat.reshape(-1, 3, 3)).reshape(-1, 72)\n\n        output = [{\n            \'theta\': torch.cat([pred_cam, pose, pred_shape], dim=1),\n            \'verts\': pred_vertices,\n            \'kp_2d\': pred_keypoints_2d,\n            \'kp_3d\': pred_joints,\n        }]\n\n        if return_features:\n            return xf, output\n        else:\n            return output\n\n\nclass Regressor(nn.Module):\n    def __init__(self, smpl_mean_params=SMPL_MEAN_PARAMS):\n        super(Regressor, self).__init__()\n\n        npose = 24 * 6\n\n        self.fc1 = nn.Linear(512 * 4 + npose + 13, 1024)\n        self.drop1 = nn.Dropout()\n        self.fc2 = nn.Linear(1024, 1024)\n        self.drop2 = nn.Dropout()\n        self.decpose = nn.Linear(1024, npose)\n        self.decshape = nn.Linear(1024, 10)\n        self.deccam = nn.Linear(1024, 3)\n        nn.init.xavier_uniform_(self.decpose.weight, gain=0.01)\n        nn.init.xavier_uniform_(self.decshape.weight, gain=0.01)\n        nn.init.xavier_uniform_(self.deccam.weight, gain=0.01)\n\n        self.smpl = SMPL(\n            SMPL_MODEL_DIR,\n            batch_size=64,\n            create_transl=False\n        )\n\n        mean_params = np.load(smpl_mean_params)\n        init_pose = torch.from_numpy(mean_params[\'pose\'][:]).unsqueeze(0)\n        init_shape = torch.from_numpy(mean_params[\'shape\'][:].astype(\'float32\')).unsqueeze(0)\n        init_cam = torch.from_numpy(mean_params[\'cam\']).unsqueeze(0)\n        self.register_buffer(\'init_pose\', init_pose)\n        self.register_buffer(\'init_shape\', init_shape)\n        self.register_buffer(\'init_cam\', init_cam)\n\n\n\n    def forward(self, x, init_pose=None, init_shape=None, init_cam=None, n_iter=3, J_regressor=None):\n        batch_size = x.shape[0]\n\n        if init_pose is None:\n            init_pose = self.init_pose.expand(batch_size, -1)\n        if init_shape is None:\n            init_shape = self.init_shape.expand(batch_size, -1)\n        if init_cam is None:\n            init_cam = self.init_cam.expand(batch_size, -1)\n\n        pred_pose = init_pose\n        pred_shape = init_shape\n        pred_cam = init_cam\n        for i in range(n_iter):\n            xc = torch.cat([x, pred_pose, pred_shape, pred_cam], 1)\n            xc = self.fc1(xc)\n            xc = self.drop1(xc)\n            xc = self.fc2(xc)\n            xc = self.drop2(xc)\n            pred_pose = self.decpose(xc) + pred_pose\n            pred_shape = self.decshape(xc) + pred_shape\n            pred_cam = self.deccam(xc) + pred_cam\n\n        pred_rotmat = rot6d_to_rotmat(pred_pose).view(batch_size, 24, 3, 3)\n\n        pred_output = self.smpl(\n            betas=pred_shape,\n            body_pose=pred_rotmat[:, 1:],\n            global_orient=pred_rotmat[:, 0].unsqueeze(1),\n            pose2rot=False\n        )\n\n        pred_vertices = pred_output.vertices\n        pred_joints = pred_output.joints\n\n        if J_regressor is not None:\n            J_regressor_batch = J_regressor[None, :].expand(pred_vertices.shape[0], -1, -1).to(pred_vertices.device)\n            pred_joints = torch.matmul(J_regressor_batch, pred_vertices)\n            pred_joints = pred_joints[:, H36M_TO_J14, :]\n\n        pred_keypoints_2d = projection(pred_joints, pred_cam)\n\n        pose = rotation_matrix_to_angle_axis(pred_rotmat.reshape(-1, 3, 3)).reshape(-1, 72)\n\n        output = [{\n            \'theta\'  : torch.cat([pred_cam, pose, pred_shape], dim=1),\n            \'verts\'  : pred_vertices,\n            \'kp_2d\'  : pred_keypoints_2d,\n            \'kp_3d\'  : pred_joints,\n            \'rotmat\' : pred_rotmat\n        }]\n        return output\n\n\ndef hmr(smpl_mean_params=SMPL_MEAN_PARAMS, pretrained=True, **kwargs):\n    """"""\n    Constructs an HMR model with ResNet50 backbone.\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = HMR(Bottleneck, [3, 4, 6, 3], smpl_mean_params, **kwargs)\n    if pretrained:\n        resnet_imagenet = resnet.resnet50(pretrained=True)\n        model.load_state_dict(resnet_imagenet.state_dict(), strict=False)\n    return model\n\n\ndef projection(pred_joints, pred_camera):\n    pred_cam_t = torch.stack([pred_camera[:, 1],\n                              pred_camera[:, 2],\n                              2 * 5000. / (224. * pred_camera[:, 0] + 1e-9)], dim=-1)\n    batch_size = pred_joints.shape[0]\n    camera_center = torch.zeros(batch_size, 2)\n    pred_keypoints_2d = perspective_projection(pred_joints,\n                                               rotation=torch.eye(3).unsqueeze(0).expand(batch_size, -1, -1).to(pred_joints.device),\n                                               translation=pred_cam_t,\n                                               focal_length=5000.,\n                                               camera_center=camera_center)\n    # Normalize keypoints to [-1,1]\n    pred_keypoints_2d = pred_keypoints_2d / (224. / 2.)\n    return pred_keypoints_2d\n\n\ndef perspective_projection(points, rotation, translation,\n                           focal_length, camera_center):\n    """"""\n    This function computes the perspective projection of a set of points.\n    Input:\n        points (bs, N, 3): 3D points\n        rotation (bs, 3, 3): Camera rotation\n        translation (bs, 3): Camera translation\n        focal_length (bs,) or scalar: Focal length\n        camera_center (bs, 2): Camera center\n    """"""\n    batch_size = points.shape[0]\n    K = torch.zeros([batch_size, 3, 3], device=points.device)\n    K[:,0,0] = focal_length\n    K[:,1,1] = focal_length\n    K[:,2,2] = 1.\n    K[:,:-1, -1] = camera_center\n\n    # Transform points\n    points = torch.einsum(\'bij,bkj->bki\', rotation, points)\n    points = points + translation.unsqueeze(1)\n\n    # Apply perspective distortion\n    projected_points = points / points[:,:,-1].unsqueeze(-1)\n\n    # Apply camera intrinsics\n    projected_points = torch.einsum(\'bij,bkj->bki\', K, projected_points)\n\n    return projected_points[:, :, :-1]\n\n\ndef get_pretrained_hmr():\n    device = \'cuda\'\n    model = hmr().to(device)\n    checkpoint = torch.load(osp.join(VIBE_DATA_DIR, \'spin_model_checkpoint.pth.tar\'))\n    model.load_state_dict(checkpoint[\'model\'], strict=False)\n    model.eval()\n    return model\n'"
lib/models/vibe.py,5,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport torch\nimport os.path as osp\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom lib.core.config import VIBE_DATA_DIR\nfrom lib.models.spin import Regressor, hmr\n\n\nclass TemporalEncoder(nn.Module):\n    def __init__(\n            self,\n            n_layers=1,\n            hidden_size=2048,\n            add_linear=False,\n            bidirectional=False,\n            use_residual=True\n    ):\n        super(TemporalEncoder, self).__init__()\n\n        self.gru = nn.GRU(\n            input_size=2048,\n            hidden_size=hidden_size,\n            bidirectional=bidirectional,\n            num_layers=n_layers\n        )\n\n        self.linear = None\n        if bidirectional:\n            self.linear = nn.Linear(hidden_size*2, 2048)\n        elif add_linear:\n            self.linear = nn.Linear(hidden_size, 2048)\n        self.use_residual = use_residual\n\n    def forward(self, x):\n        n,t,f = x.shape\n        x = x.permute(1,0,2) # NTF -> TNF\n        y, _ = self.gru(x)\n        if self.linear:\n            y = F.relu(y)\n            y = self.linear(y.view(-1, y.size(-1)))\n            y = y.view(t,n,f)\n        if self.use_residual and y.shape[-1] == 2048:\n            y = y + x\n        y = y.permute(1,0,2) # TNF -> NTF\n        return y\n\n\nclass VIBE(nn.Module):\n    def __init__(\n            self,\n            seqlen,\n            batch_size=64,\n            n_layers=1,\n            hidden_size=2048,\n            add_linear=False,\n            bidirectional=False,\n            use_residual=True,\n            pretrained=osp.join(VIBE_DATA_DIR, 'spin_model_checkpoint.pth.tar'),\n    ):\n\n        super(VIBE, self).__init__()\n\n        self.seqlen = seqlen\n        self.batch_size = batch_size\n\n        self.encoder = TemporalEncoder(\n            n_layers=n_layers,\n            hidden_size=hidden_size,\n            bidirectional=bidirectional,\n            add_linear=add_linear,\n            use_residual=use_residual,\n        )\n\n        # regressor can predict cam, pose and shape params in an iterative way\n        self.regressor = Regressor()\n\n        if pretrained and os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)['model']\n\n            self.regressor.load_state_dict(pretrained_dict, strict=False)\n            print(f'=> loaded pretrained model from \\'{pretrained}\\'')\n\n\n    def forward(self, input, J_regressor=None):\n        # input size NTF\n        batch_size, seqlen = input.shape[:2]\n\n        feature = self.encoder(input)\n        feature = feature.reshape(-1, feature.size(-1))\n\n        smpl_output = self.regressor(feature, J_regressor=J_regressor)\n        for s in smpl_output:\n            s['theta'] = s['theta'].reshape(batch_size, seqlen, -1)\n            s['verts'] = s['verts'].reshape(batch_size, seqlen, -1, 3)\n            s['kp_2d'] = s['kp_2d'].reshape(batch_size, seqlen, -1, 2)\n            s['kp_3d'] = s['kp_3d'].reshape(batch_size, seqlen, -1, 3)\n            s['rotmat'] = s['rotmat'].reshape(batch_size, seqlen, -1, 3, 3)\n\n        return smpl_output\n\n\nclass VIBE_Demo(nn.Module):\n    def __init__(\n            self,\n            seqlen,\n            batch_size=64,\n            n_layers=1,\n            hidden_size=2048,\n            add_linear=False,\n            bidirectional=False,\n            use_residual=True,\n            pretrained=osp.join(VIBE_DATA_DIR, 'spin_model_checkpoint.pth.tar'),\n    ):\n\n        super(VIBE_Demo, self).__init__()\n\n        self.seqlen = seqlen\n        self.batch_size = batch_size\n\n        self.encoder = TemporalEncoder(\n            n_layers=n_layers,\n            hidden_size=hidden_size,\n            bidirectional=bidirectional,\n            add_linear=add_linear,\n            use_residual=use_residual,\n        )\n\n        self.hmr = hmr()\n        checkpoint = torch.load(pretrained)\n        self.hmr.load_state_dict(checkpoint['model'], strict=False)\n\n        # regressor can predict cam, pose and shape params in an iterative way\n        self.regressor = Regressor()\n\n        if pretrained and os.path.isfile(pretrained):\n            pretrained_dict = torch.load(pretrained)['model']\n\n            self.regressor.load_state_dict(pretrained_dict, strict=False)\n            print(f'=> loaded pretrained model from \\'{pretrained}\\'')\n\n\n    def forward(self, input, J_regressor=None):\n        # input size NTF\n        batch_size, seqlen, nc, h, w = input.shape\n\n        feature = self.hmr.feature_extractor(input.reshape(-1, nc, h, w))\n\n        feature = feature.reshape(batch_size, seqlen, -1)\n        feature = self.encoder(feature)\n        feature = feature.reshape(-1, feature.size(-1))\n\n        smpl_output = self.regressor(feature, J_regressor=J_regressor)\n\n        for s in smpl_output:\n            s['theta'] = s['theta'].reshape(batch_size, seqlen, -1)\n            s['verts'] = s['verts'].reshape(batch_size, seqlen, -1, 3)\n            s['kp_2d'] = s['kp_2d'].reshape(batch_size, seqlen, -1, 2)\n            s['kp_3d'] = s['kp_3d'].reshape(batch_size, seqlen, -1, 3)\n            s['rotmat'] = s['rotmat'].reshape(batch_size, seqlen, -1, 3, 3)\n\n        return smpl_output\n"""
lib/smplify/losses.py,10,"b'# This script is the extended version of https://github.com/nkolot/SPIN/blob/master/smplify/losses.py to deal with\n# sequences inputs.\n\nimport torch\nfrom lib.models.spin import perspective_projection\nfrom lib.models.smpl import JOINT_IDS\n\n\ndef gmof(x, sigma):\n    """"""\n    Geman-McClure error function\n    """"""\n    x_squared = x ** 2\n    sigma_squared = sigma ** 2\n    return (sigma_squared * x_squared) / (sigma_squared + x_squared)\n\n\ndef angle_prior(pose):\n    """"""\n    Angle prior that penalizes unnatural bending of the knees and elbows\n    """"""\n    # We subtract 3 because pose does not include the global rotation of the model\n    return torch.exp(\n        pose[:, [55 - 3, 58 - 3, 12 - 3, 15 - 3]] * torch.tensor([1., -1., -1, -1.], device=pose.device)) ** 2\n\n\ndef body_fitting_loss(body_pose, betas, model_joints, camera_t, camera_center,\n                      joints_2d, joints_conf, pose_prior,\n                      focal_length=5000, sigma=100, pose_prior_weight=4.78,\n                      shape_prior_weight=5, angle_prior_weight=15.2,\n                      output=\'sum\'):\n    """"""\n    Loss function for body fitting\n    """"""\n    # pose_prior_weight = 1.\n    # shape_prior_weight = 1.\n    # angle_prior_weight = 1.\n    # sigma = 10.\n\n    batch_size = body_pose.shape[0]\n    rotation = torch.eye(3, device=body_pose.device).unsqueeze(0).expand(batch_size, -1, -1)\n    projected_joints = perspective_projection(model_joints, rotation, camera_t,\n                                              focal_length, camera_center)\n\n    # Weighted robust reprojection error\n    reprojection_error = gmof(projected_joints - joints_2d, sigma)\n    reprojection_loss = (joints_conf ** 2) * reprojection_error.sum(dim=-1)\n\n    # Pose prior loss\n    pose_prior_loss = (pose_prior_weight ** 2) * pose_prior(body_pose, betas)\n\n    # Angle prior for knees and elbows\n    angle_prior_loss = (angle_prior_weight ** 2) * angle_prior(body_pose).sum(dim=-1)\n\n    # Regularizer to prevent betas from taking large values\n    shape_prior_loss = (shape_prior_weight ** 2) * (betas ** 2).sum(dim=-1)\n\n    total_loss = reprojection_loss.sum(dim=-1) + pose_prior_loss + angle_prior_loss + shape_prior_loss\n    print(f\'joints: {reprojection_loss[0].sum().item():.2f}, \'\n          f\'pose_prior: {pose_prior_loss[0].item():.2f}, \'\n          f\'angle_prior: {angle_prior_loss[0].item():.2f}, \'\n          f\'shape_prior: {shape_prior_loss[0].item():.2f}\')\n\n    if output == \'sum\':\n        return total_loss.sum()\n    elif output == \'reprojection\':\n        return reprojection_loss\n\n\ndef camera_fitting_loss(model_joints, camera_t, camera_t_est, camera_center, joints_2d, joints_conf,\n                        focal_length=5000, depth_loss_weight=100):\n    """"""\n    Loss function for camera optimization.\n    """"""\n\n    # Project model joints\n    batch_size = model_joints.shape[0]\n    rotation = torch.eye(3, device=model_joints.device).unsqueeze(0).expand(batch_size, -1, -1)\n    projected_joints = perspective_projection(model_joints, rotation, camera_t,\n                                              focal_length, camera_center)\n\n    op_joints = [\'OP RHip\', \'OP LHip\', \'OP RShoulder\', \'OP LShoulder\']\n    op_joints_ind = [JOINT_IDS[joint] for joint in op_joints]\n    gt_joints = [\'Right Hip\', \'Left Hip\', \'Right Shoulder\', \'Left Shoulder\']\n    gt_joints_ind = [JOINT_IDS[joint] for joint in gt_joints]\n    reprojection_error_op = (joints_2d[:, op_joints_ind] -\n                             projected_joints[:, op_joints_ind]) ** 2\n    reprojection_error_gt = (joints_2d[:, gt_joints_ind] -\n                             projected_joints[:, gt_joints_ind]) ** 2\n\n    # Check if for each example in the batch all 4 OpenPose detections are valid, otherwise use the GT detections\n    # OpenPose joints are more reliable for this task, so we prefer to use them if possible\n    is_valid = (joints_conf[:, op_joints_ind].min(dim=-1)[0][:, None, None] > 0).float()\n    reprojection_loss = (is_valid * reprojection_error_op + (1 - is_valid) * reprojection_error_gt).sum(dim=(1, 2))\n\n    # Loss that penalizes deviation from depth estimate\n    depth_loss = (depth_loss_weight ** 2) * (camera_t[:, 2] - camera_t_est[:, 2]) ** 2\n\n    total_loss = reprojection_loss + depth_loss\n    return total_loss.sum()\n\n\ndef temporal_body_fitting_loss(body_pose, betas, model_joints, camera_t, camera_center,\n                               joints_2d, joints_conf, pose_prior,\n                               focal_length=5000, sigma=100, pose_prior_weight=4.78,\n                               shape_prior_weight=5, angle_prior_weight=15.2,\n                               smooth_2d_weight=0.01, smooth_3d_weight=1.0,\n                               output=\'sum\'):\n    """"""\n    Loss function for body fitting\n    """"""\n    # pose_prior_weight = 1.\n    # shape_prior_weight = 1.\n    # angle_prior_weight = 1.\n    # sigma = 10.\n\n    batch_size = body_pose.shape[0]\n    rotation = torch.eye(3, device=body_pose.device).unsqueeze(0).expand(batch_size, -1, -1)\n    projected_joints = perspective_projection(model_joints, rotation, camera_t,\n                                              focal_length, camera_center)\n\n    # Weighted robust reprojection error\n    reprojection_error = gmof(projected_joints - joints_2d, sigma)\n    reprojection_loss = (joints_conf ** 2) * reprojection_error.sum(dim=-1)\n\n    # Pose prior loss\n    pose_prior_loss = (pose_prior_weight ** 2) * pose_prior(body_pose, betas)\n\n    # Angle prior for knees and elbows\n    angle_prior_loss = (angle_prior_weight ** 2) * angle_prior(body_pose).sum(dim=-1)\n\n    # Regularizer to prevent betas from taking large values\n    shape_prior_loss = (shape_prior_weight ** 2) * (betas ** 2).sum(dim=-1)\n\n    total_loss = reprojection_loss.sum(dim=-1) + pose_prior_loss + angle_prior_loss + shape_prior_loss\n\n    # Smooth 2d joint loss\n    joint_conf_diff = joints_conf[1:]\n    joints_2d_diff = projected_joints[1:] - projected_joints[:-1]\n    smooth_j2d_loss = (joint_conf_diff ** 2) * joints_2d_diff.abs().sum(dim=-1)\n    smooth_j2d_loss = torch.cat(\n        [torch.zeros(1, smooth_j2d_loss.shape[1], device=body_pose.device), smooth_j2d_loss]\n    ).sum(dim=-1)\n    smooth_j2d_loss = (smooth_2d_weight ** 2) * smooth_j2d_loss\n\n    # Smooth 3d joint loss\n    joints_3d_diff = model_joints[1:] - model_joints[:-1]\n    # joints_3d_diff = joints_3d_diff * 100.\n    smooth_j3d_loss = (joint_conf_diff ** 2) * joints_3d_diff.abs().sum(dim=-1)\n    smooth_j3d_loss = torch.cat(\n        [torch.zeros(1, smooth_j3d_loss.shape[1], device=body_pose.device), smooth_j3d_loss]\n    ).sum(dim=-1)\n    smooth_j3d_loss = (smooth_3d_weight ** 2) * smooth_j3d_loss\n\n    total_loss += smooth_j2d_loss + smooth_j3d_loss\n\n    # print(f\'joints: {reprojection_loss[0].sum().item():.2f}, \'\n    #       f\'pose_prior: {pose_prior_loss[0].item():.2f}, \'\n    #       f\'angle_prior: {angle_prior_loss[0].item():.2f}, \'\n    #       f\'shape_prior: {shape_prior_loss[0].item():.2f}, \'\n    #       f\'smooth_j2d: {smooth_j2d_loss.sum().item()}, \'\n    #       f\'smooth_j3d: {smooth_j3d_loss.sum().item()}\')\n\n    if output == \'sum\':\n        return total_loss.sum()\n    elif output == \'reprojection\':\n        return reprojection_loss\n\n\ndef temporal_camera_fitting_loss(model_joints, camera_t, camera_t_est, camera_center, joints_2d, joints_conf,\n                                 focal_length=5000, depth_loss_weight=100):\n    """"""\n    Loss function for camera optimization.\n    """"""\n\n    # Project model joints\n    batch_size = model_joints.shape[0]\n    rotation = torch.eye(3, device=model_joints.device).unsqueeze(0).expand(batch_size, -1, -1)\n    projected_joints = perspective_projection(model_joints, rotation, camera_t,\n                                              focal_length, camera_center)\n\n    op_joints = [\'OP RHip\', \'OP LHip\', \'OP RShoulder\', \'OP LShoulder\']\n    op_joints_ind = [JOINT_IDS[joint] for joint in op_joints]\n    # gt_joints = [\'Right Hip\', \'Left Hip\', \'Right Shoulder\', \'Left Shoulder\']\n    # gt_joints_ind = [constants.JOINT_IDS[joint] for joint in gt_joints]\n    reprojection_error_op = (joints_2d[:, op_joints_ind] -\n                             projected_joints[:, op_joints_ind]) ** 2\n    # reprojection_error_gt = (joints_2d[:, gt_joints_ind] -\n    #                          projected_joints[:, gt_joints_ind]) ** 2\n\n    # Check if for each example in the batch all 4 OpenPose detections are valid, otherwise use the GT detections\n    # OpenPose joints are more reliable for this task, so we prefer to use them if possible\n    is_valid = (joints_conf[:, op_joints_ind].min(dim=-1)[0][:, None, None] > 0).float()\n    reprojection_loss = (is_valid * reprojection_error_op).sum(dim=(1, 2))\n\n    # Loss that penalizes deviation from depth estimate\n    depth_loss = (depth_loss_weight ** 2) * (camera_t[:, 2] - camera_t_est[:, 2]) ** 2\n\n    total_loss = reprojection_loss + depth_loss\n    return total_loss.sum()\n'"
lib/smplify/prior.py,27,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nfrom __future__ import absolute_import\nfrom __future__ import print_function\nfrom __future__ import division\n\nimport sys\nimport os\n\nimport time\nimport pickle\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\n\nDEFAULT_DTYPE = torch.float32\n\n\ndef create_prior(prior_type, **kwargs):\n    if prior_type == \'gmm\':\n        prior = MaxMixturePrior(**kwargs)\n    elif prior_type == \'l2\':\n        return L2Prior(**kwargs)\n    elif prior_type == \'angle\':\n        return SMPLifyAnglePrior(**kwargs)\n    elif prior_type == \'none\' or prior_type is None:\n        # Don\'t use any pose prior\n        def no_prior(*args, **kwargs):\n            return 0.0\n        prior = no_prior\n    else:\n        raise ValueError(\'Prior {}\'.format(prior_type) + \' is not implemented\')\n    return prior\n\n\nclass SMPLifyAnglePrior(nn.Module):\n    def __init__(self, dtype=torch.float32, **kwargs):\n        super(SMPLifyAnglePrior, self).__init__()\n\n        # Indices for the roration angle of\n        # 55: left elbow,  90deg bend at -np.pi/2\n        # 58: right elbow, 90deg bend at np.pi/2\n        # 12: left knee,   90deg bend at np.pi/2\n        # 15: right knee,  90deg bend at np.pi/2\n        angle_prior_idxs = np.array([55, 58, 12, 15], dtype=np.int64)\n        angle_prior_idxs = torch.tensor(angle_prior_idxs, dtype=torch.long)\n        self.register_buffer(\'angle_prior_idxs\', angle_prior_idxs)\n\n        angle_prior_signs = np.array([1, -1, -1, -1],\n                                     dtype=np.float32 if dtype == torch.float32\n                                     else np.float64)\n        angle_prior_signs = torch.tensor(angle_prior_signs,\n                                         dtype=dtype)\n        self.register_buffer(\'angle_prior_signs\', angle_prior_signs)\n\n    def forward(self, pose, with_global_pose=False):\n        \'\'\' Returns the angle prior loss for the given pose\n\n        Args:\n            pose: (Bx[23 + 1] * 3) torch tensor with the axis-angle\n            representation of the rotations of the joints of the SMPL model.\n        Kwargs:\n            with_global_pose: Whether the pose vector also contains the global\n            orientation of the SMPL model. If not then the indices must be\n            corrected.\n        Returns:\n            A sze (B) tensor containing the angle prior loss for each element\n            in the batch.\n        \'\'\'\n        angle_prior_idxs = self.angle_prior_idxs - (not with_global_pose) * 3\n        return torch.exp(pose[:, angle_prior_idxs] *\n                         self.angle_prior_signs).pow(2)\n\n\nclass L2Prior(nn.Module):\n    def __init__(self, dtype=DEFAULT_DTYPE, reduction=\'sum\', **kwargs):\n        super(L2Prior, self).__init__()\n\n    def forward(self, module_input, *args):\n        return torch.sum(module_input.pow(2))\n\n\nclass MaxMixturePrior(nn.Module):\n\n    def __init__(self, prior_folder=\'prior\',\n                 num_gaussians=6, dtype=DEFAULT_DTYPE, epsilon=1e-16,\n                 use_merged=True,\n                 **kwargs):\n        super(MaxMixturePrior, self).__init__()\n\n        if dtype == DEFAULT_DTYPE:\n            np_dtype = np.float32\n        elif dtype == torch.float64:\n            np_dtype = np.float64\n        else:\n            print(\'Unknown float type {}, exiting!\'.format(dtype))\n            sys.exit(-1)\n\n        self.num_gaussians = num_gaussians\n        self.epsilon = epsilon\n        self.use_merged = use_merged\n        gmm_fn = \'gmm_{:02d}.pkl\'.format(num_gaussians)\n\n        full_gmm_fn = os.path.join(prior_folder, gmm_fn)\n        if not os.path.exists(full_gmm_fn):\n            print(\'The path to the mixture prior ""{}""\'.format(full_gmm_fn) +\n                  \' does not exist, exiting!\')\n            sys.exit(-1)\n\n        with open(full_gmm_fn, \'rb\') as f:\n            gmm = pickle.load(f, encoding=\'latin1\')\n\n        if type(gmm) == dict:\n            means = gmm[\'means\'].astype(np_dtype)\n            covs = gmm[\'covars\'].astype(np_dtype)\n            weights = gmm[\'weights\'].astype(np_dtype)\n        elif \'sklearn.mixture.gmm.GMM\' in str(type(gmm)):\n            means = gmm.means_.astype(np_dtype)\n            covs = gmm.covars_.astype(np_dtype)\n            weights = gmm.weights_.astype(np_dtype)\n        else:\n            print(\'Unknown type for the prior: {}, exiting!\'.format(type(gmm)))\n            sys.exit(-1)\n\n        self.register_buffer(\'means\', torch.tensor(means, dtype=dtype))\n\n        self.register_buffer(\'covs\', torch.tensor(covs, dtype=dtype))\n\n        precisions = [np.linalg.inv(cov) for cov in covs]\n        precisions = np.stack(precisions).astype(np_dtype)\n\n        self.register_buffer(\'precisions\',\n                             torch.tensor(precisions, dtype=dtype))\n\n        # The constant term:\n        sqrdets = np.array([(np.sqrt(np.linalg.det(c)))\n                            for c in gmm[\'covars\']])\n        const = (2 * np.pi)**(69 / 2.)\n\n        nll_weights = np.asarray(gmm[\'weights\'] / (const *\n                                                   (sqrdets / sqrdets.min())))\n        nll_weights = torch.tensor(nll_weights, dtype=dtype).unsqueeze(dim=0)\n        self.register_buffer(\'nll_weights\', nll_weights)\n\n        weights = torch.tensor(gmm[\'weights\'], dtype=dtype).unsqueeze(dim=0)\n        self.register_buffer(\'weights\', weights)\n\n        self.register_buffer(\'pi_term\',\n                             torch.log(torch.tensor(2 * np.pi, dtype=dtype)))\n\n        cov_dets = [np.log(np.linalg.det(cov.astype(np_dtype)) + epsilon)\n                    for cov in covs]\n        self.register_buffer(\'cov_dets\',\n                             torch.tensor(cov_dets, dtype=dtype))\n\n        # The dimensionality of the random variable\n        self.random_var_dim = self.means.shape[1]\n\n    def get_mean(self):\n        \'\'\' Returns the mean of the mixture \'\'\'\n        mean_pose = torch.matmul(self.weights, self.means)\n        return mean_pose\n\n    def merged_log_likelihood(self, pose, betas):\n        diff_from_mean = pose.unsqueeze(dim=1) - self.means\n\n        prec_diff_prod = torch.einsum(\'mij,bmj->bmi\',\n                                      [self.precisions, diff_from_mean])\n        diff_prec_quadratic = (prec_diff_prod * diff_from_mean).sum(dim=-1)\n\n        curr_loglikelihood = 0.5 * diff_prec_quadratic - \\\n            torch.log(self.nll_weights)\n        #  curr_loglikelihood = 0.5 * (self.cov_dets.unsqueeze(dim=0) +\n        #  self.random_var_dim * self.pi_term +\n        #  diff_prec_quadratic\n        #  ) - torch.log(self.weights)\n\n        min_likelihood, _ = torch.min(curr_loglikelihood, dim=1)\n        return min_likelihood\n\n    def log_likelihood(self, pose, betas, *args, **kwargs):\n        \'\'\' Create graph operation for negative log-likelihood calculation\n        \'\'\'\n        likelihoods = []\n\n        for idx in range(self.num_gaussians):\n            mean = self.means[idx]\n            prec = self.precisions[idx]\n            cov = self.covs[idx]\n            diff_from_mean = pose - mean\n\n            curr_loglikelihood = torch.einsum(\'bj,ji->bi\',\n                                              [diff_from_mean, prec])\n            curr_loglikelihood = torch.einsum(\'bi,bi->b\',\n                                              [curr_loglikelihood,\n                                               diff_from_mean])\n            cov_term = torch.log(torch.det(cov) + self.epsilon)\n            curr_loglikelihood += 0.5 * (cov_term +\n                                         self.random_var_dim *\n                                         self.pi_term)\n            likelihoods.append(curr_loglikelihood)\n\n        log_likelihoods = torch.stack(likelihoods, dim=1)\n        min_idx = torch.argmin(log_likelihoods, dim=1)\n        weight_component = self.nll_weights[:, min_idx]\n        weight_component = -torch.log(weight_component)\n\n        return weight_component + log_likelihoods[:, min_idx]\n\n    def forward(self, pose, betas):\n        if self.use_merged:\n            return self.merged_log_likelihood(pose, betas)\n        else:\n            return self.log_likelihood(pose, betas)\n'"
lib/smplify/temporal_smplify.py,12,"b'# This script is the extended version of https://github.com/nkolot/SPIN/blob/master/smplify/smplify.py to deal with\n# sequences inputs.\n\nimport os\nimport torch\n\nfrom lib.core.config import VIBE_DATA_DIR\nfrom lib.models.smpl import SMPL, JOINT_IDS, SMPL_MODEL_DIR\nfrom lib.smplify.losses import temporal_camera_fitting_loss, temporal_body_fitting_loss\n\n# For the GMM prior, we use the GMM implementation of SMPLify-X\n# https://github.com/vchoutas/smplify-x/blob/master/smplifyx/prior.py\nfrom .prior import MaxMixturePrior\n\ndef arrange_betas(pose, betas):\n    batch_size = pose.shape[0]\n    num_video = betas.shape[0]\n\n    video_size = batch_size // num_video\n    betas_ext = torch.zeros(batch_size, betas.shape[-1], device=betas.device)\n    for i in range(num_video):\n        betas_ext[i*video_size:(i+1)*video_size] = betas[i]\n\n    return betas_ext\n\nclass TemporalSMPLify():\n    """"""Implementation of single-stage SMPLify.""""""\n\n    def __init__(self,\n                 step_size=1e-2,\n                 batch_size=66,\n                 num_iters=100,\n                 focal_length=5000,\n                 use_lbfgs=True,\n                 device=torch.device(\'cuda\'),\n                 max_iter=20):\n\n        # Store options\n        self.device = device\n        self.focal_length = focal_length\n        self.step_size = step_size\n        self.max_iter = max_iter\n        # Ignore the the following joints for the fitting process\n        ign_joints = [\'OP Neck\', \'OP RHip\', \'OP LHip\', \'Right Hip\', \'Left Hip\']\n        self.ign_joints = [JOINT_IDS[i] for i in ign_joints]\n        self.num_iters = num_iters\n\n        # GMM pose prior\n        self.pose_prior = MaxMixturePrior(prior_folder=VIBE_DATA_DIR,\n                                          num_gaussians=8,\n                                          dtype=torch.float32).to(device)\n        self.use_lbfgs = use_lbfgs\n        # Load SMPL model\n        self.smpl = SMPL(SMPL_MODEL_DIR,\n                         batch_size=batch_size,\n                         create_transl=False).to(self.device)\n\n    def __call__(self, init_pose, init_betas, init_cam_t, camera_center, keypoints_2d):\n        """"""Perform body fitting.\n        Input:\n            init_pose: SMPL pose estimate\n            init_betas: SMPL betas estimate\n            init_cam_t: Camera translation estimate\n            camera_center: Camera center location\n            keypoints_2d: Keypoints used for the optimization\n        Returns:\n            vertices: Vertices of optimized shape\n            joints: 3D joints of optimized shape\n            pose: SMPL pose parameters of optimized shape\n            betas: SMPL beta parameters of optimized shape\n            camera_translation: Camera translation\n            reprojection_loss: Final joint reprojection loss\n        """"""\n\n        # Make camera translation a learnable parameter\n        camera_translation = init_cam_t.clone()\n\n        # Get joint confidence\n        joints_2d = keypoints_2d[:, :, :2]\n        joints_conf = keypoints_2d[:, :, -1]\n\n        # Split SMPL pose to body pose and global orientation\n        body_pose = init_pose[:, 3:].detach().clone()\n        global_orient = init_pose[:, :3].detach().clone()\n        betas = init_betas.detach().clone()\n\n        # Step 1: Optimize camera translation and body orientation\n        # Optimize only camera translation and body orientation\n        body_pose.requires_grad = False\n        betas.requires_grad = False\n        global_orient.requires_grad = True\n        camera_translation.requires_grad = True\n\n        camera_opt_params = [global_orient, camera_translation]\n\n        if self.use_lbfgs:\n            camera_optimizer = torch.optim.LBFGS(camera_opt_params, max_iter=self.max_iter,\n                                                 lr=self.step_size, line_search_fn=\'strong_wolfe\')\n            for i in range(self.num_iters):\n                def closure():\n                    camera_optimizer.zero_grad()\n                    betas_ext = arrange_betas(body_pose, betas)\n                    smpl_output = self.smpl(global_orient=global_orient,\n                                            body_pose=body_pose,\n                                            betas=betas_ext)\n                    model_joints = smpl_output.joints\n\n\n                    loss = temporal_camera_fitting_loss(model_joints, camera_translation,\n                                               init_cam_t, camera_center,\n                                               joints_2d, joints_conf, focal_length=self.focal_length)\n                    loss.backward()\n                    return loss\n\n                camera_optimizer.step(closure)\n        else:\n            camera_optimizer = torch.optim.Adam(camera_opt_params, lr=self.step_size, betas=(0.9, 0.999))\n\n            for i in range(self.num_iters):\n                betas_ext = arrange_betas(body_pose, betas)\n                smpl_output = self.smpl(global_orient=global_orient,\n                                        body_pose=body_pose,\n                                        betas=betas_ext)\n                model_joints = smpl_output.joints\n                loss = temporal_camera_fitting_loss(model_joints, camera_translation,\n                                           init_cam_t, camera_center,\n                                           joints_2d, joints_conf, focal_length=self.focal_length)\n                camera_optimizer.zero_grad()\n                loss.backward()\n                camera_optimizer.step()\n\n        # Fix camera translation after optimizing camera\n        camera_translation.requires_grad = False\n\n        # Step 2: Optimize body joints\n        # Optimize only the body pose and global orientation of the body\n        body_pose.requires_grad = True\n        betas.requires_grad = True\n        global_orient.requires_grad = True\n        camera_translation.requires_grad = False\n        body_opt_params = [body_pose, betas, global_orient]\n\n        # For joints ignored during fitting, set the confidence to 0\n        joints_conf[:, self.ign_joints] = 0.\n\n        if self.use_lbfgs:\n            body_optimizer = torch.optim.LBFGS(body_opt_params, max_iter=self.max_iter,\n                                               lr=self.step_size, line_search_fn=\'strong_wolfe\')\n            for i in range(self.num_iters):\n                def closure():\n                    body_optimizer.zero_grad()\n                    betas_ext = arrange_betas(body_pose, betas)\n                    smpl_output = self.smpl(global_orient=global_orient,\n                                            body_pose=body_pose,\n                                            betas=betas_ext)\n                    model_joints = smpl_output.joints\n\n                    loss = temporal_body_fitting_loss(body_pose, betas, model_joints, camera_translation, camera_center,\n                                             joints_2d, joints_conf, self.pose_prior,\n                                             focal_length=self.focal_length)\n                    loss.backward()\n                    return loss\n\n                body_optimizer.step(closure)\n        else:\n            body_optimizer = torch.optim.Adam(body_opt_params, lr=self.step_size, betas=(0.9, 0.999))\n\n            for i in range(self.num_iters):\n                betas_ext = arrange_betas(body_pose, betas)\n                smpl_output = self.smpl(global_orient=global_orient,\n                                        body_pose=body_pose,\n                                        betas=betas_ext)\n                model_joints = smpl_output.joints\n                loss = temporal_body_fitting_loss(body_pose, betas, model_joints, camera_translation, camera_center,\n                                         joints_2d, joints_conf, self.pose_prior,\n                                         focal_length=self.focal_length)\n                body_optimizer.zero_grad()\n                loss.backward()\n                body_optimizer.step()\n                # scheduler.step(epoch=i)\n\n        # Get final loss value\n\n        with torch.no_grad():\n            betas_ext = arrange_betas(body_pose, betas)\n            smpl_output = self.smpl(global_orient=global_orient,\n                                    body_pose=body_pose,\n                                    betas=betas_ext, return_full_pose=True)\n            model_joints = smpl_output.joints\n            reprojection_loss = temporal_body_fitting_loss(body_pose, betas, model_joints, camera_translation,\n                                                           camera_center,\n                                                           joints_2d, joints_conf, self.pose_prior,\n                                                           focal_length=self.focal_length,\n                                                           output=\'reprojection\')\n\n        vertices = smpl_output.vertices.detach()\n        joints = smpl_output.joints.detach()\n        pose = torch.cat([global_orient, body_pose], dim=-1).detach()\n        betas = betas.detach()\n\n        # Back to weak perspective camera\n        camera_translation = torch.stack([\n            2 * 5000. / (224 * camera_translation[:,2] + 1e-9),\n            camera_translation[:,0], camera_translation[:,1]\n        ], dim=-1)\n\n        betas = betas.repeat(pose.shape[0],1)\n        output = {\n            \'theta\': torch.cat([camera_translation, pose, betas], dim=1),\n            \'verts\': vertices,\n            \'kp_3d\': joints,\n        }\n\n        return output, reprojection_loss\n        # return vertices, joints, pose, betas, camera_translation, reprojection_loss\n\n    def get_fitting_loss(self, pose, betas, cam_t, camera_center, keypoints_2d):\n        """"""Given body and camera parameters, compute reprojection loss value.\n        Input:\n            pose: SMPL pose parameters\n            betas: SMPL beta parameters\n            cam_t: Camera translation\n            camera_center: Camera center location\n            keypoints_2d: Keypoints used for the optimization\n        Returns:\n            reprojection_loss: Final joint reprojection loss\n        """"""\n\n        batch_size = pose.shape[0]\n\n        # Get joint confidence\n        joints_2d = keypoints_2d[:, :, :2]\n        joints_conf = keypoints_2d[:, :, -1]\n        # For joints ignored during fitting, set the confidence to 0\n        joints_conf[:, self.ign_joints] = 0.\n\n        # Split SMPL pose to body pose and global orientation\n        body_pose = pose[:, 3:]\n        global_orient = pose[:, :3]\n\n        with torch.no_grad():\n            smpl_output = self.smpl(global_orient=global_orient,\n                                    body_pose=body_pose,\n                                    betas=betas, return_full_pose=True)\n            model_joints = smpl_output.joints\n            reprojection_loss = temporal_body_fitting_loss(body_pose, betas, model_joints, cam_t, camera_center,\n                                                  joints_2d, joints_conf, self.pose_prior,\n                                                  focal_length=self.focal_length,\n                                                  output=\'reprojection\')\n\n        return reprojection_loss\n'"
lib/utils/__init__.py,0,b''
lib/utils/demo_utils.py,6,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport cv2\nimport time\nimport json\nimport torch\nimport subprocess\nimport numpy as np\nimport os.path as osp\nfrom pytube import YouTube\nfrom collections import OrderedDict\n\nfrom lib.utils.smooth_bbox import get_smooth_bbox_params, get_all_bbox_params\nfrom lib.data_utils.img_utils import get_single_image_crop_demo\nfrom lib.utils.geometry import rotation_matrix_to_angle_axis\nfrom lib.smplify.temporal_smplify import TemporalSMPLify\n\n\ndef preprocess_video(video, joints2d, bboxes, frames, scale=1.0, crop_size=224):\n    """"""\n    Read video, do normalize and crop it according to the bounding box.\n    If there are bounding box annotations, use them to crop the image.\n    If no bounding box is specified but openpose detections are available, use them to get the bounding box.\n\n    :param video (ndarray): input video\n    :param joints2d (ndarray, NxJx3): openpose detections\n    :param bboxes (ndarray, Nx5): bbox detections\n    :param scale (float): bbox crop scaling factor\n    :param crop_size (int): crop width and height\n    :return: cropped video, cropped and normalized video, modified bboxes, modified joints2d\n    """"""\n\n    if joints2d is not None:\n        bboxes, time_pt1, time_pt2 = get_all_bbox_params(joints2d, vis_thresh=0.3)\n        bboxes[:,2:] = 150. / bboxes[:,2:]\n        bboxes = np.stack([bboxes[:,0], bboxes[:,1], bboxes[:,2], bboxes[:,2]]).T\n\n        video = video[time_pt1:time_pt2]\n        joints2d = joints2d[time_pt1:time_pt2]\n        frames = frames[time_pt1:time_pt2]\n\n    shape = video.shape\n\n    temp_video = np.zeros((shape[0], crop_size, crop_size, shape[-1]))\n    norm_video = torch.zeros(shape[0], shape[-1], crop_size, crop_size)\n\n    for idx in range(video.shape[0]):\n\n        img = video[idx]\n        bbox = bboxes[idx]\n\n        j2d = joints2d[idx] if joints2d is not None else None\n\n        norm_img, raw_img, kp_2d = get_single_image_crop_demo(\n            img,\n            bbox,\n            kp_2d=j2d,\n            scale=scale,\n            crop_size=crop_size)\n\n        if joints2d is not None:\n            joints2d[idx] = kp_2d\n\n        temp_video[idx] = raw_img\n        norm_video[idx] = norm_img\n\n    temp_video = temp_video.astype(np.uint8)\n\n    return temp_video, norm_video, bboxes, joints2d, frames\n\n\ndef download_youtube_clip(url, download_folder):\n    return YouTube(url).streams.first().download(output_path=download_folder)\n\n\ndef smplify_runner(\n        pred_rotmat,\n        pred_betas,\n        pred_cam,\n        j2d,\n        device,\n        batch_size,\n        lr=1.0,\n        opt_steps=1,\n        use_lbfgs=True,\n        pose2aa=True\n):\n    smplify = TemporalSMPLify(\n        step_size=lr,\n        batch_size=batch_size,\n        num_iters=opt_steps,\n        focal_length=5000.,\n        use_lbfgs=use_lbfgs,\n        device=device,\n        # max_iter=10,\n    )\n    # Convert predicted rotation matrices to axis-angle\n    if pose2aa:\n        pred_pose = rotation_matrix_to_angle_axis(pred_rotmat.detach()).reshape(batch_size, -1)\n    else:\n        pred_pose = pred_rotmat\n\n    # Calculate camera parameters for smplify\n    pred_cam_t = torch.stack([\n        pred_cam[:, 1], pred_cam[:, 2],\n        2 * 5000 / (224 * pred_cam[:, 0] + 1e-9)\n    ], dim=-1)\n\n    gt_keypoints_2d_orig = j2d\n    # Before running compute reprojection error of the network\n    opt_joint_loss = smplify.get_fitting_loss(\n        pred_pose.detach(), pred_betas.detach(),\n        pred_cam_t.detach(),\n        0.5 * 224 * torch.ones(batch_size, 2, device=device),\n        gt_keypoints_2d_orig).mean(dim=-1)\n\n    best_prediction_id = torch.argmin(opt_joint_loss).item()\n    pred_betas = pred_betas[best_prediction_id].unsqueeze(0)\n    # pred_betas = pred_betas[best_prediction_id:best_prediction_id+2] # .unsqueeze(0)\n    # top5_best_idxs = torch.topk(opt_joint_loss, 5, largest=False)[1]\n    # breakpoint()\n\n    start = time.time()\n    # Run SMPLify optimization initialized from the network prediction\n    # new_opt_vertices, new_opt_joints, \\\n    # new_opt_pose, new_opt_betas, \\\n    # new_opt_cam_t, \\\n    output, new_opt_joint_loss = smplify(\n        pred_pose.detach(), pred_betas.detach(),\n        pred_cam_t.detach(),\n        0.5 * 224 * torch.ones(batch_size, 2, device=device),\n        gt_keypoints_2d_orig,\n    )\n    new_opt_joint_loss = new_opt_joint_loss.mean(dim=-1)\n    # smplify_time = time.time() - start\n    # print(f\'Smplify time: {smplify_time}\')\n    # Will update the dictionary for the examples where the new loss is less than the current one\n    update = (new_opt_joint_loss < opt_joint_loss)\n\n    new_opt_vertices = output[\'verts\']\n    new_opt_cam_t = output[\'theta\'][:,:3]\n    new_opt_pose = output[\'theta\'][:,3:75]\n    new_opt_betas = output[\'theta\'][:,75:]\n    new_opt_joints3d = output[\'kp_3d\']\n\n    return_val = [\n        update, new_opt_vertices.cpu(), new_opt_cam_t.cpu(),\n        new_opt_pose.cpu(), new_opt_betas.cpu(), new_opt_joints3d.cpu(),\n        new_opt_joint_loss, opt_joint_loss,\n    ]\n\n    return return_val\n\n\ndef trim_videos(filename, start_time, end_time, output_filename):\n    command = [\'ffmpeg\',\n               \'-i\', \'""%s""\' % filename,\n               \'-ss\', str(start_time),\n               \'-t\', str(end_time - start_time),\n               \'-c:v\', \'libx264\', \'-c:a\', \'copy\',\n               \'-threads\', \'1\',\n               \'-loglevel\', \'panic\',\n               \'""%s""\' % output_filename]\n    # command = \' \'.join(command)\n    subprocess.call(command)\n\n\ndef video_to_images(vid_file, img_folder=None, return_info=False):\n    if img_folder is None:\n        img_folder = osp.join(\'/tmp\', osp.basename(vid_file).replace(\'.\', \'_\'))\n\n    os.makedirs(img_folder, exist_ok=True)\n\n    command = [\'ffmpeg\',\n               \'-i\', vid_file,\n               \'-f\', \'image2\',\n               \'-v\', \'error\',\n               f\'{img_folder}/%06d.png\']\n    print(f\'Running \\""{"" "".join(command)}\\""\')\n    subprocess.call(command)\n\n    print(f\'Images saved to \\""{img_folder}\\""\')\n\n    img_shape = cv2.imread(osp.join(img_folder, \'000001.png\')).shape\n\n    if return_info:\n        return img_folder, len(os.listdir(img_folder)), img_shape\n    else:\n        return img_folder\n\n\ndef download_url(url, outdir):\n    print(f\'Downloading files from {url}\')\n    cmd = [\'wget\', \'-c\', url, \'-P\', outdir]\n    subprocess.call(cmd)\n\n\ndef download_ckpt(outdir=\'data/vibe_data\', use_3dpw=False):\n    os.makedirs(outdir, exist_ok=True)\n\n    if use_3dpw:\n        ckpt_file = \'data/vibe_data/vibe_model_w_3dpw.pth.tar\'\n        url = \'https://www.dropbox.com/s/41ozgqorcp095ja/vibe_model_w_3dpw.pth.tar\'\n        if not os.path.isfile(ckpt_file):\n            download_url(url=url, outdir=outdir)\n    else:\n        ckpt_file = \'data/vibe_data/vibe_model_wo_3dpw.pth.tar\'\n        url = \'https://www.dropbox.com/s/amj2p8bmf6g56k6/vibe_model_wo_3dpw.pth.tar\'\n        if not os.path.isfile(ckpt_file):\n            download_url(url=url, outdir=outdir)\n\n    return ckpt_file\n\n\ndef images_to_video(img_folder, output_vid_file):\n    os.makedirs(img_folder, exist_ok=True)\n\n    command = [\n        \'ffmpeg\', \'-y\', \'-threads\', \'16\', \'-i\', f\'{img_folder}/%06d.png\', \'-profile:v\', \'baseline\',\n        \'-level\', \'3.0\', \'-c:v\', \'libx264\', \'-pix_fmt\', \'yuv420p\', \'-an\', \'-v\', \'error\', output_vid_file,\n    ]\n\n    print(f\'Running \\""{"" "".join(command)}\\""\')\n    subprocess.call(command)\n\n\ndef convert_crop_cam_to_orig_img(cam, bbox, img_width, img_height):\n    \'\'\'\n    Convert predicted camera from cropped image coordinates\n    to original image coordinates\n    :param cam (ndarray, shape=(3,)): weak perspective camera in cropped img coordinates\n    :param bbox (ndarray, shape=(4,)): bbox coordinates (c_x, c_y, h)\n    :param img_width (int): original image width\n    :param img_height (int): original image height\n    :return:\n    \'\'\'\n    cx, cy, h = bbox[:,0], bbox[:,1], bbox[:,2]\n    hw, hh = img_width / 2., img_height / 2.\n    sx = cam[:,0] * (1. / (img_width / h))\n    sy = cam[:,0] * (1. / (img_height / h))\n    tx = ((cx - hw) / hw / sx) + cam[:,1]\n    ty = ((cy - hh) / hh / sy) + cam[:,2]\n    orig_cam = np.stack([sx, sy, tx, ty]).T\n    return orig_cam\n\n\ndef prepare_rendering_results(vibe_results, nframes):\n    frame_results = [{} for _ in range(nframes)]\n    for person_id, person_data in vibe_results.items():\n        for idx, frame_id in enumerate(person_data[\'frame_ids\']):\n            frame_results[frame_id][person_id] = {\n                \'verts\': person_data[\'verts\'][idx],\n                \'cam\': person_data[\'orig_cam\'][idx],\n            }\n\n    # naive depth ordering based on the scale of the weak perspective camera\n    for frame_id, frame_data in enumerate(frame_results):\n        # sort based on y-scale of the cam in original image coords\n        sort_idx = np.argsort([v[\'cam\'][1] for k,v in frame_data.items()])\n        frame_results[frame_id] = OrderedDict(\n            {list(frame_data.keys())[i]:frame_data[list(frame_data.keys())[i]] for i in sort_idx}\n        )\n\n    return frame_results\n'"
lib/utils/eval_utils.py,14,"b'# Some functions are borrowed from https://github.com/akanazawa/human_dynamics/blob/master/src/evaluation/eval_util.py\n# Adhere to their licence to use these functions\n\nimport torch\nimport numpy as np\n\n\ndef compute_accel(joints):\n    """"""\n    Computes acceleration of 3D joints.\n    Args:\n        joints (Nx25x3).\n    Returns:\n        Accelerations (N-2).\n    """"""\n    velocities = joints[1:] - joints[:-1]\n    acceleration = velocities[1:] - velocities[:-1]\n    acceleration_normed = np.linalg.norm(acceleration, axis=2)\n    return np.mean(acceleration_normed, axis=1)\n\n\ndef compute_error_accel(joints_gt, joints_pred, vis=None):\n    """"""\n    Computes acceleration error:\n        1/(n-2) \\sum_{i=1}^{n-1} X_{i-1} - 2X_i + X_{i+1}\n    Note that for each frame that is not visible, three entries in the\n    acceleration error should be zero\'d out.\n    Args:\n        joints_gt (Nx14x3).\n        joints_pred (Nx14x3).\n        vis (N).\n    Returns:\n        error_accel (N-2).\n    """"""\n    # (N-2)x14x3\n    accel_gt = joints_gt[:-2] - 2 * joints_gt[1:-1] + joints_gt[2:]\n    accel_pred = joints_pred[:-2] - 2 * joints_pred[1:-1] + joints_pred[2:]\n\n    normed = np.linalg.norm(accel_pred - accel_gt, axis=2)\n\n    if vis is None:\n        new_vis = np.ones(len(normed), dtype=bool)\n    else:\n        invis = np.logical_not(vis)\n        invis1 = np.roll(invis, -1)\n        invis2 = np.roll(invis, -2)\n        new_invis = np.logical_or(invis, np.logical_or(invis1, invis2))[:-2]\n        new_vis = np.logical_not(new_invis)\n\n    return np.mean(normed[new_vis], axis=1)\n\n\ndef compute_error_verts(pred_verts, target_verts=None, target_theta=None):\n    """"""\n    Computes MPJPE over 6890 surface vertices.\n    Args:\n        verts_gt (Nx6890x3).\n        verts_pred (Nx6890x3).\n    Returns:\n        error_verts (N).\n    """"""\n\n    if target_verts is None:\n        from lib.models.smpl import SMPL_MODEL_DIR\n        from lib.models.smpl import SMPL\n        device = \'cpu\'\n        smpl = SMPL(\n            SMPL_MODEL_DIR,\n            batch_size=1, # target_theta.shape[0],\n        ).to(device)\n\n        betas = torch.from_numpy(target_theta[:,75:]).to(device)\n        pose = torch.from_numpy(target_theta[:,3:75]).to(device)\n\n        target_verts = []\n        b_ = torch.split(betas, 5000)\n        p_ = torch.split(pose, 5000)\n\n        for b,p in zip(b_,p_):\n            output = smpl(betas=b, body_pose=p[:, 3:], global_orient=p[:, :3], pose2rot=True)\n            target_verts.append(output.vertices.detach().cpu().numpy())\n\n        target_verts = np.concatenate(target_verts, axis=0)\n\n    assert len(pred_verts) == len(target_verts)\n    error_per_vert = np.sqrt(np.sum((target_verts - pred_verts) ** 2, axis=2))\n    return np.mean(error_per_vert, axis=1)\n\n\ndef compute_similarity_transform(S1, S2):\n    \'\'\'\n    Computes a similarity transform (sR, t) that takes\n    a set of 3D points S1 (3 x N) closest to a set of 3D points S2,\n    where R is an 3x3 rotation matrix, t 3x1 translation, s scale.\n    i.e. solves the orthogonal Procrutes problem.\n    \'\'\'\n    transposed = False\n    if S1.shape[0] != 3 and S1.shape[0] != 2:\n        S1 = S1.T\n        S2 = S2.T\n        transposed = True\n    assert(S2.shape[1] == S1.shape[1])\n\n    # 1. Remove mean.\n    mu1 = S1.mean(axis=1, keepdims=True)\n    mu2 = S2.mean(axis=1, keepdims=True)\n    X1 = S1 - mu1\n    X2 = S2 - mu2\n\n    # 2. Compute variance of X1 used for scale.\n    var1 = np.sum(X1**2)\n\n    # 3. The outer product of X1 and X2.\n    K = X1.dot(X2.T)\n\n    # 4. Solution that Maximizes trace(R\'K) is R=U*V\', where U, V are\n    # singular vectors of K.\n    U, s, Vh = np.linalg.svd(K)\n    V = Vh.T\n    # Construct Z that fixes the orientation of R to get det(R)=1.\n    Z = np.eye(U.shape[0])\n    Z[-1, -1] *= np.sign(np.linalg.det(U.dot(V.T)))\n    # Construct R.\n    R = V.dot(Z.dot(U.T))\n\n    # 5. Recover scale.\n    scale = np.trace(R.dot(K)) / var1\n\n    # 6. Recover translation.\n    t = mu2 - scale*(R.dot(mu1))\n\n    # 7. Error:\n    S1_hat = scale*R.dot(S1) + t\n\n    if transposed:\n        S1_hat = S1_hat.T\n\n    return S1_hat\n\n\ndef compute_similarity_transform_torch(S1, S2):\n    \'\'\'\n    Computes a similarity transform (sR, t) that takes\n    a set of 3D points S1 (3 x N) closest to a set of 3D points S2,\n    where R is an 3x3 rotation matrix, t 3x1 translation, s scale.\n    i.e. solves the orthogonal Procrutes problem.\n    \'\'\'\n    transposed = False\n    if S1.shape[0] != 3 and S1.shape[0] != 2:\n        S1 = S1.T\n        S2 = S2.T\n        transposed = True\n    assert (S2.shape[1] == S1.shape[1])\n\n    # 1. Remove mean.\n    mu1 = S1.mean(axis=1, keepdims=True)\n    mu2 = S2.mean(axis=1, keepdims=True)\n    X1 = S1 - mu1\n    X2 = S2 - mu2\n\n    # print(\'X1\', X1.shape)\n\n    # 2. Compute variance of X1 used for scale.\n    var1 = torch.sum(X1 ** 2)\n\n    # print(\'var\', var1.shape)\n\n    # 3. The outer product of X1 and X2.\n    K = X1.mm(X2.T)\n\n    # 4. Solution that Maximizes trace(R\'K) is R=U*V\', where U, V are\n    # singular vectors of K.\n    U, s, V = torch.svd(K)\n    # V = Vh.T\n    # Construct Z that fixes the orientation of R to get det(R)=1.\n    Z = torch.eye(U.shape[0], device=S1.device)\n    Z[-1, -1] *= torch.sign(torch.det(U @ V.T))\n    # Construct R.\n    R = V.mm(Z.mm(U.T))\n\n    # print(\'R\', X1.shape)\n\n    # 5. Recover scale.\n    scale = torch.trace(R.mm(K)) / var1\n    # print(R.shape, mu1.shape)\n    # 6. Recover translation.\n    t = mu2 - scale * (R.mm(mu1))\n    # print(t.shape)\n\n    # 7. Error:\n    S1_hat = scale * R.mm(S1) + t\n\n    if transposed:\n        S1_hat = S1_hat.T\n\n    return S1_hat\n\n\ndef batch_compute_similarity_transform_torch(S1, S2):\n    \'\'\'\n    Computes a similarity transform (sR, t) that takes\n    a set of 3D points S1 (3 x N) closest to a set of 3D points S2,\n    where R is an 3x3 rotation matrix, t 3x1 translation, s scale.\n    i.e. solves the orthogonal Procrutes problem.\n    \'\'\'\n    transposed = False\n    if S1.shape[0] != 3 and S1.shape[0] != 2:\n        S1 = S1.permute(0,2,1)\n        S2 = S2.permute(0,2,1)\n        transposed = True\n    assert(S2.shape[1] == S1.shape[1])\n\n    # 1. Remove mean.\n    mu1 = S1.mean(axis=-1, keepdims=True)\n    mu2 = S2.mean(axis=-1, keepdims=True)\n\n    X1 = S1 - mu1\n    X2 = S2 - mu2\n\n    # 2. Compute variance of X1 used for scale.\n    var1 = torch.sum(X1**2, dim=1).sum(dim=1)\n\n    # 3. The outer product of X1 and X2.\n    K = X1.bmm(X2.permute(0,2,1))\n\n    # 4. Solution that Maximizes trace(R\'K) is R=U*V\', where U, V are\n    # singular vectors of K.\n    U, s, V = torch.svd(K)\n\n    # Construct Z that fixes the orientation of R to get det(R)=1.\n    Z = torch.eye(U.shape[1], device=S1.device).unsqueeze(0)\n    Z = Z.repeat(U.shape[0],1,1)\n    Z[:,-1, -1] *= torch.sign(torch.det(U.bmm(V.permute(0,2,1))))\n\n    # Construct R.\n    R = V.bmm(Z.bmm(U.permute(0,2,1)))\n\n    # 5. Recover scale.\n    scale = torch.cat([torch.trace(x).unsqueeze(0) for x in R.bmm(K)]) / var1\n\n    # 6. Recover translation.\n    t = mu2 - (scale.unsqueeze(-1).unsqueeze(-1) * (R.bmm(mu1)))\n\n    # 7. Error:\n    S1_hat = scale.unsqueeze(-1).unsqueeze(-1) * R.bmm(S1) + t\n\n    if transposed:\n        S1_hat = S1_hat.permute(0,2,1)\n\n    return S1_hat\n\n\ndef align_by_pelvis(joints):\n    """"""\n    Assumes joints is 14 x 3 in LSP order.\n    Then hips are: [3, 2]\n    Takes mid point of these points, then subtracts it.\n    """"""\n\n    left_id = 2\n    right_id = 3\n\n    pelvis = (joints[left_id, :] + joints[right_id, :]) / 2.0\n    return joints - np.expand_dims(pelvis, axis=0)\n\n\ndef compute_errors(gt3ds, preds):\n    """"""\n    Gets MPJPE after pelvis alignment + MPJPE after Procrustes.\n    Evaluates on the 14 common joints.\n    Inputs:\n      - gt3ds: N x 14 x 3\n      - preds: N x 14 x 3\n    """"""\n    errors, errors_pa = [], []\n    for i, (gt3d, pred) in enumerate(zip(gt3ds, preds)):\n        gt3d = gt3d.reshape(-1, 3)\n        # Root align.\n        gt3d = align_by_pelvis(gt3d)\n        pred3d = align_by_pelvis(pred)\n\n        joint_error = np.sqrt(np.sum((gt3d - pred3d)**2, axis=1))\n        errors.append(np.mean(joint_error))\n\n        # Get PA error.\n        pred3d_sym = compute_similarity_transform(pred3d, gt3d)\n        pa_error = np.sqrt(np.sum((gt3d - pred3d_sym)**2, axis=1))\n        errors_pa.append(np.mean(pa_error))\n\n    return errors, errors_pa\n'"
lib/utils/geometry.py,48,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport torch\nimport numpy as np\nfrom torch.nn import functional as F\n\n\ndef batch_rodrigues(axisang):\n    # This function is borrowed from https://github.com/MandyMo/pytorch_HMR/blob/master/src/util.py#L37\n    # axisang N x 3\n    axisang_norm = torch.norm(axisang + 1e-8, p=2, dim=1)\n    angle = torch.unsqueeze(axisang_norm, -1)\n    axisang_normalized = torch.div(axisang, angle)\n    angle = angle * 0.5\n    v_cos = torch.cos(angle)\n    v_sin = torch.sin(angle)\n    quat = torch.cat([v_cos, v_sin * axisang_normalized], dim=1)\n    rot_mat = quat2mat(quat)\n    rot_mat = rot_mat.view(rot_mat.shape[0], 9)\n    return rot_mat\n\n\ndef quat2mat(quat):\n    """"""\n    This function is borrowed from https://github.com/MandyMo/pytorch_HMR/blob/master/src/util.py#L50\n\n    Convert quaternion coefficients to rotation matrix.\n    Args:\n        quat: size = [batch_size, 4] 4 <===>(w, x, y, z)\n    Returns:\n        Rotation matrix corresponding to the quaternion -- size = [batch_size, 3, 3]\n    """"""\n    norm_quat = quat\n    norm_quat = norm_quat / norm_quat.norm(p=2, dim=1, keepdim=True)\n    w, x, y, z = norm_quat[:, 0], norm_quat[:, 1], norm_quat[:,\n                                                             2], norm_quat[:,\n                                                                           3]\n\n    batch_size = quat.size(0)\n\n    w2, x2, y2, z2 = w.pow(2), x.pow(2), y.pow(2), z.pow(2)\n    wx, wy, wz = w * x, w * y, w * z\n    xy, xz, yz = x * y, x * z, y * z\n\n    rotMat = torch.stack([\n        w2 + x2 - y2 - z2, 2 * xy - 2 * wz, 2 * wy + 2 * xz, 2 * wz + 2 * xy,\n        w2 - x2 + y2 - z2, 2 * yz - 2 * wx, 2 * xz - 2 * wy, 2 * wx + 2 * yz,\n        w2 - x2 - y2 + z2\n    ],\n                         dim=1).view(batch_size, 3, 3)\n    return rotMat\n\n\ndef rotation_matrix_to_angle_axis(rotation_matrix):\n    """"""\n    This function is borrowed from https://github.com/kornia/kornia\n\n    Convert 3x4 rotation matrix to Rodrigues vector\n\n    Args:\n        rotation_matrix (Tensor): rotation matrix.\n\n    Returns:\n        Tensor: Rodrigues vector transformation.\n\n    Shape:\n        - Input: :math:`(N, 3, 4)`\n        - Output: :math:`(N, 3)`\n\n    Example:\n        >>> input = torch.rand(2, 3, 4)  # Nx4x4\n        >>> output = tgm.rotation_matrix_to_angle_axis(input)  # Nx3\n    """"""\n    if rotation_matrix.shape[1:] == (3,3):\n        rot_mat = rotation_matrix.reshape(-1, 3, 3)\n        hom = torch.tensor([0, 0, 1], dtype=torch.float32,\n                           device=rotation_matrix.device).reshape(1, 3, 1).expand(rot_mat.shape[0], -1, -1)\n        rotation_matrix = torch.cat([rot_mat, hom], dim=-1)\n\n    quaternion = rotation_matrix_to_quaternion(rotation_matrix)\n    aa = quaternion_to_angle_axis(quaternion)\n    aa[torch.isnan(aa)] = 0.0\n    return aa\n\n\ndef quaternion_to_angle_axis(quaternion: torch.Tensor) -> torch.Tensor:\n    """"""\n    This function is borrowed from https://github.com/kornia/kornia\n\n    Convert quaternion vector to angle axis of rotation.\n\n    Adapted from ceres C++ library: ceres-solver/include/ceres/rotation.h\n\n    Args:\n        quaternion (torch.Tensor): tensor with quaternions.\n\n    Return:\n        torch.Tensor: tensor with angle axis of rotation.\n\n    Shape:\n        - Input: :math:`(*, 4)` where `*` means, any number of dimensions\n        - Output: :math:`(*, 3)`\n\n    Example:\n        >>> quaternion = torch.rand(2, 4)  # Nx4\n        >>> angle_axis = tgm.quaternion_to_angle_axis(quaternion)  # Nx3\n    """"""\n    if not torch.is_tensor(quaternion):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(quaternion)))\n\n    if not quaternion.shape[-1] == 4:\n        raise ValueError(""Input must be a tensor of shape Nx4 or 4. Got {}""\n                         .format(quaternion.shape))\n    # unpack input and compute conversion\n    q1: torch.Tensor = quaternion[..., 1]\n    q2: torch.Tensor = quaternion[..., 2]\n    q3: torch.Tensor = quaternion[..., 3]\n    sin_squared_theta: torch.Tensor = q1 * q1 + q2 * q2 + q3 * q3\n\n    sin_theta: torch.Tensor = torch.sqrt(sin_squared_theta)\n    cos_theta: torch.Tensor = quaternion[..., 0]\n    two_theta: torch.Tensor = 2.0 * torch.where(\n        cos_theta < 0.0,\n        torch.atan2(-sin_theta, -cos_theta),\n        torch.atan2(sin_theta, cos_theta))\n\n    k_pos: torch.Tensor = two_theta / sin_theta\n    k_neg: torch.Tensor = 2.0 * torch.ones_like(sin_theta)\n    k: torch.Tensor = torch.where(sin_squared_theta > 0.0, k_pos, k_neg)\n\n    angle_axis: torch.Tensor = torch.zeros_like(quaternion)[..., :3]\n    angle_axis[..., 0] += q1 * k\n    angle_axis[..., 1] += q2 * k\n    angle_axis[..., 2] += q3 * k\n    return angle_axis\n\n\ndef rotation_matrix_to_quaternion(rotation_matrix, eps=1e-6):\n    """"""\n    This function is borrowed from https://github.com/kornia/kornia\n\n    Convert 3x4 rotation matrix to 4d quaternion vector\n\n    This algorithm is based on algorithm described in\n    https://github.com/KieranWynn/pyquaternion/blob/master/pyquaternion/quaternion.py#L201\n\n    Args:\n        rotation_matrix (Tensor): the rotation matrix to convert.\n\n    Return:\n        Tensor: the rotation in quaternion\n\n    Shape:\n        - Input: :math:`(N, 3, 4)`\n        - Output: :math:`(N, 4)`\n\n    Example:\n        >>> input = torch.rand(4, 3, 4)  # Nx3x4\n        >>> output = tgm.rotation_matrix_to_quaternion(input)  # Nx4\n    """"""\n    if not torch.is_tensor(rotation_matrix):\n        raise TypeError(""Input type is not a torch.Tensor. Got {}"".format(\n            type(rotation_matrix)))\n\n    if len(rotation_matrix.shape) > 3:\n        raise ValueError(\n            ""Input size must be a three dimensional tensor. Got {}"".format(\n                rotation_matrix.shape))\n    if not rotation_matrix.shape[-2:] == (3, 4):\n        raise ValueError(\n            ""Input size must be a N x 3 x 4  tensor. Got {}"".format(\n                rotation_matrix.shape))\n\n    rmat_t = torch.transpose(rotation_matrix, 1, 2)\n\n    mask_d2 = rmat_t[:, 2, 2] < eps\n\n    mask_d0_d1 = rmat_t[:, 0, 0] > rmat_t[:, 1, 1]\n    mask_d0_nd1 = rmat_t[:, 0, 0] < -rmat_t[:, 1, 1]\n\n    t0 = 1 + rmat_t[:, 0, 0] - rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\n    q0 = torch.stack([rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\n                      t0, rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2]], -1)\n    t0_rep = t0.repeat(4, 1).t()\n\n    t1 = 1 - rmat_t[:, 0, 0] + rmat_t[:, 1, 1] - rmat_t[:, 2, 2]\n    q1 = torch.stack([rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\n                      rmat_t[:, 0, 1] + rmat_t[:, 1, 0],\n                      t1, rmat_t[:, 1, 2] + rmat_t[:, 2, 1]], -1)\n    t1_rep = t1.repeat(4, 1).t()\n\n    t2 = 1 - rmat_t[:, 0, 0] - rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\n    q2 = torch.stack([rmat_t[:, 0, 1] - rmat_t[:, 1, 0],\n                      rmat_t[:, 2, 0] + rmat_t[:, 0, 2],\n                      rmat_t[:, 1, 2] + rmat_t[:, 2, 1], t2], -1)\n    t2_rep = t2.repeat(4, 1).t()\n\n    t3 = 1 + rmat_t[:, 0, 0] + rmat_t[:, 1, 1] + rmat_t[:, 2, 2]\n    q3 = torch.stack([t3, rmat_t[:, 1, 2] - rmat_t[:, 2, 1],\n                      rmat_t[:, 2, 0] - rmat_t[:, 0, 2],\n                      rmat_t[:, 0, 1] - rmat_t[:, 1, 0]], -1)\n    t3_rep = t3.repeat(4, 1).t()\n\n    mask_c0 = mask_d2 * mask_d0_d1\n    mask_c1 = mask_d2 * ~mask_d0_d1\n    mask_c2 = ~mask_d2 * mask_d0_nd1\n    mask_c3 = ~mask_d2 * ~mask_d0_nd1\n    mask_c0 = mask_c0.view(-1, 1).type_as(q0)\n    mask_c1 = mask_c1.view(-1, 1).type_as(q1)\n    mask_c2 = mask_c2.view(-1, 1).type_as(q2)\n    mask_c3 = mask_c3.view(-1, 1).type_as(q3)\n\n    q = q0 * mask_c0 + q1 * mask_c1 + q2 * mask_c2 + q3 * mask_c3\n    q /= torch.sqrt(t0_rep * mask_c0 + t1_rep * mask_c1 +  # noqa\n                    t2_rep * mask_c2 + t3_rep * mask_c3)  # noqa\n    q *= 0.5\n    return q\n\n\ndef estimate_translation_np(S, joints_2d, joints_conf, focal_length=5000., img_size=224.):\n    """"""\n    This function is borrowed from https://github.com/nkolot/SPIN/utils/geometry.py\n\n    Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.\n    Input:\n        S: (25, 3) 3D joint locations\n        joints: (25, 3) 2D joint locations and confidence\n    Returns:\n        (3,) camera translation vector\n    """"""\n\n    num_joints = S.shape[0]\n    # focal length\n    f = np.array([focal_length,focal_length])\n    # optical center\n    center = np.array([img_size/2., img_size/2.])\n\n    # transformations\n    Z = np.reshape(np.tile(S[:,2],(2,1)).T,-1)\n    XY = np.reshape(S[:,0:2],-1)\n    O = np.tile(center,num_joints)\n    F = np.tile(f,num_joints)\n    weight2 = np.reshape(np.tile(np.sqrt(joints_conf),(2,1)).T,-1)\n\n    # least squares\n    Q = np.array([F*np.tile(np.array([1,0]),num_joints), F*np.tile(np.array([0,1]),num_joints), O-np.reshape(joints_2d,-1)]).T\n    c = (np.reshape(joints_2d,-1)-O)*Z - F*XY\n\n    # weighted least squares\n    W = np.diagflat(weight2)\n    Q = np.dot(W,Q)\n    c = np.dot(W,c)\n\n    # square matrix\n    A = np.dot(Q.T,Q)\n    b = np.dot(Q.T,c)\n\n    # solution\n    trans = np.linalg.solve(A, b)\n\n    return trans\n\n\ndef estimate_translation(S, joints_2d, focal_length=5000., img_size=224.):\n    """"""\n    This function is borrowed from https://github.com/nkolot/SPIN/utils/geometry.py\n\n    Find camera translation that brings 3D joints S closest to 2D the corresponding joints_2d.\n    Input:\n        S: (B, 49, 3) 3D joint locations\n        joints: (B, 49, 3) 2D joint locations and confidence\n    Returns:\n        (B, 3) camera translation vectors\n    """"""\n\n    device = S.device\n    # Use only joints 25:49 (GT joints)\n    S = S[:, 25:, :].cpu().numpy()\n    joints_2d = joints_2d[:, 25:, :].cpu().numpy()\n    joints_conf = joints_2d[:, :, -1]\n    joints_2d = joints_2d[:, :, :-1]\n    trans = np.zeros((S.shape[0], 3), dtype=np.float32)\n    # Find the translation for each example in the batch\n    for i in range(S.shape[0]):\n        S_i = S[i]\n        joints_i = joints_2d[i]\n        conf_i = joints_conf[i]\n        trans[i] = estimate_translation_np(S_i, joints_i, conf_i, focal_length=focal_length, img_size=img_size)\n    return torch.from_numpy(trans).to(device)\n\n\ndef rot6d_to_rotmat_spin(x):\n    """"""Convert 6D rotation representation to 3x3 rotation matrix.\n    Based on Zhou et al., ""On the Continuity of Rotation Representations in Neural Networks"", CVPR 2019\n    Input:\n        (B,6) Batch of 6-D rotation representations\n    Output:\n        (B,3,3) Batch of corresponding rotation matrices\n    """"""\n    x = x.view(-1,3,2)\n    a1 = x[:, :, 0]\n    a2 = x[:, :, 1]\n    b1 = F.normalize(a1)\n    b2 = F.normalize(a2 - torch.einsum(\'bi,bi->b\', b1, a2).unsqueeze(-1) * b1)\n\n    # inp = a2 - torch.einsum(\'bi,bi->b\', b1, a2).unsqueeze(-1) * b1\n    # denom = inp.pow(2).sum(dim=1).sqrt().unsqueeze(-1) + 1e-8\n    # b2 = inp / denom\n\n    b3 = torch.cross(b1, b2)\n    return torch.stack((b1, b2, b3), dim=-1)\n\n\ndef rot6d_to_rotmat(x):\n    x = x.view(-1,3,2)\n\n    # Normalize the first vector\n    b1 = F.normalize(x[:, :, 0], dim=1, eps=1e-6)\n\n    dot_prod = torch.sum(b1 * x[:, :, 1], dim=1, keepdim=True)\n    # Compute the second vector by finding the orthogonal complement to it\n    b2 = F.normalize(x[:, :, 1] - dot_prod * b1, dim=-1, eps=1e-6)\n\n    # Finish building the basis by taking the cross product\n    b3 = torch.cross(b1, b2, dim=1)\n    rot_mats = torch.stack([b1, b2, b3], dim=-1)\n\n    return rot_mats'"
lib/utils/pose_tracker.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport json\nimport shutil\nimport subprocess\nimport numpy as np\nimport os.path as osp\n\n\ndef run_openpose(\n        video_file,\n        output_folder,\n        staf_folder,\n        vis=False,\n):\n    pwd = os.getcwd()\n\n    os.chdir(staf_folder)\n\n    render = 1 if vis else 0\n    display = 2 if vis else 0\n    cmd = [\n        'build/examples/openpose/openpose.bin',\n        '--model_pose', 'BODY_21A',\n        '--tracking', '1',\n        '--render_pose', str(render),\n        '--video', video_file,\n        '--write_json', output_folder,\n        '--display', str(display)\n    ]\n\n    print('Executing', ' '.join(cmd))\n    subprocess.call(cmd)\n    os.chdir(pwd)\n\n\ndef read_posetrack_keypoints(output_folder):\n\n    people = dict()\n\n    for idx, result_file in enumerate(sorted(os.listdir(output_folder))):\n        json_file = osp.join(output_folder, result_file)\n        data = json.load(open(json_file))\n        # print(idx, data)\n        for person in data['people']:\n            person_id = person['person_id'][0]\n            joints2d  = person['pose_keypoints_2d']\n            if person_id in people.keys():\n                people[person_id]['joints2d'].append(joints2d)\n                people[person_id]['frames'].append(idx)\n            else:\n                people[person_id] = {\n                    'joints2d': [],\n                    'frames': [],\n                }\n                people[person_id]['joints2d'].append(joints2d)\n                people[person_id]['frames'].append(idx)\n\n    for k in people.keys():\n        people[k]['joints2d'] = np.array(people[k]['joints2d']).reshape((len(people[k]['joints2d']), -1, 3))\n        people[k]['frames'] = np.array(people[k]['frames'])\n\n    return people\n\n\ndef run_posetracker(video_file, staf_folder, posetrack_output_folder='/tmp', display=False):\n    posetrack_output_folder = os.path.join(\n        posetrack_output_folder,\n        f'{os.path.basename(video_file)}_posetrack'\n    )\n\n    # run posetrack on video\n    run_openpose(\n        video_file,\n        posetrack_output_folder,\n        vis=display,\n        staf_folder=staf_folder\n    )\n\n    people_dict = read_posetrack_keypoints(posetrack_output_folder)\n\n    shutil.rmtree(posetrack_output_folder)\n\n    return people_dict"""
lib/utils/renderer.py,0,"b""# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport math\nimport trimesh\nimport pyrender\nimport numpy as np\nfrom pyrender.constants import RenderFlags\nfrom lib.models.smpl import get_smpl_faces\n\n\nclass WeakPerspectiveCamera(pyrender.Camera):\n    def __init__(self,\n                 scale,\n                 translation,\n                 znear=pyrender.camera.DEFAULT_Z_NEAR,\n                 zfar=None,\n                 name=None):\n        super(WeakPerspectiveCamera, self).__init__(\n            znear=znear,\n            zfar=zfar,\n            name=name,\n        )\n        self.scale = scale\n        self.translation = translation\n\n    def get_projection_matrix(self, width=None, height=None):\n        P = np.eye(4)\n        P[0, 0] = self.scale[0]\n        P[1, 1] = self.scale[1]\n        P[0, 3] = self.translation[0] * self.scale[0]\n        P[1, 3] = -self.translation[1] * self.scale[1]\n        P[2, 2] = -1\n        return P\n\n\nclass Renderer:\n    def __init__(self, resolution=(224,224), orig_img=False, wireframe=False):\n        self.resolution = resolution\n\n        self.faces = get_smpl_faces()\n        self.orig_img = orig_img\n        self.wireframe = wireframe\n        self.renderer = pyrender.OffscreenRenderer(\n            viewport_width=self.resolution[0],\n            viewport_height=self.resolution[1],\n            point_size=1.0\n        )\n\n        # set the scene\n        self.scene = pyrender.Scene(bg_color=[0.0, 0.0, 0.0, 0.0], ambient_light=(0.3, 0.3, 0.3))\n\n        light = pyrender.PointLight(color=[1.0, 1.0, 1.0], intensity=1)\n\n        light_pose = np.eye(4)\n        light_pose[:3, 3] = [0, -1, 1]\n        self.scene.add(light, pose=light_pose)\n\n        light_pose[:3, 3] = [0, 1, 1]\n        self.scene.add(light, pose=light_pose)\n\n        light_pose[:3, 3] = [1, 1, 2]\n        self.scene.add(light, pose=light_pose)\n\n    def render(self, img, verts, cam, angle=None, axis=None, mesh_filename=None, color=[1.0, 1.0, 0.9]):\n\n        mesh = trimesh.Trimesh(vertices=verts, faces=self.faces, process=False)\n\n        Rx = trimesh.transformations.rotation_matrix(math.radians(180), [1, 0, 0])\n        mesh.apply_transform(Rx)\n\n        if mesh_filename is not None:\n            mesh.export(mesh_filename)\n\n        if angle and axis:\n            R = trimesh.transformations.rotation_matrix(math.radians(angle), axis)\n            mesh.apply_transform(R)\n\n        sx, sy, tx, ty = cam\n\n        camera = WeakPerspectiveCamera(\n            scale=[sx, sy],\n            translation=[tx, ty],\n            zfar=1000.\n        )\n\n        material = pyrender.MetallicRoughnessMaterial(\n            metallicFactor=0.0,\n            alphaMode='OPAQUE',\n            baseColorFactor=(color[0], color[1], color[2], 1.0)\n        )\n\n        mesh = pyrender.Mesh.from_trimesh(mesh, material=material)\n\n        mesh_node = self.scene.add(mesh, 'mesh')\n\n        camera_pose = np.eye(4)\n        cam_node = self.scene.add(camera, pose=camera_pose)\n\n        if self.wireframe:\n            render_flags = RenderFlags.RGBA | RenderFlags.ALL_WIREFRAME\n        else:\n            render_flags = RenderFlags.RGBA\n\n        rgb, _ = self.renderer.render(self.scene, flags=render_flags)\n        valid_mask = (rgb[:, :, -1] > 0)[:, :, np.newaxis]\n        output_img = rgb[:, :, :-1] * valid_mask + (1 - valid_mask) * img\n        image = output_img.astype(np.uint8)\n\n        self.scene.remove_node(mesh_node)\n        self.scene.remove_node(cam_node)\n\n        return image\n"""
lib/utils/smooth_bbox.py,0,"b'# This script is borrowed from https://github.com/akanazawa/human_dynamics/blob/master/src/util/smooth_bbox.py\n# Adhere to their licence to use this script\n\nimport numpy as np\nimport scipy.signal as signal\nfrom scipy.ndimage.filters import gaussian_filter1d\n\n\ndef get_smooth_bbox_params(kps, vis_thresh=2, kernel_size=11, sigma=3):\n    """"""\n    Computes smooth bounding box parameters from keypoints:\n      1. Computes bbox by rescaling the person to be around 150 px.\n      2. Linearly interpolates bbox params for missing annotations.\n      3. Median filtering\n      4. Gaussian filtering.\n\n    Recommended thresholds:\n      * detect-and-track: 0\n      * 3DPW: 0.1\n\n    Args:\n        kps (list): List of kps (Nx3) or None.\n        vis_thresh (float): Threshold for visibility.\n        kernel_size (int): Kernel size for median filtering (must be odd).\n        sigma (float): Sigma for gaussian smoothing.\n\n    Returns:\n        Smooth bbox params [cx, cy, scale], start index, end index\n    """"""\n    bbox_params, start, end = get_all_bbox_params(kps, vis_thresh)\n    smoothed = smooth_bbox_params(bbox_params, kernel_size, sigma)\n    smoothed = np.vstack((np.zeros((start, 3)), smoothed))\n    return smoothed, start, end\n\n\ndef kp_to_bbox_param(kp, vis_thresh):\n    """"""\n    Finds the bounding box parameters from the 2D keypoints.\n\n    Args:\n        kp (Kx3): 2D Keypoints.\n        vis_thresh (float): Threshold for visibility.\n\n    Returns:\n        [center_x, center_y, scale]\n    """"""\n    if kp is None:\n        return\n    vis = kp[:, 2] > vis_thresh\n    if not np.any(vis):\n        return\n    min_pt = np.min(kp[vis, :2], axis=0)\n    max_pt = np.max(kp[vis, :2], axis=0)\n    person_height = np.linalg.norm(max_pt - min_pt)\n    if person_height < 0.5:\n        return\n    center = (min_pt + max_pt) / 2.\n    scale = 150. / person_height\n    return np.append(center, scale)\n\n\ndef get_all_bbox_params(kps, vis_thresh=2):\n    """"""\n    Finds bounding box parameters for all keypoints.\n\n    Look for sequences in the middle with no predictions and linearly\n    interpolate the bbox params for those\n\n    Args:\n        kps (list): List of kps (Kx3) or None.\n        vis_thresh (float): Threshold for visibility.\n\n    Returns:\n        bbox_params, start_index (incl), end_index (excl)\n    """"""\n    # keeps track of how many indices in a row with no prediction\n    num_to_interpolate = 0\n    start_index = -1\n    bbox_params = np.empty(shape=(0, 3), dtype=np.float32)\n\n    for i, kp in enumerate(kps):\n        bbox_param = kp_to_bbox_param(kp, vis_thresh=vis_thresh)\n        if bbox_param is None:\n            num_to_interpolate += 1\n            continue\n\n        if start_index == -1:\n            # Found the first index with a prediction!\n            start_index = i\n            num_to_interpolate = 0\n\n        if num_to_interpolate > 0:\n            # Linearly interpolate each param.\n            previous = bbox_params[-1]\n            # This will be 3x(n+2)\n            interpolated = np.array(\n                [np.linspace(prev, curr, num_to_interpolate + 2)\n                 for prev, curr in zip(previous, bbox_param)])\n            bbox_params = np.vstack((bbox_params, interpolated.T[1:-1]))\n            num_to_interpolate = 0\n        bbox_params = np.vstack((bbox_params, bbox_param))\n\n    return bbox_params, start_index, i - num_to_interpolate + 1\n\n\ndef smooth_bbox_params(bbox_params, kernel_size=11, sigma=8):\n    """"""\n    Applies median filtering and then gaussian filtering to bounding box\n    parameters.\n\n    Args:\n        bbox_params (Nx3): [cx, cy, scale].\n        kernel_size (int): Kernel size for median filtering (must be odd).\n        sigma (float): Sigma for gaussian smoothing.\n\n    Returns:\n        Smoothed bounding box parameters (Nx3).\n    """"""\n    smoothed = np.array([signal.medfilt(param, kernel_size)\n                         for param in bbox_params.T]).T\n    return np.array([gaussian_filter1d(traj, sigma) for traj in smoothed.T]).T\n'"
lib/utils/utils.py,4,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport os\nimport yaml\nimport time\nimport torch\nimport shutil\nimport logging\nimport operator\nfrom tqdm import tqdm\nfrom os import path as osp\nfrom functools import reduce\nfrom typing import List, Union\n\n\ndef move_dict_to_device(dict, device, tensor2float=False):\n    for k,v in dict.items():\n        if isinstance(v, torch.Tensor):\n            if tensor2float:\n                dict[k] = v.float().to(device)\n            else:\n                dict[k] = v.to(device)\n\n\ndef get_from_dict(dict, keys):\n    return reduce(operator.getitem, keys, dict)\n\n\ndef tqdm_enumerate(iter):\n    i = 0\n    for y in tqdm(iter):\n        yield i, y\n        i += 1\n\n\ndef iterdict(d):\n    for k,v in d.items():\n        if isinstance(v, dict):\n            d[k] = dict(v)\n            iterdict(v)\n    return d\n\n\ndef accuracy(output, target):\n    _, pred = output.topk(1)\n    pred = pred.view(-1)\n\n    correct = pred.eq(target).sum()\n\n    return correct.item(), target.size(0) - correct.item()\n\n\ndef lr_decay(optimizer, step, lr, decay_step, gamma):\n    lr = lr * gamma ** (step/decay_step)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\n\ndef step_decay(optimizer, step, lr, decay_step, gamma):\n    lr = lr * gamma ** (step / decay_step)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n    return lr\n\n\ndef read_yaml(filename):\n    return yaml.load(open(filename, \'r\'))\n\n\ndef write_yaml(filename, object):\n    with open(filename, \'w\') as f:\n        yaml.dump(object, f)\n\n\ndef save_dict_to_yaml(obj, filename, mode=\'w\'):\n    with open(filename, mode) as f:\n        yaml.dump(obj, f, default_flow_style=False)\n\n\ndef save_to_file(obj, filename, mode=\'w\'):\n    with open(filename, mode) as f:\n        f.write(obj)\n\n\ndef concatenate_dicts(dict_list, dim=0):\n    rdict = dict.fromkeys(dict_list[0].keys())\n    for k in rdict.keys():\n        rdict[k] = torch.cat([d[k] for d in dict_list], dim=dim)\n    return rdict\n\n\ndef bool_to_string(x: Union[List[bool],bool]) ->  Union[List[str],str]:\n    """"""\n    boolean to string conversion\n    :param x: list or bool to be converted\n    :return: string converted thing\n    """"""\n    if isinstance(x, bool):\n        return [str(x)]\n    for i, j in enumerate(x):\n        x[i]=str(j)\n    return x\n\n\ndef checkpoint2model(checkpoint, key=\'gen_state_dict\'):\n    state_dict = checkpoint[key]\n    print(f\'Performance of loaded model on 3DPW is {checkpoint[""performance""]:.2f}mm\')\n    # del state_dict[\'regressor.mean_theta\']\n    return state_dict\n\n\ndef get_optimizer(model, optim_type, lr, weight_decay, momentum):\n    if optim_type in [\'sgd\', \'SGD\']:\n        opt = torch.optim.SGD(lr=lr, params=model.parameters(), momentum=momentum)\n    elif optim_type in [\'Adam\', \'adam\', \'ADAM\']:\n        opt = torch.optim.Adam(lr=lr, params=model.parameters(), weight_decay=weight_decay)\n    else:\n        raise ModuleNotFoundError\n    return opt\n\n\ndef create_logger(logdir, phase=\'train\'):\n    os.makedirs(logdir, exist_ok=True)\n\n    log_file = osp.join(logdir, f\'{phase}_log.txt\')\n\n    head = \'%(asctime)-15s %(message)s\'\n    logging.basicConfig(filename=log_file,\n                        format=head)\n    logger = logging.getLogger()\n    logger.setLevel(logging.INFO)\n    console = logging.StreamHandler()\n    logging.getLogger(\'\').addHandler(console)\n\n    return logger\n\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef prepare_output_dir(cfg, cfg_file):\n\n    # ==== create logdir\n    logtime = time.strftime(\'%d-%m-%Y_%H-%M-%S\')\n    logdir = f\'{logtime}_{cfg.EXP_NAME}\'\n\n    logdir = osp.join(cfg.OUTPUT_DIR, logdir)\n    os.makedirs(logdir, exist_ok=True)\n    shutil.copy(src=cfg_file, dst=osp.join(cfg.OUTPUT_DIR, \'config.yaml\'))\n\n    cfg.LOGDIR = logdir\n\n    # save config\n    save_dict_to_yaml(cfg, osp.join(cfg.LOGDIR, \'config.yaml\'))\n\n    return cfg\n'"
lib/utils/vis.py,14,"b'# -*- coding: utf-8 -*-\n\n# Max-Planck-Gesellschaft zur F\xc3\xb6rderung der Wissenschaften e.V. (MPG) is\n# holder of all proprietary rights on this computer program.\n# You can only use this computer program if you have closed\n# a license agreement with MPG or you get the right to use the computer\n# program from someone who is authorized to grant you that right.\n# Any use of the computer program without a valid license is prohibited and\n# liable to prosecution.\n#\n# Copyright\xc2\xa92019 Max-Planck-Gesellschaft zur F\xc3\xb6rderung\n# der Wissenschaften e.V. (MPG). acting on behalf of its Max Planck Institute\n# for Intelligent Systems. All rights reserved.\n#\n# Contact: ps-license@tuebingen.mpg.de\n\nimport cv2\nimport math\nimport time\nimport torch\nimport trimesh\nimport pyrender\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom lib.data_utils import kp_utils\nfrom lib.models.smpl import SMPL, SMPL_MODEL_DIR, get_smpl_faces\nfrom lib.data_utils.img_utils import torch2numpy, torch_vid2numpy, normalize_2d_kp\n\n\nclass WeakPerspectiveCamera(pyrender.Camera):\n    def __init__(self,\n                 scale,\n                 translation,\n                 znear=pyrender.camera.DEFAULT_Z_NEAR,\n                 zfar=None,\n                 name=None):\n        super(WeakPerspectiveCamera, self).__init__(\n            znear=znear,\n            zfar=zfar,\n            name=name,\n        )\n        self.scale = scale\n        self.translation = translation\n\n    def get_projection_matrix(self, width=None, height=None):\n        P = np.eye(4)\n        P[0, 0] = self.scale\n        P[1, 1] = self.scale\n        P[0, 3] = self.translation[0] * self.scale\n        P[1, 3] = -self.translation[1] * self.scale\n        P[2, 2] = -1\n        return P\n\n\ndef get_colors():\n    colors = {\n        \'pink\': np.array([197, 27, 125]),  # L lower leg\n        \'light_pink\': np.array([233, 163, 201]),  # L upper leg\n        \'light_green\': np.array([161, 215, 106]),  # L lower arm\n        \'green\': np.array([77, 146, 33]),  # L upper arm\n        \'red\': np.array([215, 48, 39]),  # head\n        \'light_red\': np.array([252, 146, 114]),  # head\n        \'light_orange\': np.array([252, 141, 89]),  # chest\n        \'purple\': np.array([118, 42, 131]),  # R lower leg\n        \'light_purple\': np.array([175, 141, 195]),  # R upper\n        \'light_blue\': np.array([145, 191, 219]),  # R lower arm\n        \'blue\': np.array([69, 117, 180]),  # R upper arm\n        \'gray\': np.array([130, 130, 130]),  #\n        \'white\': np.array([255, 255, 255]),  #\n    }\n    return colors\n\n\ndef render_image(img, verts, cam, faces=None, angle=None, axis=None, resolution=224, output_fn=None):\n    if faces is None:\n        faces = get_smpl_faces()\n\n    mesh = trimesh.Trimesh(vertices=verts, faces=faces)\n\n    Rx = trimesh.transformations.rotation_matrix(math.radians(180), [1, 0, 0])\n    mesh.apply_transform(Rx)\n\n    if angle and axis:\n        R = trimesh.transformations.rotation_matrix(math.radians(angle), axis)\n        mesh.apply_transform(R)\n\n    if output_fn:\n        mesh.export(output_fn)\n        camera_translation = np.array([-cam[1], cam[2], 2 * 5000. / (img.shape[0] * cam[0] + 1e-9)])\n        np.save(output_fn.replace(\'.obj\', \'.npy\'), camera_translation)\n\n        # Save the rotated mesh\n        # R = trimesh.transformations.rotation_matrix(math.radians(270), [0,1,0])\n        # rotated_mesh = mesh.copy()\n        # rotated_mesh.apply_transform(R)\n        # rotated_mesh.export(output_fn.replace(\'.obj\', \'_rot.obj\'))\n\n\n\n    scene = pyrender.Scene(bg_color=[0.0, 0.0, 0.0, 0.0],\n                           ambient_light=(0.3, 0.3, 0.3)\n                           )\n\n    material = pyrender.MetallicRoughnessMaterial(\n        metallicFactor=0.0,\n        alphaMode=\'OPAQUE\',\n        baseColorFactor=(1.0, 1.0, 0.9, 1.0)\n    )\n    mesh = pyrender.Mesh.from_trimesh(mesh, material=material)\n    scene.add(mesh, \'mesh\')\n\n    camera_pose = np.eye(4)\n\n    camera = WeakPerspectiveCamera(\n        scale=cam[0],\n        translation=cam[1:],\n        zfar=1000.\n    )\n    scene.add(camera, pose=camera_pose)\n\n    light = pyrender.PointLight(color=[1.0, 1.0, 1.0], intensity=1)\n\n    light_pose = np.eye(4)\n    light_pose[:3, 3] = [0, -1, 1]\n    scene.add(light, pose=light_pose)\n\n    light_pose[:3, 3] = [0, 1, 1]\n    scene.add(light, pose=light_pose)\n\n    light_pose[:3, 3] = [1, 1, 2]\n    scene.add(light, pose=light_pose)\n\n\n    r = pyrender.OffscreenRenderer(viewport_width=resolution,\n                                   viewport_height=resolution,\n                                   point_size=1.0)\n\n    color, _ = r.render(scene, flags=pyrender.RenderFlags.RGBA)\n    # color = color[:, ::-1, :]\n    valid_mask = (color[:, :, -1] > 0)[:, :, np.newaxis]\n\n    output_img = color[:, :, :-1] * valid_mask + (1 - valid_mask) * img\n\n    image = output_img.astype(np.uint8)\n    text = f\'s: {cam[0]:.2f}, tx: {cam[1]:.2f}, ty: {cam[2]:.2f}\'\n    cv2.putText(image, text, (5, 10), 0, 0.4, color=(0,255,0))\n\n    return image\n\n\ndef draw_SMPL_joints2D(joints2D, image, kintree_table=None, color=\'red\'):\n    rcolor = get_colors()[\'red\'].tolist()\n    lcolor = get_colors()[\'blue\'].tolist()\n    # color = get_colors()[color].tolist()\n    for i in range(1, kintree_table.shape[1]):\n        j1 = kintree_table[0][i]\n        j2 = kintree_table[1][i]\n\n        color = lcolor if i % 2 == 0 else rcolor\n\n        pt1, pt2 = (joints2D[j1, 0], joints2D[j1, 1]), (joints2D[j2, 0], joints2D[j2, 1])\n        cv2.line(image, pt1=pt1, pt2=pt2, color=color, thickness=2)\n\n        cv2.circle(image, pt1, 4, color, -1)\n        cv2.circle(image, pt2, 4, color, -1)\n\n    # for i in range(joints2D.shape[0]):\n    #     color = lcolor if i % 2 == 0 else rcolor\n    #     pt1 = (joints2D[i, 0], joints2D[i, 1])\n    #     cv2.circle(image, pt1, 4, color, -1)\n\n    return image\n\n\ndef show3Dpose(channels, ax, radius=40, lcolor=\'#ff0000\', rcolor=\'#0000ff\'):\n    vals = channels\n\n    connections = [[0, 1], [1, 2], [2, 3], [0, 4], [4, 5],\n                   [5, 6], [0, 7], [7, 8], [8, 9], [9, 10],\n                   [8, 11], [11, 12], [12, 13], [8, 14], [14, 15], [15, 16]]\n\n    LR = np.array([0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0], dtype=bool)\n\n    for ind, (i,j) in enumerate(connections):\n        x, y, z = [np.array([vals[i, c], vals[j, c]]) for c in range(3)]\n        ax.plot(x, y, z, lw=2, c=lcolor if LR[ind] else rcolor)\n\n    RADIUS = radius  # space around the subject\n    xroot, yroot, zroot = vals[0, 0], vals[0, 1], vals[0, 2]\n    ax.set_xlim3d([-RADIUS + xroot, RADIUS + xroot])\n    ax.set_zlim3d([-RADIUS + zroot, RADIUS + zroot])\n    ax.set_ylim3d([-RADIUS + yroot, RADIUS + yroot])\n\n    ax.set_xlabel(""x"")\n    ax.set_ylabel(""y"")\n    ax.set_zlabel(""z"")\n\n\ndef visualize_sequence(sequence):\n\n    seqlen, size = sequence.shape\n    sequence = sequence.reshape((seqlen, -1, 3))\n\n\n    fig = plt.figure(figsize=(12, 7))\n\n    for i in range(seqlen):\n        ax = fig.add_subplot(\'111\', projection=\'3d\', aspect=1)\n        show3Dpose(sequence[i], ax, radius=0.6)\n        ax.view_init(-75, -90)\n\n        plt.draw()\n        plt.pause(0.01)\n        plt.cla()\n\n    plt.close()\n\ndef visualize_preds(image, preds, target=None, target_exists=True, dataset=\'common\', vis_hmr=False):\n    with torch.no_grad():\n        if isinstance(image, torch.Tensor):\n            image = torch2numpy(image)\n            # import random\n            # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            # cv2.imwrite(f\'sample_images/{random.randint(0,100)}.jpg\', image)\n\n    pred_theta  = preds[\'theta\']\n    pred_camera = pred_theta[:3]\n    # pred_pose   = pred_theta[3:75]\n    # pred_shape  = pred_theta[75:]\n    pred_kp_2d  = preds[\'kp_2d\']\n    pred_verts = preds[\'verts\']\n\n    if target_exists:\n        target_kp_2d  = target[\'kp_2d\']\n\n    pred_kp_2d = np.concatenate([pred_kp_2d, np.ones((pred_kp_2d.shape[0], 1))], axis=-1)\n\n    faces = get_smpl_faces()\n\n    pred_image = draw_skeleton(image.copy(), pred_kp_2d, dataset=dataset)\n    if target_exists:\n        if vis_hmr:\n            target_verts = target[\'verts\']\n            target_cam = target[\'cam\']\n\n            target_image = render_image(\n                img=image.copy(),\n                verts=target_verts,\n                faces=faces,\n                cam=target_cam\n            )\n        else:\n            target_image = draw_skeleton(image.copy(), target_kp_2d, dataset=dataset)\n\n\n    render = render_image(\n        img=image.copy(),\n        verts=pred_verts,\n        faces=faces,\n        cam=pred_camera\n    )\n\n    white_img = np.zeros_like(image)\n    render_side = render_image(\n        img=white_img.copy(),\n        verts=pred_verts,\n        faces=faces,\n        cam=pred_camera,\n        angle=90,\n        axis=[0,1,0]\n    )\n\n    if target_exists:\n        result_image = np.hstack([image, pred_image, target_image, render, render_side])\n    else:\n        result_image = np.hstack([image, pred_image, render, render_side])\n\n    return result_image\n\n\ndef batch_visualize_preds(images, preds, target=None, max_images=16, idxs=None,\n                          target_exists=True, dataset=\'common\'):\n\n    if max_images is None or images.shape[0] < max_images:\n        max_images = images.shape[0]\n\n    # preds = preds[-1] # get the final output\n\n    with torch.no_grad():\n        for k, v in preds.items():\n            if isinstance(preds[k], torch.Tensor):\n                preds[k] = v.detach().cpu().numpy()\n        if target_exists:\n            for k, v in target.items():\n                if isinstance(target[k], torch.Tensor):\n                    target[k] = v.cpu().numpy()\n\n    result_images = []\n\n    indexes = range(max_images) if idxs is None else idxs\n\n    for idx in indexes:\n        single_pred = {}\n        for k, v in preds.items():\n            single_pred[k] = v[idx]\n\n        if target_exists:\n            single_target = {}\n            for k, v in target.items():\n                single_target[k] = v[idx]\n        else:\n            single_target = None\n\n        img = visualize_preds(images[idx], single_pred, single_target, target_exists,\n                              dataset=dataset)\n        result_images.append(img)\n\n    result_image = np.vstack(result_images)\n\n    return result_image\n\n\ndef batch_visualize_vid_preds(video, preds, target, max_video=4, vis_hmr=False, dataset=\'common\'):\n    with torch.no_grad():\n        if isinstance(video, torch.Tensor):\n            video = torch_vid2numpy(video) # NTCHW\n\n    video = np.transpose(video, (0, 1, 3, 4, 2))[:max_video]  # NTCHW->NTHWC\n\n    batch_size, tsize = video.shape[:2]\n\n    if vis_hmr:\n        features = target[\'features\']\n        target_verts, target_cam = get_regressor_output(features)\n        target[\'verts\'] = target_verts\n        target[\'cam\'] = target_cam\n\n    with torch.no_grad():\n        for k, v in preds.items():\n            if isinstance(preds[k], torch.Tensor):\n                preds[k] = v.cpu().numpy()[:max_video]\n\n        for k, v in target.items():\n            if isinstance(target[k], torch.Tensor):\n                target[k] = v.cpu().numpy()[:max_video]\n\n    batch_videos = [] # NTCHW*4\n\n    for batch_id in range(batch_size):\n\n        result_video = [] #TCHW*4\n\n        for t_id in range(tsize):\n            image = video[batch_id, t_id]\n            single_pred = {}\n            single_target = {}\n            for k, v in preds.items():\n                single_pred[k] = v[batch_id, t_id]\n\n            for k, v in target.items():\n                single_target[k] = v[batch_id, t_id]\n\n            img = visualize_preds(image, single_pred, single_target,\n                                  vis_hmr=vis_hmr, dataset=dataset)\n\n            result_video.append(img[np.newaxis, ...])\n\n        result_video = np.concatenate(result_video)\n\n        batch_videos.append(result_video[np.newaxis, ...])\n\n    final_video = np.concatenate(batch_videos)\n    final_video = np.transpose(final_video, (0, 1, 4, 2, 3))  # NTHWC->NTCHW\n    return final_video\n\n\ndef draw_skeleton(image, kp_2d, dataset=\'common\', unnormalize=True, thickness=2):\n\n    if unnormalize:\n        kp_2d[:,:2] = normalize_2d_kp(kp_2d[:,:2], 224, inv=True)\n\n    kp_2d[:,2] = kp_2d[:,2] > 0.3\n    kp_2d = np.array(kp_2d, dtype=int)\n\n    rcolor = get_colors()[\'red\'].tolist()\n    pcolor = get_colors()[\'green\'].tolist()\n    lcolor = get_colors()[\'blue\'].tolist()\n\n    skeleton = eval(f\'kp_utils.get_{dataset}_skeleton\')()\n    common_lr = [0,0,1,1,0,0,0,0,1,0,0,1,1,1,0]\n    for idx,pt in enumerate(kp_2d):\n        if pt[2] > 0: # if visible\n            cv2.circle(image, (pt[0], pt[1]), 4, pcolor, -1)\n            # cv2.putText(image, f\'{idx}\', (pt[0]+1, pt[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 255, 0))\n\n    for i,(j1,j2) in enumerate(skeleton):\n        if kp_2d[j1, 2] > 0 and kp_2d[j2, 2] > 0: # if visible\n            if dataset == \'common\':\n                color = rcolor if common_lr[i] == 0 else lcolor\n            else:\n                color = lcolor if i % 2 == 0 else rcolor\n            pt1, pt2 = (kp_2d[j1, 0], kp_2d[j1, 1]), (kp_2d[j2, 0], kp_2d[j2, 1])\n            cv2.line(image, pt1=pt1, pt2=pt2, color=color, thickness=thickness)\n\n    return image\n\n\ndef batch_draw_skeleton(images, target, max_images=8, dataset=\'common\'):\n    if max_images is None or images.shape[0] < max_images:\n        max_images = images.shape[0]\n\n\n    with torch.no_grad():\n\n        for k, v in target.items():\n            if isinstance(target[k], torch.Tensor):\n                target[k] = v.cpu().numpy()\n\n    result_images = []\n\n    for idx in range(max_images):\n        single_target = {}\n\n        for k, v in target.items():\n            single_target[k] = v[idx]\n\n        img = torch2numpy(images[idx])\n\n        img = draw_skeleton(img.copy(), single_target[\'kp_2d\'], dataset=dataset)\n        result_images.append(img)\n\n    result_image = np.vstack(result_images)\n\n    return result_image\n\n\ndef get_regressor_output(features):\n    from lib.models.spin import Regressor\n\n    batch_size, seqlen = features.shape[:2]\n\n    device = \'cuda\' if torch.cuda.is_available() else \'cpu\'\n\n    model = Regressor().to(device)\n\n    smpl = SMPL(SMPL_MODEL_DIR).to(device)\n    pretrained = torch.load(\'models/model_best.pth.tar\')[\'gen_state_dict\']\n\n    new_pretrained_dict = {}\n    for k, v in pretrained.items():\n        if \'regressor\' in k:\n            new_pretrained_dict[k[10:]] = v\n            # adapt mean theta to new batch size\n            if \'mean_theta\' in k:\n                del new_pretrained_dict[k[10:]]\n\n    model.load_state_dict(new_pretrained_dict, strict=False)\n    features = features.reshape(batch_size*seqlen, -1)\n    features = features.to(device)\n    theta = model(features)[-1]\n\n    cam = theta[:, 0:3].contiguous()\n    pose = theta[:, 3:75].contiguous()\n    shape = theta[:, 75:].contiguous()\n\n    pred_output = smpl(betas=shape, body_pose=pose[:, 3:], global_orient=pose[:, :3], pose2rot=True)\n    verts = pred_output.vertices # , _, _ = smpl(pose, shape)\n\n    verts = verts.reshape(batch_size, seqlen, -1, 3)\n    cam = cam.reshape(batch_size, seqlen, -1)\n\n    return verts, cam\n\ndef show_video(video, fps=25):\n    for fid, frame in enumerate(video):\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n        cv2.imshow(f\'frame {fid}\', frame)\n\n        if cv2.waitKey(1) & 0xFF == ord(\'q\'):\n            break\n\n        time.sleep(1./fps)\n\n    cv2.destroyAllWindows()'"
