file_path,api_count,code
common/base.py,7,"b'import os\nimport os.path as osp\nimport math\nimport time\nimport glob\nimport abc\nfrom torch.utils.data import DataLoader\nimport torch.optim\nimport torchvision.transforms as transforms\n\nfrom config import cfg\nfrom dataset import DatasetLoader\nfrom timer import Timer\nfrom logger import colorlogger\nfrom torch.nn.parallel.data_parallel import DataParallel\nfrom model import get_pose_net\n\n# dynamic dataset import\nfor i in range(len(cfg.trainset)):\n    exec(\'from \' + cfg.trainset[i] + \' import \' + cfg.trainset[i])\nexec(\'from \' + cfg.testset + \' import \' + cfg.testset)\n\nclass Base(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, log_name=\'logs.txt\'):\n        \n        self.cur_epoch = 0\n\n        # timer\n        self.tot_timer = Timer()\n        self.gpu_timer = Timer()\n        self.read_timer = Timer()\n\n        # logger\n        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n\n    @abc.abstractmethod\n    def _make_batch_generator(self):\n        return\n\n    @abc.abstractmethod\n    def _make_model(self):\n        return\n\n    def save_model(self, state, epoch):\n        file_path = osp.join(cfg.model_dir,\'snapshot_{}.pth.tar\'.format(str(epoch)))\n        torch.save(state, file_path)\n        self.logger.info(""Write snapshot into {}"".format(file_path))\n\n    def load_model(self, model, optimizer):\n        model_file_list = glob.glob(osp.join(cfg.model_dir,\'*.pth.tar\'))\n        cur_epoch = max([int(file_name[file_name.find(\'snapshot_\') + 9 : file_name.find(\'.pth.tar\')]) for file_name in model_file_list])\n        ckpt = torch.load(osp.join(cfg.model_dir, \'snapshot_\' + str(cur_epoch) + \'.pth.tar\')) \n        start_epoch = ckpt[\'epoch\'] + 1\n        model.load_state_dict(ckpt[\'network\'])\n        optimizer.load_state_dict(ckpt[\'optimizer\'])\n\n        return start_epoch, model, optimizer\n\nclass Trainer(Base):\n    \n    def __init__(self):\n        super(Trainer, self).__init__(log_name = \'train_logs.txt\')\n\n    def get_optimizer(self, model):\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n        return optimizer\n\n    def set_lr(self, epoch):\n        for e in cfg.lr_dec_epoch:\n            if epoch < e:\n                break\n        if epoch < cfg.lr_dec_epoch[-1]:\n            idx = cfg.lr_dec_epoch.index(e)\n            for g in self.optimizer.param_groups:\n                g[\'lr\'] = cfg.lr / (cfg.lr_dec_factor ** idx)\n        else:\n            for g in self.optimizer.param_groups:\n                g[\'lr\'] = cfg.lr / (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n\n    def get_lr(self):\n        for g in self.optimizer.param_groups:\n            cur_lr = g[\'lr\']\n\n        return cur_lr\n    def _make_batch_generator(self):\n        # data load and construct batch generator\n        self.logger.info(""Creating dataset..."")\n        trainset_loader = []\n        batch_generator = []\n        iterator = []\n        for i in range(len(cfg.trainset)):\n            if i > 0:\n                ref_joints_name = trainset_loader[0].joints_name\n            else:\n                ref_joints_name = None\n            trainset_loader.append(DatasetLoader(eval(cfg.trainset[i])(""train""), ref_joints_name, True, transforms.Compose([\\\n                                                                                                        transforms.ToTensor(),\n                                                                                                        transforms.Normalize(mean=cfg.pixel_mean, std=cfg.pixel_std)]\\\n                                                                                                        )))\n            batch_generator.append(DataLoader(dataset=trainset_loader[-1], batch_size=cfg.num_gpus*cfg.batch_size//len(cfg.trainset), shuffle=True, num_workers=cfg.num_thread, pin_memory=True))\n            iterator.append(iter(batch_generator[-1]))\n        \n        self.joint_num = trainset_loader[0].joint_num\n        self.itr_per_epoch = math.ceil(trainset_loader[0].__len__() / cfg.num_gpus / (cfg.batch_size // len(cfg.trainset)))\n        self.batch_generator = batch_generator\n        self.iterator = iterator\n    \n    def _make_model(self):\n        # prepare network\n        self.logger.info(""Creating graph and optimizer..."")\n        model = get_pose_net(cfg, True, self.joint_num)\n        model = DataParallel(model).cuda()\n        optimizer = self.get_optimizer(model)\n        if cfg.continue_train:\n            start_epoch, model, optimizer = self.load_model(model, optimizer)\n        else:\n            start_epoch = 0\n        model.train()\n\n        self.start_epoch = start_epoch\n        self.model = model\n        self.optimizer = optimizer\n\nclass Tester(Base):\n    \n    def __init__(self, test_epoch):\n        self.test_epoch = int(test_epoch)\n        super(Tester, self).__init__(log_name = \'test_logs.txt\')\n\n    def _make_batch_generator(self):\n        # data load and construct batch generator\n        self.logger.info(""Creating dataset..."")\n        testset = eval(cfg.testset)(""test"")\n        testset_loader = DatasetLoader(testset, None, False, transforms.Compose([\\\n                                                                                                        transforms.ToTensor(),\n                                                                                                        transforms.Normalize(mean=cfg.pixel_mean, std=cfg.pixel_std)]\\\n                                                                                                        ))\n        batch_generator = DataLoader(dataset=testset_loader, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n        \n        self.testset = testset\n        self.joint_num = testset_loader.joint_num\n        self.skeleton = testset_loader.skeleton\n        self.flip_pairs = testset.flip_pairs\n        self.batch_generator = batch_generator\n    \n    def _make_model(self):\n        \n        model_path = os.path.join(cfg.model_dir, \'snapshot_%d.pth.tar\' % self.test_epoch)\n        assert os.path.exists(model_path), \'Cannot find model at \' + model_path\n        self.logger.info(\'Load checkpoint from {}\'.format(model_path))\n        \n        # prepare network\n        self.logger.info(""Creating graph..."")\n        model = get_pose_net(cfg, False, self.joint_num)\n        model = DataParallel(model).cuda()\n        ckpt = torch.load(model_path)\n        model.load_state_dict(ckpt[\'network\'])\n        model.eval()\n\n        self.model = model\n\n    def _evaluate(self, preds, result_save_path):\n        self.testset.evaluate(preds, result_save_path)\n\n'"
common/logger.py,0,"b'import logging\nimport os\n\nOK = \'\\033[92m\'\nWARNING = \'\\033[93m\'\nFAIL = \'\\033[91m\'\nEND = \'\\033[0m\'\n\nPINK = \'\\033[95m\'\nBLUE = \'\\033[94m\'\nGREEN = OK\nRED = FAIL\nWHITE = END\nYELLOW = WARNING\n\nclass colorlogger():\n    def __init__(self, log_dir, log_name=\'train_logs.txt\'):\n        # set log\n        self._logger = logging.getLogger(log_name)\n        self._logger.setLevel(logging.INFO)\n        log_file = os.path.join(log_dir, log_name)\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        file_log = logging.FileHandler(log_file, mode=\'a\')\n        file_log.setLevel(logging.INFO)\n        console_log = logging.StreamHandler()\n        console_log.setLevel(logging.INFO)\n        formatter = logging.Formatter(\n            ""{}%(asctime)s{} %(message)s"".format(GREEN, END),\n            ""%m-%d %H:%M:%S"")\n        file_log.setFormatter(formatter)\n        console_log.setFormatter(formatter)\n        self._logger.addHandler(file_log)\n        self._logger.addHandler(console_log)\n\n    def debug(self, msg):\n        self._logger.debug(str(msg))\n\n    def info(self, msg):\n        self._logger.info(str(msg))\n\n    def warning(self, msg):\n        self._logger.warning(WARNING + \'WRN: \' + str(msg) + END)\n\n    def critical(self, msg):\n        self._logger.critical(RED + \'CRI: \' + str(msg) + END)\n\n    def error(self, msg):\n        self._logger.error(RED + \'ERR: \' + str(msg) + END)\n\n'"
common/timer.py,0,"b'# --------------------------------------------------------\r\n# Fast R-CNN\r\n# Copyright (c) 2015 Microsoft\r\n# Licensed under The MIT License [see LICENSE for details]\r\n# Written by Ross Girshick\r\n# --------------------------------------------------------\r\n\r\nimport time\r\n\r\nclass Timer(object):\r\n    """"""A simple timer.""""""\r\n    def __init__(self):\r\n        self.total_time = 0.\r\n        self.calls = 0\r\n        self.start_time = 0.\r\n        self.diff = 0.\r\n        self.average_time = 0.\r\n        self.warm_up = 0\r\n\r\n    def tic(self):\r\n        # using time.time instead of time.clock because time time.clock\r\n        # does not normalize for multithreading\r\n        self.start_time = time.time()\r\n\r\n    def toc(self, average=True):\r\n        self.diff = time.time() - self.start_time\r\n        if self.warm_up < 10:\r\n            self.warm_up += 1\r\n            return self.diff\r\n        else:\r\n            self.total_time += self.diff\r\n            self.calls += 1\r\n            self.average_time = self.total_time / self.calls\r\n\r\n        if average:\r\n            return self.average_time\r\n        else:\r\n            return self.diff\r\n'"
data/dataset.py,1,"b'import numpy as np\nimport cv2\nimport random\nimport time\nimport torch\nimport copy\nimport math\nfrom torch.utils.data.dataset import Dataset\nfrom utils.vis import vis_keypoints, vis_3d_skeleton\nfrom utils.pose_utils import fliplr_joints, transform_joint_to_other_db\nfrom config import cfg\n\nclass DatasetLoader(Dataset):\n    def __init__(self, db, ref_joints_name, is_train, transform):\n        \n        self.db = db.data\n        self.joint_num = db.joint_num\n        self.skeleton = db.skeleton\n        self.flip_pairs = db.flip_pairs\n        self.joints_have_depth = db.joints_have_depth\n        self.joints_name = db.joints_name\n        self.ref_joints_name = ref_joints_name\n        \n        self.transform = transform\n        self.is_train = is_train\n\n        if self.is_train:\n            self.do_augment = True\n        else:\n            self.do_augment = False\n\n    def __getitem__(self, index):\n        \n        joint_num = self.joint_num\n        skeleton = self.skeleton\n        flip_pairs = self.flip_pairs\n        joints_have_depth = self.joints_have_depth\n\n        data = copy.deepcopy(self.db[index])\n\n        bbox = data[\'bbox\']\n        joint_img = data[\'joint_img\']\n        joint_vis = data[\'joint_vis\']\n\n        # 1. load image\n        cvimg = cv2.imread(data[\'img_path\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n        if not isinstance(cvimg, np.ndarray):\n            raise IOError(""Fail to read %s"" % data[\'img_path\'])\n        img_height, img_width, img_channels = cvimg.shape\n\n        # 2. get augmentation params\n        if self.do_augment:\n            scale, rot, do_flip, color_scale, do_occlusion = get_aug_config()\n        else:\n            scale, rot, do_flip, color_scale, do_occlusion = 1.0, 0.0, False, [1.0, 1.0, 1.0], False\n\n        # 3. crop patch from img and perform data augmentation (flip, rot, color scale, synthetic occlusion)\n        img_patch, trans = generate_patch_image(cvimg, bbox, do_flip, scale, rot, do_occlusion)\n        for i in range(img_channels):\n            img_patch[:, :, i] = np.clip(img_patch[:, :, i] * color_scale[i], 0, 255)\n\n        # 4. generate patch joint ground truth\n        # flip joints and apply Affine Transform on joints\n        if do_flip:\n            joint_img[:, 0] = img_width - joint_img[:, 0] - 1\n            for pair in flip_pairs:\n                joint_img[pair[0], :], joint_img[pair[1], :] = joint_img[pair[1], :], joint_img[pair[0], :].copy()\n                joint_vis[pair[0], :], joint_vis[pair[1], :] = joint_vis[pair[1], :], joint_vis[pair[0], :].copy()\n\n        for i in range(len(joint_img)):\n            joint_img[i, 0:2] = trans_point2d(joint_img[i, 0:2], trans)\n            joint_img[i, 2] /= (cfg.bbox_3d_shape[0]/2.) # expect depth lies in -bbox_3d_shape[0]/2 ~ bbox_3d_shape[0]/2 -> -1.0 ~ 1.0\n            joint_img[i, 2] = (joint_img[i,2] + 1.0)/2. # 0~1 normalize\n            joint_vis[i] *= (\n                            (joint_img[i,0] >= 0) & \\\n                            (joint_img[i,0] < cfg.input_shape[1]) & \\\n                            (joint_img[i,1] >= 0) & \\\n                            (joint_img[i,1] < cfg.input_shape[0]) & \\\n                            (joint_img[i,2] >= 0) & \\\n                            (joint_img[i,2] < 1)\n                            )\n\n        vis = False\n        if vis:\n            filename = str(random.randrange(1,500))\n            tmpimg = img_patch.copy().astype(np.uint8)\n            tmpkps = np.zeros((3,joint_num))\n            tmpkps[:2,:] = joint_img[:,:2].transpose(1,0)\n            tmpkps[2,:] = joint_vis[:,0]\n            tmpimg = vis_keypoints(tmpimg, tmpkps, skeleton)\n            cv2.imwrite(filename + \'_gt.jpg\', tmpimg)\n        \n        vis = False\n        if vis:\n            vis_3d_skeleton(joint_img, joint_vis, skeleton, filename)\n\n        # change coordinates to output space\n        joint_img[:, 0] = joint_img[:, 0] / cfg.input_shape[1] * cfg.output_shape[1]\n        joint_img[:, 1] = joint_img[:, 1] / cfg.input_shape[0] * cfg.output_shape[0]\n        joint_img[:, 2] = joint_img[:, 2] * cfg.depth_dim\n        \n        if self.is_train:\n            img_patch = self.transform(img_patch)\n            \n            if self.ref_joints_name is not None:\n                joint_img = transform_joint_to_other_db(joint_img, self.joints_name, self.ref_joints_name) \n                joint_vis = transform_joint_to_other_db(joint_vis, self.joints_name, self.ref_joints_name)\n\n            joint_img = joint_img.astype(np.float32)\n            joint_vis = (joint_vis > 0).astype(np.float32)\n            joints_have_depth = np.array([joints_have_depth]).astype(np.float32)\n\n            return img_patch, joint_img, joint_vis, joints_have_depth\n        else:\n            img_patch = self.transform(img_patch)\n            return img_patch\n\n    def __len__(self):\n        return len(self.db)\n\n# helper functions\ndef get_aug_config():\n    \n    scale_factor = 0.25\n    rot_factor = 30\n    color_factor = 0.2\n    \n    scale = np.clip(np.random.randn(), -1.0, 1.0) * scale_factor + 1.0\n    rot = np.clip(np.random.randn(), -2.0,\n                  2.0) * rot_factor if random.random() <= 0.6 else 0\n    do_flip = random.random() <= 0.5\n    c_up = 1.0 + color_factor\n    c_low = 1.0 - color_factor\n    color_scale = [random.uniform(c_low, c_up), random.uniform(c_low, c_up), random.uniform(c_low, c_up)]\n\n    do_occlusion = random.random() <= 0.5\n\n    return scale, rot, do_flip, color_scale, do_occlusion\n\n\ndef generate_patch_image(cvimg, bbox, do_flip, scale, rot, do_occlusion):\n    img = cvimg.copy()\n    img_height, img_width, img_channels = img.shape\n\n    # synthetic occlusion\n    if do_occlusion:\n        while True:\n            area_min = 0.0\n            area_max = 0.7\n            synth_area = (random.random() * (area_max - area_min) + area_min) * bbox[2] * bbox[3]\n\n            ratio_min = 0.3\n            ratio_max = 1/0.3\n            synth_ratio = (random.random() * (ratio_max - ratio_min) + ratio_min)\n\n            synth_h = math.sqrt(synth_area * synth_ratio)\n            synth_w = math.sqrt(synth_area / synth_ratio)\n            synth_xmin = random.random() * (bbox[2] - synth_w - 1) + bbox[0]\n            synth_ymin = random.random() * (bbox[3] - synth_h - 1) + bbox[1]\n\n            if synth_xmin >= 0 and synth_ymin >= 0 and synth_xmin + synth_w < img_width and synth_ymin + synth_h < img_height:\n                xmin = int(synth_xmin)\n                ymin = int(synth_ymin)\n                w = int(synth_w)\n                h = int(synth_h)\n                img[ymin:ymin+h, xmin:xmin+w, :] = np.random.rand(h, w, 3) * 255\n                break\n\n    bb_c_x = float(bbox[0] + 0.5*bbox[2])\n    bb_c_y = float(bbox[1] + 0.5*bbox[3])\n    bb_width = float(bbox[2])\n    bb_height = float(bbox[3])\n\n    if do_flip:\n        img = img[:, ::-1, :]\n        bb_c_x = img_width - bb_c_x - 1\n    \n    trans = gen_trans_from_patch_cv(bb_c_x, bb_c_y, bb_width, bb_height, cfg.input_shape[1], cfg.input_shape[0], scale, rot, inv=False)\n    img_patch = cv2.warpAffine(img, trans, (int(cfg.input_shape[1]), int(cfg.input_shape[0])), flags=cv2.INTER_LINEAR)\n\n    img_patch = img_patch[:,:,::-1].copy()\n    img_patch = img_patch.astype(np.float32)\n\n    return img_patch, trans\n\ndef rotate_2d(pt_2d, rot_rad):\n    x = pt_2d[0]\n    y = pt_2d[1]\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    xx = x * cs - y * sn\n    yy = x * sn + y * cs\n    return np.array([xx, yy], dtype=np.float32)\n\ndef gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, scale, rot, inv=False):\n    # augment size with scale\n    src_w = src_width * scale\n    src_h = src_height * scale\n    src_center = np.array([c_x, c_y], dtype=np.float32)\n\n    # augment rotation\n    rot_rad = np.pi * rot / 180\n    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n\n    dst_w = dst_width\n    dst_h = dst_height\n    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = src_center\n    src[1, :] = src_center + src_downdir\n    src[2, :] = src_center + src_rightdir\n\n    dst = np.zeros((3, 2), dtype=np.float32)\n    dst[0, :] = dst_center\n    dst[1, :] = dst_center + dst_downdir\n    dst[2, :] = dst_center + dst_rightdir\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\ndef trans_point2d(pt_2d, trans):\n    src_pt = np.array([pt_2d[0], pt_2d[1], 1.]).T\n    dst_pt = np.dot(trans, src_pt)\n    return dst_pt[0:2]\n\n\n'"
main/config.py,0,"b'import os\nimport os.path as osp\nimport sys\nimport numpy as np\n\nclass Config:\n    \n    ## dataset\n    # training set\n    # 3D: Human36M, MuCo\n    # 2D: MSCOCO, MPII \n    # Note that list must consists of one 3D dataset (first element of the list) + several 2D datasets\n    trainset = [\'Human36M\', \'MPII\']   \n\n    # testing set\n    # Human36M, MuPoTS, MSCOCO\n    testset = \'Human36M\'\n\n    ## directory\n    cur_dir = osp.dirname(os.path.abspath(__file__))\n    root_dir = osp.join(cur_dir, \'..\')\n    data_dir = osp.join(root_dir, \'data\')\n    output_dir = osp.join(root_dir, \'output\')\n    model_dir = osp.join(output_dir, \'model_dump\')\n    vis_dir = osp.join(output_dir, \'vis\')\n    log_dir = osp.join(output_dir, \'log\')\n    result_dir = osp.join(output_dir, \'result\')\n \n    ## model setting\n    resnet_type = 50 # 50, 101, 152\n    \n    ## input, output\n    input_shape = (256, 256) \n    output_shape = (input_shape[0]//4, input_shape[1]//4)\n    depth_dim = 64\n    bbox_3d_shape = (2000, 2000, 2000) # depth, height, width\n    pixel_mean = (0.485, 0.456, 0.406)\n    pixel_std = (0.229, 0.224, 0.225)\n\n    ## training config\n    lr_dec_epoch = [17, 21]\n    end_epoch = 25\n    lr = 1e-3\n    lr_dec_factor = 10\n    batch_size = 32\n\n    ## testing config\n    test_batch_size = 32\n    flip_test = True\n    use_gt_info = True\n\n    ## others\n    num_thread = 20\n    gpu_ids = \'0\'\n    num_gpus = 1\n    continue_train = False\n\n    def set_args(self, gpu_ids, continue_train=False):\n        self.gpu_ids = gpu_ids\n        self.num_gpus = len(self.gpu_ids.split(\',\'))\n        self.continue_train = continue_train\n        os.environ[""CUDA_VISIBLE_DEVICES""] = self.gpu_ids\n        print(\'>>> Using GPU: {}\'.format(self.gpu_ids))\n\ncfg = Config()\n\nsys.path.insert(0, osp.join(cfg.root_dir, \'common\'))\nfrom utils.dir_utils import add_pypath, make_folder\nadd_pypath(osp.join(cfg.data_dir))\nfor i in range(len(cfg.trainset)):\n    add_pypath(osp.join(cfg.data_dir, cfg.trainset[i]))\nadd_pypath(osp.join(cfg.data_dir, cfg.testset))\nmake_folder(cfg.model_dir)\nmake_folder(cfg.vis_dir)\nmake_folder(cfg.log_dir)\nmake_folder(cfg.result_dir)\n\n'"
main/model.py,7,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom nets.resnet import ResNetBackbone\nfrom config import cfg\n\nclass HeadNet(nn.Module):\n\n    def __init__(self, joint_num):\n        self.inplanes = 2048\n        self.outplanes = 256\n\n        super(HeadNet, self).__init__()\n\n        self.deconv_layers = self._make_deconv_layer(3)\n        self.final_layer = nn.Conv2d(\n            in_channels=self.inplanes,\n            out_channels=joint_num * cfg.depth_dim,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n\n    def _make_deconv_layer(self, num_layers):\n        layers = []\n        for i in range(num_layers):\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=self.inplanes,\n                    out_channels=self.outplanes,\n                    kernel_size=4,\n                    stride=2,\n                    padding=1,\n                    output_padding=0,\n                    bias=False))\n            layers.append(nn.BatchNorm2d(self.outplanes))\n            layers.append(nn.ReLU(inplace=True))\n            self.inplanes = self.outplanes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.deconv_layers(x)\n        x = self.final_layer(x)\n\n        return x\n\n    def init_weights(self):\n        for name, m in self.deconv_layers.named_modules():\n            if isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        for m in self.final_layer.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n\ndef soft_argmax(heatmaps, joint_num):\n\n    heatmaps = heatmaps.reshape((-1, joint_num, cfg.depth_dim*cfg.output_shape[0]*cfg.output_shape[1]))\n    heatmaps = F.softmax(heatmaps, 2)\n    heatmaps = heatmaps.reshape((-1, joint_num, cfg.depth_dim, cfg.output_shape[0], cfg.output_shape[1]))\n\n    accu_x = heatmaps.sum(dim=(2,3))\n    accu_y = heatmaps.sum(dim=(2,4))\n    accu_z = heatmaps.sum(dim=(3,4))\n\n    accu_x = accu_x * torch.cuda.comm.broadcast(torch.arange(1,cfg.output_shape[1]+1).type(torch.cuda.FloatTensor), devices=[accu_x.device.index])[0]\n    accu_y = accu_y * torch.cuda.comm.broadcast(torch.arange(1,cfg.output_shape[0]+1).type(torch.cuda.FloatTensor), devices=[accu_y.device.index])[0]\n    accu_z = accu_z * torch.cuda.comm.broadcast(torch.arange(1,cfg.depth_dim+1).type(torch.cuda.FloatTensor), devices=[accu_z.device.index])[0]\n\n    accu_x = accu_x.sum(dim=2, keepdim=True) -1\n    accu_y = accu_y.sum(dim=2, keepdim=True) -1\n    accu_z = accu_z.sum(dim=2, keepdim=True) -1\n\n    coord_out = torch.cat((accu_x, accu_y, accu_z), dim=2)\n\n    return coord_out\n\nclass ResPoseNet(nn.Module):\n    def __init__(self, backbone, head, joint_num):\n        super(ResPoseNet, self).__init__()\n        self.backbone = backbone\n        self.head = head\n        self.joint_num = joint_num\n\n    def forward(self, input_img, target=None):\n        fm = self.backbone(input_img)\n        hm = self.head(fm)\n        coord = soft_argmax(hm, self.joint_num)\n        \n        if target is None:\n            return coord\n        else:\n            target_coord = target['coord']\n            target_vis = target['vis']\n            target_have_depth = target['have_depth']\n            \n            ## coordinate loss\n            loss_coord = torch.abs(coord - target_coord) * target_vis\n            loss_coord = (loss_coord[:,:,0] + loss_coord[:,:,1] + loss_coord[:,:,2] * target_have_depth)/3.\n            \n            return loss_coord\n\ndef get_pose_net(cfg, is_train, joint_num):\n    \n    backbone = ResNetBackbone(cfg.resnet_type)\n    head_net = HeadNet(joint_num)\n    if is_train:\n        backbone.init_weights()\n        head_net.init_weights()\n\n    model = ResPoseNet(backbone, head_net, joint_num)\n    return model\n\n"""
main/test.py,2,"b'import argparse\nfrom tqdm import tqdm\nimport numpy as np\nimport cv2\nfrom config import cfg\nimport torch\nfrom base import Tester\nfrom utils.vis import vis_keypoints\nfrom utils.pose_utils import flip\nimport torch.backends.cudnn as cudnn\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', type=str, dest=\'gpu_ids\')\n    parser.add_argument(\'--test_epoch\', type=str, dest=\'test_epoch\')\n    args = parser.parse_args()\n\n    # test gpus\n    if not args.gpu_ids:\n        assert 0, print(""Please set proper gpu ids"")\n\n    if \'-\' in args.gpu_ids:\n        gpus = args.gpu_ids.split(\'-\')\n        gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\n        gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\n        args.gpu_ids = \',\'.join(map(lambda x: str(x), list(range(*gpus))))\n    \n    assert args.test_epoch, \'Test epoch is required.\'\n    return args\n\ndef main():\n\n    args = parse_args()\n    cfg.set_args(args.gpu_ids)\n    cudnn.fastest = True\n    cudnn.benchmark = True\n    cudnn.deterministic = False\n    cudnn.enabled = True\n\n    tester = Tester(args.test_epoch)\n    tester._make_batch_generator()\n    tester._make_model()\n\n    preds = []\n\n    with torch.no_grad():\n        for itr, input_img in enumerate(tqdm(tester.batch_generator)):\n            \n            # forward\n            coord_out = tester.model(input_img)\n\n            if cfg.flip_test:\n                flipped_input_img = flip(input_img, dims=3)\n                flipped_coord_out = tester.model(flipped_input_img)\n                flipped_coord_out[:, :, 0] = cfg.output_shape[1] - flipped_coord_out[:, :, 0] - 1\n                for pair in tester.flip_pairs:\n                    flipped_coord_out[:, pair[0], :], flipped_coord_out[:, pair[1], :] = flipped_coord_out[:, pair[1], :].clone(), flipped_coord_out[:, pair[0], :].clone()\n                coord_out = (coord_out + flipped_coord_out)/2.\n\n            vis = False\n            if vis:\n                filename = str(itr)\n                tmpimg = input_img[0].cpu().numpy()\n                tmpimg = tmpimg * np.array(cfg.pixel_std).reshape(3,1,1) + np.array(cfg.pixel_mean).reshape(3,1,1)\n                tmpimg = tmpimg.astype(np.uint8)\n                tmpimg = tmpimg[::-1, :, :]\n                tmpimg = np.transpose(tmpimg,(1,2,0)).copy()\n                tmpkps = np.zeros((3,tester.joint_num))\n                tmpkps[:2,:] = coord_out[0,:,:2].cpu().numpy().transpose(1,0) / cfg.output_shape[0] * cfg.input_shape[0]\n                tmpkps[2,:] = 1\n                tmpimg = vis_keypoints(tmpimg, tmpkps, tester.skeleton)\n                cv2.imwrite(filename + \'_output.jpg\', tmpimg)\n\n            coord_out = coord_out.cpu().numpy()\n            preds.append(coord_out)\n            \n    # evaluate\n    preds = np.concatenate(preds, axis=0)\n    tester._evaluate(preds, cfg.result_dir)    \n\nif __name__ == ""__main__"":\n    main()\n'"
main/train.py,8,"b'import argparse\nfrom config import cfg\nimport torch\nfrom base import Trainer\nimport torch.backends.cudnn as cudnn\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', type=str, dest=\'gpu_ids\')\n    parser.add_argument(\'--continue\', dest=\'continue_train\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.gpu_ids:\n        assert 0, print(""Please set proper gpu ids"")\n\n    if \'-\' in args.gpu_ids:\n        gpus = args.gpu_ids.split(\'-\')\n        gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\n        gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\n        args.gpu_ids = \',\'.join(map(lambda x: str(x), list(range(*gpus))))\n\n    return args\n\ndef main():\n    \n    # argument parse and create log\n    args = parse_args()\n    cfg.set_args(args.gpu_ids, args.continue_train)\n    cudnn.fastest = True\n    cudnn.benchmark = True\n\n    trainer = Trainer()\n    trainer._make_batch_generator()\n    trainer._make_model()\n\n    # train\n    for epoch in range(trainer.start_epoch, cfg.end_epoch):\n        \n        trainer.set_lr(epoch)\n        trainer.tot_timer.tic()\n        trainer.read_timer.tic()\n\n        for itr in range(trainer.itr_per_epoch):\n            \n            input_img_list, joint_img_list, joint_vis_list, joints_have_depth_list = [], [], [], []\n            for i in range(len(cfg.trainset)):\n                try:\n                    input_img, joint_img, joint_vis, joints_have_depth = next(trainer.iterator[i])\n                except StopIteration:\n                    trainer.iterator[i] = iter(trainer.batch_generator[i])\n                    input_img, joint_img, joint_vis, joints_have_depth = next(trainer.iterator[i])\n\n                input_img_list.append(input_img)\n                joint_img_list.append(joint_img)\n                joint_vis_list.append(joint_vis)\n                joints_have_depth_list.append(joints_have_depth)\n            \n            # aggregate items from different datasets into one single batch\n            input_img = torch.cat(input_img_list,dim=0)\n            joint_img = torch.cat(joint_img_list,dim=0)\n            joint_vis = torch.cat(joint_vis_list,dim=0)\n            joints_have_depth = torch.cat(joints_have_depth_list,dim=0)\n            \n            # shuffle items from different datasets\n            rand_idx = []\n            for i in range(len(cfg.trainset)):\n                rand_idx.append(torch.arange(i,input_img.shape[0],len(cfg.trainset)))\n            rand_idx = torch.cat(rand_idx,dim=0)\n            rand_idx = rand_idx[torch.randperm(input_img.shape[0])]\n            input_img = input_img[rand_idx]; joint_img = joint_img[rand_idx]; joint_vis = joint_vis[rand_idx]; joints_have_depth = joints_have_depth[rand_idx];\n            target = {\'coord\': joint_img, \'vis\': joint_vis, \'have_depth\': joints_have_depth}\n\n            trainer.read_timer.toc()\n            trainer.gpu_timer.tic()\n\n            trainer.optimizer.zero_grad()\n            \n            # forward\n            loss_coord = trainer.model(input_img, target)\n            loss_coord = loss_coord.mean()\n\n            # backward\n            loss = loss_coord\n            loss.backward()\n            trainer.optimizer.step()\n            \n            trainer.gpu_timer.toc()\n            screen = [\n                \'Epoch %d/%d itr %d/%d:\' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n                \'lr: %g\' % (trainer.get_lr()),\n                \'speed: %.2f(%.2fs r%.2f)s/itr\' % (\n                    trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n                \'%.2fh/epoch\' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n                \'%s: %.4f\' % (\'loss_coord\', loss_coord.detach()),\n                ]\n            trainer.logger.info(\' \'.join(screen))\n            trainer.tot_timer.toc()\n            trainer.tot_timer.tic()\n            trainer.read_timer.tic()\n\n        trainer.save_model({\n            \'epoch\': epoch,\n            \'network\': trainer.model.state_dict(),\n            \'optimizer\': trainer.optimizer.state_dict(),\n        }, epoch)\n        \n\nif __name__ == ""__main__"":\n    main()\n'"
vis/coco_img_name.py,0,"b""import os\nimport os.path as osp\nimport scipy.io as sio\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport json\nimport cv2\nimport random\nimport math\n\nannot_path = osp.join('coco', 'person_keypoints_val2017.json')\n\ndata = []\ndb = COCO(annot_path)\nfp = open('coco_img_name.txt','w') \nfor iid in db.imgs.keys():\n    img = db.imgs[iid]\n    imgname = img['file_name']\n    imgname = 'coco_' + imgname.split('.')[0]\n    fp.write(imgname + '\\n')\nfp.close()\n\n"""
vis/mupots_img_name.py,0,"b""import os\nimport os.path as osp\nimport scipy.io as sio\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport json\nimport cv2\nimport random\nimport math\n\nannot_path = osp.join('mupots', 'MuPoTS-3D.json')\n\ndata = []\ndb = COCO(annot_path)\nfp = open('mupots_img_name.txt','w') \nfor iid in db.imgs.keys():\n    img = db.imgs[iid]\n    imgname = img['file_name'].split('/')\n    folder_id = int(imgname[0][2:])\n    frame_id = int(imgname[1].split('.')[0][4:])\n    fp.write(str(folder_id) + ' ' + str(frame_id) + '\\n')\nfp.close()\n\n"""
common/nets/resnet.py,2,"b'import torch\nimport torch.nn as nn\nfrom torchvision.models.resnet import BasicBlock, Bottleneck\nfrom torchvision.models.resnet import model_urls\n\nclass ResNetBackbone(nn.Module):\n\n    def __init__(self, resnet_type):\n\t\n        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], \'resnet18\'),\n\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], \'resnet34\'),\n\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], \'resnet50\'),\n\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], \'resnet101\'),\n\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], \'resnet152\')}\n        block, layers, channels, name = resnet_spec[resnet_type]\n        \n        self.name = name\n        self.inplanes = 64\n        super(ResNetBackbone, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n    def init_weights(self):\n        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n        # drop orginal resnet fc layer, add \'None\' in case of no fc layer, that will raise error\n        org_resnet.pop(\'fc.weight\', None)\n        org_resnet.pop(\'fc.bias\', None)\n        self.load_state_dict(org_resnet)\n        print(""Initialize resnet from model zoo"")\n\n\n'"
common/utils/__init__.py,0,b''
common/utils/dir_utils.py,0,"b'import os\nimport sys\n\ndef make_folder(folder_name):\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n'"
common/utils/pose_utils.py,2,"b'import torch\nimport numpy as np\nfrom config import cfg\nimport copy\n\ndef cam2pixel(cam_coord, f, c):\n    x = cam_coord[:, 0] / (cam_coord[:, 2] + 1e-8) * f[0] + c[0]\n    y = cam_coord[:, 1] / (cam_coord[:, 2] + 1e-8) * f[1] + c[1]\n    z = cam_coord[:, 2]\n    img_coord = np.concatenate((x[:,None], y[:,None], z[:,None]),1)\n    return img_coord\n\ndef pixel2cam(pixel_coord, f, c):\n    x = (pixel_coord[:, 0] - c[0]) / f[0] * pixel_coord[:, 2]\n    y = (pixel_coord[:, 1] - c[1]) / f[1] * pixel_coord[:, 2]\n    z = pixel_coord[:, 2]\n    cam_coord = np.concatenate((x[:,None], y[:,None], z[:,None]),1)\n    return cam_coord\n\ndef world2cam(world_coord, R, t):\n    cam_coord = np.dot(R, world_coord.transpose(1,0)).transpose(1,0) + t.reshape(1,3)\n    return cam_coord\n\ndef rigid_transform_3D(A, B):\n    centroid_A = np.mean(A, axis = 0)\n    centroid_B = np.mean(B, axis = 0)\n    H = np.dot(np.transpose(A - centroid_A), B - centroid_B)\n    U, s, V = np.linalg.svd(H)\n    R = np.dot(np.transpose(V), np.transpose(U))\n    if np.linalg.det(R) < 0:\n        V[2] = -V[2]\n        R = np.dot(np.transpose(V), np.transpose(U))\n    t = -np.dot(R, np.transpose(centroid_A)) + np.transpose(centroid_B)\n    return R, t\n\ndef rigid_align(A, B):\n    R, t = rigid_transform_3D(A, B)\n    A2 = np.transpose(np.dot(R, np.transpose(A))) + t\n    return A2\n\ndef get_bbox(joint_img):\n    # bbox extract from keypoint coordinates\n    bbox = np.zeros((4))\n    xmin = np.min(joint_img[:,0])\n    ymin = np.min(joint_img[:,1])\n    xmax = np.max(joint_img[:,0])\n    ymax = np.max(joint_img[:,1])\n    width = xmax - xmin - 1\n    height = ymax - ymin - 1\n    \n    bbox[0] = (xmin + xmax)/2. - width/2*1.2\n    bbox[1] = (ymin + ymax)/2. - height/2*1.2\n    bbox[2] = width*1.2\n    bbox[3] = height*1.2\n\n    return bbox\n\ndef process_bbox(bbox, width, height):\n    # sanitize bboxes\n    x, y, w, h = bbox\n    x1 = np.max((0, x))\n    y1 = np.max((0, y))\n    x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n    y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n    if w*h > 0 and x2 >= x1 and y2 >= y1:\n        bbox = np.array([x1, y1, x2-x1, y2-y1])\n    else:\n        return None\n\n    # aspect ratio preserving bbox\n    w = bbox[2]\n    h = bbox[3]\n    c_x = bbox[0] + w/2.\n    c_y = bbox[1] + h/2.\n    aspect_ratio = cfg.input_shape[1]/cfg.input_shape[0]\n    if w > aspect_ratio * h:\n        h = w / aspect_ratio\n    elif w < aspect_ratio * h:\n        w = h * aspect_ratio\n    bbox[2] = w*1.25\n    bbox[3] = h*1.25\n    bbox[0] = c_x - bbox[2]/2.\n    bbox[1] = c_y - bbox[3]/2.\n    return bbox\n\ndef transform_joint_to_other_db(src_joint, src_name, dst_name):\n    src_joint_num = len(src_name)\n    dst_joint_num = len(dst_name)\n\n    new_joint = np.zeros(((dst_joint_num,) + src_joint.shape[1:]))\n\n    for src_idx in range(len(src_name)):\n        name = src_name[src_idx]\n        if name in dst_name:\n            dst_idx = dst_name.index(name)\n            new_joint[dst_idx] = src_joint[src_idx]\n\n    return new_joint\n\n\ndef fliplr_joints(_joints, width, matched_parts):\n    """"""\n    flip coords\n    joints: numpy array, nJoints * dim, dim == 2 [x, y] or dim == 3  [x, y, z]\n    width: image width\n    matched_parts: list of pairs\n    """"""\n    joints = _joints.copy()\n    # Flip horizontal\n    joints[:, 0] = width - joints[:, 0] - 1\n\n    # Change left-right parts\n    for pair in matched_parts:\n        joints[pair[0], :], joints[pair[1], :] = joints[pair[1], :], joints[pair[0], :].copy()\n\n    return joints\n\ndef multi_meshgrid(*args):\n    """"""\n    Creates a meshgrid from possibly many\n    elements (instead of only 2).\n    Returns a nd tensor with as many dimensions\n    as there are arguments\n    """"""\n    args = list(args)\n    template = [1 for _ in args]\n    for i in range(len(args)):\n        n = args[i].shape[0]\n        template_copy = template.copy()\n        template_copy[i] = n\n        args[i] = args[i].view(*template_copy)\n        # there will be some broadcast magic going on\n    return tuple(args)\n\n\ndef flip(tensor, dims):\n    if not isinstance(dims, (tuple, list)):\n        dims = [dims]\n    indices = [torch.arange(tensor.shape[dim] - 1, -1, -1,\n                            dtype=torch.int64) for dim in dims]\n    multi_indices = multi_meshgrid(*indices)\n    final_indices = [slice(i) for i in tensor.shape]\n    for i, dim in enumerate(dims):\n        final_indices[dim] = multi_indices[i]\n    flipped = tensor[final_indices]\n    assert flipped.device == tensor.device\n    assert flipped.requires_grad == tensor.requires_grad\n    return flipped\n\n'"
common/utils/vis.py,0,"b""import os\nimport cv2\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom config import cfg\n\ndef vis_keypoints(img, kps, kps_lines, kp_thresh=0.4, alpha=1):\n\n    # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n    cmap = plt.get_cmap('rainbow')\n    colors = [cmap(i) for i in np.linspace(0, 1, len(kps_lines) + 2)]\n    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n    # Perform the drawing on a copy of the image, to allow for blending.\n    kp_mask = np.copy(img)\n\n    # Draw the keypoints.\n    for l in range(len(kps_lines)):\n        i1 = kps_lines[l][0]\n        i2 = kps_lines[l][1]\n        p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n        p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n        if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n            cv2.line(\n                kp_mask, p1, p2,\n                color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n        if kps[2, i1] > kp_thresh:\n            cv2.circle(\n                kp_mask, p1,\n                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n        if kps[2, i2] > kp_thresh:\n            cv2.circle(\n                kp_mask, p2,\n                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n    # Blend the keypoints.\n    return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\ndef vis_3d_skeleton(kpt_3d, kpt_3d_vis, kps_lines, filename=None):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n    cmap = plt.get_cmap('rainbow')\n    colors = [cmap(i) for i in np.linspace(0, 1, len(kps_lines) + 2)]\n    colors = [np.array((c[2], c[1], c[0])) for c in colors]\n\n    for l in range(len(kps_lines)):\n        i1 = kps_lines[l][0]\n        i2 = kps_lines[l][1]\n        x = np.array([kpt_3d[i1,0], kpt_3d[i2,0]])\n        y = np.array([kpt_3d[i1,1], kpt_3d[i2,1]])\n        z = np.array([kpt_3d[i1,2], kpt_3d[i2,2]])\n\n        if kpt_3d_vis[i1,0] > 0 and kpt_3d_vis[i2,0] > 0:\n            ax.plot(x, z, -y, c=colors[l], linewidth=2)\n        if kpt_3d_vis[i1,0] > 0:\n            ax.scatter(kpt_3d[i1,0], kpt_3d[i1,2], -kpt_3d[i1,1], c=colors[l], marker='o')\n        if kpt_3d_vis[i2,0] > 0:\n            ax.scatter(kpt_3d[i2,0], kpt_3d[i2,2], -kpt_3d[i2,1], c=colors[l], marker='o')\n\n    x_r = np.array([0, cfg.input_shape[1]], dtype=np.float32)\n    y_r = np.array([0, cfg.input_shape[0]], dtype=np.float32)\n    z_r = np.array([0, 1], dtype=np.float32)\n    \n    if filename is None:\n        ax.set_title('3D vis')\n    else:\n        ax.set_title(filename)\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Z Label')\n    ax.set_zlabel('Y Label')\n    #ax.set_xlim([0,cfg.input_shape[1]])\n    #ax.set_ylim([0,1])\n    #ax.set_zlim([-cfg.input_shape[0],0])\n    ax.legend()\n\n    plt.show()\n    cv2.waitKey(0)\n\n"""
data/Human36M/Human36M.py,0,"b'import os\nimport os.path as osp\nfrom pycocotools.coco import COCO\nimport numpy as np\nfrom config import cfg\nfrom utils.pose_utils import world2cam, cam2pixel, pixel2cam, rigid_align, process_bbox\nimport cv2\nimport random\nimport json\nfrom utils.vis import vis_keypoints, vis_3d_skeleton\n\nclass Human36M:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'Human36M\', \'images\')\n        self.annot_path = osp.join(\'..\', \'data\', \'Human36M\', \'annotations\')\n        self.human_bbox_root_dir = osp.join(\'..\', \'data\', \'Human36M\', \'bbox_root\', \'bbox_root_human36m_output.json\')\n        self.joint_num = 18 # original:17, but manually added \'Thorax\'\n        self.joints_name = (\'Pelvis\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Torso\', \'Neck\', \'Nose\', \'Head\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'Thorax\')\n        self.flip_pairs = ( (1, 4), (2, 5), (3, 6), (14, 11), (15, 12), (16, 13) )\n        self.skeleton = ( (0, 7), (7, 8), (8, 9), (9, 10), (8, 11), (11, 12), (12, 13), (8, 14), (14, 15), (15, 16), (0, 1), (1, 2), (2, 3), (0, 4), (4, 5), (5, 6) )\n        self.joints_have_depth = True\n        self.eval_joint = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9,  10, 11, 12, 13, 14, 15, 16) # exclude Thorax\n\n        self.action_name = [\'Directions\', \'Discussion\', \'Eating\', \'Greeting\', \'Phoning\', \'Posing\', \'Purchases\', \'Sitting\', \'SittingDown\', \'Smoking\', \'Photo\', \'Waiting\', \'Walking\', \'WalkDog\', \'WalkTogether\']\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.lshoulder_idx = self.joints_name.index(\'L_Shoulder\')\n        self.rshoulder_idx = self.joints_name.index(\'R_Shoulder\')\n        self.protocol = 2\n        self.data = self.load_data()\n\n    def get_subsampling_ratio(self):\n        if self.data_split == \'train\':\n            return 5\n        elif self.data_split == \'test\':\n            return 64\n        else:\n            assert 0, print(\'Unknown subset\')\n\n    def get_subject(self):\n        if self.data_split == \'train\':\n            if self.protocol == 1:\n                subject = [1,5,6,7,8,9]\n            elif self.protocol == 2:\n                subject = [1,5,6,7,8]\n        elif self.data_split == \'test\':\n            if self.protocol == 1:\n                subject = [11]\n            elif self.protocol == 2:\n                subject = [9,11]\n        else:\n            assert 0, print(""Unknown subset"")\n\n        return subject\n    \n    def add_thorax(self, joint_coord):\n        thorax = (joint_coord[self.lshoulder_idx, :] + joint_coord[self.rshoulder_idx, :]) * 0.5\n        thorax = thorax.reshape((1, 3))\n        joint_coord = np.concatenate((joint_coord, thorax), axis=0)\n        return joint_coord\n\n    def load_data(self):\n        print(\'Load data of H36M Protocol \' + str(self.protocol))\n\n        subject_list = self.get_subject()\n        sampling_ratio = self.get_subsampling_ratio()\n        \n        # aggregate annotations from each subject\n        db = COCO()\n        cameras = {}\n        joints = {}\n        for subject in subject_list:\n            # data load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_data.json\'),\'r\') as f:\n                annot = json.load(f)\n            if len(db.dataset) == 0:\n                for k,v in annot.items():\n                    db.dataset[k] = v\n            else:\n                for k,v in annot.items():\n                    db.dataset[k] += v\n            # camera load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_camera.json\'),\'r\') as f:\n                cameras[str(subject)] = json.load(f)\n            # joint coordinate load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_joint_3d.json\'),\'r\') as f:\n                joints[str(subject)] = json.load(f)\n        db.createIndex()\n       \n        if self.data_split == \'test\' and not cfg.use_gt_info:\n            print(""Get bounding box and root from "" + self.human_bbox_root_dir)\n            bbox_root_result = {}\n            with open(self.human_bbox_root_dir) as f:\n                annot = json.load(f)\n            for i in range(len(annot)):\n                bbox_root_result[str(annot[i][\'image_id\'])] = {\'bbox\': np.array(annot[i][\'bbox\']), \'root\': np.array(annot[i][\'root_cam\'])}\n        else:\n            print(""Get bounding box and root from groundtruth"")\n\n        data = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n            image_id = ann[\'image_id\']\n            img = db.loadImgs(image_id)[0]\n            img_path = osp.join(self.img_dir, img[\'file_name\'])\n            img_width, img_height = img[\'width\'], img[\'height\']\n           \n            # check subject and frame_idx\n            subject = img[\'subject\']; frame_idx = img[\'frame_idx\'];\n            if subject not in subject_list:\n                continue\n            if frame_idx % sampling_ratio != 0:\n                continue\n\n            # camera parameter\n            cam_idx = img[\'cam_idx\']\n            cam_param = cameras[str(subject)][str(cam_idx)]\n            R,t,f,c = np.array(cam_param[\'R\'], dtype=np.float32), np.array(cam_param[\'t\'], dtype=np.float32), np.array(cam_param[\'f\'], dtype=np.float32), np.array(cam_param[\'c\'], dtype=np.float32)\n                \n            # project world coordinate to cam, image coordinate space\n            action_idx = img[\'action_idx\']; subaction_idx = img[\'subaction_idx\']; frame_idx = img[\'frame_idx\'];\n            joint_world = np.array(joints[str(subject)][str(action_idx)][str(subaction_idx)][str(frame_idx)], dtype=np.float32)\n            joint_world = self.add_thorax(joint_world)\n            joint_cam = world2cam(joint_world, R, t)\n            joint_img = cam2pixel(joint_cam, f, c)\n            joint_img[:,2] = joint_img[:,2] - joint_cam[self.root_idx,2]\n            joint_vis = np.ones((self.joint_num,1))\n            \n            if self.data_split == \'test\' and not cfg.use_gt_info:\n                bbox = bbox_root_result[str(image_id)][\'bbox\'] # bbox should be aspect ratio preserved-extended. It is done in RootNet.\n                root_cam = bbox_root_result[str(image_id)][\'root\']\n            else:\n                bbox = process_bbox(np.array(ann[\'bbox\']), img_width, img_height)\n                if bbox is None: continue\n                root_cam = joint_cam[self.root_idx]\n               \n            data.append({\n                \'img_path\': img_path,\n                \'img_id\': image_id,\n                \'bbox\': bbox,\n                \'joint_img\': joint_img, # [org_img_x, org_img_y, depth - root_depth]\n                \'joint_cam\': joint_cam, # [X, Y, Z] in camera coordinate\n                \'joint_vis\': joint_vis,\n                \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                \'f\': f,\n                \'c\': c})\n           \n        return data\n\n    def evaluate(self, preds, result_dir):\n        \n        print(\'Evaluation start...\')\n        gts = self.data\n        assert len(gts) == len(preds)\n        sample_num = len(gts)\n        \n        pred_save = []\n        error = np.zeros((sample_num, self.joint_num-1)) # joint error\n        error_action = [ [] for _ in range(len(self.action_name)) ] # error for each sequence\n        for n in range(sample_num):\n            gt = gts[n]\n            image_id = gt[\'img_id\']\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\']\n            gt_3d_root = gt[\'root_cam\']\n            gt_3d_kpt = gt[\'joint_cam\']\n            gt_vis = gt[\'joint_vis\']\n            \n            # restore coordinates to original space\n            pred_2d_kpt = preds[n].copy()\n            pred_2d_kpt[:,0] = pred_2d_kpt[:,0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_2d_kpt[:,1] = pred_2d_kpt[:,1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n            pred_2d_kpt[:,2] = (pred_2d_kpt[:,2] / cfg.depth_dim * 2 - 1) * (cfg.bbox_3d_shape[0]/2) + gt_3d_root[2]\n\n            vis = False\n            if vis:\n                cvimg = cv2.imread(gt[\'img_path\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n                filename = str(random.randrange(1,500))\n                tmpimg = cvimg.copy().astype(np.uint8)\n                tmpkps = np.zeros((3,self.joint_num))\n                tmpkps[0,:], tmpkps[1,:] = pred_2d_kpt[:,0], pred_2d_kpt[:,1]\n                tmpkps[2,:] = 1\n                tmpimg = vis_keypoints(tmpimg, tmpkps, self.skeleton)\n                cv2.imwrite(filename + \'_output.jpg\', tmpimg)\n\n            # back project to camera coordinate system\n            pred_3d_kpt = pixel2cam(pred_2d_kpt, f, c)\n \n            # root joint alignment\n            pred_3d_kpt = pred_3d_kpt - pred_3d_kpt[self.root_idx]\n            gt_3d_kpt  = gt_3d_kpt - gt_3d_kpt[self.root_idx]\n           \n            if self.protocol == 1:\n                # rigid alignment for PA MPJPE (protocol #1)\n                pred_3d_kpt = rigid_align(pred_3d_kpt, gt_3d_kpt)\n            \n            # exclude thorax\n            pred_3d_kpt = np.take(pred_3d_kpt, self.eval_joint, axis=0)\n            gt_3d_kpt = np.take(gt_3d_kpt, self.eval_joint, axis=0)\n           \n            # error calculate\n            error[n] = np.sqrt(np.sum((pred_3d_kpt - gt_3d_kpt)**2,1))\n            img_name = gt[\'img_path\']\n            action_idx = int(img_name[img_name.find(\'act\')+4:img_name.find(\'act\')+6]) - 2\n            error_action[action_idx].append(error[n].copy())\n\n            # prediction save\n            pred_save.append({\'image_id\': image_id, \'joint_cam\': pred_3d_kpt.tolist(), \'bbox\': bbox.tolist(), \'root_cam\': gt_3d_root.tolist()}) # joint_cam is root-relative coordinate\n\n        # total error\n        tot_err = np.mean(error)\n        metric = \'PA MPJPE\' if self.protocol == 1 else \'MPJPE\'\n        eval_summary = \'Protocol \' + str(self.protocol) + \' error (\' + metric + \') >> tot: %.2f\\n\' % (tot_err)\n\n        # error for each action\n        for i in range(len(error_action)):\n            err = np.mean(np.array(error_action[i]))\n            eval_summary += (self.action_name[i] + \': %.2f \' % err)\n           \n        print(eval_summary)\n\n        # prediction save\n        output_path = osp.join(result_dir, \'bbox_root_pose_human36m_output.json\')\n        with open(output_path, \'w\') as f:\n            json.dump(pred_save, f)\n        print(""Test result is saved at "" + output_path)\n\n'"
data/MPII/MPII.py,0,"b""import os\nimport os.path as osp\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom utils.pose_utils import process_bbox\nfrom config import cfg\n\nclass MPII:\n\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join('..', 'data', 'MPII')\n        self.train_annot_path = osp.join('..', 'data', 'MPII', 'annotations', 'train.json')\n        self.joint_num = 16\n        self.joints_name = ('R_Ankle', 'R_Knee', 'R_Hip', 'L_Hip', 'L_Knee', 'L_Ankle', 'Pelvis', 'Thorax', 'Neck', 'Head', 'R_Wrist', 'R_Elbow', 'R_Shoulder', 'L_Shoulder', 'L_Elbow', 'L_Wrist')\n        self.flip_pairs = ( (0, 5), (1, 4), (2, 3), (10, 15), (11, 14), (12, 13) )\n        self.skeleton = ( (0, 1), (1, 2), (2, 6), (7, 12), (12, 11), (11, 10), (5, 4), (4, 3), (3, 6), (7, 13), (13, 14), (14, 15), (6, 7), (7, 8), (8, 9) )\n        self.joints_have_depth = False\n        self.data = self.load_data()\n\n    def load_data(self):\n        \n        if self.data_split == 'train':\n            db = COCO(self.train_annot_path)\n        else:\n            print('Unknown data subset')\n            assert 0\n\n        data = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n            img = db.loadImgs(ann['image_id'])[0]\n            width, height = img['width'], img['height']\n\n            if ann['num_keypoints'] == 0:\n                continue\n            \n            bbox = process_bbox(ann['bbox'], width, height)\n            if bbox is None: continue\n\n            # joints and vis\n            joint_img = np.array(ann['keypoints']).reshape(self.joint_num,3)\n            joint_vis = joint_img[:,2].copy().reshape(-1,1)\n            joint_img[:,2] = 0\n\n            imgname = img['file_name']\n            img_path = osp.join(self.img_dir, imgname)\n            data.append({\n                'img_path': img_path,\n                'bbox': bbox,\n                'joint_img': joint_img, # [org_img_x, org_img_y, 0]\n                'joint_vis': joint_vis,\n            })\n\n        return data\n\n"""
data/MSCOCO/MSCOCO.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom config import cfg\nimport scipy.io as sio\nimport json\nimport cv2\nimport random\nimport math\nfrom utils.pose_utils import pixel2cam\nfrom utils.vis import vis_keypoints, vis_3d_skeleton\n\n\nclass MSCOCO:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MSCOCO\', \'images\')\n        self.train_annot_path = osp.join(\'..\', \'data\', \'MSCOCO\', \'annotations\', \'person_keypoints_train2017.json\')\n        self.test_annot_path = osp.join(\'..\', \'data\', \'MSCOCO\', \'annotations\', \'person_keypoints_val2017.json\')\n        self.human_3d_bbox_root_dir = osp.join(\'..\', \'data\', \'MSCOCO\', \'bbox_root\', \'bbox_root_coco_output.json\')\n        \n        if self.data_split == \'train\':\n            self.joint_num = 19 # original: 17, but manually added \'Thorax\', \'Pelvis\'\n            self.joints_name = (\'Nose\', \'L_Eye\', \'R_Eye\', \'L_Ear\', \'R_Ear\', \'L_Shoulder\', \'R_Shoulder\', \'L_Elbow\', \'R_Elbow\', \'L_Wrist\', \'R_Wrist\', \'L_Hip\', \'R_Hip\', \'L_Knee\', \'R_Knee\', \'L_Ankle\', \'R_Ankle\', \'Thorax\', \'Pelvis\')\n            self.flip_pairs = ( (1, 2), (3, 4), (5, 6), (7, 8), (9, 10), (11, 12), (13, 14), (15, 16) )\n            self.skeleton = ( (1, 2), (0, 1), (0, 2), (2, 4), (1, 3), (6, 8), (8, 10), (5, 7), (7, 9), (12, 14), (14, 16), (11, 13), (13, 15), (5, 6), (11, 12) )\n            self.joints_have_depth = False\n\n            self.lshoulder_idx = self.joints_name.index(\'L_Shoulder\')\n            self.rshoulder_idx = self.joints_name.index(\'R_Shoulder\')\n            self.lhip_idx = self.joints_name.index(\'L_Hip\')\n            self.rhip_idx = self.joints_name.index(\'R_Hip\')\n       \n        else:\n            ## testing settings (when test model trained on the MuCo-3DHP dataset)\n            self.joint_num = 21 # MuCo-3DHP\n            self.joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\', \'R_Hand\', \'L_Hand\', \'R_Toe\', \'L_Toe\') # MuCo-3DHP\n            self.original_joint_num = 17 # MuPoTS\n            self.original_joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\') # MuPoTS\n            self.flip_pairs = ( (2, 5), (3, 6), (4, 7), (8, 11), (9, 12), (10, 13) )\n            self.skeleton = ( (0, 16), (16, 1), (1, 15), (15, 14), (14, 8), (14, 11), (8, 9), (9, 10), (11, 12), (12, 13), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7) )\n            self.eval_joint = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)\n            self.joints_have_depth = False\n\n        self.data = self.load_data()\n\n    def load_data(self):\n\n        if self.data_split == \'train\':\n            db = COCO(self.train_annot_path)\n            data = []\n            for aid in db.anns.keys():\n                ann = db.anns[aid]\n                img = db.loadImgs(ann[\'image_id\'])[0]\n                width, height = img[\'width\'], img[\'height\']\n\n                if (ann[\'image_id\'] not in db.imgs) or ann[\'iscrowd\'] or (ann[\'num_keypoints\'] == 0):\n                    continue\n                \n                bbox = process_bbox(ann[\'bbox\'], width, height) \n                if bbox is None: continue\n\n                # joints and vis\n                joint_img = np.array(ann[\'keypoints\']).reshape(-1,3)\n                # add Thorax\n                thorax = (joint_img[self.lshoulder_idx, :] + joint_img[self.rshoulder_idx, :]) * 0.5\n                thorax[2] = joint_img[self.lshoulder_idx,2] * joint_img[self.rshoulder_idx,2]\n                thorax = thorax.reshape((1, 3))\n                # add Pelvis\n                pelvis = (joint_img[self.lhip_idx, :] + joint_img[self.rhip_idx, :]) * 0.5\n                pelvis[2] = joint_img[self.lhip_idx,2] * joint_img[self.rhip_idx,2]\n                pelvis = pelvis.reshape((1, 3))\n\n                joint_img = np.concatenate((joint_img, thorax, pelvis), axis=0)\n\n                joint_vis = (joint_img[:,2].copy().reshape(-1,1) > 0)\n                joint_img[:,2] = 0\n\n                imgname = osp.join(\'train2017\', db.imgs[ann[\'image_id\']][\'file_name\'])\n                img_path = osp.join(self.img_dir, imgname)\n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'joint_img\': joint_img, # [org_img_x, org_img_y, 0]\n                    \'joint_vis\': joint_vis,\n                    \'f\': np.array([1500, 1500]), \n                    \'c\': np.array([width/2, height/2]) \n                })\n\n        elif self.data_split == \'test\':\n            db = COCO(self.test_annot_path)\n            with open(self.human_3d_bbox_root_dir) as f:\n                annot = json.load(f)\n            data = [] \n            for i in range(len(annot)):\n                image_id = annot[i][\'image_id\']\n                img = db.loadImgs(image_id)[0]\n                img_path = osp.join(self.img_dir, \'val2017\', img[\'file_name\'])\n                fx, fy, cx, cy = 1500, 1500, img[\'width\']/2, img[\'height\']/2\n                f = np.array([fx, fy]); c = np.array([cx, cy]);\n                root_cam = np.array(annot[i][\'root_cam\']).reshape(3)\n                bbox = np.array(annot[i][\'bbox\']).reshape(4)\n\n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'joint_img\': np.zeros((self.original_joint_num, 3)), # dummy\n                    \'joint_cam\': np.zeros((self.original_joint_num, 3)), # dummy\n                    \'joint_vis\': np.zeros((self.original_joint_num, 1)), # dummy\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'f\': f,\n                    \'c\': c,\n                })\n\n        else:\n            print(\'Unknown data subset\')\n            assert 0\n\n\n        return data\n\n    def evaluate(self, preds, result_dir):\n        \n        print(\'Evaluation start...\')\n        gts = self.data\n        sample_num = len(preds)\n        joint_num = self.original_joint_num\n\n        pred_2d_save = {}\n        pred_3d_save = {}\n        for n in range(sample_num):\n            \n            gt = gts[n]\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\']\n            gt_3d_root = gt[\'root_cam\']\n            img_name = gt[\'img_path\'].split(\'/\')\n            img_name = \'coco_\' + img_name[-1].split(\'.\')[0] # e.g., coco_00000000\n            \n            # restore coordinates to original space\n            pred_2d_kpt = preds[n].copy()\n            # only consider eval_joint\n            pred_2d_kpt = np.take(pred_2d_kpt, self.eval_joint, axis=0)\n            pred_2d_kpt[:,0] = pred_2d_kpt[:,0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_2d_kpt[:,1] = pred_2d_kpt[:,1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n            pred_2d_kpt[:,2] = (pred_2d_kpt[:,2] / cfg.depth_dim * 2 - 1) * (cfg.bbox_3d_shape[0]/2) + gt_3d_root[2]\n\n            # 2d kpt save\n            if img_name in pred_2d_save:\n                pred_2d_save[img_name].append(pred_2d_kpt[:,:2])\n            else:\n                pred_2d_save[img_name] = [pred_2d_kpt[:,:2]]\n\n            vis = False\n            if vis:\n                cvimg = cv2.imread(gt[\'img_path\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n                filename = str(random.randrange(1,500))\n                tmpimg = cvimg.copy().astype(np.uint8)\n                tmpkps = np.zeros((3,joint_num))\n                tmpkps[0,:], tmpkps[1,:] = pred_2d_kpt[:,0], pred_2d_kpt[:,1]\n                tmpkps[2,:] = 1\n                tmpimg = vis_keypoints(tmpimg, tmpkps, self.skeleton)\n                cv2.imwrite(filename + \'_output.jpg\', tmpimg)\n\n            # back project to camera coordinate system\n            pred_3d_kpt = pixel2cam(pred_2d_kpt, f, c)\n            \n            # 3d kpt save\n            if img_name in pred_3d_save:\n                pred_3d_save[img_name].append(pred_3d_kpt)\n            else:\n                pred_3d_save[img_name] = [pred_3d_kpt]\n        \n        output_path = osp.join(result_dir,\'preds_2d_kpt_coco.mat\')\n        sio.savemat(output_path, pred_2d_save)\n        print(""Testing result is saved at "" + output_path)\n        output_path = osp.join(result_dir,\'preds_3d_kpt_coco.mat\')\n        sio.savemat(output_path, pred_3d_save)\n        print(""Testing result is saved at "" + output_path)\n\n'"
data/MuCo/MuCo.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\nimport math\nfrom utils.pose_utils import process_bbox\nfrom pycocotools.coco import COCO\nfrom config import cfg\n\nclass MuCo:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MuCo\', \'data\')\n        self.train_annot_path = osp.join(\'..\', \'data\', \'MuCo\', \'data\', \'MuCo-3DHP.json\')\n        self.joint_num = 21\n        self.joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\', \'R_Hand\', \'L_Hand\', \'R_Toe\', \'L_Toe\')\n        self.flip_pairs = ( (2, 5), (3, 6), (4, 7), (8, 11), (9, 12), (10, 13), (17, 18), (19, 20) )\n        self.skeleton = ( (0, 16), (16, 1), (1, 15), (15, 14), (14, 8), (14, 11), (8, 9), (9, 10), (10, 19), (11, 12), (12, 13), (13, 20), (1, 2), (2, 3), (3, 4), (4, 17), (1, 5), (5, 6), (6, 7), (7, 18) )\n        self.joints_have_depth = True\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.data = self.load_data()\n\n    def load_data(self):\n\n        if self.data_split == \'train\':\n            db = COCO(self.train_annot_path)\n        else:\n            print(\'Unknown data subset\')\n            assert 0\n\n        data = []\n        for iid in db.imgs.keys():\n            img = db.imgs[iid]\n            img_id = img[""id""]\n            img_width, img_height = img[\'width\'], img[\'height\']\n            imgname = img[\'file_name\']\n            img_path = osp.join(self.img_dir, imgname)\n            f = img[""f""]\n            c = img[""c""]\n\n            # crop the closest person to the camera\n            ann_ids = db.getAnnIds(img_id)\n            anns = db.loadAnns(ann_ids)\n\n            root_depths = [ann[\'keypoints_cam\'][self.root_idx][2] for ann in anns]\n            closest_pid = root_depths.index(min(root_depths))\n            pid_list = [closest_pid]\n            for i in range(len(anns)):\n                if i == closest_pid:\n                    continue\n                picked = True\n                for j in range(len(anns)):\n                    if i == j:\n                        continue\n                    dist = (np.array(anns[i][\'keypoints_cam\'][self.root_idx]) - np.array(anns[j][\'keypoints_cam\'][self.root_idx])) ** 2\n                    dist_2d = math.sqrt(np.sum(dist[:2]))\n                    dist_3d = math.sqrt(np.sum(dist))\n                    if dist_2d < 500 or dist_3d < 500:\n                        picked = False\n                if picked:\n                    pid_list.append(i)\n            \n            for pid in pid_list:\n                joint_cam = np.array(anns[pid][\'keypoints_cam\'])\n                root_cam = joint_cam[self.root_idx]\n                \n                joint_img = np.array(anns[pid][\'keypoints_img\'])\n                joint_img = np.concatenate([joint_img, joint_cam[:,2:]],1)\n                joint_img[:,2] = joint_img[:,2] - root_cam[2]\n                joint_vis = np.ones((self.joint_num,1))\n\n                bbox = process_bbox(anns[pid][\'bbox\'], img_width, img_height)\n                if bbox is None: continue\n\n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'joint_img\': joint_img, # [org_img_x, org_img_y, depth - root_depth]\n                    \'joint_cam\': joint_cam, # [X, Y, Z] in camera coordinate\n                    \'joint_vis\': joint_vis,\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'f\': f,\n                    \'c\': c\n                })\n\n\n        return data\n\n\n'"
data/MuPoTS/MuPoTS.py,0,"b'import os\nimport os.path as osp\nimport scipy.io as sio\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom config import cfg\nimport json\nimport cv2\nimport random\nimport math\nfrom utils.pose_utils import pixel2cam, process_bbox\nfrom utils.vis import vis_keypoints, vis_3d_skeleton\n\nclass MuPoTS:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MuPoTS\', \'data\', \'MultiPersonTestSet\')\n        self.test_annot_path = osp.join(\'..\', \'data\', \'MuPoTS\', \'data\', \'MuPoTS-3D.json\')\n        self.human_bbox_root_dir = osp.join(\'..\', \'data\', \'MuPoTS\', \'bbox_root\', \'bbox_root_mupots_output.json\')\n        self.joint_num = 21 # MuCo-3DHP\n        self.joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\', \'R_Hand\', \'L_Hand\', \'R_Toe\', \'L_Toe\') # MuCo-3DHP\n        self.original_joint_num = 17 # MuPoTS\n        self.original_joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\') # MuPoTS\n\n        self.flip_pairs = ( (2, 5), (3, 6), (4, 7), (8, 11), (9, 12), (10, 13) )\n        self.skeleton = ( (0, 16), (16, 1), (1, 15), (15, 14), (14, 8), (14, 11), (8, 9), (9, 10), (11, 12), (12, 13), (1, 2), (2, 3), (3, 4), (1, 5), (5, 6), (6, 7) )\n        self.eval_joint = (0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)\n        self.joints_have_depth = True\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.data = self.load_data()\n\n    def load_data(self):\n        \n        if self.data_split != \'test\':\n            print(\'Unknown data subset\')\n            assert 0\n        \n        data = []\n        db = COCO(self.test_annot_path)\n\n        # use gt bbox and root\n        if cfg.use_gt_info:\n            print(""Get bounding box and root from groundtruth"")\n            for aid in db.anns.keys():\n                ann = db.anns[aid]\n                if ann[\'is_valid\'] == 0:\n                    continue\n\n                image_id = ann[\'image_id\']\n                img = db.loadImgs(image_id)[0]\n                img_path = osp.join(self.img_dir, img[\'file_name\'])\n                fx, fy, cx, cy = img[\'intrinsic\']\n                f = np.array([fx, fy]); c = np.array([cx, cy]);\n\n                joint_cam = np.array(ann[\'keypoints_cam\'])\n                root_cam = joint_cam[self.root_idx]\n\n                joint_img = np.array(ann[\'keypoints_img\'])\n                joint_img = np.concatenate([joint_img, joint_cam[:,2:]],1)\n                joint_img[:,2] = joint_img[:,2] - root_cam[2]\n                joint_vis = np.ones((self.original_joint_num,1))\n                \n                bbox = np.array(ann[\'bbox\'])\n                img_width, img_height = img[\'width\'], img[\'height\']\n                bbox = process_bbox(bbox, img_width, img_height)\n                if bbox is None: continue\n                \n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox, \n                    \'joint_img\': joint_img, # [org_img_x, org_img_y, depth - root_depth]\n                    \'joint_cam\': joint_cam, # [X, Y, Z] in camera coordinate\n                    \'joint_vis\': joint_vis,\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'f\': f,\n                    \'c\': c,\n                })\n           \n        else:\n            print(""Get bounding box and root from "" + self.human_bbox_root_dir)\n            with open(self.human_bbox_root_dir) as f:\n                annot = json.load(f)\n            \n            for i in range(len(annot)):\n                image_id = annot[i][\'image_id\']\n                img = db.loadImgs(image_id)[0]\n                img_width, img_height = img[\'width\'], img[\'height\']\n                img_path = osp.join(self.img_dir, img[\'file_name\'])\n                fx, fy, cx, cy = img[\'intrinsic\']\n                f = np.array([fx, fy]); c = np.array([cx, cy]);\n                root_cam = np.array(annot[i][\'root_cam\']).reshape(3)\n                bbox = np.array(annot[i][\'bbox\']).reshape(4)\n\n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'joint_img\': np.zeros((self.original_joint_num, 3)), # dummy\n                    \'joint_cam\': np.zeros((self.original_joint_num, 3)), # dummy\n                    \'joint_vis\': np.zeros((self.original_joint_num, 1)), # dummy\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'f\': f,\n                    \'c\': c,\n                })\n\n        return data\n\n    def evaluate(self, preds, result_dir):\n        \n        print(\'Evaluation start...\')\n        gts = self.data\n        sample_num = len(preds)\n        joint_num = self.original_joint_num\n \n        pred_2d_save = {}\n        pred_3d_save = {}\n        for n in range(sample_num):\n            \n            gt = gts[n]\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\']\n            gt_3d_root = gt[\'root_cam\']\n            img_name = gt[\'img_path\'].split(\'/\')\n            img_name = img_name[-2] + \'_\' + img_name[-1].split(\'.\')[0] # e.g., TS1_img_0001\n            \n            # restore coordinates to original space\n            pred_2d_kpt = preds[n].copy()\n            # only consider eval_joint\n            pred_2d_kpt = np.take(pred_2d_kpt, self.eval_joint, axis=0)\n            pred_2d_kpt[:,0] = pred_2d_kpt[:,0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_2d_kpt[:,1] = pred_2d_kpt[:,1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n            pred_2d_kpt[:,2] = (pred_2d_kpt[:,2] / cfg.depth_dim * 2 - 1) * (cfg.bbox_3d_shape[0]/2) + gt_3d_root[2]\n\n            # 2d kpt save\n            if img_name in pred_2d_save:\n                pred_2d_save[img_name].append(pred_2d_kpt[:,:2])\n            else:\n                pred_2d_save[img_name] = [pred_2d_kpt[:,:2]]\n\n            vis = False\n            if vis:\n                cvimg = cv2.imread(gt[\'img_path\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n                filename = str(random.randrange(1,500))\n                tmpimg = cvimg.copy().astype(np.uint8)\n                tmpkps = np.zeros((3,joint_num))\n                tmpkps[0,:], tmpkps[1,:] = pred_2d_kpt[:,0], pred_2d_kpt[:,1]\n                tmpkps[2,:] = 1\n                tmpimg = vis_keypoints(tmpimg, tmpkps, self.skeleton)\n                cv2.imwrite(filename + \'_output.jpg\', tmpimg)\n\n            # back project to camera coordinate system\n            pred_3d_kpt = pixel2cam(pred_2d_kpt, f, c)\n            \n            # 3d kpt save\n            if img_name in pred_3d_save:\n                pred_3d_save[img_name].append(pred_3d_kpt)\n            else:\n                pred_3d_save[img_name] = [pred_3d_kpt]\n        \n        output_path = osp.join(result_dir,\'preds_2d_kpt_mupots.mat\')\n        sio.savemat(output_path, pred_2d_save)\n        print(""Testing result is saved at "" + output_path)\n        output_path = osp.join(result_dir,\'preds_3d_kpt_mupots.mat\')\n        sio.savemat(output_path, pred_3d_save)\n        print(""Testing result is saved at "" + output_path)\n\n'"
tool/Human36M/h36m2coco.py,0,"b'import os\nimport os.path as osp\nimport scipy.io as sio\nimport numpy as np\nimport cv2\nimport random\nimport json\nimport math\nfrom tqdm import tqdm\n\nroot_dir = \'./images\' # define path here\nsave_dir = \'./annotations\' # define path here\n\njoint_num = 17\nsubject_list = [1, 5, 6, 7, 8, 9, 11]\naction_idx = (2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16)\nsubaction_idx = (1, 2)\ncamera_idx = (1, 2, 3, 4)\naction_name = [\'Directions\', \'Discussion\', \'Eating\', \'Greeting\', \'Phoning\', \'Posing\', \'Purchases\', \'Sitting\', \'SittingDown\', \'Smoking\', \'Photo\', \'Waiting\', \'Walking\', \'WalkDog\', \'WalkTogether\']\n\ndef load_h36m_annot_file(annot_file):\n    data = sio.loadmat(annot_file)\n    joint_world = data[\'pose3d_world\'] # 3D world coordinates of keypoints\n    R = data[\'R\'] # extrinsic\n    T = np.reshape(data[\'T\'],(3)) # extrinsic\n    f = np.reshape(data[\'f\'],(-1)) # focal legnth\n    c = np.reshape(data[\'c\'],(-1)) # principal points\n    img_heights = np.reshape(data[\'img_height\'],(-1))\n    img_widths = np.reshape(data[\'img_width\'],(-1))\n   \n    return joint_world, R, T, f, c, img_widths, img_heights\n\ndef _H36FolderName(subject_id, act_id, subact_id, camera_id):\n    return ""s_%02d_act_%02d_subact_%02d_ca_%02d"" % \\\n           (subject_id, act_id, subact_id, camera_id)\n\ndef _H36ImageName(folder_name, frame_id):\n    return ""%s_%06d.jpg"" % (folder_name, frame_id + 1)\n\ndef cam2pixel(cam_coord, f, c):\n    x = cam_coord[..., 0] / cam_coord[..., 2] * f[0] + c[0]\n    y = cam_coord[..., 1] / cam_coord[..., 2] * f[1] + c[1]\n    return x,y\n\ndef world2cam(world_coord, R, t):\n    cam_coord = np.dot(R, world_coord - t)\n    return cam_coord\n\ndef get_bbox(joint_img):\n    bbox = np.zeros((4))\n    xmin = np.min(joint_img[:,0])\n    ymin = np.min(joint_img[:,1])\n    xmax = np.max(joint_img[:,0])\n    ymax = np.max(joint_img[:,1])\n    width = xmax - xmin - 1\n    height = ymax - ymin - 1\n    \n    bbox[0] = (xmin + xmax)/2. - width/2*1.2\n    bbox[1] = (ymin + ymax)/2. - height/2*1.2\n    bbox[2] = width*1.2\n    bbox[3] = height*1.2\n\n    return bbox\n\nimg_id = 0; annot_id = 0\nfor subject in tqdm(subject_list):\n    cam_param = {}\n    joint_3d = {}\n    images = []; annotations = [];\n    for aid in tqdm(action_idx):\n        for said in tqdm(subaction_idx):\n            for cid in tqdm(camera_idx):\n                folder = _H36FolderName(subject,aid,said,cid)\n                if folder == \'s_11_act_02_subact_02_ca_01\':\n                    continue\n               \n                joint_world, R, t, f, c, img_widths, img_heights = load_h36m_annot_file(osp.join(root_dir, folder, \'h36m_meta.mat\'))\n\n                if str(aid) not in joint_3d:\n                    joint_3d[str(aid)] = {}\n                if str(said) not in joint_3d[str(aid)]:\n                    joint_3d[str(aid)][str(said)] = {}\n\n                img_num = np.shape(joint_world)[0]\n                for n in range(img_num):\n                    img_dict = {}\n                    img_dict[\'id\'] = img_id\n                    img_dict[\'file_name\'] = osp.join(folder, _H36ImageName(folder, n))\n                    img_dict[\'width\'] = int(img_widths[n])\n                    img_dict[\'height\'] = int(img_heights[n])\n                    img_dict[\'subject\'] = subject\n                    img_dict[\'action_name\'] = action_name[aid-2]\n                    img_dict[\'action_idx\'] = aid\n                    img_dict[\'subaction_idx\'] = said\n                    img_dict[\'cam_idx\'] = cid\n                    img_dict[\'frame_idx\'] = n\n                    images.append(img_dict)\n                    \n                    if str(cid) not in cam_param:\n                        cam_param[str(cid)] = {\'R\': R.tolist(), \'t\': t.tolist(), \'f\': f.tolist(), \'c\': c.tolist()}\n                    if str(n) not in joint_3d[str(aid)][str(said)]:\n                        joint_3d[str(aid)][str(said)][str(n)] = joint_world[n].tolist()\n\n                    annot_dict = {}\n                    annot_dict[\'id\'] = annot_id\n                    annot_dict[\'image_id\'] = img_id\n\n                    # project world coordinate to cam, image coordinate space\n                    joint_cam = np.zeros((joint_num,3))\n                    for j in range(joint_num):\n                        joint_cam[j] = world2cam(joint_world[n][j], R, t)\n                    joint_img = np.zeros((joint_num,2))\n                    joint_img[:,0], joint_img[:,1] = cam2pixel(joint_cam, f, c)\n                    joint_vis = (joint_img[:,0] >= 0) * (joint_img[:,0] < img_widths[n]) * (joint_img[:,1] >= 0) * (joint_img[:,1] < img_heights[n])\n                    annot_dict[\'keypoints_vis\'] = joint_vis.tolist()\n                    \n                    bbox = get_bbox(joint_img)\n                    annot_dict[\'bbox\'] = bbox.tolist() # xmin, ymin, width, height\n                    annotations.append(annot_dict)\n\n                    img_id += 1\n                    annot_id += 1\n    \n    data = {\'images\': images, \'annotations\': annotations}\n    with open(osp.join(save_dir, \'Human36M_subject\' + str(subject) + \'_data.json\'), \'w\') as f:\n        json.dump(data, f)    \n    with open(osp.join(save_dir, \'Human36M_subject\' + str(subject) + \'_camera.json\'), \'w\') as f:\n        json.dump(cam_param, f)\n    with open(osp.join(save_dir, \'Human36M_subject\' + str(subject) + \'_joint_3d.json\'), \'w\') as f:\n        json.dump(joint_3d, f)\n'"
