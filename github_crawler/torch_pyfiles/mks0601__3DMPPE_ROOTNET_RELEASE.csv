file_path,api_count,code
common/base.py,7,"b'import os\nimport os.path as osp\nimport math\nimport time\nimport glob\nimport abc\nfrom torch.utils.data import DataLoader\nimport torch.optim\nimport torchvision.transforms as transforms\n\nfrom config import cfg\nfrom dataset import DatasetLoader\nfrom timer import Timer\nfrom logger import colorlogger\nfrom torch.nn.parallel.data_parallel import DataParallel\nfrom model import get_pose_net\n\n# dynamic dataset import\nfor i in range(len(cfg.trainset)):\n    exec(\'from \' + cfg.trainset[i] + \' import \' + cfg.trainset[i])\nexec(\'from \' + cfg.testset + \' import \' + cfg.testset)\n\nclass Base(object):\n    __metaclass__ = abc.ABCMeta\n\n    def __init__(self, log_name=\'logs.txt\'):\n        \n        self.cur_epoch = 0\n\n        # timer\n        self.tot_timer = Timer()\n        self.gpu_timer = Timer()\n        self.read_timer = Timer()\n\n        # logger\n        self.logger = colorlogger(cfg.log_dir, log_name=log_name)\n\n    @abc.abstractmethod\n    def _make_batch_generator(self):\n        return\n\n    @abc.abstractmethod\n    def _make_model(self):\n        return\n\n    def save_model(self, state, epoch):\n        file_path = osp.join(cfg.model_dir,\'snapshot_{}.pth.tar\'.format(str(epoch)))\n        torch.save(state, file_path)\n        self.logger.info(""Write snapshot into {}"".format(file_path))\n\n    def load_model(self, model, optimizer):\n        model_file_list = glob.glob(osp.join(cfg.model_dir,\'*.pth.tar\'))\n        cur_epoch = max([int(file_name[file_name.find(\'snapshot_\') + 9 : file_name.find(\'.pth.tar\')]) for file_name in model_file_list])\n        ckpt = torch.load(osp.join(cfg.model_dir, \'snapshot_\' + str(cur_epoch) + \'.pth.tar\')) \n        start_epoch = ckpt[\'epoch\'] + 1\n        model.load_state_dict(ckpt[\'network\'])\n        optimizer.load_state_dict(ckpt[\'optimizer\'])\n\n        return start_epoch, model, optimizer\n\nclass Trainer(Base):\n    \n    def __init__(self):\n        super(Trainer, self).__init__(log_name = \'train_logs.txt\')\n\n    def get_optimizer(self, model):\n        optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n        return optimizer\n\n    def set_lr(self, epoch):\n        for e in cfg.lr_dec_epoch:\n            if epoch < e:\n                break\n        if epoch < cfg.lr_dec_epoch[-1]:\n            idx = cfg.lr_dec_epoch.index(e)\n            for g in self.optimizer.param_groups:\n                g[\'lr\'] = cfg.lr / (cfg.lr_dec_factor ** idx)\n        else:\n            for g in self.optimizer.param_groups:\n                g[\'lr\'] = cfg.lr / (cfg.lr_dec_factor ** len(cfg.lr_dec_epoch))\n\n    def get_lr(self):\n        for g in self.optimizer.param_groups:\n            cur_lr = g[\'lr\']\n        return cur_lr\n\n    def _make_batch_generator(self):\n        # data load and construct batch generator\n        self.logger.info(""Creating dataset..."")\n        trainset_loader = []\n        batch_generator = []\n        iterator = []\n        for i in range(len(cfg.trainset)):\n            trainset_loader.append(DatasetLoader(eval(cfg.trainset[i])(""train""), True, transforms.Compose([\\\n                                                                                                        transforms.ToTensor(),\n                                                                                                        transforms.Normalize(mean=cfg.pixel_mean, std=cfg.pixel_std)]\\\n                                                                                                        )))\n            batch_generator.append(DataLoader(dataset=trainset_loader[-1], batch_size=cfg.num_gpus*cfg.batch_size//len(cfg.trainset), shuffle=True, num_workers=cfg.num_thread, pin_memory=True))\n            iterator.append(iter(batch_generator[-1]))\n        \n        self.itr_per_epoch = math.ceil(trainset_loader[0].__len__() / cfg.num_gpus / (cfg.batch_size // len(cfg.trainset)))\n        self.batch_generator = batch_generator\n        self.iterator = iterator\n\n    def _make_model(self):\n        # prepare network\n        self.logger.info(""Creating graph and optimizer..."")\n        model = get_pose_net(cfg, True)\n        model = DataParallel(model).cuda()\n        optimizer = self.get_optimizer(model)\n        if cfg.continue_train:\n            start_epoch, model, optimizer = self.load_model(model, optimizer)\n        else:\n            start_epoch = 0\n        model.train()\n\n        self.start_epoch = start_epoch\n        self.model = model\n        self.optimizer = optimizer\n\nclass Tester(Base):\n    \n    def __init__(self, test_epoch):\n        self.test_epoch = int(test_epoch)\n        super(Tester, self).__init__(log_name = \'test_logs.txt\')\n\n    def _make_batch_generator(self):\n        # data load and construct batch generator\n        self.logger.info(""Creating dataset..."")\n        testset = eval(cfg.testset)(""test"")\n        testset_loader = DatasetLoader(testset, False, transforms.Compose([\\\n                                                                        transforms.ToTensor(),\n                                                                        transforms.Normalize(mean=cfg.pixel_mean, std=cfg.pixel_std)]\\\n                                                                        ))\n        batch_generator = DataLoader(dataset=testset_loader, batch_size=cfg.num_gpus*cfg.test_batch_size, shuffle=False, num_workers=cfg.num_thread, pin_memory=True)\n        \n        self.testset = testset\n        self.batch_generator = batch_generator\n    \n    def _make_model(self):\n        \n        model_path = os.path.join(cfg.model_dir, \'snapshot_%d.pth.tar\' % self.test_epoch)\n        assert os.path.exists(model_path), \'Cannot find model at \' + model_path\n        self.logger.info(\'Load checkpoint from {}\'.format(model_path))\n        \n        # prepare network\n        self.logger.info(""Creating graph..."")\n        model = get_pose_net(cfg, False)\n        model = DataParallel(model).cuda()\n        ckpt = torch.load(model_path)\n        model.load_state_dict(ckpt[\'network\'])\n        model.eval()\n\n        self.model = model\n\n    def _evaluate(self, preds, result_save_path):\n        self.testset.evaluate(preds, result_save_path)\n'"
common/logger.py,0,"b'import logging\nimport os\n\nOK = \'\\033[92m\'\nWARNING = \'\\033[93m\'\nFAIL = \'\\033[91m\'\nEND = \'\\033[0m\'\n\nPINK = \'\\033[95m\'\nBLUE = \'\\033[94m\'\nGREEN = OK\nRED = FAIL\nWHITE = END\nYELLOW = WARNING\n\nclass colorlogger():\n    def __init__(self, log_dir, log_name=\'train_logs.txt\'):\n        # set log\n        self._logger = logging.getLogger(log_name)\n        self._logger.setLevel(logging.INFO)\n        log_file = os.path.join(log_dir, log_name)\n        if not os.path.exists(log_dir):\n            os.makedirs(log_dir)\n        file_log = logging.FileHandler(log_file, mode=\'a\')\n        file_log.setLevel(logging.INFO)\n        console_log = logging.StreamHandler()\n        console_log.setLevel(logging.INFO)\n        formatter = logging.Formatter(\n            ""{}%(asctime)s{} %(message)s"".format(GREEN, END),\n            ""%m-%d %H:%M:%S"")\n        file_log.setFormatter(formatter)\n        console_log.setFormatter(formatter)\n        self._logger.addHandler(file_log)\n        self._logger.addHandler(console_log)\n\n    def debug(self, msg):\n        self._logger.debug(str(msg))\n\n    def info(self, msg):\n        self._logger.info(str(msg))\n\n    def warning(self, msg):\n        self._logger.warning(WARNING + \'WRN: \' + str(msg) + END)\n\n    def critical(self, msg):\n        self._logger.critical(RED + \'CRI: \' + str(msg) + END)\n\n    def error(self, msg):\n        self._logger.error(RED + \'ERR: \' + str(msg) + END)\n\n'"
common/timer.py,0,"b'# --------------------------------------------------------\r\n# Fast R-CNN\r\n# Copyright (c) 2015 Microsoft\r\n# Licensed under The MIT License [see LICENSE for details]\r\n# Written by Ross Girshick\r\n# --------------------------------------------------------\r\n\r\nimport time\r\n\r\nclass Timer(object):\r\n    """"""A simple timer.""""""\r\n    def __init__(self):\r\n        self.total_time = 0.\r\n        self.calls = 0\r\n        self.start_time = 0.\r\n        self.diff = 0.\r\n        self.average_time = 0.\r\n        self.warm_up = 0\r\n\r\n    def tic(self):\r\n        # using time.time instead of time.clock because time time.clock\r\n        # does not normalize for multithreading\r\n        self.start_time = time.time()\r\n\r\n    def toc(self, average=True):\r\n        self.diff = time.time() - self.start_time\r\n        if self.warm_up < 10:\r\n            self.warm_up += 1\r\n            return self.diff\r\n        else:\r\n            self.total_time += self.diff\r\n            self.calls += 1\r\n            self.average_time = self.total_time / self.calls\r\n\r\n        if average:\r\n            return self.average_time\r\n        else:\r\n            return self.diff\r\n'"
data/dataset.py,1,"b'import numpy as np\nimport cv2\nimport random\nimport time\nimport torch\nimport copy\nimport math\nfrom torch.utils.data.dataset import Dataset\nfrom config import cfg\n\nclass DatasetLoader(Dataset):\n    def __init__(self, db, is_train, transform):\n        \n        self.db = db.data\n        self.joint_num = db.joint_num\n        self.root_idx = db.root_idx\n        self.joints_have_depth = db.joints_have_depth\n        \n        self.transform = transform\n        self.is_train = is_train\n\n        if self.is_train:\n            self.do_augment = True\n        else:\n            self.do_augment = False\n\n    def __getitem__(self, index):\n        \n        joints_have_depth = self.joints_have_depth \n        data = copy.deepcopy(self.db[index])\n\n        bbox = data[\'bbox\']\n        root_img = np.array(data[\'root_img\'])\n        root_vis = np.array(data[\'root_vis\'])\n        area = data[\'area\']\n        f = data[\'f\']\n\n        # 1. load image\n        cvimg = cv2.imread(data[\'img_path\'], cv2.IMREAD_COLOR | cv2.IMREAD_IGNORE_ORIENTATION)\n        if not isinstance(cvimg, np.ndarray):\n            raise IOError(""Fail to read %s"" % data[\'img_path\'])\n        img_height, img_width, img_channels = cvimg.shape\n        \n        # 2. get augmentation params\n        if self.do_augment:\n            rot, do_flip, color_scale = get_aug_config()\n        else:\n            rot, do_flip, color_scale = 0, False, [1.0, 1.0, 1.0]\n\n        # 3. crop patch from img and perform data augmentation (flip, rot, color scale)\n        img_patch, trans = generate_patch_image(cvimg, bbox, do_flip, rot)\n        for i in range(img_channels):\n            img_patch[:, :, i] = np.clip(img_patch[:, :, i] * color_scale[i], 0, 255)\n\n        # 4. generate patch joint, area_ratio, and ground truth\n        # flip joints and apply Affine Transform on joints\n        if do_flip:\n            root_img[0] = img_width - root_img[0] - 1\n        root_img[0:2] = trans_point2d(root_img[0:2], trans)\n        root_vis *= (\n                        (root_img[0] >= 0) & \\\n                        (root_img[0] < cfg.input_shape[1]) & \\\n                        (root_img[1] >= 0) & \\\n                        (root_img[1] < cfg.input_shape[0])\n                        )\n        \n        # change coordinates to output space\n        root_img[0] = root_img[0] / cfg.input_shape[1] * cfg.output_shape[1]\n        root_img[1] = root_img[1] / cfg.input_shape[0] * cfg.output_shape[0]\n        \n        if self.is_train:\n            img_patch = self.transform(img_patch)\n            k_value = np.array([math.sqrt(cfg.bbox_real[0]*cfg.bbox_real[1]*f[0]*f[1]/(area))]).astype(np.float32)\n            root_img = root_img.astype(np.float32)\n            root_vis = root_vis.astype(np.float32)\n            joints_have_depth = np.array([joints_have_depth]).astype(np.float32)\n\n            return img_patch, k_value, root_img, root_vis, joints_have_depth\n        else:\n            img_patch = self.transform(img_patch)\n            k_value = np.array([math.sqrt(cfg.bbox_real[0]*cfg.bbox_real[1]*f[0]*f[1]/(area))]).astype(np.float32)\n          \n            return img_patch, k_value\n\n    def __len__(self):\n        return len(self.db)\n\n# helper functions\ndef get_aug_config():\n   \n    rot_factor = 30\n    color_factor = 0.2\n    \n    rot = np.clip(np.random.randn(), -2.0,\n                  2.0) * rot_factor if random.random() <= 0.6 else 0\n    do_flip = random.random() <= 0.5\n    c_up = 1.0 + color_factor\n    c_low = 1.0 - color_factor\n    color_scale = [random.uniform(c_low, c_up), random.uniform(c_low, c_up), random.uniform(c_low, c_up)]\n\n    return rot, do_flip, color_scale\n\ndef generate_patch_image(cvimg, bbox, do_flip, rot):\n    img = cvimg.copy()\n    img_height, img_width, img_channels = img.shape\n\n    bb_c_x = float(bbox[0] + 0.5*bbox[2])\n    bb_c_y = float(bbox[1] + 0.5*bbox[3])\n    bb_width = float(bbox[2])\n    bb_height = float(bbox[3])\n\n    if do_flip:\n        img = img[:, ::-1, :]\n        bb_c_x = img_width - bb_c_x - 1\n    \n    trans = gen_trans_from_patch_cv(bb_c_x, bb_c_y, bb_width, bb_height, cfg.input_shape[1], cfg.input_shape[0], rot, inv=False)\n    img_patch = cv2.warpAffine(img, trans, (int(cfg.input_shape[1]), int(cfg.input_shape[0])), flags=cv2.INTER_LINEAR)\n\n    img_patch = img_patch[:,:,::-1].copy()\n    img_patch = img_patch.astype(np.float32)\n\n    return img_patch, trans\n\ndef rotate_2d(pt_2d, rot_rad):\n    x = pt_2d[0]\n    y = pt_2d[1]\n    sn, cs = np.sin(rot_rad), np.cos(rot_rad)\n    xx = x * cs - y * sn\n    yy = x * sn + y * cs\n    return np.array([xx, yy], dtype=np.float32)\n\ndef gen_trans_from_patch_cv(c_x, c_y, src_width, src_height, dst_width, dst_height, rot, inv=False):\n    src_w = src_width\n    src_h = src_height\n    src_center = np.array([c_x, c_y], dtype=np.float32)\n    # augment rotation\n    rot_rad = np.pi * rot / 180\n    src_downdir = rotate_2d(np.array([0, src_h * 0.5], dtype=np.float32), rot_rad)\n    src_rightdir = rotate_2d(np.array([src_w * 0.5, 0], dtype=np.float32), rot_rad)\n\n    dst_w = dst_width\n    dst_h = dst_height\n    dst_center = np.array([dst_w * 0.5, dst_h * 0.5], dtype=np.float32)\n    dst_downdir = np.array([0, dst_h * 0.5], dtype=np.float32)\n    dst_rightdir = np.array([dst_w * 0.5, 0], dtype=np.float32)\n\n    src = np.zeros((3, 2), dtype=np.float32)\n    src[0, :] = src_center\n    src[1, :] = src_center + src_downdir\n    src[2, :] = src_center + src_rightdir\n\n    dst = np.zeros((3, 2), dtype=np.float32)\n    dst[0, :] = dst_center\n    dst[1, :] = dst_center + dst_downdir\n    dst[2, :] = dst_center + dst_rightdir\n\n    if inv:\n        trans = cv2.getAffineTransform(np.float32(dst), np.float32(src))\n    else:\n        trans = cv2.getAffineTransform(np.float32(src), np.float32(dst))\n\n    return trans\n\ndef trans_point2d(pt_2d, trans):\n    src_pt = np.array([pt_2d[0], pt_2d[1], 1.]).T\n    dst_pt = np.dot(trans, src_pt)\n    return dst_pt[0:2]\n\n'"
main/config.py,0,"b'import os\nimport os.path as osp\nimport sys\nimport numpy as np\n\nclass Config:\n    \n    ## dataset\n    # training set\n    # 3D: Human36M, MuCo, PW3D\n    # 2D: MSCOCO, MPII \n    # Note that list must consists of one 3D dataset (first element of the list) + several 2D datasets\n    trainset = [\'Human36M\', \'MPII\'] \n\n    # testing set\n    # Human36M, MuPoTS, MSCOCO, PW3D\n    testset = \'PW3D\'\n\n    ## directory\n    cur_dir = osp.dirname(os.path.abspath(__file__))\n    root_dir = osp.join(cur_dir, \'..\')\n    data_dir = osp.join(root_dir, \'data\')\n    output_dir = osp.join(root_dir, \'output\')\n    model_dir = osp.join(output_dir, \'model_dump\')\n    vis_dir = osp.join(output_dir, \'vis\')\n    log_dir = osp.join(output_dir, \'log\')\n    result_dir = osp.join(output_dir, \'result\')\n \n    ## model setting\n    resnet_type = 50 # 50, 101, 152\n    \n    ## input, output\n    input_shape = (256, 256)\n    output_shape = (input_shape[0]//4, input_shape[1]//4)\n    pixel_mean = (0.485, 0.456, 0.406)\n    pixel_std = (0.229, 0.224, 0.225)\n    bbox_real = (2000, 2000) # Human36M, MuCo, MuPoTS: (2000, 2000), PW3D: (2, 2)\n\n    ## training config\n    lr_dec_epoch = [17]\n    end_epoch = 20\n    lr = 1e-3\n    lr_dec_factor = 10\n    batch_size = 32\n\n    ## testing config\n    test_batch_size = 32\n    use_gt_bbox = True\n\n    ## others\n    num_thread = 8\n    gpu_ids = \'0\'\n    num_gpus = 1\n    continue_train = False\n\n    def set_args(self, gpu_ids, continue_train=False):\n        self.gpu_ids = gpu_ids\n        self.num_gpus = len(self.gpu_ids.split(\',\'))\n        self.continue_train = continue_train\n        os.environ[""CUDA_VISIBLE_DEVICES""] = self.gpu_ids\n        print(\'>>> Using GPU: {}\'.format(self.gpu_ids))\n\ncfg = Config()\n\nsys.path.insert(0, osp.join(cfg.root_dir, \'common\'))\nfrom utils.dir_utils import add_pypath, make_folder\nadd_pypath(osp.join(cfg.data_dir))\nfor i in range(len(cfg.trainset)):\n    add_pypath(osp.join(cfg.data_dir, cfg.trainset[i]))\nadd_pypath(osp.join(cfg.data_dir, cfg.testset))\nmake_folder(cfg.model_dir)\nmake_folder(cfg.vis_dir)\nmake_folder(cfg.log_dir)\nmake_folder(cfg.result_dir)\n\n'"
main/model.py,8,"b""import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom nets.resnet import ResNetBackbone\nfrom config import cfg\n\nclass RootNet(nn.Module):\n\n    def __init__(self):\n        self.inplanes = 2048\n        self.outplanes = 256\n\n        super(RootNet, self).__init__()\n       \tself.deconv_layers = self._make_deconv_layer(3)\n        self.xy_layer = nn.Conv2d(\n            in_channels=self.outplanes,\n            out_channels=1,\n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n        self.depth_layer = nn.Conv2d(\n            in_channels=self.inplanes,\n            out_channels=1, \n            kernel_size=1,\n            stride=1,\n            padding=0\n        )\n\n    def _make_deconv_layer(self, num_layers):\n        layers = []\n        inplanes = self.inplanes\n        outplanes = self.outplanes\n        for i in range(num_layers):\n            layers.append(\n                nn.ConvTranspose2d(\n                    in_channels=inplanes,\n                    out_channels=outplanes,\n                    kernel_size=4,\n                    stride=2,\n                    padding=1,\n                    output_padding=0,\n                    bias=False))\n            layers.append(nn.BatchNorm2d(outplanes))\n            layers.append(nn.ReLU(inplace=True))\n            inplanes = outplanes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x, k_value):\n        # x,y\n        xy = self.deconv_layers(x)\n        xy = self.xy_layer(xy)\n        xy = xy.view(-1,1,cfg.output_shape[0]*cfg.output_shape[1])\n        xy = F.softmax(xy,2)\n        xy = xy.view(-1,1,cfg.output_shape[0],cfg.output_shape[1])\n\n        hm_x = xy.sum(dim=(2))\n        hm_y = xy.sum(dim=(3))\n\n        coord_x = hm_x * torch.cuda.comm.broadcast(torch.arange(1,cfg.output_shape[1]+1).type(torch.cuda.FloatTensor), devices=[hm_x.device.index])[0]\n        coord_y = hm_y * torch.cuda.comm.broadcast(torch.arange(1,cfg.output_shape[0]+1).type(torch.cuda.FloatTensor), devices=[hm_y.device.index])[0]\n        \n        coord_x = coord_x.sum(dim=2) - 1\n        coord_y = coord_y.sum(dim=2) - 1\n\n        # z\n        img_feat = torch.mean(x.view(x.size(0), x.size(1), x.size(2)*x.size(3)), dim=2) # global average pooling\n        img_feat = torch.unsqueeze(img_feat,2); img_feat = torch.unsqueeze(img_feat,3);\n        gamma = self.depth_layer(img_feat)\n        gamma = gamma.view(-1,1)\n        depth = gamma * k_value.view(-1,1)\n\n        coord = torch.cat((coord_x, coord_y, depth), dim=1)\n        return coord\n\n    def init_weights(self):\n        for name, m in self.deconv_layers.named_modules():\n            if isinstance(m, nn.ConvTranspose2d):\n                nn.init.normal_(m.weight, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n        for m in self.xy_layer.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n        for m in self.depth_layer.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.normal_(m.weight, std=0.001)\n                nn.init.constant_(m.bias, 0)\n\nclass ResPoseNet(nn.Module):\n    def __init__(self, backbone, root):\n        super(ResPoseNet, self).__init__()\n        self.backbone = backbone\n        self.root = root\n\n    def forward(self, input_img, k_value, target=None):\n        fm = self.backbone(input_img)\n        coord = self.root(fm, k_value)\n\n        if target is None:\n            return coord\n        else:\n            target_coord = target['coord']\n            target_vis = target['vis']\n            target_have_depth = target['have_depth']\n            \n            ## coordrinate loss\n            loss_coord = torch.abs(coord - target_coord) * target_vis\n            loss_coord = (loss_coord[:,0] + loss_coord[:,1] + loss_coord[:,2] * target_have_depth.view(-1))/3.\n            return loss_coord\n\ndef get_pose_net(cfg, is_train):\n    \n    backbone = ResNetBackbone(cfg.resnet_type)\n    root_net = RootNet()\n    if is_train:\n        backbone.init_weights()\n        root_net.init_weights()\n\n    model = ResPoseNet(backbone, root_net)\n    return model\n\n\n"""
main/test.py,2,"b'import argparse\nfrom tqdm import tqdm\nimport numpy as np\nimport cv2\nfrom config import cfg\nimport torch\nfrom base import Tester\nfrom utils.vis import vis_keypoints\nimport torch.backends.cudnn as cudnn\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', type=str, dest=\'gpu_ids\')\n    parser.add_argument(\'--test_epoch\', type=str, dest=\'test_epoch\')\n    args = parser.parse_args()\n\n    # test gpus\n    if not args.gpu_ids:\n        assert 0, print(""Please set proper gpu ids"")\n\n    if \'-\' in args.gpu_ids:\n        gpus = args.gpu_ids.split(\'-\')\n        gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\n        gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\n        args.gpu_ids = \',\'.join(map(lambda x: str(x), list(range(*gpus))))\n    \n    assert args.test_epoch, \'Test epoch is required.\'\n    return args\n\ndef main():\n\n    args = parse_args()\n    cfg.set_args(args.gpu_ids)\n    cudnn.fastest = True\n    cudnn.benchmark = True\n\n    tester = Tester(args.test_epoch)\n    tester._make_batch_generator()\n    tester._make_model()\n\n    preds = []\n    with torch.no_grad():\n        for itr, (input_img, cam_param) in enumerate(tqdm(tester.batch_generator)):\n            \n            coord_out = tester.model(input_img, cam_param)\n            coord_out = coord_out.cpu().numpy()\n            preds.append(coord_out)\n            \n    # evaluate\n    preds = np.concatenate(preds, axis=0)\n    tester._evaluate(preds, cfg.result_dir)    \n\nif __name__ == ""__main__"":\n    main()\n'"
main/train.py,9,"b'import argparse\nfrom config import cfg\nimport torch\nfrom base import Trainer\nimport torch.backends.cudnn as cudnn\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--gpu\', type=str, dest=\'gpu_ids\')\n    parser.add_argument(\'--continue\', dest=\'continue_train\', action=\'store_true\')\n    args = parser.parse_args()\n\n    if not args.gpu_ids:\n        assert 0, print(""Please set proper gpu ids"")\n\n    if \'-\' in args.gpu_ids:\n        gpus = args.gpu_ids.split(\'-\')\n        gpus[0] = 0 if not gpus[0].isdigit() else int(gpus[0])\n        gpus[1] = len(mem_info()) if not gpus[1].isdigit() else int(gpus[1]) + 1\n        args.gpu_ids = \',\'.join(map(lambda x: str(x), list(range(*gpus))))\n\n    return args\n\ndef main():\n    \n    # argument parse and create log\n    args = parse_args()\n    cfg.set_args(args.gpu_ids, args.continue_train)\n    cudnn.fastest = True\n    cudnn.benchmark = True\n\n    trainer = Trainer()\n    trainer._make_batch_generator()\n    trainer._make_model()\n\n    # train\n    for epoch in range(trainer.start_epoch, cfg.end_epoch):\n        \n        trainer.set_lr(epoch)\n        trainer.tot_timer.tic()\n        trainer.read_timer.tic()\n\n        for itr in range(trainer.itr_per_epoch):\n            \n            input_img_list, k_value_list, root_img_list, root_vis_list, joints_have_depth_list = [], [], [], [], []\n            for i in range(len(cfg.trainset)):\n                try:\n                    input_img, k_value, root_img, root_vis, joints_have_depth = next(trainer.iterator[i])\n                except StopIteration:\n                    trainer.iterator[i] = iter(trainer.batch_generator[i])\n                    input_img, k_value, root_img, root_vis, joints_have_depth = next(trainer.iterator[i])\n\n                input_img_list.append(input_img)\n                k_value_list.append(k_value)\n                root_img_list.append(root_img)\n                root_vis_list.append(root_vis)\n                joints_have_depth_list.append(joints_have_depth)\n            \n            # aggregate items from different datasets into one single batch\n            input_img = torch.cat(input_img_list,dim=0)\n            k_value = torch.cat(k_value_list,dim=0)\n            root_img = torch.cat(root_img_list,dim=0)\n            root_vis = torch.cat(root_vis_list,dim=0)\n            joints_have_depth = torch.cat(joints_have_depth_list,dim=0)\n            \n            # shuffle items from different datasets\n            rand_idx = []\n            for i in range(len(cfg.trainset)):\n                rand_idx.append(torch.arange(i,input_img.shape[0],len(cfg.trainset)))\n            rand_idx = torch.cat(rand_idx,dim=0)\n            rand_idx = rand_idx[torch.randperm(input_img.shape[0])]\n            input_img = input_img[rand_idx]; k_value = k_value[rand_idx]; root_img = root_img[rand_idx]; root_vis = root_vis[rand_idx]; joints_have_depth = joints_have_depth[rand_idx];\n            target = {\'coord\': root_img, \'vis\': root_vis, \'have_depth\': joints_have_depth}\n\n            trainer.read_timer.toc()\n            trainer.gpu_timer.tic()\n\n            trainer.optimizer.zero_grad()\n            \n            # forward\n            loss_coord = trainer.model(input_img, k_value, target)\n            loss_coord = loss_coord.mean();\n\n            # backward\n            loss = loss_coord\n\n            loss.backward()\n            trainer.optimizer.step()\n            \n            trainer.gpu_timer.toc()\n\n            screen = [\n                \'Epoch %d/%d itr %d/%d:\' % (epoch, cfg.end_epoch, itr, trainer.itr_per_epoch),\n                \'lr: %g\' % (trainer.get_lr()),\n                \'speed: %.2f(%.2fs r%.2f)s/itr\' % (\n                    trainer.tot_timer.average_time, trainer.gpu_timer.average_time, trainer.read_timer.average_time),\n                \'%.2fh/epoch\' % (trainer.tot_timer.average_time / 3600. * trainer.itr_per_epoch),\n                \'%s: %.4f\' % (\'loss_coord\', loss_coord.detach()),\n                ]\n            trainer.logger.info(\' \'.join(screen))\n\n            trainer.tot_timer.toc()\n            trainer.tot_timer.tic()\n            trainer.read_timer.tic()\n\n        trainer.save_model({\n            \'epoch\': epoch,\n            \'network\': trainer.model.state_dict(),\n            \'optimizer\': trainer.optimizer.state_dict(),\n        }, epoch)\n        \n\nif __name__ == ""__main__"":\n    main()\n'"
common/nets/resnet.py,2,"b'import torch\nimport torch.nn as nn\nfrom torchvision.models.resnet import BasicBlock, Bottleneck\nfrom torchvision.models.resnet import model_urls\n\nclass ResNetBackbone(nn.Module):\n\n    def __init__(self, resnet_type):\n\t\n        resnet_spec = {18: (BasicBlock, [2, 2, 2, 2], [64, 64, 128, 256, 512], \'resnet18\'),\n\t\t       34: (BasicBlock, [3, 4, 6, 3], [64, 64, 128, 256, 512], \'resnet34\'),\n\t\t       50: (Bottleneck, [3, 4, 6, 3], [64, 256, 512, 1024, 2048], \'resnet50\'),\n\t\t       101: (Bottleneck, [3, 4, 23, 3], [64, 256, 512, 1024, 2048], \'resnet101\'),\n\t\t       152: (Bottleneck, [3, 8, 36, 3], [64, 256, 512, 1024, 2048], \'resnet152\')}\n        block, layers, channels, name = resnet_spec[resnet_type]\n        \n        self.name = name\n        self.inplanes = 64\n        super(ResNetBackbone, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                # nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n                nn.init.normal_(m.weight, mean=0, std=0.001)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        return x\n\n    def init_weights(self):\n        org_resnet = torch.utils.model_zoo.load_url(model_urls[self.name])\n        # drop orginal resnet fc layer, add \'None\' in case of no fc layer, that will raise error\n        org_resnet.pop(\'fc.weight\', None)\n        org_resnet.pop(\'fc.bias\', None)\n        self.load_state_dict(org_resnet)\n        print(""Initialize resnet from model zoo"")\n\n\n'"
common/utils/__init__.py,0,b''
common/utils/dir_utils.py,0,"b'import os\nimport sys\n\ndef make_folder(folder_name):\n    if not os.path.exists(folder_name):\n        os.makedirs(folder_name)\n\ndef add_pypath(path):\n    if path not in sys.path:\n        sys.path.insert(0, path)\n\n'"
common/utils/pose_utils.py,2,"b'import torch\nimport numpy as np\nfrom config import cfg\nimport copy\n\ndef cam2pixel(cam_coord, f, c):\n    x = cam_coord[:, 0] / (cam_coord[:, 2] + 1e-8) * f[0] + c[0]\n    y = cam_coord[:, 1] / (cam_coord[:, 2] + 1e-8) * f[1] + c[1]\n    z = cam_coord[:, 2]\n    img_coord = np.concatenate((x[:,None], y[:,None], z[:,None]),1)\n    return img_coord\n\ndef pixel2cam(pixel_coord, f, c):\n    x = (pixel_coord[:, 0] - c[0]) / f[0] * pixel_coord[:, 2]\n    y = (pixel_coord[:, 1] - c[1]) / f[1] * pixel_coord[:, 2]\n    z = pixel_coord[:, 2]\n    cam_coord = np.concatenate((x[:,None], y[:,None], z[:,None]),1)\n    return cam_coord\n\ndef world2cam(world_coord, R, t):\n    cam_coord = np.dot(R, world_coord.transpose(1,0)).transpose(1,0) + t.reshape(1,3)\n    return cam_coord\n\ndef get_bbox(joint_img):\n    # bbox extract from keypoint coordinates\n    bbox = np.zeros((4))\n    xmin = np.min(joint_img[:,0])\n    ymin = np.min(joint_img[:,1])\n    xmax = np.max(joint_img[:,0])\n    ymax = np.max(joint_img[:,1])\n    width = xmax - xmin - 1\n    height = ymax - ymin - 1\n    \n    bbox[0] = (xmin + xmax)/2. - width/2*1.2\n    bbox[1] = (ymin + ymax)/2. - height/2*1.2\n    bbox[2] = width*1.2\n    bbox[3] = height*1.2\n\n    return bbox\n\ndef process_bbox(bbox, width, height):\n    # sanitize bboxes\n    x, y, w, h = bbox\n    x1 = np.max((0, x))\n    y1 = np.max((0, y))\n    x2 = np.min((width - 1, x1 + np.max((0, w - 1))))\n    y2 = np.min((height - 1, y1 + np.max((0, h - 1))))\n    if w*h > 0 and x2 >= x1 and y2 >= y1:\n        bbox = np.array([x1, y1, x2-x1, y2-y1])\n    else:\n        return None\n\n    # aspect ratio preserving bbox\n    w = bbox[2]\n    h = bbox[3]\n    c_x = bbox[0] + w/2.\n    c_y = bbox[1] + h/2.\n    aspect_ratio = cfg.input_shape[1]/cfg.input_shape[0]\n    if w > aspect_ratio * h:\n        h = w / aspect_ratio\n    elif w < aspect_ratio * h:\n        w = h * aspect_ratio\n    bbox[2] = w*1.25\n    bbox[3] = h*1.25\n    bbox[0] = c_x - bbox[2]/2.\n    bbox[1] = c_y - bbox[3]/2.\n    return bbox\n\ndef multi_meshgrid(*args):\n    """"""\n    Creates a meshgrid from possibly many\n    elements (instead of only 2).\n    Returns a nd tensor with as many dimensions\n    as there are arguments\n    """"""\n    args = list(args)\n    template = [1 for _ in args]\n    for i in range(len(args)):\n        n = args[i].shape[0]\n        template_copy = template.copy()\n        template_copy[i] = n\n        args[i] = args[i].view(*template_copy)\n        # there will be some broadcast magic going on\n    return tuple(args)\n\n\ndef flip(tensor, dims):\n    if not isinstance(dims, (tuple, list)):\n        dims = [dims]\n    indices = [torch.arange(tensor.shape[dim] - 1, -1, -1,\n                            dtype=torch.int64) for dim in dims]\n    multi_indices = multi_meshgrid(*indices)\n    final_indices = [slice(i) for i in tensor.shape]\n    for i, dim in enumerate(dims):\n        final_indices[dim] = multi_indices[i]\n    flipped = tensor[final_indices]\n    assert flipped.device == tensor.device\n    assert flipped.requires_grad == tensor.requires_grad\n    return flipped\n\n'"
common/utils/vis.py,0,"b""import os\nimport cv2\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom config import cfg\n\ndef vis_keypoints(img, kps, kps_lines, kp_thresh=0.4, alpha=1):\n\n    # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n    cmap = plt.get_cmap('rainbow')\n    colors = [cmap(i) for i in np.linspace(0, 1, len(kps_lines) + 2)]\n    colors = [(c[2] * 255, c[1] * 255, c[0] * 255) for c in colors]\n\n    # Perform the drawing on a copy of the image, to allow for blending.\n    kp_mask = np.copy(img)\n\n    # Draw the keypoints.\n    for l in range(len(kps_lines)):\n        i1 = kps_lines[l][0]\n        i2 = kps_lines[l][1]\n        p1 = kps[0, i1].astype(np.int32), kps[1, i1].astype(np.int32)\n        p2 = kps[0, i2].astype(np.int32), kps[1, i2].astype(np.int32)\n        if kps[2, i1] > kp_thresh and kps[2, i2] > kp_thresh:\n            cv2.line(\n                kp_mask, p1, p2,\n                color=colors[l], thickness=2, lineType=cv2.LINE_AA)\n        if kps[2, i1] > kp_thresh:\n            cv2.circle(\n                kp_mask, p1,\n                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n        if kps[2, i2] > kp_thresh:\n            cv2.circle(\n                kp_mask, p2,\n                radius=3, color=colors[l], thickness=-1, lineType=cv2.LINE_AA)\n\n    # Blend the keypoints.\n    return cv2.addWeighted(img, 1.0 - alpha, kp_mask, alpha, 0)\n\ndef vis_3d_skeleton(kpt_3d, kpt_3d_vis, kps_lines, filename=None):\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='3d')\n\n    # Convert from plt 0-1 RGBA colors to 0-255 BGR colors for opencv.\n    cmap = plt.get_cmap('rainbow')\n    colors = [cmap(i) for i in np.linspace(0, 1, len(kps_lines) + 2)]\n    colors = [np.array((c[2], c[1], c[0])) for c in colors]\n\n    for l in range(len(kps_lines)):\n        i1 = kps_lines[l][0]\n        i2 = kps_lines[l][1]\n        x = np.array([kpt_3d[i1,0], kpt_3d[i2,0]])\n        y = np.array([kpt_3d[i1,1], kpt_3d[i2,1]])\n        z = np.array([kpt_3d[i1,2], kpt_3d[i2,2]])\n\n        if kpt_3d_vis[i1,0] > 0 and kpt_3d_vis[i2,0] > 0:\n            ax.plot(x, z, -y, c=colors[l], linewidth=2)\n        if kpt_3d_vis[i1,0] > 0:\n            ax.scatter(kpt_3d[i1,0], kpt_3d[i1,2], -kpt_3d[i1,1], c=colors[l], marker='o')\n        if kpt_3d_vis[i2,0] > 0:\n            ax.scatter(kpt_3d[i2,0], kpt_3d[i2,2], -kpt_3d[i2,1], c=colors[l], marker='o')\n\n    x_r = np.array([0, cfg.input_shape[1]], dtype=np.float32)\n    y_r = np.array([0, cfg.input_shape[0]], dtype=np.float32)\n    z_r = np.array([0, 1], dtype=np.float32)\n    \n    if filename is None:\n        ax.set_title('3D vis')\n    else:\n        ax.set_title(filename)\n\n    ax.set_xlabel('X Label')\n    ax.set_ylabel('Z Label')\n    ax.set_zlabel('Y Label')\n    #ax.set_xlim([0,cfg.input_shape[1]])\n    #ax.set_ylim([0,1])\n    #ax.set_zlim([-cfg.input_shape[0],0])\n    ax.legend()\n\n    plt.show()\n    cv2.waitKey(0)\n\n"""
data/Human36M/Human36M.py,0,"b'import os\nimport os.path as osp\nfrom pycocotools.coco import COCO\nimport numpy as np\nfrom config import cfg\nfrom utils.pose_utils import world2cam, cam2pixel, pixel2cam, process_bbox\nimport cv2\nimport random\nimport json\nimport math\n\nclass Human36M:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'Human36M\', \'images\')\n        self.annot_path = osp.join(\'..\', \'data\', \'Human36M\', \'annotations\')\n        self.human_bbox_dir = osp.join(\'..\', \'data\', \'Human36M\', \'bbox\', \'bbox_human36m_output.json\')\n        self.joint_num = 17\n        self.joints_name = (\'Pelvis\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Torso\', \'Neck\', \'Nose\', \'Head\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\')\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.joints_have_depth = True\n        self.protocol = 2\n        self.data = self.load_data()\n        self.action_name = [\'Directions\', \'Discussion\', \'Eating\', \'Greeting\', \'Phoning\', \'Posing\', \'Purchases\', \'Sitting\', \'SittingDown\', \'Smoking\', \'Photo\', \'Waiting\', \'Walking\', \'WalkDog\', \'WalkTogether\']\n\n    def get_subsampling_ratio(self):\n        if self.data_split == \'train\':\n            return 5\n        elif self.data_split == \'test\':\n            return 64\n        else:\n            assert 0, print(\'Unknown subset\')\n\n    def get_subject(self):\n        if self.data_split == \'train\':\n            if self.protocol == 1:\n                subject = [1,5,6,7,8,9]\n            elif self.protocol == 2:\n                subject = [1,5,6,7,8]\n        elif self.data_split == \'test\':\n            if self.protocol == 1:\n                subject = [11]\n            elif self.protocol == 2:\n                subject = [9,11]\n        else:\n            assert 0, print(""Unknown subset"")\n\n        return subject\n    \n    def load_data(self):\n        print(\'Load data of H36M Protocol \' + str(self.protocol))\n        subject_list = self.get_subject()\n        sampling_ratio = self.get_subsampling_ratio()\n\n        # aggregate annotations from each subject\n        db = COCO()\n        cameras = {}\n        joints = {}\n        for subject in subject_list:\n            # data load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_data.json\'),\'r\') as f:\n                annot = json.load(f)\n            if len(db.dataset) == 0:\n                for k,v in annot.items():\n                    db.dataset[k] = v\n            else:\n                for k,v in annot.items():\n                    db.dataset[k] += v\n            # camera load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_camera.json\'),\'r\') as f:\n                cameras[str(subject)] = json.load(f)\n            # joint coordinate load\n            with open(osp.join(self.annot_path, \'Human36M_subject\' + str(subject) + \'_joint_3d.json\'),\'r\') as f:\n                joints[str(subject)] = json.load(f)\n        db.createIndex()\n\n        if self.data_split == \'test\' and not cfg.use_gt_bbox:\n            print(""Get bounding box from "" + self.human_bbox_dir)\n            bbox_result = {}\n            with open(self.human_bbox_dir) as f:\n                annot = json.load(f)\n            for i in range(len(annot)):\n                bbox_result[str(annot[i][\'image_id\'])] = np.array(annot[i][\'bbox\'])\n        else:\n            print(""Get bounding box from groundtruth"")\n\n        data = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n            image_id = ann[\'image_id\']\n            img = db.loadImgs(image_id)[0]\n            img_path = osp.join(self.img_dir, img[\'file_name\'])\n            img_width, img_height = img[\'width\'], img[\'height\']\n\n            # check subject and frame_idx\n            subject = img[\'subject\']; frame_idx = img[\'frame_idx\'];\n            if subject not in subject_list:\n                continue\n            if frame_idx % sampling_ratio != 0:\n                continue\n\n            # camera parameter\n            cam_idx = img[\'cam_idx\']\n            cam_param = cameras[str(subject)][str(cam_idx)]\n            R,t,f,c = np.array(cam_param[\'R\'], dtype=np.float32), np.array(cam_param[\'t\'], dtype=np.float32), np.array(cam_param[\'f\'], dtype=np.float32), np.array(cam_param[\'c\'], dtype=np.float32)\n                \n            # project world coordinate to cam, image coordinate space\n            action_idx = img[\'action_idx\']; subaction_idx = img[\'subaction_idx\']; frame_idx = img[\'frame_idx\'];\n            root_world = np.array(joints[str(subject)][str(action_idx)][str(subaction_idx)][str(frame_idx)], dtype=np.float32)[self.root_idx]\n            root_cam = world2cam(root_world[None,:], R, t)[0]\n            root_img = cam2pixel(root_cam[None,:], f, c)[0]\n            joint_vis = np.ones((self.joint_num,1))\n            root_vis = np.array(ann[\'keypoints_vis\'])[self.root_idx,None]\n            \n            # bbox load\n            if self.data_split == \'test\' and not cfg.use_gt_bbox:\n                bbox = bbox_result[str(image_id)]\n            else:\n                bbox = np.array(ann[\'bbox\'])\n            bbox = process_bbox(bbox, img_width, img_height)\n            if bbox is None: continue\n            area = bbox[2]*bbox[3]\n            \n            data.append({\n                \'img_path\': img_path,\n                \'img_id\': image_id,\n                \'bbox\': bbox,\n                \'area\': area,\n                \'root_img\': root_img, # [org_img_x, org_img_y, depth]\n                \'root_cam\': root_cam,\n                \'root_vis\': root_vis,\n                \'f\': f,\n                \'c\': c\n            })\n\n        return data\n\n    def evaluate(self, preds, result_dir):\n        print(\'Evaluation start...\')\n        gts = self.data\n        assert len(gts) == len(preds)\n        sample_num = len(gts)\n \n        pred_save = []\n        error = np.zeros((sample_num, 1, 3)) # MRPE\n        error_action = [ [] for _ in range(len(self.action_name)) ] # MRPE for each action\n        for n in range(sample_num):\n            gt = gts[n]\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\']\n            gt_root = gt[\'root_cam\']\n            \n            # warp output to original image space\n            pred_root = preds[n]\n            pred_root[0] = pred_root[0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_root[1] = pred_root[1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n            \n            # back-project to camera coordinate space\n            pred_root = pixel2cam(pred_root[None,:], f, c)[0]\n\n            # prediction save\n            img_id = gt[\'img_id\']\n            pred_save.append({\'image_id\': img_id, \'bbox\': bbox.tolist(), \'root_cam\': pred_root.tolist()})\n\n            # error calculate\n            error[n] = (pred_root - gt_root)**2\n            img_name = gt[\'img_path\']\n            action_idx = int(img_name[img_name.find(\'act\')+4:img_name.find(\'act\')+6]) - 2\n            error_action[action_idx].append(error[n].copy())\n\n        # total error\n        tot_err = np.mean(np.sqrt(np.sum(error,axis=2)))\n        x_err = np.mean(np.sqrt(error[:,:,0]))\n        y_err = np.mean(np.sqrt(error[:,:,1]))\n        z_err = np.mean(np.sqrt(error[:,:,2]))\n        eval_summary = \'MRPE >> tot: %.2f, x: %.2f, y: %.2f, z: %.2f\\n\' % (tot_err, x_err, y_err, z_err)\n       \n        # error for each action\n        for i in range(len(error_action)):\n            err = np.array(error_action[i])\n            err = np.mean(np.power(np.sum(err,axis=2),0.5))\n            eval_summary += (self.action_name[i] + \': %.2f \' % err)\n        print(eval_summary)\n\n        output_path = osp.join(result_dir, \'bbox_root_human36m_output.json\')\n        with open(output_path, \'w\') as f:\n            json.dump(pred_save, f)\n        print(""Test result is saved at "" + output_path)\n\n\n'"
data/MPII/MPII.py,0,"b""import os\nimport os.path as osp\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom utils.pose_utils import process_bbox\nfrom config import cfg\n\nclass MPII:\n\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join('..', 'data', 'MPII')\n        self.train_annot_path = osp.join('..', 'data', 'MPII', 'annotations', 'train.json')\n        self.joint_num = 16\n        self.joints_name = ('R_Ankle', 'R_Knee', 'R_Hip', 'L_Hip', 'L_Knee', 'L_Ankle', 'Pelvis', 'Thorax', 'Neck', 'Head', 'R_Wrist', 'R_Elbow', 'R_Shoulder', 'L_Shoulder', 'L_Elbow', 'L_Wrist')\n        self.joints_have_depth = False\n        self.root_idx = self.joints_name.index('Pelvis')\n        self.data = self.load_data()\n\n    def load_data(self):\n        \n        if self.data_split == 'train':\n            db = COCO(self.train_annot_path)\n        else:\n            print('Unknown data subset')\n            assert 0\n\n        data = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n            img = db.loadImgs(ann['image_id'])[0]\n            width, height = img['width'], img['height']\n\n            if ann['num_keypoints'] == 0:\n                continue\n            \n            bbox = process_bbox(ann['bbox'], width, height)\n            if bbox is None: continue\n            area = bbox[2]*bbox[3]\n\n            # joints and vis\n            joint_img = np.array(ann['keypoints']).reshape(self.joint_num,3)\n            joint_vis = joint_img[:,2].copy().reshape(-1,1)\n            joint_img[:,2] = 0\n            root_img = joint_img[self.root_idx]\n            root_vis = joint_vis[self.root_idx]\n\n            imgname = db.imgs[ann['image_id']]['file_name']\n            img_path = osp.join(self.img_dir, imgname)\n            data.append({\n                'img_path': img_path,\n                'bbox': bbox,\n                'area': area,\n                'root_img': root_img, # [org_img_x, org_img_y, 0]\n                'root_vis': root_vis,\n                'f': np.array([1500, 1500]) # dummy value\n            })\n\n        return data\n"""
data/MSCOCO/MSCOCO.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom config import cfg\nfrom utils.pose_utils import pixel2cam, process_bbox\nimport json\n\nclass MSCOCO:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MSCOCO\', \'images\')\n        self.annot_path = osp.join(\'..\', \'data\', \'MSCOCO\', \'annotations\')\n        self.human_bbox_dir = osp.join(\'..\', \'data\', \'MSCOCO\', \'bbox_coco_output.json\')\n        self.joint_num = 19 # original: 17, but manually added \'Thorax\', \'Pelvis\'\n        self.joints_name = (\'Nose\', \'L_Eye\', \'R_Eye\', \'L_Ear\', \'R_Ear\', \'L_Shoulder\', \'R_Shoulder\', \'L_Elbow\', \'R_Elbow\', \'L_Wrist\', \'R_Wrist\', \'L_Hip\', \'R_Hip\', \'L_Knee\', \'R_Knee\', \'L_Ankle\', \'R_Ankle\', \'Thorax\', \'Pelvis\')\n        self.joints_have_depth = False\n\n        self.lshoulder_idx = self.joints_name.index(\'L_Shoulder\')\n        self.rshoulder_idx = self.joints_name.index(\'R_Shoulder\')\n        self.lhip_idx = self.joints_name.index(\'L_Hip\')\n        self.rhip_idx = self.joints_name.index(\'R_Hip\')\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.data = self.load_data()\n\n    def load_data(self):\n\n        if self.data_split == \'train\':\n            name = \'train2017\'\n        else:\n            name = \'val2017\'\n\n        db = COCO(osp.join(self.annot_path, \'person_keypoints_\' + name + \'.json\'))\n        data = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n            img = db.loadImgs(ann[\'image_id\'])[0]\n            width, height = img[\'width\'], img[\'height\']\n\n            if (ann[\'image_id\'] not in db.imgs) or ann[\'iscrowd\'] or (ann[\'num_keypoints\'] == 0):\n                continue\n            \n            bbox = process_bbox(ann[\'bbox\'], width, height)\n            if bbox is None: continue\n            area = bbox[2]*bbox[3]\n\n            # joints and vis\n            joint_img = np.array(ann[\'keypoints\']).reshape(-1,3)\n            # add Thorax\n            thorax = (joint_img[self.lshoulder_idx, :] + joint_img[self.rshoulder_idx, :]) * 0.5\n            thorax[2] = joint_img[self.lshoulder_idx,2] * joint_img[self.rshoulder_idx,2]\n            thorax = thorax.reshape((1, 3))\n            # add Pelvis\n            pelvis = (joint_img[self.lhip_idx, :] + joint_img[self.rhip_idx, :]) * 0.5\n            pelvis[2] = joint_img[self.lhip_idx,2] * joint_img[self.rhip_idx,2]\n            pelvis = pelvis.reshape((1, 3))\n\n            joint_img = np.concatenate((joint_img, thorax, pelvis), axis=0)\n\n            joint_vis = (joint_img[:,2].copy().reshape(-1,1) > 0)\n            joint_img[:,2] = 0\n\n            root_img = joint_img[self.root_idx]\n            root_vis = joint_vis[self.root_idx]\n\n            imgname = osp.join(name, img[\'file_name\'])\n            img_path = osp.join(self.img_dir, imgname)\n            data.append({\n                \'img_path\': img_path,\n                \'image_id\': ann[\'image_id\'],\n                \'bbox\': bbox,\n                \'area\': area,\n                \'root_img\': root_img, # [org_img_x, org_img_y, 0]\n                \'root_vis\': root_vis,\n                \'f\': np.array([1500, 1500]), # dummy value\n                \'c\': np.array([width/2, height/2]) # dummy value\n            })\n\n        return data\n\n    def evaluate(self, preds, result_dir):\n        \n        print(\'Evaluation start...\')\n        gts = self.data\n        sample_num = len(preds)\n        pred_save = []\n        for n in range(sample_num):\n            \n            gt = gts[n]\n            image_id = gt[\'image_id\']\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\'].tolist()\n            \n            # restore coordinates to original space\n            pred_root = preds[n].copy()\n            pred_root[0] = pred_root[0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_root[1] = pred_root[1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n\n            # back project to camera coordinate system\n            pred_root = pixel2cam(pred_root[None,:], f, c)[0]\n\n            pred_save.append({\'image_id\': image_id, \'root_cam\': pred_root.tolist(), \'bbox\': bbox})\n        \n        output_path = osp.join(result_dir, \'bbox_root_coco_output.json\')\n        with open(output_path, \'w\') as f:\n            json.dump(pred_save, f)\n        print(""Testing result is saved at "" + output_path)\n'"
data/MuCo/MuCo.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\nfrom utils.pose_utils import process_bbox\nfrom pycocotools.coco import COCO\nfrom config import cfg\nimport math\n\nclass MuCo:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MuCo\', \'data\')\n        self.train_annot_path = osp.join(\'..\', \'data\', \'MuCo\', \'data\', \'MuCo-3DHP.json\')\n        self.joint_num = 21\n        self.joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\', \'R_Hand\', \'L_Hand\', \'R_Toe\', \'L_Toe\')\n        self.min_depth = 1500\n        self.max_depth = 7500\n        self.joints_have_depth = True\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.data = self.load_data()\n\n    def load_data(self):\n\n        if self.data_split == \'train\':\n            db = COCO(self.train_annot_path)\n        else:\n            print(\'Unknown data subset\')\n            assert 0\n\n        data = []\n        for iid in db.imgs.keys():\n            img = db.imgs[iid]\n            img_id = img[""id""]\n            img_width, img_height = img[\'width\'], img[\'height\']\n            imgname = img[\'file_name\']\n            img_path = osp.join(self.img_dir, imgname)\n            f = img[""f""]\n            c = img[""c""]\n\n            # crop the closest person to the camera\n            ann_ids = db.getAnnIds(img_id)\n            anns = db.loadAnns(ann_ids)\n            \n            # exclude too close persons\n            root_depths = [ann[\'keypoints_cam\'][self.root_idx][2] for ann in anns]\n            closest_pid = root_depths.index(min(root_depths))\n            pid_list = [closest_pid]\n            for i in range(len(anns)):\n                if i == closest_pid:\n                    continue\n                picked = True\n                for j in range(len(anns)):\n                    if i == j:\n                        continue\n                    dist = (np.array(anns[i][\'keypoints_cam\'][self.root_idx]) - np.array(anns[j][\'keypoints_cam\'][self.root_idx])) ** 2\n                    dist_2d = math.sqrt(np.sum(dist[:2]))\n                    dist_3d = math.sqrt(np.sum(dist))\n                    if dist_2d < 500 or dist_3d < 500:\n                        picked = False\n                if picked:\n                    pid_list.append(i)\n            \n            for pid in pid_list:\n                joint_cam = np.array(anns[pid][\'keypoints_cam\'])\n                root_cam = joint_cam[self.root_idx]\n\n                if root_cam[2] < self.min_depth or root_cam[2] > self.max_depth:\n                    continue\n                \n                joint_img = np.array(anns[pid][\'keypoints_img\'])\n                joint_img = np.concatenate([joint_img, joint_cam[:,2:]],1)\n                root_img = joint_img[self.root_idx]\n                \n                joint_vis = np.array(anns[pid][\'keypoints_vis\'])\n                root_vis = joint_vis[self.root_idx,None]\n                bbox = process_bbox(anns[pid][\'bbox\'], img_width, img_height)\n                if bbox is None: continue\n                area = bbox[2]*bbox[3]\n\n                data.append({\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'area\': area,\n                    \'root_img\': root_img, # [org_img_x, org_img_y, depth]\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'root_vis\': root_vis,\n                    \'f\': f,\n                    \'c\': c\n                })\n        return data\n\n\n'"
data/MuPoTS/MuPoTS.py,0,"b'import os\nimport os.path as osp\nimport scipy.io as sio\nimport numpy as np\nfrom pycocotools.coco import COCO\nfrom config import cfg\nimport json\nimport cv2\nimport random\nimport math\nfrom utils.pose_utils import pixel2cam, process_bbox\nfrom sklearn.metrics import average_precision_score\nfrom MuPoTS_eval import calculate_score\n\nclass MuPoTS:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.img_dir = osp.join(\'..\', \'data\', \'MuPoTS\', \'data\', \'MultiPersonTestSet\')\n        self.annot_path = osp.join(\'..\', \'data\', \'MuPoTS\', \'data\', \'MuPoTS-3D.json\')\n        self.human_bbox_dir = osp.join(\'..\', \'data\', \'MuPoTS\', \'bbox\', \'bbox_mupots_output.json\')\n        self.joint_num = 21 # MuCo-3DHP\n        self.joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\', \'R_Hand\', \'L_Hand\', \'R_Toe\', \'L_Toe\') # MuCo-3DHP\n        self.original_joint_num = 17 # MuPoTS\n        self.original_joints_name = (\'Head_top\', \'Thorax\', \'R_Shoulder\', \'R_Elbow\', \'R_Wrist\', \'L_Shoulder\', \'L_Elbow\', \'L_Wrist\', \'R_Hip\', \'R_Knee\', \'R_Ankle\', \'L_Hip\', \'L_Knee\', \'L_Ankle\', \'Pelvis\', \'Spine\', \'Head\') # MuPoTS\n\n        self.joints_have_depth = True\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.data = self.load_data()\n\n    def load_data(self):\n        \n        if self.data_split != \'test\':\n            print(\'Unknown data subset\')\n            assert 0\n        \n        data = []\n        db = COCO(self.annot_path)\n        if cfg.use_gt_bbox:\n            print(""Get bounding box from groundtruth"")\n\n            for aid in db.anns.keys():\n                ann = db.anns[aid]\n                if ann[\'is_valid\'] == 0:\n                    continue\n\n                image_id = ann[\'image_id\']\n                img = db.loadImgs(image_id)[0]\n                img_path = osp.join(self.img_dir, img[\'file_name\'])\n                fx, fy, cx, cy = img[\'intrinsic\']\n                f = np.array([fx, fy]); c = np.array([cx, cy]);\n\n                joint_cam = np.array(ann[\'keypoints_cam\'])\n                joint_img = np.array(ann[\'keypoints_img\'])\n                joint_img = np.concatenate([joint_img, joint_cam[:,2:]],1)\n                joint_vis = np.array(ann[\'keypoints_vis\'])\n\n                root_cam = joint_cam[self.root_idx]\n                root_img = joint_img[self.root_idx]\n                root_vis = joint_vis[self.root_idx,None]\n \n                bbox = np.array(ann[\'bbox\'])\n                img_width, img_height = img[\'width\'], img[\'height\']\n                bbox = process_bbox(bbox, img_width, img_height)\n                if bbox is None: continue\n                area = bbox[2]*bbox[3]\n\n                data.append({\n                    \'image_id\': ann[\'image_id\'],\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'area\': area,\n                    \'root_img\': root_img, # [org_img_x, org_img_y, depth - root_depth]\n                    \'root_cam\': root_cam, # [X, Y, Z] in camera coordinate\n                    \'root_vis\': root_vis,\n                    \'f\': f,\n                    \'c\': c,\n                    \'score\': 1.0\n                })\n        else:\n            with open(self.human_bbox_dir) as f:\n                annot = json.load(f)\n            print(""Get bounding box from "" + self.human_bbox_dir)\n\n            for i in range(len(annot)):\n                image_id = annot[i][\'image_id\']\n                img = db.loadImgs(image_id)[0]\n                img_path = osp.join(self.img_dir, img[\'file_name\'])\n                fx, fy, cx, cy = img[\'intrinsic\']\n                f = np.array([fx, fy]); c = np.array([cx, cy]);\n\n                bbox = np.array(annot[i][\'bbox\']).reshape(4)\n                img_width, img_height = img[\'width\'], img[\'height\']\n                bbox = process_bbox(bbox, img_width, img_height)\n                if bbox is None: continue\n                area = bbox[2]*bbox[3]\n\n                data.append({\n                    \'image_id\': image_id,\n                    \'img_path\': img_path,\n                    \'bbox\': bbox,\n                    \'area\': area,\n                    \'root_img\': np.ones((3)), # dummy\n                    \'root_cam\': np.ones((3)), # dummy\n                    \'root_vis\': np.ones((1)), # dummy\n                    \'f\': f,\n                    \'c\': c,\n                    \'score\': annot[i][\'score\']\n                })\n        return data\n\n    def evaluate(self, preds, result_dir):\n        \n        print(\'Evaluation start...\')\n        pred_save = []\n\n        gts = self.data\n        sample_num = len(preds)\n        for n in range(sample_num):\n            \n            gt = gts[n]\n            image_id = gt[\'image_id\']\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\'].tolist()\n            score = gt[\'score\']\n            \n            # restore coordinates to original space\n            pred_root = preds[n].copy()\n            pred_root[0] = pred_root[0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_root[1] = pred_root[1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n\n            # back project to camera coordinate system\n            pred_root = pixel2cam(pred_root[None,:], f, c)[0]\n\n            pred_save.append({\'image_id\': image_id, \'root_cam\': pred_root.tolist(), \'bbox\': bbox, \'score\': score})\n        \n        output_path = osp.join(result_dir, \'bbox_root_mupots_output.json\')\n        with open(output_path, \'w\') as f:\n            json.dump(pred_save, f)\n        print(""Test result is saved at "" + output_path)\n\n        calculate_score(output_path, self.annot_path, 250)\n\n \n\n'"
data/MuPoTS/MuPoTS_eval.py,0,"b""import json\nfrom pycocotools.coco import COCO\nimport os\nimport os.path as osp\nimport numpy as np\nimport math\n\ndef calculate_score(output_path, annot_path, thr):\n\n    with open(output_path, 'r') as f:\n        output = json.load(f)\n\n    # AP measure\n    def return_score(pred):\n        return pred['score']\n    output.sort(reverse=True, key=return_score)\n\n    db = COCO(annot_path)\n    gt_num = len([k for k,v in db.anns.items() if v['is_valid'] == 1])\n    tp_acc = 0\n    fp_acc = 0\n    precision = []; recall = [];\n    is_matched = {}\n    for n in range(len(output)):\n        image_id = output[n]['image_id']\n        pred_root = output[n]['root_cam']\n        score = output[n]['score']\n\n        img = db.loadImgs(image_id)[0]\n        ann_ids = db.getAnnIds(image_id)\n        anns = db.loadAnns(ann_ids)\n        valid_frame_num = len([item for item in anns if item['is_valid'] == 1])\n        if valid_frame_num == 0:\n            continue\n\n        if str(image_id) not in is_matched:\n            is_matched[str(image_id)] = [0 for _ in range(len(anns))]\n        \n        min_dist = 9999\n        save_ann_id = -1\n        for ann_id,ann in enumerate(anns):\n            if ann['is_valid'] == 0:\n                continue\n            gt_root = np.array(ann['keypoints_cam'])\n            root_idx = 14\n            gt_root = gt_root[root_idx]\n\n            dist = math.sqrt(np.sum((pred_root - gt_root) ** 2))\n            if min_dist > dist:\n                min_dist = dist\n                save_ann_id = ann_id\n        \n        is_tp = False\n        if save_ann_id != -1 and min_dist < thr:\n            if is_matched[str(image_id)][save_ann_id] == 0:\n                is_tp = True\n                is_matched[str(image_id)][save_ann_id] = 1\n        \n        if is_tp:\n            tp_acc += 1\n        else:\n            fp_acc += 1\n            \n        precision.append(tp_acc/(tp_acc + fp_acc))\n        recall.append(tp_acc/gt_num)\n\n    AP = 0\n    for n in range(len(precision)-1):\n        AP += precision[n+1] * (recall[n+1] - recall[n])\n\n    print('AP_root: ' + str(AP))\n\nif __name__ == '__main__':\n    output_path = './bbox_root_mupots_output.json'\n    annot_path = osp.join('..', '..', 'data', 'MuPoTS', 'data', 'MuPoTS-3D.json')\n    thr = 250\n    calculate_score(output_path, annot_path, thr)\n\n"""
data/PW3D/PW3D.py,0,"b'import os\nimport os.path as osp\nimport numpy as np\nimport json\nfrom pycocotools.coco import COCO\nfrom config import cfg\nfrom utils.pose_utils import world2cam, cam2pixel, pixel2cam, process_bbox\n\n\nclass PW3D:\n    def __init__(self, data_split):\n        self.data_split = data_split\n        self.data_path = osp.join(\'..\', \'data\', \'PW3D\', \'data\')\n        self.joint_num = 24\n        self.joints_name = (\'Pelvis\', \'L_Hip\', \'R_Hip\', \'Torso\', \'L_Knee\', \'R_Knee\', \'Spine\', \'L_Ankle\', \'R_Ankle\', \'Chest\', \'L_Toe\', \'R_Toe\', \'Neck\', \'L_Thorax\', \'R_Thorax\', \'Head\', \'L_Shoulder\', \'R_Shoulder\', \'L_Elbow\', \'R_Elbow\', \'L_Wrist\', \'R_Wrist\', \'L_Hand\', \'R_Hand\', \'Nose\', \'L_Eye\', \'R_Eye\', \'L_Ear\', \'R_Ear\')\n        self.root_idx = self.joints_name.index(\'Pelvis\')\n        self.joints_have_depth = True\n        self.data = self.load_data()\n\n    def load_data(self):\n        db = COCO(osp.join(self.data_path, \'3DPW_\' + self.data_split + \'.json\'))\n\n        datalist = []\n        for aid in db.anns.keys():\n            ann = db.anns[aid]\n\n            image_id = ann[\'image_id\']\n            img = db.loadImgs(image_id)[0]\n            img_width, img_height = img[\'width\'], img[\'height\']\n            sequence_name = img[\'sequence\']\n            img_name = img[\'file_name\']\n            img_path = osp.join(self.data_path, \'imageFiles\', sequence_name, img_name)\n\n            cam_param = {k: np.array(v, dtype=np.float32) for k,v in img[\'cam_param\'].items()}\n            joint_cam = np.array(ann[\'joint_cam\'], dtype=np.float32).reshape(-1,3)\n            joint_img = cam2pixel(joint_cam, cam_param[\'focal\'], cam_param[\'princpt\'])\n            joint_valid = ((joint_img[:,0] >= 0) * (joint_img[:,0] < img_width) * (joint_img[:,1] >= 0) * (joint_img[:,1] < img_height)).astype(np.float32)\n\n            root_cam = joint_cam[self.root_idx]\n            root_img = joint_img[self.root_idx]\n            root_vis = joint_valid[self.root_idx]\n\n            bbox = process_bbox(ann[\'bbox\'], img_width, img_height)\n            if bbox is None: continue\n            area = bbox[2]*bbox[3]\n            \n            datalist.append({\n                \'img_path\': img_path,\n                \'img_id\': image_id,\n                \'ann_id\': aid,\n                \'bbox\': bbox,\n                \'area\': area,\n                \'root_img\': root_img,\n                \'root_cam\': root_cam,\n                \'root_vis\': root_vis,\n                \'f\': cam_param[\'focal\'],\n                \'c\': cam_param[\'princpt\']})\n           \n        return datalist\n\n    def evaluate(self, preds, result_dir):\n        print(\'Evaluation start...\')\n        gts = self.data\n        assert len(gts) == len(preds)\n        sample_num = len(gts)\n \n        pred_save = []\n        errors = np.zeros((sample_num,3))\n        for n in range(sample_num):\n            gt = gts[n]\n            f = gt[\'f\']\n            c = gt[\'c\']\n            bbox = gt[\'bbox\']\n\n            pred_root_coord = preds[n]\n            pred_root_coord[0] = pred_root_coord[0] / cfg.output_shape[1] * bbox[2] + bbox[0]\n            pred_root_coord[1] = pred_root_coord[1] / cfg.output_shape[0] * bbox[3] + bbox[1]\n            pred_root_coord = pixel2cam(pred_root_coord[None,:], f, c)\n\n            # error calculate\n            pred_root_coord = pred_root_coord.reshape(3)\n            gt_root_coord = gt[\'root_cam\'].reshape(3)\n            errors[n] = (pred_root_coord - gt_root_coord)**2\n\n            # prediction save\n            img_id = gt[\'img_id\']\n            ann_id = gt[\'ann_id\']\n            pred_root_coord = pred_root_coord.reshape(3)\n            pred_save.append({\'image_id\': img_id, \'ann_id\': ann_id, \'bbox\': bbox.tolist(), \'root_cam\': pred_root_coord.tolist()})\n       \n        err_x = np.mean(np.sqrt(errors[:,0]))\n        err_y = np.mean(np.sqrt(errors[:,1]))\n        err_z = np.mean(np.sqrt(errors[:,2]))\n        err_total = np.mean(np.sqrt(np.sum(errors,1)))\n        print(\'MRPE >> x: \' + str(err_x) + \' y: \' + str(err_y) + \' z: \' + str(err_z) + \' total: \' + str(err_total)) # error print (meter)\n\n        output_path = osp.join(result_dir, \'rootnet_pw3d_output.json\')\n        with open(output_path, \'w\') as f:\n            json.dump(pred_save, f)\n        print(""Test result is saved at "" + output_path)\n\n'"
