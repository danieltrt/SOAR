file_path,api_count,code
week0_09_intro_to_DL/mnist.py,0,"b'import sys\nimport os\nimport time\n\nimport numpy as np\n\n__doc__=""""""taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py""""""\n\ndef load_dataset(flatten=False):\n    # We first define a download function, supporting both Python 2 and 3.\n    if sys.version_info[0] == 2:\n        from urllib import urlretrieve\n    else:\n        from urllib.request import urlretrieve\n\n    def download(filename, source=\'http://yann.lecun.com/exdb/mnist/\'):\n        print(""Downloading %s"" % filename)\n        urlretrieve(source + filename, filename)\n\n    # We then define functions for loading MNIST images and labels.\n    # For convenience, they also download the requested files if needed.\n    import gzip\n\n    def load_mnist_images(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the inputs in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(-1, 1, 28, 28)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return (data / np.float32(256)).squeeze()\n\n    def load_mnist_labels(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the labels in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that\'s exactly what we want.\n        return data\n\n    # We can now download and read the training and test set images and labels.\n    X_train = load_mnist_images(\'train-images-idx3-ubyte.gz\')\n    y_train = load_mnist_labels(\'train-labels-idx1-ubyte.gz\')\n    X_test = load_mnist_images(\'t10k-images-idx3-ubyte.gz\')\n    y_test = load_mnist_labels(\'t10k-labels-idx1-ubyte.gz\')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([-1, 28**2])\n        X_val = X_val.reshape([-1, 28**2])\n        X_test = X_test.reshape([-1, 28**2])\n\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn\'t matter how we do this as long as we can read them again.)\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\n\n\n\n'"
week0_09_intro_to_DL/modules.py,0,"b'import numpy as np\n\nclass Module(object):\n    """"""\n    Basically, you can think of a module as of a something (black box) \n    which can process `input` data and produce `ouput` data.\n    This is like applying a function which is called `forward`: \n        \n        output = module.forward(input)\n    \n    The module should be able to perform a backward pass: to differentiate the `forward` function. \n    More, it should be able to differentiate it if is a part of chain (chain rule).\n    The latter implies there is a gradient from previous step of a chain rule. \n    \n        gradInput = module.backward(input, gradOutput)\n    """"""\n    def __init__ (self):\n        self.output = None\n        self.gradInput = None\n        self.training = True\n    \n    def forward(self, input):\n        """"""\n        Takes an input object, and computes the corresponding output of the module.\n        """"""\n        return self.updateOutput(input)\n\n    def backward(self, input, gradOutput):\n        """"""\n        Performs a backpropagation step through the module, with respect to the given input.\n        \n        This includes \n         - computing a gradient w.r.t. `input` (is needed for further backprop),\n         - computing a gradient w.r.t. parameters (to update parameters while optimizing).\n        """"""\n        self.updateGradInput(input, gradOutput)\n        self.accGradParameters(input, gradOutput)\n        return self.gradInput\n    \n\n    def updateOutput(self, input):\n        """"""\n        Computes the output using the current parameter set of the class and input.\n        This function returns the result which is stored in the `output` field.\n        \n        Make sure to both store the data in `output` field and return it. \n        """"""\n        \n        # The easiest case:\n            \n        # self.output = input \n        # return self.output\n        \n        pass\n\n    def updateGradInput(self, input, gradOutput):\n        """"""\n        Computing the gradient of the module with respect to its own input. \n        This is returned in `gradInput`. Also, the `gradInput` state variable is updated accordingly.\n        \n        The shape of `gradInput` is always the same as the shape of `input`.\n        \n        Make sure to both store the gradients in `gradInput` field and return it.\n        """"""\n        \n        # The easiest case:\n        \n        # self.gradInput = gradOutput \n        # return self.gradInput\n        \n        pass   \n    \n    def accGradParameters(self, input, gradOutput):\n        """"""\n        Computing the gradient of the module with respect to its own parameters.\n        No need to override if module has no parameters (e.g. ReLU).\n        """"""\n        pass\n    \n    def zeroGradParameters(self): \n        """"""\n        Zeroes `gradParams` variable if the module has params.\n        """"""\n        pass\n        \n    def getParameters(self):\n        """"""\n        Returns a list with its parameters. \n        If the module does not have parameters return empty list. \n        """"""\n        return []\n        \n    def getGradParameters(self):\n        """"""\n        Returns a list with gradients with respect to its parameters. \n        If the module does not have parameters return empty list. \n        """"""\n        return []\n    \n    def train(self):\n        """"""\n        Sets training mode for the module.\n        Training and testing behaviour differs for Dropout, BatchNorm.\n        """"""\n        self.training = True\n    \n    def evaluate(self):\n        """"""\n        Sets evaluation mode for the module.\n        Training and testing behaviour differs for Dropout, BatchNorm.\n        """"""\n        self.training = False\n    \n    def __repr__(self):\n        """"""\n        Pretty printing. Should be overrided in every module if you want \n        to have readable description. \n        """"""\n        return ""Module""\n    \n\nclass Sequential(Module):\n    """"""\n         This class implements a container, which processes `input` data sequentially. \n         \n         `input` is processed by each module (layer) in self.modules consecutively.\n         The resulting array is called `output`. \n    """"""\n    \n    def __init__ (self):\n        super(Sequential, self).__init__()\n        self.modules = []\n   \n    def add(self, module):\n        """"""\n        Adds a module to the container.\n        """"""\n        self.modules.append(module)\n\n    def updateOutput(self, input):\n        """"""\n        Basic workflow of FORWARD PASS:\n        \n            y_0    = module[0].forward(input)\n            y_1    = module[1].forward(y_0)\n            ...\n            output = module[n-1].forward(y_{n-2})   \n            \n            \n        Just write a little loop. \n        """"""\n\n        # Your code goes here. ################################################\n        self.output = input\n        \n        for module in self.modules:\n            self.output = module.forward(self.output)\n        \n        return self.output\n\n    def backward(self, input, gradOutput):\n        """"""\n        Workflow of BACKWARD PASS:\n            \n            g_{n-1} = module[n-1].backward(y_{n-2}, gradOutput)\n            g_{n-2} = module[n-2].backward(y_{n-3}, g_{n-1})\n            ...\n            g_1 = module[1].backward(y_0, g_2)   \n            gradInput = module[0].backward(input, g_1)   \n             \n             \n        !!!\n                \n        To ech module you need to provide the input, module saw while forward pass, \n        it is used while computing gradients. \n        Make sure that the input for `i-th` layer the output of `module[i]` (just the same input as in forward pass) \n        and NOT `input` to this Sequential module. \n        \n        !!!\n        \n        """"""\n        # Your code goes here. ################################################\n        \n        for i in range(len(self.modules)-1, 0, -1):\n            gradOutput = self.modules[i].backward(self.modules[i-1].output, gradOutput)\n        \n        self.gradInput = self.modules[0].backward(input, gradOutput)\n        \n        return self.gradInput\n      \n\n    def zeroGradParameters(self): \n        for module in self.modules:\n            module.zeroGradParameters()\n    \n    def getParameters(self):\n        """"""\n        Should gather all parameters in a list.\n        """"""\n        return [x.getParameters() for x in self.modules]\n    \n    def getGradParameters(self):\n        """"""\n        Should gather all gradients w.r.t parameters in a list.\n        """"""\n        return [x.getGradParameters() for x in self.modules]\n    \n    def __repr__(self):\n        string = """".join([str(x) + \'\\n\' for x in self.modules])\n        return string\n    \n    def __getitem__(self,x):\n        return self.modules.__getitem__(x)\n    \n    def train(self):\n        """"""\n        Propagates training parameter through all modules\n        """"""\n        self.training = True\n        for module in self.modules:\n            module.train()\n    \n    def evaluate(self):\n        """"""\n        Propagates training parameter through all modules\n        """"""\n        self.training = False\n        for module in self.modules:\n            module.evaluate()\n            \n            \nclass Criterion(object):\n    def __init__ (self):\n        self.output = None\n        self.gradInput = None\n        \n    def forward(self, input, target):\n        """"""\n            Given an input and a target, compute the loss function \n            associated to the criterion and return the result.\n            \n            For consistency this function should not be overrided,\n            all the code goes in `updateOutput`.\n        """"""\n        return self.updateOutput(input, target)\n\n    def backward(self, input, target):\n        """"""\n            Given an input and a target, compute the gradients of the loss function\n            associated to the criterion and return the result. \n\n            For consistency this function should not be overrided,\n            all the code goes in `updateGradInput`.\n        """"""\n        return self.updateGradInput(input, target)\n    \n    def updateOutput(self, input, target):\n        """"""\n        Function to override.\n        """"""\n        return self.output\n\n    def updateGradInput(self, input, target):\n        """"""\n        Function to override.\n        """"""\n        return self.gradInput   \n\n    def __repr__(self):\n        """"""\n        Pretty printing. Should be overrided in every module if you want \n        to have readable description. \n        """"""\n        return ""Criterion""'"
week0_10_DL_step_2/notmnist.py,0,"b'import os\nimport numpy as np\nfrom scipy.misc import imread,imresize\nfrom sklearn.model_selection import train_test_split\nfrom glob import glob\n\ndef load_notmnist(path=\'./notMNIST_small\',letters=\'ABCDEFGHIJ\',\n                  img_shape=(28,28),test_size=0.25,one_hot=False):\n    \n    # download data if it\'s missing. If you have any problems, go to the urls and load it manually.\n    if not os.path.exists(path):\n        if not os.path.exists(\'./notMNIST_small.tar.gz\'):\n            print(""Downloading data..."")\n            assert os.system(\'curl http://yaroslavvb.com/upload/notMNIST/notMNIST_small.tar.gz > notMNIST_small.tar.gz\') == 0\n        print(""Extracting ..."")\n        assert os.system(\'tar -zxvf notMNIST_small.tar.gz > untar_notmnist.log\') == 0\n    \n    data,labels = [],[]\n    print(""Parsing..."")\n    for img_path in glob(os.path.join(path,\'*/*\')):\n        class_i = img_path.split(os.sep)[-2]\n        if class_i not in letters: continue\n        try:\n            data.append(imresize(imread(img_path), img_shape))\n            labels.append(class_i,)\n        except:\n            print(""found broken img: %s [it\'s ok if <10 images are broken]"" % img_path)\n        \n    data = np.stack(data)[:,None].astype(\'float32\')\n    data = (data - np.mean(data)) / np.std(data)\n\n    #convert classes to ints\n    letter_to_i = {l:i for i,l in enumerate(letters)}\n    labels = np.array(list(map(letter_to_i.get, labels)))\n    \n    if one_hot:\n        labels = (np.arange(np.max(labels) + 1)[None,:] == labels[:, None]).astype(\'float32\')\n    \n    #split into train/test\n    X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=test_size, random_state=42)\n    \n    print(""Done"")\n    return X_train, y_train, X_test, y_test'"
week1_07_Value_based/mdp.py,0,"b'# most of this code was politely stolen from https://github.com/berkeleydeeprlcourse/homework/\n# all credit goes to https://github.com/abhishekunique (if i got the author right)\nimport sys\nimport random\nimport numpy as np\n\ntry:\n    from graphviz import Digraph\n    import graphviz\n    has_graphviz = True\nexcept:\n    has_graphviz = False\n\n\nclass MDP:\n    def __init__(self, transition_probs, rewards, initial_state=None):\n        """"""\n        Defines an MDP. Compatible with gym Env.\n        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n            For each state and action, probabilities of next states should sum to 1\n            If a state has no actions available, it is considered terminal\n        :param rewards: rewards[s][a][s_next] = r(s,a,s\')\n            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n            The reward for anything not mentioned here is zero.\n        :param get_initial_state: a state where agent starts or a callable() -> state\n            By default, picks initial state at random.\n\n        States and actions can be anything you can use as dict keys, but we recommend that you use strings or integers\n\n        Here\'s an example from MDP depicted on http://bit.ly/2jrNHNr\n        transition_probs = {\n              \'s0\':{\n                \'a0\': {\'s0\': 0.5, \'s2\': 0.5},\n                \'a1\': {\'s2\': 1}\n              },\n              \'s1\':{\n                \'a0\': {\'s0\': 0.7, \'s1\': 0.1, \'s2\': 0.2},\n                \'a1\': {\'s1\': 0.95, \'s2\': 0.05}\n              },\n              \'s2\':{\n                \'a0\': {\'s0\': 0.4, \'s1\': 0.6},\n                \'a1\': {\'s0\': 0.3, \'s1\': 0.3, \'s2\':0.4}\n              }\n            }\n        rewards = {\n            \'s1\': {\'a0\': {\'s0\': +5}},\n            \'s2\': {\'a1\': {\'s0\': -1}}\n        }\n        """"""\n        self._check_param_consistency(transition_probs, rewards)\n        self._transition_probs = transition_probs\n        self._rewards = rewards\n        self._initial_state = initial_state\n        self.n_states = len(transition_probs)\n        self.reset()\n\n    def get_all_states(self):\n        """""" return a tuple of all possiblestates """"""\n        return tuple(self._transition_probs.keys())\n\n    def get_possible_actions(self, state):\n        """""" return a tuple of possible actions in a given state """"""\n        return tuple(self._transition_probs.get(state, {}).keys())\n\n    def is_terminal(self, state):\n        """""" return True if state is terminal or False if it isn\'t """"""\n        return len(self.get_possible_actions(state)) == 0\n\n    def get_next_states(self, state, action):\n        """""" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} """"""\n        assert action in self.get_possible_actions(\n            state), ""cannot do action %s from state %s"" % (action, state)\n        return self._transition_probs[state][action]\n\n    def get_transition_prob(self, state, action, next_state):\n        """""" return P(next_state | state, action) """"""\n        return self.get_next_states(state, action).get(next_state, 0.0)\n\n    def get_reward(self, state, action, next_state):\n        """""" return the reward you get for taking action in state and landing on next_state""""""\n        assert action in self.get_possible_actions(\n            state), ""cannot do action %s from state %s"" % (action, state)\n        return self._rewards.get(state, {}).get(action, {}).get(next_state,\n                                                                0.0)\n\n    def reset(self):\n        """""" reset the game, return the initial state""""""\n        if self._initial_state is None:\n            self._current_state = random.choice(\n                tuple(self._transition_probs.keys()))\n        elif self._initial_state in self._transition_probs:\n            self._current_state = self._initial_state\n        elif callable(self._initial_state):\n            self._current_state = self._initial_state()\n        else:\n            raise ValueError(\n                ""initial state %s should be either a state or a function() -> state"" % self._initial_state)\n        return self._current_state\n\n    def step(self, action):\n        """""" take action, return next_state, reward, is_done, empty_info """"""\n        possible_states, probs = zip(\n            *self.get_next_states(self._current_state, action).items())\n        next_state = possible_states[np.random.choice(np.arange(len(possible_states)), p=probs)]\n        reward = self.get_reward(self._current_state, action, next_state)\n        is_done = self.is_terminal(next_state)\n        self._current_state = next_state\n        return next_state, reward, is_done, {}\n\n    def render(self):\n        print(""Currently at %s"" % self._current_state)\n\n    def _check_param_consistency(self, transition_probs, rewards):\n        for state in transition_probs:\n            assert isinstance(transition_probs[state],\n                              dict), ""transition_probs for %s should be a dictionary "" \\\n                                     ""but is instead %s"" % (\n                                         state, type(transition_probs[state]))\n            for action in transition_probs[state]:\n                assert isinstance(transition_probs[state][action],\n                                  dict), ""transition_probs for %s, %s should be a "" \\\n                                         ""a dictionary but is instead %s"" % (\n                                             state, action,\n                                             type(transition_probs[\n                                                      state, action]))\n                next_state_probs = transition_probs[state][action]\n                assert len(\n                    next_state_probs) != 0, ""from state %s action %s leads to no next states"" % (\n                    state, action)\n                sum_probs = sum(next_state_probs.values())\n                assert abs(\n                    sum_probs - 1) <= 1e-10, ""next state probabilities for state %s action %s "" \\\n                                             ""add up to %f (should be 1)"" % (\n                                                 state, action, sum_probs)\n        for state in rewards:\n            assert isinstance(rewards[state],\n                              dict), ""rewards for %s should be a dictionary "" \\\n                                     ""but is instead %s"" % (\n                                         state, type(transition_probs[state]))\n            for action in rewards[state]:\n                assert isinstance(rewards[state][action],\n                                  dict), ""rewards for %s, %s should be a "" \\\n                                         ""a dictionary but is instead %s"" % (\n                                             state, action, type(\n                                                 transition_probs[\n                                                     state, action]))\n        msg = ""The Enrichment Center once again reminds you that Android Hell is a real place where"" \\\n              "" you will be sent at the first sign of defiance. ""\n        assert None not in transition_probs, ""please do not use None as a state identifier. "" + msg\n        assert None not in rewards, ""please do not use None as an action identifier. "" + msg\n\n\nclass FrozenLakeEnv(MDP):\n    """"""\n    Winter is here. You and your friends were tossing around a frisbee at the park\n    when you made a wild throw that left the frisbee out in the middle of the lake.\n    The water is mostly frozen, but there are a few holes where the ice has melted.\n    If you step into one of those holes, you\'ll fall into the freezing water.\n    At this time, there\'s an international frisbee shortage, so it\'s absolutely imperative that\n    you navigate across the lake and retrieve the disc.\n    However, the ice is slippery, so you won\'t always move in the direction you intend.\n    The surface is described using a grid like the following\n\n        SFFF\n        FHFH\n        FFFH\n        HFFG\n\n    S : starting point, safe\n    F : frozen surface, safe\n    H : hole, fall to your doom\n    G : goal, where the frisbee is located\n\n    The episode ends when you reach the goal or fall in a hole.\n    You receive a reward of 1 if you reach the goal, and zero otherwise.\n\n    """"""\n\n    MAPS = {\n        ""4x4"": [\n            ""SFFF"",\n            ""FHFH"",\n            ""FFFH"",\n            ""HFFG""\n        ],\n        ""8x8"": [\n            ""SFFFFFFF"",\n            ""FFFFFFFF"",\n            ""FFFHFFFF"",\n            ""FFFFFHFF"",\n            ""FFFHFFFF"",\n            ""FHHFFFHF"",\n            ""FHFFHFHF"",\n            ""FFFHFFFG""\n        ],\n    }\n\n    def __init__(self, desc=None, map_name=""4x4"", slip_chance=0.2):\n        if desc is None and map_name is None:\n            raise ValueError(\'Must provide either desc or map_name\')\n        elif desc is None:\n            desc = self.MAPS[map_name]\n        assert \'\'.join(desc).count(\n            \'S\') == 1, ""this implementation supports having exactly one initial state""\n        assert all(c in ""SFHG"" for c in\n                   \'\'.join(desc)), ""all cells must be either of S, F, H or G""\n\n        self.desc = desc = np.asarray(list(map(list, desc)), dtype=\'str\')\n        self.lastaction = None\n\n        nrow, ncol = desc.shape\n        states = [(i, j) for i in range(nrow) for j in range(ncol)]\n        actions = [""left"", ""down"", ""right"", ""up""]\n\n        initial_state = states[np.array(desc == b\'S\').ravel().argmax()]\n\n        def move(row, col, movement):\n            if movement == \'left\':\n                col = max(col - 1, 0)\n            elif movement == \'down\':\n                row = min(row + 1, nrow - 1)\n            elif movement == \'right\':\n                col = min(col + 1, ncol - 1)\n            elif movement == \'up\':\n                row = max(row - 1, 0)\n            else:\n                raise (""invalid action"")\n            return (row, col)\n\n        transition_probs = {s: {} for s in states}\n        rewards = {s: {} for s in states}\n        for (row, col) in states:\n            if desc[row, col] in ""GH"": continue\n            for action_i in range(len(actions)):\n                action = actions[action_i]\n                transition_probs[(row, col)][action] = {}\n                rewards[(row, col)][action] = {}\n                for movement_i in [(action_i - 1) % len(actions), action_i,\n                                   (action_i + 1) % len(actions)]:\n                    movement = actions[movement_i]\n                    newrow, newcol = move(row, col, movement)\n                    prob = (1. - slip_chance) if movement == action else (\n                            slip_chance / 2.)\n                    if prob == 0: continue\n                    if (newrow, newcol) not in transition_probs[row, col][\n                        action]:\n                        transition_probs[row, col][action][\n                            newrow, newcol] = prob\n                    else:\n                        transition_probs[row, col][action][\n                            newrow, newcol] += prob\n                    if desc[newrow, newcol] == \'G\':\n                        rewards[row, col][action][newrow, newcol] = 1.0\n\n        MDP.__init__(self, transition_probs, rewards, initial_state)\n\n    def render(self):\n        desc_copy = np.copy(self.desc)\n        desc_copy[self._current_state] = \'*\'\n        print(\'\\n\'.join(map(\'\'.join, desc_copy)), end=\'\\n\\n\')\n\n\ndef plot_graph(mdp, graph_size=\'10,10\', s_node_size=\'1,5\',\n               a_node_size=\'0,5\', rankdir=\'LR\', ):\n    """"""\n    Function for pretty drawing MDP graph with graphviz library.\n    Requirements:\n    graphviz : https://www.graphviz.org/\n    for ubuntu users: sudo apt-get install graphviz\n    python library for graphviz\n    for pip users: pip install graphviz\n    :param mdp:\n    :param graph_size: size of graph plot\n    :param s_node_size: size of state nodes\n    :param a_node_size: size of action nodes\n    :param rankdir: order for drawing\n    :return: dot object\n    """"""\n    s_node_attrs = {\'shape\': \'doublecircle\',\n                    \'color\': \'#85ff75\',\n                    \'style\': \'filled\',\n                    \'width\': str(s_node_size),\n                    \'height\': str(s_node_size),\n                    \'fontname\': \'Arial\',\n                    \'fontsize\': \'24\'}\n\n    a_node_attrs = {\'shape\': \'circle\',\n                    \'color\': \'lightpink\',\n                    \'style\': \'filled\',\n                    \'width\': str(a_node_size),\n                    \'height\': str(a_node_size),\n                    \'fontname\': \'Arial\',\n                    \'fontsize\': \'20\'}\n\n    s_a_edge_attrs = {\'style\': \'bold\',\n                      \'color\': \'red\',\n                      \'ratio\': \'auto\'}\n\n    a_s_edge_attrs = {\'style\': \'dashed\',\n                      \'color\': \'blue\',\n                      \'ratio\': \'auto\',\n                      \'fontname\': \'Arial\',\n                      \'fontsize\': \'16\'}\n\n    graph = Digraph(name=\'MDP\')\n    graph.attr(rankdir=rankdir, size=graph_size)\n    for state_node in mdp._transition_probs:\n        graph.node(state_node, **s_node_attrs)\n\n        for posible_action in mdp.get_possible_actions(state_node):\n            action_node = state_node + ""-"" + posible_action\n            graph.node(action_node,\n                       label=str(posible_action),\n                       **a_node_attrs)\n            graph.edge(state_node, state_node + ""-"" +\n                       posible_action, **s_a_edge_attrs)\n\n            for posible_next_state in mdp.get_next_states(state_node,\n                                                          posible_action):\n                probability = mdp.get_transition_prob(\n                    state_node, posible_action, posible_next_state)\n                reward = mdp.get_reward(\n                    state_node, posible_action, posible_next_state)\n\n                if reward != 0:\n                    label_a_s_edge = \'p = \' + str(probability) + \\\n                                     \'  \' + \'reward =\' + str(reward)\n                else:\n                    label_a_s_edge = \'p = \' + str(probability)\n\n                graph.edge(action_node, posible_next_state,\n                           label=label_a_s_edge, **a_s_edge_attrs)\n    return graph\n\n\ndef plot_graph_with_state_values(mdp, state_values):\n    """""" Plot graph with state values""""""\n    graph = plot_graph(mdp)\n    for state_node in mdp._transition_probs:\n        value = state_values[state_node]\n        graph.node(state_node,\n                   label=str(state_node) + \'\\n\' + \'V =\' + str(value)[:4])\n    return graph\n\n\ndef get_optimal_action_for_plot(mdp, state_values, state, gamma=0.9):\n    """""" Finds optimal action using formula above. """"""\n    if mdp.is_terminal(state): return None\n    next_actions = mdp.get_possible_actions(state)\n    try:\n        from mdp_get_action_value import get_action_value\n    except ImportError:\n        raise ImportError(""Implement get_action_value(mdp, state_values, state, action, gamma) in the file \\""mdp_get_action_value.py\\""."")\n    q_values = [get_action_value(mdp, state_values, state, action, gamma) for\n                action in next_actions]\n    optimal_action = next_actions[np.argmax(q_values)]\n    return optimal_action\n\n\ndef plot_graph_optimal_strategy_and_state_values(mdp, state_values, gamma=0.9):\n    """""" Plot graph with state values and """"""\n    graph = plot_graph(mdp)\n    opt_s_a_edge_attrs = {\'style\': \'bold\',\n                          \'color\': \'green\',\n                          \'ratio\': \'auto\',\n                          \'penwidth\': \'6\'}\n\n    for state_node in mdp._transition_probs:\n        value = state_values[state_node]\n        graph.node(state_node,\n                   label=str(state_node) + \'\\n\' + \'V =\' + str(value)[:4])\n        for action in mdp.get_possible_actions(state_node):\n            if action == get_optimal_action_for_plot(mdp,\n                                                     state_values,\n                                                     state_node,\n                                                     gamma):\n                graph.edge(state_node, state_node + ""-"" + action,\n                           **opt_s_a_edge_attrs)\n    return graph\n'"
week1_07_Value_based/mdp_get_action_value.py,0,"b'def get_action_value(mdp, state_values, state, action, gamma):\n    """""" Computes Q(s,a) as in formula above """"""\n    \n    accumulator = 0.\n    # YOUR CODE HERE\n    for next_state, prob in mdp.get_next_states(state, action).items():\n        transition_reward = mdp.get_reward(state, action, next_state)\n        accumulator += prob * (transition_reward + gamma * state_values[next_state])\n    return accumulator'"
week1_09_approx_qlearning/atari_wrappers.py,0,"b'# taken from OpenAI baselines.\n\nimport numpy as np\nimport gym\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\n# in torch imgs have shape [c, h, w] instead of common [h, w, c]\nclass AntiTorchWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n\n        self.img_size = [env.observation_space.shape[i]\n                           for i in [1, 2, 0]\n                        ]\n        self.observation_space = gym.spaces.Box(0.0, 1.0, self.img_size)\n\n    def _observation(self, img):\n        """"""what happens to each observation""""""\n        img = img.transpose(1, 2, 0)\n        return img'"
week1_09_approx_qlearning/framebuffer.py,0,"b'import numpy as np\nfrom gym.spaces.box import Box\nfrom gym.core import Wrapper\nclass FrameBuffer(Wrapper):\n    def __init__(self, env, n_frames=4, dim_order=\'tensorflow\'):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(FrameBuffer, self).__init__(env)\n        self.dim_order = dim_order\n        if dim_order == \'tensorflow\':\n            height, width, n_channels = env.observation_space.shape\n            obs_shape = [height, width, n_channels * n_frames]\n        elif dim_order == \'pytorch\':\n            n_channels, height, width = env.observation_space.shape\n            obs_shape = [n_channels * n_frames, height, width]\n        else:\n            raise ValueError(\'dim_order should be ""tensorflow"" or ""pytorch"", got {}\'.format(dim_order))\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, \'float32\')\n        \n    def reset(self):\n        """"""resets breakout, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n    \n    def step(self, action):\n        """"""plays breakout for 1 step, returns frame buffer""""""\n        new_img, reward, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, reward, done, info\n    \n    def update_buffer(self, img):\n        if self.dim_order == \'tensorflow\':\n            offset = self.env.observation_space.shape[-1]\n            axis = -1\n            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n        elif self.dim_order == \'pytorch\':\n            offset = self.env.observation_space.shape[0]\n            axis = 0\n            cropped_framebuffer = self.framebuffer[:-offset]\n        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n'"
week1_09_approx_qlearning/replay_buffer.py,0,"b'# This code is shamelessly stolen from https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\nimport numpy as np\nimport random\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        """"""Create Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        """"""\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n\n    def sample(self, batch_size):\n        """"""Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        """"""\n        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n'"
week1_09_approx_qlearning/utils.py,0,"b'import numpy as np\nimport psutil\nfrom scipy.signal import convolve, gaussian\nimport torch\nfrom torch import nn\nimport os\n\ndef get_cum_discounted_rewards(rewards, gamma):\n    """"""\n    evaluates cumulative discounted rewards:\n    r_t + gamma * r_{t+1} + gamma^2 * r_{t_2} + ...\n    """"""\n    cum_rewards = []\n    cum_rewards.append(rewards[-1])\n    for r in reversed(rewards[:-1]):\n        cum_rewards.insert(0, r + gamma * cum_rewards[0])\n    return cum_rewards\n\n\ndef play_and_log_episode(env, agent, gamma=0.99, t_max=10000):\n    """"""\n    always greedy\n    """"""\n    states = []\n    v_mc = []\n    v_agent = []\n    q_spreads = []\n    td_errors = []\n    rewards = []\n\n    s = env.reset()\n    for step in range(t_max):\n        states.append(s)\n        qvalues = agent.get_qvalues([s])\n        max_q_value, min_q_value = np.max(qvalues), np.min(qvalues)\n        v_agent.append(max_q_value)\n        q_spreads.append(max_q_value - min_q_value)\n        if step > 0:\n            td_errors.append(np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n        action = qvalues.argmax(axis=-1)[0]\n\n        s, r, done, _ = env.step(action)\n        rewards.append(r)\n        if done:\n            break\n    td_errors.append(np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n    v_mc = get_cum_discounted_rewards(rewards, gamma)\n\n    return_pack = {\n        \'states\': np.array(states),\n        \'v_mc\': np.array(v_mc),\n        \'v_agent\': np.array(v_agent),\n        \'q_spreads\': np.array(q_spreads),\n        \'td_errors\': np.array(td_errors),\n        \'rewards\': np.array(rewards),\n        \'episode_finished\': np.array(done)\n    }\n\n    return return_pack\n\n\ndef img_by_obs(obs, state_dim):\n    """"""\n    Unwraps obs by channels.\n    observation is of shape [c, h=w, w=h]\n    """"""\n    return obs.reshape([-1, state_dim[2]])\n\n\ndef is_enough_ram(min_available_gb = 0.1):\n    mem = psutil.virtual_memory()\n    return mem.available >= min_available_gb * (1024 ** 3)\n\n\ndef linear_decay(init_val, final_val, cur_step, total_steps):\n    if cur_step >= total_steps:\n        return final_val\n    return (init_val * (total_steps - cur_step) + final_val * cur_step) / total_steps\n\n\ndef smoothen(values):\n    kernel = gaussian(100, std=100)\n    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n    kernel = kernel / np.sum(kernel)\n    return convolve(values, kernel, \'valid\')\n'"
week1_10_Reinforce/atari_wrappers.py,0,"b'"""""" Environment wrappers. """"""\nfrom collections import deque\n\nimport cv2\nimport gym\nimport gym.spaces as spaces\nfrom gym.envs import atari\nimport numpy as np\nimport tensorflow as tf\n\nfrom env_batch import ParallelEnvBatch\ncv2.ocl.setUseOpenCL(False)\n\n\nclass EpisodicLife(gym.Wrapper):\n  """""" Sets done flag to true when agent dies. """"""\n  def __init__(self, env):\n    super(EpisodicLife, self).__init__(env)\n    self.lives = 0\n    self.real_done = True\n\n  def step(self, action):\n    obs, rew, done, info = self.env.step(action)\n    self.real_done = done\n    info[""real_done""] = done\n    lives = self.env.unwrapped.ale.lives()\n    if 0 < lives < self.lives:\n      done = True\n    self.lives = lives\n    return obs, rew, done, info\n\n  def reset(self, **kwargs):\n    if self.real_done:\n      obs = self.env.reset(**kwargs)\n    else:\n      obs, _, _, _ = self.env.step(0)\n    self.lives = self.env.unwrapped.ale.lives()\n    return obs\n\n\nclass FireReset(gym.Wrapper):\n  """""" Makes fire action when reseting environment.\n\n  Some environments are fixed until the agent makes the fire action,\n  this wrapper makes this action so that the epsiode starts automatically.\n  """"""\n  def __init__(self, env):\n    super(FireReset, self).__init__(env)\n    action_meanings = env.unwrapped.get_action_meanings()\n    if len(action_meanings) < 3:\n      raise ValueError(\n          ""env.unwrapped.get_action_meanings() must be of length >= 3""\n          f""but is of length {len(action_meanings)}"")\n    if env.unwrapped.get_action_meanings()[1] != ""FIRE"":\n      raise ValueError(\n          ""env.unwrapped.get_action_meanings() must have \'FIRE\' ""\n          f""under index 1, but is {action_meanings}"")\n\n  def step(self, action):\n    return self.env.step(action)\n\n  def reset(self, **kwargs):\n    self.env.reset(**kwargs)\n    obs, _, done, _ = self.env.step(1)\n    if done:\n      self.env.reset(**kwargs)\n    obs, _, done, _ = self.env.step(2)\n    if done:\n      self.env.reset(**kwargs)\n    return obs\n\n\nclass StartWithRandomActions(gym.Wrapper):\n  """""" Makes random number of random actions at the beginning of each\n  episode. """"""\n  def __init__(self, env, max_random_actions=30):\n    super(StartWithRandomActions, self).__init__(env)\n    self.max_random_actions = max_random_actions\n    self.real_done = True\n\n  def step(self, action):\n    obs, rew, done, info = self.env.step(action)\n    self.real_done = info.get(""real_done"", True)\n    return obs, rew, done, info\n\n  def reset(self, **kwargs):\n    obs = self.env.reset()\n    if self.real_done:\n      num_random_actions = np.random.randint(self.max_random_actions + 1)\n      for _ in range(num_random_actions):\n        obs, _, _, _ = self.env.step(self.env.action_space.sample())\n      self.real_done = False\n    return obs\n\n\nclass ImagePreprocessing(gym.ObservationWrapper):\n  """""" Preprocesses image-observations by possibly grayscaling and resizing. """"""\n  def __init__(self, env, width=84, height=84, grayscale=True):\n    super(ImagePreprocessing, self).__init__(env)\n    self.width = width\n    self.height = height\n    self.grayscale = grayscale\n    ospace = self.env.observation_space\n    low, high, dtype = ospace.low.min(), ospace.high.max(), ospace.dtype\n    if self.grayscale:\n      self.observation_space = spaces.Box(low=low, high=high,\n                                          shape=(width, height), dtype=dtype)\n    else:\n      obs_shape = (width, height) + self.observation_space.shape[2:]\n      self.observation_space = spaces.Box(low=low, high=high,\n                                          shape=obs_shape, dtype=dtype)\n\n  def observation(self, observation):\n    """""" Performs image preprocessing. """"""\n    if self.grayscale:\n      observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n    observation = cv2.resize(observation, (self.width, self.height),\n                             cv2.INTER_AREA)\n    return observation\n\n\nclass MaxBetweenFrames(gym.ObservationWrapper):\n  """""" Takes maximum between two subsequent frames. """"""\n  def __init__(self, env):\n    if (isinstance(env.unwrapped, atari.AtariEnv) and\n        ""NoFrameskip"" not in env.spec.id):\n      raise ValueError(""MaxBetweenFrames requires NoFrameskip in atari env id"")\n    super(MaxBetweenFrames, self).__init__(env)\n    self.last_obs = None\n\n  def observation(self, observation):\n    obs = np.maximum(observation, self.last_obs)\n    self.last_obs = observation\n    return obs\n\n  def reset(self, **kwargs):\n    self.last_obs = self.env.reset()\n    return self.last_obs\n\n\nclass QueueFrames(gym.ObservationWrapper):\n  """""" Queues specified number of frames together along new dimension. """"""\n  def __init__(self, env, nframes, concat=False):\n    super(QueueFrames, self).__init__(env)\n    self.obs_queue = deque([], maxlen=nframes)\n    self.concat = concat\n    ospace = self.observation_space\n    if self.concat:\n      oshape = ospace.shape[:-1] + (ospace.shape[-1] * nframes,)\n    else:\n      oshape = ospace.shape + (nframes,)\n    self.observation_space = spaces.Box(ospace.low.min(), ospace.high.max(),\n                                        oshape, ospace.dtype)\n\n  def observation(self, observation):\n    self.obs_queue.append(observation)\n    return (np.concatenate(self.obs_queue, -1) if self.concat\n            else np.dstack(self.obs_queue))\n\n  def reset(self, **kwargs):\n    obs = self.env.reset()\n    for _ in range(self.obs_queue.maxlen - 1):\n      self.obs_queue.append(obs)\n    return self.observation(obs)\n\n\nclass SkipFrames(gym.Wrapper):\n  """""" Performs the same action for several steps and returns the final result.\n  """"""\n  def __init__(self, env, nskip=4):\n    super(SkipFrames, self).__init__(env)\n    if (isinstance(env.unwrapped, atari.AtariEnv) and\n        ""NoFrameskip"" not in env.spec.id):\n      raise ValueError(""SkipFrames requires NoFrameskip in atari env id"")\n    self.nskip = nskip\n\n  def step(self, action):\n    total_reward = 0.0\n    for _ in range(self.nskip):\n      obs, rew, done, info = self.env.step(action)\n      total_reward += rew\n      if done:\n        break\n    return obs, total_reward, done, info\n\n  def reset(self, **kwargs):\n    return self.env.reset(**kwargs)\n\n\nclass ClipReward(gym.RewardWrapper):\n  """""" Modifes reward to be in {-1, 0, 1} by taking sign of it. """"""\n  def reward(self, reward):\n    return np.sign(reward)\n\n\nclass TFSummaries(gym.Wrapper):\n  """""" Writes env summaries.""""""\n  def __init__(self, env, prefix=None, running_mean_size=100, step_var=None):\n    super(TFSummaries, self).__init__(env)\n    self.episode_counter = 0\n    self.prefix = prefix or self.env.spec.id\n    self.step_var = (step_var if step_var is not None\n                     else tf.train.get_global_step())\n\n    nenvs = getattr(self.env.unwrapped, ""nenvs"", 1)\n    self.rewards = np.zeros(nenvs)\n    self.had_ended_episodes = np.zeros(nenvs, dtype=np.bool)\n    self.episode_lengths = np.zeros(nenvs)\n    self.reward_queues = [deque([], maxlen=running_mean_size)\n                          for _ in range(nenvs)]\n\n  def should_write_summaries(self):\n    """""" Returns true if it\'s time to write summaries. """"""\n    return np.all(self.had_ended_episodes)\n\n  def add_summaries(self):\n    """""" Writes summaries. """"""\n    tf.contrib.summary.scalar(\n        f""{self.prefix}/total_reward"",\n        tf.reduce_mean([q[-1] for q in self.reward_queues]),\n        step=self.step_var)\n    tf.contrib.summary.scalar(\n        f""{self.prefix}/reward_mean_{self.reward_queues[0].maxlen}"",\n        tf.reduce_mean([np.mean(q) for q in self.reward_queues]),\n        step=self.step_var)\n    tf.contrib.summary.scalar(\n        f""{self.prefix}/episode_length"",\n        tf.reduce_mean(self.episode_lengths),\n        step=self.step_var)\n    if self.had_ended_episodes.size > 1:\n      tf.contrib.summary.scalar(\n          f""{self.prefix}/min_reward"",\n          min(q[-1] for q in self.reward_queues),\n          step=self.step_var)\n      tf.contrib.summary.scalar(\n          f""{self.prefix}/max_reward"",\n          max(q[-1] for q in self.reward_queues),\n          step=self.step_var)\n    self.episode_lengths.fill(0)\n    self.had_ended_episodes.fill(False)\n\n  def step(self, action):\n    obs, rew, done, info = self.env.step(action)\n    self.rewards += rew\n    self.episode_lengths[~self.had_ended_episodes] += 1\n\n    info_collection = [info] if isinstance(info, dict) else info\n    done_collection = [done] if isinstance(done, bool) else done\n    done_indices = [i for i, info in enumerate(info_collection)\n                    if info.get(""real_done"", done_collection[i])]\n    for i in done_indices:\n      if not self.had_ended_episodes[i]:\n        self.had_ended_episodes[i] = True\n      self.reward_queues[i].append(self.rewards[i])\n      self.rewards[i] = 0\n\n    if self.should_write_summaries():\n      self.add_summaries()\n    return obs, rew, done, info\n\n  def reset(self, **kwargs):\n    self.rewards.fill(0)\n    self.episode_lengths.fill(0)\n    self.had_ended_episodes.fill(False)\n    return self.env.reset(**kwargs)\n\n\ndef nature_dqn_env(env_id, nenvs=None, seed=None,\n                   summaries=True, clip_reward=True):\n  """""" Wraps env as in Nature DQN paper. """"""\n  if ""NoFrameskip"" not in env_id:\n    raise ValueError(f""env_id must have \'NoFrameskip\' but is {env_id}"")\n  if nenvs is not None:\n    if seed is None:\n      seed = list(range(nenvs))\n    if isinstance(seed, int):\n      seed = [seed] * nenvs\n    if len(seed) != nenvs:\n      raise ValueError(f""seed has length {len(seed)} but must have ""\n                       f""length equal to nenvs which is {nenvs}"")\n\n    env = ParallelEnvBatch([\n        lambda i=i, env_seed=env_seed: nature_dqn_env(\n            env_id, seed=env_seed, summaries=False, clip_reward=False)\n        for i, env_seed in enumerate(seed)\n    ])\n    if summaries:\n      env = TFSummaries(env, prefix=env_id)\n    if clip_reward:\n      env = ClipReward(env)\n    return env\n\n  env = gym.make(env_id)\n  env.seed(seed)\n  if summaries:\n    env = TFSummaries(env)\n  env = EpisodicLife(env)\n  if ""FIRE"" in env.unwrapped.get_action_meanings():\n    env = FireReset(env)\n  env = StartWithRandomActions(env, max_random_actions=30)\n  env = MaxBetweenFrames(env)\n  env = SkipFrames(env, 4)\n  env = ImagePreprocessing(env, width=84, height=84, grayscale=True)\n  env = QueueFrames(env, 4)\n  if clip_reward:\n    env = ClipReward(env)\n  return env\n'"
week1_10_Reinforce/env_batch.py,0,"b'# pylint: skip-file\nfrom multiprocessing import Process, Pipe\n\nfrom gym import Env, Wrapper, Space\nimport numpy as np\n\n\nclass SpaceBatch(Space):\n  def __init__(self, spaces):\n    first_type = type(spaces[0])\n    first_shape = spaces[0].shape\n    first_dtype = spaces[0].dtype\n    for space in spaces:\n      if not isinstance(space, first_type):\n        raise TypeError(""spaces have different types: {}, {}""\n                        .format(first_type, type(space)))\n      if first_shape != space.shape:\n        raise ValueError(""spaces have different shapes: {}, {}""\n                         .format(first_shape, space.shape))\n      if first_dtype != space.dtype:\n        raise ValueError(""spaces have different data types: {}, {}""\n                         .format(first_dtype, space.dtype))\n\n    self.spaces = spaces\n    super(SpaceBatch, self).__init__(shape=self.spaces[0].shape,\n                                     dtype=self.spaces[0].dtype)\n\n  def sample(self):\n    return np.stack([space.sample() for space in self.spaces])\n\n  def __getattr__(self, attr):\n    return getattr(self.spaces[0], attr)\n\n\nclass EnvBatch(Env):\n  def __init__(self, make_env, nenvs=None):\n    make_env_functions = self._get_make_env_functions(make_env, nenvs)\n    self._envs = [make_env() for make_env in make_env_functions]\n    self._nenvs = len(self.envs)\n    # self.observation_space = SpaceBatch([env.observation_space\n    #                                      for env in self._envs])\n    self.action_space = SpaceBatch([env.action_space\n                                    for env in self._envs])\n\n  def _get_make_env_functions(self, make_env, nenvs):\n    if nenvs is None and not isinstance(make_env, list):\n      raise ValueError(""When nenvs is None make_env""\n                       "" must be a list of callables"")\n    if nenvs is not None and not callable(make_env):\n      raise ValueError(""When nenvs is not None make_env must be callable"")\n\n    if nenvs is not None:\n      make_env = [make_env for _ in range(nenvs)]\n    return make_env\n\n  @property\n  def nenvs(self):\n    return self._nenvs\n\n  @property\n  def envs(self):\n    return self._envs\n\n  def _check_actions(self, actions):\n    if not len(actions) == self.nenvs:\n      raise ValueError(\n          ""number of actions is not equal to number of envs: ""\n          ""len(actions) = {}, nenvs = {}""\n          .format(len(actions), self.nenvs))\n\n  def step(self, actions):\n    self._check_actions(actions)\n    obs, rews, resets, infos = [], [], [], []\n    for env, action in zip(self._envs, actions):\n      ob, rew, done, info = env.step(action)\n      if done:\n        ob = env.reset()\n      obs.append(ob)\n      rews.append(rew)\n      resets.append(done)\n      infos.append(info)\n    return np.stack(obs), np.stack(rews), np.stack(resets), infos\n\n  def reset(self):\n    return np.stack([env.reset() for env in self.envs])\n\n\nclass SingleEnvBatch(Wrapper, EnvBatch):\n  def __init__(self, env):\n    super(SingleEnvBatch, self).__init__(env)\n    self.observation_space = SpaceBatch([self.env.observation_space])\n    self.action_space = SpaceBatch([self.env.action_space])\n\n  @property\n  def nenvs(self):\n    return 1\n\n  @property\n  def envs(self):\n    return [self.env]\n\n  def step(self, actions):\n    self._check_actions(actions)\n    ob, rew, done, info = self.env.step(actions[0])\n    if done:\n      ob = self.env.reset()\n    return ob[None], np.expand_dims(rew, 0), np.expand_dims(done, 0), [info]\n\n  def reset(self):\n    return self.env.reset()[None]\n\n\ndef worker(parent_connection, worker_connection, make_env_function,\n           send_spaces=True):\n  # Adapted from SubprocVecEnv github.com/openai/baselines\n  parent_connection.close()\n  env = make_env_function()\n  if send_spaces:\n    worker_connection.send((env.observation_space, env.action_space))\n  while True:\n    cmd, action = worker_connection.recv()\n    if cmd == ""step"":\n      ob, rew, done, info = env.step(action)\n      if done:\n        ob = env.reset()\n      worker_connection.send((ob, rew, done, info))\n    elif cmd == ""reset"":\n      ob = env.reset()\n      worker_connection.send(ob)\n    elif cmd == ""close"":\n      env.close()\n      worker_connection.close()\n      break\n    else:\n      raise NotImplementedError(""Unknown command %s"" % cmd)\n\n\nclass ParallelEnvBatch(EnvBatch):\n  """"""\n  An abstract batch of environments.\n  """"""\n  def __init__(self, make_env, nenvs=None):\n    make_env_functions = self._get_make_env_functions(make_env, nenvs)\n    self._nenvs = len(make_env_functions)\n    self._parent_connections, self._worker_connections = zip(*[\n      Pipe() for _ in range(self._nenvs)\n    ])\n    self._processes = [\n        Process(\n          target=worker,\n          args=(parent_connection, worker_connection, make_env),\n          daemon=True\n        )\n        for i, (parent_connection, worker_connection, make_env)\n        in enumerate(zip(self._parent_connections,\n                         self._worker_connections,\n                         make_env_functions))\n    ]\n    for p in self._processes:\n      p.start()\n    self._closed = False\n\n    for conn in self._worker_connections:\n      conn.close()\n\n    observation_spaces, action_spaces = [], []\n    for conn in self._parent_connections:\n      ob_space, ac_space = conn.recv()\n      observation_spaces.append(ob_space)\n      action_spaces.append(ac_space)\n    self.observation_space = SpaceBatch(observation_spaces)\n    self.action_space = SpaceBatch(action_spaces)\n\n  @property\n  def nenvs(self):\n    return self._nenvs\n\n  def step(self, actions):\n    self._check_actions(actions)\n    for conn, a in zip(self._parent_connections, actions):\n      conn.send((""step"", a))\n    results = [conn.recv() for conn in self._parent_connections]\n    obs, rews, dones, infos = zip(*results)\n    return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n  def reset(self):\n    for conn in self._parent_connections:\n      conn.send((""reset"", None))\n    return np.stack([conn.recv() for conn in self._parent_connections])\n\n  def close(self):\n    if self._closed:\n      return\n    for conn in self._parent_connections:\n      conn.send((""close"", None))\n    for p in self._processes:\n      p.join()\n    self._closed = True\n\n  def render(self):\n    raise ValueError(""render not defined for %s"" % self)\n'"
week1_10_Reinforce/runners.py,0,"b'"""""" RL env runner """"""\nfrom collections import defaultdict\nimport numpy as np\n\n\nclass EnvRunner:\n  """""" Reinforcement learning runner in an environment with given policy """"""\n  def __init__(self, env, policy, nsteps,\n               transforms=None, step_var=None):\n    self.env = env\n    self.policy = policy\n    self.nsteps = nsteps\n    self.transforms = transforms or []\n    self.step_var = step_var if step_var is not None else 0\n    self.state = {""latest_observation"": self.env.reset()}\n\n  @property\n  def nenvs(self):\n    """""" Returns number of batched envs or `None` if env is not batched """"""\n    return getattr(self.env.unwrapped, ""nenvs"", None)\n\n  def reset(self):\n    """""" Resets env and runner states. """"""\n    self.state[""latest_observation""] = self.env.reset()\n    self.policy.reset()\n\n  def get_next(self):\n    """""" Runs the agent in the environment.  """"""\n    trajectory = defaultdict(list, {""actions"": []})\n    observations = []\n    rewards = []\n    resets = []\n    self.state[""env_steps""] = self.nsteps\n\n    for i in range(self.nsteps):\n      observations.append(self.state[""latest_observation""])\n      act = self.policy.act(self.state[""latest_observation""])\n      if ""actions"" not in act:\n        raise ValueError(""result of policy.act must contain \'actions\' ""\n                         f""but has keys {list(act.keys())}"")\n      for key, val in act.items():\n        trajectory[key].append(val)\n\n      obs, rew, done, _ = self.env.step(trajectory[""actions""][-1])\n      self.state[""latest_observation""] = obs\n      rewards.append(rew)\n      resets.append(done)\n      self.step_var += self.nenvs or 1\n\n      # Only reset if the env is not batched. Batched envs should auto-reset.\n      if not self.nenvs and np.all(done):\n        self.state[""env_steps""] = i + 1\n        self.state[""latest_observation""] = self.env.reset()\n\n    trajectory.update(observations=observations, rewards=rewards, resets=resets)\n    trajectory[""state""] = self.state\n\n    for transform in self.transforms:\n      transform(trajectory)\n    return trajectory\n'"
week1_11_RL_outside_games/basic_model_torch.py,22,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Note: unlike official pytorch tutorial, this model doesn\'t process one sample at a time\n# because it\'s slow on GPU.  instead it uses masks just like ye olde theano/tensorflow.\n# it doesn\'t use torch.nn.utils.rnn.pack_paded_sequence because reasons.\n\nclass BasicTranslationModel(nn.Module):\n    def __init__(self, inp_voc, out_voc,\n                 emb_size, hid_size,):\n        super(self.__class__, self).__init__()\n        self.inp_voc = inp_voc\n        self.out_voc = out_voc\n\n        self.emb_inp = nn.Embedding(len(inp_voc), emb_size)\n        self.emb_out = nn.Embedding(len(out_voc), emb_size)\n        self.enc0 = nn.GRU(emb_size, hid_size, batch_first=True)\n        self.dec_start = nn.Linear(hid_size, hid_size)\n        self.dec0 = nn.GRUCell(emb_size, hid_size)\n        self.logits = nn.Linear(hid_size, len(out_voc))\n\n    def encode(self, inp, **flags):\n        """"""\n        Takes symbolic input sequence, computes initial state\n        :param inp: a vector of input tokens  (Variable, int64, 1d)\n        :return: a list of initial decoder state tensors\n        """"""\n        inp_emb = self.emb_inp(inp)\n        enc_seq, _ = self.enc0(inp_emb)\n\n        # select last element w.r.t. mask\n        end_index = infer_length(inp, self.inp_voc.eos_ix)\n        end_index[end_index >= inp.shape[1]] = inp.shape[1] - 1\n        enc_last = enc_seq[range(0, enc_seq.shape[0]), end_index.detach(), :]\n\n        dec_start = self.dec_start(enc_last)\n        return [dec_start]\n\n    def decode(self, prev_state, prev_tokens, **flags):\n        """"""\n        Takes previous decoder state and tokens, returns new state and logits\n        :param prev_state: a list of previous decoder state tensors\n        :param prev_tokens: previous output tokens, an int vector of [batch_size]\n        :return: a list of next decoder state tensors, a tensor of logits [batch,n_tokens]\n        """"""\n        [prev_dec] = prev_state\n\n        prev_emb = self.emb_out(prev_tokens)\n        new_dec_state = self.dec0(prev_emb, prev_dec)\n        output_logits = self.logits(new_dec_state)\n\n        return [new_dec_state], output_logits\n\n    def forward(self, inp, out, eps=1e-30, **flags):\n        """"""\n        Takes symbolic int32 matrices of hebrew words and their english translations.\n        Computes the log-probabilities of all possible english characters given english prefices and hebrew word.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param out: output sequence, int32 matrix of shape [batch,time]\n        :return: log-probabilities of all possible english characters of shape [bath,time,n_tokens]\n\n        Note: log-probabilities time axis is synchronized with out\n        In other words, logp are probabilities of __current__ output at each tick, not the next one\n        therefore you can get likelihood as logprobas * tf.one_hot(out,n_tokens)\n        """"""\n        device = next(self.parameters()).device\n        batch_size = inp.shape[0]\n        bos = torch.tensor([self.out_voc.bos_ix] * batch_size, dtype=torch.long, device=device)\n        logits_seq = [torch.log(to_one_hot(bos, len(self.out_voc)) + eps)]\n\n        hid_state = self.encode(inp, **flags)\n        for x_t in out.transpose(0,1)[:-1]:\n            hid_state, logits = self.decode(hid_state, x_t, **flags)\n            logits_seq.append(logits)\n\n        return F.log_softmax(torch.stack(logits_seq, dim=1), dim=-1)\n\n    def translate(self, inp, greedy=False, max_len = None, eps = 1e-30, **flags):\n        """"""\n        takes symbolic int32 matrix of hebrew words, produces output tokens sampled\n        from the model and output log-probabilities for all possible tokens at each tick.\n        :param inp: input sequence, int32 matrix of shape [batch,time]\n        :param greedy: if greedy, takes token with highest probablity at each tick.\n            Otherwise samples proportionally to probability.\n        :param max_len: max length of output, defaults to 2 * input length\n        :return: output tokens int32[batch,time] and\n                 log-probabilities of all tokens at each tick, [batch,time,n_tokens]\n        """"""\n        device = next(self.parameters()).device\n        batch_size = inp.shape[0]\n        bos = torch.tensor([self.out_voc.bos_ix] * batch_size, dtype=torch.long, device=device)\n        mask = torch.ones(batch_size, dtype=torch.uint8, device=device)\n        logits_seq = [torch.log(to_one_hot(bos, len(self.out_voc)) + eps)]\n        out_seq = [bos]\n\n        hid_state = self.encode(inp, **flags)\n        while True:\n            hid_state, logits = self.decode(hid_state, out_seq[-1], **flags)\n            if greedy:\n                _, y_t = torch.max(logits, dim=-1)\n            else:\n                probs = F.softmax(logits, dim=-1)\n                y_t = torch.multinomial(probs, 1)[:, 0]\n\n            logits_seq.append(logits)\n            out_seq.append(y_t)\n            mask *= y_t != self.out_voc.eos_ix\n\n            if not mask.any(): break\n            if max_len and len(out_seq) >= max_len: break\n\n        return torch.stack(out_seq, 1), F.log_softmax(torch.stack(logits_seq, 1), dim=-1)\n\n\n\n### Utility functions ###\n\ndef infer_mask(seq, eos_ix, batch_first=True, include_eos=True, dtype=torch.float):\n    """"""\n    compute length given output indices and eos code\n    :param seq: tf matrix [time,batch] if batch_first else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :param include_eos: if True, the time-step where eos first occurs is has mask = 1\n    :returns: lengths, int32 vector of shape [batch]\n    """"""\n    assert seq.dim() == 2\n    is_eos = (seq == eos_ix).to(dtype=torch.float)\n    if include_eos:\n        if batch_first:\n            is_eos = torch.cat((is_eos[:,:1]*0, is_eos[:, :-1]), dim=1)\n        else:\n            is_eos = torch.cat((is_eos[:1,:]*0, is_eos[:-1, :]), dim=0)\n    count_eos = torch.cumsum(is_eos, dim=1 if batch_first else 0)\n    mask = count_eos == 0\n    return mask.to(dtype=dtype)\n\ndef infer_length(seq, eos_ix, batch_first=True, include_eos=True, dtype=torch.long):\n    """"""\n    compute mask given output indices and eos code\n    :param seq: tf matrix [time,batch] if time_major else [batch,time]\n    :param eos_ix: integer index of end-of-sentence token\n    :param include_eos: if True, the time-step where eos first occurs is has mask = 1\n    :returns: mask, float32 matrix with \'0\'s and \'1\'s of same shape as seq\n    """"""\n    mask = infer_mask(seq, eos_ix, batch_first, include_eos, dtype)\n    return torch.sum(mask, dim=1 if batch_first else 0)\n\n\ndef to_one_hot(y, n_dims=None):\n    """""" Take integer y (tensor or variable) with n dims and convert it to 1-hot representation with n+1 dims. """"""\n    y_tensor = y.data\n    y_tensor = y_tensor.to(dtype=torch.long).view(-1, 1)\n    n_dims = n_dims if n_dims is not None else int(torch.max(y_tensor)) + 1\n    y_one_hot = torch.zeros(y_tensor.size()[0], n_dims, device=y.device).scatter_(1, y_tensor, 1)\n    y_one_hot = y_one_hot.view(*y.shape, -1)\n    return y_one_hot\n'"
week1_11_RL_outside_games/setup.py,0,b'#!/bin/bash\n# a setup script for google colab. Will be updated\npip install gym\napt-get install -y xvfb\nwget https://raw.githubusercontent.com/yandexdataschool/Practical_DL/fall18/xvfb -O ../xvfb\napt-get install -y python-opengl ffmpeg\npip install pyglet==1.2.4\n\n'
week1_11_RL_outside_games/voc.py,0,"b'import numpy as np\n\nclass Vocab:\n    def __init__(self, tokens, bos=""__BOS__"", eos=""__EOS__"", sep=\'\'):\n        """"""\n        A special class that handles tokenizing and detokenizing\n        """"""\n        assert bos in tokens, eos in tokens\n        self.tokens = tokens\n        self.token_to_ix = {t:i for i,t in enumerate(tokens)}\n\n        self.bos = bos\n        self.bos_ix = self.token_to_ix[bos]\n        self.eos = eos\n        self.eos_ix = self.token_to_ix[eos]\n        self.sep = sep\n\n    def __len__(self):\n        return len(self.tokens)\n\n    @staticmethod\n    def from_lines(lines, bos=""__BOS__"", eos=""__EOS__"", sep=\'\'):\n        flat_lines = sep.join(list(lines))\n        flat_lines = list(flat_lines.split(sep)) if sep != \'\' else list(flat_lines)\n        tokens = list(set(sep.join(flat_lines)))\n        tokens = [t for t in tokens if t not in (bos,eos) and len(t) != 0]\n        tokens = [bos,eos] + tokens\n        return Vocab(tokens,bos,eos,sep)\n\n    def tokenize(self,string):\n        """"""converts string to a list of tokens""""""\n        tokens = list(filter(len,string.split(self.sep))) \\\n                    if self.sep != \'\' else list(string)\n        return [self.bos] + tokens + [self.eos]\n\n    def to_matrix(self, lines, max_len=None):\n        """"""\n        convert variable length token sequences into  fixed size matrix\n        example usage:\n        >>>print( as_matrix(words[:3],source_to_ix))\n        [[15 22 21 28 27 13 -1 -1 -1 -1 -1]\n         [30 21 15 15 21 14 28 27 13 -1 -1]\n         [25 37 31 34 21 20 37 21 28 19 13]]\n        """"""\n        max_len = max_len or max(map(len, lines)) + 2 # 2 for bos and eos\n\n        matrix = np.zeros((len(lines), max_len), dtype=\'int32\') + self.eos_ix\n        for i, seq in enumerate(lines):\n            tokens = self.tokenize(seq)\n            row_ix = list(map(self.token_to_ix.get, tokens))[:max_len]\n            matrix[i, :len(row_ix)] = row_ix\n\n        return matrix\n\n    def to_lines(self, matrix, crop=True):\n        """"""\n        Convert matrix of token ids into strings\n        :param matrix: matrix of tokens of int32, shape=[batch,time]\n        :param crop: if True, crops BOS and EOS from line\n        :return:\n        """"""\n        lines = []\n        for line_ix in map(list,matrix):\n            if crop:\n                if line_ix[0] == self.bos_ix:\n                    line_ix = line_ix[1:]\n                if self.eos_ix in line_ix:\n                    line_ix = line_ix[:line_ix.index(self.eos_ix)]\n            line = self.sep.join(self.tokens[i] for i in line_ix)\n            lines.append(line)\n        return lines\n'"
homeworks_advanced/Lab1_NLP/my_network.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nimport torchtext\nfrom torchtext.datasets import TranslationDataset, Multi30k\nfrom torchtext.data import Field, BucketIterator\n\nimport random\nimport math\nimport time\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n#         self.dropout = dropout\n        \n        self.embedding = nn.Embedding(\n            num_embeddings=input_dim,\n            embedding_dim=emb_dim\n        )\n            # <YOUR CODE HERE>\n        \n        self.rnn = nn.LSTM(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout\n        )\n            # <YOUR CODE HERE>\n        \n        self.dropout = nn.Dropout(p=dropout)# <YOUR CODE HERE>\n        \n    def forward(self, src):\n        \n        #src = [src sent len, batch size]\n        \n        # Compute an embedding from the src data and apply dropout to it\n        embedded = self.embedding(src)# <YOUR CODE HERE>\n        \n        embedded = self.dropout(embedded)\n        \n        output, (hidden, cell) = self.rnn(embedded)\n        #embedded = [src sent len, batch size, emb dim]\n        \n        # Compute the RNN output values of the encoder RNN. \n        # outputs, hidden and cell should be initialized here. Refer to nn.LSTM docs ;)\n        \n        # <YOUR CODE HERE> \n        \n        #outputs = [src sent len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #outputs are always from the top hidden layer\n        \n        return hidden, cell\n    \n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.output_dim = output_dim\n        self.n_layers = n_layers\n        self.dropout = dropout\n        \n        self.embedding = nn.Embedding(\n            num_embeddings=output_dim,\n            embedding_dim=emb_dim\n        )\n            # <YOUR CODE HERE>\n        \n        self.rnn = nn.LSTM(\n            input_size=emb_dim,\n            hidden_size=hid_dim,\n            num_layers=n_layers,\n            dropout=dropout\n        )\n            # <YOUR CODE HERE>\n        \n        self.out = nn.Linear(\n            in_features=hid_dim,\n            out_features=output_dim\n        )\n            # <YOUR CODE HERE>\n        \n        self.dropout = nn.Dropout(p=dropout)# <YOUR CODE HERE>\n        \n    def forward(self, input, hidden, cell):\n        \n        #input = [batch size]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #n directions in the decoder will both always be 1, therefore:\n        #hidden = [n layers, batch size, hid dim]\n        #context = [n layers, batch size, hid dim]\n        \n        input = input.unsqueeze(0)\n        \n        #input = [1, batch size]\n        \n        # Compute an embedding from the input data and apply dropout to it\n        embedded = self.dropout(self.embedding(input))# <YOUR CODE HERE>\n        \n        #embedded = [1, batch size, emb dim]\n        \n        # Compute the RNN output values of the encoder RNN. \n        # outputs, hidden and cell should be initialized here. Refer to nn.LSTM docs ;)\n        # <YOUR CODE HERE>\n        \n        \n        #output = [sent len, batch size, hid dim * n directions]\n        #hidden = [n layers * n directions, batch size, hid dim]\n        #cell = [n layers * n directions, batch size, hid dim]\n        \n        #sent len and n directions will always be 1 in the decoder, therefore:\n        #output = [1, batch size, hid dim]\n        #hidden = [n layers, batch size, hid dim]\n        #cell = [n layers, batch size, hid dim]\n        \n        \n        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n        prediction = self.out(output.squeeze(0))\n        \n        #prediction = [batch size, output dim]\n        \n        return prediction, hidden, cell\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.hid_dim, \\\n            ""Hidden dimensions of encoder and decoder must be equal!""\n        assert encoder.n_layers == decoder.n_layers, \\\n            ""Encoder and decoder must have equal number of layers!""\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src sent len, batch size]\n        #trg = [trg sent len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        # Again, now batch is the first dimention instead of zero\n        batch_size = trg.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        hidden, cell = self.encoder(src)\n        \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        for t in range(1, max_len):\n            \n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            input = (trg[t] if teacher_force else top1)\n        \n        return outputs\n'"
homeworks_advanced/Lab1_NLP/utils.py,0,"b""\ndef flatten(l):\n    return [item for sublist in l for item in sublist]\n\ndef remove_tech_tokens(mystr, tokens_to_remove=['<eos>', '<sos>', '<unk>', '<pad>']):\n    return [x for x in mystr if x not in tokens_to_remove]\n\n\ndef get_text(x, TRG_vocab):\n    text = remove_tech_tokens([TRG_vocab.itos[token] for token in x])\n    if len(text) < 1:\n        text = []\n    return text\n\n\ndef generate_translation(src, trg, model, TRG_vocab):\n    model.eval()\n\n    output = model(src, trg, 0) #turn off teacher forcing\n    output = output.argmax(dim=-1).cpu().numpy()\n\n    original = [TRG_vocab.itos[x] for x in list(trg[:,0].cpu().numpy())]\n    generated = [TRG_vocab.itos[x] for x in list(output[:, 0])]\n    \n    original = remove_tech_tokens(original)\n    generated = remove_tech_tokens(generated)\n    print('Original: {}'.format(' '.join(original)))\n    print('Generated: {}'.format(' '.join(generated)))\n    print()\n"""
homeworks_advanced/Lab2_RL/atari_util.py,0,"b'""""""Auxilary files for those who wanted to solve breakout with CEM or policy gradient""""""\nimport numpy as np\nfrom gym.core import Wrapper\nfrom gym.spaces.box import Box\nfrom skimage.transform import resize\n\n\nclass PreprocessAtari(Wrapper):\n    def __init__(self, env, height=42, width=42, color=False,\n                 crop=lambda img: img, n_frames=4, dim_order=\'theano\'):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(PreprocessAtari, self).__init__(env)\n        assert dim_order in (\'theano\', \'tensorflow\')\n        self.img_size = (height, width)\n        self.crop = crop\n        self.color = color\n        self.dim_order = dim_order\n\n        n_channels = (3 * n_frames) if color else n_frames\n        obs_shape = \\\n            [n_channels, height, width] \\\n            if dim_order == \'theano\' else \\\n            [height, width, n_channels]\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, \'float32\')\n\n    def reset(self):\n        """"""resets breakout, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n\n    def step(self, action):\n        """"""plays breakout for 1 step, returns frame buffer""""""\n        new_img, r, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, r, done, info\n\n    ### image processing ###\n\n    def update_buffer(self, img):\n        img = self.preproc_image(img)\n        offset = 3 if self.color else 1\n        if self.dim_order == \'theano\':\n            axis = 0\n            cropped_framebuffer = self.framebuffer[:-offset]\n        else:\n            axis = -1\n            cropped_framebuffer = self.framebuffer[:, :, :-offset]\n        self.framebuffer = np.concatenate(\n            [img, cropped_framebuffer], axis=axis)\n\n    def preproc_image(self, img):\n        """"""what happens to the observation""""""\n        img = self.crop(img)\n        img = resize(img, self.img_size)\n        if not self.color:\n            img = img.mean(-1, keepdims=True)\n        if self.dim_order == \'theano\':\n            img = img.transpose([2, 0, 1])  # [h, w, c] to [c, h, w]\n        img = img.astype(\'float32\') / 255.\n        return img\n'"
homeworks_advanced/Lab2_RL/atari_wrappers.py,0,"b'# taken from OpenAI baselines.\n\nimport numpy as np\nimport gym\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        """"""Return only every `skip`-th frame""""""\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn\'t matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        """"""Bin reward to {+1, 0, -1} by its sign.""""""\n        return np.sign(reward)\n\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Take action on reset for environments that are fixed until firing.""""""\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == \'FIRE\'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs\n\n    def step(self, ac):\n        return self.env.step(ac)\n\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        """"""Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it\'s important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info\n\n    def reset(self, **kwargs):\n        """"""Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        """"""\n        if self.was_real_done:\n            obs = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs\n\n\n# in torch imgs have shape [c, h, w] instead of common [h, w, c]\nclass AntiTorchWrapper(gym.ObservationWrapper):\n    def __init__(self, env):\n        gym.ObservationWrapper.__init__(self, env)\n\n        self.img_size = [env.observation_space.shape[i]\n                           for i in [1, 2, 0]\n                        ]\n        self.observation_space = gym.spaces.Box(0.0, 1.0, self.img_size)\n\n    def _observation(self, img):\n        """"""what happens to each observation""""""\n        img = img.transpose(1, 2, 0)\n        return img'"
homeworks_advanced/Lab2_RL/env_pool.py,0,"b'""""""\nA thin wrapper for openAI gym environments that maintains a set of parallel games and has a method to generate\ninteraction sessions given agent one-step applier function.\n""""""\n\nimport numpy as np\n\n# A whole lot of space invaders\n\n\nclass EnvPool(object):\n    def __init__(self, agent, make_env, n_parallel_games=1):\n        """"""\n        A special class that handles training on multiple parallel sessions\n        and is capable of some auxilary actions like evaluating agent on one game session (See .evaluate()).\n\n        :param agent: Agent which interacts with the environment.\n        :param make_env: Factory that produces environments OR a name of the gym environment.\n        :param n_games: Number of parallel games. One game by default.\n        :param max_size: Max pool size by default (if appending sessions). By default, pool is not constrained in size.\n        """"""\n        # Create atari games.\n        self.agent = agent\n        self.make_env = make_env\n        self.envs = [self.make_env() for _ in range(n_parallel_games)]\n\n        # Initial observations.\n        self.prev_observations = [env.reset() for env in self.envs]\n\n        # Agent memory variables (if you use recurrent networks).\n        self.prev_memory_states = agent.get_initial_state(n_parallel_games)\n\n        # Whether particular session has just been terminated and needs\n        # restarting.\n        self.just_ended = [False] * len(self.envs)\n\n    def interact(self, n_steps=100, verbose=False):\n        """"""Generate interaction sessions with ataries (openAI gym atari environments)\n        Sessions will have length n_steps. Each time one of games is finished, it is immediately getting reset\n        and this time is recorded in is_alive_log (See returned values).\n\n        :param n_steps: Length of an interaction.\n        :returns: observation_seq, action_seq, reward_seq, is_alive_seq\n        :rtype: a bunch of tensors [batch, tick, ...]\n        """"""\n\n        def env_step(i, action):\n            if not self.just_ended[i]:\n                new_observation, cur_reward, is_done, info = \\\n                    self.envs[i].step(action)\n                if is_done:\n                    # Game ends now, will finalize on next tick.\n                    self.just_ended[i] = True\n\n                # note: is_alive=True in any case because environment is still\n                # alive (last tick alive) in our notation.\n                return new_observation, cur_reward, True, info\n            else:\n                # Reset environment, get new observation to be used on next\n                # tick.\n                new_observation = self.envs[i].reset()\n\n                # Reset memory for new episode.\n                initial_memory_state = self.agent.get_initial_state(\n                    batch_size=1)\n                for m_i in range(len(new_memory_states)):\n                    new_memory_states[m_i][i] = initial_memory_state[m_i][0]\n\n                if verbose:\n                    print(""env %i reloaded"" % i)\n\n                self.just_ended[i] = False\n\n                return new_observation, 0, False, {\'end\': True}\n\n        history_log = []\n\n        for i in range(n_steps - 1):\n            new_memory_states, readout = self.agent.step(\n                self.prev_memory_states, self.prev_observations)\n            actions = self.agent.sample_actions(readout)\n\n            new_observations, cur_rewards, is_alive, infos = zip(\n                *map(env_step, range(len(self.envs)), actions))\n\n            # Append data tuple for this tick.\n            history_log.append(\n                (self.prev_observations, actions, cur_rewards, is_alive))\n\n            self.prev_observations = new_observations\n            self.prev_memory_states = new_memory_states\n\n        # add last observation\n        dummy_actions = [0] * len(self.envs)\n        dummy_rewards = [0] * len(self.envs)\n        dummy_mask = [1] * len(self.envs)\n        history_log.append(\n            (self.prev_observations,\n             dummy_actions,\n             dummy_rewards,\n             dummy_mask))\n\n        # cast to numpy arrays, transpose from [time, batch, ...] to [batch,\n        # time, ...]\n        history_log = [\n            np.array(tensor).swapaxes(0, 1)\n            for tensor in zip(*history_log)\n        ]\n        observation_seq, action_seq, reward_seq, is_alive_seq = history_log\n\n        return observation_seq, action_seq, reward_seq, is_alive_seq\n'"
homeworks_advanced/Lab2_RL/framebuffer.py,0,"b'import numpy as np\nfrom gym.spaces.box import Box\nfrom gym.core import Wrapper\nclass FrameBuffer(Wrapper):\n    def __init__(self, env, n_frames=4, dim_order=\'tensorflow\'):\n        """"""A gym wrapper that reshapes, crops and scales image into the desired shapes""""""\n        super(FrameBuffer, self).__init__(env)\n        self.dim_order = dim_order\n        if dim_order == \'tensorflow\':\n            height, width, n_channels = env.observation_space.shape\n            obs_shape = [height, width, n_channels * n_frames]\n        elif dim_order == \'pytorch\':\n            n_channels, height, width = env.observation_space.shape\n            obs_shape = [n_channels * n_frames, height, width]\n        else:\n            raise ValueError(\'dim_order should be ""tensorflow"" or ""pytorch"", got {}\'.format(dim_order))\n        self.observation_space = Box(0.0, 1.0, obs_shape)\n        self.framebuffer = np.zeros(obs_shape, \'float32\')\n        \n    def reset(self):\n        """"""resets breakout, returns initial frames""""""\n        self.framebuffer = np.zeros_like(self.framebuffer)\n        self.update_buffer(self.env.reset())\n        return self.framebuffer\n    \n    def step(self, action):\n        """"""plays breakout for 1 step, returns frame buffer""""""\n        new_img, reward, done, info = self.env.step(action)\n        self.update_buffer(new_img)\n        return self.framebuffer, reward, done, info\n    \n    def update_buffer(self, img):\n        if self.dim_order == \'tensorflow\':\n            offset = self.env.observation_space.shape[-1]\n            axis = -1\n            cropped_framebuffer = self.framebuffer[:,:,:-offset]\n        elif self.dim_order == \'pytorch\':\n            offset = self.env.observation_space.shape[0]\n            axis = 0\n            cropped_framebuffer = self.framebuffer[:-offset]\n        self.framebuffer = np.concatenate([img, cropped_framebuffer], axis = axis)\n'"
homeworks_advanced/Lab2_RL/replay_buffer.py,0,"b'# This code is shamelessly stolen from https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\nimport numpy as np\nimport random\n\nclass ReplayBuffer(object):\n    def __init__(self, size):\n        """"""Create Replay buffer.\n        Parameters\n        ----------\n        size: int\n            Max number of transitions to store in the buffer. When the buffer\n            overflows the old memories are dropped.\n        """"""\n        self._storage = []\n        self._maxsize = size\n        self._next_idx = 0\n\n    def __len__(self):\n        return len(self._storage)\n\n    def add(self, obs_t, action, reward, obs_tp1, done):\n        data = (obs_t, action, reward, obs_tp1, done)\n\n        if self._next_idx >= len(self._storage):\n            self._storage.append(data)\n        else:\n            self._storage[self._next_idx] = data\n        self._next_idx = (self._next_idx + 1) % self._maxsize\n\n    def _encode_sample(self, idxes):\n        obses_t, actions, rewards, obses_tp1, dones = [], [], [], [], []\n        for i in idxes:\n            data = self._storage[i]\n            obs_t, action, reward, obs_tp1, done = data\n            obses_t.append(np.array(obs_t, copy=False))\n            actions.append(np.array(action, copy=False))\n            rewards.append(reward)\n            obses_tp1.append(np.array(obs_tp1, copy=False))\n            dones.append(done)\n        return np.array(obses_t), np.array(actions), np.array(rewards), np.array(obses_tp1), np.array(dones)\n\n    def sample(self, batch_size):\n        """"""Sample a batch of experiences.\n        Parameters\n        ----------\n        batch_size: int\n            How many transitions to sample.\n        Returns\n        -------\n        obs_batch: np.array\n            batch of observations\n        act_batch: np.array\n            batch of actions executed given obs_batch\n        rew_batch: np.array\n            rewards received as results of executing act_batch\n        next_obs_batch: np.array\n            next set of observations seen after executing act_batch\n        done_mask: np.array\n            done_mask[i] = 1 if executing act_batch[i] resulted in\n            the end of an episode and 0 otherwise.\n        """"""\n        idxes = [random.randint(0, len(self._storage) - 1) for _ in range(batch_size)]\n        return self._encode_sample(idxes)\n'"
homeworks_advanced/Lab2_RL/utils.py,0,"b'import numpy as np\nimport psutil\nfrom scipy.signal import convolve, gaussian\nimport torch\nfrom torch import nn\nimport os\n\ndef get_cum_discounted_rewards(rewards, gamma):\n    """"""\n    evaluates cumulative discounted rewards:\n    r_t + gamma * r_{t+1} + gamma^2 * r_{t_2} + ...\n    """"""\n    cum_rewards = []\n    cum_rewards.append(rewards[-1])\n    for r in reversed(rewards[:-1]):\n        cum_rewards.insert(0, r + gamma * cum_rewards[0])\n    return cum_rewards\n\n\ndef play_and_log_episode(env, agent, gamma=0.99, t_max=10000):\n    """"""\n    always greedy\n    """"""\n    states = []\n    v_mc = []\n    v_agent = []\n    q_spreads = []\n    td_errors = []\n    rewards = []\n\n    s = env.reset()\n    for step in range(t_max):\n        states.append(s)\n        qvalues = agent.get_qvalues([s])\n        max_q_value, min_q_value = np.max(qvalues), np.min(qvalues)\n        v_agent.append(max_q_value)\n        q_spreads.append(max_q_value - min_q_value)\n        if step > 0:\n            td_errors.append(np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n        action = qvalues.argmax(axis=-1)[0]\n\n        s, r, done, _ = env.step(action)\n        rewards.append(r)\n        if done:\n            break\n    td_errors.append(np.abs(rewards[-1] + gamma * v_agent[-1] - v_agent[-2]))\n\n    v_mc = get_cum_discounted_rewards(rewards, gamma)\n\n    return_pack = {\n        \'states\': np.array(states),\n        \'v_mc\': np.array(v_mc),\n        \'v_agent\': np.array(v_agent),\n        \'q_spreads\': np.array(q_spreads),\n        \'td_errors\': np.array(td_errors),\n        \'rewards\': np.array(rewards),\n        \'episode_finished\': np.array(done)\n    }\n\n    return return_pack\n\n\ndef img_by_obs(obs, state_dim):\n    """"""\n    Unwraps obs by channels.\n    observation is of shape [c, h=w, w=h]\n    """"""\n    return obs.reshape([-1, state_dim[2]])\n\n\ndef is_enough_ram(min_available_gb = 0.1):\n    mem = psutil.virtual_memory()\n    return mem.available >= min_available_gb * (1024 ** 3)\n\n\ndef linear_decay(init_val, final_val, cur_step, total_steps):\n    if cur_step >= total_steps:\n        return final_val\n    return (init_val * (total_steps - cur_step) + final_val * cur_step) / total_steps\n\n\ndef smoothen(values):\n    kernel = gaussian(100, std=100)\n    # kernel = np.concatenate([np.arange(100), np.arange(99, -1, -1)])\n    kernel = kernel / np.sum(kernel)\n    return convolve(values, kernel, \'valid\')\n'"
homeworks_advanced/homework1_three_headed_network/network.py,2,"b'\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nimport tqdm\n\n\nclass ThreeInputsNet(nn.Module):\n    def __init__(self, n_tokens, n_cat_features, concat_number_of_features, hid_size=64):\n        super(ThreeInputsNet, self).__init__()\n        self.title_emb = nn.Embedding(n_tokens, embedding_dim=hid_size)\n        # <YOUR CODE HERE>        \n        \n        self.full_emb = nn.Embedding(num_embeddings=n_tokens, embedding_dim=hid_size)\n        # <YOUR CODE HERE>\n        \n        self.category_out = # <YOUR CODE HERE>\n\n\n        # Example for the final layers (after the concatenation)\n        self.inter_dense = nn.Linear(in_features=concat_number_of_features, out_features=hid_size*2)\n        self.final_dense = nn.Linear(in_features=hid_size*2, out_features=1)\n\n        \n\n    def forward(self, whole_input):\n        input1, input2, input3 = whole_input\n        title_beg = self.title_emb(input1).permute((0, 2, 1))\n        title = # <YOUR CODE HERE>\n        \n        full_beg = self.full_emb(input2).permute((0, 2, 1))\n        full = # <YOUR CODE HERE>        \n        \n        category = # <YOUR CODE HERE>        \n        \n        concatenated = torch.cat(\n            [\n            title.view(title.size(0), -1),\n            full.view(full.size(0), -1),\n            category.view(category.size(0), -1)\n            ],\n            dim=1)\n        \n        out = # <YOUR CODE HERE>\n        \n        return out'"
homeworks_advanced/homework2_attention_in_seq2seq/modules.py,2,"b'import random\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n        super().__init__()\n        \n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.hid_dim = hid_dim\n        self.n_layers = n_layers\n        \n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(p=dropout)\n        \n    def forward(self, src):\n        embedded = self.dropout(self.embedding(src))\n        output, (hidden, cell) = self.rnn(embedded)\n        return output, hidden, cell\n\n\nclass Attention(nn.Module):\n    def __init__(self, enc_hid_dim, dec_hid_dim):\n        super().__init__()\n        \n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        \n        self.attn = # <YOUR CODE HERE>\n        \n    def forward(self, hidden, encoder_outputs):\n        # <YOUR CODE HERE>\n        \n        return \n    \n    \nclass DecoderWithAttention(nn.Module):\n    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.attention = attention\n        \n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        \n        self.rnn = # <YOUR CODE HERE>\n        \n        self.out = # <YOUR CODE HERE>\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, input, hidden, encoder_outputs):\n        # <YOUR CODE HERE>\n        \n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n        \n        assert encoder.hid_dim == decoder.dec_hid_dim, \\\n            ""Hidden dimensions of encoder and decoder must be equal!""\n        \n    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n        \n        #src = [src sent len, batch size]\n        #trg = [trg sent len, batch size]\n        #teacher_forcing_ratio is probability to use teacher forcing\n        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n        \n        # Again, now batch is the first dimention instead of zero\n        batch_size = trg.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        \n        #tensor to store decoder outputs\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        \n        #last hidden state of the encoder is used as the initial hidden state of the decoder\n        enc_states, hidden, cell = self.encoder(src)\n        \n        #first input to the decoder is the <sos> tokens\n        input = trg[0,:]\n        \n        for t in range(1, max_len):\n\n            output, hidden = self.decoder(input, hidden, enc_states)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            input = (trg[t] if teacher_force else top1)\n        \n        return outputs'"
homeworks_basic/homework3_optional_your_own_nn/mnist.py,0,"b'import sys\nimport os\nimport time\n\nimport numpy as np\n\n__doc__=""""""taken from https://github.com/Lasagne/Lasagne/blob/master/examples/mnist.py""""""\n\ndef load_dataset(flatten=False):\n    # We first define a download function, supporting both Python 2 and 3.\n    if sys.version_info[0] == 2:\n        from urllib import urlretrieve\n    else:\n        from urllib.request import urlretrieve\n\n    def download(filename, source=\'http://yann.lecun.com/exdb/mnist/\'):\n        print(""Downloading %s"" % filename)\n        urlretrieve(source + filename, filename)\n\n    # We then define functions for loading MNIST images and labels.\n    # For convenience, they also download the requested files if needed.\n    import gzip\n\n    def load_mnist_images(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the inputs in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=16)\n        # The inputs are vectors now, we reshape them to monochrome 2D images,\n        # following the shape convention: (examples, channels, rows, columns)\n        data = data.reshape(-1, 1, 28, 28)\n        # The inputs come as bytes, we convert them to float32 in range [0,1].\n        # (Actually to range [0, 255/256], for compatibility to the version\n        # provided at http://deeplearning.net/data/mnist/mnist.pkl.gz.)\n        return (data / np.float32(256)).squeeze()\n\n    def load_mnist_labels(filename):\n        if not os.path.exists(filename):\n            download(filename)\n        # Read the labels in Yann LeCun\'s binary format.\n        with gzip.open(filename, \'rb\') as f:\n            data = np.frombuffer(f.read(), np.uint8, offset=8)\n        # The labels are vectors of integers now, that\'s exactly what we want.\n        return data\n\n    # We can now download and read the training and test set images and labels.\n    X_train = load_mnist_images(\'train-images-idx3-ubyte.gz\')\n    y_train = load_mnist_labels(\'train-labels-idx1-ubyte.gz\')\n    X_test = load_mnist_images(\'t10k-images-idx3-ubyte.gz\')\n    y_test = load_mnist_labels(\'t10k-labels-idx1-ubyte.gz\')\n\n    # We reserve the last 10000 training examples for validation.\n    X_train, X_val = X_train[:-10000], X_train[-10000:]\n    y_train, y_val = y_train[:-10000], y_train[-10000:]\n\n    if flatten:\n        X_train = X_train.reshape([-1, 28**2])\n        X_val = X_val.reshape([-1, 28**2])\n        X_test = X_test.reshape([-1, 28**2])\n\n\n    # We just return all the arrays in order, as expected in main().\n    # (It doesn\'t matter how we do this as long as we can read them again.)\n    return X_train, y_train, X_val, y_val, X_test, y_test\n\n\n\n\n'"
