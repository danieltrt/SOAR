file_path,api_count,code
NLP-attention/glove_model.py,2,"b""import os\n\nimport torch\nfrom torchtext import data\nfrom util_glove import get_args, makedirs\n\nargs = get_args()\ntorch.cuda.set_device(args.gpu)\ndevice = torch.device('cuda:{}'.format(args.gpu))\n\ninputs = data.Field(lower=args.lower, tokenize='spacy')\n"""
NLP-attention/models.py,10,"b""from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport os\n\n\nclass BiLSTM(nn.Module):\n\n    def __init__(self, config):\n        super(BiLSTM, self).__init__()\n        self.drop = nn.Dropout(config['dropout'])\n        self.encoder = nn.Embedding(config['ntoken'], config['ninp'])\n        self.bilstm = nn.LSTM(config['ninp'], config['nhid'], config['nlayers'], dropout=config['dropout'],\n                              bidirectional=True)\n        self.nlayers = config['nlayers']\n        self.nhid = config['nhid']\n        self.pooling = config['pooling']\n        self.dictionary = config['dictionary']\n#        self.init_weights()\n        self.encoder.weight.data[self.dictionary.word2idx['<pad>']] = 0\n        if os.path.exists(config['word-vector']):\n            print('Loading word vectors from', config['word-vector'])\n            vectors = torch.load(config['word-vector'])\n            #print(vectors[3], config['ninp'])\n            assert vectors[3] >= config['ninp']#2\n            vocab = vectors[1]#0\n            vectors = vectors[2]#1\n            loaded_cnt = 0\n            for word in self.dictionary.word2idx:\n                if word not in vocab:\n                    continue\n                real_id = self.dictionary.word2idx[word]\n                loaded_id = vocab[word]\n                self.encoder.weight.data[real_id] = vectors[loaded_id][:config['ninp']]\n                loaded_cnt += 1\n            print('%d words from external word vectors loaded.' % loaded_cnt)\n\n    # note: init_range constraints the value of initial weights\n    def init_weights(self, init_range=0.1):\n        self.encoder.weight.data.uniform_(-init_range, init_range)\n\n    def forward(self, inp, hidden):\n        emb = self.drop(self.encoder(inp))\n        outp = self.bilstm(emb, hidden)[0]\n        if self.pooling == 'mean':\n            outp = torch.mean(outp, 0).squeeze()\n        elif self.pooling == 'max':\n            outp = torch.max(outp, 0)[0].squeeze()\n        elif self.pooling == 'all' or self.pooling == 'all-word':\n            outp = torch.transpose(outp, 0, 1).contiguous()\n        return outp, emb\n\n    def init_hidden( self, bsz ):\n        # trick see https://discuss.pytorch.org/t/when-to-initialize-lstm-hidden-state/2323\n        weight = next(self.parameters())\n        # attention: hidden contains both hidden state and cell state\n        return (weight.new_zeros(self.nlayers*2, bsz, self.nhid),\n                weight.new_zeros(self.nlayers*2, bsz, self.nhid))\n    \n\nclass SelfAttentiveEncoder(nn.Module):\n\n    def __init__(self, config):\n        super(SelfAttentiveEncoder, self).__init__()\n        self.bilstm = BiLSTM(config)\n        self.drop = nn.Dropout(config['dropout'])\n        self.ws1 = nn.Linear(config['nhid'] * 2, config['attention-unit'], bias=False)\n        self.ws2 = nn.Linear(config['attention-unit'], config['attention-hops'], bias=False)\n        self.tanh = nn.Tanh()\n        self.softmax = nn.Softmax(dim=1)\n        self.dictionary = config['dictionary']\n#        self.init_weights()\n        self.attention_hops = config['attention-hops']\n\n    def init_weights(self, init_range=0.1):\n        self.ws1.weight.data.uniform_(-init_range, init_range)\n        self.ws2.weight.data.uniform_(-init_range, init_range)\n\n    def forward(self, inp, hidden):\n        outp = self.bilstm.forward(inp, hidden)[0]\n        size = outp.size()  # [bsz, len, nhid]\n        compressed_embeddings = outp.view(-1, size[2])  # [bsz*len, nhid*2]\n        transformed_inp = torch.transpose(inp, 0, 1).contiguous()  # [bsz, len]\n        transformed_inp = transformed_inp.view(size[0], 1, size[1])  # [bsz, 1, len]\n        concatenated_inp = [transformed_inp for i in range(self.attention_hops)]\n        concatenated_inp = torch.cat(concatenated_inp, 1)  # [bsz, hop, len]\n\n        hbar = self.tanh(self.ws1(self.drop(compressed_embeddings)))  # [bsz*len, attention-unit]\n        alphas = self.ws2(hbar).view(size[0], size[1], -1)  # [bsz, len, hop]\n        alphas = torch.transpose(alphas, 1, 2).contiguous()  # [bsz, hop, len]\n        penalized_alphas = alphas + (\n            -10000 * (concatenated_inp == self.dictionary.word2idx['<pad>']).float())\n            # [bsz, hop, len] + [bsz, hop, len]\n        alphas = self.softmax(penalized_alphas.view(-1, size[1]))  # [bsz*hop, len]\n        alphas = alphas.view(size[0], self.attention_hops, size[1])  # [bsz, hop, len]\n        return torch.bmm(alphas, outp), alphas\n\n    def init_hidden(self, bsz):\n        return self.bilstm.init_hidden(bsz)\n\n\nclass Classifier(nn.Module):\n\n    def __init__(self, config):\n        super(Classifier, self).__init__()\n        if config['pooling'] == 'mean' or config['pooling'] == 'max':\n            self.encoder = BiLSTM(config)\n            self.fc = nn.Linear(config['nhid'] * 2, config['nfc'])\n        elif config['pooling'] == 'all':\n            self.encoder = SelfAttentiveEncoder(config)\n            self.fc = nn.Linear(config['nhid'] * 2 * config['attention-hops'], config['nfc'])\n        else:\n            raise Exception('Error when initializing Classifier')\n        self.drop = nn.Dropout(config['dropout'])\n        self.tanh = nn.Tanh()\n        self.pred = nn.Linear(config['nfc'], config['class-number'])\n        self.dictionary = config['dictionary']\n#        self.init_weights()\n\n    def init_weights(self, init_range=0.1):\n        self.fc.weight.data.uniform_(-init_range, init_range)\n        self.fc.bias.data.fill_(0)\n        self.pred.weight.data.uniform_(-init_range, init_range)\n        self.pred.bias.data.fill_(0)\n\n    def forward(self, inp, hidden):\n        outp, attention = self.encoder.forward(inp, hidden)\n        outp = outp.view(outp.size(0), -1)\n        fc = self.tanh(self.fc(self.drop(outp)))\n        pred = self.pred(self.drop(fc))\n        out_bool = True\n        if type(self.encoder) == BiLSTM:\n            attention = None\n            out_bool = False\n        return pred, attention, out_bool \n\n    def init_hidden(self, bsz):\n        return self.encoder.init_hidden(bsz)\n\n    def encode(self, inp, hidden):\n        return self.encoder.forward(inp, hidden)[0]\n"""
NLP-attention/tokenizer-yelp.py,0,"b""from __future__ import print_function\nimport argparse\nimport json\nimport random\nfrom util import Dictionary\nimport spacy\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser('Tokenizer')\n    parser.add_argument('--input', type=str, default='', help='input file')\n    parser.add_argument('--output', type=str, default='', help='output file')\n    parser.add_argument('--dict', type=str, default='', help='dictionary file')\n    args = parser.parse_args()\n    tokenizer = spacy.load('en')\n    dictionary = Dictionary()\n    dictionary.add_word('<pad>')  # add padding word\n    with open(args.output, 'w') as fout:\n        lines = open(args.input, encoding='utf-8').readlines()\n        random.shuffle(lines)\n        for i, line in enumerate(lines):\n            item = json.loads(line)\n            words = tokenizer(' '.join(item['text'].split()))\n            data = {\n                'label': item['stars'] - 1,\n                'text': list(map(lambda x: x.text.lower(), words))\n            }\n            fout.write(json.dumps(data) + '\\n')\n            for item in data['text']:\n                dictionary.add_word(item)\n            if i % 100 == 99:\n                print('%d/%d files done, dictionary size: %d' %\n                      (i + 1, len(lines), len(dictionary)))\n        fout.close()\n\n    with open(args.dict, 'w') as fout:  # save dictionary for fast next process\n        fout.write(json.dumps(dictionary.idx2word) + '\\n')\n        fout.close()\n"""
NLP-attention/train.py,20,"b'from __future__ import print_function\nfrom models import *\n\nfrom util import Dictionary, get_args\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n#from torch.autograd import Variable\n\nimport json\nimport time\nimport random\nimport os\n\n\ndef Frobenius(mat):\n    size = mat.size()\n    if len(size) == 3:  # batched matrix\n        ret = (torch.sum(torch.sum((mat ** 2), 1), 1).squeeze() + 1e-10) ** 0.5\n        return torch.sum(ret) / size[0]\n    else:\n        raise Exception(\'matrix for computing Frobenius norm should be with 3 dims\')\n\n\ndef package(data):\n    """"""Package data for training / evaluation.""""""\n    data = list(map(lambda x: json.loads(x), data))\n    dat = list(map(lambda x: list(map(lambda y: dictionary.word2idx[y], x[\'text\'])), data))\n    maxlen = 0\n    for item in dat:\n        maxlen = max(maxlen, len(item))\n    targets = list(map(lambda x: x[\'label\'], data))\n    maxlen = min(maxlen, 500)\n    for i in range(len(data)):\n        if maxlen < len(dat[i]):\n            dat[i] = dat[i][:maxlen]\n        else:\n            for j in range(maxlen - len(dat[i])):\n                dat[i].append(dictionary.word2idx[\'<pad>\'])\n    dat = torch.LongTensor(dat)\n    targets = torch.LongTensor(targets)\n    return dat.t(), targets\n\n@torch.no_grad()\ndef evaluate():\n    """"""evaluate the model while training""""""\n    model.eval()  # turn on the eval() switch to disable dropout\n    total_loss = 0\n    total_correct = 0\n    for batch, i in enumerate(range(0, len(data_val), args.batch_size)):\n        data, targets = package(data_val[i:min(len(data_val), i+args.batch_size)])\n        if args.cuda:\n            data = data.cuda()\n            targets = targets.cuda()\n        hidden = model.init_hidden(data.size(1))\n        output, attention, _ = model.forward(data, hidden)\n        output_flat = output.view(data.size(1), -1)\n        total_loss += criterion(output_flat, targets).data\n        prediction = torch.max(output_flat, 1)[1]\n        total_correct += torch.sum((prediction == targets).float())\n    return total_loss.item() / (len(data_val) // args.batch_size), total_correct.data.item() / len(data_val)\n\n\ndef train(epoch_number):\n    global best_val_loss, best_acc\n    model.train()\n    total_loss = 0\n    total_pure_loss = 0  # without the penalization term\n    start_time = time.time()\n    for batch, i in enumerate(range(0, len(data_train), args.batch_size)):\n        data, targets = package(data_train[i:i+args.batch_size])\n        if args.cuda:\n            data = data.cuda()\n            targets = targets.cuda()\n        hidden = model.init_hidden(data.size(1))\n        output, attention, out_bool = model.forward(data, hidden)\n        loss = criterion(output.view(data.size(1), -1), targets)\n        total_pure_loss += loss.data.item()\n        if out_bool:  # add penalization term\n            attentionT = torch.transpose(attention, 1, 2).contiguous()\n            extra_loss = Frobenius(torch.bmm(attention, attentionT) - I[:attention.size(0)])\n            loss += args.penalization_coeff * extra_loss\n        optimizer.zero_grad()\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(model.parameters(), args.clip)\n        optimizer.step()\n\n        total_loss += loss.data.item()\n\n        if batch % args.log_interval == 0 and batch > 0:\n            elapsed = time.time() - start_time\n            print(\'| epoch {:3d} | {:5d}/{:5d} batches | ms/batch {:5.2f} | loss {:5.4f} | pure loss {:5.4f}\'.format(\n                  epoch_number, batch, len(data_train) // args.batch_size,\n                  elapsed * 1000 / args.log_interval, total_loss / args.log_interval,\n                  total_pure_loss / args.log_interval))\n            total_loss = 0\n            total_pure_loss = 0\n            start_time = time.time()\n\n#            for item in model.parameters():\n#                print item.size(), torch.sum(item.data ** 2), torch.sum(item.grad ** 2).data[0]\n#            print model.encoder.ws2.weight.grad.data\n#            exit()\n    evaluate_start_time = time.time()\n    val_loss, acc = evaluate()\n    print(\'-\' * 89)\n    fmt = \'| evaluation | time: {:5.2f}s | valid loss (pure) {:5.4f} | Acc {:8.4f}\'\n    print(fmt.format((time.time() - evaluate_start_time), val_loss, acc))\n    print(\'-\' * 89)\n    # Save the model, if the validation loss is the best we\'ve seen so far.\n    if not best_val_loss or val_loss < best_val_loss:\n        with open(args.save, \'wb\') as f:\n            torch.save(model, f)\n        f.close()\n        best_val_loss = val_loss\n    else:  # if loss doesn\'t go down, divide the learning rate by 5.\n        for param_group in optimizer.param_groups:\n            param_group[\'lr\'] = param_group[\'lr\'] * 0.2\n    if not best_acc or acc > best_acc:\n        with open(args.save[:-3]+\'.best_acc.pt\', \'wb\') as f:\n            torch.save(model, f)\n        f.close()\n        best_acc = acc\n    with open(args.save[:-3]+\'.epoch-{:02d}.pt\'.format(epoch_number), \'wb\') as f:\n        torch.save(model, f)\n    f.close()\n\n\nif __name__ == \'__main__\':\n    # parse the arguments\n    args = get_args()\n\n    # Set the random seed manually for reproducibility.\n    torch.manual_seed(args.seed)\n    if torch.cuda.is_available():\n        if not args.cuda:\n            print(""WARNING: You have a CUDA device, so you should probably run with --cuda"")\n        else:\n            torch.cuda.manual_seed(args.seed)\n    random.seed(args.seed)\n\n    # Load Dictionary\n    assert os.path.exists(args.train_data)\n    assert os.path.exists(args.val_data)\n    print(\'Begin to load the dictionary.\')\n    dictionary = Dictionary(path=args.dictionary)\n\n    best_val_loss = None\n    best_acc = None\n\n    n_token = len(dictionary)\n    \n    model = Classifier({\n        \'dropout\': args.dropout,\n        \'ntoken\': n_token,\n        \'nlayers\': args.nlayers,\n        \'nhid\': args.nhid,\n        \'ninp\': args.emsize,\n        \'pooling\': \'all\',\n        \'attention-unit\': args.attention_unit,\n        \'attention-hops\': args.attention_hops,\n        \'nfc\': args.nfc,\n        \'dictionary\': dictionary,\n        \'word-vector\': args.word_vector,\n        \'class-number\': args.class_number\n    })\n    if args.cuda:\n        model = model.cuda()\n\n    I = torch.zeros(args.batch_size, args.attention_hops, args.attention_hops)\n    for i in range(args.batch_size):\n        for j in range(args.attention_hops):\n            I.data[i][j][j] = 1\n    if args.cuda:\n        I = I.cuda()\n\n    criterion = nn.CrossEntropyLoss()\n    if args.optimizer == \'Adam\':\n        optimizer = optim.Adam(model.parameters(), lr=args.lr, betas=[0.9, 0.999], eps=1e-8, weight_decay=0)\n    elif args.optimizer == \'SGD\':\n        optimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=0.9, weight_decay=0.01)\n    else:\n        raise Exception(\'For other optimizers, please add it yourself. \'\n                        \'supported ones are: SGD and Adam.\')\n    print(\'Begin to load data.\')\n    data_train = open(args.train_data).readlines()\n    data_val = open(args.val_data).readlines()\n    try:\n        for epoch in range(args.epochs):\n            train(epoch)\n    except KeyboardInterrupt:\n        print(\'-\' * 89)\n        print(\'Exit from training early.\')\n        data_val = open(args.test_data).readlines()\n        evaluate_start_time = time.time()\n        test_loss, acc = evaluate()\n        print(\'-\' * 89)\n        fmt = \'| test | time: {:5.2f}s | test loss (pure) {:5.4f} | Acc {:8.4f}\'\n        print(fmt.format((time.time() - evaluate_start_time), test_loss, acc))\n        print(\'-\' * 89)\n        exit(0)\n'"
NLP-attention/util.py,0,"b""import json\nimport argparse\n\n\nclass Dictionary(object):\n    def __init__(self, path=''):\n        self.word2idx = dict()\n        self.idx2word = list()\n        if path != '':  # load an external dictionary\n            words = json.loads(open(path, 'r').readline())\n            for item in words:\n                self.add_word(item)\n\n    def add_word(self, word):\n        if word not in self.word2idx:\n            self.idx2word.append(word)\n            self.word2idx[word] = len(self.idx2word) - 1\n        return self.word2idx[word]\n\n    def __len__(self):\n        return len(self.idx2word)\n\n\ndef get_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--emsize', type=int, default=300,\n                        help='size of word embeddings')\n    parser.add_argument('--nhid', type=int, default=300,\n                        help='number of hidden units per layer')\n    parser.add_argument('--nlayers', type=int, default=2,\n                        help='number of layers in BiLSTM')\n    parser.add_argument('--attention-unit', type=int, default=350,\n                        help='number of attention unit')\n    parser.add_argument('--attention-hops', type=int, default=1,\n                        help='number of attention hops, for multi-hop attention model')\n    parser.add_argument('--dropout', type=float, default=0.5,\n                        help='dropout applied to layers (0 = no dropout)')\n    parser.add_argument('--clip', type=float, default=0.5,\n                        help='clip to prevent the too large grad in LSTM')\n    parser.add_argument('--nfc', type=int, default=512,\n                        help='hidden (fully connected) layer size for classifier MLP')\n    parser.add_argument('--lr', type=float, default=.001,\n                        help='initial learning rate')\n    parser.add_argument('--epochs', type=int, default=40,\n                        help='upper epoch limit')\n    parser.add_argument('--seed', type=int, default=1111,\n                        help='random seed')\n    parser.add_argument('--cuda', action='store_true',\n                        help='use CUDA')\n    parser.add_argument('--log-interval', type=int, default=200, metavar='N',\n                        help='report interval')\n    parser.add_argument('--save', type=str, default='',\n                        help='path to save the final model')\n    parser.add_argument('--dictionary', type=str, default='',\n                        help='path to save the dictionary, for faster corpus loading')\n    parser.add_argument('--word-vector', type=str, default='',\n                        help='path for pre-trained word vectors (e.g. GloVe), should be a PyTorch model.')\n    parser.add_argument('--train-data', type=str, default='',\n                        help='location of the training data, should be a json file')\n    parser.add_argument('--val-data', type=str, default='',\n                        help='location of the development data, should be a json file')\n    parser.add_argument('--test-data', type=str, default='',\n                        help='location of the test data, should be a json file')\n    parser.add_argument('--batch-size', type=int, default=32,\n                        help='batch size for training')\n    parser.add_argument('--class-number', type=int, default=2,\n                        help='number of classes')\n    parser.add_argument('--optimizer', type=str, default='Adam',\n                        help='type of optimizer')\n    parser.add_argument('--penalization-coeff', type=float, default=1, \n                        help='the penalization coefficient')\n    return parser.parse_args()\n\n"""
NLP-attention/util_glove.py,0,"b'# from https://github.com/pytorch/examples/tree/master/snli\nimport os\n\nfrom argparse import ArgumentParser\n\ndef makedirs(name):\n    """"""helper function for python 2 and 3 to call os.makedirs()\n       avoiding an error if the directory to be created already exists""""""\n\n    import os, errno\n\n    try:\n        os.makedirs(name)\n    except OSError as ex:\n        if ex.errno == errno.EEXIST and os.path.isdir(name):\n            # ignore existing directory\n            pass\n        else:\n            # a different error happened\n            raise\n\ndef get_args():\n    parser = ArgumentParser(description=\'Glove with PyTorch\')\n    parser.add_argument(\'--preserve-case\', action=\'store_false\', dest=\'lower\')\n    parser.add_argument(\'--gpu\', type=int, default=0)\n    parser.add_argument(\'--vector_cache\', type=str, default=os.path.join(os.getcwd(), \'.vector_cache/input_vectors.pt\'))\n    parser.add_argument(\'--word_vectors\', type=str, default=\'glove.6B.100d\')\n    args = parser.parse_args()\nreturn args'"
Notebooks/NLPModel.py,9,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nimport torch.optim as optim\n\nimport torch_utils; \nfrom torch_utils import gpu, minibatch_sentences, shuffle_sentences, accuracy_one\nfrom nlp_models import FirstModel\n\nclass SentimentModel(object):\n\n\tdef __init__(self, embedding_dim = 30, n_iter = 2, batch_size = 64,\n\t\tlearning_rate = 1e-3, \n\t\tnet = None, loss = None, use_cuda=False,\n\t\tvocab_size = 1, seq_len = 1):\n\t\tself._embedding_dim = embedding_dim\n\t\tself._n_iter = n_iter\n\t\tself._batch_size = batch_size\n\t\tself._learning_rate = learning_rate\n\t\tself._use_cuda = use_cuda\n\t\t\n\t\tif net != None:\n\t\t\tself._net = gpu(net,use_cuda)\n\t\telse:\n\t\t\tself._net = None\n\t\tself._loss = loss\n\t\tself._optimizer = None\n\t\tself._vocab_size = vocab_size\n\t\tself._seq_len = seq_len\n\n\n\tdef _initialize(self):\n\t\tif self._net is None:\n\t\t\tself._net = gpu(FirstModel(self._embedding_dim,self._vocab_size,self._seq_len),self._use_cuda)\n\t\t\t\n\t\tself._optimizer = optim.Adam(self._net.parameters(),lr=self._learning_rate, weight_decay=0)\n\t\t\t\t\n\t\tif self._loss is None:\n\t\t\tself._loss = torch.nn.BCELoss()\n\n\t@property\n\tdef _initialized(self):\n\n\t\treturn self._optimizer is not None\n\n\n\tdef fit(self, word_ids, sentiment, word_ids_test, sentiment_test, verbose=True):\n\n\t\tword_ids = word_ids.astype(np.int64)\n\t\tword_ids_test = word_ids_test.astype(np.int64)\n\t\t\t\t\n\t\tif not self._initialized:\n\t\t\tself._initialize()\n\n\t\tself._net.train(True)\n\n\t\tfor epoch_num in range(self._n_iter):\n\n\t\t\twords, sents = shuffle_sentences(word_ids,np.asarray(sentiment).astype(np.float32))\n\t\t\tword_ids_tensor = gpu(torch.from_numpy(words), self._use_cuda)\n\t\t\tsent_tensor = gpu(torch.from_numpy(sents), self._use_cuda)\n\t\t\tepoch_loss = 0.0\n\t\t\tepoch_acc = 0.0\n\t\t\tfor (minibatch_num, (batch_word, batch_sent)) in enumerate(minibatch_sentences(self._batch_size, word_ids_tensor, sent_tensor)):\n\t\t\t\tword_var = Variable(batch_word)\n\t\t\t\tsent_var = Variable(batch_sent.unsqueeze(1),requires_grad=False)\n\t\t\t\tpredictions = self._net(word_var)\n\t\t\t\t#print(sent_var.size())\n\t\t\t\tpreds = accuracy_one(predictions.data).view(-1,1)\n\t\t\t\t\n\t\t\t\tself._optimizer.zero_grad()\n\t\t\t\t\n\t\t\t\tloss = self._loss(predictions, sent_var)\n\t\t\t\t\n\t\t\t\tepoch_loss += loss.data.item()\n\t\t\t\tepoch_acc += torch.sum(preds != sent_var.data.byte()).data.item()/float(sent_var.size(0))\n\n\t\t\t\tloss.backward()\n\t\t\t\t\n\t\t\t\tself._optimizer.step()\n\t\t\t#print(epoch_acc)\n\t\t\t#print(sent_var.size())\n\t\t\t\t\n\t\t\tepoch_loss = epoch_loss / (minibatch_num + 1)\n\t\t\tepoch_acc = epoch_acc/ float(minibatch_num + 1)\n\n\t\t\tif verbose:\n\t\t\t\tval_loss, val_acc = self.test(word_ids_test, sentiment_test)\n\t\t\t\t#val_loss = 0\n\t\t\t\t#val_acc = 0\n\t\t\t\tprint('Epoch {}: train loss {}'.format(epoch_num, epoch_loss), \n\t\t\t\t\t'train acc', epoch_acc,\n\t\t\t\t\t'validation loss', val_loss,\n\t\t\t\t\t'validation acc', val_acc)\n\t\t\t\tself._net.train(True)\n\n\n\tdef test(self, word_ids, sentiment):\n\t\tself._net.train(False)\n\t\tword_ids = word_ids.astype(np.int64)\n\t\t\n\t\tword_ids_tensor = gpu(torch.from_numpy(word_ids), self._use_cuda)\n\t\tsent_tensor = gpu(torch.from_numpy(np.asarray(sentiment).astype(np.float32)), self._use_cuda)\n\t\tepoch_loss = 0.0\n\t\tepoch_acc = 0.0\n\t\tfor (minibatch_num, (batch_word, batch_sent)) in enumerate(minibatch_sentences(self._batch_size, word_ids_tensor, sent_tensor)):\n\t\t\tword_var = Variable(batch_word)\n\t\t\tsent_var = Variable(batch_sent.unsqueeze(1),requires_grad=False)\n\t\t\tpredictions = self._net(word_var)\n\t\t\tpreds = accuracy_one(predictions.data).view(-1,1)\n\t\t\tloss = self._loss(predictions, sent_var)\n\t\t\tepoch_loss = epoch_loss + loss.data.item()\n\t\t\tepoch_acc += torch.sum(preds != sent_var.data.byte()).data.item()/float(sent_var.size(0))\n\t\tepoch_loss = epoch_loss / (minibatch_num + 1)\n\t\tepoch_acc = epoch_acc / float(minibatch_num + 1)\n\n\t\treturn epoch_loss, epoch_acc \n\n\n\t\t\t\t\t\t\t\t\t\t\t\t\n"""
Notebooks/nlp_models.py,3,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch_utils import ScaledEmbedding\n\nclass FirstModel(nn.Module):\n    \n    def __init__(self,\n                 embedding_dim=30,vocab_size = 1,seq_len = 1):\n        \n        super(FirstModel, self).__init__()\n        \n        self._seq_len = seq_len\n        self._embedding_dim = embedding_dim\n\n        #self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.embeddings = ScaledEmbedding(vocab_size, embedding_dim)\n        self.fc1 = nn.Linear(seq_len*embedding_dim,100)\n        self.fc2 = nn.Linear(100,1)\n        \n    def forward(self, words_id):\n        \n        words_embedding = self.embeddings(words_id).view(-1,self._seq_len*self._embedding_dim)\n        words_embedding = words_embedding.squeeze()\n        x = F.relu(self.fc1(words_embedding))\n        x = self.fc2(F.dropout(x,0.7))\n        return F.sigmoid(x)\n\n\nclass ConvModel(nn.Module):\n\n    def __init__(self,embedding_dim = 30, vocab_size = 1, seq_len = 1):\n\n        super(ConvModel,self).__init__()\n\n        self._seq_len = seq_len\n        self._embedding_dim = embedding_dim\n        self.embeddings = ScaledEmbedding(vocab_size, embedding_dim)\n        self.conv = nn.Conv1d(embedding_dim,64,5,padding=2)\n        self.fc1 = nn.Linear(32*seq_len,100)\n        self.mp = nn.MaxPool1d(2)\n        self.fc2 = nn.Linear(100,1)\n\n    def forward(self,words_id):\n        words_embedding = self.embeddings(words_id).permute(0,2,1)\n        x = F.dropout(words_embedding,0.2)\n        x = F.relu(self.conv(x))\n        x = self.mp(F.dropout(x,0.2))\n        x = x.view(-1,self._seq_len*32)\n        x = F.dropout(self.fc1(x),0.7)\n        return F.sigmoid(self.fc2(x))\n\n\n\n\n        \n'"
Notebooks/torch_utils.py,3,"b'import numpy as np\n\nimport torch\nimport torch.nn as nn\n\n\ndef gpu(tensor, gpu=False):\n\n    if gpu:\n        return tensor.cuda()\n    else:\n        return tensor\n\n\ndef accuracy_one(x):\n    \n    return x[:,0] < 0.5\n\n\ndef shuffle_sentences(word,sent):\n\n    random_state = np.random.RandomState()\n    shuffle_indices = np.arange(len(sent))\n    random_state.shuffle(shuffle_indices)\n    \n    return tuple([word[shuffle_indices,:], sent[shuffle_indices]])\n\n\ndef shuffle(*arrays):\n\n    random_state = np.random.RandomState()\n    shuffle_indices = np.arange(len(arrays[0]))\n    random_state.shuffle(shuffle_indices)\n\n    if len(arrays) == 1:\n        return arrays[0][shuffle_indices]\n    else:\n        return tuple(x[shuffle_indices] for x in arrays)\n\ndef minibatch_sentences(batch_size, word, sent):\n    for i in range(0, len(sent), batch_size):\n        yield tuple([word[i:i+batch_size,:], sent[i:i+batch_size]])\n\ndef minibatch(batch_size, *tensors):\n\n    if len(tensors) == 1:\n        tensor = tensors[0]\n        for i in range(0, len(tensor), batch_size):\n            yield tensor[i:i + batch_size]\n    else:\n        for i in range(0, len(tensors[0]), batch_size):\n            yield tuple(x[i:i + batch_size] for x in tensors)\n\n\ndef regression_loss(observed_ratings, predicted_ratings):\n    return ((observed_ratings - predicted_ratings) ** 2).mean()\n\n\ndef l1_loss(observed_ratings, predicted_ratings):\n    return (torch.abs(observed_ratings-predicted_ratings)).mean()\n\n\ndef hinge_loss(positive_predictions, negative_predictions):\n    loss = torch.clamp(negative_predictions -\n                       positive_predictions +\n                       1.0, 0.0)\n    return loss.mean()\n\n\nclass ScaledEmbedding(nn.Embedding):\n    """"""\n    Embedding layer that initialises its values\n    to using a normal variable scaled by the inverse\n    of the emedding dimension.\n    """"""\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters.\n        """"""\n\n        self.weight.data.normal_(0, 1.0 / self.embedding_dim)\n        if self.padding_idx is not None:\n            self.weight.data[self.padding_idx].fill_(0)\n\n\nclass ZeroEmbedding(nn.Embedding):\n    """"""\n    Used for biases.\n    """"""\n\n    def reset_parameters(self):\n        """"""\n        Initialize parameters.\n        """"""\n\n        self.weight.data.zero_()\n        if self.padding_idx is not None:\n            self.weight.data[self.padding_idx].fill_(0)\n\n\ndef sample_items(num_items, shape, random_state=None):\n    """"""\n    Randomly sample a number of items.\n\n    Parameters\n    ----------\n\n    num_items: int\n        Total number of items from which we should sample:\n        the maximum value of a sampled item id will be smaller\n        than this.\n    shape: int or tuple of ints\n        Shape of the sampled array.\n    random_state: np.random.RandomState instance, optional\n        Random state to use for sampling.\n\n    Returns\n    -------\n\n    items: np.array of shape [shape]\n        Sampled item ids.\n    """"""\n\n    if random_state is None:\n        random_state = np.random.RandomState()\n\n    items = random_state.randint(0, num_items, shape)\n\n    return items\n'"
Notebooks/utils.py,0,"b""import torchvision\nfrom torchvision import models,transforms,datasets\nimport torch\nimport os\nimport bcolz\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\ndef save_array(fname, arr):\n    c=bcolz.carray(arr, rootdir=fname, mode='w')\n    c.flush()\ndef load_array(fname):\n    return bcolz.open(fname)[:]\n\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\nprep1 = transforms.Compose([\n                transforms.CenterCrop(224),\n                #transforms.RandomSizedCrop(224),\n                #transforms.RandomHorizontalFlip(),\n                transforms.ToTensor(),\n                normalize,\n            ])\n\ndef imshow(inp, title=None):\n#   Imshow for Tensor.\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# shuffle only to get nice pictures\ndef shuffle_valtrain(x):\n    if x == 'train':\n        return True\n    else:\n        return False\n\ndef var_cgpu(x,use_gpu):\n    if use_gpu:\n        return x.cuda()\n    else:\n        return x\n\n\n"""
