file_path,api_count,code
cifar.py,1,"b'import torch.nn.functional as F\nfrom homura import optim, lr_scheduler, callbacks, reporters\nfrom homura.trainers import SupervisedTrainer as Trainer\nfrom homura.vision.data.loaders import cifar10_loaders\n\nfrom senet.baseline import resnet20\nfrom senet.se_resnet import se_resnet20\n\n\ndef main():\n    train_loader, test_loader = cifar10_loaders(args.batch_size)\n\n    if args.baseline:\n        model = resnet20()\n    else:\n        model = se_resnet20(num_classes=10, reduction=args.reduction)\n    optimizer = optim.SGD(lr=1e-1, momentum=0.9, weight_decay=1e-4)\n    scheduler = lr_scheduler.StepLR(80, 0.1)\n    tqdm_rep = reporters.TQDMReporter(range(args.epochs))\n    _callbacks = [tqdm_rep, callbacks.AccuracyCallback()]\n    with Trainer(model, optimizer, F.cross_entropy, scheduler=scheduler, callbacks=_callbacks) as trainer:\n        for _ in tqdm_rep:\n            trainer.train(train_loader)\n            trainer.test(test_loader)\n\n\nif __name__ == \'__main__\':\n    import argparse\n\n    p = argparse.ArgumentParser()\n    p.add_argument(""--epochs"", type=int, default=200)\n    p.add_argument(""--batch_size"", type=int, default=64)\n    p.add_argument(""--reduction"", type=int, default=16)\n    p.add_argument(""--baseline"", action=""store_true"")\n    args = p.parse_args()\n    main()\n'"
hubconf.py,0,"b'dependencies = [""torch"", ""math""]\n\n\ndef se_resnet20(**kwargs):\n    from senet.se_resnet import se_resnet20 as _se_resnet20\n\n    return _se_resnet20(**kwargs)\n\n\ndef se_resnet56(**kwargs):\n    from senet.se_resnet import se_resnet56 as _se_resnet56\n\n    return _se_resnet56(**kwargs)\n\n\ndef se_resnet50(**kwargs):\n    from senet.se_resnet import se_resnet50 as _se_resnet50\n\n    return _se_resnet50(**kwargs)\n\n\ndef se_resnet101(**kwargs):\n    from senet.se_resnet import se_resnet101 as _se_resnet101\n\n    return _se_resnet101(**kwargs)\n'"
imagenet.py,3,"b'import torch\nfrom homura import optim, lr_scheduler, callbacks, reporters, init_distributed\nfrom homura.trainers import SupervisedTrainer \nfrom homura.vision.data import imagenet_loaders\nfrom torch.nn import functional as F\n\nfrom senet.se_resnet import se_resnet50\n\n\ndef main():\n    if args.distributed:\n        init_distributed()\n\n    model = se_resnet50(num_classes=1000)\n\n    optimizer = optim.SGD(lr=0.6 / 1024 * args.batch_size,\n                          momentum=0.9, weight_decay=1e-4)\n    scheduler = lr_scheduler.MultiStepLR([50, 70])\n    train_loader, test_loader = imagenet_loaders(args.root, args.batch_size, distributed=args.distributed,\n                                                 num_train_samples=args.batch_size * 10 if args.debug else None,\n                                                 num_test_samples=args.batch_size * 10 if args.debug else None)\n\n    c = [callbacks.AccuracyCallback(), callbacks.AccuracyCallback(k=5),\n         callbacks.LossCallback(),\n         callbacks.WeightSave(\'.\'),\n         reporters.TensorboardReporter(\'.\'),\n         reporters.TQDMReporter(range(args.epochs))]\n\n    with SupervisedTrainer(model, optimizer, F.cross_entropy,\n                           callbacks=c,\n                           scheduler=scheduler,\n                           ) as trainer:\n        for _ in c[-1]:\n            trainer.train(train_loader)\n            trainer.test(test_loader)\n\n\nif __name__ == \'__main__\':\n    import miniargs\n    import warnings\n\n    warnings.filterwarnings(\n        ""ignore"", ""(Possibly )?corrupt EXIF data"", UserWarning)\n\n    p = miniargs.ArgumentParser()\n    p.add_str(""root"")\n    p.add_int(""--epochs"", default=90)\n    p.add_int(""--batch_size"", default=128)\n    p.add_true(""--distributed"")\n    p.add_int(""--local_rank"", default=-1)\n    p.add_true(""--debug"", help=""Use less images and less epochs"")\n    args, _else = p.parse(return_unknown=True)\n    num_device = torch.cuda.device_count()\n\n    print(args)\n    if args.distributed and args.local_rank == -1:\n        raise RuntimeError(\n            f""For distributed training, use python -m torch.distributed.launch ""\n            f""--nproc_per_node={num_device} {__file__} {args.root} ..."")\n    main()\n'"
senet/__init__.py,0,b''
senet/baseline.py,1,"b'""""""\nResNet for CIFAR dataset proposed in He+15, p 7. and\nhttps://github.com/facebook/fb.resnet.torch/blob/master/models/resnet.lua\n""""""\n\nimport torch.nn as nn\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    ""3x3 convolution with padding""\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(planes))\n        else:\n            self.downsample = lambda x: x\n        self.stride = stride\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass PreActBasicBlock(BasicBlock):\n    def __init__(self, inplanes, planes, stride):\n        super(PreActBasicBlock, self).__init__(inplanes, planes, stride)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False))\n        else:\n            self.downsample = lambda x: x\n        self.bn1 = nn.BatchNorm2d(inplanes)\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.bn1(x)\n        out = self.relu(out)\n        out = self.conv1(out)\n\n        out = self.bn2(out)\n        out = self.relu(out)\n        out = self.conv2(out)\n\n        out += residual\n\n        return out\n\n\nclass ResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=10):\n        super(ResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, blocks=n_size, stride=1)\n        self.layer2 = self._make_layer(block, 32, blocks=n_size, stride=2)\n        self.layer3 = self._make_layer(block, 64, blocks=n_size, stride=2)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n\n        self.initialize()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride):\n\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inplane, planes, stride))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass PreActResNet(ResNet):\n    def __init__(self, block, n_size, num_classes=10):\n        super(PreActResNet, self).__init__(block, n_size, num_classes)\n\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.initialize()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef resnet20(**kwargs):\n    model = ResNet(BasicBlock, 3, **kwargs)\n    return model\n\n\ndef resnet32(**kwargs):\n    model = ResNet(BasicBlock, 5, **kwargs)\n    return model\n\n\ndef resnet56(**kwargs):\n    model = ResNet(BasicBlock, 9, **kwargs)\n    return model\n\n\ndef resnet110(**kwargs):\n    model = ResNet(BasicBlock, 18, **kwargs)\n    return model\n\n\ndef preact_resnet20(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 3, **kwargs)\n    return model\n\n\ndef preact_resnet32(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 5, **kwargs)\n    return model\n\n\ndef preact_resnet56(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 9, **kwargs)\n    return model\n\n\ndef preact_resnet110(**kwargs):\n    model = PreActResNet(PreActBasicBlock, 18, **kwargs)\n    return model\n'"
senet/se_inception.py,0,"b'from senet.se_module import SELayer\nfrom torch import nn\nfrom torchvision.models.inception import Inception3\n\n\nclass SEInception3(nn.Module):\n    def __init__(self, num_classes, aux_logits=True, transform_input=False):\n        super(SEInception3, self).__init__()\n        model = Inception3(num_classes=num_classes, aux_logits=aux_logits,\n                           transform_input=transform_input)\n        model.Mixed_5b.add_module(""SELayer"", SELayer(192))\n        model.Mixed_5c.add_module(""SELayer"", SELayer(256))\n        model.Mixed_5d.add_module(""SELayer"", SELayer(288))\n        model.Mixed_6a.add_module(""SELayer"", SELayer(288))\n        model.Mixed_6b.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6c.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6d.add_module(""SELayer"", SELayer(768))\n        model.Mixed_6e.add_module(""SELayer"", SELayer(768))\n        if aux_logits:\n            model.AuxLogits.add_module(""SELayer"", SELayer(768))\n        model.Mixed_7a.add_module(""SELayer"", SELayer(768))\n        model.Mixed_7b.add_module(""SELayer"", SELayer(1280))\n        model.Mixed_7c.add_module(""SELayer"", SELayer(2048))\n\n        self.model = model\n\n    def forward(self, x):\n        _, _, h, w = x.size()\n        if (h, w) != (299, 299):\n            raise ValueError(""input size must be (299, 299)"")\n\n        return self.model(x)\n\n\ndef se_inception_v3(**kwargs):\n    return SEInception3(**kwargs)\n'"
senet/se_module.py,0,"b'from torch import nn\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction, bias=False),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y.expand_as(x)\n'"
senet/se_resnet.py,2,"b'import torch.nn as nn\nfrom torch.hub import load_state_dict_from_url\nfrom torchvision.models import ResNet\nfrom senet.se_module import SELayer\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None,\n                 *, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n                 base_width=64, dilation=1, norm_layer=None,\n                 *, reduction=16):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes * 4, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef se_resnet18(num_classes=1_000):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet34(num_classes=1_000):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet50(num_classes=1_000, pretrained=False):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    if pretrained:\n        model.load_state_dict(load_state_dict_from_url(\n            ""https://github.com/moskomule/senet.pytorch/releases/download/archive/seresnet50-60a8950a85b2b.pkl""))\n    return model\n\n\ndef se_resnet101(num_classes=1_000):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet152(num_classes=1_000):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\nclass CifarSEBasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, reduction=16):\n        super(CifarSEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(planes))\n        else:\n            self.downsample = lambda x: x\n        self.stride = stride\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass CifarSEResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(\n            3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(\n            block, 16, blocks=n_size, stride=1, reduction=reduction)\n        self.layer2 = self._make_layer(\n            block, 32, blocks=n_size, stride=2, reduction=reduction)\n        self.layer3 = self._make_layer(\n            block, 64, blocks=n_size, stride=2, reduction=reduction)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n        self.initialize()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride, reduction):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inplane, planes, stride, reduction))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\nclass CifarSEPreActResNet(CifarSEResNet):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEPreActResNet, self).__init__(\n            block, n_size, num_classes, reduction)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.initialize()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n\ndef se_resnet20(**kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 3, **kwargs)\n    return model\n\n\ndef se_resnet32(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 5, **kwargs)\n    return model\n\n\ndef se_resnet56(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEResNet(CifarSEBasicBlock, 9, **kwargs)\n    return model\n\n\ndef se_preactresnet20(**kwargs):\n    """"""Constructs a ResNet-18 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 3, **kwargs)\n    return model\n\n\ndef se_preactresnet32(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 5, **kwargs)\n    return model\n\n\ndef se_preactresnet56(**kwargs):\n    """"""Constructs a ResNet-34 model.\n\n    """"""\n    model = CifarSEPreActResNet(CifarSEBasicBlock, 9, **kwargs)\n    return model\n'"
