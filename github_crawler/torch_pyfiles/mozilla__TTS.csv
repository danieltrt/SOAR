file_path,api_count,code
__init__.py,0,b''
compute_statistics.py,0,"b'#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\nimport os\nimport argparse\n\nimport numpy as np\nfrom tqdm import tqdm\n\nfrom TTS.datasets.preprocess import load_meta_data\nfrom TTS.utils.io import load_config\nfrom TTS.utils.audio import AudioProcessor\n\ndef main():\n    """"""Run preprocessing process.""""""\n    parser = argparse.ArgumentParser(\n        description=""Compute mean and variance of spectrogtram features."")\n    parser.add_argument(""--config_path"", type=str, required=True,\n                        help=""TTS config file path."")\n    parser.add_argument(""--out_path"", default=None, type=str,\n                        help=""directory to save the output file."")\n    args = parser.parse_args()\n\n    # load config\n    CONFIG = load_config(args.config_path)\n    CONFIG.audio[\'signal_norm\'] = False  # do not apply earlier normalization\n    CONFIG.audio[\'stats_path\'] = None  # discard pre-defined stats\n\n    # load audio processor\n    ap = AudioProcessor(**CONFIG.audio)\n\n    # load the meta data of target dataset\n    dataset_items = load_meta_data(CONFIG.datasets)[0]  # take only train data\n    print(f"" > There are {len(dataset_items)} files."")\n\n    mel_sum = 0\n    mel_square_sum = 0\n    linear_sum = 0\n    linear_square_sum = 0\n    N = 0\n    for item in tqdm(dataset_items):\n        # compute features\n        wav = ap.load_wav(item[1])\n        linear = ap.spectrogram(wav)\n        mel = ap.melspectrogram(wav)\n\n        # compute stats\n        N += mel.shape[1]\n        mel_sum += mel.sum(1)\n        linear_sum += linear.sum(1)\n        mel_square_sum += (mel ** 2).sum(axis=1)\n        linear_square_sum += (linear ** 2).sum(axis=1)\n\n    mel_mean = mel_sum / N\n    mel_scale = np.sqrt(mel_square_sum / N - mel_mean ** 2)\n    linear_mean = linear_sum / N\n    linear_scale = np.sqrt(linear_square_sum / N - linear_mean ** 2)\n\n    output_file_path = os.path.join(args.out_path, ""scale_stats.npy"")\n    stats = {}\n    stats[\'mel_mean\'] = mel_mean\n    stats[\'mel_std\'] = mel_scale\n    stats[\'linear_mean\'] = linear_mean\n    stats[\'linear_std\'] = linear_scale\n\n    # set default config values for mean-var scaling\n    CONFIG.audio[\'stats_path\'] = output_file_path\n    CONFIG.audio[\'signal_norm\'] = True\n    # remove redundant values\n    del CONFIG.audio[\'max_norm\']\n    del CONFIG.audio[\'min_level_db\']\n    del CONFIG.audio[\'symmetric_norm\']\n    del CONFIG.audio[\'clip_norm\']\n    stats[\'audio_config\'] = CONFIG.audio\n    np.save(output_file_path, stats, allow_pickle=True)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
distribute.py,9,"b'# edited from https://github.com/fastai/imagenet-fast/blob/master/imagenet_nv/distributed.py\nimport os, sys\nimport math\nimport time\nimport subprocess\nimport argparse\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.sampler import Sampler\nfrom torch.autograd import Variable\nfrom torch._utils import _flatten_dense_tensors, _unflatten_dense_tensors\nfrom TTS.utils.generic_utils import create_experiment_folder\n\n\nclass DistributedSampler(Sampler):\n    """"""\n    Non shuffling Distributed Sampler\n    """"""\n\n    def __init__(self, dataset, num_replicas=None, rank=None):\n        super(DistributedSampler, self).__init__(dataset)\n        if num_replicas is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            num_replicas = dist.get_world_size()\n        if rank is None:\n            if not dist.is_available():\n                raise RuntimeError(""Requires distributed package to be available"")\n            rank = dist.get_rank()\n        self.dataset = dataset\n        self.num_replicas = num_replicas\n        self.rank = rank\n        self.epoch = 0\n        self.num_samples = int(math.ceil(len(self.dataset) * 1.0 / self.num_replicas))\n        self.total_size = self.num_samples * self.num_replicas\n\n    def __iter__(self):\n        indices = torch.arange(len(self.dataset)).tolist()\n\n        # add extra samples to make it evenly divisible\n        indices += indices[:(self.total_size - len(indices))]\n        assert len(indices) == self.total_size\n\n        # subsample\n        indices = indices[self.rank:self.total_size:self.num_replicas]\n        assert len(indices) == self.num_samples\n\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def set_epoch(self, epoch):\n        self.epoch = epoch\n\n\ndef reduce_tensor(tensor, num_gpus):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= num_gpus\n    return rt\n\n\ndef init_distributed(rank, num_gpus, group_name, dist_backend, dist_url):\n    assert torch.cuda.is_available(), ""Distributed mode requires CUDA.""\n\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n\n    # Initialize distributed communication\n    dist.init_process_group(\n        dist_backend,\n        init_method=dist_url,\n        world_size=num_gpus,\n        rank=rank,\n        group_name=group_name)\n\n\ndef apply_gradient_allreduce(module):\n\n    # sync model parameters\n    for p in module.state_dict().values():\n        if not torch.is_tensor(p):\n            continue\n        dist.broadcast(p, 0)\n\n    def allreduce_params():\n        if module.needs_reduction:\n            module.needs_reduction = False\n            # bucketing params based on value types\n            buckets = {}\n            for param in module.parameters():\n                if param.requires_grad and param.grad is not None:\n                    tp = type(param.data)\n                    if tp not in buckets:\n                        buckets[tp] = []\n                    buckets[tp].append(param)\n            for tp in buckets:\n                bucket = buckets[tp]\n                grads = [param.grad.data for param in bucket]\n                coalesced = _flatten_dense_tensors(grads)\n                dist.all_reduce(coalesced, op=dist.reduce_op.SUM)\n                coalesced /= dist.get_world_size()\n                for buf, synced in zip(\n                        grads, _unflatten_dense_tensors(coalesced, grads)):\n                    buf.copy_(synced)\n\n    for param in list(module.parameters()):\n\n        def allreduce_hook(*_):\n            Variable._execution_engine.queue_callback(allreduce_params)\n\n        if param.requires_grad:\n            param.register_hook(allreduce_hook)\n\n    def set_needs_reduction(self, *_):\n        self.needs_reduction = True\n\n    module.register_forward_hook(set_needs_reduction)\n    return module\n\n\ndef main():\n    """"""\n    Call train.py as a new process and pass command arguments\n    """"""\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--continue_path\',\n        type=str,\n        help=\'Training output folder to continue training. Use to continue a training. If it is used, ""config_path"" is ignored.\',\n        default=\'\',\n        required=\'--config_path\' not in sys.argv)\n    parser.add_argument(\n        \'--restore_path\',\n        type=str,\n        help=\'Model file to be restored. Use to finetune a model.\',\n        default=\'\')\n    parser.add_argument(\n        \'--config_path\',\n        type=str,\n        help=\'Path to config file for training.\',\n        required=\'--continue_path\' not in sys.argv\n    )\n    args = parser.parse_args()\n\n    # OUT_PATH = create_experiment_folder(CONFIG.output_path, CONFIG.run_name,\n                                        # True)\n    # stdout_path = os.path.join(OUT_PATH, ""process_stdout/"")\n\n    num_gpus = torch.cuda.device_count()\n    group_id = time.strftime(""%Y_%m_%d-%H%M%S"")\n\n    # set arguments for train.py\n    command = [\'train.py\']\n    command.append(\'--continue_path={}\'.format(args.continue_path))\n    command.append(\'--restore_path={}\'.format(args.restore_path))\n    command.append(\'--config_path={}\'.format(args.config_path))\n    command.append(\'--group_id=group_{}\'.format(group_id))\n    command.append(\'\')\n\n    # run processes\n    processes = []\n    for i in range(num_gpus):\n        my_env = os.environ.copy()\n        my_env[""PYTHON_EGG_CACHE""] = ""/tmp/tmp{}"".format(i)\n        command[-1] = \'--rank={}\'.format(i)\n        stdout = None if i == 0 else open(os.devnull, \'w\')\n        p = subprocess.Popen([\'python3\'] + command, stdout=stdout, env=my_env)\n        processes.append(p)\n        print(command)\n\n    for p in processes:\n        p.wait()\n\n\nif __name__ == \'__main__\':\n    main()\n'"
setup.py,0,"b'#!/usr/bin/env python\n\nimport argparse\nimport os\nimport shutil\nimport subprocess\nimport sys\n\nfrom setuptools import setup, find_packages\nimport setuptools.command.develop\nimport setuptools.command.build_py\n\n\nparser = argparse.ArgumentParser(add_help=False, allow_abbrev=False)\nparser.add_argument(\'--checkpoint\', type=str, help=\'Path to checkpoint file to embed in wheel.\')\nparser.add_argument(\'--model_config\', type=str, help=\'Path to model configuration file to embed in wheel.\')\nargs, unknown_args = parser.parse_known_args()\n\n# Remove our arguments from argv so that setuptools doesn\'t see them\nsys.argv = [sys.argv[0]] + unknown_args\n\nversion = \'0.0.3\'\n\n# Adapted from https://github.com/pytorch/pytorch\ncwd = os.path.dirname(os.path.abspath(__file__))\nif os.getenv(\'TTS_PYTORCH_BUILD_VERSION\'):\n    version = os.getenv(\'TTS_PYTORCH_BUILD_VERSION\')\nelse:\n    try:\n        sha = subprocess.check_output(\n            [\'git\', \'rev-parse\', \'HEAD\'], cwd=cwd).decode(\'ascii\').strip()\n        version += \'+\' + sha[:7]\n    except subprocess.CalledProcessError:\n        pass\n    except IOError:  # FileNotFoundError for python 3\n        pass\n\n\nclass build_py(setuptools.command.build_py.build_py):\n    def run(self):\n        self.create_version_file()\n        setuptools.command.build_py.build_py.run(self)\n\n    @staticmethod\n    def create_version_file():\n        print(\'-- Building version \' + version)\n        version_path = os.path.join(cwd, \'version.py\')\n        with open(version_path, \'w\') as f:\n            f.write(""__version__ = \'{}\'\\n"".format(version))\n\n\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        build_py.create_version_file()\n        setuptools.command.develop.develop.run(self)\n\n\n# The documentation for this feature is in server/README.md\npackage_data = [\'server/templates/*\']\n\nif \'bdist_wheel\' in unknown_args and args.checkpoint and args.model_config:\n    print(\'Embedding model in wheel file...\')\n    model_dir = os.path.join(\'server\', \'model\')\n    tts_dir = os.path.join(model_dir, \'tts\')\n    os.makedirs(tts_dir, exist_ok=True)\n    embedded_checkpoint_path = os.path.join(tts_dir, \'checkpoint.pth.tar\')\n    shutil.copy(args.checkpoint, embedded_checkpoint_path)\n    embedded_config_path = os.path.join(tts_dir, \'config.json\')\n    shutil.copy(args.model_config, embedded_config_path)\n    package_data.extend([embedded_checkpoint_path, embedded_config_path])\n\nsetup(\n    name=\'TTS\',\n    version=version,\n    url=\'https://github.com/mozilla/TTS\',\n    description=\'Text to Speech with Deep Learning\',\n    license=\'MPL-2.0\',\n    package_dir={\'\': \'tts_namespace\'},\n    packages=find_packages(\'tts_namespace\'),\n    package_data={\n        \'TTS\': package_data,\n    },\n    project_urls={\n        \'Documentation\': \'https://github.com/mozilla/TTS/wiki\',\n        \'Tracker\': \'https://github.com/mozilla/TTS/issues\',\n        \'Repository\': \'https://github.com/mozilla/TTS\',\n        \'Discussions\': \'https://discourse.mozilla.org/c/tts\',\n    },\n    cmdclass={\n        \'build_py\': build_py,\n        \'develop\': develop,\n    },\n    install_requires=[\n        ""scipy>=0.19.0"",\n        ""torch>=1.5"",\n        ""numpy>=1.16.0"",\n        ""librosa==0.6.2"",\n        ""unidecode==0.4.20"",\n        ""attrdict"",\n        ""tensorboardX"",\n        ""matplotlib"",\n        ""Pillow"",\n        ""flask"",\n        # ""lws"",\n        ""tqdm"",\n        ""bokeh==1.4.0"",\n        ""soundfile"",\n        ""phonemizer @ https://github.com/bootphon/phonemizer/tarball/master"",\n    ],\n    dependency_links=[\n        ""http://github.com/bootphon/phonemizer/tarball/master#egg=phonemizer-1.0.1""\n    ]\n)\n'"
synthesize.py,3,"b'# pylint: disable=redefined-outer-name, unused-argument\nimport os\nimport time\nimport argparse\nimport torch\nimport json\nimport string\n\nfrom TTS.utils.synthesis import synthesis\nfrom TTS.utils.generic_utils import setup_model\nfrom TTS.utils.io import load_config\nfrom TTS.utils.text.symbols import make_symbols, symbols, phonemes\nfrom TTS.utils.audio import AudioProcessor\n\n\ndef tts(model,\n        vocoder_model,\n        C,\n        VC,\n        text,\n        ap,\n        ap_vocoder,\n        use_cuda,\n        batched_vocoder,\n        speaker_id=None,\n        figures=False):\n    t_1 = time.time()\n    use_vocoder_model = vocoder_model is not None\n    waveform, alignment, _, postnet_output, stop_tokens, _ = synthesis(\n        model, text, C, use_cuda, ap, speaker_id, style_wav=False,\n        truncated=False, enable_eos_bos_chars=C.enable_eos_bos_chars,\n        use_griffin_lim=(not use_vocoder_model), do_trim_silence=True)\n\n    if C.model == ""Tacotron"" and use_vocoder_model:\n        postnet_output = ap.out_linear_to_mel(postnet_output.T).T\n    # correct if there is a scale difference b/w two models\n    if use_vocoder_model:\n        postnet_output = ap._denormalize(postnet_output)\n        postnet_output = ap_vocoder._normalize(postnet_output)\n        vocoder_input = torch.FloatTensor(postnet_output.T).unsqueeze(0)\n        waveform = vocoder_model.generate(\n            vocoder_input.cuda() if use_cuda else vocoder_input,\n            batched=batched_vocoder,\n            target=8000,\n            overlap=400)\n    print("" >  Run-time: {}"".format(time.time() - t_1))\n    return alignment, postnet_output, stop_tokens, waveform\n\n\nif __name__ == ""__main__"":\n\n    global symbols, phonemes\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'text\', type=str, help=\'Text to generate speech.\')\n    parser.add_argument(\'config_path\',\n                        type=str,\n                        help=\'Path to model config file.\')\n    parser.add_argument(\n        \'model_path\',\n        type=str,\n        help=\'Path to model file.\',\n    )\n    parser.add_argument(\n        \'out_path\',\n        type=str,\n        help=\'Path to save final wav file. Wav file will be names as the text given.\',\n    )\n    parser.add_argument(\'--use_cuda\',\n                        type=bool,\n                        help=\'Run model on CUDA.\',\n                        default=False)\n    parser.add_argument(\n        \'--vocoder_path\',\n        type=str,\n        help=\n        \'Path to vocoder model file. If it is not defined, model uses GL as vocoder. Please make sure that you installed vocoder library before (WaveRNN).\',\n        default="""",\n    )\n    parser.add_argument(\'--vocoder_config_path\',\n                        type=str,\n                        help=\'Path to vocoder model config file.\',\n                        default="""")\n    parser.add_argument(\n        \'--batched_vocoder\',\n        type=bool,\n        help=""If True, vocoder model uses faster batch processing."",\n        default=True)\n    parser.add_argument(\'--speakers_json\',\n                        type=str,\n                        help=""JSON file for multi-speaker model."",\n                        default="""")\n    parser.add_argument(\n        \'--speaker_id\',\n        type=int,\n        help=""target speaker_id if the model is multi-speaker."",\n        default=None)\n    args = parser.parse_args()\n\n    if args.vocoder_path != """":\n        assert args.use_cuda, "" [!] Enable cuda for vocoder.""\n        from WaveRNN.models.wavernn import Model as VocoderModel\n\n    # load the config\n    C = load_config(args.config_path)\n    C.forward_attn_mask = True\n\n    # load the audio processor\n    ap = AudioProcessor(**C.audio)\n\n    # if the vocabulary was passed, replace the default\n    if \'characters\' in C.keys():\n        symbols, phonemes = make_symbols(**C.characters)\n\n    # load speakers\n    if args.speakers_json != \'\':\n        speakers = json.load(open(args.speakers_json, \'r\'))\n        num_speakers = len(speakers)\n    else:\n        num_speakers = 0\n\n    # load the model\n    num_chars = len(phonemes) if C.use_phonemes else len(symbols)\n    model = setup_model(num_chars, num_speakers, C)\n    cp = torch.load(args.model_path)\n    model.load_state_dict(cp[\'model\'])\n    model.eval()\n    if args.use_cuda:\n        model.cuda()\n    model.decoder.set_r(cp[\'r\'])\n\n    # load vocoder model\n    if args.vocoder_path != """":\n        VC = load_config(args.vocoder_config_path)\n        ap_vocoder = AudioProcessor(**VC.audio)\n        bits = 10\n        vocoder_model = VocoderModel(rnn_dims=512,\n                                     fc_dims=512,\n                                     mode=VC.mode,\n                                     mulaw=VC.mulaw,\n                                     pad=VC.pad,\n                                     upsample_factors=VC.upsample_factors,\n                                     feat_dims=VC.audio[""num_mels""],\n                                     compute_dims=128,\n                                     res_out_dims=128,\n                                     res_blocks=10,\n                                     hop_length=ap.hop_length,\n                                     sample_rate=ap.sample_rate,\n                                     use_aux_net=True,\n                                     use_upsample_net=True)\n\n        check = torch.load(args.vocoder_path)\n        vocoder_model.load_state_dict(check[\'model\'])\n        vocoder_model.eval()\n        if args.use_cuda:\n            vocoder_model.cuda()\n    else:\n        vocoder_model = None\n        VC = None\n        ap_vocoder = None\n\n    # synthesize voice\n    print("" > Text: {}"".format(args.text))\n    _, _, _, wav = tts(model,\n                       vocoder_model,\n                       C,\n                       VC,\n                       args.text,\n                       ap,\n                       ap_vocoder,\n                       args.use_cuda,\n                       args.batched_vocoder,\n                       speaker_id=args.speaker_id,\n                       figures=False)\n\n    # save the results\n    file_name = args.text.replace("" "", ""_"")\n    file_name = file_name.translate(\n        str.maketrans(\'\', \'\', string.punctuation.replace(\'_\', \'\'))) + \'.wav\'\n    out_path = os.path.join(args.out_path, file_name)\n    print("" > Saving output to {}"".format(out_path))\n    ap.save_wav(wav, out_path)\n'"
train.py,11,"b'import argparse\nimport os\nimport sys\nimport glob\nimport time\nimport traceback\n\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom TTS.datasets.TTSDataset import MyDataset\nfrom distribute import (DistributedSampler, apply_gradient_allreduce,\n                        init_distributed, reduce_tensor)\nfrom TTS.layers.losses import TacotronLoss\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.generic_utils import (count_parameters, create_experiment_folder, remove_experiment_folder,\n                                     get_git_branch, set_init_dict,\n                                     setup_model, KeepAverage, check_config)\nfrom TTS.utils.io import (save_best_model, save_checkpoint,\n                          load_config, copy_config_file)\nfrom TTS.utils.training import (NoamLR, check_update, adam_weight_decay,\n                                gradual_training_scheduler, set_weight_decay)\nfrom TTS.utils.tensorboard_logger import TensorboardLogger\nfrom TTS.utils.console_logger import ConsoleLogger\nfrom TTS.utils.speakers import load_speaker_mapping, save_speaker_mapping, \\\n    get_speakers\nfrom TTS.utils.synthesis import synthesis\nfrom TTS.utils.text.symbols import make_symbols, phonemes, symbols\nfrom TTS.utils.visual import plot_alignment, plot_spectrogram\nfrom TTS.datasets.preprocess import load_meta_data\nfrom TTS.utils.radam import RAdam\nfrom TTS.utils.measures import alignment_diagonal_score\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = False\ntorch.manual_seed(54321)\nuse_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint("" > Using CUDA: "", use_cuda)\nprint("" > Number of GPUs: "", num_gpus)\n\n\ndef setup_loader(ap, r, is_val=False, verbose=False):\n    if is_val and not c.run_eval:\n        loader = None\n    else:\n        dataset = MyDataset(\n            r,\n            c.text_cleaner,\n            compute_linear_spec=True if c.model.lower() == \'tacotron\' else False,\n            meta_data=meta_data_eval if is_val else meta_data_train,\n            ap=ap,\n            tp=c.characters if \'characters\' in c.keys() else None,\n            batch_group_size=0 if is_val else c.batch_group_size *\n            c.batch_size,\n            min_seq_len=c.min_seq_len,\n            max_seq_len=c.max_seq_len,\n            phoneme_cache_path=c.phoneme_cache_path,\n            use_phonemes=c.use_phonemes,\n            phoneme_language=c.phoneme_language,\n            enable_eos_bos=c.enable_eos_bos_chars,\n            verbose=verbose)\n        sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        loader = DataLoader(\n            dataset,\n            batch_size=c.eval_batch_size if is_val else c.batch_size,\n            shuffle=False,\n            collate_fn=dataset.collate_fn,\n            drop_last=False,\n            sampler=sampler,\n            num_workers=c.num_val_loader_workers\n            if is_val else c.num_loader_workers,\n            pin_memory=False)\n    return loader\n\n\ndef format_data(data):\n    if c.use_speaker_embedding:\n        speaker_mapping = load_speaker_mapping(OUT_PATH)\n\n    # setup input data\n    text_input = data[0]\n    text_lengths = data[1]\n    speaker_names = data[2]\n    linear_input = data[3] if c.model in [""Tacotron""] else None\n    mel_input = data[4]\n    mel_lengths = data[5]\n    stop_targets = data[6]\n    avg_text_length = torch.mean(text_lengths.float())\n    avg_spec_length = torch.mean(mel_lengths.float())\n\n    if c.use_speaker_embedding:\n        speaker_ids = [\n            speaker_mapping[speaker_name] for speaker_name in speaker_names\n        ]\n        speaker_ids = torch.LongTensor(speaker_ids)\n    else:\n        speaker_ids = None\n\n    # set stop targets view, we predict a single stop token per iteration.\n    stop_targets = stop_targets.view(text_input.shape[0],\n                                     stop_targets.size(1) // c.r, -1)\n    stop_targets = (stop_targets.sum(2) >\n                    0.0).unsqueeze(2).float().squeeze(2)\n\n    # dispatch data to GPU\n    if use_cuda:\n        text_input = text_input.cuda(non_blocking=True)\n        text_lengths = text_lengths.cuda(non_blocking=True)\n        mel_input = mel_input.cuda(non_blocking=True)\n        mel_lengths = mel_lengths.cuda(non_blocking=True)\n        linear_input = linear_input.cuda(non_blocking=True) if c.model in [""Tacotron""] else None\n        stop_targets = stop_targets.cuda(non_blocking=True)\n        if speaker_ids is not None:\n            speaker_ids = speaker_ids.cuda(non_blocking=True)\n    return text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, avg_text_length, avg_spec_length\n\n\ndef train(model, criterion, optimizer, optimizer_st, scheduler,\n          ap, global_step, epoch):\n    data_loader = setup_loader(ap, model.decoder.r, is_val=False,\n                               verbose=(epoch == 0))\n    model.train()\n    epoch_time = 0\n    train_values = {\n        \'avg_postnet_loss\': 0,\n        \'avg_decoder_loss\': 0,\n        \'avg_stopnet_loss\': 0,\n        \'avg_align_error\': 0,\n        \'avg_step_time\': 0,\n        \'avg_loader_time\': 0\n    }\n    if c.bidirectional_decoder:\n        train_values[\'avg_decoder_b_loss\'] = 0  # decoder backward loss\n        train_values[\'avg_decoder_c_loss\'] = 0  # decoder consistency loss\n    if c.ga_alpha > 0:\n        train_values[\'avg_ga_loss\'] = 0  # guidede attention loss\n    keep_avg = KeepAverage()\n    keep_avg.add_values(train_values)\n    if use_cuda:\n        batch_n_iter = int(\n            len(data_loader.dataset) / (c.batch_size * num_gpus))\n    else:\n        batch_n_iter = int(len(data_loader.dataset) / c.batch_size)\n    end_time = time.time()\n    c_logger.print_train_start()\n    for num_iter, data in enumerate(data_loader):\n        start_time = time.time()\n\n        # format data\n        text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, avg_text_length, avg_spec_length = format_data(data)\n        loader_time = time.time() - end_time\n\n        global_step += 1\n\n        # setup lr\n        if c.noam_schedule:\n            scheduler.step()\n        optimizer.zero_grad()\n        if optimizer_st:\n            optimizer_st.zero_grad()\n\n        # forward pass model\n        if c.bidirectional_decoder:\n            decoder_output, postnet_output, alignments, stop_tokens, decoder_backward_output, alignments_backward = model(\n                text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\n        else:\n            decoder_output, postnet_output, alignments, stop_tokens = model(\n                text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\n            decoder_backward_output = None\n\n        # set the alignment lengths wrt reduction factor for guided attention\n        if mel_lengths.max() % model.decoder.r != 0:\n            alignment_lengths = (mel_lengths + (model.decoder.r - (mel_lengths.max() % model.decoder.r))) // model.decoder.r\n        else:\n            alignment_lengths = mel_lengths //  model.decoder.r\n\n        # compute loss\n        loss_dict = criterion(postnet_output, decoder_output, mel_input,\n                              linear_input, stop_tokens, stop_targets,\n                              mel_lengths, decoder_backward_output,\n                              alignments, alignment_lengths, text_lengths)\n        if c.bidirectional_decoder:\n            keep_avg.update_values({\'avg_decoder_b_loss\': loss_dict[\'decoder_backward_loss\'].item(),\n                                    \'avg_decoder_c_loss\': loss_dict[\'decoder_c_loss\'].item()})\n        if c.ga_alpha > 0:\n            keep_avg.update_values({\'avg_ga_loss\': loss_dict[\'ga_loss\'].item()})\n\n        # backward pass\n        loss_dict[\'loss\'].backward()\n        optimizer, current_lr = adam_weight_decay(optimizer)\n        grad_norm, _ = check_update(model, c.grad_clip, ignore_stopnet=True)\n        optimizer.step()\n\n        # compute alignment error (the lower the better )\n        align_error = 1 - alignment_diagonal_score(alignments)\n        keep_avg.update_value(\'avg_align_error\', align_error)\n        loss_dict[\'align_error\'] = align_error\n\n        # backpass and check the grad norm for stop loss\n        if c.separate_stopnet:\n            loss_dict[\'stopnet_loss\'].backward()\n            optimizer_st, _ = adam_weight_decay(optimizer_st)\n            grad_norm_st, _ = check_update(model.decoder.stopnet, 1.0)\n            optimizer_st.step()\n        else:\n            grad_norm_st = 0\n\n        step_time = time.time() - start_time\n        epoch_time += step_time\n\n        # update avg stats\n        update_train_values = {\n            \'avg_postnet_loss\': float(loss_dict[\'postnet_loss\'].item()),\n            \'avg_decoder_loss\': float(loss_dict[\'decoder_loss\'].item()),\n            \'avg_stopnet_loss\': loss_dict[\'stopnet_loss\'].item() \\\n                if isinstance(loss_dict[\'stopnet_loss\'], float) else float(loss_dict[\'stopnet_loss\'].item()),\n            \'avg_step_time\': step_time,\n            \'avg_loader_time\': loader_time\n        }\n        keep_avg.update_values(update_train_values)\n\n        if global_step % c.print_step == 0:\n            c_logger.print_train_step(batch_n_iter, num_iter, global_step,\n                                      avg_spec_length, avg_text_length,\n                                      step_time, loader_time, current_lr,\n                                      loss_dict, keep_avg.avg_values)\n\n        # aggregate losses from processes\n        if num_gpus > 1:\n            loss_dict[\'postnet_loss\'] = reduce_tensor(loss_dict[\'postnet_loss\'].data, num_gpus)\n            loss_dict[\'decoder_loss\'] = reduce_tensor(loss_dict[\'decoder_loss\'].data, num_gpus)\n            loss_dict[\'loss\'] = reduce_tensor(loss_dict[\'loss\'] .data, num_gpus)\n            loss_dict[\'stopnet_loss\'] = reduce_tensor(loss_dict[\'stopnet_loss\'].data, num_gpus) if c.stopnet else loss_dict[\'stopnet_loss\']\n\n        if args.rank == 0:\n            # Plot Training Iter Stats\n            # reduce TB load\n            if global_step % 10 == 0:\n                iter_stats = {\n                    ""loss_posnet"": loss_dict[\'postnet_loss\'].item(),\n                    ""loss_decoder"": loss_dict[\'decoder_loss\'].item(),\n                    ""lr"": current_lr,\n                    ""grad_norm"": grad_norm,\n                    ""grad_norm_st"": grad_norm_st,\n                    ""step_time"": step_time\n                }\n                tb_logger.tb_train_iter_stats(global_step, iter_stats)\n\n            if global_step % c.save_step == 0:\n                if c.checkpoint:\n                    # save model\n                    save_checkpoint(model, optimizer, global_step, epoch, model.decoder.r, OUT_PATH,\n                                    optimizer_st=optimizer_st,\n                                    model_loss=loss_dict[\'postnet_loss\'].item())\n\n                # Diagnostic visualizations\n                const_spec = postnet_output[0].data.cpu().numpy()\n                gt_spec = linear_input[0].data.cpu().numpy() if c.model in [\n                    ""Tacotron"", ""TacotronGST""\n                ] else mel_input[0].data.cpu().numpy()\n                align_img = alignments[0].data.cpu().numpy()\n\n                figures = {\n                    ""prediction"": plot_spectrogram(const_spec, ap),\n                    ""ground_truth"": plot_spectrogram(gt_spec, ap),\n                    ""alignment"": plot_alignment(align_img),\n                }\n\n                if c.bidirectional_decoder:\n                    figures[""alignment_backward""] = plot_alignment(alignments_backward[0].data.cpu().numpy())\n\n                tb_logger.tb_train_figures(global_step, figures)\n\n                # Sample audio\n                if c.model in [""Tacotron"", ""TacotronGST""]:\n                    train_audio = ap.inv_spectrogram(const_spec.T)\n                else:\n                    train_audio = ap.inv_melspectrogram(const_spec.T)\n                tb_logger.tb_train_audios(global_step,\n                                          {\'TrainAudio\': train_audio},\n                                          c.audio[""sample_rate""])\n        end_time = time.time()\n\n    # print epoch stats\n    c_logger.print_train_epoch_end(global_step, epoch, epoch_time, keep_avg)\n\n    # Plot Epoch Stats\n    if args.rank == 0:\n        # Plot Training Epoch Stats\n        epoch_stats = {\n            ""loss_postnet"": keep_avg[\'avg_postnet_loss\'],\n            ""loss_decoder"": keep_avg[\'avg_decoder_loss\'],\n            ""stopnet_loss"": keep_avg[\'avg_stopnet_loss\'],\n            ""alignment_score"": keep_avg[\'avg_align_error\'],\n            ""epoch_time"": epoch_time\n        }\n        if c.ga_alpha > 0:\n            epoch_stats[\'guided_attention_loss\'] = keep_avg[\'avg_ga_loss\']\n        tb_logger.tb_train_epoch_stats(global_step, epoch_stats)\n        if c.tb_model_param_stats:\n            tb_logger.tb_model_weights(model, global_step)\n    return keep_avg.avg_values, global_step\n\n\n@torch.no_grad()\ndef evaluate(model, criterion, ap, global_step, epoch):\n    data_loader = setup_loader(ap, model.decoder.r, is_val=True)\n    model.eval()\n    epoch_time = 0\n    eval_values_dict = {\n        \'avg_postnet_loss\': 0,\n        \'avg_decoder_loss\': 0,\n        \'avg_stopnet_loss\': 0,\n        \'avg_align_error\': 0\n    }\n    if c.bidirectional_decoder:\n        eval_values_dict[\'avg_decoder_b_loss\'] = 0  # decoder backward loss\n        eval_values_dict[\'avg_decoder_c_loss\'] = 0  # decoder consistency loss\n    if c.ga_alpha > 0:\n        eval_values_dict[\'avg_ga_loss\'] = 0  # guidede attention loss\n    keep_avg = KeepAverage()\n    keep_avg.add_values(eval_values_dict)\n\n    c_logger.print_eval_start()\n    if data_loader is not None:\n        for num_iter, data in enumerate(data_loader):\n            start_time = time.time()\n\n            # format data\n            text_input, text_lengths, mel_input, mel_lengths, linear_input, stop_targets, speaker_ids, _, _ = format_data(data)\n            assert mel_input.shape[1] % model.decoder.r == 0\n\n            # forward pass model\n            if c.bidirectional_decoder:\n                decoder_output, postnet_output, alignments, stop_tokens, decoder_backward_output, alignments_backward = model(\n                    text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\n            else:\n                decoder_output, postnet_output, alignments, stop_tokens = model(\n                    text_input, text_lengths, mel_input, speaker_ids=speaker_ids)\n                decoder_backward_output = None\n\n            # set the alignment lengths wrt reduction factor for guided attention\n            if mel_lengths.max() % model.decoder.r != 0:\n                alignment_lengths = (mel_lengths + (model.decoder.r - (mel_lengths.max() % model.decoder.r))) // model.decoder.r\n            else:\n                alignment_lengths = mel_lengths //  model.decoder.r\n\n            # compute loss\n            loss_dict = criterion(postnet_output, decoder_output, mel_input,\n                                  linear_input, stop_tokens, stop_targets,\n                                  mel_lengths, decoder_backward_output,\n                                  alignments, alignment_lengths, text_lengths)\n            if c.bidirectional_decoder:\n                keep_avg.update_values({\'avg_decoder_b_loss\': loss_dict[\'decoder_b_loss\'].item(),\n                                        \'avg_decoder_c_loss\': loss_dict[\'decoder_c_loss\'].item()})\n            if c.ga_alpha > 0:\n                keep_avg.update_values({\'avg_ga_loss\': loss_dict[\'ga_loss\'].item()})\n\n            # step time\n            step_time = time.time() - start_time\n            epoch_time += step_time\n\n            # compute alignment score\n            align_error = 1 - alignment_diagonal_score(alignments)\n            keep_avg.update_value(\'avg_align_error\', align_error)\n\n            # aggregate losses from processes\n            if num_gpus > 1:\n                loss_dict[\'postnet_loss\'] = reduce_tensor(loss_dict[\'postnet_loss\'].data, num_gpus)\n                loss_dict[\'decoder_loss\'] = reduce_tensor(loss_dict[\'decoder_loss\'].data, num_gpus)\n                if c.stopnet:\n                    loss_dict[\'stopnet_loss\'] = reduce_tensor(loss_dict[\'stopnet_loss\'].data, num_gpus)\n\n            keep_avg.update_values({\n                \'avg_postnet_loss\':\n                float(loss_dict[\'postnet_loss\'].item()),\n                \'avg_decoder_loss\':\n                float(loss_dict[\'decoder_loss\'].item()),\n                \'avg_stopnet_loss\':\n                float(loss_dict[\'stopnet_loss\'].item()),\n            })\n\n            if c.print_eval:\n                c_logger.print_eval_step(num_iter, loss_dict, keep_avg.avg_values)\n\n        if args.rank == 0:\n            # Diagnostic visualizations\n            idx = np.random.randint(mel_input.shape[0])\n            const_spec = postnet_output[idx].data.cpu().numpy()\n            gt_spec = linear_input[idx].data.cpu().numpy() if c.model in [\n                ""Tacotron"", ""TacotronGST""\n            ] else mel_input[idx].data.cpu().numpy()\n            align_img = alignments[idx].data.cpu().numpy()\n\n            eval_figures = {\n                ""prediction"": plot_spectrogram(const_spec, ap),\n                ""ground_truth"": plot_spectrogram(gt_spec, ap),\n                ""alignment"": plot_alignment(align_img)\n            }\n\n            # Sample audio\n            if c.model in [""Tacotron"", ""TacotronGST""]:\n                eval_audio = ap.inv_spectrogram(const_spec.T)\n            else:\n                eval_audio = ap.inv_melspectrogram(const_spec.T)\n            tb_logger.tb_eval_audios(global_step, {""ValAudio"": eval_audio},\n                                     c.audio[""sample_rate""])\n\n            # Plot Validation Stats\n            epoch_stats = {\n                ""loss_postnet"": keep_avg[\'avg_postnet_loss\'],\n                ""loss_decoder"": keep_avg[\'avg_decoder_loss\'],\n                ""stopnet_loss"": keep_avg[\'avg_stopnet_loss\'],\n                ""alignment_score"": keep_avg[\'avg_align_error\'],\n            }\n\n            if c.bidirectional_decoder:\n                epoch_stats[\'loss_decoder_backward\'] = keep_avg[\'avg_decoder_b_loss\']\n                align_b_img = alignments_backward[idx].data.cpu().numpy()\n                eval_figures[\'alignment_backward\'] = plot_alignment(align_b_img)\n            if c.ga_alpha > 0:\n                epoch_stats[\'guided_attention_loss\'] = keep_avg[\'avg_ga_loss\']\n            tb_logger.tb_eval_stats(global_step, epoch_stats)\n            tb_logger.tb_eval_figures(global_step, eval_figures)\n\n    if args.rank == 0 and epoch > c.test_delay_epochs:\n        if c.test_sentences_file is None:\n            test_sentences = [\n                ""It took me quite a long time to develop a voice, and now that I have it I\'m not going to be silent."",\n                ""Be a voice, not an echo."",\n                ""I\'m sorry Dave. I\'m afraid I can\'t do that."",\n                ""This cake is great. It\'s so delicious and moist.""\n            ]\n        else:\n            with open(c.test_sentences_file, ""r"") as f:\n                test_sentences = [s.strip() for s in f.readlines()]\n\n        # test sentences\n        test_audios = {}\n        test_figures = {}\n        print("" | > Synthesizing test sentences"")\n        speaker_id = 0 if c.use_speaker_embedding else None\n        style_wav = c.get(""style_wav_for_test"")\n        for idx, test_sentence in enumerate(test_sentences):\n            try:\n                wav, alignment, decoder_output, postnet_output, stop_tokens, inputs = synthesis(\n                    model,\n                    test_sentence,\n                    c,\n                    use_cuda,\n                    ap,\n                    speaker_id=speaker_id,\n                    style_wav=style_wav,\n                    truncated=False,\n                    enable_eos_bos_chars=c.enable_eos_bos_chars, #pylint: disable=unused-argument\n                    use_griffin_lim=True,\n                    do_trim_silence=False)\n\n                file_path = os.path.join(AUDIO_PATH, str(global_step))\n                os.makedirs(file_path, exist_ok=True)\n                file_path = os.path.join(file_path,\n                                         ""TestSentence_{}.wav"".format(idx))\n                ap.save_wav(wav, file_path)\n                test_audios[\'{}-audio\'.format(idx)] = wav\n                test_figures[\'{}-prediction\'.format(idx)] = plot_spectrogram(\n                    postnet_output, ap)\n                test_figures[\'{}-alignment\'.format(idx)] = plot_alignment(\n                    alignment)\n            except:\n                print("" !! Error creating Test Sentence -"", idx)\n                traceback.print_exc()\n        tb_logger.tb_test_audios(global_step, test_audios,\n                                 c.audio[\'sample_rate\'])\n        tb_logger.tb_test_figures(global_step, test_figures)\n    return keep_avg.avg_values\n\n\n# FIXME: move args definition/parsing inside of main?\ndef main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data_train, meta_data_eval, symbols, phonemes\n    # Audio processor\n    ap = AudioProcessor(**c.audio)\n    if \'characters\' in c.keys():\n        symbols, phonemes = make_symbols(**c.characters)\n\n    # DISTRUBUTED\n    if num_gpus > 1:\n        init_distributed(args.rank, num_gpus, args.group_id,\n                         c.distributed[""backend""], c.distributed[""url""])\n    num_chars = len(phonemes) if c.use_phonemes else len(symbols)\n\n    # load data instances\n    meta_data_train, meta_data_eval = load_meta_data(c.datasets)\n\n    # parse speakers\n    if c.use_speaker_embedding:\n        speakers = get_speakers(meta_data_train)\n        if args.restore_path:\n            prev_out_path = os.path.dirname(args.restore_path)\n            speaker_mapping = load_speaker_mapping(prev_out_path)\n            assert all([speaker in speaker_mapping\n                        for speaker in speakers]), ""As of now you, you cannot "" \\\n                                                   ""introduce new speakers to "" \\\n                                                   ""a previously trained model.""\n        else:\n            speaker_mapping = {name: i for i, name in enumerate(speakers)}\n        save_speaker_mapping(OUT_PATH, speaker_mapping)\n        num_speakers = len(speaker_mapping)\n        print(""Training with {} speakers: {}"".format(num_speakers,\n                                                     "", "".join(speakers)))\n    else:\n        num_speakers = 0\n\n    model = setup_model(num_chars, num_speakers, c)\n\n    print("" | > Num output units : {}"".format(ap.num_freq), flush=True)\n\n    params = set_weight_decay(model, c.wd)\n    optimizer = RAdam(params, lr=c.lr, weight_decay=0)\n    if c.stopnet and c.separate_stopnet:\n        optimizer_st = RAdam(model.decoder.stopnet.parameters(),\n                             lr=c.lr,\n                             weight_decay=0)\n    else:\n        optimizer_st = None\n\n    # setup criterion\n    criterion = TacotronLoss(c, stopnet_pos_weight=10.0, ga_sigma=0.4)\n\n    if args.restore_path:\n        checkpoint = torch.load(args.restore_path, map_location=\'cpu\')\n        try:\n            # TODO: fix optimizer init, model.cuda() needs to be called before\n            # optimizer restore\n            # optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            if c.reinit_layers:\n                raise RuntimeError\n            model.load_state_dict(checkpoint[\'model\'])\n        except:\n            print("" > Partial model initialization."")\n            model_dict = model.state_dict()\n            model_dict = set_init_dict(model_dict, checkpoint, c)\n            model.load_state_dict(model_dict)\n            del model_dict\n        for group in optimizer.param_groups:\n            group[\'lr\'] = c.lr\n        print("" > Model restored from step %d"" % checkpoint[\'step\'],\n              flush=True)\n        args.restore_step = checkpoint[\'step\']\n    else:\n        args.restore_step = 0\n\n    if use_cuda:\n        model.cuda()\n        criterion.cuda()\n\n    # DISTRUBUTED\n    if num_gpus > 1:\n        model = apply_gradient_allreduce(model)\n\n    if c.noam_schedule:\n        scheduler = NoamLR(optimizer,\n                           warmup_steps=c.warmup_steps,\n                           last_epoch=args.restore_step - 1)\n    else:\n        scheduler = None\n\n    num_params = count_parameters(model)\n    print(""\\n > Model has {} parameters"".format(num_params), flush=True)\n\n    if \'best_loss\' not in locals():\n        best_loss = float(\'inf\')\n\n    global_step = args.restore_step\n    for epoch in range(0, c.epochs):\n        c_logger.print_epoch_start(epoch, c.epochs)\n        # set gradual training\n        if c.gradual_training is not None:\n            r, c.batch_size = gradual_training_scheduler(global_step, c)\n            c.r = r\n            model.decoder.set_r(r)\n            if c.bidirectional_decoder:\n                model.decoder_backward.set_r(r)\n            print(""\\n > Number of output frames:"", model.decoder.r)\n\n        train_avg_loss_dict, global_step = train(model, criterion, optimizer,\n                                                 optimizer_st, scheduler, ap,\n                                                 global_step, epoch)\n        eval_avg_loss_dict = evaluate(model, criterion, ap, global_step, epoch)\n        c_logger.print_epoch_end(epoch, eval_avg_loss_dict)\n        target_loss = train_avg_loss_dict[\'avg_postnet_loss\']\n        if c.run_eval:\n            target_loss = eval_avg_loss_dict[\'avg_postnet_loss\']\n        best_loss = save_best_model(target_loss, best_loss, model, optimizer, global_step, epoch, c.r,\n                                    OUT_PATH)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--continue_path\',\n        type=str,\n        help=\'Training output folder to continue training. Use to continue a training. If it is used, ""config_path"" is ignored.\',\n        default=\'\',\n        required=\'--config_path\' not in sys.argv)\n    parser.add_argument(\n        \'--restore_path\',\n        type=str,\n        help=\'Model file to be restored. Use to finetune a model.\',\n        default=\'\')\n    parser.add_argument(\n        \'--config_path\',\n        type=str,\n        help=\'Path to config file for training.\',\n        required=\'--continue_path\' not in sys.argv\n    )\n    parser.add_argument(\'--debug\',\n                        type=bool,\n                        default=False,\n                        help=\'Do not verify commit integrity to run training.\')\n\n    # DISTRUBUTED\n    parser.add_argument(\n        \'--rank\',\n        type=int,\n        default=0,\n        help=\'DISTRIBUTED: process rank for distributed training.\')\n    parser.add_argument(\'--group_id\',\n                        type=str,\n                        default="""",\n                        help=\'DISTRIBUTED: process group id.\')\n    args = parser.parse_args()\n\n    if args.continue_path != \'\':\n        args.output_path = args.continue_path\n        args.config_path = os.path.join(args.continue_path, \'config.json\')\n        list_of_files = glob.glob(args.continue_path + ""/*.pth.tar"") # * means all if need specific format then *.csv\n        latest_model_file = max(list_of_files, key=os.path.getctime)\n        args.restore_path = latest_model_file\n        print(f"" > Training continues for {args.restore_path}"")\n\n    # setup output paths and read configs\n    c = load_config(args.config_path)\n    check_config(c)\n    _ = os.path.dirname(os.path.realpath(__file__))\n\n    OUT_PATH = args.continue_path\n    if args.continue_path == \'\':\n        OUT_PATH = create_experiment_folder(c.output_path, c.run_name, args.debug)\n\n    AUDIO_PATH = os.path.join(OUT_PATH, \'test_audios\')\n\n    c_logger = ConsoleLogger()\n\n    if args.rank == 0:\n        os.makedirs(AUDIO_PATH, exist_ok=True)\n        new_fields = {}\n        if args.restore_path:\n            new_fields[""restore_path""] = args.restore_path\n        new_fields[""github_branch""] = get_git_branch()\n        copy_config_file(args.config_path,\n                         os.path.join(OUT_PATH, \'config.json\'), new_fields)\n        os.chmod(AUDIO_PATH, 0o775)\n        os.chmod(OUT_PATH, 0o775)\n\n        LOG_DIR = OUT_PATH\n        tb_logger = TensorboardLogger(LOG_DIR)\n\n        # write model desc to tensorboard\n        tb_logger.tb_add_text(\'model-description\', c[\'run_description\'], 0)\n\n    try:\n        main(args)\n    except KeyboardInterrupt:\n        remove_experiment_folder(OUT_PATH)\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)  # pylint: disable=protected-access\n    except Exception:  # pylint: disable=broad-except\n        remove_experiment_folder(OUT_PATH)\n        traceback.print_exc()\n        sys.exit(1)\n'"
dataset_analysis/analyze.py,0,"b'# visualisation tools for mimic2\r\nimport matplotlib.pyplot as plt\r\nfrom statistics import stdev, mode, mean, median\r\nfrom statistics import StatisticsError\r\nimport argparse\r\nimport os\r\nimport csv\r\nimport seaborn as sns\r\nimport random\r\nfrom text.cmudict import CMUDict\r\n\r\ndef get_audio_seconds(frames):\r\n    return (frames*12.5)/1000\r\n\r\n\r\ndef append_data_statistics(meta_data):\r\n    # get data statistics\r\n    for char_cnt in meta_data:\r\n        data = meta_data[char_cnt][""data""]\r\n        audio_len_list = [d[""audio_len""] for d in data]\r\n        mean_audio_len = mean(audio_len_list)\r\n        try:\r\n            mode_audio_list = [round(d[""audio_len""], 2) for d in data]\r\n            mode_audio_len = mode(mode_audio_list)\r\n        except StatisticsError:\r\n            mode_audio_len = audio_len_list[0]\r\n        median_audio_len = median(audio_len_list)\r\n\r\n        try:\r\n            std = stdev(\r\n                d[""audio_len""] for d in data\r\n            )\r\n        except StatisticsError:\r\n            std = 0\r\n\r\n        meta_data[char_cnt][""mean""] = mean_audio_len\r\n        meta_data[char_cnt][""median""] = median_audio_len\r\n        meta_data[char_cnt][""mode""] = mode_audio_len\r\n        meta_data[char_cnt][""std""] = std\r\n    return meta_data\r\n\r\n\r\ndef process_meta_data(path):\r\n    meta_data = {}\r\n\r\n    # load meta data\r\n    with open(path, \'r\') as f:\r\n        data = csv.reader(f, delimiter=\'|\')\r\n        for row in data:\r\n            frames = int(row[2])\r\n            utt = row[3]\r\n            audio_len = get_audio_seconds(frames)\r\n            char_count = len(utt)\r\n            if not meta_data.get(char_count):\r\n                meta_data[char_count] = {\r\n                    ""data"": []\r\n                }\r\n\r\n            meta_data[char_count][""data""].append(\r\n                {\r\n                    ""utt"": utt,\r\n                    ""frames"": frames,\r\n                    ""audio_len"": audio_len,\r\n                    ""row"": ""{}|{}|{}|{}"".format(row[0], row[1], row[2], row[3])\r\n                }\r\n            )\r\n\r\n    meta_data = append_data_statistics(meta_data)\r\n\r\n    return meta_data\r\n\r\n\r\ndef get_data_points(meta_data):\r\n    x = [char_cnt for char_cnt in meta_data]\r\n    y_avg = [meta_data[d][\'mean\'] for d in meta_data]\r\n    y_mode = [meta_data[d][\'mode\'] for d in meta_data]\r\n    y_median = [meta_data[d][\'median\'] for d in meta_data]\r\n    y_std = [meta_data[d][\'std\'] for d in meta_data]\r\n    y_num_samples = [len(meta_data[d][\'data\']) for d in meta_data]\r\n    return {\r\n        ""x"": x,\r\n        ""y_avg"": y_avg,\r\n        ""y_mode"": y_mode,\r\n        ""y_median"": y_median,\r\n        ""y_std"": y_std,\r\n        ""y_num_samples"": y_num_samples\r\n    }\r\n\r\n\r\ndef save_training(file_path, meta_data):\r\n    rows = []\r\n    for char_cnt in meta_data:\r\n        data = meta_data[char_cnt][\'data\']\r\n        for d in data:\r\n            rows.append(d[\'row\'] + ""\\n"")\r\n\r\n    random.shuffle(rows)\r\n    with open(file_path, \'w+\') as f:\r\n        for row in rows:\r\n            f.write(row)\r\n\r\n\r\ndef plot(meta_data, save_path=None):\r\n    save = False\r\n    if save_path:\r\n        save = True\r\n\r\n    graph_data = get_data_points(meta_data)\r\n    x = graph_data[\'x\']\r\n    y_avg = graph_data[\'y_avg\']\r\n    y_std = graph_data[\'y_std\']\r\n    y_mode = graph_data[\'y_mode\']\r\n    y_median = graph_data[\'y_median\']\r\n    y_num_samples = graph_data[\'y_num_samples\']\r\n\r\n    plt.figure()\r\n    plt.plot(x, y_avg, \'ro\')\r\n    plt.xlabel(""character lengths"", fontsize=30)\r\n    plt.ylabel(""avg seconds"", fontsize=30)\r\n    if save:\r\n        name = ""char_len_vs_avg_secs""\r\n        plt.savefig(os.path.join(save_path, name))\r\n\r\n    plt.figure()\r\n    plt.plot(x, y_mode, \'ro\')\r\n    plt.xlabel(""character lengths"", fontsize=30)\r\n    plt.ylabel(""mode seconds"", fontsize=30)\r\n    if save:\r\n        name = ""char_len_vs_mode_secs""\r\n        plt.savefig(os.path.join(save_path, name))\r\n\r\n    plt.figure()\r\n    plt.plot(x, y_median, \'ro\')\r\n    plt.xlabel(""character lengths"", fontsize=30)\r\n    plt.ylabel(""median seconds"", fontsize=30)\r\n    if save:\r\n        name = ""char_len_vs_med_secs""\r\n        plt.savefig(os.path.join(save_path, name))\r\n\r\n    plt.figure()\r\n    plt.plot(x, y_std, \'ro\')\r\n    plt.xlabel(""character lengths"", fontsize=30)\r\n    plt.ylabel(""standard deviation"", fontsize=30)\r\n    if save:\r\n        name = ""char_len_vs_std""\r\n        plt.savefig(os.path.join(save_path, name))\r\n\r\n    plt.figure()\r\n    plt.plot(x, y_num_samples, \'ro\')\r\n    plt.xlabel(""character lengths"", fontsize=30)\r\n    plt.ylabel(""number of samples"", fontsize=30)\r\n    if save:\r\n        name = ""char_len_vs_num_samples""\r\n        plt.savefig(os.path.join(save_path, name))\r\n\r\n\r\ndef plot_phonemes(train_path, cmu_dict_path, save_path):\r\n    cmudict = CMUDict(cmu_dict_path)\r\n\r\n    phonemes = {}\r\n\r\n    with open(train_path, \'r\') as f:\r\n        data = csv.reader(f, delimiter=\'|\')\r\n        phonemes[""None""] = 0\r\n        for row in data:\r\n            words = row[3].split()\r\n            for word in words:\r\n                pho = cmudict.lookup(word)\r\n                if pho:\r\n                    indie = pho[0].split()\r\n                    for nemes in indie:\r\n                        if phonemes.get(nemes):\r\n                            phonemes[nemes] += 1\r\n                        else:\r\n                            phonemes[nemes] = 1\r\n                else:\r\n                    phonemes[""None""] += 1\r\n\r\n    x, y = [], []\r\n    for key in phonemes:\r\n        x.append(key)\r\n        y.append(phonemes[key])\r\n\r\n    plt.figure()\r\n    plt.rcParams[""figure.figsize""] = (50, 20)\r\n    barplot = sns.barplot(x, y)\r\n    if save_path:\r\n        fig = barplot.get_figure()\r\n        fig.savefig(os.path.join(save_path, ""phoneme_dist""))\r\n\r\n\r\ndef main():\r\n    parser = argparse.ArgumentParser()\r\n    parser.add_argument(\r\n        \'--train_file_path\', required=True,\r\n        help=\'this is the path to the train.txt file that the preprocess.py script creates\'\r\n    )\r\n    parser.add_argument(\r\n        \'--save_to\', help=\'path to save charts of data to\'\r\n    )\r\n    parser.add_argument(\r\n        \'--cmu_dict_path\', help=\'give cmudict-0.7b to see phoneme distribution\'\r\n    )\r\n    args = parser.parse_args()\r\n    meta_data = process_meta_data(args.train_file_path)\r\n    plt.rcParams[""figure.figsize""] = (10, 5)\r\n    plot(meta_data, save_path=args.save_to)\r\n    if args.cmu_dict_path:\r\n        plt.rcParams[""figure.figsize""] = (30, 10)\r\n        plot_phonemes(args.train_file_path, args.cmu_dict_path, args.save_to)\r\n\r\n    plt.show()\r\n\r\nif __name__ == \'__main__\':\r\n    main()\r\n'"
datasets/TTSDataset.py,9,"b'import os\nimport numpy as np\nimport collections\nimport torch\nimport random\nfrom torch.utils.data import Dataset\n\nfrom TTS.utils.text import text_to_sequence, phoneme_to_sequence, pad_with_eos_bos\nfrom TTS.utils.data import prepare_data, prepare_tensor, prepare_stop_target\n\n\nclass MyDataset(Dataset):\n    def __init__(self,\n                 outputs_per_step,\n                 text_cleaner,\n                 compute_linear_spec,\n                 ap,\n                 meta_data,\n                 tp=None,\n                 batch_group_size=0,\n                 min_seq_len=0,\n                 max_seq_len=float(""inf""),\n                 use_phonemes=True,\n                 phoneme_cache_path=None,\n                 phoneme_language=""en-us"",\n                 enable_eos_bos=False,\n                 verbose=False):\n        """"""\n        Args:\n            outputs_per_step (int): number of time frames predicted per step.\n            text_cleaner (str): text cleaner used for the dataset.\n            compute_linear_spec (bool): compute linear spectrogram if True.\n            ap (TTS.utils.AudioProcessor): audio processor object.\n            meta_data (list): list of dataset instances.\n            batch_group_size (int): (0) range of batch randomization after sorting\n                sequences by length.\n            min_seq_len (int): (0) minimum sequence length to be processed\n                by the loader.\n            max_seq_len (int): (float(""inf"")) maximum sequence length.\n            use_phonemes (bool): (true) if true, text converted to phonemes.\n            phoneme_cache_path (str): path to cache phoneme features.\n            phoneme_language (str): one the languages from\n                https://github.com/bootphon/phonemizer#languages\n            enable_eos_bos (bool): enable end of sentence and beginning of sentences characters.\n            verbose (bool): print diagnostic information.\n        """"""\n        self.batch_group_size = batch_group_size\n        self.items = meta_data\n        self.outputs_per_step = outputs_per_step\n        self.sample_rate = ap.sample_rate\n        self.cleaners = text_cleaner\n        self.compute_linear_spec = compute_linear_spec\n        self.min_seq_len = min_seq_len\n        self.max_seq_len = max_seq_len\n        self.ap = ap\n        self.tp = tp\n        self.use_phonemes = use_phonemes\n        self.phoneme_cache_path = phoneme_cache_path\n        self.phoneme_language = phoneme_language\n        self.enable_eos_bos = enable_eos_bos\n        self.verbose = verbose\n        if use_phonemes and not os.path.isdir(phoneme_cache_path):\n            os.makedirs(phoneme_cache_path, exist_ok=True)\n        if self.verbose:\n            print(""\\n > DataLoader initialization"")\n            print("" | > Use phonemes: {}"".format(self.use_phonemes))\n            if use_phonemes:\n                print(""   | > phoneme language: {}"".format(phoneme_language))\n            print("" | > Number of instances : {}"".format(len(self.items)))\n        self.sort_items()\n\n    def load_wav(self, filename):\n        audio = self.ap.load_wav(filename)\n        return audio\n\n    @staticmethod\n    def load_np(filename):\n        data = np.load(filename).astype(\'float32\')\n        return data\n\n    def _generate_and_cache_phoneme_sequence(self, text, cache_path):\n        """"""generate a phoneme sequence from text.\n        since the usage is for subsequent caching, we never add bos and\n        eos chars here. Instead we add those dynamically later; based on the\n        config option.""""""\n        phonemes = phoneme_to_sequence(text, [self.cleaners],\n                                       language=self.phoneme_language,\n                                       enable_eos_bos=False,\n                                       tp=self.tp)\n        phonemes = np.asarray(phonemes, dtype=np.int32)\n        np.save(cache_path, phonemes)\n        return phonemes\n\n    def _load_or_generate_phoneme_sequence(self, wav_file, text):\n        file_name = os.path.basename(wav_file).split(\'.\')[0]\n        cache_path = os.path.join(self.phoneme_cache_path,\n                                  file_name + \'_phoneme.npy\')\n        try:\n            phonemes = np.load(cache_path)\n        except FileNotFoundError:\n            phonemes = self._generate_and_cache_phoneme_sequence(text,\n                                                                 cache_path)\n        except (ValueError, IOError):\n            print("" > ERROR: failed loading phonemes for {}. ""\n                  ""Recomputing."".format(wav_file))\n            phonemes = self._generate_and_cache_phoneme_sequence(text,\n                                                                 cache_path)\n        if self.enable_eos_bos:\n            phonemes = pad_with_eos_bos(phonemes, tp=self.tp)\n            phonemes = np.asarray(phonemes, dtype=np.int32)\n        return phonemes\n\n    def load_data(self, idx):\n        text, wav_file, speaker_name = self.items[idx]\n        wav = np.asarray(self.load_wav(wav_file), dtype=np.float32)\n\n        if self.use_phonemes:\n            text = self._load_or_generate_phoneme_sequence(wav_file, text)\n        else:\n            text = np.asarray(\n                text_to_sequence(text, [self.cleaners], tp=self.tp), dtype=np.int32)\n\n        assert text.size > 0, self.items[idx][1]\n        assert wav.size > 0, self.items[idx][1]\n\n        sample = {\n            \'text\': text,\n            \'wav\': wav,\n            \'item_idx\': self.items[idx][1],\n            \'speaker_name\': speaker_name\n        }\n        return sample\n\n    def sort_items(self):\n        r""""""Sort instances based on text length in ascending order""""""\n        lengths = np.array([len(ins[0]) for ins in self.items])\n\n        idxs = np.argsort(lengths)\n        new_items = []\n        ignored = []\n        for i, idx in enumerate(idxs):\n            length = lengths[idx]\n            if length < self.min_seq_len or length > self.max_seq_len:\n                ignored.append(idx)\n            else:\n                new_items.append(self.items[idx])\n        # shuffle batch groups\n        if self.batch_group_size > 0:\n            for i in range(len(new_items) // self.batch_group_size):\n                offset = i * self.batch_group_size\n                end_offset = offset + self.batch_group_size\n                temp_items = new_items[offset:end_offset]\n                random.shuffle(temp_items)\n                new_items[offset:end_offset] = temp_items\n        self.items = new_items\n\n        if self.verbose:\n            print("" | > Max length sequence: {}"".format(np.max(lengths)))\n            print("" | > Min length sequence: {}"".format(np.min(lengths)))\n            print("" | > Avg length sequence: {}"".format(np.mean(lengths)))\n            print("" | > Num. instances discarded by max-min (max={}, min={}) seq limits: {}"".format(\n                self.max_seq_len, self.min_seq_len, len(ignored)))\n            print("" | > Batch group size: {}."".format(self.batch_group_size))\n\n    def __len__(self):\n        return len(self.items)\n\n    def __getitem__(self, idx):\n        return self.load_data(idx)\n\n    def collate_fn(self, batch):\n        r""""""\n            Perform preprocessing and create a final data batch:\n            1. Sort batch instances by text-length\n            2. Convert Audio signal to Spectrograms.\n            3. PAD sequences wrt r.\n            4. Load to Torch.\n        """"""\n\n        # Puts each data field into a tensor with outer dimension batch size\n        if isinstance(batch[0], collections.Mapping):\n\n            text_lenghts = np.array([len(d[""text""]) for d in batch])\n\n            # sort items with text input length for RNN efficiency\n            text_lenghts, ids_sorted_decreasing = torch.sort(\n                torch.LongTensor(text_lenghts), dim=0, descending=True)\n\n            wav = [batch[idx][\'wav\'] for idx in ids_sorted_decreasing]\n            item_idxs = [\n                batch[idx][\'item_idx\'] for idx in ids_sorted_decreasing\n            ]\n            text = [batch[idx][\'text\'] for idx in ids_sorted_decreasing]\n            speaker_name = [batch[idx][\'speaker_name\']\n                            for idx in ids_sorted_decreasing]\n\n            # compute features\n            mel = [self.ap.melspectrogram(w).astype(\'float32\') for w in wav]\n\n            mel_lengths = [m.shape[1] for m in mel]\n\n            # compute \'stop token\' targets\n            stop_targets = [\n                np.array([0.] * (mel_len - 1) + [1.]) for mel_len in mel_lengths\n            ]\n\n            # PAD stop targets\n            stop_targets = prepare_stop_target(stop_targets,\n                                               self.outputs_per_step)\n\n            # PAD sequences with longest instance in the batch\n            text = prepare_data(text).astype(np.int32)\n\n            # PAD features with longest instance\n            mel = prepare_tensor(mel, self.outputs_per_step)\n\n            # B x D x T --> B x T x D\n            mel = mel.transpose(0, 2, 1)\n\n            # convert things to pytorch\n            text_lenghts = torch.LongTensor(text_lenghts)\n            text = torch.LongTensor(text)\n            mel = torch.FloatTensor(mel).contiguous()\n            mel_lengths = torch.LongTensor(mel_lengths)\n            stop_targets = torch.FloatTensor(stop_targets)\n\n            # compute linear spectrogram\n            if self.compute_linear_spec:\n                linear = [self.ap.spectrogram(w).astype(\'float32\') for w in wav]\n                linear = prepare_tensor(linear, self.outputs_per_step)\n                linear = linear.transpose(0, 2, 1)\n                assert mel.shape[1] == linear.shape[1]\n                linear = torch.FloatTensor(linear).contiguous()\n            else:\n                linear = None\n            return text, text_lenghts, speaker_name, linear, mel, mel_lengths, \\\n                   stop_targets, item_idxs\n\n        raise TypeError((""batch must contain tensors, numbers, dicts or lists;\\\n                         found {}"".format(type(batch[0]))))\n'"
datasets/__init__.py,0,b''
datasets/preprocess.py,0,"b'import os\nfrom glob import glob\nimport re\nimport sys\nfrom TTS.utils.generic_utils import split_dataset\n\n\ndef load_meta_data(datasets):\n    meta_data_train_all = []\n    meta_data_eval_all = []\n    for dataset in datasets:\n        name = dataset[\'name\']\n        root_path = dataset[\'path\']\n        meta_file_train = dataset[\'meta_file_train\']\n        meta_file_val = dataset[\'meta_file_val\']\n        preprocessor = get_preprocessor_by_name(name)\n\n        meta_data_train = preprocessor(root_path, meta_file_train)\n        if meta_file_val is None:\n            meta_data_eval, meta_data_train = split_dataset(meta_data_train)\n        else:\n            meta_data_eval = preprocessor(root_path, meta_file_val)\n        meta_data_train_all += meta_data_train\n        meta_data_eval_all += meta_data_eval\n    return meta_data_train_all, meta_data_eval_all\n\n\ndef get_preprocessor_by_name(name):\n    """"""Returns the respective preprocessing function.""""""\n    thismodule = sys.modules[__name__]\n    return getattr(thismodule, name.lower())\n\n\ndef tweb(root_path, meta_file):\n    """"""Normalize TWEB dataset.\n    https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset\n    """"""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""tweb""\n    with open(txt_file, \'r\') as ttf:\n        for line in ttf:\n            cols = line.split(\'\\t\')\n            wav_file = os.path.join(root_path, cols[0] + \'.wav\')\n            text = cols[1]\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\n# def kusal(root_path, meta_file):\n#     txt_file = os.path.join(root_path, meta_file)\n#     texts = []\n#     wavs = []\n#     with open(txt_file, ""r"", encoding=""utf8"") as f:\n#         frames = [\n#             line.split(\'\\t\') for line in f\n#             if line.split(\'\\t\')[0] in self.wav_files_dict.keys()\n#         ]\n#     # TODO: code the rest\n#     return  {\'text\': texts, \'wavs\': wavs}\n\n\ndef mozilla(root_path, meta_file):\n    """"""Normalizes Mozilla meta data files to TTS format""""""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""mozilla""\n    with open(txt_file, \'r\') as ttf:\n        for line in ttf:\n            cols = line.split(\'|\')\n            wav_file = cols[1].strip()\n            text = cols[0].strip()\n            wav_file = os.path.join(root_path, ""wavs"", wav_file)\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\ndef mozilla_de(root_path, meta_file):\n    """"""Normalizes Mozilla meta data files to TTS format""""""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""mozilla""\n    with open(txt_file, \'r\', encoding=""ISO 8859-1"") as ttf:\n        for line in ttf:\n            cols = line.strip().split(\'|\')\n            wav_file = cols[0].strip()\n            text = cols[1].strip()\n            folder_name = f""BATCH_{wav_file.split(\'_\')[0]}_FINAL""\n            wav_file = os.path.join(root_path, folder_name, wav_file)\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\ndef mailabs(root_path, meta_files=None):\n    """"""Normalizes M-AI-Labs meta data files to TTS format""""""\n    speaker_regex = re.compile(""by_book/(male|female)/(?P<speaker_name>[^/]+)/"")\n    if meta_files is None:\n        csv_files = glob(root_path+""/**/metadata.csv"", recursive=True)\n    else:\n        csv_files = meta_files\n    # meta_files = [f.strip() for f in meta_files.split("","")]\n    items = []\n    for csv_file in csv_files:\n        txt_file = os.path.join(root_path, csv_file)\n        folder = os.path.dirname(txt_file)\n        # determine speaker based on folder structure...\n        speaker_name_match = speaker_regex.search(txt_file)\n        if speaker_name_match is None:\n            continue\n        speaker_name = speaker_name_match.group(""speaker_name"")\n        print("" | > {}"".format(csv_file))\n        with open(txt_file, \'r\') as ttf:\n            for line in ttf:\n                cols = line.split(\'|\')\n                if meta_files is None:\n                    wav_file = os.path.join(folder, \'wavs\', cols[0] + \'.wav\')\n                else:\n                    wav_file = os.path.join(root_path, folder.replace(""metadata.csv"", """"), \'wavs\', cols[0] + \'.wav\')\n                if os.path.isfile(wav_file):\n                    text = cols[1].strip()\n                    items.append([text, wav_file, speaker_name])\n                else:\n                    raise RuntimeError(""> File %s does not exist!""%(wav_file))\n    return items\n\n\ndef ljspeech(root_path, meta_file):\n    """"""Normalizes the Nancy meta data file to TTS format""""""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""ljspeech""\n    with open(txt_file, \'r\') as ttf:\n        for line in ttf:\n            cols = line.split(\'|\')\n            wav_file = os.path.join(root_path, \'wavs\', cols[0] + \'.wav\')\n            text = cols[1]\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\ndef nancy(root_path, meta_file):\n    """"""Normalizes the Nancy meta data file to TTS format""""""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""nancy""\n    with open(txt_file, \'r\') as ttf:\n        for line in ttf:\n            utt_id = line.split()[1]\n            text = line[line.find(\'""\') + 1:line.rfind(\'""\') - 1]\n            wav_file = os.path.join(root_path, ""wavn"", utt_id + "".wav"")\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\ndef common_voice(root_path, meta_file):\n    """"""Normalize the common voice meta data file to TTS format.""""""\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    with open(txt_file, \'r\') as ttf:\n        for line in ttf:\n            if line.startswith(""client_id""):\n                continue\n            cols = line.split(""\\t"")\n            text = cols[2]\n            speaker_name = cols[0]\n            wav_file = os.path.join(root_path, ""clips"", cols[1] + "".wav"")\n            items.append([text, wav_file, speaker_name])\n    return items\n\n\ndef libri_tts(root_path, meta_files=None):\n    """"""https://ai.google/tools/datasets/libri-tts/""""""\n    items = []\n    if meta_files is None:\n        meta_files = glob(f""{root_path}/**/*trans.tsv"", recursive=True)\n    for meta_file in meta_files:\n        _meta_file = os.path.basename(meta_file).split(\'.\')[0]\n        speaker_name = _meta_file.split(\'_\')[0]\n        chapter_id = _meta_file.split(\'_\')[1]\n        _root_path = os.path.join(root_path, f""{speaker_name}/{chapter_id}"")\n        with open(meta_file, \'r\') as ttf:\n            for line in ttf:\n                cols = line.split(\'\\t\')\n                wav_file = os.path.join(_root_path, cols[0] + \'.wav\')\n                text = cols[1]\n                items.append([text, wav_file, speaker_name])\n    for item in items:\n        assert os.path.exists(item[1]), f"" [!] wav files don\'t exist - {item[1]}""\n    return items\n\n\ndef custom_turkish(root_path, meta_file):\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = ""turkish-female""\n    skipped_files = []\n    with open(txt_file, \'r\', encoding=\'utf-8\') as ttf:\n        for line in ttf:\n            cols = line.split(\'|\')\n            wav_file = os.path.join(root_path, \'wavs\', cols[0].strip() + \'.wav\')\n            if not os.path.exists(wav_file):\n                skipped_files.append(wav_file)\n                continue\n            text = cols[1].strip()\n            items.append([text, wav_file, speaker_name])\n    print(f"" [!] {len(skipped_files)} files skipped. They don\'t exist..."")\n    return items\n'"
layers/__init__.py,0,b''
layers/common_layers.py,33,"b'import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\n\nclass Linear(nn.Module):\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 bias=True,\n                 init_gain=\'linear\'):\n        super(Linear, self).__init__()\n        self.linear_layer = torch.nn.Linear(\n            in_features, out_features, bias=bias)\n        self._init_w(init_gain)\n\n    def _init_w(self, init_gain):\n        torch.nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=torch.nn.init.calculate_gain(init_gain))\n\n    def forward(self, x):\n        return self.linear_layer(x)\n\n\nclass LinearBN(nn.Module):\n    def __init__(self,\n                 in_features,\n                 out_features,\n                 bias=True,\n                 init_gain=\'linear\'):\n        super(LinearBN, self).__init__()\n        self.linear_layer = torch.nn.Linear(\n            in_features, out_features, bias=bias)\n        self.batch_normalization = nn.BatchNorm1d(out_features, momentum=0.1, eps=1e-5)\n        self._init_w(init_gain)\n\n    def _init_w(self, init_gain):\n        torch.nn.init.xavier_uniform_(\n            self.linear_layer.weight,\n            gain=torch.nn.init.calculate_gain(init_gain))\n\n    def forward(self, x):\n        out = self.linear_layer(x)\n        if len(out.shape) == 3:\n            out = out.permute(1, 2, 0)\n        out = self.batch_normalization(out)\n        if len(out.shape) == 3:\n            out = out.permute(2, 0, 1)\n        return out\n\n\nclass Prenet(nn.Module):\n    def __init__(self,\n                 in_features,\n                 prenet_type=""original"",\n                 prenet_dropout=True,\n                 out_features=[256, 256],\n                 bias=True):\n        super(Prenet, self).__init__()\n        self.prenet_type = prenet_type\n        self.prenet_dropout = prenet_dropout\n        in_features = [in_features] + out_features[:-1]\n        if prenet_type == ""bn"":\n            self.linear_layers = nn.ModuleList([\n                LinearBN(in_size, out_size, bias=bias)\n                for (in_size, out_size) in zip(in_features, out_features)\n            ])\n        elif prenet_type == ""original"":\n            self.linear_layers = nn.ModuleList([\n                Linear(in_size, out_size, bias=bias)\n                for (in_size, out_size) in zip(in_features, out_features)\n            ])\n\n    def forward(self, x):\n        for linear in self.linear_layers:\n            if self.prenet_dropout:\n                x = F.dropout(F.relu(linear(x)), p=0.5, training=self.training)\n            else:\n                x = F.relu(linear(x))\n        return x\n\n\n####################\n# ATTENTION MODULES\n####################\n\n\nclass LocationLayer(nn.Module):\n    def __init__(self,\n                 attention_dim,\n                 attention_n_filters=32,\n                 attention_kernel_size=31):\n        super(LocationLayer, self).__init__()\n        self.location_conv1d = nn.Conv1d(\n            in_channels=2,\n            out_channels=attention_n_filters,\n            kernel_size=attention_kernel_size,\n            stride=1,\n            padding=(attention_kernel_size - 1) // 2,\n            bias=False)\n        self.location_dense = Linear(\n            attention_n_filters, attention_dim, bias=False, init_gain=\'tanh\')\n\n    def forward(self, attention_cat):\n        processed_attention = self.location_conv1d(attention_cat)\n        processed_attention = self.location_dense(\n            processed_attention.transpose(1, 2))\n        return processed_attention\n\n\nclass GravesAttention(nn.Module):\n    """""" Discretized Graves attention:\n        - https://arxiv.org/abs/1910.10288\n        - https://arxiv.org/pdf/1906.01083.pdf\n    """"""\n    COEF = 0.3989422917366028  # numpy.sqrt(1/(2*numpy.pi))\n\n    def __init__(self, query_dim, K):\n        super(GravesAttention, self).__init__()\n        self._mask_value = 1e-8\n        self.K = K\n        # self.attention_alignment = 0.05\n        self.eps = 1e-5\n        self.J = None\n        self.N_a = nn.Sequential(\n            nn.Linear(query_dim, query_dim, bias=True),\n            nn.ReLU(),\n            nn.Linear(query_dim, 3*K, bias=True))\n        self.attention_weights = None\n        self.mu_prev = None\n        self.init_layers()\n\n    def init_layers(self):\n        torch.nn.init.constant_(self.N_a[2].bias[(2*self.K):(3*self.K)], 1.)  # bias mean\n        torch.nn.init.constant_(self.N_a[2].bias[self.K:(2*self.K)], 10)  # bias std\n\n    def init_states(self, inputs):\n        if self.J is None or inputs.shape[1]+1 > self.J.shape[-1]:\n            self.J = torch.arange(0, inputs.shape[1]+2.0).to(inputs.device) + 0.5\n        self.attention_weights = torch.zeros(inputs.shape[0], inputs.shape[1]).to(inputs.device)\n        self.mu_prev = torch.zeros(inputs.shape[0], self.K).to(inputs.device)\n\n    # pylint: disable=R0201\n    # pylint: disable=unused-argument\n    def preprocess_inputs(self, inputs):\n        return None\n\n    def forward(self, query, inputs, processed_inputs, mask):\n        """"""\n        shapes:\n            query: B x D_attention_rnn\n            inputs: B x T_in x D_encoder\n            processed_inputs: place_holder\n            mask: B x T_in\n        """"""\n        gbk_t = self.N_a(query)\n        gbk_t = gbk_t.view(gbk_t.size(0), -1, self.K)\n\n        # attention model parameters\n        # each B x K\n        g_t = gbk_t[:, 0, :]\n        b_t = gbk_t[:, 1, :]\n        k_t = gbk_t[:, 2, :]\n\n        # dropout to decorrelate attention heads\n        g_t = torch.nn.functional.dropout(g_t, p=0.5, training=self.training)\n\n        # attention GMM parameters\n        sig_t = torch.nn.functional.softplus(b_t) + self.eps\n\n        mu_t = self.mu_prev + torch.nn.functional.softplus(k_t)\n        g_t = torch.softmax(g_t, dim=-1) + self.eps\n\n        j = self.J[:inputs.size(1)+1]\n\n        # attention weights\n        phi_t = g_t.unsqueeze(-1) * (1 / (1 + torch.sigmoid((mu_t.unsqueeze(-1) - j) / sig_t.unsqueeze(-1))))\n\n        # discritize attention weights\n        alpha_t = torch.sum(phi_t, 1)\n        alpha_t = alpha_t[:, 1:] - alpha_t[:, :-1]\n        alpha_t[alpha_t == 0] = 1e-8\n\n        # apply masking\n        if mask is not None:\n            alpha_t.data.masked_fill_(~mask, self._mask_value)\n\n        context = torch.bmm(alpha_t.unsqueeze(1), inputs).squeeze(1)\n        self.attention_weights = alpha_t\n        self.mu_prev = mu_t\n        return context\n\n\nclass OriginalAttention(nn.Module):\n    """"""Following the methods proposed here:\n        - https://arxiv.org/abs/1712.05884\n        - https://arxiv.org/abs/1807.06736 + state masking at inference\n        - Using sigmoid instead of softmax normalization\n        - Attention windowing at inference time\n    """"""\n    # Pylint gets confused by PyTorch conventions here\n    #pylint: disable=attribute-defined-outside-init\n    def __init__(self, query_dim, embedding_dim, attention_dim,\n                 location_attention, attention_location_n_filters,\n                 attention_location_kernel_size, windowing, norm, forward_attn,\n                 trans_agent, forward_attn_mask):\n        super(OriginalAttention, self).__init__()\n        self.query_layer = Linear(\n            query_dim, attention_dim, bias=False, init_gain=\'tanh\')\n        self.inputs_layer = Linear(\n            embedding_dim, attention_dim, bias=False, init_gain=\'tanh\')\n        self.v = Linear(attention_dim, 1, bias=True)\n        if trans_agent:\n            self.ta = nn.Linear(\n                query_dim + embedding_dim, 1, bias=True)\n        if location_attention:\n            self.location_layer = LocationLayer(\n                attention_dim,\n                attention_location_n_filters,\n                attention_location_kernel_size,\n            )\n        self._mask_value = -float(""inf"")\n        self.windowing = windowing\n        self.win_idx = None\n        self.norm = norm\n        self.forward_attn = forward_attn\n        self.trans_agent = trans_agent\n        self.forward_attn_mask = forward_attn_mask\n        self.location_attention = location_attention\n\n    def init_win_idx(self):\n        self.win_idx = -1\n        self.win_back = 2\n        self.win_front = 6\n\n    def init_forward_attn(self, inputs):\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        self.alpha = torch.cat(\n            [torch.ones([B, 1]),\n             torch.zeros([B, T])[:, :-1] + 1e-7], dim=1).to(inputs.device)\n        self.u = (0.5 * torch.ones([B, 1])).to(inputs.device)\n\n    def init_location_attention(self, inputs):\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        self.attention_weights_cum = Variable(inputs.data.new(B, T).zero_())\n\n    def init_states(self, inputs):\n        B = inputs.shape[0]\n        T = inputs.shape[1]\n        self.attention_weights = Variable(inputs.data.new(B, T).zero_())\n        if self.location_attention:\n            self.init_location_attention(inputs)\n        if self.forward_attn:\n            self.init_forward_attn(inputs)\n        if self.windowing:\n            self.init_win_idx()\n\n    def preprocess_inputs(self, inputs):\n        return self.inputs_layer(inputs)\n\n    def update_location_attention(self, alignments):\n        self.attention_weights_cum += alignments\n\n    def get_location_attention(self, query, processed_inputs):\n        attention_cat = torch.cat((self.attention_weights.unsqueeze(1),\n                                   self.attention_weights_cum.unsqueeze(1)),\n                                  dim=1)\n        processed_query = self.query_layer(query.unsqueeze(1))\n        processed_attention_weights = self.location_layer(attention_cat)\n        energies = self.v(\n            torch.tanh(processed_query + processed_attention_weights +\n                       processed_inputs))\n        energies = energies.squeeze(-1)\n        return energies, processed_query\n\n    def get_attention(self, query, processed_inputs):\n        processed_query = self.query_layer(query.unsqueeze(1))\n        energies = self.v(torch.tanh(processed_query + processed_inputs))\n        energies = energies.squeeze(-1)\n        return energies, processed_query\n\n    def apply_windowing(self, attention, inputs):\n        back_win = self.win_idx - self.win_back\n        front_win = self.win_idx + self.win_front\n        if back_win > 0:\n            attention[:, :back_win] = -float(""inf"")\n        if front_win < inputs.shape[1]:\n            attention[:, front_win:] = -float(""inf"")\n        # this is a trick to solve a special problem.\n        # but it does not hurt.\n        if self.win_idx == -1:\n            attention[:, 0] = attention.max()\n        # Update the window\n        self.win_idx = torch.argmax(attention, 1).long()[0].item()\n        return attention\n\n    def apply_forward_attention(self, alignment):\n        # forward attention\n        fwd_shifted_alpha = F.pad(self.alpha[:, :-1].clone().to(alignment.device),\n                            (1, 0, 0, 0))\n        # compute transition potentials\n        alpha = ((1 - self.u) * self.alpha\n                 + self.u * fwd_shifted_alpha\n                 + 1e-8) * alignment\n        # force incremental alignment\n        if not self.training and self.forward_attn_mask:\n            _, n = fwd_shifted_alpha.max(1)\n            val, n2 = alpha.max(1)\n            for b in range(alignment.shape[0]):\n                alpha[b, n[b] + 3:] = 0\n                alpha[b, :(\n                    n[b] - 1\n                )] = 0  # ignore all previous states to prevent repetition.\n                alpha[b,\n                      (n[b] - 2\n                       )] = 0.01 * val[b]  # smoothing factor for the prev step\n        # renormalize attention weights\n        alpha = alpha / alpha.sum(dim=1, keepdim=True)\n        return alpha\n\n    def forward(self, query, inputs, processed_inputs, mask):\n        """"""\n        shapes:\n            query: B x D_attn_rnn\n            inputs: B x T_en x D_en\n            processed_inputs:: B x T_en x D_attn\n            mask: B x T_en\n        """"""\n        if self.location_attention:\n            attention, _ = self.get_location_attention(\n                query, processed_inputs)\n        else:\n            attention, _ = self.get_attention(\n                query, processed_inputs)\n        # apply masking\n        if mask is not None:\n            attention.data.masked_fill_(~mask, self._mask_value)\n        # apply windowing - only in eval mode\n        if not self.training and self.windowing:\n            attention = self.apply_windowing(attention, inputs)\n\n        # normalize attention values\n        if self.norm == ""softmax"":\n            alignment = torch.softmax(attention, dim=-1)\n        elif self.norm == ""sigmoid"":\n            alignment = torch.sigmoid(attention) / torch.sigmoid(\n                attention).sum(\n                    dim=1, keepdim=True)\n        else:\n            raise ValueError(""Unknown value for attention norm type"")\n\n        if self.location_attention:\n            self.update_location_attention(alignment)\n\n        # apply forward attention if enabled\n        if self.forward_attn:\n            alignment = self.apply_forward_attention(alignment)\n            self.alpha = alignment\n\n        context = torch.bmm(alignment.unsqueeze(1), inputs)\n        context = context.squeeze(1)\n        self.attention_weights = alignment\n\n        # compute transition agent\n        if self.forward_attn and self.trans_agent:\n            ta_input = torch.cat([context, query.squeeze(1)], dim=-1)\n            self.u = torch.sigmoid(self.ta(ta_input))\n        return context\n\n\ndef init_attn(attn_type, query_dim, embedding_dim, attention_dim,\n              location_attention, attention_location_n_filters,\n              attention_location_kernel_size, windowing, norm, forward_attn,\n              trans_agent, forward_attn_mask, attn_K):\n    if attn_type == ""original"":\n        return OriginalAttention(query_dim, embedding_dim, attention_dim,\n                                 location_attention,\n                                 attention_location_n_filters,\n                                 attention_location_kernel_size, windowing,\n                                 norm, forward_attn, trans_agent,\n                                 forward_attn_mask)\n    if attn_type == ""graves"":\n        return GravesAttention(query_dim, attn_K)\n    raise RuntimeError(\n        "" [!] Given Attention Type \'{attn_type}\' is not exist."")\n'"
layers/custom_layers.py,0,"b'# coding: utf-8\n# import torch\n# from torch import nn\n\n# class StopProjection(nn.Module):\n#     r"""""" Simple projection layer to predict the ""stop token""\n\n#     Args:\n#         in_features (int): size of the input vector\n#         out_features (int or list): size of each output vector. aka number\n#             of predicted frames.\n#     """"""\n\n#     def __init__(self, in_features, out_features):\n#         super(StopProjection, self).__init__()\n#         self.linear = nn.Linear(in_features, out_features)\n#         self.dropout = nn.Dropout(0.5)\n#         self.sigmoid = nn.Sigmoid()\n\n#     def forward(self, inputs):\n#         out = self.dropout(inputs)\n#         out = self.linear(out)\n#         out = self.sigmoid(out)\n#         return out\n'"
layers/gst_layers.py,14,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass GST(nn.Module):\n    """"""Global Style Token Module for factorizing prosody in speech.\n\n    See https://arxiv.org/pdf/1803.09017""""""\n\n    def __init__(self, num_mel, num_heads, num_style_tokens, embedding_dim):\n        super().__init__()\n        self.encoder = ReferenceEncoder(num_mel, embedding_dim)\n        self.style_token_layer = StyleTokenLayer(num_heads, num_style_tokens,\n                                                 embedding_dim)\n\n    def forward(self, inputs):\n        enc_out = self.encoder(inputs)\n        style_embed = self.style_token_layer(enc_out)\n\n        return style_embed\n\n\nclass ReferenceEncoder(nn.Module):\n    """"""NN module creating a fixed size prosody embedding from a spectrogram.\n\n    inputs: mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    """"""\n\n    def __init__(self, num_mel, embedding_dim):\n\n        super().__init__()\n        self.num_mel = num_mel\n        filters = [1] + [32, 32, 64, 64, 128, 128]\n        num_layers = len(filters) - 1\n        convs = [\n            nn.Conv2d(\n                in_channels=filters[i],\n                out_channels=filters[i + 1],\n                kernel_size=(3, 3),\n                stride=(2, 2),\n                padding=(1, 1)) for i in range(num_layers)\n        ]\n        self.convs = nn.ModuleList(convs)\n        self.bns = nn.ModuleList([\n            nn.BatchNorm2d(num_features=filter_size)\n            for filter_size in filters[1:]\n        ])\n\n        post_conv_height = self.calculate_post_conv_height(\n            num_mel, 3, 2, 1, num_layers)\n        self.recurrence = nn.GRU(\n            input_size=filters[-1] * post_conv_height,\n            hidden_size=embedding_dim // 2,\n            batch_first=True)\n\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        x = inputs.view(batch_size, 1, -1, self.num_mel)\n        # x: 4D tensor [batch_size, num_channels==1, num_frames, num_mel]\n        for conv, bn in zip(self.convs, self.bns):\n            x = conv(x)\n            x = bn(x)\n            x = F.relu(x)\n\n        x = x.transpose(1, 2)\n        # x: 4D tensor [batch_size, post_conv_width,\n        #               num_channels==128, post_conv_height]\n        post_conv_width = x.size(1)\n        x = x.contiguous().view(batch_size, post_conv_width, -1)\n        # x: 3D tensor [batch_size, post_conv_width,\n        #               num_channels*post_conv_height]\n        self.recurrence.flatten_parameters()\n        memory, out = self.recurrence(x)\n        # out: 3D tensor [seq_len==1, batch_size, encoding_size=128]\n\n        return out.squeeze(0)\n\n    @staticmethod\n    def calculate_post_conv_height(height, kernel_size, stride, pad,\n                                   n_convs):\n        """"""Height of spec after n convolutions with fixed kernel/stride/pad.""""""\n        for _ in range(n_convs):\n            height = (height - kernel_size + 2 * pad) // stride + 1\n        return height\n\n\nclass StyleTokenLayer(nn.Module):\n    """"""NN Module attending to style tokens based on prosody encodings.""""""\n\n    def __init__(self, num_heads, num_style_tokens,\n                 embedding_dim):\n        super().__init__()\n        self.query_dim = embedding_dim // 2\n        self.key_dim = embedding_dim // num_heads\n        self.style_tokens = nn.Parameter(\n            torch.FloatTensor(num_style_tokens, self.key_dim))\n        nn.init.orthogonal_(self.style_tokens)\n        self.attention = MultiHeadAttention(\n            query_dim=self.query_dim,\n            key_dim=self.key_dim,\n            num_units=embedding_dim,\n            num_heads=num_heads)\n\n    def forward(self, inputs):\n        batch_size = inputs.size(0)\n        prosody_encoding = inputs.unsqueeze(1)\n        # prosody_encoding: 3D tensor [batch_size, 1, encoding_size==128]\n        tokens = torch.tanh(self.style_tokens) \\\n            .unsqueeze(0) \\\n            .expand(batch_size, -1, -1)\n        # tokens: 3D tensor [batch_size, num tokens, token embedding size]\n        style_embed = self.attention(prosody_encoding, tokens)\n\n        return style_embed\n\n\nclass MultiHeadAttention(nn.Module):\n    \'\'\'\n    input:\n        query --- [N, T_q, query_dim]\n        key --- [N, T_k, key_dim]\n    output:\n        out --- [N, T_q, num_units]\n    \'\'\'\n\n    def __init__(self, query_dim, key_dim, num_units, num_heads):\n\n        super().__init__()\n        self.num_units = num_units\n        self.num_heads = num_heads\n        self.key_dim = key_dim\n\n        self.W_query = nn.Linear(\n            in_features=query_dim, out_features=num_units, bias=False)\n        self.W_key = nn.Linear(\n            in_features=key_dim, out_features=num_units, bias=False)\n        self.W_value = nn.Linear(\n            in_features=key_dim, out_features=num_units, bias=False)\n\n    def forward(self, query, key):\n        queries = self.W_query(query)  # [N, T_q, num_units]\n        keys = self.W_key(key)  # [N, T_k, num_units]\n        values = self.W_value(key)\n\n        split_size = self.num_units // self.num_heads\n        queries = torch.stack(\n            torch.split(queries, split_size, dim=2),\n            dim=0)  # [h, N, T_q, num_units/h]\n        keys = torch.stack(\n            torch.split(keys, split_size, dim=2),\n            dim=0)  # [h, N, T_k, num_units/h]\n        values = torch.stack(\n            torch.split(values, split_size, dim=2),\n            dim=0)  # [h, N, T_k, num_units/h]\n\n        # score = softmax(QK^T / (d_k ** 0.5))\n        scores = torch.matmul(queries, keys.transpose(2, 3))  # [h, N, T_q, T_k]\n        scores = scores / (self.key_dim**0.5)\n        scores = F.softmax(scores, dim=3)\n\n        # out = score * V\n        out = torch.matmul(scores, values)  # [h, N, T_q, num_units/h]\n        out = torch.cat(\n            torch.split(out, 1, dim=0),\n            dim=3).squeeze(0)  # [N, T_q, num_units]\n\n        return out\n'"
layers/losses.py,13,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn import functional\nfrom TTS.utils.generic_utils import sequence_mask\n\n\nclass L1LossMasked(nn.Module):\n\n    def __init__(self, seq_len_norm):\n        super(L1LossMasked, self).__init__()\n        self.seq_len_norm = seq_len_norm\n\n    def forward(self, x, target, length):\n        """"""\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len, dim) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        """"""\n        # mask: (batch, max_len, 1)\n        target.requires_grad = False\n        mask = sequence_mask(\n            sequence_length=length, max_len=target.size(1)).unsqueeze(2).float()\n        if self.seq_len_norm:\n            norm_w = mask / mask.sum(dim=1, keepdim=True)\n            out_weights = norm_w.div(target.shape[0] * target.shape[2])\n            mask = mask.expand_as(x)\n            loss = functional.l1_loss(\n                x * mask, target * mask, reduction=\'none\')\n            loss = loss.mul(out_weights.to(loss.device)).sum()\n        else:\n            mask = mask.expand_as(x)\n            loss = functional.l1_loss(\n                x * mask, target * mask, reduction=\'sum\')\n            loss = loss / mask.sum()\n        return loss\n\n\nclass MSELossMasked(nn.Module):\n\n    def __init__(self, seq_len_norm):\n        super(MSELossMasked, self).__init__()\n        self.seq_len_norm = seq_len_norm\n\n    def forward(self, x, target, length):\n        """"""\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len, dim) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        """"""\n        # mask: (batch, max_len, 1)\n        target.requires_grad = False\n        mask = sequence_mask(\n            sequence_length=length, max_len=target.size(1)).unsqueeze(2).float()\n        if self.seq_len_norm:\n            norm_w = mask / mask.sum(dim=1, keepdim=True)\n            out_weights = norm_w.div(target.shape[0] * target.shape[2])\n            mask = mask.expand_as(x)\n            loss = functional.mse_loss(\n                x * mask, target * mask, reduction=\'none\')\n            loss = loss.mul(out_weights.to(loss.device)).sum()\n        else:\n            mask = mask.expand_as(x)\n            loss = functional.mse_loss(\n                x * mask, target * mask, reduction=\'sum\')\n            loss = loss / mask.sum()\n        return loss\n\n\nclass AttentionEntropyLoss(nn.Module):\n    # pylint: disable=R0201\n    def forward(self, align):\n        """"""\n        Forces attention to be more decisive by penalizing\n        soft attention weights\n\n        TODO: arguments\n        TODO: unit_test\n        """"""\n        entropy = torch.distributions.Categorical(probs=align).entropy()\n        loss = (entropy / np.log(align.shape[1])).mean()\n        return loss\n\n\nclass BCELossMasked(nn.Module):\n\n    def __init__(self, pos_weight):\n        super(BCELossMasked, self).__init__()\n        self.pos_weight = pos_weight\n\n    def forward(self, x, target, length):\n        """"""\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len) which contains the\n                unnormalized probability for each class.\n            target: A Variable containing a LongTensor of size\n                (batch, max_len) which contains the index of the true\n                class for each corresponding step.\n            length: A Variable containing a LongTensor of size (batch,)\n                which contains the length of each data in a batch.\n        Returns:\n            loss: An average loss value in range [0, 1] masked by the length.\n        """"""\n        # mask: (batch, max_len, 1)\n        target.requires_grad = False\n        mask = sequence_mask(sequence_length=length, max_len=target.size(1)).float()\n        loss = functional.binary_cross_entropy_with_logits(\n            x * mask, target * mask, pos_weight=self.pos_weight, reduction=\'sum\')\n        loss = loss / mask.sum()\n        return loss\n\n\nclass GuidedAttentionLoss(torch.nn.Module):\n    def __init__(self, sigma=0.4):\n        super(GuidedAttentionLoss, self).__init__()\n        self.sigma = sigma\n\n    def _make_ga_masks(self, ilens, olens):\n        B = len(ilens)\n        max_ilen = max(ilens)\n        max_olen = max(olens)\n        ga_masks = torch.zeros((B, max_olen, max_ilen))\n        for idx, (ilen, olen) in enumerate(zip(ilens, olens)):\n            ga_masks[idx, :olen, :ilen] = self._make_ga_mask(ilen, olen, self.sigma)\n        return ga_masks\n\n    def forward(self, att_ws, ilens, olens):\n        ga_masks = self._make_ga_masks(ilens, olens).to(att_ws.device)\n        seq_masks = self._make_masks(ilens, olens).to(att_ws.device)\n        losses = ga_masks * att_ws\n        loss = torch.mean(losses.masked_select(seq_masks))\n        return loss\n\n    @staticmethod\n    def _make_ga_mask(ilen, olen, sigma):\n        grid_x, grid_y = torch.meshgrid(torch.arange(olen), torch.arange(ilen))\n        grid_x, grid_y = grid_x.float(), grid_y.float()\n        return 1.0 - torch.exp(-(grid_y / ilen - grid_x / olen) ** 2 / (2 * (sigma ** 2)))\n\n    @staticmethod\n    def _make_masks(ilens, olens):\n        in_masks = sequence_mask(ilens)\n        out_masks = sequence_mask(olens)\n        return out_masks.unsqueeze(-1) & in_masks.unsqueeze(-2)\n\n\nclass TacotronLoss(torch.nn.Module):\n    def __init__(self, c, stopnet_pos_weight=10, ga_sigma=0.4):\n        super(TacotronLoss, self).__init__()\n        self.stopnet_pos_weight = stopnet_pos_weight\n        self.ga_alpha = c.ga_alpha\n        self.config = c\n        # postnet decoder loss\n        if c.loss_masking:\n            self.criterion = L1LossMasked(c.seq_len_norm) if c.model in [\n                ""Tacotron""\n            ] else MSELossMasked(c.seq_len_norm)\n        else:\n            self.criterion = nn.L1Loss() if c.model in [""Tacotron""\n                                                        ] else nn.MSELoss()\n        # guided attention loss\n        if c.ga_alpha > 0:\n            self.criterion_ga = GuidedAttentionLoss(sigma=ga_sigma)\n        # stopnet loss\n        # pylint: disable=not-callable\n        self.criterion_st = BCELossMasked(pos_weight=torch.tensor(stopnet_pos_weight)) if c.stopnet else None\n\n    def forward(self, postnet_output, decoder_output, mel_input, linear_input,\n                stopnet_output, stopnet_target, output_lens, decoder_b_output,\n                alignments, alignment_lens, input_lens):\n\n        return_dict = {}\n        # decoder and postnet losses\n        if self.config.loss_masking:\n            decoder_loss = self.criterion(decoder_output, mel_input,\n                                          output_lens)\n            if self.config.model in [""Tacotron"", ""TacotronGST""]:\n                postnet_loss = self.criterion(postnet_output, linear_input,\n                                              output_lens)\n            else:\n                postnet_loss = self.criterion(postnet_output, mel_input,\n                                              output_lens)\n        else:\n            decoder_loss = self.criterion(decoder_output, mel_input)\n            if self.config.model in [""Tacotron"", ""TacotronGST""]:\n                postnet_loss = self.criterion(postnet_output, linear_input)\n            else:\n                postnet_loss = self.criterion(postnet_output, mel_input)\n        loss = decoder_loss + postnet_loss\n        return_dict[\'decoder_loss\'] = decoder_loss\n        return_dict[\'postnet_loss\'] = postnet_loss\n\n        # stopnet loss\n        stop_loss = self.criterion_st(\n            stopnet_output, stopnet_target,\n            output_lens) if self.config.stopnet else torch.zeros(1)\n        if not self.config.separate_stopnet and self.config.stopnet:\n            loss += stop_loss\n        return_dict[\'stopnet_loss\'] = stop_loss\n\n        # backward decoder loss (if enabled)\n        if self.config.bidirectional_decoder:\n            if self.config.loss_masking:\n                decoder_b_loss = self.criterion(torch.flip(decoder_b_output, dims=(1, )), mel_input, output_lens)\n            else:\n                decoder_b_loss = self.criterion(torch.flip(decoder_b_output, dims=(1, )), mel_input)\n            decoder_c_loss = torch.nn.functional.l1_loss(torch.flip(decoder_b_output, dims=(1, )), decoder_output)\n            loss += decoder_b_loss + decoder_c_loss\n            return_dict[\'decoder_b_loss\'] = decoder_b_loss\n            return_dict[\'decoder_c_loss\'] = decoder_c_loss\n\n        # guided attention loss (if enabled)\n        if self.config.ga_alpha > 0:\n            ga_loss = self.criterion_ga(alignments, input_lens, alignment_lens)\n            loss += ga_loss * self.ga_alpha\n            return_dict[\'ga_loss\'] = ga_loss * self.ga_alpha\n\n        return_dict[\'loss\'] = loss\n        return return_dict\n\n'"
layers/tacotron.py,26,"b'# coding: utf-8\nimport torch\nfrom torch import nn\nfrom .common_layers import Prenet, init_attn, Linear\n\n\nclass BatchNormConv1d(nn.Module):\n    r""""""A wrapper for Conv1d with BatchNorm. It sets the activation\n    function between Conv and BatchNorm layers. BatchNorm layer\n    is initialized with the TF default values for momentum and eps.\n\n    Args:\n        in_channels: size of each input sample\n        out_channels: size of each output samples\n        kernel_size: kernel size of conv filters\n        stride: stride of conv filters\n        padding: padding of conv filters\n        activation: activation function set b/w Conv1d and BatchNorm\n\n    Shapes:\n        - input: batch x dims\n        - output: batch x dims\n    """"""\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding,\n                 activation=None):\n\n        super(BatchNormConv1d, self).__init__()\n        self.padding = padding\n        self.padder = nn.ConstantPad1d(padding, 0)\n        self.conv1d = nn.Conv1d(\n            in_channels,\n            out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=0,\n            bias=False)\n        # Following tensorflow\'s default parameters\n        self.bn = nn.BatchNorm1d(out_channels, momentum=0.99, eps=1e-3)\n        self.activation = activation\n        # self.init_layers()\n\n    def init_layers(self):\n        if type(self.activation) == torch.nn.ReLU:\n            w_gain = \'relu\'\n        elif type(self.activation) == torch.nn.Tanh:\n            w_gain = \'tanh\'\n        elif self.activation is None:\n            w_gain = \'linear\'\n        else:\n            raise RuntimeError(\'Unknown activation function\')\n        torch.nn.init.xavier_uniform_(\n            self.conv1d.weight, gain=torch.nn.init.calculate_gain(w_gain))\n\n    def forward(self, x):\n        x = self.padder(x)\n        x = self.conv1d(x)\n        x = self.bn(x)\n        if self.activation is not None:\n            x = self.activation(x)\n        return x\n\n\nclass Highway(nn.Module):\n    # TODO: Try GLU layer\n    def __init__(self, in_size, out_size):\n        super(Highway, self).__init__()\n        self.H = nn.Linear(in_size, out_size)\n        self.H.bias.data.zero_()\n        self.T = nn.Linear(in_size, out_size)\n        self.T.bias.data.fill_(-1)\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        # self.init_layers()\n\n    def init_layers(self):\n        torch.nn.init.xavier_uniform_(\n            self.H.weight, gain=torch.nn.init.calculate_gain(\'relu\'))\n        torch.nn.init.xavier_uniform_(\n            self.T.weight, gain=torch.nn.init.calculate_gain(\'sigmoid\'))\n\n    def forward(self, inputs):\n        H = self.relu(self.H(inputs))\n        T = self.sigmoid(self.T(inputs))\n        return H * T + inputs * (1.0 - T)\n\n\nclass CBHG(nn.Module):\n    """"""CBHG module: a recurrent neural network composed of:\n        - 1-d convolution banks\n        - Highway networks + residual connections\n        - Bidirectional gated recurrent units\n\n        Args:\n            in_features (int): sample size\n            K (int): max filter size in conv bank\n            projections (list): conv channel sizes for conv projections\n            num_highways (int): number of highways layers\n\n        Shapes:\n            - input: B x D x T_in\n            - output: B x T_in x D*2\n    """"""\n\n    def __init__(self,\n                 in_features,\n                 K=16,\n                 conv_bank_features=128,\n                 conv_projections=[128, 128],\n                 highway_features=128,\n                 gru_features=128,\n                 num_highways=4):\n        super(CBHG, self).__init__()\n        self.in_features = in_features\n        self.conv_bank_features = conv_bank_features\n        self.highway_features = highway_features\n        self.gru_features = gru_features\n        self.conv_projections = conv_projections\n        self.relu = nn.ReLU()\n        # list of conv1d bank with filter size k=1...K\n        # TODO: try dilational layers instead\n        self.conv1d_banks = nn.ModuleList([\n            BatchNormConv1d(in_features,\n                            conv_bank_features,\n                            kernel_size=k,\n                            stride=1,\n                            padding=[(k - 1) // 2, k // 2],\n                            activation=self.relu) for k in range(1, K + 1)\n        ])\n        # max pooling of conv bank, with padding\n        # TODO: try average pooling OR larger kernel size\n        out_features = [K * conv_bank_features] + conv_projections[:-1]\n        activations = [self.relu] * (len(conv_projections) - 1)\n        activations += [None]\n        # setup conv1d projection layers\n        layer_set = []\n        for (in_size, out_size, ac) in zip(out_features, conv_projections,\n                                           activations):\n            layer = BatchNormConv1d(in_size,\n                                    out_size,\n                                    kernel_size=3,\n                                    stride=1,\n                                    padding=[1, 1],\n                                    activation=ac)\n            layer_set.append(layer)\n        self.conv1d_projections = nn.ModuleList(layer_set)\n        # setup Highway layers\n        if self.highway_features != conv_projections[-1]:\n            self.pre_highway = nn.Linear(conv_projections[-1],\n                                         highway_features,\n                                         bias=False)\n        self.highways = nn.ModuleList([\n            Highway(highway_features, highway_features)\n            for _ in range(num_highways)\n        ])\n        # bi-directional GPU layer\n        self.gru = nn.GRU(gru_features,\n                          gru_features,\n                          1,\n                          batch_first=True,\n                          bidirectional=True)\n\n    def forward(self, inputs):\n        # (B, in_features, T_in)\n        x = inputs\n        # (B, hid_features*K, T_in)\n        # Concat conv1d bank outputs\n        outs = []\n        for conv1d in self.conv1d_banks:\n            out = conv1d(x)\n            outs.append(out)\n        x = torch.cat(outs, dim=1)\n        assert x.size(1) == self.conv_bank_features * len(self.conv1d_banks)\n        for conv1d in self.conv1d_projections:\n            x = conv1d(x)\n        x += inputs\n        x = x.transpose(1, 2)\n        if self.highway_features != self.conv_projections[-1]:\n            x = self.pre_highway(x)\n        # Residual connection\n        # TODO: try residual scaling as in Deep Voice 3\n        # TODO: try plain residual layers\n        for highway in self.highways:\n            x = highway(x)\n        # (B, T_in, hid_features*2)\n        # TODO: replace GRU with convolution as in Deep Voice 3\n        self.gru.flatten_parameters()\n        outputs, _ = self.gru(x)\n        return outputs\n\n\nclass EncoderCBHG(nn.Module):\n    def __init__(self):\n        super(EncoderCBHG, self).__init__()\n        self.cbhg = CBHG(\n            128,\n            K=16,\n            conv_bank_features=128,\n            conv_projections=[128, 128],\n            highway_features=128,\n            gru_features=128,\n            num_highways=4)\n\n    def forward(self, x):\n        return self.cbhg(x)\n\n\nclass Encoder(nn.Module):\n    r""""""Encapsulate Prenet and CBHG modules for encoder""""""\n\n    def __init__(self, in_features):\n        super(Encoder, self).__init__()\n        self.prenet = Prenet(in_features, out_features=[256, 128])\n        self.cbhg = EncoderCBHG()\n\n    def forward(self, inputs):\n        r""""""\n        Args:\n            inputs (FloatTensor): embedding features\n\n        Shapes:\n            - inputs: batch x time x in_features\n            - outputs: batch x time x 128*2\n        """"""\n        # B x T x prenet_dim\n        outputs = self.prenet(inputs)\n        outputs = self.cbhg(outputs.transpose(1, 2))\n        return outputs\n\n\nclass PostCBHG(nn.Module):\n    def __init__(self, mel_dim):\n        super(PostCBHG, self).__init__()\n        self.cbhg = CBHG(\n            mel_dim,\n            K=8,\n            conv_bank_features=128,\n            conv_projections=[256, mel_dim],\n            highway_features=128,\n            gru_features=128,\n            num_highways=4)\n\n    def forward(self, x):\n        return self.cbhg(x)\n\n\nclass Decoder(nn.Module):\n    """"""Decoder module.\n\n    Args:\n        in_features (int): input vector (encoder output) sample size.\n        memory_dim (int): memory vector (prev. time-step output) sample size.\n        r (int): number of outputs per time step.\n        memory_size (int): size of the past window. if <= 0 memory_size = r\n        TODO: arguments\n    """"""\n\n    # Pylint gets confused by PyTorch conventions here\n    #pylint: disable=attribute-defined-outside-init\n\n    def __init__(self, in_features, memory_dim, r, memory_size, attn_type, attn_windowing,\n                 attn_norm, prenet_type, prenet_dropout, forward_attn,\n                 trans_agent, forward_attn_mask, location_attn, attn_K,\n                 separate_stopnet, speaker_embedding_dim):\n        super(Decoder, self).__init__()\n        self.r_init = r\n        self.r = r\n        self.in_features = in_features\n        self.max_decoder_steps = 500\n        self.use_memory_queue = memory_size > 0\n        self.memory_size = memory_size if memory_size > 0 else r\n        self.memory_dim = memory_dim\n        self.separate_stopnet = separate_stopnet\n        self.query_dim = 256\n        # memory -> |Prenet| -> processed_memory\n        prenet_dim = memory_dim * self.memory_size + speaker_embedding_dim if self.use_memory_queue else memory_dim + speaker_embedding_dim\n        self.prenet = Prenet(\n            prenet_dim,\n            prenet_type,\n            prenet_dropout,\n            out_features=[256, 128])\n        # processed_inputs, processed_memory -> |Attention| -> Attention, attention, RNN_State\n        # attention_rnn generates queries for the attention mechanism\n        self.attention_rnn = nn.GRUCell(in_features + 128, self.query_dim)\n\n        self.attention = init_attn(attn_type=attn_type,\n                                   query_dim=self.query_dim,\n                                   embedding_dim=in_features,\n                                   attention_dim=128,\n                                   location_attention=location_attn,\n                                   attention_location_n_filters=32,\n                                   attention_location_kernel_size=31,\n                                   windowing=attn_windowing,\n                                   norm=attn_norm,\n                                   forward_attn=forward_attn,\n                                   trans_agent=trans_agent,\n                                   forward_attn_mask=forward_attn_mask,\n                                   attn_K=attn_K)\n        # (processed_memory | attention context) -> |Linear| -> decoder_RNN_input\n        self.project_to_decoder_in = nn.Linear(256 + in_features, 256)\n        # decoder_RNN_input -> |RNN| -> RNN_state\n        self.decoder_rnns = nn.ModuleList(\n            [nn.GRUCell(256, 256) for _ in range(2)])\n        # RNN_state -> |Linear| -> mel_spec\n        self.proj_to_mel = nn.Linear(256, memory_dim * self.r_init)\n        # learn init values instead of zero init.\n        self.stopnet = StopNet(256 + memory_dim * self.r_init)\n\n    def set_r(self, new_r):\n        self.r = new_r\n\n    def _reshape_memory(self, memory):\n        """"""\n        Reshape the spectrograms for given \'r\'\n        """"""\n        # Grouping multiple frames if necessary\n        if memory.size(-1) == self.memory_dim:\n            memory = memory.view(memory.shape[0], memory.size(1) // self.r, -1)\n        # Time first (T_decoder, B, memory_dim)\n        memory = memory.transpose(0, 1)\n        return memory\n\n    def _init_states(self, inputs):\n        """"""\n        Initialization of decoder states\n        """"""\n        B = inputs.size(0)\n        T = inputs.size(1)\n        # go frame as zeros matrix\n        if self.use_memory_queue:\n            self.memory_input = torch.zeros(1, device=inputs.device).repeat(B, self.memory_dim * self.memory_size)\n        else:\n            self.memory_input = torch.zeros(1, device=inputs.device).repeat(B, self.memory_dim)\n        # decoder states\n        self.attention_rnn_hidden = torch.zeros(1, device=inputs.device).repeat(B, 256)\n        self.decoder_rnn_hiddens = [\n            torch.zeros(1, device=inputs.device).repeat(B, 256)\n            for idx in range(len(self.decoder_rnns))\n        ]\n        self.context_vec = inputs.data.new(B, self.in_features).zero_()\n        # cache attention inputs\n        self.processed_inputs = self.attention.preprocess_inputs(inputs)\n\n    def _parse_outputs(self, outputs, attentions, stop_tokens):\n        # Back to batch first\n        attentions = torch.stack(attentions).transpose(0, 1)\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        outputs = outputs.view(\n            outputs.size(0), -1, self.memory_dim)\n        outputs = outputs.transpose(1, 2)\n        return outputs, attentions, stop_tokens\n\n    def decode(self, inputs, mask=None):\n        # Prenet\n        processed_memory = self.prenet(self.memory_input)\n        # Attention RNN\n        self.attention_rnn_hidden = self.attention_rnn(\n            torch.cat((processed_memory, self.context_vec), -1),\n            self.attention_rnn_hidden)\n        self.context_vec = self.attention(\n            self.attention_rnn_hidden, inputs, self.processed_inputs, mask)\n        # Concat RNN output and attention context vector\n        decoder_input = self.project_to_decoder_in(\n            torch.cat((self.attention_rnn_hidden, self.context_vec), -1))\n\n        # Pass through the decoder RNNs\n        for idx in range(len(self.decoder_rnns)):\n            self.decoder_rnn_hiddens[idx] = self.decoder_rnns[idx](\n                decoder_input, self.decoder_rnn_hiddens[idx])\n            # Residual connection\n            decoder_input = self.decoder_rnn_hiddens[idx] + decoder_input\n        decoder_output = decoder_input\n\n        # predict mel vectors from decoder vectors\n        output = self.proj_to_mel(decoder_output)\n        # output = torch.sigmoid(output)\n        # predict stop token\n        stopnet_input = torch.cat([decoder_output, output], -1)\n        if self.separate_stopnet:\n            stop_token = self.stopnet(stopnet_input.detach())\n        else:\n            stop_token = self.stopnet(stopnet_input)\n        output = output[:, : self.r * self.memory_dim]\n        return output, stop_token, self.attention.attention_weights\n\n    def _update_memory_input(self, new_memory):\n        if self.use_memory_queue:\n            if self.memory_size > self.r:\n                # memory queue size is larger than number of frames per decoder iter\n                self.memory_input = torch.cat([\n                    new_memory, self.memory_input[:, :(\n                        self.memory_size - self.r) * self.memory_dim].clone()\n                ], dim=-1)\n            else:\n                # memory queue size smaller than number of frames per decoder iter\n                self.memory_input = new_memory[:, :self.memory_size * self.memory_dim]\n        else:\n            # use only the last frame prediction\n            # assert new_memory.shape[-1] == self.r * self.memory_dim\n            self.memory_input = new_memory[:, self.memory_dim * (self.r - 1):]\n\n    def forward(self, inputs, memory, mask, speaker_embeddings=None):\n        """"""\n        Args:\n            inputs: Encoder outputs.\n            memory: Decoder memory (autoregression. If None (at eval-time),\n              decoder outputs are used as decoder inputs. If None, it uses the last\n              output as the input.\n            mask: Attention mask for sequence padding.\n\n        Shapes:\n            - inputs: batch x time x encoder_out_dim\n            - memory: batch x #mel_specs x mel_spec_dim\n        """"""\n        # Run greedy decoding if memory is None\n        memory = self._reshape_memory(memory)\n        outputs = []\n        attentions = []\n        stop_tokens = []\n        t = 0\n        self._init_states(inputs)\n        self.attention.init_states(inputs)\n        while len(outputs) < memory.size(0):\n            if t > 0:\n                new_memory = memory[t - 1]\n                self._update_memory_input(new_memory)\n            if speaker_embeddings is not None:\n                self.memory_input = torch.cat([self.memory_input, speaker_embeddings], dim=-1)\n            output, stop_token, attention = self.decode(inputs, mask)\n            outputs += [output]\n            attentions += [attention]\n            stop_tokens += [stop_token.squeeze(1)]\n            t += 1\n        return self._parse_outputs(outputs, attentions, stop_tokens)\n\n    def inference(self, inputs, speaker_embeddings=None):\n        """"""\n        Args:\n            inputs: encoder outputs.\n            speaker_embeddings: speaker vectors.\n\n        Shapes:\n            - inputs: batch x time x encoder_out_dim\n            - speaker_embeddings: batch x embed_dim\n        """"""\n        outputs = []\n        attentions = []\n        stop_tokens = []\n        t = 0\n        self._init_states(inputs)\n        self.attention.init_win_idx()\n        self.attention.init_states(inputs)\n        while True:\n            if t > 0:\n                new_memory = outputs[-1]\n                self._update_memory_input(new_memory)\n            if speaker_embeddings is not None:\n                self.memory_input = torch.cat([self.memory_input, speaker_embeddings], dim=-1)\n            output, stop_token, attention = self.decode(inputs, None)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [output]\n            attentions += [attention]\n            stop_tokens += [stop_token]\n            t += 1\n            if t > inputs.shape[1] / 4 and (stop_token > 0.6\n                                            or attention[:, -1].item() > 0.6):\n                break\n            elif t > self.max_decoder_steps:\n                print(""   | > Decoder stopped with \'max_decoder_steps"")\n                break\n        return self._parse_outputs(outputs, attentions, stop_tokens)\n\n\nclass StopNet(nn.Module):\n    r""""""\n    Args:\n        in_features (int): feature dimension of input.\n    """"""\n\n    def __init__(self, in_features):\n        super(StopNet, self).__init__()\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(in_features, 1)\n        torch.nn.init.xavier_uniform_(\n            self.linear.weight, gain=torch.nn.init.calculate_gain(\'linear\'))\n\n    def forward(self, inputs):\n        outputs = self.dropout(inputs)\n        outputs = self.linear(outputs)\n        return outputs\n'"
layers/tacotron2.py,21,"b'import torch\nfrom torch.autograd import Variable\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom .common_layers import init_attn, Prenet, Linear\n\n\nclass ConvBNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, activation=None):\n        super(ConvBNBlock, self).__init__()\n        assert (kernel_size - 1) % 2 == 0\n        padding = (kernel_size - 1) // 2\n        self.convolution1d = nn.Conv1d(in_channels,\n                                       out_channels,\n                                       kernel_size,\n                                       padding=padding)\n        self.batch_normalization = nn.BatchNorm1d(out_channels, momentum=0.1, eps=1e-5)\n        self.dropout = nn.Dropout(p=0.5)\n        if activation == \'relu\':\n            self.activation = nn.ReLU()\n        elif activation == \'tanh\':\n            self.activation = nn.Tanh()\n        else:\n            self.activation = nn.Identity()\n\n    def forward(self, x):\n        o = self.convolution1d(x)\n        o = self.batch_normalization(o)\n        o = self.activation(o)\n        o = self.dropout(o)\n        return o\n\n\nclass Postnet(nn.Module):\n    def __init__(self, output_dim, num_convs=5):\n        super(Postnet, self).__init__()\n        self.convolutions = nn.ModuleList()\n        self.convolutions.append(\n            ConvBNBlock(output_dim, 512, kernel_size=5, activation=\'tanh\'))\n        for _ in range(1, num_convs - 1):\n            self.convolutions.append(\n                ConvBNBlock(512, 512, kernel_size=5, activation=\'tanh\'))\n        self.convolutions.append(\n            ConvBNBlock(512, output_dim, kernel_size=5, activation=None))\n\n    def forward(self, x):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        return o\n\n\nclass Encoder(nn.Module):\n    def __init__(self, output_input_dim=512):\n        super(Encoder, self).__init__()\n        self.convolutions = nn.ModuleList()\n        for _ in range(3):\n            self.convolutions.append(\n                ConvBNBlock(output_input_dim, output_input_dim, 5, \'relu\'))\n        self.lstm = nn.LSTM(output_input_dim,\n                            int(output_input_dim / 2),\n                            num_layers=1,\n                            batch_first=True,\n                            bias=True,\n                            bidirectional=True)\n        self.rnn_state = None\n\n    def forward(self, x, input_lengths):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        o = nn.utils.rnn.pack_padded_sequence(o,\n                                              input_lengths,\n                                              batch_first=True)\n        self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        o, _ = nn.utils.rnn.pad_packed_sequence(o, batch_first=True)\n        return o\n\n    def inference(self, x):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o)\n        o = o.transpose(1, 2)\n        # self.lstm.flatten_parameters()\n        o, _ = self.lstm(o)\n        return o\n\n\n# adapted from https://github.com/NVIDIA/tacotron2/\nclass Decoder(nn.Module):\n    # Pylint gets confused by PyTorch conventions here\n    #pylint: disable=attribute-defined-outside-init\n    def __init__(self, input_dim, frame_dim, r, attn_type, attn_win, attn_norm,\n                 prenet_type, prenet_dropout, forward_attn, trans_agent,\n                 forward_attn_mask, location_attn, attn_K, separate_stopnet,\n                 speaker_embedding_dim):\n        super(Decoder, self).__init__()\n        self.frame_dim = frame_dim\n        self.r_init = r\n        self.r = r\n        self.encoder_embedding_dim = input_dim\n        self.separate_stopnet = separate_stopnet\n        self.max_decoder_steps = 1000\n        self.gate_threshold = 0.5\n\n        # model dimensions\n        self.query_dim = 1024\n        self.decoder_rnn_dim = 1024\n        self.prenet_dim = 256\n        self.attn_dim = 128\n        self.p_attention_dropout = 0.1\n        self.p_decoder_dropout = 0.1\n\n        # memory -> |Prenet| -> processed_memory\n        prenet_dim = self.frame_dim\n        self.prenet = Prenet(prenet_dim,\n                             prenet_type,\n                             prenet_dropout,\n                             out_features=[self.prenet_dim, self.prenet_dim],\n                             bias=False)\n\n        self.attention_rnn = nn.LSTMCell(self.prenet_dim + input_dim,\n                                         self.query_dim,\n                                         bias=True)\n\n        self.attention = init_attn(attn_type=attn_type,\n                                   query_dim=self.query_dim,\n                                   embedding_dim=input_dim,\n                                   attention_dim=128,\n                                   location_attention=location_attn,\n                                   attention_location_n_filters=32,\n                                   attention_location_kernel_size=31,\n                                   windowing=attn_win,\n                                   norm=attn_norm,\n                                   forward_attn=forward_attn,\n                                   trans_agent=trans_agent,\n                                   forward_attn_mask=forward_attn_mask,\n                                   attn_K=attn_K)\n\n        self.decoder_rnn = nn.LSTMCell(self.query_dim + input_dim,\n                                       self.decoder_rnn_dim,\n                                       bias=True)\n\n        self.linear_projection = Linear(self.decoder_rnn_dim + input_dim,\n                                        self.frame_dim * self.r_init)\n\n        self.stopnet = nn.Sequential(\n            nn.Dropout(0.1),\n            Linear(self.decoder_rnn_dim + self.frame_dim * self.r_init,\n                   1,\n                   bias=True,\n                   init_gain=\'sigmoid\'))\n        self.memory_truncated = None\n\n    def set_r(self, new_r):\n        self.r = new_r\n\n    def get_go_frame(self, inputs):\n        B = inputs.size(0)\n        memory = torch.zeros(1, device=inputs.device).repeat(B,\n                             self.frame_dim * self.r)\n        return memory\n\n    def _init_states(self, inputs, mask, keep_states=False):\n        B = inputs.size(0)\n        # T = inputs.size(1)\n        if not keep_states:\n            self.query = torch.zeros(1, device=inputs.device).repeat(\n                B, self.query_dim)\n            self.attention_rnn_cell_state = torch.zeros(\n                1, device=inputs.device).repeat(B, self.query_dim)\n            self.decoder_hidden = torch.zeros(1, device=inputs.device).repeat(\n                B, self.decoder_rnn_dim)\n            self.decoder_cell = torch.zeros(1, device=inputs.device).repeat(\n                B, self.decoder_rnn_dim)\n            self.context = torch.zeros(1, device=inputs.device).repeat(\n                B, self.encoder_embedding_dim)\n        self.inputs = inputs\n        self.processed_inputs = self.attention.preprocess_inputs(inputs)\n        self.mask = mask\n\n    def _reshape_memory(self, memory):\n        """"""\n        Reshape the spectrograms for given \'r\'\n        """"""\n        # Grouping multiple frames if necessary\n        if memory.size(-1) == self.frame_dim:\n            memory = memory.view(memory.shape[0], memory.size(1) // self.r, -1)\n        # Time first (T_decoder, B, frame_dim)\n        memory = memory.transpose(0, 1)\n        return memory\n\n    def _parse_outputs(self, outputs, stop_tokens, alignments):\n        alignments = torch.stack(alignments).transpose(0, 1)\n        stop_tokens = torch.stack(stop_tokens).transpose(0, 1)\n        outputs = torch.stack(outputs).transpose(0, 1).contiguous()\n        outputs = outputs.view(outputs.size(0), -1, self.frame_dim)\n        outputs = outputs.transpose(1, 2)\n        return outputs, stop_tokens, alignments\n\n    def _update_memory(self, memory):\n        if len(memory.shape) == 2:\n            return memory[:, self.frame_dim * (self.r - 1):]\n        return memory[:, :, self.frame_dim * (self.r - 1):]\n\n    def decode(self, memory):\n        \'\'\'\n         shapes:\n            - memory: B x r * self.frame_dim\n        \'\'\'\n        # self.context: B x D_en\n        # query_input: B x D_en + (r * self.frame_dim)\n        query_input = torch.cat((memory, self.context), -1)\n        # self.query and self.attention_rnn_cell_state : B x D_attn_rnn\n        self.query, self.attention_rnn_cell_state = self.attention_rnn(\n            query_input, (self.query, self.attention_rnn_cell_state))\n        self.query = F.dropout(self.query, self.p_attention_dropout,\n                               self.training)\n        self.attention_rnn_cell_state = F.dropout(\n            self.attention_rnn_cell_state, self.p_attention_dropout,\n            self.training)\n        # B x D_en\n        self.context = self.attention(self.query, self.inputs,\n                                      self.processed_inputs, self.mask)\n        # B x (D_en + D_attn_rnn)\n        decoder_rnn_input = torch.cat((self.query, self.context), -1)\n        # self.decoder_hidden and self.decoder_cell: B x D_decoder_rnn\n        self.decoder_hidden, self.decoder_cell = self.decoder_rnn(\n            decoder_rnn_input, (self.decoder_hidden, self.decoder_cell))\n        self.decoder_hidden = F.dropout(self.decoder_hidden,\n                                        self.p_decoder_dropout, self.training)\n        # B x (D_decoder_rnn + D_en)\n        decoder_hidden_context = torch.cat((self.decoder_hidden, self.context),\n                                           dim=1)\n        # B x (self.r * self.frame_dim)\n        decoder_output = self.linear_projection(decoder_hidden_context)\n        # B x (D_decoder_rnn + (self.r * self.frame_dim))\n        stopnet_input = torch.cat((self.decoder_hidden, decoder_output), dim=1)\n        if self.separate_stopnet:\n            stop_token = self.stopnet(stopnet_input.detach())\n        else:\n            stop_token = self.stopnet(stopnet_input)\n        # select outputs for the reduction rate self.r\n        decoder_output = decoder_output[:, :self.r * self.frame_dim]\n        return decoder_output, self.attention.attention_weights, stop_token\n\n    def forward(self, inputs, memories, mask, speaker_embeddings=None):\n        memory = self.get_go_frame(inputs).unsqueeze(0)\n        memories = self._reshape_memory(memories)\n        memories = torch.cat((memory, memories), dim=0)\n        memories = self._update_memory(memories)\n        if speaker_embeddings is not None:\n            memories = torch.cat([memories, speaker_embeddings], dim=-1)\n        memories = self.prenet(memories)\n\n        self._init_states(inputs, mask=mask)\n        self.attention.init_states(inputs)\n\n        outputs, stop_tokens, alignments = [], [], []\n        while len(outputs) < memories.size(0) - 1:\n            memory = memories[len(outputs)]\n            decoder_output, attention_weights, stop_token = self.decode(memory)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token.squeeze(1)]\n            alignments += [attention_weights]\n\n        outputs, stop_tokens, alignments = self._parse_outputs(\n            outputs, stop_tokens, alignments)\n        return outputs, alignments, stop_tokens\n\n    def inference(self, inputs, speaker_embeddings=None):\n        memory = self.get_go_frame(inputs)\n        memory = self._update_memory(memory)\n\n        self._init_states(inputs, mask=None)\n        self.attention.init_states(inputs)\n\n        outputs, stop_tokens, alignments, t = [], [], [], 0\n        while True:\n            memory = self.prenet(memory)\n            if speaker_embeddings is not None:\n                memory = torch.cat([memory, speaker_embeddings], dim=-1)\n            decoder_output, alignment, stop_token = self.decode(memory)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token]\n            alignments += [alignment]\n\n            if stop_token > 0.7 and t > inputs.shape[0] / 2:\n                break\n            if len(outputs) == self.max_decoder_steps:\n                print(""   | > Decoder stopped with \'max_decoder_steps"")\n                break\n\n            memory = self._update_memory(decoder_output)\n            t += 1\n\n        outputs, stop_tokens, alignments = self._parse_outputs(\n            outputs, stop_tokens, alignments)\n\n        return outputs, alignments, stop_tokens\n\n    def inference_truncated(self, inputs):\n        """"""\n        Preserve decoder states for continuous inference\n        """"""\n        if self.memory_truncated is None:\n            self.memory_truncated = self.get_go_frame(inputs)\n            self._init_states(inputs, mask=None, keep_states=False)\n        else:\n            self._init_states(inputs, mask=None, keep_states=True)\n\n        self.attention.init_win_idx()\n        self.attention.init_states(inputs)\n        outputs, stop_tokens, alignments, t = [], [], [], 0\n        stop_flags = [True, False, False]\n        while True:\n            memory = self.prenet(self.memory_truncated)\n            decoder_output, alignment, stop_token = self.decode(memory)\n            stop_token = torch.sigmoid(stop_token.data)\n            outputs += [decoder_output.squeeze(1)]\n            stop_tokens += [stop_token]\n            alignments += [alignment]\n\n            if stop_token > 0.7:\n                break\n            if len(outputs) == self.max_decoder_steps:\n                print(""   | > Decoder stopped with \'max_decoder_steps"")\n                break\n\n            self.memory_truncated = decoder_output\n            t += 1\n\n        outputs, stop_tokens, alignments = self._parse_outputs(\n            outputs, stop_tokens, alignments)\n\n        return outputs, alignments, stop_tokens\n\n    def inference_step(self, inputs, t, memory=None):\n        """"""\n        For debug purposes\n        """"""\n        if t == 0:\n            memory = self.get_go_frame(inputs)\n            self._init_states(inputs, mask=None)\n\n        memory = self.prenet(memory)\n        decoder_output, stop_token, alignment = self.decode(memory)\n        stop_token = torch.sigmoid(stop_token.data)\n        memory = decoder_output\n        return decoder_output, stop_token, alignment\n'"
models/__init__.py,0,b''
models/tacotron.py,3,"b'# coding: utf-8\nimport torch\nimport copy\nfrom torch import nn\nfrom TTS.layers.tacotron import Encoder, Decoder, PostCBHG\nfrom TTS.utils.generic_utils import sequence_mask\nfrom TTS.layers.gst_layers import GST\n\n\nclass Tacotron(nn.Module):\n    def __init__(self,\n                 num_chars,\n                 num_speakers,\n                 r=5,\n                 postnet_output_dim=1025,\n                 decoder_output_dim=80,\n                 memory_size=5,\n                 attn_type=\'original\',\n                 attn_win=False,\n                 gst=False,\n                 attn_norm=""sigmoid"",\n                 prenet_type=""original"",\n                 prenet_dropout=True,\n                 forward_attn=False,\n                 trans_agent=False,\n                 forward_attn_mask=False,\n                 location_attn=True,\n                 attn_K=5,\n                 separate_stopnet=True,\n                 bidirectional_decoder=False):\n        super(Tacotron, self).__init__()\n        self.r = r\n        self.decoder_output_dim = decoder_output_dim\n        self.postnet_output_dim = postnet_output_dim\n        self.gst = gst\n        self.num_speakers = num_speakers\n        self.bidirectional_decoder = bidirectional_decoder\n        decoder_dim = 512 if num_speakers > 1 else 256\n        encoder_dim = 512 if num_speakers > 1 else 256\n        proj_speaker_dim = 80 if num_speakers > 1 else 0\n        # embedding layer\n        self.embedding = nn.Embedding(num_chars, 256, padding_idx=0)\n        self.embedding.weight.data.normal_(0, 0.3)\n        # boilerplate model\n        self.encoder = Encoder(encoder_dim)\n        self.decoder = Decoder(decoder_dim, decoder_output_dim, r, memory_size, attn_type, attn_win,\n                               attn_norm, prenet_type, prenet_dropout,\n                               forward_attn, trans_agent, forward_attn_mask,\n                               location_attn, attn_K, separate_stopnet,\n                               proj_speaker_dim)\n        if self.bidirectional_decoder:\n            self.decoder_backward = copy.deepcopy(self.decoder)\n        self.postnet = PostCBHG(decoder_output_dim)\n        self.last_linear = nn.Linear(self.postnet.cbhg.gru_features * 2,\n                                     postnet_output_dim)\n        # speaker embedding layers\n        if num_speakers > 1:\n            self.speaker_embedding = nn.Embedding(num_speakers, 256)\n            self.speaker_embedding.weight.data.normal_(0, 0.3)\n            self.speaker_project_mel = nn.Sequential(\n                nn.Linear(256, proj_speaker_dim), nn.Tanh())\n            self.speaker_embeddings = None\n            self.speaker_embeddings_projected = None\n        # global style token layers\n        if self.gst:\n            gst_embedding_dim = 256\n            self.gst_layer = GST(num_mel=80,\n                                 num_heads=4,\n                                 num_style_tokens=10,\n                                 embedding_dim=gst_embedding_dim)\n\n    def _init_states(self):\n        self.speaker_embeddings = None\n        self.speaker_embeddings_projected = None\n\n    def compute_speaker_embedding(self, speaker_ids):\n        if hasattr(self, ""speaker_embedding"") and speaker_ids is None:\n            raise RuntimeError(\n                "" [!] Model has speaker embedding layer but speaker_id is not provided""\n            )\n        if hasattr(self, ""speaker_embedding"") and speaker_ids is not None:\n            self.speaker_embeddings = self._compute_speaker_embedding(\n                speaker_ids)\n            self.speaker_embeddings_projected = self.speaker_project_mel(\n                self.speaker_embeddings).squeeze(1)\n\n    def compute_gst(self, inputs, mel_specs):\n        gst_outputs = self.gst_layer(mel_specs)\n        inputs = self._add_speaker_embedding(inputs, gst_outputs)\n        return inputs\n\n    def forward(self, characters, text_lengths, mel_specs, speaker_ids=None):\n        """"""\n        Shapes:\n            - characters: B x T_in\n            - text_lengths: B\n            - mel_specs: B x T_out x D\n            - speaker_ids: B x 1\n        """"""\n        self._init_states()\n        mask = sequence_mask(text_lengths).to(characters.device)\n        # B x T_in x embed_dim\n        inputs = self.embedding(characters)\n        # B x speaker_embed_dim\n        self.compute_speaker_embedding(speaker_ids)\n        if self.num_speakers > 1:\n            # B x T_in x embed_dim + speaker_embed_dim\n            inputs = self._concat_speaker_embedding(inputs,\n                                                    self.speaker_embeddings)\n        # B x T_in x encoder_dim\n        encoder_outputs = self.encoder(inputs)\n        if self.gst:\n            # B x gst_dim\n            encoder_outputs = self.compute_gst(encoder_outputs, mel_specs)\n        if self.num_speakers > 1:\n            encoder_outputs = self._concat_speaker_embedding(\n                encoder_outputs, self.speaker_embeddings)\n        # decoder_outputs: B x decoder_dim x T_out\n        # alignments: B x T_in x encoder_dim\n        # stop_tokens: B x T_in\n        decoder_outputs, alignments, stop_tokens = self.decoder(\n            encoder_outputs, mel_specs, mask,\n            self.speaker_embeddings_projected)\n        # B x T_out x decoder_dim\n        postnet_outputs = self.postnet(decoder_outputs)\n        # B x T_out x posnet_dim\n        postnet_outputs = self.last_linear(postnet_outputs)\n        # B x T_out x decoder_dim\n        decoder_outputs = decoder_outputs.transpose(1, 2).contiguous()\n        if self.bidirectional_decoder:\n            decoder_outputs_backward, alignments_backward = self._backward_inference(mel_specs, encoder_outputs, mask)\n            return decoder_outputs, postnet_outputs, alignments, stop_tokens, decoder_outputs_backward, alignments_backward\n        return decoder_outputs, postnet_outputs, alignments, stop_tokens\n\n    @torch.no_grad()\n    def inference(self, characters, speaker_ids=None, style_mel=None):\n        inputs = self.embedding(characters)\n        self._init_states()\n        self.compute_speaker_embedding(speaker_ids)\n        if self.num_speakers > 1:\n            inputs = self._concat_speaker_embedding(inputs,\n                                                    self.speaker_embeddings)\n        encoder_outputs = self.encoder(inputs)\n        if self.gst and style_mel is not None:\n            encoder_outputs = self.compute_gst(encoder_outputs, style_mel)\n        if self.num_speakers > 1:\n            encoder_outputs = self._concat_speaker_embedding(\n                encoder_outputs, self.speaker_embeddings)\n        decoder_outputs, alignments, stop_tokens = self.decoder.inference(\n            encoder_outputs, self.speaker_embeddings_projected)\n        postnet_outputs = self.postnet(decoder_outputs)\n        postnet_outputs = self.last_linear(postnet_outputs)\n        decoder_outputs = decoder_outputs.transpose(1, 2)\n        return decoder_outputs, postnet_outputs, alignments, stop_tokens\n\n    def _backward_inference(self, mel_specs, encoder_outputs, mask):\n        decoder_outputs_b, alignments_b, _ = self.decoder_backward(\n            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,\n            self.speaker_embeddings_projected)\n        decoder_outputs_b = decoder_outputs_b.transpose(1, 2).contiguous()\n        return decoder_outputs_b, alignments_b\n\n    def _compute_speaker_embedding(self, speaker_ids):\n        speaker_embeddings = self.speaker_embedding(speaker_ids)\n        return speaker_embeddings.unsqueeze_(1)\n\n    @staticmethod\n    def _add_speaker_embedding(outputs, speaker_embeddings):\n        speaker_embeddings_ = speaker_embeddings.expand(\n            outputs.size(0), outputs.size(1), -1)\n        outputs = outputs + speaker_embeddings_\n        return outputs\n\n    @staticmethod\n    def _concat_speaker_embedding(outputs, speaker_embeddings):\n        speaker_embeddings_ = speaker_embeddings.expand(\n            outputs.size(0), outputs.size(1), -1)\n        outputs = torch.cat([outputs, speaker_embeddings_], dim=-1)\n        return outputs\n'"
models/tacotron2.py,2,"b'import copy\nimport torch\nfrom math import sqrt\nfrom torch import nn\nfrom TTS.layers.tacotron2 import Encoder, Decoder, Postnet\nfrom TTS.utils.generic_utils import sequence_mask\n\n\n# TODO: match function arguments with tacotron\nclass Tacotron2(nn.Module):\n    def __init__(self,\n                 num_chars,\n                 num_speakers,\n                 r,\n                 postnet_output_dim=80,\n                 decoder_output_dim=80,\n                 attn_type=\'original\',\n                 attn_win=False,\n                 attn_norm=""softmax"",\n                 prenet_type=""original"",\n                 prenet_dropout=True,\n                 forward_attn=False,\n                 trans_agent=False,\n                 forward_attn_mask=False,\n                 location_attn=True,\n                 attn_K=5,\n                 separate_stopnet=True,\n                 bidirectional_decoder=False):\n        super(Tacotron2, self).__init__()\n        self.postnet_output_dim = postnet_output_dim\n        self.decoder_output_dim = decoder_output_dim\n        self.r = r\n        self.bidirectional_decoder = bidirectional_decoder\n        decoder_dim = 512 if num_speakers > 1 else 512\n        encoder_dim = 512 if num_speakers > 1 else 512\n        proj_speaker_dim = 80 if num_speakers > 1 else 0\n        # embedding layer\n        self.embedding = nn.Embedding(num_chars, 512, padding_idx=0)\n        std = sqrt(2.0 / (num_chars + 512))\n        val = sqrt(3.0) * std  # uniform bounds for std\n        self.embedding.weight.data.uniform_(-val, val)\n        if num_speakers > 1:\n            self.speaker_embedding = nn.Embedding(num_speakers, 512)\n            self.speaker_embedding.weight.data.normal_(0, 0.3)\n            self.speaker_embeddings = None\n            self.speaker_embeddings_projected = None\n        self.encoder = Encoder(encoder_dim)\n        self.decoder = Decoder(decoder_dim, self.decoder_output_dim, r, attn_type, attn_win,\n                               attn_norm, prenet_type, prenet_dropout,\n                               forward_attn, trans_agent, forward_attn_mask,\n                               location_attn, attn_K, separate_stopnet, proj_speaker_dim)\n        if self.bidirectional_decoder:\n            self.decoder_backward = copy.deepcopy(self.decoder)\n        self.postnet = Postnet(self.postnet_output_dim)\n\n    def _init_states(self):\n        self.speaker_embeddings = None\n        self.speaker_embeddings_projected = None\n\n    @staticmethod\n    def shape_outputs(mel_outputs, mel_outputs_postnet, alignments):\n        mel_outputs = mel_outputs.transpose(1, 2)\n        mel_outputs_postnet = mel_outputs_postnet.transpose(1, 2)\n        return mel_outputs, mel_outputs_postnet, alignments\n\n    def forward(self, text, text_lengths, mel_specs=None, speaker_ids=None):\n        self._init_states()\n        # compute mask for padding\n        mask = sequence_mask(text_lengths).to(text.device)\n        embedded_inputs = self.embedding(text).transpose(1, 2)\n        encoder_outputs = self.encoder(embedded_inputs, text_lengths)\n        encoder_outputs = self._add_speaker_embedding(encoder_outputs,\n                                                      speaker_ids)\n        decoder_outputs, alignments, stop_tokens = self.decoder(\n            encoder_outputs, mel_specs, mask)\n        postnet_outputs = self.postnet(decoder_outputs)\n        postnet_outputs = decoder_outputs + postnet_outputs\n        decoder_outputs, postnet_outputs, alignments = self.shape_outputs(\n            decoder_outputs, postnet_outputs, alignments)\n        if self.bidirectional_decoder:\n            decoder_outputs_backward, alignments_backward = self._backward_inference(mel_specs, encoder_outputs, mask)\n            return decoder_outputs, postnet_outputs, alignments, stop_tokens, decoder_outputs_backward, alignments_backward\n        return decoder_outputs, postnet_outputs, alignments, stop_tokens\n\n    @torch.no_grad()\n    def inference(self, text, speaker_ids=None):\n        embedded_inputs = self.embedding(text).transpose(1, 2)\n        encoder_outputs = self.encoder.inference(embedded_inputs)\n        encoder_outputs = self._add_speaker_embedding(encoder_outputs,\n                                                      speaker_ids)\n        mel_outputs, alignments, stop_tokens = self.decoder.inference(\n            encoder_outputs)\n        mel_outputs_postnet = self.postnet(mel_outputs)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n        mel_outputs, mel_outputs_postnet, alignments = self.shape_outputs(\n            mel_outputs, mel_outputs_postnet, alignments)\n        return mel_outputs, mel_outputs_postnet, alignments, stop_tokens\n\n    def inference_truncated(self, text, speaker_ids=None):\n        """"""\n        Preserve model states for continuous inference\n        """"""\n        embedded_inputs = self.embedding(text).transpose(1, 2)\n        encoder_outputs = self.encoder.inference_truncated(embedded_inputs)\n        encoder_outputs = self._add_speaker_embedding(encoder_outputs,\n                                                      speaker_ids)\n        mel_outputs, alignments, stop_tokens = self.decoder.inference_truncated(\n            encoder_outputs)\n        mel_outputs_postnet = self.postnet(mel_outputs)\n        mel_outputs_postnet = mel_outputs + mel_outputs_postnet\n        mel_outputs, mel_outputs_postnet, alignments = self.shape_outputs(\n            mel_outputs, mel_outputs_postnet, alignments)\n        return mel_outputs, mel_outputs_postnet, alignments, stop_tokens\n\n    def _backward_inference(self, mel_specs, encoder_outputs, mask):\n        decoder_outputs_b, alignments_b, _ = self.decoder_backward(\n            encoder_outputs, torch.flip(mel_specs, dims=(1,)), mask,\n            self.speaker_embeddings_projected)\n        decoder_outputs_b = decoder_outputs_b.transpose(1, 2)\n        return decoder_outputs_b, alignments_b\n\n    def _add_speaker_embedding(self, encoder_outputs, speaker_ids):\n        if hasattr(self, ""speaker_embedding"") and speaker_ids is None:\n            raise RuntimeError("" [!] Model has speaker embedding layer but speaker_id is not provided"")\n        if hasattr(self, ""speaker_embedding"") and speaker_ids is not None:\n            speaker_embeddings = self.speaker_embedding(speaker_ids)\n\n            speaker_embeddings.unsqueeze_(1)\n            speaker_embeddings = speaker_embeddings.expand(encoder_outputs.size(0),\n                                                           encoder_outputs.size(1),\n                                                           -1)\n            encoder_outputs = encoder_outputs + speaker_embeddings\n        return encoder_outputs\n'"
server/__init__.py,0,b''
server/server.py,0,"b'#!flask/bin/python\nimport argparse\nimport os\n\nfrom flask import Flask, request, render_template, send_file\nfrom TTS.server.synthesizer import Synthesizer\n\n\ndef create_argparser():\n    def convert_boolean(x):\n        return x.lower() in [\'true\', \'1\', \'yes\']\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--tts_checkpoint\', type=str, help=\'path to TTS checkpoint file\')\n    parser.add_argument(\'--tts_config\', type=str, help=\'path to TTS config.json file\')\n    parser.add_argument(\'--tts_speakers\', type=str, help=\'path to JSON file containing speaker ids, if speaker ids are used in the model\')\n    parser.add_argument(\'--wavernn_lib_path\', type=str, default=None, help=\'path to WaveRNN project folder to be imported. If this is not passed, model uses Griffin-Lim for synthesis.\')\n    parser.add_argument(\'--wavernn_file\', type=str, default=None, help=\'path to WaveRNN checkpoint file.\')\n    parser.add_argument(\'--wavernn_config\', type=str, default=None, help=\'path to WaveRNN config file.\')\n    parser.add_argument(\'--is_wavernn_batched\', type=convert_boolean, default=False, help=\'true to use batched WaveRNN.\')\n    parser.add_argument(\'--pwgan_lib_path\', type=str, default=None, help=\'path to ParallelWaveGAN project folder to be imported. If this is not passed, model uses Griffin-Lim for synthesis.\')\n    parser.add_argument(\'--pwgan_file\', type=str, default=None, help=\'path to ParallelWaveGAN checkpoint file.\')\n    parser.add_argument(\'--pwgan_config\', type=str, default=None, help=\'path to ParallelWaveGAN config file.\')\n    parser.add_argument(\'--port\', type=int, default=5002, help=\'port to listen on.\')\n    parser.add_argument(\'--use_cuda\', type=convert_boolean, default=False, help=\'true to use CUDA.\')\n    parser.add_argument(\'--debug\', type=convert_boolean, default=False, help=\'true to enable Flask debug mode.\')\n    return parser\n\n\nsynthesizer = None\n\nembedded_models_folder = os.path.join(os.path.dirname(os.path.realpath(__file__)), \'model\')\n\nembedded_tts_folder = os.path.join(embedded_models_folder, \'tts\')\ntts_checkpoint_file = os.path.join(embedded_tts_folder, \'checkpoint.pth.tar\')\ntts_config_file = os.path.join(embedded_tts_folder, \'config.json\')\n\nembedded_wavernn_folder = os.path.join(embedded_models_folder, \'wavernn\')\nwavernn_checkpoint_file = os.path.join(embedded_wavernn_folder, \'checkpoint.pth.tar\')\nwavernn_config_file = os.path.join(embedded_wavernn_folder, \'config.json\')\n\nembedded_pwgan_folder = os.path.join(embedded_models_folder, \'pwgan\')\npwgan_checkpoint_file = os.path.join(embedded_pwgan_folder, \'checkpoint.pkl\')\npwgan_config_file = os.path.join(embedded_pwgan_folder, \'config.yml\')\n\nargs = create_argparser().parse_args()\n\n# If these were not specified in the CLI args, use default values with embedded model files\nif not args.tts_checkpoint and os.path.isfile(tts_checkpoint_file):\n    args.tts_checkpoint = tts_checkpoint_file\nif not args.tts_config and os.path.isfile(tts_config_file):\n    args.tts_config = tts_config_file\nif not args.wavernn_file and os.path.isfile(wavernn_checkpoint_file):\n    args.wavernn_file = wavernn_checkpoint_file\nif not args.wavernn_config and os.path.isfile(wavernn_config_file):\n    args.wavernn_config = wavernn_config_file\nif not args.pwgan_file and os.path.isfile(pwgan_checkpoint_file):\n    args.pwgan_file = pwgan_checkpoint_file\nif not args.pwgan_config and os.path.isfile(pwgan_config_file):\n    args.pwgan_config = pwgan_config_file\n\nsynthesizer = Synthesizer(args)\n\napp = Flask(__name__)\n\n@app.route(\'/\')\ndef index():\n    return render_template(\'index.html\')\n\n\n@app.route(\'/api/tts\', methods=[\'GET\'])\ndef tts():\n    text = request.args.get(\'text\')\n    print("" > Model input: {}"".format(text))\n    data = synthesizer.tts(text)\n    return send_file(data, mimetype=\'audio/wav\')\n\n\nif __name__ == \'__main__\':\n    app.run(debug=args.debug, host=\'0.0.0.0\', port=args.port)\n'"
server/synthesizer.py,8,"b'import io\nimport re\nimport sys\n\nimport numpy as np\nimport torch\nimport yaml\n\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.io import load_config\nfrom TTS.utils.generic_utils import setup_model\nfrom TTS.utils.speakers import load_speaker_mapping\n# pylint: disable=unused-wildcard-import\n# pylint: disable=wildcard-import\nfrom TTS.utils.synthesis import *\n\nfrom TTS.utils.text import make_symbols, phonemes, symbols\n\nalphabets = r""([A-Za-z])""\nprefixes = r""(Mr|St|Mrs|Ms|Dr)[.]""\nsuffixes = r""(Inc|Ltd|Jr|Sr|Co)""\nstarters = r""(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)""\nacronyms = r""([A-Z][.][A-Z][.](?:[A-Z][.])?)""\nwebsites = r""[.](com|net|org|io|gov)""\n\n\nclass Synthesizer(object):\n    def __init__(self, config):\n        self.wavernn = None\n        self.pwgan = None\n        self.config = config\n        self.use_cuda = self.config.use_cuda\n        if self.use_cuda:\n            assert torch.cuda.is_available(), ""CUDA is not availabe on this machine.""\n        self.load_tts(self.config.tts_checkpoint, self.config.tts_config,\n                      self.config.use_cuda)\n        if self.config.wavernn_lib_path:\n            self.load_wavernn(self.config.wavernn_lib_path, self.config.wavernn_file,\n                              self.config.wavernn_config, self.config.use_cuda)\n        if self.config.pwgan_lib_path:\n            self.load_pwgan(self.config.pwgan_lib_path, self.config.pwgan_file,\n                            self.config.pwgan_config, self.config.use_cuda)\n\n    def load_tts(self, tts_checkpoint, tts_config, use_cuda):\n        # pylint: disable=global-statement\n        global symbols, phonemes\n\n        print("" > Loading TTS model ..."")\n        print("" | > model config: "", tts_config)\n        print("" | > checkpoint file: "", tts_checkpoint)\n\n        self.tts_config = load_config(tts_config)\n        self.use_phonemes = self.tts_config.use_phonemes\n        self.ap = AudioProcessor(**self.tts_config.audio)\n\n        if \'characters\' in self.tts_config.keys():\n            symbols, phonemes = make_symbols(**self.tts_config.characters)\n\n        if self.use_phonemes:\n            self.input_size = len(phonemes)\n        else:\n            self.input_size = len(symbols)\n        # TODO: fix this for multi-speaker model - load speakers\n        if self.config.tts_speakers is not None:\n            self.tts_speakers = load_speaker_mapping(self.config.tts_speakers)\n            num_speakers = len(self.tts_speakers)\n        else:\n            num_speakers = 0\n        self.tts_model = setup_model(self.input_size, num_speakers=num_speakers, c=self.tts_config)\n        # load model state\n        cp = torch.load(tts_checkpoint, map_location=torch.device(\'cpu\'))\n        # load the model\n        self.tts_model.load_state_dict(cp[\'model\'])\n        if use_cuda:\n            self.tts_model.cuda()\n        self.tts_model.eval()\n        self.tts_model.decoder.max_decoder_steps = 3000\n        if \'r\' in cp:\n            self.tts_model.decoder.set_r(cp[\'r\'])\n\n    def load_wavernn(self, lib_path, model_file, model_config, use_cuda):\n        # TODO: set a function in wavernn code base for model setup and call it here.\n        sys.path.append(lib_path) # set this if WaveRNN is not installed globally\n        #pylint: disable=import-outside-toplevel\n        from WaveRNN.models.wavernn import Model\n        print("" > Loading WaveRNN model ..."")\n        print("" | > model config: "", model_config)\n        print("" | > model file: "", model_file)\n        self.wavernn_config = load_config(model_config)\n        # This is the default architecture we use for our models.\n        # You might need to update it\n        self.wavernn = Model(\n            rnn_dims=512,\n            fc_dims=512,\n            mode=self.wavernn_config.mode,\n            mulaw=self.wavernn_config.mulaw,\n            pad=self.wavernn_config.pad,\n            use_aux_net=self.wavernn_config.use_aux_net,\n            use_upsample_net=self.wavernn_config.use_upsample_net,\n            upsample_factors=self.wavernn_config.upsample_factors,\n            feat_dims=80,\n            compute_dims=128,\n            res_out_dims=128,\n            res_blocks=10,\n            hop_length=self.ap.hop_length,\n            sample_rate=self.ap.sample_rate,\n        ).cuda()\n\n        check = torch.load(model_file, map_location=""cpu"")\n        self.wavernn.load_state_dict(check[\'model\'])\n        if use_cuda:\n            self.wavernn.cuda()\n        self.wavernn.eval()\n\n    def load_pwgan(self, lib_path, model_file, model_config, use_cuda):\n        sys.path.append(lib_path) # set this if ParallelWaveGAN is not installed globally\n        #pylint: disable=import-outside-toplevel\n        from parallel_wavegan.models import ParallelWaveGANGenerator\n        print("" > Loading PWGAN model ..."")\n        print("" | > model config: "", model_config)\n        print("" | > model file: "", model_file)\n        with open(model_config) as f:\n            self.pwgan_config = yaml.load(f, Loader=yaml.Loader)\n        self.pwgan = ParallelWaveGANGenerator(**self.pwgan_config[""generator_params""])\n        self.pwgan.load_state_dict(torch.load(model_file, map_location=""cpu"")[""model""][""generator""])\n        self.pwgan.remove_weight_norm()\n        if use_cuda:\n            self.pwgan.cuda()\n        self.pwgan.eval()\n\n    def save_wav(self, wav, path):\n        # wav *= 32767 / max(1e-8, np.max(np.abs(wav)))\n        wav = np.array(wav)\n        self.ap.save_wav(wav, path)\n\n    @staticmethod\n    def split_into_sentences(text):\n        text = "" "" + text + ""  <stop>""\n        text = text.replace(""\\n"", "" "")\n        text = re.sub(prefixes, ""\\\\1<prd>"", text)\n        text = re.sub(websites, ""<prd>\\\\1"", text)\n        if ""Ph.D"" in text:\n            text = text.replace(""Ph.D."", ""Ph<prd>D<prd>"")\n        text = re.sub(r""\\s"" + alphabets + ""[.] "", "" \\\\1<prd> "", text)\n        text = re.sub(acronyms+"" ""+starters, ""\\\\1<stop> \\\\2"", text)\n        text = re.sub(alphabets + ""[.]"" + alphabets + ""[.]"" + alphabets + ""[.]"", ""\\\\1<prd>\\\\2<prd>\\\\3<prd>"", text)\n        text = re.sub(alphabets + ""[.]"" + alphabets + ""[.]"", ""\\\\1<prd>\\\\2<prd>"", text)\n        text = re.sub("" ""+suffixes+""[.] ""+starters, "" \\\\1<stop> \\\\2"", text)\n        text = re.sub("" ""+suffixes+""[.]"", "" \\\\1<prd>"", text)\n        text = re.sub("" "" + alphabets + ""[.]"", "" \\\\1<prd>"", text)\n        if ""\xe2\x80\x9d"" in text:\n            text = text.replace("".\xe2\x80\x9d"", ""\xe2\x80\x9d."")\n        if ""\\"""" in text:\n            text = text.replace("".\\"""", ""\\""."")\n        if ""!"" in text:\n            text = text.replace(""!\\"""", ""\\""!"")\n        if ""?"" in text:\n            text = text.replace(""?\\"""", ""\\""?"")\n        text = text.replace(""."", "".<stop>"")\n        text = text.replace(""?"", ""?<stop>"")\n        text = text.replace(""!"", ""!<stop>"")\n        text = text.replace(""<prd>"", ""."")\n        sentences = text.split(""<stop>"")\n        sentences = sentences[:-1]\n        sentences = list(filter(None, [s.strip() for s in sentences])) # remove empty sentences\n        return sentences\n\n    def tts(self, text, speaker_id=None):\n        wavs = []\n        sens = self.split_into_sentences(text)\n        print(sens)\n        speaker_id = id_to_torch(speaker_id)\n        if speaker_id is not None and self.use_cuda:\n            speaker_id = speaker_id.cuda()\n\n        for sen in sens:\n            # preprocess the given text\n            inputs = text_to_seqvec(sen, self.tts_config)\n            inputs = numpy_to_torch(inputs, torch.long, cuda=self.use_cuda)\n            inputs = inputs.unsqueeze(0)\n            # synthesize voice\n            decoder_output, postnet_output, alignments, stop_tokens = run_model_torch(\n                self.tts_model, inputs, self.tts_config, False, speaker_id, None)\n            # convert outputs to numpy\n            postnet_output, decoder_output, _, _ = parse_outputs_torch(\n                postnet_output, decoder_output, alignments, stop_tokens)\n\n            if self.pwgan:\n                vocoder_input = torch.FloatTensor(postnet_output.T).unsqueeze(0)\n                if self.use_cuda:\n                    vocoder_input.cuda()\n                wav = self.pwgan.inference(vocoder_input, hop_size=self.ap.hop_length)\n            elif self.wavernn:\n                vocoder_input = None\n                if self.tts_config.model == ""Tacotron"":\n                    vocoder_input = torch.FloatTensor(self.ap.out_linear_to_mel(linear_spec=postnet_output.T).T).T.unsqueeze(0)\n                else:\n                    vocoder_input = torch.FloatTensor(postnet_output.T).unsqueeze(0)\n\n                if self.use_cuda:\n                    vocoder_input.cuda()\n                wav = self.wavernn.generate(vocoder_input, batched=self.config.is_wavernn_batched, target=11000, overlap=550)\n            else:\n                wav = inv_spectrogram(postnet_output, self.ap, self.tts_config)\n            # trim silence\n            wav = trim_silence(wav, self.ap)\n\n            wavs += list(wav)\n            wavs += [0] * 10000\n\n        out = io.BytesIO()\n        self.save_wav(wavs, out)\n        return out\n'"
speaker_encoder/__init__.py,0,b''
speaker_encoder/compute_embeddings.py,2,"b'import argparse\nimport glob\nimport os\n\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nfrom TTS.speaker_encoder.model import SpeakerEncoder\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.generic_utils import load_config\n\nparser = argparse.ArgumentParser(\n    description=\'Compute embedding vectors for each wav file in a dataset. \')\nparser.add_argument(\n    \'model_path\',\n    type=str,\n    help=\'Path to model outputs (checkpoint, tensorboard etc.).\')\nparser.add_argument(\n    \'config_path\',\n    type=str,\n    help=\'Path to config file for training.\',\n)\nparser.add_argument(\n    \'data_path\',\n    type=str,\n    help=\'Data path for wav files - directory or CSV file\')\nparser.add_argument(\n    \'output_path\',\n    type=str,\n    help=\'path for training outputs.\')\nparser.add_argument(\n    \'--use_cuda\', type=bool, help=\'flag to set cuda.\', default=False\n)\nparser.add_argument(\n    \'--separator\', type=str, help=\'Separator used in file if CSV is passed for data_path\', default=\'|\'\n)\nargs = parser.parse_args()\n\n\nc = load_config(args.config_path)\nap = AudioProcessor(**c[\'audio\'])\n\ndata_path = args.data_path\nsplit_ext = os.path.splitext(data_path)\nsep = args.separator\n\nif len(split_ext) > 0 and split_ext[1].lower() == \'.csv\':\n    # Parse CSV\n    print(f\'CSV file: {data_path}\')\n    with open(data_path) as f:\n        wav_path = os.path.join(os.path.dirname(data_path), \'wavs\')\n        wav_files = []\n        print(f\'Separator is: {sep}\')\n        for line in f:\n            components = line.split(sep)\n            if len(components) != 2:\n                print(""Invalid line"")\n                continue\n            wav_file = os.path.join(wav_path, components[0] + \'.wav\')\n            #print(f\'wav_file: {wav_file}\')\n            if os.path.exists(wav_file):\n                wav_files.append(wav_file)\n    print(f\'Count of wavs imported: {len(wav_files)}\')\nelse:\n    # Parse all wav files in data_path\n    wav_path = data_path\n    wav_files = glob.glob(data_path + \'/**/*.wav\', recursive=True)\n\noutput_files = [wav_file.replace(wav_path, args.output_path).replace(\n    \'.wav\', \'.npy\') for wav_file in wav_files]\n\nfor output_file in output_files:\n    os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\nmodel = SpeakerEncoder(**c.model)\nmodel.load_state_dict(torch.load(args.model_path)[\'model\'])\nmodel.eval()\nif args.use_cuda:\n    model.cuda()\n\nfor idx, wav_file in enumerate(tqdm(wav_files)):\n    mel_spec = ap.melspectrogram(ap.load_wav(wav_file)).T\n    mel_spec = torch.FloatTensor(mel_spec[None, :, :])\n    if args.use_cuda:\n        mel_spec = mel_spec.cuda()\n    embedd = model.compute_embedding(mel_spec)\n    np.save(output_files[idx], embedd.detach().cpu().numpy())\n'"
speaker_encoder/dataset.py,3,"b'import numpy as np\nimport torch\nimport random\nfrom torch.utils.data import Dataset\n\n\nclass MyDataset(Dataset):\n    def __init__(self, ap, meta_data, voice_len=1.6, num_speakers_in_batch=64,\n                 num_utter_per_speaker=10, skip_speakers=False, verbose=False):\n        """"""\n        Args:\n            ap (TTS.utils.AudioProcessor): audio processor object.\n            meta_data (list): list of dataset instances.\n            seq_len (int): voice segment length in seconds.\n            verbose (bool): print diagnostic information.\n        """"""\n        self.items = meta_data\n        self.sample_rate = ap.sample_rate\n        self.voice_len = voice_len\n        self.seq_len = int(voice_len * self.sample_rate)\n        self.num_speakers_in_batch = num_speakers_in_batch\n        self.num_utter_per_speaker = num_utter_per_speaker\n        self.skip_speakers = skip_speakers\n        self.ap = ap\n        self.verbose = verbose\n        self.__parse_items()\n        if self.verbose:\n            print(""\\n > DataLoader initialization"")\n            print(f"" | > Number of instances : {len(self.items)}"")\n            print(f"" | > Sequence length: {self.seq_len}"")\n            print(f"" | > Num speakers: {len(self.speakers)}"")\n\n    def load_wav(self, filename):\n        audio = self.ap.load_wav(filename)\n        return audio\n\n    def load_data(self, idx):\n        text, wav_file, speaker_name = self.items[idx]\n        wav = np.asarray(self.load_wav(wav_file), dtype=np.float32)\n        mel = self.ap.melspectrogram(wav).astype(""float32"")\n        # sample seq_len\n\n        assert text.size > 0, self.items[idx][1]\n        assert wav.size > 0, self.items[idx][1]\n\n        sample = {\n            ""mel"": mel,\n            ""item_idx"": self.items[idx][1],\n            ""speaker_name"": speaker_name,\n        }\n        return sample\n\n    def __parse_items(self):\n        """"""\n        Find unique speaker ids and create a dict mapping utterances from speaker id\n        """"""\n        speakers = list({item[-1] for item in self.items})\n        self.speaker_to_utters = {}\n        self.speakers = []\n        for speaker in speakers:\n            speaker_utters = [item[1] for item in self.items if item[2] == speaker]\n            if len(speaker_utters) < self.num_utter_per_speaker and self.skip_speakers:\n                print(\n                    f"" [!] Skipped speaker {speaker}. Not enough utterances {self.num_utter_per_speaker} vs {len(speaker_utters)}.""\n                )\n            else:\n                self.speakers.append(speaker)\n                self.speaker_to_utters[speaker] = speaker_utters\n\n    def __len__(self):\n        return int(1e10)\n\n    def __sample_speaker(self):\n        speaker = random.sample(self.speakers, 1)[0]\n        if self.num_utter_per_speaker > len(self.speaker_to_utters[speaker]):\n            utters = random.choices(\n                self.speaker_to_utters[speaker], k=self.num_utter_per_speaker\n            )\n        else:\n            utters = random.sample(\n                self.speaker_to_utters[speaker], self.num_utter_per_speaker\n            )\n        return speaker, utters\n\n    def __sample_speaker_utterances(self, speaker):\n        """"""\n        Sample all M utterances for the given speaker.\n        """"""\n        feats = []\n        labels = []\n        for _ in range(self.num_utter_per_speaker):\n            # TODO:dummy but works\n            while True:\n                if len(self.speaker_to_utters[speaker]) > 0:\n                    utter = random.sample(self.speaker_to_utters[speaker], 1)[0]\n                else:\n                    self.speakers.remove(speaker)\n                    speaker, _ = self.__sample_speaker()\n                    continue\n                wav = self.load_wav(utter)\n                if wav.shape[0] - self.seq_len > 0:\n                    break\n                self.speaker_to_utters[speaker].remove(utter)\n\n            offset = random.randint(0, wav.shape[0] - self.seq_len)\n            mel = self.ap.melspectrogram(wav[offset : offset + self.seq_len])\n            feats.append(torch.FloatTensor(mel))\n            labels.append(speaker)\n        return feats, labels\n\n    def __getitem__(self, idx):\n        speaker, _ = self.__sample_speaker()\n        return speaker\n\n    def collate_fn(self, batch):\n        labels = []\n        feats = []\n        for speaker in batch:\n            feats_, labels_ = self.__sample_speaker_utterances(speaker)\n            labels.append(labels_)\n            feats.extend(feats_)\n        feats = torch.stack(feats)\n        return feats.transpose(1, 2), labels\n'"
speaker_encoder/generic_utils.py,2,"b'import os\nimport datetime\nimport torch\n\n\ndef save_checkpoint(model, optimizer, model_loss, out_path,\n                    current_step, epoch):\n    checkpoint_path = \'checkpoint_{}.pth.tar\'.format(current_step)\n    checkpoint_path = os.path.join(out_path, checkpoint_path)\n    print("" | | > Checkpoint saving : {}"".format(checkpoint_path))\n\n    new_state_dict = model.state_dict()\n    state = {\n        \'model\': new_state_dict,\n        \'optimizer\': optimizer.state_dict() if optimizer is not None else None,\n        \'step\': current_step,\n        \'epoch\': epoch,\n        \'GE2Eloss\': model_loss,\n        \'date\': datetime.date.today().strftime(""%B %d, %Y""),\n    }\n    torch.save(state, checkpoint_path)\n\n\ndef save_best_model(model, optimizer, model_loss, best_loss, out_path,\n                    current_step):\n    if model_loss < best_loss:\n        new_state_dict = model.state_dict()\n        state = {\n            \'model\': new_state_dict,\n            \'optimizer\': optimizer.state_dict(),\n            \'step\': current_step,\n            \'GE2Eloss\': model_loss,\n            \'date\': datetime.date.today().strftime(""%B %d, %Y""),\n        }\n        best_loss = model_loss\n        bestmodel_path = \'best_model.pth.tar\'\n        bestmodel_path = os.path.join(out_path, bestmodel_path)\n        print(""\\n > BEST MODEL ({0:.5f}) : {1:}"".format(\n            model_loss, bestmodel_path))\n        torch.save(state, bestmodel_path)\n    return best_loss'"
speaker_encoder/loss.py,22,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# adapted from https://github.com/cvqluu/GE2E-Loss\nclass GE2ELoss(nn.Module):\n    def __init__(self, init_w=10.0, init_b=-5.0, loss_method=""softmax""):\n        """"""\n        Implementation of the Generalized End-to-End loss defined in https://arxiv.org/abs/1710.10467 [1]\n        Accepts an input of size (N, M, D)\n            where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector (e.g. d-vector)\n        Args:\n            - init_w (float): defines the initial value of w in Equation (5) of [1]\n            - init_b (float): definies the initial value of b in Equation (5) of [1]\n        """"""\n        super(GE2ELoss, self).__init__()\n        # pylint: disable=E1102\n        self.w = nn.Parameter(torch.tensor(init_w))\n        # pylint: disable=E1102\n        self.b = nn.Parameter(torch.tensor(init_b))\n        self.loss_method = loss_method\n\n        assert self.loss_method in [""softmax"", ""contrast""]\n\n        if self.loss_method == ""softmax"":\n            self.embed_loss = self.embed_loss_softmax\n        if self.loss_method == ""contrast"":\n            self.embed_loss = self.embed_loss_contrast\n\n    # pylint: disable=R0201\n    def calc_new_centroids(self, dvecs, centroids, spkr, utt):\n        """"""\n        Calculates the new centroids excluding the reference utterance\n        """"""\n        excl = torch.cat((dvecs[spkr, :utt], dvecs[spkr, utt + 1 :]))\n        excl = torch.mean(excl, 0)\n        new_centroids = []\n        for i, centroid in enumerate(centroids):\n            if i == spkr:\n                new_centroids.append(excl)\n            else:\n                new_centroids.append(centroid)\n        return torch.stack(new_centroids)\n\n    def calc_cosine_sim(self, dvecs, centroids):\n        """"""\n        Make the cosine similarity matrix with dims (N,M,N)\n        """"""\n        cos_sim_matrix = []\n        for spkr_idx, speaker in enumerate(dvecs):\n            cs_row = []\n            for utt_idx, utterance in enumerate(speaker):\n                new_centroids = self.calc_new_centroids(\n                    dvecs, centroids, spkr_idx, utt_idx\n                )\n                # vector based cosine similarity for speed\n                cs_row.append(\n                    torch.clamp(\n                        torch.mm(\n                            utterance.unsqueeze(1).transpose(0, 1),\n                            new_centroids.transpose(0, 1),\n                        )\n                        / (torch.norm(utterance) * torch.norm(new_centroids, dim=1)),\n                        1e-6,\n                    )\n                )\n            cs_row = torch.cat(cs_row, dim=0)\n            cos_sim_matrix.append(cs_row)\n        return torch.stack(cos_sim_matrix)\n\n    # pylint: disable=R0201\n    def embed_loss_softmax(self, dvecs, cos_sim_matrix):\n        """"""\n        Calculates the loss on each embedding $L(e_{ji})$ by taking softmax\n        """"""\n        N, M, _ = dvecs.shape\n        L = []\n        for j in range(N):\n            L_row = []\n            for i in range(M):\n                L_row.append(-F.log_softmax(cos_sim_matrix[j, i], 0)[j])\n            L_row = torch.stack(L_row)\n            L.append(L_row)\n        return torch.stack(L)\n\n    # pylint: disable=R0201\n    def embed_loss_contrast(self, dvecs, cos_sim_matrix):\n        """"""\n        Calculates the loss on each embedding $L(e_{ji})$ by contrast loss with closest centroid\n        """"""\n        N, M, _ = dvecs.shape\n        L = []\n        for j in range(N):\n            L_row = []\n            for i in range(M):\n                centroids_sigmoids = torch.sigmoid(cos_sim_matrix[j, i])\n                excl_centroids_sigmoids = torch.cat(\n                    (centroids_sigmoids[:j], centroids_sigmoids[j + 1 :])\n                )\n                L_row.append(\n                    1.0\n                    - torch.sigmoid(cos_sim_matrix[j, i, j])\n                    + torch.max(excl_centroids_sigmoids)\n                )\n            L_row = torch.stack(L_row)\n            L.append(L_row)\n        return torch.stack(L)\n\n    def forward(self, dvecs):\n        """"""\n        Calculates the GE2E loss for an input of dimensions (num_speakers, num_utts_per_speaker, dvec_feats)\n        """"""\n        centroids = torch.mean(dvecs, 1)\n        cos_sim_matrix = self.calc_cosine_sim(dvecs, centroids)\n        torch.clamp(self.w, 1e-6)\n        cos_sim_matrix = self.w * cos_sim_matrix + self.b\n        L = self.embed_loss(dvecs, cos_sim_matrix)\n        return L.mean()\n'"
speaker_encoder/model.py,2,"b'import torch\nfrom torch import nn\n\n\nclass LSTMWithProjection(nn.Module):\n    def __init__(self, input_size, hidden_size, proj_size):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.proj_size = proj_size\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.linear = nn.Linear(hidden_size, proj_size, bias=False)\n\n    def forward(self, x):\n        self.lstm.flatten_parameters()\n        o, (_, _) = self.lstm(x)\n        return self.linear(o)\n\n\nclass SpeakerEncoder(nn.Module):\n    def __init__(self, input_dim, proj_dim=256, lstm_dim=768, num_lstm_layers=3):\n        super().__init__()\n        layers = []\n        layers.append(LSTMWithProjection(input_dim, lstm_dim, proj_dim))\n        for _ in range(num_lstm_layers - 1):\n            layers.append(LSTMWithProjection(proj_dim, lstm_dim, proj_dim))\n        self.layers = nn.Sequential(*layers)\n        self._init_layers()\n\n    def _init_layers(self):\n        for name, param in self.layers.named_parameters():\n            if ""bias"" in name:\n                nn.init.constant_(param, 0.0)\n            elif ""weight"" in name:\n                nn.init.xavier_normal_(param)\n\n    def forward(self, x):\n        # TODO: implement state passing for lstms\n        d = self.layers(x)\n        d = torch.nn.functional.normalize(d[:, -1], p=2, dim=1)\n        return d\n\n    def inference(self, x):\n        d = self.layers.forward(x)\n        d = torch.nn.functional.normalize(d[:, -1], p=2, dim=1)\n        return d\n\n    def compute_embedding(self, x, num_frames=160, overlap=0.5):\n        """"""\n        Generate embeddings for a batch of utterances\n        x: 1xTxD\n        """"""\n        num_overlap = int(num_frames * overlap)\n        max_len = x.shape[1]\n        embed = None\n        cur_iter = 0\n        for offset in range(0, max_len, num_frames - num_overlap):\n            cur_iter += 1\n            end_offset = min(x.shape[1], offset + num_frames)\n            frames = x[:, offset:end_offset]\n            if embed is None:\n                embed = self.inference(frames)\n            else:\n                embed += self.inference(frames)\n        return embed / cur_iter\n\n    def batch_compute_embedding(self, x, seq_lens, num_frames=160, overlap=0.5):\n        """"""\n        Generate embeddings for a batch of utterances\n        x: BxTxD\n        """"""\n        num_overlap = num_frames * overlap\n        max_len = x.shape[1]\n        embed = None\n        num_iters = seq_lens / (num_frames - num_overlap)\n        cur_iter = 0\n        for offset in range(0, max_len, num_frames - num_overlap):\n            cur_iter += 1\n            end_offset = min(x.shape[1], offset + num_frames)\n            frames = x[:, offset:end_offset]\n            if embed is None:\n                embed = self.inference(frames)\n            else:\n                embed[cur_iter <= num_iters, :] += self.inference(\n                    frames[cur_iter <= num_iters, :, :]\n                )\n        return embed / num_iters\n\n'"
speaker_encoder/tests.py,1,"b'import os\nimport unittest\nimport torch as T\n\nfrom TTS.speaker_encoder.model import SpeakerEncoder\nfrom TTS.speaker_encoder.loss import GE2ELoss\nfrom TTS.utils.io import load_config\n\n\nfile_path = os.path.dirname(os.path.realpath(__file__)) + ""/../tests/""\nc = load_config(os.path.join(file_path, ""test_config.json""))\n\n\nclass SpeakerEncoderTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        dummy_input = T.rand(4, 20, 80)  # B x T x D\n        dummy_hidden = [T.rand(2, 4, 128), T.rand(2, 4, 128)]\n        model = SpeakerEncoder(\n            input_dim=80, proj_dim=256, lstm_dim=768, num_lstm_layers=3\n        )\n        # computing d vectors\n        output = model.forward(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 256\n        output = model.inference(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 256\n        # compute d vectors by passing LSTM hidden\n        # output = model.forward(dummy_input, dummy_hidden)\n        # assert output.shape[0] == 4\n        # assert output.shape[1] == 20\n        # assert output.shape[2] == 256\n        # check normalization\n        output_norm = T.nn.functional.normalize(output, dim=1, p=2)\n        assert_diff = (output_norm - output).sum().item()\n        assert output.type() == ""torch.FloatTensor""\n        assert (\n            abs(assert_diff) < 1e-4\n        ), f"" [!] output_norm has wrong values - {assert_diff}""\n        # compute d for a given batch\n        dummy_input = T.rand(1, 240, 80)  # B x T x D\n        output = model.compute_embedding(dummy_input, num_frames=160, overlap=0.5)\n        assert output.shape[0] == 1\n        assert output.shape[1] == 256\n        assert len(output.shape) == 2\n\n\nclass GE2ELossTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        # check random input\n        dummy_input = T.rand(4, 5, 64)  # num_speaker x num_utterance x dim\n        loss = GE2ELoss(loss_method=""softmax"")\n        output = loss.forward(dummy_input)\n        assert output.item() >= 0.0\n        # check all zeros\n        dummy_input = T.ones(4, 5, 64)  # num_speaker x num_utterance x dim\n        loss = GE2ELoss(loss_method=""softmax"")\n        output = loss.forward(dummy_input)\n        # check speaker loss with orthogonal d-vectors\n        dummy_input = T.empty(3, 64)\n        dummy_input = T.nn.init.orthogonal(dummy_input)\n        dummy_input = T.cat(\n            [\n                dummy_input[0].repeat(5, 1, 1).transpose(0, 1),\n                dummy_input[1].repeat(5, 1, 1).transpose(0, 1),\n                dummy_input[2].repeat(5, 1, 1).transpose(0, 1),\n            ]\n        )  # num_speaker x num_utterance x dim\n        loss = GE2ELoss(loss_method=""softmax"")\n        output = loss.forward(dummy_input)\n        assert output.item() < 0.005\n\n\n# class LoaderTest(unittest.TestCase):\n#     def test_output(self):\n#         items = libri_tts(""/home/erogol/Data/Libri-TTS/train-clean-360/"")\n#         ap = AudioProcessor(**c[\'audio\'])\n#         dataset = MyDataset(ap, items, 1.6, 64, 10)\n#         loader = DataLoader(dataset, batch_size=32, shuffle=False, num_workers=0, collate_fn=dataset.collate_fn)\n#         count = 0\n#         for mel, spk in loader:\n#             print(mel.shape)\n#             if count == 4:\n#                 break\n#             count += 1\n'"
speaker_encoder/train.py,7,"b'import argparse\nimport os\nimport sys\nimport time\nimport traceback\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom TTS.datasets.preprocess import load_meta_data\nfrom TTS.speaker_encoder.dataset import MyDataset\nfrom TTS.speaker_encoder.loss import GE2ELoss\nfrom TTS.speaker_encoder.model import SpeakerEncoder\nfrom TTS.speaker_encoder.visual import plot_embeddings\nfrom TTS.speaker_encoder.generic_utils import save_best_model\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.generic_utils import (create_experiment_folder, get_git_branch,\n                                     remove_experiment_folder, set_init_dict)\nfrom TTS.utils.io import load_config, copy_config_file\nfrom TTS.utils.training import check_update, NoamLR\nfrom TTS.utils.tensorboard_logger import TensorboardLogger\nfrom TTS.utils.radam import RAdam\n\ntorch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\ntorch.manual_seed(54321)\nuse_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint("" > Using CUDA: "", use_cuda)\nprint("" > Number of GPUs: "", num_gpus)\n\n\ndef setup_loader(ap, is_val=False, verbose=False):\n    if is_val:\n        loader = None\n    else:\n        dataset = MyDataset(ap,\n                            meta_data_eval if is_val else meta_data_train,\n                            voice_len=1.6,\n                            num_utter_per_speaker=10,\n                            skip_speakers=False,\n                            verbose=verbose)\n        # sampler = DistributedSampler(dataset) if num_gpus > 1 else None\n        loader = DataLoader(dataset,\n                            batch_size=c.num_speakers_in_batch,\n                            shuffle=False,\n                            num_workers=c.num_loader_workers,\n                            collate_fn=dataset.collate_fn)\n    return loader\n\n\ndef train(model, criterion, optimizer, scheduler, ap, global_step):\n    data_loader = setup_loader(ap, is_val=False, verbose=True)\n    model.train()\n    epoch_time = 0\n    best_loss = float(\'inf\')\n    avg_loss = 0\n    end_time = time.time()\n    for _, data in enumerate(data_loader):\n        start_time = time.time()\n\n        # setup input data\n        inputs = data[0]\n        loader_time = time.time() - end_time\n        global_step += 1\n\n        # setup lr\n        if c.lr_decay:\n            scheduler.step()\n        optimizer.zero_grad()\n\n        # dispatch data to GPU\n        if use_cuda:\n            inputs = inputs.cuda(non_blocking=True)\n            # labels = labels.cuda(non_blocking=True)\n\n        # forward pass model\n        outputs = model(inputs)\n\n        # loss computation\n        loss = criterion(\n            outputs.view(c.num_speakers_in_batch,\n                         outputs.shape[0] // c.num_speakers_in_batch, -1))\n        loss.backward()\n        grad_norm, _ = check_update(model, c.grad_clip)\n        optimizer.step()\n\n        step_time = time.time() - start_time\n        epoch_time += step_time\n\n        avg_loss = 0.01 * loss.item(\n        ) + 0.99 * avg_loss if avg_loss != 0 else loss.item()\n        current_lr = optimizer.param_groups[0][\'lr\']\n\n        if global_step % c.steps_plot_stats == 0:\n            # Plot Training Epoch Stats\n            train_stats = {\n                ""GE2Eloss"": avg_loss,\n                ""lr"": current_lr,\n                ""grad_norm"": grad_norm,\n                ""step_time"": step_time\n            }\n            tb_logger.tb_train_epoch_stats(global_step, train_stats)\n            figures = {\n                # FIXME: not constant\n                ""UMAP Plot"": plot_embeddings(outputs.detach().cpu().numpy(),\n                                             10),\n            }\n            tb_logger.tb_train_figures(global_step, figures)\n\n        if global_step % c.print_step == 0:\n            print(\n                ""   | > Step:{}  Loss:{:.5f}  AvgLoss:{:.5f}  GradNorm:{:.5f}  ""\n                ""StepTime:{:.2f}  LoaderTime:{:.2f}  LR:{:.6f}"".format(\n                    global_step, loss.item(), avg_loss, grad_norm, step_time,\n                    loader_time, current_lr),\n                flush=True)\n\n        # save best model\n        best_loss = save_best_model(model, optimizer, avg_loss, best_loss,\n                                    OUT_PATH, global_step)\n\n        end_time = time.time()\n    return avg_loss, global_step\n\n\ndef main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data_train\n    global meta_data_eval\n\n    ap = AudioProcessor(**c.audio)\n    model = SpeakerEncoder(input_dim=40,\n                           proj_dim=128,\n                           lstm_dim=384,\n                           num_lstm_layers=3)\n    optimizer = RAdam(model.parameters(), lr=c.lr)\n    criterion = GE2ELoss(loss_method=\'softmax\')\n\n    if args.restore_path:\n        checkpoint = torch.load(args.restore_path)\n        try:\n            # TODO: fix optimizer init, model.cuda() needs to be called before\n            # optimizer restore\n            # optimizer.load_state_dict(checkpoint[\'optimizer\'])\n            if c.reinit_layers:\n                raise RuntimeError\n            model.load_state_dict(checkpoint[\'model\'])\n        except KeyError:\n            print("" > Partial model initialization."")\n            model_dict = model.state_dict()\n            model_dict = set_init_dict(model_dict, checkpoint, c)\n            model.load_state_dict(model_dict)\n            del model_dict\n        for group in optimizer.param_groups:\n            group[\'lr\'] = c.lr\n        print("" > Model restored from step %d"" % checkpoint[\'step\'],\n              flush=True)\n        args.restore_step = checkpoint[\'step\']\n    else:\n        args.restore_step = 0\n\n    if use_cuda:\n        model = model.cuda()\n        criterion.cuda()\n\n    if c.lr_decay:\n        scheduler = NoamLR(optimizer,\n                           warmup_steps=c.warmup_steps,\n                           last_epoch=args.restore_step - 1)\n    else:\n        scheduler = None\n\n    num_params = count_parameters(model)\n    print(""\\n > Model has {} parameters"".format(num_params), flush=True)\n\n    # pylint: disable=redefined-outer-name\n    meta_data_train, meta_data_eval = load_meta_data(c.datasets)\n\n    global_step = args.restore_step\n    train_loss, global_step = train(model, criterion, optimizer, scheduler, ap,\n                                    global_step)\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \'--restore_path\',\n        type=str,\n        help=\'Path to model outputs (checkpoint, tensorboard etc.).\',\n        default=0)\n    parser.add_argument(\n        \'--config_path\',\n        type=str,\n        help=\'Path to config file for training.\',\n    )\n    parser.add_argument(\'--debug\',\n                        type=bool,\n                        default=True,\n                        help=\'Do not verify commit integrity to run training.\')\n    parser.add_argument(\n        \'--data_path\',\n        type=str,\n        default=\'\',\n        help=\'Defines the data path. It overwrites config.json.\')\n    parser.add_argument(\'--output_path\',\n                        type=str,\n                        help=\'path for training outputs.\',\n                        default=\'\')\n    parser.add_argument(\'--output_folder\',\n                        type=str,\n                        default=\'\',\n                        help=\'folder name for training outputs.\')\n    args = parser.parse_args()\n\n    # setup output paths and read configs\n    c = load_config(args.config_path)\n    _ = os.path.dirname(os.path.realpath(__file__))\n    if args.data_path != \'\':\n        c.data_path = args.data_path\n\n    if args.output_path == \'\':\n        OUT_PATH = os.path.join(_, c.output_path)\n    else:\n        OUT_PATH = args.output_path\n\n    if args.output_folder == \'\':\n        OUT_PATH = create_experiment_folder(OUT_PATH, c.run_name, args.debug)\n    else:\n        OUT_PATH = os.path.join(OUT_PATH, args.output_folder)\n\n    new_fields = {}\n    if args.restore_path:\n        new_fields[""restore_path""] = args.restore_path\n    new_fields[""github_branch""] = get_git_branch()\n    copy_config_file(args.config_path, os.path.join(OUT_PATH, \'config.json\'),\n                     new_fields)\n\n    LOG_DIR = OUT_PATH\n    tb_logger = TensorboardLogger(LOG_DIR)\n\n    try:\n        main(args)\n    except KeyboardInterrupt:\n        remove_experiment_folder(OUT_PATH)\n        try:\n            sys.exit(0)\n        except SystemExit:\n            os._exit(0)  # pylint: disable=protected-access\n    except Exception:  # pylint: disable=broad-except\n        remove_experiment_folder(OUT_PATH)\n        traceback.print_exc()\n        sys.exit(1)\n'"
speaker_encoder/visual.py,0,"b'import umap\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nmatplotlib.use(""Agg"")\n\n\ncolormap = (\n    np.array(\n        [\n            [76, 255, 0],\n            [0, 127, 70],\n            [255, 0, 0],\n            [255, 217, 38],\n            [0, 135, 255],\n            [165, 0, 165],\n            [255, 167, 255],\n            [0, 255, 255],\n            [255, 96, 38],\n            [142, 76, 0],\n            [33, 0, 127],\n            [0, 0, 0],\n            [183, 183, 183],\n        ],\n        dtype=np.float,\n    )\n    / 255\n)\n\n\ndef plot_embeddings(embeddings, num_utter_per_speaker):\n    embeddings = embeddings[: 10 * num_utter_per_speaker]\n    model = umap.UMAP()\n    projection = model.fit_transform(embeddings)\n    num_speakers = embeddings.shape[0] // num_utter_per_speaker\n    ground_truth = np.repeat(np.arange(num_speakers), num_utter_per_speaker)\n    colors = [colormap[i] for i in ground_truth]\n\n    fig, ax = plt.subplots(figsize=(16, 10))\n    _ = ax.scatter(projection[:, 0], projection[:, 1], c=colors)\n    plt.gca().set_aspect(""equal"", ""datalim"")\n    plt.title(""UMAP projection"")\n    plt.tight_layout()\n    plt.savefig(""umap"")\n    return fig\n'"
tests/__init__.py,0,"b'import os\n\n\ndef get_tests_path():\n    """"""Returns the path to the test directory.""""""\n    return os.path.dirname(os.path.realpath(__file__))\n\n\ndef get_tests_input_path():\n    """"""Returns the path to the test data directory.""""""\n    return os.path.join(get_tests_path(), ""inputs"")\n\n\ndef get_tests_output_path():\n    """"""Returns the path to the directory for test outputs.""""""\n    return os.path.join(get_tests_path(), ""outputs"")\n'"
tests/generic_utils_text.py,0,"b""import unittest\nimport torch as T\n\nfrom TTS.utils.generic_utils import save_checkpoint, save_best_model\nfrom TTS.layers.tacotron import Prenet\n\nOUT_PATH = '/tmp/test.pth.tar'\n\n\nclass ModelSavingTests(unittest.TestCase):\n    def save_checkpoint_test(self):\n        # create a dummy model\n        model = Prenet(128, out_features=[256, 128])\n        model = T.nn.DataParallel(layer) #FIXME: undefined variable layer\n\n        # save the model\n        save_checkpoint(model, None, 100, OUT_PATH, 1, 1)\n\n        # load the model to CPU\n        model_dict = T.load(\n            MODEL_PATH, map_location=lambda storage, loc: storage) #FIXME: undefined variable MODEL_PATH\n        model.load_state_dict(model_dict['model'])\n\n    def save_best_model_test(self):\n        # create a dummy model\n        model = Prenet(256, out_features=[256, 256])\n        model = T.nn.DataParallel(layer)\n\n        # save the model\n        save_best_model(model, None, 0, 100, OUT_PATH, 10, 1)\n\n        # load the model to CPU\n        model_dict = T.load(\n            MODEL_PATH, map_location=lambda storage, loc: storage)\n        model.load_state_dict(model_dict['model'])\n"""
tests/symbols_tests.py,0,"b'import unittest\n\nfrom TTS.utils.text import phonemes\n\nclass SymbolsTest(unittest.TestCase):\n    def test_uniqueness(self):  #pylint: disable=no-self-use\n        assert sorted(phonemes) == sorted(list(set(phonemes))), "" {} vs {} "".format(len(phonemes), len(set(phonemes)))\n        '"
tests/test_audio.py,0,"b'import os\nimport unittest\n\nfrom TTS.tests import get_tests_path, get_tests_input_path, get_tests_output_path\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.utils.io import load_config\n\nTESTS_PATH = get_tests_path()\nOUT_PATH = os.path.join(get_tests_output_path(), ""audio_tests"")\nWAV_FILE = os.path.join(get_tests_input_path(), ""example_1.wav"")\n\nos.makedirs(OUT_PATH, exist_ok=True)\nconf = load_config(os.path.join(TESTS_PATH, \'test_config.json\'))\n\n\n# pylint: disable=protected-access\nclass TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestAudio, self).__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf.audio)\n\n    def test_audio_synthesis(self):\n        """""" 1. load wav\n            2. set normalization parameters\n            3. extract mel-spec\n            4. invert to wav and save the output\n        """"""\n        print("" > Sanity check for the process wav -> mel -> wav"")\n\n        def _test(max_norm, signal_norm, symmetric_norm, clip_norm):\n            self.ap.max_norm = max_norm\n            self.ap.signal_norm = signal_norm\n            self.ap.symmetric_norm = symmetric_norm\n            self.ap.clip_norm = clip_norm\n            wav = self.ap.load_wav(WAV_FILE)\n            mel = self.ap.melspectrogram(wav)\n            wav_ = self.ap.inv_melspectrogram(mel)\n            file_name = ""/audio_test-melspec_max_norm_{}-signal_norm_{}-symmetric_{}-clip_norm_{}.wav""\\\n                .format(max_norm, signal_norm, symmetric_norm, clip_norm)\n            print("" | > Creating wav file at : "", file_name)\n            self.ap.save_wav(wav_, OUT_PATH + file_name)\n\n        # maxnorm = 1.0\n        _test(1., False, False, False)\n        _test(1., True, False, False)\n        _test(1., True, True, False)\n        _test(1., True, False, True)\n        _test(1., True, True, True)\n        # maxnorm = 4.0\n        _test(4., False, False, False)\n        _test(4., True, False, False)\n        _test(4., True, True, False)\n        _test(4., True, False, True)\n        _test(4., True, True, True)\n\n    def test_normalize(self):\n        """"""Check normalization and denormalization for range values and consistency """"""\n        print("" > Testing normalization and denormalization."")\n        wav = self.ap.load_wav(WAV_FILE)\n        wav = self.ap.sound_norm(wav)  # normalize audio to get abetter normalization range below.\n        self.ap.signal_norm = False\n        x = self.ap.melspectrogram(wav)\n        x_old = x\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = False\n        self.ap.clip_norm = False\n        self.ap.max_norm = 4.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n        assert (x_old - x).sum() == 0\n        # check value range\n        assert x_norm.max() <= self.ap.max_norm + 1, x_norm.max()\n        assert x_norm.min() >= 0 - 1, x_norm.min()\n        # check denorm.\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3, (x - x_).mean()\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = False\n        self.ap.clip_norm = True\n        self.ap.max_norm = 4.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n\n\n        assert (x_old - x).sum() == 0\n        # check value range\n        assert x_norm.max() <= self.ap.max_norm, x_norm.max()\n        assert x_norm.min() >= 0, x_norm.min()\n        # check denorm.\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3, (x - x_).mean()\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = True\n        self.ap.clip_norm = False\n        self.ap.max_norm = 4.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n\n\n        assert (x_old - x).sum() == 0\n        # check value range\n        assert x_norm.max() <= self.ap.max_norm + 1, x_norm.max()\n        assert x_norm.min() >= -self.ap.max_norm - 2, x_norm.min()\n        assert x_norm.min() <= 0, x_norm.min()\n        # check denorm.\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3, (x - x_).mean()\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = True\n        self.ap.clip_norm = True\n        self.ap.max_norm = 4.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n\n\n        assert (x_old - x).sum() == 0\n        # check value range\n        assert x_norm.max() <= self.ap.max_norm, x_norm.max()\n        assert x_norm.min() >= -self.ap.max_norm, x_norm.min()\n        assert x_norm.min() <= 0, x_norm.min()\n        # check denorm.\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3, (x - x_).mean()\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = False\n        self.ap.max_norm = 1.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n\n\n        assert (x_old - x).sum() == 0\n        assert x_norm.max() <= self.ap.max_norm, x_norm.max()\n        assert x_norm.min() >= 0, x_norm.min()\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3\n\n        self.ap.signal_norm = True\n        self.ap.symmetric_norm = True\n        self.ap.max_norm = 1.0\n        x_norm = self.ap._normalize(x)\n        print(f"" > MaxNorm: {self.ap.max_norm}, ClipNorm:{self.ap.clip_norm}, SymmetricNorm:{self.ap.symmetric_norm}, SignalNorm:{self.ap.signal_norm} Range-> {x_norm.max()} --  {x_norm.min()}"")\n\n\n        assert (x_old - x).sum() == 0\n        assert x_norm.max() <= self.ap.max_norm, x_norm.max()\n        assert x_norm.min() >= -self.ap.max_norm, x_norm.min()\n        assert x_norm.min() < 0, x_norm.min()\n        x_ = self.ap._denormalize(x_norm)\n        assert (x - x_).sum() < 1e-3\n\n    def test_scaler(self):\n        scaler_stats_path = os.path.join(get_tests_input_path(), \'scale_stats.npy\')\n        conf.audio[\'stats_path\'] = scaler_stats_path\n        conf.audio[\'preemphasis\'] = 0.0\n        conf.audio[\'do_trim_silence\'] = True\n        conf.audio[\'signal_norm\'] = True\n\n        ap = AudioProcessor(**conf.audio)\n        mel_mean, mel_std, linear_mean, linear_std, _ = ap.load_stats(scaler_stats_path)\n        ap.setup_scaler(mel_mean, mel_std, linear_mean, linear_std)\n\n        self.ap.signal_norm = False\n        self.ap.preemphasis = 0.0\n\n        # test scaler forward and backward transforms\n        wav = self.ap.load_wav(WAV_FILE)\n        mel_reference = self.ap.melspectrogram(wav)\n        mel_norm = ap.melspectrogram(wav)\n        mel_denorm = ap._denormalize(mel_norm)\n        assert abs(mel_reference - mel_denorm).max() < 1e-4\n'"
tests/test_demo_server.py,0,"b'import os\nimport unittest\n\nimport torch as T\n\nfrom TTS.server.synthesizer import Synthesizer\nfrom TTS.tests import get_tests_input_path, get_tests_output_path\nfrom TTS.utils.text.symbols import make_symbols, phonemes, symbols\nfrom TTS.utils.generic_utils import setup_model\nfrom TTS.utils.io import load_config, save_checkpoint\n\n\nclass DemoServerTest(unittest.TestCase):\n    # pylint: disable=R0201\n    def _create_random_model(self):\n        # pylint: disable=global-statement\n        global symbols, phonemes\n        config = load_config(os.path.join(get_tests_output_path(), \'dummy_model_config.json\'))\n        if \'characters\' in config.keys():\n            symbols, phonemes = make_symbols(**config.characters)\n\n        num_chars = len(phonemes) if config.use_phonemes else len(symbols)\n        model = setup_model(num_chars, 0, config)\n        output_path = os.path.join(get_tests_output_path())\n        save_checkpoint(model, None, 10, 10, 1, output_path)\n\n    def test_in_out(self):\n        self._create_random_model()\n        config = load_config(os.path.join(get_tests_input_path(), \'server_config.json\'))\n        tts_root_path = get_tests_output_path()\n        config[\'tts_checkpoint\'] = os.path.join(tts_root_path, config[\'tts_checkpoint\'])\n        config[\'tts_config\'] = os.path.join(tts_root_path, config[\'tts_config\'])\n        synthesizer = Synthesizer(config)\n        synthesizer.tts(""Better this test works!!"")\n'"
tests/test_layers.py,0,"b'import unittest\nimport torch as T\n\nfrom TTS.layers.tacotron import Prenet, CBHG, Decoder, Encoder\nfrom TTS.layers.losses import L1LossMasked\nfrom TTS.utils.generic_utils import sequence_mask\n\n# pylint: disable=unused-variable\n\n\nclass PrenetTests(unittest.TestCase):\n    def test_in_out(self):\n        layer = Prenet(128, out_features=[256, 128])\n        dummy_input = T.rand(4, 128)\n\n        print(layer)\n        output = layer(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 128\n\n\nclass CBHGTests(unittest.TestCase):\n    def test_in_out(self):\n        #pylint: disable=attribute-defined-outside-init\n        layer = self.cbhg = CBHG(\n            128,\n            K=8,\n            conv_bank_features=80,\n            conv_projections=[160, 128],\n            highway_features=80,\n            gru_features=80,\n            num_highways=4)\n        # B x D x T\n        dummy_input = T.rand(4, 128, 8) \n\n        print(layer)\n        output = layer(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 8\n        assert output.shape[2] == 160\n\n\nclass DecoderTests(unittest.TestCase):\n    @staticmethod\n    def test_in_out():\n        layer = Decoder(\n            in_features=256,\n            memory_dim=80,\n            r=2,\n            memory_size=4,\n            attn_windowing=False,\n            attn_norm=""sigmoid"",\n            attn_K=5,\n            attn_type=""original"",\n            prenet_type=\'original\',\n            prenet_dropout=True,\n            forward_attn=True,\n            trans_agent=True,\n            forward_attn_mask=True,\n            location_attn=True,\n            separate_stopnet=True,\n            speaker_embedding_dim=0)\n        dummy_input = T.rand(4, 8, 256)\n        dummy_memory = T.rand(4, 2, 80)\n\n        output, alignment, stop_tokens = layer(\n            dummy_input, dummy_memory, mask=None)\n\n        assert output.shape[0] == 4\n        assert output.shape[1] == 80, ""size not {}"".format(output.shape[1])\n        assert output.shape[2] == 2, ""size not {}"".format(output.shape[2])\n        assert stop_tokens.shape[0] == 4\n\n    @staticmethod\n    def test_in_out_multispeaker():\n        layer = Decoder(\n            in_features=256,\n            memory_dim=80,\n            r=2,\n            memory_size=4,\n            attn_windowing=False,\n            attn_norm=""sigmoid"",\n            attn_K=5,\n            attn_type=""graves"",\n            prenet_type=\'original\',\n            prenet_dropout=True,\n            forward_attn=True,\n            trans_agent=True,\n            forward_attn_mask=True,\n            location_attn=True,\n            separate_stopnet=True,\n            speaker_embedding_dim=80)\n        dummy_input = T.rand(4, 8, 256)\n        dummy_memory = T.rand(4, 2, 80)\n        dummy_embed = T.rand(4, 80)\n\n        output, alignment, stop_tokens = layer(\n            dummy_input, dummy_memory, mask=None, speaker_embeddings=dummy_embed)\n\n        assert output.shape[0] == 4\n        assert output.shape[1] == 80, ""size not {}"".format(output.shape[1])\n        assert output.shape[2] == 2, ""size not {}"".format(output.shape[2])\n        assert stop_tokens.shape[0] == 4\n\n\nclass EncoderTests(unittest.TestCase):\n    def test_in_out(self):\n        layer = Encoder(128)\n        dummy_input = T.rand(4, 8, 128)\n\n        print(layer)\n        output = layer(dummy_input)\n        print(output.shape)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 8\n        assert output.shape[2] == 256  # 128 * 2 BiRNN\n\n\nclass L1LossMaskedTests(unittest.TestCase):\n    def test_in_out(self):\n        # test input == target\n        layer = L1LossMasked(seq_len_norm=False)\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.ones(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 0.0\n\n        # test input != target\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.zeros(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 1.0, ""1.0 vs {}"".format(output.item())\n\n        # test if padded values of input makes any difference\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.zeros(4, 8, 128).float()\n        dummy_length = (T.arange(5, 9)).long()\n        mask = (\n            (sequence_mask(dummy_length).float() - 1.0) * 100.0).unsqueeze(2)\n        output = layer(dummy_input + mask, dummy_target, dummy_length)\n        assert output.item() == 1.0, ""1.0 vs {}"".format(output.item())\n\n        dummy_input = T.rand(4, 8, 128).float()\n        dummy_target = dummy_input.detach()\n        dummy_length = (T.arange(5, 9)).long()\n        mask = (\n            (sequence_mask(dummy_length).float() - 1.0) * 100.0).unsqueeze(2)\n        output = layer(dummy_input + mask, dummy_target, dummy_length)\n        assert output.item() == 0, ""0 vs {}"".format(output.item())\n\n        # seq_len_norm = True\n        # test input == target\n        layer = L1LossMasked(seq_len_norm=True)\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.ones(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 0.0\n\n        # test input != target\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.zeros(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 1.0, ""1.0 vs {}"".format(output.item())\n\n        # test if padded values of input makes any difference\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.zeros(4, 8, 128).float()\n        dummy_length = (T.arange(5, 9)).long()\n        mask = (\n            (sequence_mask(dummy_length).float() - 1.0) * 100.0).unsqueeze(2)\n        output = layer(dummy_input + mask, dummy_target, dummy_length)\n        assert abs(output.item() - 1.0) < 1e-5, ""1.0 vs {}"".format(output.item())\n\n        dummy_input = T.rand(4, 8, 128).float()\n        dummy_target = dummy_input.detach()\n        dummy_length = (T.arange(5, 9)).long()\n        mask = (\n            (sequence_mask(dummy_length).float() - 1.0) * 100.0).unsqueeze(2)\n        output = layer(dummy_input + mask, dummy_target, dummy_length)\n        assert output.item() == 0, ""0 vs {}"".format(output.item())\n'"
tests/test_loader.py,2,"b'import os\nimport unittest\nimport shutil\nimport torch\nimport numpy as np\n\nfrom torch.utils.data import DataLoader\nfrom TTS.utils.io import load_config\nfrom TTS.utils.audio import AudioProcessor\nfrom TTS.datasets import TTSDataset\nfrom TTS.datasets.preprocess import ljspeech\n\n#pylint: disable=unused-variable\n\nfile_path = os.path.dirname(os.path.realpath(__file__))\nOUTPATH = os.path.join(file_path, ""outputs/loader_tests/"")\nos.makedirs(OUTPATH, exist_ok=True)\nc = load_config(os.path.join(file_path, \'test_config.json\'))\nok_ljspeech = os.path.exists(c.data_path)\n\nDATA_EXIST = True\nif not os.path.exists(c.data_path):\n    DATA_EXIST = False\n\nprint("" > Dynamic data loader test: {}"".format(DATA_EXIST))\n\n\nclass TestTTSDataset(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super(TestTTSDataset, self).__init__(*args, **kwargs)\n        self.max_loader_iter = 4\n        self.ap = AudioProcessor(**c.audio)\n\n    def _create_dataloader(self, batch_size, r, bgs):\n        items = ljspeech(c.data_path,\'metadata.csv\')\n        dataset = TTSDataset.MyDataset(\n            r,\n            c.text_cleaner,\n            compute_linear_spec=True,\n            ap=self.ap,\n            meta_data=items,\n            tp=c.characters if \'characters\' in c.keys() else None,\n            batch_group_size=bgs,\n            min_seq_len=c.min_seq_len,\n            max_seq_len=float(""inf""),\n            use_phonemes=False)\n        dataloader = DataLoader(\n            dataset,\n            batch_size=batch_size,\n            shuffle=False,\n            collate_fn=dataset.collate_fn,\n            drop_last=True,\n            num_workers=c.num_loader_workers)\n        return dataloader, dataset\n\n    def test_loader(self):\n        if ok_ljspeech:\n            dataloader, dataset = self._create_dataloader(2, c.r, 0)\n\n            for i, data in enumerate(dataloader):\n                if i == self.max_loader_iter:\n                    break\n                text_input = data[0]\n                text_lengths = data[1]\n                speaker_name = data[2]\n                linear_input = data[3]\n                mel_input = data[4]\n                mel_lengths = data[5]\n                stop_target = data[6]\n                item_idx = data[7]\n\n                neg_values = text_input[text_input < 0]\n                check_count = len(neg_values)\n                assert check_count == 0, \\\n                    "" !! Negative values in text_input: {}"".format(check_count)\n                # TODO: more assertion here\n                assert type(speaker_name[0]) is str\n                assert linear_input.shape[0] == c.batch_size\n                assert linear_input.shape[2] == self.ap.num_freq\n                assert mel_input.shape[0] == c.batch_size\n                assert mel_input.shape[2] == c.audio[\'num_mels\']\n                # check normalization ranges\n                if self.ap.symmetric_norm:\n                    assert mel_input.max() <= self.ap.max_norm\n                    assert mel_input.min() >= -self.ap.max_norm\n                    assert mel_input.min() < 0\n                else:\n                    assert mel_input.max() <= self.ap.max_norm\n                    assert mel_input.min() >= 0\n\n    def test_batch_group_shuffle(self):\n        if ok_ljspeech:\n            dataloader, dataset = self._create_dataloader(2, c.r, 16)\n            last_length = 0\n            frames = dataset.items\n            for i, data in enumerate(dataloader):\n                if i == self.max_loader_iter:\n                    break\n                text_input = data[0]\n                text_lengths = data[1]\n                speaker_name = data[2]\n                linear_input = data[3]\n                mel_input = data[4]\n                mel_lengths = data[5]\n                stop_target = data[6]\n                item_idx = data[7]\n\n                avg_length = mel_lengths.numpy().mean()\n                assert avg_length >= last_length\n            dataloader.dataset.sort_items()\n            is_items_reordered = False\n            for idx, item in enumerate(dataloader.dataset.items):\n                if item != frames[idx]:\n                    is_items_reordered = True\n                    break\n            assert is_items_reordered\n\n    def test_padding_and_spec(self):\n        if ok_ljspeech:\n            dataloader, dataset = self._create_dataloader(1, 1, 0)\n\n            for i, data in enumerate(dataloader):\n                if i == self.max_loader_iter:\n                    break\n                text_input = data[0]\n                text_lengths = data[1]\n                speaker_name = data[2]\n                linear_input = data[3]\n                mel_input = data[4]\n                mel_lengths = data[5]\n                stop_target = data[6]\n                item_idx = data[7]\n\n                # check mel_spec consistency\n                wav = np.asarray(self.ap.load_wav(item_idx[0]), dtype=np.float32)\n                mel = self.ap.melspectrogram(wav).astype(\'float32\')\n                mel = torch.FloatTensor(mel).contiguous()\n                mel_dl = mel_input[0]\n                # NOTE: Below needs to check == 0 but due to an unknown reason\n                # there is a slight difference between two matrices.\n                # TODO: Check this assert cond more in detail.\n                assert abs(mel.T - mel_dl).max() < 1e-5, abs(mel.T - mel_dl).max()\n\n                # check mel-spec correctness\n                mel_spec = mel_input[0].cpu().numpy()\n                wav = self.ap.inv_melspectrogram(mel_spec.T)\n                self.ap.save_wav(wav, OUTPATH + \'/mel_inv_dataloader.wav\')\n                shutil.copy(item_idx[0], OUTPATH + \'/mel_target_dataloader.wav\')\n\n                # check linear-spec\n                linear_spec = linear_input[0].cpu().numpy()\n                wav = self.ap.inv_spectrogram(linear_spec.T)\n                self.ap.save_wav(wav, OUTPATH + \'/linear_inv_dataloader.wav\')\n                shutil.copy(item_idx[0],\n                            OUTPATH + \'/linear_target_dataloader.wav\')\n\n                # check the last time step to be zero padded\n                assert linear_input[0, -1].sum() != 0\n                assert linear_input[0, -2].sum() != 0\n                assert mel_input[0, -1].sum() != 0\n                assert mel_input[0, -2].sum() != 0\n                assert stop_target[0, -1] == 1\n                assert stop_target[0, -2] == 0\n                assert stop_target.sum() == 1\n                assert len(mel_lengths.shape) == 1\n                assert mel_lengths[0] == linear_input[0].shape[0]\n                assert mel_lengths[0] == mel_input[0].shape[0]\n\n            # Test for batch size 2\n            dataloader, dataset = self._create_dataloader(2, 1, 0)\n\n            for i, data in enumerate(dataloader):\n                if i == self.max_loader_iter:\n                    break\n                text_input = data[0]\n                text_lengths = data[1]\n                speaker_name = data[2]\n                linear_input = data[3]\n                mel_input = data[4]\n                mel_lengths = data[5]\n                stop_target = data[6]\n                item_idx = data[7]\n\n                if mel_lengths[0] > mel_lengths[1]:\n                    idx = 0\n                else:\n                    idx = 1\n\n                # check the first item in the batch\n                assert linear_input[idx, -1].sum() != 0\n                assert linear_input[idx, -2].sum() != 0, linear_input\n                assert mel_input[idx, -1].sum() != 0\n                assert mel_input[idx, -2].sum() != 0, mel_input\n                assert stop_target[idx, -1] == 1\n                assert stop_target[idx, -2] == 0\n                assert stop_target[idx].sum() == 1\n                assert len(mel_lengths.shape) == 1\n                assert mel_lengths[idx] == mel_input[idx].shape[0]\n                assert mel_lengths[idx] == linear_input[idx].shape[0]\n\n                # check the second itme in the batch\n                assert linear_input[1 - idx, -1].sum() == 0\n                assert mel_input[1 - idx, -1].sum() == 0\n                assert stop_target[1, mel_lengths[1]-1] == 1\n                assert stop_target[1, mel_lengths[1]:].sum() == 0\n                assert len(mel_lengths.shape) == 1\n\n                # check batch zero-frame conditions (zero-frame disabled)\n                # assert (linear_input * stop_target.unsqueeze(2)).sum() == 0\n                # assert (mel_input * stop_target.unsqueeze(2)).sum() == 0\n'"
tests/test_preprocessors.py,0,"b'import unittest\nimport os\nfrom TTS.tests import get_tests_input_path\n\nfrom TTS.datasets.preprocess import common_voice\n\n\nclass TestPreprocessors(unittest.TestCase):\n\n    def test_common_voice_preprocessor(self):\n        root_path = get_tests_input_path()\n        meta_file = ""common_voice.tsv""\n        items = common_voice(root_path, meta_file)\n        assert items[0][0] == ""Man sollte den L\xc3\xa4nderfinanzausgleich durch "" \\\n                              ""einen Bundesliga-Soli ersetzen.""\n        assert items[0][1] == os.path.join(get_tests_input_path(), ""clips"",\n                                           ""21fce545b24d9a5af0403b949e95e8dd3""\n                                           ""c10c4ff3e371f14e4d5b4ebf588670b7c""\n                                           ""9e618285fc872d94a89ed7f0217d9019f""\n                                           ""e5de33f1577b49dcd518eacf63c4b.wav"")\n\n        assert items[-1][0] == ""Warum werden da keine strafrechtlichen "" \\\n                               ""Konsequenzen gezogen?""\n        assert items[-1][1] == os.path.join(get_tests_input_path(), ""clips"",\n                                            ""ad2f69e053b0e20e01c82b9821fe5787f1""\n                                            ""cc8e4b0b97f0e4cab1e9a652c577169c82""\n                                            ""44fb222281a60ee3081854014113e04c4c""\n                                            ""a43643100b7c01dab0fac11974.wav"")\n'"
tests/test_tacotron2_model.py,13,"b'import os\nimport copy\nimport torch\nimport unittest\nimport numpy as np\n\nfrom torch import optim\nfrom torch import nn\nfrom TTS.utils.io import load_config\nfrom TTS.layers.losses import MSELossMasked\nfrom TTS.models.tacotron2 import Tacotron2\n\n#pylint: disable=unused-variable\n\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nfile_path = os.path.dirname(os.path.realpath(__file__))\nc = load_config(os.path.join(file_path, \'test_config.json\'))\n\n\nclass TacotronTrainTest(unittest.TestCase):\n    def test_train_step(self):\n        input = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8, )).long().to(device)\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\n        mel_spec = torch.rand(8, 30, c.audio[\'num_mels\']).to(device)\n        mel_postnet_spec = torch.rand(8, 30, c.audio[\'num_mels\']).to(device)\n        mel_lengths = torch.randint(20, 30, (8, )).long().to(device)\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\n        speaker_ids = torch.randint(0, 5, (8, )).long().to(device)\n\n        for idx in mel_lengths:\n            stop_targets[:, int(idx.item()):, 0] = 1.0\n\n        stop_targets = stop_targets.view(input.shape[0],\n                                         stop_targets.size(1) // c.r, -1)\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\n\n        criterion = MSELossMasked(seq_len_norm=False).to(device)\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\n        model = Tacotron2(num_chars=24, r=c.r, num_speakers=5).to(device)\n        model.train()\n        model_ref = copy.deepcopy(model)\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            assert (param - param_ref).sum() == 0, param\n            count += 1\n        optimizer = optim.Adam(model.parameters(), lr=c.lr)\n        for i in range(5):\n            mel_out, mel_postnet_out, align, stop_tokens = model.forward(\n                input, input_lengths, mel_spec, speaker_ids)\n            assert torch.sigmoid(stop_tokens).data.max() <= 1.0\n            assert torch.sigmoid(stop_tokens).data.min() >= 0.0\n            optimizer.zero_grad()\n            loss = criterion(mel_out, mel_spec, mel_lengths)\n            stop_loss = criterion_st(stop_tokens, stop_targets)\n            loss = loss + criterion(mel_postnet_out, mel_postnet_spec, mel_lengths) + stop_loss\n            loss.backward()\n            optimizer.step()\n        # check parameter changes\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            # ignore pre-higway layer since it works conditional\n            # if count not in [145, 59]:\n            assert (param != param_ref).any(\n            ), ""param {} with shape {} not updated!! \\n{}\\n{}"".format(\n                count, param.shape, param, param_ref)\n            count += 1\n'"
tests/test_tacotron2_tf_model.py,11,"b'import os\nimport torch\nimport unittest\nimport numpy as np\nimport tensorflow as tf\n\nfrom TTS.utils.io import load_config\nfrom TTS.tf.models.tacotron2 import Tacotron2\n\n#pylint: disable=unused-variable\n\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nfile_path = os.path.dirname(os.path.realpath(__file__))\nc = load_config(os.path.join(file_path, \'test_config.json\'))\n\n\nclass TacotronTFTrainTest(unittest.TestCase):\n\n    @staticmethod\n    def generate_dummy_inputs():\n        chars_seq = torch.randint(0, 24, (8, 128)).long().to(device)\n        chars_seq_lengths = torch.randint(100, 128, (8, )).long().to(device)\n        chars_seq_lengths = torch.sort(chars_seq_lengths, descending=True)[0]\n        mel_spec = torch.rand(8, 30, c.audio[\'num_mels\']).to(device)\n        mel_postnet_spec = torch.rand(8, 30, c.audio[\'num_mels\']).to(device)\n        mel_lengths = torch.randint(20, 30, (8, )).long().to(device)\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\n        speaker_ids = torch.randint(0, 5, (8, )).long().to(device)\n\n        chars_seq = tf.convert_to_tensor(chars_seq.cpu().numpy())\n        chars_seq_lengths = tf.convert_to_tensor(chars_seq_lengths.cpu().numpy())\n        mel_spec = tf.convert_to_tensor(mel_spec.cpu().numpy())\n        return chars_seq, chars_seq_lengths, mel_spec, mel_postnet_spec, mel_lengths,\\\n            stop_targets, speaker_ids\n\n    def test_train_step(self):\n        \'\'\' test forward pass \'\'\'\n        chars_seq, chars_seq_lengths, mel_spec, mel_postnet_spec, mel_lengths,\\\n            stop_targets, speaker_ids = self.generate_dummy_inputs()\n\n        for idx in mel_lengths:\n            stop_targets[:, int(idx.item()):, 0] = 1.0\n\n        stop_targets = stop_targets.view(chars_seq.shape[0],\n                                         stop_targets.size(1) // c.r, -1)\n        stop_targets = (stop_targets.sum(2) > 0.0).unsqueeze(2).float().squeeze()\n\n        model = Tacotron2(num_chars=24, r=c.r, num_speakers=5)\n        # training pass\n        output = model(chars_seq, chars_seq_lengths, mel_spec, training=True)\n\n        # check model output shapes\n        assert np.all(output[0].shape == mel_spec.shape)\n        assert np.all(output[1].shape == mel_spec.shape)\n        assert output[2].shape[2] == chars_seq.shape[1]\n        assert output[2].shape[1] == (mel_spec.shape[1] // model.decoder.r)\n        assert output[3].shape[1] == (mel_spec.shape[1] // model.decoder.r)\n\n        # inference pass\n        output = model(chars_seq, training=False)\n'"
tests/test_tacotron_model.py,17,"b'import os\nimport copy\nimport torch\nimport unittest\n\nfrom torch import optim\nfrom torch import nn\nfrom TTS.utils.io import load_config\nfrom TTS.layers.losses import L1LossMasked\nfrom TTS.models.tacotron import Tacotron\n\n#pylint: disable=unused-variable\n\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(""cuda:0"" if torch.cuda.is_available() else ""cpu"")\n\nfile_path = os.path.dirname(os.path.realpath(__file__))\nc = load_config(os.path.join(file_path, \'test_config.json\'))\n\n\ndef count_parameters(model):\n    r""""""Count number of trainable parameters in a network""""""\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (8, )).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(8, 30, c.audio[\'num_mels\']).to(device)\n        linear_spec = torch.rand(8, 30, c.audio[\'num_freq\']).to(device)\n        mel_lengths = torch.randint(20, 30, (8, )).long().to(device)\n        stop_targets = torch.zeros(8, 30, 1).float().to(device)\n        speaker_ids = torch.randint(0, 5, (8, )).long().to(device)\n\n        for idx in mel_lengths:\n            stop_targets[:, int(idx.item()):, 0] = 1.0\n\n        stop_targets = stop_targets.view(input_dummy.shape[0],\n                                         stop_targets.size(1) // c.r, -1)\n        stop_targets = (stop_targets.sum(2) >\n                        0.0).unsqueeze(2).float().squeeze()\n\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\n        model = Tacotron(\n            num_chars=32,\n            num_speakers=5,\n            postnet_output_dim=c.audio[\'num_freq\'],\n            decoder_output_dim=c.audio[\'num_mels\'],\n            r=c.r,\n            memory_size=c.memory_size\n        ).to(device)  #FIXME: missing num_speakers parameter to Tacotron ctor\n        model.train()\n        print("" > Num parameters for Tacotron model:%s"" %\n              (count_parameters(model)))\n        model_ref = copy.deepcopy(model)\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            assert (param - param_ref).sum() == 0, param\n            count += 1\n        optimizer = optim.Adam(model.parameters(), lr=c.lr)\n        for _ in range(5):\n            mel_out, linear_out, align, stop_tokens = model.forward(\n                input_dummy, input_lengths, mel_spec, speaker_ids)\n            optimizer.zero_grad()\n            loss = criterion(mel_out, mel_spec, mel_lengths)\n            stop_loss = criterion_st(stop_tokens, stop_targets)\n            loss = loss + criterion(linear_out, linear_spec,\n                                    mel_lengths) + stop_loss\n            loss.backward()\n            optimizer.step()\n        # check parameter changes\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            # ignore pre-higway layer since it works conditional\n            # if count not in [145, 59]:\n            assert (param != param_ref).any(\n            ), ""param {} with shape {} not updated!! \\n{}\\n{}"".format(\n                count, param.shape, param, param_ref)\n            count += 1\n\n\nclass TacotronGSTTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (8, )).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(8, 120, c.audio[\'num_mels\']).to(device)\n        linear_spec = torch.rand(8, 120, c.audio[\'num_freq\']).to(device)\n        mel_lengths = torch.randint(20, 120, (8, )).long().to(device)\n        stop_targets = torch.zeros(8, 120, 1).float().to(device)\n        speaker_ids = torch.randint(0, 5, (8, )).long().to(device)\n\n        for idx in mel_lengths:\n            stop_targets[:, int(idx.item()):, 0] = 1.0\n\n        stop_targets = stop_targets.view(input_dummy.shape[0],\n                                         stop_targets.size(1) // c.r, -1)\n        stop_targets = (stop_targets.sum(2) >\n                        0.0).unsqueeze(2).float().squeeze()\n\n        criterion = L1LossMasked(seq_len_norm=False).to(device)\n        criterion_st = nn.BCEWithLogitsLoss().to(device)\n        model = Tacotron(\n            num_chars=32,\n            num_speakers=5,\n            gst=True,\n            postnet_output_dim=c.audio[\'num_freq\'],\n            decoder_output_dim=c.audio[\'num_mels\'],\n            r=c.r,\n            memory_size=c.memory_size\n        ).to(device)  #FIXME: missing num_speakers parameter to Tacotron ctor\n        model.train()\n        print(model)\n        print("" > Num parameters for Tacotron GST model:%s"" %\n              (count_parameters(model)))\n        model_ref = copy.deepcopy(model)\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            assert (param - param_ref).sum() == 0, param\n            count += 1\n        optimizer = optim.Adam(model.parameters(), lr=c.lr)\n        for _ in range(10):\n            mel_out, linear_out, align, stop_tokens = model.forward(\n                input_dummy, input_lengths, mel_spec, speaker_ids)\n            optimizer.zero_grad()\n            loss = criterion(mel_out, mel_spec, mel_lengths)\n            stop_loss = criterion_st(stop_tokens, stop_targets)\n            loss = loss + criterion(linear_out, linear_spec,\n                                    mel_lengths) + stop_loss\n            loss.backward()\n            optimizer.step()\n        # check parameter changes\n        count = 0\n        for param, param_ref in zip(model.parameters(),\n                                    model_ref.parameters()):\n            # ignore pre-higway layer since it works conditional\n            assert (param != param_ref).any(\n            ), ""param {} with shape {} not updated!! \\n{}\\n{}"".format(\n                count, param.shape, param, param_ref)\n            count += 1\n'"
tests/test_text_processing.py,0,"b'import os\n# pylint: disable=unused-wildcard-import\n# pylint: disable=wildcard-import\n# pylint: disable=unused-import\nimport unittest\nfrom TTS.utils.text import *\nfrom TTS.tests import get_tests_path\nfrom TTS.utils.io import load_config\n\nTESTS_PATH = get_tests_path()\nconf = load_config(os.path.join(TESTS_PATH, \'test_config.json\'))\n\ndef test_phoneme_to_sequence():\n    text = ""Recent research at Harvard has shown meditating for as little as 8 weeks can actually increase, the grey matter in the parts of the brain responsible for emotional regulation and learning!""\n    text_cleaner = [""phoneme_cleaners""]\n    lang = ""en-us""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""\xc9\xb9i\xcb\x90s\xc9\x99nt \xc9\xb9\xc9\xaas\xc9\x9c\xcb\x90t\xca\x83 \xc3\xa6t h\xc9\x91\xcb\x90\xc9\xb9v\xc9\x9ad h\xc9\x90z \xca\x83o\xca\x8an m\xc9\x9bd\xe1\xb5\xbbte\xc9\xaa\xc9\xbe\xc9\xaa\xc5\x8b f\xc9\x94\xcb\x90\xc9\xb9 \xc3\xa6z l\xc9\xaa\xc9\xbe\xc9\x99l \xc3\xa6z e\xc9\xaat wi\xcb\x90ks k\xc3\xa6n \xc3\xa6kt\xca\x83u\xcb\x90\xc9\x99li \xc9\xaank\xc9\xb9i\xcb\x90s, \xc3\xb0\xc9\x99 \xc9\xa1\xc9\xb9e\xc9\xaa m\xc3\xa6\xc9\xbe\xc9\x9a\xc9\xb9 \xc9\xaan\xc3\xb0\xc9\x99 p\xc9\x91\xcb\x90\xc9\xb9ts \xca\x8cv\xc3\xb0\xc9\x99 b\xc9\xb9e\xc9\xaan \xc9\xb9\xc9\xaasp\xc9\x91\xcb\x90ns\xc9\x99b\xc9\x99l f\xc9\x94\xcb\x90\xc9\xb9 \xc9\xaamo\xca\x8a\xca\x83\xc9\x99n\xc9\x99l \xc9\xb9\xc9\x9b\xc9\xa1ju\xcb\x90le\xc9\xaa\xca\x83\xc9\x99n \xc3\xa6nd l\xc9\x9c\xcb\x90n\xc9\xaa\xc5\x8b!""\n    assert text_hat == text_hat_with_params == gt \n\n    # multiple punctuations\n    text = ""Be a voice, not an! echo?""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n! \xc9\x9bko\xca\x8a?""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\n    # not ending with punctuation\n    text = ""Be a voice, not an! echo""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n! \xc9\x9bko\xca\x8a""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\n    # original\n    text = ""Be a voice, not an echo!""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n \xc9\x9bko\xca\x8a!""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\n    # extra space after the sentence\n    text = ""Be a voice, not an! echo.  ""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n! \xc9\x9bko\xca\x8a.""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\n    # extra space after the sentence\n    text = ""Be a voice, not an! echo.  ""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang, True)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""^bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n! \xc9\x9bko\xca\x8a.~""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\n    # padding char\n    text = ""_Be a _voice, not an! echo_""\n    sequence = phoneme_to_sequence(text, text_cleaner, lang)\n    text_hat = sequence_to_phoneme(sequence)\n    sequence_with_params = phoneme_to_sequence(text, text_cleaner, lang, tp=conf.characters)\n    text_hat_with_params = sequence_to_phoneme(sequence, tp=conf.characters)\n    gt = ""bi\xcb\x90 \xc9\x90 v\xc9\x94\xc9\xaas, n\xc9\x91\xcb\x90t \xc9\x90n! \xc9\x9bko\xca\x8a""\n    print(text_hat)\n    print(len(sequence))\n    assert text_hat == text_hat_with_params == gt\n\ndef test_text2phone():\n    text = ""Recent research at Harvard has shown meditating for as little as 8 weeks can actually increase, the grey matter in the parts of the brain responsible for emotional regulation and learning!""\n    gt = ""\xc9\xb9|i\xcb\x90|s|\xc9\x99|n|t| |\xc9\xb9|\xc9\xaa|s|\xc9\x9c\xcb\x90|t\xca\x83| |\xc3\xa6|t| |h|\xc9\x91\xcb\x90\xc9\xb9|v|\xc9\x9a|d| |h|\xc9\x90|z| |\xca\x83|o\xca\x8a|n| |m|\xc9\x9b|d|\xe1\xb5\xbb|t|e\xc9\xaa|\xc9\xbe|\xc9\xaa|\xc5\x8b| |f|\xc9\x94\xcb\x90|\xc9\xb9| |\xc3\xa6|z| |l|\xc9\xaa|\xc9\xbe|\xc9\x99l| |\xc3\xa6|z| |e\xc9\xaa|t| |w|i\xcb\x90|k|s| |k|\xc3\xa6|n| |\xc3\xa6|k|t\xca\x83|u\xcb\x90|\xc9\x99l|i| |\xc9\xaa|n|k|\xc9\xb9|i\xcb\x90|s|,| |\xc3\xb0|\xc9\x99| |\xc9\xa1|\xc9\xb9|e\xc9\xaa| |m|\xc3\xa6|\xc9\xbe|\xc9\x9a|\xc9\xb9| |\xc9\xaa|n|\xc3\xb0|\xc9\x99| |p|\xc9\x91\xcb\x90\xc9\xb9|t|s| |\xca\x8c|v|\xc3\xb0|\xc9\x99| |b|\xc9\xb9|e\xc9\xaa|n| |\xc9\xb9|\xc9\xaa|s|p|\xc9\x91\xcb\x90|n|s|\xc9\x99|b|\xc9\x99l| |f|\xc9\x94\xcb\x90|\xc9\xb9| |\xc9\xaa|m|o\xca\x8a|\xca\x83|\xc9\x99|n|\xc9\x99l| |\xc9\xb9|\xc9\x9b|\xc9\xa1|j|u\xcb\x90|l|e\xc9\xaa|\xca\x83|\xc9\x99|n| |\xc3\xa6|n|d| |l|\xc9\x9c\xcb\x90|n|\xc9\xaa|\xc5\x8b|!""\n    lang = ""en-us""\n    ph = text2phone(text, lang)\n    assert gt == ph, f""\\n{phonemes} \\n vs \\n{gt}""\n'"
tf/convert_tacotron2_torch_to_tf.py,7,"b""# %%\nimport sys\nsys.path.append('/home/erogol/Projects')\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = ''\n# %%\nimport argparse\nimport numpy as np\nimport torch\nimport tensorflow as tf\nfrom fuzzywuzzy import fuzz\n\nfrom TTS.utils.text.symbols import phonemes, symbols\nfrom TTS.utils.generic_utils import setup_model\nfrom TTS.utils.io import load_config\nfrom TTS.tf.models.tacotron2 import Tacotron2\nfrom TTS.tf.utils.convert_torch_to_tf_utils import compare_torch_tf, tf_create_dummy_inputs, transfer_weights_torch_to_tf, convert_tf_name\nfrom TTS.tf.utils.generic_utils import save_checkpoint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--torch_model_path',\n                    type=str,\n                    help='Path to target torch model to be converted to TF.')\nparser.add_argument('--config_path',\n                    type=str,\n                    help='Path to config file of torch model.')\nparser.add_argument('--output_path',\n                    type=str,\n                    help='path to save TF model weights.')\nargs = parser.parse_args()\n\n# load model config\nconfig_path = args.config_path\nc = load_config(config_path)\nnum_speakers = 0\n\n# init torch model\nnum_chars = len(phonemes) if c.use_phonemes else len(symbols)\nmodel = setup_model(num_chars, num_speakers, c)\ncheckpoint = torch.load(args.torch_model_path,\n                        map_location=torch.device('cpu'))\nstate_dict = checkpoint['model']\nmodel.load_state_dict(state_dict)\n\n# init tf model\nmodel_tf = Tacotron2(num_chars=num_chars,\n                     num_speakers=num_speakers,\n                     r=model.decoder.r,\n                     postnet_output_dim=c.audio['num_mels'],\n                     decoder_output_dim=c.audio['num_mels'],\n                     attn_type=c.attention_type,\n                     attn_win=c.windowing,\n                     attn_norm=c.attention_norm,\n                     prenet_type=c.prenet_type,\n                     prenet_dropout=c.prenet_dropout,\n                     forward_attn=c.use_forward_attn,\n                     trans_agent=c.transition_agent,\n                     forward_attn_mask=c.forward_attn_mask,\n                     location_attn=c.location_attn,\n                     attn_K=c.attention_heads,\n                     separate_stopnet=c.separate_stopnet,\n                     bidirectional_decoder=c.bidirectional_decoder)\n\n# set initial layer mapping - these are not captured by the below heuristic approach\n# TODO: set layer names so that we can remove these manual matching\ncommon_sufix = '/.ATTRIBUTES/VARIABLE_VALUE'\nvar_map = [\n    ('tacotron2/embedding/embeddings:0', 'embedding.weight'),\n    ('tacotron2/encoder/lstm/forward_lstm/lstm_cell_1/kernel:0',\n     'encoder.lstm.weight_ih_l0'),\n    ('tacotron2/encoder/lstm/forward_lstm/lstm_cell_1/recurrent_kernel:0',\n     'encoder.lstm.weight_hh_l0'),\n    ('tacotron2/encoder/lstm/backward_lstm/lstm_cell_2/kernel:0',\n     'encoder.lstm.weight_ih_l0_reverse'),\n    ('tacotron2/encoder/lstm/backward_lstm/lstm_cell_2/recurrent_kernel:0',\n     'encoder.lstm.weight_hh_l0_reverse'),\n    ('tacotron2/encoder/lstm/forward_lstm/lstm_cell_1/bias:0',\n     ('encoder.lstm.bias_ih_l0', 'encoder.lstm.bias_hh_l0')),\n    ('tacotron2/encoder/lstm/backward_lstm/lstm_cell_2/bias:0',\n     ('encoder.lstm.bias_ih_l0_reverse', 'encoder.lstm.bias_hh_l0_reverse')),\n    ('attention/v/kernel:0', 'decoder.attention.v.linear_layer.weight'),\n    ('decoder/linear_projection/kernel:0',\n     'decoder.linear_projection.linear_layer.weight'),\n    ('decoder/stopnet/kernel:0', 'decoder.stopnet.1.linear_layer.weight')\n]\n\n# %%\n# get tf_model graph\ninput_ids, input_lengths, mel_outputs, mel_lengths = tf_create_dummy_inputs()\nmel_pred = model_tf(input_ids, training=False)\n\n# get tf variables\ntf_vars = model_tf.weights\n\n# match variable names with fuzzy logic\ntorch_var_names = list(state_dict.keys())\ntf_var_names = [we.name for we in model_tf.weights]\nfor tf_name in tf_var_names:\n    # skip re-mapped layer names\n    if tf_name in [name[0] for name in var_map]:\n        continue\n    tf_name_edited = convert_tf_name(tf_name)\n    ratios = [\n        fuzz.ratio(torch_name, tf_name_edited)\n        for torch_name in torch_var_names\n    ]\n    max_idx = np.argmax(ratios)\n    matching_name = torch_var_names[max_idx]\n    del torch_var_names[max_idx]\n    var_map.append((tf_name, matching_name))\n\n# %%\n# print variable match\nfrom pprint import pprint\npprint(var_map)\npprint(torch_var_names)\n\n# pass weights\ntf_vars = transfer_weights_torch_to_tf(tf_vars, dict(var_map), state_dict)\n\n# Compare TF and TORCH models\n# %%\n# check embedding outputs\nmodel.eval()\ninput_ids = torch.randint(0, 24, (1, 128)).long()\n\no_t = model.embedding(input_ids)\no_tf = model_tf.embedding(input_ids.detach().numpy())\nassert abs(o_t.detach().numpy() -\n           o_tf.numpy()).sum() < 1e-5, abs(o_t.detach().numpy() -\n                                           o_tf.numpy()).sum()\n\n# compare encoder outputs\noo_en = model.encoder.inference(o_t.transpose(1, 2))\nooo_en = model_tf.encoder(o_t.detach().numpy(), training=False)\nassert compare_torch_tf(oo_en, ooo_en) < 1e-5\n\n#pylint: disable=redefined-builtin\n# compare decoder.attention_rnn\ninp = torch.rand([1, 768])\ninp_tf = inp.numpy()\nmodel.decoder._init_states(oo_en, mask=None)  #pylint: disable=protected-access\noutput, cell_state = model.decoder.attention_rnn(inp)\nstates = model_tf.decoder.build_decoder_initial_states(1, 512, 128)\noutput_tf, memory_state = model_tf.decoder.attention_rnn(inp_tf,\n                                                         states[2],\n                                                         training=False)\nassert compare_torch_tf(output, output_tf).mean() < 1e-5\n\nquery = output\ninputs = torch.rand([1, 128, 512])\nquery_tf = query.detach().numpy()\ninputs_tf = inputs.numpy()\n\n# compare decoder.attention\nmodel.decoder.attention.init_states(inputs)\nprocesses_inputs = model.decoder.attention.preprocess_inputs(inputs)\nloc_attn, proc_query = model.decoder.attention.get_location_attention(\n    query, processes_inputs)\ncontext = model.decoder.attention(query, inputs, processes_inputs, None)\n\nattention_states = model_tf.decoder.build_decoder_initial_states(1, 512, 128)[-1]\nmodel_tf.decoder.attention.process_values(tf.convert_to_tensor(inputs_tf))\nloc_attn_tf, proc_query_tf = model_tf.decoder.attention.get_loc_attn(query_tf, attention_states)\ncontext_tf, attention, attention_states = model_tf.decoder.attention(query_tf, attention_states, training=False)\n\nassert compare_torch_tf(loc_attn, loc_attn_tf).mean() < 1e-5\nassert compare_torch_tf(proc_query, proc_query_tf).mean() < 1e-5\nassert compare_torch_tf(context, context_tf) < 1e-5\n\n# compare decoder.decoder_rnn\ninput = torch.rand([1, 1536])\ninput_tf = input.numpy()\nmodel.decoder._init_states(oo_en, mask=None)  #pylint: disable=protected-access\noutput, cell_state = model.decoder.decoder_rnn(\n    input, [model.decoder.decoder_hidden, model.decoder.decoder_cell])\nstates = model_tf.decoder.build_decoder_initial_states(1, 512, 128)\noutput_tf, memory_state = model_tf.decoder.decoder_rnn(input_tf,\n                                                       states[3],\n                                                       training=False)\nassert abs(input - input_tf).mean() < 1e-5\nassert compare_torch_tf(output, output_tf).mean() < 1e-5\n\n# compare decoder.linear_projection\ninput = torch.rand([1, 1536])\ninput_tf = input.numpy()\noutput = model.decoder.linear_projection(input)\noutput_tf = model_tf.decoder.linear_projection(input_tf, training=False)\nassert compare_torch_tf(output, output_tf) < 1e-5\n\n# compare decoder outputs\nmodel.decoder.max_decoder_steps = 100\nmodel_tf.decoder.set_max_decoder_steps(100)\noutput, align, stop = model.decoder.inference(oo_en)\nstates = model_tf.decoder.build_decoder_initial_states(1, 512, 128)\noutput_tf, align_tf, stop_tf = model_tf.decoder(ooo_en, states, training=False)\nassert compare_torch_tf(output.transpose(1, 2), output_tf) < 1e-4\n\n# compare the whole model output\noutputs_torch = model.inference(input_ids)\noutputs_tf = model_tf(tf.convert_to_tensor(input_ids.numpy()))\nprint(abs(outputs_torch[0].numpy()[:, 0] - outputs_tf[0].numpy()[:, 0]).mean())\nassert compare_torch_tf(outputs_torch[2][:, 50, :],\n                        outputs_tf[2][:, 50, :]) < 1e-5\nassert compare_torch_tf(outputs_torch[0], outputs_tf[0]) < 1e-4\n\n# %%\n# save tf model\nsave_checkpoint(model_tf, None, checkpoint['step'], checkpoint['epoch'],\n                checkpoint['r'], args.output_path)\nprint(' > Model conversion is successfully completed :).')\n"""
utils/__init__.py,0,b''
utils/audio.py,0,"b'import librosa\nimport soundfile as sf\nimport numpy as np\nimport scipy.io\nimport scipy.signal\n\nfrom TTS.utils.data import StandardScaler\n\n\nclass AudioProcessor(object):\n    def __init__(self,\n                 sample_rate=None,\n                 num_mels=None,\n                 min_level_db=None,\n                 frame_shift_ms=None,\n                 frame_length_ms=None,\n                 hop_length=None,\n                 win_length=None,\n                 ref_level_db=None,\n                 num_freq=None,\n                 power=None,\n                 preemphasis=0.0,\n                 signal_norm=None,\n                 symmetric_norm=None,\n                 max_norm=None,\n                 mel_fmin=None,\n                 mel_fmax=None,\n                 clip_norm=True,\n                 griffin_lim_iters=None,\n                 do_trim_silence=False,\n                 trim_db=60,\n                 do_sound_norm=False,\n                 stats_path=None,\n                 **_):\n\n        print("" > Setting up Audio Processor..."")\n        # setup class attributed\n        self.sample_rate = sample_rate\n        self.num_mels = num_mels\n        self.min_level_db = min_level_db or 0\n        self.frame_shift_ms = frame_shift_ms\n        self.frame_length_ms = frame_length_ms\n        self.ref_level_db = ref_level_db\n        self.num_freq = num_freq\n        self.power = power\n        self.preemphasis = preemphasis\n        self.griffin_lim_iters = griffin_lim_iters\n        self.signal_norm = signal_norm\n        self.symmetric_norm = symmetric_norm\n        self.mel_fmin = mel_fmin or 0\n        self.mel_fmax = mel_fmax\n        self.max_norm = 1.0 if max_norm is None else float(max_norm)\n        self.clip_norm = clip_norm\n        self.do_trim_silence = do_trim_silence\n        self.trim_db = trim_db\n        self.do_sound_norm = do_sound_norm\n        self.stats_path = stats_path\n        # setup stft parameters\n        if hop_length is None:\n            self.n_fft, self.hop_length, self.win_length = self._stft_parameters()\n        else:\n            self.hop_length = hop_length\n            self.win_length = win_length\n            self.n_fft = (self.num_freq - 1) * 2\n        assert min_level_db != 0.0, "" [!] min_level_db is 0""\n        members = vars(self)\n        for key, value in members.items():\n            print("" | > {}:{}"".format(key, value))\n        # create spectrogram utils\n        self.mel_basis = self._build_mel_basis()\n        self.inv_mel_basis = np.linalg.pinv(self._build_mel_basis())\n        # setup scaler\n        if stats_path:\n            mel_mean, mel_std, linear_mean, linear_std, _ = self.load_stats(stats_path)\n            self.setup_scaler(mel_mean, mel_std, linear_mean, linear_std)\n            self.signal_norm = True\n            self.max_norm = None\n            self.clip_norm = None\n            self.symmetric_norm = None\n\n    ### setting up the parameters ###\n    def _build_mel_basis(self, ):\n        if self.mel_fmax is not None:\n            assert self.mel_fmax <= self.sample_rate // 2\n        return librosa.filters.mel(\n            self.sample_rate,\n            self.n_fft,\n            n_mels=self.num_mels,\n            fmin=self.mel_fmin,\n            fmax=self.mel_fmax)\n\n    def _stft_parameters(self, ):\n        """"""Compute necessary stft parameters with given time values""""""\n        n_fft = (self.num_freq - 1) * 2\n        factor = self.frame_length_ms / self.frame_shift_ms\n        assert (factor).is_integer(), "" [!] frame_shift_ms should divide frame_length_ms""\n        hop_length = int(self.frame_shift_ms / 1000.0 * self.sample_rate)\n        win_length = int(hop_length * factor)\n        return n_fft, hop_length, win_length\n\n    ### normalization ###\n    def _normalize(self, S):\n        """"""Put values in [0, self.max_norm] or [-self.max_norm, self.max_norm]""""""\n        #pylint: disable=no-else-return\n        S = S.copy()\n        if self.signal_norm:\n            # mean-var scaling\n            if hasattr(self, \'mel_scaler\'):\n                if S.shape[0] == self.num_mels:\n                    return self.mel_scaler.transform(S.T).T\n                elif S.shape[0] == self.n_fft / 2:\n                    return self.linear_scaler.transform(S.T).T\n                else:\n                    raise RuntimeError(\' [!] Mean-Var stats does not match the given feature dimensions.\')\n            # range normalization\n            S -= self.ref_level_db  # discard certain range of DB assuming it is air noise\n            S_norm = ((S - self.min_level_db) / (-self.min_level_db))\n            if self.symmetric_norm:\n                S_norm = ((2 * self.max_norm) * S_norm) - self.max_norm\n                if self.clip_norm:\n                    S_norm = np.clip(S_norm, -self.max_norm, self.max_norm)\n                return S_norm\n            else:\n                S_norm = self.max_norm * S_norm\n                if self.clip_norm:\n                    S_norm = np.clip(S_norm, 0, self.max_norm)\n                return S_norm\n        else:\n            return S\n\n    def _denormalize(self, S):\n        """"""denormalize values""""""\n        #pylint: disable=no-else-return\n        S_denorm = S.copy()\n        if self.signal_norm:\n            # mean-var scaling\n            if hasattr(self, \'mel_scaler\'):\n                if S_denorm.shape[0] == self.num_mels:\n                    return self.mel_scaler.inverse_transform(S_denorm.T).T\n                elif S_denorm.shape[0] == self.n_fft / 2:\n                    return self.linear_scaler.inverse_transform(S_denorm.T).T\n                else:\n                    raise RuntimeError(\' [!] Mean-Var stats does not match the given feature dimensions.\')\n            if self.symmetric_norm:\n                if self.clip_norm:\n                    S_denorm = np.clip(S_denorm, -self.max_norm, self.max_norm)\n                S_denorm = ((S_denorm + self.max_norm) * -self.min_level_db / (2 * self.max_norm)) + self.min_level_db\n                return S_denorm + self.ref_level_db\n            else:\n                if self.clip_norm:\n                    S_denorm = np.clip(S_denorm, 0, self.max_norm)\n                S_denorm = (S_denorm * -self.min_level_db /\n                            self.max_norm) + self.min_level_db\n                return S_denorm + self.ref_level_db\n        else:\n            return S_denorm\n\n    ### Mean-STD scaling ###\n    def load_stats(self, stats_path):\n        stats = np.load(stats_path, allow_pickle=True).item()  #pylint: disable=unexpected-keyword-arg\n        mel_mean = stats[\'mel_mean\']\n        mel_std = stats[\'mel_std\']\n        linear_mean = stats[\'linear_mean\']\n        linear_std = stats[\'linear_std\']\n        stats_config = stats[\'audio_config\']\n        # check all audio parameters used for computing stats\n        skip_parameters = [\'griffin_lim_iters\', \'stats_path\', \'do_trim_silence\', \'ref_level_db\', \'power\']\n        for key in stats_config.keys():\n            if key in skip_parameters:\n                continue\n            assert stats_config[key] == self.__dict__[key],\\\n                f"" [!] Audio param {key} does not match the value used for computing mean-var stats. {stats_config[key]} vs {self.__dict__[key]}""\n        return mel_mean, mel_std, linear_mean, linear_std, stats_config\n\n    # pylint: disable=attribute-defined-outside-init\n    def setup_scaler(self, mel_mean, mel_std, linear_mean, linear_std):\n        self.mel_scaler = StandardScaler()\n        self.mel_scaler.set_stats(mel_mean, mel_std)\n        self.linear_scaler = StandardScaler()\n        self.linear_scaler.set_stats(linear_mean, linear_std)\n\n    ### DB and AMP conversion ###\n    # pylint: disable=no-self-use\n    def _amp_to_db(self, x):\n        return 20 * np.log10(np.maximum(1e-5, x))\n\n    # pylint: disable=no-self-use\n    def _db_to_amp(self, x):\n        return np.power(10.0, x * 0.05)\n\n    ### Preemphasis ###\n    def apply_preemphasis(self, x):\n        if self.preemphasis == 0:\n            raise RuntimeError("" [!] Preemphasis is set 0.0."")\n        return scipy.signal.lfilter([1, -self.preemphasis], [1], x)\n\n    def apply_inv_preemphasis(self, x):\n        if self.preemphasis == 0:\n            raise RuntimeError("" [!] Preemphasis is set 0.0."")\n        return scipy.signal.lfilter([1], [1, -self.preemphasis], x)\n\n    ### SPECTROGRAMs ###\n    def _linear_to_mel(self, spectrogram):\n        return np.dot(self.mel_basis, spectrogram)\n\n    def _mel_to_linear(self, mel_spec):\n        return np.maximum(1e-10, np.dot(self.inv_mel_basis, mel_spec))\n\n    def spectrogram(self, y):\n        if self.preemphasis != 0:\n            D = self._stft(self.apply_preemphasis(y))\n        else:\n            D = self._stft(y)\n        S = self._amp_to_db(np.abs(D))\n        return self._normalize(S)\n\n    def melspectrogram(self, y):\n        if self.preemphasis != 0:\n            D = self._stft(self.apply_preemphasis(y))\n        else:\n            D = self._stft(y)\n        S = self._amp_to_db(self._linear_to_mel(np.abs(D)))\n        return self._normalize(S)\n\n    def inv_spectrogram(self, spectrogram):\n        """"""Converts spectrogram to waveform using librosa""""""\n        S = self._denormalize(spectrogram)\n        S = self._db_to_amp(S)\n        # Reconstruct phase\n        if self.preemphasis != 0:\n            return self.apply_inv_preemphasis(self._griffin_lim(S**self.power))\n        return self._griffin_lim(S**self.power)\n\n    def inv_melspectrogram(self, mel_spectrogram):\n        \'\'\'Converts melspectrogram to waveform using librosa\'\'\'\n        D = self._denormalize(mel_spectrogram)\n        S = self._db_to_amp(D)\n        S = self._mel_to_linear(S)  # Convert back to linear\n        if self.preemphasis != 0:\n            return self.apply_inv_preemphasis(self._griffin_lim(S**self.power))\n        return self._griffin_lim(S**self.power)\n\n    def out_linear_to_mel(self, linear_spec):\n        S = self._denormalize(linear_spec)\n        S = self._db_to_amp(S)\n        S = self._linear_to_mel(np.abs(S))\n        S = self._amp_to_db(S)\n        mel = self._normalize(S)\n        return mel\n\n    ### STFT and ISTFT ###\n    def _stft(self, y):\n        return librosa.stft(\n            y=y,\n            n_fft=self.n_fft,\n            hop_length=self.hop_length,\n            win_length=self.win_length,\n            pad_mode=\'constant\'\n        )\n\n    def _istft(self, y):\n        return librosa.istft(\n            y, hop_length=self.hop_length, win_length=self.win_length)\n\n    def _griffin_lim(self, S):\n        angles = np.exp(2j * np.pi * np.random.rand(*S.shape))\n        S_complex = np.abs(S).astype(np.complex)\n        y = self._istft(S_complex * angles)\n        for _ in range(self.griffin_lim_iters):\n            angles = np.exp(1j * np.angle(self._stft(y)))\n            y = self._istft(S_complex * angles)\n        return y\n\n    def compute_stft_paddings(self, x, pad_sides=1):\n        \'\'\'compute right padding (final frame) or both sides padding (first and final frames)\n        \'\'\'\n        assert pad_sides in (1, 2)\n        pad = (x.shape[0] // self.hop_length + 1) * self.hop_length - x.shape[0]\n        if pad_sides == 1:\n            return 0, pad\n        return pad // 2, pad // 2 + pad % 2\n\n    ### Audio Processing ###\n    def find_endpoint(self, wav, threshold_db=-40, min_silence_sec=0.8):\n        window_length = int(self.sample_rate * min_silence_sec)\n        hop_length = int(window_length / 4)\n        threshold = self._db_to_amp(threshold_db)\n        for x in range(hop_length, len(wav) - window_length, hop_length):\n            if np.max(wav[x:x + window_length]) < threshold:\n                return x + hop_length\n        return len(wav)\n\n    def trim_silence(self, wav):\n        """""" Trim silent parts with a threshold and 0.01 sec margin """"""\n        margin = int(self.sample_rate * 0.01)\n        wav = wav[margin:-margin]\n        return librosa.effects.trim(\n            wav, top_db=self.trim_db, frame_length=self.win_length, hop_length=self.hop_length)[0]\n\n    @staticmethod\n    def sound_norm(x):\n        return x / abs(x).max() * 0.9\n\n    ### save and load ###\n    def load_wav(self, filename, sr=None):\n        if sr is None:\n            x, sr = sf.read(filename)\n        else:\n            x, sr = librosa.load(filename, sr=sr)\n        if self.do_trim_silence:\n            try:\n                x = self.trim_silence(x)\n            except ValueError:\n                print(f\' [!] File cannot be trimmed for silence - {filename}\')\n        assert self.sample_rate == sr, ""%s vs %s""%(self.sample_rate, sr)\n        if self.do_sound_norm:\n            x = self.sound_norm(x)\n        return x\n\n    def save_wav(self, wav, path):\n        wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n        scipy.io.wavfile.write(path, self.sample_rate, wav_norm.astype(np.int16))\n\n    @staticmethod\n    def mulaw_encode(wav, qc):\n        mu = 2 ** qc - 1\n        # wav_abs = np.minimum(np.abs(wav), 1.0)\n        signal = np.sign(wav) * np.log(1 + mu * np.abs(wav)) / np.log(1. + mu)\n        # Quantize signal to the specified number of levels.\n        signal = (signal + 1) / 2 * mu + 0.5\n        return np.floor(signal,)\n\n    @staticmethod\n    def mulaw_decode(wav, qc):\n        """"""Recovers waveform from quantized values.""""""\n        mu = 2 ** qc - 1\n        x = np.sign(wav) / mu * ((1 + mu) ** np.abs(wav) - 1)\n        return x\n\n\n    @staticmethod\n    def encode_16bits(x):\n        return np.clip(x * 2**15, -2**15, 2**15 - 1).astype(np.int16)\n\n    @staticmethod\n    def quantize(x, bits):\n        return (x + 1.) * (2**bits - 1) / 2\n\n    @staticmethod\n    def dequantize(x, bits):\n        return 2 * x / (2**bits - 1) - 1\n'"
utils/console_logger.py,0,"b'import datetime\nfrom TTS.utils.io import AttrDict\n\n\ntcolors = AttrDict({\n    \'OKBLUE\': \'\\033[94m\',\n    \'HEADER\': \'\\033[95m\',\n    \'OKGREEN\': \'\\033[92m\',\n    \'WARNING\': \'\\033[93m\',\n    \'FAIL\': \'\\033[91m\',\n    \'ENDC\': \'\\033[0m\',\n    \'BOLD\': \'\\033[1m\',\n    \'UNDERLINE\': \'\\033[4m\'\n})\n\n\nclass ConsoleLogger():\n    def __init__(self):\n        # TODO: color code for value changes\n        # use these to compare values between iterations\n        self.old_train_loss_dict = None\n        self.old_epoch_loss_dict = None\n        self.old_eval_loss_dict = None\n\n    # pylint: disable=no-self-use\n    def get_time(self):\n        now = datetime.datetime.now()\n        return now.strftime(""%Y-%m-%d %H:%M:%S"")\n\n    def print_epoch_start(self, epoch, max_epoch):\n        print(""\\n{}{} > EPOCH: {}/{}{}"".format(tcolors.UNDERLINE, tcolors.BOLD,\n                                               epoch, max_epoch, tcolors.ENDC),\n              flush=True)\n\n    def print_train_start(self):\n        print(f""\\n{tcolors.BOLD} > TRAINING ({self.get_time()}) {tcolors.ENDC}"")\n\n    def print_train_step(self, batch_steps, step, global_step, avg_spec_length,\n                         avg_text_length, step_time, loader_time, lr,\n                         loss_dict, avg_loss_dict):\n        indent = ""     | > ""\n        print()\n        log_text = ""{}   --> STEP: {}/{} -- GLOBAL_STEP: {}{}\\n"".format(\n            tcolors.BOLD, step, batch_steps, global_step, tcolors.ENDC)\n        for key, value in loss_dict.items():\n            # print the avg value if given\n            if f\'avg_{key}\' in avg_loss_dict.keys():\n                log_text += ""{}{}: {:.5f}  ({:.5f})\\n"".format(indent, key, value, avg_loss_dict[f\'avg_{key}\'])\n            else:\n                log_text += ""{}{}: {:.5f} \\n"".format(indent, key, value)\n        log_text += f""{indent}avg_spec_len: {avg_spec_length}\\n{indent}avg_text_len: {avg_text_length}\\n{indent}""\\\n            f""step_time: {step_time:.2f}\\n{indent}loader_time: {loader_time:.2f}\\n{indent}lr: {lr:.5f}""\n        print(log_text, flush=True)\n\n    # pylint: disable=unused-argument\n    def print_train_epoch_end(self, global_step, epoch, epoch_time,\n                              print_dict):\n        indent = ""     | > ""\n        log_text = f""\\n{tcolors.BOLD}   --> TRAIN PERFORMACE -- EPOCH TIME: {epoch} sec -- GLOBAL_STEP: {global_step}{tcolors.ENDC}\\n""\n        for key, value in print_dict.items():\n            log_text += ""{}{}: {:.5f}\\n"".format(indent, key, value)\n        print(log_text, flush=True)\n\n    def print_eval_start(self):\n        print(f""{tcolors.BOLD} > EVALUATION {tcolors.ENDC}\\n"")\n\n    def print_eval_step(self, step, loss_dict, avg_loss_dict):\n        indent = ""     | > ""\n        print()\n        log_text = f""{tcolors.BOLD}   --> STEP: {step}{tcolors.ENDC}\\n""\n        for key, value in loss_dict.items():\n            # print the avg value if given\n            if f\'avg_{key}\' in avg_loss_dict.keys():\n                log_text += ""{}{}: {:.5f}  ({:.5f})\\n"".format(indent, key, value, avg_loss_dict[f\'avg_{key}\'])\n            else:\n                log_text += ""{}{}: {:.5f} \\n"".format(indent, key, value)\n        print(log_text, flush=True)\n\n    def print_epoch_end(self, epoch, avg_loss_dict):\n        indent = ""     | > ""\n        log_text = ""  {}--> EVAL PERFORMANCE{}\\n"".format(\n            tcolors.BOLD, tcolors.ENDC)\n        for key, value in avg_loss_dict.items():\n            # print the avg value if given\n            color = tcolors.FAIL\n            sign = \'+\'\n            diff = 0\n            if self.old_eval_loss_dict is not None:\n                diff = value - self.old_eval_loss_dict[key]\n                if diff <= 0:\n                    color = tcolors.OKGREEN\n                    sign = \'\'\n            log_text += ""{}{}:{} {:.5f} {}({}{:.5f})\\n"".format(indent, key, color, value, tcolors.ENDC, sign, diff)\n        self.old_eval_loss_dict = avg_loss_dict\n        print(log_text, flush=True)\n'"
utils/data.py,0,"b'import numpy as np\n\n\ndef _pad_data(x, length):\n    _pad = 0\n    assert x.ndim == 1\n    return np.pad(\n        x, (0, length - x.shape[0]), mode=\'constant\', constant_values=_pad)\n\n\ndef prepare_data(inputs):\n    max_len = max((len(x) for x in inputs))\n    return np.stack([_pad_data(x, max_len) for x in inputs])\n\n\ndef _pad_tensor(x, length):\n    _pad = 0.\n    assert x.ndim == 2\n    x = np.pad(\n        x, [[0, 0], [0, length - x.shape[1]]],\n        mode=\'constant\',\n        constant_values=_pad)\n    return x\n\n\ndef prepare_tensor(inputs, out_steps):\n    max_len = max((x.shape[1] for x in inputs))\n    remainder = max_len % out_steps\n    pad_len = max_len + (out_steps - remainder) if remainder > 0 else max_len\n    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\n\n\ndef _pad_stop_target(x, length):\n    _pad = 0.\n    assert x.ndim == 1\n    return np.pad(\n        x, (0, length - x.shape[0]), mode=\'constant\', constant_values=_pad)\n\n\ndef prepare_stop_target(inputs, out_steps):\n    """""" Pad row vectors with 1. """"""\n    max_len = max((x.shape[0] for x in inputs))\n    remainder = max_len % out_steps\n    pad_len = max_len + (out_steps - remainder) if remainder > 0 else max_len\n    return np.stack([_pad_stop_target(x, pad_len) for x in inputs])\n\n\ndef pad_per_step(inputs, pad_len):\n    return np.pad(\n        inputs, [[0, 0], [0, 0], [0, pad_len]],\n        mode=\'constant\',\n        constant_values=0.0)\n\n\n# pylint: disable=attribute-defined-outside-init\nclass StandardScaler():\n\n    def set_stats(self, mean, scale):\n        self.mean_ = mean\n        self.scale_ = scale\n\n    def reset_stats(self):\n        delattr(self, \'mean_\')\n        delattr(self, \'scale_\')\n\n    def transform(self, X):\n        X = np.asarray(X)\n        X -= self.mean_\n        X /= self.scale_\n        return X\n\n    def inverse_transform(self, X):\n        X = np.asarray(X)\n        X *= self.scale_\n        X += self.mean_\n        return X\n\n'"
utils/generic_utils.py,1,"b'import os\nimport glob\nimport torch\nimport shutil\nimport datetime\nimport subprocess\nimport importlib\nimport numpy as np\nfrom collections import Counter\n\n\ndef get_git_branch():\n    try:\n        out = subprocess.check_output([""git"", ""branch""]).decode(""utf8"")\n        current = next(line for line in out.split(""\\n"")\n                       if line.startswith(""*""))\n        current.replace(""* "", """")\n    except subprocess.CalledProcessError:\n        current = ""inside_docker""\n    return current\n\n\ndef get_commit_hash():\n    """"""https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script""""""\n    # try:\n    #     subprocess.check_output([\'git\', \'diff-index\', \'--quiet\',\n    #                              \'HEAD\'])  # Verify client is clean\n    # except:\n    #     raise RuntimeError(\n    #         "" !! Commit before training to get the commit hash."")\n    try:\n        commit = subprocess.check_output(\n            [\'git\', \'rev-parse\', \'--short\', \'HEAD\']).decode().strip()\n    # Not copying .git folder into docker container\n    except subprocess.CalledProcessError:\n        commit = ""0000000""\n    print(\' > Git Hash: {}\'.format(commit))\n    return commit\n\n\ndef create_experiment_folder(root_path, model_name, debug):\n    """""" Create a folder with the current date and time """"""\n    date_str = datetime.datetime.now().strftime(""%B-%d-%Y_%I+%M%p"")\n    if debug:\n        commit_hash = \'debug\'\n    else:\n        commit_hash = get_commit_hash()\n    output_folder = os.path.join(\n        root_path, model_name + \'-\' + date_str + \'-\' + commit_hash)\n    os.makedirs(output_folder, exist_ok=True)\n    print("" > Experiment folder: {}"".format(output_folder))\n    return output_folder\n\n\ndef remove_experiment_folder(experiment_path):\n    """"""Check folder if there is a checkpoint, otherwise remove the folder""""""\n\n    checkpoint_files = glob.glob(experiment_path + ""/*.pth.tar"")\n    if not checkpoint_files:\n        if os.path.exists(experiment_path):\n            shutil.rmtree(experiment_path, ignore_errors=True)\n            print("" ! Run is removed from {}"".format(experiment_path))\n    else:\n        print("" ! Run is kept in {}"".format(experiment_path))\n\n\ndef count_parameters(model):\n    r""""""Count number of trainable parameters in a network""""""\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\ndef split_dataset(items):\n    is_multi_speaker = False\n    speakers = [item[-1] for item in items]\n    is_multi_speaker = len(set(speakers)) > 1\n    eval_split_size = 500 if len(items) * 0.01 > 500 else int(\n        len(items) * 0.01)\n    assert len(eval_split_size) > 0, "" [!] You do not have enough samples to train. You need at least 100 samples.""\n    np.random.seed(0)\n    np.random.shuffle(items)\n    if is_multi_speaker:\n        items_eval = []\n        # most stupid code ever -- Fix it !\n        while len(items_eval) < eval_split_size:\n            speakers = [item[-1] for item in items]\n            speaker_counter = Counter(speakers)\n            item_idx = np.random.randint(0, len(items))\n            if speaker_counter[items[item_idx][-1]] > 1:\n                items_eval.append(items[item_idx])\n                del items[item_idx]\n        return items_eval, items\n    return items[:eval_split_size], items[eval_split_size:]\n\n\n# from https://gist.github.com/jihunchoi/f1434a77df9db1bb337417854b398df1\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.data.max()\n    batch_size = sequence_length.size(0)\n    seq_range = torch.arange(0, max_len).long()\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.to(sequence_length.device)\n    seq_length_expand = (\n        sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n    # B x T_max\n    return seq_range_expand < seq_length_expand\n\n\ndef set_init_dict(model_dict, checkpoint, c):\n    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n    for k, v in checkpoint[\'model\'].items():\n        if k not in model_dict:\n            print("" | > Layer missing in the model definition: {}"".format(k))\n    # 1. filter out unnecessary keys\n    pretrained_dict = {\n        k: v\n        for k, v in checkpoint[\'model\'].items() if k in model_dict\n    }\n    # 2. filter out different size layers\n    pretrained_dict = {\n        k: v\n        for k, v in pretrained_dict.items()\n        if v.numel() == model_dict[k].numel()\n    }\n    # 3. skip reinit layers\n    if c.reinit_layers is not None:\n        for reinit_layer_name in c.reinit_layers:\n            pretrained_dict = {\n                k: v\n                for k, v in pretrained_dict.items()\n                if reinit_layer_name not in k\n            }\n    # 4. overwrite entries in the existing state dict\n    model_dict.update(pretrained_dict)\n    print("" | > {} / {} layers are restored."".format(len(pretrained_dict),\n                                                     len(model_dict)))\n    return model_dict\n\n\ndef setup_model(num_chars, num_speakers, c):\n    print("" > Using model: {}"".format(c.model))\n    MyModel = importlib.import_module(\'TTS.models.\' + c.model.lower())\n    MyModel = getattr(MyModel, c.model)\n    if c.model.lower() in ""tacotron"":\n        model = MyModel(num_chars=num_chars,\n                        num_speakers=num_speakers,\n                        r=c.r,\n                        postnet_output_dim=c.audio[\'num_freq\'],\n                        decoder_output_dim=c.audio[\'num_mels\'],\n                        gst=c.use_gst,\n                        memory_size=c.memory_size,\n                        attn_type=c.attention_type,\n                        attn_win=c.windowing,\n                        attn_norm=c.attention_norm,\n                        prenet_type=c.prenet_type,\n                        prenet_dropout=c.prenet_dropout,\n                        forward_attn=c.use_forward_attn,\n                        trans_agent=c.transition_agent,\n                        forward_attn_mask=c.forward_attn_mask,\n                        location_attn=c.location_attn,\n                        attn_K=c.attention_heads,\n                        separate_stopnet=c.separate_stopnet,\n                        bidirectional_decoder=c.bidirectional_decoder)\n    elif c.model.lower() == ""tacotron2"":\n        model = MyModel(num_chars=num_chars,\n                        num_speakers=num_speakers,\n                        r=c.r,\n                        postnet_output_dim=c.audio[\'num_mels\'],\n                        decoder_output_dim=c.audio[\'num_mels\'],\n                        attn_type=c.attention_type,\n                        attn_win=c.windowing,\n                        attn_norm=c.attention_norm,\n                        prenet_type=c.prenet_type,\n                        prenet_dropout=c.prenet_dropout,\n                        forward_attn=c.use_forward_attn,\n                        trans_agent=c.transition_agent,\n                        forward_attn_mask=c.forward_attn_mask,\n                        location_attn=c.location_attn,\n                        attn_K=c.attention_heads,\n                        separate_stopnet=c.separate_stopnet,\n                        bidirectional_decoder=c.bidirectional_decoder)\n    return model\n\nclass KeepAverage():\n    def __init__(self):\n        self.avg_values = {}\n        self.iters = {}\n\n    def __getitem__(self, key):\n        return self.avg_values[key]\n\n    def items(self):\n        return self.avg_values.items()\n\n    def add_value(self, name, init_val=0, init_iter=0):\n        self.avg_values[name] = init_val\n        self.iters[name] = init_iter\n\n    def update_value(self, name, value, weighted_avg=False):\n        if weighted_avg:\n            self.avg_values[name] = 0.99 * self.avg_values[name] + 0.01 * value\n            self.iters[name] += 1\n        else:\n            self.avg_values[name] = self.avg_values[name] * \\\n                self.iters[name] + value\n            self.iters[name] += 1\n            self.avg_values[name] /= self.iters[name]\n\n    def add_values(self, name_dict):\n        for key, value in name_dict.items():\n            self.add_value(key, init_val=value)\n\n    def update_values(self, value_dict):\n        for key, value in value_dict.items():\n            self.update_value(key, value)\n\n\ndef _check_argument(name, c, enum_list=None, max_val=None, min_val=None, restricted=False, val_type=None, alternative=None):\n    if alternative in c.keys() and c[alternative] is not None:\n        return\n    if restricted:\n        assert name in c.keys(), f\' [!] {name} not defined in config.json\'\n    if name in c.keys():\n        if max_val:\n            assert c[name] <= max_val, f\' [!] {name} is larger than max value {max_val}\'\n        if min_val:\n            assert c[name] >= min_val, f\' [!] {name} is smaller than min value {min_val}\'\n        if enum_list:\n            assert c[name].lower() in enum_list, f\' [!] {name} is not a valid value\'\n        if val_type:\n            assert isinstance(c[name], val_type) or c[name] is None, f\' [!] {name} has wrong type - {type(c[name])} vs {val_type}\'\n\n\ndef check_config(c):\n    _check_argument(\'model\', c, enum_list=[\'tacotron\', \'tacotron2\'], restricted=True, val_type=str)\n    _check_argument(\'run_name\', c, restricted=True, val_type=str)\n    _check_argument(\'run_description\', c, val_type=str)\n\n    # AUDIO\n    _check_argument(\'audio\', c, restricted=True, val_type=dict)\n\n    # audio processing parameters\n    _check_argument(\'num_mels\', c[\'audio\'], restricted=True, val_type=int, min_val=10, max_val=2056)\n    _check_argument(\'num_freq\', c[\'audio\'], restricted=True, val_type=int, min_val=128, max_val=4058)\n    _check_argument(\'sample_rate\', c[\'audio\'], restricted=True, val_type=int, min_val=512, max_val=100000)\n    _check_argument(\'frame_length_ms\', c[\'audio\'], restricted=True, val_type=float, min_val=10, max_val=1000, alternative=\'win_length\')\n    _check_argument(\'frame_shift_ms\', c[\'audio\'], restricted=True, val_type=float, min_val=1, max_val=1000, alternative=\'hop_length\')\n    _check_argument(\'preemphasis\', c[\'audio\'], restricted=True, val_type=float, min_val=0, max_val=1)\n    _check_argument(\'min_level_db\', c[\'audio\'], restricted=True, val_type=int, min_val=-1000, max_val=10)\n    _check_argument(\'ref_level_db\', c[\'audio\'], restricted=True, val_type=int, min_val=0, max_val=1000)\n    _check_argument(\'power\', c[\'audio\'], restricted=True, val_type=float, min_val=1, max_val=5)\n    _check_argument(\'griffin_lim_iters\', c[\'audio\'], restricted=True, val_type=int, min_val=10, max_val=1000)\n\n    # vocabulary parameters\n    _check_argument(\'characters\', c, restricted=False, val_type=dict)\n    _check_argument(\'pad\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n    _check_argument(\'eos\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n    _check_argument(\'bos\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n    _check_argument(\'characters\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n    _check_argument(\'phonemes\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n    _check_argument(\'punctuations\', c[\'characters\'] if \'characters\' in c.keys() else {}, restricted=\'characters\' in c.keys(), val_type=str)\n\n    # normalization parameters\n    _check_argument(\'signal_norm\', c[\'audio\'], restricted=True, val_type=bool)\n    _check_argument(\'symmetric_norm\', c[\'audio\'], restricted=True, val_type=bool)\n    _check_argument(\'max_norm\', c[\'audio\'], restricted=True, val_type=float, min_val=0.1, max_val=1000)\n    _check_argument(\'clip_norm\', c[\'audio\'], restricted=True, val_type=bool)\n    _check_argument(\'mel_fmin\', c[\'audio\'], restricted=True, val_type=float, min_val=0.0, max_val=1000)\n    _check_argument(\'mel_fmax\', c[\'audio\'], restricted=True, val_type=float, min_val=500.0)\n    _check_argument(\'do_trim_silence\', c[\'audio\'], restricted=True, val_type=bool)\n    _check_argument(\'trim_db\', c[\'audio\'], restricted=True, val_type=int)\n\n    # training parameters\n    _check_argument(\'batch_size\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'eval_batch_size\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'r\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'gradual_training\', c, restricted=False, val_type=list)\n    _check_argument(\'loss_masking\', c, restricted=True, val_type=bool)\n    # _check_argument(\'grad_accum\', c, restricted=True, val_type=int, min_val=1, max_val=100)\n\n    # validation parameters\n    _check_argument(\'run_eval\', c, restricted=True, val_type=bool)\n    _check_argument(\'test_delay_epochs\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'test_sentences_file\', c, restricted=False, val_type=str)\n\n    # optimizer\n    _check_argument(\'noam_schedule\', c, restricted=False, val_type=bool)\n    _check_argument(\'grad_clip\', c, restricted=True, val_type=float, min_val=0.0)\n    _check_argument(\'epochs\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'lr\', c, restricted=True, val_type=float, min_val=0)\n    _check_argument(\'wd\', c, restricted=True, val_type=float, min_val=0)\n    _check_argument(\'warmup_steps\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'seq_len_norm\', c, restricted=True, val_type=bool)\n\n    # tacotron prenet\n    _check_argument(\'memory_size\', c, restricted=True, val_type=int, min_val=-1)\n    _check_argument(\'prenet_type\', c, restricted=True, val_type=str, enum_list=[\'original\', \'bn\'])\n    _check_argument(\'prenet_dropout\', c, restricted=True, val_type=bool)\n\n    # attention\n    _check_argument(\'attention_type\', c, restricted=True, val_type=str, enum_list=[\'graves\', \'original\'])\n    _check_argument(\'attention_heads\', c, restricted=True, val_type=int)\n    _check_argument(\'attention_norm\', c, restricted=True, val_type=str, enum_list=[\'sigmoid\', \'softmax\'])\n    _check_argument(\'windowing\', c, restricted=True, val_type=bool)\n    _check_argument(\'use_forward_attn\', c, restricted=True, val_type=bool)\n    _check_argument(\'forward_attn_mask\', c, restricted=True, val_type=bool)\n    _check_argument(\'transition_agent\', c, restricted=True, val_type=bool)\n    _check_argument(\'transition_agent\', c, restricted=True, val_type=bool)\n    _check_argument(\'location_attn\', c, restricted=True, val_type=bool)\n    _check_argument(\'bidirectional_decoder\', c, restricted=True, val_type=bool)\n\n    # stopnet\n    _check_argument(\'stopnet\', c, restricted=True, val_type=bool)\n    _check_argument(\'separate_stopnet\', c, restricted=True, val_type=bool)\n\n    # tensorboard\n    _check_argument(\'print_step\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'save_step\', c, restricted=True, val_type=int, min_val=1)\n    _check_argument(\'checkpoint\', c, restricted=True, val_type=bool)\n    _check_argument(\'tb_model_param_stats\', c, restricted=True, val_type=bool)\n\n    # dataloading\n    # pylint: disable=import-outside-toplevel\n    from TTS.utils.text import cleaners\n    _check_argument(\'text_cleaner\', c, restricted=True, val_type=str, enum_list=dir(cleaners))\n    _check_argument(\'enable_eos_bos_chars\', c, restricted=True, val_type=bool)\n    _check_argument(\'num_loader_workers\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'num_val_loader_workers\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'batch_group_size\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'min_seq_len\', c, restricted=True, val_type=int, min_val=0)\n    _check_argument(\'max_seq_len\', c, restricted=True, val_type=int, min_val=10)\n\n    # paths\n    _check_argument(\'output_path\', c, restricted=True, val_type=str)\n\n    # multi-speaker gst\n    _check_argument(\'use_speaker_embedding\', c, restricted=True, val_type=bool)\n    _check_argument(\'style_wav_for_test\', c, restricted=True, val_type=str)\n    _check_argument(\'use_gst\', c, restricted=True, val_type=bool)\n\n    # datasets - checking only the first entry\n    _check_argument(\'datasets\', c, restricted=True, val_type=list)\n    for dataset_entry in c[\'datasets\']:\n        _check_argument(\'name\', dataset_entry, restricted=True, val_type=str)\n        _check_argument(\'path\', dataset_entry, restricted=True, val_type=str)\n        _check_argument(\'meta_file_train\', dataset_entry, restricted=True, val_type=str)\n        _check_argument(\'meta_file_val\', dataset_entry, restricted=True, val_type=str)\n'"
utils/io.py,2,"b'import os\nimport json\nimport re\nimport torch\nimport datetime\n\n\nclass AttrDict(dict):\n    def __init__(self, *args, **kwargs):\n        super(AttrDict, self).__init__(*args, **kwargs)\n        self.__dict__ = self\n\n\ndef load_config(config_path):\n    config = AttrDict()\n    with open(config_path, ""r"") as f:\n        input_str = f.read()\n    input_str = re.sub(r\'\\\\\\n\', \'\', input_str)\n    input_str = re.sub(r\'//.*\\n\', \'\\n\', input_str)\n    data = json.loads(input_str)\n    config.update(data)\n    return config\n\n\ndef copy_config_file(config_file, out_path, new_fields):\n    config_lines = open(config_file, ""r"").readlines()\n    # add extra information fields\n    for key, value in new_fields.items():\n        if isinstance(value, str):\n            new_line = \'""{}"":""{}"",\\n\'.format(key, value)\n        else:\n            new_line = \'""{}"":{},\\n\'.format(key, value)\n        config_lines.insert(1, new_line)\n    config_out_file = open(out_path, ""w"")\n    config_out_file.writelines(config_lines)\n    config_out_file.close()\n\n\ndef load_checkpoint(model, checkpoint_path, use_cuda=False):\n    state = torch.load(checkpoint_path, map_location=torch.device(\'cpu\'))\n    model.load_state_dict(state[\'model\'])\n    if use_cuda:\n        model.cuda()\n    # set model stepsize\n    if \'r\' in state.keys():\n        model.decoder.set_r(state[\'r\'])\n    return model, state\n\n\ndef save_model(model, optimizer, current_step, epoch, r, output_path, **kwargs):\n    new_state_dict = model.state_dict()\n    state = {\n        \'model\': new_state_dict,\n        \'optimizer\': optimizer.state_dict() if optimizer is not None else None,\n        \'step\': current_step,\n        \'epoch\': epoch,\n        \'date\': datetime.date.today().strftime(""%B %d, %Y""),\n        \'r\': r\n    }\n    state.update(kwargs)\n    torch.save(state, output_path)\n\n\ndef save_checkpoint(model, optimizer, current_step, epoch, r, output_folder, **kwargs):\n    file_name = \'checkpoint_{}.pth.tar\'.format(current_step)\n    checkpoint_path = os.path.join(output_folder, file_name)\n    print("" > CHECKPOINT : {}"".format(checkpoint_path))\n    save_model(model, optimizer, current_step, epoch, r, checkpoint_path, **kwargs)\n\n\ndef save_best_model(target_loss, best_loss, model, optimizer, current_step, epoch, r, output_folder, **kwargs):\n    if target_loss < best_loss:\n        file_name = \'best_model.pth.tar\'\n        checkpoint_path = os.path.join(output_folder, file_name)\n        print("" > BEST MODEL : {}"".format(checkpoint_path))\n        save_model(model, optimizer, current_step, epoch, r, checkpoint_path, model_loss=target_loss, **kwargs)\n        best_loss = target_loss\n    return best_loss\n'"
utils/measures.py,1,"b'import torch\n\n\ndef alignment_diagonal_score(alignments, binary=False):\n    """"""\n    Compute how diagonal alignment predictions are. It is useful\n    to measure the alignment consistency of a model\n    Args:\n        alignments (torch.Tensor): batch of alignments.\n        binary (bool): if True, ignore scores and consider attention\n        as a binary mask.\n    Shape:\n        alignments : batch x decoder_steps x encoder_steps\n    """"""\n    maxs = alignments.max(dim=1)[0]\n    if binary:\n        maxs[maxs > 0] = 1\n    return maxs.mean(dim=1).mean(dim=0).item()\n'"
utils/radam.py,3,"b'# from https://github.com/LiyuanLucasLiu/RAdam\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if lr < 0.0:\n            raise ValueError(""Invalid learning rate: {}"".format(lr))\n        if eps < 0.0:\n            raise ValueError(""Invalid epsilon value: {}"".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 0: {}"".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(""Invalid beta parameter at index 1: {}"".format(betas[1]))\n\n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if \'betas\' in param and (param[\'betas\'][0] != betas[0] or param[\'betas\'][1] != betas[1]):\n                    param[\'buffer\'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group[\'params\']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError(\'RAdam does not support sparse gradients\')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state[\'step\'] = 0\n                    state[\'exp_avg\'] = torch.zeros_like(p_data_fp32)\n                    state[\'exp_avg_sq\'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state[\'exp_avg\'] = state[\'exp_avg\'].type_as(p_data_fp32)\n                    state[\'exp_avg_sq\'] = state[\'exp_avg_sq\'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state[\'exp_avg\'], state[\'exp_avg_sq\']\n                beta1, beta2 = group[\'betas\']\n\n                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n\n                state[\'step\'] += 1\n                buffered = group[\'buffer\'][int(state[\'step\'] % 10)]\n                if state[\'step\'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state[\'step\']\n                    beta2_t = beta2 ** state[\'step\']\n                    N_sma_max = 2 / (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state[\'step\'] * beta2_t / (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it\'s an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state[\'step\'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 / (1 - beta1 ** state[\'step\'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it\'s an approximated value\n                if N_sma >= 5:\n                    if group[\'weight_decay\'] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\'weight_decay\'] * group[\'lr\'])\n                    denom = exp_avg_sq.sqrt().add_(group[\'eps\'])\n                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size * group[\'lr\'])\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group[\'weight_decay\'] != 0:\n                        p_data_fp32.add_(p_data_fp32, alpha=-group[\'weight_decay\'] * group[\'lr\'])\n                    p_data_fp32.add_(exp_avg, alpha=-step_size * group[\'lr\'])\n                    p.data.copy_(p_data_fp32)\n\n        return loss\n'"
utils/speakers.py,0,"b'import os\nimport json\n\nfrom TTS.datasets.preprocess import get_preprocessor_by_name\n\n\ndef make_speakers_json_path(out_path):\n    """"""Returns conventional speakers.json location.""""""\n    return os.path.join(out_path, ""speakers.json"")\n\n\ndef load_speaker_mapping(out_path):\n    """"""Loads speaker mapping if already present.""""""\n    try:\n        with open(make_speakers_json_path(out_path)) as f:\n            return json.load(f)\n    except FileNotFoundError:\n        return {}\n\n\ndef save_speaker_mapping(out_path, speaker_mapping):\n    """"""Saves speaker mapping if not yet present.""""""\n    speakers_json_path = make_speakers_json_path(out_path)\n    with open(speakers_json_path, ""w"") as f:\n        json.dump(speaker_mapping, f, indent=4)\n\n\ndef get_speakers(items):\n    """"""Returns a sorted, unique list of speakers in a given dataset.""""""\n    speakers = {e[2] for e in items}\n    return sorted(speakers)\n'"
utils/synthesis.py,4,"b'import pkg_resources\ninstalled = {pkg.key for pkg in pkg_resources.working_set}  #pylint: disable=not-an-iterable\nif \'tensorflow\' in installed or \'tensorflow-gpu\' in installed:\n    import tensorflow as tf\nimport torch\nimport numpy as np\nfrom .text import text_to_sequence, phoneme_to_sequence\n\n\ndef text_to_seqvec(text, CONFIG):\n    text_cleaner = [CONFIG.text_cleaner]\n    # text ot phonemes to sequence vector\n    if CONFIG.use_phonemes:\n        seq = np.asarray(\n            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n                                CONFIG.enable_eos_bos_chars,\n                                tp=CONFIG.characters if \'characters\' in CONFIG.keys() else None),\n            dtype=np.int32)\n    else:\n        seq = np.asarray(text_to_sequence(text, text_cleaner, tp=CONFIG.characters if \'characters\' in CONFIG.keys() else None), dtype=np.int32)\n    return seq\n\n\ndef numpy_to_torch(np_array, dtype, cuda=False):\n    if np_array is None:\n        return None\n    tensor = torch.as_tensor(np_array, dtype=dtype)\n    if cuda:\n        return tensor.cuda()\n    return tensor\n\n\ndef numpy_to_tf(np_array, dtype):\n    if np_array is None:\n        return None\n    tensor = tf.convert_to_tensor(np_array, dtype=dtype)\n    return tensor\n\n\ndef compute_style_mel(style_wav, ap):\n    style_mel = ap.melspectrogram(\n        ap.load_wav(style_wav)).expand_dims(0)\n    return style_mel\n\n\ndef run_model_torch(model, inputs, CONFIG, truncated, speaker_id=None, style_mel=None):\n    if CONFIG.use_gst:\n        decoder_output, postnet_output, alignments, stop_tokens = model.inference(\n            inputs, style_mel=style_mel, speaker_ids=speaker_id)\n    else:\n        if truncated:\n            decoder_output, postnet_output, alignments, stop_tokens = model.inference_truncated(\n                inputs, speaker_ids=speaker_id)\n        else:\n            decoder_output, postnet_output, alignments, stop_tokens = model.inference(\n                inputs, speaker_ids=speaker_id)\n    return decoder_output, postnet_output, alignments, stop_tokens\n\n\ndef run_model_tf(model, inputs, CONFIG, truncated, speaker_id=None, style_mel=None):\n    if CONFIG.use_gst and style_mel is not None:\n        raise NotImplementedError(\' [!] GST inference not implemented for TF\')\n    if truncated:\n        raise NotImplementedError(\' [!] Truncated inference not implemented for TF\')\n    if speaker_id is not None:\n        raise NotImplementedError(\' [!] Multi-Speaker not implemented for TF\')\n    # TODO: handle multispeaker case\n    decoder_output, postnet_output, alignments, stop_tokens = model(\n        inputs, training=False)\n    return decoder_output, postnet_output, alignments, stop_tokens\n\n\ndef parse_outputs_torch(postnet_output, decoder_output, alignments, stop_tokens):\n    postnet_output = postnet_output[0].data.cpu().numpy()\n    decoder_output = decoder_output[0].data.cpu().numpy()\n    alignment = alignments[0].cpu().data.numpy()\n    stop_tokens = stop_tokens[0].cpu().numpy()\n    return postnet_output, decoder_output, alignment, stop_tokens\n\n\ndef parse_outputs_tf(postnet_output, decoder_output, alignments, stop_tokens):\n    postnet_output = postnet_output[0].numpy()\n    decoder_output = decoder_output[0].numpy()\n    alignment = alignments[0].numpy()\n    stop_tokens = stop_tokens[0].numpy()\n    return postnet_output, decoder_output, alignment, stop_tokens\n\n\ndef trim_silence(wav, ap):\n    return wav[:ap.find_endpoint(wav)]\n\n\ndef inv_spectrogram(postnet_output, ap, CONFIG):\n    if CONFIG.model in [""Tacotron"", ""TacotronGST""]:\n        wav = ap.inv_spectrogram(postnet_output.T)\n    else:\n        wav = ap.inv_melspectrogram(postnet_output.T)\n    return wav\n\n\ndef id_to_torch(speaker_id):\n    if speaker_id is not None:\n        speaker_id = np.asarray(speaker_id)\n        speaker_id = torch.from_numpy(speaker_id).unsqueeze(0)\n    return speaker_id\n\n\n# TODO: perform GL with pytorch for batching\ndef apply_griffin_lim(inputs, input_lens, CONFIG, ap):\n    \'\'\'Apply griffin-lim to each sample iterating throught the first dimension.\n    Args:\n        inputs (Tensor or np.Array): Features to be converted by GL. First dimension is the batch size.\n        input_lens (Tensor or np.Array): 1D array of sample lengths.\n        CONFIG (Dict): TTS config.\n        ap (AudioProcessor): TTS audio processor.\n    \'\'\'\n    wavs = []\n    for idx, spec in enumerate(inputs):\n        wav_len = (input_lens[idx] * ap.hop_length) - ap.hop_length  # inverse librosa padding\n        wav = inv_spectrogram(spec, ap, CONFIG)\n        # assert len(wav) == wav_len, f"" [!] wav lenght: {len(wav)} vs expected: {wav_len}""\n        wavs.append(wav[:wav_len])\n    return wavs\n\n\ndef synthesis(model,\n              text,\n              CONFIG,\n              use_cuda,\n              ap,\n              speaker_id=None,\n              style_wav=None,\n              truncated=False,\n              enable_eos_bos_chars=False, #pylint: disable=unused-argument\n              use_griffin_lim=False,\n              do_trim_silence=False,\n              backend=\'torch\'):\n    """"""Synthesize voice for the given text.\n\n        Args:\n            model (TTS.models): model to synthesize.\n            text (str): target text\n            CONFIG (dict): config dictionary to be loaded from config.json.\n            use_cuda (bool): enable cuda.\n            ap (TTS.utils.audio.AudioProcessor): audio processor to process\n                model outputs.\n            speaker_id (int): id of speaker\n            style_wav (str): Uses for style embedding of GST.\n            truncated (bool): keep model states after inference. It can be used\n                for continuous inference at long texts.\n            enable_eos_bos_chars (bool): enable special chars for end of sentence and start of sentence.\n            do_trim_silence (bool): trim silence after synthesis.\n            backend (str): tf or torch\n    """"""\n    # GST processing\n    style_mel = None\n    if CONFIG.model == ""TacotronGST"" and style_wav is not None:\n        style_mel = compute_style_mel(style_wav, ap)\n    # preprocess the given text\n    inputs = text_to_seqvec(text, CONFIG)\n    # pass tensors to backend\n    if backend == \'torch\':\n        speaker_id = id_to_torch(speaker_id)\n        style_mel = numpy_to_torch(style_mel, torch.float, cuda=use_cuda)\n        inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n        inputs = inputs.unsqueeze(0)\n    else:\n        # TODO: handle speaker id for tf model\n        style_mel = numpy_to_tf(style_mel, tf.float32)\n        inputs = numpy_to_tf(inputs, tf.int32)\n        inputs = tf.expand_dims(inputs, 0)\n    # synthesize voice\n    if backend == \'torch\':\n        decoder_output, postnet_output, alignments, stop_tokens = run_model_torch(\n            model, inputs, CONFIG, truncated, speaker_id, style_mel)\n        postnet_output, decoder_output, alignment, stop_tokens = parse_outputs_torch(\n            postnet_output, decoder_output, alignments, stop_tokens)\n    else:\n        decoder_output, postnet_output, alignments, stop_tokens = run_model_tf(\n            model, inputs, CONFIG, truncated, speaker_id, style_mel)\n        postnet_output, decoder_output, alignment, stop_tokens = parse_outputs_tf(\n            postnet_output, decoder_output, alignments, stop_tokens)\n    # convert outputs to numpy\n    # plot results\n    wav = None\n    if use_griffin_lim:\n        wav = inv_spectrogram(postnet_output, ap, CONFIG)\n        # trim silence\n        if do_trim_silence:\n            wav = trim_silence(wav, ap)\n    return wav, alignment, decoder_output, postnet_output, stop_tokens, inputs\n'"
utils/tensorboard_logger.py,0,"b'import traceback\nfrom tensorboardX import SummaryWriter\n\n\nclass TensorboardLogger(object):\n    def __init__(self, log_dir):\n        self.writer = SummaryWriter(log_dir)\n        self.train_stats = {}\n        self.eval_stats = {}\n\n    def tb_model_weights(self, model, step):\n        layer_num = 1\n        for name, param in model.named_parameters():\n            if param.numel() == 1:\n                self.writer.add_scalar(\n                    ""layer{}-{}/value"".format(layer_num, name),\n                    param.max(), step)\n            else:\n                self.writer.add_scalar(\n                    ""layer{}-{}/max"".format(layer_num, name),\n                    param.max(), step)\n                self.writer.add_scalar(\n                    ""layer{}-{}/min"".format(layer_num, name),\n                    param.min(), step)\n                self.writer.add_scalar(\n                    ""layer{}-{}/mean"".format(layer_num, name),\n                    param.mean(), step)\n                self.writer.add_scalar(\n                    ""layer{}-{}/std"".format(layer_num, name),\n                    param.std(), step)\n                self.writer.add_histogram(\n                    ""layer{}-{}/param"".format(layer_num, name), param, step)\n                self.writer.add_histogram(\n                    ""layer{}-{}/grad"".format(layer_num, name), param.grad, step)\n            layer_num += 1\n\n    def dict_to_tb_scalar(self, scope_name, stats, step):\n        for key, value in stats.items():\n            self.writer.add_scalar(\'{}/{}\'.format(scope_name, key), value, step)\n\n    def dict_to_tb_figure(self, scope_name, figures, step):\n        for key, value in figures.items():\n            self.writer.add_figure(\'{}/{}\'.format(scope_name, key), value, step)\n\n    def dict_to_tb_audios(self, scope_name, audios, step, sample_rate):\n        for key, value in audios.items():\n            try:\n                self.writer.add_audio(\'{}/{}\'.format(scope_name, key), value, step, sample_rate=sample_rate)\n            except:\n                traceback.print_exc()\n\n    def tb_train_iter_stats(self, step, stats):\n        self.dict_to_tb_scalar(""TrainIterStats"", stats, step)\n\n    def tb_train_epoch_stats(self, step, stats):\n        self.dict_to_tb_scalar(""TrainEpochStats"", stats, step)\n\n    def tb_train_figures(self, step, figures):\n        self.dict_to_tb_figure(""TrainFigures"", figures, step)\n\n    def tb_train_audios(self, step, audios, sample_rate):\n        self.dict_to_tb_audios(""TrainAudios"", audios, step, sample_rate)\n\n    def tb_eval_stats(self, step, stats):\n        self.dict_to_tb_scalar(""EvalStats"", stats, step)\n\n    def tb_eval_figures(self, step, figures):\n        self.dict_to_tb_figure(""EvalFigures"", figures, step)\n\n    def tb_eval_audios(self, step, audios, sample_rate):\n        self.dict_to_tb_audios(""EvalAudios"", audios, step, sample_rate)\n\n    def tb_test_audios(self, step, audios, sample_rate):\n        self.dict_to_tb_audios(""TestAudios"", audios, step, sample_rate)\n\n    def tb_test_figures(self, step, figures):\n        self.dict_to_tb_figure(""TestFigures"", figures, step)\n\n    def tb_add_text(self, title, text, step):\n        self.writer.add_text(title, text, step)\n'"
utils/training.py,5,"b'import torch\nimport numpy as np\n\n\ndef check_update(model, grad_clip, ignore_stopnet=False):\n    r\'\'\'Check model gradient against unexpected jumps and failures\'\'\'\n    skip_flag = False\n    if ignore_stopnet:\n        grad_norm = torch.nn.utils.clip_grad_norm_([param for name, param in model.named_parameters() if \'stopnet\' not in name], grad_clip)\n    else:\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n    # compatibility with different torch versions\n    if isinstance(grad_norm, float):\n        if np.isinf(grad_norm):\n            print("" | > Gradient is INF !!"")\n            skip_flag = True\n    else:\n        if torch.isinf(grad_norm):\n            print("" | > Gradient is INF !!"")\n            skip_flag = True\n    return grad_norm, skip_flag\n\n\ndef lr_decay(init_lr, global_step, warmup_steps):\n    r\'\'\'from https://github.com/r9y9/tacotron_pytorch/blob/master/train.py\'\'\'\n    warmup_steps = float(warmup_steps)\n    step = global_step + 1.\n    lr = init_lr * warmup_steps**0.5 * np.minimum(step * warmup_steps**-1.5,\n                                                  step**-0.5)\n    return lr\n\n\ndef adam_weight_decay(optimizer):\n    """"""\n    Custom weight decay operation, not effecting grad values.\n    """"""\n    for group in optimizer.param_groups:\n        for param in group[\'params\']:\n            current_lr = group[\'lr\']\n            weight_decay = group[\'weight_decay\']\n            factor = -weight_decay * group[\'lr\']\n            param.data = param.data.add(param.data,\n                                        alpha=factor)\n    return optimizer, current_lr\n\n# pylint: disable=dangerous-default-value\ndef set_weight_decay(model, weight_decay, skip_list={""decoder.attention.v"", ""rnn"", ""lstm"", ""gru"", ""embedding""}):\n    """"""\n    Skip biases, BatchNorm parameters, rnns.\n    and attention projection layer v\n    """"""\n    decay = []\n    no_decay = []\n    for name, param in model.named_parameters():\n        if not param.requires_grad:\n            continue\n\n        if len(param.shape) == 1 or any([skip_name in name for skip_name in skip_list]):\n            no_decay.append(param)\n        else:\n            decay.append(param)\n    return [{\n        \'params\': no_decay,\n        \'weight_decay\': 0.\n    }, {\n        \'params\': decay,\n        \'weight_decay\': weight_decay\n    }]\n\n\n# pylint: disable=protected-access\nclass NoamLR(torch.optim.lr_scheduler._LRScheduler):\n    def __init__(self, optimizer, warmup_steps=0.1, last_epoch=-1):\n        self.warmup_steps = float(warmup_steps)\n        super(NoamLR, self).__init__(optimizer, last_epoch)\n\n    def get_lr(self):\n        step = max(self.last_epoch, 1)\n        return [\n            base_lr * self.warmup_steps**0.5 *\n            min(step * self.warmup_steps**-1.5, step**-0.5)\n            for base_lr in self.base_lrs\n        ]\n\n\ndef gradual_training_scheduler(global_step, config):\n    """"""Setup the gradual training schedule wrt number\n    of active GPUs""""""\n    num_gpus = torch.cuda.device_count()\n    if num_gpus == 0:\n        num_gpus = 1\n    new_values = None\n    # we set the scheduling wrt num_gpus\n    for values in config.gradual_training:\n        if global_step * num_gpus >= values[0]:\n            new_values = values\n    return new_values[1], new_values[2]\n'"
utils/visual.py,2,"b'import torch\nimport librosa\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nfrom TTS.utils.text import phoneme_to_sequence, sequence_to_phoneme\n\n\ndef plot_alignment(alignment, info=None, fig_size=(16, 10), title=None):\n    if isinstance(alignment, torch.Tensor):\n        alignment_ = alignment.detach().cpu().numpy().squeeze()\n    else:\n        alignment_ = alignment\n    fig, ax = plt.subplots(figsize=fig_size)\n    im = ax.imshow(\n        alignment_.T, aspect=\'auto\', origin=\'lower\', interpolation=\'none\')\n    fig.colorbar(im, ax=ax)\n    xlabel = \'Decoder timestep\'\n    if info is not None:\n        xlabel += \'\\n\\n\' + info\n    plt.xlabel(xlabel)\n    plt.ylabel(\'Encoder timestep\')\n    # plt.yticks(range(len(text)), list(text))\n    plt.tight_layout()\n    if title is not None:\n        plt.title(title)\n    return fig\n\n\ndef plot_spectrogram(linear_output, audio, fig_size=(16, 10)):\n    if isinstance(linear_output, torch.Tensor):\n        linear_output_ = linear_output.detach().cpu().numpy().squeeze()\n    else:\n        linear_output_ = linear_output\n    spectrogram = audio._denormalize(linear_output_.T)  # pylint: disable=protected-access\n    fig = plt.figure(figsize=fig_size)\n    plt.imshow(spectrogram, aspect=""auto"", origin=""lower"")\n    plt.colorbar()\n    plt.tight_layout()\n    return fig\n\n\ndef visualize(alignment, postnet_output, stop_tokens, text, hop_length, CONFIG, decoder_output=None, output_path=None, figsize=(8, 24)):\n    if decoder_output is not None:\n        num_plot = 4\n    else:\n        num_plot = 3\n\n    label_fontsize = 16\n    fig = plt.figure(figsize=figsize)\n\n    plt.subplot(num_plot, 1, 1)\n    plt.imshow(alignment.T, aspect=""auto"", origin=""lower"", interpolation=None)\n    plt.xlabel(""Decoder timestamp"", fontsize=label_fontsize)\n    plt.ylabel(""Encoder timestamp"", fontsize=label_fontsize)\n    # compute phoneme representation and back\n    if CONFIG.use_phonemes:\n        seq = phoneme_to_sequence(text, [CONFIG.text_cleaner], CONFIG.phoneme_language, CONFIG.enable_eos_bos_chars, tp=CONFIG.characters if \'characters\' in CONFIG.keys() else None)\n        text = sequence_to_phoneme(seq, tp=CONFIG.characters if \'characters\' in CONFIG.keys() else None)\n        print(text)\n    plt.yticks(range(len(text)), list(text))\n    plt.colorbar()\n    # plot stopnet predictions\n    plt.subplot(num_plot, 1, 2)\n    plt.plot(range(len(stop_tokens)), list(stop_tokens))\n    # plot postnet spectrogram\n    plt.subplot(num_plot, 1, 3)\n    librosa.display.specshow(postnet_output.T, sr=CONFIG.audio[\'sample_rate\'],\n                             hop_length=hop_length, x_axis=""time"", y_axis=""linear"",\n                             fmin=CONFIG.audio[\'mel_fmin\'],\n                             fmax=CONFIG.audio[\'mel_fmax\'])\n\n    plt.xlabel(""Time"", fontsize=label_fontsize)\n    plt.ylabel(""Hz"", fontsize=label_fontsize)\n    plt.tight_layout()\n    plt.colorbar()\n\n    if decoder_output is not None:\n        plt.subplot(num_plot, 1, 4)\n        librosa.display.specshow(decoder_output.T, sr=CONFIG.audio[\'sample_rate\'],\n                                 hop_length=hop_length, x_axis=""time"", y_axis=""linear"",\n                                 fmin=CONFIG.audio[\'mel_fmin\'],\n                                 fmax=CONFIG.audio[\'mel_fmax\'])\n        plt.xlabel(""Time"", fontsize=label_fontsize)\n        plt.ylabel(""Hz"", fontsize=label_fontsize)\n        plt.tight_layout()\n        plt.colorbar()\n\n    if output_path:\n        print(output_path)\n        fig.savefig(output_path)\n        plt.close()\n'"
tf/layers/common_layers.py,0,"b'import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.python.ops import math_ops\n# from tensorflow_addons.seq2seq import BahdanauAttention\n\n\nclass Linear(keras.layers.Layer):\n    def __init__(self, units, use_bias, **kwargs):\n        super(Linear, self).__init__(**kwargs)\n        self.linear_layer = keras.layers.Dense(units, use_bias=use_bias, name=\'linear_layer\')\n        self.activation = keras.layers.ReLU()\n\n    def call(self, x):\n        """"""\n        shapes:\n            x: B x T x C\n        """"""\n        return self.activation(self.linear_layer(x))\n\n\nclass LinearBN(keras.layers.Layer):\n    def __init__(self, units, use_bias, **kwargs):\n        super(LinearBN, self).__init__(**kwargs)\n        self.linear_layer = keras.layers.Dense(units, use_bias=use_bias, name=\'linear_layer\')\n        self.batch_normalization = keras.layers.BatchNormalization(axis=-1, momentum=0.90, epsilon=1e-5, name=\'batch_normalization\')\n        self.activation = keras.layers.ReLU()\n\n    def call(self, x, training=None):\n        """"""\n        shapes:\n            x: B x T x C\n        """"""\n        out = self.linear_layer(x)\n        out = self.batch_normalization(out, training=training)\n        return self.activation(out)\n\n\nclass Prenet(keras.layers.Layer):\n    def __init__(self,\n                 prenet_type,\n                 prenet_dropout,\n                 units,\n                 bias,\n                 **kwargs):\n        super(Prenet, self).__init__(**kwargs)\n        self.prenet_type = prenet_type\n        self.prenet_dropout = prenet_dropout\n        self.linear_layers = []\n        if prenet_type == ""bn"":\n            self.linear_layers += [LinearBN(unit, use_bias=bias, name=f\'linear_layer_{idx}\') for idx, unit in enumerate(units)]\n        elif prenet_type == ""original"":\n            self.linear_layers += [Linear(unit, use_bias=bias, name=f\'linear_layer_{idx}\') for idx, unit in enumerate(units)]\n        else:\n            raise RuntimeError(\' [!] Unknown prenet type.\')\n        if prenet_dropout:\n            self.dropout = keras.layers.Dropout(rate=0.5)\n\n    def call(self, x, training=None):\n        """"""\n        shapes:\n            x: B x T x C\n        """"""\n        for linear in self.linear_layers:\n            if self.prenet_dropout:\n                x = self.dropout(linear(x), training=training)\n            else:\n                x = linear(x)\n        return x\n\n\ndef _sigmoid_norm(score):\n    attn_weights = tf.nn.sigmoid(score)\n    attn_weights = attn_weights / tf.reduce_sum(attn_weights, axis=1, keepdims=True)\n    return attn_weights\n\n\nclass Attention(keras.layers.Layer):\n    """"""TODO: implement forward_attention\n    TODO: location sensitive attention\n    TODO: implement attention windowing """"""\n    def __init__(self, attn_dim, use_loc_attn, loc_attn_n_filters,\n                 loc_attn_kernel_size, use_windowing, norm, use_forward_attn,\n                 use_trans_agent, use_forward_attn_mask, **kwargs):\n        super(Attention, self).__init__(**kwargs)\n        self.use_loc_attn = use_loc_attn\n        self.loc_attn_n_filters = loc_attn_n_filters\n        self.loc_attn_kernel_size = loc_attn_kernel_size\n        self.use_windowing = use_windowing\n        self.norm = norm\n        self.use_forward_attn = use_forward_attn\n        self.use_trans_agent = use_trans_agent\n        self.use_forward_attn_mask = use_forward_attn_mask\n        self.query_layer = tf.keras.layers.Dense(attn_dim, use_bias=False, name=\'query_layer/linear_layer\')\n        self.inputs_layer = tf.keras.layers.Dense(attn_dim, use_bias=False, name=f\'{self.name}/inputs_layer/linear_layer\')\n        self.v = tf.keras.layers.Dense(1, use_bias=True, name=\'v/linear_layer\')\n        if use_loc_attn:\n            self.location_conv1d = keras.layers.Conv1D(\n                filters=loc_attn_n_filters,\n                kernel_size=loc_attn_kernel_size,\n                padding=\'same\',\n                use_bias=False,\n                name=\'location_layer/location_conv1d\')\n            self.location_dense = keras.layers.Dense(attn_dim, use_bias=False, name=\'location_layer/location_dense\')\n        if norm == \'softmax\':\n            self.norm_func = tf.nn.softmax\n        elif norm == \'sigmoid\':\n            self.norm_func = _sigmoid_norm\n        else:\n            raise ValueError(""Unknown value for attention norm type"")\n\n    def init_states(self, batch_size, value_length):\n        states = ()\n        if self.use_loc_attn:\n            attention_cum = tf.zeros([batch_size, value_length])\n            attention_old = tf.zeros([batch_size, value_length])\n            states = (attention_cum, attention_old)\n        return states\n\n    def process_values(self, values):\n        """""" cache values for decoder iterations """"""\n        #pylint: disable=attribute-defined-outside-init\n        self.processed_values = self.inputs_layer(values)\n        self.values = values\n\n    def get_loc_attn(self, query, states):\n        """""" compute location attention, query layer and\n        unnorm. attention weights""""""\n        attention_cum, attention_old = states\n        attn_cat = tf.stack([attention_old, attention_cum], axis=2)\n\n        processed_query = self.query_layer(tf.expand_dims(query, 1))\n        processed_attn = self.location_dense(self.location_conv1d(attn_cat))\n        score = self.v(\n            tf.nn.tanh(self.processed_values + processed_query +\n                       processed_attn))\n        score = tf.squeeze(score, axis=2)\n        return score, processed_query\n\n    def get_attn(self, query):\n        """""" compute query layer and unnormalized attention weights """"""\n        processed_query = self.query_layer(tf.expand_dims(query, 1))\n        score = self.v(tf.nn.tanh(self.processed_values + processed_query))\n        score = tf.squeeze(score, axis=2)\n        return score, processed_query\n\n    def apply_score_masking(self, score, mask):  #pylint: disable=no-self-use\n        """""" ignore sequence paddings """"""\n        padding_mask = tf.expand_dims(math_ops.logical_not(mask), 2)\n        # Bias so padding positions do not contribute to attention distribution.\n        score -= 1.e9 * math_ops.cast(padding_mask, dtype=tf.float32)\n        return score\n\n    def call(self, query, states):\n        """"""\n        shapes:\n            query: B x D\n        """"""\n        if self.use_loc_attn:\n            score, _ = self.get_loc_attn(query, states)\n        else:\n            score, _ = self.get_attn(query)\n\n        # TODO: masking\n        # if mask is not None:\n        # self.apply_score_masking(score, mask)\n        # attn_weights shape == (batch_size, max_length, 1)\n\n        attn_weights = self.norm_func(score)\n\n        # update attention states\n        if self.use_loc_attn:\n            states = (states[0] + attn_weights, attn_weights)\n        else:\n            states = ()\n\n        # context_vector shape after sum == (batch_size, hidden_size)\n        context_vector = tf.matmul(tf.expand_dims(attn_weights, axis=2), self.values, transpose_a=True, transpose_b=False)\n        context_vector = tf.squeeze(context_vector, axis=1)\n        return context_vector, attn_weights, states\n\n\n# def _location_sensitive_score(processed_query, keys, processed_loc, attention_v, attention_b):\n#     dtype = processed_query.dtype\n#     num_units = keys.shape[-1].value or array_ops.shape(keys)[-1]\n#     return tf.reduce_sum(attention_v * tf.tanh(keys + processed_query + processed_loc + attention_b), [2])\n\n\n# class LocationSensitiveAttention(BahdanauAttention):\n#     def __init__(self,\n#                  units,\n#                  memory=None,\n#                  memory_sequence_length=None,\n#                  normalize=False,\n#                  probability_fn=""softmax"",\n#                  kernel_initializer=""glorot_uniform"",\n#                  dtype=None,\n#                  name=""LocationSensitiveAttention"",\n#                  location_attention_filters=32,\n#                  location_attention_kernel_size=31):\n\n#         super(LocationSensitiveAttention,\n#                     self).__init__(units=units,\n#                                     memory=memory,\n#                                     memory_sequence_length=memory_sequence_length,\n#                                     normalize=normalize,\n#                                     probability_fn=\'softmax\',  ## parent module default\n#                                     kernel_initializer=kernel_initializer,\n#                                     dtype=dtype,\n#                                     name=name)\n#         if probability_fn == \'sigmoid\':\n#             self.probability_fn = lambda score, _: self._sigmoid_normalization(score)\n#         self.location_conv = keras.layers.Conv1D(filters=location_attention_filters, kernel_size=location_attention_kernel_size, padding=\'same\', use_bias=False)\n#         self.location_dense = keras.layers.Dense(units, use_bias=False)\n#         # self.v = keras.layers.Dense(1, use_bias=True)\n\n#     def  _location_sensitive_score(self, processed_query, keys, processed_loc):\n#         processed_query = tf.expand_dims(processed_query, 1)\n#         return tf.reduce_sum(self.attention_v * tf.tanh(keys + processed_query + processed_loc), [2])\n\n#     def _location_sensitive(self, alignment_cum, alignment_old):\n#         alignment_cat = tf.stack([alignment_cum, alignment_old], axis=2)\n#         return self.location_dense(self.location_conv(alignment_cat))\n\n#     def _sigmoid_normalization(self, score):\n#         return tf.nn.sigmoid(score) / tf.reduce_sum(tf.nn.sigmoid(score), axis=-1, keepdims=True)\n\n#     # def _apply_masking(self, score, mask):\n#     #     padding_mask = tf.expand_dims(math_ops.logical_not(mask), 2)\n#     #     # Bias so padding positions do not contribute to attention distribution.\n#     #     score -= 1.e9 * math_ops.cast(padding_mask, dtype=tf.float32)\n#     #     return score\n\n#     def _calculate_attention(self, query, state):\n#         alignment_cum, alignment_old = state[:2]\n#         processed_query = self.query_layer(\n#             query) if self.query_layer else query\n#         processed_loc = self._location_sensitive(alignment_cum, alignment_old)\n#         score = self._location_sensitive_score(\n#             processed_query,\n#             self.keys,\n#             processed_loc)\n#         alignment = self.probability_fn(score, state)\n#         alignment_cum = alignment_cum + alignment\n#         state[0] = alignment_cum\n#         state[1] = alignment\n#         return alignment, state\n\n#     def compute_context(self, alignments):\n#         expanded_alignments = tf.expand_dims(alignments, 1)\n#         context = tf.matmul(expanded_alignments, self.values)\n#         context = tf.squeeze(context, [1])\n#         return context\n\n#     # def call(self, query, state):\n#     #     alignment, next_state = self._calculate_attention(query, state)\n#     #     return alignment, next_state\n'"
tf/layers/tacotron2.py,0,"b""\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom TTS.tf.utils.tf_utils import shape_list\nfrom TTS.tf.layers.common_layers import Prenet, Attention\n# from tensorflow_addons.seq2seq import AttentionWrapper\n\n\nclass ConvBNBlock(keras.layers.Layer):\n    def __init__(self, filters, kernel_size, activation, **kwargs):\n        super(ConvBNBlock, self).__init__(**kwargs)\n        self.convolution1d = keras.layers.Conv1D(filters, kernel_size, padding='same', name='convolution1d')\n        self.batch_normalization = keras.layers.BatchNormalization(axis=2, momentum=0.90, epsilon=1e-5, name='batch_normalization')\n        self.dropout = keras.layers.Dropout(rate=0.5, name='dropout')\n        self.activation = keras.layers.Activation(activation, name='activation')\n\n    def call(self, x, training=None):\n        o = self.convolution1d(x)\n        o = self.batch_normalization(o, training=training)\n        o = self.activation(o)\n        o = self.dropout(o, training=training)\n        return o\n\n\nclass Postnet(keras.layers.Layer):\n    def __init__(self, output_filters, num_convs, **kwargs):\n        super(Postnet, self).__init__(**kwargs)\n        self.convolutions = []\n        self.convolutions.append(ConvBNBlock(512, 5, 'tanh', name='convolutions_0'))\n        for idx in range(1, num_convs - 1):\n            self.convolutions.append(ConvBNBlock(512, 5, 'tanh', name=f'convolutions_{idx}'))\n        self.convolutions.append(ConvBNBlock(output_filters, 5, 'linear', name=f'convolutions_{idx+1}'))\n\n    def call(self, x, training=None):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o, training=training)\n        return o\n\n\nclass Encoder(keras.layers.Layer):\n    def __init__(self, output_input_dim, **kwargs):\n        super(Encoder, self).__init__(**kwargs)\n        self.convolutions = []\n        for idx in range(3):\n            self.convolutions.append(ConvBNBlock(output_input_dim, 5, 'relu', name=f'convolutions_{idx}'))\n        self.lstm = keras.layers.Bidirectional(keras.layers.LSTM(output_input_dim // 2, return_sequences=True, use_bias=True), name='lstm')\n\n    def call(self, x, training=None):\n        o = x\n        for layer in self.convolutions:\n            o = layer(o, training=training)\n        o = self.lstm(o)\n        return o\n\n\nclass Decoder(keras.layers.Layer):\n    #pylint: disable=unused-argument\n    def __init__(self, frame_dim, r, attn_type, use_attn_win, attn_norm, prenet_type,\n                 prenet_dropout, use_forward_attn, use_trans_agent, use_forward_attn_mask,\n                 use_location_attn, attn_K, separate_stopnet, speaker_emb_dim, **kwargs):\n        super(Decoder, self).__init__(**kwargs)\n        self.frame_dim = frame_dim\n        self.r_init = tf.constant(r, dtype=tf.int32)\n        self.r = tf.constant(r, dtype=tf.int32)\n        self.separate_stopnet = separate_stopnet\n        self.max_decoder_steps = tf.constant(1000, dtype=tf.int32)\n        self.stop_thresh = tf.constant(0.5, dtype=tf.float32)\n\n        # model dimensions\n        self.query_dim = 1024\n        self.decoder_rnn_dim = 1024\n        self.prenet_dim = 256\n        self.attn_dim = 128\n        self.p_attention_dropout = 0.1\n        self.p_decoder_dropout = 0.1\n\n        self.prenet = Prenet(prenet_type,\n                             prenet_dropout,\n                             [self.prenet_dim, self.prenet_dim],\n                             bias=False,\n                             name='prenet')\n        self.attention_rnn = keras.layers.LSTMCell(self.query_dim, use_bias=True, name=f'{self.name}/attention_rnn', )\n        self.attention_rnn_dropout = keras.layers.Dropout(0.5)\n\n        # TODO: implement other attn options\n        self.attention = Attention(attn_dim=self.attn_dim,\n                                   use_loc_attn=True,\n                                   loc_attn_n_filters=32,\n                                   loc_attn_kernel_size=31,\n                                   use_windowing=False,\n                                   norm=attn_norm,\n                                   use_forward_attn=use_forward_attn,\n                                   use_trans_agent=use_trans_agent,\n                                   use_forward_attn_mask=use_forward_attn_mask,\n                                   name='attention')\n        self.decoder_rnn = keras.layers.LSTMCell(self.decoder_rnn_dim, use_bias=True, name=f'{self.name}/decoder_rnn')\n        self.decoder_rnn_dropout = keras.layers.Dropout(0.5)\n        self.linear_projection = keras.layers.Dense(self.frame_dim * r, name=f'{self.name}/linear_projection/linear_layer')\n        self.stopnet = keras.layers.Dense(1, name=f'{self.name}/stopnet/linear_layer')\n\n\n    def set_max_decoder_steps(self, new_max_steps):\n        self.max_decoder_steps = tf.constant(new_max_steps, dtype=tf.int32)\n\n    def set_r(self, new_r):\n        self.r = tf.constant(new_r, dtype=tf.int32)\n\n    def build_decoder_initial_states(self, batch_size, memory_dim, memory_length):\n        zero_frame = tf.zeros([batch_size, self.frame_dim])\n        zero_context = tf.zeros([batch_size, memory_dim])\n        attention_rnn_state = self.attention_rnn.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        decoder_rnn_state = self.decoder_rnn.get_initial_state(batch_size=batch_size, dtype=tf.float32)\n        attention_states = self.attention.init_states(batch_size, memory_length)\n        return zero_frame, zero_context, attention_rnn_state, decoder_rnn_state, attention_states\n\n    def step(self, prenet_next, states,\n             memory_seq_length=None, training=None):\n        _, context_next, attention_rnn_state, decoder_rnn_state, attention_states = states\n        attention_rnn_input = tf.concat([prenet_next, context_next], -1)\n        attention_rnn_output, attention_rnn_state = \\\n                self.attention_rnn(attention_rnn_input,\n                                   attention_rnn_state, training=training)\n        attention_rnn_output = self.attention_rnn_dropout(attention_rnn_output, training=training)\n        context, attention, attention_states = self.attention(attention_rnn_output, attention_states, training=training)\n        decoder_rnn_input = tf.concat([attention_rnn_output, context], -1)\n        decoder_rnn_output, decoder_rnn_state = \\\n                self.decoder_rnn(decoder_rnn_input, decoder_rnn_state, training=training)\n        decoder_rnn_output = self.decoder_rnn_dropout(decoder_rnn_output, training=training)\n        linear_projection_input = tf.concat([decoder_rnn_output, context], -1)\n        output_frame = self.linear_projection(linear_projection_input, training=training)\n        stopnet_input = tf.concat([decoder_rnn_output, output_frame], -1)\n        stopnet_output = self.stopnet(stopnet_input, training=training)\n        output_frame = output_frame[:, :self.r * self.frame_dim]\n        states = (output_frame[:, self.frame_dim * (self.r - 1):], context, attention_rnn_state, decoder_rnn_state, attention_states)\n        return output_frame, stopnet_output, states, attention\n\n    def decode(self, memory, states, frames, memory_seq_length=None):\n        B, _, _ = shape_list(memory)\n        num_iter = shape_list(frames)[1] // self.r\n        # init states\n        frame_zero = tf.expand_dims(states[0], 1)\n        frames = tf.concat([frame_zero, frames], axis=1)\n        outputs = tf.TensorArray(dtype=tf.float32, size=num_iter)\n        attentions = tf.TensorArray(dtype=tf.float32, size=num_iter)\n        stop_tokens = tf.TensorArray(dtype=tf.float32, size=num_iter)\n        # pre-computes\n        self.attention.process_values(memory)\n        prenet_output = self.prenet(frames, training=True)\n        step_count = tf.constant(0, dtype=tf.int32)\n\n        def _body(step, memory, prenet_output, states, outputs, stop_tokens, attentions):\n            prenet_next = prenet_output[:, step]\n            output, stop_token, states, attention = self.step(prenet_next,\n                                                              states,\n                                                              memory_seq_length)\n            outputs = outputs.write(step, output)\n            attentions = attentions.write(step, attention)\n            stop_tokens = stop_tokens.write(step, stop_token)\n            return step + 1, memory, prenet_output, states, outputs, stop_tokens, attentions\n        _, memory, _, states, outputs, stop_tokens, attentions = \\\n                tf.while_loop(lambda *arg: True,\n                              _body,\n                              loop_vars=(step_count, memory, prenet_output,\n                                         states, outputs, stop_tokens, attentions),\n                              parallel_iterations=32,\n                              swap_memory=True,\n                              maximum_iterations=num_iter)\n\n        outputs = outputs.stack()\n        attentions = attentions.stack()\n        stop_tokens = stop_tokens.stack()\n        outputs = tf.transpose(outputs, [1, 0, 2])\n        attentions = tf.transpose(attentions, [1, 0, 2])\n        stop_tokens = tf.transpose(stop_tokens, [1, 0, 2])\n        stop_tokens = tf.squeeze(stop_tokens, axis=2)\n        outputs = tf.reshape(outputs, [B, -1, self.frame_dim])\n        return outputs, stop_tokens, attentions\n\n    def decode_inference(self, memory, states):\n        B, _, _ = shape_list(memory)\n        # init states\n        outputs = tf.TensorArray(dtype=tf.float32, size=0, clear_after_read=False, dynamic_size=True)\n        attentions = tf.TensorArray(dtype=tf.float32, size=0, clear_after_read=False, dynamic_size=True)\n        stop_tokens = tf.TensorArray(dtype=tf.float32, size=0, clear_after_read=False, dynamic_size=True)\n        # pre-computes\n        self.attention.process_values(memory)\n\n        # iter vars\n        stop_flag = tf.constant(False, dtype=tf.bool)\n        step_count = tf.constant(0, dtype=tf.int32)\n\n        def _body(step, memory, states, outputs, stop_tokens, attentions, stop_flag):\n            frame_next = states[0]\n            prenet_next = self.prenet(frame_next, training=False)\n            output, stop_token, states, attention = self.step(prenet_next,\n                                                              states,\n                                                              None,\n                                                              training=False)\n            stop_token = tf.math.sigmoid(stop_token)\n            outputs = outputs.write(step, output)\n            attentions = attentions.write(step, attention)\n            stop_tokens = stop_tokens.write(step, stop_token)\n            stop_flag = tf.greater(stop_token, self.stop_thresh)\n            stop_flag = tf.reduce_all(stop_flag)\n            return step + 1, memory, states, outputs, stop_tokens, attentions, stop_flag\n\n        cond = lambda step, m, s, o, st, a, stop_flag: tf.equal(stop_flag, tf.constant(False, dtype=tf.bool))\n        _, memory, states, outputs, stop_tokens, attentions, stop_flag = \\\n                tf.while_loop(cond,\n                              _body,\n                              loop_vars=(step_count, memory, states, outputs,\n                                         stop_tokens, attentions, stop_flag),\n                              parallel_iterations=32,\n                              swap_memory=True,\n                              maximum_iterations=self.max_decoder_steps)\n\n        outputs = outputs.stack()\n        attentions = attentions.stack()\n        stop_tokens = stop_tokens.stack()\n\n        outputs = tf.transpose(outputs, [1, 0, 2])\n        attentions = tf.transpose(attentions, [1, 0, 2])\n        stop_tokens = tf.transpose(stop_tokens, [1, 0, 2])\n        stop_tokens = tf.squeeze(stop_tokens, axis=2)\n        outputs = tf.reshape(outputs, [B, -1, self.frame_dim])\n        return outputs, stop_tokens, attentions\n\n    def call(self, memory, states, frames=None, memory_seq_length=None, training=False):\n        if training:\n            return self.decode(memory, states, frames, memory_seq_length)\n        return self.decode_inference(memory, states)\n"""
tf/models/tacotron2.py,0,"b'from tensorflow import keras\n\nfrom TTS.tf.layers.tacotron2 import Encoder, Decoder, Postnet\nfrom TTS.tf.utils.tf_utils import shape_list\n\n\n#pylint: disable=too-many-ancestors\nclass Tacotron2(keras.models.Model):\n    def __init__(self,\n                 num_chars,\n                 num_speakers,\n                 r,\n                 postnet_output_dim=80,\n                 decoder_output_dim=80,\n                 attn_type=\'original\',\n                 attn_win=False,\n                 attn_norm=""softmax"",\n                 attn_K=4,\n                 prenet_type=""original"",\n                 prenet_dropout=True,\n                 forward_attn=False,\n                 trans_agent=False,\n                 forward_attn_mask=False,\n                 location_attn=True,\n                 separate_stopnet=True,\n                 bidirectional_decoder=False):\n        super(Tacotron2, self).__init__()\n        self.r = r\n        self.decoder_output_dim = decoder_output_dim\n        self.postnet_output_dim = postnet_output_dim\n        self.bidirectional_decoder = bidirectional_decoder\n        self.num_speakers = num_speakers\n        self.speaker_embed_dim = 256\n\n        self.embedding = keras.layers.Embedding(num_chars, 512, name=\'embedding\')\n        self.encoder = Encoder(512, name=\'encoder\')\n        # TODO: most of the decoder args have no use at the momment\n        self.decoder = Decoder(decoder_output_dim,\n                               r,\n                               attn_type=attn_type,\n                               use_attn_win=attn_win,\n                               attn_norm=attn_norm,\n                               prenet_type=prenet_type,\n                               prenet_dropout=prenet_dropout,\n                               use_forward_attn=forward_attn,\n                               use_trans_agent=trans_agent,\n                               use_forward_attn_mask=forward_attn_mask,\n                               use_location_attn=location_attn,\n                               attn_K=attn_K,\n                               separate_stopnet=separate_stopnet,\n                               speaker_emb_dim=self.speaker_embed_dim)\n        self.postnet = Postnet(postnet_output_dim, 5, name=\'postnet\')\n\n    def call(self, characters, text_lengths=None, frames=None, training=None):\n        if training:\n            return self.training(characters, text_lengths, frames)\n        if not training:\n            return self.inference(characters)\n        raise RuntimeError(\' [!] Set model training mode True or False\')\n\n    def training(self, characters, text_lengths, frames):\n        B, T = shape_list(characters)\n        embedding_vectors = self.embedding(characters, training=True)\n        encoder_output = self.encoder(embedding_vectors, training=True)\n        decoder_states = self.decoder.build_decoder_initial_states(B, 512, T)\n        decoder_frames, stop_tokens, attentions = self.decoder(encoder_output, decoder_states, frames, text_lengths, training=True)\n        postnet_frames = self.postnet(decoder_frames, training=True)\n        output_frames = decoder_frames + postnet_frames\n        return decoder_frames, output_frames, attentions, stop_tokens\n\n    def inference(self, characters):\n        B, T = shape_list(characters)\n        embedding_vectors = self.embedding(characters, training=False)\n        encoder_output = self.encoder(embedding_vectors, training=False)\n        decoder_states = self.decoder.build_decoder_initial_states(B, 512, T)\n        decoder_frames, stop_tokens, attentions = self.decoder(encoder_output, decoder_states, training=False)\n        postnet_frames = self.postnet(decoder_frames, training=False)\n        output_frames = decoder_frames + postnet_frames\n        print(output_frames.shape)\n        return decoder_frames, output_frames, attentions, stop_tokens\n\n'"
tf/utils/convert_torch_to_tf_utils.py,0,"b'import numpy as np\nimport tensorflow as tf\n\n\ndef tf_create_dummy_inputs():\n    """""" Create dummy inputs for TF Tacotron2 model """"""\n    batch_size = 4\n    max_input_length = 32\n    max_mel_length = 128\n    pad = 1\n    n_chars = 24\n    input_ids = tf.random.uniform([batch_size, max_input_length + pad], maxval=n_chars, dtype=tf.int32)\n    input_lengths = np.random.randint(0, high=max_input_length+1 + pad, size=[batch_size])\n    input_lengths[-1] = max_input_length\n    input_lengths = tf.convert_to_tensor(input_lengths, dtype=tf.int32)\n    mel_outputs = tf.random.uniform(shape=[batch_size, max_mel_length + pad, 80])\n    mel_lengths = np.random.randint(0, high=max_mel_length+1 + pad, size=[batch_size])\n    mel_lengths[-1] = max_mel_length\n    mel_lengths = tf.convert_to_tensor(mel_lengths, dtype=tf.int32)\n    return input_ids, input_lengths, mel_outputs, mel_lengths\n\n\ndef compare_torch_tf(torch_tensor, tf_tensor):\n    """""" Compute the average absolute difference b/w torch and tf tensors """"""\n    return abs(torch_tensor.detach().numpy() - tf_tensor.numpy()).mean()\n\n\ndef convert_tf_name(tf_name):\n    """""" Convert certain patterns in TF layer names to Torch patterns """"""\n    tf_name_tmp = tf_name\n    tf_name_tmp = tf_name_tmp.replace(\':0\', \'\')\n    tf_name_tmp = tf_name_tmp.replace(\'/forward_lstm/lstm_cell_1/recurrent_kernel\', \'/weight_hh_l0\')\n    tf_name_tmp = tf_name_tmp.replace(\'/forward_lstm/lstm_cell_2/kernel\', \'/weight_ih_l1\')\n    tf_name_tmp = tf_name_tmp.replace(\'/recurrent_kernel\', \'/weight_hh\')\n    tf_name_tmp = tf_name_tmp.replace(\'/kernel\', \'/weight\')\n    tf_name_tmp = tf_name_tmp.replace(\'/gamma\', \'/weight\')\n    tf_name_tmp = tf_name_tmp.replace(\'/beta\', \'/bias\')\n    tf_name_tmp = tf_name_tmp.replace(\'/\', \'.\')\n    return tf_name_tmp\n\n\ndef transfer_weights_torch_to_tf(tf_vars, var_map_dict, state_dict):\n    """""" Transfer weigths from torch state_dict to TF variables """"""\n    print("" > Passing weights from Torch to TF ..."")\n    for tf_var in tf_vars:\n        torch_var_name = var_map_dict[tf_var.name]\n        print(f\' | > {tf_var.name} <-- {torch_var_name}\')\n        # if tuple, it is a bias variable\n        if not isinstance(torch_var_name, tuple):\n            torch_layer_name = \'.\'.join(torch_var_name.split(\'.\')[-2:])\n            torch_weight = state_dict[torch_var_name]\n        if \'convolution1d/kernel\' in tf_var.name or \'conv1d/kernel\' in tf_var.name:\n            # out_dim, in_dim, filter -> filter, in_dim, out_dim\n            numpy_weight = torch_weight.permute([2, 1, 0]).detach().cpu().numpy()\n        elif \'lstm_cell\' in tf_var.name and \'kernel\' in tf_var.name:\n            numpy_weight = torch_weight.transpose(0, 1).detach().cpu().numpy()\n        # if variable is for bidirectional lstm and it is a bias vector there\n        # needs to be pre-defined two matching torch bias vectors\n        elif \'_lstm/lstm_cell_\' in tf_var.name and \'bias\' in tf_var.name:\n            bias_vectors = [value for key, value in state_dict.items() if key in torch_var_name]\n            assert len(bias_vectors) == 2\n            numpy_weight = bias_vectors[0] + bias_vectors[1]\n        elif \'rnn\' in tf_var.name and \'kernel\' in tf_var.name:\n            numpy_weight = torch_weight.transpose(0, 1).detach().cpu().numpy()\n        elif \'rnn\' in tf_var.name and \'bias\' in tf_var.name:\n            bias_vectors = [value for key, value in state_dict.items() if torch_var_name[:-2] in key]\n            assert len(bias_vectors) == 2\n            numpy_weight = bias_vectors[0] + bias_vectors[1]\n        elif \'linear_layer\' in torch_layer_name and \'weight\' in torch_var_name:\n            numpy_weight = torch_weight.transpose(0, 1).detach().cpu().numpy()\n        else:\n            numpy_weight = torch_weight.detach().cpu().numpy()\n        assert np.all(tf_var.shape == numpy_weight.shape), f"" [!] weight shapes does not match: {tf_var.name} vs {torch_var_name} --> {tf_var.shape} vs {numpy_weight.shape}""\n        tf.keras.backend.set_value(tf_var, numpy_weight)\n    return tf_vars\n\n\ndef load_tf_vars(model_tf, tf_vars):\n    for tf_var in tf_vars:\n        model_tf.get_layer(tf_var.name).set_weights(tf_var)\n    return model_tf\n'"
tf/utils/generic_utils.py,0,"b'import os\nimport datetime\nimport importlib\nimport pickle\nimport numpy as np\nimport tensorflow as tf\n\n\ndef save_checkpoint(model, optimizer, current_step, epoch, r, output_folder, **kwargs):\n    checkpoint_path = \'tts_tf_checkpoint_{}.pkl\'.format(current_step)\n    checkpoint_path = os.path.join(output_folder, checkpoint_path)\n    state = {\n        \'model\': model.weights,\n        \'optimizer\': optimizer,\n        \'step\': current_step,\n        \'epoch\': epoch,\n        \'date\': datetime.date.today().strftime(""%B %d, %Y""),\n        \'r\': r\n    }\n    state.update(kwargs)\n    pickle.dump(state, open(checkpoint_path, \'wb\'))\n\n\ndef load_checkpoint(model, checkpoint_path):\n    checkpoint = pickle.load(open(checkpoint_path, \'rb\'))\n    chkp_var_dict = {var.name: var.numpy() for var in checkpoint[\'model\']}\n    tf_vars = model.weights\n    for tf_var in tf_vars:\n        layer_name = tf_var.name\n        chkp_var_value = chkp_var_dict[layer_name]\n        tf.keras.backend.set_value(tf_var, chkp_var_value)\n    if \'r\' in checkpoint.keys():\n        model.decoder.set_r(checkpoint[\'r\'])\n    return model\n\n\ndef sequence_mask(sequence_length, max_len=None):\n    if max_len is None:\n        max_len = sequence_length.max()\n    batch_size = sequence_length.size(0)\n    seq_range = np.empty([0, max_len], dtype=np.int8)\n    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n    if sequence_length.is_cuda:\n        seq_range_expand = seq_range_expand.cuda()\n    seq_length_expand = (\n        sequence_length.unsqueeze(1).expand_as(seq_range_expand))\n    # B x T_max\n    return seq_range_expand < seq_length_expand\n\n\n# @tf.custom_gradient\ndef check_gradient(x, grad_clip):\n    x_normed = tf.clip_by_norm(x, grad_clip)\n    grad_norm = tf.norm(grad_clip)\n    return x_normed, grad_norm\n\n\ndef count_parameters(model, c):\n    try:\n        return model.count_params()\n    except RuntimeError:\n        input_dummy = tf.convert_to_tensor(np.random.rand(8, 128).astype(\'int32\'))\n        input_lengths = np.random.randint(100, 129, (8, ))\n        input_lengths[-1] = 128\n        input_lengths = tf.convert_to_tensor(input_lengths.astype(\'int32\'))\n        mel_spec = np.random.rand(8, 2 * c.r,\n                                  c.audio[\'num_mels\']).astype(\'float32\')\n        mel_spec = tf.convert_to_tensor(mel_spec)\n        speaker_ids = np.random.randint(\n            0, 5, (8, )) if c.use_speaker_embedding else None\n        _ = model(input_dummy, input_lengths, mel_spec, speaker_ids=speaker_ids)\n        return model.count_params()\n\n\ndef setup_model(num_chars, num_speakers, c):\n    print("" > Using model: {}"".format(c.model))\n    MyModel = importlib.import_module(\'TTS.tf.models.\' + c.model.lower())\n    MyModel = getattr(MyModel, c.model)\n    if c.model.lower() in ""tacotron"":\n        raise NotImplementedError(\' [!] Tacotron model is not ready.\')\n    # tacotron2\n    model = MyModel(num_chars=num_chars,\n                    num_speakers=num_speakers,\n                    r=c.r,\n                    postnet_output_dim=c.audio[\'num_mels\'],\n                    decoder_output_dim=c.audio[\'num_mels\'],\n                    attn_type=c.attention_type,\n                    attn_win=c.windowing,\n                    attn_norm=c.attention_norm,\n                    prenet_type=c.prenet_type,\n                    prenet_dropout=c.prenet_dropout,\n                    forward_attn=c.use_forward_attn,\n                    trans_agent=c.transition_agent,\n                    forward_attn_mask=c.forward_attn_mask,\n                    location_attn=c.location_attn,\n                    attn_K=c.attention_heads,\n                    separate_stopnet=c.separate_stopnet,\n                    bidirectional_decoder=c.bidirectional_decoder)\n    return model\n'"
tf/utils/tf_utils.py,0,"b'import tensorflow as tf\n\n\ndef shape_list(x):\n    """"""Deal with dynamic shape in tensorflow cleanly.""""""\n    static = x.shape.as_list()\n    dynamic = tf.shape(x)\n    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n'"
utils/text/__init__.py,0,"b'# -*- coding: utf-8 -*-\n\nimport re\nfrom packaging import version\nimport phonemizer\nfrom phonemizer.phonemize import phonemize\nfrom TTS.utils.text import cleaners\nfrom TTS.utils.text.symbols import make_symbols, symbols, phonemes, _phoneme_punctuations, _bos, \\\n    _eos\n\n# Mappings from symbol to numeric ID and vice versa:\n_symbol_to_id = {s: i for i, s in enumerate(symbols)}\n_id_to_symbol = {i: s for i, s in enumerate(symbols)}\n\n_phonemes_to_id = {s: i for i, s in enumerate(phonemes)}\n_id_to_phonemes = {i: s for i, s in enumerate(phonemes)}\n\n# Regular expression matching text enclosed in curly braces:\n_CURLY_RE = re.compile(r\'(.*?)\\{(.+?)\\}(.*)\')\n\n# Regular expression matching punctuations, ignoring empty space\nPHONEME_PUNCTUATION_PATTERN = r\'[\'+_phoneme_punctuations+\']+\'\n\n\ndef text2phone(text, language):\n    \'\'\'\n    Convert graphemes to phonemes.\n    \'\'\'\n    seperator = phonemizer.separator.Separator(\' |\', \'\', \'|\')\n    #try:\n    punctuations = re.findall(PHONEME_PUNCTUATION_PATTERN, text)\n    if version.parse(phonemizer.__version__) < version.parse(\'2.1\'):\n        ph = phonemize(text, separator=seperator, strip=False, njobs=1, backend=\'espeak\', language=language)\n        ph = ph[:-1].strip() # skip the last empty character\n        # phonemizer does not tackle punctuations. Here we do.\n        # Replace \\n with matching punctuations.\n        if punctuations:\n            # if text ends with a punctuation.\n            if text[-1] == punctuations[-1]:\n                for punct in punctuations[:-1]:\n                    ph = ph.replace(\'| |\\n\', \'|\'+punct+\'| |\', 1)\n                    ph = ph + punctuations[-1]\n            else:\n                for punct in punctuations:\n                    ph = ph.replace(\'| |\\n\', \'|\'+punct+\'| |\', 1)\n    elif version.parse(phonemizer.__version__) >= version.parse(\'2.1\'):\n        ph = phonemize(text, separator=seperator, strip=False, njobs=1, backend=\'espeak\', language=language, preserve_punctuation=True)\n        # this is a simple fix for phonemizer.\n        # https://github.com/bootphon/phonemizer/issues/32\n        if punctuations:\n            for punctuation in punctuations:\n                ph = ph.replace(f""| |{punctuation} "", f""|{punctuation}| |"").replace(f""| |{punctuation}"", f""|{punctuation}| |"")\n            ph = ph[:-3]\n    else:\n        raise RuntimeError("" [!] Use \'phonemizer\' version 2.1 or older."")\n\n    return ph\n\n\ndef pad_with_eos_bos(phoneme_sequence, tp=None):\n    # pylint: disable=global-statement\n    global _phonemes_to_id, _bos, _eos\n    if tp:\n        _bos = tp[\'bos\']\n        _eos = tp[\'eos\']\n        _, _phonemes = make_symbols(**tp)\n        _phonemes_to_id = {s: i for i, s in enumerate(_phonemes)}\n\n    return [_phonemes_to_id[_bos]] + list(phoneme_sequence) + [_phonemes_to_id[_eos]]\n\n\ndef phoneme_to_sequence(text, cleaner_names, language, enable_eos_bos=False, tp=None):\n    # pylint: disable=global-statement\n    global _phonemes_to_id\n    if tp:\n        _, _phonemes = make_symbols(**tp)\n        _phonemes_to_id = {s: i for i, s in enumerate(_phonemes)}\n\n    sequence = []\n    text = text.replace("":"", """")\n    clean_text = _clean_text(text, cleaner_names)\n    to_phonemes = text2phone(clean_text, language)\n    if to_phonemes is None:\n        print(""!! After phoneme conversion the result is None. -- {} "".format(clean_text))\n    # iterate by skipping empty strings - NOTE: might be useful to keep it to have a better intonation.\n    for phoneme in filter(None, to_phonemes.split(\'|\')):\n        sequence += _phoneme_to_sequence(phoneme)\n    # Append EOS char\n    if enable_eos_bos:\n        sequence = pad_with_eos_bos(sequence, tp=tp)\n    return sequence\n\n\ndef sequence_to_phoneme(sequence, tp=None):\n    # pylint: disable=global-statement\n    \'\'\'Converts a sequence of IDs back to a string\'\'\'\n    global _id_to_phonemes\n    result = \'\'\n    if tp:\n        _, _phonemes = make_symbols(**tp)\n        _id_to_phonemes = {i: s for i, s in enumerate(_phonemes)}\n\n    for symbol_id in sequence:\n        if symbol_id in _id_to_phonemes:\n            s = _id_to_phonemes[symbol_id]\n            result += s\n    return result.replace(\'}{\', \' \')\n\n\ndef text_to_sequence(text, cleaner_names, tp=None):\n    \'\'\'Converts a string of text to a sequence of IDs corresponding to the symbols in the text.\n\n      The text can optionally have ARPAbet sequences enclosed in curly braces embedded\n      in it. For example, ""Turn left on {HH AW1 S S T AH0 N} Street.""\n\n      Args:\n        text: string to convert to a sequence\n        cleaner_names: names of the cleaner functions to run the text through\n\n      Returns:\n        List of integers corresponding to the symbols in the text\n    \'\'\'\n    # pylint: disable=global-statement\n    global _symbol_to_id\n    if tp:\n        _symbols, _ = make_symbols(**tp)\n        _symbol_to_id = {s: i for i, s in enumerate(_symbols)}\n\n    sequence = []\n    # Check for curly braces and treat their contents as ARPAbet:\n    while text:\n        m = _CURLY_RE.match(text)\n        if not m:\n            sequence += _symbols_to_sequence(_clean_text(text, cleaner_names))\n            break\n        sequence += _symbols_to_sequence(\n            _clean_text(m.group(1), cleaner_names))\n        sequence += _arpabet_to_sequence(m.group(2))\n        text = m.group(3)\n    return sequence\n\n\ndef sequence_to_text(sequence, tp=None):\n    \'\'\'Converts a sequence of IDs back to a string\'\'\'\n    # pylint: disable=global-statement\n    global _id_to_symbol\n    if tp:\n        _symbols, _ = make_symbols(**tp)\n        _id_to_symbol = {i: s for i, s in enumerate(_symbols)}\n\n    result = \'\'\n    for symbol_id in sequence:\n        if symbol_id in _id_to_symbol:\n            s = _id_to_symbol[symbol_id]\n            # Enclose ARPAbet back in curly braces:\n            if len(s) > 1 and s[0] == \'@\':\n                s = \'{%s}\' % s[1:]\n            result += s\n    return result.replace(\'}{\', \' \')\n\n\ndef _clean_text(text, cleaner_names):\n    for name in cleaner_names:\n        cleaner = getattr(cleaners, name)\n        if not cleaner:\n            raise Exception(\'Unknown cleaner: %s\' % name)\n        text = cleaner(text)\n    return text\n\n\ndef _symbols_to_sequence(syms):\n    return [_symbol_to_id[s] for s in syms if _should_keep_symbol(s)]\n\n\ndef _phoneme_to_sequence(phons):\n    return [_phonemes_to_id[s] for s in list(phons) if _should_keep_phoneme(s)]\n\n\ndef _arpabet_to_sequence(text):\n    return _symbols_to_sequence([\'@\' + s for s in text.split()])\n\n\ndef _should_keep_symbol(s):\n    return s in _symbol_to_id and s not in [\'~\', \'^\', \'_\']\n\n\ndef _should_keep_phoneme(p):\n    return p in _phonemes_to_id and p not in [\'~\', \'^\', \'_\']\n'"
utils/text/cleaners.py,0,"b'\'\'\'\nCleaners are transformations that run over the input text at both training and eval time.\n\nCleaners can be selected by passing a comma-delimited list of cleaner names as the ""cleaners""\nhyperparameter. Some cleaners are English-specific. You\'ll typically want to use:\n  1. ""english_cleaners"" for English text\n  2. ""transliteration_cleaners"" for non-English text that can be transliterated to ASCII using\n     the Unidecode library (https://pypi.python.org/pypi/Unidecode)\n  3. ""basic_cleaners"" if you do not want to transliterate (in this case, you should also update\n     the symbols in symbols.py to match your data).\n\'\'\'\n\nimport re\nfrom unidecode import unidecode\nfrom .number_norm import normalize_numbers\n\n# Regular expression matching whitespace:\n_whitespace_re = re.compile(r\'\\s+\')\n\n# List of (regular expression, replacement) pairs for abbreviations:\n_abbreviations = [(re.compile(\'\\\\b%s\\\\.\' % x[0], re.IGNORECASE), x[1])\n                  for x in [\n                      (\'mrs\', \'misess\'),\n                      (\'mr\', \'mister\'),\n                      (\'dr\', \'doctor\'),\n                      (\'st\', \'saint\'),\n                      (\'co\', \'company\'),\n                      (\'jr\', \'junior\'),\n                      (\'maj\', \'major\'),\n                      (\'gen\', \'general\'),\n                      (\'drs\', \'doctors\'),\n                      (\'rev\', \'reverend\'),\n                      (\'lt\', \'lieutenant\'),\n                      (\'hon\', \'honorable\'),\n                      (\'sgt\', \'sergeant\'),\n                      (\'capt\', \'captain\'),\n                      (\'esq\', \'esquire\'),\n                      (\'ltd\', \'limited\'),\n                      (\'col\', \'colonel\'),\n                      (\'ft\', \'fort\'),\n                  ]]\n\n\ndef expand_abbreviations(text):\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\n\n\ndef expand_numbers(text):\n    return normalize_numbers(text)\n\n\ndef lowercase(text):\n    return text.lower()\n\n\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \' \', text).strip()\n\n\ndef convert_to_ascii(text):\n    return unidecode(text)\n\n\ndef remove_aux_symbols(text):\n    text = re.sub(r\'[\\<\\>\\(\\)\\[\\]\\""]+\', \'\', text)\n    return text\n\n\ndef replace_symbols(text):\n    text = text.replace(\';\', \',\')\n    text = text.replace(\'-\', \' \')\n    text = text.replace(\':\', \' \')\n    text = text.replace(\'&\', \'and\')\n    return text\n\n\ndef basic_cleaners(text):\n    \'\'\'Basic pipeline that lowercases and collapses whitespace without transliteration.\'\'\'\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef transliteration_cleaners(text):\n    \'\'\'Pipeline for non-English text that transliterates to ASCII.\'\'\'\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\n# TODO: elaborate it\ndef basic_turkish_cleaners(text):\n    \'\'\'Pipeline for Turkish text\'\'\'\n    text = text.replace(""I"", ""\xc4\xb1"")\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef english_cleaners(text):\n    \'\'\'Pipeline for English text, including number and abbreviation expansion.\'\'\'\n    text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n\n\ndef phoneme_cleaners(text):\n    \'\'\'Pipeline for phonemes mode, including number and abbreviation expansion.\'\'\'\n    text = convert_to_ascii(text)\n    text = expand_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\n'"
utils/text/cmudict.py,0,"b'# -*- coding: utf-8 -*-\n\nimport re\n\nVALID_SYMBOLS = [\n    \'AA\', \'AA0\', \'AA1\', \'AA2\', \'AE\', \'AE0\', \'AE1\', \'AE2\', \'AH\', \'AH0\', \'AH1\',\n    \'AH2\', \'AO\', \'AO0\', \'AO1\', \'AO2\', \'AW\', \'AW0\', \'AW1\', \'AW2\', \'AY\', \'AY0\',\n    \'AY1\', \'AY2\', \'B\', \'CH\', \'D\', \'DH\', \'EH\', \'EH0\', \'EH1\', \'EH2\', \'ER\', \'ER0\',\n    \'ER1\', \'ER2\', \'EY\', \'EY0\', \'EY1\', \'EY2\', \'F\', \'G\', \'HH\', \'IH\', \'IH0\',\n    \'IH1\', \'IH2\', \'IY\', \'IY0\', \'IY1\', \'IY2\', \'JH\', \'K\', \'L\', \'M\', \'N\', \'NG\',\n    \'OW\', \'OW0\', \'OW1\', \'OW2\', \'OY\', \'OY0\', \'OY1\', \'OY2\', \'P\', \'R\', \'S\', \'SH\',\n    \'T\', \'TH\', \'UH\', \'UH0\', \'UH1\', \'UH2\', \'UW\', \'UW0\', \'UW1\', \'UW2\', \'V\', \'W\',\n    \'Y\', \'Z\', \'ZH\'\n]\n\n\nclass CMUDict:\n    \'\'\'Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\'\'\'\n\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\'latin-1\') as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {\n                word: pron\n                for word, pron in entries.items() if len(pron) == 1\n            }\n        self._entries = entries\n\n    def __len__(self):\n        return len(self._entries)\n\n    def lookup(self, word):\n        \'\'\'Returns list of ARPAbet pronunciations of the given word.\'\'\'\n        return self._entries.get(word.upper())\n\n    @staticmethod\n    def get_arpabet(word, cmudict, punctuation_symbols):\n        first_symbol, last_symbol = \'\', \'\'\n        if word and word[0] in punctuation_symbols:\n            first_symbol = word[0]\n            word = word[1:]\n        if word and word[-1] in punctuation_symbols:\n            last_symbol = word[-1]\n            word = word[:-1]\n        arpabet = cmudict.lookup(word)\n        if arpabet is not None:\n            return first_symbol + \'{%s}\' % arpabet[0] + last_symbol\n        return first_symbol + word + last_symbol\n\n\n_alt_re = re.compile(r\'\\([0-9]+\\)\')\n\n\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if line and (line[0] >= \'A\' and line[0] <= \'Z\' or line[0] == ""\'""):\n            parts = line.split(\'  \')\n            word = re.sub(_alt_re, \'\', parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:\n                    cmudict[word].append(pronunciation)\n                else:\n                    cmudict[word] = [pronunciation]\n    return cmudict\n\n\ndef _get_pronunciation(s):\n    parts = s.strip().split(\' \')\n    for part in parts:\n        if part not in VALID_SYMBOLS:\n            return None\n    return \' \'.join(parts)\n'"
utils/text/number_norm.py,0,"b""import re\n\n_comma_number_re = re.compile(r'([0-9][0-9\\,]+[0-9])')\n_decimal_number_re = re.compile(r'([0-9]+\\.[0-9]+)')\n_pounds_re = re.compile(r'\xc2\xa3([0-9\\,]*[0-9]+)')\n_dollars_re = re.compile(r'\\$([0-9\\.\\,]*[0-9]+)')\n_ordinal_re = re.compile(r'([0-9]+)(st|nd|rd|th)')\n_number_re = re.compile(r'[0-9]+')\n\n_units = [\n    '', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine',\n    'ten', 'eleven', 'twelve', 'thirteen', 'fourteen', 'fifteen', 'sixteen',\n    'seventeen', 'eighteen', 'nineteen'\n]\n\n_tens = [\n    '',\n    'ten',\n    'twenty',\n    'thirty',\n    'forty',\n    'fifty',\n    'sixty',\n    'seventy',\n    'eighty',\n    'ninety',\n]\n\n_digit_groups = [\n    '',\n    'thousand',\n    'million',\n    'billion',\n    'trillion',\n    'quadrillion',\n]\n\n_ordinal_suffixes = [\n    ('one', 'first'),\n    ('two', 'second'),\n    ('three', 'third'),\n    ('five', 'fifth'),\n    ('eight', 'eighth'),\n    ('nine', 'ninth'),\n    ('twelve', 'twelfth'),\n    ('ty', 'tieth'),\n]\n\n\ndef _remove_commas(m):\n    return m.group(1).replace(',', '')\n\n\ndef _expand_decimal_point(m):\n    return m.group(1).replace('.', ' point ')\n\n\ndef _expand_dollars(m):\n    match = m.group(1)\n    parts = match.split('.')\n    if len(parts) > 2:\n        return match + ' dollars'  # Unexpected format\n    dollars = int(parts[0]) if parts[0] else 0\n    cents = int(parts[1]) if len(parts) > 1 and parts[1] else 0\n    if dollars and cents:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s, %s %s' % (dollars, dollar_unit, cents, cent_unit)\n    if dollars:\n        dollar_unit = 'dollar' if dollars == 1 else 'dollars'\n        return '%s %s' % (dollars, dollar_unit)\n    if cents:\n        cent_unit = 'cent' if cents == 1 else 'cents'\n        return '%s %s' % (cents, cent_unit)\n    return 'zero dollars'\n\n\ndef _standard_number_to_words(n, digit_group):\n    parts = []\n    if n >= 1000:\n        # Format next higher digit group.\n        parts.append(_standard_number_to_words(n // 1000, digit_group + 1))\n        n = n % 1000\n\n    if n >= 100:\n        parts.append('%s hundred' % _units[n // 100])\n    if n % 100 >= len(_units):\n        parts.append(_tens[(n % 100) // 10])\n        parts.append(_units[(n % 100) % 10])\n    else:\n        parts.append(_units[n % 100])\n    if n > 0:\n        parts.append(_digit_groups[digit_group])\n    return ' '.join([x for x in parts if x])\n\n\ndef _number_to_words(n):\n    # Handle special cases first, then go to the standard case:\n    if n >= 1000000000000000000:\n        return str(n)  # Too large, just return the digits\n    if n == 0:\n        return 'zero'\n    if n % 100 == 0 and n % 1000 != 0 and n < 3000:\n        return _standard_number_to_words(n // 100, 0) + ' hundred'\n    return _standard_number_to_words(n, 0)\n\n\ndef _expand_number(m):\n    return _number_to_words(int(m.group(0)))\n\n\ndef _expand_ordinal(m):\n    num = _number_to_words(int(m.group(1)))\n    for suffix, replacement in _ordinal_suffixes:\n        if num.endswith(suffix):\n            return num[:-len(suffix)] + replacement\n    return num + 'th'\n\n\ndef normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_pounds_re, r'\\1 pounds', text)\n    text = re.sub(_dollars_re, _expand_dollars, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text\n"""
utils/text/symbols.py,0,"b'# -*- coding: utf-8 -*-\n\'\'\'\nDefines the set of symbols used in text input to the model.\n\nThe default is a set of ASCII characters that works well for English or text that has been run\nthrough Unidecode. For other data, you can modify _characters. See TRAINING_DATA.md for details.\n\'\'\'\ndef make_symbols(characters, phonemes, punctuations=\'!\\\'(),-.:;? \', pad=\'_\', eos=\'~\', bos=\'^\'):# pylint: disable=redefined-outer-name\n    \'\'\' Function to create symbols and phonemes \'\'\'\n    _phonemes_sorted = sorted(list(phonemes))\n\n    # Prepend ""@"" to ARPAbet symbols to ensure uniqueness (some are the same as uppercase letters):\n    _arpabet = [\'@\' + s for s in _phonemes_sorted]\n\n    # Export all symbols:\n    _symbols = [pad, eos, bos] + list(characters) + _arpabet\n    _phonemes = [pad, eos, bos] + list(_phonemes_sorted) + list(punctuations)\n\n    return _symbols, _phonemes\n\n_pad = \'_\'\n_eos = \'~\'\n_bos = \'^\'\n_characters = \'ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz!\\\'(),-.:;? \'\n_punctuations = \'!\\\'(),-.:;? \'\n_phoneme_punctuations = \'.!;:,?\'\n\n# Phonemes definition\n_vowels = \'iy\xc9\xa8\xca\x89\xc9\xafu\xc9\xaa\xca\x8f\xca\x8ae\xc3\xb8\xc9\x98\xc9\x99\xc9\xb5\xc9\xa4o\xc9\x9b\xc5\x93\xc9\x9c\xc9\x9e\xca\x8c\xc9\x94\xc3\xa6\xc9\x90a\xc9\xb6\xc9\x91\xc9\x92\xe1\xb5\xbb\'\n_non_pulmonic_consonants = \'\xca\x98\xc9\x93\xc7\x80\xc9\x97\xc7\x83\xca\x84\xc7\x82\xc9\xa0\xc7\x81\xca\x9b\'\n_pulmonic_consonants = \'pbtd\xca\x88\xc9\x96c\xc9\x9fk\xc9\xa1q\xc9\xa2\xca\x94\xc9\xb4\xc5\x8b\xc9\xb2\xc9\xb3n\xc9\xb1m\xca\x99r\xca\x80\xe2\xb1\xb1\xc9\xbe\xc9\xbd\xc9\xb8\xce\xb2fv\xce\xb8\xc3\xb0sz\xca\x83\xca\x92\xca\x82\xca\x90\xc3\xa7\xca\x9dx\xc9\xa3\xcf\x87\xca\x81\xc4\xa7\xca\x95h\xc9\xa6\xc9\xac\xc9\xae\xca\x8b\xc9\xb9\xc9\xbbj\xc9\xb0l\xc9\xad\xca\x8e\xca\x9f\'\n_suprasegmentals = \'\xcb\x88\xcb\x8c\xcb\x90\xcb\x91\'\n_other_symbols = \'\xca\x8dw\xc9\xa5\xca\x9c\xca\xa2\xca\xa1\xc9\x95\xca\x91\xc9\xba\xc9\xa7\'\n_diacrilics = \'\xc9\x9a\xcb\x9e\xc9\xab\'\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\n\nsymbols, phonemes = make_symbols(_characters, _phonemes, _punctuations, _pad, _eos, _bos)\n\n# Generate ALIEN language\n# from random import shuffle\n# shuffle(phonemes)\n\nif __name__ == \'__main__\':\n    print("" > TTS symbols {}"".format(len(symbols)))\n    print(symbols)\n    print("" > TTS phonemes {}"".format(len(phonemes)))\n    print(phonemes)\n'"
