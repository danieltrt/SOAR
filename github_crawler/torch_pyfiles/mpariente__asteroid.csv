file_path,api_count,code
hubconf.py,0,"b'from asteroid import models\n\ndependencies = [\'torch\']\n\n\ndef conv_tasnet(name_url_or_file=None, *args, **kwargs):\n    """""" Load (pretrained) ConvTasNet model\n\n    Args:\n        name_url_or_file (str): Model name (we\'ll find the URL),\n            model URL to download model, path to model file.\n            If None (default), ConvTasNet is instantiated but no pretrained\n            weights are loaded.\n        *args: Arguments to pass to ConvTasNet.\n        **kwargs: Keyword arguments to pass to ConvTasNet.\n\n    Returns:\n        ConvTasNet instance (with ot without pretrained weights).\n\n    Examples:\n        >>> from torch import hub\n        >>> # Instantiate without pretrained weights\n        >>> model = hub.load(\'mpariente/asteroid\', \'conv_tasnet\', n_src=2)\n        >>> # Use pretrained weights\n        >>> URL = ""TOCOME""\n        >>> model = hub.load(\'mpariente/asteroid\', \'conv_tasnet\', URL)\n    """"""\n    # No pretrained weights\n    if name_url_or_file is None:\n        return models.ConvTasNet(*args, **kwargs)\n    return models.ConvTasNet.from_pretrained(name_url_or_file, *args, **kwargs)\n\n\ndef dprnn_tasnet(name_url_or_file=None, *args, **kwargs):\n    """""" Load (pretrained) DPRNNTasNet model\n\n    Args:\n        name_url_or_file (str): Model name (we\'ll find the URL),\n            model URL to download model, path to model file.\n            If None (default), DPRNNTasNet is instantiated but no pretrained\n            weights are loaded.\n        *args: Arguments to pass to DPRNNTasNet.\n        **kwargs: Keyword arguments to pass to DPRNNTasNet.\n\n    Returns:\n        DPRNNTasNet instance (with ot without pretrained weights).\n\n    Examples:\n        >>> from torch import hub\n        >>> # Instantiate without pretrained weights\n        >>> model = hub.load(\'mpariente/asteroid\', \'dprnn_tasnet\')\n        >>> # Use pretrained weights\n        >>> URL = ""TOCOME""\n        >>> model = hub.load(\'mpariente/asteroid\', \'dprnn_tasnet\', URL)\n    """"""\n    # No pretrained weights\n    if name_url_or_file is None:\n        return models.DPRNNTasNet(*args, **kwargs)\n    return models.DPRNNTasNet.from_pretrained(name_url_or_file)\n'"
setup.py,0,"b'from setuptools import setup, find_packages\n\nasteroid_version = ""0.2.1""\n\nwith open(""README.md"", encoding=\'utf-8\') as fh:\n    long_description = fh.read()\n\nsetup(\n    name=\'asteroid\',\n    version=asteroid_version,\n    author=\'Manuel Pariente\',\n    author_email=\'manuel.pariente@loria.fr\',\n    url=""https://github.com/mpariente/asteroid"",\n    description=\'PyTorch-based audio source separation toolkit\',\n    long_description=long_description,\n    long_description_content_type=""text/markdown"",\n    license=\'MIT\',\n    python_requires=\'>=3.6\',\n    install_requires=[\'numpy\',\n                      \'scipy\',\n                      \'soundfile\',\n                      \'pyyaml\',\n                      \'torch\',\n                      \'pytorch-lightning\',\n                      \'torch_optimizer\',\n                      \'pb_bss_eval\',\n                      \'torch_stoi\',\n                      ],\n    extras_require={\n        \'visualize\': [\'seaborn\'],\n        \'tests\': [\'pytest\'],\n    },\n    entry_points={\n        \'console_scripts\': [\'asteroid-upload=asteroid.scripts.asteroid_cli:upload\'],\n    },\n    packages=find_packages(),\n    include_package_data=True,\n    classifiers=[\n        \'Development Status :: 4 - Beta\',\n        ""Programming Language :: Python :: 3"",\n        \'Programming Language :: Python :: 3.6\',\n        \'Programming Language :: Python :: 3.7\',\n        \'Programming Language :: Python :: 3.8\',\n        ""License :: OSI Approved :: MIT License"",\n        ""Operating System :: OS Independent"",\n    ],\n)'"
asteroid/__init__.py,0,"b""import pathlib\nfrom .utils import deprecation_utils, torch_utils\nfrom .models import ConvTasNet, DPRNNTasNet\n\nproject_root = str(pathlib.Path(__file__).expanduser().absolute().parent.parent)\n__version__ = '0.2.1'\n\n\ndef show_available_models():\n    from .utils.hub_utils import MODELS_URLS_HASHTABLE\n    print(' \\n'.join(list(MODELS_URLS_HASHTABLE.keys())))\n"""
asteroid/metrics.py,0,"b'from .utils import average_arrays_in_dic\nfrom pb_bss_eval import InputMetrics, OutputMetrics\nALL_METRICS = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\', \'pesq\']\n\n\ndef get_metrics(mix, clean, estimate, sample_rate=16000, metrics_list=\'all\',\n                average=True, compute_permutation=False):\n    """""" Get speech separation/enhancement metrics from mix/clean/estimate.\n\n    Args:\n        mix (np.array): \'Shape(D, N)\' or \'Shape(N, )\'.\n        clean (np.array): \'Shape(K_source, N)\' or \'Shape(N, )\'.\n        estimate (np.array): \'Shape(K_target, N)\' or \'Shape(N, )\'.\n        sample_rate (int): sampling rate of the audio clips.\n        metrics_list (Union [str, list]): List of metrics to compute.\n            Defaults to \'all\' ([\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\', \'pesq\']).\n        average (bool): Return dict([float]) if True, else dict([array]).\n        compute_permutation (bool): Whether to compute the permutation on\n            estimate sources for the output metrics (default False)\n\n    Returns:\n        dict: Dictionary with all requested metrics, with `\'input_\'` prefix\n            for metrics at the input (mixture against clean), no prefix at the\n            output (estimate against clean). Output format depends on average.\n\n    Examples:\n        >>> import numpy as np\n        >>> import pprint\n        >>> from asteroid.metrics import get_metrics\n        >>> mix = np.random.randn(1, 16000)\n        >>> clean = np.random.randn(2, 16000)\n        >>> est = np.random.randn(2, 16000)\n        >>> metrics_dict = get_metrics(mix, clean, est, sample_rate=8000,\n        >>>                            metrics_list=\'all\')\n        >>> pprint.pprint(metrics_dict)\n        {\'input_pesq\': 1.924380898475647,\n         \'input_sar\': -11.67667585294225,\n         \'input_sdr\': -14.88667106190552,\n         \'input_si_sdr\': -52.43849784881705,\n         \'input_sir\': -0.10419427290163795,\n         \'input_stoi\': 0.015112115177091223,\n         \'pesq\': 1.7713886499404907,\n         \'sar\': -11.610963379923195,\n         \'sdr\': -14.527246041125844,\n         \'si_sdr\': -46.26557128489802,\n         \'sir\': 0.4799929272243427,\n         \'stoi\': 0.022023073540350643}\n\n    """"""\n    if metrics_list == \'all\':\n        metrics_list = ALL_METRICS\n    if isinstance(metrics_list, str):\n        metrics_list = [metrics_list]\n    # For each utterance, we get a dictionary with the input and output metrics\n    input_metrics = InputMetrics(observation=mix,\n                                 speech_source=clean,\n                                 enable_si_sdr=True,\n                                 sample_rate=sample_rate)\n    utt_metrics = {\'input_\' + n: input_metrics[n] for n in metrics_list}\n\n    output_metrics = OutputMetrics(speech_prediction=estimate,\n                                   speech_source=clean,\n                                   enable_si_sdr=True,\n                                   sample_rate=sample_rate,\n                                   compute_permutation=compute_permutation)\n    utt_metrics.update(output_metrics[metrics_list])\n    if average is True:\n        return average_arrays_in_dic(utt_metrics)\n    else:\n        return utt_metrics\n'"
tests/metrics_test.py,0,"b'import numpy as np\nimport pytest\nfrom asteroid.metrics import get_metrics\n\n\n@pytest.mark.parametrize(""fs"", [8000, 16000])\ndef test_get_metrics(fs):\n    mix = np.random.randn(1, 16000)\n    clean = np.random.randn(2, 16000)\n    est = np.random.randn(2, 16000)\n    metrics_dict = get_metrics(mix, clean, est, sample_rate=fs,\n                               metrics_list=\'si_sdr\')\n    # Test no average & squeezing\n    metrics_dict_bis = get_metrics(mix[0], clean, est, sample_rate=fs,\n                                   metrics_list=\'si_sdr\', average=False)\n    assert float(np.mean(metrics_dict_bis[\'si_sdr\'])) == metrics_dict[\'si_sdr\']\n    assert (float(np.mean(metrics_dict_bis[\'input_si_sdr\'])) ==\n            metrics_dict[\'input_si_sdr\'])\n\n\ndef test_all_metrics():\n    # This is separated because very slow (sdr, pesq, stoi)\n    mix = np.random.randn(1, 4000)\n    clean = np.random.randn(1, 4000)\n    est = np.random.randn(1, 4000)\n    metrics_dict = get_metrics(mix, clean, est, sample_rate=8000,\n                               metrics_list=\'all\')\n\n\ndef test_get_metrics_multichannel():\n    mix = np.random.randn(2, 16000)\n    clean = np.random.randn(2, 16000)\n    est = np.random.randn(2, 16000)\n    metrics_dict_bis = get_metrics(mix, clean, est, sample_rate=8000,\n                               metrics_list=\'si_sdr\', average=False)'"
asteroid/data/__init__.py,0,b'from .wham_dataset import WhamDataset\nfrom .whamr_dataset import WhamRDataset\nfrom .dns_dataset import DNSDataset\nfrom .librimix_dataset import LibriMix\nfrom .wsj0_mix import Wsj0mixDataset\nfrom .musdb18_dataset import MUSDB18Dataset\nfrom .kinect_wsj import KinectWsjMixDataset\n\n'
asteroid/data/avspeech_dataset.py,8,"b'import re\nimport os\nimport cv2\nimport librosa\nimport numpy as np\nfrom pathlib import Path\nimport torch\nfrom torch.utils import data\nfrom torch.nn import functional as F\nimport pandas as pd\nfrom typing import Callable, Tuple, List, Union\nfrom asteroid.filterbanks import Encoder, Decoder, STFTFB, transforms\n\nEPS = 1e-8\n\n\ndef get_frames(video):\n    frame_count = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n    frame_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n    frame_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    buffer_video = np.empty(\n        (frame_count, frame_height, frame_width, 3), np.dtype(""uint8"")\n    )\n\n    frame = 0\n    ret = True\n\n    while frame < frame_count and ret:\n        ret, f = video.read()\n        buffer_video[frame] = cv2.cvtColor(f, cv2.COLOR_BGR2RGB)\n\n        frame += 1\n    video.release()\n    return buffer_video\n\n\nclass Signal:\n    """"""This class holds the video frames and the audio signal.\n\n        Args:\n            video_path (str,Path): Path to video (mp4).\n            audio_path (str,Path): Path to audio (wav).\n            embed_dir (str,Path): Path to directory that stores embeddings.\n            sr (int): sampling rate of audio.\n            video_start_length: video part no. [1]\n            fps (int): fps of video.\n            signal_len (int): length of the signal\n\n        Note:\n            [1]: each video consists of multiple parts which consists of fps*signal_len frames.\n    """"""\n\n    def __init__(\n        self,\n        video_path: Union[str, Path],\n        audio_path: Union[str, Path],\n        embed_dir: Union[str, Path],\n        sr=16_000,\n        video_start_length=0,\n        fps=25,\n        signal_len=3,\n    ):\n        if isinstance(video_path, str):\n            video_path = Path(video_path)\n        if isinstance(audio_path, str):\n            audio_path = Path(audio_path)\n        if isinstance(embed_dir, str):\n            embed_dir = Path(embed_dir)\n\n        self.video_path = video_path\n        self.audio_path = audio_path\n        self.video_start_length = video_start_length\n\n        self.embed_path = None\n        self.embed = None\n        self.embed_dir = embed_dir\n\n        self.fps = fps\n        self.signal_len = signal_len\n        self.sr = sr\n\n        self._load(sr=sr)\n        self._check_video_embed()\n\n    def _load(self, sr: int):\n        self.audio, _ = librosa.load(self.audio_path.as_posix(), sr=sr)\n        self.video = cv2.VideoCapture(self.video_path.as_posix())\n\n    def _check_video_embed(self, embed_ext="".npy""):\n        # convert mp4 location to embedding...\n        video_name_stem = self.video_path.stem\n\n        embed_dir = self.embed_dir\n        if not embed_dir.is_dir():\n            # check embed_dir=""../../dir"" or embed_dir=""dir""\n            embed_dir = Path(*embed_dir.parts[2:])\n\n        self.embed_path = Path(\n            embed_dir, f""{video_name_stem}_part{self.video_start_length}{embed_ext}""\n        )\n        if self.embed_path.is_file():\n            self.embed = np.load(self.embed_path.as_posix())\n        else:\n            raise ValueError(\n                f""Embeddings not found in {self.embed_dir} for {self.video_path} ""\n                f""for part: {self.video_start_length}""\n            )\n\n    def get_embed(self):\n        return self.embed\n\n    def get_audio(self):\n        return self.audio\n\n\nclass AVSpeechDataset(data.Dataset):\n    """"""Audio Visual Speech Separation dataset as described in [1].\n\n        Args:\n            input_df_path (str,Path): path for combination dataset.\n            embed_dir (str,Path): path where embeddings are stored.\n            n_src (int): number of sources.\n\n        References:\n            [1]: \'Looking to Listen at the Cocktail Party:\n            A Speaker-Independent Audio-Visual Model for Speech Separation\' Ephrat et. al\n            https://arxiv.org/abs/1804.03619\n    """"""\n\n    dataset_name = ""AVSpeech""\n\n    def __init__(\n        self, input_df_path: Union[str, Path], embed_dir: Union[str, Path], n_src=2\n    ):\n        if isinstance(input_df_path, str):\n            input_df_path = Path(input_df_path)\n        if isinstance(embed_dir, str):\n            embed_dir = Path(embed_dir)\n\n        self.n_src = n_src\n        self.embed_dir = embed_dir\n        self.input_df = pd.read_csv(input_df_path.as_posix())\n        self.stft_encoder = Encoder(STFTFB(n_filters=512, kernel_size=400, stride=160))\n\n    @staticmethod\n    def encode(x: np.ndarray, p=0.3, stft_encoder=None):\n        if stft_encoder is None:\n            stft_encoder = Encoder(STFTFB(n_filters=512, kernel_size=400, stride=160))\n\n        x = torch.from_numpy(x).float()\n\n        # time domain to time-frequency representation\n        tf_rep = stft_encoder(x).squeeze(0) + EPS\n        # power law on complex numbers\n        tf_rep = (torch.abs(tf_rep) ** p) * torch.sign(tf_rep)\n        return tf_rep\n\n    @staticmethod\n    def decode(tf_rep: np.ndarray, p=0.3, stft_decoder=None, final_len=48000):\n        if stft_decoder is None:\n            stft_decoder = Decoder(STFTFB(n_filters=512, kernel_size=400, stride=160))\n\n        tf_rep = torch.from_numpy(tf_rep).float()\n\n        # power law on complex numbers\n        tf_rep = (torch.abs(tf_rep) ** (1 / p)) * torch.sign(tf_rep)\n        # time domain to time-frequency representation\n        x = stft_decoder(tf_rep)\n\n        length = len(x)\n        if length != final_len:\n            x = F.pad(x, [0, final_len - length])\n        return x\n\n    def __len__(self):\n        return len(self.input_df)\n\n    def __getitem__(self, idx):\n        row = self.input_df.iloc[idx, :]\n        all_signals = []\n\n        for i in range(self.n_src):\n            # get audio, video path from combination dataframe\n            video_path = row.loc[f""video_{i+1}""]\n            audio_path = row.loc[f""audio_{i+1}""]\n\n            # video length is 3-10 seconds, hence, part index can take values 0-2\n            re_match = re.search(r""_part\\d"", audio_path)\n            video_length_idx = 0\n            if re_match:\n                video_length_idx = int(re_match.group(0)[-1])\n\n            signal = Signal(\n                video_path,\n                audio_path,\n                self.embed_dir,\n                video_start_length=video_length_idx,\n            )\n            all_signals.append(signal)\n\n        # input audio signal is the last column.\n        mixed_signal, _ = librosa.load(row.loc[""mixed_audio""], sr=16_000)\n        mixed_signal_tensor = self.encode(mixed_signal, stft_encoder=self.stft_encoder)\n\n        audio_tensors = []\n        video_tensors = []\n\n        for i in range(self.n_src):\n            # audio to spectrogram\n            spectrogram = self.encode(\n                all_signals[i].get_audio(), stft_encoder=self.stft_encoder\n            )\n            audio_tensors.append(spectrogram)\n\n            # get embed\n            embeddings = torch.from_numpy(all_signals[i].get_embed())\n            video_tensors.append(embeddings)\n\n        audio_tensors = torch.stack(audio_tensors)\n\n        return audio_tensors, video_tensors, mixed_signal_tensor\n'"
asteroid/data/dns_dataset.py,4,"b'import torch\nfrom torch.utils import data\nimport json\nimport os\nimport soundfile as sf\n\n\nclass DNSDataset(data.Dataset):\n    dataset_name = \'DNS\'\n\n    def __init__(self, json_dir):\n        super(DNSDataset, self).__init__()\n        self.json_dir = json_dir\n        with open(os.path.join(json_dir, \'file_infos.json\'), \'r\') as f:\n            self.mix_infos = json.load(f)\n\n        self.wav_ids = list(self.mix_infos.keys())\n\n    def __len__(self):\n        return len(self.wav_ids)\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, vstack([source_arrays])\n        """"""\n        utt_info = self.mix_infos[self.wav_ids[idx]]\n        # Load mixture\n        x = torch.from_numpy(sf.read(utt_info[\'mix\'], dtype=\'float32\')[0])\n        # Load clean\n        speech = torch.from_numpy(sf.read(utt_info[\'clean\'],\n                                          dtype=\'float32\')[0])\n        # Load noise\n        noise = torch.from_numpy(sf.read(utt_info[\'noise\'],\n                                         dtype=\'float32\')[0])\n        return x, speech, noise\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self.dataset_name\n        infos[\'task\'] = \'enhancement\'\n        infos[\'licenses\'] = [dns_license]\n        return infos\n\n\ndns_license = dict(\n    title=\'Deep Noise Suppression (DNS) Challenge\',\n    title_link=\'https://github.com/microsoft/DNS-Challenge\',\n    author=\'Microsoft\',\n    author_link=\'https://www.microsoft.com/fr-fr/\',\n    license=\'CC BY-NC 4.0\',\n    license_link=\'https://creativecommons.org/licenses/by-nc/4.0/\',\n    non_commercial=False,\n)\n'"
asteroid/data/kinect_wsj.py,4,"b'import torch\nfrom torch.utils import data\nimport os\nimport numpy as np\nimport soundfile as sf\nfrom .wsj0_mix import Wsj0mixDataset\n\n\ndef make_dataloaders(train_dir, valid_dir, n_src=2, sample_rate=16000,\n                     segment=4.0, batch_size=4, num_workers=None,\n                     **kwargs):\n    num_workers = num_workers if num_workers else batch_size\n    train_set = KinectWsjMixDataset(train_dir, n_src=n_src,\n                                    sample_rate=sample_rate,\n                                    segment=segment)\n    val_set = KinectWsjMixDataset(valid_dir, n_src=n_src,\n                                  sample_rate=sample_rate,\n                                  segment=segment)\n    train_loader = data.DataLoader(train_set, shuffle=True,\n                                   batch_size=batch_size,\n                                   num_workers=num_workers,\n                                   drop_last=True)\n    val_loader = data.DataLoader(val_set, shuffle=True,\n                                 batch_size=batch_size,\n                                 num_workers=num_workers,\n                                 drop_last=True)\n    return train_loader, val_loader\n\n\nclass KinectWsjMixDataset(Wsj0mixDataset):\n    dataset_name = \'Kinect-WSJ\'\n\n    def __init__(self, json_dir, n_src=2, sample_rate=16000, segment=4.0):\n        super().__init__(\n            json_dir, n_src=n_src, sample_rate=sample_rate, segment=segment\n        )\n        noises = []\n        for i in range(len(self.mix)):  \n            path = self.mix[i][0]\n            # Warning: linux specific\n            path_splits = path.split(\'/\')\n            path_splits[-2] = \'noise\'\n            noise_path = \'/\' + os.path.join(*path_splits)\n            noises.append([noise_path, self.mix[i][1]])\n        self.noises = noises\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, stack([source_arrays]), noise\n            mixture is of dimension [samples, channels]\n            sources are of dimension [n_src, samples, channels]\n        """"""\n        # Random start\n        if self.mix[idx][1] == self.seg_len or self.like_test:\n            rand_start = 0\n        else:\n            rand_start = np.random.randint(0, self.mix[idx][1] - self.seg_len)\n        if self.like_test:\n            stop = None\n        else:\n            stop = rand_start + self.seg_len\n        # Load mixture\n        x, _ = sf.read(self.mix[idx][0], start=rand_start, stop=stop,\n                       dtype=\'float32\', always_2d=True)\n        noise, _ = sf.read(self.noises[idx][0], start=rand_start, stop=stop,\n                           dtype=\'float32\', always_2d=True)\n        seg_len = torch.as_tensor([len(x)])\n        # Load sources\n        source_arrays = []\n        for src in self.sources:\n            if src[idx] is None:\n                # Target is filled with zeros if n_src > default_nsrc\n                s = np.zeros_like(x)\n            else:\n                s, _ = sf.read(src[idx][0], start=rand_start,\n                               stop=stop, dtype=\'float32\', always_2d=True)\n            source_arrays.append(s)\n        sources = torch.from_numpy(np.stack(source_arrays))\n        return torch.from_numpy(x), sources, torch.from_numpy(noise)\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = super().get_infos()\n        infos[\'licenses\'].append(chime5_license)\n        return infos\n\n\nchime5_license = dict(\n    title=\'The CHiME-5 speech corpus\',\n    title_link=\'http://spandh.dcs.shef.ac.uk/chime_challenge/CHiME5/index.html\',\n    author=\'Jon Barker, Shinji Watanabe and Emmanuel Vincent\',\n    author_link=\'http://spandh.dcs.shef.ac.uk/chime_challenge/chime2018/contact.html\',\n    license=\'CHiME-5 data licence - non-commercial 1.00\',\n    license_link=\'https://licensing.sheffield.ac.uk/i/data/chime5.html\',\n    non_commercial=True\n)\n'"
asteroid/data/librimix_dataset.py,3,"b'import numpy as np\nimport pandas as pd\nimport soundfile as sf\nimport torch\nfrom torch import hub\nfrom torch.utils.data import Dataset\nimport random as random\nimport os\nimport shutil\nimport zipfile\n\nfrom .wham_dataset import wham_noise_license\nMINI_URL = \'https://zenodo.org/record/3871592/files/MiniLibriMix.zip?download=1\'\n\n\nclass LibriMix(Dataset):\n    """""""" Dataset class for Librimix source separation tasks.\n\n    Args:\n        csv_dir (str): The path to the metatdata file\n        task (str): One of ``\'enh_single\'``, ``\'enh_both\'``, ``\'sep_clean\'`` or\n            ``\'sep_noisy\'``.\n\n            * ``\'enh_single\'`` for single speaker speech enhancement.\n            * ``\'enh_both\'`` for multi speaker speech enhancement.\n            * ``\'sep_clean\'`` for two-speaker clean source separation.\n            * ``\'sep_noisy\'`` for two-speaker noisy source separation.\n\n        sample_rate (int) : The sample rate of the sources and mixtures\n        n_src (int) : The number of sources in the mixture\n        segment (int) : The desired sources and mixtures length in s\n    """"""\n    dataset_name = \'LibriMix\'\n\n    def __init__(self, csv_dir, task=\'sep_clean\', sample_rate=16000, n_src=2,\n                 segment=3):\n        self.csv_dir = csv_dir\n        self.task = task\n        # Get the csv corresponding to the task\n        if task == \'enh_single\':\n            md_file = [f for f in os.listdir(csv_dir) if \'single\' in f][0]\n            self.csv_path = os.path.join(self.csv_dir, md_file)\n        elif task == \'enh_both\':\n            md_file = [f for f in os.listdir(csv_dir) if \'both\' in f][0]\n            self.csv_path = os.path.join(self.csv_dir, md_file)\n            md_clean_file = [f for f in os.listdir(csv_dir) if \'clean\' in f][0]\n            self.df_clean = pd.read_csv(os.path.join(csv_dir, md_clean_file))\n        elif task == \'sep_clean\':\n            md_file = [f for f in os.listdir(csv_dir) if \'clean\' in f][0]\n            self.csv_path = os.path.join(self.csv_dir, md_file)\n        elif task == \'sep_noisy\':\n            md_file = [f for f in os.listdir(csv_dir) if \'both\' in f][0]\n            self.csv_path = os.path.join(self.csv_dir, md_file)\n        self.segment = segment\n        self.sample_rate = sample_rate\n        # Open csv file\n        self.df = pd.read_csv(self.csv_path)\n        # Get rid of the utterances too short\n        if self.segment is not None:\n            max_len = len(self.df)\n            self.seg_len = int(self.segment * self.sample_rate)\n            # Ignore the file shorter than the desired_length\n            self.df = self.df[self.df[\'length\'] >= self.seg_len]\n            print(f""Drop {max_len - len(self.df)} utterances from {max_len} ""\n                  f""(shorter than {segment} seconds)"")\n        else:\n            self.seg_len = None\n        self.n_src = n_src\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # Get the row in dataframe\n        row = self.df.iloc[idx]\n        # Get mixture path\n        self.mixture_path = row[\'mixture_path\']\n        sources_list = []\n        # If there is a seg start point is set randomly\n        if self.seg_len is not None:\n            start = random.randint(0, row[\'length\'] - self.seg_len)\n            stop = start + self.seg_len\n        else:\n            start = 0\n            stop = None\n        # If task is enh_both then the source is the clean mixture\n        if \'enh_both\' in self.task:\n            mix_clean_path = self.df_clean.iloc[idx][\'mixture_path\']\n            s, _ = sf.read(mix_clean_path, dtype=\'float32\', start=start,\n                           stop=stop)\n            sources_list.append(s)\n\n        else:\n            # Read sources\n            for i in range(self.n_src):\n                source_path = row[f\'source_{i + 1}_path\']\n                s, _ = sf.read(source_path, dtype=\'float32\', start=start,\n                               stop=stop)\n                sources_list.append(s)\n        # Read the mixture\n        mixture, _ = sf.read(self.mixture_path, dtype=\'float32\', start=start,\n                             stop=stop)\n        # Convert to torch tensor\n        mixture = torch.from_numpy(mixture)\n        # Stack sources\n        sources = np.vstack(sources_list)\n        # Convert sources to tensor\n        sources = torch.from_numpy(sources)\n        return mixture, sources\n\n    @classmethod\n    def mini_from_download(cls, **kwargs):\n        """""" Downloads MiniLibriMix and returns train and validation Dataset.\n        If you want to instantiate the Dataset by yourself, call\n        `mini_download` that returns the path to the path to the metadata files.\n\n        Args:\n            **kwargs: keyword arguments to pass the `LibriMix`, see `__init__`.\n                The kwargs will be fed to both the training set and validation\n                set\n\n        Returns:\n            train_set, val_set: training and validation instances of\n                `LibriMix` (data.Dataset).\n\n        Examples:\n            >>> from asteroid.data import LibriMix\n            >>> train_set, val_set = LibriMix.mini_from_download(task=\'sep_clean\')\n        """"""\n        # kwargs checks\n        assert \'csv_dir\' not in kwargs, \'Cannot specify csv_dir when downloading.\'\n        assert kwargs.get(\'task\', \'sep_clean\') in [\'sep_clean\', \'sep_noisy\'], (\n            \'Only clean and noisy separation are supported in MiniLibriMix.\'\n        )\n        assert kwargs.get(\'sample_rate\', 8000) == 8000, (\n            \'Only 8kHz sample rate is supported in MiniLibriMix.\'\n        )\n        # Download LibriMix in current directory\n        meta_path = cls.mini_download()\n        # Create dataset instances\n        train_set = cls(os.path.join(meta_path, \'train\'), sample_rate=8000, **kwargs)\n        val_set = cls(os.path.join(meta_path, \'val\'), sample_rate=8000, **kwargs)\n        return train_set, val_set\n\n    @staticmethod\n    def mini_download():\n        """""" Downloads MiniLibriMix from Zenodo in current directory\n\n        Returns:\n            The path to the metadata directory.\n        """"""\n        mini_dir = \'./MiniLibriMix/\'\n        os.makedirs(mini_dir, exist_ok=True)\n        # Download zip (or cached)\n        zip_path = mini_dir + \'MiniLibriMix.zip\'\n        if not os.path.isfile(zip_path):\n            hub.download_url_to_file(MINI_URL, zip_path)\n        # Unzip zip\n        cond = all([os.path.isdir(\'MiniLibriMix/\' + f)\n                    for f in [\'train\', \'val\', \'metadata\']])\n        if not cond:\n            with zipfile.ZipFile(zip_path, \'r\') as zip_ref:\n                zip_ref.extractall(\'./\')  # Will unzip in MiniLibriMix\n        # Reorder metadata\n        src = \'MiniLibriMix/metadata/\'\n        for mode in [\'train\', \'val\']:\n            dst = f\'MiniLibriMix/metadata/{mode}/\'\n            os.makedirs(dst, exist_ok=True)\n            [shutil.copyfile(src + f, dst + f) for f in os.listdir(src)\n             if mode in f and os.path.isfile(src + f)]\n        return \'./MiniLibriMix/metadata\'\n\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self._dataset_name()\n        infos[\'task\'] = self.task\n        if self.task == \'sep_clean\':\n            data_license = [librispeech_license]\n        else:\n            data_license = [librispeech_license, wham_noise_license]\n        infos[\'licenses\'] = data_license\n        return infos\n\n    def _dataset_name(self):\n        """""" Differentiate between 2 and 3 sources.""""""\n        return f\'Libri{self.n_src}Mix\'\n\n\nlibrispeech_license = dict(\n    title=\'LibriSpeech ASR corpus\',\n    title_link=\'http://www.openslr.org/12\',\n    author=\'Vassil Panayotov\',\n    author_link=\'https://github.com/vdp\',\n    license=\'CC BY 4.0\',\n    license_link=\'https://creativecommons.org/licenses/by/4.0/\',\n    non_commercial=False\n)\n'"
asteroid/data/musdb18_dataset.py,5,"b'from pathlib import Path\nimport torch.utils.data\nimport random\nimport torch\nimport tqdm\nimport soundfile as sf\n\n\nclass MUSDB18Dataset(torch.utils.data.Dataset):\n    """"""MUSDB18 music separation dataset\n\n    The dataset consists of 150 full lengths music tracks (~10h duration) of\n    different genres along with their isolated stems:\n        `drums`, `bass`, `vocals` and `others`.\n\n    Out-of-the-box, asteroid does only support MUSDB18-HQ which comes as\n    uncompressed WAV files. To use the MUSDB18, please convert it to WAV first:\n\n    MUSDB18 HQ: https://zenodo.org/record/3338373   \n    MUSDB18     https://zenodo.org/record/1117372 \n\n    Note: The datasets are hosted on Zenodo and require that users\n          request access, since the tracks can only be used for\n          academic purposes. We manually check this requests.\n\n    This dataset asssumes music tracks in (sub)folders where each folder\n    has a fixed number of sources (defaults to 4). For each track, a list\n    of `sources` and a common `suffix` can be specified.\n    A linear mix is performed on the fly by summing up the sources\n\n    Due to the fact that all tracks comprise the exact same set\n    of sources, random track mixing can be used can be used,\n    where sources from different tracks are mixed together.\n\n    Folder Structure:\n        train/1/vocals.wav ---------------\\\n        train/1/drums.wav -----------------+--> input (mix), output[target]\n        train/1/bass.wav ------------------|\n        train/1/other.wav ----------------/\n\n    Args:\n        root (str): Root path of dataset\n        sources (:obj:`list` of :obj:`str`, optional): List of source names\n            that composes the mixture.\n            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n        targets (list or None, optional): List of source names to be used as\n            targets. If None, a dict with the 4 stems is returned.\n             If e.g [`vocals`, `drums`], a tensor with stacked `vocals` and\n             `drums` is returned instead of a dict. Defaults to None.\n        suffix (str, optional): Filename suffix, defaults to `.wav`.\n        split (str, optional): Dataset subfolder, defaults to `train`.\n        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n            list of tracks to be loaded, defaults to `None` (loads all tracks).\n        segment (float, optional): Duration of segments in seconds,\n            defaults to ``None`` which loads the full-length audio tracks.\n        samples_per_track (int, optional):\n            Number of samples yielded from each track, can be used to increase \n            dataset size, defaults to `1`.\n        random_segments (boolean, optional): Enables random offset for track segments.\n        random_track_mix boolean: enables mixing of random sources from\n            different tracks to assemble mix.\n        source_augmentations (:obj:`list` of `obj`:`callable`):\n            list of augmentation function names,\n            defaults to no-op augmentations (input = output)\n        sample_rate (int, optional): Samplerate of files in dataset.\n\n    Attributes:\n        root (str): Root path of dataset\n        sources (:obj:`list` of :obj:`str`, optional): List of source names.\n            Defaults to MUSDB18 4 stem scenario: `vocals`, `drums`, `bass`, `other`.\n        suffix (str, optional): Filename suffix, defaults to `.wav`.\n        split (str, optional): Dataset subfolder, defaults to `train`.\n        subset (:obj:`list` of :obj:`str`, optional): Selects a specific of\n            list of tracks to be loaded, defaults to `None` (loads all tracks).\n        segment (float, optional): Duration of segments in seconds,\n            defaults to ``None`` which loads the full-length audio tracks.\n        samples_per_track (int, optional):\n            Number of samples yielded from each track, can be used to increase\n            dataset size, defaults to `1`.\n        random_segments (boolean, optional): Enables random offset for track segments.\n        random_track_mix boolean: enables mixing of random sources from\n            different tracks to assemble mix.\n        source_augmentations (:obj:`list` of `obj`:`callable`):\n            list of augmentation function names,\n            defaults to no-op augmentations (input = output)\n        sample_rate (int, optional): Samplerate of files in dataset.\n        tracks (:obj:`list` of :obj:`Dict`): List of track metadata\n\n    """"""\n    dataset_name = \'MUSDB18\'\n\n    def __init__(self,\n                 root,\n                 sources=[\'vocals\', \'bass\', \'drums\', \'other\'],\n                 targets=None,\n                 suffix=\'.wav\',\n                 split=\'train\',\n                 subset=None,\n                 segment=None,\n                 samples_per_track=1,\n                 random_segments=False,\n                 random_track_mix=False,\n                 source_augmentations=lambda audio: audio,\n                 sample_rate=44100):\n\n        self.root = Path(root).expanduser()\n        self.split = split\n        self.sample_rate = sample_rate\n        self.segment = segment\n        self.random_track_mix = random_track_mix\n        self.random_segments = random_segments\n        self.source_augmentations = source_augmentations\n        self.sources = sources\n        self.targets = targets\n        self.suffix = suffix\n        self.subset = subset\n        self.samples_per_track = samples_per_track\n        self.tracks = list(self.get_tracks())\n        if not self.tracks:\n            raise RuntimeError(""No tracks found."")\n\n    def __getitem__(self, index):\n        # assemble the mixture of target and interferers\n        audio_sources = {}\n\n        # get track_id\n        track_id = index // self.samples_per_track\n        if self.random_segments:\n            start = random.uniform(\n                0, self.tracks[track_id][\'min_duration\'] - self.segment\n            )\n        else:\n            start = 0\n\n        # load sources\n        for source in self.sources:\n            # optionally select a random track for each source\n            if self.random_track_mix:\n                # load a different track\n                track_id = random.choice(range(len(self.tracks)))\n                if self.random_segments:\n                    start = random.uniform(\n                        0, self.tracks[track_id][\'min_duration\'] - self.segment\n                    )\n\n            # loads the full track duration\n            start_sample = int(start * self.sample_rate)\n            # check if dur is none\n            if self.segment:\n                # stop in soundfile is calc in samples, not seconds\n                stop_sample = start_sample + int(\n                    self.segment * self.sample_rate\n                )\n            else:\n                # set to None for reading complete file\n                stop_sample = None\n\n            # load actual audio\n            audio, _ = sf.read(\n                Path(\n                    self.tracks[track_id][\'path\'] / source\n                ).with_suffix(self.suffix),\n                always_2d=True,\n                start=start_sample,\n                stop=stop_sample\n            )\n            # convert to torch tensor\n            audio = torch.tensor(audio.T, dtype=torch.float)\n            # apply source-wise augmentations\n            audio = self.source_augmentations(audio)\n            audio_sources[source] = audio\n\n        # apply linear mix over source index=0\n        audio_mix = torch.stack(list(audio_sources.values())).sum(0)\n        if self.targets:\n            audio_sources = torch.stack([\n                wav for src, wav in audio_sources.items() if src in self.targets\n            ], dim=0)\n        return audio_mix, audio_sources\n\n    def __len__(self):\n        return len(self.tracks) * self.samples_per_track\n\n    def get_tracks(self):\n        """"""Loads input and output tracks""""""\n        p = Path(self.root, self.split)\n        for track_path in tqdm.tqdm(p.iterdir()):\n            if track_path.is_dir():\n                if self.subset and track_path.stem not in self.subset:\n                    # skip this track\n                    continue\n\n                source_paths = [\n                    track_path / (s + self.suffix) for s in self.sources\n                ]\n                if not all(sp.exists() for sp in source_paths):\n                    print(\n                        ""Exclude track due to non-existing source"",\n                        track_path\n                    )\n                    continue\n\n                # get metadata\n                infos = list(map(sf.info, source_paths))\n                if not all(\n                    i.samplerate == self.sample_rate for i in infos\n                ):\n                    print(\n                        ""Exclude track due to different sample rate "",\n                        track_path\n                    )\n                    continue\n\n                if self.segment is not None:\n                    # get minimum duration of track\n                    min_duration = min(i.duration for i in infos)\n                    if min_duration > self.segment:\n                        yield({\n                            \'path\': track_path,\n                            \'min_duration\': min_duration\n                        })\n                else:\n                    yield({\n                        \'path\': track_path,\n                        \'min_duration\': None\n                    })\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self.dataset_name\n        infos[\'task\'] = \'enhancement\'\n        infos[\'licenses\'] = [musdb_license]\n        return infos\n\n\nmusdb_license = dict(\n)\n'"
asteroid/data/wham_dataset.py,4,"b'import torch\nfrom torch.utils import data\nimport json\nimport os\nimport numpy as np\nimport soundfile as sf\nfrom .wsj0_mix import wsj0_license\nEPS = 1e-8\n\nDATASET = \'WHAM\'\n# WHAM tasks\nenh_single = {\'mixture\': \'mix_single\',\n              \'sources\': [\'s1\'],\n              \'infos\': [\'noise\'],\n              \'default_nsrc\': 1}\nenh_both = {\'mixture\': \'mix_both\',\n            \'sources\': [\'mix_clean\'],\n            \'infos\': [\'noise\'],\n            \'default_nsrc\': 1}\nsep_clean = {\'mixture\': \'mix_clean\',\n             \'sources\': [\'s1\', \'s2\'],\n             \'infos\': [],\n             \'default_nsrc\': 2}\nsep_noisy = {\'mixture\': \'mix_both\',\n             \'sources\': [\'s1\', \'s2\'],\n             \'infos\': [\'noise\'],\n             \'default_nsrc\': 2}\n\nWHAM_TASKS = {\'enhance_single\': enh_single,\n              \'enhance_both\': enh_both,\n              \'sep_clean\': sep_clean,\n              \'sep_noisy\': sep_noisy}\n# Aliases.\nWHAM_TASKS[\'enh_single\'] = WHAM_TASKS[\'enhance_single\']\nWHAM_TASKS[\'enh_both\'] = WHAM_TASKS[\'enhance_both\']\n\n\ndef normalize_tensor_wav(wav_tensor, eps=1e-8, std=None):\n    mean = wav_tensor.mean(-1, keepdim=True)\n    if std is None:\n        std = wav_tensor.std(-1, keepdim=True)\n    return (wav_tensor - mean) / (std + eps)\n\n\nclass WhamDataset(data.Dataset):\n    """""" Dataset class for WHAM source separation and speech enhancement tasks.\n\n    Args:\n        json_dir (str): The path to the directory containing the json files.\n        task (str): One of ``\'enh_single\'``, ``\'enh_both\'``, ``\'sep_clean\'`` or\n            ``\'sep_noisy\'``.\n\n            * ``\'enh_single\'`` for single speaker speech enhancement.\n            * ``\'enh_both\'`` for multi speaker speech enhancement.\n            * ``\'sep_clean\'`` for two-speaker clean source separation.\n            * ``\'sep_noisy\'`` for two-speaker noisy source separation.\n\n        sample_rate (int, optional): The sampling rate of the wav files.\n        segment (float, optional): Length of the segments used for training,\n            in seconds. If None, use full utterances (e.g. for test).\n        nondefault_nsrc (int, optional): Number of sources in the training\n            targets.\n            If None, defaults to one for enhancement tasks and two for\n            separation tasks.\n        normalize_audio (bool): If True then both sources and the mixture are\n            normalized with the standard deviation of the mixture.\n    """"""\n    dataset_name = \'WHAM!\'\n\n    def __init__(self, json_dir, task, sample_rate=8000, segment=4.0,\n                 nondefault_nsrc=None, normalize_audio=False):\n        super(WhamDataset, self).__init__()\n        if task not in WHAM_TASKS.keys():\n            raise ValueError(\'Unexpected task {}, expected one of \'\n                             \'{}\'.format(task, WHAM_TASKS.keys()))\n        # Task setting\n        self.json_dir = json_dir\n        self.task = task\n        self.task_dict = WHAM_TASKS[task]\n        self.sample_rate = sample_rate\n        self.normalize_audio = normalize_audio\n        self.seg_len = None if segment is None else int(segment * sample_rate)\n        if not nondefault_nsrc:\n            self.n_src = self.task_dict[\'default_nsrc\']\n        else:\n            assert nondefault_nsrc >= self.task_dict[\'default_nsrc\']\n            self.n_src = nondefault_nsrc\n        self.like_test = self.seg_len is None\n        # Load json files\n        mix_json = os.path.join(json_dir, self.task_dict[\'mixture\'] + \'.json\')\n        sources_json = [os.path.join(json_dir, source + \'.json\') for\n                        source in self.task_dict[\'sources\']]\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        sources_infos = []\n        for src_json in sources_json:\n            with open(src_json, \'r\') as f:\n                sources_infos.append(json.load(f))\n        # Filter out short utterances only when segment is specified\n        orig_len = len(mix_infos)\n        drop_utt, drop_len = 0, 0\n        if not self.like_test:\n            for i in range(len(mix_infos) - 1, -1, -1):  # Go backward\n                if mix_infos[i][1] < self.seg_len:\n                    drop_utt += 1\n                    drop_len += mix_infos[i][1]\n                    del mix_infos[i]\n                    for src_inf in sources_infos:\n                        del src_inf[i]\n\n        print(""Drop {} utts({:.2f} h) from {} (shorter than {} samples)"".format(\n            drop_utt, drop_len/sample_rate/36000, orig_len, self.seg_len))\n        self.mix = mix_infos\n        # Handle the case n_src > default_nsrc\n        while len(sources_infos) < self.n_src:\n            sources_infos.append([None for _ in range(len(self.mix))])\n        self.sources = sources_infos\n\n    def __add__(self, wham):\n        if self.n_src != wham.n_src:\n            raise ValueError(\'Only datasets having the same number of sources\'\n                             \'can be added together. Received \'\n                             \'{} and {}\'.format(self.n_src, wham.n_src))\n        if self.seg_len != wham.seg_len:\n            self.seg_len = min(self.seg_len, wham.seg_len)\n            print(\'Segment length mismatched between the two Dataset\'\n                  \'passed one the smallest to the sum.\')\n        self.mix = self.mix + wham.mix\n        self.sources = [a + b for a, b in zip(self.sources, wham.sources)]\n\n    def __len__(self):\n        return len(self.mix)\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, vstack([source_arrays])\n        """"""\n        # Random start\n        if self.mix[idx][1] == self.seg_len or self.like_test:\n            rand_start = 0\n        else:\n            rand_start = np.random.randint(0, self.mix[idx][1] - self.seg_len)\n        if self.like_test:\n            stop = None\n        else:\n            stop = rand_start + self.seg_len\n        # Load mixture\n        x, _ = sf.read(self.mix[idx][0], start=rand_start,\n                       stop=stop, dtype=\'float32\')\n        seg_len = torch.as_tensor([len(x)])\n        # Load sources\n        source_arrays = []\n        for src in self.sources:\n            if src[idx] is None:\n                # Target is filled with zeros if n_src > default_nsrc\n                s = np.zeros((seg_len, ))\n            else:\n                s, _ = sf.read(src[idx][0], start=rand_start,\n                               stop=stop, dtype=\'float32\')\n            source_arrays.append(s)\n        sources = torch.from_numpy(np.vstack(source_arrays))\n        mixture = torch.from_numpy(x)\n\n        if self.normalize_audio:\n            m_std = mixture.std(-1, keepdim=True)\n            mixture = normalize_tensor_wav(mixture, eps=EPS, std=m_std)\n            sources = normalize_tensor_wav(sources, eps=EPS, std=m_std)\n        return mixture, sources\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self.dataset_name\n        infos[\'task\'] = self.task\n        if self.task == \'sep_clean\':\n            data_license = [wsj0_license]\n        else:\n            data_license = [wsj0_license, wham_noise_license]\n        infos[\'licenses\'] = data_license\n        return infos\n\n\nwham_noise_license = dict(\n    title=\'The WSJ0 Hipster Ambient Mixtures dataset\',\n    title_link=\'http://wham.whisper.ai/\',\n    author=\'Whisper.ai\',\n    author_link=\'https://whisper.ai/\',\n    license=\'CC BY-NC 4.0\',\n    license_link=\'https://creativecommons.org/licenses/by-nc/4.0/\',\n    non_commercial=True,\n)\n'"
asteroid/data/whamr_dataset.py,4,"b'import torch\nfrom torch.utils import data\nimport json\nimport os\nimport numpy as np\nimport soundfile as sf\nfrom .wsj0_mix import wsj0_license\nfrom .wham_dataset import wham_noise_license\n\nDATASET = \'WHAMR\'\n\n# WHAMR tasks\n# Many tasks can be considered with this dataset, we only consider the 4 core\n# separation tasks presented in the paper for now.\nsep_clean = {\'mixture\': \'mix_clean_anechoic\',\n             \'sources\': [\'s1_anechoic\', \'s2_anechoic\'],\n             \'infos\': [],\n             \'default_nsrc\': 2}\nsep_noisy = {\'mixture\': \'mix_both_anechoic\',\n             \'sources\': [\'s1_anechoic\', \'s2_anechoic\'],\n             \'infos\': [\'noise\'],\n             \'default_nsrc\': 2}\nsep_reverb = {\'mixture\': \'mix_clean_reverb\',\n              \'sources\': [\'s1_anechoic\', \'s2_anechoic\'],\n              \'infos\': [],\n              \'default_nsrc\': 2}\nsep_reverb_noisy = {\'mixture\': \'mix_both_reverb\',\n                    \'sources\': [\'s1_anechoic\', \'s2_anechoic\'],\n                    \'infos\': [\'noise\'],\n                    \'default_nsrc\': 2}\n\nWHAMR_TASKS = {\'sep_clean\': sep_clean,\n               \'sep_noisy\': sep_noisy,\n               \'sep_reverb\': sep_reverb,\n               \'sep_reverb_noisy\': sep_reverb_noisy}\n# Support both order, confusion is easy\nWHAMR_TASKS[\'sep_noisy_reverb\'] = WHAMR_TASKS[\'sep_reverb_noisy\']\n\n\nclass WhamRDataset(data.Dataset):\n    """""" Dataset class for WHAMR source separation and speech enhancement tasks.\n\n    Args:\n        json_dir (str): The path to the directory containing the json files.\n        task (str): One of ``\'sep_clean\'``, ``\'sep_noisy\'``, ``\'sep_reverb\'``\n            or ``\'sep_reverb_noisy\'``.\n\n            * ``\'sep_clean\'`` for two-speaker clean (anechoic) source\n                separation.\n            * ``\'sep_noisy\'`` for two-speaker noisy (anechoic) source\n                separation.\n            * ``\'sep_reverb\'`` for two-speaker clean reverberant\n                source separation.\n            * ``\'sep_reverb_noisy\'`` for two-speaker noisy reverberant source\n                separation.\n\n        sample_rate (int, optional): The sampling rate of the wav files.\n        segment (float, optional): Length of the segments used for training,\n            in seconds. If None, use full utterances (e.g. for test).\n        nondefault_nsrc (int, optional): Number of sources in the training\n            targets.\n            If None, defaults to one for enhancement tasks and two for\n            separation tasks.\n    """"""\n    dataset_name = \'WHAMR!\'\n\n    def __init__(self, json_dir, task, sample_rate=8000, segment=4.0,\n                 nondefault_nsrc=None):\n        super(WhamRDataset, self).__init__()\n        if task not in WHAMR_TASKS.keys():\n            raise ValueError(\'Unexpected task {}, expected one of \'\n                             \'{}\'.format(task, WHAMR_TASKS.keys()))\n        # Task setting\n        self.json_dir = json_dir\n        self.task = task\n        self.task_dict = WHAMR_TASKS[task]\n        self.sample_rate = sample_rate\n        self.seg_len = None if segment is None else int(segment * sample_rate)\n        if not nondefault_nsrc:\n            self.n_src = self.task_dict[\'default_nsrc\']\n        else:\n            assert nondefault_nsrc >= self.task_dict[\'default_nsrc\']\n            self.n_src = nondefault_nsrc\n        self.like_test = self.seg_len is None\n        # Load json files\n        mix_json = os.path.join(json_dir, self.task_dict[\'mixture\'] + \'.json\')\n        sources_json = [os.path.join(json_dir, source + \'.json\') for\n                        source in self.task_dict[\'sources\']]\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        sources_infos = []\n        for src_json in sources_json:\n            with open(src_json, \'r\') as f:\n                sources_infos.append(json.load(f))\n        # Filter out short utterances only when segment is specified\n        orig_len = len(mix_infos)\n        drop_utt, drop_len = 0, 0\n        if not self.like_test:\n            for i in range(len(mix_infos) - 1, -1, -1):  # Go backward\n                if mix_infos[i][1] < self.seg_len:\n                    drop_utt += 1\n                    drop_len += mix_infos[i][1]\n                    del mix_infos[i]\n                    for src_inf in sources_infos:\n                        del src_inf[i]\n\n        print(""Drop {} utts({:.2f} h) from {} (shorter than {} samples)"".format(\n            drop_utt, drop_len/sample_rate/36000, orig_len, self.seg_len))\n        self.mix = mix_infos\n        # Handle the case n_src > default_nsrc\n        while len(sources_infos) < self.n_src:\n            sources_infos.append([None for _ in range(len(self.mix))])\n        self.sources = sources_infos\n\n    def __add__(self, wham):\n        if self.n_src != wham.n_src:\n            raise ValueError(\'Only datasets having the same number of sources\'\n                             \'can be added together. Received \'\n                             \'{} and {}\'.format(self.n_src, wham.n_src))\n        if self.seg_len != wham.seg_len:\n            self.seg_len = min(self.seg_len, wham.seg_len)\n            print(\'Segment length mismatched between the two Dataset\'\n                  \'passed one the smallest to the sum.\')\n        self.mix = self.mix + wham.mix\n        self.sources = [a + b for a, b in zip(self.sources, wham.sources)]\n\n    def __len__(self):\n        return len(self.mix)\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, vstack([source_arrays])\n        """"""\n        # Random start\n        if self.mix[idx][1] == self.seg_len or self.like_test:\n            rand_start = 0\n        else:\n            rand_start = np.random.randint(0, self.mix[idx][1] - self.seg_len)\n        if self.like_test:\n            stop = None\n        else:\n            stop = rand_start + self.seg_len\n        # Load mixture\n        x, _ = sf.read(self.mix[idx][0], start=rand_start,\n                       stop=stop, dtype=\'float32\')\n        seg_len = torch.as_tensor([len(x)])\n        # Load sources\n        source_arrays = []\n        for src in self.sources:\n            if src[idx] is None:\n                # Target is filled with zeros if n_src > default_nsrc\n                s = np.zeros((seg_len, ))\n            else:\n                s, _ = sf.read(src[idx][0], start=rand_start,\n                               stop=stop, dtype=\'float32\')\n            source_arrays.append(s)\n        sources = torch.from_numpy(np.vstack(source_arrays))\n        return torch.from_numpy(x), sources\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self.dataset_name\n        infos[\'task\'] = self.task\n        if self.task == \'sep_clean\':\n            data_license = [wsj0_license]\n        else:\n            data_license = [wsj0_license, wham_noise_license]\n        infos[\'licenses\'] = data_license\n        return infos'"
asteroid/data/wsj0_mix.py,4,"b'import torch\nfrom torch.utils import data\nimport json\nimport os\nimport numpy as np\nimport soundfile as sf\n\n\ndef make_dataloaders(train_dir, valid_dir, n_src=2, sample_rate=8000,\n                     segment=4.0, batch_size=4, num_workers=None,\n                     **kwargs):\n    num_workers = num_workers if num_workers else batch_size\n    train_set = Wsj0mixDataset(train_dir, n_src=n_src,\n                               sample_rate=sample_rate,\n                               segment=segment)\n    val_set = Wsj0mixDataset(valid_dir, n_src=n_src,\n                             sample_rate=sample_rate,\n                             segment=segment)\n    train_loader = data.DataLoader(train_set, shuffle=True,\n                                   batch_size=batch_size,\n                                   num_workers=num_workers,\n                                   drop_last=True)\n    val_loader = data.DataLoader(val_set, shuffle=True,\n                                 batch_size=batch_size,\n                                 num_workers=num_workers,\n                                 drop_last=True)\n    return train_loader, val_loader\n\n\nclass Wsj0mixDataset(data.Dataset):\n    dataset_name = \'wsj0-mix\'\n\n    def __init__(self, json_dir, n_src=2, sample_rate=8000, segment=4.0):\n        super().__init__()\n        # Task setting\n        self.json_dir = json_dir\n        self.sample_rate = sample_rate\n        if segment is None:\n            self.seg_len = None\n        else:\n            self.seg_len = int(segment * sample_rate)\n        self.n_src = n_src\n        self.like_test = self.seg_len is None\n        # Load json files\n        mix_json = os.path.join(json_dir, \'mix.json\')\n        sources_json = [os.path.join(json_dir, source + \'.json\') for\n                        source in [f""s{n+1}"" for n in range(n_src)]]\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        sources_infos = []\n        for src_json in sources_json:\n            with open(src_json, \'r\') as f:\n                sources_infos.append(json.load(f))\n        # Filter out short utterances only when segment is specified\n        orig_len = len(mix_infos)\n        drop_utt, drop_len = 0, 0\n        if not self.like_test:\n            for i in range(len(mix_infos) - 1, -1, -1):  # Go backward\n                if mix_infos[i][1] < self.seg_len:\n                    drop_utt += 1\n                    drop_len += mix_infos[i][1]\n                    del mix_infos[i]\n                    for src_inf in sources_infos:\n                        del src_inf[i]\n\n        print(""Drop {} utts({:.2f} h) from {} (shorter than {} samples)"".format(\n            drop_utt, drop_len/sample_rate/36000, orig_len, self.seg_len))\n        self.mix = mix_infos\n        self.sources = sources_infos\n\n    def __len__(self):\n        return len(self.mix)\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, vstack([source_arrays])\n        """"""\n        # Random start\n        if self.mix[idx][1] == self.seg_len or self.like_test:\n            rand_start = 0\n        else:\n            rand_start = np.random.randint(0, self.mix[idx][1] - self.seg_len)\n        if self.like_test:\n            stop = None\n        else:\n            stop = rand_start + self.seg_len\n        # Load mixture\n        x, _ = sf.read(self.mix[idx][0], start=rand_start,\n                       stop=stop, dtype=\'float32\')\n        seg_len = torch.as_tensor([len(x)])\n        # Load sources\n        source_arrays = []\n        for src in self.sources:\n            if src[idx] is None:\n                # Target is filled with zeros if n_src > default_nsrc\n                s = np.zeros((seg_len, ))\n            else:\n                s, _ = sf.read(src[idx][0], start=rand_start,\n                               stop=stop, dtype=\'float32\')\n            source_arrays.append(s)\n        sources = torch.from_numpy(np.vstack(source_arrays))\n        return torch.from_numpy(x), sources\n\n    def get_infos(self):\n        """""" Get dataset infos (for publishing models).\n\n        Returns:\n            dict, dataset infos with keys `dataset`, `task` and `licences`.\n        """"""\n        infos = dict()\n        infos[\'dataset\'] = self.dataset_name\n        infos[\'task\'] = \'sep_clean\'\n        infos[\'licenses\'] = [wsj0_license]\n        return infos\n\n\nwsj0_license = dict(\n    title=\'CSR-I (WSJ0) Complete\',\n    title_link=\'https://catalog.ldc.upenn.edu/LDC93S6A\',\n    author=\'LDC\',\n    author_link=\'https://www.ldc.upenn.edu/\',\n    license=\'LDC User Agreement for Non-Members\',\n    license_link=\'https://catalog.ldc.upenn.edu/license/ldc-non-members-agreement.pdf\',\n    non_commercial=True\n)\n'"
asteroid/engine/__init__.py,0,b'from .system import System\n'
asteroid/engine/optimizers.py,6,"b'from torch.optim.optimizer import Optimizer\nfrom torch.optim import (\n    Adam, RMSprop, SGD, Adadelta, Adagrad, Adamax, AdamW, ASGD\n)\nfrom torch_optimizer import (\n    AccSGD, AdaBound, AdaMod, DiffGrad, Lamb, NovoGrad, PID, QHAdam,\n    QHM, RAdam, SGDW, Yogi, Ranger, RangerQH, RangerVA\n)\n\n\ndef make_optimizer(params, optimizer=\'adam\', **kwargs):\n    """"""\n\n    Args:\n        params (iterable): Output of `nn.Module.parameters()`.\n        optimizer (str or :class:`torch.optim.Optimizer`): Identifier understood\n            by :func:`~.get`.\n        **kwargs (dict): keyword arguments for the optimizer.\n\n    Returns:\n        torch.optim.Optimizer\n    Examples:\n        >>> from torch import nn\n        >>> model = nn.Sequential(nn.Linear(10, 10))\n        >>> optimizer = make_optimizer(model.parameters(), optimizer=\'sgd\',\n        >>>                            lr=1e-3)\n    """"""\n    return get(optimizer)(params, **kwargs)\n\n\ndef get(identifier):\n    """""" Returns an optimizer function from a string. Returns its input if it\n    is callable (already a :class:`torch.optim.Optimizer` for example).\n\n    Args:\n        identifier (str or Callable): the optimizer identifier.\n\n    Returns:\n        :class:`torch.optim.Optimizer` or None\n    """"""\n    if isinstance(identifier, Optimizer):\n        return identifier\n    elif isinstance(identifier, str):\n        to_get = {k.lower(): v for k, v in globals().items()}\n        cls = to_get.get(identifier.lower())\n        if cls is None:\n            raise ValueError(f\'Could not interpret optimizer : {str(identifier)}\')\n        return cls\n    raise ValueError(f\'Could not interpret optimizer : {str(identifier)}\')\n'"
asteroid/engine/system.py,13,"b'import torch\nimport pytorch_lightning as pl\nfrom argparse import Namespace\n\nfrom ..utils import flatten_dict\n\n\nclass System(pl.LightningModule):\n    """""" Base class for deep learning systems.\n    Contains a model, an optimizer, a loss function, training and validation\n    dataloaders and learning rate scheduler.\n\n    Args:\n        model (torch.nn.Module): Instance of model.\n        optimizer (torch.optim.Optimizer): Instance or list of optimizers.\n        loss_func (callable): Loss function with signature\n            (est_targets, targets).\n        train_loader (torch.utils.data.DataLoader): Training dataloader.\n        val_loader (torch.utils.data.DataLoader): Validation dataloader.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Instance, or list\n            of learning rate schedulers.\n        config: Anything to be saved with the checkpoints during training.\n            The config dictionary to re-instantiate the run for example.\n    .. note:: By default, `training_step` (used by `pytorch-lightning` in the\n        training loop) and `validation_step` (used for the validation loop)\n        share `common_step`. If you want different behavior for the training\n        loop and the validation loop, overwrite both `training_step` and\n        `validation_step` instead.\n    """"""\n    def __init__(self, model, optimizer, loss_func, train_loader,\n                 val_loader=None, scheduler=None, config=None):\n        super().__init__()\n        self.model = model\n        self.optimizer = optimizer\n        self.loss_func = loss_func\n        self.train_loader = train_loader\n        self.val_loader = val_loader\n        self.scheduler = scheduler\n        config = {} if config is None else config\n        self.config = config\n        # hparams will be logged to Tensorboard as text variables.\n        # torch doesn\'t support None in the summary writer for now, convert\n        # None to strings temporarily.\n        # See https://github.com/pytorch/pytorch/issues/33140\n        self.hparams = Namespace(**self.config_to_hparams(config))\n\n    def forward(self, *args, **kwargs):\n        """""" Applies forward pass of the model.\n\n        Returns:\n            :class:`torch.Tensor`\n        """"""\n        return self.model(*args, **kwargs)\n\n    def common_step(self, batch, batch_nb, train=True):\n        """""" Common forward step between training and validation.\n\n        The function of this method is to unpack the data given by the loader,\n        forward the batch through the model and compute the loss.\n        Pytorch-lightning handles all the rest.\n\n        Args:\n            batch: the object returned by the loader (a list of torch.Tensor\n                in most cases) but can be something else.\n            batch_nb (int): The number of the batch in the epoch.\n            train (bool): Whether in training mode. Needed only if the training\n                and validation steps are fundamentally different, otherwise,\n                pytorch-lightning handles the usual differences.\n\n        Returns:\n            :class:`torch.Tensor` : The loss value on this batch.\n\n        .. note:: This is typically the method to overwrite when subclassing\n            `System`. If the training and validation steps are somehow\n            different (except for loss.backward() and optimzer.step()),\n            the argument `train` can be used to switch behavior.\n            Otherwise, `training_step` and `validation_step` can be overwriten.\n        """"""\n        inputs, targets = batch\n        est_targets = self(inputs)\n        loss = self.loss_func(est_targets, targets)\n        return loss\n\n    def training_step(self, batch, batch_nb):\n        """""" Pass data through the model and compute the loss.\n\n        Backprop is **not** performed (meaning PL will do it for you).\n\n        Args:\n            batch: the object returned by the loader (a list of torch.Tensor\n                in most cases) but can be something else.\n            batch_nb (int): The number of the batch in the epoch.\n\n        Returns:\n            dict:\n\n            ``\'loss\'``: loss\n\n            ``\'log\'``: dict with tensorboard logs\n\n        """"""\n        loss = self.common_step(batch, batch_nb, train=True)\n        tensorboard_logs = {\'train_loss\': loss}\n        return {\'loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_step(self, batch, batch_nb):\n        """""" Need to overwrite PL validation_step to do validation.\n\n        Args:\n            batch: the object returned by the loader (a list of torch.Tensor\n                in most cases) but can be something else.\n            batch_nb (int): The number of the batch in the epoch.\n\n        Returns:\n            dict:\n\n            ``\'val_loss\'``: loss\n        """"""\n        loss = self.common_step(batch, batch_nb, train=False)\n        return {\'val_loss\': loss}\n\n    def validation_epoch_end(self, outputs):\n        """""" How to aggregate outputs of `validation_step` for logging.\n\n        Args:\n           outputs (list[dict]): List of validation losses, each with a\n           ``\'val_loss\'`` key\n\n        Returns:\n            dict: Average loss\n\n            ``\'val_loss\'``: Average loss on `outputs`\n\n            ``\'log\'``: Tensorboard logs\n\n            ``\'progress_bar\'``: Tensorboard logs\n        """"""\n        avg_loss = torch.stack([x[\'val_loss\'] for x in outputs]).mean()\n        tensorboard_logs = {\'val_loss\': avg_loss}\n        return {\'val_loss\': avg_loss, \'log\': tensorboard_logs,\n                \'progress_bar\': tensorboard_logs}\n\n    def unsqueeze_if_dp_or_ddp(self, *values):\n        """""" Apply unsqueeze(0) to all values if training is done with dp\n            or ddp. Unused now.""""""\n        if self.trainer.use_dp or self.trainer.use_ddp2:\n            values = [v.unsqueeze(0) for v in values]\n        if len(values) == 1:\n            return values[0]\n        return values\n\n    def configure_optimizers(self):\n        """""" Required by pytorch-lightning. """"""\n        if self.scheduler is not None:\n            return [self.optimizer], [self.scheduler]\n        return self.optimizer\n\n    def train_dataloader(self):\n        return self.train_loader\n\n    def val_dataloader(self):\n        return self.val_loader\n\n    def on_save_checkpoint(self, checkpoint):\n        """""" Overwrite if you want to save more things in the checkpoint.""""""\n        checkpoint[\'training_config\'] = self.config\n        return checkpoint\n\n    def on_batch_start(self, batch):\n        """""" Overwrite if needed. Called by pytorch-lightning""""""\n        pass\n\n    def on_batch_end(self):\n        """""" Overwrite if needed. Called by pytorch-lightning""""""\n        pass\n\n    def on_epoch_start(self):\n        """""" Overwrite if needed. Called by pytorch-lightning""""""\n        pass\n\n    def on_epoch_end(self):\n        """""" Overwrite if needed. Called by pytorch-lightning""""""\n        pass\n\n    @staticmethod\n    def config_to_hparams(dic):\n        """""" Sanitizes the config dict to be handled correctly by torch\n        SummaryWriter. It flatten the config dict, converts `None` to\n         ``\'None\'`` and any list and tuple into torch.Tensors.\n\n        Args:\n            dic (dict): Dictionary to be transformed.\n\n        Returns:\n            dict: Transformed dictionary.\n        """"""\n        dic = flatten_dict(dic)\n        for k, v in dic.items():\n            if v is None:\n                dic[k] = str(v)\n            elif isinstance(v, (list, tuple)):\n                dic[k] = torch.Tensor(v)\n        return dic\n'"
asteroid/filterbanks/__init__.py,0,"b'from .analytic_free_fb import AnalyticFreeFB\nfrom .free_fb import FreeFB\nfrom .param_sinc_fb import ParamSincFB\nfrom .stft_fb import STFTFB\nfrom .enc_dec import Filterbank, Encoder, Decoder\nfrom .griffin_lim import griffin_lim, misi\nfrom .multiphase_gammatone_fb import MultiphaseGammatoneFB\n\n\ndef make_enc_dec(fb_name, n_filters, kernel_size, stride=None,\n                 who_is_pinv=None, **kwargs):\n    """""" Creates congruent encoder and decoder from the same filterbank family.\n\n    Args:\n        fb_name (str, className): Filterbank family from which to make encoder\n            and decoder. To choose among [``\'free\'``, ``\'analytic_free\'``,\n            ``\'param_sinc\'``, ``\'stft\'``]. Can also be a class defined in a\n            submodule in this subpackade (e.g. :class:`~.FreeFB`).\n        n_filters (int): Number of filters.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution.\n            If None (default), set to ``kernel_size // 2``.\n        who_is_pinv (str, optional): If `None`, no pseudo-inverse filters will\n            be used. If string (among [``\'encoder\'``, ``\'decoder\'``]), decides\n            which of ``Encoder`` or ``Decoder`` will be the pseudo inverse of\n            the other one.\n        **kwargs: Arguments which will be passed to the filterbank class\n            additionally to the usual `n_filters`, `kernel_size` and `stride`.\n            Depends on the filterbank family.\n    Returns:\n        :class:`.Encoder`, :class:`.Decoder`\n    """"""\n    fb_class = get(fb_name)\n\n    if who_is_pinv in [\'dec\', \'decoder\']:\n        fb = fb_class(n_filters, kernel_size, stride=stride, **kwargs)\n        enc = Encoder(fb)\n        # Decoder filterbank is pseudo inverse of encoder filterbank.\n        dec = Decoder.pinv_of(fb)\n    elif who_is_pinv in [\'enc\', \'encoder\']:\n        fb = fb_class(n_filters, kernel_size, stride=stride, **kwargs)\n        dec = Decoder(fb)\n        # Encoder filterbank is pseudo inverse of decoder filterbank.\n        enc = Encoder.pinv_of(fb)\n    else:\n        fb = fb_class(n_filters, kernel_size, stride=stride, **kwargs)\n        enc = Encoder(fb)\n        # Filters between encoder and decoder should not be shared.\n        fb = fb_class(n_filters, kernel_size, stride=stride, **kwargs)\n        dec = Decoder(fb)\n    return enc, dec\n\n\ndef get(identifier):\n    """""" Returns a filterbank class from a string. Returns its input if it\n    is callable (already a :class:`.Filterbank` for example).\n\n    Args:\n        identifier (str or Callable or None): the filterbank identifier.\n\n    Returns:\n        :class:`.Filterbank` or None\n    """"""\n    if identifier is None:\n        return None\n    elif callable(identifier):\n        return identifier\n    elif isinstance(identifier, str):\n        cls = globals().get(identifier)\n        if cls is None:\n            raise ValueError(\'Could not interpret filterbank identifier: \' +\n                             str(identifier))\n        return cls\n    else:\n        raise ValueError(\'Could not interpret filterbank identifier: \' +\n                         str(identifier))\n\n\n# Aliases.\nfree = FreeFB\nanalytic_free = AnalyticFreeFB\nparam_sinc = ParamSincFB\nstft = STFTFB\nmultiphase_gammatone = mpgtf = MultiphaseGammatoneFB\n\n# For the docs\n__all__ = [\'Filterbank\', \'Encoder\', \'Decoder\', \'FreeFB\', \'STFTFB\',\n           \'AnalyticFreeFB\', \'ParamSincFB\', \'MultiphaseGammatoneFB\']\n'"
asteroid/filterbanks/analytic_free_fb.py,6,"b'import torch\nimport torch.nn as nn\nimport numpy as np\nimport warnings\nfrom .enc_dec import Filterbank\n\n\nclass AnalyticFreeFB(Filterbank):\n    """""" Free analytic (fully learned with analycity constraints) filterbank.\n    For more details, see [1].\n\n    Args:\n        n_filters (int): Number of filters. Half of `n_filters` will\n            have parameters, the other half will be the hilbert transforms.\n            `n_filters` should be even.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution.\n            If None (default), set to ``kernel_size // 2``.\n\n    Attributes:\n        n_feats_out (int): Number of output filters.\n\n    References:\n        [1] : ""Filterbank design for end-to-end speech separation"".\n        Submitted to ICASSP 2020. Manuel Pariente, Samuele Cornell,\n        Antoine Deleforge, Emmanuel Vincent.\n    """"""\n    def __init__(self, n_filters, kernel_size, stride=None, **kwargs):\n        super(AnalyticFreeFB, self).__init__(n_filters, kernel_size,\n                                             stride=stride)\n        self.cutoff = int(n_filters // 2)\n        self.n_feats_out = 2 * self.cutoff\n        if n_filters % 2 != 0:\n            print(\'If the number of filters `n_filters` is odd, the \'\n                  \'output size of the layer will be `n_filters - 1`.\')\n\n        self._filters = nn.Parameter(torch.ones(n_filters // 2, 1, kernel_size),\n                                     requires_grad=True)\n        for p in self.parameters():\n            nn.init.xavier_normal_(p, gain=1./np.sqrt(2.))\n\n    @property\n    def filters(self):\n        ft_f = torch.rfft(self._filters, 1, normalized=True)\n        hft_f = torch.stack([ft_f[:, :, :, 1], - ft_f[:, :, :, 0]], dim=-1)\n        hft_f = torch.irfft(hft_f, 1, normalized=True,\n                            signal_sizes=(self.kernel_size, ))\n        return torch.cat([self._filters, hft_f], dim=0)\n'"
asteroid/filterbanks/enc_dec.py,5,"b'import warnings\nimport torch\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass Filterbank(nn.Module):\n    """""" Base Filterbank class.\n    Each subclass has to implement a `filters` property.\n\n    Args:\n        n_filters (int): Number of filters.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the conv or transposed conv. (Hop size).\n            If None (default), set to ``kernel_size // 2``.\n\n    Attributes:\n        n_feats_out (int): Number of output filters.\n    """"""\n    def __init__(self, n_filters, kernel_size, stride=None):\n        super(Filterbank, self).__init__()\n        self.n_filters = n_filters\n        self.kernel_size = kernel_size\n        self.stride = stride if stride else self.kernel_size // 2\n        # If not specified otherwise in the filterbank\'s init, output\n        # number of features is equal to number of required filters.\n        self.n_feats_out = n_filters\n\n    @property\n    def filters(self):\n        """""" Abstract method for filters. """"""\n        raise NotImplementedError\n\n    def get_config(self):\n        """""" Returns dictionary of arguments to re-instantiate the class. """"""\n        config = {\n            \'fb_name\': self.__class__.__name__,\n            \'n_filters\': self.n_filters,\n            \'kernel_size\': self.kernel_size,\n            \'stride\': self.stride\n        }\n        return config\n\n\nclass _EncDec(nn.Module):\n    """""" Base private class for Encoder and Decoder.\n\n    Common parameters and methods.\n\n    Args:\n        filterbank (:class:`Filterbank`): Filterbank instance. The filterbank\n            to use as an encoder or a decoder.\n        is_pinv (bool): Whether to be the pseudo inverse of filterbank.\n\n    Attributes:\n        filterbank (:class:`Filterbank`)\n        stride (int)\n        is_pinv (bool)\n    """"""\n    def __init__(self, filterbank, is_pinv=False):\n        super(_EncDec, self).__init__()\n        self.filterbank = filterbank\n        self.stride = self.filterbank.stride\n        self.is_pinv = is_pinv\n\n    @property\n    def filters(self):\n        return self.filterbank.filters\n\n    def compute_filter_pinv(self, filters):\n        """""" Computes pseudo inverse filterbank of given filters.""""""\n        scale = self.filterbank.stride / self.filterbank.kernel_size\n        shape = filters.shape\n        ifilt = torch.pinverse(filters.squeeze()).transpose(-1, -2).view(shape)\n        # Compensate for the overlap-add.\n        return ifilt * scale\n\n    def get_filters(self):\n        """""" Returns filters or pinv filters depending on `is_pinv` attribute """"""\n        if self.is_pinv:\n            return self.compute_filter_pinv(self.filters)\n        else:\n            return self.filters\n\n    def get_config(self):\n        """""" Returns dictionary of arguments to re-instantiate the class.""""""\n        config = {\'is_pinv\': self.is_pinv}\n        base_config = self.filterbank.get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nclass Encoder(_EncDec):\n    """""" Encoder class.\n\n    Add encoding methods to Filterbank classes.\n    Not intended to be subclassed.\n\n    Args:\n        filterbank (:class:`Filterbank`): The filterbank to use\n            as an encoder.\n        is_pinv (bool): Whether to be the pseudo inverse of filterbank.\n        as_conv1d (bool): Whether to behave like nn.Conv1d.\n            If True (default), forwarding input with shape (batch, 1, time)\n            will output a tensor of shape (batch, freq, conv_time).\n            If False, will output a tensor of shape (batch, 1, freq, conv_time).\n        padding (int): Zero-padding added to both sides of the input.\n\n    Notes:\n        (time, ) --> (freq, conv_time)\n        (batch, time) --> (batch, freq, conv_time)  # Avoid\n        if as_conv1d:\n            (batch, 1, time) --> (batch, freq, conv_time)\n            (batch, chan, time) --> (batch, chan, freq, conv_time)\n        else:\n            (batch, chan, time) --> (batch, chan, freq, conv_time)\n        (batch, any, dim, time) --> (batch, any, dim, freq, conv_time)\n\n    """"""\n    def __init__(self, filterbank, is_pinv=False, as_conv1d=True, padding=0):\n        super(Encoder, self).__init__(filterbank, is_pinv=is_pinv)\n        self.as_conv1d = as_conv1d\n        self.n_feats_out = self.filterbank.n_feats_out\n        self.padding = padding\n\n    @classmethod\n    def pinv_of(cls, filterbank, **kwargs):\n        """""" Returns an :class:`~.Encoder`, pseudo inverse of a\n        :class:`~.Filterbank` or :class:`~.Decoder`.""""""\n        if isinstance(filterbank, Filterbank):\n            return cls(filterbank, is_pinv=True, **kwargs)\n        elif isinstance(filterbank, Decoder):\n            return cls(filterbank.filterbank, is_pinv=True, **kwargs)\n\n    def forward(self, waveform):\n        """""" Convolve 1D torch.Tensor with the filters from a filterbank.""""""\n        filters = self.get_filters()\n        if waveform.ndim == 1:\n            # Assumes 1D input with shape (time,)\n            # Output will be (freq, conv_time)\n            return F.conv1d(waveform[None, None], filters,\n                            stride=self.stride, padding=self.padding).squeeze()\n        elif waveform.ndim == 2:\n            # Assume 2D input with shape (batch or channels, time)\n            # Output will be (batch or channels, freq, conv_time)\n            warnings.warn(""Input tensor was 2D. Applying the corresponding ""\n                          ""Decoder to the current output will result in a 3D ""\n                          ""tensor. This behaviours was introduced to match ""\n                          ""Conv1D and ConvTranspose1D, please use 3D inputs ""\n                          ""to avoid it. For example, this can be done with ""\n                          ""input_tensor.unsqueeze(1)."")\n            return F.conv1d(waveform.unsqueeze(1), filters,\n                            stride=self.stride, padding=self.padding)\n        elif waveform.ndim == 3:\n            batch, channels, time_len = waveform.shape\n            if channels == 1 and self.as_conv1d:\n                # That\'s the common single channel case (batch, 1, time)\n                # Output will be (batch, freq, stft_time), behaves as Conv1D\n                return F.conv1d(waveform, filters, stride=self.stride,\n                                padding=self.padding)\n            else:\n                # Return batched convolution, input is (batch, 3, time),\n                # output will be (batch, 3, freq, conv_time).\n                # Useful for multichannel transforms\n                # If as_conv1d is false, (batch, 1, time) will output\n                # (batch, 1, freq, conv_time), useful for consistency.\n                return self.batch_1d_conv(waveform, filters)\n        else:  # waveform.ndim > 3\n            # This is to compute ""multi""multichannel convolution.\n            # Input can be (*, time), output will be (*, freq, conv_time)\n            return self.batch_1d_conv(waveform, filters)\n\n    def batch_1d_conv(self, inp, filters):\n        # Here we perform multichannel / multi-source convolution. Ou\n        # Output should be (batch, channels, freq, conv_time)\n        batched_conv = F.conv1d(inp.view(-1, 1, inp.shape[-1]),\n                                filters, stride=self.stride,\n                                padding=self.padding)\n        output_shape = inp.shape[:-1] + batched_conv.shape[-2:]\n        return batched_conv.view(output_shape)\n\n\nclass Decoder(_EncDec):\n    """""" Decoder class.\n    \n    Add decoding methods to Filterbank classes.\n    Not intended to be subclassed.\n\n    Args:\n        filterbank (:class:`Filterbank`): The filterbank to use as an decoder.\n        is_pinv (bool): Whether to be the pseudo inverse of filterbank.\n        padding (int): Zero-padding added to both sides of the input.\n        output_padding (int): Additional size added to one side of the\n            output shape.\n\n    Notes\n        `padding` and `output_padding` arguments are directly passed to\n        F.conv_transpose1d.\n    """"""\n    def __init__(self, filterbank, is_pinv=False, padding=0, output_padding=0):\n        super().__init__(filterbank, is_pinv=is_pinv)\n        self.padding = padding\n        self.output_padding = output_padding\n\n    @classmethod\n    def pinv_of(cls, filterbank):\n        """""" Returns an Decoder, pseudo inverse of a filterbank or Encoder.""""""\n        if isinstance(filterbank, Filterbank):\n            return cls(filterbank, is_pinv=True)\n        elif isinstance(filterbank, Encoder):\n            return cls(filterbank.filterbank, is_pinv=True)\n\n    def forward(self, spec):\n        """""" Applies transposed convolution to a TF representation.\n\n        This is equivalent to overlap-add.\n\n        Args:\n            spec (:class:`torch.Tensor`): 3D or 4D Tensor. The TF\n                representation. (Output of :func:`Encoder.forward`).\n        Returns:\n            :class:`torch.Tensor`: The corresponding time domain signal.\n        """"""\n        filters = self.get_filters()\n        if spec.ndim == 2:\n            # Input is (freq, conv_time), output is (time)\n            return F.conv_transpose1d(\n                spec.unsqueeze(0),\n                filters,\n                stride=self.stride,\n                padding=self.padding,\n                output_padding=self.output_padding\n            ).squeeze()\n        if spec.ndim == 3:\n            # Input is (batch, freq, conv_time), output is (batch, 1, time)\n            return F.conv_transpose1d(spec, filters, stride=self.stride,\n                                      padding=self.padding,\n                                      output_padding=self.output_padding)\n        elif spec.ndim > 3:\n            # Multiply all the left dimensions together and group them in the\n            # batch. Make the convolution and restore.\n            view_as = (-1,) + spec.shape[-2:]\n            out = F.conv_transpose1d(spec.view(view_as),\n                                     filters, stride=self.stride,\n                                     padding=self.padding,\n                                     output_padding=self.output_padding)\n            return out.view(spec.shape[:-2] + (-1,))\n'"
asteroid/filterbanks/free_fb.py,2,"b'import torch\nimport torch.nn as nn\nfrom .enc_dec import Filterbank\n\n\nclass FreeFB(Filterbank):\n    """""" Free filterbank without any constraints. Equivalent to\n    :class:`nn.Conv1d`.\n\n    Args:\n        n_filters (int): Number of filters.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution.\n            If None (default), set to ``kernel_size // 2``.\n\n    Attributes:\n        n_feats_out (int): Number of output filters.\n\n    References:\n        [1] : ""Filterbank design for end-to-end speech separation"".\n        Submitted to ICASSP 2020. Manuel Pariente, Samuele Cornell,\n        Antoine Deleforge, Emmanuel Vincent.\n    """"""\n    def __init__(self, n_filters, kernel_size, stride=None, **kwargs):\n        super(FreeFB, self).__init__(n_filters, kernel_size, stride=stride)\n        self._filters = nn.Parameter(torch.ones(n_filters, 1, kernel_size))\n        for p in self.parameters():\n            nn.init.xavier_normal_(p)\n\n    @property\n    def filters(self):\n        return self._filters\n\n'"
asteroid/filterbanks/griffin_lim.py,12,"b'import torch\nimport math\n\nfrom . import Encoder, Decoder, STFTFB\nfrom .stft_fb import perfect_synthesis_window\nfrom . import transforms\nfrom ..masknn.consistency import mixture_consistency\n\n\ndef griffin_lim(mag_specgram, stft_enc, angles=None, istft_dec=None, n_iter=6,\n                momentum=0.9):\n    """""" Estimates matching phase from magnitude spectogram using the\n    \'fast\' Griffin Lim algorithm [1].\n\n    Args:\n        mag_specgram (torch.Tensor): (any, dim, ension, freq, frames) as\n            returned by `Encoder(STFTFB)`, the magnitude spectrogram to be\n            inverted.\n        stft_enc (Encoder[STFTFB]): The `Encoder(STFTFB())` object that was\n            used to compute the input `mag_spec`.\n        angles (None or Tensor): Angles to use to initialize the algorithm.\n            If None (default), angles are init with uniform ditribution.\n        istft_dec (None or Decoder[STFTFB]): Optional Decoder to use to get\n            back to the time domain. If None (default), a perfect\n            reconstruction Decoder is built from `stft_enc`.\n        n_iter (int): Number of griffin-lim iterations to run.\n        momentum (float): The momentum of fast Griffin-Lim. Original\n            Griffin-Lim is obtained for momentum=0.\n\n    Returns:\n        torch.Tensor: estimated waveforms of shape (any, dim, ension, time).\n\n    Examples:\n        >>> stft = Encoder(STFTFB(n_filters=256, kernel_size=256, stride=128))\n        >>> wav = torch.randn(2, 1, 8000)\n        >>> spec = stft(wav)\n        >>> masked_spec = spec * torch.sigmoid(torch.randn_like(spec))\n        >>> mag = transforms.take_mag(masked_spec, -2)\n        >>> est_wav = griffin_lim(mag, stft, n_iter=32)\n\n    References:\n        [1] Perraudin et al. ""A fast Griffin-Lim algorithm,"" WASPAA 2013.\n        [2] D. W. Griffin and J. S. Lim:  ""Signal estimation from modified\n        short-time Fourier transform,"" ASSP 1984.\n\n    """"""\n    # We can create perfect iSTFT from STFT Encoder\n    if istft_dec is None:\n        # Compute window for perfect resynthesis\n        syn_win = perfect_synthesis_window(stft_enc.filterbank.window,\n                                           stft_enc.stride)\n        istft_dec = Decoder(STFTFB(**stft_enc.get_config(), window=syn_win))\n\n    # If no intitial phase is provided initialize uniformly\n    if angles is None:\n        angles = 2 * math.pi * torch.rand_like(mag_specgram,\n                                               device=mag_specgram.device)\n    else:\n        angles = angles.view(*mag_specgram.shape)\n\n    # Initialize rebuilt (useful to use momentum)\n    rebuilt = 0.\n    for _ in range(n_iter):\n        prev_built = rebuilt\n        # Go to the time domain\n        complex_specgram = transforms.from_mag_and_phase(mag_specgram, angles)\n        waveform = istft_dec(complex_specgram)\n        # And back to TF domain\n        rebuilt = stft_enc(waveform)\n        # Update phase estimates (with momentum)\n        diff = rebuilt - momentum / (1 + momentum) * prev_built\n        angles = transforms.angle(diff)\n\n    final_complex_spec = transforms.from_mag_and_phase(mag_specgram, angles)\n    return istft_dec(final_complex_spec)\n\n\ndef misi(mixture_wav, mag_specgrams, stft_enc, angles=None, istft_dec=None,\n         n_iter=6, momentum=0., src_weights=None, dim=1):\n    """""" Jointly estimates matching phase from magnitude spectograms using the\n    Multiple Input Spectrogram Inversion (MISI) algorithm [1].\n\n    Args:\n        mixture_wav (torch.Tensor): (batch, time)\n        mag_specgrams (torch.Tensor): (batch, n_src, freq, frames) as\n            returned by `Encoder(STFTFB)`, the magnitude spectrograms to be\n            jointly inverted using MISI (modified or not).\n        stft_enc (Encoder[STFTFB]): The `Encoder(STFTFB())` object that was\n            used to compute the input `mag_spec`.\n        angles (None or Tensor): Angles to use to initialize the algorithm.\n            If None (default), angles are init with uniform ditribution.\n        istft_dec (None or Decoder[STFTFB]): Optional Decoder to use to get\n            back to the time domain. If None (default), a perfect\n            reconstruction Decoder is built from `stft_enc`.\n        n_iter (int): Number of MISI iterations to run.\n        momentum (float): Momentum on updates (this argument comes from\n            GriffinLim). Defaults to 0 as it was never proposed anywhere.\n        src_weights (None or torch.Tensor): Consistency weight for each source.\n            Shape needs to be broadcastable to `istft_dec(mag_specgrams)`.\n            We make sure that the weights sum up to 1 along dim `dim`.\n            If `src_weights` is None, compute them based on relative power.\n        dim (int): Axis which contains the sources in `mag_specgrams`.\n            Used for consistency constraint.\n\n    Returns:\n        torch.Tensor: estimated waveforms of shape (batch, n_src, time).\n\n    Examples:\n        >>> stft = Encoder(STFTFB(n_filters=256, kernel_size=256, stride=128))\n        >>> wav = torch.randn(2, 3, 8000)\n        >>> specs = stft(wav)\n        >>> masked_specs = specs * torch.sigmoid(torch.randn_like(specs))\n        >>> mag = transforms.take_mag(masked_specs, -2)\n        >>> est_wav = misi(wav.sum(1), mag, stft, n_iter=32)\n\n    References:\n        [1] Gunawan and Sen, ""Iterative Phase Estimation for the Synthesis of\n        Separated Sources From Single-Channel Mixtures,"" in IEEE Signal\n        Processing Letters, 2010.\n        [2] Wang, LeRoux et al. \xe2\x80\x9cEnd-to-End Speech Separation with Unfolded\n        Iterative Phase Reconstruction.\xe2\x80\x9d Interspeech 2018 (2018)\n    """"""\n    # We can create perfect iSTFT from STFT Encoder\n    if istft_dec is None:\n        # Compute window for perfect resynthesis\n        syn_win = perfect_synthesis_window(stft_enc.filterbank.window,\n                                           stft_enc.stride)\n        istft_dec = Decoder(STFTFB(**stft_enc.get_config(), window=syn_win))\n\n    # If no intitial phase is provided initialize uniformly\n    if angles is None:\n        angles = 2 * math.pi * torch.rand_like(mag_specgrams,\n                                               device=mag_specgrams.device)\n    # wav_dim is used in mixture_consistency.\n    # Transform spec src dim to wav src dim for positive and negative dim\n    wav_dim = dim if dim >= 0 else dim + 1\n\n    # We forward/backward the mixture through STFT to have matching shapes\n    # with the input spectrograms as well as  account for potential modulations\n    # if the window were not chosen to enable perfect reconstruction.\n    mixture_wav = istft_dec(stft_enc(mixture_wav))\n\n    # Initialize rebuilt (useful to use momentum)\n    rebuilt = 0.\n    for _ in range(n_iter):\n        prev_built = rebuilt\n        # Go to the time domain\n        complex_specgram = transforms.from_mag_and_phase(mag_specgrams, angles)\n        wavs = istft_dec(complex_specgram)\n        # Make wavs sum up to the mixture\n        consistent_wavs = mixture_consistency(mixture_wav, wavs,\n                                              src_weights=src_weights,\n                                              dim=wav_dim)\n        # Back to TF domain\n        rebuilt = stft_enc(consistent_wavs)\n        # Update phase estimates (with momentum). Keep the momentum here\n        # in case. Was shown useful in GF, might be here. We\'ll see.\n        diff = rebuilt - momentum / (1 + momentum) * prev_built\n        angles = transforms.angle(diff)\n    # Final source estimates\n    final_complex_spec = transforms.from_mag_and_phase(mag_specgrams, angles)\n    return istft_dec(final_complex_spec)\n'"
asteroid/filterbanks/multiphase_gammatone_fb.py,1,"b'import numpy as np\nimport torch\nfrom .enc_dec import Filterbank\n\n\nclass MultiphaseGammatoneFB(Filterbank):\n    """""" Multi-Phase Gammatone Filterbank as described in [1].\n    Please cite [1] whenever using this.\n    Original code repository: `<https://github.com/sp-uhh/mp-gtf>`\n\n    Args:\n        n_filters (int): Number of filters.\n        kernel_size (int): Length of the filters.\n        sample_rate (int, optional): The sample rate (used for initialization).\n        stride (int, optional): Stride of the convolution. If None (default),\n            set to ``kernel_size // 2``.\n\n    References:\n    [1] David Ditter, Timo Gerkmann, ""A Multi-Phase Gammatone Filterbank for\n        Speech Separation via TasNet"", ICASSP 2020\n        Available: `<https://ieeexplore.ieee.org/document/9053602/>`\n    """"""\n    def __init__(self, n_filters=128, kernel_size=16, sample_rate=8000,\n                 stride=None, **kwargs):\n        super().__init__(n_filters, kernel_size, stride=stride)\n        self.sample_rate = sample_rate\n        self.n_feats_out = n_filters\n        length_in_seconds = kernel_size / sample_rate\n        mpgtf = generate_mpgtf(sample_rate, length_in_seconds, n_filters)\n        filters = torch.from_numpy(mpgtf).unsqueeze(1).float()\n        self.register_buffer(""_filters"", filters)\n\n    @property\n    def filters(self):\n        return self._filters\n\n\ndef generate_mpgtf(samplerate_hz, len_sec, n_filters):\n    # Set parameters\n    center_freq_hz_min = 100\n    n_center_freqs = 24\n    len_sample = int(np.floor(samplerate_hz * len_sec))\n\n    # Initialize variables\n    index = 0\n    filterbank = np.zeros((n_filters, len_sample))\n    current_center_freq_hz = center_freq_hz_min\n\n    # Determine number of phase shifts per center frequency\n    phase_pair_count = (\n        np.ones(n_center_freqs) * np.floor(n_filters / 2 / n_center_freqs)\n    ).astype(int)\n    remaining_phase_pairs = (\n                (n_filters - np.sum(phase_pair_count) * 2) / 2\n    ).astype(int)\n    if remaining_phase_pairs > 0:\n        phase_pair_count[:remaining_phase_pairs] = (\n            phase_pair_count[:remaining_phase_pairs] + 1\n        )\n\n    # Generate all filters for each center frequencies\n    for i in range(n_center_freqs):\n        # Generate all filters for all phase shifts\n        for phase_index in range(phase_pair_count[i]):\n            # First half of filtes: phase_shifts in [0,pi)\n            current_phase_shift = (\n                np.float(phase_index) / phase_pair_count[i] * np.pi\n            )\n            filterbank[index, :] = gammatone_impulse_response(\n                samplerate_hz,\n                len_sec,\n                current_center_freq_hz,\n                current_phase_shift,\n            )\n            index = index + 1\n\n        # Second half of filters: phase_shifts in [pi, 2*pi)\n        filterbank[index: index + phase_pair_count[i], :] = -filterbank[\n            index - phase_pair_count[i]: index, :\n        ]\n\n        # Prepare for next center frequency\n        index = index + phase_pair_count[i]\n        current_center_freq_hz = erb_scale_2_freq_hz(\n            freq_hz_2_erb_scale(current_center_freq_hz) + 1\n        )\n\n    filterbank = normalize_filters(filterbank)\n    return filterbank\n\n\ndef gammatone_impulse_response(samplerate_hz, len_sec, center_freq_hz,\n                               phase_shift):\n    """""" Generate single parametrized gammatone filter """"""\n    p = 2  # filter order\n    erb = 24.7 + 0.108 * center_freq_hz  # equivalent rectangular bandwidth\n    divisor = (\n        np.pi * np.math.factorial(2 * p - 2) * np.power(2, float(-(2 * p - 2)))\n    ) / np.square(np.math.factorial(p - 1))\n    b = erb / divisor  # bandwidth parameter\n    a = 1.0  # amplitude. This is varied later by the normalization process.\n    len_sample = int(np.floor(samplerate_hz * len_sec))\n    t = np.linspace(1.0 / samplerate_hz, len_sec, len_sample)\n    gammatone_ir = (\n        a\n        * np.power(t, p - 1)\n        * np.exp(-2 * np.pi * b * t)\n        * np.cos(2 * np.pi * center_freq_hz * t + phase_shift)\n    )\n    return gammatone_ir\n\n\ndef erb_scale_2_freq_hz(freq_erb):\n    """""" Convert frequency on ERB scale to frequency in Hertz """"""\n    freq_hz = (np.exp(freq_erb / 9.265) - 1) * 24.7 * 9.265\n    return freq_hz\n\n\ndef freq_hz_2_erb_scale(freq_hz):\n    """""" Convert frequency in Hertz to frequency on ERB scale """"""\n    freq_erb = 9.265 * np.log(1 + freq_hz / (24.7 * 9.265))\n    return freq_erb\n\n\ndef normalize_filters(filterbank):\n    """""" Normalizes a filterbank such that all filters\n    have the same root mean square (RMS). """"""\n    rms_per_filter = np.sqrt(np.mean(np.square(filterbank), axis=1))\n    rms_normalization_values = 1.0 / (rms_per_filter / np.amax(rms_per_filter))\n    normalized_filterbank = (\n        filterbank * rms_normalization_values[:, np.newaxis]\n    )\n    return normalized_filterbank\n'"
asteroid/filterbanks/param_sinc_fb.py,16,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport warnings\nfrom .enc_dec import Filterbank\n\n\nclass ParamSincFB(Filterbank):\n    """"""Extension of the parameterized filterbank from [1] proposed in [2].\n    Modified and extended from from `<https://github.com/mravanelli/SincNet>`__\n\n    Args:\n        n_filters (int): Number of filters. Half of `n_filters` (the real\n            parts) will have parameters, the other half will correspond to the\n            imaginary parts. `n_filters` should be even.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution. If None (default),\n            set to ``kernel_size // 2``.\n        sample_rate (int, optional): The sample rate (used for initialization).\n        min_low_hz (int, optional): Lowest low frequency allowed (Hz).\n        min_band_hz (int, optional): Lowest band frequency allowed (Hz).\n\n    Attributes:\n        n_feats_out (int): Number of output filters.\n\n    References:\n        [1] : ""Speaker Recognition from raw waveform with SincNet"". SLT 2018.\n        Mirco Ravanelli, Yoshua Bengio.  https://arxiv.org/abs/1808.00158\n\n        [2] : ""Filterbank design for end-to-end speech separation"".\n        Submitted to ICASSP 2020. Manuel Pariente, Samuele Cornell,\n        Antoine Deleforge, Emmanuel Vincent. https://arxiv.org/abs/1910.10400\n    """"""\n    def __init__(self, n_filters, kernel_size, stride=None,\n                 sample_rate=16000, min_low_hz=50, min_band_hz=50):\n        if kernel_size % 2 == 0:\n            print(\'Received kernel_size={}, force \'.format(kernel_size) +\n                  \'kernel_size={} so filters are odd\'.format(kernel_size+1))\n            kernel_size += 1\n        super(ParamSincFB, self).__init__(n_filters, kernel_size, stride=stride)\n        self.sample_rate = sample_rate\n        self.min_low_hz, self.min_band_hz = min_low_hz, min_band_hz\n\n        self.half_kernel = self.kernel_size // 2\n        self.cutoff = int(n_filters // 2)\n        self.n_feats_out = 2 * self.cutoff\n        self._initialize_filters()\n        if n_filters % 2 != 0:\n            print(\'If the number of filters `n_filters` is odd, the \'\n                  \'output size of the layer will be `n_filters - 1`.\')\n\n        window_ = np.hamming(self.kernel_size)[:self.half_kernel]  # Half window\n        n_ = 2 * np.pi * (torch.arange(-self.half_kernel, 0.).view(1, -1) /\n                          self.sample_rate)  # Half time vector\n        self.register_buffer(\'window_\', torch.from_numpy(window_).float())\n        self.register_buffer(\'n_\', n_)\n\n    def _initialize_filters(self):\n        """""" Filter Initialization along the Mel scale""""""\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n        mel = np.linspace(self.to_mel(low_hz),\n                          self.to_mel(high_hz),\n                          self.n_filters // 2 + 1, dtype=\'float32\')\n        hz = self.to_hz(mel)\n        # filters parameters (out_channels // 2, 1)\n        self.low_hz_ = nn.Parameter(torch.from_numpy(hz[:-1]).view(-1, 1))\n        self.band_hz_ = nn.Parameter(torch.from_numpy(np.diff(hz)).view(-1, 1))\n\n    @property\n    def filters(self):\n        """""" Compute filters from parameters """"""\n        low = self.min_low_hz + torch.abs(self.low_hz_)\n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),\n                           self.min_low_hz, self.sample_rate / 2)\n        cos_filters = self.make_filters(low, high, filt_type=\'cos\')\n        sin_filters = self.make_filters(low, high, filt_type=\'sin\')\n        return torch.cat([cos_filters, sin_filters], dim=0)\n\n    def make_filters(self, low, high, filt_type=\'cos\'):\n        band = (high - low)[:, 0]\n        ft_low = torch.matmul(low, self.n_)\n        ft_high = torch.matmul(high, self.n_)\n        if filt_type == \'cos\':  # Even filters from the SincNet paper.\n            bp_left = ((torch.sin(ft_high) - torch.sin(ft_low)) /\n                       (self.n_ / 2)) * self.window_\n            bp_center = 2 * band.view(-1, 1)\n            bp_right = torch.flip(bp_left, dims=[1])\n        elif filt_type == \'sin\':  # Extension including odd filters\n            bp_left = ((torch.cos(ft_low) - torch.cos(ft_high)) /\n                       (self.n_ / 2)) * self.window_\n            bp_center = torch.zeros_like(band.view(-1, 1))\n            bp_right = - torch.flip(bp_left, dims=[1])\n        else:\n            raise ValueError(\'Invalid filter type {}\'.format(filt_type))\n        band_pass = torch.cat([bp_left, bp_center, bp_right], dim=1)\n        band_pass = band_pass / (2 * band[:, None])\n        return band_pass.view(self.n_filters // 2, 1, self.kernel_size)\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def get_config(self):\n        """""" Returns dictionary of arguments to re-instantiate the class.""""""\n        config = {\n            \'sample_rate\': self.sample_rate,\n            \'min_low_hz\': self.min_low_hz,\n            \'min_band_hz\': self.min_band_hz\n        }\n        base_config = super(ParamSincFB, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n'"
asteroid/filterbanks/stft_fb.py,1,"b'import torch\nimport numpy as np\nfrom .enc_dec import Filterbank\n\n\nclass STFTFB(Filterbank):\n    """""" STFT filterbank.\n\n    Args:\n        n_filters (int): Number of filters. Determines the length of the STFT\n            filters before windowing.\n        kernel_size (int): Length of the filters (i.e the window).\n        stride (int, optional): Stride of the convolution (hop size). If None\n            (default), set to ``kernel_size // 2``.\n        window (:class:`numpy.ndarray`, optional): If None, defaults to\n            ``np.sqrt(np.hanning())``.\n\n    Attributes:\n        n_feats_out (int): Number of output filters.\n    """"""\n    def __init__(self, n_filters, kernel_size, stride=None, window=None,\n                 **kwargs):\n        super(STFTFB, self).__init__(n_filters, kernel_size, stride=stride)\n        assert n_filters >= kernel_size\n        self.cutoff = int(n_filters/2 + 1)\n        self.n_feats_out = 2 * self.cutoff\n\n        if window is None:\n            self.window = np.hanning(kernel_size + 1)[:-1]**.5\n        else:\n            ws = window.size\n            if not (ws == kernel_size):\n                raise AssertionError(\'Expected window of size {}.\'\n                                     \'Received window of size {} instead.\'\n                                     \'\'.format(kernel_size, ws))\n            self.window = window\n        # Create and normalize DFT filters (can be overcomplete)\n        filters = np.fft.fft(np.eye(n_filters))\n        filters /= (0.5 * np.sqrt(kernel_size * n_filters / self.stride))\n\n        # Keep only the windowed centered part to save computation.\n        lpad = int((n_filters - kernel_size) // 2)\n        rpad = int(n_filters - kernel_size - lpad)\n        indexes = list(range(lpad, n_filters - rpad))\n        filters = np.vstack([np.real(filters[:self.cutoff, indexes]),\n                             np.imag(filters[:self.cutoff, indexes])])\n\n        filters[0, :] /= np.sqrt(2)\n        filters[n_filters // 2, :] /= np.sqrt(2)\n        filters = torch.from_numpy(filters * self.window).unsqueeze(1).float()\n        self.register_buffer(\'_filters\', filters)\n\n    @property\n    def filters(self):\n        return self._filters\n\n\ndef perfect_synthesis_window(analysis_window, hop_size):\n    """""" Computes a window for perfect synthesis given an analysis window and\n        a hop size.\n\n    Args:\n        analysis_window (np.array): Analysis window of the transform.\n        hop_size (int): Hop size in number of samples.\n\n    Returns:\n        np.array : the synthesis window to use for perfectly inverting the STFT.\n    """"""\n    win_size = len(analysis_window)\n    den = np.zeros_like(analysis_window)\n\n    loop_on = (win_size - 1) // hop_size\n    for win_idx in range(-loop_on, loop_on + 1):\n        shifted = np.roll(analysis_window ** 2, win_idx * hop_size)\n        if win_idx < 0:\n            shifted[win_idx * hop_size:] = 0\n        elif win_idx > 0:\n            shifted[:win_idx * hop_size] = 0\n        den += shifted\n    den = np.where(den != 0., den, np.finfo(den.dtype).tiny)\n    correction = int(0.5 * len(analysis_window) / hop_size)\n    return correction * analysis_window / den\n'"
asteroid/filterbanks/transforms.py,44,"b'import torch\nimport numpy as np\nEPS = 1e-8\n\n\ndef mul_c(inp, other, dim=-2):\n    """""" Entrywise product for complex valued tensors.\n\n    Operands are assumed to have the real parts of each entry followed by the\n    imaginary parts of each entry along dimension `dim`, e.g. for,\n    ``dim = 1``, the matrix\n\n    .. code::\n\n        [[1, 2, 3, 4],\n         [5, 6, 7, 8]]\n\n    is interpreted as\n\n    .. code::\n\n        [[1 + 3j, 2 + 4j],\n         [5 + 7j, 6 + 8j]\n\n    where `j` is such that `j * j = -1`.\n\n    Args:\n        inp (:class:`torch.Tensor`): The first operand with real and\n            imaginary parts concatenated on the `dim` axis.\n        other (:class:`torch.Tensor`): The second operand.\n        dim (int, optional): frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n    Returns:\n        :class:`torch.Tensor`:\n            The complex multiplication between `inp` and `other`\n\n            For now, it assumes that `other` has the same shape as `inp` along\n            `dim`.\n    """"""\n    check_complex(inp, dim=dim)\n    check_complex(other, dim=dim)\n    real1, imag1 = inp.chunk(2, dim=dim)\n    real2, imag2 = other.chunk(2, dim=dim)\n    return torch.cat([real1 * real2 - imag1 * imag2,\n                      real1 * imag2 + imag1 * real2], dim=dim)\n\n\ndef take_reim(x, dim=-2):\n    return x\n\n\ndef take_mag(x, dim=-2):\n    """""" Takes the magnitude of a complex tensor.\n\n    The operands is assumed to have the real parts of each entry followed by\n    the imaginary parts of each entry along dimension `dim`, e.g. for,\n    ``dim = 1``, the matrix\n\n    .. code::\n\n        [[1, 2, 3, 4],\n         [5, 6, 7, 8]]\n\n    is interpreted as\n\n    .. code::\n\n        [[1 + 3j, 2 + 4j],\n         [5 + 7j, 6 + 8j]\n\n    where `j` is such that `j * j = -1`.\n\n    Args:\n        x (:class:`torch.Tensor`): Complex valued tensor.\n        dim (int): frequency (or equivalent) dimension along which real and\n            imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`: The magnitude of x.\n    """"""\n    check_complex(x, dim=dim)\n    power = torch.stack(torch.chunk(x, 2, dim=dim), dim=-1).pow(2).sum(dim=-1)\n    power = power + EPS\n    return power.pow(0.5)\n\n\ndef take_cat(x, dim=-2):\n    return torch.cat([take_mag(x, dim=dim), x], dim=dim)\n\n\ndef apply_real_mask(tf_rep, mask, dim=-2):\n    """""" Applies a real-valued mask to a real-valued representation.\n\n    It corresponds to ReIm mask in [1].\n\n    Args:\n        tf_rep (:class:`torch.Tensor`): The time frequency representation to\n            apply the mask to.\n        mask (:class:`torch.Tensor`): The real-valued mask to be applied.\n        dim (int): Kept to have the same interface with the other ones.\n    Returns:\n        :class:`torch.Tensor`: `tf_rep` multiplied by the `mask`.\n    """"""\n    return tf_rep * mask\n\n\ndef apply_mag_mask(tf_rep, mask, dim=-2):\n    """""" Applies a real-valued mask to a complex-valued representation.\n\n    If `tf_rep` has 2N elements along `dim`, `mask` has N elements, `mask` is\n    duplicated along `dim` to apply the same mask to both the Re and Im.\n\n    `tf_rep` is assumed to have the real parts of each entry followed by\n    the imaginary parts of each entry along dimension `dim`, e.g. for,\n    ``dim = 1``, the matrix\n\n    .. code::\n\n        [[1, 2, 3, 4],\n         [5, 6, 7, 8]]\n\n    is interpreted as\n\n    .. code::\n\n        [[1 + 3j, 2 + 4j],\n         [5 + 7j, 6 + 8j]\n\n    where `j` is such that `j * j = -1`.\n\n    Args:\n        tf_rep (:class:`torch.Tensor`): The time frequency representation to\n            apply the mask to. Re and Im are concatenated along `dim`.\n        mask (:class:`torch.Tensor`): The real-valued mask to be applied.\n        dim (int): The frequency (or equivalent) dimension of both `tf_rep` and\n            `mask` along which real and imaginary values are concatenated.\n    Returns:\n        :class:`torch.Tensor`: `tf_rep` multiplied by the `mask`.\n    """"""\n    check_complex(tf_rep, dim=dim)\n    mask = torch.cat([mask, mask], dim=dim)\n    return tf_rep * mask\n\n\ndef apply_complex_mask(tf_rep, mask, dim=-2):\n    """""" Applies a complex-valued mask to a complex-valued representation.\n\n    Operands are assumed to have the real parts of each entry followed by the\n    imaginary parts of each entry along dimension `dim`, e.g. for,\n    ``dim = 1``, the matrix\n\n    .. code::\n\n        [[1, 2, 3, 4],\n         [5, 6, 7, 8]]\n\n    is interpreted as\n\n    .. code::\n\n        [[1 + 3j, 2 + 4j],\n         [5 + 7j, 6 + 8j]\n\n    where `j` is such that `j * j = -1`.\n\n    Args:\n        tf_rep (:class:`torch.Tensor`): The time frequency representation to\n            apply the mask to.\n        mask (class:`torch.Tensor`): The complex-valued mask to be applied.\n        dim (int): The frequency (or equivalent) dimension of both `tf_rep` an\n            `mask` along which real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            `tf_rep` multiplied by the `mask` in the complex sense.\n    """"""\n    check_complex(tf_rep, dim=dim)\n    return mul_c(tf_rep, mask, dim=dim)\n\n\ndef check_complex(tensor, dim=-2):\n    """""" Assert tensor in complex-like in a given dimension.\n\n    Args:\n        tensor (torch.Tensor): tensor to be checked.\n        dim(int): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Raises:\n        AssertionError if dimension is not even in the specified dimension\n\n    """"""\n    if tensor.shape[dim] % 2 != 0:\n        raise AssertionError(\'Could not equally chunk the tensor (shape {}) \'\n                             \'along the given dimension ({}). Dim axis is \'\n                             \'probably wrong\')\n\n\ndef to_numpy(tensor, dim=-2):\n    """""" Convert complex-like torch tensor to numpy complex array\n\n    Args:\n        tensor (torch.Tensor): Complex tensor to convert to numpy.\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n    Returns:\n        :class:`numpy.array`:\n            Corresponding complex array.\n    """"""\n    check_complex(tensor, dim=dim)\n    real, imag = torch.chunk(tensor, 2, dim=dim)\n    return real.data.numpy() + 1j * imag.data.numpy()\n\n\ndef from_numpy(array, dim=-2):\n    """""" Convert complex numpy array to complex-like torch tensor.\n\n    Args:\n        array (np.array): array to be converted.\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            Corresponding torch.Tensor (complex axis in dim `dim`=\n    """"""\n    return torch.cat([torch.from_numpy(np.real(array)),\n                      torch.from_numpy(np.imag(array))], dim=dim)\n\n\ndef to_torchaudio(tensor, dim=-2):\n    """""" Converts complex-like torch tensor to torchaudio style complex tensor.\n\n    Args:\n        tensor (torch.tensor): asteroid-style complex-like torch tensor.\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            torchaudio-style complex-like torch tensor.\n    """"""\n    return torch.stack(torch.chunk(tensor, 2, dim=dim), dim=-1)\n\n\ndef from_torchaudio(tensor, dim=-2):\n    """""" Converts torchaudio style complex tensor to complex-like torch tensor.\n\n    Args:\n        tensor (torch.tensor): torchaudio-style complex-like torch tensor.\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            asteroid-style complex-like torch tensor.\n    """"""\n    return torch.cat([tensor[..., 0], tensor[..., 1]], dim=dim)\n\n\ndef angle(tensor, dim=-2):\n    """""" Return the angle of the complex-like torch tensor.\n\n    Args:\n        tensor (torch.Tensor): the complex tensor from which to extract the\n            phase.\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            The counterclockwise angle from the positive real axis on\n            the complex plane in radians.\n    """"""\n    check_complex(tensor, dim=dim)\n    real, imag = torch.chunk(tensor, 2, dim=dim)\n    return torch.atan2(imag, real)\n\n\ndef from_mag_and_phase(mag, phase, dim=-2):\n    """""" Return a complex-like torch tensor from magnitude and phase components.\n\n    Args:\n        mag (torch.tensor): magnitude of the tensor.\n        phase (torch.tensor): angle of the tensor\n        dim(int, optional): the frequency (or equivalent) dimension along which\n            real and imaginary values are concatenated.\n\n    Returns:\n        :class:`torch.Tensor`:\n            The corresponding complex-like torch tensor.\n    """"""\n    return torch.cat([mag*torch.cos(phase), mag*torch.sin(phase)], dim=dim)\n\n\ndef ebased_vad(mag_spec, th_db=40):\n    """""" Compute energy-based VAD from a magnitude spectrogram (or equivalent).\n\n    Args:\n        mag_spec (torch.Tensor): the spectrogram to perform VAD on.\n            Expected shape (batch, *, freq, time).\n            The VAD mask will be computed independently for all the leading\n            dimensions until the last two. Independent of the ordering of the\n            last two dimensions.\n        th_db (int): The threshold in dB from which a TF-bin is considered\n            silent.\n\n    Returns:\n        torch.BoolTensor, the VAD mask.\n\n\n    Examples:\n        >>> import torch\n        >>> mag_spec = torch.abs(torch.randn(10, 2, 65, 16))\n        >>> batch_src_mask = ebased_vad(mag_spec)\n    """"""\n    log_mag = 20 * torch.log10(mag_spec)\n    # Compute VAD for each utterance in a batch independently.\n    to_view = list(mag_spec.shape[:-2]) + [1, -1]\n    max_log_mag = torch.max(log_mag.view(to_view), -1, keepdim=True)[0]\n    return log_mag > (max_log_mag - th_db)\n\n\n_inputs = {\n    \'reim\': (take_reim, 1),\n    \'mag\': (take_mag, 1/2),\n    \'cat\': (take_cat, 1 + 1/2)\n}\n_inputs[\'real\'] = _inputs[\'reim\']\n_inputs[\'mod\'] = _inputs[\'mag\']\n_inputs[\'concat\'] = _inputs[\'cat\']\n\n\n_masks = {\n    \'reim\': (apply_real_mask, 1),\n    \'mag\': (apply_mag_mask, 1/2),\n    \'complex\': (apply_complex_mask, 1)\n}\n_masks[\'real\'] = _masks[\'reim\']\n_masks[\'mod\'] = _masks[\'mag\']\n_masks[\'comp\'] = _masks[\'complex\']\n'"
asteroid/losses/__init__.py,0,"b'from .pit_wrapper import PITLossWrapper\nfrom .sdr import singlesrc_neg_sisdr, multisrc_neg_sisdr\nfrom .sdr import singlesrc_neg_sdsdr, multisrc_neg_sdsdr\nfrom .sdr import singlesrc_neg_snr, multisrc_neg_snr\nfrom .mse import singlesrc_mse, multisrc_mse\nfrom .cluster import deep_clustering_loss\nfrom .pmsqe import SingleSrcPMSQE\nfrom .stoi import NegSTOILoss as SingleSrcNegSTOI\n\n# Legacy\nfrom .sdr import pairwise_neg_sisdr, nosrc_neg_sisdr, nonpit_neg_sisdr\nfrom .sdr import pairwise_neg_sdsdr, nosrc_neg_sdsdr, nonpit_neg_sdsdr\nfrom .sdr import pairwise_neg_snr, nosrc_neg_snr, nonpit_neg_snr\nfrom .sdr import PairwiseNegSDR\nfrom .mse import pairwise_mse, nosrc_mse, nonpit_mse\n'"
asteroid/losses/cluster.py,15,"b'import torch\n\n\ndef deep_clustering_loss(embedding, tgt_index, binary_mask=None):\n    """""" Compute the deep clustering loss defined in [1].\n\n    Args:\n        embedding (torch.Tensor): Estimated embeddings.\n            Expected shape  (batch, frequency x frame, embedding_dim)\n        tgt_index (torch.Tensor): Dominating source index in each TF bin.\n            Expected shape: [batch, frequency, frame]\n        binary_mask (torch.Tensor): VAD in TF plane. Bool or Float.\n            See asteroid.filterbanks.transforms.ebased_vad.\n\n    Returns:\n         `torch.Tensor`. Deep clustering loss for every batch sample.\n\n    Examples:\n        >>> import torch\n        >>> from asteroid.losses.cluster import deep_clustering_loss\n        >>> spk_cnt = 3\n        >>> embedding = torch.randn(10, 5*400, 20)\n        >>> targets = torch.LongTensor([10, 400, 5]).random_(0, spk_cnt)\n        >>> loss = deep_clustering_loss(embedding, targets)\n\n    Reference:\n        [1] Zhong-Qiu Wang, Jonathan Le Roux, John R. Hershey\n            ""ALTERNATIVE OBJECTIVE FUNCTIONS FOR DEEP CLUSTERING""\n\n    Notes:\n        Be careful in viewing the embedding tensors. The target indices\n        `tgt_index` are of shape (batch, freq, frames). Even if the embedding\n        is of shape (batch, freq*frames, emb), the underlying view should be\n        (batch, freq, frames, emb) and not (batch, frames, freq, emb).\n    """"""\n    spk_cnt = len(tgt_index.unique())\n\n    batch, bins, frames = tgt_index.shape\n    if binary_mask is None:\n        binary_mask = torch.ones(batch, bins * frames, 1)\n    binary_mask = binary_mask.float()\n    if len(binary_mask.shape) == 3:\n        binary_mask = binary_mask.view(batch, bins * frames, 1)\n    # If boolean mask, make it float.\n    binary_mask = binary_mask.to(tgt_index.device)\n\n    # Fill in one-hot vector for each TF bin\n    tgt_embedding = torch.zeros(batch, bins * frames, spk_cnt,\n                                device=tgt_index.device)\n    tgt_embedding.scatter_(2, tgt_index.view(batch, bins * frames, 1), 1)\n\n    # Compute VAD-weighted DC loss\n    tgt_embedding = tgt_embedding * binary_mask\n    embedding = embedding * binary_mask\n    est_proj = torch.einsum(\'ijk,ijl->ikl\', embedding, embedding)\n    true_proj = torch.einsum(\'ijk,ijl->ikl\', tgt_embedding, tgt_embedding)\n    true_est_proj = torch.einsum(\'ijk,ijl->ikl\', embedding, tgt_embedding)\n    # Equation (1) in [1]\n    cost = batch_matrix_norm(est_proj) + batch_matrix_norm(true_proj)\n    cost = cost - 2 * batch_matrix_norm(true_est_proj)\n    # Divide by number of active bins, for each element in batch\n    return cost / torch.sum(binary_mask, dim=[1, 2])\n\n\ndef batch_matrix_norm(matrix, norm_order=2):\n    """""" Normalize a matrix according to `norm_order`\n\n    Args:\n        matrix (torch.Tensor): Expected shape [batch, *]\n        norm_order (int): Norm order.\n\n    Returns:\n        torch.Tensor, normed matrix of shape [batch]\n    """"""\n    keep_batch = list(range(1, matrix.ndim))\n    return torch.norm(matrix, p=norm_order, dim=keep_batch) ** norm_order\n'"
asteroid/losses/mse.py,11,"b'from ..utils.deprecation_utils import DeprecationMixin\nfrom torch.nn.modules.loss import _Loss\n\n\nclass PairwiseMSE(_Loss):\n    """""" Measure pairwise mean square error on a batch.\n\n    Shape:\n        est_targets (:class:`torch.Tensor`): Expected shape [batch, nsrc, *].\n            The batch of target estimates.\n        targets (:class:`torch.Tensor`): Expected shape [batch, nsrc, *].\n            The batch of training targets\n\n    Returns:\n        :class:`torch.Tensor`: with shape [batch, nsrc, nsrc]\n\n    Examples:\n\n        >>> import torch\n        >>> from asteroid.losses import PITLossWrapper\n        >>> targets = torch.randn(10, 2, 32000)\n        >>> est_targets = torch.randn(10, 2, 32000)\n        >>> loss_func = PITLossWrapper(PairwiseMSE(), pit_from=\'pairwise\')\n        >>> loss = loss_func(est_targets, targets)\n    """"""\n    def forward(self, est_targets, targets):\n        targets = targets.unsqueeze(1)\n        est_targets = est_targets.unsqueeze(2)\n        pw_loss = (targets - est_targets)**2\n        # Need to return [batch, nsrc, nsrc]\n        mean_over = list(range(3, pw_loss.ndim))\n        return pw_loss.mean(dim=mean_over)\n\n\nclass SingleSrcMSE(_Loss):\n    """""" Measure mean square error on a batch.\n    Supports both tensors with and without source axis.\n\n    Shape:\n        est_targets (:class:`torch.Tensor`): Expected shape [batch, *].\n            The batch of target estimates.\n        targets (:class:`torch.Tensor`): Expected shape [batch, *].\n            The batch of training targets.\n\n    Returns:\n        :class:`torch.Tensor`: with shape [batch]\n\n    Examples:\n\n        >>> import torch\n        >>> from asteroid.losses import PITLossWrapper\n        >>> targets = torch.randn(10, 2, 32000)\n        >>> est_targets = torch.randn(10, 2, 32000)\n        >>> # singlesrc_mse / multisrc_mse support both \'pw_pt\' and \'perm_avg\'.\n        >>> loss_func = PITLossWrapper(singlesrc_mse, pit_from=\'pw_pt\')\n        >>> loss = loss_func(est_targets, targets)\n    """"""\n    def forward(self, est_targets, targets):\n        loss = (targets - est_targets)**2\n        mean_over = list(range(1, loss.ndim))\n        return loss.mean(dim=mean_over)\n\n\n# aliases\nMultiSrcMSE = SingleSrcMSE\npairwise_mse = PairwiseMSE()\nsinglesrc_mse = SingleSrcMSE()\nmultisrc_mse = MultiSrcMSE()\n\n\n# Legacy\nclass NoSrcMSE(SingleSrcMSE, DeprecationMixin):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.warn_deprecated()\n\n\nNonPitMSE = NoSrcMSE\nnosrc_mse = singlesrc_mse\nnonpit_mse = multisrc_mse\n'"
asteroid/losses/multi_scale_spectral.py,13,"b'import torch\nimport torch.nn as nn\nfrom torch.nn.modules.loss import _Loss\nfrom asteroid.filterbanks import STFTFB, Encoder\nfrom asteroid.filterbanks.transforms import take_mag\n\nEPS = 1e-8\n\n\nclass SingleSrcMultiScaleSpectral(_Loss):\n    """""" Measure multi-scale spectral loss as described in [1]\n\n    Args:\n        n_filters (list): list containing the number of filter desired for\n            each STFT\n        windows_size (list): list containing the size of the window desired for\n            each STFT\n        hops_size (list): list containing the size of the hop desired for\n            each STFT\n\n    Shape:\n        est_targets (:class:`torch.Tensor`): Expected shape [batch, time].\n            Batch of target estimates.\n        targets (:class:`torch.Tensor`): Expected shape [batch, time].\n            Batch of training targets.\n        alpha (float) : Weighting factor for the log term\n\n    Returns:\n        :class:`torch.Tensor`: with shape [batch]\n\n    Examples:\n        >>> import torch\n        >>> targets = torch.randn(10, 32000)\n        >>> est_targets = torch.randn(10, 32000)\n        >>> # Using it by itself on a pair of source/estimate\n        >>> loss_func = SingleSrcMultiScaleSpectral()\n        >>> loss = loss_func(est_targets, targets)\n\n        >>> import torch\n        >>> from asteroid.losses import PITLossWrapper\n        >>> targets = torch.randn(10, 2, 32000)\n        >>> est_targets = torch.randn(10, 2, 32000)\n        >>> # Using it with PITLossWrapper with sets of source/estimates\n        >>> loss_func = PITLossWrapper(SingleSrcMultiScaleSpectral(),\n        >>>                            pit_from=\'pw_pt\')\n        >>> loss = loss_func(est_targets, targets)\n\n    References:\n        [1] Jesse Engel and Lamtharn (Hanoi) Hantrakul and Chenjie Gu and\n        Adam Roberts DDSP: Differentiable Digital Signal Processing\n        International Conference on Learning Representations ICLR 2020 $\n    """"""\n\n    def __init__(self, n_filters=None, windows_size=None,\n                 hops_size=None, alpha=1.):\n        super().__init__()\n\n        if windows_size is None:\n            windows_size = [2048, 1024, 512, 256, 128, 64, 32]\n        if n_filters is None:\n            n_filters = [2048, 1024, 512, 256, 128, 64, 32]\n        if hops_size is None:\n            hops_size = [1024, 512, 256, 128, 64, 32, 16]\n\n        self.windows_size = windows_size\n        self.n_filters = n_filters\n        self.hops_size = hops_size\n        self.alpha = alpha\n\n        self.encoders = nn.ModuleList(\n            Encoder(STFTFB(n_filters[i], windows_size[i], hops_size[i])) for i\n            in range(len(self.n_filters)))\n\n    def forward(self, est_target, target):\n        batch_size = est_target.shape[0]\n        est_target = est_target.unsqueeze(1)\n        target = target.unsqueeze(1)\n\n        loss = torch.zeros(batch_size, device=est_target.device)\n        for encoder in self.encoders:\n            loss += self.compute_spectral_loss(encoder, est_target, target)\n        return loss\n\n    def compute_spectral_loss(self, encoder, est_target, target):\n        batch_size = est_target.shape[0]\n        spect_est_target = take_mag(encoder(est_target)).view(batch_size, -1)\n        spect_target = take_mag(encoder(target)).view(batch_size, -1)\n        linear_loss = self.norm1(spect_est_target - spect_target)\n        log_loss = self.norm1(torch.log(spect_est_target + EPS) -\n                              torch.log(spect_target + EPS))\n        return linear_loss + self.alpha * log_loss\n\n    @staticmethod\n    def norm1(a):\n        return torch.norm(a, p=1, dim=1)\n'"
asteroid/losses/pit_wrapper.py,35,"b'from itertools import permutations\nimport torch\nfrom torch import nn\n\n\nclass PITLossWrapper(nn.Module):\n    """""" Permutation invariant loss wrapper.\n\n    Args:\n        loss_func: function with signature (targets, est_targets, **kwargs).\n        pit_from (str): Determines how PIT is applied.\n\n            * ``\'pw_mtx\'`` (pairwise matrix): `loss_func` computes pairwise\n              losses and returns a torch.Tensor of shape\n              :math:`(batch, n\\_src, n\\_src)`. Each element\n              :math:`[batch, i, j]` corresponds to the loss between\n              :math:`targets[:, i]` and :math:`est\\_targets[:, j]`\n            * ``\'pw_pt\'`` (pairwise point): `loss_func` computes the loss for\n              a batch of single source and single estimates (tensors won\'t\n              have the source axis). Output shape : :math:`(batch)`.\n              See :meth:`~PITLossWrapper.get_pw_losses`.\n            * ``\'perm_avg\'``(permutation average): `loss_func` computes the\n              average loss for a given permutations of the sources and\n              estimates. Output shape : :math:`(batch)`.\n              See :meth:`~PITLossWrapper.best_perm_from_perm_avg_loss`.\n\n            In terms of efficiency, ``\'perm_avg\'`` is the least efficicient.\n\n        perm_reduce (Callable): torch function to reduce permutation losses.\n            Defaults to None (equivalent to mean). Signature of the func\n            (pwl_set, **kwargs) : (B, n_src!, n_src) --> (B, n_src!).\n            `perm_reduce` can receive **kwargs during forward using the\n            `reduce_kwargs` argument (dict). If those argument are static,\n            consider defining a small function or using `functools.partial`.\n            Only used in `\'pw_mtx\'` and `\'pw_pt\'` `pit_from` modes.\n\n    For each of these modes, the best permutation and reordering will be\n    automatically computed.\n\n    Examples:\n        >>> import torch\n        >>> from asteroid.losses import pairwise_neg_sisdr\n        >>> sources = torch.randn(10, 3, 16000)\n        >>> est_sources = torch.randn(10, 3, 16000)\n        >>> # Compute PIT loss based on pairwise losses\n        >>> loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n        >>> loss_val = loss_func(est_sources, sources)\n        >>>\n        >>> # Using reduce\n        >>> def reduce(perm_loss, src):\n        >>>     weighted = perm_loss * src.norm(dim=-1, keepdim=True)\n        >>>     return torch.mean(weighted, dim=-1)\n        >>>\n        >>> loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\',\n        >>>                            perm_reduce=reduce)\n        >>> reduce_kwargs = {\'src\': sources}\n        >>> loss_val = loss_func(est_sources, sources,\n        >>>                      reduce_kwargs=reduce_kwargs)\n    """"""\n    def __init__(self, loss_func, pit_from=\'pw_mtx\', perm_reduce=None):\n        super().__init__()\n        self.loss_func = loss_func\n        self.pit_from = pit_from\n        self.perm_reduce = perm_reduce\n        if self.pit_from not in [\'pw_mtx\', \'pw_pt\', \'perm_avg\']:\n            raise ValueError(\'Unsupported loss function type for now. Expected\'\n                             \'one of [`pw_mtx`, `pw_pt`, `perm_avg`]\')\n\n    def forward(self, est_targets, targets, return_est=False,\n                reduce_kwargs=None, **kwargs):\n        """""" Find the best permutation and return the loss.\n\n        Args:\n            est_targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of target estimates.\n            targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of training targets\n            return_est: Boolean. Whether to return the reordered targets\n                estimates (To compute metrics or to save example).\n            reduce_kwargs (dict or None): kwargs that will be passed to the\n                pairwise losses reduce function (`perm_reduce`).\n            **kwargs: additional keyword argument that will be passed to the\n                loss function.\n\n        Returns:\n            - Best permutation loss for each batch sample, average over\n                the batch. torch.Tensor(loss_value)\n            - The reordered targets estimates if return_est is True.\n                torch.Tensor of shape [batch, nsrc, *].\n        """"""\n        n_src = targets.shape[1]\n        assert n_src < 10, f""Expected source axis along dim 1, found {n_src}""\n        if self.pit_from == \'pw_mtx\':\n            # Loss function already returns pairwise losses\n            pw_losses = self.loss_func(est_targets, targets, **kwargs)\n        elif self.pit_from == \'pw_pt\':\n            # Compute pairwise losses with a for loop.\n            pw_losses = self.get_pw_losses(self.loss_func, est_targets,\n                                           targets, **kwargs)\n        elif self.pit_from == \'perm_avg\':\n            # Cannot get pairwise losses from this type of loss.\n            # Find best permutation directly.\n            min_loss, min_loss_idx = self.best_perm_from_perm_avg_loss(\n                self.loss_func, est_targets, targets, **kwargs\n            )\n            # Take the mean over the batch\n            mean_loss = torch.mean(min_loss)\n            if not return_est:\n                return mean_loss\n            reordered = self.reorder_source(est_targets, n_src, min_loss_idx)\n            return mean_loss, reordered\n        else:\n            return\n\n        assert pw_losses.ndim == 3, (""Something went wrong with the loss ""\n                                     ""function, please read the docs."")\n        assert (pw_losses.shape[0] ==\n                targets.shape[0]), ""PIT loss needs same batch dim as input""\n\n        reduce_kwargs = reduce_kwargs if reduce_kwargs is not None else dict()\n        min_loss, min_loss_idx = self.find_best_perm(\n            pw_losses, n_src, perm_reduce=self.perm_reduce, **reduce_kwargs\n        )\n        mean_loss = torch.mean(min_loss)\n        if not return_est:\n            return mean_loss\n        reordered = self.reorder_source(est_targets, n_src, min_loss_idx)\n        return mean_loss, reordered\n\n    @staticmethod\n    def get_pw_losses(loss_func, est_targets, targets, **kwargs):\n        """""" Get pair-wise losses between the training targets and its estimate\n        for a given loss function.\n\n        Args:\n            loss_func: function with signature (targets, est_targets, **kwargs)\n                The loss function to get pair-wise losses from.\n            est_targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of target estimates.\n            targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of training targets.\n            **kwargs: additional keyword argument that will be passed to the\n                loss function.\n\n        Returns:\n            torch.Tensor or size [batch, nsrc, nsrc], losses computed for\n            all permutations of the targets and est_targets.\n\n        This function can be called on a loss function which returns a tensor\n        of size [batch]. There are more efficient ways to compute pair-wise\n        losses using broadcasting.\n        """"""\n        batch_size, n_src, *_ = targets.shape\n        pair_wise_losses = targets.new_empty(batch_size, n_src, n_src)\n        for est_idx, est_src in enumerate(est_targets.transpose(0, 1)):\n            for target_idx, target_src in enumerate(targets.transpose(0, 1)):\n                pair_wise_losses[:, est_idx, target_idx] = loss_func(\n                    est_src, target_src, **kwargs)\n        return pair_wise_losses\n\n    @staticmethod\n    def best_perm_from_perm_avg_loss(loss_func, est_targets, targets, **kwargs):\n        """""" Find best permutation from loss function with source axis.\n\n        Args:\n            loss_func: function with signature (targets, est_targets, **kwargs)\n                The loss function batch losses from.\n            est_targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of target estimates.\n            targets: torch.Tensor. Expected shape [batch, nsrc, *].\n                The batch of training targets.\n            **kwargs: additional keyword argument that will be passed to the\n                loss function.\n\n        Returns:\n            tuple:\n                :class:`torch.Tensor`: The loss corresponding to the best\n                permutation of size (batch,).\n\n                :class:`torch.LongTensor`: The indexes of the best permutations.\n        """"""\n        n_src = targets.shape[1]\n        perms = list(permutations(range(n_src)))\n        loss_set = torch.stack([loss_func(est_targets[:, perm],\n                                          targets,\n                                          **kwargs) for perm in perms],\n                               dim=1)\n        # Indexes and values of min losses for each batch element\n        min_loss, min_loss_idx = torch.min(loss_set, dim=1, keepdim=True)\n        return min_loss, min_loss_idx[:, 0]\n\n    @staticmethod\n    def find_best_perm(pair_wise_losses, n_src, perm_reduce=None, **kwargs):\n        """"""Find the best permutation, given the pair-wise losses.\n\n        Args:\n            pair_wise_losses (:class:`torch.Tensor`):\n                Tensor of shape [batch, n_src, n_src]. Pairwise losses.\n            n_src (int): Number of sources.\n            perm_reduce (Callable): torch function to reduce permutation losses.\n                Defaults to None (equivalent to mean). Signature of the func\n                (pwl_set, **kwargs) : (B, n_src!, n_src) --> (B, n_src!)\n            **kwargs: additional keyword argument that will be passed to the\n                permutation reduce function.\n\n        Returns:\n            tuple:\n                :class:`torch.Tensor`: The loss corresponding to the best\n                permutation of size (batch,).\n\n                :class:`torch.LongTensor`: The indexes of the best permutations.\n\n        MIT Copyright (c) 2018 Kaituo XU.\n        See `Original code\n        <https://github.com/kaituoxu/Conv-TasNet/blob/master>`__ and `License\n        <https://github.com/kaituoxu/Conv-TasNet/blob/master/LICENSE>`__.\n        """"""\n        # After transposition, dim 1 corresp. to sources and dim 2 to estimates\n        pwl = pair_wise_losses.transpose(-1, -2)\n        perms = pwl.new_tensor(list(permutations(range(n_src))),\n                               dtype=torch.long)\n        # Column permutation indices\n        idx = torch.unsqueeze(perms, 2)\n        # Loss mean of each permutation\n        if perm_reduce is None:\n            # one-hot, [n_src!, n_src, n_src]\n            perms_one_hot = pwl.new_zeros((*perms.size(), n_src)).scatter_(2, idx, 1)\n            loss_set = torch.einsum(\'bij,pij->bp\', [pwl, perms_one_hot])\n            loss_set /= n_src\n        else:\n            batch = pwl.shape[0]\n            n_perm = idx.shape[0]\n            # [batch, n_src!, n_src] : Pairwise losses for each permutation.\n            pwl_set = pwl[:, torch.arange(n_src), idx.squeeze(-1)]\n            # Apply reduce [batch, n_src!, n_src] --> [batch, n_src!]\n            loss_set = perm_reduce(pwl_set, **kwargs)\n        # Indexes and values of min losses for each batch element\n        min_loss_idx = torch.argmin(loss_set, dim=1)\n        min_loss, _ = torch.min(loss_set, dim=1, keepdim=True)\n        return min_loss, min_loss_idx\n\n    @staticmethod\n    def reorder_source(source, n_src, min_loss_idx):\n        """""" Reorder sources according to the best permutation.\n\n        Args:\n            source (torch.Tensor): Tensor of shape [batch, n_src, time]\n            n_src (int): Number of sources.\n            min_loss_idx (torch.LongTensor): Tensor of shape [batch],\n                each item is in [0, n_src!).\n\n        Returns:\n            :class:`torch.Tensor`:\n                Reordered sources of shape [batch, n_src, time].\n\n        MIT Copyright (c) 2018 Kaituo XU.\n        See `Original code\n        <https://github.com/kaituoxu/Conv-TasNet/blob/master>`__ and `License\n        <https://github.com/kaituoxu/Conv-TasNet/blob/master/LICENSE>`__.\n        """"""\n        perms = source.new_tensor(list(permutations(range(n_src))),\n                                  dtype=torch.long)\n        # Reorder estimate targets according the best permutation\n        min_loss_perm = torch.index_select(perms, dim=0, index=min_loss_idx)\n        # maybe use torch.gather()/index_select()/scatter() to impl this?\n        reordered_sources = torch.zeros_like(source)\n        for b in range(source.shape[0]):\n            for c in range(n_src):\n                reordered_sources[b, c] = source[b, min_loss_perm[b][c]]\n        return reordered_sources\n'"
asteroid/losses/pmsqe.py,49,"b'import numpy as np\nimport torch\nfrom torch import tensor\nimport torch.nn as nn\nfrom scipy.io import loadmat\nimport pathlib\nimport os\n\n\nclass SingleSrcPMSQE(nn.Module):\n    """""" Computes the Perceptual Metric for Speech Quality Evaluation (PMSQE)\n    as described in [1].\n    This version is only designed for 16 kHz (512 length DFT).\n    Adaptation to 8 kHz could be done by changing the parameters of the\n    class (see Tensorflow implementation).\n    The SLL, frequency and gain equalization are applied in each\n    sequence independently.\n\n    Parameters:\n        window_name (str): Select the used window function for the correct\n            factor to be applied. Defaults to sqrt hanning window.\n            Among [\'rect\', \'hann\', \'sqrt_hann\', \'hamming\', \'flatTop\'].\n        window_weight (float, optional): Correction to the window factor\n            applied.\n        bark_eq (bool, optional): Whether to apply bark equalization.\n        gain_eq (bool, optional): Whether to apply gain equalization.\n        sample_rate (int): Sample rate of the input audio.\n\n    References:\n        [1] J.M.Martin, A.M.Gomez, J.A.Gonzalez, A.M.Peinado \'A Deep Learning\n        Loss Function based on the Perceptual Evaluation of the\n        Speech Quality\', IEEE Signal Processing Letters, 2018.\n        Implemented by Juan M. Martin. Contact: mdjuamart@ugr.es\n        Copyright 2019: University of Granada, Signal Processing, Multimedia\n        Transmission and Speech/Audio Technologies (SigMAT) Group.\n\n    Notes:\n        Inspired on the Perceptual Evaluation of the Speech Quality (PESQ)\n        algorithm, this function consists of two regularization factors :\n        the symmetrical and asymmetrical distortion in the loudness domain.\n\n    Examples:\n        >>> import torch\n        >>> from asteroid.filterbanks import STFTFB, Encoder, transforms\n        >>> from asteroid.losses import PITLossWrapper, SingleSrcPMSQE\n        >>> stft = Encoder(STFTFB(kernel_size=512, n_filters=512, stride=256))\n        >>> # Usage by itself\n        >>> ref, est = torch.randn(2, 1, 16000), torch.randn(2, 1, 16000)\n        >>> ref_spec = transforms.take_mag(stft(ref))\n        >>> est_spec = transforms.take_mag(stft(est))\n        >>> loss_func = SingleSrcPMSQE()\n        >>> loss_value = loss_func(est_spec, ref_spec)\n        >>> # Usage with PITLossWrapper\n        >>> loss_func = PITLossWrapper(SingleSrcPMSQE(), pit_from=\'pw_pt\')\n        >>> ref, est = torch.randn(2, 3, 16000), torch.randn(2, 3, 16000)\n        >>> ref_spec = transforms.take_mag(stft(ref))\n        >>> est_spec = transforms.take_mag(stft(est))\n        >>> loss_value = loss_func(ref_spec, est_spec)\n    """"""\n    def __init__(self, window_name=\'sqrt_hann\', window_weight=1.0,\n                 bark_eq=True, gain_eq=True, sample_rate=16000):\n        super().__init__()\n        self.window_name = window_name\n        self.window_weight = window_weight\n        self.bark_eq = bark_eq\n        self.gain_eq = gain_eq\n\n        if sample_rate not in [16000, 8000]:\n            raise ValueError(""Unsupported sample rate {}"".format(sample_rate))\n        self.sample_rate = sample_rate\n        if sample_rate == 16000:\n            self.Sp = 6.910853e-006\n            self.Sl = 1.866055e-001\n            self.nbins = 512\n            self.nbark = 49\n        else:\n            self.Sp = 2.764344e-5\n            self.Sl = 1.866055e-1\n            self.nbins = 256\n            self.nbark = 42\n        # As described in [1] and used in the TF implementation.\n        self.alpha = 0.1\n        self.beta = 0.309 * self.alpha\n\n        pow_correc_factor = self.get_correction_factor(window_name)\n        self.pow_correc_factor = pow_correc_factor * self.window_weight\n        # Initialize to None and populate as a function of sample rate.\n        self.abs_thresh_power = None\n        self.modified_zwicker_power = None\n        self.width_of_band_bark = None\n        self.bark_matrix = None\n        self.mask_sll = None\n        self.populate_constants(self.sample_rate)\n        self.sqrt_total_width = torch.sqrt(torch.sum(self.width_of_band_bark))\n        self.EPS = 1e-8\n\n    def forward(self, est_targets, targets, pad_mask=None):\n        """"""\n        Args\n            est_targets (torch.Tensor): Dimensions (B, T, F).\n                Padded degraded power spectrum in time-frequency domain.\n            targets (torch.Tensor): Dimensions (B, T, F).\n                Zero-Padded reference power spectrum in time-frequency domain.\n            pad_mask (torch.Tensor, optional):  Dimensions (B, T, 1). Mask\n                to indicate the padding frames. Defaults to all ones.\n\n        Dimensions\n            B: Number of sequences in the batch.\n            T: Number of time frames.\n            F: Number of frequency bins.\n\n        Returns\n            torch.tensor of shape (B, ), wD + 0.309 * wDA\n\n        Notes\n            Dimensions (B, F, T) are also supported by SingleSrcPMSQE but are\n            less efficient because input tensors are transposed (not inplace).\n\n        Examples\n\n        """"""\n        assert est_targets.shape == targets.shape\n        # Need transpose? Find it out\n        try:\n            freq_idx = est_targets.shape.index(self.nbins//2 + 1)\n        except ValueError:\n            raise ValueError(""Could not find dimension with {} elements in ""\n                             ""input tensors, verify your inputs""\n                             """".format(self.nbins//2 + 1))\n        if freq_idx == 1:\n            est_targets = est_targets.transpose(1, 2)\n            targets = targets.transpose(1, 2)\n        if pad_mask is not None:\n            # Transpose the pad mask as well if needed.\n            pad_mask = pad_mask.transpose(1, 2) if freq_idx == 1 else pad_mask\n        else:\n            # Suppose no padding if no pad_mask is provided.\n            pad_mask = torch.ones(est_targets.shape[0], est_targets.shape[1], 1, device=est_targets.device)\n        # SLL equalization\n        ref_spectra = self.magnitude_at_sll(targets, pad_mask)\n        deg_spectra = self.magnitude_at_sll(est_targets, pad_mask)\n\n        # Bark spectra computation\n        ref_bark_spectra = self.bark_computation(ref_spectra)\n        deg_bark_spectra = self.bark_computation(deg_spectra)\n\n        # (Optional) frequency and gain equalization\n        if self.bark_eq:\n            deg_bark_spectra = self.bark_freq_equalization(ref_bark_spectra,\n                                                           deg_bark_spectra)\n\n        if self.gain_eq:\n            deg_bark_spectra = self.bark_gain_equalization(ref_bark_spectra,\n                                                           deg_bark_spectra)\n\n        # Distortion matrix computation\n        sym_d, asym_d = self.compute_distortion_tensors(ref_bark_spectra,\n                                                        deg_bark_spectra)\n\n        # Per-frame distortion\n        audible_power_ref = self.compute_audible_power(ref_bark_spectra, 1.0)\n        wd_frame, wda_frame = self.per_frame_distortion(sym_d, asym_d,\n                                                        audible_power_ref)\n        # Mean distortions over frames : keep batch dims\n        dims = [-1, -2]\n        pmsqe_frame = (self.alpha * wd_frame + self.beta * wda_frame) * pad_mask\n        pmsqe = torch.sum(pmsqe_frame, dim=dims) / pad_mask.sum(dims)\n        return pmsqe\n\n    def magnitude_at_sll(self, spectra, pad_mask):\n        # Apply padding and SLL masking\n        masked_spectra = spectra * pad_mask * self.mask_sll\n        # Compute mean over frequency\n        freq_mean_masked_spectra = torch.mean(masked_spectra, dim=-1,\n                                              keepdim=True)\n        # Compute mean over time (taking into account padding)\n        sum_spectra = torch.sum(freq_mean_masked_spectra, dim=-2, keepdim=True)\n        seq_len = torch.sum(pad_mask, dim=-2, keepdim=True)\n        mean_pow = sum_spectra / seq_len\n        # Compute final SLL spectra\n        return 10000000.0 * spectra / mean_pow\n\n    def bark_computation(self, spectra):\n        return self.Sp * torch.matmul(spectra, self.bark_matrix)\n\n    def compute_audible_power(self, bark_spectra, factor=1.0):\n        # Apply absolute hearing threshold to each band\n        thr_bark = torch.where(bark_spectra > self.abs_thresh_power * factor,\n                               bark_spectra, torch.zeros_like(bark_spectra))\n        # Sum band power over frequency\n        return torch.sum(thr_bark, dim=-1, keepdim=True)\n\n    def bark_gain_equalization(self, ref_bark_spectra, deg_bark_spectra):\n        # Compute audible power\n        audible_power_ref = self.compute_audible_power(ref_bark_spectra, 1.0)\n        audible_power_deg = self.compute_audible_power(deg_bark_spectra, 1.0)\n        # Compute gain factor\n        gain = (audible_power_ref + 5.0e3) / (audible_power_deg + 5.0e3)\n        # Limit the range of the gain factor\n        limited_gain = torch.min(gain, 5.0 * torch.ones_like(gain))\n        limited_gain = torch.max(limited_gain, 3.0e-4 *\n                                 torch.ones_like(limited_gain))\n        # Apply gain correction on degraded\n        return limited_gain * deg_bark_spectra\n\n    def bark_freq_equalization(self, ref_bark_spectra, deg_bark_spectra):\n        """"""This version is applied in the degraded directly.""""""\n        # Identification of speech active frames\n        audible_power_x100 = self.compute_audible_power(ref_bark_spectra, 100.0)\n        not_silent = audible_power_x100 >= 1.0e7\n        # Threshold for active bark bins\n        cond_thr = ref_bark_spectra >= self.abs_thresh_power * 100.0\n        ref_thresholded = torch.where(cond_thr, ref_bark_spectra,\n                                      torch.zeros_like(ref_bark_spectra))\n        deg_thresholded = torch.where(cond_thr, deg_bark_spectra,\n                                      torch.zeros_like(deg_bark_spectra))\n        # Total power per bark bin (ppb)\n        avg_ppb_ref = torch.sum(torch.where(not_silent, ref_thresholded,\n                                            torch.zeros_like(ref_thresholded)),\n                                dim=-2, keepdim=True)\n        avg_ppb_deg = torch.sum(torch.where(not_silent, deg_thresholded,\n                                            torch.zeros_like(deg_thresholded)),\n                                dim=-2, keepdim=True)\n        # Compute equalizer\n        equalizer = (avg_ppb_ref + 1000.0) / (avg_ppb_deg + 1000.0)\n        equalizer = torch.min(equalizer, 100.0 * torch.ones_like(equalizer))\n        equalizer = torch.max(equalizer, 0.01 * torch.ones_like(equalizer))\n        # Apply frequency correction on degraded\n        return equalizer * deg_bark_spectra\n\n    def loudness_computation(self, bark_spectra):\n        # Bark spectra transformed to a sone loudness scale using Zwicker\'s law\n        aterm = torch.pow(self.abs_thresh_power / 0.5,\n                          self.modified_zwicker_power)\n        bterm = torch.pow(0.5 + 0.5 * bark_spectra / self.abs_thresh_power,\n                          self.modified_zwicker_power) - 1.0\n        loudness_dens = self.Sl * aterm * bterm\n        cond = bark_spectra < self.abs_thresh_power\n        return torch.where(cond, torch.zeros_like(loudness_dens), loudness_dens)\n\n    def compute_distortion_tensors(self, ref_bark_spec, deg_bark_spec):\n        # After bark spectra are compensated, transform to sone loudness\n        original_loudness = self.loudness_computation(ref_bark_spec)\n        distorted_loudness = self.loudness_computation(deg_bark_spec)\n        # Loudness difference\n        r = torch.abs(distorted_loudness - original_loudness)\n        # Masking effect computation\n        m = 0.25 * torch.min(original_loudness, distorted_loudness)\n        # Center clipping using masking effect\n        sym_d = torch.max(r - m, torch.ones_like(r)*self.EPS)\n        # Asymmetry factor computation\n        asym = torch.pow((deg_bark_spec + 50.0) / (ref_bark_spec + 50.0), 1.2)\n        cond = asym < 3.0 * torch.ones_like(asym)\n        asym_factor = torch.where(cond, torch.zeros_like(asym),\n                                  torch.min(asym, 12.0 * torch.ones_like(asym)))\n        # Asymmetric Disturbance matrix computation\n        asym_d = asym_factor * sym_d\n        return sym_d, asym_d\n\n    def per_frame_distortion(self, sym_d, asym_d, total_power_ref):\n        # Computation of the norms over bark bands for each frame\n        # 2 and 1 for sym_d and asym_d, respectively\n        d_frame = torch.sum(torch.pow(sym_d * self.width_of_band_bark, 2.0)+self.EPS,\n                            dim=-1, keepdim=True)\n#        a = torch.pow(sym_d * self.width_of_band_bark, 2.0)\n#        b = sym_d\n#        print(a.min(),a.max(),b.min(),b.max(), d_frame.min(), d_frame.max())\n#        print(self.width_of_band_bark.requires_grad)\n#        print(d_frame.requires_grad)\n        d_frame = torch.sqrt(d_frame) * self.sqrt_total_width\n        da_frame = torch.sum(asym_d * self.width_of_band_bark,\n                             dim=-1, keepdim=True)\n        # Weighting by the audible power raised to 0.04\n        weights = torch.pow((total_power_ref + 1e5) / 1e7, 0.04)\n        # Bounded computation of the per frame distortion metric\n        wd_frame = torch.min(d_frame / weights,\n                             45.0 * torch.ones_like(d_frame))\n        wda_frame = torch.min(da_frame / weights,\n                              45.0 * torch.ones_like(da_frame))\n        return wd_frame, wda_frame\n\n    @staticmethod\n    def get_correction_factor(window_name):\n        """""" Returns the power correction factor depending on the window. """"""\n        if window_name == \'rect\':\n            return 1.0\n        elif window_name == \'hann\':\n            return 2.666666666666754\n        elif window_name == \'sqrt_hann\':\n            return 2.0\n        elif window_name == \'hamming\':\n            return 2.51635879188799\n        elif window_name == \'flatTop\':\n            return 5.70713295690759\n        else:\n            raise ValueError(\'Unexpected window type {}\'.format(window_name))\n\n    def populate_constants(self, sample_rate):\n        if sample_rate == 8000:\n            self.register_8k_constants()\n        elif sample_rate == 16000:\n            self.register_16k_constants()\n        # Mask SSL\n        mask_sll = np.zeros(shape=[self.nbins // 2 + 1], dtype=np.float32)\n        mask_sll[11] = 0.5 * 25.0 / 31.25\n        mask_sll[12:104] = 1.0\n        mask_sll[104] = 0.5\n        correction = self.pow_correc_factor * (self.nbins + 2.0) / self.nbins**2\n        mask_sll = mask_sll * correction\n        self.mask_sll = nn.Parameter(tensor(mask_sll), requires_grad=False)\n\n    def register_16k_constants(self):\n        # Absolute threshold power\n        abs_thresh_power = [\n            51286152.00, 2454709.500, 70794.593750, 4897.788574, 1174.897705,\n            389.045166, 104.712860, 45.708820, 17.782795, 9.772372, 4.897789,\n            3.090296, 1.905461, 1.258925, 0.977237, 0.724436, 0.562341,\n            0.457088, 0.389045, 0.331131, 0.295121, 0.269153, 0.257040,\n            0.251189, 0.251189, 0.251189, 0.251189, 0.263027, 0.288403,\n            0.309030, 0.338844, 0.371535, 0.398107, 0.436516, 0.467735,\n            0.489779, 0.501187, 0.501187, 0.512861, 0.524807, 0.524807,\n            0.524807, 0.512861, 0.478630, 0.426580, 0.371535, 0.363078,\n            0.416869, 0.537032]\n        self.abs_thresh_power = nn.Parameter(tensor(abs_thresh_power),\n                                             requires_grad=False)\n        # Modified zwicker power\n        modif_zwicker_power = [\n            0.25520097857560436, 0.25520097857560436, 0.25520097857560436,\n            0.25520097857560436, 0.25168783742879913, 0.24806665731869609,\n            0.244767379124259, 0.24173800119368227, 0.23893798876066405,\n            0.23633516221479894, 0.23390360348392067, 0.23162209128929445,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23,\n            0.23, 0.23, 0.23, 0.23]\n        self.modified_zwicker_power = nn.Parameter(tensor(modif_zwicker_power),\n                                                   requires_grad=False)\n        # Width of band bark\n        width_of_band_bark = [\n            0.157344, 0.317994, 0.322441, 0.326934, 0.331474, 0.336061,\n            0.340697, 0.345381, 0.350114, 0.354897, 0.359729, 0.364611,\n            0.369544, 0.374529, 0.379565, 0.384653, 0.389794, 0.394989,\n            0.400236, 0.405538, 0.410894, 0.416306, 0.421773, 0.427297,\n            0.432877, 0.438514, 0.444209, 0.449962, 0.455774, 0.461645,\n            0.467577, 0.473569, 0.479621, 0.485736, 0.491912, 0.498151,\n            0.504454, 0.510819, 0.517250, 0.523745, 0.530308, 0.536934,\n            0.543629, 0.550390, 0.557220, 0.564119, 0.571085, 0.578125,\n            0.585232]\n        self.width_of_band_bark = nn.Parameter(tensor(width_of_band_bark),\n                                               requires_grad=False)\n        # Bark matrix\n        local_path = pathlib.Path(__file__).parent.absolute()\n        bark_path = os.path.join(local_path, \'bark_matrix_16k.mat\')\n        bark_matrix = loadmat(bark_path)[""Bark_matrix_16k""].astype(\'float32\')\n        self.bark_matrix = nn.Parameter(tensor(bark_matrix),\n                                        requires_grad=False)\n\n    def register_8k_constants(self):\n        # Absolute threshold power\n        abs_thresh_power = [\n            51286152, 2454709.500, 70794.593750, 4897.788574, 1174.897705,\n            389.045166, 104.712860, 45.708820, 17.782795, 9.772372, 4.897789,\n            3.090296, 1.905461, 1.258925, 0.977237, 0.724436, 0.562341,\n            0.457088, 0.389045, 0.331131, 0.295121, 0.269153, 0.257040,\n            0.251189, 0.251189, 0.251189, 0.251189, 0.263027, 0.288403,\n            0.309030, 0.338844, 0.371535, 0.398107, 0.436516, 0.467735,\n            0.489779, 0.501187, 0.501187, 0.512861, 0.524807, 0.524807,\n            0.524807]\n        self.abs_thresh_power = nn.Parameter(tensor(abs_thresh_power),\n                                             requires_grad=False)\n        # Modified zwicker power\n        modif_zwicker_power = [\n            0.25520097857560436, 0.25520097857560436, 0.25520097857560436,\n            0.25520097857560436, 0.25168783742879913, 0.24806665731869609,\n            0.244767379124259, 0.24173800119368227, 0.23893798876066405,\n            0.23633516221479894, 0.23390360348392067, 0.23162209128929445,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23,\n            0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23, 0.23]\n        self.modified_zwicker_power = nn.Parameter(tensor(modif_zwicker_power),\n                                                   requires_grad=False)\n        # Width of band bark\n        width_of_band_bark = [\n            0.157344, 0.317994, 0.322441, 0.326934, 0.331474, 0.336061,\n            0.340697, 0.345381, 0.350114, 0.354897, 0.359729, 0.364611,\n            0.369544, 0.374529, 0.379565, 0.384653, 0.389794, 0.394989,\n            0.400236, 0.405538, 0.410894, 0.416306, 0.421773, 0.427297,\n            0.432877, 0.438514, 0.444209, 0.449962, 0.455774, 0.461645,\n            0.467577, 0.473569, 0.479621, 0.485736, 0.491912, 0.498151,\n            0.504454, 0.510819, 0.517250, 0.523745, 0.530308, 0.536934]\n        self.width_of_band_bark = nn.Parameter(tensor(width_of_band_bark),\n                                               requires_grad=False)\n        # Bark matrix\n        local_path = pathlib.Path(__file__).parent.absolute()\n        bark_path = os.path.join(local_path, \'bark_matrix_8k.mat\')\n        bark_matrix = loadmat(bark_path)[""Bark_matrix_8k""].astype(\'float32\')\n        self.bark_matrix = nn.Parameter(tensor(bark_matrix),\n                                        requires_grad=False)\n'"
asteroid/losses/sdr.py,40,"b'import torch\nfrom torch.nn.modules.loss import _Loss\nfrom ..utils.deprecation_utils import DeprecationMixin\n\nEPS = 1e-8\n\n\nclass PairwiseNegSDR(_Loss):\n    """""" Base class for pairwise negative SI-SDR, SD-SDR and SNR on a batch.\n\n        Args:\n            sdr_type (str): choose between ""snr"" for plain SNR, ""sisdr"" for\n                SI-SDR and ""sdsdr"" for SD-SDR [1].\n            zero_mean (bool, optional): by default it zero mean the target\n                and estimate before computing the loss.\n            take_log (bool, optional): by default the log10 of sdr is returned.\n\n        Shape:\n            est_targets (:class:`torch.Tensor`): Expected shape\n                [batch, n_src, time]. Batch of target estimates.\n            targets (:class:`torch.Tensor`): Expected shape\n                [batch, n_src, time]. Batch of training targets.\n\n        Returns:\n            :class:`torch.Tensor`: with shape [batch, n_src, n_src].\n            Pairwise losses.\n\n        Examples:\n\n            >>> import torch\n            >>> from asteroid.losses import PITLossWrapper\n            >>> targets = torch.randn(10, 2, 32000)\n            >>> est_targets = torch.randn(10, 2, 32000)\n            >>> loss_func = PITLossWrapper(PairwiseNegSDR(""sisdr""),\n            >>>                            pit_from=\'pairwise\')\n            >>> loss = loss_func(est_targets, targets)\n\n        References:\n            [1] Le Roux, Jonathan, et al. ""SDR half-baked or well done."" IEEE\n            International Conference on Acoustics, Speech and Signal\n            Processing (ICASSP) 2019.\n        """"""\n    def __init__(self, sdr_type, zero_mean=True, take_log=True):\n        super(PairwiseNegSDR, self).__init__()\n        assert sdr_type in [""snr"", ""sisdr"", ""sdsdr""]\n        self.sdr_type = sdr_type\n        self.zero_mean = zero_mean\n        self.take_log = take_log\n\n    def forward(self, est_targets, targets):\n        assert targets.size() == est_targets.size()\n        # Step 1. Zero-mean norm\n        if self.zero_mean:\n            mean_source = torch.mean(targets, dim=2, keepdim=True)\n            mean_estimate = torch.mean(est_targets, dim=2, keepdim=True)\n            targets = targets - mean_source\n            est_targets = est_targets - mean_estimate\n        # Step 2. Pair-wise SI-SDR. (Reshape to use broadcast)\n        s_target = torch.unsqueeze(targets, dim=1)\n        s_estimate = torch.unsqueeze(est_targets, dim=2)\n\n        if self.sdr_type in [""sisdr"", ""sdsdr""]:\n            # [batch, n_src, n_src, 1]\n            pair_wise_dot = torch.sum(s_estimate * s_target, dim=3,\n                                      keepdim=True)\n            # [batch, 1, n_src, 1]\n            s_target_energy = torch.sum(s_target**2, dim=3, keepdim=True) + EPS\n            # [batch, n_src, n_src, time]\n            pair_wise_proj = pair_wise_dot * s_target / s_target_energy\n        else:\n            # [batch, n_src, n_src, time]\n            pair_wise_proj = s_target.repeat(1, s_target.shape[2], 1, 1)\n        if self.sdr_type in [""sdsdr"", ""snr""]:\n            e_noise = s_estimate - s_target\n        else:\n            e_noise = s_estimate - pair_wise_proj\n        # [batch, n_src, n_src]\n        pair_wise_sdr = torch.sum(pair_wise_proj ** 2, dim=3) / (\n                torch.sum(e_noise ** 2, dim=3) + EPS)\n        if self.take_log:\n            pair_wise_sdr = 10 * torch.log10(pair_wise_sdr + EPS)\n        return - pair_wise_sdr\n\n\nclass SingleSrcNegSDR(_Loss):\n    """""" Base class for single-source negative SI-SDR, SD-SDR and SNR.\n\n        Args:\n            sdr_type (string): choose between ""snr"" for plain SNR, ""sisdr"" for\n                SI-SDR and ""sdsdr"" for SD-SDR [1].\n            zero_mean (bool, optional): by default it zero mean the target and\n                estimate before computing the loss.\n            take_log (bool, optional): by default the log10 of sdr is returned.\n            reduction (string, optional): Specifies the reduction to apply to\n                the output:\n            ``\'none\'`` | ``\'mean\'``. ``\'none\'``: no reduction will be applied,\n            ``\'mean\'``: the sum of the output will be divided by the number of\n            elements in the output.\n\n        Shape:\n            est_targets (:class:`torch.Tensor`): Expected shape [batch, time].\n                Batch of target estimates.\n            targets (:class:`torch.Tensor`): Expected shape [batch, time].\n                Batch of training targets.\n\n        Returns:\n            :class:`torch.Tensor`: with shape [batch] if reduction=\'none\' else\n                [] scalar if reduction=\'mean\'.\n\n        Examples:\n\n            >>> import torch\n            >>> from asteroid.losses import PITLossWrapper\n            >>> targets = torch.randn(10, 2, 32000)\n            >>> est_targets = torch.randn(10, 2, 32000)\n            >>> loss_func = PITLossWrapper(SingleSrcNegSDR(""sisdr""),\n            >>>                            pit_from=\'pw_pt\')\n            >>> loss = loss_func(est_targets, targets)\n\n        References:\n            [1] Le Roux, Jonathan, et al. ""SDR half-baked or well done."" IEEE\n            International Conference on Acoustics, Speech and Signal\n            Processing (ICASSP) 2019.\n        """"""\n    def __init__(self, sdr_type, zero_mean=True, take_log=True,\n                 reduction=\'none\'):\n        assert reduction != \'sum\', NotImplementedError\n        super().__init__(reduction=reduction)\n\n        assert sdr_type in [""snr"", ""sisdr"", ""sdsdr""]\n        self.sdr_type = sdr_type\n        self.zero_mean = zero_mean\n        self.take_log = take_log\n\n    def forward(self, est_target, target):\n        assert target.size() == est_target.size()\n        # Step 1. Zero-mean norm\n        if self.zero_mean:\n            mean_source = torch.mean(target, dim=1, keepdim=True)\n            mean_estimate = torch.mean(est_target, dim=1, keepdim=True)\n            target = target - mean_source\n            est_target = est_target - mean_estimate\n        # Step 2. Pair-wise SI-SDR.\n        if self.sdr_type in [""sisdr"", ""sdsdr""]:\n            # [batch, 1]\n            dot = torch.sum(est_target * target, dim=1, keepdim=True)\n            # [batch, 1]\n            s_target_energy = torch.sum(target ** 2, dim=1,\n                                        keepdim=True) + EPS\n            # [batch, time]\n            scaled_target = dot * target / s_target_energy\n        else:\n            # [batch, time]\n            scaled_target = target\n        if self.sdr_type in [""sdsdr"", ""snr""]:\n            e_noise = est_target - target\n        else:\n            e_noise = est_target - scaled_target\n        # [batch]\n        losses = torch.sum(scaled_target ** 2, dim=1) / (\n                torch.sum(e_noise ** 2, dim=1) + EPS)\n        if self.take_log:\n            losses = 10 * torch.log10(losses + EPS)\n        losses = losses.mean() if self.reduction == \'mean\' else losses\n        return - losses\n\n\nclass MultiSrcNegSDR(_Loss):\n    """""" Base class for computing negative SI-SDR, SD-SDR and SNR for a given\n        permutation of source and their estimates.\n\n        Args:\n            sdr_type (string): choose between ""snr"" for plain SNR, ""sisdr"" for\n                SI-SDR and ""sdsdr"" for SD-SDR [1].\n            zero_mean (bool, optional): by default it zero mean the target\n                and estimate before computing the loss.\n            take_log (bool, optional): by default the log10 of sdr is returned.\n\n        Shape:\n            est_targets (:class:`torch.Tensor`): Expected shape [batch, time].\n                Batch of target estimates.\n            targets (:class:`torch.Tensor`): Expected shape [batch, time].\n                Batch of training targets.\n\n        Returns:\n            :class:`torch.Tensor`: with shape [batch] if reduction=\'none\' else\n                [] scalar if reduction=\'mean\'.\n\n        Examples:\n\n            >>> import torch\n            >>> from asteroid.losses import PITLossWrapper\n            >>> targets = torch.randn(10, 2, 32000)\n            >>> est_targets = torch.randn(10, 2, 32000)\n            >>> loss_func = PITLossWrapper(MultiSrcNegSDR(""sisdr""),\n            >>>                            pit_from=\'perm_avg\')\n            >>> loss = loss_func(est_targets, targets)\n\n        References:\n            [1] Le Roux, Jonathan, et al. ""SDR half-baked or well done."" IEEE\n            International Conference on Acoustics, Speech and Signal\n            Processing (ICASSP) 2019.\n\n        """"""\n    def __init__(self, sdr_type, zero_mean=True, take_log=True):\n        super().__init__()\n\n        assert sdr_type in [""snr"", ""sisdr"", ""sdsdr""]\n        self.sdr_type = sdr_type\n        self.zero_mean = zero_mean\n        self.take_log = take_log\n\n    def forward(self, est_targets, targets):\n        assert targets.size() == est_targets.size()\n        # Step 1. Zero-mean norm\n        if self.zero_mean:\n            mean_source = torch.mean(targets, dim=2, keepdim=True)\n            mean_estimate = torch.mean(est_targets, dim=2, keepdim=True)\n            targets = targets - mean_source\n            est_targets = est_targets - mean_estimate\n        # Step 2. Pair-wise SI-SDR.\n        if self.sdr_type in [""sisdr"", ""sdsdr""]:\n            # [batch, n_src]\n            pair_wise_dot = torch.sum(est_targets * targets, dim=2,\n                                      keepdim=True)\n            # [batch, n_src]\n            s_target_energy = torch.sum(targets ** 2, dim=2,\n                                        keepdim=True) + EPS\n            # [batch, n_src, time]\n            scaled_targets = pair_wise_dot * targets / s_target_energy\n        else:\n            # [batch, n_src, time]\n            scaled_targets = targets\n        if self.sdr_type in [""sdsdr"", ""snr""]:\n            e_noise = est_targets - targets\n        else:\n            e_noise = est_targets - scaled_targets\n        # [batch, n_src]\n        pair_wise_sdr = torch.sum(scaled_targets ** 2, dim=2) / (\n                torch.sum(e_noise ** 2, dim=2) + EPS)\n        if self.take_log:\n            pair_wise_sdr = 10 * torch.log10(pair_wise_sdr + EPS)\n        return - torch.mean(pair_wise_sdr, dim=-1)\n\n\n# aliases\npairwise_neg_sisdr = PairwiseNegSDR(""sisdr"")\npairwise_neg_sdsdr = PairwiseNegSDR(""sdsdr"")\npairwise_neg_snr = PairwiseNegSDR(""snr"")\nsinglesrc_neg_sisdr = SingleSrcNegSDR(""sisdr"")\nsinglesrc_neg_sdsdr = SingleSrcNegSDR(""sdsdr"")\nsinglesrc_neg_snr = SingleSrcNegSDR(""snr"")\nmultisrc_neg_sisdr = MultiSrcNegSDR(""sisdr"")\nmultisrc_neg_sdsdr = MultiSrcNegSDR(""sdsdr"")\nmultisrc_neg_snr = MultiSrcNegSDR(""snr"")\n\n\n# Legacy\nclass NonPitSDR(MultiSrcNegSDR, DeprecationMixin):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.warn_deprecated()\n\n\nclass NoSrcSDR(SingleSrcNegSDR, DeprecationMixin):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.warn_deprecated()\n\n\nnosrc_neg_sisdr = singlesrc_neg_sisdr\nnosrc_neg_sdsdr = singlesrc_neg_sdsdr\nnosrc_neg_snr = singlesrc_neg_snr\nnonpit_neg_sisdr = multisrc_neg_sisdr\nnonpit_neg_sdsdr = multisrc_neg_sdsdr\nnonpit_neg_snr = multisrc_neg_snr\n'"
asteroid/losses/stoi.py,2,"b'from torch_stoi import NegSTOILoss\n\nasteroid_examples = """"""\nExamples:\n    >>> import torch\n    >>> from asteroid.losses import PITLossWrapper\n    >>> targets = torch.randn(10, 2, 32000)\n    >>> est_targets = torch.randn(10, 2, 32000)\n    >>> loss_func = PITLossWrapper(NegSTOILoss(), pit_from=\'pw_pt\')\n    >>> loss = loss_func(est_targets, targets)\n""""""\n\nNegSTOILoss.__doc__ += asteroid_examples\n'"
asteroid/masknn/__init__.py,0,b'from .convolutional import TDConvNet\nfrom .recurrent import DPRNN\n'
asteroid/masknn/activations.py,0,"b'from torch import nn\n\n\ndef linear():\n    return nn.Identity()\n\n\ndef relu():\n    return nn.ReLU()\n\n\ndef prelu():\n    return nn.PReLU()\n\n\ndef leaky_relu():\n    return nn.LeakyReLU()\n\n\ndef sigmoid():\n    return nn.Sigmoid()\n\n\ndef softmax(dim=None):\n    return nn.Softmax(dim=dim)\n\n\ndef tanh():\n    return nn.Tanh()\n\n\ndef get(identifier):\n    """""" Returns an activation function from a string. Returns its input if it\n    is callable (already an activation for example).\n\n    Args:\n        identifier (str or Callable or None): the activation identifier.\n\n    Returns:\n        :class:`nn.Module` or None\n    """"""\n    if identifier is None:\n        return None\n    elif callable(identifier):\n        return identifier\n    elif isinstance(identifier, str):\n        cls = globals().get(identifier)\n        if cls is None:\n            raise ValueError(\'Could not interpret activation identifier: \' +\n                             str(identifier))\n        return cls\n    else:\n        raise ValueError(\'Could not interpret activation identifier: \' +\n                         str(identifier))\n'"
asteroid/masknn/blocks.py,0,"b'import warnings\nfrom numpy import VisibleDeprecationWarning\nwarnings.warn(""`blocks` has been splited between `convolutional` and ""\n              ""`recurrent` since asteroid v0.2.0 and will be removed ""\n              ""in v0.3.0"", VisibleDeprecationWarning)\nfrom .convolutional import *\nfrom .recurrent import *\n'"
asteroid/masknn/consistency.py,10,"b'import torch\n\n\ndef mixture_consistency(mixture, est_sources, src_weights=None, dim=1):\n    """""" Applies mixture consistency to a tensor of estimated sources.\n\n    Args\n        mixture (torch.Tensor): Mixture waveform or TF representation.\n        est_sources (torch.Tensor): Estimated sources waveforms or TF\n            representations.\n        src_weights (torch.Tensor): Consistency weight for each source.\n            Shape needs to be broadcastable to `est_source`.\n            We make sure that the weights sum up to 1 along dim `dim`.\n            If `src_weights` is None, compute them based on relative power.\n        dim (int): Axis which contains the sources in `est_sources`.\n\n    Returns\n        torch.Tensor with same shape as `est_sources`, after applying mixture\n        consistency.\n\n    Notes\n        This method can be used only in \'complete\' separation tasks, otherwise\n        the residual error will contain unwanted sources. For example, this\n        won\'t work with the task `sep_noisy` from WHAM.\n\n    Examples\n        >>> # Works on waveforms\n        >>> mix = torch.randn(10, 16000)\n        >>> est_sources = torch.randn(10, 2, 16000)\n        >>> new_est_sources = mixture_consistency(mix, est_sources, dim=1)\n        >>> # Also works on spectrograms\n        >>> mix = torch.randn(10, 514, 400)\n        >>> est_sources = torch.randn(10, 2, 514, 400)\n        >>> new_est_sources = mixture_consistency(mix, est_sources, dim=1)\n\n    References\n        Scott Wisdom, John R Hershey, Kevin Wilson, Jeremy Thorpe, Michael\n        Chinen, Brian Patton, and Rif A Saurous. ""Differentiable consistency\n        constraints for improved deep speech enhancement"", ICASSP 2019.\n    """"""\n    # If the source weights are not specified, the weights are the relative\n    # power of each source to the sum. w_i = P_i / (P_all), P for power.\n    if src_weights is None:\n        all_dims = list(range(est_sources.ndim))\n        all_dims.pop(dim)  # Remove source axis\n        all_dims.pop(0)  # Remove batch dim\n        src_weights = torch.mean(est_sources**2, dim=all_dims, keepdim=True)\n    # Make sure that the weights sum up to 1\n    norm_weights = torch.sum(src_weights, dim=dim, keepdim=True) + 1e-8\n    src_weights = src_weights / norm_weights\n\n    # Compute residual mix - sum(est_sources)\n    if mixture.ndim == est_sources.ndim - 1:\n        # mixture (batch, *), est_sources (batch, n_src, *)\n        residual = (mixture - est_sources.sum(dim=dim)).unsqueeze(dim)\n    elif mixture.ndim == est_sources.ndim:\n        # mixture (batch, 1, *), est_sources (batch, n_src, *)\n        residual = mixture - est_sources.sum(dim=dim, keepdim=True)\n    else:\n        n, m = est_sources.ndim, mixture.ndim\n        raise RuntimeError(f\'The size of the mixture tensor should match the \'\n                           f\'size of the est_sources tensor. Expected mixture\'\n                           f\'tensor to have {n} or {n-1} dimension, found {m}.\')\n    # Compute remove\n    new_sources = est_sources + src_weights * residual\n    return new_sources\n'"
asteroid/masknn/convolutional.py,2,"b'import torch\nfrom torch import nn\nimport warnings\nfrom numpy import VisibleDeprecationWarning\n\nfrom . import norms, activations\nfrom ..utils import has_arg\n\n\nclass Conv1DBlock(nn.Module):\n    """"""One dimensional convolutional block, as proposed in [1].\n\n    Args:\n        in_chan (int): Number of input channels.\n        hid_chan (int): Number of hidden channels in the depth-wise\n            convolution.\n        skip_out_chan (int): Number of channels in the skip convolution.\n            If 0 or None, `Conv1DBlock` won\'t have any skip connections.\n            Corresponds to the the block in v1 or the paper. The `forward`\n            return res instead of [res, skip] in this case.\n        kernel_size (int): Size of the depth-wise convolutional kernel.\n        padding (int): Padding of the depth-wise convolution.\n        dilation (int): Dilation of the depth-wise convolution.\n        norm_type (str, optional): Type of normalization to use. To choose from\n\n            -  ``\'gLN\'``: global Layernorm\n            -  ``\'cLN\'``: channelwise Layernorm\n            -  ``\'cgLN\'``: cumulative global Layernorm\n\n    References:\n        [1] : ""Conv-TasNet: Surpassing ideal time-frequency magnitude masking\n        for speech separation"" TASLP 2019 Yi Luo, Nima Mesgarani\n        https://arxiv.org/abs/1809.07454\n    """"""\n    def __init__(self, in_chan, hid_chan, skip_out_chan, kernel_size, padding,\n                 dilation, norm_type=""gLN""):\n        super(Conv1DBlock, self).__init__()\n        self.skip_out_chan = skip_out_chan\n        conv_norm = norms.get(norm_type)\n        in_conv1d = nn.Conv1d(in_chan, hid_chan, 1)\n        depth_conv1d = nn.Conv1d(hid_chan, hid_chan, kernel_size,\n                                 padding=padding, dilation=dilation,\n                                 groups=hid_chan)\n        self.shared_block = nn.Sequential(in_conv1d, nn.PReLU(),\n                                          conv_norm(hid_chan), depth_conv1d,\n                                          nn.PReLU(), conv_norm(hid_chan))\n        self.res_conv = nn.Conv1d(hid_chan, in_chan, 1)\n        if skip_out_chan:\n            self.skip_conv = nn.Conv1d(hid_chan, skip_out_chan, 1)\n\n    def forward(self, x):\n        """""" Input shape [batch, feats, seq]""""""\n        shared_out = self.shared_block(x)\n        res_out = self.res_conv(shared_out)\n        if not self.skip_out_chan:\n            return res_out\n        skip_out = self.skip_conv(shared_out)\n        return res_out, skip_out\n\n\nclass TDConvNet(nn.Module):\n    """""" Temporal Convolutional network used in ConvTasnet.\n\n    Args:\n        in_chan (int): Number of input filters.\n        n_src (int): Number of masks to estimate.\n        out_chan (int, optional): Number of bins in the estimated masks.\n            If ``None``, `out_chan = in_chan`.\n        n_blocks (int, optional): Number of convolutional blocks in each\n            repeat. Defaults to 8.\n        n_repeats (int, optional): Number of repeats. Defaults to 3.\n        bn_chan (int, optional): Number of channels after the bottleneck.\n        hid_chan (int, optional): Number of channels in the convolutional\n            blocks.\n        skip_chan (int, optional): Number of channels in the skip connections.\n            If 0 or None, TDConvNet won\'t have any skip connections and the\n            masks will be computed from the residual output.\n            Corresponds to the ConvTasnet architecture in v1 or the paper.\n        conv_kernel_size (int, optional): Kernel size in convolutional blocks.\n        norm_type (str, optional): To choose from ``\'BN\'``, ``\'gLN\'``,\n            ``\'cLN\'``.\n        mask_act (str, optional): Which non-linear function to generate mask.\n\n    References:\n        [1] : ""Conv-TasNet: Surpassing ideal time-frequency magnitude masking\n        for speech separation"" TASLP 2019 Yi Luo, Nima Mesgarani\n        https://arxiv.org/abs/1809.07454\n    """"""\n    def __init__(self, in_chan, n_src, out_chan=None, n_blocks=8, n_repeats=3,\n                 bn_chan=128, hid_chan=512, skip_chan=128, conv_kernel_size=3,\n                 norm_type=""gLN"", mask_act=\'relu\', kernel_size=None):\n        super(TDConvNet, self).__init__()\n        self.in_chan = in_chan\n        self.n_src = n_src\n        out_chan = out_chan if out_chan else in_chan\n        self.out_chan = out_chan\n        self.n_blocks = n_blocks\n        self.n_repeats = n_repeats\n        self.bn_chan = bn_chan\n        self.hid_chan = hid_chan\n        self.skip_chan = skip_chan\n        if kernel_size is not None:\n            # warning\n            warnings.warn(\'`kernel_size` argument is deprecated since v0.2.1 \'\n                          \'and will be remove in v0.3.0. Use argument \'\n                          \'`conv_kernel_size` instead\',\n                          VisibleDeprecationWarning)\n            conv_kernel_size = kernel_size\n        self.conv_kernel_size = conv_kernel_size\n        self.norm_type = norm_type\n        self.mask_act = mask_act\n\n        layer_norm = norms.get(norm_type)(in_chan)\n        bottleneck_conv = nn.Conv1d(in_chan, bn_chan, 1)\n        self.bottleneck = nn.Sequential(layer_norm, bottleneck_conv)\n        # Succession of Conv1DBlock with exponentially increasing dilation.\n        self.TCN = nn.ModuleList()\n        for r in range(n_repeats):\n            for x in range(n_blocks):\n                padding = (conv_kernel_size - 1) * 2**x // 2\n                self.TCN.append(Conv1DBlock(bn_chan, hid_chan, skip_chan,\n                                            conv_kernel_size, padding=padding,\n                                            dilation=2**x, norm_type=norm_type))\n        mask_conv_inp = skip_chan if skip_chan else bn_chan\n        mask_conv = nn.Conv1d(mask_conv_inp, n_src*out_chan, 1)\n        self.mask_net = nn.Sequential(nn.PReLU(), mask_conv)\n        # Get activation function.\n        mask_nl_class = activations.get(mask_act)\n        # For softmax, feed the source dimension.\n        if has_arg(mask_nl_class, \'dim\'):\n            self.output_act = mask_nl_class(dim=1)\n        else:\n            self.output_act = mask_nl_class()\n\n    def forward(self, mixture_w):\n        """"""\n\n        Args:\n            mixture_w (:class:`torch.Tensor`): Tensor of shape\n                [batch, n_filters, n_frames]\n\n        Returns:\n            :class:`torch.Tensor`:\n                estimated mask of shape [batch, n_src, n_filters, n_frames]\n        """"""\n        batch, n_filters, n_frames = mixture_w.size()\n        output = self.bottleneck(mixture_w)\n        skip_connection = 0.\n        for i in range(len(self.TCN)):\n            # Common to w. skip and w.o skip architectures\n            tcn_out = self.TCN[i](output)\n            if self.skip_chan:\n                residual, skip = tcn_out\n                skip_connection = skip_connection + skip\n            else:\n                residual = tcn_out\n            output = output + residual\n        # Use residual output when no skip connection\n        mask_inp = skip_connection if self.skip_chan else output\n        score = self.mask_net(mask_inp)\n        score = score.view(batch, self.n_src, self.out_chan, n_frames)\n        est_mask = self.output_act(score)\n        return est_mask\n\n    def get_config(self):\n        config = {\n            \'in_chan\': self.in_chan,\n            \'out_chan\': self.out_chan,\n            \'bn_chan\': self.bn_chan,\n            \'hid_chan\': self.hid_chan,\n            \'skip_chan\': self.skip_chan,\n            \'conv_kernel_size\': self.conv_kernel_size,\n            \'n_blocks\': self.n_blocks,\n            \'n_repeats\': self.n_repeats,\n            \'n_src\': self.n_src,\n            \'norm_type\': self.norm_type,\n            \'mask_act\': self.mask_act\n        }\n        return config\n'"
asteroid/masknn/norms.py,15,"b'import torch\nfrom torch import nn\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\nEPS = 1e-8\n\n\nclass _LayerNorm(nn.Module):\n    """"""Layer Normalization base class.""""""\n    def __init__(self, channel_size):\n        super(_LayerNorm, self).__init__()\n        self.channel_size = channel_size\n        self.gamma = nn.Parameter(torch.ones(channel_size),\n                                  requires_grad=True)\n        self.beta = nn.Parameter(torch.zeros(channel_size),\n                                 requires_grad=True)\n\n    def apply_gain_and_bias(self, normed_x):\n        """""" Assumes input of size `[batch, chanel, *]`. """"""\n        return (self.gamma * normed_x.transpose(1, -1) +\n                self.beta).transpose(1, -1)\n\n\nclass GlobLN(_LayerNorm):\n    """"""Global Layer Normalization (globLN).""""""\n    def forward(self, x):\n        """""" Applies forward pass.\n        \n        Works for any input size > 2D.\n\n        Args:\n            x (:class:`torch.Tensor`): Shape `[batch, chan, *]`\n\n        Returns:\n            :class:`torch.Tensor`: gLN_x `[batch, chan, *]`\n        """"""\n        dims = list(range(1, len(x.shape)))\n        mean = x.mean(dim=dims, keepdim=True)\n        var = torch.pow(x - mean, 2).mean(dim=dims, keepdim=True)\n        return self.apply_gain_and_bias((x - mean) / (var + EPS).sqrt())\n\n\nclass ChanLN(_LayerNorm):\n    """"""Channel-wise Layer Normalization (chanLN).""""""\n    def forward(self, x):\n        """""" Applies forward pass.\n        \n        Works for any input size > 2D.\n\n        Args:\n            x (:class:`torch.Tensor`): `[batch, chan, *]`\n\n        Returns:\n            :class:`torch.Tensor`: chanLN_x `[batch, chan, *]`\n        """"""\n        mean = torch.mean(x, dim=1, keepdim=True)\n        var = torch.var(x, dim=1, keepdim=True, unbiased=False)\n        return self.apply_gain_and_bias((x - mean) / (var + EPS).sqrt())\n\n\nclass CumLN(_LayerNorm):\n    """"""Cumulative Global layer normalization(cumLN).""""""\n    def forward(self, x):\n        """"""\n\n        Args:\n            x (:class:`torch.Tensor`): Shape `[batch, channels, length]`\n        Returns:\n             :class:`torch.Tensor`: cumLN_x `[batch, channels, length]`\n        """"""\n        batch, chan, spec_len = x.size()\n        cum_sum = torch.cumsum(x.sum(1, keepdim=True), dim=-1)\n        cum_pow_sum = torch.cumsum(x.pow(2).sum(1, keepdim=True), dim=-1)\n        cnt = torch.arange(start=chan, end=chan*(spec_len+1),\n                           step=chan, dtype=x.dtype).view(1, 1, -1)\n        cum_mean = cum_sum / cnt\n        cum_var = cum_pow_sum - cum_mean.pow(2)\n        return self.apply_gain_and_bias((x - cum_mean) / (cum_var + EPS).sqrt())\n\n\nclass BatchNorm(_BatchNorm):\n    """"""Wrapper class for pytorch BatchNorm1D and BatchNorm2D""""""\n    def _check_input_dim(self, input):\n        if input.dim() < 2 or input.dim() > 4:\n            raise ValueError(\'expected 4D or 3D input (got {}D input)\'\n                             .format(input.dim()))\n\n# Aliases.\ngLN = GlobLN\ncLN = ChanLN\ncgLN = CumLN\nbN = BatchNorm\n\n\ndef get(identifier):\n    """""" Returns a norm class from a string. Returns its input if it\n    is callable (already a :class:`._LayerNorm` for example).\n\n    Args:\n        identifier (str or Callable or None): the norm identifier.\n\n    Returns:\n        :class:`._LayerNorm` or None\n    """"""\n    if identifier is None:\n        return None\n    elif callable(identifier):\n        return identifier\n    elif isinstance(identifier, str):\n        cls = globals().get(identifier)\n        if cls is None:\n            raise ValueError(\'Could not interpret normalization identifier: \' +\n                             str(identifier))\n        return cls\n    else:\n        raise ValueError(\'Could not interpret normalization identifier: \' +\n                         str(identifier))\n'"
asteroid/masknn/recurrent.py,4,"b'import torch\nfrom torch import nn\nfrom torch.nn.functional import fold, unfold\n\nfrom . import norms, activations\nfrom ..utils import has_arg\n\n\nclass SingleRNN(nn.Module):\n    """""" Module for a RNN block.\n\n    Inspired from https://github.com/yluo42/TAC/blob/master/utility/models.py\n    Licensed under CC BY-NC-SA 3.0 US.\n\n    Args:\n        rnn_type (str): Select from ``\'RNN\'``, ``\'LSTM\'``, ``\'GRU\'``. Can\n            also be passed in lowercase letters.\n        input_size (int): Dimension of the input feature. The input should have\n            shape [batch, seq_len, input_size].\n        hidden_size (int): Dimension of the hidden state.\n        n_layers (int, optional): Number of layers used in RNN. Default is 1.\n        dropout (float, optional): Dropout ratio. Default is 0.\n        bidirectional (bool, optional): Whether the RNN layers are\n            bidirectional. Default is ``False``.\n    """"""\n\n    def __init__(self, rnn_type, input_size, hidden_size, n_layers=1,\n                 dropout=0, bidirectional=False):\n        super(SingleRNN, self).__init__()\n        assert rnn_type.upper() in [""RNN"", ""LSTM"", ""GRU""]\n        rnn_type = rnn_type.upper()\n        self.rnn_type = rnn_type\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.rnn = getattr(nn, rnn_type)(input_size, hidden_size,\n                                         num_layers=n_layers,\n                                         dropout=dropout,\n                                         batch_first=True,\n                                         bidirectional=bool(bidirectional))\n\n    def forward(self, inp):\n        """""" Input shape [batch, seq, feats] """"""\n        self.rnn.flatten_parameters()  # Enables faster multi-GPU training.\n        output = inp\n        rnn_output, _ = self.rnn(output)\n        return rnn_output\n\n\nclass StackedResidualRNN(nn.Module):\n    """""" Stacked RNN with builtin residual connection.\n    Only supports forward RNNs.\n    See StackedResidualBiRNN for bidirectional ones.\n\n    Args:\n        rnn_type (str): Select from ``\'RNN\'``, ``\'LSTM\'``, ``\'GRU\'``. Can\n            also be passed in lowercase letters.\n        n_units (int): Number of units in recurrent layers. This will also be\n            the expected input size.\n        n_layers (int): Number of recurrent layers.\n        dropout (float): Dropout value, between 0. and 1. (Default: 0.)\n        bidirectional (bool): If True, use bidirectional RNN, else\n            unidirectional. (Default: False)\n    """"""\n\n    def __init__(self, rnn_type, n_units, n_layers=4, dropout=0.,\n                 bidirectional=False):\n        super(StackedResidualRNN, self).__init__()\n        self.rnn_type = rnn_type\n        self.n_units = n_units\n        self.n_layers = n_layers\n        self.dropout = dropout\n        assert bidirectional is False, ""Bidirectional not supported yet""\n        self.bidirectional = bidirectional\n\n        self.layers = nn.ModuleList()\n        for _ in range(n_layers):\n            self.layers.append(SingleRNN(rnn_type, input_size=n_units,\n                                         hidden_size=n_units,\n                                         bidirectional=bidirectional))\n        self.dropout_layer = nn.Dropout(self.dropout)\n\n    def forward(self, x):\n        """""" Builtin residual connections + dropout applied before residual.\n            Input shape : [batch, time_axis, feat_axis]\n        """"""\n        for rnn in self.layers:\n            rnn_out = rnn(x)\n            dropped_out = self.dropout_layer(rnn_out)\n            x = x + dropped_out\n        return x\n\n\nclass StackedResidualBiRNN(nn.Module):\n    """""" Stacked Bidirectional RNN with builtin residual connection.\n    Residual connections are applied on both RNN directions.\n    Only supports bidiriectional RNNs.\n    See StackedResidualRNN for unidirectional ones.\n\n    Args:\n        rnn_type (str): Select from ``\'RNN\'``, ``\'LSTM\'``, ``\'GRU\'``. Can\n            also be passed in lowercase letters.\n        n_units (int): Number of units in recurrent layers. This will also be\n            the expected input size.\n        n_layers (int): Number of recurrent layers.\n        dropout (float): Dropout value, between 0. and 1. (Default: 0.)\n        bidirectional (bool): If True, use bidirectional RNN, else\n            unidirectional. (Default: False)\n    """"""\n\n    def __init__(self, rnn_type, n_units, n_layers=4, dropout=0.,\n                 bidirectional=True):\n        super().__init__()\n        self.rnn_type = rnn_type\n        self.n_units = n_units\n        self.n_layers = n_layers\n        self.dropout = dropout\n        assert bidirectional is True, ""Only bidirectional not supported yet""\n        self.bidirectional = bidirectional\n\n        # The first layer has as many units as input size\n        self.first_layer = SingleRNN(rnn_type, input_size=n_units,\n                                     hidden_size=n_units,\n                                     bidirectional=bidirectional)\n        # As the first layer outputs 2*n_units, the following layers need\n        # 2*n_units as input size\n        self.layers = nn.ModuleList()\n        for i in range(n_layers - 1):\n            input_size = 2 * n_units\n            self.layers.append(SingleRNN(rnn_type, input_size=input_size,\n                                         hidden_size=n_units,\n                                         bidirectional=bidirectional))\n        self.dropout_layer = nn.Dropout(self.dropout)\n\n    def forward(self, x):\n        """""" Builtin residual connections + dropout applied before residual.\n            Input shape : [batch, time_axis, feat_axis]\n        """"""\n        # First layer\n        rnn_out = self.first_layer(x)\n        dropped_out = self.dropout_layer(rnn_out)\n        x = torch.cat([x, x], dim=-1) + dropped_out\n        # Rest of the layers\n        for rnn in self.layers:\n            rnn_out = rnn(x)\n            dropped_out = self.dropout_layer(rnn_out)\n            x = x + dropped_out\n        return x\n\n\nclass DPRNNBlock(nn.Module):\n    """""" Dual-Path RNN Block as proposed in [1].\n\n    Args:\n        in_chan (int): Number of input channels.\n        hid_size (int): Number of hidden neurons in the RNNs.\n        norm_type (str, optional): Type of normalization to use. To choose from\n            - ``\'gLN\'``: global Layernorm\n            - ``\'cLN\'``: channelwise Layernorm\n        bidirectional (bool, optional): True for bidirectional Inter-Chunk RNN.\n        rnn_type (str, optional): Type of RNN used. Choose from ``\'RNN\'``,\n            ``\'LSTM\'`` and ``\'GRU\'``.\n        num_layers (int, optional): Number of layers used in each RNN.\n        dropout (float, optional): Dropout ratio. Must be in [0, 1].\n\n    References:\n        [1] ""Dual-path RNN: efficient long sequence modeling for\n        time-domain single-channel speech separation"", Yi Luo, Zhuo Chen\n        and Takuya Yoshioka. https://arxiv.org/abs/1910.06379\n    """"""\n    def __init__(self, in_chan, hid_size, norm_type=""gLN"", bidirectional=True,\n                 rnn_type=""LSTM"", num_layers=1, dropout=0):\n        super(DPRNNBlock, self).__init__()\n        # IntraRNN and linear projection layer (always bi-directional)\n        self.intra_RNN = SingleRNN(rnn_type, in_chan, hid_size, num_layers,\n                                   dropout=dropout, bidirectional=True)\n        self.intra_linear = nn.Linear(hid_size * 2, in_chan)\n        self.intra_norm = norms.get(norm_type)(in_chan)\n        # InterRNN block and linear projection layer (uni or bi-directional)\n        self.inter_RNN = SingleRNN(rnn_type, in_chan, hid_size, num_layers,\n                                   dropout=dropout, bidirectional=bidirectional)\n        num_direction = int(bidirectional) + 1\n        self.inter_linear = nn.Linear(hid_size * num_direction, in_chan)\n        self.inter_norm = norms.get(norm_type)(in_chan)\n\n    def forward(self, x):\n        """""" Input shape : [batch, feats, chunk_size, num_chunks] """"""\n        B, N, K, L = x.size()\n        output = x  # for skip connection\n        # Intra-chunk processing\n        x = x.transpose(1, -1).reshape(B * L, K, N)\n        x = self.intra_RNN(x)\n        x = self.intra_linear(x)\n        x = x.reshape(B, L, K, N).transpose(1, -1)\n        x = self.intra_norm(x)\n        output = output + x\n        # Inter-chunk processing\n        x = output.transpose(1, 2).transpose(2, -1).reshape(B * K, L, N)\n        x = self.inter_RNN(x)\n        x = self.inter_linear(x)\n        x = x.reshape(B, K, L, N).transpose(1, -1).transpose(2, -1)\n        x = self.inter_norm(x)\n        return output + x\n\n\nclass DPRNN(nn.Module):\n    """""" Dual-path RNN Network for Single-Channel Source Separation\n        introduced in [1].\n\n    Args:\n        in_chan (int): Number of input filters.\n        n_src (int): Number of masks to estimate.\n        out_chan  (int or None): Number of bins in the estimated masks.\n            Defaults to `in_chan`.\n        bn_chan (int): Number of channels after the bottleneck.\n            Defaults to 128.\n        hid_size (int): Number of neurons in the RNNs cell state.\n            Defaults to 128.\n        chunk_size (int): window size of overlap and add processing.\n            Defaults to 100.\n        hop_size (int or None): hop size (stride) of overlap and add processing.\n            Default to `chunk_size // 2` (50% overlap).\n        n_repeats (int): Number of repeats. Defaults to 6.\n        norm_type (str, optional): Type of normalization to use. To choose from\n\n            - ``\'gLN\'``: global Layernorm\n            - ``\'cLN\'``: channelwise Layernorm\n        mask_act (str, optional): Which non-linear function to generate mask.\n        bidirectional (bool, optional): True for bidirectional Inter-Chunk RNN\n            (Intra-Chunk is always bidirectional).\n        rnn_type (str, optional): Type of RNN used. Choose between ``\'RNN\'``,\n            ``\'LSTM\'`` and ``\'GRU\'``.\n        num_layers (int, optional): Number of layers in each RNN.\n        dropout (float, optional): Dropout ratio, must be in [0,1].\n\n    References:\n        [1] ""Dual-path RNN: efficient long sequence modeling for\n            time-domain single-channel speech separation"", Yi Luo, Zhuo Chen\n            and Takuya Yoshioka. https://arxiv.org/abs/1910.06379\n    """"""\n    def __init__(self, in_chan, n_src, out_chan=None, bn_chan=128, hid_size=128,\n                 chunk_size=100, hop_size=None, n_repeats=6, norm_type=""gLN"",\n                 mask_act=\'relu\', bidirectional=True, rnn_type=""LSTM"",\n                 num_layers=1, dropout=0):\n        super(DPRNN, self).__init__()\n        self.in_chan = in_chan\n        out_chan = out_chan if out_chan is not None else in_chan\n        self.out_chan = out_chan\n        self.bn_chan = bn_chan\n        self.hid_size = hid_size\n        self.chunk_size = chunk_size\n        hop_size = hop_size if hop_size is not None else chunk_size // 2\n        self.hop_size = hop_size\n        self.n_repeats = n_repeats\n        self.n_src = n_src\n        self.norm_type = norm_type\n        self.mask_act = mask_act\n        self.bidirectional = bidirectional\n        self.rnn_type = rnn_type\n        self.num_layers = num_layers\n        self.dropout = dropout\n\n        layer_norm = norms.get(norm_type)(in_chan)\n        bottleneck_conv = nn.Conv1d(in_chan, bn_chan, 1)\n        self.bottleneck = nn.Sequential(layer_norm, bottleneck_conv)\n\n        # Succession of DPRNNBlocks.\n        net = []\n        for x in range(self.n_repeats):\n            net += [DPRNNBlock(bn_chan, hid_size, norm_type=norm_type,\n                               bidirectional=bidirectional, rnn_type=rnn_type,\n                               num_layers=num_layers, dropout=dropout)]\n        self.net = nn.Sequential(*net)\n        # Masking in 3D space\n        net_out_conv = nn.Conv2d(bn_chan, n_src*bn_chan, 1)\n        self.first_out = nn.Sequential(nn.PReLU(), net_out_conv)\n        # Gating and masking in 2D space (after fold)\n        self.net_out = nn.Sequential(nn.Conv1d(bn_chan, bn_chan, 1), nn.Tanh())\n        self.net_gate = nn.Sequential(nn.Conv1d(bn_chan, bn_chan, 1),\n                                      nn.Sigmoid())\n        self.mask_net = nn.Conv1d(bn_chan, out_chan, 1, bias=False)\n\n        # Get activation function.\n        mask_nl_class = activations.get(mask_act)\n        # For softmax, feed the source dimension.\n        if has_arg(mask_nl_class, \'dim\'):\n            self.output_act = mask_nl_class(dim=1)\n        else:\n            self.output_act = mask_nl_class()\n\n    def forward(self, mixture_w):\n        """"""\n        Args:\n            mixture_w (:class:`torch.Tensor`): Tensor of shape\n                [batch, n_filters, n_frames]\n        Returns:\n            :class:`torch.Tensor`\n                estimated mask of shape [batch, n_src, n_filters, n_frames]\n        """"""\n        batch, n_filters, n_frames = mixture_w.size()\n        output = self.bottleneck(mixture_w)  # [batch, bn_chan, n_frames]\n        output = unfold(output.unsqueeze(-1), kernel_size=(self.chunk_size, 1),\n                        padding=(self.chunk_size, 0), stride=(self.hop_size, 1))\n        n_chunks = output.size(-1)\n        output = output.reshape(batch, self.bn_chan, self.chunk_size, n_chunks)\n        # Apply stacked DPRNN Blocks sequentially\n        output = self.net(output)\n        # Map to sources with kind of 2D masks\n        output = self.first_out(output)\n        output = output.reshape(batch * self.n_src, self.bn_chan,\n                                self.chunk_size, n_chunks)\n        # Overlap and add:\n        # [batch, out_chan, chunk_size, n_chunks] -> [batch, out_chan, n_frames]\n        to_unfold = self.bn_chan * self.chunk_size\n        output = fold(output.reshape(batch * self.n_src, to_unfold, n_chunks),\n                      (n_frames, 1), kernel_size=(self.chunk_size, 1),\n                      padding=(self.chunk_size, 0),\n                      stride=(self.hop_size, 1))\n        # Apply gating\n        output = output.reshape(batch * self.n_src, self.bn_chan, -1)\n        output = self.net_out(output) * self.net_gate(output)\n        # Compute mask\n        score = self.mask_net(output)\n        est_mask = self.output_act(score)\n        est_mask = est_mask.view(batch, self.n_src, self.out_chan, n_frames)\n        return est_mask\n\n    def get_config(self):\n        config = {\n            \'in_chan\': self.in_chan,\n            \'out_chan\': self.out_chan,\n            \'bn_chan\': self.bn_chan,\n            \'hid_size\': self.hid_size,\n            \'chunk_size\': self.chunk_size,\n            \'hop_size\': self.hop_size,\n            \'n_repeats\': self.n_repeats,\n            \'n_src\': self.n_src,\n            \'norm_type\': self.norm_type,\n            \'mask_act\': self.mask_act,\n            \'bidirectional\': self.bidirectional,\n            \'rnn_type\': self.rnn_type,\n            \'num_layers\': self.num_layers,\n            \'dropout\': self.dropout\n        }\n        return config\n'"
asteroid/models/__init__.py,0,"b'# Models\nfrom .conv_tasnet import ConvTasNet\nfrom .dprnn_tasnet import DPRNNTasNet\n\n# Sharing-related\nfrom .publisher import save_publishable, upload_publishable\n'"
asteroid/models/base_models.py,10,"b'import torch\nfrom torch import nn\nimport numpy as np\n\nfrom .. import torch_utils\nfrom ..utils.hub_utils import cached_download\n\n\nclass BaseTasNet(nn.Module):\n    """""" Base class for encoder-masker-decoder separation models.\n\n    Args:\n        encoder (Encoder): Encoder instance.\n        masker (nn.Module): masker network.\n        decoder (Decoder): Decoder instance.\n    """"""\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n\n    def forward(self, wav):\n        """""" Enc/Mask/Dec model forward\n\n        Args:\n            wav (torch.Tensor): waveform tensor. 1D, 2D or 3D tensor, time last.\n\n        Returns:\n            torch.Tensor, of shape (batch, n_src, time) or (n_src, time).\n        """"""\n        # Handle 1D, 2D or n-D inputs\n        was_one_d = False\n        if wav.ndim == 1:\n            was_one_d = True\n            wav = wav.unsqueeze(0).unsqueeze(1)\n        if wav.ndim == 2:\n            wav = wav.unsqueeze(1)\n        # Real forward\n        tf_rep = self.encoder(wav)\n        est_masks = self.masker(tf_rep)\n        masked_tf_rep = est_masks * tf_rep.unsqueeze(1)\n        out_wavs = torch_utils.pad_x_to_y(self.decoder(masked_tf_rep), wav)\n        if was_one_d:\n            return out_wavs.squeeze(0)\n        return out_wavs\n\n    def separate(self, wav):\n        """""" Infer separated sources from input waveforms.\n\n        Args:\n            wav (Union[torch.Tensor, numpy.ndarray]): waveform array/tensor.\n                Shape: 1D, 2D or 3D tensor, time last.\n\n        Returns:\n            Union[torch.Tensor, numpy.ndarray], the estimated sources.\n                (batch, n_src, time) or (n_src, time) w/o batch dim.\n        """"""\n        return self._separate(wav)\n\n    def _separate(self, wav):\n        """""" Hidden separation method\n\n        Args:\n            wav (Union[torch.Tensor, numpy.ndarray]): waveform array/tensor.\n                Shape: 1D, 2D or 3D tensor, time last.\n\n        Returns:\n            Union[torch.Tensor, numpy.ndarray], the estimated sources.\n                (batch, n_src, time) or (n_src, time) w/o batch dim.\n        """"""\n        # Handle numpy inputs\n        was_numpy = False\n        if isinstance(wav, np.ndarray):\n            was_numpy = True\n            wav = torch.from_numpy(wav)\n        # Handle device placement\n        input_device = wav.device\n        model_device = next(self.parameters()).device\n        wav = wav.to(model_device)\n        # Forward\n        out_wavs = self.forward(wav)\n        # Back to input device (and numpy if necessary)\n        out_wavs = out_wavs.to(input_device)\n        if was_numpy:\n            return out_wavs.cpu().data.numpy()\n        return out_wavs\n\n    @classmethod\n    def from_pretrained(cls, pretrained_model_conf_or_path, *args, **kwargs):\n        """""" Instantiate separation model from a model config (file or dict).\n\n        Args:\n            pretrained_model_conf_or_path (Union[dict, str]): model conf as\n                returned by `serialize`, or path to it. Need to contain\n                `model_args` and `state_dict` keys.\n\n        Returns:\n            Instance of BaseTasNet\n\n        Raises:\n            ValueError if the input config file doesn\'t contain the keys\n                `model_args` and `state_dict`.\n        """"""\n        if isinstance(pretrained_model_conf_or_path, str):\n            cached_model = cached_download(pretrained_model_conf_or_path)\n            conf = torch.load(cached_model, map_location=\'cpu\')\n        else:\n            conf = pretrained_model_conf_or_path\n        if \'model_args\' not in conf.keys():\n            raise ValueError(\'Expected config dictionary to have field \'\n                             \'model_args`. Found only: {}\'.format(conf.keys()))\n        if \'state_dict\' not in conf.keys():\n            raise ValueError(\'Expected config dictionary to have field \'\n                             \'state_dict`. Found only: {}\'.format(conf.keys()))\n        model = cls(*args, **conf[\'model_args\'], **kwargs)\n        model.load_state_dict(conf[\'state_dict\'])\n        return model\n\n    def serialize(self):\n        """""" Serialize model and output dictionary.\n\n        Returns:\n            dict, serialized model with keys `model_args` and `state_dict`.\n        """"""\n        from .. import __version__ as asteroid_version  # Avoid circular imports\n        import pytorch_lightning as pl  # Not used in torch.hub\n        model_conf = dict()\n        fb_config = self.encoder.filterbank.get_config()\n        masknet_config = self.masker.get_config()\n        # Assert both dict are disjoint\n        if not all(k not in fb_config for k in masknet_config):\n            raise AssertionError(""Filterbank and Mask network config share""\n                                 ""common keys. Merging them is not safe."")\n        # Merge all args under model_args.\n        model_conf[\'model_name\'] = self.__class__.__name__\n        model_conf[\'model_args\'] = {**fb_config, **masknet_config}\n        model_conf[\'state_dict\'] = self.state_dict()\n        # Additional infos\n        infos = dict()\n        infos[\'software_versions\'] = dict(\n            torch_version=torch.__version__,\n            pytorch_lightning_version=pl.__version__,\n            asteroid_version=asteroid_version,\n        )\n        model_conf[\'infos\'] = infos\n        return model_conf\n'"
asteroid/models/conv_tasnet.py,0,"b'from ..filterbanks import make_enc_dec\nfrom ..masknn import TDConvNet\nfrom .base_models import BaseTasNet\n\n\nclass ConvTasNet(BaseTasNet):\n    """""" ConvTasNet separation model, as described in [1].\n\n    Args:\n        n_src (int): Number of sources in the input mixtures.\n        out_chan (int, optional): Number of bins in the estimated masks.\n            If ``None``, `out_chan = in_chan`.\n        n_blocks (int, optional): Number of convolutional blocks in each\n            repeat. Defaults to 8.\n        n_repeats (int, optional): Number of repeats. Defaults to 3.\n        bn_chan (int, optional): Number of channels after the bottleneck.\n        hid_chan (int, optional): Number of channels in the convolutional\n            blocks.\n        skip_chan (int, optional): Number of channels in the skip connections.\n            If 0 or None, TDConvNet won\'t have any skip connections and the\n            masks will be computed from the residual output.\n            Corresponds to the ConvTasnet architecture in v1 or the paper.\n        conv_kernel_size (int, optional): Kernel size in convolutional blocks.\n        norm_type (str, optional): To choose from ``\'BN\'``, ``\'gLN\'``,\n            ``\'cLN\'``.\n        mask_act (str, optional): Which non-linear function to generate mask.\n        in_chan (int, optional): Number of input channels, should be equal to\n            n_filters.\n        fb_name (str, className): Filterbank family from which to make encoder\n            and decoder. To choose among [``\'free\'``, ``\'analytic_free\'``,\n            ``\'param_sinc\'``, ``\'stft\'``].\n        n_filters (int): Number of filters / Input dimension of the masker net.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution.\n            If None (default), set to ``kernel_size // 2``.\n        **fb_kwargs (dict): Additional kwards to pass to the filterbank\n            creation.\n\n    References:\n        [1] : ""Conv-TasNet: Surpassing ideal time-frequency magnitude masking\n        for speech separation"" TASLP 2019 Yi Luo, Nima Mesgarani\n        https://arxiv.org/abs/1809.07454\n    """"""\n    def __init__(self, n_src, out_chan=None, n_blocks=8, n_repeats=3,\n                 bn_chan=128, hid_chan=512, skip_chan=128, conv_kernel_size=3,\n                 norm_type=""gLN"", mask_act=\'relu\', in_chan=None, fb_name=\'free\',\n                 kernel_size=16, n_filters=512, stride=8, **fb_kwargs):\n        encoder, decoder = make_enc_dec(\n            fb_name, kernel_size=kernel_size, n_filters=n_filters,\n            stride=stride, **fb_kwargs\n        )\n        n_feats = encoder.n_feats_out\n        if in_chan is not None:\n            assert in_chan == n_feats, (\'Number of filterbank output channels\'\n                                        \' and number of input channels should \'\n                                        \'be the same. Received \'\n                                        f\'{n_feats} and {in_chan}\')\n        # Update in_chan\n        masker = TDConvNet(\n            n_feats, n_src, out_chan=out_chan, n_blocks=n_blocks,\n            n_repeats=n_repeats, bn_chan=bn_chan, hid_chan=hid_chan,\n            skip_chan=skip_chan, conv_kernel_size=conv_kernel_size,\n            norm_type=norm_type, mask_act=mask_act\n        )\n        super().__init__(encoder, masker, decoder)\n'"
asteroid/models/dprnn_tasnet.py,0,"b'from ..filterbanks import make_enc_dec\nfrom ..masknn import DPRNN\nfrom .base_models import BaseTasNet\n\n\nclass DPRNNTasNet(BaseTasNet):\n    """""" DPRNN separation model, as described in [1].\n\n    Args:\n        n_src (int): Number of masks to estimate.\n        out_chan  (int or None): Number of bins in the estimated masks.\n            Defaults to `in_chan`.\n        bn_chan (int): Number of channels after the bottleneck.\n            Defaults to 128.\n        hid_size (int): Number of neurons in the RNNs cell state.\n            Defaults to 128.\n        chunk_size (int): window size of overlap and add processing.\n            Defaults to 100.\n        hop_size (int or None): hop size (stride) of overlap and add processing.\n            Default to `chunk_size // 2` (50% overlap).\n        n_repeats (int): Number of repeats. Defaults to 6.\n        norm_type (str, optional): Type of normalization to use. To choose from\n\n            - ``\'gLN\'``: global Layernorm\n            - ``\'cLN\'``: channelwise Layernorm\n        mask_act (str, optional): Which non-linear function to generate mask.\n        bidirectional (bool, optional): True for bidirectional Inter-Chunk RNN\n            (Intra-Chunk is always bidirectional).\n        rnn_type (str, optional): Type of RNN used. Choose between ``\'RNN\'``,\n            ``\'LSTM\'`` and ``\'GRU\'``.\n        num_layers (int, optional): Number of layers in each RNN.\n        dropout (float, optional): Dropout ratio, must be in [0,1].\n        in_chan (int, optional): Number of input channels, should be equal to\n            n_filters.\n        fb_name (str, className): Filterbank family from which to make encoder\n            and decoder. To choose among [``\'free\'``, ``\'analytic_free\'``,\n            ``\'param_sinc\'``, ``\'stft\'``].\n        n_filters (int): Number of filters / Input dimension of the masker net.\n        kernel_size (int): Length of the filters.\n        stride (int, optional): Stride of the convolution.\n            If None (default), set to ``kernel_size // 2``.\n        **fb_kwargs (dict): Additional kwards to pass to the filterbank\n            creation.\n\n    References:\n        [1] ""Dual-path RNN: efficient long sequence modeling for\n            time-domain single-channel speech separation"", Yi Luo, Zhuo Chen\n            and Takuya Yoshioka. https://arxiv.org/abs/1910.06379\n    """"""\n    def __init__(self, n_src, out_chan=None, bn_chan=128, hid_size=128,\n                 chunk_size=100, hop_size=None, n_repeats=6, norm_type=""gLN"",\n                 mask_act=\'relu\', bidirectional=True, rnn_type=""LSTM"",\n                 num_layers=1, dropout=0, in_chan=None, fb_name=\'free\',\n                 kernel_size=16, n_filters=512, stride=8, **fb_kwargs):\n        encoder, decoder = make_enc_dec(\n            fb_name, kernel_size=kernel_size, n_filters=n_filters,\n            stride=stride, **fb_kwargs\n        )\n        n_feats = encoder.n_feats_out\n        if in_chan is not None:\n            assert in_chan == n_feats, (\'Number of filterbank output channels\'\n                                        \' and number of input channels should \'\n                                        \'be the same. Received \'\n                                        f\'{n_feats} and {in_chan}\')\n        # Update in_chan\n        masker = DPRNN(\n            n_feats, n_src, out_chan=out_chan, bn_chan=bn_chan,\n            hid_size=hid_size, chunk_size=chunk_size, hop_size=hop_size,\n            n_repeats=n_repeats, norm_type=norm_type, mask_act=mask_act,\n            bidirectional=bidirectional, rnn_type=rnn_type,\n            num_layers=num_layers, dropout=dropout\n        )\n        super().__init__(encoder, masker, decoder)\n\n'"
asteroid/models/publisher.py,4,"b'import os\nimport torch\nimport subprocess\nfrom pprint import pprint\n\nfrom .zenodo import Zenodo\n\nPLEASE_PUBLISH = (\n    ""\\nDon\'t forget to share your pretrained models at ""\n    ""https://zenodo.org/communities/asteroid-models/ ! =)\\n""\n    ""You can directly use our CLI for that, run this: \\n""\n    ""`asteroid-upload {} --uploader \\""Your name here\\""`\\n""\n)\n\nHREF = \'<a href=""{}"">{}</a>\'\nCC_SA = \'Attribution-ShareAlike 3.0 Unported\'\nCC_SA_LINK = \'https://creativecommons.org/licenses/by-sa/3.0/\'\nASTEROID_REF = HREF.format(\'https://github.com/mpariente/asteroid\', \'Asteroid\')\n\n\ndef save_publishable(publish_dir, model_dict, metrics=None, train_conf=None):\n    """""" Save models to prepare for publication / model sharing.\n\n    Args:\n        publish_dir (str): Path to the publishing directory.\n            Usually under exp/exp_name/publish_dir\n        model_dict (dict): dict at least with keys `model_args`,\n            `state_dict`,`dataset` or `licenses`\n        metrics (dict): dict with evaluation metrics.\n        train_conf (dict): Training configuration dict (from conf.yml).\n\n    Returns:\n        dict, same as `model_dict` with added fields.\n\n    Raises:\n        AssertionError when either `model_args`, `state_dict`,`dataset` or\n            `licenses` are not present is `model_dict.keys()`\n    """"""\n    assert \'model_args\' in model_dict.keys(), ""`model_args` not found in model dict.""\n    assert \'state_dict\' in model_dict.keys(), ""`state_dict` not found in model dict.""\n    assert \'dataset\' in model_dict.keys(), ""`dataset` not found in model dict.""\n    assert \'licenses\' in model_dict.keys(), ""`licenses` not found in model dict.""\n    assert isinstance(metrics, dict), ""Cannot upload a model without metrics.""\n    # Additional infos.\n    if os.path.exists(os.path.join(publish_dir, \'recipe_name.txt\')):\n        recipe_name = next(open(os.path.join(publish_dir, \'recipe_name.txt\')))\n        recipe_name.replace(\'\\n\', \'\')  # remove next line\n    else:\n        recipe_name = \'Unknown\'\n    model_dict[\'infos\'][\'recipe_name\'] = recipe_name\n    model_dict[\'infos\'][\'training_config\'] = train_conf\n    model_dict[\'infos\'][\'final_metrics\'] = metrics\n    torch.save(model_dict, os.path.join(publish_dir, \'model.pth\'))\n    print(PLEASE_PUBLISH.format(publish_dir))\n    return model_dict\n\n\ndef upload_publishable(publish_dir, uploader=None, affiliation=None,\n                       git_username=None, token=None, force_publish=False,\n                       use_sandbox=False, unit_test=False):\n    """""" Entry point to upload publishable model.\n\n    Args:\n        publish_dir (str): Path to the publishing directory.\n            Usually under exp/exp_name/publish_dir\n        uploader (str): Full name of the uploader (Ex: Manuel Pariente)\n        affiliation (str, optional): Affiliation (no accent).\n        git_username (str, optional): GitHub username.\n        token (str): Access token generated to upload depositions.\n        force_publish (bool): Whether to directly publish without\n            asking confirmation before. Defaults to False.\n        use_sandbox (bool): Whether to use Zenodo\'s sandbox instead of\n            the official Zenodo.\n        unit_test (bool): If True, we do not ask user input and do not publish.\n\n    """"""\n    def get_answer():\n        out = input(\'\\n\\nDo you want to publish it now (irreversible)? y/n\'\n                    \'(Recommended: n).\\n\')\n        if out not in [\'y\', \'n\']:\n            print(f\'\\nExpected one of [`y`, `n`], received {out}, please retry.\')\n            return get_answer()\n        return out\n\n    if uploader is None:\n        raise ValueError(\'Need uploader name\')\n\n    # Make publishable model and save it\n    model_path = os.path.join(publish_dir, \'model.pth\')\n    publish_model_path = os.path.join(publish_dir, \'published_model.pth\')\n    model = torch.load(model_path)\n    model = _populate_publishable(\n        model,\n        uploader=uploader,\n        affiliation=affiliation,\n        git_username=git_username,\n    )\n    torch.save(model, publish_model_path)\n\n    # Get Zenodo access token\n    if token is None:\n        token = os.getenv(\'ACCESS_TOKEN\')\n        if token is None:\n            raise ValueError(\n                \'Need an access token to Zenodo to upload the model. Either \'\n                \'set ACCESS_TOKEN environment variable or pass it directly \'\n                \'(`asteroid-upload --token ...`).\'\n                \'If you do not have a access token, first create a Zenodo \'\n                \'account (https://zenodo.org/signup/), create a token \'\n                \'https://zenodo.org/account/settings/applications/tokens/new/\'\n                \'and you are all set to help us ! =)\'\n            )\n\n    # Do the actual upload\n    zen, dep_id = zenodo_upload(model, token, model_path=publish_model_path,\n                                use_sandbox=use_sandbox)\n    address = os.path.join(zen.zenodo_address, \'deposit\', str(dep_id))\n    if force_publish:\n        r_publish = zen.publish_deposition(dep_id)\n        pprint(r_publish.json())\n        print(""You can also visit it at {}"".format(address))\n        return r_publish\n    # Give choice\n    current = zen.get_deposition(dep_id)\n    print(f""\\n\\n This is the current state of the deposition ""\n          f""(see here {address}): "")\n    pprint(current.json())\n    # Patch to run unit test\n    if unit_test:\n        return zen, current\n    else:\n        inp = get_answer()\n    # Get user input\n    if inp == \'y\':\n        _ = zen.publish_deposition(dep_id)\n        print(""Visit it at {}"".format(address))\n    else:\n        print(f\'Did not finalize the upload, please visit {address} to finalize \'\n              f\'it.\')\n\n\ndef _populate_publishable(model, uploader=None, affiliation=None,\n                          git_username=None):\n    """""" Populate infos in publishable model.\n\n    Args:\n        model (dict): Model to publish, with `infos` key, at least.\n        uploader (str): Full name of the uploader (Ex: Manuel Pariente)\n        affiliation (str, optional): Affiliation (no accent).\n        git_username (str, optional): GitHub username.\n\n    Returns:\n        dict (model), same as input `model`\n\n    Notes:\n        If a `git_username` is not specified, we look for it somehow, or take\n        the laptop username.\n    """"""\n    # Get username somehow\n    if git_username is None:\n        git_username = get_username()\n\n    # Example: mpariente/ConvTasNet_WHAM_sepclean\n    model_name = \'_\'.join([model[\'model_name\'], model[\'dataset\'],\n                           model[\'task\'].replace(\'_\', \'\')])\n    upload_name = git_username + \'/\' + model_name\n    # Write License Notice\n    license_note = make_license_notice(model_name, model[\'licenses\'],\n                                       uploader=uploader)\n    # Add infos\n    model[\'infos\'][\'uploader\'] = uploader\n    model[\'infos\'][\'git_username\'] = git_username\n    model[\'infos\'][\'affiliation\'] = affiliation if affiliation else ""Unknown""\n    model[\'infos\'][\'upload_name\'] = upload_name\n    model[\'infos\'][\'license_note\'] = license_note\n    return model\n\n\ndef get_username():\n    """""" Get git of FS username for upload. """"""\n    username = subprocess.check_output([""git"", ""config"", ""user.name""])\n    username = username.decode(\'utf-8\')[:-1]\n    if not username:  # Empty string\n        import getpass\n        username = getpass.getuser()\n    return username\n\n\ndef make_license_notice(model_name, licenses, uploader=None):\n    """""" Make license notice based on license dicts.\n\n    Args:\n        model_name (str): Name of the model.\n        licenses (List[dict]): List of dict with\n            keys (`title`, `title_link`, `author`, `author_link`,\n                  `licence`, `licence_link`).\n        uploader (str): Name of the uploader such as ""Manuel Pariente"".\n\n    Returns:\n        str, the license note describing the model, it\'s attribution,\n            the original licenses, what we license it under and the licensor.\n    """"""\n    if uploader is None:\n        raise ValueError(""Cannot share model without uploader."")\n    note = ""This work \\""{}\\"" is a derivative "".format(model_name)\n    for l_dict in licenses:\n        # Clickable links in HTML.\n        title = HREF.format(l_dict[\'title_link\'], l_dict[\'title\'])\n        author = HREF.format(l_dict[\'author_link\'], l_dict[\'author\'])\n        license_h = HREF.format(l_dict[\'license_link\'], l_dict[\'license\'])\n        comm = "" (Research only)"" if l_dict[\'non_commercial\'] else """"\n        note += f""of {title} by {author}, used under {license_h}{comm}""\n        note += ""; ""\n    note = note[:-2] + \'. \'  # Remove the last ;\n    cc_sa = HREF.format(CC_SA_LINK, CC_SA)\n    note += f""\\""{model_name}\\"" is licensed under {cc_sa} by {uploader}.""\n    return note\n\n\ndef zenodo_upload(model, token, model_path=None, use_sandbox=False):\n    """""" Create deposit and upload metadata + model\n\n    Args:\n        model (dict):\n        token (str): Access token.\n        model_path (str): Saved model path.\n        use_sandbox (bool): Whether to use Zenodo\'s sandbox instead of\n            the official Zenodo.\n\n    Returns:\n        Zenodo (Zenodo instance with access token)\n        int (deposit ID)\n\n    Notes:\n        If `model_path` is not specified, save the model in tmp.pth and\n        remove it after upload.\n    """"""\n    model_path_was_none = False\n    if model_path is None:\n        model_path_was_none = True\n        model_path = \'tmp.pth\'\n        torch.save(model, model_path)\n        # raise ValueError(""Need path"")\n\n    zen = Zenodo(token, use_sandbox=use_sandbox)\n    metadata = make_metadata_from_model(model)\n    r = zen.create_new_deposition(metadata=metadata)\n    if r.status_code != 200:\n        print(r.json())\n        raise RuntimeError(\'Could not create the deposition, check the \'\n                           \'provided token.\')\n    dep_id = r.json()[""id""]\n    _ = zen.upload_new_file_to_deposition(dep_id, model_path, name=\'model.pth\')\n    if model_path_was_none:\n        os.remove(model_path)\n    return zen, dep_id\n\n\ndef make_metadata_from_model(model):\n    """""" Create Zenodo deposit metadata for a given publishable model.\n    Args:\n        model (dict): Dictionary with all infos needed to publish.\n            More info to come.\n\n    Returns:\n        dict, the metadata to create the Zenodo deposit with.\n\n    Notes:\n        We remove the PESQ from the final results as a license is needed to\n        use it.\n    """"""\n    infos = model[\'infos\']\n    # Description section\n    description = \'<p><strong>Description: </strong></p>\'\n    tmp = \'This model was trained by {} using the {} recipe in {}. \'\n    description += tmp.format(infos[\'uploader\'], infos[\'recipe_name\'],\n                              ASTEROID_REF)\n    tmp = \'</a>It was trained on the <code>{}</code> task of the {} dataset.</p>\'\n    description += tmp.format(model[\'task\'], model[\'dataset\'])\n\n    # Training config section\n    description += \'<p>&nbsp;</p>\'\n    description += \'<p><strong>Training config:</strong></p>\'\n    description += two_level_dict_html(infos[\'training_config\'])\n\n    # Results section\n    description += \'<p>&nbsp;</p>\'\n    description += \'<p><strong>Results:</strong></p>\'\n    display_result = {\n        k: v for k, v in infos[\'final_metrics\'].items()\n        if \'pesq\' not in k.lower()\n    }\n    description += display_one_level_dict(display_result)\n\n    # License section\n    description += \'<p>&nbsp;</p>\'\n    description += \'<p><strong>License notice:</strong></p>\'\n    description += infos[\'license_note\']\n\n    # Putting it together.\n    metadata = {\n        \'title\': infos[\'upload_name\'],\n        \'upload_type\': \'software\',\n        \'description\': description,\n        \'creators\': [{\'name\': infos[\'uploader\'],\n                      \'affiliation\': infos[\'affiliation\']}],\n        \'communities\': [{\'identifier\': \'zenodo\'},\n                        {\'identifier\': \'asteroid-models\'}],\n        \'keywords\': [\'Asteroid\', \'audio source separation\', model[\'dataset\'],\n                     model[\'task\'], model[\'model_name\'], \'pretrained model\'],\n        \'license\': \'CC-BY-SA-3.0\'\n    }\n    return metadata\n\n\ndef two_level_dict_html(dic):\n    """""" Two-level dict to HTML.\n    Args:\n        dic (dict): two-level dict\n\n    Returns:\n        str for HTML-encoded two level dic\n    """"""\n    html = ""<ul>""\n    for k in dic.keys():\n        # Open field\n        html += f\'<li>{k}: <ul>\'\n        for k2 in dic[k].keys():\n            val = str(dic[k][k2])\n            html += f\'<li>{k2}: {val}</li>\'\n        # Close field\n        html += \'</il></ul>\'\n    html += \'</ul>\'\n    return html\n\n\ndef display_one_level_dict(dic):\n    """""" Single level dict to HTML\n    Args:\n        dic (dict):\n\n    Returns:\n        str for HTML-encoded single level dic\n    """"""\n    html = ""<ul>""\n    for k in dic.keys():\n        # Open field\n        val = str(dic[k])\n        html += f\'<li>{k}: {val} </li>\'\n    html += \'</ul>\'\n    return html\n'"
asteroid/models/zenodo.py,1,"b'import os\nimport json\nimport requests\nfrom io import BufferedReader, BytesIO\nimport torch\n\n\nclass Zenodo(object):\n    """""" Faciliate Zenodo\'s REST API.\n\n    Args:\n        api_key (str): Access token generated to upload depositions.\n        use_sandbox (bool): Whether to use the sandbox (default: True)\n            Note that `api_key` are different in sandbox.\n\n    Methods (all methods return the requests response):\n        create_new_deposition\n        change_metadata_in_deposition,\n        upload_new_file_to_deposition\n        publish_deposition\n        get_deposition\n        remove_deposition\n        remove_all_depositions\n\n    Notes:\n        A Zenodo record is something that is public and cannot be deleted.\n        A Zenodo deposit has not yet been published, is private and can be\n        deleted.\n    """"""\n    def __init__(self, api_key=None, use_sandbox=True):\n        if api_key is None:\n            api_key = os.getenv(\'ACCESS_TOKEN\', None)\n        if api_key is None:\n            raise ValueError(\n                \'Need to set `api_key` somehow. Either through the functions\'\n                \'arguments or by setting ACCESS_TOKEN env variable in bash.\'\n            )\n        self.use_sandbox = use_sandbox\n        if use_sandbox is True:\n            self.zenodo_address = \'https://sandbox.zenodo.org\'\n        else:\n            self.zenodo_address = \'https://zenodo.org\'\n\n        self.api_key = api_key\n        self.auth_header = {\'Authorization\': f""Bearer {self.api_key}""}\n        self.headers = {""Content-Type"": ""application/json"",\n                        \'Authorization\': f""Bearer {self.api_key}""}\n\n    def create_new_deposition(self, metadata=None):\n        """""" Creates a new deposition.\n\n        Args:\n            metadata (dict, optional): Metadata dict to upload on the new\n                deposition.\n        """"""\n        r = requests.post(\n            f\'{self.zenodo_address}/api/deposit/depositions\',\n            json={}, headers=self.headers\n        )\n\n        if r.status_code != 201:\n            print(""Creation failed (status code: {})"".format(r.status_code))\n            return r\n\n        if metadata is not None and isinstance(metadata, dict):\n            return self.change_metadata_in_deposition(r.json()[""id""], metadata)\n        else:\n            print(f""Could not interpret metadata type ({type(metadata)}), ""\n                  ""expected dict"")\n        return r\n\n    def change_metadata_in_deposition(self, dep_id, metadata):\n        """""" Set or replace metadata in given deposition\n\n        Args:\n            dep_id (int): deposition id. You cna get it with\n                `r = create_new_deposition(); dep_id = r.json()[\'id\']`\n            metadata (dict): Metadata dict.\n\n        Examples:\n            metadata = {\n                \'title\': \'My first upload\',\n                \'upload_type\': \'poster\',\n                \'description\': \'This is my first upload\',\n                \'creators\': [{\'name\': \'Doe, John\',\n                              \'affiliation\': \'Zenodo\'}]\n            }\n        """"""\n        data = {""metadata"": metadata}\n        r = requests.put(\n            f\'{self.zenodo_address}/api/deposit/depositions/{dep_id}\',\n            data=json.dumps(data), headers=self.headers\n        )\n        return r\n\n    def upload_new_file_to_deposition(self, dep_id, file, name=None):\n        """""" Upload one file to existing deposition.\n        Args:\n            dep_id (int): deposition id. You cna get it with\n                `r = create_new_deposition(); dep_id = r.json()[\'id\']`\n            file (str or io.BufferedReader): path to a file, or already opened\n                file (path prefered).\n            name (str, optional): name given to the uploaded file.\n                Defaults to the path.\n\n        (More: https://developers.zenodo.org/#deposition-files)\n        """"""\n        if isinstance(file, BufferedReader):\n            files = {\'file\': file}\n            filename = name if name else ""Unknown""\n        elif isinstance(file, str):\n            if os.path.isfile(file):\n                # This is a file, read it\n                files = {\'file\': open(os.path.expanduser(file), \'rb\')}\n                filename = name if name else os.path.basename(file)\n            else:\n                # This is a string, convert to BytesIO\n                files = {\'file\': BytesIO(bytes(file, \'utf-8\'))}\n                filename = name if name else ""Unknown""\n        else:\n            raise ValueError(\'Unknown file format , expected str or Bytes \')\n        data = {""name"": filename}\n        print(""Submitting Data: {} and Files: {}"".format(data, files))\n\n        r = requests.post(\n            f\'{self.zenodo_address}/api/deposit/depositions/{dep_id}/files\',\n            headers=self.auth_header, data=data,\n            files=files\n        )\n        print(""Zenodo received : {}"".format(r.content))\n        return r\n\n    def publish_deposition(self, dep_id):  # pragma: no cover (Cannot publish)\n        """""" Publish given deposition (Cannot be deleted) !\n\n        Args:\n            dep_id (int): deposition id. You cna get it with\n                `r = create_new_deposition(); dep_id = r.json()[\'id\']`\n        """"""\n        r = requests.post(\n            f\'{self.zenodo_address}/api/deposit/depositions/{dep_id}/actions/publish\',\n            headers=self.headers\n        )\n        return r\n\n    def get_deposition(self, dep_id=-1):\n        """""" Get deposition by deposition id. Get all dep_id is -1 (default).""""""\n        if dep_id > -1:\n            print(f""Get deposition {dep_id} from Zenodo"")\n            r = requests.get(\n                f""{self.zenodo_address}/api/deposit/depositions/{dep_id}"",\n                headers=self.headers\n            )\n        else:\n            print(""Get all depositions from Zenodo"")\n            r = requests.get(\n                f""{self.zenodo_address}/api/deposit/depositions"",\n                headers=self.headers\n            )\n        print(""Get Depositions: Status Code: {}"".format(r.status_code))\n        return r\n\n    def remove_deposition(self, dep_id):\n        """""" Remove deposition with deposition id `dep_id`""""""\n        print(f\'Delete deposition number {dep_id}\')\n        r = requests.delete(\n            f\'{self.zenodo_address}/api/deposit/depositions/{dep_id}\',\n            headers=self.auth_header\n        )\n        return r\n\n    def remove_all_depositions(self):\n        """""" Removes all unpublished deposition (not records).""""""\n        all_depositions = self.get_deposition()\n        for dep in all_depositions.json():\n            self.remove_deposition(dep[""id""])\n\n\n# Probably remove that.\n# sandbox_asteroid_url = \'https://sandbox.zenodo.org/deposit/new?c=asteroid-models\'\n# zenodo_asteroid_url = \'https://zenodo.org/deposit/new?c=asteroid-models\'\nclass AsteroidZenodo(Zenodo):\n    REQUIRED_KEYS = []\n    def share_model(self, model_path):\n        # Load model_path\n        model = torch.load(model_path)\n        # Assert all keys are there\n        if not all(k in model.keys() for k in self.REQUIRED_KEYS):\n            missing = [k for k in self.REQUIRED_KEYS if k not in model.keys()]\n            raise ValueError(f""Expected all keys {self.REQUIRED_KEYS} but ""\n                             f""{missing} were missing."")\n'"
asteroid/scripts/__init__.py,0,b''
asteroid/scripts/asteroid_cli.py,0,"b'import os\nimport yaml\n\nimport asteroid\nfrom asteroid.models.publisher import upload_publishable\n\n\ndef upload():\n    import argparse\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'publish_dir\', type=str,\n                        help=\'Path to the publish dir.\')\n    parser.add_argument(\'--uploader\', default=None, type=str,\n                        help=\'Name of the uploader. Ex: `Manuel Pariente`\')\n    parser.add_argument(\'--affiliation\', default=None, type=str,\n                        help=\'Affiliation of the uploader. Ex `INRIA` \')\n    parser.add_argument(\'--git_username\', default=None, type=str,\n                        help=\'Username in GitHub\')\n    parser.add_argument(\'--token\', default=None, type=str,\n                        help=\'Access token for Zenodo (or sandbox)\')\n    parser.add_argument(\'--force_publish\', default=False, action=\'store_true\',\n                        help=\'Whether to  without asking confirmation\')\n    parser.add_argument(\'--use_sandbox\', default=False, action=\'store_true\',\n                        help=\'Whether to use Zenodo sandbox.\')\n    args = parser.parse_args()\n    args_as_dict = dict(vars(args))\n    # Load uploader info if present\n    info_file = os.path.join(asteroid.project_root, \'uploader_info.yml\')\n    if os.path.exists(info_file):\n        uploader_info = yaml.safe_load(open(info_file, \'r\'))\n        # Replace fields that where not specified (CLI dominates)\n        for k, v in uploader_info.items():\n            if args_as_dict[k] == parser.get_default(k):\n                args_as_dict[k] = v\n\n    upload_publishable(**args_as_dict)\n    # Suggest creating uploader_infos.yml\n    if not os.path.exists(info_file):\n        example = """"""\n        ```asteroid/uploader_infos.yml\n        uploader: Manuel Pariente\n        affiliation: Universite Lorraine, CNRS, Inria, LORIA, France\n        git_username: mpariente\n        token: XXX\n        ```\n        """"""\n        print(\'You can create a `uploader_infos.yml` file in `Asteroid` root\'\n              f\'to stop passing your name, affiliation etc. to the CLI. \'\n              f\'Here is an example {example}\')\n        print(""Thanks a lot for sharing your model! Don\'t forget to create""\n              ""a model card in the repo! "")'"
asteroid/utils/__init__.py,0,"b'from .parser_utils import (\n    prepare_parser_from_dict, parse_args_as_dict, str_int_float, str2bool,\n    str2bool_arg, isfloat, isint\n)\nfrom .torch_utils import tensors_to_device, to_cuda\nfrom .generic_utils import (\n    has_arg, flatten_dict, average_arrays_in_dic, get_wav_random_start_stop\n)\n# The functions above were all in asteroid/utils.py before refactoring into\n# asteroid/utils/*_utils.py files. They are imported for backward compatibility.\n'"
asteroid/utils/deprecation_utils.py,0,"b'import warnings\nfrom numpy import VisibleDeprecationWarning\n\n\nclass DeprecationMixin:\n    """""" Deprecation mixin. Example to come """"""\n    def warn_deprecated(self):\n        warnings.warn(\'{} is deprecated since v0.1.0, it will be removed in \'\n                      \'v0.2.0. Please use {} instead.\'\n                      \'\'.format(self.__class__.__name__,\n                                self.__class__.__bases__[0].__name__),\n                      VisibleDeprecationWarning)\n\n\ndef deprecate_func(func, old_name):\n    """""" Function to return DeprecationWarning when a deprecated function\n    is called. Example to come.""""""\n    def func_with_warning(*args, **kwargs):\n        """""" Deprecated function, please read your warnings. """"""\n        warnings.warn(\'{} is deprecated since v0.1.0, it will be removed in \'\n                      \'v0.2.0. Please use {} instead.\'\n                      \'\'.format(old_name, func.__name__),\n                      VisibleDeprecationWarning)\n        return func(*args, **kwargs)\n    return func_with_warning\n'"
asteroid/utils/generic_utils.py,0,"b'import inspect\nimport collections\nimport numpy as np\n\n\ndef has_arg(fn, name):\n    """""" Checks if a callable accepts a given keyword argument.\n\n    Args:\n        fn (callable): Callable to inspect.\n        name (str): Check if `fn` can be called with `name` as a keyword\n            argument.\n\n    Returns:\n        bool: whether `fn` accepts a `name` keyword argument.\n    """"""\n    signature = inspect.signature(fn)\n    parameter = signature.parameters.get(name)\n    if parameter is None:\n        return False\n    return (parameter.kind in (inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                               inspect.Parameter.KEYWORD_ONLY))\n\n\ndef flatten_dict(d, parent_key=\'\', sep=\'_\'):\n    """""" Flattens a dictionary into a single-level dictionary while preserving\n    parent keys. Taken from https://stackoverflow.com/questions/6027558/\n    flatten-nested-dictionaries-compressing-keys?answertab=votes#tab-top\n\n    Args:\n        d (collections.MutableMapping): Dictionary to be flattened.\n        parent_key (str): String to use as a prefix to all subsequent keys.\n        sep (str): String to use as a separator between two key levels.\n\n    Returns:\n        dict: Single-level dictionary, flattened.\n    """"""\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n\n\ndef average_arrays_in_dic(dic):\n    """""" Take average of numpy arrays in a dictionary.\n\n    Args:\n        dic (dict): Input dictionary to take average from\n\n    Returns:\n        dict: New dictionary with array averaged.\n\n    """"""\n    # Copy dic first\n    dic = dict(dic)\n    for k, v in dic.items():\n        if isinstance(v, np.ndarray):\n            dic[k] = float(v.mean())\n    return dic\n\n\ndef get_wav_random_start_stop(signal_len, desired_len=4*8000):\n    """""" Get indexes for a chunk of signal of a given length.\n\n    Args:\n        signal_len (int): length of the signal to trim.\n        desired_len (int): the length of [start:stop]\n\n    Returns:\n        tuple: random start integer, stop integer.\n    """"""\n    if signal_len == desired_len or desired_len is None:\n        rand_start = 0\n    else:\n        rand_start = np.random.randint(0, signal_len - desired_len)\n    if desired_len is None:\n        stop = None\n    else:\n        stop = rand_start + desired_len\n    return rand_start, stop\n'"
asteroid/utils/hub_utils.py,2,"b'import os\nfrom torch import hub\nfrom hashlib import sha256\n\n\nCACHE_DIR = os.getenv(\n    \'ASTEROID_CACHE\',\n    os.path.expanduser(\'~/.cache/torch/asteroid\'),\n)\nMODELS_URLS_HASHTABLE = {\n    \'mpariente/ConvTasNet_WHAM!_sepclean\': \'https://zenodo.org/record/3862942/files/model.pth?download=1\',\n    \'mpariente/DPRNNTasNet_WHAM!_sepclean\': \'https://zenodo.org/record/3873670/files/model.pth?download=1\',\n    \'Cosentino/ConvTasNet_LibriMix_sep_clean\': \'https://zenodo.org/record/3873572/files/model.pth?download=1\',\n    \'Cosentino/ConvTasNet_LibriMix_sep_noisy\': \'https://zenodo.org/record/3874420/files/model.pth?download=1\',\n}\n\n\ndef cached_download(filename_or_url):\n    """""" Download from URL with torch.hub and cache the result in ASTEROID_CACHE.\n\n    Args:\n        filename_or_url (str): Name of a model as named on the Zenodo Community\n            page (ex: mpariente/ConvTasNet_WHAM!_sepclean), or an URL to a model\n            file (ex: https://zenodo.org/.../model.pth), or a filename\n            that exists locally (ex: local/tmp_model.pth)\n\n    Returns:\n        str, normalized path to the downloaded (or not) model\n    """"""\n    if os.path.isfile(filename_or_url):\n        return filename_or_url\n\n    if filename_or_url in MODELS_URLS_HASHTABLE:\n        url = MODELS_URLS_HASHTABLE[filename_or_url]\n    else:\n        # Give a chance to direct URL, torch.hub will handle exceptions\n        url = filename_or_url\n    cached_filename = url_to_filename(url)\n    cached_dir = os.path.join(get_cache_dir(), cached_filename)\n    cached_path = os.path.join(cached_dir, \'model.pth\')\n\n    os.makedirs(cached_dir, exist_ok=True)\n    if not os.path.isfile(cached_path):\n        hub.download_url_to_file(url, cached_path)\n        return cached_path\n    # It was already downloaded\n    print(f\'Using cached model `{filename_or_url}`\')\n    return cached_path\n\n\ndef url_to_filename(url):\n    """""" Consistently convert `url` into a filename. """"""\n    _bytes = url.encode(""utf-8"")\n    _hash = sha256(_bytes)\n    filename = _hash.hexdigest()\n    return filename\n\n\ndef get_cache_dir():\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    return CACHE_DIR\n'"
asteroid/utils/parser_utils.py,0,"b'import argparse\n\n\ndef prepare_parser_from_dict(dic, parser=None):\n    """""" Prepare an argparser from a dictionary.\n\n    Args:\n        dic (dict): Two-level config dictionary with unique bottom-level keys.\n        parser (argparse.ArgumentParser, optional): If a parser already\n            exists, add the keys from the dictionary on the top of it.\n\n    Returns:\n        argparse.ArgumentParser:\n            Parser instance with groups corresponding to the first level keys\n            and arguments corresponding to the second level keys with default\n            values given by the values.\n    """"""\n    def standardized_entry_type(value):\n        """""" If the default value is None, replace NoneType by str_int_float.\n            If the default value is boolean, look for boolean strings.""""""\n        if value is None:\n            return str_int_float\n        if isinstance(str2bool(value), bool):\n            return str2bool_arg\n        return type(value)\n\n    if parser is None:\n        parser = argparse.ArgumentParser()\n    for k in dic.keys():\n        group = parser.add_argument_group(k)\n        for kk in dic[k].keys():\n            entry_type = standardized_entry_type(dic[k][kk])\n            group.add_argument(\'--\' + kk, default=dic[k][kk],\n                               type=entry_type)\n    return parser\n\n\ndef str_int_float(value):\n    """""" Type to convert strings to int, float (in this order) if possible.\n\n    Args:\n        value (str): Value to convert.\n\n    Returns:\n        int, float, str: Converted value.\n    """"""\n    if isint(value):\n        return int(value)\n    if isfloat(value):\n        return float(value)\n    elif isinstance(value, str):\n        return value\n\n\ndef str2bool(value):\n    """""" Type to convert strings to Boolean (returns input if not boolean) """"""\n    if not isinstance(value, str):\n        return value\n    if value.lower() in (\'yes\', \'true\', \'y\', \'1\'):\n        return True\n    elif value.lower() in (\'no\', \'false\', \'n\', \'0\'):\n        return False\n    else:\n        return value\n\n\ndef str2bool_arg(value):\n    """""" Argparse type to convert strings to Boolean """"""\n    value = str2bool(value)\n    if isinstance(value, bool):\n        return value\n    raise argparse.ArgumentTypeError(\'Boolean value expected.\')\n\n\ndef isfloat(value):\n    """""" Computes whether `value` can be cast to a float.\n\n    Args:\n        value (str): Value to check.\n\n    Returns:\n        bool: Whether `value` can be cast to a float.\n\n    """"""\n    try:\n        float(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef isint(value):\n    """""" Computes whether `value` can be cast to an int\n\n    Args:\n        value (str): Value to check.\n\n    Returns:\n        bool: Whether `value` can be cast to an int.\n\n    """"""\n    try:\n        int(value)\n        return True\n    except ValueError:\n        return False\n\n\ndef parse_args_as_dict(parser, return_plain_args=False, args=None):\n    """""" Get a dict of dicts out of process `parser.parse_args()`\n\n    Top-level keys corresponding to groups and bottom-level keys corresponding\n    to arguments. Under `\'main_args\'`, the arguments which don\'t belong to a\n    argparse group (i.e main arguments defined before parsing from a dict) can\n    be found.\n\n    Args:\n        parser (argparse.ArgumentParser): ArgumentParser instance containing\n            groups. Output of `prepare_parser_from_dict`.\n        return_plain_args (bool): Whether to return the output or\n            `parser.parse_args()`.\n        args (list): List of arguments as read from the command line.\n            Used for unit testing.\n\n    Returns:\n        dict:\n            Dictionary of dictionaries containing the arguments. Optionally the\n            direct output `parser.parse_args()`.\n    """"""\n    args = parser.parse_args(args=args)\n    args_dic = {}\n    for group in parser._action_groups:\n        group_dict = {a.dest: getattr(args, a.dest, None)\n                      for a in group._group_actions}\n        args_dic[group.title] = group_dict\n    args_dic[\'main_args\'] = args_dic[\'optional arguments\']\n    del args_dic[\'optional arguments\']\n    if return_plain_args:\n        return args_dic, args\n    return args_dic\n'"
asteroid/utils/torch_utils.py,14,"b'import torch\nfrom torch import nn\nfrom collections import OrderedDict\n\n\ndef to_cuda(tensors):  # pragma: no cover (No CUDA on travis)\n    """""" Transfer tensor, dict or list of tensors to GPU.\n\n    Args:\n        tensors (:class:`torch.Tensor`, list or dict): May be a single, a\n            list or a dictionary of tensors.\n\n    Returns:\n        :class:`torch.Tensor`:\n            Same as input but transferred to cuda. Goes through lists and dicts\n            and transfers the torch.Tensor to cuda. Leaves the rest untouched.\n    """"""\n    if isinstance(tensors, torch.Tensor):\n        return tensors.cuda()\n    if isinstance(tensors, list):\n        return [to_cuda(tens) for tens in tensors]\n    if isinstance(tensors, dict):\n        for key in tensors.keys():\n            tensors[key] = to_cuda(tensors[key])\n        return tensors\n    raise TypeError(\'tensors must be a tensor or a list or dict of tensors. \'\n                    \' Got tensors of type {}\'.format(type(tensors)))\n\n\ndef tensors_to_device(tensors, device):\n    """""" Transfer tensor, dict or list of tensors to device.\n\n    Args:\n        tensors (:class:`torch.Tensor`): May be a single, a list or a\n            dictionary of tensors.\n        device (:class: `torch.device`): the device where to place the tensors.\n\n    Returns:\n        Union [:class:`torch.Tensor`, list, tuple, dict]:\n            Same as input but transferred to device.\n            Goes through lists and dicts and transfers the torch.Tensor to\n            device. Leaves the rest untouched.\n    """"""\n    if isinstance(tensors, torch.Tensor):\n        return tensors.to(device)\n    elif isinstance(tensors, (list, tuple)):\n        return [tensors_to_device(tens, device) for tens in tensors]\n    elif isinstance(tensors, dict):\n        for key in tensors.keys():\n            tensors[key] = tensors_to_device(tensors[key], device)\n        return tensors\n    else:\n        return tensors\n\n\ndef pad_x_to_y(x, y, axis=-1):\n    """"""  Pad first argument to have same size as second argument\n\n    Args:\n        x (torch.Tensor): Tensor to be padded.\n        y (torch.Tensor): Tensor to pad x to.\n        axis (int): Axis to pad on.\n\n    Returns:\n        torch.Tensor, x padded to match y\'s shape.\n    """"""\n    if axis != -1:\n        raise NotImplementedError\n    inp_len = y.size(axis)\n    output_len = x.size(axis)\n    return nn.functional.pad(x, [0, inp_len - output_len])\n\n\ndef load_state_dict_in(state_dict, model):\n    """""" Strictly loads state_dict in model, or the next submodel.\n        Useful to load standalone model after training it with System.\n\n    Args:\n        state_dict (OrderedDict): the state_dict to load.\n        model (torch.nn.Module): the model to load it into\n\n    Returns:\n        torch.nn.Module: model with loaded weights.\n\n    # Note :\n        Keys in a state_dict look like object1.object2.layer_name.weight.etc\n        We first try to load the model in the classic way.\n        If this fail we removes the first left part of the key to obtain\n        object2.layer_name.weight.etc.\n        Blindly loading with strictly=False should be done with some logging\n        of the missing keys in the state_dict and the model.\n\n    """"""\n    try:\n        # This can fail if the model was included into a bigger nn.Module\n        # object. For example, into System.\n        model.load_state_dict(state_dict, strict=True)\n    except RuntimeError:\n        # keys look like object1.object2.layer_name.weight.etc\n        # The following will remove the first left part of the key to obtain\n        # object2.layer_name.weight.etc.\n        # Blindly loading with strictly=False should be done with some\n        # new_state_dict of the missing keys in the state_dict and the model.\n        new_state_dict = OrderedDict()\n        for k, v in state_dict.items():\n            new_k = k[k.find(\'.\') + 1:]\n            new_state_dict[new_k] = v\n        model.load_state_dict(new_state_dict, strict=True)\n    return model\n\n\ndef are_models_equal(model1, model2):\n    """""" Check for weights equality between models.\n\n    Args:\n        model1 (nn.Module): model instance to be compared.\n        model2 (nn.Module): second model instance to be compared.\n\n    Returns:\n        bool: Whether all model weights are equal.\n    """"""\n    for p1, p2 in zip(model1.parameters(), model2.parameters()):\n        if p1.data.ne(p2.data).sum() > 0:\n            return False\n    return True\n'"
docs/source/conf.py,1,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file does only contain a selection of the most common options. For a\n# full list see the documentation:\n# http://www.sphinx-doc.org/en/master/config\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n\nimport os\nimport sys\nimport glob\nimport shutil\n\nimport builtins\n# TODO : change it to asteroid_sphinx_theme\n\nPATH_HERE = os.path.abspath(os.path.dirname(__file__))\nPATH_ROOT = os.path.join(PATH_HERE, \'..\', \'..\')\nsys.path.insert(0, os.path.abspath(PATH_ROOT))\n\n# -- Project information -----------------------------------------------------\n\nproject = \'asteroid\'\ncopyright = \'2019, Oncoming\'\nauthor = \'Manuel Pariente et al.\'\n# The short X.Y version\nversion = \'0.0.1\'\n# The full version, including alpha/beta/rc tags\nrelease = \'0.0.1\'\n\n# -- General configuration ---------------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\nneeds_sphinx = \'1.4\'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    \'sphinx.ext.autodoc\',\n    \'sphinx.ext.autosummary\',\n    \'sphinx.ext.doctest\',\n    \'sphinx.ext.intersphinx\',\n    \'sphinx.ext.todo\',\n    \'sphinx.ext.coverage\',\n    \'sphinx.ext.napoleon\',\n    \'sphinx.ext.autosectionlabel\',\n    # \'sphinxcontrib.mockautodoc\',\n    # \'sphinxcontrib.fulltoc\',  # breaks pytorch-theme with unexpected\n    # w argument \'titles_only\'\n    # We can either use viewcode, which shows source code in the doc page\n    \'sphinx.ext.viewcode\',\n    # Or linkcode to find the corresponding code in github. Start with viewcode\n    # \'sphinx.ext.linkcode\',\n    \'recommonmark\',\n    # \'m2r\',\n    \'nbsphinx\',\n]\n\n# Napoleon config\nnapoleon_include_special_with_doc = True\nnapoleon_use_ivar = True\nnapoleon_use_rtype = False\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\'_templates\']\n\n# https://berkeley-stat159-f17.github.io/stat159-f17/lectures/14-sphinx..html#conf.py-(cont.)\n# https://stackoverflow.com/questions/38526888/embed-ipython-notebook-in-sphinx-document\n# I execute the notebooks manually in advance. If notebooks test the code,\n# they should be run at build time.\nnbsphinx_execute = \'never\'\nnbsphinx_allow_errors = True\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n#\n# source_suffix = [\'.rst\', \'.md\']\n# source_suffix = [\'.rst\', \'.md\', \'.ipynb\']\nsource_suffix = {\n    \'.rst\': \'restructuredtext\',\n    \'.txt\': \'markdown\',\n    \'.md\': \'markdown\',\n    \'.ipynb\': \'nbsphinx\',\n}\n\n# The master toctree document.\nmaster_doc = \'index\'\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set ""language"" from the command line for these cases.\nlanguage = None\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = []\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = None\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n# http://www.sphinx-doc.org/en/master/usage/theming.html#builtin-themes\n# html_theme = \'bizstyle\'\n# https://sphinx-themes.org\n\nhtml_theme = \'sphinx_rtd_theme\'\n\n# import pt_lightning_sphinx_theme\n# html_theme = \'pt_lightning_sphinx_theme\'\n# html_theme_path = [pt_lightning_sphinx_theme.get_html_theme_path()]\n\n# import pytorch_sphinx_theme\n# html_theme = \'pytorch_sphinx_theme\'\n# html_theme_path = [pytorch_sphinx_theme.get_html_theme_path()]\n#\n# Theme options are theme-specific and customize the look and feel of a theme\n# further.  For a list of options available for each theme, see the\n# documentation.\n\nhtml_theme_options = {\n    \'pytorch_project\': \'docs\',\n    \'canonical_url\': \'https://github.com/mpariente/asteroid\',\n    \'collapse_navigation\': False,\n    \'display_version\': True,\n    \'logo_only\': False,\n}\n\nhtml_logo = \'_static/images/asteroid_logo.svg\'\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [\'_static\']\n\n# Custom sidebar templates, must be a dictionary that maps document names\n# to template names.\n#\n# The default sidebars (for documents that don\'t match any pattern) are\n# defined by theme itself.  Builtin themes are using these templates by\n# default: ``[\'localtoc.html\', \'relations.html\', \'sourcelink.html\',\n# \'searchbox.html\']``.\n#\n# html_sidebars = {}\n\n\n# -- Options for HTMLHelp output ---------------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = project + \'-doc\'\n\n# -- Options for LaTeX output ------------------------------------------------\n\nlatex_elements = {\n    # The paper size (\'letterpaper\' or \'a4paper\').\n    # \'papersize\': \'letterpaper\',\n\n    # The font size (\'10pt\', \'11pt\' or \'12pt\').\n    # \'pointsize\': \'10pt\',\n\n    # Additional stuff for the LaTeX preamble.\n    # \'preamble\': \'\',\n\n    # Latex figure (float) alignment\n    \'figure_align\': \'htbp\',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title,\n#  author, documentclass [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, project + \'.tex\', project + \' Documentation\', author, \'manual\'),\n]\n\n# -- Options for manual page output ------------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    (master_doc, project, project + \' Documentation\', [author], 1)\n]\n\n# -- Options for Texinfo output ----------------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (master_doc, project, project + \' Documentation\', author, project,\n     \'One line description of project.\', \'Miscellaneous\'),\n]\n\n# -- Options for Epub output -------------------------------------------------\n\n# Bibliographic Dublin Core info.\nepub_title = project\n\n# The unique identifier of the text. This can be a ISBN number\n# or the project homepage.\n#\n# epub_identifier = \'\'\n\n# A unique identification for the text.\n#\n# epub_uid = \'\'\n\n# A list of files that should not be packed into the epub file.\nepub_exclude_files = [\'search.html\']\n\n# -- Extension configuration -------------------------------------------------\n\n# -- Options for intersphinx extension ---------------------------------------\n\n# Intersphinx config\nintersphinx_mapping = {\n    \'python\': (\'https://docs.python.org/3\', None),\n    \'numpy\': (\'http://docs.scipy.org/doc/numpy/\', None),\n    \'torch\': (\'https://pytorch.org/docs/master/\', None),\n    \'pytorch_lightning\':\n        (\'https://pytorch-lightning.readthedocs.io/en/latest/\', None)\n}\n\n# -- Options for todo extension ----------------------------------------------\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = True\n\n# https://github.com/rtfd/readthedocs.org/issues/1139\n# I use sphinx-apidoc to auto-generate API documentation for my project.\n# Right now I have to commit these auto-generated files to my repository\n# so that RTD can build them into HTML docs. It\'d be cool if RTD could run\n# sphinx-apidoc for me, since it\'s easy to forget to regen API docs\n# and commit them to my repo after making changes to my code.\n\nPACKAGES = [\n    \'asteroid\'\n]\n\n\ndef run_apidoc(_):\n    os.makedirs(os.path.join(PATH_HERE, \'apidoc\'), exist_ok=True)\n    for pkg in PACKAGES:\n        argv = [\'-e\', \'-o\', os.path.join(PATH_HERE, \'apidoc\'),\n                os.path.join(PATH_HERE, PATH_ROOT, pkg), \'**/test_*\',\n                \'--force\', \'--private\', \'--module-first\']\n        try:\n            # Sphinx 1.7+\n            from sphinx.ext import apidoc\n            apidoc.main(argv)\n        except ImportError:\n            # Sphinx 1.6 (and earlier)\n            from sphinx import apidoc\n            argv.insert(0, apidoc.__file__)\n            apidoc.main(argv)\n\n\ndef setup(app):\n    app.connect(\'builder-inited\', run_apidoc)\n\n\n# copy all notebooks to local folder #FIXME : temp fix\n# path_nbs = os.path.join(PATH_HERE, \'notebooks\')\n# if not os.path.isdir(path_nbs):\n#     os.mkdir(path_nbs)\n# for path_ipynb in glob.glob(os.path.join(PATH_ROOT, \'notebooks\', \'*.ipynb\')):\n#     path_ipynb2 = os.path.join(path_nbs, os.path.basename(path_ipynb))\n#     shutil.copy(path_ipynb, path_ipynb2)\n\n# Ignoring Third-party packages\n# https://stackoverflow.com/questions/15889621/sphinx-how-to-exclude-imports-in-automodule\n\nMOCK_REQUIRE_PACKAGES = []\nwith open(os.path.join(PATH_ROOT, \'requirements.txt\'), \'r\') as fp:\n    for ln in fp.readlines():\n        found = [ln.index(ch) for ch in list(\',=<>#\') if ch in ln]\n        pkg = ln[:min(found)] if found else ln\n        if pkg.rstrip():\n            MOCK_REQUIRE_PACKAGES.append(pkg.rstrip())\n\n# TODO: better parse from package since the import name and package name may differ\nMOCK_MANUAL_PACKAGES = [\'torch\', \'torchvision\']\nautodoc_mock_imports = MOCK_REQUIRE_PACKAGES + MOCK_MANUAL_PACKAGES\n# for mod_name in MOCK_REQUIRE_PACKAGES:\n#     sys.modules[mod_name] = mock.Mock()\n\n\n# Options for the linkcode extension\n# ----------------------------------\n# github_user = \'mpariente\'\n# github_repo = \'asteroid\'\n#\n#\n# # Resolve function\n# # This function is used to populate the (source) links in the API\n# def linkcode_resolve(domain, info):\n#     def find_source():\n#         # try to find the file and line number, based on code from numpy:\n#         # https://github.com/numpy/numpy/blob/master/doc/source/conf.py#L286\n#         obj = sys.modules[info[\'module\']]\n#         for part in info[\'fullname\'].split(\'.\'):\n#             obj = getattr(obj, part)\n#         fname = inspect.getsourcefile(obj)\n#         # https://github.com/rtfd/readthedocs.org/issues/5735\n#         if any([s in fname for s in (\'readthedocs\', \'rtfd\', \'checkouts\')]):\n#             # /home/docs/checkouts/readthedocs.org/user_builds/pytorch_lightning/checkouts/\n#             #  devel/pytorch_lightning/utilities/cls_experiment.py#L26-L176\n#             path_top = os.path.abspath(os.path.join(\'..\', \'..\', \'..\'))\n#             fname = os.path.relpath(fname, start=path_top)\n#         else:\n#             # Local build, imitate master\n#             fname = \'master/\' + os.path.relpath(fname, start=os.path.abspath(\'..\'))\n#         source, lineno = inspect.getsourcelines(obj)\n#         return fname, lineno, lineno + len(source) - 1\n#\n#     if domain != \'py\' or not info[\'module\']:\n#         return None\n#     try:\n#         filename = \'%s#L%d-L%d\' % find_source()\n#     except Exception:\n#         filename = info[\'module\'].replace(\'.\', \'/\') + \'.py\'\n#     # import subprocess\n#     # tag = subprocess.Popen([\'git\', \'rev-parse\', \'HEAD\'], stdout=subprocess.PIPE,\n#     #                        universal_newlines=True).communicate()[0][:-1]\n#     branch = filename.split(\'/\')[0]\n#     # do mapping from latest tags to master\n#     branch = {\'latest\': \'master\', \'stable\': \'master\'}.get(branch, branch)\n#     filename = \'/\'.join([branch] + filename.split(\'/\')[1:])\n#     return ""https://github.com/%s/%s/blob/%s"" \\\n#            % (github_user, github_repo, filename)\n\n\n\n# Autodoc config\nautodoc_inherit_docstring = False\nautodoc_default_flags = [\'members\', \'show-inheritance\']\n# Order functions by appearance in source (default \'alphabetical\')\nautodoc_member_order = \'groupwise\'\n\n\n# autodoc_member_order = \'groupwise\'\n# # autoclass_content = \'both\'\n# # autodoc_default_flags = [\n# #     \'members\', \'undoc-members\', \'show-inheritance\', \'private-members\',\n# #     # \'special-members\', \'inherited-members\'\n# # ]\n# autodoc_default_flags = [\'members\', \'show-inheritance\']\n#\n# # Autodoc config\n# autodoc_inherit_docstring = True\n# # autodoc_default_flags = [\'members\', \'show-inheritance\']\n# # Order functions by appearance in source (default \'alphabetical\')\n# # autodoc_member_order = \'bysource\''"
tests/engine/optimizers_test.py,0,"b'import pytest\nfrom torch import nn, optim\nfrom asteroid.engine import optimizers\nfrom torch_optimizer import Ranger\n\n\ndef optim_mapping():\n    mapping_list = [\n        (optim.Adam, \'adam\'),\n        (optim.SGD, \'sgd\'),\n        (optim.RMSprop, \'rmsprop\'),\n        (Ranger, \'ranger\')\n    ]\n    return mapping_list\n\n\nglobal_model = nn.Sequential(nn.Linear(10, 10),\n                             nn.ReLU())\n\n\n@pytest.mark.parametrize(""opt"", [\n    \'Adam\', \'RMSprop\', \'SGD\', \'Adadelta\', \'Adagrad\', \'Adamax\', \'AdamW\', \'ASGD\',\n    \'AccSGD\', \'AdaBound\', \'AdaMod\', \'DiffGrad\', \'Lamb\', \'NovoGrad\', \'PID\',\n    \'QHAdam\', \'QHM\', \'RAdam\', \'SGDW\', \'Yogi\', \'Ranger\', \'RangerQH\', \'RangerVA\'\n])\ndef test_all_get(opt):\n    asteroid_optim = optimizers.get(opt)(global_model.parameters(), lr=1e-3)\n\n\n@pytest.mark.parametrize(""opt_tuple"", optim_mapping())\ndef test_get_str_returns_instance(opt_tuple):\n    torch_optim = opt_tuple[0](global_model.parameters(), lr=1e-3)\n    asteroid_optim = optimizers.get(opt_tuple[1])(global_model.parameters(),\n                                                  lr=1e-3)\n    assert type(torch_optim) == type(asteroid_optim)\n    assert torch_optim.param_groups == asteroid_optim.param_groups\n\n\n@pytest.mark.parametrize(""opt"", [optim.Adam, optim.SGD, optim.Adadelta])\ndef test_get_instance_returns_instance(opt):\n    torch_optim = opt(global_model.parameters(), lr=1e-3)\n    asteroid_optim = optimizers.get(torch_optim)\n    assert torch_optim == asteroid_optim\n\n\n@pytest.mark.parametrize(""wrong"", [""wrong_string"", 12, object()])\ndef test_get_errors(wrong):\n    with pytest.raises(ValueError):\n        # Should raise for anything not a Optimizer instance + unknown string\n        optimizers.get(wrong)\n\n\ndef test_make_optimizer():\n    optimizers.make_optimizer(global_model.parameters(), ""adam"", lr=1e-3)\n'"
tests/engine/system_test.py,2,"b'import torch\nfrom torch import nn, optim\nfrom torch.utils import data\nfrom pytorch_lightning import Trainer\n\nfrom asteroid.engine.system import System\n\n\nclass DummyDataset(data.Dataset):\n    def __init__(self):\n        self.inp_dim = 10\n        self.out_dim = 10\n\n    def __len__(self):\n        return 20\n\n    def __getitem__(self, idx):\n        return torch.randn(1, self.inp_dim), torch.randn(1, self.out_dim)\n\n\ndef test_system():\n    model = nn.Sequential(nn.Linear(10, 10), nn.ReLU())\n    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer)\n    dataset = DummyDataset()\n    loader = data.DataLoader(dataset, batch_size=2, num_workers=4)\n    system = System(model, optimizer, loss_func=nn.MSELoss(),\n                    train_loader=loader, val_loader=loader,\n                    scheduler=scheduler)\n    trainer = Trainer(max_epochs=1, fast_dev_run=True)\n    trainer.fit(system)\n'"
tests/filterbanks/filterbanks_test.py,5,"b'import random\nimport pytest\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom asteroid import filterbanks\nfrom asteroid.filterbanks import Encoder, Decoder\nfrom asteroid.filterbanks import FreeFB, AnalyticFreeFB, ParamSincFB, MultiphaseGammatoneFB\nfrom asteroid.filterbanks import make_enc_dec\n\n\ndef fb_config_list():\n    keys = [\'n_filters\', \'kernel_size\', \'stride\']\n    param_list = [\n        [256, 256, 128],\n        [256, 256, 64],\n        [500, 16, 8],\n        [200, 80, None],\n    ]\n    return [dict(zip(keys, values)) for values in param_list]\n\n\n@pytest.mark.parametrize(""fb_class"", [FreeFB, AnalyticFreeFB, ParamSincFB, MultiphaseGammatoneFB])\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_fb_def_and_forward_lowdim(fb_class, fb_config):\n    """""" Test filterbank definition and encoder/decoder forward.""""""\n    # Definition\n    enc = Encoder(fb_class(**fb_config))\n    dec = Decoder(fb_class(**fb_config))\n    # Forward\n    inp = torch.randn(1, 1, 16000)\n    tf_out = enc(inp)\n    # Assert for 2D inputs\n    with pytest.warns(UserWarning):\n        # STFT(2D) gives 3D and iSTFT(3D) gives 3D. UserWarning about that.\n        assert_allclose(enc(inp), enc(inp[0]))\n    # Assert for 1D inputs\n    assert_allclose(enc(inp)[0], enc(inp[0, 0]))\n\n    out = dec(tf_out)\n    # Assert for 4D inputs\n    tf_out_4d = tf_out.repeat(1, 2, 1, 1)\n    out_4d = dec(tf_out_4d)\n    assert_allclose(out, out_4d[:, 0])\n    # Asser for 2D inputs\n    assert_allclose(out[0, 0], dec(tf_out[0]))\n    assert tf_out.shape[1] == enc.filterbank.n_feats_out\n\n\n@pytest.mark.parametrize(""fb_class"", [FreeFB, AnalyticFreeFB, ParamSincFB, MultiphaseGammatoneFB])\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_fb_def_and_forward_all_dims(fb_class, fb_config):\n    """""" Test encoder/decoder on other shapes than 3D""""""\n    # Definition\n    enc = Encoder(fb_class(**fb_config))\n    dec = Decoder(fb_class(**fb_config))\n\n    # 3D Forward with one channel\n    inp = torch.randn(3, 1, 32000)\n    tf_out = enc(inp)\n    assert tf_out.shape[:2] == (3, enc.filterbank.n_feats_out)\n    out = dec(tf_out)\n    assert out.shape[:-1] == inp.shape[:-1]  # Time axis can differ\n\n\n@pytest.mark.parametrize(""fb_class"", [FreeFB, AnalyticFreeFB, ParamSincFB, MultiphaseGammatoneFB])\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\n@pytest.mark.parametrize(""ndim"", [2, 3, 4])\ndef test_fb_forward_multichannel(fb_class, fb_config, ndim):\n    """""" Test encoder/decoder in multichannel setting""""""\n    # Definition\n    enc = Encoder(fb_class(**fb_config))\n    dec = Decoder(fb_class(**fb_config))\n    # 3D Forward with several channels\n    tensor_shape = tuple([random.randint(2, 4) for _ in range(ndim)]) + (4000,)\n    inp = torch.randn(tensor_shape)\n    tf_out = enc(inp)\n    assert tf_out.shape[:ndim+1] == (tensor_shape[:-1] +\n                                     (enc.filterbank.n_feats_out,))\n    out = dec(tf_out)\n    assert out.shape[:-1] == inp.shape[:-1]  # Time axis can differ\n\n\n@pytest.mark.parametrize(""fb_class"", [AnalyticFreeFB, ParamSincFB])\n@pytest.mark.parametrize(""n_filters"", [256, 257])\n@pytest.mark.parametrize(""kernel_size"", [256, 257])\ndef test_complexfb_shapes(fb_class, n_filters, kernel_size):\n    fb = fb_class(n_filters, kernel_size)\n    assert fb.filters.shape[0] == 2 * (n_filters // 2)\n\n\n@pytest.mark.parametrize(""kernel_size"", [256, 257, 128, 129])\ndef test_paramsinc_shape(kernel_size):\n    """""" ParamSincFB has odd length filters """"""\n    fb = ParamSincFB(n_filters=200, kernel_size=kernel_size)\n    assert fb.filters.shape[-1] == 2 * (kernel_size // 2) + 1\n\n\n@pytest.mark.parametrize(""fb_class"", [FreeFB, AnalyticFreeFB, ParamSincFB, MultiphaseGammatoneFB])\ndef test_pinv_of(fb_class):\n    fb = fb_class(n_filters=500, kernel_size=16, stride=8)\n    encoder = Encoder(fb)\n    # Pseudo inverse can be taken from an Encoder/Decoder class or Filterbank.\n    decoder_e = Decoder.pinv_of(encoder)\n    decoder_f = Decoder.pinv_of(fb)\n    assert_allclose(decoder_e.filters, decoder_f.filters)\n\n    # Check filter computing\n    inp = torch.randn(1, 1, 32000)\n    _ = decoder_e(encoder(inp))\n\n    decoder = Decoder(fb)\n    # Pseudo inverse can be taken from an Encoder/Decoder class or Filterbank.\n    encoder_e = Encoder.pinv_of(decoder)\n    encoder_f = Encoder.pinv_of(fb)\n    assert_allclose(encoder_e.filters, encoder_f.filters)\n\n\n@pytest.mark.parametrize(""who"", [""enc"", ""dec""])\ndef test_make_enc_dec(who):\n    fb_config = {""n_filters"": 500,\n                 ""kernel_size"": 16,\n                 ""stride"": 8}\n    enc, dec = make_enc_dec(""free"", who_is_pinv=who, **fb_config)\n    enc, dec = make_enc_dec(FreeFB, who_is_pinv=who, **fb_config)\n    assert enc.filterbank == filterbanks.get(enc.filterbank)\n\n\n@pytest.mark.parametrize(""wrong"", [""wrong_string"", 12, object()])\ndef test_get_errors(wrong):\n    with pytest.raises(ValueError):\n        # Should raise for anything not a Optimizer instance + unknown string\n        filterbanks.get(wrong)\n\n\ndef test_get_none():\n    assert filterbanks.get(None) is None\n'"
tests/filterbanks/stft_test.py,6,"b'import pytest\nimport torch\nfrom torch import testing\nimport numpy as np\nfrom scipy.signal import get_window\n\nfrom asteroid.filterbanks import Encoder, Decoder, STFTFB\nfrom asteroid.filterbanks import make_enc_dec, griffin_lim, misi\nfrom asteroid.filterbanks.stft_fb import perfect_synthesis_window\nfrom asteroid.filterbanks import transforms\n\n\ndef fb_config_list():\n    keys = [\'n_filters\', \'kernel_size\', \'stride\']\n    param_list = [\n        [256, 256, 128],  # Usual STFT, 50% overlap\n        [256, 256, 64],  # Usual STFT, 25% overlap\n        [512, 32, 16],  # Overcomplete STFT, 50% overlap\n    ]\n    return [dict(zip(keys, values)) for values in param_list]\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_stft_def(fb_config):\n    """""" Check consistency between two calls.""""""\n    fb = STFTFB(**fb_config)\n    enc = Encoder(fb)\n    dec = Decoder(fb)\n    enc2, dec2 = make_enc_dec(\'stft\', **fb_config)\n    testing.assert_allclose(enc.filterbank.filters, enc2.filterbank.filters)\n    testing.assert_allclose(dec.filterbank.filters, dec2.filterbank.filters)\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_stft_windows(fb_config):\n    n_filters, kernel_size = fb_config[""n_filters""], fb_config[""kernel_size""]\n    win = np.hanning(kernel_size)\n    fb = STFTFB(**fb_config, window=win)\n    with pytest.raises(AssertionError):\n        win = np.hanning(kernel_size + 1)\n        fb = STFTFB(**fb_config, window=win)\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_filter_shape(fb_config):\n    # Instantiate STFT\n    fb = STFTFB(**fb_config)\n    # Check filter shape.\n    assert fb.filters.shape == (fb_config[\'n_filters\'] + 2, 1,\n                                fb_config[\'kernel_size\'])\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\ndef test_perfect_istft_default_parameters(fb_config):\n    """""" Unit test perfect reconstruction with default values. """"""\n    kernel_size = fb_config[\'kernel_size\']\n    enc, dec = make_enc_dec(\'stft\', **fb_config)\n    inp_wav = torch.randn(2, 1, 32000)\n    out_wav = dec(enc(inp_wav))[:, :, kernel_size: -kernel_size]\n    inp_test = inp_wav[:, :, kernel_size: -kernel_size]\n    testing.assert_allclose(inp_test, out_wav)\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\n@pytest.mark.parametrize(""analysis_window_name"", [\n    \'blackman\', \'hamming\', \'hann\', \'bartlett\', \'boxcar\'\n])\ndef test_perfect_resyn_window(fb_config, analysis_window_name):\n    """""" Unit test perfect reconstruction """"""\n    kernel_size = fb_config[\'kernel_size\']\n    window = get_window(analysis_window_name, kernel_size)\n\n    enc = Encoder(STFTFB(**fb_config, window=window))\n    # Compute window for perfect resynthesis\n    synthesis_window = perfect_synthesis_window(enc.filterbank.window,\n                                                enc.stride)\n    dec = Decoder(STFTFB(**fb_config, window=synthesis_window))\n    inp_wav = torch.ones(1, 1, 32000)\n    out_wav = dec(enc(inp_wav))[:, :, kernel_size: -kernel_size]\n    inp_test = inp_wav[:, :, kernel_size: -kernel_size]\n    testing.assert_allclose(inp_test,\n                            out_wav)\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\n@pytest.mark.parametrize(""feed_istft"", [True, False])\n@pytest.mark.parametrize(""feed_angle"", [True, False])\ndef test_griffinlim(fb_config, feed_istft, feed_angle):\n    stft = Encoder(STFTFB(**fb_config))\n    istft = None if not feed_istft else Decoder(STFTFB(**fb_config))\n    wav = torch.randn(2, 1, 8000)\n    spec = stft(wav)\n    tf_mask = torch.sigmoid(torch.randn_like(spec))\n    masked_spec = spec * tf_mask\n    mag = transforms.take_mag(masked_spec, -2)\n    angles = None if not feed_angle else transforms.angle(masked_spec, -2)\n    griffin_lim(mag, stft, angles=angles, istft_dec=istft, n_iter=3)\n\n\n@pytest.mark.parametrize(""fb_config"", fb_config_list())\n@pytest.mark.parametrize(""feed_istft"", [True, False])\n@pytest.mark.parametrize(""feed_angle"", [True, False])\ndef test_misi(fb_config, feed_istft, feed_angle):\n    stft = Encoder(STFTFB(**fb_config))\n    istft = None if not feed_istft else Decoder(STFTFB(**fb_config))\n    n_src = 3\n    # Create mixture\n    wav = torch.randn(2, 1, 8000)\n    spec = stft(wav).unsqueeze(1)\n    # Create n_src masks on mixture spec and apply them\n    shape = list(spec.shape)\n    shape[1] *= n_src\n    tf_mask = torch.sigmoid(torch.randn(*shape))\n    masked_specs = spec * tf_mask\n    # Separate mag and angle.\n    mag = transforms.take_mag(masked_specs, -2)\n    angles = None if not feed_angle else transforms.angle(masked_specs, -2)\n    est_wavs = misi(wav, mag, stft, angles=angles, istft_dec=istft, n_iter=2)\n    # We actually don\'t know the last dim because ISTFT(STFT()) cuts the end\n    assert est_wavs.shape[:-1] == (2, n_src)\n'"
tests/filterbanks/transforms_test.py,23,"b'import random\nimport pytest\nimport torch\nfrom torch.testing import assert_allclose\nimport numpy as np\n\nfrom asteroid import filterbanks as fb\nfrom asteroid.filterbanks import transforms\n\n\nCOMPLEX_FBS = [\n    fb.STFTFB,\n    fb.ParamSincFB,\n    fb.AnalyticFreeFB\n]\n\n\n@pytest.fixture(scope=""module"")\ndef fb_config_list():\n    keys = [\'n_filters\', \'kernel_size\', \'stride\']\n    param_list = [\n        [256, 256, 128],\n        [256, 256, 64],\n        [512, 32, None],\n        [512, 16, 8],\n    ]\n    return [dict(zip(keys, values)) for values in param_list]\n\n\n@pytest.fixture(scope=""module"")\ndef encoder_list(fb_config_list):\n    enc_list = []\n    for fb_class in COMPLEX_FBS:\n        for fb_config in fb_config_list:\n            enc_list.append(make_encoder_from(fb_class, fb_config))\n    return enc_list\n\n\ndef make_encoder_from(fb_class, config):\n    enc = fb.Encoder(fb_class(**config))\n    fb_dim = enc.filterbank.n_feats_out\n    return enc, fb_dim\n\n\ndef test_mag_mask(encoder_list):\n    """""" Assert identity mask works. """"""\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 8000))  # [batch, freq, time]\n        id_mag_mask = torch.ones((1, fb_dim//2, 1))\n        masked = transforms.apply_mag_mask(tf_rep, id_mag_mask, dim=1)\n        assert_allclose(masked, tf_rep)\n\n\ndef test_reim_mask(encoder_list):\n    """""" Assert identity mask works. """"""\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 8000))  # [batch, freq, time]\n        id_reim_mask = torch.ones((1, fb_dim, 1))\n        masked = transforms.apply_real_mask(tf_rep, id_reim_mask, dim=1)\n        assert_allclose(masked, tf_rep)\n\n\ndef test_comp_mask(encoder_list):\n    """""" Assert identity mask works. """"""\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 8000))  # [batch, freq, time]\n        id_complex_mask = torch.cat((torch.ones((1, fb_dim // 2, 1)),\n                                     torch.zeros((1, fb_dim // 2, 1))),\n                                    dim=1)\n        masked = transforms.apply_complex_mask(tf_rep, id_complex_mask,\n                                               dim=1)\n        assert_allclose(masked, tf_rep)\n\n\ndef test_reim(encoder_list):\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 16000))  # [batch, freq, time]\n        assert_allclose(tf_rep, transforms.take_reim(tf_rep))\n\n\ndef test_mag(encoder_list):\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 16000))  # [batch, freq, time]\n        batch, freq, time = tf_rep.shape\n        mag = transforms.take_mag(tf_rep, dim=1)\n        assert mag.shape == (batch, freq // 2, time)\n\n\ndef test_cat(encoder_list):\n    for (enc, fb_dim) in encoder_list:\n        tf_rep = enc(torch.randn(2, 1, 16000))  # [batch, freq, time]\n        batch, freq, time = tf_rep.shape\n        mag = transforms.take_cat(tf_rep, dim=1)\n        assert mag.shape == (batch, 3 * (freq // 2), time)\n\n\n@pytest.mark.parametrize(""np_torch_tuple"", [\n    ([0], [0, 0]),\n    ([1j], [0, 1]),\n    ([-1], [-1, 0]),\n    ([-1j], [0, -1]),\n    ([1 + 1j], [1, 1]),\n])\n@pytest.mark.parametrize(""dim"", [0, 1, 2])\ndef test_to_numpy(np_torch_tuple, dim):\n    """""" Test torch --> np conversion (right angles)""""""\n    from_np, from_torch = np_torch_tuple\n    if dim == 0:\n        np_array = np.array(from_np)\n        torch_tensor = torch.tensor(from_torch)\n    elif dim == 1:\n        np_array = np.array([from_np])\n        torch_tensor = torch.tensor([from_torch])\n    elif dim == 2:\n        np_array = np.array([[from_np]])\n        torch_tensor = torch.tensor([[from_torch]])\n    else:\n        return\n    np_from_torch = transforms.to_numpy(torch_tensor, dim=dim)\n    np.testing.assert_allclose(np_array, np_from_torch)\n\n\n@pytest.mark.parametrize(""np_torch_tuple"", [\n    ([0], [0, 0]),\n    ([1j], [0, 1]),\n    ([-1], [-1, 0]),\n    ([-1j], [0, -1]),\n    ([1 + 1j], [1, 1]),\n])\n@pytest.mark.parametrize(""dim"", [0, 1, 2])\ndef test_from_numpy(np_torch_tuple, dim):\n    """""" Test np --> torch conversion (right angles)""""""\n    from_np, from_torch = np_torch_tuple\n    if dim == 0:\n        np_array = np.array(from_np)\n        torch_tensor = torch.tensor(from_torch)\n    elif dim == 1:\n        np_array = np.array([from_np])\n        torch_tensor = torch.tensor([from_torch])\n    elif dim == 2:\n        np_array = np.array([[from_np]])\n        torch_tensor = torch.tensor([[from_torch]])\n    else:\n        return\n    torch_from_np = transforms.from_numpy(np_array, dim=dim)\n    np.testing.assert_allclose(torch_tensor, torch_from_np)\n\n\n@pytest.mark.parametrize(""dim"", [0, 1, 2, 3])\ndef test_return_ticket_np_torch(dim):\n    """""" Test torch --> np --> torch --> np conversion""""""\n    max_tested_ndim = 4\n    # Random tensor shape\n    tensor_shape = [random.randint(1, 10) for _ in range(max_tested_ndim)]\n    # Make sure complex dimension has even shape\n    tensor_shape[dim] = 2 * tensor_shape[dim]\n    complex_tensor = torch.randn(tensor_shape)\n    np_array = transforms.to_numpy(complex_tensor, dim=dim)\n    tensor_back = transforms.from_numpy(np_array, dim=dim)\n    np_back = transforms.to_numpy(tensor_back, dim=dim)\n    # Check torch --> np --> torch\n    assert_allclose(complex_tensor, tensor_back)\n    # Check np --> torch --> np\n    np.testing.assert_allclose(np_array, np_back)\n\n\n@pytest.mark.parametrize(""dim"", [0, 1, 2, 3])\ndef test_angle_mag_recompostion(dim):\n    """""" Test complex --> (mag, angle) --> complex conversions""""""\n    max_tested_ndim = 4\n    # Random tensor shape\n    tensor_shape = [random.randint(1, 10) for _ in range(max_tested_ndim)]\n    # Make sure complex dimension has even shape\n    tensor_shape[dim] = 2 * tensor_shape[dim]\n    complex_tensor = torch.randn(tensor_shape)\n    phase = transforms.angle(complex_tensor, dim=dim)\n    mag = transforms.take_mag(complex_tensor, dim=dim)\n    tensor_back = transforms.from_mag_and_phase(mag, phase, dim=dim)\n    assert_allclose(complex_tensor, tensor_back)\n\n\n@pytest.mark.parametrize(""dim"", [0, 1, 2, 3])\ndef test_check_complex_error(dim):\n    """""" Test error in angle """"""\n    not_complex = torch.randn(3, 5, 7, 9, 15)\n    with pytest.raises(AssertionError):\n        phase = transforms.check_complex(not_complex, dim=dim)\n\n\n@pytest.mark.parametrize(""dim"", [0, 1, 2, 3])\ndef test_torchaudio_format(dim):\n    max_tested_ndim = 4\n    # Random tensor shape\n    tensor_shape = [random.randint(1, 10) for _ in range(max_tested_ndim)]\n    # Make sure complex dimension has even shape\n    tensor_shape[dim] = 2 * tensor_shape[dim]\n    complex_tensor = torch.randn(tensor_shape)\n    ta_tensor = transforms.to_torchaudio(complex_tensor, dim=dim)\n    tensor_back = transforms.from_torchaudio(ta_tensor, dim=dim)\n    assert_allclose(complex_tensor, tensor_back)\n    assert ta_tensor.shape[-1] == 2\n\n\ndef test_ebased_vad():\n    mag_spec = torch.abs(torch.randn(10, 2, 65, 16))  # Need positive inputs\n    batch_src_mask = transforms.ebased_vad(mag_spec)\n\n    assert isinstance(batch_src_mask, torch.BoolTensor)\n    batch_1_mask = transforms.ebased_vad(mag_spec[:, 0])\n    # Assert independence of VAD output\n    assert (batch_src_mask[:, 0] == batch_1_mask).all()\n'"
tests/losses/loss_functions_test.py,12,"b'import pytest\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom asteroid.filterbanks import STFTFB, Encoder, transforms\nfrom asteroid.losses import PITLossWrapper\nfrom asteroid.losses import sdr\nfrom asteroid.losses import singlesrc_mse, pairwise_mse, multisrc_mse\nfrom asteroid.losses import deep_clustering_loss, SingleSrcPMSQE\nfrom asteroid.losses import SingleSrcNegSTOI\nfrom asteroid.losses.multi_scale_spectral import SingleSrcMultiScaleSpectral\n\n\n@pytest.mark.parametrize(""n_src"", [2, 3, 4])\n@pytest.mark.parametrize(""function_triplet"", [\n    [sdr.pairwise_neg_sisdr, sdr.singlesrc_neg_sisdr, sdr.multisrc_neg_sisdr],\n    [sdr.pairwise_neg_sdsdr, sdr.singlesrc_neg_sdsdr, sdr.multisrc_neg_sdsdr],\n    [sdr.pairwise_neg_snr, sdr.singlesrc_neg_snr, sdr.multisrc_neg_snr],\n    [pairwise_mse, singlesrc_mse, multisrc_mse],\n])\ndef test_sisdr(n_src, function_triplet):\n    # Unpack the triplet\n    pairwise, nosrc, nonpit = function_triplet\n    # Fake targets and estimates\n    targets = torch.randn(2, n_src, 10000)\n    est_targets = torch.randn(2, n_src, 10000)\n    # Create the 3 PIT wrappers\n    pw_wrapper = PITLossWrapper(pairwise, pit_from=\'pw_mtx\')\n    wo_src_wrapper = PITLossWrapper(nosrc, pit_from=\'pw_pt\')\n    w_src_wrapper = PITLossWrapper(nonpit, pit_from=\'perm_avg\')\n\n    # Circular tests on value\n    assert_allclose(pw_wrapper(est_targets, targets),\n                    wo_src_wrapper(est_targets, targets))\n    assert_allclose(wo_src_wrapper(est_targets, targets),\n                    w_src_wrapper(est_targets, targets))\n\n    # Circular tests on returned estimates\n    assert_allclose(pw_wrapper(est_targets, targets, return_est=True)[1],\n                    wo_src_wrapper(est_targets, targets, return_est=True)[1])\n    assert_allclose(wo_src_wrapper(est_targets, targets, return_est=True)[1],\n                    w_src_wrapper(est_targets, targets, return_est=True)[1])\n\n\n@pytest.mark.parametrize(""spk_cnt"", [2, 3, 4])\ndef test_dc(spk_cnt):\n    embedding = torch.randn(10, 5*400, 20)\n    targets = torch.zeros(10, 400, 5).random_(0, spk_cnt).long()\n    loss = deep_clustering_loss(embedding, targets)\n    assert loss.shape[0] == 10\n\n\n@pytest.mark.parametrize(""n_src"", [2, 3])\ndef test_multi_scale_spectral_PIT(n_src):\n    # Test in with reduced number of STFT scales.\n    filt_list = [512, 256, 32]\n    # Fake targets and estimates\n    targets = torch.randn(2, n_src, 8000)\n    est_targets = torch.randn(2, n_src, 8000)\n    # Create PITLossWrapper in \'pw_pt\' mode\n    pt_loss = SingleSrcMultiScaleSpectral(windows_size=filt_list,\n                                          n_filters=filt_list,\n                                          hops_size=filt_list)\n    loss_func = PITLossWrapper(pt_loss, pit_from=\'pw_pt\')\n    # Compute the loss\n    loss = loss_func(targets, est_targets)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2])\ndef test_multi_scale_spectral_shape(batch_size):\n    # Test in with reduced number of STFT scales.\n    filt_list = [512, 256, 32]\n    # Fake targets and estimates\n    targets = torch.randn(batch_size, 8000)\n    est_targets = torch.randn(batch_size, 8000)\n    # Create PITLossWrapper in \'pw_pt\' mode\n    loss_func = SingleSrcMultiScaleSpectral(windows_size=filt_list,\n                                            n_filters=filt_list,\n                                            hops_size=filt_list)\n    # Compute the loss\n    loss = loss_func(targets, est_targets)\n    assert loss.shape[0] == batch_size\n\n\n@pytest.mark.parametrize(""sample_rate"", [8000, 16000])\ndef test_pmsqe(sample_rate):\n    # Define supported STFT\n    if sample_rate == 16000:\n        stft = Encoder(STFTFB(kernel_size=512, n_filters=512, stride=256))\n    else:\n        stft = Encoder(STFTFB(kernel_size=256, n_filters=256, stride=128))\n     # Usage by itself\n    ref, est = torch.randn(2, 1, 16000), torch.randn(2, 1, 16000)\n    ref_spec = transforms.take_mag(stft(ref))\n    est_spec = transforms.take_mag(stft(est))\n    loss_func = SingleSrcPMSQE(sample_rate=sample_rate)\n    loss_value = loss_func(est_spec, ref_spec)\n    # Assert output has shape (batch,)\n    assert loss_value.shape[0] == ref.shape[0]\n    # Assert support for transposed inputs.\n    tr_loss_value = loss_func(est_spec.transpose(1, 2),\n                              ref_spec.transpose(1, 2))\n    assert_allclose(loss_value, tr_loss_value)\n\n\n@pytest.mark.parametrize(""n_src"", [2, 3])\n@pytest.mark.parametrize(""sample_rate"", [8000, 16000])\ndef test_pmsqe_pit(n_src, sample_rate):\n    # Define supported STFT\n    if sample_rate == 16000:\n        stft = Encoder(STFTFB(kernel_size=512, n_filters=512, stride=256))\n    else:\n        stft = Encoder(STFTFB(kernel_size=256, n_filters=256, stride=128))\n     # Usage by itself\n    ref, est = torch.randn(2, n_src, 16000), torch.randn(2, n_src, 16000)\n    ref_spec = transforms.take_mag(stft(ref))\n    est_spec = transforms.take_mag(stft(est))\n    loss_func = PITLossWrapper(SingleSrcPMSQE(sample_rate=sample_rate),\n                               pit_from=\'pw_pt\')\n    # Assert forward ok.\n    loss_value = loss_func(est_spec, ref_spec)\n\n\n@pytest.mark.parametrize(""n_src"", [2, 3])\n@pytest.mark.parametrize(""sample_rate"", [8000, 16000])\n@pytest.mark.parametrize(""use_vad"", [True, False])\n@pytest.mark.parametrize(""extended"", [True, False])\ndef test_negstoi_pit(n_src, sample_rate, use_vad, extended):\n    ref, est = torch.randn(2, n_src, 16000), torch.randn(2, n_src, 16000)\n    singlesrc_negstoi = SingleSrcNegSTOI(sample_rate=sample_rate,\n                                         use_vad=use_vad,\n                                         extended=extended)\n    loss_func = PITLossWrapper(singlesrc_negstoi, pit_from=\'pw_pt\')\n    # Assert forward ok.\n    loss_value = loss_func(est, ref)\n'"
tests/losses/pit_wrapper_test.py,20,"b'import pytest\nimport itertools\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom asteroid.losses import PITLossWrapper, pairwise_mse\n\n\ndef bad_loss_func_ndim0(y_pred, y_true):\n    return torch.randn(1).mean()\n\n\ndef bad_loss_func_ndim1(y_pred, y_true):\n    return torch.randn(1)\n\n\ndef good_batch_loss_func(y_pred, y_true):\n    batch, *_ = y_true.shape\n    return torch.randn(batch)\n\n\ndef good_pairwise_loss_func(y_pred, y_true):\n    batch, n_src, *_ = y_true.shape\n    return torch.randn(batch, n_src, n_src)\n\n\n@pytest.mark.parametrize(""batch_size"", [1, 2, 8])\n@pytest.mark.parametrize(""n_src"", [2, 3, 4])\n@pytest.mark.parametrize(""time"", [16000, 1221])\ndef test_wrapper(batch_size, n_src, time):\n    targets = torch.randn(batch_size, n_src, time)\n    est_targets = torch.randn(batch_size, n_src, time)\n    for bad_loss_func in [bad_loss_func_ndim0, bad_loss_func_ndim1]:\n        loss = PITLossWrapper(bad_loss_func)\n        with pytest.raises(AssertionError):\n            loss(est_targets, targets)\n    # wo_src loss function / With and without return estimates\n    loss = PITLossWrapper(good_batch_loss_func, pit_from=\'pw_pt\')\n    loss_value_no_return = loss(est_targets, targets)\n    loss_value, reordered_est = loss(est_targets, targets, return_est=True)\n    assert reordered_est.shape == est_targets.shape\n\n    # pairwise loss function / With and without return estimates\n    loss = PITLossWrapper(good_pairwise_loss_func, pit_from=\'pw_mtx\')\n    loss_value_no_return = loss(est_targets, targets)\n    loss_value, reordered_est = loss(est_targets, targets, return_est=True)\n    assert reordered_est.shape == est_targets.shape\n\n    # w_src loss function / With and without return estimates\n    loss = PITLossWrapper(good_batch_loss_func, pit_from=\'perm_avg\')\n    loss_value_no_return = loss(est_targets, targets)\n    loss_value, reordered_est = loss(est_targets, targets, return_est=True)\n    assert reordered_est.shape == est_targets.shape\n\n\n@pytest.mark.parametrize(""perm"", list(itertools.permutations([0, 1, 2])) +\n                                 list(itertools.permutations([0, 1, 2, 3])) +\n                                 list(itertools.permutations([0, 1, 2, 3, 4])))\ndef test_permutation(perm):\n    """""" Construct fake target/estimates pair. Check the value and reordering.""""""\n    n_src = len(perm)\n    perm_tensor = torch.Tensor(perm)\n    source_base = torch.ones(1, n_src, 10)\n    sources = torch.arange(n_src).unsqueeze(-1) * source_base\n    est_sources = perm_tensor.unsqueeze(-1) * source_base\n\n    loss_func = PITLossWrapper(pairwise_mse)\n    loss_value, reordered = loss_func(est_sources, sources, return_est=True)\n\n    assert loss_value.item() == 0\n    assert_allclose(sources, reordered)\n\n\ndef test_permreduce():\n    from functools import partial\n    n_src = 3\n    sources = torch.randn(10, n_src, 8000)\n    est_sources = torch.randn(10, n_src, 8000)\n    wo_reduce = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\')\n    w_mean_reduce = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\',\n                                   # perm_reduce=partial(torch.mean, dim=-1))\n                                   perm_reduce=lambda x: torch.mean(x, dim=-1))\n    w_sum_reduce = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\',\n                                  perm_reduce=partial(torch.sum, dim=-1))\n\n    wo = wo_reduce(est_sources, sources)\n    w_mean = w_mean_reduce(est_sources, sources)\n    w_sum = w_sum_reduce(est_sources, sources)\n\n    assert_allclose(wo, w_mean)\n    assert_allclose(wo, w_sum/n_src)\n\n\ndef test_permreduce_args():\n    def reduce_func(perm_losses, class_weights=None):\n        # perm_losses is (batch , n_perms, n_src) for now\n        if class_weights is None:\n            return torch.mean(perm_losses, dim=-1)\n        if class_weights.ndim == 2:\n            class_weights = class_weights.unsqueeze(1)\n        return torch.mean(perm_losses * class_weights, -1)\n\n    n_src = 3\n    sources = torch.randn(10, n_src, 8000)\n    est_sources = torch.randn(10, n_src, 8000)\n    loss_func = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\',\n                               perm_reduce=reduce_func)\n    weights = torch.softmax(torch.randn(10, n_src), dim=-1)\n    loss = loss_func(est_sources, sources,\n                     reduce_kwargs={\'class_weights\': weights})\n'"
tests/masknn/activations_test.py,3,"b'# Call activations and get the same output from torch?\n# list of strings / function pair in parametrize?\nimport pytest\nimport torch\nfrom torch.testing import assert_allclose\n\nfrom asteroid.masknn import activations\nfrom torch import nn\n\n\ndef activation_mapping():\n    mapping_list = [\n        (nn.Identity, ""linear""),\n        (nn.ReLU, ""relu""),\n        (nn.PReLU, ""prelu""),\n        (nn.LeakyReLU, ""leaky_relu""),\n        (nn.Sigmoid, ""sigmoid""),\n        (nn.Tanh, ""tanh""),\n    ]\n    return mapping_list\n\n\n@pytest.mark.parametrize(""activation_tuple"", activation_mapping())\ndef test_activations(activation_tuple):\n    torch_act, asteroid_act = activation_tuple\n    torch_act = torch_act()\n    asteroid_act = activations.get(asteroid_act)()\n\n    inp = torch.randn(10, 11, 12)\n    assert_allclose(torch_act(inp), asteroid_act(inp))\n\n\ndef test_softmax():\n    torch_softmax = nn.Softmax(dim=-1)\n    asteroid_softmax = activations.get(""softmax"")(dim=-1)\n    inp = torch.randn(10, 11, 12)\n    assert_allclose(torch_softmax(inp), asteroid_softmax(inp))\n    assert torch_softmax == activations.get(torch_softmax)\n\n\n@pytest.mark.parametrize(""wrong"", [""wrong_string"", 12, object()])\ndef test_get_errors(wrong):\n    with pytest.raises(ValueError):\n        # Should raise for anything not a Optimizer instance + unknown string\n        activations.get(wrong)\n\n\ndef test_get_none():\n    assert activations.get(None) is None\n'"
tests/masknn/consistency_test.py,8,"b'import torch\nfrom torch.testing import assert_allclose\nimport pytest\n\nfrom asteroid.masknn.consistency import mixture_consistency\n\n\n@pytest.mark.parametrize(""mix_shape"", [[2, 1600], [2, 130, 10]])\n@pytest.mark.parametrize(""dim"", [1, 2])\n@pytest.mark.parametrize(""n_src"", [1, 2, 3])\ndef test_consistency_noweight(mix_shape, dim, n_src):\n    mix = torch.randn(mix_shape)\n    est_shape = mix_shape[:dim] + [n_src] + mix_shape[dim:]\n    est_sources = torch.randn(est_shape)\n    consistent_est_sources = mixture_consistency(mix, est_sources, dim=dim)\n    assert_allclose(mix, consistent_est_sources.sum(dim))\n\n\n@pytest.mark.parametrize(""mix_shape"", [[2, 1600], [2, 130, 10]])\n@pytest.mark.parametrize(""dim"", [1, 2])\n@pytest.mark.parametrize(""n_src"", [1, 2, 3])\ndef test_consistency_withweight(mix_shape, dim, n_src):\n    mix = torch.randn(mix_shape)\n    est_shape = mix_shape[:dim] + [n_src] + mix_shape[dim:]\n    est_sources = torch.randn(est_shape)\n    # Create source weights : should have the same number of dims as\n    # est_sources with ones out of batch and n_src dims.\n    ones = [1 for _ in range(len(mix_shape) - 1)]\n    src_weights_shape = mix_shape[:1] + ones[:dim-1] + [n_src] + ones[dim-1:]\n    src_weights = torch.softmax(torch.randn(src_weights_shape), dim=dim)\n    # Apply mixture consitency\n    consistent_est_sources = mixture_consistency(mix, est_sources,\n                                                 src_weights=src_weights,\n                                                 dim=dim)\n    assert_allclose(mix, consistent_est_sources.sum(dim))\n\n\ndef test_consistency_raise():\n    mix = torch.randn(10, 1, 1, 160)\n    est = torch.randn(10, 2, 160)\n    with pytest.raises(RuntimeError):\n        mixture_consistency(mix, est, dim=1)\n'"
tests/masknn/convolutional_test.py,1,"b'import pytest\nimport torch\nfrom asteroid.masknn import TDConvNet\n\n\n@pytest.mark.parametrize(""mask_act"", [""relu"", ""softmax""])\n@pytest.mark.parametrize(""out_chan"", [None, 10])\n@pytest.mark.parametrize(""skip_chan"", [0, 12])\ndef test_tdconvnet(mask_act, out_chan, skip_chan):\n    in_chan, n_src = 20, 2\n    model = TDConvNet(in_chan=in_chan, n_src=n_src, mask_act=mask_act,\n                      n_blocks=2, n_repeats=2, bn_chan=10, hid_chan=11,\n                      skip_chan=skip_chan, out_chan=out_chan)\n    batch, n_frames = 2, 24\n    inp = torch.randn(batch, in_chan, n_frames)\n    out = model(inp)\n    _ = model.get_config()\n    out_chan = out_chan if out_chan else in_chan\n    assert out.shape == (batch, n_src, out_chan, n_frames)\n'"
tests/masknn/norms_test.py,1,"b'import pytest\nimport torch\n\nfrom asteroid.masknn import norms\n\n\n@pytest.mark.parametrize(""norm_str"", [""gLN"", ""cLN"", ""cgLN"", ""bN""])\n@pytest.mark.parametrize(""channel_size"", [8, 128, 4])\ndef test_norms(norm_str, channel_size):\n    norm_layer = norms.get(norm_str)\n    # Use get on the class\n    out_from_get = norms.get(norm_layer)\n    assert out_from_get == norm_layer\n    # Use get on the instance\n    norm_layer = norm_layer(channel_size)\n    out_from_get = norms.get(norm_layer)\n    assert out_from_get == norm_layer\n\n    # Test forward\n    inp = torch.randn(4, channel_size, 12)\n    _ = norm_layer(inp)\n\n\n@pytest.mark.parametrize(""wrong"", [""wrong_string"", 12, object()])\ndef test_get_errors(wrong):\n    with pytest.raises(ValueError):\n        # Should raise for anything not a Optimizer instance + unknown string\n        norms.get(wrong)\n\n\ndef test_get_none():\n    assert norms.get(None) is None\n'"
tests/masknn/recurrent_test.py,3,"b'import pytest\nimport torch\nfrom asteroid.masknn import recurrent as rec\n\n\n@pytest.mark.parametrize(""mask_act"", [""relu"", ""softmax""])\n@pytest.mark.parametrize(""out_chan"", [None, 10])\n@pytest.mark.parametrize(""hop_size"", [None, 5])\ndef test_dprnn(mask_act, out_chan, hop_size):\n    in_chan, n_src = 20, 2\n    model = rec.DPRNN(in_chan=in_chan, n_src=n_src, mask_act=mask_act,\n                      chunk_size=20, n_repeats=2, bn_chan=10, hid_size=11,\n                      out_chan=out_chan, hop_size=hop_size)\n    batch, n_frames = 2, 78\n    inp = torch.randn(batch, in_chan, n_frames)\n    out = model(inp)\n    _ = model.get_config()\n    out_chan = out_chan if out_chan else in_chan\n    assert out.shape == (batch, n_src, out_chan, n_frames)\n\n\n@pytest.mark.parametrize(""rnn_type"", [""LSTM"", ""GRU"", ""RNN""])\n@pytest.mark.parametrize(""dropout"", [0., 0.2])\ndef test_res_rnn(rnn_type, dropout):\n    n_units, n_layers = 20, 3\n    model = rec.StackedResidualRNN(rnn_type, n_units, n_layers=n_layers,\n                                   dropout=dropout, bidirectional=False)\n    batch, n_frames = 2, 78\n    inp = torch.randn(batch, n_frames, n_units)\n    out = model(inp)\n    assert out.shape == (batch, n_frames, n_units)\n\n\n@pytest.mark.parametrize(""rnn_type"", [""LSTM"", ""GRU"", ""RNN""])\n@pytest.mark.parametrize(""dropout"", [0., 0.2])\ndef test_res_birnn(rnn_type, dropout):\n    n_units, n_layers = 20, 3\n    model = rec.StackedResidualBiRNN(rnn_type, n_units, n_layers=n_layers,\n                                     dropout=dropout, bidirectional=True)\n    batch, n_frames = 2, 78\n    inp = torch.randn(batch, n_frames, n_units)\n    out = model(inp)\n    assert out.shape == (batch, n_frames, 2 * n_units)\n'"
tests/models/models_test.py,7,"b""import torch\nimport pytest\nfrom torch.testing import assert_allclose\nimport numpy as np\nfrom asteroid.models import ConvTasNet, DPRNNTasNet\n\n\ndef test_convtasnet_sep():\n    nnet = ConvTasNet(n_src=2, n_repeats=2, n_blocks=3, bn_chan=16,\n                      hid_chan=4, skip_chan=8, n_filters=32)\n    # Test torch input\n    wav = torch.rand(1, 800)\n    out = nnet.separate(wav)\n    assert isinstance(out, torch.Tensor)\n    # Test numpy input\n    wav = np.random.randn(1, 800).astype('float32')\n    out = nnet.separate(wav)\n    assert isinstance(out, np.ndarray)\n\n\n@pytest.mark.parametrize('fb', ['free', 'stft', 'analytic_free', 'param_sinc'])\ndef test_save_and_load_convtasnet(fb):\n    model1 = ConvTasNet(n_src=2, n_repeats=2, n_blocks=2, bn_chan=16,\n                        hid_chan=4, skip_chan=8, n_filters=32, fb_name=fb)\n    test_input = torch.randn(1, 800)\n    model_conf = model1.serialize()\n\n    reconstructed_model = ConvTasNet.from_pretrained(model_conf)\n    assert_allclose(model1.separate(test_input),\n                    reconstructed_model(test_input))\n\n\ndef test_dprnntasnet_sep():\n    nnet = DPRNNTasNet(n_src=2, n_repeats=2, bn_chan=16, hid_size=4,\n                       chunk_size=20, n_filters=32)\n    # Test torch input\n    wav = torch.rand(1, 800)\n    out = nnet.separate(wav)\n    assert isinstance(out, torch.Tensor)\n    # Test numpy input\n    wav = np.random.randn(1, 800).astype('float32')\n    out = nnet.separate(wav)\n    assert isinstance(out, np.ndarray)\n\n\n@pytest.mark.parametrize('fb', ['free', 'stft', 'analytic_free', 'param_sinc'])\ndef test_save_and_load_dprnn(fb):\n    model1 = DPRNNTasNet(n_src=2, n_repeats=2, bn_chan=16, hid_size=4,\n                         chunk_size=20, n_filters=32, fb_name=fb)\n    test_input = torch.randn(1, 800)\n    model_conf = model1.serialize()\n\n    reconstructed_model = DPRNNTasNet.from_pretrained(model_conf)\n    assert_allclose(model1.separate(test_input),\n                    reconstructed_model(test_input))\n"""
tests/models/publish_test.py,0,"b'import os\nimport json\nimport shutil\n\nfrom asteroid.models import save_publishable, upload_publishable, ConvTasNet\nfrom asteroid.data import WhamDataset\n\n\ndef populate_wham_dir(path):\n    wham_files = [\'s1\', \'s2\', \'noise\', \'mix_single\', \'mix_clean\', \'mix_both\']\n    os.makedirs(path, exist_ok=True)\n    for source in wham_files:\n        json_file = os.path.join(path, source + \'.json\')\n        with open(os.path.join(json_file), \'w\') as f:\n            json.dump(dict(), f)\n\n\ndef test_upload():\n    # Make dirs\n    os.makedirs(\'tmp/publish_dir\', exist_ok=True)\n    populate_wham_dir(\'tmp/wham\')\n\n    # Dataset and NN\n    train_set = WhamDataset(\'tmp/wham\', task=\'sep_clean\')\n    model = ConvTasNet(n_src=2, n_repeats=2, n_blocks=2, bn_chan=16,\n                       hid_chan=4, skip_chan=8, n_filters=32)\n\n    # Save publishable\n    model_conf = model.serialize()\n    model_conf.update(train_set.get_infos())\n    save_publishable(\'tmp/publish_dir\', model_conf, metrics={}, train_conf={})\n\n    if False:\n        # Upload\n        zen, current = upload_publishable(\n            \'tmp/publish_dir\',\n            uploader=""Manuel Pariente"",\n            affiliation=""INRIA"",\n            use_sandbox=True,\n            unit_test=True,  # Remove this argument and monkeypatch `input()`\n        )\n\n        # Assert metadata is correct\n        meta = current.json()[\'metadata\']\n        assert meta[\'creators\'][0][\'name\'] == ""Manuel Pariente""\n        assert meta[\'creators\'][0][\'affiliation\'] == ""INRIA""\n        assert \'asteroid-models\' in [d[\'identifier\'] for d in meta[\'communities\']]\n\n        # Clean up\n        zen.remove_deposition(current.json()[\'id\'])\n    shutil.rmtree(\'tmp/wham\')\n\n\nif __name__ == \'__main__\':\n    test_upload()'"
tests/utils/hub_utils_test.py,0,"b""import os\nfrom asteroid.utils import hub_utils\n\n\ndef test_download():\n    # We download\n    path1 = hub_utils.cached_download('mpariente/ConvTasNet_WHAM!_sepclean')\n    assert os.path.isfile(path1)\n    # We use cache\n    path2 = hub_utils.cached_download('mpariente/ConvTasNet_WHAM!_sepclean')\n    assert path1 == path2\n"""
tests/utils/torch_utils_test.py,4,"b'import torch\nimport pytest\nfrom torch import nn\nfrom asteroid import torch_utils\n\n\ndef test_pad():\n    x = torch.randn(10, 1, 16000)\n    y = torch.randn(10, 1, 16234)\n    padded_x = torch_utils.pad_x_to_y(x, y)\n    assert padded_x.shape == y.shape\n\n\ndef test_pad_fail():\n    x = torch.randn(10, 16000, 1)\n    y = torch.randn(10, 16234, 1)\n    with pytest.raises(NotImplementedError):\n        padded_x = torch_utils.pad_x_to_y(x, y, axis=1)\n\n\ndef test_model_equal():\n    model = nn.Sequential(nn.Linear(10, 10))\n    assert torch_utils.are_models_equal(model, model)\n    model_2 = nn.Sequential(nn.Linear(10, 10))\n    assert not torch_utils.are_models_equal(model, model_2)\n\n\ndef test_loader_module():\n    model = nn.Sequential(nn.Linear(10, 10))\n    state_dict = model.state_dict()\n    model_2 = nn.Sequential(nn.Linear(10, 10))\n    model_2 = torch_utils.load_state_dict_in(state_dict, model_2)\n    assert torch_utils.are_models_equal(model, model_2)\n\n\ndef test_loader_submodule():\n    class SuperModule(nn.Module):\n        """""" nn.Module subclass that holds a model under self.whoever """"""\n        def __init__(self, sub_model):\n            super().__init__()\n            self.whoever = sub_model\n    model = SuperModule(nn.Sequential(nn.Linear(10, 10)))\n    # Keys in state_dict will be whoever.0.weight, whoever.0.bias\n    state_dict = model.state_dict()\n    # We want to load it in model_2 (has keys 0.weight, 0.bias)\n    model_2 = nn.Sequential(nn.Linear(10, 10))\n    # Keys are not the same, torch raises an error for that.\n    with pytest.raises(RuntimeError):\n        model_2.load_state_dict(state_dict)\n    # We can try loose model loading (assert it doesn\'t work)\n    model_2.load_state_dict(state_dict, strict=False)\n    assert not torch_utils.are_models_equal(model, model_2)\n    # Apply our workaround torch_utils.load_state_dict_in and assert True.\n    model_2 = torch_utils.load_state_dict_in(state_dict, model_2)\n    assert torch_utils.are_models_equal(model, model_2)\n'"
tests/utils/utils_test.py,9,"b'import argparse\nimport collections\nimport torch\nfrom torch.testing import assert_allclose\nimport pytest\nimport numpy as np\n\nfrom asteroid import utils\nfrom asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n\n@pytest.fixture(scope=""module"")\ndef parser():\n    # Create dictionary as from .yml file\n    def_conf = dict(\n        top1=dict(key1=2),\n        top2=dict(key2=None, key3=True)\n    )\n    # Create empty parser and add top level keys\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--main_key\', default=\'\')\n    # Populate parser from def_conf\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    return parser\n\n\ndef test_namespace_dic(parser):\n    fake_args = [\'--key2\', \'hey\', \'--key3\', \'0\']\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True,\n                                             args=fake_args)\n    assert arg_dic[\'main_args\'][\'main_key\'] == plain_args.main_key\n    assert arg_dic[\'top2\'][\'key3\'] == plain_args.key3\n\n\n@pytest.mark.parametrize(""inp"", [\'one_string\', 3, 3.14])\ndef test_none_default(parser, inp):\n    # If the default is None, convert the input string into an int, a float\n    # or string.\n    fake_args = [\'--key2\', str(inp)]  # Note : inp is converted to string\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True,\n                                             args=fake_args)\n    assert type(plain_args.key2) == type(inp)\n\n\ndef test_boolean(parser):\n    fake_args = [\'--key3\', \'y\']\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True,\n                                             args=fake_args)\n    assert plain_args.key3 is True\n\n    fake_args = [\'--key3\', \'n\']\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True,\n                                             args=fake_args)\n    assert plain_args.key3 is False\n\n\n@pytest.mark.parametrize(""tensors"", [\n    torch.randn(10, 10),  # tensor\n    dict(tensor_a=torch.randn(10, 10), tensor_b=torch.randn(12, 12)),  # dict\n    [torch.randn(10, 10), torch.randn(12, 12)],  # list\n    dict(\n        tensor_a=torch.randn(10, 10),\n        tensor_list=[torch.randn(12, 12), torch.randn(14, 14)],\n        tensor_dict=dict(u=torch.randn(8, 10), v=torch.randn(10, 8))\n    ),\n    [dict(u=torch.randn(8, 10), v=torch.randn(10, 8)), torch.randn(10, 10)]\n])\ndef test_transfer(tensors):\n    if isinstance(tensors, torch.Tensor):\n        assert_allclose(utils.tensors_to_device(tensors, \'cpu\'), tensors)\n    if isinstance(tensors, list):\n        assert list(utils.tensors_to_device(tensors, \'cpu\')) == list(tensors)\n    if isinstance(tensors, dict):\n        assert dict(utils.tensors_to_device(tensors, \'cpu\')) == dict(tensors)\n\n\ndef test_flatten_dict():\n    to_test = dict(\n        top1=[1, 2],\n        top2=dict(\n            sub1=\'hey\',\n            sub2=dict(\n                subsub1=True,\n                subsub2=[\'This\', \'is\', \'a\', \'list\']\n            ),\n            sub3=False\n        )\n    )\n    flat_dic = utils.flatten_dict(to_test)\n    for k, v in flat_dic.items():\n        assert not isinstance(v, collections.MutableMapping)\n\n\ndef test_average_array_in_dic():\n    d = dict(\n        a=\'hey\',\n        b=np.array([1., 3.]),\n        c=2\n    )\n    av_d = utils.average_arrays_in_dic(d)\n    d_should_be = dict(\n        a=\'hey\',\n        b=2.,\n        c=2\n    )\n    # We need the arrays to be averaged\n    assert av_d == d_should_be\n\n\n@pytest.mark.parametrize(""desired"", [50, 100])\ndef test_get_start_stop(desired):\n    sig = np.random.randn(100)\n    start, stop = utils.get_wav_random_start_stop(len(sig), desired_len=desired)\n'"
egs/avspeech/looking-to-listen/eval.py,0,"b'import sys\nimport yaml\nimport collections\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nimport torch\nimport numpy as np\nfrom catalyst.dl import utils\nfrom catalyst.dl.runner import SupervisedRunner\nfrom asteroid.data.avspeech_dataset import AVSpeechDataset\n\nfrom local.loader.constants import EMBED_DIR\nfrom model import make_model_and_optimizer, load_best_model\nfrom train import SNRCallback, SDRCallback, ParamConfig\n\n\ndef validate(model, val_dataset, config):\n    loaders = collections.OrderedDict()\n    val_loader = utils.get_loader(\n        val_dataset,\n        open_fn=lambda x: {""input_audio"": x[-1], ""input_video"": x[1], ""targets"": x[0]},\n        batch_size=config.batch_size,\n        num_workers=config.workers,\n        shuffle=False,\n    )\n\n    loaders[""valid""] = val_loader\n\n    runner = SupervisedRunner(\n        input_key=[""input_audio"", ""input_video""]\n    )  # parameters of the model in forward(...)\n    runner.infer(\n        model,\n        loaders,\n        callbacks=collections.OrderedDict(\n            {""snr_callback"": SNRCallback(), ""sdr_callback"": SDRCallback()}\n        ),\n        verbose=True,\n    )\n\n\ndef main(conf):\n    config = ParamConfig(\n        conf[""training""][""batch_size""],\n        conf[""training""][""epochs""],\n        conf[""training""][""num_workers""],\n        cuda=True,\n        use_half=False,\n        learning_rate=conf[""optim""][""lr""],\n    )\n\n    val_dataset = AVSpeechDataset(\n        Path(""data/val.csv""), Path(EMBED_DIR), conf[""main_args""][""n_src""]\n    )\n\n    model = load_best_model(conf, conf[""main_args""][""exp_dir""])\n\n    print(\n        f""AVFusion has {sum(np.prod(i.shape) for i in model.parameters()):,} parameters""\n    )\n\n    validate(model, val_dataset, config)\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser()\n    parser.add_argument(""--gpus"", type=str, help=""list of GPUs"", default=""-1"")\n    parser.add_argument(\n        ""--n-src"",\n        type=int,\n        help=""number of inputs to neural network"",\n        default=2,\n    )\n    parser.add_argument(\n        ""--exp_dir"",\n        default=""exp/logdir"",\n        help=""Full path to save best validation model"",\n    )\n\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(""local/conf.yml"") as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/avspeech/looking-to-listen/model.py,43,"b'import torch\nimport numpy as np\nfrom torch import nn\nimport torchvision\nimport torch.nn.functional as F\nfrom pathlib import Path\nfrom asteroid import torch_utils\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.filterbanks import transforms\n\n\n# Reference: https://github.com/bill9800/speech_separation/blob/master/model/lib/utils.py\ndef generate_cRM(Y, S):\n    """"""Generate CRM.\n\n    Args:\n        Y (torch.Tensor): mixed/noisy stft.\n        S (torch.Tensor): clean stft.\n\n    Returns:\n        M (torch.Tensor): structed cRM.\n\n    """"""\n    M = torch.zeros(Y.shape)\n    epsilon = 1e-8\n    # real part\n    M_real = (Y[..., 0] * S[..., 0]) + (Y[..., 1] * S[..., 1])\n    square_real = (Y[..., 0] ** 2) + (Y[..., 1] ** 2)\n    M_real = M_real / (square_real + epsilon)\n    M[..., 0] = M_real\n    # imaginary part\n    M_img = (Y[..., 0] * S[..., 1]) - (Y[..., 1] * S[..., 0])\n    square_img = (Y[..., 0] ** 2) + (Y[..., 1] ** 2)\n    M_img = M_img / (square_img + epsilon)\n    M[..., 1] = M_img\n    return M\n\n\ndef cRM_tanh_compress(M, K=10, C=0.1):\n    """"""CRM tanh compress.\n\n    Args:\n        M (torch.Tensor): crm.\n        K (torch.Tensor): parameter to control the compression.\n        C (torch.Tensor): parameter to control the compression.\n\n    Returns:\n        crm (torch.Tensor): compressed crm.\n\n    """"""\n\n    numerator = 1 - torch.exp(-C * M)\n    numerator[numerator == inf] = 1\n    numerator[numerator == -inf] = -1\n    denominator = 1 + torch.exp(-C * M)\n    denominator[denominator == inf] = 1\n    denominator[denominator == -inf] = -1\n    crm = K * (numerator / denominator)\n\n    return crm\n\n\ndef cRM_tanh_recover(O, K=10, C=0.1):\n    """"""CRM tanh recover.\n\n    Args:\n        O (torch.Tensor): predicted compressed crm.\n        K (torch.Tensor): parameter to control the compression.\n        C (torch.Tensor): parameter to control the compression.\n\n    Returns:\n        M (torch.Tensor): uncompressed crm.\n\n    """"""\n\n    numerator = K - O\n    denominator = K + O\n    M = -((1.0 / C) * torch.log((numerator / denominator)))\n\n    return M\n\n\ndef fast_cRM(Fclean, Fmix, K=10, C=0.1):\n    """"""Fast CRM.\n\n    Args:\n        Fmix (torch.Tensor): mixed/noisy stft.\n        Fclean (torch.Tensor): clean stft.\n        K (torch.Tensor): parameter to control the compression.\n        C (torch.Tensor): parameter to control the compression.\n\n    Returns:\n        crm (torch.Tensor): compressed crm.\n\n    """"""\n    M = generate_cRM(Fmix, Fclean)\n    crm = cRM_tanh_compress(M, K, C)\n    return crm\n\n\ndef fast_icRM(Y, crm, K=10, C=0.1):\n    """"""fast iCRM.\n\n    Args:\n        Y (torch.Tensor): mixed/noised stft.\n        crm (torch.Tensor): DNN output of compressed crm.\n        K (torch.Tensor): parameter to control the compression.\n        C (torch.Tensor): parameter to control the compression.\n\n    Returns:\n        S (torch.Tensor): clean stft.\n\n    """"""\n    M = cRM_tanh_recover(crm, K, C)\n    S = torch.zeros(M.shape)\n    S[:, 0, ...] = (M[:, 0, ...] * Y[:, 0, ...]) - (M[:, 1, ...] * Y[:, 1, ...])\n    S[:, 1, ...] = (M[:, 0, ...] * Y[:, 1, ...]) + (M[:, 1, ...] * Y[:, 0, ...])\n    return S\n\n\nclass Audio_Model(nn.Module):\n    def __init__(self, last_shape=8):\n        super(Audio_Model, self).__init__()\n\n        # Audio model layers , name of layers as per table 1 given in paper.\n\n        self.conv1 = nn.Conv2d(\n            2,\n            96,\n            kernel_size=(1, 7),\n            padding=self.get_padding((1, 7), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv2 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(7, 1),\n            padding=self.get_padding((7, 1), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv3 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv4 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (2, 1)),\n            dilation=(2, 1),\n        )\n\n        self.conv5 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (4, 1)),\n            dilation=(4, 1),\n        )\n\n        self.conv6 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (8, 1)),\n            dilation=(8, 1),\n        )\n\n        self.conv7 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (16, 1)),\n            dilation=(16, 1),\n        )\n\n        self.conv8 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (32, 1)),\n            dilation=(32, 1),\n        )\n\n        self.conv9 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv10 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (2, 2)),\n            dilation=(2, 2),\n        )\n\n        self.conv11 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (4, 4)),\n            dilation=(4, 4),\n        )\n\n        self.conv12 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (8, 8)),\n            dilation=(8, 8),\n        )\n\n        self.conv13 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (16, 16)),\n            dilation=(16, 16),\n        )\n\n        self.conv14 = nn.Conv2d(\n            96,\n            96,\n            kernel_size=(5, 5),\n            padding=self.get_padding((5, 5), (32, 32)),\n            dilation=(32, 32),\n        )\n\n        self.conv15 = nn.Conv2d(\n            96,\n            last_shape,\n            kernel_size=(1, 1),\n            padding=self.get_padding((1, 1), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        # Batch normalization layers\n\n        self.batch_norm1 = nn.BatchNorm2d(96)\n        self.batch_norm2 = nn.BatchNorm2d(96)\n        self.batch_norm3 = nn.BatchNorm2d(96)\n        self.batch_norm4 = nn.BatchNorm2d(96)\n        self.batch_norm5 = nn.BatchNorm2d(96)\n        self.batch_norm6 = nn.BatchNorm2d(96)\n        self.batch_norm7 = nn.BatchNorm2d(96)\n        self.batch_norm8 = nn.BatchNorm2d(96)\n        self.batch_norm9 = nn.BatchNorm2d(96)\n        self.batch_norm10 = nn.BatchNorm2d(96)\n        self.batch_norm11 = nn.BatchNorm2d(96)\n        self.batch_norm11 = nn.BatchNorm2d(96)\n        self.batch_norm12 = nn.BatchNorm2d(96)\n        self.batch_norm13 = nn.BatchNorm2d(96)\n        self.batch_norm14 = nn.BatchNorm2d(96)\n        self.batch_norm15 = nn.BatchNorm2d(last_shape)\n\n    def get_padding(self, kernel_size, dilation):\n        padding = (\n            ((dilation[0]) * (kernel_size[0] - 1)) // 2,\n            ((dilation[1]) * (kernel_size[1] - 1)) // 2,\n        )\n        return padding\n\n    def forward(self, input_audio):\n        # input audio will be (2,298,257)\n\n        output_layer = F.relu(self.batch_norm1(self.conv1(input_audio)))\n        output_layer = F.relu(self.batch_norm2(self.conv2(output_layer)))\n        output_layer = F.relu(self.batch_norm3(self.conv3(output_layer)))\n        output_layer = F.relu(self.batch_norm4(self.conv4(output_layer)))\n        output_layer = F.relu(self.batch_norm5(self.conv5(output_layer)))\n        output_layer = F.relu(self.batch_norm6(self.conv6(output_layer)))\n        output_layer = F.relu(self.batch_norm7(self.conv7(output_layer)))\n        output_layer = F.relu(self.batch_norm8(self.conv8(output_layer)))\n        output_layer = F.relu(self.batch_norm9(self.conv9(output_layer)))\n        output_layer = F.relu(self.batch_norm10(self.conv10(output_layer)))\n        output_layer = F.relu(self.batch_norm11(self.conv11(output_layer)))\n        output_layer = F.relu(self.batch_norm12(self.conv12(output_layer)))\n        output_layer = F.relu(self.batch_norm13(self.conv13(output_layer)))\n        output_layer = F.relu(self.batch_norm14(self.conv14(output_layer)))\n        output_layer = F.relu(self.batch_norm15(self.conv15(output_layer)))\n\n        # output_layer will be (N,8,298,257)\n        # we want it to be (N,8*257,298,1)\n        batch_size = output_layer.size(0)  # N\n        height = output_layer.size(2)  # 298\n\n        output_layer = output_layer.transpose(-1, -2).reshape(\n            (batch_size, -1, height, 1)\n        )\n        return output_layer\n\n\nclass Video_Model(nn.Module):\n    def __init__(self, last_shape=256):\n        super(Video_Model, self).__init__()\n\n        self.conv1 = nn.Conv2d(\n            512,\n            256,\n            kernel_size=(7, 1),\n            padding=self.get_padding((7, 1), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv2 = nn.Conv2d(\n            256,\n            256,\n            kernel_size=(5, 1),\n            padding=self.get_padding((5, 1), (1, 1)),\n            dilation=(1, 1),\n        )\n\n        self.conv3 = nn.Conv2d(\n            256,\n            256,\n            kernel_size=(5, 1),\n            padding=self.get_padding((5, 1), (2, 1)),\n            dilation=(2, 1),\n        )\n\n        self.conv4 = nn.Conv2d(\n            256,\n            256,\n            kernel_size=(5, 1),\n            padding=self.get_padding((5, 1), (4, 1)),\n            dilation=(4, 1),\n        )\n\n        self.conv5 = nn.Conv2d(\n            256,\n            256,\n            kernel_size=(5, 1),\n            padding=self.get_padding((5, 1), (8, 1)),\n            dilation=(8, 1),\n        )\n\n        self.conv6 = nn.Conv2d(\n            256,\n            256,\n            kernel_size=(5, 1),\n            padding=self.get_padding((5, 1), (16, 1)),\n            dilation=(16, 1),\n        )\n\n        # Batch normalization layers\n\n        self.batch_norm1 = nn.BatchNorm2d(256)\n        self.batch_norm2 = nn.BatchNorm2d(256)\n        self.batch_norm3 = nn.BatchNorm2d(256)\n        self.batch_norm4 = nn.BatchNorm2d(256)\n        self.batch_norm5 = nn.BatchNorm2d(256)\n        self.batch_norm6 = nn.BatchNorm2d(last_shape)\n\n    def get_padding(self, kernel_size, dilation):\n        padding = (\n            ((dilation[0]) * (kernel_size[0] - 1)) // 2,\n            ((dilation[1]) * (kernel_size[1] - 1)) // 2,\n        )\n        return padding\n\n    def forward(self, input_video):\n        # input video will be (512,75,1)\n        if len(input_video.shape) == 3:\n            input_video = input_video.unsqueeze(1)\n        # input_video = torch.transpose(input_video,1,3) # (1,75,512)\n        # input_video = self.linear_for_512_to_1024(input_video) # (1,75,1024)\n\n        input_video = torch.transpose(input_video, 1, 3)  # (1024,75,1)\n\n        output_layer = F.relu(self.batch_norm1(self.conv1(input_video)))\n        output_layer = F.relu(self.batch_norm2(self.conv2(output_layer)))\n        output_layer = F.relu(self.batch_norm3(self.conv3(output_layer)))\n        output_layer = F.relu(self.batch_norm4(self.conv4(output_layer)))\n        output_layer = F.relu(self.batch_norm5(self.conv5(output_layer)))\n        output_layer = F.relu(self.batch_norm6(self.conv6(output_layer)))\n\n        # for upsampling , as mentioned in paper\n        output_layer = nn.functional.interpolate(\n            output_layer, size=(298, 1), mode=""nearest""\n        )\n\n        return output_layer\n\n\n# so now , video_output is (N,256,298,1)\n# and audio_output is  (N,8*257,298,1)\n# where N = batch_size\n\n\nclass Audio_Visual_Fusion(nn.Module):\n    """"""Audio Visual Speech Separation model as described in [1].\n    All default values are the same as paper.\n\n        Args:\n            num_person (int): total number of persons (as i/o).\n            device (torch.Device): device used to return the final tensor.\n            audio_last_shape (int): relevant last shape for tensor in audio network.\n            video_last_shape (int): relevant last shape for tensor in video network.\n            input_spectrogram_shape (tuple(int)): shape of input spectrogram.\n\n        References:\n            [1]: \'Looking to Listen at the Cocktail Party:\n            A Speaker-Independent Audio-Visual Model for Speech Separation\' Ephrat et. al\n            https://arxiv.org/abs/1804.03619\n    """"""\n\n    def __init__(\n        self,\n        num_person=2,\n        device=None,\n        audio_last_shape=8,\n        video_last_shape=256,\n        input_spectrogram_shape=(298, 257, 2),\n    ):\n        if isinstance(device, str):\n            device = torch.device(device)\n\n        self.device = device\n        super(Audio_Visual_Fusion, self).__init__()\n        self.num_person = num_person\n        self.input_dim = (\n            audio_last_shape * input_spectrogram_shape[1]\n            + video_last_shape * self.num_person\n        )\n\n        self.audio_output = Audio_Model(last_shape=audio_last_shape)\n        self.video_output = Video_Model(last_shape=video_last_shape)\n\n        self.lstm = nn.LSTM(\n            self.input_dim,\n            400,\n            num_layers=1,\n            bias=True,\n            batch_first=True,\n            bidirectional=True,\n        )\n\n        self.fc1 = nn.Linear(400, 600)\n        torch.nn.init.xavier_uniform_(self.fc1.weight)\n        self.fc2 = nn.Linear(600, 600)\n        torch.nn.init.xavier_uniform_(self.fc2.weight)\n        self.fc3 = nn.Linear(600, 600)\n        torch.nn.init.xavier_uniform_(self.fc3.weight)\n\n        self.complex_mask_layer = nn.Linear(600, 2 * 257 * self.num_person)\n        torch.nn.init.xavier_uniform_(self.complex_mask_layer.weight)\n\n        self.drop1 = nn.Dropout(0.2)\n        self.drop2 = nn.Dropout(0.2)\n        self.drop3 = nn.Dropout(0.2)\n\n        self.batch_norm1 = nn.BatchNorm1d(298)\n        self.batch_norm2 = nn.BatchNorm1d(298)\n        self.batch_norm3 = nn.BatchNorm1d(298)\n\n    def forward(self, input_audio, input_video):\n        # input_audio will be (N,514,298)\n        # input_video will be list of size = num_person , so each item of list will be of (N,512,75,1)\n\n        input_audio = transforms.to_torchaudio(input_audio).transpose(1, 3)\n        audio_out = self.audio_output(input_audio)\n        # audio_out will be (N,256,298,1)\n        AVFusion = [audio_out]\n        for i in range(self.num_person):\n            video_out = self.video_output(input_video[i])\n            AVFusion.append(video_out)\n\n        mixed_av = torch.cat(AVFusion, dim=1)\n\n        mixed_av = mixed_av.squeeze(3)  # (N,input_dim,298)\n        mixed_av = torch.transpose(mixed_av, 1, 2)  # (N,298,input_dim)\n\n        self.lstm.flatten_parameters()\n        mixed_av, (h, c) = self.lstm(mixed_av)\n        mixed_av = mixed_av[..., :400] + mixed_av[..., 400:]\n\n        mixed_av = self.batch_norm1((F.relu(self.fc1(mixed_av))))\n        mixed_av = self.drop1(mixed_av)\n\n        mixed_av = self.batch_norm2(F.relu(self.fc2(mixed_av)))\n        mixed_av = self.drop2(mixed_av)\n\n        mixed_av = self.batch_norm3(F.relu(self.fc3(mixed_av)))  # (N,298,600)\n        mixed_av = self.drop3(mixed_av)\n\n        complex_mask = torch.sigmoid(\n            self.complex_mask_layer(mixed_av)\n        )  # (N,298,2*257*num_person)\n\n        batch_size = complex_mask.size(0)  # N\n        complex_mask = complex_mask.view(batch_size, 2, 298, 257, self.num_person)\n\n        output_audio = torch.zeros_like(complex_mask, device=self.device)\n        for i in range(self.num_person):\n            output_audio[..., i] = fast_icRM(input_audio, complex_mask[..., i])\n\n        output_audio = output_audio.permute(0, 4, 1, 3, 2).reshape(\n            batch_size, self.num_person, 514, 298\n        )\n        return output_audio\n\n\ndef make_model_and_optimizer(conf, gpu_ids=[0]):\n    """"""Define model and optimizer.\n\n    Args:\n        conf: Configuration for model and optimizer.\n\n    Returns:\n        model, optimizer\n    """"""\n    device = torch.device(conf[""training""][""device""])\n    model = Audio_Visual_Fusion(conf[""main_args""][""n_src""], device)\n    model = model.to(device)\n    device_count = torch.cuda.device_count()\n    if len(gpu_ids) > 1 and device_count > 1:\n        if len(gpu_ids) != device_count:\n            print(f""Using {gpu_ids} GPUs"")\n        else:\n            print(f""Using all {device_count} GPUs"")\n        model = torch.nn.DataParallel(model, device_ids=gpu_ids)\n\n    optimizer = make_optimizer(model.parameters(), **conf[""optim""])\n    return model, optimizer\n\n\ndef load_best_model(train_conf, exp_dir):\n    """"""Load best model.\n    NOTE: This function is not needed during training. Catalyst has\n    a `resume` parameter that takes in the logdir location.\n\n    Args:\n        train_conf: Configuration used during training.\n        exp_dir: Logdir created by Catalyst.\n\n    Returns:\n        model\n    """"""\n    model, optimizer = make_model_and_optimizer(train_conf)\n\n    # Catalyst stores the best model as: logdir/checkpoints/best_full.pth\n    exp_dir = Path(exp_dir) if isinstance(exp_dir, str) else exp_dir\n    best_model_path = exp_dir / ""checkpoints"" / ""best_full.pth""\n    if not best_model_path.is_file():\n        print(f""No best path in logdir: {exp_dir}. Initializing model..."")\n        return model\n\n    checkpoint = torch.load(best_model_path)\n    model = torch_utils.load_state_dict_in(checkpoint[""model_state_dict""], model)\n    return model\n'"
egs/avspeech/looking-to-listen/train.py,3,"b'import sys\nimport yaml\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom argparse import ArgumentParser\nfrom asteroid.data.avspeech_dataset import AVSpeechDataset\n\nfrom local.loader.constants import EMBED_DIR\nfrom train import train, ParamConfig\nfrom model import make_model_and_optimizer, load_best_model\n\n\nclass DiscriminativeLoss(torch.nn.Module):\n    # Reference: https://github.com/bill9800/speech_separation/blob/master/model/lib/model_loss.py\n\n    def __init__(self, n_src=2, gamma=0.1):\n        super(DiscriminativeLoss, self).__init__()\n\n        self.n_src = n_src\n        self.gamma = gamma\n\n    def forward(self, input, target):\n\n        sum_mtr = torch.zeros_like(input[:, 0, ...])\n        for i in range(self.n_src):\n            sum_mtr += (target[:, i, ...] - input[:, i, ...]) ** 2\n            for j in range(self.n_src):\n                if i != j:\n                    sum_mtr -= self.gamma * (\n                        (target[:, i, ...] - input[:, j, ...]) ** 2\n                    )\n        sum_mtr = torch.mean(sum_mtr.view(-1))\n\n        return sum_mtr\n\n\ndef main(conf):\n    config = ParamConfig(\n        conf[""training""][""batch_size""],\n        conf[""training""][""epochs""],\n        conf[""training""][""num_workers""],\n        cuda=True,\n        use_half=False,\n        learning_rate=conf[""optim""][""lr""],\n    )\n\n    dataset = AVSpeechDataset(\n        Path(""data/train.csv""), Path(EMBED_DIR), conf[""main_args""][""n_src""]\n    )\n    val_dataset = AVSpeechDataset(\n        Path(""data/val.csv""), Path(EMBED_DIR), conf[""main_args""][""n_src""]\n    )\n\n    model, optimizer = make_model_and_optimizer(conf)\n    print(\n        f""AVFusion has {sum(np.prod(i.shape) for i in model.parameters()):,} parameters""\n    )\n\n    criterion = DiscriminativeLoss()\n\n    model_path = Path(conf[""main_args""][""exp_dir""]) / ""checkpoints"" / ""best_full.pth""\n    if model_path.is_file():\n        print(""Loading saved model..."")\n        resume = model_path.as_posix()\n    else:\n        resume = None\n\n    train(\n        model,\n        dataset,\n        optimizer,\n        criterion,\n        config,\n        val_dataset=val_dataset,\n        resume=resume,\n        logdir=conf[""main_args""][""exp_dir""],\n    )\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser()\n    parser.add_argument(""--gpus"", type=str, help=""list of GPUs"", default=""-1"")\n    parser.add_argument(\n        ""--n-src"",\n        type=int,\n        help=""number of inputs to neural network"",\n        default=2,\n    )\n    parser.add_argument(\n        ""--exp_dir"",\n        default=""exp/logdir"",\n        help=""Full path to save best validation model"",\n    )\n\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(""local/conf.yml"") as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/dns_challenge/baseline/denoise.py,2,"b""import glob\nimport os\n\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nfrom tqdm import tqdm\n\nfrom model import load_best_model\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--denoise_path', type=str, required=True,\n                    help='Directory containing wav files, or file path')\nparser.add_argument('--use_gpu', type=int, default=0,\n                    help='Whether to use the GPU for model execution')\nparser.add_argument('--exp_dir', default='exp/tmp',\n                    help='Experiment root')\n\n\ndef main(conf):\n    # Get best trained model\n    model = load_best_model(conf['train_conf'], conf['exp_dir'])\n    if conf['use_gpu']:\n        model = model.cuda()\n    model_device = next(model.parameters()).device\n    # Get a list of wav files (or single wav file)\n    save_folder = os.path.join(conf['exp_dir'], 'denoise')\n    os.makedirs(save_folder, exist_ok=True)\n    if os.path.isfile(conf['denoise_path']):\n        all_wavs = [conf['denoise_path']]\n    else:\n        # If this is a bunch of files we need to denoise, call the subdir\n        # of denoise the same way as the basename of the denoise dir.\n        save_folder = os.path.join(save_folder,\n                                   os.path.basename(conf['denoise_path']))\n        all_wavs = glob.glob(conf['denoise_path'] + '*.wav')\n\n    for wav_path in tqdm(all_wavs):\n        mix, fs = sf.read(wav_path, dtype='float32')\n        with torch.no_grad():\n            net_inp = torch.tensor(mix)[None].to(model_device)\n            estimate = model.denoise(net_inp).squeeze().cpu().data.numpy()\n        # Save the estimate speech\n        wav_name = os.path.basename(wav_path)\n        sf.write(os.path.join(save_folder, wav_name), estimate, fs)\n\n\nif __name__ == '__main__':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, 'conf.yml')\n    with open(conf_path) as conf_file:\n        train_conf = yaml.safe_load(conf_file)\n    arg_dic['train_conf'] = train_conf\n\n    main(arg_dic)\n"""
egs/dns_challenge/baseline/eval_on_synthetic.py,2,"b'import glob\nimport os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.metrics import get_metrics\n\nfrom model import load_best_model\nfrom local.preprocess_dns import make_wav_id_dict\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including wav files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=50,\n                    help=\'Number of audio examples to save, -1 means all\')\n\nALL_METRICS = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\', \'pesq\']\nCOMPUTE_METRICS = ALL_METRICS\n\n\ndef main(conf):\n    # Get best trained model\n    model = load_best_model(conf[\'train_conf\'], conf[\'exp_dir\'])\n    if conf[\'use_gpu\']:\n        model = model.cuda()\n    # Evaluate performances separately w/ and w/o reverb\n    for subdir in [\'with_reverb\', \'no_reverb\']:\n        dict_list = get_wavs_dict_list(os.path.join(conf[\'test_dir\'], subdir))\n        save_dir = os.path.join(conf[\'exp_dir\'], subdir + \'examples/\')\n        os.makedirs(save_dir, exist_ok=True)\n        all_metrics_df = evaluate(dict_list, model, conf=conf,\n                                  save_dir=save_dir)\n        all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'],\n                                           \'all_metrics_{}.csv\'.format(subdir)))\n        # Print and save summary metrics\n        final_results = {}\n        for metric_name in COMPUTE_METRICS:\n            input_metric_name = \'input_\' + metric_name\n            ldf = all_metrics_df[metric_name] - all_metrics_df[\n                input_metric_name]\n            final_results[metric_name] = all_metrics_df[metric_name].mean()\n            final_results[metric_name + \'_imp\'] = ldf.mean()\n        print(\'Overall metrics {} :\'.format(subdir))\n        pprint(final_results)\n        filename = os.path.join(conf[\'exp_dir\'],\n                                \'final_metrics_{}.json\'.format(subdir))\n        with open(filename, \'w\') as f:\n            json.dump(final_results, f, indent=0)\n\n\ndef get_wavs_dict_list(test_dir):\n    """""" Creates a list of example pair dictionaries.\n\n    Args:\n        test_dir (str): Directory where clean/ and noisy/ subdirectories can\n            be found.\n    Returns:\n        List[dict] : list of noisy/clean pair dictionaries.\n            Each dict looks like :\n                {\'clean\': clean_path,\n                \'noisy\': noisy_path,\n                \'id\': 3}\n    """"""\n    # Find all clean files and make an {id: filepath} dictionary\n    clean_wavs = glob.glob(os.path.join(test_dir, \'clean/*.wav\'))\n    clean_dic = make_wav_id_dict(clean_wavs)\n    # Same for noisy files\n    noisy_wavs = glob.glob(os.path.join(test_dir, \'noisy/*.wav\'))\n    noisy_dic = make_wav_id_dict(noisy_wavs)\n    assert clean_dic.keys() == noisy_dic.keys()\n    # Combine both dictionaries\n    dict_list = [dict(clean=clean_dic[k], noisy=noisy_dic[k], id=k)\n                 for k in clean_dic.keys()]\n    return dict_list\n\n\ndef evaluate(dict_list, model, conf, save_dir=None):\n    model_device = next(model.parameters()).device\n    # Randomly choose the indexes of sentences to save.\n    if save_dir is None:\n        conf[\'n_save_ex\'] = 0\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(dict_list)\n    save_idx = random.sample(range(len(dict_list)), conf[\'n_save_ex\'])\n    series_list = []\n    for idx, wav_dic in enumerate(tqdm(dict_list)):\n        # Forward the network on the mixture.\n        noisy_np, clean_np, fs = load_wav_dic(wav_dic)\n        with torch.no_grad():\n            net_input = torch.tensor(noisy_np)[None, None].to(model_device)\n            est_clean_np = model.denoise(net_input).squeeze().cpu().data.numpy()\n\n        utt_metrics = get_metrics(mix=noisy_np, clean=clean_np,\n                                  estimate=est_clean_np,\n                                  sample_rate=fs,\n                                  metrics_list=COMPUTE_METRICS)\n        utt_metrics[\'noisy_path\'] = wav_dic[\'noisy\']\n        utt_metrics[\'clean_path\'] = wav_dic[\'clean\']\n        series_list.append(pd.Series(utt_metrics))\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""noisy.wav"", noisy_np,\n                     fs)\n            sf.write(local_save_dir + ""clean.wav"", clean_np,\n                     fs)\n            sf.write(local_save_dir + ""estimate.wav"", est_clean_np,\n                     fs)\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    return all_metrics_df\n\n\ndef load_wav_dic(wav_dic):\n    """""" Load wavs files from a dictionary with path entries.\n\n    Returns:\n        tuple: noisy speech waveform, clean speech waveform.\n    """"""\n    noisy_path, clean_path = wav_dic[\'noisy\'], wav_dic[\'clean\']\n    noisy, fs = sf.read(noisy_path, dtype=\'float32\')\n    clean, fs = sf.read(clean_path, dtype=\'float32\')\n    return noisy, clean, fs\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as conf_file:\n        train_conf = yaml.safe_load(conf_file)\n    arg_dic[\'train_conf\'] = train_conf\n\n    main(arg_dic)\n'"
egs/dns_challenge/baseline/model.py,6,"b'import json\nimport os\n\nimport torch\nfrom torch import nn\n\nfrom asteroid.engine.system import System\nfrom asteroid.filterbanks import make_enc_dec\nfrom asteroid.filterbanks.transforms import take_cat, take_mag\nfrom asteroid.filterbanks.transforms import apply_real_mask\nfrom asteroid.filterbanks.transforms import apply_mag_mask\nfrom asteroid.masknn import blocks\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid import torch_utils\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    # Define building blocks for local model\n    stft, istft = make_enc_dec(\'stft\', **conf[\'filterbank\'])\n    # Because we concatenate (re, im, mag) as input and compute a complex mask.\n    if conf[\'main_args\'][\'is_complex\']:\n        inp_size = int(stft.n_feats_out * 3 / 2)\n        output_size = stft.n_feats_out\n    else:\n        inp_size = output_size = int(stft.n_feats_out / 2)\n    # Add these fields to the mask model dict\n    conf[\'masknet\'].update(dict(input_size=inp_size,\n                                output_size=output_size))\n    masker = SimpleModel(**conf[\'masknet\'])\n    # Make the complete model\n    model = Model(stft, masker, istft,\n                  is_complex=conf[\'main_args\'][\'is_complex\'])\n    # Define optimizer of this model\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\nclass Model(nn.Module):\n    """""" Speech enhancement model.\n\n    Args:\n        encoder (~.Encoder): instance of a complex filterbank encoder\n            `Encoder(STFTBFB(**))`.\n        masker (nn.Module): Mask estimator network.\n        decoder (~.Decoder): instance of a complex filterbank decoder\n            `Decoder(STFTBFB(**))`.\n        is_complex (bool): If the network works on the complex domain.\n\n    If `is_complex` is `True`, the input to the network are complex features,\n    the network estimates a complex mask and returns a complex speech estimate.\n    Else, the input is the magnitude, the network estimates a magnitude mask\n    and the returns a **complex** speech estimate.\n    The loss function needs to be adapted to complex representations.\n    """"""\n    def __init__(self, encoder, masker, decoder, is_complex=True):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        # Decoder is not used for training but eventually, we want to invert\n        # the encoder. Might as well include it in the model.\n        self.decoder = decoder\n        self.is_complex = is_complex\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        # Compute STFT\n        tf_rep = self.encoder(x)\n        # Estimate TF mask from STFT features : cat([re, im, mag])\n        if self.is_complex:\n            to_masker = take_cat(tf_rep)\n        else:\n            to_masker = take_mag(tf_rep)\n        # LSTM masker expects a feature dimension last (not like 1D conv)\n        est_masks = self.masker(to_masker.transpose(1, 2)).transpose(1, 2)\n        # Apply TF mask\n        if self.is_complex:\n            masked_tf_rep = apply_real_mask(tf_rep, est_masks)\n        else:\n            masked_tf_rep = apply_mag_mask(tf_rep, est_masks)\n        return masked_tf_rep\n\n    def denoise(self, x):\n        estimate_stft = self(x)\n        wav = self.decoder(estimate_stft)\n        return torch_utils.pad_x_to_y(wav, x)\n\n\nclass SimpleModel(nn.Module):\n    """""" Simple recurrent model for the DNS challenge.\n\n    Args:\n        input_size (int): input size along the features dimension\n        hidden_size (int): hidden size in the recurrent net\n        output_size (int): output size, defaults to `:attr:` input_size\n        rnn_type (str): Select from ``\'RNN\'``, ``\'LSTM\'``, ``\'GRU\'``. Can also\n            be passed in lowercase letters.\n        n_layers (int): Number of recurrent layers.\n        dropout (float): dropout value between recurrent layers.\n    """"""\n    def __init__(self, input_size, hidden_size, output_size=None,\n                 rnn_type=\'gru\', n_layers=3, dropout=0.3):\n        super(SimpleModel, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        output_size = input_size if output_size is None else output_size\n        self.output_size = output_size\n        self.in_proj_layer = nn.Linear(input_size, hidden_size)\n        self.residual_rec = blocks.StackedResidualRNN(rnn_type, hidden_size,\n                                                      n_layers=n_layers,\n                                                      dropout=dropout)\n        self.out_proj_layer = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        """""" Mask estimator\'s forward pass. Expects [batch, time, input_size]""""""\n        # Non negative features from input\n        out_rec = self.residual_rec(torch.relu(self.in_proj_layer(x)))\n        # Activation is relu on the mask (better gradients allegedly)\n        return torch.relu(self.out_proj_layer(out_rec))\n\n\nclass SimpleSystem(System):\n    def common_step(self, batch, batch_nb):\n        mixture, speech, noise = batch\n        estimate = self(mixture.unsqueeze(1))\n        speech_stft = self.model.encoder(speech.unsqueeze(1))\n        # The loss function can be something like\n        # loss_func = partial(distance, is_complex=some_bool)\n        loss = self.loss_func(estimate, speech_stft)\n        return loss\n\n\ndef distance(estimate, target, is_complex=True):\n    """""" Compute the average distance in the complex plane. Makes more sense\n    when the network computes a complex mask.\n\n    Args:\n        estimate (torch.Tensor): Estimate complex spectrogram.\n        target (torch.Tensor): Speech target complex spectrogram.\n        is_complex (bool): Whether to compute the distance in the complex or\n            the magnitude space.\n\n    Returns:\n        torch.Tensor the loss value, in a tensor of size 1.\n    """"""\n    if is_complex:\n        # Take the difference in the complex plane and compute the squared norm\n        # of the remaining vector.\n        return take_mag(estimate - target).pow(2).mean()\n    else:\n        # Compute the mean difference between magnitudes.\n        return (take_mag(estimate) - take_mag(target)).pow(2).mean()\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` there.\n\n    Returns:\n        nn.Module the best pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    # Last best model summary\n    with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n        best_k = json.load(f)\n    best_model_path = min(best_k, key=best_k.get)\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'],\n                                           model)\n    model.eval()\n    return model\n'"
egs/dns_challenge/baseline/train.py,4,"b'import argparse\nimport json\nimport os\nfrom functools import partial\n\nimport pytorch_lightning as pl\nimport torch\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader, random_split\n\nfrom asteroid.data import DNSDataset\nfrom asteroid.utils import str2bool_arg\nfrom model import make_model_and_optimizer, SimpleSystem, distance\n\ntorch.manual_seed(17)  # Reproducibility on the dataset spliting\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\nparser.add_argument(\'--is_complex\', default=True, type=str2bool_arg)\n\n\ndef main(conf):\n    total_set = DNSDataset(conf[\'data\'][\'json_dir\'])\n    train_len = int(len(total_set) * (1 - conf[\'data\'][\'val_prop\']))\n    val_len = len(total_set) - train_len\n    train_set, val_set = random_split(total_set, [train_len, val_len])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n\n    # Define model and optimizer in a local function (defined in the recipe).\n    # Two advantages to this : re-instantiating the model and optimizer\n    # for retraining and evaluating is straight-forward.\n    model, optimizer = make_model_and_optimizer(conf)\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = partial(distance, is_complex=conf[\'main_args\'][\'is_complex\'])\n    system = SimpleSystem(model=model, loss_func=loss_func, optimizer=optimizer,\n                          train_loader=train_loader, val_loader=val_loader,\n                          scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=5.,)\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/kinect-wsj/DeepClustering/eval.py,1,"b'import os\nimport json\nimport yaml\nimport argparse\nimport random\nimport torch\nfrom tqdm import tqdm\nimport pandas as pd\nimport soundfile as sf\nfrom pprint import pprint\n\nfrom asteroid.utils import tensors_to_device\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.metrics import get_metrics\n\nfrom model import load_best_model\nfrom asteroid.data import KinectWsjMixDataset\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Local data directory with test files\')\nparser.add_argument(\'--n_src\', type=int, default=2)\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=-1,\n                    help=\'Number of audio examples to save, -1 means all\')\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model = load_best_model(conf[\'train_conf\'], conf[\'exp_dir\'])\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = KinectWsjMixDataset(conf[\'test_dir\'], n_src=conf[\'n_src\'],\n                              segment=None)\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources, noises = tensors_to_device(test_set[idx], device=model_device)\n        mix = mix[..., 0]\n        sources = sources[...,0]\n        #noise = noise[..., 0]\n        if conf[\'train_conf\'][\'training\'][\'loss_alpha\'] == 1:\n            # If Deep clustering only, use DC masks.\n            est_sources, dic_out = model.dc_head_separate(mix[None, None])\n        else:\n            # If Chimera, use mask-inference head masks\n            est_sources, dic_out = model.separate(mix[None, None])\n\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n    main(arg_dic)\n'"
egs/kinect-wsj/DeepClustering/model.py,5,"b'import json\nimport os\nimport torch\nfrom torch import nn\nfrom sklearn.cluster import KMeans\n\nfrom asteroid import torch_utils\nimport asteroid.filterbanks as fb\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.filterbanks.transforms import take_mag, apply_mag_mask, ebased_vad\nfrom asteroid.masknn.blocks import SingleRNN\nfrom asteroid.utils.torch_utils import pad_x_to_y\n\nEPS = 1e-8\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    enc, dec = fb.make_enc_dec(\'stft\', **conf[\'filterbank\'])\n    masker = Chimera(enc.n_feats_out // 2,\n                     **conf[\'masknet\'])\n    model = Model(enc, masker, dec)\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\nclass Chimera(nn.Module):\n    def __init__(self, in_chan, n_src, rnn_type=\'lstm\', n_layers=2,\n                 hidden_size=600, bidirectional=True, dropout=0.3,\n                 embedding_dim=20, take_log=False):\n        super().__init__()\n        self.input_dim = in_chan\n        self.n_src = n_src\n        self.take_log = take_log\n        # RNN common\n        self.embedding_dim = embedding_dim\n        self.rnn = SingleRNN(rnn_type, in_chan, hidden_size,\n                             n_layers=n_layers, dropout=dropout,\n                             bidirectional=bidirectional)\n        self.dropout = nn.Dropout(dropout)\n        rnn_out_dim = hidden_size * 2 if bidirectional else hidden_size\n        # Mask heads\n        self.mask_layer = nn.Linear(rnn_out_dim, in_chan * self.n_src)\n        self.mask_act = nn.Sigmoid()  # sigmoid or relu or softmax\n        # DC head\n        self.embedding_layer = nn.Linear(rnn_out_dim, in_chan * embedding_dim)\n        self.embedding_act = nn.Tanh()  # sigmoid or tanh\n\n    def forward(self, input_data):\n        batch, _, n_frames = input_data.shape\n        if self.take_log:\n            input_data = torch.log(input_data + EPS)\n        # Common net\n        out = self.rnn(input_data.permute(0, 2, 1))\n        out = self.dropout(out)\n\n        # DC head\n        proj = self.embedding_layer(out)  # batch, time, freq * emb\n        proj = self.embedding_act(proj)\n        proj = proj.view(batch, n_frames, -1, self.embedding_dim).transpose(1, 2)\n        # (batch, freq * frames, emb)\n        proj = proj.reshape(batch, -1, self.embedding_dim)\n        proj_norm = torch.norm(proj, p=2, dim=-1, keepdim=True)\n        projection_final = proj / (proj_norm + EPS)\n\n        # Mask head\n        mask_out = self.mask_layer(out).view(batch, n_frames,\n                                             self.n_src, self.input_dim)\n        mask_out = mask_out.permute(0, 2, 3, 1)\n        mask_out = self.mask_act(mask_out)\n        return projection_final, mask_out\n\n\nclass Model(nn.Module):\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        final_proj, mask_out = self.masker(take_mag(tf_rep))\n        return final_proj, mask_out\n\n    def separate(self, x):\n        """""" Separate with mask-inference head, output waveforms """"""\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        proj, mask_out = self.masker(take_mag(tf_rep))\n        masked = apply_mag_mask(tf_rep.unsqueeze(1), mask_out)\n        wavs = torch_utils.pad_x_to_y(self.decoder(masked), x)\n        dic_out = dict(tfrep=tf_rep, mask=mask_out, masked_tfrep=masked,\n                       proj=proj)\n        return wavs, dic_out\n\n    def dc_head_separate(self, x):\n        """""" Cluster embeddings to produce binary masks, output waveforms """"""\n        kmeans = KMeans(n_clusters=self.masker.n_src)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        mag_spec = take_mag(tf_rep)\n        proj, mask_out = self.masker(mag_spec)\n        active_bins = ebased_vad(mag_spec)\n        active_proj = proj[active_bins.view(1, -1)]\n        #\n        bin_clusters = kmeans.fit_predict(active_proj.cpu().data.numpy())\n        # Create binary masks\n        est_mask_list = []\n        for i in range(self.masker.n_src):\n            # Add ones in all inactive bins in each mask.\n            mask = ~active_bins\n            mask[active_bins] = torch.from_numpy((bin_clusters == i)).to(mask.device)\n            est_mask_list.append(mask.float())  # Need float, not bool\n        # Go back to time domain\n        est_masks = torch.stack(est_mask_list, dim=1)\n        masked = apply_mag_mask(tf_rep, est_masks)\n        wavs = pad_x_to_y(self.decoder(masked), x)\n        dic_out = dict(tfrep=tf_rep, mask=mask_out, masked_tfrep=masked,\n                       proj=proj)\n        return wavs, dic_out\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` of `checkpoints` directory in it.\n\n    Returns:\n        nn.Module the best (or last) pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    try:\n        # Last best model summary\n        with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n            best_k = json.load(f)\n        best_model_path = min(best_k, key=best_k.get)\n    except FileNotFoundError:\n        # Get last checkpoint\n        all_ckpt = os.listdir(os.path.join(exp_dir, \'checkpoints/\'))\n        all_ckpt.sort()\n        best_model_path = os.path.join(exp_dir, \'checkpoints\', all_ckpt[-1])\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'], model)\n    model.eval()\n    return model\n'"
egs/kinect-wsj/DeepClustering/train.py,12,"b'import os\nimport argparse\nimport json\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_mse\nfrom asteroid.losses import deep_clustering_loss\nfrom asteroid.filterbanks.transforms import take_mag, ebased_vad\n\nfrom asteroid.data.kinect_wsj import make_dataloaders\nfrom model import make_model_and_optimizer\n\nEPS = 1e-8\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    # Define Dataloader\n    train_loader, val_loader = make_dataloaders(**conf[\'data\'],\n                                                **conf[\'training\'])\n    conf[\'masknet\'].update({\'n_src\': conf[\'data\'][\'n_src\']})\n    # Define model, optimizer + scheduler\n    model, optimizer = make_model_and_optimizer(conf)\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n\n    # Save config\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define loss function\n    loss_func = ChimeraLoss(alpha=conf[\'training\'][\'loss_alpha\'])\n    # Put together in System\n    system = ChimeraSystem(model=model, loss_func=loss_func,\n                           optimizer=optimizer, train_loader=train_loader,\n                           val_loader=val_loader, scheduler=scheduler,\n                           config=conf)\n\n    # Callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n    gpus=-1\n    # Don\'t ask GPU if they are not available.\n    if not torch.cuda.is_available():\n        print(\'No available GPU were found, set gpus to None\')\n        gpus = None\n\n    # Train model\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=200,)\n    trainer.fit(system)\n\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(checkpoint.best_k_models, f, indent=0)\n    # Save last model for convenience\n    torch.save(system.model.state_dict(),\n               os.path.join(exp_dir, \'checkpoints/final.pth\'))\n\n\n# TODO:Should ideally be inherited from wsj0-mix\nclass ChimeraSystem(System):\n    def __init__(self, *args, mask_mixture=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mask_mixture = mask_mixture\n\n    def common_step(self, batch, batch_nb, train=False):\n        inputs, targets, masks = self.unpack_data(batch)\n        embeddings, est_masks = self(inputs)\n        spec = take_mag(self.model.encoder(inputs.unsqueeze(1)))\n        if self.mask_mixture:\n            est_masks = est_masks * spec.unsqueeze(1)\n            masks = masks * spec.unsqueeze(1)\n        loss, loss_dic = self.loss_func(embeddings, targets, est_src=est_masks,\n                                        target_src=masks, mix_spec=spec)\n        return loss, loss_dic\n\n    def training_step(self, batch, batch_nb):\n        loss, loss_dic = self.common_step(batch, batch_nb, train=True)\n        tensorboard_logs = dict(train_loss=loss,\n                                train_dc_loss=loss_dic[\'dc_loss\'],\n                                train_pit_loss=loss_dic[\'pit_loss\'])\n        return {\'loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_step(self, batch, batch_nb):\n        loss, loss_dic = self.common_step(batch, batch_nb, train=False)\n        tensorboard_logs = dict(val_loss=loss,\n                                val_dc_loss=loss_dic[\'dc_loss\'],\n                                val_pit_loss=loss_dic[\'pit_loss\'])\n        return {\'val_loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_end(self, outputs):\n        # Not so pretty for now but it helps.\n        avg_loss = torch.stack([x[\'val_loss\']\n                                for x in outputs]).mean()\n        avg_dc_loss = torch.stack([x[\'log\'][\'val_dc_loss\']\n                                   for x in outputs]).mean()\n        avg_pit_loss = torch.stack([x[\'log\'][\'val_pit_loss\']\n                                    for x in outputs]).mean()\n        tensorboard_logs = dict(val_loss=avg_loss,\n                                val_dc_loss=avg_dc_loss,\n                                val_pit_loss=avg_pit_loss)\n        return {\'val_loss\': avg_loss, \'log\': tensorboard_logs,\n                \'progress_bar\': {\'val_loss\': avg_loss}}\n\n    def unpack_data(self, batch):\n        mix, sources, noise = batch\n        # Take only the first channel\n        mix = mix[..., 0]\n        sources = sources[...,0]\n        noise = noise[..., 0]\n        noise = noise.unsqueeze(1)\n        # Compute magnitude spectrograms and IRM\n        src_mag_spec = take_mag(self.model.encoder(sources))\n        noise_mag_spec = take_mag(self.model.encoder(noise))\n        noise_mag_spec = noise_mag_spec.unsqueeze(1)\n        real_mask = src_mag_spec / ( noise_mag_spec+src_mag_spec.sum(1, keepdim=True) + EPS)\n        # Get the src idx having the maximum energy\n        binary_mask = real_mask.argmax(1)\n        return mix, binary_mask, real_mask\n\n\nclass ChimeraLoss(nn.Module):\n    """""" Combines Deep clustering loss and mask inference loss for ChimeraNet.\n\n    Args:\n        alpha (float): loss weight. Total loss will be :\n            `alpha` * dc_loss + (1 - `alpha`) * mask_mse_loss.\n    """"""\n    def __init__(self, alpha=0.1):\n        super().__init__()\n        assert alpha >= 0, ""Negative alpha values don\'t make sense.""\n        assert alpha <= 1, ""Alpha values above 1 don\'t make sense.""\n        # PIT loss\n        self.src_mse = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\')\n        self.alpha = alpha\n\n    def forward(self, est_embeddings, target_indices, est_src=None,\n                target_src=None, mix_spec=None):\n        """"""\n\n        Args:\n            est_embeddings (torch.Tensor): Estimated embedding from the DC head.\n            target_indices (torch.Tensor): Target indices that\'ll be passed to\n                the DC loss.\n            est_src (torch.Tensor): Estimated magnitude spectrograms (or masks).\n            target_src (torch.Tensor): Target magnitude spectrograms (or masks).\n            mix_spec (torch.Tensor): The magnitude spectrogram of the mixture\n                from which VAD will be computed. If None, no VAD is used.\n\n        Returns:\n            torch.Tensor, the total loss, averaged over the batch.\n            dict with `dc_loss` and `pit_loss` keys, unweighted losses.\n        """"""\n        if self.alpha != 0 and (est_src is None or target_src is None):\n            raise ValueError(\'Expected target and estimated spectrograms to \'\n                             \'compute the PIT loss, found None.\')\n        binary_mask = None\n        if mix_spec is not None:\n            binary_mask = ebased_vad(mix_spec)\n        # Dc loss is already divided by VAD in the loss function.\n        dc_loss = deep_clustering_loss(embedding=est_embeddings,\n                                       tgt_index=target_indices,\n                                       binary_mask=binary_mask)\n        src_pit_loss = self.src_mse(est_src, target_src)\n        # Equation (4) from Chimera paper.\n        tot = self.alpha * dc_loss.mean() + (1 - self.alpha) * src_pit_loss\n        # Return unweighted losses as well for logging.\n        loss_dict = dict(dc_loss=dc_loss.mean(),\n                         pit_loss=src_pit_loss)\n        return tot, loss_dict\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    pprint(arg_dic)\n    main(arg_dic)\n'"
egs/librimix/ConvTasNet/eval.py,2,"b'import os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.metrics import get_metrics\nfrom asteroid.data.librimix_dataset import LibriMix\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid import ConvTasNet\nfrom asteroid.models import save_publishable\nfrom asteroid.utils import tensors_to_device\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the csv files\')\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--out_dir\', type=str, required=True,\n                    help=\'Directory in exp_dir where the eval results\'\n                         \' will be stored\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=10,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model_path = os.path.join(conf[\'exp_dir\'], \'best_model.pth\')\n    model = ConvTasNet.from_pretrained(model_path)\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = LibriMix(csv_dir=conf[\'test_dir\'],\n                        task=conf[\'task\'],\n                        sample_rate=conf[\'sample_rate\'],\n                        n_src=conf[\'train_conf\'][\'data\'][\'n_src\'],\n                        segment=None)  # Uses all segment length\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    eval_save_dir = os.path.join(conf[\'exp_dir\'], conf[\'out_dir\'])\n    ex_save_dir = os.path.join(eval_save_dir, \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix.unsqueeze(0))\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix.cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        # For each utterance, we get a dictionary with the mixture path,\n        # the input and output metrics\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mixture_path\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np,\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                est_src *= np.max(np.abs(mix_np))/np.max(np.abs(est_src))\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(eval_save_dir, \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(eval_save_dir, \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n    model_dict = torch.load(model_path, map_location=\'cpu\')\n    os.makedirs(os.path.join(conf[\'exp_dir\'], \'publish_dir\'), exist_ok=True)\n    publishable = save_publishable(\n        os.path.join(conf[\'exp_dir\'], \'publish_dir\'), model_dict,\n        metrics=final_results, train_conf=train_conf\n    )\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/librimix/ConvTasNet/model.py,1,"b'import json\nimport os\nimport torch\nfrom torch import nn\n\nimport asteroid.filterbanks as fb\nfrom asteroid import torch_utils\nfrom asteroid.masknn import TDConvNet\nfrom asteroid.engine.optimizers import make_optimizer\n\n\nclass Model(nn.Module):\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        est_masks = self.masker(tf_rep)\n        masked_tf_rep = est_masks * tf_rep.unsqueeze(1)\n        return torch_utils.pad_x_to_y(self.decoder(masked_tf_rep), x)\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    # Define building blocks for local model\n    enc, dec = fb.make_enc_dec(\'free\', **conf[\'filterbank\'])\n    masker = TDConvNet(in_chan=enc.filterbank.n_feats_out,\n                       out_chan=enc.filterbank.n_feats_out,\n                       **conf[\'masknet\'])\n    model = Model(enc, masker, dec)\n    # Define optimizer of this model\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` there.\n\n    Returns:\n        nn.Module the best pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    # Last best model summary\n    with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n        best_k = json.load(f)\n    best_model_path = min(best_k, key=best_k.get)\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'],\n                                           model)\n    model.eval()\n    return model\n'"
egs/librimix/ConvTasNet/train.py,5,"b'import os\nimport argparse\nimport json\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.models import ConvTasNet\nfrom asteroid.data import LibriMix\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\n\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    train_set = LibriMix(csv_dir=conf[\'data\'][\'train_dir\'],\n                         task=conf[\'data\'][\'task\'],\n                         sample_rate=conf[\'data\'][\'sample_rate\'],\n                         n_src=conf[\'data\'][\'n_src\'],\n                         segment=conf[\'data\'][\'segment\'])\n\n    val_set = LibriMix(csv_dir=conf[\'data\'][\'valid_dir\'],\n                       task=conf[\'data\'][\'task\'],\n                       sample_rate=conf[\'data\'][\'sample_rate\'],\n                       n_src=conf[\'data\'][\'n_src\'],\n                       segment=conf[\'data\'][\'segment\'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n    conf[\'masknet\'].update({\'n_src\': conf[\'data\'][\'n_src\']})\n\n    model = ConvTasNet(**conf[\'filterbank\'], **conf[\'masknet\'])\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=5.)\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n    # Save best model (next PL version will make this easier)\n    best_path = [b for b, v in best_k.items() if v == min(best_k.values())][0]\n    state_dict = torch.load(best_path)\n    system.load_state_dict(state_dict=state_dict[\'state_dict\'])\n    system.cpu()\n\n    to_save = system.model.serialize()\n    to_save.update(train_set.get_infos())\n    torch.save(to_save, os.path.join(exp_dir, \'best_model.pth\'))\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/musdb18/OpenUnmix/test_dataloader.py,4,"b'from asteroid.data import MUSDB18Dataset\nimport argparse\nimport torch\nfrom pathlib import Path\nimport tqdm\n\n\nclass Compose(object):\n    """"""Composes several augmentation transforms.\n    Args:\n        augmentations: list of augmentations to compose.\n    """"""\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, audio):\n        for transform in self.transforms:\n            audio = transform(audio)\n        return audio\n\n\ndef _augment_gain(audio, low=0.25, high=1.25):\n    """"""Applies a random gain to each source between `low` and `high`""""""\n    gain = low + torch.rand(1) * (high - low)\n    return audio * gain\n\n\ndef _augment_channelswap(audio):\n    """"""Randomly swap channels of stereo sources""""""\n    if audio.shape[0] == 2 and torch.FloatTensor(1).uniform_() < 0.5:\n        return torch.flip(audio, [0])\n\n    return audio\n\n\nif __name__ == ""__main__"":\n    """"""dataset tests -> these parameters will go into recipes""""""\n    parser = argparse.ArgumentParser(description=\'MUSDB18 dataset test\')\n\n    parser.add_argument(\n        \'--root\', type=str, help=\'root path of dataset\'\n    )\n\n    parser.add_argument(\'--seed\', type=int, default=42)\n\n    parser.add_argument(\n        \'--seq-dur\', type=float, default=6.0,\n        help=\'Duration of <=0.0 will result in the full audio\'\n    )\n\n    parser.add_argument(\n        \'--samples-per-track\', type=int, default=64,\n        help=\'draws a fixed number of samples per track\'\n    )\n\n    parser.add_argument(\n        \'--random-track-mix\',\n        action=\'store_true\', default=False,\n        help=\'Apply random track mixing augmentation\'\n    )\n    parser.add_argument(\n        \'--source-augmentations\', type=str, nargs=\'+\',\n        default=[\'gain\', \'channelswap\']\n    )\n    parser.add_argument(\'--batch-size\', type=int, default=16)\n\n    args = parser.parse_args()\n\n    dataset_kwargs = {\n        \'root\': Path(args.root),\n    }\n\n    source_augmentations = Compose(\n        [globals()[\'_augment_\' + aug] for aug in args.source_augmentations]\n    )\n\n    train_dataset = MUSDB18Dataset(\n        split=\'train\',\n        source_augmentations=source_augmentations,\n        random_track_mix=args.random_track_mix,\n        segment=args.seq_dur,\n        random_segments=True,\n        samples_per_track=64,\n        **dataset_kwargs\n    )\n\n    # List of MUSDB18 validation tracks as being used in the `musdb` package\n    # See https://github.com/sigsep/sigsep-mus-db/blob/master/musdb/configs/mus.yaml#L41\n    validation_tracks = [\n        \'Actions - One Minute Smile\',\n        \'Clara Berry And Wooldog - Waltz For My Victims\',\n        \'Johnny Lokke - Promises & Lies\',\n        \'Patrick Talbot - A Reason To Leave\',\n        \'Triviul - Angelsaint\',\n        \'Alexander Ross - Goodbye Bolero\',\n        \'Fergessen - Nos Palpitants\',\n        \'Leaf - Summerghost\',\n        \'Skelpolu - Human Mistakes\',\n        \'Young Griffo - Pennies\',\n        \'ANiMAL - Rockshow\',\n        \'James May - On The Line\',\n        \'Meaxic - Take A Step\',\n        \'Traffic Experiment - Sirens\'\n    ]\n\n    valid_dataset = MUSDB18Dataset(\n        split=\'train\',\n        subset=validation_tracks,\n        segment=None,\n        **dataset_kwargs\n    )\n\n    test_dataset = MUSDB18Dataset(\n        split=\'test\',\n        subset=None,\n        segment=None,\n        **dataset_kwargs\n    )\n\n    print(""Number of train tracks: "", len(train_dataset.tracks))\n    print(""Number of validation tracks: "", len(valid_dataset.tracks))\n    print(""Number of test tracks: "", len(test_dataset.tracks))\n\n    print(""Number of train samples: "", len(train_dataset))\n    print(""Number of validation samples: "", len(valid_dataset))\n    print(""Number of test samples: "", len(test_dataset))\n\n    train_sampler = torch.utils.data.DataLoader(\n        train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=0,\n    )\n\n    for x, y in tqdm.tqdm(train_sampler):\n        pass\n'"
egs/sms_wsj/CaCGMM/start_evaluation.py,0,"b""import argparse\n\nimport dlp_mpi\nimport yaml\nfrom asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\nfrom sms_wsj.examples.reference_systems import experiment\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--json_path', default='data/sms_wsj.json',\n                    help='Full path to sms_wsj.json')\n\n\ndef main(conf):\n    experiment.run(config_updates=dict(\n        json_path=conf['main_args']['json_path'],\n        **conf['mm_config'])\n    )\n\n\nif __name__ == '__main__':\n    if dlp_mpi.IS_MASTER:\n        # We start with opening the config file conf.yml as a dictionary from\n        # which we can create parsers. Each top level key in the dictionary defined\n        # by the YAML file creates a group in the parser.\n        with open('local/conf.yml') as f:\n            def_conf = yaml.safe_load(f)\n        parser = prepare_parser_from_dict(def_conf, parser=parser)\n        # Arguments are then parsed into a hierarchical dictionary (instead of\n        # flat, as returned by argparse) to falicitate calls to the different\n        # asteroid methods (see in main).\n        # plain_args is the direct output of parser.parse_args() and contains all\n        # the attributes in an non-hierarchical structure. It can be useful to also\n        # have it so we included it here but it is not used.\n        arg_dict, plain_args = parse_args_as_dict(\n            parser, return_plain_args=True)\n    else:\n        arg_dict = None\n    arg_dict = dlp_mpi.bcast(arg_dict, root=dlp_mpi.MASTER)\n    main(arg_dict)\n"""
egs/wham/ConvTasNet/eval.py,2,"b'import os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.metrics import get_metrics\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.models import ConvTasNet\nfrom asteroid.utils import tensors_to_device\nfrom asteroid.models import save_publishable\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the json files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=50,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model_path = os.path.join(conf[\'exp_dir\'], \'best_model.pth\')\n    model = ConvTasNet.from_pretrained(model_path)\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = WhamDataset(conf[\'test_dir\'], conf[\'task\'],\n                           sample_rate=conf[\'sample_rate\'],\n                           nondefault_nsrc=model.masker.n_src,\n                           segment=None)  # Uses all segment length\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix[None, None])\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n    model_dict = torch.load(model_path, map_location=\'cpu\')\n    os.makedirs(os.path.join(conf[\'exp_dir\'], \'publish_dir\'), exist_ok=True)\n    publishable = save_publishable(\n        os.path.join(conf[\'exp_dir\'], \'publish_dir\'), model_dict,\n        metrics=final_results, train_conf=train_conf\n    )\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/wham/ConvTasNet/train.py,5,"b'import os\nimport argparse\nimport json\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.models import ConvTasNet\n\n\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    train_set = WhamDataset(conf[\'data\'][\'train_dir\'], conf[\'data\'][\'task\'],\n                            sample_rate=conf[\'data\'][\'sample_rate\'],\n                            nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n    val_set = WhamDataset(conf[\'data\'][\'valid_dir\'], conf[\'data\'][\'task\'],\n                          sample_rate=conf[\'data\'][\'sample_rate\'],\n                          nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update({\'n_src\': train_set.n_src})\n\n    # Define model and optimizer\n    model = ConvTasNet(**conf[\'filterbank\'], **conf[\'masknet\'])\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=5.)\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n    # Save best model (next PL version will make this easier)\n    best_path = [b for b, v in best_k.items() if v == min(best_k.values())][0]\n    state_dict = torch.load(best_path)\n    system.load_state_dict(state_dict=state_dict[\'state_dict\'])\n    system.cpu()\n\n    to_save = system.model.serialize()\n    to_save.update(train_set.get_infos())\n    torch.save(to_save, os.path.join(exp_dir, \'best_model.pth\'))\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/wham/DPRNN/eval.py,2,"b'import os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid import DPRNNTasNet\nfrom asteroid.metrics import get_metrics\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.models import save_publishable\nfrom asteroid.utils import tensors_to_device\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the json files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=50,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model_path = os.path.join(conf[\'exp_dir\'], \'best_model.pth\')\n    model = DPRNNTasNet.from_pretrained(model_path)\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = WhamDataset(conf[\'test_dir\'], conf[\'task\'],\n                           sample_rate=conf[\'sample_rate\'],\n                           nondefault_nsrc=model.masker.n_src,\n                           segment=None)  # Uses all segment length\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix[None, None])\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n    model_dict = torch.load(model_path, map_location=\'cpu\')\n\n    publishable = save_publishable(\n        os.path.join(conf[\'exp_dir\'], \'publish_dir\'), model_dict,\n        metrics=final_results, train_conf=train_conf\n    )\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/wham/DPRNN/train.py,5,"b'import os\nimport argparse\nimport json\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid import DPRNNTasNet\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\n\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    train_set = WhamDataset(conf[\'data\'][\'train_dir\'], conf[\'data\'][\'task\'],\n                            sample_rate=conf[\'data\'][\'sample_rate\'], segment=conf[\'data\'][\'segment\'],\n                            nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n    val_set = WhamDataset(conf[\'data\'][\'valid_dir\'], conf[\'data\'][\'task\'],\n                          sample_rate=conf[\'data\'][\'sample_rate\'],\n                          nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update({\'n_src\': train_set.n_src})\n\n    model = DPRNNTasNet(**conf[\'filterbank\'], **conf[\'masknet\'])\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'ddp\',\n                         gradient_clip_val=conf[\'training\'][""gradient_clipping""])\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n    # Save best model (next PL version will make this easier)\n    best_path = [b for b, v in best_k.items() if v == min(best_k.values())][0]\n    state_dict = torch.load(best_path)\n    system.load_state_dict(state_dict=state_dict[\'state_dict\'])\n    system.cpu()\n\n    to_save = system.model.serialize()\n    to_save.update(train_set.get_infos())\n    torch.save(to_save, os.path.join(exp_dir, \'best_model.pth\'))\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/wham/DynamicMixing/eval.py,1,"b'import os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.metrics import get_metrics\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.utils import tensors_to_device\n\nfrom model import load_best_model\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the json files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=50,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model = load_best_model(conf[\'train_conf\'], conf[\'exp_dir\'])\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = WhamDataset(conf[\'test_dir\'], conf[\'task\'],\n                           sample_rate=conf[\'sample_rate\'],\n                           nondefault_nsrc=model.masker.n_src,\n                           segment=None)  # Uses all segment length\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix[None, None])\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/wham/DynamicMixing/model.py,1,"b'import json\nimport os\nimport torch\nfrom torch import nn\n\nimport asteroid.filterbanks as fb\nfrom asteroid import torch_utils\nfrom asteroid.masknn import DPRNN\nfrom asteroid.engine.optimizers import make_optimizer\n\n\nclass Model(nn.Module):\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        est_masks = self.masker(tf_rep)\n        masked_tf_rep = est_masks * tf_rep.unsqueeze(1)\n        return self.pad_output_to_inp(self.decoder(masked_tf_rep), x)\n\n    @staticmethod\n    def pad_output_to_inp(output, inp):\n        """""" Pad first argument to have same size as second argument""""""\n        inp_len = inp.size(-1)\n        output_len = output.size(-1)\n        return nn.functional.pad(output, [0, inp_len - output_len])\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    # Define building blocks for local model\n    enc, dec = fb.make_enc_dec(\'free\', **conf[\'filterbank\'])\n    masker = DPRNN(**conf[\'masknet\'])\n    model = Model(enc, masker, dec)\n    # Define optimizer of this model\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` there.\n\n    Returns:\n        nn.Module the best pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    # Last best model summary\n    with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n        best_k = json.load(f)\n    best_model_path = min(best_k, key=best_k.get)\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'],\n                                           model)\n    model.eval()\n    return model\n'"
egs/wham/DynamicMixing/train.py,3,"b'import os\nimport argparse\nimport json\n\nimport torch\nimport warnings\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\n\nfrom model import make_model_and_optimizer\n\n\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\nwarnings.simplefilter(""ignore"", UserWarning)\n\ndef main(conf):\n    if conf[""data""][""data_augmentation""]:\n        from local.augmented_wham import AugmentedWhamDataset\n        train_set = AugmentedWhamDataset(task=conf[\'data\'][\'task\'],\n                                         segment = conf[\'data\'][\'segment\'],\n                                         json_dir = conf[""data""][""train_dir""], sample_rate = conf[\'data\'][\'sample_rate\'],\n                                         nondefault_nsrc = conf[\'data\'][\'nondefault_nsrc\'], **conf[""augmentation""])\n    else:\n        train_set = WhamDataset(conf[\'data\'][\'train_dir\'], conf[\'data\'][\'task\'],\n                            sample_rate=conf[\'data\'][\'sample_rate\'], segment=conf[\'data\'][\'segment\'],\n                            nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n    val_set = WhamDataset(conf[\'data\'][\'valid_dir\'], conf[\'data\'][\'task\'],\n                          sample_rate=conf[\'data\'][\'sample_rate\'],\n                          nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update({\'n_src\': train_set.n_src})\n\n    # Define model and optimizer in a local function (defined in the recipe).\n    # Two advantages to this : re-instantiating the model and optimizer\n    # for retraining and evaluating is straight-forward.\n    model, optimizer = make_model_and_optimizer(conf)\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         gradient_clip_val=conf[\'training\'][""gradient_clipping""])\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/wham/FilterbankDesign/model.py,0,"b'\nfrom torch import nn\n\nimport asteroid.filterbanks as fb\nfrom asteroid.masknn import TDConvNet\nfrom asteroid.engine.optimizers import make_optimizer\n\n\nclass Model(nn.Module):\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n        self.inp_mode = encoder.inp_mode\n        self.mask_mode = encoder.mask_mode\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        # Encode the waveform\n        tf_rep = self.encoder(x)\n        # Post process TF representation (take magnitude or keep [Re, Im] etc)\n        # FIXME, that\'s removed\n        # masker_input = self.encoder.post_process_inputs(tf_rep)\n        masker_input = tf_rep\n        # Estimate masks (Size [batch, n_scr, bins, time])\n        est_masks = self.masker(masker_input)\n        # Apply mask to TF representation\n        masked_tf_reps = self.encoder.apply_mask(tf_rep.unsqueeze(1),\n                                                 est_masks, dim=2)\n        # Map back TF representation to time domain\n        return self.pad_output_to_inp(self.decoder(masked_tf_reps), x)\n\n    @staticmethod\n    def pad_output_to_inp(output, inp):\n        """""" Pad first argument to have same size as second argument""""""\n        inp_len = inp.size(-1)\n        output_len = output.size(-1)\n        return nn.functional.pad(output, [0, inp_len - output_len])\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    # Define building blocks for local model\n    # The encoder and decoder can directly be made from the dictionary.\n    encoder, decoder = fb.make_enc_dec(**conf[\'filterbank\'])\n\n    # The input post-processing changes the dimensions of input features to\n    # the mask network. Different type of masks impose different output\n    # dimensions to the mask network\'s output. We correct for these here.\n    nn_in = int(encoder.n_feats_out * encoder.in_chan_mul)\n    nn_out = int(encoder.n_feats_out * encoder.out_chan_mul)\n    masker = TDConvNet(in_chan=nn_in, out_chan=nn_out,\n                       **conf[\'masknet\'])\n    # Another possibility is to correct for these effects inside of Model,\n    # but then instantiation of masker should also be done inside.\n    model = Model(encoder, masker, decoder)\n\n    # The model is defined in Container, which is passed to DataParallel.\n\n    # Define optimizer : can be instantiate from dictonary as well.\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n'"
egs/wham/FilterbankDesign/train.py,2,"b""import os\nimport argparse\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom model import make_model_and_optimizer\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--exp_dir', default='exp/tmp',\n                    help='Full path to save best validation model')\n\n\ndef main(conf):\n    # from asteroid.data.toy_data import WavSet\n    # train_set = WavSet(n_ex=1000, n_src=2, ex_len=32000)\n    # val_set = WavSet(n_ex=1000, n_src=2, ex_len=32000)\n    # Define data pipeline\n    train_set = WhamDataset(conf['data']['train_dir'], conf['data']['task'],\n                            sample_rate=conf['data']['sample_rate'],\n                            nondefault_nsrc=conf['data']['nondefault_nsrc'])\n    val_set = WhamDataset(conf['data']['valid_dir'], conf['data']['task'],\n                          sample_rate=conf['data']['sample_rate'],\n                          nondefault_nsrc=conf['data']['nondefault_nsrc'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf['training']['batch_size'],\n                              num_workers=conf['training']['num_workers'])\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf['training']['batch_size'],\n                            num_workers=conf['training']['num_workers'])\n    conf['masknet'].update({'n_src': train_set.n_src})\n\n    # Define model and optimizer in a local function (defined in the recipe).\n    # Two advantages to this : re-instantiating the model and optimizer\n    # for retraining and evaluating is straight-forward.\n    model, optimizer = make_model_and_optimizer(conf)\n\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf['main_args']['exp_dir']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, 'conf.yml')\n    with open(conf_path, 'w') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from='pw_mtx')\n    # loss_class = PITLossContainer(pairwise_neg_sisdr, n_src=train_set.n_src)\n    # Checkpointing callback can monitor any quantity which is returned by\n    # validation step, defaults to val_loss here (see System).\n    checkpoint_dir = os.path.join(exp_dir, 'checkpoints/')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor='val_loss',\n                                 mode='min', save_best_only=False)\n    # New PL version will come the 7th of december / will have save_top_k\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    config=conf)\n    # Don't ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_nb_epochs=conf['training']['epochs'],\n                         checkpoint_callback=checkpoint,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend='dp')\n    trainer.fit(system)\n\n\nif __name__ == '__main__':\n    import yaml\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    with open('local/conf.yml') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n\n    # Arg_dic is a dictionary following the structure of `conf.yml`\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure.\n    main(arg_dic)\n"""
egs/wham/TwoStep/eval.py,1,"b'import os\nimport random\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom asteroid.metrics import get_metrics\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom asteroid.utils import tensors_to_device\n\nfrom model import load_best_separator_if_available\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the json files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=10,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model = load_best_separator_if_available(conf[\'train_conf\'])\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = WhamDataset(conf[\'test_dir\'], conf[\'task\'],\n                           sample_rate=conf[\'sample_rate\'],\n                           nondefault_nsrc=model.separator.n_sources,\n                           segment=None, normalize_audio=True)\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    cnt = 0\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix.unsqueeze(0))\n        min_len = min(est_sources.shape[-1], sources.shape[-1], mix.shape[-1])\n        est_sources = est_sources[..., :min_len]\n        mix, sources = mix[..., :min_len], sources[..., :min_len]\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n        cnt += 1\n        if cnt > 50:\n            break\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/wham/TwoStep/model.py,6,"b'import os\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid import torch_utils\n\nimport torch\nimport torch.nn as nn\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass TwoStepTDCN(nn.Module):\n    """"""\n        A time-dilated convolutional network (TDCN) similar to the initial\n        ConvTasNet architecture where the encoder and decoder have been\n        pre-trained separately. The TwoStepTDCN infers masks directly on the\n        latent space and works using an signal to distortion ratio (SDR) loss\n        directly on the ideal latent masks.\n        Adaptive basis encoder and decoder with inference of ideal masks.\n        Copied from: https://github.com/etzinis/two_step_mask_learning/\n\n        Args:\n            pretrained_filterbank: A pretrained encoder decoder like the one\n                implemented in asteroid.filterbanks.simple_adaptive\n            n_sources (int, optional): Number of masks to estimate.\n            n_blocks (int, optional): Number of convolutional blocks in each\n                repeat. Defaults to 8.\n            n_repeats (int, optional): Number of repeats. Defaults to 4.\n            bn_chan (int, optional): Number of channels after the bottleneck.\n            hid_chan (int, optional): Number of channels in the convolutional\n                blocks.\n            kernel_size (int, optional): Kernel size in convolutional blocks.\n                n_sources: The number of sources\n        References:\n            Tzinis, E., Venkataramani, S., Wang, Z., Subakan, Y. C., and\n            Smaragdis, P., ""Two-Step Sound Source Separation:\n            Training on Learned Latent Targets."" In Acoustics, Speech\n            and Signal Processing (ICASSP), 2020 IEEE International Conference.\n            https://arxiv.org/abs/1910.09804\n    """"""\n    def __init__(self, pretrained_filterbank,\n                 bn_chan=256, hid_chan=512, kernel_size=3, n_blocks=8,\n                 n_repeats=4, n_sources=2):\n        super(TwoStepTDCN, self).__init__()\n        try:\n            self.pretrained_filterbank = pretrained_filterbank\n            self.encoder = self.pretrained_filterbank.mix_encoder\n            self.decoder = self.pretrained_filterbank.decoder\n            self.fbank_basis = self.encoder.conv.out_channels\n            self.fbank_kernel_size = self.encoder.conv.kernel_size[0]\n\n            # Freeze the encoder and the decoder weights:\n            self.encoder.conv.weight.requires_grad = False\n            self.encoder.conv.bias.requires_grad = False\n            self.decoder.deconv.weight.requires_grad = False\n            self.decoder.deconv.bias.requires_grad = False\n        except Exception as e:\n            print(e)\n            raise ValueError(""Could not load features form the pretrained ""\n                             ""adaptive filterbank."")\n\n        self.n_blocks = n_blocks\n        self.n_repeats = n_repeats\n        self.bn_chan = bn_chan\n        self.hid_chan = hid_chan\n        self.kernel_size = kernel_size\n        self.n_sources = n_sources\n\n        # Norm before the rest, and apply one more dense layer\n        self.ln_in = nn.BatchNorm1d(self.fbank_basis)\n        # self.ln_in = GlobalLayerNorm(self.fbank_basis)\n        self.l1 = nn.Conv1d(in_channels=self.fbank_basis,\n                            out_channels=self.bn_chan,\n                            kernel_size=1)\n\n        # Separation module\n        self.separator = nn.Sequential(\n            *[SeparableDilatedConv1DBlock(in_chan=self.bn_chan,\n                                          hid_chan=self.hid_chan,\n                                          kernel_size=self.kernel_size,\n                                          dilation=2**d)\n              for _ in range(self.n_blocks) for d in range(self.n_repeats)])\n\n        # Masks layer\n        self.mask_layer = nn.Conv2d(\n            in_channels=1, out_channels=self.n_sources,\n            kernel_size=(self.fbank_basis + 1, 1),\n            padding=(self.fbank_basis - self.fbank_basis // 2, 0))\n\n        # Reshaping if needed\n        if self.bn_chan != self.fbank_basis:\n            self.out_reshape = nn.Conv1d(in_channels=self.bn_chan,\n                                         out_channels=self.fbank_basis,\n                                         kernel_size=1)\n        self.ln_mask_in = nn.BatchNorm1d(self.fbank_basis)\n        # self.ln_mask_in = GlobalLayerNorm(self.fbank_basis)\n\n    def forward(self, x):\n        # Front end\n        x = self.encoder(x)\n        encoded_mixture = x.clone()\n\n        # Separation module\n        x = self.ln_in(x)\n        x = self.l1(x)\n        x = self.separator(x)\n\n        if self.bn_chan != self.fbank_basis:\n            x = self.out_reshape(x)\n\n        x = self.ln_mask_in(x)\n        x = nn.functional.relu(x)\n        x = self.mask_layer(x.unsqueeze(1))\n        masks = nn.functional.softmax(x, dim=1)\n        return masks * encoded_mixture.unsqueeze(1)\n\n    def infer_source_signals(self, mixture_wav):\n        adfe_sources = self.forward(mixture_wav)\n        rec_wavs = self.decoder(adfe_sources.view(adfe_sources.shape[0],\n                                                  -1,\n                                                  adfe_sources.shape[-1]))\n        return rec_wavs\n\n\nclass SeparableDilatedConv1DBlock(nn.Module):\n    """"""One dimensional convolutional block, as proposed in [1] without skip\n        output. As used in the two step approach [2]. This block uses the\n        groupnorm across features and also produces always a padded output.\n\n    Args:\n        in_chan (int): Number of input channels.\n        hid_chan (int): Number of hidden channels in the depth-wise\n            convolution.\n        kernel_size (int): Size of the depth-wise convolutional kernel.\n        dilation (int): Dilation of the depth-wise convolution.\n\n    References:\n        [1]: ""Conv-TasNet: Surpassing ideal time-frequency magnitude masking\n             for speech separation"" TASLP 2019 Yi Luo, Nima Mesgarani\n             https://arxiv.org/abs/1809.07454\n        [2]: Tzinis, E., Venkataramani, S., Wang, Z., Subakan, Y. C., and\n            Smaragdis, P., ""Two-Step Sound Source Separation:\n            Training on Learned Latent Targets."" In Acoustics, Speech\n            and Signal Processing (ICASSP), 2020 IEEE International Conference.\n            https://arxiv.org/abs/1910.09804\n    """"""\n    def __init__(self, in_chan=256, hid_chan=512, kernel_size=3, dilation=1):\n        super(SeparableDilatedConv1DBlock, self).__init__()\n        self.module = nn.Sequential(\n            nn.Conv1d(in_channels=in_chan, out_channels=hid_chan,\n                      kernel_size=1),\n            nn.PReLU(),\n            nn.GroupNorm(1, hid_chan, eps=1e-08),\n            nn.Conv1d(in_channels=hid_chan, out_channels=hid_chan,\n                      kernel_size=kernel_size,\n                      padding=(dilation * (kernel_size - 1)) // 2,\n                      dilation=dilation, groups=hid_chan),\n            nn.PReLU(),\n            nn.GroupNorm(1, hid_chan, eps=1e-08),\n            nn.Conv1d(in_channels=hid_chan, out_channels=in_chan, kernel_size=1)\n        )\n\n    def forward(self, x):\n        """""" Input shape [batch, feats, seq]""""""\n        y = x.clone()\n        return x + self.module(y)\n\n\nclass AdaptiveEncoder1D(nn.Module):\n    """"""\n        A 1D convolutional block that transforms signal in wave form into higher\n        dimension.\n\n        Args:\n            input shape: [batch, 1, n_samples]\n            output shape: [batch, freq_res, n_samples//sample_res]\n            freq_res: number of output frequencies for the encoding convolution\n            sample_res: int, length of the encoding filter\n    """"""\n\n    def __init__(self, freq_res, sample_res):\n        super().__init__()\n        self.conv = nn.Conv1d(1,\n                              freq_res,\n                              sample_res,\n                              stride=sample_res // 2,\n                              padding=sample_res // 2)\n\n    def forward(self, s):\n        return F.relu(self.conv(s))\n\n\nclass AdaptiveDecoder1D(nn.Module):\n    """""" A 1D deconvolutional block that transforms encoded representation\n    into wave form.\n    input shape: [batch, freq_res, sample_res]\n    output shape: [batch, 1, sample_res*n_samples]\n    freq_res: number of output frequencies for the encoding convolution\n    sample_res: length of the encoding filter\n    """"""\n\n    def __init__(self, freq_res, sample_res, n_sources):\n        super().__init__()\n        self.deconv = nn.ConvTranspose1d(n_sources * freq_res,\n                                         n_sources,\n                                         sample_res,\n                                         padding=sample_res // 2,\n                                         stride=sample_res // 2,\n                                         groups=n_sources,\n                                         output_padding=(sample_res // 2) - 1)\n\n    def forward(self, x):\n        return self.deconv(x)\n\n\nclass AdaptiveEncoderDecoder(nn.Module):\n    """"""\n        Adaptive basis encoder and decoder with inference of ideal masks.\n        Copied from: https://github.com/etzinis/two_step_mask_learning/\n\n        Args:\n            freq_res: The number of frequency like representations\n            sample_res: The number of samples in kernel 1D convolutions\n            n_sources: The number of sources\n        References:\n            Tzinis, E., Venkataramani, S., Wang, Z., Subakan, Y. C., and\n            Smaragdis, P., ""Two-Step Sound Source Separation:\n            Training on Learned Latent Targets."" In Acoustics, Speech\n            and Signal Processing (ICASSP), 2020 IEEE International Conference.\n            https://arxiv.org/abs/1910.09804\n    """"""\n\n    def __init__(self,\n                 freq_res=256,\n                 sample_res=21,\n                 n_sources=2):\n        super().__init__()\n        self.freq_res = freq_res\n        self.sample_res = sample_res\n        self.mix_encoder = AdaptiveEncoder1D(freq_res, sample_res)\n        self.decoder = AdaptiveDecoder1D(freq_res, sample_res, n_sources)\n        self.n_sources = n_sources\n\n    def get_target_masks(self, clean_sources):\n        """"""\n        Get target masks for the given clean sources\n        :param clean_sources: [batch, n_sources, time_samples]\n        :return: Ideal masks for the given sources:\n        [batch, n_sources, time_samples//(sample_res // 2)]\n        """"""\n        enc_mask_list = [self.mix_encoder(clean_sources[:, i, :].unsqueeze(1))\n                         for i in range(self.n_sources)]\n        total_mask = torch.stack(enc_mask_list, dim=1)\n        return F.softmax(total_mask, dim=1)\n\n    def reconstruct(self, mixture):\n        enc_mixture = self.mix_encoder(mixture.unsqueeze(1))\n        return self.decoder(enc_mixture)\n\n    def get_encoded_sources(self, mixture, clean_sources):\n        enc_mixture = self.mix_encoder(mixture.unsqueeze(1))\n        enc_masks = self.get_target_masks(clean_sources)\n        s_recon_enc = enc_masks * enc_mixture.unsqueeze(1)\n        return s_recon_enc\n\n    def forward(self, mixture, clean_sources):\n        enc_mixture = self.mix_encoder(mixture.unsqueeze(1))\n        enc_masks = self.get_target_masks(clean_sources)\n\n        s_recon_enc = enc_masks * enc_mixture.unsqueeze(1)\n        recon_sources = self.decoder(s_recon_enc.view(s_recon_enc.shape[0],\n                                                      -1,\n                                                      s_recon_enc.shape[-1]))\n        return recon_sources, enc_masks\n\n\nclass Model(nn.Module):\n    def __init__(self, pretrained_filterbank, conf):\n        super().__init__()\n        self.pretrained_filterbank = pretrained_filterbank\n        self.separator = TwoStepTDCN(\n            pretrained_filterbank,\n            bn_chan=conf[\'masknet\'][\'bn_chan\'],\n            hid_chan=conf[\'masknet\'][\'hid_chan\'],\n            kernel_size=conf[\'masknet\'][\'conv_kernel_size\'],\n            n_blocks=conf[\'masknet\'][\'n_blocks\'],\n            n_repeats=conf[\'masknet\'][\'n_repeats\'],\n            n_sources=conf[\'masknet\'][\'n_src\'])\n\n    def get_ideal_targets(self, mixture, clean_sources):\n        """"""\n        Get the latent targets for all sources\n        :param mixture: Input mixture in time domain [batch, timesamples]\n        :param clean_sources: Clean sources that constitute to the mixture in\n            time domain [batch, n_sources, timesamples].\n        :return: Latent representations for the sources which can be used as\n            targets for training:\n            [batch, n_sources, timesamples//encoder_stride]\n        """"""\n        return self.pretrained_filterbank.get_encoded_sources(mixture,\n                                                              clean_sources)\n\n    def estimate_latent_representations(self, mixture):\n        return self.separator(mixture.unsqueeze(1))\n\n    def get_ideal_latent_targets(self, mixture, clean_sources):\n        return self.pretrained_filterbank.get_encoded_sources(mixture,\n                                                              clean_sources)\n\n    def forward(self, x):\n        # Infer sources in the time domain\n        return self.separator.infer_source_signals(x.unsqueeze(1))\n\n\ndef get_encoded_paths(conf, train_part=None):\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    N = conf[\'filterbank\'][\'n_filters\']\n    L = conf[\'filterbank\'][\'kernel_size\']\n    checkpoint_dir = os.path.join(\n        exp_dir, train_part, \'N_{}_L_{}_checkpoints/\'.format(N, L))\n    return exp_dir, checkpoint_dir\n\n\ndef load_best_separator_if_available(conf):\n    filterbank = load_best_filterbank_if_available(conf)\n    _, f_checkpoint_dir = get_encoded_paths(conf, \'filterbank\')\n    if filterbank is None:\n        raise FileNotFoundError(\n            \'There are no available filterbanks under: {}\\n\'\n            \'Consider training one using train.py.\'.format(f_checkpoint_dir)\n        )\n\n    exp_dir, checkpoint_dir = get_encoded_paths(conf, train_part=\'separator\')\n    model_available = False\n    if os.path.exists(checkpoint_dir):\n        available_models = [p for p in os.listdir(checkpoint_dir)\n                            if \'.ckpt\' in p]\n        if available_models:\n            model_available = True\n\n    if not model_available:\n        raise FileNotFoundError(\'There is no available separator model at: {}\'\n                                \'\'.format(checkpoint_dir))\n\n    model_path = os.path.join(checkpoint_dir, available_models[0])\n    print(\'Going to load from: {}\'.format(model_path))\n    checkpoint = torch.load(model_path, map_location=\'cpu\')\n    model_c, _ = make_model_and_optimizer(conf, model_part=\'separator\',\n                                          pretrained_filterbank=filterbank)\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'], model_c)\n    print(\'Successfully loaded separator from: {}\'.format(model_path))\n    return model\n\n\ndef load_best_filterbank_if_available(conf):\n    exp_dir, checkpoint_dir = get_encoded_paths(conf, train_part=\'filterbank\')\n\n    filterbank_available = False\n    if os.path.exists(checkpoint_dir):\n        available_filter_banks = [p for p in os.listdir(checkpoint_dir)\n                                  if \'.ckpt\' in p]\n        if available_filter_banks:\n           filterbank_available = True\n\n    if not filterbank_available:\n        return None\n\n    filterbank_path = os.path.join(checkpoint_dir, available_filter_banks[0])\n    print(\'Going to load from: {}\'.format(filterbank_path))\n    checkpoint = torch.load(filterbank_path, map_location=\'cpu\')\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update(\n        {\'n_src\': checkpoint[\'training_config\'][\'masknet\'][\'n_src\']})\n    filterbank, _ = make_model_and_optimizer(conf, model_part=\'filterbank\')\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'], filterbank)\n    print(\'Successfully loaded filterbank from: {}\'.format(filterbank_path))\n    return model\n\n\ndef make_model_and_optimizer(conf, model_part=\'filterbank\',\n                             pretrained_filterbank=None):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n        model_part: Either filterbank (in other words adaptive front-end and\n        back-end) or separator.\n        pretrained_filterbank: The separator needs a pre-trained filterbank\n            in order to be initialized appropriately.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    # Define building blocks for local model\n    if model_part == \'filterbank\':\n        model = AdaptiveEncoderDecoder(\n            freq_res=conf[\'filterbank\'][\'n_filters\'],\n            sample_res=conf[\'filterbank\'][\'kernel_size\'],\n            n_sources=conf[\'masknet\'][\'n_src\'])\n    elif model_part == \'separator\':\n        if pretrained_filterbank is None:\n            raise ValueError(\'A pretrained filterbank is required for the \'\n                             \'initialization of the separator.\')\n        model = Model(pretrained_filterbank, conf)\n    else:\n        raise ValueError(\'Part to train: {} is not available.\'.format(\n            model_part))\n    # Define optimizer of this model\n    optimizer = make_optimizer(\n        model.parameters(),\n        optimizer=conf[model_part + \'_training\'][model_part[0] + \'_optimizer\'],\n        lr=conf[model_part + \'_training\'][model_part[0] + \'_lr\'])\n    return model, optimizer\n'"
egs/wham/TwoStep/system.py,6,"b'from asteroid.engine.system import System as SystemCore\n\n\nclass SystemTwoStep(SystemCore):\n    """"""\n    Inherits from the core system class and overrides the methods for the\n    common steps as well the train and evaluation steps for the two-step\n    source separation.\n\n    Args:\n        model (torch.nn.Module): Instance of model.\n        optimizer (torch.optim.Optimizer): Instance or list of optimizers.\n        loss_func (callable): Loss function with signature\n            (est_targets, targets).\n        train_loader (torch.utils.data.DataLoader): Training dataloader.\n        val_loader (torch.utils.data.DataLoader): Validation dataloader.\n        scheduler (torch.optim.lr_scheduler._LRScheduler): Instance, or list\n            of learning rate schedulers.\n        config: Anything to be saved with the checkpoints during training.\n            The config dictionary to re-instantiate the run for example.\n        module (str):\n            \'separator\': The two step is used for training or evaluation the\n                         separation module only.\n            \'filterbank\': The two step approach is used for training only the\n                          adaptive encoder/decoder part or in other words the\n                          filterbank.\n            For more info take a look at method common_step_two_step_separtion()\n    .. note:: By default, `training_step` (used by `pytorch-lightning` in the\n        training loop) and `validation_step` (used for the validation loop)\n        share `common_step`. If you want different behavior for the training\n        loop and the validation loop, overwrite both `training_step` and\n        `validation_step` instead.\n    """"""\n\n    def __init__(self, model, optimizer, loss_func, train_loader,\n                 val_loader=None, scheduler=None, config=None,\n                 module=None):\n        super().__init__(model, optimizer, loss_func, train_loader,\n                         val_loader=val_loader, scheduler=scheduler,\n                         config=config)\n        assert module in [\'filterbank\', \'separator\'], \'If the two-step  \' \\\n            \'approach is used then either filterbank or separator has \' \\\n            \'to be used but got: {}\'.format(module)\n        self.module = module\n\n    def common_step(self, batch, train=True):\n        """""" Common forward step between training and validation.\n\n        The function of this method is to unpack the data given by the loader,\n        forward the batch through the model and compute the loss for the\n        separation module and the filterbank when the optimization process\n        for source separation is breaken in two distinct processes as\n        proposed in [1].\n\n        Args:\n            batch: the object returned by the loader (a list of torch.Tensor\n                in most cases) but can be something else.\n            train (bool): In case of training or validation the\n                filterbank will return the estimated time signals. However,\n                in training mode the separator will be trained using the\n                ideal latent targets and it will estimate the corresponding\n                latent representations of the sources as proposed in [1].\n        Returns:\n            dict:\n\n            ``\'loss\'``: loss\n\n            ``\'log\'``: dict with tensorboard logs\n\n        References:\n            [1]: Tzinis, E., Venkataramani, S., Wang, Z., Subakan, Y. C., and\n                 Smaragdis, P., ""Two-Step Sound Source Separation:\n                 Training on Learned Latent Targets."" In Acoustics, Speech\n                 and Signal Processing (ICASSP), 2020 IEEE International\n                 Conference. https://arxiv.org/abs/1910.09804\n        """"""\n        mixture_time, true_sources_time = batch\n        if self.module == \'filterbank\':\n            est_sources_time, _ = self(mixture_time, true_sources_time)\n            est_sources_time_mean = est_sources_time.mean(-1, keepdims=True)\n            true_sources_time_mean = true_sources_time.mean(-1, keepdims=True)\n            return self.loss_func(est_sources_time - est_sources_time_mean,\n                                  true_sources_time - true_sources_time_mean)\n\n        # Here we train or validate the separator. In training we need the\n        # latent targets to regress on. In validation we just provide the\n        # estimated time domain signals.\n        if train:\n            latent_targets = self.model.get_ideal_latent_targets(\n                mixture_time, true_sources_time)\n            est_latents = self.model.estimate_latent_representations(\n                mixture_time)\n            batch_size, n_sources = est_latents.shape[0], est_latents.shape[1]\n            # See section 2.2 of the paper\n            return self.loss_func(\n                est_latents.view(batch_size, n_sources, -1),\n                latent_targets.view(batch_size, n_sources, -1))\n        else:\n            est_sources_time = self.model(mixture_time)\n            est_sources_time_mean = est_sources_time.mean(-1, keepdims=True)\n            true_sources_time_mean = true_sources_time.mean(-1, keepdims=True)\n            return self.loss_func(est_sources_time - est_sources_time_mean,\n                                  true_sources_time - true_sources_time_mean)\n\n    def training_step(self, batch, batch_nb):\n        loss = self.common_step(batch, train=True)\n        tensorboard_logs = {\'train_loss\': loss}\n        return {\'loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_step(self, batch, batch_nb):\n        loss = self.common_step(batch, train=False)\n        return {\'val_loss\': loss}\n'"
egs/wham/TwoStep/train.py,3,"b'import os\nimport argparse\nimport json\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.data.wham_dataset import WhamDataset\nfrom system import SystemTwoStep\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr, PairwiseNegSDR\n\nfrom model import get_encoded_paths\nfrom model import load_best_filterbank_if_available\nfrom model import make_model_and_optimizer\n\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/model_logs\',\n                    help=\'Full path to save best validation model\')\n\n\ndef get_data_loaders(conf, train_part=\'filterbank\'):\n    train_set = WhamDataset(conf[\'data\'][\'train_dir\'], conf[\'data\'][\'task\'],\n                            sample_rate=conf[\'data\'][\'sample_rate\'],\n                            nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'],\n                            normalize_audio=True)\n    val_set = WhamDataset(conf[\'data\'][\'valid_dir\'], conf[\'data\'][\'task\'],\n                          sample_rate=conf[\'data\'][\'sample_rate\'],\n                          nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'],\n                          normalize_audio=True)\n\n    if train_part not in [\'filterbank\', \'separator\']:\n        raise ValueError(\'Part to train: {} is not available.\'.format(\n            train_part))\n\n    train_loader = DataLoader(train_set, shuffle=True, drop_last=True,\n                              batch_size=conf[train_part + \'_training\'][\n                                  train_part[0] + \'_batch_size\'],\n                              num_workers=conf[train_part + \'_training\'][\n                                  train_part[0] + \'_num_workers\'])\n    val_loader = DataLoader(val_set, shuffle=False, drop_last=True,\n                            batch_size=conf[train_part + \'_training\'][\n                                train_part[0] + \'_batch_size\'],\n                            num_workers=conf[train_part + \'_training\'][\n                                train_part[0] + \'_num_workers\'])\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update({\'n_src\': train_set.n_src})\n\n    return train_loader, val_loader\n\n\ndef train_model_part(conf, train_part=\'filterbank\', pretrained_filterbank=None):\n    train_loader, val_loader = get_data_loaders(conf, train_part=train_part)\n\n    # Define model and optimizer in a local function (defined in the recipe).\n    # Two advantages to this : re-instantiating the model and optimizer\n    # for retraining and evaluating is straight-forward.\n    model, optimizer = make_model_and_optimizer(\n        conf, model_part=train_part, pretrained_filterbank=pretrained_filterbank\n    )\n    # Define scheduler\n    scheduler = None\n    if conf[train_part + \'_training\'][train_part[0] + \'_half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir, checkpoint_dir = get_encoded_paths(conf, train_part)\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(PairwiseNegSDR(\'sisdr\', zero_mean=False),\n                               pit_from=\'pw_mtx\')\n    system = SystemTwoStep(model=model, loss_func=loss_func,\n                           optimizer=optimizer, train_loader=train_loader,\n                           val_loader=val_loader, scheduler=scheduler,\n                           config=conf, module=train_part)\n\n    # Define callbacks\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=1, verbose=1)\n    early_stopping = False\n    if conf[train_part + \'_training\'][train_part[0] + \'_early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(\n        max_nb_epochs=conf[train_part + \'_training\'][train_part[0] + \'_epochs\'],\n        checkpoint_callback=checkpoint,\n        early_stop_callback=early_stopping,\n        default_save_path=exp_dir,\n        gpus=gpus,\n        distributed_backend=\'dp\',\n        train_percent_check=1.0,  # Useful for fast experiment\n        gradient_clip_val=5.)\n    trainer.fit(system)\n\n    with open(os.path.join(checkpoint_dir, ""best_k_models.json""), ""w"") as file:\n        json.dump(checkpoint.best_k_models, file, indent=0)\n\n\ndef main(conf):\n    filterbank = load_best_filterbank_if_available(conf)\n    _, checkpoint_dir = get_encoded_paths(conf, \'filterbank\')\n    if filterbank is None:\n        print(\'There are no available filterbanks under: {}. Going to \'\n              \'training.\'.format(checkpoint_dir))\n        train_model_part(conf, train_part=\'filterbank\')\n        filterbank = load_best_filterbank_if_available(conf)\n    else:\n        print(\'Found available filterbank at: {}\'.format(checkpoint_dir))\n        if not conf[\'filterbank_training\'][\'reuse_pretrained_filterbank\']:\n            print(\'Refining filterbank...\')\n            train_model_part(conf, train_part=\'filterbank\')\n            filterbank = load_best_filterbank_if_available(conf)\n    train_model_part(conf, train_part=\'separator\',\n                     pretrained_filterbank=filterbank)\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/whamr/TasNet/eval.py,1,"b'import os\nimport random\nimport warnings\nimport soundfile as sf\nimport torch\nimport yaml\nimport json\nimport argparse\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pprint import pprint\n\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.metrics import get_metrics\nfrom asteroid.data import WhamRDataset\nfrom asteroid.utils import tensors_to_device\n\nfrom model import load_best_model\nwarnings.simplefilter(""ignore"", UserWarning)\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--task\', type=str, required=True,\n                    help=\'One of `enh_single`, `enh_both`, \'\n                         \'`sep_clean` or `sep_noisy`\')\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Test directory including the json files\')\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=50,\n                    help=\'Number of audio examples to save, -1 means all\')\n\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model = load_best_model(conf[\'train_conf\'], conf[\'exp_dir\'])\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = WhamRDataset(conf[\'test_dir\'], conf[\'task\'],\n                           sample_rate=conf[\'sample_rate\'],\n                           nondefault_nsrc=model.n_src,\n                           segment=None)  # Uses all segment length\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        est_sources = model(mix[None, None])\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n\n    if args.task != arg_dic[\'train_conf\'][\'data\'][\'task\']:\n        print(""Warning : the task used to test is different than ""\n              ""the one from training, be sure this is what you want."")\n\n    main(arg_dic)\n'"
egs/whamr/TasNet/model.py,3,"b'import json\nimport os\nimport torch\nfrom torch import nn\n\nfrom asteroid import torch_utils\nfrom asteroid import torch_utils\nfrom asteroid.filterbanks import Encoder, Decoder, FreeFB\nfrom asteroid.masknn.blocks import SingleRNN\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.masknn.norms import GlobLN\n\n\nclass TasNet(nn.Module):\n    """""" Some kind of TasNet, but not the original one\n    Differences:\n        - Overlap-add support (strided convolutions)\n        - No frame-wise normalization on the wavs\n        - GlobLN as bottleneck layer.\n        - No skip connection.\n\n    Args:\n        fb_conf (dict): see local/conf.yml\n        mask_conf (dict): see local/conf.yml\n    """"""\n    def __init__(self, fb_conf, mask_conf):\n        super().__init__()\n        self.n_src = mask_conf[\'n_src\']\n        self.n_filters = fb_conf[\'n_filters\']\n        # Create TasNet encoders and decoders (could use nn.Conv1D as well)\n        self.encoder_sig = Encoder(FreeFB(**fb_conf))\n        self.encoder_relu = Encoder(FreeFB(**fb_conf))\n        self.decoder = Decoder(FreeFB(**fb_conf))\n        self.bn_layer = GlobLN(fb_conf[\'n_filters\'])\n\n        # Create TasNet masker\n        self.masker = nn.Sequential(\n            SingleRNN(\'lstm\', fb_conf[\'n_filters\'],\n                      hidden_size=mask_conf[\'n_units\'],\n                      n_layers=mask_conf[\'n_layers\'],\n                      bidirectional=True,\n                      dropout=mask_conf[\'dropout\']),\n            nn.Linear(2 * mask_conf[\'n_units\'], self.n_src * self.n_filters),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encode(x)\n        to_sep = self.bn_layer(tf_rep)\n        est_masks = self.masker(to_sep.transpose(-1, -2)).transpose(-1, -2)\n        est_masks = est_masks.view(batch_size, self.n_src, self.n_filters, -1)\n        masked_tf_rep = tf_rep.unsqueeze(1) * est_masks\n        return torch_utils.pad_x_to_y(self.decoder(masked_tf_rep), x)\n\n    def encode(self, x):\n        relu_out = torch.relu(self.encoder_relu(x))\n        sig_out = torch.sigmoid(self.encoder_sig(x))\n        return sig_out * relu_out\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    model = TasNet(conf[\'filterbank\'], conf[\'masknet\'])\n    # Define optimizer of this model\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` there.\n\n    Returns:\n        nn.Module the best pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    # Last best model summary\n    with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n        best_k = json.load(f)\n    best_model_path = min(best_k, key=best_k.get)\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'],\n                                           model)\n    model.eval()\n    return model\n'"
egs/whamr/TasNet/train.py,3,"b'import os\nimport argparse\nimport json\nimport warnings\n\nimport torch\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom torch.utils.data import DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.data import WhamRDataset\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\n\nfrom model import make_model_and_optimizer\nwarnings.simplefilter(""ignore"", UserWarning)\n# Keys which are not in the conf.yml file can be added here.\n# In the hierarchical dictionary created when parsing, the key `key` can be\n# found at dic[\'main_args\'][key]\n\n# By default train.py will use all available GPUs. The `id` option in run.sh\n# will limit the number of available GPUs for train.py .\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    train_set = WhamRDataset(conf[\'data\'][\'train_dir\'], conf[\'data\'][\'task\'],\n                             sample_rate=conf[\'data\'][\'sample_rate\'],\n                             nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n    val_set = WhamRDataset(conf[\'data\'][\'valid_dir\'], conf[\'data\'][\'task\'],\n                           sample_rate=conf[\'data\'][\'sample_rate\'],\n                           nondefault_nsrc=conf[\'data\'][\'nondefault_nsrc\'])\n\n    train_loader = DataLoader(train_set, shuffle=True,\n                              batch_size=conf[\'training\'][\'batch_size\'],\n                              num_workers=conf[\'training\'][\'num_workers\'],\n                              drop_last=True)\n    val_loader = DataLoader(val_set, shuffle=False,\n                            batch_size=conf[\'training\'][\'batch_size\'],\n                            num_workers=conf[\'training\'][\'num_workers\'],\n                            drop_last=True)\n    # Update number of source values (It depends on the task)\n    conf[\'masknet\'].update({\'n_src\': train_set.n_src})\n\n    # Define model and optimizer in a local function (defined in the recipe).\n    # Two advantages to this : re-instantiating the model and optimizer\n    # for retraining and evaluating is straight-forward.\n    model, optimizer = make_model_and_optimizer(conf)\n    # Define scheduler\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n    # Just after instantiating, save the args. Easy loading in the future.\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define Loss function.\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n    system = System(model=model, loss_func=loss_func, optimizer=optimizer,\n                    train_loader=train_loader, val_loader=val_loader,\n                    scheduler=scheduler, config=conf)\n\n    # Define callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n\n    # Don\'t ask GPU if they are not available.\n    gpus = -1 if torch.cuda.is_available() else None\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=5.,)\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint as print\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    # We start with opening the config file conf.yml as a dictionary from\n    # which we can create parsers. Each top level key in the dictionary defined\n    # by the YAML file creates a group in the parser.\n    with open(\'./local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    # Arguments are then parsed into a hierarchical dictionary (instead of\n    # flat, as returned by argparse) to facilitate calls to the different\n    # asteroid methods (see in main).\n    # plain_args is the direct output of parser.parse_args() and contains all\n    # the attributes in an non-hierarchical structure. It can be useful to also\n    # have it so we included it here but it is not used.\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    print(arg_dic)\n    main(arg_dic)\n'"
egs/wsj0-mix/DeepClustering/eval.py,1,"b'import os\nimport json\nimport yaml\nimport argparse\nimport random\nimport torch\nfrom tqdm import tqdm\nimport pandas as pd\nimport soundfile as sf\nfrom pprint import pprint\n\nfrom asteroid.utils import tensors_to_device\nfrom asteroid.losses import PITLossWrapper, pairwise_neg_sisdr\nfrom asteroid.metrics import get_metrics\n\nfrom model import load_best_model\nfrom asteroid.data import Wsj0mixDataset\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--test_dir\', type=str, required=True,\n                    help=\'Local data directory with test files\')\nparser.add_argument(\'--n_src\', type=int, default=2)\nparser.add_argument(\'--use_gpu\', type=int, default=0,\n                    help=\'Whether to use the GPU for model execution\')\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Experiment root\')\nparser.add_argument(\'--n_save_ex\', type=int, default=-1,\n                    help=\'Number of audio examples to save, -1 means all\')\ncompute_metrics = [\'si_sdr\', \'sdr\', \'sir\', \'sar\', \'stoi\']\n\n\ndef main(conf):\n    model = load_best_model(conf[\'train_conf\'], conf[\'exp_dir\'])\n    # Handle device placement\n    if conf[\'use_gpu\']:\n        model.cuda()\n    model_device = next(model.parameters()).device\n    test_set = Wsj0mixDataset(conf[\'test_dir\'], n_src=conf[\'n_src\'],\n                              segment=None)\n    # Used to reorder sources only\n    loss_func = PITLossWrapper(pairwise_neg_sisdr, pit_from=\'pw_mtx\')\n\n    # Randomly choose the indexes of sentences to save.\n    ex_save_dir = os.path.join(conf[\'exp_dir\'], \'examples/\')\n    if conf[\'n_save_ex\'] == -1:\n        conf[\'n_save_ex\'] = len(test_set)\n    save_idx = random.sample(range(len(test_set)), conf[\'n_save_ex\'])\n    series_list = []\n    torch.no_grad().__enter__()\n    for idx in tqdm(range(len(test_set))):\n        # Forward the network on the mixture.\n        mix, sources = tensors_to_device(test_set[idx], device=model_device)\n        if conf[\'train_conf\'][\'training\'][\'loss_alpha\'] == 1:\n            # If Deep clustering only, use DC masks.\n            est_sources, dic_out = model.dc_head_separate(mix[None, None])\n        else:\n            # If Chimera, use mask-inference head masks\n            est_sources, dic_out = model.separate(mix[None, None])\n\n        loss, reordered_sources = loss_func(est_sources, sources[None],\n                                            return_est=True)\n        mix_np = mix[None].cpu().data.numpy()\n        sources_np = sources.cpu().data.numpy()\n        est_sources_np = reordered_sources.squeeze(0).cpu().data.numpy()\n        utt_metrics = get_metrics(mix_np, sources_np, est_sources_np,\n                                  sample_rate=conf[\'sample_rate\'],\n                                  metrics_list=compute_metrics)\n        utt_metrics[\'mix_path\'] = test_set.mix[idx][0]\n        series_list.append(pd.Series(utt_metrics))\n\n        # Save some examples in a folder. Wav files and metrics as text.\n        if idx in save_idx:\n            local_save_dir = os.path.join(ex_save_dir, \'ex_{}/\'.format(idx))\n            os.makedirs(local_save_dir, exist_ok=True)\n            sf.write(local_save_dir + ""mixture.wav"", mix_np[0],\n                     conf[\'sample_rate\'])\n            # Loop over the sources and estimates\n            for src_idx, src in enumerate(sources_np):\n                sf.write(local_save_dir + ""s{}.wav"".format(src_idx+1), src,\n                         conf[\'sample_rate\'])\n            for src_idx, est_src in enumerate(est_sources_np):\n                sf.write(local_save_dir + ""s{}_estimate.wav"".format(src_idx+1),\n                         est_src, conf[\'sample_rate\'])\n            # Write local metrics to the example folder.\n            with open(local_save_dir + \'metrics.json\', \'w\') as f:\n                json.dump(utt_metrics, f, indent=0)\n\n    # Save all metrics to the experiment folder.\n    all_metrics_df = pd.DataFrame(series_list)\n    all_metrics_df.to_csv(os.path.join(conf[\'exp_dir\'], \'all_metrics.csv\'))\n\n    # Print and save summary metrics\n    final_results = {}\n    for metric_name in compute_metrics:\n        input_metric_name = \'input_\' + metric_name\n        ldf = all_metrics_df[metric_name] - all_metrics_df[input_metric_name]\n        final_results[metric_name] = all_metrics_df[metric_name].mean()\n        final_results[metric_name + \'_imp\'] = ldf.mean()\n    print(\'Overall metrics :\')\n    pprint(final_results)\n    with open(os.path.join(conf[\'exp_dir\'], \'final_metrics.json\'), \'w\') as f:\n        json.dump(final_results, f, indent=0)\n\n\nif __name__ == \'__main__\':\n    args = parser.parse_args()\n    arg_dic = dict(vars(args))\n    # Load training config\n    conf_path = os.path.join(args.exp_dir, \'conf.yml\')\n    with open(conf_path) as f:\n        train_conf = yaml.safe_load(f)\n\n    arg_dic[\'sample_rate\'] = train_conf[\'data\'][\'sample_rate\']\n    arg_dic[\'train_conf\'] = train_conf\n    main(arg_dic)\n'"
egs/wsj0-mix/DeepClustering/model.py,5,"b'import json\nimport os\nimport torch\nfrom torch import nn\nfrom sklearn.cluster import KMeans\n\nfrom asteroid import torch_utils\nimport asteroid.filterbanks as fb\nfrom asteroid.engine.optimizers import make_optimizer\nfrom asteroid.filterbanks.transforms import take_mag, apply_mag_mask, ebased_vad\nfrom asteroid.masknn.blocks import SingleRNN\nfrom asteroid.utils.torch_utils import pad_x_to_y\n\nEPS = 1e-8\n\n\ndef make_model_and_optimizer(conf):\n    """""" Function to define the model and optimizer for a config dictionary.\n    Args:\n        conf: Dictionary containing the output of hierachical argparse.\n    Returns:\n        model, optimizer.\n    The main goal of this function is to make reloading for resuming\n    and evaluation very simple.\n    """"""\n    enc, dec = fb.make_enc_dec(\'stft\', **conf[\'filterbank\'])\n    masker = Chimera(enc.n_feats_out // 2,\n                     **conf[\'masknet\'])\n    model = Model(enc, masker, dec)\n    optimizer = make_optimizer(model.parameters(), **conf[\'optim\'])\n    return model, optimizer\n\n\nclass Chimera(nn.Module):\n    def __init__(self, in_chan, n_src, rnn_type=\'lstm\', n_layers=2,\n                 hidden_size=600, bidirectional=True, dropout=0.3,\n                 embedding_dim=20, take_log=False):\n        super().__init__()\n        self.input_dim = in_chan\n        self.n_src = n_src\n        self.take_log = take_log\n        # RNN common\n        self.embedding_dim = embedding_dim\n        self.rnn = SingleRNN(rnn_type, in_chan, hidden_size,\n                             n_layers=n_layers, dropout=dropout,\n                             bidirectional=bidirectional)\n        self.dropout = nn.Dropout(dropout)\n        rnn_out_dim = hidden_size * 2 if bidirectional else hidden_size\n        # Mask heads\n        self.mask_layer = nn.Linear(rnn_out_dim, in_chan * self.n_src)\n        self.mask_act = nn.Sigmoid()  # sigmoid or relu or softmax\n        # DC head\n        self.embedding_layer = nn.Linear(rnn_out_dim, in_chan * embedding_dim)\n        self.embedding_act = nn.Tanh()  # sigmoid or tanh\n\n    def forward(self, input_data):\n        batch, _, n_frames = input_data.shape\n        if self.take_log:\n            input_data = torch.log(input_data + EPS)\n        # Common net\n        out = self.rnn(input_data.permute(0, 2, 1))\n        out = self.dropout(out)\n\n        # DC head\n        proj = self.embedding_layer(out)  # batch, time, freq * emb\n        proj = self.embedding_act(proj)\n        proj = proj.view(batch, n_frames, -1, self.embedding_dim).transpose(1, 2)\n        # (batch, freq * frames, emb)\n        proj = proj.reshape(batch, -1, self.embedding_dim)\n        proj_norm = torch.norm(proj, p=2, dim=-1, keepdim=True)\n        projection_final = proj / (proj_norm + EPS)\n\n        # Mask head\n        mask_out = self.mask_layer(out).view(batch, n_frames,\n                                             self.n_src, self.input_dim)\n        mask_out = mask_out.permute(0, 2, 3, 1)\n        mask_out = self.mask_act(mask_out)\n        return projection_final, mask_out\n\n\nclass Model(nn.Module):\n    def __init__(self, encoder, masker, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.masker = masker\n        self.decoder = decoder\n\n    def forward(self, x):\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        final_proj, mask_out = self.masker(take_mag(tf_rep))\n        return final_proj, mask_out\n\n    def separate(self, x):\n        """""" Separate with mask-inference head, output waveforms """"""\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        proj, mask_out = self.masker(take_mag(tf_rep))\n        masked = apply_mag_mask(tf_rep.unsqueeze(1), mask_out)\n        wavs = torch_utils.pad_x_to_y(self.decoder(masked), x)\n        dic_out = dict(tfrep=tf_rep, mask=mask_out, masked_tfrep=masked,\n                       proj=proj)\n        return wavs, dic_out\n\n    def dc_head_separate(self, x):\n        """""" Cluster embeddings to produce binary masks, output waveforms """"""\n        kmeans = KMeans(n_clusters=self.masker.n_src)\n        if len(x.shape) == 2:\n            x = x.unsqueeze(1)\n        tf_rep = self.encoder(x)\n        mag_spec = take_mag(tf_rep)\n        proj, mask_out = self.masker(mag_spec)\n        active_bins = ebased_vad(mag_spec)\n        active_proj = proj[active_bins.view(1, -1)]\n        #\n        bin_clusters = kmeans.fit_predict(active_proj.cpu().data.numpy())\n        # Create binary masks\n        est_mask_list = []\n        for i in range(self.masker.n_src):\n            # Add ones in all inactive bins in each mask.\n            mask = ~active_bins\n            mask[active_bins] = torch.from_numpy((bin_clusters == i)).to(mask.device)\n            est_mask_list.append(mask.float())  # Need float, not bool\n        # Go back to time domain\n        est_masks = torch.stack(est_mask_list, dim=1)\n        masked = apply_mag_mask(tf_rep, est_masks)\n        wavs = pad_x_to_y(self.decoder(masked), x)\n        dic_out = dict(tfrep=tf_rep, mask=mask_out, masked_tfrep=masked,\n                       proj=proj)\n        return wavs, dic_out\n\n\ndef load_best_model(train_conf, exp_dir):\n    """""" Load best model after training.\n\n    Args:\n        train_conf (dict): dictionary as expected by `make_model_and_optimizer`\n        exp_dir(str): Experiment directory. Expects to find\n            `\'best_k_models.json\'` of `checkpoints` directory in it.\n\n    Returns:\n        nn.Module the best (or last) pretrained model according to the val_loss.\n    """"""\n    # Create the model from recipe-local function\n    model, _ = make_model_and_optimizer(train_conf)\n    try:\n        # Last best model summary\n        with open(os.path.join(exp_dir, \'best_k_models.json\'), ""r"") as f:\n            best_k = json.load(f)\n        best_model_path = min(best_k, key=best_k.get)\n    except FileNotFoundError:\n        # Get last checkpoint\n        all_ckpt = os.listdir(os.path.join(exp_dir, \'checkpoints/\'))\n        all_ckpt.sort()\n        best_model_path = os.path.join(exp_dir, \'checkpoints\', all_ckpt[-1])\n    # Load checkpoint\n    checkpoint = torch.load(best_model_path, map_location=\'cpu\')\n    # Load state_dict into model.\n    model = torch_utils.load_state_dict_in(checkpoint[\'state_dict\'], model)\n    model.eval()\n    return model\n'"
egs/wsj0-mix/DeepClustering/train.py,12,"b'import os\nimport argparse\nimport json\nimport torch\nfrom torch import nn\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom asteroid.engine.system import System\nfrom asteroid.losses import PITLossWrapper, pairwise_mse\nfrom asteroid.losses import deep_clustering_loss\nfrom asteroid.filterbanks.transforms import take_mag, ebased_vad\n\nfrom asteroid.data.wsj0_mix import make_dataloaders\nfrom model import make_model_and_optimizer\n\nEPS = 1e-8\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--exp_dir\', default=\'exp/tmp\',\n                    help=\'Full path to save best validation model\')\n\n\ndef main(conf):\n    exp_dir = conf[\'main_args\'][\'exp_dir\']\n    # Define Dataloader\n    train_loader, val_loader = make_dataloaders(**conf[\'data\'],\n                                                **conf[\'training\'])\n    conf[\'masknet\'].update({\'n_src\': conf[\'data\'][\'n_src\']})\n    # Define model, optimizer + scheduler\n    model, optimizer = make_model_and_optimizer(conf)\n    scheduler = None\n    if conf[\'training\'][\'half_lr\']:\n        scheduler = ReduceLROnPlateau(optimizer=optimizer, factor=0.5,\n                                      patience=5)\n\n    # Save config\n    os.makedirs(exp_dir, exist_ok=True)\n    conf_path = os.path.join(exp_dir, \'conf.yml\')\n    with open(conf_path, \'w\') as outfile:\n        yaml.safe_dump(conf, outfile)\n\n    # Define loss function\n    loss_func = ChimeraLoss(alpha=conf[\'training\'][\'loss_alpha\'])\n    # Put together in System\n    system = ChimeraSystem(model=model, loss_func=loss_func,\n                           optimizer=optimizer, train_loader=train_loader,\n                           val_loader=val_loader, scheduler=scheduler,\n                           config=conf)\n\n    # Callbacks\n    checkpoint_dir = os.path.join(exp_dir, \'checkpoints/\')\n    checkpoint = ModelCheckpoint(checkpoint_dir, monitor=\'val_loss\',\n                                 mode=\'min\', save_top_k=5, verbose=1)\n    early_stopping = False\n    if conf[\'training\'][\'early_stop\']:\n        early_stopping = EarlyStopping(monitor=\'val_loss\', patience=10,\n                                       verbose=1)\n    gpus=-1\n    # Don\'t ask GPU if they are not available.\n    if not torch.cuda.is_available():\n        print(\'No available GPU were found, set gpus to None\')\n        gpus = None\n\n    # Train model\n    trainer = pl.Trainer(max_nb_epochs=conf[\'training\'][\'epochs\'],\n                         checkpoint_callback=checkpoint,\n                         early_stop_callback=early_stopping,\n                         default_save_path=exp_dir,\n                         gpus=gpus,\n                         distributed_backend=\'dp\',\n                         train_percent_check=1.0,  # Useful for fast experiment\n                         gradient_clip_val=200,)\n    trainer.fit(system)\n\n    best_k = {k: v.item() for k, v in checkpoint.best_k_models.items()}\n    with open(os.path.join(exp_dir, ""best_k_models.json""), ""w"") as f:\n        json.dump(best_k, f, indent=0)\n    # Save last model for convenience\n    torch.save(system.model.state_dict(),\n               os.path.join(exp_dir, \'checkpoints/final.pth\'))\n\n\nclass ChimeraSystem(System):\n    def __init__(self, *args, mask_mixture=True, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.mask_mixture = mask_mixture\n\n    def common_step(self, batch, batch_nb, train=False):\n        inputs, targets, masks = self.unpack_data(batch)\n        embeddings, est_masks = self(inputs)\n        spec = take_mag(self.model.encoder(inputs.unsqueeze(1)))\n        if self.mask_mixture:\n            est_masks = est_masks * spec.unsqueeze(1)\n            masks = masks * spec.unsqueeze(1)\n        loss, loss_dic = self.loss_func(embeddings, targets, est_src=est_masks,\n                                        target_src=masks, mix_spec=spec)\n        return loss, loss_dic\n\n    def training_step(self, batch, batch_nb):\n        loss, loss_dic = self.common_step(batch, batch_nb, train=True)\n        tensorboard_logs = dict(train_loss=loss,\n                                train_dc_loss=loss_dic[\'dc_loss\'],\n                                train_pit_loss=loss_dic[\'pit_loss\'])\n        return {\'loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_step(self, batch, batch_nb):\n        loss, loss_dic = self.common_step(batch, batch_nb, train=False)\n        tensorboard_logs = dict(val_loss=loss,\n                                val_dc_loss=loss_dic[\'dc_loss\'],\n                                val_pit_loss=loss_dic[\'pit_loss\'])\n        return {\'val_loss\': loss, \'log\': tensorboard_logs}\n\n    def validation_end(self, outputs):\n        # Not so pretty for now but it helps.\n        avg_loss = torch.stack([x[\'val_loss\']\n                                for x in outputs]).mean()\n        avg_dc_loss = torch.stack([x[\'log\'][\'val_dc_loss\']\n                                   for x in outputs]).mean()\n        avg_pit_loss = torch.stack([x[\'log\'][\'val_pit_loss\']\n                                    for x in outputs]).mean()\n        tensorboard_logs = dict(val_loss=avg_loss,\n                                val_dc_loss=avg_dc_loss,\n                                val_pit_loss=avg_pit_loss)\n        return {\'val_loss\': avg_loss, \'log\': tensorboard_logs,\n                \'progress_bar\': {\'val_loss\': avg_loss}}\n\n    def unpack_data(self, batch):\n        mix, sources = batch\n        # Compute magnitude spectrograms and IRM\n        src_mag_spec = take_mag(self.model.encoder(sources))\n        real_mask = src_mag_spec / (src_mag_spec.sum(1, keepdim=True) + EPS)\n        # Get the src idx having the maximum energy\n        binary_mask = real_mask.argmax(1)\n        return mix, binary_mask, real_mask\n\n\nclass ChimeraLoss(nn.Module):\n    """""" Combines Deep clustering loss and mask inference loss for ChimeraNet.\n\n    Args:\n        alpha (float): loss weight. Total loss will be :\n            `alpha` * dc_loss + (1 - `alpha`) * mask_mse_loss.\n    """"""\n    def __init__(self, alpha=0.1):\n        super().__init__()\n        assert alpha >= 0, ""Negative alpha values don\'t make sense.""\n        assert alpha <= 1, ""Alpha values above 1 don\'t make sense.""\n        # PIT loss\n        self.src_mse = PITLossWrapper(pairwise_mse, pit_from=\'pw_mtx\')\n        self.alpha = alpha\n\n    def forward(self, est_embeddings, target_indices, est_src=None,\n                target_src=None, mix_spec=None):\n        """"""\n\n        Args:\n            est_embeddings (torch.Tensor): Estimated embedding from the DC head.\n            target_indices (torch.Tensor): Target indices that\'ll be passed to\n                the DC loss.\n            est_src (torch.Tensor): Estimated magnitude spectrograms (or masks).\n            target_src (torch.Tensor): Target magnitude spectrograms (or masks).\n            mix_spec (torch.Tensor): The magnitude spectrogram of the mixture\n                from which VAD will be computed. If None, no VAD is used.\n\n        Returns:\n            torch.Tensor, the total loss, averaged over the batch.\n            dict with `dc_loss` and `pit_loss` keys, unweighted losses.\n        """"""\n        if self.alpha != 0 and (est_src is None or target_src is None):\n            raise ValueError(\'Expected target and estimated spectrograms to \'\n                             \'compute the PIT loss, found None.\')\n        binary_mask = None\n        if mix_spec is not None:\n            binary_mask = ebased_vad(mix_spec)\n        # Dc loss is already divided by VAD in the loss function.\n        dc_loss = deep_clustering_loss(embedding=est_embeddings,\n                                       tgt_index=target_indices,\n                                       binary_mask=binary_mask)\n        src_pit_loss = self.src_mse(est_src, target_src)\n        # Equation (4) from Chimera paper.\n        tot = self.alpha * dc_loss.mean() + (1 - self.alpha) * src_pit_loss\n        # Return unweighted losses as well for logging.\n        loss_dict = dict(dc_loss=dc_loss.mean(),\n                         pit_loss=src_pit_loss)\n        return tot, loss_dict\n\n\nif __name__ == \'__main__\':\n    import yaml\n    from pprint import pprint\n    from asteroid.utils import prepare_parser_from_dict, parse_args_as_dict\n\n    with open(\'local/conf.yml\') as f:\n        def_conf = yaml.safe_load(f)\n    parser = prepare_parser_from_dict(def_conf, parser=parser)\n    arg_dic, plain_args = parse_args_as_dict(parser, return_plain_args=True)\n    pprint(arg_dic)\n    main(arg_dic)\n'"
egs/avspeech/looking-to-listen/local/__init__.py,0,"b'from .postprocess import filter_audio, shelf\n\n'"
egs/avspeech/looking-to-listen/train/__init__.py,0,"b'from .config import ParamConfig\nfrom .metric_utils import snr, sdr\nfrom .callbacks import SNRCallback, SDRCallback\nfrom .trainer import train\n'"
egs/avspeech/looking-to-listen/train/callbacks.py,0,"b'import sys\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom catalyst.dl.core import Callback, MetricCallback, CallbackOrder\n\nfrom train import snr, sdr\n\n\nclass SNRCallback(MetricCallback):\n    """"""SNR callback.\n\n    Args:\n        input_key (str): input key to use for dice calculation;\n            specifies our y_true.\n        output_key (str): output key to use for dice calculation;\n            specifies our y_pred.\n\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""snr"",\n        mixed_audio_key: str=""input_audio""\n    ):\n        self.mixed_audio_key = mixed_audio_key\n        super().__init__(\n            prefix=prefix,\n            metric_fn=snr,\n            input_key=input_key,\n            output_key=output_key\n        )\n\n    def on_batch_end(self, state):\n        output_audios = state.output[self.output_key]\n        true_audios = state.input[self.input_key]\n\n        if hasattr(state.model, ""module""):\n            num_person = state.model.module.num_person\n        else:\n            num_person = state.model.num_person\n\n        avg_snr = 0\n        for n in range(num_person):\n            output_audio = output_audios[:, n, ...]\n            true_audio = true_audios[:, n, ...]\n\n            snr_value = snr(output_audio, true_audio).item()\n            avg_snr += snr_value\n\n        avg_snr /= num_person\n        state.metrics.add_batch_value(name=self.prefix, value=avg_snr)\n\n\nclass SDRCallback(MetricCallback):\n    """"""SDR callback.\n\n    Args:\n        input_key (str): input key to use for dice calculation;\n            specifies our y_true.\n        output_key (str): output key to use for dice calculation;\n            specifies our y_pred.\n\n    """"""\n\n    def __init__(\n        self,\n        input_key: str = ""targets"",\n        output_key: str = ""logits"",\n        prefix: str = ""sdr"",\n        mixed_audio_key: str=""input_audio""\n    ):\n        self.mixed_audio_key = mixed_audio_key\n        super().__init__(\n            prefix=prefix,\n            metric_fn=snr,\n            input_key=input_key,\n            output_key=output_key\n        )\n\n    def on_batch_end(self, state):\n        output_audios = state.output[self.output_key]\n        true_audios = state.input[self.input_key]\n\n        num_person = state.model.num_person\n        batch = output_audios.shape[0]\n\n        avg_sdr = 0\n        for n in range(batch):\n            output_audio = output_audios[n, ...]\n            true_audio = true_audios[n, ...]\n\n            output_audio = output_audio.detach().cpu().numpy()\n            true_audio = true_audio.detach().cpu().numpy()\n\n            sdr_value = sdr(output_audio, true_audio)\n            sdr_value = np.mean(sdr_value)\n            avg_sdr += sdr_value\n\n        avg_sdr /= batch\n        state.metrics.add_batch_value(name=self.prefix, value=avg_sdr)\n'"
egs/avspeech/looking-to-listen/train/config.py,0,"b'#use dataclass if >=python3.7\nclass ParamConfig:\n\n    def __init__(self, batch_size, epochs, workers, cuda, use_half, learning_rate):\n        self.batch_size = batch_size\n        self.epochs = epochs\n        self.workers = workers\n        self.cuda = cuda\n        self.use_half = use_half\n        self.learning_rate = learning_rate\n'"
egs/avspeech/looking-to-listen/train/metric_utils.py,7,"b'import sys\nimport torch\nimport mir_eval\nimport numpy as np\nfrom asteroid.data.avspeech_dataset import AVSpeechDataset\n\n\ndef snr(pred_signal: torch.Tensor, true_signal: torch.Tensor) -> torch.FloatTensor:\n    """"""\n        Calculate the Signal-to-Noise Ratio\n        from two signals\n\n        Args:\n            pred_signal (torch.Tensor): predicted signal spectrogram.\n            true_signal (torch.Tensor): original signal spectrogram.\n\n    """"""\n    inter_signal = true_signal - pred_signal\n\n    true_power = (true_signal ** 2).sum()\n    inter_power = (inter_signal ** 2).sum()\n\n    snr = 10*torch.log10(true_power / inter_power)\n\n    return snr\n\ndef sdr(pred_signal: torch.Tensor, true_signal: torch.Tensor) -> torch.FloatTensor:\n    """"""\n        Calculate the Signal-to-Distortion Ratio\n        from two signals\n\n        Args:\n            pred_signal (torch.Tensor): predicted signal spectrogram.\n            true_signal (torch.Tensor): original signal spectrogram.\n\n    """"""\n    n_sources = pred_signal.shape[0]\n\n    y_pred_wav = np.zeros((n_sources, 48_000))\n    y_wav = np.zeros((n_sources, 48_000))\n\n    for i in range(n_sources):\n        y_pred_wav[i] = AVSpeechDataset.decode(pred_signal[i, ...]).numpy()\n        y_wav[i] = AVSpeechDataset.decode(true_signal[i, ...]).numpy()\n    sdr, sir, sar, _ = mir_eval.separation.bss_eval_sources(y_wav, y_pred_wav)\n\n    return sdr\n\n'"
egs/avspeech/looking-to-listen/train/trainer.py,4,"b'from typing import Union\nimport torch\nimport collections\nfrom pathlib import Path\nfrom catalyst.dl import utils\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import EarlyStoppingCallback\nfrom catalyst.dl.callbacks.scheduler import SchedulerCallback\n\nfrom train import ParamConfig\nfrom train import SNRCallback\n\n\ndef train(model: torch.nn.Module, dataset: torch.utils.data.Dataset,\n          optimizer: torch.optim.Optimizer, criterion: torch.nn.Module,\n          config: ParamConfig, val_dataset: torch.utils.data.Dataset=None,\n          logdir: str=""./logdir"", resume: Union[str, None]=""logdir/checkpoints/best_full.pth"") -> None:\n    """"""\n        train the model with specified paremeters\n        Args:\n            model: neural network model\n            dataset: training dataset\n            optimizer: optimizer\n            criterion: loss function\n            val_dataset: validation dataset\n            logdir: logdir location to save checkpoints\n            resume: path where the partially trained model is stored\n    """"""\n\n    loaders = collections.OrderedDict()\n    train_loader = utils.get_loader(dataset,\n                                    open_fn=lambda x: {""input_audio"": x[-1],\n                                                       ""input_video"": x[1],\n                                                       ""targets"": x[0]},\n                                    batch_size=config.batch_size,\n                                    num_workers=config.workers, shuffle=True)\n    val_loader = utils.get_loader(val_dataset,\n                                  open_fn=lambda x: {""input_audio"": x[-1],\n                                                     ""input_video"": x[1],\n                                                     ""targets"": x[0]},\n                                  batch_size=config.batch_size,\n                                  num_workers=config.workers, shuffle=True)\n    loaders = {""train"": train_loader, ""valid"": val_loader}\n\n    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=config.learning_rate,\n                                                  max_lr=config.learning_rate * 10,\n                                                  step_size_up=4*len(train_loader),\n                                                  mode=""triangular"", cycle_momentum=False)\n\n    runner = SupervisedRunner(input_key=[""input_audio"", ""input_video""])\n    runner.train(model=model, criterion=criterion,\n                 optimizer=optimizer, scheduler=scheduler,\n                 loaders=loaders, logdir=logdir, verbose=True,\n                 num_epochs=config.epochs, resume=resume,\n                 callbacks=collections.OrderedDict({""snr_callback"": SNRCallback(),\n                                                    ""sched_callback"": SchedulerCallback(mode=""batch"")})\n                 )\n'"
egs/dns_challenge/baseline/local/preprocess_dns.py,0,"b'import argparse\nimport glob\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_dns(in_dir, out_dir=\'./data\'):\n    """""" Create json file from dataset folder.\n\n    Args:\n        in_dir (str): Location of the DNS data\n        out_dir (str): Where to save the json files.\n    """"""\n    # Get all file ids\n    clean_wavs = glob.glob(os.path.join(in_dir, \'clean/*.wav\'))\n    clean_dic = make_wav_id_dict(clean_wavs)\n\n    mix_wavs = glob.glob(os.path.join(in_dir, \'noisy/*.wav\'))\n    mix_dic = make_wav_id_dict(mix_wavs)\n\n    noise_wavs = glob.glob(os.path.join(in_dir, \'noise/*.wav\'))\n    noise_dic = make_wav_id_dict(noise_wavs)\n    assert clean_dic.keys() == mix_dic.keys() == noise_dic.keys()\n    file_infos = {k: dict(\n        mix=mix_dic[k],\n        clean=clean_dic[k],\n        noise=noise_dic[k],\n        snr=get_snr_from_mix_path(mix_dic[k]),\n        file_len=len(sf.SoundFile(mix_dic[k]))\n    ) for k in clean_dic.keys()}\n\n    # Save to JSON\n    with open(os.path.join(out_dir, \'file_infos.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=2)\n\n\ndef make_wav_id_dict(file_list):\n    """"""\n    Args:\n        file_list(List[str]): List of DNS challenge filenames.\n\n    Returns:\n        dict: Look like {file_id: filename, ...}\n    """"""\n    return {get_file_id(fp): fp for fp in file_list}\n\n\ndef get_file_id(fp):\n    """""" Split string to get wave id in DNS challenge dataset.""""""\n    return fp.split(\'_\')[-1].split(\'.\')[0]\n\n\ndef get_snr_from_mix_path(mix_path):\n    """""" Retrieves mixing SNR from mixture filename.\n\n    Args:\n        mix_path (str): Path to the mixture. Something like :\n        book_11346_chp_0012_reader_08537_8_kFu2mH7D77k-5YOmLILWHyg-\\\n        gWMWteRIgiw_snr6_tl-35_fileid_3614.wav\n\n    Returns:\n        int or None: the SNR value if we could parse it.\n    """"""\n    snr_str = mix_path.split(\'snr\')[-1].split(\'_\')[0]\n    try:\n        snr = int(snr_str)\n    except ValueError:\n        snr = None\n    return snr\n\n\nif __name__ == \'__main__\':\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--data_dir\', required=True, help=\'Location of data\')\n    parser.add_argument(\'--json_dir\', default=\'./data\',\n                        help=\'Where to save the json file\')\n\n    args = parser.parse_args()\n\n    os.makedirs(args.json_dir, exist_ok=True)\n    preprocess_dns(args.data_dir, args.json_dir)\n'"
egs/kinect-wsj/DeepClustering/local/preprocess_kinect_wsj.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix\'] + [f""s{n+1}"" for n in range(inp_args.n_src)]\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""Kinect-WSJ data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--n_src\', type=int, default=2,\n                        help=\'Number of sources in wsj0-mix\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/librimix/ConvTasNet/local/create_local_metadata.py,0,"b'import os\nimport shutil\nimport argparse\nfrom glob import glob\n\n# Command line arguments\nparser = argparse.ArgumentParser()\nparser.add_argument(\'--librimix_dir\', type=str, default=None,\n                    help=\'Path to librispeech root directory\')\n\n\ndef main(args):\n    librimix_dir = args.librimix_dir\n    create_local_metadata(librimix_dir)\n\n\ndef create_local_metadata(librimix_dir):\n\n    md_dirs = [f for f in glob(os.path.join(librimix_dir, \'*/*/*\'))\n               if f.endswith(\'metadata\')]\n    for md_dir in md_dirs:\n        md_files = [f for f in os.listdir(md_dir) if f.startswith(\'mix\')]\n        for md_file in md_files:\n            subset = md_file.split(\'_\')[1]\n            local_path = os.path.join(\n                \'data\', os.path.relpath(md_dir,librimix_dir), subset).replace(\n                \'/metadata\', """")\n            os.makedirs(local_path, exist_ok=True)\n            shutil.copy(os.path.join(md_dir, md_file), local_path)\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    main(args)\n'"
egs/wham/ConvTasNet/local/preprocess_wham.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix_both\', \'mix_clean\',\n                    \'mix_single\', \'s1\', \'s2\', \'noise\']\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WHAM data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/wham/DPRNN/local/preprocess_wham.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix_both\', \'mix_clean\',\n                    \'mix_single\', \'s1\', \'s2\', \'noise\']\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WHAM data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/wham/DynamicMixing/local/augmented_wham.py,5,"b'import os\nimport glob\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom asteroid.data.wham_dataset import WHAM_TASKS\nimport soundfile as sf\nimport random\nfrom pysndfx import AudioEffectsChain\nimport json\n\n\nclass AugmentedWhamDataset(Dataset):\n    """""" Dataset class for WHAM source separation and speech enhancement tasks.\n\n    Args:\n        wsj_train_dir (str): The path to the directory containing the wsj train/dev/test .wav files.\n        task (str): One of ``\'enh_single\'``, ``\'enh_both\'``, ``\'sep_clean\'`` or\n            ``\'sep_noisy\'``.\n\n            * ``\'enh_single\'`` for single speaker speech enhancement.\n            * ``\'enh_both\'`` for multi speaker speech enhancement.\n            * ``\'sep_clean\'`` for two-speaker clean source separation.\n            * ``\'sep_noisy\'`` for two-speaker noisy source separation.\n        noise_dir (str, optional): The path to the directory containing the WHAM train/dev/test .wav files\n        sample_rate (int, optional): The sampling rate of the wav files.\n        json_dir: (str, optional):\n        segment (float, optional): Length of the segments used for training,\n            in seconds. If None, use full utterances (e.g. for test).\n        nondefault_nsrc (int, optional): Number of sources in the training\n            targets.\n            If None, defaults to one for enhancement tasks and two for\n            separation tasks.\n        global_db_range: (tuple, optional): Minimum and maximum bounds for each source (and noise) (dB).\n        global_stats: (tuple, optional): Mean and standard deviation for level in dB of first source.\n        rel_stats: (tuple, optional): Mean and standard deviation for level in dB of second source relative to the first source.\n        noise_stats: (tuple, optional): Mean and standard deviation for level in dB of noise relative to the first source.\n        speed_perturb: (tuple, optional): Range for SoX speed perturbation transformation.\n    """"""\n\n    def __init__(self, wsj0train, task, noise_dir=None, json_dir=None, orig_percentage=0., sample_rate=8000,\n                 segment=4.0, nondefault_nsrc=None, global_db_range=(-45, 0), abs_stats=(-16.7, 7),\n                 rel_stats=(2.52, 4), noise_stats=(5.1, 6.4), speed_perturb=(0.95, 1.05)):\n        super(AugmentedWhamDataset, self).__init__()\n        if task not in WHAM_TASKS.keys():\n            raise ValueError(\'Unexpected task {}, expected one of \'\n                             \'{}\'.format(task, WHAM_TASKS.keys()))\n        # Task setting\n        self.task = task\n        if self.task in [""sep_noisy"", ""enh_single""] and not noise_dir:\n            raise RuntimeError(""noise directory must be specified if task is sep_noisy or enh_single"")\n        self.task_dict = WHAM_TASKS[task]\n        self.orig_percentage = orig_percentage\n        if json_dir:\n            self.use_original = True\n        else:\n            self.use_original = False\n        self.sample_rate = sample_rate\n        self.seg_len = None if segment is None else int(segment * sample_rate)\n\n        self.global_db_range = global_db_range\n        self.abs_stats = abs_stats\n        self.rel_stats = rel_stats\n        self.noise_stats = noise_stats\n        self.speed_perturb = speed_perturb\n\n        if not nondefault_nsrc:\n            self.n_src = self.task_dict[\'default_nsrc\']\n        else:\n            assert nondefault_nsrc >= self.task_dict[\'default_nsrc\']\n            self.n_src = nondefault_nsrc\n        if json_dir:\n            self.wham_mix, self.wham_sources = self.parse_wham(json_dir)\n        self.hashtab_synth = self.parse_wsj0(wsj0train, noise_dir)\n\n    def parse_wham(self, json_dir):\n        mix_json = os.path.join(json_dir, self.task_dict[\'mixture\'] + \'.json\')\n        sources_json = [os.path.join(json_dir, source + \'.json\') for\n                        source in self.task_dict[\'sources\']]\n        with open(mix_json, \'r\') as f:\n            mix_infos = json.load(f)\n        sources_infos = []\n        for src_json in sources_json:\n            with open(src_json, \'r\') as f:\n                sources_infos.append(json.load(f))\n        # Filter out short utterances only when segment is specified\n        orig_len = len(mix_infos)\n        drop_utt, drop_len = 0, 0\n\n        for i in range(len(mix_infos) - 1, -1, -1):  # Go backward\n            if mix_infos[i][1] < self.seg_len:\n                drop_utt += 1\n                drop_len += mix_infos[i][1]\n                del mix_infos[i]\n                for src_inf in sources_infos:\n                    del src_inf[i]\n\n        print(""Drop {} utts({:.2f} h) from {} (shorter than {} samples)"".format(\n            drop_utt, drop_len / self.sample_rate / 36000, orig_len, self.seg_len))\n        mix = mix_infos\n        # Handle the case n_src > default_nsrc\n        while len(sources_infos) < self.n_src:\n            sources_infos.append([None for _ in range(len(mix))])\n        sources = sources_infos\n        return mix, sources\n\n    def parse_wsj0(self, wsj_train_dir, noise_dir):\n\n        # Load json files\n        utterances = glob.glob(os.path.join(wsj_train_dir, ""**/*.wav""), recursive=True)\n        noises = None\n        if self.task in [""sep_noisy"", ""enh_single"", ""enhance_single"", ""enh_both""]:\n            noises = glob.glob(os.path.join(noise_dir, ""*.wav""))\n            assert len(noises) > 0, ""No noises parsed. Wrong path ?""\n\n        # parse utterances according to speaker\n        drop_utt, drop_len = 0, 0\n        print(""Parsing WSJ speakers"")\n        examples_hashtab = {}\n        for utt in utterances:\n            # exclude if too short\n            meta = sf.SoundFile(utt)\n            c_len = len(meta)\n            assert meta.samplerate == self.sample_rate\n\n            target_length = int(np.ceil(self.speed_perturb[1] * self.seg_len)) if self.speed_perturb else self.seg_len\n            if c_len < target_length:  # speed perturbation\n                drop_utt += 1\n                drop_len += c_len\n                continue\n            speaker = utt.split(""/"")[-2]\n            if speaker not in examples_hashtab.keys():\n                examples_hashtab[speaker] = [(utt, c_len)]\n            else:\n                examples_hashtab[speaker].append((utt, c_len))\n\n        print(""Drop {} utts({:.2f} h) from {} (shorter than {} samples)"".format(\n            drop_utt, drop_len / self.sample_rate / 36000, len(utterances), self.seg_len))\n\n        drop_utt, drop_len = 0, 0\n        if noises:\n            examples_hashtab[""noise""] = []\n            for noise in noises:\n                meta = sf.SoundFile(noise)\n                c_len = len(meta)\n                assert meta.samplerate == self.sample_rate\n                target_length = int(\n                    np.ceil(self.speed_perturb[1] * self.seg_len)) if self.speed_perturb else self.seg_len\n                if c_len < target_length:  # speed perturbation\n                    drop_utt += 1\n                    drop_len += c_len\n                    continue\n                examples_hashtab[""noise""].append((noise, c_len))\n\n            print(""Drop {} noises({:.2f} h) from {} (shorter than {} samples)"".format(\n                drop_utt, drop_len / self.sample_rate / 36000, len(noises), self.seg_len))\n\n        return examples_hashtab\n\n    def __add__(self, wham):\n        raise NotImplementedError  # It will require different handling of other datasets, I suggest using dicts\n\n    def __len__(self):\n        if self.use_original:\n            return len(\n                self.wham_mix)  # same length as original wham (actually if orig_percentage = 1 the data is original wham)\n        else:\n            return sum(\n                [len(self.hashtab_synth[x]) for x in self.hashtab_synth.keys()])  # we account only the wsj0 length\n\n    def random_data_augmentation(self, signal, c_gain, speed):\n        if self.speed_perturb:\n            fx = (AudioEffectsChain().speed(speed).custom(\n                ""norm {}"".format(c_gain)))  # speed perturb and then apply gain\n        else:\n            fx = (AudioEffectsChain().custom(\n                ""norm {}"".format(c_gain)))\n        signal = fx(signal)\n\n        return signal\n\n    @staticmethod\n    def get_random_subsegment(array, desired_len, tot_length):\n\n        offset = 0\n        if desired_len < tot_length:\n            offset = np.random.randint(0, tot_length - desired_len)\n\n        out, _ = sf.read(array, start=offset, stop=offset + desired_len, dtype=""float32"")\n\n        if len(out.shape) > 1:\n            out = out[:, random.randint(0, 1)]\n\n        return out\n\n    def __getitem__(self, idx):\n        """""" Gets a mixture/sources pair.\n        Returns:\n            mixture, vstack([source_arrays])\n        """"""\n        if self.use_original == True:\n            if random.random() <= self.orig_percentage:  # if true sample wham example\n                mix_file, mixlen = self.wham_mix[idx]\n\n                offset = 0\n                if self.seg_len < mixlen:\n                    offset = np.random.randint(0, mixlen - self.seg_len)\n\n                x, _ = sf.read(mix_file, start=offset, stop=offset + self.seg_len, dtype=""float32"")\n\n                seg_len = torch.as_tensor([len(x)])\n                # Load sources\n                source_arrays = []\n                for src in self.wham_sources:\n                    if src[idx] is None:\n                        # Target is filled with zeros if n_src > default_nsrc\n                        s = np.zeros((seg_len,))\n                    else:\n                        s, _ = sf.read(src[idx][0], start=offset,\n                                       stop=offset + self.seg_len, dtype=\'float32\')\n                    source_arrays.append(s)\n                sources = torch.from_numpy(np.vstack(source_arrays))\n                return torch.from_numpy(x), sources\n\n        # else return augmented data: Sample k speakers randomly\n        c_speakers = np.random.choice([x for x in self.hashtab_synth.keys() if x != ""noise""], self.n_src)\n\n        sources = []\n        first_lvl = None\n        floor, ceil = self.global_db_range\n        for i, spk in enumerate(c_speakers):\n            tmp, tmp_spk_len = random.choice(self.hashtab_synth[c_speakers[i]])\n            # account for sample reduction in speed perturb\n            if self.speed_perturb:\n                c_speed = random.uniform(*self.speed_perturb)\n                target_len = int(np.ceil(c_speed * self.seg_len))\n            else:\n                target_len = self.seg_len\n            tmp = self.get_random_subsegment(tmp, target_len, tmp_spk_len)\n            if i == 0:  # we model the signal level distributions with gaussians\n                c_lvl = np.clip(random.normalvariate(*self.abs_stats), floor, ceil)\n                first_lvl = c_lvl\n            else:\n                c_lvl = np.clip(first_lvl - random.normalvariate(*self.rel_stats), floor, ceil)\n            tmp = self.random_data_augmentation(tmp, c_lvl, c_speed)\n            tmp = tmp[:self.seg_len]\n            sources.append(tmp)\n\n        if self.task in [""sep_noisy"", ""enh_single"", ""enh_both"", ""enhance_single""]:\n            # add also noise\n            tmp, tmp_spk_len = random.choice(self.hashtab_synth[""noise""])\n            if self.speed_perturb:\n                c_speed = random.uniform(*self.speed_perturb)\n                target_len = int(np.ceil(c_speed * self.seg_len))\n            else:\n                target_len = self.seg_len\n            tmp = self.get_random_subsegment(tmp, target_len, tmp_spk_len)\n            c_lvl = np.clip(first_lvl - random.normalvariate(*self.noise_stats), floor, ceil)\n            tmp = self.random_data_augmentation(tmp, c_lvl, c_speed)\n            tmp = tmp[:self.seg_len]\n            sources.append(tmp)\n\n        mix = np.mean(np.stack(sources), 0)\n\n        if self.task in [""sep_noisy"", ""enh_single"", ""enhance_single"", ""enh_both""]:\n            sources = sources[:-1]  # discard noise\n\n        sources = np.stack(sources)\n        sources = sources\n        return torch.from_numpy(mix).float(), torch.from_numpy(sources).float()'"
egs/wham/DynamicMixing/local/preprocess_wham.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix_both\', \'mix_clean\',\n                    \'mix_single\', \'s1\', \'s2\', \'noise\']\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WHAM data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/wham/DynamicMixing/local/resample_dataset.py,0,"b'import argparse\nimport os\nfrom glob import glob\nfrom distutils.dir_util import copy_tree\nfrom scipy.signal import resample_poly\nimport soundfile as sf\n\nparser = argparse.ArgumentParser(""Script for resampling a dataset"")\nparser.add_argument(""source_dir"", type=str)\nparser.add_argument(""out_dir"", type=str)\nparser.add_argument(""original_sr"", type=int)\nparser.add_argument(""target_sr"", type=int)\nparser.add_argument(""--extension"", type=str, default=""wav"")\n\n\ndef main(out_dir, original_sr, target_sr, extension):\n    assert original_sr >= target_sr, ""Upsampling not supported""\n    wavs = glob(os.path.join(out_dir, ""**/*.{}"".format(extension)), recursive=True)\n    for wav in wavs:\n        data, fs = sf.read(wav)\n        assert fs == original_sr\n        data = resample_poly(data, target_sr, fs)\n        sf.write(wav, data, samplerate=target_sr)\n\n\nif __name__ == ""__main__"":\n    args = parser.parse_args()\n    copy_tree(args.source_dir, args.out_dir) # first we copy then we resample\n    main(args.out_dir, args.original_sr, args.target_sr, args.extension)'"
egs/wham/DynamicMixing/utils/get_training_stats.py,0,"b'""""""\nThis script can be used to get dB lvl stats for WHAM sources and noise.\n""""""\nimport soundfile as sf\nfrom glob import glob\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nWHAM_ROOT = ""/media/sam/Data/WSJ/wham_scripts/2speakers_wham/wav8k""\n\nfor mode in [""min"", ""max""]:\n    for split in [""tr""]:\n\n        noises = glob(os.path.join(WHAM_ROOT, mode, split, ""noise"", ""*.wav""))\n        s1 = glob(os.path.join(WHAM_ROOT, mode, split, ""s1"", ""*.wav""))\n        s2 = glob(os.path.join(WHAM_ROOT, mode, split, ""s2"", ""*.wav""))\n\n        # stat joint\n        joint_src_stats = []\n\n        for i in range(len(s1)):\n            c_s1 = s1[i]\n            c_s2 = os.path.join(WHAM_ROOT, mode, split, ""s2"", c_s1.split(""/"")[-1])\n            noise = os.path.join(WHAM_ROOT, mode, split, ""noise"", c_s1.split(""/"")[-1])\n\n            c_s1_audio, _ = sf.read(c_s1)\n            c_s2_audio, _ = sf.read(c_s2)\n            noise, _ = sf.read(noise)\n\n            c_s1_lvl = 20 * np.log10(np.max(np.abs(c_s1_audio)))\n            c_s2_lvl = 20 * np.log10(np.max(np.abs(c_s2_audio)))\n            noises_lvl = 20 * np.log10(np.max(np.abs(noise)))\n\n            joint_src_stats.append([c_s1_lvl, c_s2_lvl, noises_lvl])\n\n        plt.hist2d([x[0] for x in joint_src_stats], [x[1] for x in joint_src_stats], 100, [[-50, 0], [-50, 0]])\n        plt.hist2d([x[0] for x in joint_src_stats], [x[2] for x in joint_src_stats], 100, [[-50, 0], [-50, 0]])\n'"
egs/wham/TwoStep/local/preprocess_wham.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix_both\', \'mix_clean\',\n                    \'mix_single\', \'s1\', \'s2\', \'noise\']\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WHAM data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/whamr/TasNet/local/preprocess_whamr.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir):\n    """""" Create list of list for one condition, each list contains\n    [path, wav_length].""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    return file_infos\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    # The filenames are shared between directories (and lengths as well) so\n    # we can just search once and replace directory name after.\n    speaker_list = [\'s1_anechoic\', \'s2_anechoic\', \'s1_reverb\', \'s2_reverb\',\n                    \'mix_single_anechoic\', \'mix_clean_anechoic\',\n                    \'mix_both_anechoic\', \'mix_single_reverb\',\n                    \'mix_clean_reverb\', \'mix_both_reverb\']\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        out_dir = os.path.join(inp_args.out_dir, data_type)\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n        # Create the first list of wavs\n        spk_0 = speaker_list[0]\n        to_json = preprocess_one_dir(os.path.join(inp_args.in_dir,\n                                                  data_type, spk_0))\n        # Replace directory names to match all conditions\n        for spk in speaker_list:\n            local_to_json = []\n            for wav_info in to_json:\n                name, wav_len = wav_info\n                local_to_json.append([name.replace(spk_0, spk), wav_len])\n            with open(os.path.join(out_dir, spk + \'.json\'), \'w\') as f:\n                json.dump(local_to_json, f, indent=4)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WHAM data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/wsj0-mix/DeepClustering/local/preprocess_wsj0mix.py,0,"b'import argparse\nimport json\nimport os\nimport soundfile as sf\n\n\ndef preprocess_one_dir(in_dir, out_dir, out_filename):\n    """""" Create .json file for one condition.""""""\n    file_infos = []\n    in_dir = os.path.abspath(in_dir)\n    wav_list = os.listdir(in_dir)\n    wav_list.sort()\n    for wav_file in wav_list:\n        if not wav_file.endswith(\'.wav\'):\n            continue\n        wav_path = os.path.join(in_dir, wav_file)\n        samples = sf.SoundFile(wav_path)\n        file_infos.append((wav_path, len(samples)))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n    with open(os.path.join(out_dir, out_filename + \'.json\'), \'w\') as f:\n        json.dump(file_infos, f, indent=4)\n\n\ndef preprocess(inp_args):\n    """""" Create .json files for all conditions.""""""\n    speaker_list = [\'mix\'] + [f""s{n+1}"" for n in range(inp_args.n_src)]\n    for data_type in [\'tr\', \'cv\', \'tt\']:\n        for spk in speaker_list:\n            preprocess_one_dir(os.path.join(inp_args.in_dir, data_type, spk),\n                               os.path.join(inp_args.out_dir, data_type),\n                               spk)\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(""WSJ0-MIX data preprocessing"")\n    parser.add_argument(\'--in_dir\', type=str, default=None,\n                        help=\'Directory path of wham including tr, cv and tt\')\n    parser.add_argument(\'--n_src\', type=int, default=2,\n                        help=\'Number of sources in wsj0-mix\')\n    parser.add_argument(\'--out_dir\', type=str, default=None,\n                        help=\'Directory path to put output files\')\n    args = parser.parse_args()\n    print(args)\n    preprocess(args)\n'"
egs/avspeech/looking-to-listen/local/loader/__init__.py,0,b'from .frames import input_face_embeddings\n'
egs/avspeech/looking-to-listen/local/loader/audio_mixer_generator.py,0,"b'import os\nimport re\nimport math\nimport glob\nimport random\nimport itertools\nimport subprocess\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom argparse import ArgumentParser\n\nfrom constants import (\n    AUDIO_MIX_COMMAND_PREFIX,\n    AUDIO_DIR,\n    MIXED_AUDIO_DIR,\n    VIDEO_DIR,\n    AUDIO_SET_DIR,\n    STORAGE_LIMIT,\n)\n\n\ndef sample_audio_set():\n    """"""\n        sample random audio files as a noise from audio set dataset\n    """"""\n    audio_files = glob.glob(os.path.join(AUDIO_SET_DIR, ""*""))\n    total_files = len(audio_files)\n\n    total_choices = max(1, int(random.gauss(mu=1, sigma=1)))\n    choices = list(range(total_files))\n    random.shuffle(choices)\n\n    return [audio_files[i] for i in choices[:total_choices]]\n\n\ndef requires_excess_storage_space(n, r):\n    # r will be very small anyway\n    total = n ** r / math.factorial(r)\n    # total bytes\n    storage_space = (\n        total * 96\n    )  # approximate storage requirement is (600K for spec and 90K for audio)\n\n    if storage_space > STORAGE_LIMIT:\n        return storage_space, True\n\n    return storage_space, False\n\n\ndef nCr(n, r):\n    return math.factorial(n) / (math.factorial(r) * math.factorial(n - r))\n\n\ndef audio_mixer(\n    dataset_size: int,\n    n_src=2,\n    video_ext="".mp4"",\n    audio_ext="".wav"",\n    file_name=""temp.csv"",\n    audio_set=False,\n    validation_size=0.3,\n    remove_random_chance=0.9,\n) -> None:\n    """"""\n        generate the combination dataframe used in data_loader.py\n\n        Args:\n            dataset_size: restrict total possible combinations\n            n_src: input size\n            video_ext: extension of video\n            audio_ext: extension of audio\n            file_name: file name of combination dataframe to save\n            audio_set: use audio set dataset\n    """"""\n    audio_mix_command_suffix = ""-filter_complex amix=inputs={}:duration=longest ""\n    audio_files = glob.glob(os.path.join(AUDIO_DIR, ""*""))\n    total_audio_files = len(audio_files)\n\n    total_val_files = int(total_audio_files * validation_size)\n    total_train_files = total_audio_files - total_val_files\n\n    train_files = audio_files[:total_train_files]\n    val_files = audio_files[-total_val_files:]\n\n    def retrieve_name(f):\n        f = os.path.splitext(os.path.basename(f))[0]\n        return re.sub(r""_part\\d"", """", f)\n\n    def mix(audio_filtered_files, file_name_df, offset):\n        # Generate all combinations and trim total possibilities\n        audio_combinations = itertools.combinations(audio_filtered_files, n_src)\n        audio_combinations = itertools.islice(audio_combinations, dataset_size)\n\n        storage_space, excess_storage = requires_excess_storage_space(\n            len(audio_filtered_files), n_src\n        )\n\n        if excess_storage:\n            storage_space = (1 - remove_random_chance) * storage_space\n            print(f""Removing {REMOVE_RANDOM_CHANCE * 100} percent of combinations"")\n            print(\n                f""Saving total space: {storage_space - storage_space * REMOVE_RANDOM_CHANCE:,} Kbytes""\n            )\n\n        print(f""Occupying space: {storage_space:,} Kbytes"")\n\n        # Store list of tuples, consisting of `n_src`\n        # Audio and their corresponding video path\n        video_inputs = []\n        audio_inputs = []\n        mixed_audio = []\n        noises = []\n\n        total_comb_size = nCr(len(audio_filtered_files), n_src)\n        for indx, audio_comb in tqdm(\n            enumerate(audio_combinations), total=total_comb_size\n        ):\n            # skip few combinations if required storage is very high\n            try:\n                if excess_storage and random.random() < remove_random_chance:\n                    continue\n\n                base_names = [os.path.basename(fname)[:11] for fname in audio_comb]\n                if len(base_names) != len(set(base_names)):\n                    # if audio from the same video, assume same speaker and ignore it.\n                    continue\n                if audio_set:\n                    noise_input = sample_audio_set()\n                    noises.append("":"".join(noise_input))\n                    audio_comb = (*audio_comb, *noise_input)\n\n                audio_inputs.append(audio_comb)\n                # Convert audio file path to corresponding video path\n                video_inputs.append(\n                    tuple(\n                        os.path.join(VIDEO_DIR, retrieve_name(f) + video_ext)\n                        for f in audio_comb\n                    )\n                )\n\n                audio_mix_input = """"\n                for audio in audio_comb:\n                    audio_mix_input += f""-i {audio} ""\n\n                mixed_audio_name = os.path.join(\n                    MIXED_AUDIO_DIR, f""{indx+offset}{audio_ext}""\n                )\n                audio_command = (\n                    AUDIO_MIX_COMMAND_PREFIX\n                    + audio_mix_input\n                    + audio_mix_command_suffix.format(len(audio_comb))\n                    + mixed_audio_name\n                )\n\n                process = subprocess.Popen(\n                    audio_command,\n                    shell=True,\n                    stdout=subprocess.PIPE,\n                    stderr=subprocess.PIPE,\n                )  # .communicate()\n                mixed_audio.append(mixed_audio_name)\n                # print(video_inputs, audio_inputs, mixed_audio, noises)\n            except KeyboardInterrupt as e:\n                print(""Caught Interrupt!"")\n                break\n\n        combinations = {}\n        for i in range(n_src):\n            combinations[f""video_{i+1}""] = []\n            combinations[f""audio_{i+1}""] = []\n        combinations[""mixed_audio""] = []\n\n        min_length = min(min(len(video_inputs), len(audio_inputs)), len(mixed_audio))\n        print(f""Total combinations: {min_length}"")\n\n        video_inputs = video_inputs[:min_length]\n        audio_inputs = audio_inputs[:min_length]\n        mixed_audio = mixed_audio[:min_length]\n\n        assert len(video_inputs) == len(audio_inputs) == len(mixed_audio)\n\n        for videos, audios, mixed in zip(video_inputs, audio_inputs, mixed_audio):\n            # fix proper path issue\n            mixed = re.sub(r""../../"", """", mixed)\n            for i in range(n_src):\n                v = re.sub(r""../../"", """", videos[i])\n                a = re.sub(r""../../"", """", audios[i])\n\n                combinations[f""video_{i+1}""].append(v)\n                combinations[f""audio_{i+1}""].append(a)\n            combinations[""mixed_audio""].append(mixed)\n\n        columns = (\n            [f""video_{i+1}"" for i in range(n_src)]\n            + [f""audio_{i+1}"" for i in range(n_src)]\n            + [""mixed_audio""]\n        )\n        df = pd.DataFrame(combinations).reindex(columns=columns)\n        df.to_csv(file_name_df, index=False)\n\n        if audio_set:\n            pd.Series(noises).to_csv(\n                ""../../data/noise_only.csv"", index=False, header=False\n            )\n        return df.shape[0]\n\n    offset = mix(train_files, ""../../data/train.csv"", 0)\n    mix(val_files, ""../../data/val.csv"", offset)\n\n\nif __name__ == ""__main__"":\n    parser = ArgumentParser()\n\n    parser.add_argument(\n        ""--remove-random"",\n        ""-r"",\n        default=0.9,\n        type=float,\n        help=""ratio of combination to remove"",\n    )\n    parser.add_argument(\n        ""--use-audio-set"", ""-u"", dest=""use_audio_set"", action=""store_true""\n    )\n    parser.add_argument(\n        ""--file-limit"",\n        ""-l"",\n        default=100_000_000,\n        type=int,\n        help=""restrict total number of files generated"",\n    )\n    parser.add_argument(\n        ""--validation-size"",\n        ""-v"",\n        default=0.3,\n        type=float,\n        help=""ratio of files to use in validation data"",\n    )\n\n    args = parser.parse_args()\n\n    audio_mixer(\n        args.file_limit,\n        audio_set=args.use_audio_set,\n        validation_size=args.validation_size,\n        remove_random_chance=args.remove_random,\n    )\n'"
egs/avspeech/looking-to-listen/local/loader/download.py,0,"b'import os\nimport time\nimport tqdm\nimport argparse\nimport subprocess\nimport pandas as pd\nfrom pathlib import Path\nimport concurrent.futures\n\nfrom constants import VIDEO_DIR\n\n\ndef download(link, path, final_name=None):\n    command = ""youtube-dl {} --no-check-certificate --output {}.mp4 -f \'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4\'""\n    if os.path.exists(path) and os.path.isfile(path):\n        print(""File already downloaded"")\n        return False\n    if final_name is not None and os.path.isfile(final_name):\n        print(""File already cropped"")\n        return True\n\n    p = subprocess.Popen(\n        command.format(link, path),\n        shell=True,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    ).communicate()\n    return False\n\n\ndef crop(path, start, end, downloaded_name):\n    command = (\n        ""ffmpeg -y -i {}.mp4 -ss {} -t {} -c:v libx264 -crf 18 -preset veryfast -pix_fmt yuv420p ""\n        ""-c:a aac -b:a 128k -strict experimental -r 25 {}""\n    )\n\n    start_minute, start_second = int(start // 60), int(start % 60)\n    end_minute, end_second = int(end // 60) - start_minute, int(end % 60) - start_second\n\n    new_filepath = downloaded_name + ""_final.mp4""\n\n    if os.path.exists(new_filepath) and os.path.isfile(new_filepath):\n        return\n\n    command = command.format(\n        downloaded_name,\n        f""{start_minute}:{start_second}"",\n        f""{end_minute}:{end_second}"",\n        new_filepath,\n    )\n    subprocess.Popen(\n        command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    ).communicate()\n\n    remove_orig_file = f""rm -f {downloaded_name}.mp4""\n    subprocess.Popen(\n        remove_orig_file, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n    ).communicate()\n\n\ndef save_video(zargs):\n    link, path, start, end, pos_x, pos_y = zargs\n    x = int(pos_x * 10000)\n    y = int(pos_y * 10000)\n    downloaded_name = path.as_posix() + f""_{x}_{y}""\n    cropped = download(link, downloaded_name, final_name=downloaded_name + ""_final.mp4"")\n    if not cropped:\n        crop(path, start, end, downloaded_name)\n\n\ndef main(args):\n    df = pd.read_csv(args.path)\n    links = df.iloc[:, 0][args.start : args.end]\n    start_times = df.iloc[:, 1][args.start : args.end]\n    end_times = df.iloc[:, 2][args.start : args.end]\n    pos_x = df.iloc[:, 3][args.start : args.end]\n    pos_y = df.iloc[:, 4][args.start : args.end]\n\n    yt_links = [""https://youtube.com/watch\\?v\\="" + l for l in links]\n    paths = [Path(os.path.join(args.vid_dir, f)) for f in links]\n\n    link_path = zip(yt_links, paths, start_times, end_times, pos_x, pos_y)\n    with concurrent.futures.ThreadPoolExecutor(args.jobs) as executor:\n        results = list(tqdm.tqdm(executor.map(save_video, link_path), total=len(links)))\n\n\nif __name__ == ""__main__"":\n    parse = argparse.ArgumentParser(description=""Download parameters"")\n    parse.add_argument(""--jobs"", type=int, default=1)\n    parse.add_argument(\n        ""--path"", type=str, default=""../../data/audio_visual/avspeech_train.csv""\n    )\n    parse.add_argument(""--vid-dir"", type=str, default=VIDEO_DIR)\n    parse.add_argument(""--start"", type=int, default=0)\n    parse.add_argument(""--end"", type=int, default=10_000)\n    args = parse.parse_args()\n    main(args)\n'"
egs/avspeech/looking-to-listen/local/loader/extract_audio.py,0,"b'import os\nimport cv2\nimport time\nimport argparse\nimport subprocess\nimport pandas as pd\nfrom pathlib import Path\nimport concurrent.futures\nfrom tqdm import tqdm\n\nfrom constants import AUDIO_DIR, VIDEO_DIR\n\n\ndef extract(path):\n    name = path.stem\n\n    dir_name = path.parents[0]\n    audio_dir = args.aud_dir\n    audio_path = os.path.join(audio_dir, name)\n    video = cv2.VideoCapture(path.as_posix())\n    length_orig_video = video.get(cv2.CAP_PROP_FRAME_COUNT)\n    # already pre-processed at 25 fps for 3 or more seconds\n    length = int(length_orig_video) // 25 // 3\n    for i in range(length):\n        t = i * 3\n        command = (\n            f""ffmpeg -y -i {path.as_posix()} -f {args.audio_extension} -ab 64000 ""\n            f""-vn -ar {args.sampling_rate} -ac {args.audio_channel} - | sox -t ""\n            f""{args.audio_extension} - -r 16000 -c 1 -b 8 ""\n            f""{audio_path}_part{i}.{args.audio_extension} trim {t} 00:{args.duration:02d}""\n        )\n\n        subprocess.Popen(\n            command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n        ).communicate()\n\n\ndef main(args):\n    file_names = [\n        Path(os.path.join(args.vid_dir, i))\n        for i in os.listdir(args.vid_dir)\n        if i.endswith(""_final.mp4"")\n    ]\n\n    with concurrent.futures.ThreadPoolExecutor(args.jobs) as executor:\n        results = list(tqdm(executor.map(extract, file_names), total=len(file_names)))\n\n\nif __name__ == ""__main__"":\n    parse = argparse.ArgumentParser(description=""Extract parameters"")\n    parse.add_argument(""--jobs"", type=int, default=2)\n    parse.add_argument(""--aud-dir"", type=str, default=AUDIO_DIR)\n    parse.add_argument(""--vid-dir"", type=str, default=VIDEO_DIR)\n    parse.add_argument(""--sampling-rate"", type=int, default=16_000)\n    parse.add_argument(""--audio-channel"", type=int, default=2)\n    parse.add_argument(""--audio-extension"", type=str, default=""wav"")\n    parse.add_argument(""--duration"", type=int, default=3)\n    args = parse.parse_args()\n    main(args)\n'"
egs/avspeech/looking-to-listen/local/loader/frames.py,10,"b'from PIL import Image\nimport torch\nimport numpy as np\nfrom typing import Union, List\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n\ncpu_device = torch.device(""cpu"")\n\n\ndef input_face_embeddings(\n    frames: Union[List[str], np.ndarray],\n    is_path: bool,\n    mtcnn: MTCNN,\n    resnet: InceptionResnetV1,\n    face_embed_cuda: bool,\n    use_half: bool,\n    coord: List,\n    name: str = None,\n    save_frames: bool = False,\n) -> torch.Tensor:\n    """"""\n        Get the face embedding\n\n        NOTE: If a face is not detected by the detector,\n        instead of throwing an error it zeros the input\n        for embedder.\n\n        NOTE: Memory hungry function, hence the profiler.\n\n        Args:\n            frames: Frames from the video\n            is_path: Whether to read from filesystem or memory\n            mtcnn: face detector\n            resnet: face embedder\n            face_embed_cuda: use cuda for model\n            use_half: use half precision\n\n        Returns:\n            emb: Embedding for all input frames\n    """"""\n    if face_embed_cuda and torch.cuda.is_available():\n        device = torch.device(""cuda:0"")\n    else:\n        device = torch.device(""cpu"")\n    result_cropped_tensors = []\n    no_face_indices = []\n    for i, f in enumerate(frames):\n        if is_path:\n            frame = Image.open(f)\n        else:\n            frame = Image.fromarray(f.astype(""uint8""))\n\n        with torch.no_grad():\n            cropped_tensors = None\n            height, width, c = f.shape\n            bounding_box, prob = mtcnn.detect(frame)\n\n            if bounding_box is not None:\n                for box in bounding_box:\n                    x1, y1, x2, y2 = box\n                    if x1 > x2:\n                        x1, x2 = x2, x1\n                    if y1 > y2:\n                        y1, y2 = y2, y1\n\n                    # for point in coord:\n                    x, y = coord[0], coord[1]\n                    x *= width\n                    y *= height\n                    if x >= x1 and y >= y1 and x <= x2 and y <= y2:\n                        cropped_tensors = extract_face(frame, box)\n                        # print(""found"", box, x, y, end=\'\\r\')\n                        break\n\n        if cropped_tensors is None:\n            # Face not detected, for some reason\n            cropped_tensors = torch.zeros((3, 160, 160))\n            no_face_indices.append(i)\n\n        if save_frames:\n            name = name.replace("".mp4"", """")\n            saveimg = cropped_tensors.detach().cpu().numpy().astype(""uint8"")\n            saveimg = np.squeeze(saveimg.transpose(1, 2, 0))\n            Image.fromarray(saveimg).save(f""{name}_{i}.png"")\n\n        result_cropped_tensors.append(cropped_tensors.to(device))\n\n    if len(no_face_indices) > 20:\n        # few videos start with silence, allow 0.5 seconds of silence else remove\n        return None\n    del frames\n    # Stack all frames\n    result_cropped_tensors = torch.stack(result_cropped_tensors)\n    # Embed all frames\n    result_cropped_tensors = result_cropped_tensors.to(device)\n    if use_half:\n        result_cropped_tensors = result_cropped_tensors.half()\n\n    with torch.no_grad():\n        emb = resnet(result_cropped_tensors)\n    if use_half:\n        emb = emb.float()\n    return emb.to(cpu_device)\n\n\nif __name__ == ""__main__"":\n    mtcnn = MTCNN(keep_all=True).eval()\n    resnet = InceptionResnetV1(pretrained=""vggface2"").eval()\n    device = torch.device(""cpu"")\n    res = input_face_embeddings([""a.jpg"", ""b.jpg""], True, mtcnn, resnet, device)\n    print(res.shape)  # 512D\n    print(""Passed"")\n'"
egs/avspeech/looking-to-listen/local/loader/generate_video_embedding.py,3,"b'import cv2\nimport torch\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom pathlib import Path\nfrom asteroid.data.avspeech_dataset import get_frames\nfrom facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n\nfrom frames import input_face_embeddings\nfrom constants import VIDEO_DIR, EMBED_DIR\n\nFRAMES = 75\n\n\ndef _get_video(df):\n    video_columns = [i for i in list(df) if ""video"" in i]\n\n    video_paths = df[video_columns].values.reshape(-1).tolist()\n    video_paths = sorted(set(video_paths), key=video_paths.index)\n\n    return video_paths\n\n\ndef store_corrupt(path):\n    with open(args.corrupt_file, ""a"") as f:\n        f.write(path.as_posix() + ""\\n"")\n\n\ndef cache_embed(path, mtcnn, resnet, args):\n    orig_path = path\n    if not path.is_file():\n        path = ""../.."" / path\n    video_file_name = path.stem.split(""_"")\n\n    if len(video_file_name) < 3:\n        store_corrupt(orig_path)\n        return\n\n    try:\n        pos_x, pos_y = (\n            int(video_file_name[-3]) / 10000,\n            int(video_file_name[-2]) / 10000,\n        )\n    except ValueError as e:\n        print(str(e))\n        store_corrupt(orig_path)\n        return\n\n    video_buffer = get_frames(cv2.VideoCapture(path.as_posix()))\n    total_frames = video_buffer.shape[0]\n\n    video_parts = total_frames // FRAMES  # (25fps * 3)\n\n    embeddings = []\n    for part in range(video_parts):\n        frame_name = path.stem + f""_part{part}""\n        embed_path = Path(args.embed_dir, frame_name + "".npy"")\n        if embed_path.is_file():\n            continue\n        raw_frames = video_buffer[part * FRAMES : (part + 1) * FRAMES]\n\n        embed = input_face_embeddings(\n            raw_frames,\n            is_path=False,\n            mtcnn=mtcnn,\n            resnet=resnet,\n            face_embed_cuda=args.cuda,\n            use_half=args.use_half,\n            coord=[pos_x, pos_y],\n        )\n\n        if embed is None:\n            store_corrupt(orig_path)\n            print(""Corrupt"", path)\n            return\n\n        embeddings.append((embed, embed_path))\n\n    # save if all parts are not corrupted\n    for embed, embed_path in embeddings:\n        np.save(embed_path, embed.cpu().numpy())\n\n\ndef main(args):\n    train_df = pd.read_csv(args.train_path)\n    val_df = pd.read_csv(args.val_path)\n\n    if args.cuda and torch.cuda.is_available():\n        device = torch.device(""cuda:0"")\n    else:\n        device = torch.device(""cpu"")\n\n    mtcnn = MTCNN(keep_all=True).eval().to(device)\n    mtcnn.device = device\n\n    resnet = InceptionResnetV1(pretrained=""vggface2"").eval().to(device)\n\n    video_paths = _get_video(train_df)\n    video_paths += _get_video(val_df)\n\n    print(f""Total embeddings: {len(video_paths)}"")\n    for path in tqdm(video_paths, total=len(video_paths)):\n        cache_embed(Path(path), mtcnn, resnet, args)\n\n\nif __name__ == ""__main__"":\n    from argparse import ArgumentParser\n\n    parser = ArgumentParser()\n    parser.add_argument(""--video-dir"", default=Path(VIDEO_DIR), type=Path)\n    parser.add_argument(""--embed-dir"", default=Path(EMBED_DIR), type=Path)\n    parser.add_argument(""--train-path"", default=Path(""../../data/train.csv""), type=Path)\n    parser.add_argument(""--val-path"", default=Path(""../../data/val.csv""), type=Path)\n    parser.add_argument(""--cuda"", dest=""cuda"", action=""store_true"")\n    parser.add_argument(""--use-half"", dest=""use_half"", action=""store_true"")\n    parser.add_argument(\n        ""--corrupt-file"", default=Path(""../../data/corrupt_frames_list.txt""), type=Path\n    )\n\n    args = parser.parse_args()\n\n    main(args)\n'"
egs/avspeech/looking-to-listen/local/loader/remove_corrupt.py,0,"b'import os\nimport pandas as pd\n\ndf_train = pd.read_csv(""../../data/train.csv"")\ndf_val = pd.read_csv(""../../data/val.csv"")\n\nprint(df_train.shape)\nprint(df_val.shape)\n\ncorrupt_files = []\n\nwith open(""../../data/corrupt_frames_list.txt"") as f:\n    corrupt_files = f.readlines()\n\ncorrupt_files = set(corrupt_files)\nprint(len(corrupt_files))\ncorrupt_files = [c[:-1] for c in corrupt_files]\nprint(corrupt_files)\n\ndf_train = df_train[~df_train[""video_1""].isin(corrupt_files)]\ndf_val = df_val[~df_val[""video_1""].isin(corrupt_files)]\n\ndf_train = df_train[~df_train[""video_2""].isin(corrupt_files)]\ndf_val = df_val[~df_val[""video_2""].isin(corrupt_files)]\n\nprint(df_train.shape)\nprint(df_val.shape)\n\ndf_train.to_csv(""../../data/train.csv"", index=False)\n\ndf_val.to_csv(""../../data/val.csv"", index=False)\n'"
egs/avspeech/looking-to-listen/local/loader/remove_empty_audio.py,0,"b'""""""\nExtremely fast mixing (100+ audio files per second)\ngenerates a lot of empty/corrupted files\n""""""\nimport os\nimport pandas as pd\nfrom pathlib import Path\nfrom argparse import ArgumentParser\n\nfrom constants import MIXED_AUDIO_DIR\n\n\ndef remove_corrupt_audio(audio_dir, df, path, expected_audio_size=96_000):\n    files = audio_dir.rglob(""*wav"")\n\n    corrupt_audio = []\n\n    for f in files:\n        size = f.stat().st_size\n        if size < expected_audio_size:\n            corrupt_audio.append(Path(*f.parts[1:]).as_posix())\n\n    print(f""Found total corrupted files: {len(corrupt_audio)}"")\n\n    filtered_df = df[~df[""mixed_audio""].isin(corrupt_audio)]\n\n    filtered_df.to_csv(path, index=False)\n\n\nif __name__ == ""__main__"":\n\n    parser = ArgumentParser()\n\n    parser.add_argument(""--mixed-dir"", default=Path(MIXED_AUDIO_DIR), type=Path)\n    parser.add_argument(""--train-df"", default=Path(""../../data/train.csv""), type=Path)\n    parser.add_argument(""--val-df"", default=Path(""../../data/val.csv""), type=Path)\n\n    args = parser.parse_args()\n\n    train_df = pd.read_csv(args.train_df)\n    val_df = pd.read_csv(args.val_df)\n\n    remove_corrupt_audio(args.mixed_dir, train_df, args.train_df)\n    remove_corrupt_audio(args.mixed_dir, val_df, args.val_df)\n'"
egs/avspeech/looking-to-listen/local/postprocess/__init__.py,0,"b'from .postprocess_audio import filter_audio, shelf\n'"
egs/avspeech/looking-to-listen/local/postprocess/postprocess_audio.py,0,"b""import scipy.signal as sg\nfrom pysndfx import AudioEffectsChain\n\ndef filter_audio(y, sr=16_000, cutoff=15_000, low_cutoff=1, filter_order=5):\n    sos = sg.butter(filter_order, [low_cutoff / sr / 2, cutoff / sr / 2], btype='band', analog=False, output='sos')\n    filtered = sg.sosfilt(sos, y)\n\n    return filtered\n\ndef shelf(y, sr=16_000, gain=5, frequency=500, slope=0.5, high_frequency=7_000):\n    afc = AudioEffectsChain()\n    fx = afc.lowshelf(gain=gain, frequency=frequency, slope=slope)\\\n            .highshelf(gain=-gain, frequency=high_frequency, slope=slope)\n\n    y = fx(y, sample_in=sr, sample_out=sr)\n\n    return y\n\n"""
egs/avspeech/looking-to-listen/local/loader/constants/__init__.py,0,"b'import os\n\nSTORAGE_DIR = os.environ.get(""STORAGE_DIR"", ""storage_dir"")\nif not STORAGE_DIR.startswith(""/""):\n    STORAGE_DIR = os.path.join(""../.."", STORAGE_DIR)  # We are in local/loader\n\nAUDIO_MIX_COMMAND_PREFIX = ""ffmpeg -y -t 00:00:03 -ac 1 ""\n\nAUDIO_DIR = f""{STORAGE_DIR}/storage/audio""\nVIDEO_DIR = f""{STORAGE_DIR}/storage/video""\nEMBED_DIR = f""{STORAGE_DIR}/storage/embed""\nMIXED_AUDIO_DIR = f""{STORAGE_DIR}/storage/mixed""\nSPEC_DIR = f""{STORAGE_DIR}/storage/spec""\n\nAUDIO_SET_DIR = f""{STORAGE_DIR}/audio_set/audio""\n\nSTORAGE_LIMIT = 5_000_000_000\n'"
