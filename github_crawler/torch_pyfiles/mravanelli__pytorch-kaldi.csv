file_path,api_count,code
core.py,24,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport sys\nimport configparser\nimport os\nfrom utils import is_sequential_dict, model_init, optimizer_init, forward_model, progress\nfrom data_io import load_counts\nimport numpy as np\nimport random\nimport torch\nfrom distutils.util import strtobool\nimport time\nimport threading\nimport torch\n\nfrom data_io import read_lab_fea, open_or_fd, write_mat\nfrom utils import shift\n\n\ndef read_next_chunk_into_shared_list_with_subprocess(\n    read_lab_fea, shared_list, cfg_file, is_production, output_folder, wait_for_process\n):\n    p = threading.Thread(target=read_lab_fea, args=(cfg_file, is_production, shared_list, output_folder))\n    p.start()\n    if wait_for_process:\n        p.join()\n        return None\n    else:\n        return p\n\n\ndef extract_data_from_shared_list(shared_list):\n    data_name = shared_list[0]\n    data_end_index_fea = shared_list[1]\n    data_end_index_lab = shared_list[2]\n    fea_dict = shared_list[3]\n    lab_dict = shared_list[4]\n    arch_dict = shared_list[5]\n    data_set = shared_list[6]\n    return data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set\n\n\ndef convert_numpy_to_torch(data_set_dict, save_gpumem, use_cuda):\n    if not (save_gpumem) and use_cuda:\n        data_set_inp = torch.from_numpy(data_set_dict[""input""]).float().cuda()\n        data_set_ref = torch.from_numpy(data_set_dict[""ref""]).float().cuda()\n    else:\n        data_set_inp = torch.from_numpy(data_set_dict[""input""]).float()\n        data_set_ref = torch.from_numpy(data_set_dict[""ref""]).float()\n    data_set_ref = data_set_ref.view((data_set_ref.shape[0], 1))\n    return data_set_inp, data_set_ref\n\n\ndef run_nn_refac01(\n    data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict, cfg_file, processed_first, next_config_file\n):\n    def _read_chunk_specific_config(cfg_file):\n        if not (os.path.exists(cfg_file)):\n            sys.stderr.write(""ERROR: The config file %s does not exist!\\n"" % (cfg_file))\n            sys.exit(0)\n        else:\n            config = configparser.ConfigParser()\n            config.read(cfg_file)\n        return config\n\n    def _get_batch_size_from_config(config, to_do):\n        if to_do == ""train"":\n            batch_size = int(config[""batches""][""batch_size_train""])\n        elif to_do == ""valid"":\n            batch_size = int(config[""batches""][""batch_size_valid""])\n        elif to_do == ""forward"":\n            batch_size = 1\n        return batch_size\n\n    def _initialize_random_seed(config):\n        seed = int(config[""exp""][""seed""])\n        torch.manual_seed(seed)\n        random.seed(seed)\n        np.random.seed(seed)\n\n    def _load_model_and_optimizer(fea_dict, model, config, arch_dict, use_cuda, multi_gpu, to_do):\n        inp_out_dict = fea_dict\n        nns, costs = model_init(inp_out_dict, model, config, arch_dict, use_cuda, multi_gpu, to_do)\n        optimizers = optimizer_init(nns, config, arch_dict)\n        for net in nns.keys():\n            pt_file_arch = config[arch_dict[net][0]][""arch_pretrain_file""]\n            if pt_file_arch != ""none"":\n                if use_cuda:\n                    checkpoint_load = torch.load(pt_file_arch)\n                else:\n                    checkpoint_load = torch.load(pt_file_arch, map_location=""cpu"")\n                nns[net].load_state_dict(checkpoint_load[""model_par""])\n                if net in optimizers:\n                    optimizers[net].load_state_dict(checkpoint_load[""optimizer_par""])\n                    optimizers[net].param_groups[0][""lr""] = float(\n                        config[arch_dict[net][0]][""arch_lr""]\n                    )  # loading lr of the cfg file for pt\n            if multi_gpu:\n                nns[net] = torch.nn.DataParallel(nns[net])\n        return nns, costs, optimizers, inp_out_dict\n\n    def _open_forward_output_files_and_get_file_handles(forward_outs, require_decodings, info_file, output_folder):\n        post_file = {}\n        for out_id in range(len(forward_outs)):\n            if require_decodings[out_id]:\n                out_file = info_file.replace("".info"", ""_"" + forward_outs[out_id] + ""_to_decode.ark"")\n            else:\n                out_file = info_file.replace("".info"", ""_"" + forward_outs[out_id] + "".ark"")\n            post_file[forward_outs[out_id]] = open_or_fd(out_file, output_folder, ""wb"")\n        return post_file\n\n    def _get_batch_config(data_set_input, seq_model, to_do, data_name, batch_size):\n        N_snt = None\n        N_ex_tr = None\n        N_batches = None\n        if seq_model or to_do == ""forward"":\n            N_snt = len(data_name)\n            N_batches = int(N_snt / batch_size)\n        else:\n            N_ex_tr = data_set_input.shape[0]\n            N_batches = int(N_ex_tr / batch_size)\n        return N_snt, N_ex_tr, N_batches\n\n    def _prepare_input(\n        snt_index,\n        batch_size,\n        inp_dim,\n        ref_dim,\n        beg_snt_fea,\n        beg_snt_lab,\n        data_end_index_fea,\n        data_end_index_lab,\n        beg_batch,\n        end_batch,\n        seq_model,\n        arr_snt_len_fea,\n        arr_snt_len_lab,\n        data_set_inp,\n        data_set_ref,\n        use_cuda,\n    ):\n        def _zero_padding(\n            inp,\n            ref,\n            max_len_fea,\n            max_len_lab,\n            data_end_index_fea,\n            data_end_index_lab,\n            data_set_inp,\n            data_set_ref,\n            beg_snt_fea,\n            beg_snt_lab,\n            snt_index,\n            k,\n        ):\n            def _input_and_ref_have_same_time_dimension(N_zeros_fea, N_zeros_lab):\n                if N_zeros_fea == N_zeros_lab:\n                    return True\n                return False\n\n            snt_len_fea = data_end_index_fea[snt_index] - beg_snt_fea\n            snt_len_lab = data_end_index_lab[snt_index] - beg_snt_lab\n            N_zeros_fea = max_len_fea - snt_len_fea\n            N_zeros_lab = max_len_lab - snt_len_lab\n            if _input_and_ref_have_same_time_dimension(N_zeros_fea, N_zeros_lab):\n                N_zeros_fea_left = random.randint(0, N_zeros_fea)\n                N_zeros_lab_left = N_zeros_fea_left\n            else:\n                N_zeros_fea_left = 0\n                N_zeros_lab_left = 0\n            inp[N_zeros_fea_left : N_zeros_fea_left + snt_len_fea, k, :] = data_set_inp[\n                beg_snt_fea : beg_snt_fea + snt_len_fea, :\n            ]\n            ref[N_zeros_lab_left : N_zeros_lab_left + snt_len_lab, k, :] = data_set_ref[\n                beg_snt_lab : beg_snt_lab + snt_len_lab, :\n            ]\n            return inp, ref, snt_len_fea, snt_len_lab\n\n        if len(data_set_ref.shape) == 1:\n            data_set_ref = data_set_ref.shape.view((data_set_ref.shape[0], 1))\n        max_len = 0\n        if seq_model:\n            max_len_fea = int(max(arr_snt_len_fea[snt_index : snt_index + batch_size]))\n            max_len_lab = int(max(arr_snt_len_lab[snt_index : snt_index + batch_size]))\n            inp = torch.zeros(max_len_fea, batch_size, inp_dim).contiguous()\n            ref = torch.zeros(max_len_lab, batch_size, ref_dim).contiguous()\n            for k in range(batch_size):\n                inp, ref, snt_len_fea, snt_len_lab = _zero_padding(\n                    inp,\n                    ref,\n                    max_len_fea,\n                    max_len_lab,\n                    data_end_index_fea,\n                    data_end_index_lab,\n                    data_set_inp,\n                    data_set_ref,\n                    beg_snt_fea,\n                    beg_snt_lab,\n                    snt_index,\n                    k,\n                )\n                beg_snt_fea = data_end_index_fea[snt_index]\n                beg_snt_lab = data_end_index_lab[snt_index]\n                snt_index = snt_index + 1\n        else:\n            if to_do != ""forward"":\n                inp = data_set[beg_batch:end_batch, :].contiguous()\n            else:\n                snt_len_fea = data_end_index_fea[snt_index] - beg_snt_fea\n                snt_len_lab = data_end_index_lab[snt_index] - beg_snt_lab\n                inp = data_set_inp[beg_snt_fea : beg_snt_fea + snt_len_fea, :].contiguous()\n                ref = data_set_ref[beg_snt_lab : beg_snt_lab + snt_len_lab, :].contiguous()\n                beg_snt_fea = data_end_index_fea[snt_index]\n                beg_snt_lab = data_end_index_lab[snt_index]\n                snt_index = snt_index + 1\n        if use_cuda:\n            inp = inp.cuda()\n            ref = ref.cuda()\n        return inp, ref, max_len_fea, max_len_lab, snt_len_fea, snt_len_lab, beg_snt_fea, beg_snt_lab, snt_index\n\n    def _optimization_step(optimizers, outs_dict, config, arch_dict):\n        for opt in optimizers.keys():\n            optimizers[opt].zero_grad()\n        outs_dict[""loss_final""].backward()\n        for opt in optimizers.keys():\n            if not (strtobool(config[arch_dict[opt][0]][""arch_freeze""])):\n                optimizers[opt].step()\n\n    def _update_progress_bar(to_do, i, N_batches, loss_sum):\n        if to_do == ""train"":\n            status_string = (\n                ""Training | (Batch ""\n                + str(i + 1)\n                + ""/""\n                + str(N_batches)\n                + "")""\n                + "" | L:""\n                + str(round(loss_sum.cpu().item() / (i + 1), 3))\n            )\n            if i == N_batches - 1:\n                status_string = ""Training | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n        if to_do == ""valid"":\n            status_string = ""Validating | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n        if to_do == ""forward"":\n            status_string = ""Forwarding | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n        progress(i, N_batches, status=status_string)\n\n    def _write_info_file(info_file, to_do, loss_tot, err_tot, elapsed_time_chunk):\n        with open(info_file, ""w"") as text_file:\n            text_file.write(""[results]\\n"")\n            if to_do != ""forward"":\n                text_file.write(""loss=%s\\n"" % loss_tot.cpu().numpy())\n                text_file.write(""err=%s\\n"" % err_tot.cpu().numpy())\n            text_file.write(""elapsed_time_chunk=%f\\n"" % elapsed_time_chunk)\n        text_file.close()\n\n    def _save_model(to_do, nns, multi_gpu, optimizers, info_file, arch_dict):\n        if to_do == ""train"":\n            for net in nns.keys():\n                checkpoint = {}\n                if multi_gpu:\n                    checkpoint[""model_par""] = nns[net].module.state_dict()\n                else:\n                    checkpoint[""model_par""] = nns[net].state_dict()\n                if net in optimizers:\n                    checkpoint[""optimizer_par""] = optimizers[net].state_dict()\n                else:\n                    checkpoint[""optimizer_par""] = dict()\n                out_file = info_file.replace("".info"", ""_"" + arch_dict[net][0] + "".pkl"")\n                torch.save(checkpoint, out_file)\n\n    def _get_dim_from_data_set(data_set_inp, data_set_ref):\n        inp_dim = data_set_inp.shape[1]\n        ref_dim = 1\n        if len(data_set_ref.shape) > 1:\n            ref_dim = data_set_ref.shape[1]\n        return inp_dim, ref_dim\n\n    from data_io import read_lab_fea_refac01 as read_lab_fea\n    from utils import forward_model_refac01 as forward_model\n\n    config = _read_chunk_specific_config(cfg_file)\n    _initialize_random_seed(config)\n\n    output_folder = config[""exp""][""out_folder""]\n    use_cuda = strtobool(config[""exp""][""use_cuda""])\n    multi_gpu = strtobool(config[""exp""][""multi_gpu""])\n    to_do = config[""exp""][""to_do""]\n    info_file = config[""exp""][""out_info""]\n    model = config[""model""][""model""].split(""\\n"")\n    forward_outs = config[""forward""][""forward_out""].split("","")\n    forward_normalize_post = list(map(strtobool, config[""forward""][""normalize_posteriors""].split("","")))\n    forward_count_files = config[""forward""][""normalize_with_counts_from""].split("","")\n    require_decodings = list(map(strtobool, config[""forward""][""require_decoding""].split("","")))\n    save_gpumem = strtobool(config[""exp""][""save_gpumem""])\n    is_production = strtobool(config[""exp""][""production""])\n    batch_size = _get_batch_size_from_config(config, to_do)\n\n    if processed_first:\n        shared_list = list()\n        p = read_next_chunk_into_shared_list_with_subprocess(\n            read_lab_fea, shared_list, cfg_file, is_production, output_folder, wait_for_process=True\n        )\n        data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set_dict = extract_data_from_shared_list(\n            shared_list\n        )\n        data_set_inp, data_set_ref = convert_numpy_to_torch(data_set_dict, save_gpumem, use_cuda)\n    else:\n        data_set_inp = data_set[""input""]\n        data_set_ref = data_set[""ref""]\n        data_end_index_fea = data_end_index[""fea""]\n        data_end_index_lab = data_end_index[""lab""]\n    shared_list = list()\n    data_loading_process = None\n    if not next_config_file is None:\n        data_loading_process = read_next_chunk_into_shared_list_with_subprocess(\n            read_lab_fea, shared_list, next_config_file, is_production, output_folder, wait_for_process=False\n        )\n    nns, costs, optimizers, inp_out_dict = _load_model_and_optimizer(\n        fea_dict, model, config, arch_dict, use_cuda, multi_gpu, to_do\n    )\n    if to_do == ""forward"":\n        post_file = _open_forward_output_files_and_get_file_handles(\n            forward_outs, require_decodings, info_file, output_folder\n        )\n\n    seq_model = is_sequential_dict(config, arch_dict)\n    N_snt, N_ex_tr, N_batches = _get_batch_config(data_set_inp, seq_model, to_do, data_name, batch_size)\n    beg_batch = 0\n    end_batch = batch_size\n    snt_index = 0\n    beg_snt_fea = 0\n    beg_snt_lab = 0\n    arr_snt_len_fea = shift(shift(data_end_index_fea, -1, 0) - data_end_index_fea, 1, 0)\n    arr_snt_len_lab = shift(shift(data_end_index_lab, -1, 0) - data_end_index_lab, 1, 0)\n    arr_snt_len_fea[0] = data_end_index_fea[0]\n    arr_snt_len_lab[0] = data_end_index_lab[0]\n    data_set_inp_dim, data_set_ref_dim = _get_dim_from_data_set(data_set_inp, data_set_ref)\n    inp_dim = data_set_inp_dim + data_set_ref_dim\n    loss_sum = 0\n    err_sum = 0\n    start_time = time.time()\n    for i in range(N_batches):\n        inp, ref, max_len_fea, max_len_lab, snt_len_fea, snt_len_lab, beg_snt_fea, beg_snt_lab, snt_index = _prepare_input(\n            snt_index,\n            batch_size,\n            data_set_inp_dim,\n            data_set_ref_dim,\n            beg_snt_fea,\n            beg_snt_lab,\n            data_end_index_fea,\n            data_end_index_lab,\n            beg_batch,\n            end_batch,\n            seq_model,\n            arr_snt_len_fea,\n            arr_snt_len_lab,\n            data_set_inp,\n            data_set_ref,\n            use_cuda,\n        )\n        if to_do == ""train"":\n            outs_dict = forward_model(\n                fea_dict,\n                lab_dict,\n                arch_dict,\n                model,\n                nns,\n                costs,\n                inp,\n                ref,\n                inp_out_dict,\n                max_len_fea,\n                max_len_lab,\n                batch_size,\n                to_do,\n                forward_outs,\n            )\n            _optimization_step(optimizers, outs_dict, config, arch_dict)\n        else:\n            with torch.no_grad():\n                outs_dict = forward_model(\n                    fea_dict,\n                    lab_dict,\n                    arch_dict,\n                    model,\n                    nns,\n                    costs,\n                    inp,\n                    ref,\n                    inp_out_dict,\n                    max_len_fea,\n                    max_len_lab,\n                    batch_size,\n                    to_do,\n                    forward_outs,\n                )\n        if to_do == ""forward"":\n            for out_id in range(len(forward_outs)):\n                out_save = outs_dict[forward_outs[out_id]].data.cpu().numpy()\n                if forward_normalize_post[out_id]:\n                    counts = load_counts(forward_count_files[out_id])\n                    out_save = out_save - np.log(counts / np.sum(counts))\n                write_mat(output_folder, post_file[forward_outs[out_id]], out_save, data_name[i])\n        else:\n            loss_sum = loss_sum + outs_dict[""loss_final""].detach()\n            err_sum = err_sum + outs_dict[""err_final""].detach()\n        beg_batch = end_batch\n        end_batch = beg_batch + batch_size\n        _update_progress_bar(to_do, i, N_batches, loss_sum)\n    elapsed_time_chunk = time.time() - start_time\n    loss_tot = loss_sum / N_batches\n    err_tot = err_sum / N_batches\n    del inp, ref, outs_dict, data_set_inp_dim, data_set_ref_dim\n    _save_model(to_do, nns, multi_gpu, optimizers, info_file, arch_dict)\n    if to_do == ""forward"":\n        for out_name in forward_outs:\n            post_file[out_name].close()\n    _write_info_file(info_file, to_do, loss_tot, err_tot, elapsed_time_chunk)\n    if not data_loading_process is None:\n        data_loading_process.join()\n        data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set_dict = extract_data_from_shared_list(\n            shared_list\n        )\n        data_set_inp, data_set_ref = convert_numpy_to_torch(data_set_dict, save_gpumem, use_cuda)\n        data_set = {""input"": data_set_inp, ""ref"": data_set_ref}\n        data_end_index = {""fea"": data_end_index_fea, ""lab"": data_end_index_lab}\n        return [data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict]\n    else:\n        return [None, None, None, None, None, None]\n\n\ndef run_nn(\n    data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict, cfg_file, processed_first, next_config_file\n):\n\n    # This function processes the current chunk using the information in cfg_file. In parallel, the next chunk is load into the CPU memory\n\n    # Reading chunk-specific cfg file (first argument-mandatory file)\n    if not (os.path.exists(cfg_file)):\n        sys.stderr.write(""ERROR: The config file %s does not exist!\\n"" % (cfg_file))\n        sys.exit(0)\n    else:\n        config = configparser.ConfigParser()\n        config.read(cfg_file)\n\n    # Setting torch seed\n    seed = int(config[""exp""][""seed""])\n    torch.manual_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n\n    # Reading config parameters\n    output_folder = config[""exp""][""out_folder""]\n    use_cuda = strtobool(config[""exp""][""use_cuda""])\n    multi_gpu = strtobool(config[""exp""][""multi_gpu""])\n\n    to_do = config[""exp""][""to_do""]\n    info_file = config[""exp""][""out_info""]\n\n    model = config[""model""][""model""].split(""\\n"")\n\n    forward_outs = config[""forward""][""forward_out""].split("","")\n    forward_normalize_post = list(map(strtobool, config[""forward""][""normalize_posteriors""].split("","")))\n    forward_count_files = config[""forward""][""normalize_with_counts_from""].split("","")\n    require_decodings = list(map(strtobool, config[""forward""][""require_decoding""].split("","")))\n\n    use_cuda = strtobool(config[""exp""][""use_cuda""])\n    save_gpumem = strtobool(config[""exp""][""save_gpumem""])\n    is_production = strtobool(config[""exp""][""production""])\n\n    if to_do == ""train"":\n        batch_size = int(config[""batches""][""batch_size_train""])\n\n    if to_do == ""valid"":\n        batch_size = int(config[""batches""][""batch_size_valid""])\n\n    if to_do == ""forward"":\n        batch_size = 1\n\n    # ***** Reading the Data********\n    if processed_first:\n\n        # Reading all the features and labels for this chunk\n        shared_list = []\n\n        p = threading.Thread(target=read_lab_fea, args=(cfg_file, is_production, shared_list, output_folder))\n        p.start()\n        p.join()\n\n        data_name = shared_list[0]\n        data_end_index = shared_list[1]\n        fea_dict = shared_list[2]\n        lab_dict = shared_list[3]\n        arch_dict = shared_list[4]\n        data_set = shared_list[5]\n\n        # converting numpy tensors into pytorch tensors and put them on GPUs if specified\n        if not (save_gpumem) and use_cuda:\n            data_set = torch.from_numpy(data_set).float().cuda()\n        else:\n            data_set = torch.from_numpy(data_set).float()\n\n    # Reading all the features and labels for the next chunk\n    shared_list = []\n    p = threading.Thread(target=read_lab_fea, args=(next_config_file, is_production, shared_list, output_folder))\n    p.start()\n\n    # Reading model and initialize networks\n    inp_out_dict = fea_dict\n\n    [nns, costs] = model_init(inp_out_dict, model, config, arch_dict, use_cuda, multi_gpu, to_do)\n\n    # optimizers initialization\n    optimizers = optimizer_init(nns, config, arch_dict)\n\n    # pre-training and multi-gpu init\n    for net in nns.keys():\n        pt_file_arch = config[arch_dict[net][0]][""arch_pretrain_file""]\n\n        if pt_file_arch != ""none"":\n            if use_cuda:\n                checkpoint_load = torch.load(pt_file_arch)\n            else:\n                checkpoint_load = torch.load(pt_file_arch, map_location=""cpu"")\n            nns[net].load_state_dict(checkpoint_load[""model_par""])\n            optimizers[net].load_state_dict(checkpoint_load[""optimizer_par""])\n            optimizers[net].param_groups[0][""lr""] = float(\n                config[arch_dict[net][0]][""arch_lr""]\n            )  # loading lr of the cfg file for pt\n\n        if multi_gpu:\n            nns[net] = torch.nn.DataParallel(nns[net])\n\n    if to_do == ""forward"":\n\n        post_file = {}\n        for out_id in range(len(forward_outs)):\n            if require_decodings[out_id]:\n                out_file = info_file.replace("".info"", ""_"" + forward_outs[out_id] + ""_to_decode.ark"")\n            else:\n                out_file = info_file.replace("".info"", ""_"" + forward_outs[out_id] + "".ark"")\n            post_file[forward_outs[out_id]] = open_or_fd(out_file, output_folder, ""wb"")\n\n    # check automatically if the model is sequential\n    seq_model = is_sequential_dict(config, arch_dict)\n\n    # ***** Minibatch Processing loop********\n    if seq_model or to_do == ""forward"":\n        N_snt = len(data_name)\n        N_batches = int(N_snt / batch_size)\n    else:\n        N_ex_tr = data_set.shape[0]\n        N_batches = int(N_ex_tr / batch_size)\n\n    beg_batch = 0\n    end_batch = batch_size\n\n    snt_index = 0\n    beg_snt = 0\n\n    start_time = time.time()\n\n    # array of sentence lengths\n    arr_snt_len = shift(shift(data_end_index, -1, 0) - data_end_index, 1, 0)\n    arr_snt_len[0] = data_end_index[0]\n\n    loss_sum = 0\n    err_sum = 0\n\n    inp_dim = data_set.shape[1]\n    for i in range(N_batches):\n\n        max_len = 0\n\n        if seq_model:\n\n            max_len = int(max(arr_snt_len[snt_index : snt_index + batch_size]))\n            inp = torch.zeros(max_len, batch_size, inp_dim).contiguous()\n\n            for k in range(batch_size):\n\n                snt_len = data_end_index[snt_index] - beg_snt\n                N_zeros = max_len - snt_len\n\n                # Appending a random number of initial zeros, tge others are at the end.\n                N_zeros_left = random.randint(0, N_zeros)\n\n                # randomizing could have a regularization effect\n                inp[N_zeros_left : N_zeros_left + snt_len, k, :] = data_set[beg_snt : beg_snt + snt_len, :]\n\n                beg_snt = data_end_index[snt_index]\n                snt_index = snt_index + 1\n\n        else:\n            # features and labels for batch i\n            if to_do != ""forward"":\n                inp = data_set[beg_batch:end_batch, :].contiguous()\n            else:\n                snt_len = data_end_index[snt_index] - beg_snt\n                inp = data_set[beg_snt : beg_snt + snt_len, :].contiguous()\n                beg_snt = data_end_index[snt_index]\n                snt_index = snt_index + 1\n\n        # use cuda\n        if use_cuda:\n            inp = inp.cuda()\n\n        if to_do == ""train"":\n            # Forward input, with autograd graph active\n            outs_dict = forward_model(\n                fea_dict,\n                lab_dict,\n                arch_dict,\n                model,\n                nns,\n                costs,\n                inp,\n                inp_out_dict,\n                max_len,\n                batch_size,\n                to_do,\n                forward_outs,\n            )\n\n            for opt in optimizers.keys():\n                optimizers[opt].zero_grad()\n\n            outs_dict[""loss_final""].backward()\n\n            # Gradient Clipping (th 0.1)\n            # for net in nns.keys():\n            #    torch.nn.utils.clip_grad_norm_(nns[net].parameters(), 0.1)\n\n            for opt in optimizers.keys():\n                if not (strtobool(config[arch_dict[opt][0]][""arch_freeze""])):\n                    optimizers[opt].step()\n        else:\n            with torch.no_grad():  # Forward input without autograd graph (save memory)\n                outs_dict = forward_model(\n                    fea_dict,\n                    lab_dict,\n                    arch_dict,\n                    model,\n                    nns,\n                    costs,\n                    inp,\n                    inp_out_dict,\n                    max_len,\n                    batch_size,\n                    to_do,\n                    forward_outs,\n                )\n\n        if to_do == ""forward"":\n            for out_id in range(len(forward_outs)):\n\n                out_save = outs_dict[forward_outs[out_id]].data.cpu().numpy()\n\n                if forward_normalize_post[out_id]:\n                    # read the config file\n                    counts = load_counts(forward_count_files[out_id])\n                    out_save = out_save - np.log(counts / np.sum(counts))\n\n                # save the output\n                write_mat(output_folder, post_file[forward_outs[out_id]], out_save, data_name[i])\n        else:\n            loss_sum = loss_sum + outs_dict[""loss_final""].detach()\n            err_sum = err_sum + outs_dict[""err_final""].detach()\n\n        # update it to the next batch\n        beg_batch = end_batch\n        end_batch = beg_batch + batch_size\n\n        # Progress bar\n        if to_do == ""train"":\n            status_string = (\n                ""Training | (Batch ""\n                + str(i + 1)\n                + ""/""\n                + str(N_batches)\n                + "")""\n                + "" | L:""\n                + str(round(loss_sum.cpu().item() / (i + 1), 3))\n            )\n            if i == N_batches - 1:\n                status_string = ""Training | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n\n        if to_do == ""valid"":\n            status_string = ""Validating | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n        if to_do == ""forward"":\n            status_string = ""Forwarding | (Batch "" + str(i + 1) + ""/"" + str(N_batches) + "")""\n\n        progress(i, N_batches, status=status_string)\n\n    elapsed_time_chunk = time.time() - start_time\n\n    loss_tot = loss_sum / N_batches\n    err_tot = err_sum / N_batches\n\n    # clearing memory\n    del inp, outs_dict, data_set\n\n    # save the model\n    if to_do == ""train"":\n\n        for net in nns.keys():\n            checkpoint = {}\n            if multi_gpu:\n                checkpoint[""model_par""] = nns[net].module.state_dict()\n            else:\n                checkpoint[""model_par""] = nns[net].state_dict()\n\n            checkpoint[""optimizer_par""] = optimizers[net].state_dict()\n\n            out_file = info_file.replace("".info"", ""_"" + arch_dict[net][0] + "".pkl"")\n            torch.save(checkpoint, out_file)\n\n    if to_do == ""forward"":\n        for out_name in forward_outs:\n            post_file[out_name].close()\n\n    # Write info file\n    with open(info_file, ""w"") as text_file:\n        text_file.write(""[results]\\n"")\n        if to_do != ""forward"":\n            text_file.write(""loss=%s\\n"" % loss_tot.cpu().numpy())\n            text_file.write(""err=%s\\n"" % err_tot.cpu().numpy())\n        text_file.write(""elapsed_time_chunk=%f\\n"" % elapsed_time_chunk)\n\n    text_file.close()\n\n    # Getting the data for the next chunk (read in parallel)\n    p.join()\n    data_name = shared_list[0]\n    data_end_index = shared_list[1]\n    fea_dict = shared_list[2]\n    lab_dict = shared_list[3]\n    arch_dict = shared_list[4]\n    data_set = shared_list[5]\n\n    # converting numpy tensors into pytorch tensors and put them on GPUs if specified\n    if not (save_gpumem) and use_cuda:\n        data_set = torch.from_numpy(data_set).float().cuda()\n    else:\n        data_set = torch.from_numpy(data_set).float()\n\n    return [data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict]\n'"
data_io.py,0,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport numpy as np\nimport sys\nfrom utils import compute_cw_max, dict_fea_lab_arch, is_sequential_dict\nimport os\nimport configparser\nimport re, gzip, struct\n\n\ndef load_dataset(\n    fea_scp, fea_opts, lab_folder, lab_opts, left, right, max_sequence_length, output_folder, fea_only=False\n):\n    def _input_is_wav_file(fea_scp):\n        with open(fea_scp, ""r"") as f:\n            first_line = f.readline()\n        ark_file = first_line.split("" "")[1].split("":"")[0]\n        with open(ark_file, ""rb"") as f:\n            first_ark_line = f.readline()\n        return b""RIFF"" in first_ark_line\n\n    def _input_is_feature_file(fea_scp):\n        return not _input_is_wav_file(fea_scp)\n\n    def _read_features_and_labels_with_kaldi(fea_scp, fea_opts, fea_only, lab_folder, lab_opts, output_folder):\n        fea = dict()\n        lab = dict()\n        if _input_is_feature_file(fea_scp):\n            kaldi_bin = ""copy-feats""\n            read_function = read_mat_ark\n        elif _input_is_wav_file(fea_scp):\n            kaldi_bin = ""wav-copy""\n            read_function = read_vec_flt_ark\n        fea = {\n            k: m\n            for k, m in read_function(""ark:"" + kaldi_bin + "" scp:"" + fea_scp + "" ark:- |"" + fea_opts, output_folder)\n        }\n        if not fea_only:\n            lab = {\n                k: v\n                for k, v in read_vec_int_ark(\n                    ""gunzip -c "" + lab_folder + ""/ali*.gz | "" + lab_opts + "" "" + lab_folder + ""/final.mdl ark:- ark:-|"",\n                    output_folder,\n                )\n                if k in fea\n            }  # Note that I\'m copying only the aligments of the loaded fea\n            fea = {\n                k: v for k, v in fea.items() if k in lab\n            }  # This way I remove all the features without an aligment (see log file in alidir ""Did not Succeded"")\n        return fea, lab\n\n    def _chunk_features_and_labels(max_sequence_length, fea, lab, fea_only, input_is_wav):\n        def _append_to_concat_list(fea_chunked, lab_chunked, fea_conc, lab_conc, name):\n            for j in range(0, len(fea_chunked)):\n                fea_conc.append(fea_chunked[j])\n                lab_conc.append(lab_chunked[j])\n                if len(fea_chunked) > 1:\n                    snt_name.append(name + ""_split"" + str(j))\n                else:\n                    snt_name.append(k)\n            return fea_conc, lab_conc\n\n        def _chunk(max_sequence_length, fea, lab, fea_only):\n            def _chunk_by_input_and_output_chunk_config(chunk_config, fea, lab, fea_only):\n                """""" \n                If the sequence length is above the threshold, we split it with a minimal length max/4\n                If max length = 500, then the split will start at 500 + (500/4) = 625. \n                A seq of length 625 will be splitted in one of 500 and one of 125\n                """"""\n                chunk_size_fea, chunk_step_fea, chunk_size_lab, chunk_step_lab = (\n                    chunk_config[""chunk_size_fea""],\n                    chunk_config[""chunk_step_fea""],\n                    chunk_config[""chunk_size_lab""],\n                    chunk_config[""chunk_step_lab""],\n                )\n                fea_chunked = list()\n                lab_chunked = list()\n                split_threshold_fea = chunk_size_fea + (chunk_size_fea / 4)\n                if (len(fea) > chunk_size_fea) and chunk_size_fea > 0:\n                    nr_of_chunks = (len(fea) + chunk_size_fea - 1) // chunk_size_fea\n                    for i in range(nr_of_chunks):\n                        chunk_start_fea = i * chunk_step_fea\n                        if len(fea[chunk_start_fea:]) > split_threshold_fea:\n                            chunk_end_fea = chunk_start_fea + chunk_size_fea\n                            fea_chunk = fea[chunk_start_fea:chunk_end_fea]\n                            if not fea_only:\n                                chunk_start_lab = i * chunk_step_lab\n                                chunk_end_lab = chunk_start_lab + chunk_size_lab\n                                lab_chunk = lab[chunk_start_lab:chunk_end_lab]\n                            else:\n                                lab_chunk = np.zeros((fea_chunk.shape[0],))\n                            fea_chunked.append(fea_chunk)\n                            lab_chunked.append(lab_chunk)\n                        else:\n                            fea_chunk = fea[chunk_start_fea:]\n                            if not fea_only:\n                                chunk_start_lab = i * chunk_step_lab\n                                lab_chunk = lab[chunk_start_lab:]\n                            else:\n                                lab_chunk = np.zeros((fea_chunk.shape[0],))\n                            lab_chunked.append(lab_chunk)\n                            fea_chunked.append(fea_chunk)\n                            break\n                else:\n                    fea_chunked.append(fea)\n                    if not fea_only:\n                        lab_chunked.append(lab)\n                    else:\n                        lab_chunked.append(np.zeros((fea.shape[0],)))\n                return fea_chunked, lab_chunked\n\n            chunk_config = dict()\n            if type(max_sequence_length) == dict:\n                chunk_config[""chunk_size_fea""] = max_sequence_length[""chunk_size_fea""]\n                chunk_config[""chunk_step_fea""] = max_sequence_length[""chunk_step_fea""]\n                chunk_config[""chunk_size_lab""] = max_sequence_length[""chunk_size_lab""]\n                chunk_config[""chunk_step_lab""] = max_sequence_length[""chunk_step_lab""]\n            elif type(max_sequence_length) == int:\n                chunk_config[""chunk_size_fea""] = max_sequence_length\n                chunk_config[""chunk_step_fea""] = max_sequence_length\n                chunk_config[""chunk_size_lab""] = max_sequence_length\n                chunk_config[""chunk_step_lab""] = max_sequence_length\n            else:\n                raise ValueError(""Unknown type of max_sequence_length"")\n            return _chunk_by_input_and_output_chunk_config(chunk_config, fea, lab, fea_only)\n\n        snt_name = list()\n        fea_conc = list()\n        lab_conc = list()\n        feature_keys_soted_by_sequence_length = sorted(sorted(fea.keys()), key=lambda k: len(fea[k]))\n        for k in feature_keys_soted_by_sequence_length:\n            fea_el = fea[k]\n            lab_el = None\n            if not fea_only:\n                lab_el = lab[k]\n            fea_chunked, lab_chunked = _chunk(max_sequence_length, fea_el, lab_el, fea_only)\n            fea_conc, lab_conc = _append_to_concat_list(fea_chunked, lab_chunked, fea_conc, lab_conc, k)\n        return fea_conc, lab_conc, snt_name\n\n    def _concatenate_features_and_labels(fea_conc, lab_conc):\n        def _sort_chunks_by_length(fea_conc, lab_conc):\n            fea_zipped = zip(fea_conc, lab_conc)\n            fea_sorted = sorted(fea_zipped, key=lambda x: x[0].shape[0])\n            fea_conc, lab_conc = zip(*fea_sorted)\n            return fea_conc, lab_conc\n\n        def _get_end_index_from_list(conc):\n            end_snt = 0\n            end_index = list()\n            for entry in conc:\n                end_snt = end_snt + entry.shape[0]\n                end_index.append(end_snt)\n            return end_index\n\n        fea_conc, lab_conc = _sort_chunks_by_length(fea_conc, lab_conc)\n        end_index_fea = _get_end_index_from_list(fea_conc)\n        end_index_lab = _get_end_index_from_list(lab_conc)\n        fea_conc = np.concatenate(fea_conc)\n        lab_conc = np.concatenate(lab_conc)\n        return fea_conc, lab_conc, end_index_fea, end_index_lab\n\n    def _match_feature_and_label_sequence_lengths(fea, lab, max_sequence_length):\n        ALLOW_FRAME_DIFF_LARGER_ONE = False\n\n        def _adjust_feature_sequence_length(fea, nr_of_fea_for_lab):\n            nr_of_fea = fea.shape[0]\n            if nr_of_fea > nr_of_fea_for_lab:\n                fea_adj = np.take(fea, range(nr_of_fea_for_lab), axis=0)\n            elif nr_of_fea < nr_of_fea_for_lab:\n                padding = np.zeros(shape=(nr_of_fea_for_lab - nr_of_fea,) + fea.shape[1:])\n                fea_adj = np.concatenate([fea, padding], axis=0)\n            else:\n                fea_adj = fea\n            return fea_adj\n\n        chunk_size_fea = max_sequence_length[""chunk_size_fea""]\n        chunk_step_fea = max_sequence_length[""chunk_step_fea""]\n        chunk_size_lab = max_sequence_length[""chunk_size_lab""]\n        chunk_step_lab = max_sequence_length[""chunk_step_lab""]\n        window_shift = max_sequence_length[""window_shift""]\n        window_size = max_sequence_length[""window_size""]\n        for k in fea.keys():\n            nr_of_fea = fea[k].shape[0]\n            nr_of_lab = lab[k].shape[0]\n            nr_of_fea_for_lab = (nr_of_lab - 1) * window_shift + window_size\n            if abs(nr_of_fea - nr_of_fea_for_lab) > window_shift and not ALLOW_FRAME_DIFF_LARGER_ONE:\n                raise ValueError(\n                    ""Nr. of features: ""\n                    + str(nr_of_fea)\n                    + "" does not match nr. of labels: ""\n                    + str(nr_of_lab)\n                    + "" with expected nr. of features: ""\n                    + str(nr_of_fea_for_lab)\n                )\n            fea[k] = _adjust_feature_sequence_length(fea[k], nr_of_fea_for_lab)\n        return fea, lab\n\n    fea, lab = _read_features_and_labels_with_kaldi(fea_scp, fea_opts, fea_only, lab_folder, lab_opts, output_folder)\n    if _input_is_wav_file(fea_scp) and (not fea_only):\n        fea, lab = _match_feature_and_label_sequence_lengths(fea, lab, max_sequence_length)\n    fea_chunks, lab_chunks, chunk_names = _chunk_features_and_labels(\n        max_sequence_length, fea, lab, fea_only, _input_is_wav_file(fea_scp)\n    )\n    fea_conc, lab_conc, end_index_fea, end_index_lab = _concatenate_features_and_labels(fea_chunks, lab_chunks)\n    return [chunk_names, fea_conc, lab_conc, np.asarray(end_index_fea), np.asarray(end_index_lab)]\n\n\ndef context_window_old(fea, left, right):\n\n    N_row = fea.shape[0]\n    N_fea = fea.shape[1]\n    frames = np.empty((N_row - left - right, N_fea * (left + right + 1)))\n\n    for frame_index in range(left, N_row - right):\n        right_context = fea[frame_index + 1 : frame_index + right + 1].flatten()  # right context\n        left_context = fea[frame_index - left : frame_index].flatten()  # left context\n        current_frame = np.concatenate([left_context, fea[frame_index], right_context])\n        frames[frame_index - left] = current_frame\n\n    return frames\n\n\ndef context_window(fea, left, right):\n\n    N_elem = fea.shape[0]\n    N_fea = fea.shape[1]\n\n    fea_conc = np.empty([N_elem, N_fea * (left + right + 1)])\n\n    index_fea = 0\n    for lag in range(-left, right + 1):\n        fea_conc[:, index_fea : index_fea + fea.shape[1]] = np.roll(fea, -lag, axis=0)\n        index_fea = index_fea + fea.shape[1]\n\n    fea_conc = fea_conc[left : fea_conc.shape[0] - right]\n    return fea_conc\n\n\ndef load_chunk(\n    fea_scp, fea_opts, lab_folder, lab_opts, left, right, max_sequence_length, output_folder, fea_only=False\n):\n\n    # open the file\n    [data_name, data_set, data_lab, end_index_fea, end_index_lab] = load_dataset(\n        fea_scp, fea_opts, lab_folder, lab_opts, left, right, max_sequence_length, output_folder, fea_only\n    )\n\n    # TODO: currently end_index_lab is ignored\n\n    # Context window\n    if left != 0 or right != 0:\n        data_set = context_window(data_set, left, right)\n\n    end_index_fea = end_index_fea - left\n    end_index_fea[-1] = end_index_fea[-1] - right\n\n    # mean and variance normalization\n    data_set = (data_set - np.mean(data_set, axis=0)) / np.std(data_set, axis=0)\n\n    # Label processing\n    data_lab = data_lab - data_lab.min()\n    if right > 0:\n        data_lab = data_lab[left:-right]\n    else:\n        data_lab = data_lab[left:]\n\n    data_set = np.column_stack((data_set, data_lab))\n\n    return [data_name, data_set, end_index_fea]\n\n\ndef load_counts(class_counts_file):\n    with open(class_counts_file) as f:\n        row = next(f).strip().strip(""[]"").strip()\n        counts = np.array([np.float32(v) for v in row.split()])\n    return counts\n\n\ndef read_lab_fea_refac01(cfg_file, fea_only, shared_list, output_folder):\n    def _read_chunk_specific_config(cfg_file):\n        if not (os.path.exists(cfg_file)):\n            sys.stderr.write(""ERROR: The config file %s does not exist!\\n"" % (cfg_file))\n            sys.exit(0)\n        else:\n            config = configparser.ConfigParser()\n            config.read(cfg_file)\n        return config\n\n    def _read_from_config(config, fea_only):\n        def _get_max_seq_length_from_config_str(config_str):\n            max_seq_length = [int(e) for e in config_str.split("","")]\n            if len(max_seq_length) == 1:\n                max_seq_length = max_seq_length[0]\n            else:\n                assert len(max_seq_length) == 6\n                max_seq_length_list = max_seq_length\n                max_seq_length = dict()\n                max_seq_length[""chunk_size_fea""] = max_seq_length_list[0]\n                max_seq_length[""chunk_step_fea""] = max_seq_length_list[1]\n                max_seq_length[""chunk_size_lab""] = max_seq_length_list[2]\n                max_seq_length[""chunk_step_lab""] = max_seq_length_list[3]\n                max_seq_length[""window_shift""] = max_seq_length_list[4]\n                max_seq_length[""window_size""] = max_seq_length_list[5]\n            return max_seq_length\n\n        to_do = config[""exp""][""to_do""]\n        if to_do == ""train"":\n            max_seq_length = _get_max_seq_length_from_config_str(config[""batches""][""max_seq_length_train""])\n        if to_do == ""valid"":\n            max_seq_length = _get_max_seq_length_from_config_str(config[""batches""][""max_seq_length_valid""])\n        if to_do == ""forward"":\n            max_seq_length = -1  # do to break forward sentences\n            fea_only = True\n        fea_dict, lab_dict, arch_dict = dict_fea_lab_arch(config, fea_only)\n        seq_model = is_sequential_dict(config, arch_dict)\n        return to_do, max_seq_length, fea_dict, lab_dict, arch_dict, seq_model\n\n    def _read_features_and_labels(fea_dict, lab_dict, max_seq_length, fea_only, output_folder):\n        def _get_fea_config_from_dict(fea_dict_entr):\n            fea_scp = fea_dict_entr[1]\n            fea_opts = fea_dict_entr[2]\n            cw_left = int(fea_dict_entr[3])\n            cw_right = int(fea_dict_entr[4])\n            return fea_scp, fea_opts, cw_left, cw_right\n\n        def _get_lab_config_from_dict(lab_dict_entr, fea_only):\n            if fea_only:\n                lab_folder = None\n                lab_opts = None\n            else:\n                lab_folder = lab_dict_entr[1]\n                lab_opts = lab_dict_entr[2]\n            return lab_folder, lab_opts\n\n        def _compensate_for_different_context_windows(\n            data_set_fea,\n            data_set_lab,\n            cw_left_max,\n            cw_left,\n            cw_right_max,\n            cw_right,\n            data_end_index_fea,\n            data_end_index_lab,\n        ):\n            data_set_lab = np.take(\n                data_set_lab,\n                range(cw_left_max - cw_left, data_set_lab.shape[0] - (cw_right_max - cw_right)),\n                axis=0,\n                mode=""clip"",\n            )\n            data_set_fea = np.take(\n                data_set_fea,\n                range(cw_left_max - cw_left, data_set_fea.shape[0] - (cw_right_max - cw_right)),\n                axis=0,\n                mode=""clip"",\n            )\n            data_end_index_fea = data_end_index_fea - (cw_left_max - cw_left)\n            data_end_index_lab = data_end_index_lab - (cw_left_max - cw_left)\n            data_end_index_fea[-1] = data_end_index_fea[-1] - (cw_right_max - cw_right)\n            data_end_index_lab[-1] = data_end_index_lab[-1] - (cw_right_max - cw_right)\n            return data_set_lab, data_set_fea, data_end_index_fea, data_end_index_lab\n\n        def _update_data(data_set, labs, fea_dict, fea, fea_index, data_set_fea, labs_fea, cnt_fea, cnt_lab):\n            if cnt_fea == 0 and cnt_lab == 0:\n                data_set = data_set_fea\n                labs = labs_fea\n                fea_dict[fea].append(fea_index)\n                fea_index = fea_index + data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6] - fea_dict[fea][5])\n            elif cnt_fea == 0 and (not cnt_lab == 0):\n                labs = np.column_stack((labs, labs_fea))\n            elif (not cnt_fea == 0) and cnt_lab == 0:\n                data_set = np.column_stack((data_set, data_set_fea))\n                fea_dict[fea].append(fea_index)\n                fea_index = fea_index + data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6] - fea_dict[fea][5])\n            return data_set, labs, fea_dict, fea_index\n\n        def _check_consistency(\n            data_name,\n            data_name_fea,\n            data_end_index_fea_ini,\n            data_end_index_fea,\n            data_end_index_lab_ini,\n            data_end_index_lab,\n        ):\n            if not (data_name == data_name_fea):\n                sys.stderr.write(\n                    \'ERROR: different sentence ids are detected for the different features. Plase check again input feature lists""\\n\'\n                )\n                sys.exit(0)\n            if not (data_end_index_fea_ini == data_end_index_fea).all():\n                sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                sys.exit(0)\n            if not (data_end_index_lab_ini == data_end_index_lab).all():\n                sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                sys.exit(0)\n\n        def _update_lab_dict(lab_dict, data_set):\n            cnt_lab = 0\n            for lab in lab_dict.keys():\n                lab_dict[lab].append(data_set.shape[1] + cnt_lab)\n                cnt_lab = cnt_lab + 1\n            return lab_dict\n\n        def _load_chunk_refac01(\n            fea_scp, fea_opts, lab_folder, lab_opts, left, right, max_sequence_length, output_folder, fea_only=False\n        ):\n            [data_name, data_set, data_lab, end_index_fea, end_index_lab] = load_dataset(\n                fea_scp, fea_opts, lab_folder, lab_opts, left, right, max_sequence_length, output_folder, fea_only\n            )\n            # TODO: this function will currently only work well if no context window is given or fea and lab have the same time dimensionality\n            # Context window\n            if left != 0 or right != 0:\n                data_set = context_window(data_set, left, right)\n            end_index_fea = end_index_fea - left\n            end_index_lab = end_index_lab - left\n            end_index_fea[-1] = end_index_fea[-1] - right\n            end_index_lab[-1] = end_index_lab[-1] - right\n            # mean and variance normalization\n            data_set = (data_set - np.mean(data_set, axis=0)) / np.std(data_set, axis=0)\n            # Label processing\n            data_lab = data_lab - data_lab.min()\n            if right > 0:\n                data_lab = data_lab[left:-right]\n            else:\n                data_lab = data_lab[left:]\n            if len(data_set.shape) == 1:\n                data_set = np.expand_dims(data_set, -1)\n            return [data_name, data_set, data_lab, end_index_fea, end_index_lab]\n\n        cw_left_max, cw_right_max = compute_cw_max(fea_dict)\n        fea_index = 0\n        cnt_fea = 0\n        data_name = None\n        data_end_index_fea_ini = None\n        data_end_index_lab_ini = None\n        data_set = None\n        labs = None\n        for fea in fea_dict.keys():\n            fea_scp, fea_opts, cw_left, cw_right = _get_fea_config_from_dict(fea_dict[fea])\n            cnt_lab = 0\n            if fea_only:\n                lab_dict.update({""lab_name"": ""none""})\n            for lab in lab_dict.keys():\n                lab_folder, lab_opts = _get_lab_config_from_dict(lab_dict[lab], fea_only)\n                data_name_fea, data_set_fea, data_set_lab, data_end_index_fea, data_end_index_lab = _load_chunk_refac01(\n                    fea_scp, fea_opts, lab_folder, lab_opts, cw_left, cw_right, max_seq_length, output_folder, fea_only\n                )\n                if sum([abs(e) for e in [cw_left_max, cw_right_max, cw_left, cw_right]]) != 0:\n                    data_set_lab, data_set_fea, data_end_index_fea, data_end_index_lab = _compensate_for_different_context_windows(\n                        data_set_fea,\n                        data_set_lab,\n                        cw_left_max,\n                        cw_left,\n                        cw_right_max,\n                        cw_right,\n                        data_end_index_fea,\n                        data_end_index_lab,\n                    )\n                if cnt_fea == 0 and cnt_lab == 0:\n                    data_end_index_fea_ini = data_end_index_fea\n                    data_end_index_lab_ini = data_end_index_lab\n                    data_name = data_name_fea\n                data_set, labs, fea_dict, fea_index = _update_data(\n                    data_set, labs, fea_dict, fea, fea_index, data_set_fea, data_set_lab, cnt_fea, cnt_lab\n                )\n                _check_consistency(\n                    data_name,\n                    data_name_fea,\n                    data_end_index_fea_ini,\n                    data_end_index_fea,\n                    data_end_index_lab_ini,\n                    data_end_index_lab,\n                )\n                cnt_lab = cnt_lab + 1\n            cnt_fea = cnt_fea + 1\n        if not fea_only:\n            lab_dict = _update_lab_dict(lab_dict, data_set)\n        return data_name, data_end_index_fea_ini, data_end_index_lab_ini, fea_dict, lab_dict, data_set, labs\n\n    def _reorder_data_set(data_set, labs, seq_model, to_do):\n        if not (seq_model) and to_do != ""forward"" and (data_set.shape[0] == labs.shape[0]):\n            data_set_shape = data_set.shape[1]\n            data_set_joint = np.column_stack((data_set, labs))\n            np.random.shuffle(data_set)\n            data_set = data_set_joint[:, :data_set_shape]\n            labs = np.squeeze(data_set_joint[:, data_set_shape:], axis=-1)\n        return data_set, labs\n\n    def _append_to_shared_list(\n        shared_list, data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set\n    ):\n        shared_list.append(data_name)\n        shared_list.append(data_end_index_fea)\n        shared_list.append(data_end_index_lab)\n        shared_list.append(fea_dict)\n        shared_list.append(lab_dict)\n        shared_list.append(arch_dict)\n        shared_list.append(data_set)\n        return shared_list\n\n    config = _read_chunk_specific_config(cfg_file)\n    to_do, max_seq_length, fea_dict, lab_dict, arch_dict, seq_model = _read_from_config(config, fea_only)\n    data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, data_set, labs = _read_features_and_labels(\n        fea_dict, lab_dict, max_seq_length, fea_only, output_folder\n    )\n    data_set, labs = _reorder_data_set(data_set, labs, seq_model, to_do)\n    data_set = {""input"": data_set, ""ref"": labs}\n    shared_list = _append_to_shared_list(\n        shared_list, data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set\n    )\n\n\ndef read_lab_fea(cfg_file, fea_only, shared_list, output_folder):\n\n    # Reading chunk-specific cfg file (first argument-mandatory file)\n    if not (os.path.exists(cfg_file)):\n        sys.stderr.write(""ERROR: The config file %s does not exist!\\n"" % (cfg_file))\n        sys.exit(0)\n    else:\n        config = configparser.ConfigParser()\n        config.read(cfg_file)\n\n    # Reading some cfg parameters\n    to_do = config[""exp""][""to_do""]\n\n    if to_do == ""train"":\n        max_seq_length = int(\n            config[""batches""][""max_seq_length_train""]\n        )  # *(int(info_file[-13:-10])+1) # increasing over the epochs\n\n    if to_do == ""valid"":\n        max_seq_length = int(config[""batches""][""max_seq_length_valid""])\n\n    if to_do == ""forward"":\n        max_seq_length = -1  # do to break forward sentences\n\n    [fea_dict, lab_dict, arch_dict] = dict_fea_lab_arch(config, fea_only)\n    [cw_left_max, cw_right_max] = compute_cw_max(fea_dict)\n\n    fea_index = 0\n    cnt_fea = 0\n    for fea in fea_dict.keys():\n\n        # reading the features\n        fea_scp = fea_dict[fea][1]\n        fea_opts = fea_dict[fea][2]\n        cw_left = int(fea_dict[fea][3])\n        cw_right = int(fea_dict[fea][4])\n\n        cnt_lab = 0\n\n        # Production case, we don\'t have labels (lab_name = none)\n        if fea_only:\n            lab_dict.update({""lab_name"": ""none""})\n        for lab in lab_dict.keys():\n            # Production case, we don\'t have labels (lab_name = none)\n            if fea_only:\n                lab_folder = None\n                lab_opts = None\n            else:\n                lab_folder = lab_dict[lab][1]\n                lab_opts = lab_dict[lab][2]\n\n            [data_name_fea, data_set_fea, data_end_index_fea] = load_chunk(\n                fea_scp, fea_opts, lab_folder, lab_opts, cw_left, cw_right, max_seq_length, output_folder, fea_only\n            )\n\n            # making the same dimenion for all the features (compensating for different context windows)\n            labs_fea = data_set_fea[cw_left_max - cw_left : data_set_fea.shape[0] - (cw_right_max - cw_right), -1]\n            data_set_fea = data_set_fea[cw_left_max - cw_left : data_set_fea.shape[0] - (cw_right_max - cw_right), 0:-1]\n            data_end_index_fea = data_end_index_fea - (cw_left_max - cw_left)\n            data_end_index_fea[-1] = data_end_index_fea[-1] - (cw_right_max - cw_right)\n\n            if cnt_fea == 0 and cnt_lab == 0:\n                data_set = data_set_fea\n                labs = labs_fea\n                data_end_index = data_end_index_fea\n                data_end_index = data_end_index_fea\n                data_name = data_name_fea\n\n                fea_dict[fea].append(fea_index)\n                fea_index = fea_index + data_set_fea.shape[1]\n                fea_dict[fea].append(fea_index)\n                fea_dict[fea].append(fea_dict[fea][6] - fea_dict[fea][5])\n\n            else:\n                if cnt_fea == 0:\n                    labs = np.column_stack((labs, labs_fea))\n\n                if cnt_lab == 0:\n                    data_set = np.column_stack((data_set, data_set_fea))\n                    fea_dict[fea].append(fea_index)\n                    fea_index = fea_index + data_set_fea.shape[1]\n                    fea_dict[fea].append(fea_index)\n                    fea_dict[fea].append(fea_dict[fea][6] - fea_dict[fea][5])\n\n                # Checks if lab_names are the same for all the features\n                if not (data_name == data_name_fea):\n                    sys.stderr.write(\n                        \'ERROR: different sentence ids are detected for the different features. Plase check again input feature lists""\\n\'\n                    )\n                    sys.exit(0)\n\n                # Checks if end indexes are the same for all the features\n                if not (data_end_index == data_end_index_fea).all():\n                    sys.stderr.write(\'ERROR end_index must be the same for all the sentences""\\n\')\n                    sys.exit(0)\n\n            cnt_lab = cnt_lab + 1\n\n        cnt_fea = cnt_fea + 1\n\n    cnt_lab = 0\n    if not fea_only:\n        for lab in lab_dict.keys():\n            lab_dict[lab].append(data_set.shape[1] + cnt_lab)\n            cnt_lab = cnt_lab + 1\n\n    data_set = np.column_stack((data_set, labs))\n\n    # check automatically if the model is sequential\n    seq_model = is_sequential_dict(config, arch_dict)\n\n    # Randomize if the model is not sequential\n    if not (seq_model) and to_do != ""forward"":\n        np.random.shuffle(data_set)\n\n    # Split dataset in many part. If the dataset is too big, we can have issues to copy it into the shared memory (due to pickle limits)\n    # N_split=10\n    # data_set=np.array_split(data_set, N_split)\n\n    # Adding all the elements in the shared list\n    shared_list.append(data_name)\n    shared_list.append(data_end_index)\n    shared_list.append(fea_dict)\n    shared_list.append(lab_dict)\n    shared_list.append(arch_dict)\n    shared_list.append(data_set)\n\n\n# The following libraries are copied from kaldi-io-for-python project (https://github.com/vesis84/kaldi-io-for-python)\n\n# Copyright 2014-2016  Brno University of Technology (author: Karel Vesely)\n# Licensed under the Apache License, Version 2.0 (the ""License"")\n\n#################################################\n# Define all custom exceptions,\nclass UnsupportedDataType(Exception):\n    pass\n\n\nclass UnknownVectorHeader(Exception):\n    pass\n\n\nclass UnknownMatrixHeader(Exception):\n    pass\n\n\nclass BadSampleSize(Exception):\n    pass\n\n\nclass BadInputFormat(Exception):\n    pass\n\n\nclass SubprocessFailed(Exception):\n    pass\n\n\n#################################################\n# Data-type independent helper functions,\n\n\ndef open_or_fd(file, output_folder, mode=""rb""):\n    """""" fd = open_or_fd(file)\n   Open file, gzipped file, pipe, or forward the file-descriptor.\n   Eventually seeks in the \'file\' argument contains \':offset\' suffix.\n  """"""\n    offset = None\n\n    try:\n        # strip \'ark:\' prefix from r{x,w}filename (optional),\n        if re.search(""^(ark|scp)(,scp|,b|,t|,n?f|,n?p|,b?o|,n?s|,n?cs)*:"", file):\n            (prefix, file) = file.split("":"", 1)\n        # separate offset from filename (optional),\n        if re.search("":[0-9]+$"", file):\n            (file, offset) = file.rsplit("":"", 1)\n        # input pipe?\n        if file[-1] == ""|"":\n            fd = popen(file[:-1], output_folder, ""rb"")  # custom,\n        # output pipe?\n        elif file[0] == ""|"":\n            fd = popen(file[1:], output_folder, ""wb"")  # custom,\n        # is it gzipped?\n        elif file.split(""."")[-1] == ""gz"":\n            fd = gzip.open(file, mode)\n        # a normal file...\n        else:\n            fd = open(file, mode)\n    except TypeError:\n        # \'file\' is opened file descriptor,\n        fd = file\n    # Eventually seek to offset,\n    if offset != None:\n        fd.seek(int(offset))\n\n    return fd\n\n\n# based on \'/usr/local/lib/python3.4/os.py\'\ndef popen(cmd, output_folder, mode=""rb""):\n    if not isinstance(cmd, str):\n        raise TypeError(""invalid cmd type (%s, expected string)"" % type(cmd))\n\n    import subprocess, io, threading\n\n    # cleanup function for subprocesses,\n    def cleanup(proc, cmd):\n        ret = proc.wait()\n        if ret > 0:\n            raise SubprocessFailed(""cmd %s returned %d !"" % (cmd, ret))\n        return\n\n    # text-mode,\n    if mode == ""r"":\n        err = open(output_folder + ""/log.log"", ""a"")\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=err)\n        threading.Thread(target=cleanup, args=(proc, cmd)).start()  # clean-up thread,\n        return io.TextIOWrapper(proc.stdout)\n    elif mode == ""w"":\n        err = open(output_folder + ""/log.log"", ""a"")\n        proc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stderr=err)\n        threading.Thread(target=cleanup, args=(proc, cmd)).start()  # clean-up thread,\n        return io.TextIOWrapper(proc.stdin)\n    # binary,\n    elif mode == ""rb"":\n        err = open(output_folder + ""/log.log"", ""a"")\n        proc = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=err)\n        threading.Thread(target=cleanup, args=(proc, cmd)).start()  # clean-up thread,\n        return proc.stdout\n    elif mode == ""wb"":\n        err = open(output_folder + ""/log.log"", ""a"")\n        proc = subprocess.Popen(cmd, shell=True, stdin=subprocess.PIPE, stderr=err)\n        threading.Thread(target=cleanup, args=(proc, cmd)).start()  # clean-up thread,\n        return proc.stdin\n    # sanity,\n    else:\n        raise ValueError(""invalid mode %s"" % mode)\n\n\ndef read_key(fd):\n    """""" [key] = read_key(fd)\n   Read the utterance-key from the opened ark/stream descriptor \'fd\'.\n  """"""\n    key = """"\n    while 1:\n        char = fd.read(1).decode(""latin1"")\n        if char == """":\n            break\n        if char == "" "":\n            break\n        key += char\n    key = key.strip()\n    if key == """":\n        return None  # end of file,\n    assert re.match(""^\\S+$"", key) != None  # check format (no whitespace!)\n    return key\n\n\n#################################################\n# Integer vectors (alignments, ...),\n\n\ndef read_ali_ark(file_or_fd, output_folder):\n    """""" Alias to \'read_vec_int_ark()\' """"""\n    return read_vec_int_ark(file_or_fd, output_folder)\n\n\ndef read_vec_int_ark(file_or_fd, output_folder):\n    """""" generator(key,vec) = read_vec_int_ark(file_or_fd)\n   Create generator of (key,vector<int>) tuples, which reads from the ark file/stream.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Read ark to a \'dictionary\':\n   d = { u:d for u,d in kaldi_io.read_vec_int_ark(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        key = read_key(fd)\n        while key:\n            ali = read_vec_int(fd, output_folder)\n            yield key, ali\n            key = read_key(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_vec_int(file_or_fd, output_folder):\n    """""" [int-vec] = read_vec_int(file_or_fd)\n   Read kaldi integer vector, ascii or binary input,\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    binary = fd.read(2).decode()\n    if binary == ""\\0B"":  # binary flag\n        assert fd.read(1).decode() == ""\\4""\n        # int-size\n        vec_size = np.frombuffer(fd.read(4), dtype=""int32"", count=1)[0]  # vector dim\n        if vec_size == 0:\n            return np.array([], dtype=""int32"")\n        # Elements from int32 vector are sored in tuples: (sizeof(int32), value),\n        vec = np.frombuffer(fd.read(vec_size * 5), dtype=[(""size"", ""int8""), (""value"", ""int32"")], count=vec_size)\n        assert vec[0][""size""] == 4  # int32 size,\n        ans = vec[:][""value""]  # values are in 2nd column,\n    else:  # ascii,\n        arr = (binary + fd.readline().decode()).strip().split()\n        try:\n            arr.remove(""["")\n            arr.remove(""]"")  # optionally\n        except ValueError:\n            pass\n        ans = np.array(arr, dtype=int)\n    if fd is not file_or_fd:\n        fd.close()  # cleanup\n    return ans\n\n\n# Writing,\ndef write_vec_int(file_or_fd, output_folder, v, key=""""):\n    """""" write_vec_int(f, v, key=\'\')\n   Write a binary kaldi integer vector to filename or stream.\n   Arguments:\n   file_or_fd : filename or opened file descriptor for writing,\n   v : the vector to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the vector.\n\n   Example of writing single vector:\n   kaldi_io.write_vec_int(filename, vec)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,vec in dict.iteritems():\n       kaldi_io.write_vec_flt(f, vec, key=key)\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder, mode=""wb"")\n    if sys.version_info[0] == 3:\n        assert fd.mode == ""wb""\n    try:\n        if key != """":\n            fd.write((key + "" "").encode(""latin1""))  # ark-files have keys (utterance-id),\n        fd.write(""\\0B"".encode())  # we write binary!\n        # dim,\n        fd.write(""\\4"".encode())  # int32 type,\n        fd.write(struct.pack(np.dtype(""int32"").char, v.shape[0]))\n        # data,\n        for i in range(len(v)):\n            fd.write(""\\4"".encode())  # int32 type,\n            fd.write(struct.pack(np.dtype(""int32"").char, v[i]))  # binary,\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\n#################################################\n# Float vectors (confidences, ivectors, ...),\n\n# Reading,\ndef read_vec_flt_scp(file_or_fd, output_folder):\n    """""" generator(key,mat) = read_vec_flt_scp(file_or_fd)\n   Returns generator of (key,vector) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,vec in kaldi_io.read_vec_flt_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_scp(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        for line in fd:\n            (key, rxfile) = line.decode().split("" "")\n            vec = read_vec_flt(rxfile, output_folder)\n            yield key, vec\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_vec_flt_ark(file_or_fd, output_folder):\n    """""" generator(key,vec) = read_vec_flt_ark(file_or_fd)\n   Create generator of (key,vector<float>) tuples, reading from an ark file/stream.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Read ark to a \'dictionary\':\n   d = { u:d for u,d in kaldi_io.read_vec_flt_ark(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        key = read_key(fd)\n        while key:\n            ali = read_vec_flt(fd, output_folder)\n            yield key, ali\n            key = read_key(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_vec_flt(file_or_fd, output_folder):\n    """""" [flt-vec] = read_vec_flt(file_or_fd)\n   Read kaldi float vector, ascii or binary input,\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    binary = fd.read(2).decode()\n    if binary == ""\\0B"":  # binary flag\n        return _read_vec_flt_binary(fd)\n    elif binary == ""RI"":\n        return _read_vec_flt_riff(fd)\n    else:  # ascii,\n        arr = (binary + fd.readline().decode()).strip().split()\n        try:\n            arr.remove(""["")\n            arr.remove(""]"")  # optionally\n        except ValueError:\n            pass\n        ans = np.array(arr, dtype=float)\n    if fd is not file_or_fd:\n        fd.close()  # cleanup\n    return ans\n\n\ndef _read_vec_flt_riff(fd):\n    RIFF_CHUNK_DESCR_HEADER_SIZE = 12\n    ALREADY_READ_HEADER_BYTES = 2\n    SUB_CHUNK_HEADER_SIZE = 8\n    DATA_CHUNK_HEADER_SIZE = 8\n\n    def pcm2float(signal, dtype=""float32""):\n        signal = np.asarray(signal)\n        dtype = np.dtype(dtype)\n        return signal.astype(dtype) / dtype.type(-np.iinfo(signal.dtype).min)\n\n    import struct\n\n    header = fd.read(RIFF_CHUNK_DESCR_HEADER_SIZE - ALREADY_READ_HEADER_BYTES)\n    assert header[:2] == b""FF""\n    chunk_header = fd.read(SUB_CHUNK_HEADER_SIZE)\n    subchunk_id, subchunk_size = struct.unpack(""<4sI"", chunk_header)\n    aformat, channels, samplerate, byterate, block_align, bps = struct.unpack(""HHIIHH"", fd.read(subchunk_size))\n    subchunk2_id, subchunk2_size = struct.unpack(""<4sI"", fd.read(DATA_CHUNK_HEADER_SIZE))\n    pcm_data = np.frombuffer(fd.read(subchunk2_size), dtype=""int"" + str(bps))\n    return pcm2float(pcm_data)\n\n\ndef _read_vec_flt_binary(fd):\n    header = fd.read(3).decode()\n    if header == ""FV "":\n        sample_size = 4  # floats\n    elif header == ""DV "":\n        sample_size = 8  # doubles\n    else:\n        raise UnknownVectorHeader(""The header contained \'%s\'"" % header)\n    assert sample_size > 0\n    # Dimension,\n    assert fd.read(1).decode() == ""\\4""\n    # int-size\n    vec_size = np.frombuffer(fd.read(4), dtype=""int32"", count=1)[0]  # vector dim\n    if vec_size == 0:\n        return np.array([], dtype=""float32"")\n    # Read whole vector,\n    buf = fd.read(vec_size * sample_size)\n    if sample_size == 4:\n        ans = np.frombuffer(buf, dtype=""float32"")\n    elif sample_size == 8:\n        ans = np.frombuffer(buf, dtype=""float64"")\n    else:\n        raise BadSampleSize\n    return ans\n\n\n# Writing,\ndef write_vec_flt(file_or_fd, output_folder, v, key=""""):\n    """""" write_vec_flt(f, v, key=\'\')\n   Write a binary kaldi vector to filename or stream. Supports 32bit and 64bit floats.\n   Arguments:\n   file_or_fd : filename or opened file descriptor for writing,\n   v : the vector to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the vector.\n\n   Example of writing single vector:\n   kaldi_io.write_vec_flt(filename, vec)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,vec in dict.iteritems():\n       kaldi_io.write_vec_flt(f, vec, key=key)\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder, mode=""wb"")\n    if sys.version_info[0] == 3:\n        assert fd.mode == ""wb""\n    try:\n        if key != """":\n            fd.write((key + "" "").encode(""latin1""))  # ark-files have keys (utterance-id),\n        fd.write(""\\0B"".encode())  # we write binary!\n        # Data-type,\n        if v.dtype == ""float32"":\n            fd.write(""FV "".encode())\n        elif v.dtype == ""float64"":\n            fd.write(""DV "".encode())\n        else:\n            raise UnsupportedDataType(""\'%s\', please use \'float32\' or \'float64\'"" % v.dtype)\n        # Dim,\n        fd.write(""\\04"".encode())\n        fd.write(struct.pack(np.dtype(""uint32"").char, v.shape[0]))  # dim\n        # Data,\n        fd.write(v.tobytes())\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\n#################################################\n# Float matrices (features, transformations, ...),\n\n# Reading,\ndef read_mat_scp(file_or_fd, output_folder):\n    """""" generator(key,mat) = read_mat_scp(file_or_fd)\n   Returns generator of (key,matrix) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,mat in kaldi_io.read_mat_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_scp(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        for line in fd:\n            (key, rxfile) = line.decode().split("" "")\n            mat = read_mat(rxfile, output_folder)\n            yield key, mat\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_mat_ark(file_or_fd, output_folder):\n    """""" generator(key,mat) = read_mat_ark(file_or_fd)\n   Returns generator of (key,matrix) tuples, read from ark file/stream.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,mat in kaldi_io.read_mat_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:mat for key,mat in kaldi_io.read_mat_ark(file) }\n  """"""\n\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        key = read_key(fd)\n        while key:\n            mat = read_mat(fd, output_folder)\n            yield key, mat\n            key = read_key(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_mat(file_or_fd, output_folder):\n    """""" [mat] = read_mat(file_or_fd)\n   Reads single kaldi matrix, supports ascii and binary.\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        binary = fd.read(2).decode()\n        if binary == ""\\0B"":\n            mat = _read_mat_binary(fd)\n        else:\n            assert binary == "" [""\n            mat = _read_mat_ascii(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n    return mat\n\n\ndef _read_mat_binary(fd):\n    # Data type\n    header = fd.read(3).decode()\n    # \'CM\', \'CM2\', \'CM3\' are possible values,\n    if header.startswith(""CM""):\n        return _read_compressed_mat(fd, header)\n    elif header == ""FM "":\n        sample_size = 4  # floats\n    elif header == ""DM "":\n        sample_size = 8  # doubles\n    else:\n        raise UnknownMatrixHeader(""The header contained \'%s\'"" % header)\n    assert sample_size > 0\n    # Dimensions\n    s1, rows, s2, cols = np.frombuffer(fd.read(10), dtype=""int8,int32,int8,int32"", count=1)[0]\n    # Read whole matrix\n    buf = fd.read(rows * cols * sample_size)\n    if sample_size == 4:\n        vec = np.frombuffer(buf, dtype=""float32"")\n    elif sample_size == 8:\n        vec = np.frombuffer(buf, dtype=""float64"")\n    else:\n        raise BadSampleSize\n    mat = np.reshape(vec, (rows, cols))\n    return mat\n\n\ndef _read_mat_ascii(fd):\n    rows = []\n    while 1:\n        line = fd.readline().decode()\n        if len(line) == 0:\n            raise BadInputFormat  # eof, should not happen!\n        if len(line.strip()) == 0:\n            continue  # skip empty line\n        arr = line.strip().split()\n        if arr[-1] != ""]"":\n            rows.append(np.array(arr, dtype=""float32""))  # not last line\n        else:\n            rows.append(np.array(arr[:-1], dtype=""float32""))  # last line\n            mat = np.vstack(rows)\n            return mat\n\n\ndef _read_compressed_mat(fd, format):\n    """""" Read a compressed matrix,\n      see: https://github.com/kaldi-asr/kaldi/blob/master/src/matrix/compressed-matrix.h\n      methods: CompressedMatrix::Read(...), CompressedMatrix::CopyToMat(...),\n  """"""\n    assert format == ""CM ""  # The formats CM2, CM3 are not supported...\n\n    # Format of header \'struct\',\n    global_header = np.dtype(\n        [(""minvalue"", ""float32""), (""range"", ""float32""), (""num_rows"", ""int32""), (""num_cols"", ""int32"")]\n    )  # member \'.format\' is not written,\n    per_col_header = np.dtype(\n        [\n            (""percentile_0"", ""uint16""),\n            (""percentile_25"", ""uint16""),\n            (""percentile_75"", ""uint16""),\n            (""percentile_100"", ""uint16""),\n        ]\n    )\n\n    # Read global header,\n    globmin, globrange, rows, cols = np.frombuffer(fd.read(16), dtype=global_header, count=1)[0]\n\n    # The data is structed as [Colheader, ... , Colheader, Data, Data , .... ]\n    #                         {           cols           }{     size         }\n    col_headers = np.frombuffer(fd.read(cols * 8), dtype=per_col_header, count=cols)\n    col_headers = np.array(\n        [np.array([x for x in y]) * globrange * 1.52590218966964e-05 + globmin for y in col_headers], dtype=np.float32\n    )\n    data = np.reshape(\n        np.frombuffer(fd.read(cols * rows), dtype=""uint8"", count=cols * rows), newshape=(cols, rows)\n    )  # stored as col-major,\n\n    mat = np.zeros((cols, rows), dtype=""float32"")\n    p0 = col_headers[:, 0].reshape(-1, 1)\n    p25 = col_headers[:, 1].reshape(-1, 1)\n    p75 = col_headers[:, 2].reshape(-1, 1)\n    p100 = col_headers[:, 3].reshape(-1, 1)\n    mask_0_64 = data <= 64\n    mask_193_255 = data > 192\n    mask_65_192 = ~(mask_0_64 | mask_193_255)\n\n    mat += (p0 + (p25 - p0) / 64.0 * data) * mask_0_64.astype(np.float32)\n    mat += (p25 + (p75 - p25) / 128.0 * (data - 64)) * mask_65_192.astype(np.float32)\n    mat += (p75 + (p100 - p75) / 63.0 * (data - 192)) * mask_193_255.astype(np.float32)\n\n    return mat.T  # transpose! col-major -> row-major,\n\n\n# Writing,\ndef write_mat(output_folder, file_or_fd, m, key=""""):\n    """""" write_mat(f, m, key=\'\')\n  Write a binary kaldi matrix to filename or stream. Supports 32bit and 64bit floats.\n  Arguments:\n   file_or_fd : filename of opened file descriptor for writing,\n   m : the matrix to be stored,\n   key (optional) : used for writing ark-file, the utterance-id gets written before the matrix.\n\n   Example of writing single matrix:\n   kaldi_io.write_mat(filename, mat)\n\n   Example of writing arkfile:\n   with open(ark_file,\'w\') as f:\n     for key,mat in dict.iteritems():\n       kaldi_io.write_mat(f, mat, key=key)\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder, mode=""wb"")\n    if sys.version_info[0] == 3:\n        assert fd.mode == ""wb""\n    try:\n        if key != """":\n            fd.write((key + "" "").encode(""latin1""))  # ark-files have keys (utterance-id),\n        fd.write(""\\0B"".encode())  # we write binary!\n        # Data-type,\n        if m.dtype == ""float32"":\n            fd.write(""FM "".encode())\n        elif m.dtype == ""float64"":\n            fd.write(""DM "".encode())\n        else:\n            raise UnsupportedDataType(""\'%s\', please use \'float32\' or \'float64\'"" % m.dtype)\n        # Dims,\n        fd.write(""\\04"".encode())\n        fd.write(struct.pack(np.dtype(""uint32"").char, m.shape[0]))  # rows\n        fd.write(""\\04"".encode())\n        fd.write(struct.pack(np.dtype(""uint32"").char, m.shape[1]))  # cols\n        # Data,\n        fd.write(m.tobytes())\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\n#################################################\n# \'Posterior\' kaldi type (posteriors, confusion network, nnet1 training targets, ...)\n# Corresponds to: vector<vector<tuple<int,float> > >\n# - outer vector: time axis\n# - inner vector: records at the time\n# - tuple: int = index, float = value\n#\n\n\ndef read_cnet_ark(file_or_fd, output_folder):\n    """""" Alias of function \'read_post_ark()\', \'cnet\' = confusion network """"""\n    return read_post_ark(file_or_fd, output_folder)\n\n\ndef read_post_rxspec(file_):\n    """""" adaptor to read both \'ark:...\' and \'scp:...\' inputs of posteriors,\n  """"""\n    if file_.startswith(""ark:""):\n        return read_post_ark(file_)\n    elif file_.startswith(""scp:""):\n        return read_post_scp(file_)\n    else:\n        print(""unsupported intput type: %s"" % file_)\n        print(""it should begint with \'ark:\' or \'scp:\'"")\n        sys.exit(1)\n\n\ndef read_post_scp(file_or_fd, output_folder):\n    """""" generator(key,post) = read_post_scp(file_or_fd)\n   Returns generator of (key,post) tuples, read according to kaldi scp.\n   file_or_fd : scp, gzipped scp, pipe or opened file descriptor.\n\n   Iterate the scp:\n   for key,post in kaldi_io.read_post_scp(file):\n     ...\n\n   Read scp to a \'dictionary\':\n   d = { key:post for key,post in kaldi_io.read_post_scp(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        for line in fd:\n            (key, rxfile) = line.decode().split("" "")\n            post = read_post(rxfile)\n            yield key, post\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_post_ark(file_or_fd, output_folder):\n    """""" generator(key,vec<vec<int,float>>) = read_post_ark(file)\n   Returns generator of (key,posterior) tuples, read from ark file.\n   file_or_fd : ark, gzipped ark, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,post in kaldi_io.read_post_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:post for key,post in kaldi_io.read_post_ark(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        key = read_key(fd)\n        while key:\n            post = read_post(fd)\n            yield key, post\n            key = read_key(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_post(file_or_fd, output_folder):\n    """""" [post] = read_post(file_or_fd)\n   Reads single kaldi \'Posterior\' in binary format.\n\n   The \'Posterior\' is C++ type \'vector<vector<tuple<int,float> > >\',\n   the outer-vector is usually time axis, inner-vector are the records\n   at given time,  and the tuple is composed of an \'index\' (integer)\n   and a \'float-value\'. The \'float-value\' can represent a probability\n   or any other numeric value.\n\n   Returns vector of vectors of tuples.\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    ans = []\n    binary = fd.read(2).decode()\n    assert binary == ""\\0B""\n    # binary flag\n    assert fd.read(1).decode() == ""\\4""\n    # int-size\n    outer_vec_size = np.frombuffer(fd.read(4), dtype=""int32"", count=1)[0]  # number of frames (or bins)\n\n    # Loop over \'outer-vector\',\n    for i in range(outer_vec_size):\n        assert fd.read(1).decode() == ""\\4""\n        # int-size\n        inner_vec_size = np.frombuffer(fd.read(4), dtype=""int32"", count=1)[0]  # number of records for frame (or bin)\n        data = np.frombuffer(\n            fd.read(inner_vec_size * 10),\n            dtype=[(""size_idx"", ""int8""), (""idx"", ""int32""), (""size_post"", ""int8""), (""post"", ""float32"")],\n            count=inner_vec_size,\n        )\n        assert data[0][""size_idx""] == 4\n        assert data[0][""size_post""] == 4\n        ans.append(data[[""idx"", ""post""]].tolist())\n\n    if fd is not file_or_fd:\n        fd.close()\n    return ans\n\n\n#################################################\n# Kaldi Confusion Network bin begin/end times,\n# (kaldi stores CNs time info separately from the Posterior).\n#\n\n\ndef read_cntime_ark(file_or_fd, output_folder):\n    """""" generator(key,vec<tuple<float,float>>) = read_cntime_ark(file_or_fd)\n   Returns generator of (key,cntime) tuples, read from ark file.\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n\n   Iterate the ark:\n   for key,time in kaldi_io.read_cntime_ark(file):\n     ...\n\n   Read ark to a \'dictionary\':\n   d = { key:time for key,time in kaldi_io.read_post_ark(file) }\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    try:\n        key = read_key(fd)\n        while key:\n            cntime = read_cntime(fd)\n            yield key, cntime\n            key = read_key(fd)\n    finally:\n        if fd is not file_or_fd:\n            fd.close()\n\n\ndef read_cntime(file_or_fd, output_folder):\n    """""" [cntime] = read_cntime(file_or_fd)\n   Reads single kaldi \'Confusion Network time info\', in binary format:\n   C++ type: vector<tuple<float,float> >.\n   (begin/end times of bins at the confusion network).\n\n   Binary layout is \'<num-bins> <beg1> <end1> <beg2> <end2> ...\'\n\n   file_or_fd : file, gzipped file, pipe or opened file descriptor.\n\n   Returns vector of tuples.\n  """"""\n    fd = open_or_fd(file_or_fd, output_folder)\n    binary = fd.read(2).decode()\n    assert binary == ""\\0B""\n    # assuming it\'s binary\n\n    assert fd.read(1).decode() == ""\\4""\n    # int-size\n    vec_size = np.frombuffer(fd.read(4), dtype=""int32"", count=1)[0]  # number of frames (or bins)\n\n    data = np.frombuffer(\n        fd.read(vec_size * 10),\n        dtype=[(""size_beg"", ""int8""), (""t_beg"", ""float32""), (""size_end"", ""int8""), (""t_end"", ""float32"")],\n        count=vec_size,\n    )\n    assert data[0][""size_beg""] == 4\n    assert data[0][""size_end""] == 4\n    ans = data[[""t_beg"", ""t_end""]].tolist()  # Return vector of tuples (t_beg,t_end),\n\n    if fd is not file_or_fd:\n        fd.close()\n    return ans\n\n\n#################################################\n# Segments related,\n#\n\n# Segments as \'Bool vectors\' can be handy,\n# - for \'superposing\' the segmentations,\n# - for frame-selection in Speaker-ID experiments,\ndef read_segments_as_bool_vec(segments_file):\n    """""" [ bool_vec ] = read_segments_as_bool_vec(segments_file)\n   using kaldi \'segments\' file for 1 wav, format : \'<utt> <rec> <t-beg> <t-end>\'\n   - t-beg, t-end is in seconds,\n   - assumed 100 frames/second,\n  """"""\n    segs = np.loadtxt(segments_file, dtype=""object,object,f,f"", ndmin=1)\n    # Sanity checks,\n    assert len(segs) > 0  # empty segmentation is an error,\n    assert len(np.unique([rec[1] for rec in segs])) == 1  # segments with only 1 wav-file,\n    # Convert time to frame-indexes,\n    start = np.rint([100 * rec[2] for rec in segs]).astype(int)\n    end = np.rint([100 * rec[3] for rec in segs]).astype(int)\n    # Taken from \'read_lab_to_bool_vec\', htk.py,\n    frms = np.repeat(\n        np.r_[np.tile([False, True], len(end)), False], np.r_[np.c_[start - np.r_[0, end[:-1]], end - start].flat, 0]\n    )\n    assert np.sum(end - start) == np.sum(frms)\n    return frms\n'"
neural_networks.py,125,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom distutils.util import strtobool\nimport math\nimport json\n\n# uncomment below if you want to use SRU\n# and you need to install SRU: pip install sru[cuda].\n# or you can install it from source code: https://github.com/taolei87/sru.\n# import sru\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super(LayerNorm, self).__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n\n\ndef act_fun(act_type):\n\n    if act_type == ""relu"":\n        return nn.ReLU()\n\n    if act_type == ""tanh"":\n        return nn.Tanh()\n\n    if act_type == ""sigmoid"":\n        return nn.Sigmoid()\n\n    if act_type == ""leaky_relu"":\n        return nn.LeakyReLU(0.2)\n\n    if act_type == ""elu"":\n        return nn.ELU()\n\n    if act_type == ""softmax"":\n        return nn.LogSoftmax(dim=1)\n\n    if act_type == ""linear"":\n        return nn.LeakyReLU(1)  # initializzed like this, but not used in forward!\n\n\nclass MLP(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(MLP, self).__init__()\n\n        self.input_dim = inp_dim\n        self.dnn_lay = list(map(int, options[""dnn_lay""].split("","")))\n        self.dnn_drop = list(map(float, options[""dnn_drop""].split("","")))\n        self.dnn_use_batchnorm = list(map(strtobool, options[""dnn_use_batchnorm""].split("","")))\n        self.dnn_use_laynorm = list(map(strtobool, options[""dnn_use_laynorm""].split("","")))\n        self.dnn_use_laynorm_inp = strtobool(options[""dnn_use_laynorm_inp""])\n        self.dnn_use_batchnorm_inp = strtobool(options[""dnn_use_batchnorm_inp""])\n        self.dnn_act = options[""dnn_act""].split("","")\n\n        self.wx = nn.ModuleList([])\n        self.bn = nn.ModuleList([])\n        self.ln = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n\n        # input layer normalization\n        if self.dnn_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # input batch normalization\n        if self.dnn_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_dnn_lay = len(self.dnn_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_dnn_lay):\n\n            # dropout\n            self.drop.append(nn.Dropout(p=self.dnn_drop[i]))\n\n            # activation\n            self.act.append(act_fun(self.dnn_act[i]))\n\n            add_bias = True\n\n            # layer norm initialization\n            self.ln.append(LayerNorm(self.dnn_lay[i]))\n            self.bn.append(nn.BatchNorm1d(self.dnn_lay[i], momentum=0.05))\n\n            if self.dnn_use_laynorm[i] or self.dnn_use_batchnorm[i]:\n                add_bias = False\n\n            # Linear operations\n            self.wx.append(nn.Linear(current_input, self.dnn_lay[i], bias=add_bias))\n\n            # weight initialization\n            self.wx[i].weight = torch.nn.Parameter(\n                torch.Tensor(self.dnn_lay[i], current_input).uniform_(\n                    -np.sqrt(0.01 / (current_input + self.dnn_lay[i])),\n                    np.sqrt(0.01 / (current_input + self.dnn_lay[i])),\n                )\n            )\n            self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.dnn_lay[i]))\n\n            current_input = self.dnn_lay[i]\n\n        self.out_dim = current_input\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.dnn_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.dnn_use_batchnorm_inp):\n\n            x = self.bn0((x))\n\n        for i in range(self.N_dnn_lay):\n\n            if self.dnn_use_laynorm[i] and not (self.dnn_use_batchnorm[i]):\n                x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n\n            if self.dnn_use_batchnorm[i] and not (self.dnn_use_laynorm[i]):\n                x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n\n            if self.dnn_use_batchnorm[i] == True and self.dnn_use_laynorm[i] == True:\n                x = self.drop[i](self.act[i](self.bn[i](self.ln[i](self.wx[i](x)))))\n\n            if self.dnn_use_batchnorm[i] == False and self.dnn_use_laynorm[i] == False:\n                x = self.drop[i](self.act[i](self.wx[i](x)))\n\n        return x\n\n\nclass LSTM_cudnn(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(LSTM_cudnn, self).__init__()\n\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[""hidden_size""])\n        self.num_layers = int(options[""num_layers""])\n        self.bias = bool(strtobool(options[""bias""]))\n        self.batch_first = bool(strtobool(options[""batch_first""]))\n        self.dropout = float(options[""dropout""])\n        self.bidirectional = bool(strtobool(options[""bidirectional""]))\n\n        self.lstm = nn.ModuleList(\n            [\n                nn.LSTM(\n                    self.input_dim,\n                    self.hidden_size,\n                    self.num_layers,\n                    bias=self.bias,\n                    dropout=self.dropout,\n                    bidirectional=self.bidirectional,\n                )\n            ]\n        )\n\n        for name,param in self.lstm[0].named_parameters():\n            if \'weight_hh\' in name:\n                if self.batch_first:\n                    nn.init.orthogonal_(param)\n            elif \'bias\' in name:\n                nn.init.zeros_(param)\n\n\n        self.out_dim = self.hidden_size + self.bidirectional * self.hidden_size\n\n    def forward(self, x):\n\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers * 2, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers * 2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n            c0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n\n        if x.is_cuda:\n            h0 = h0.cuda()\n            c0 = c0.cuda()\n\n        output, (hn, cn) = self.lstm[0](x, (h0, c0))\n\n        return output\n\n\nclass GRU_cudnn(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(GRU_cudnn, self).__init__()\n\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[""hidden_size""])\n        self.num_layers = int(options[""num_layers""])\n        self.bias = bool(strtobool(options[""bias""]))\n        self.batch_first = bool(strtobool(options[""batch_first""]))\n        self.dropout = float(options[""dropout""])\n        self.bidirectional = bool(strtobool(options[""bidirectional""]))\n\n        self.gru = nn.ModuleList(\n            [\n                nn.GRU(\n                    self.input_dim,\n                    self.hidden_size,\n                    self.num_layers,\n                    bias=self.bias,\n                    dropout=self.dropout,\n                    bidirectional=self.bidirectional,\n                )\n            ]\n        )\n\n        for name,param in self.gru[0].named_parameters():\n            if \'weight_hh\' in name:\n                nn.init.orthogonal_(param)\n            elif \'weight_ih\' in name:\n                nn.init.xavier_uniform_(param)\n            elif \'bias\' in name:\n                nn.init.zeros_(param)\n\n        self.out_dim = self.hidden_size + self.bidirectional * self.hidden_size\n\n    def forward(self, x):\n\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers * 2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n\n        if x.is_cuda:\n            h0 = h0.cuda()\n\n        output, hn = self.gru[0](x, h0)\n\n        return output\n\n\nclass RNN_cudnn(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(RNN_cudnn, self).__init__()\n\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[""hidden_size""])\n        self.num_layers = int(options[""num_layers""])\n        self.nonlinearity = options[""nonlinearity""]\n        self.bias = bool(strtobool(options[""bias""]))\n        self.batch_first = bool(strtobool(options[""batch_first""]))\n        self.dropout = float(options[""dropout""])\n        self.bidirectional = bool(strtobool(options[""bidirectional""]))\n\n        self.rnn = nn.ModuleList(\n            [\n                nn.RNN(\n                    self.input_dim,\n                    self.hidden_size,\n                    self.num_layers,\n                    nonlinearity=self.nonlinearity,\n                    bias=self.bias,\n                    dropout=self.dropout,\n                    bidirectional=self.bidirectional,\n                )\n            ]\n        )\n\n        self.out_dim = self.hidden_size + self.bidirectional * self.hidden_size\n\n    def forward(self, x):\n\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers * 2, x.shape[1], self.hidden_size)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n\n        if x.is_cuda:\n            h0 = h0.cuda()\n\n        output, hn = self.rnn[0](x, h0)\n\n        return output\n\n\nclass LSTM(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(LSTM, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.lstm_lay = list(map(int, options[""lstm_lay""].split("","")))\n        self.lstm_drop = list(map(float, options[""lstm_drop""].split("","")))\n        self.lstm_use_batchnorm = list(map(strtobool, options[""lstm_use_batchnorm""].split("","")))\n        self.lstm_use_laynorm = list(map(strtobool, options[""lstm_use_laynorm""].split("","")))\n        self.lstm_use_laynorm_inp = strtobool(options[""lstm_use_laynorm_inp""])\n        self.lstm_use_batchnorm_inp = strtobool(options[""lstm_use_batchnorm_inp""])\n        self.lstm_act = options[""lstm_act""].split("","")\n        self.lstm_orthinit = strtobool(options[""lstm_orthinit""])\n\n        self.bidir = strtobool(options[""lstm_bidir""])\n        self.use_cuda = strtobool(options[""use_cuda""])\n        self.to_do = options[""to_do""]\n\n        if self.to_do == ""train"":\n            self.test_flag = False\n        else:\n            self.test_flag = True\n\n        # List initialization\n        self.wfx = nn.ModuleList([])  # Forget\n        self.ufh = nn.ModuleList([])  # Forget\n\n        self.wix = nn.ModuleList([])  # Input\n        self.uih = nn.ModuleList([])  # Input\n\n        self.wox = nn.ModuleList([])  # Output\n        self.uoh = nn.ModuleList([])  # Output\n\n        self.wcx = nn.ModuleList([])  # Cell state\n        self.uch = nn.ModuleList([])  # Cell state\n\n        self.ln = nn.ModuleList([])  # Layer Norm\n        self.bn_wfx = nn.ModuleList([])  # Batch Norm\n        self.bn_wix = nn.ModuleList([])  # Batch Norm\n        self.bn_wox = nn.ModuleList([])  # Batch Norm\n        self.bn_wcx = nn.ModuleList([])  # Batch Norm\n\n        self.act = nn.ModuleList([])  # Activations\n\n        # Input layer normalization\n        if self.lstm_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # Input batch normalization\n        if self.lstm_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_lstm_lay = len(self.lstm_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_lstm_lay):\n\n            # Activations\n            self.act.append(act_fun(self.lstm_act[i]))\n\n            add_bias = True\n\n            if self.lstm_use_laynorm[i] or self.lstm_use_batchnorm[i]:\n                add_bias = False\n\n            # Feed-forward connections\n            self.wfx.append(nn.Linear(current_input, self.lstm_lay[i], bias=add_bias))\n            self.wix.append(nn.Linear(current_input, self.lstm_lay[i], bias=add_bias))\n            self.wox.append(nn.Linear(current_input, self.lstm_lay[i], bias=add_bias))\n            self.wcx.append(nn.Linear(current_input, self.lstm_lay[i], bias=add_bias))\n\n            # Recurrent connections\n            self.ufh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i], bias=False))\n            self.uih.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i], bias=False))\n            self.uoh.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i], bias=False))\n            self.uch.append(nn.Linear(self.lstm_lay[i], self.lstm_lay[i], bias=False))\n\n            if self.lstm_orthinit:\n                nn.init.orthogonal_(self.ufh[i].weight)\n                nn.init.orthogonal_(self.uih[i].weight)\n                nn.init.orthogonal_(self.uoh[i].weight)\n                nn.init.orthogonal_(self.uch[i].weight)\n\n            # batch norm initialization\n            self.bn_wfx.append(nn.BatchNorm1d(self.lstm_lay[i], momentum=0.05))\n            self.bn_wix.append(nn.BatchNorm1d(self.lstm_lay[i], momentum=0.05))\n            self.bn_wox.append(nn.BatchNorm1d(self.lstm_lay[i], momentum=0.05))\n            self.bn_wcx.append(nn.BatchNorm1d(self.lstm_lay[i], momentum=0.05))\n\n            self.ln.append(LayerNorm(self.lstm_lay[i]))\n\n            if self.bidir:\n                current_input = 2 * self.lstm_lay[i]\n            else:\n                current_input = self.lstm_lay[i]\n\n        self.out_dim = self.lstm_lay[i] + self.bidir * self.lstm_lay[i]\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.lstm_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.lstm_use_batchnorm_inp):\n            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n\n        for i in range(self.N_lstm_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2 * x.shape[1], self.lstm_lay[i])\n                x = torch.cat([x, flip(x, 0)], 1)\n            else:\n                h_init = torch.zeros(x.shape[1], self.lstm_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag == False:\n                drop_mask = torch.bernoulli(torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.lstm_drop[i]))\n            else:\n                drop_mask = torch.FloatTensor([1 - self.lstm_drop[i]])\n\n            if self.use_cuda:\n                h_init = h_init.cuda()\n                drop_mask = drop_mask.cuda()\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out = self.wfx[i](x)\n            wix_out = self.wix[i](x)\n            wox_out = self.wox[i](x)\n            wcx_out = self.wcx[i](x)\n\n            # Apply batch norm if needed (all steos in parallel)\n            if self.lstm_use_batchnorm[i]:\n\n                wfx_out_bn = self.bn_wfx[i](wfx_out.view(wfx_out.shape[0] * wfx_out.shape[1], wfx_out.shape[2]))\n                wfx_out = wfx_out_bn.view(wfx_out.shape[0], wfx_out.shape[1], wfx_out.shape[2])\n\n                wix_out_bn = self.bn_wix[i](wix_out.view(wix_out.shape[0] * wix_out.shape[1], wix_out.shape[2]))\n                wix_out = wix_out_bn.view(wix_out.shape[0], wix_out.shape[1], wix_out.shape[2])\n\n                wox_out_bn = self.bn_wox[i](wox_out.view(wox_out.shape[0] * wox_out.shape[1], wox_out.shape[2]))\n                wox_out = wox_out_bn.view(wox_out.shape[0], wox_out.shape[1], wox_out.shape[2])\n\n                wcx_out_bn = self.bn_wcx[i](wcx_out.view(wcx_out.shape[0] * wcx_out.shape[1], wcx_out.shape[2]))\n                wcx_out = wcx_out_bn.view(wcx_out.shape[0], wcx_out.shape[1], wcx_out.shape[2])\n\n            # Processing time steps\n            hiddens = []\n            ct = h_init\n            ht = h_init\n\n            for k in range(x.shape[0]):\n\n                # LSTM equations\n                ft = torch.sigmoid(wfx_out[k] + self.ufh[i](ht))\n                it = torch.sigmoid(wix_out[k] + self.uih[i](ht))\n                ot = torch.sigmoid(wox_out[k] + self.uoh[i](ht))\n                ct = it * self.act[i](wcx_out[k] + self.uch[i](ht)) * drop_mask + ft * ct\n                ht = ot * self.act[i](ct)\n\n                if self.lstm_use_laynorm[i]:\n                    ht = self.ln[i](ht)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h = torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f = h[:, 0 : int(x.shape[1] / 2)]\n                h_b = flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), 0)\n                h = torch.cat([h_f, h_b], 2)\n\n            # Setup x for the next hidden layer\n            x = h\n\n        return x\n\n\nclass GRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(GRU, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.gru_lay = list(map(int, options[""gru_lay""].split("","")))\n        self.gru_drop = list(map(float, options[""gru_drop""].split("","")))\n        self.gru_use_batchnorm = list(map(strtobool, options[""gru_use_batchnorm""].split("","")))\n        self.gru_use_laynorm = list(map(strtobool, options[""gru_use_laynorm""].split("","")))\n        self.gru_use_laynorm_inp = strtobool(options[""gru_use_laynorm_inp""])\n        self.gru_use_batchnorm_inp = strtobool(options[""gru_use_batchnorm_inp""])\n        self.gru_orthinit = strtobool(options[""gru_orthinit""])\n        self.gru_act = options[""gru_act""].split("","")\n        self.bidir = strtobool(options[""gru_bidir""])\n        self.use_cuda = strtobool(options[""use_cuda""])\n        self.to_do = options[""to_do""]\n\n        if self.to_do == ""train"":\n            self.test_flag = False\n        else:\n            self.test_flag = True\n\n        # List initialization\n        self.wh = nn.ModuleList([])\n        self.uh = nn.ModuleList([])\n\n        self.wz = nn.ModuleList([])  # Update Gate\n        self.uz = nn.ModuleList([])  # Update Gate\n\n        self.wr = nn.ModuleList([])  # Reset Gate\n        self.ur = nn.ModuleList([])  # Reset Gate\n\n        self.ln = nn.ModuleList([])  # Layer Norm\n        self.bn_wh = nn.ModuleList([])  # Batch Norm\n        self.bn_wz = nn.ModuleList([])  # Batch Norm\n        self.bn_wr = nn.ModuleList([])  # Batch Norm\n\n        self.act = nn.ModuleList([])  # Activations\n\n        # Input layer normalization\n        if self.gru_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # Input batch normalization\n        if self.gru_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_gru_lay = len(self.gru_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_gru_lay):\n\n            # Activations\n            self.act.append(act_fun(self.gru_act[i]))\n\n            add_bias = True\n\n            if self.gru_use_laynorm[i] or self.gru_use_batchnorm[i]:\n                add_bias = False\n\n            # Feed-forward connections\n            self.wh.append(nn.Linear(current_input, self.gru_lay[i], bias=add_bias))\n            self.wz.append(nn.Linear(current_input, self.gru_lay[i], bias=add_bias))\n            self.wr.append(nn.Linear(current_input, self.gru_lay[i], bias=add_bias))\n\n            # Recurrent connections\n            self.uh.append(nn.Linear(self.gru_lay[i], self.gru_lay[i], bias=False))\n            self.uz.append(nn.Linear(self.gru_lay[i], self.gru_lay[i], bias=False))\n            self.ur.append(nn.Linear(self.gru_lay[i], self.gru_lay[i], bias=False))\n\n            if self.gru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n                nn.init.orthogonal_(self.ur[i].weight)\n\n            # batch norm initialization\n            self.bn_wh.append(nn.BatchNorm1d(self.gru_lay[i], momentum=0.05))\n            self.bn_wz.append(nn.BatchNorm1d(self.gru_lay[i], momentum=0.05))\n            self.bn_wr.append(nn.BatchNorm1d(self.gru_lay[i], momentum=0.05))\n\n            self.ln.append(LayerNorm(self.gru_lay[i]))\n\n            if self.bidir:\n                current_input = 2 * self.gru_lay[i]\n            else:\n                current_input = self.gru_lay[i]\n\n        self.out_dim = self.gru_lay[i] + self.bidir * self.gru_lay[i]\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.gru_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.gru_use_batchnorm_inp):\n            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n\n        for i in range(self.N_gru_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2 * x.shape[1], self.gru_lay[i])\n                x = torch.cat([x, flip(x, 0)], 1)\n            else:\n                h_init = torch.zeros(x.shape[1], self.gru_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag == False:\n                drop_mask = torch.bernoulli(torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.gru_drop[i]))\n            else:\n                drop_mask = torch.FloatTensor([1 - self.gru_drop[i]])\n\n            if self.use_cuda:\n                h_init = h_init.cuda()\n                drop_mask = drop_mask.cuda()\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out = self.wh[i](x)\n            wz_out = self.wz[i](x)\n            wr_out = self.wr[i](x)\n\n            # Apply batch norm if needed (all steos in parallel)\n            if self.gru_use_batchnorm[i]:\n\n                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n\n                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n\n                wr_out_bn = self.bn_wr[i](wr_out.view(wr_out.shape[0] * wr_out.shape[1], wr_out.shape[2]))\n                wr_out = wr_out_bn.view(wr_out.shape[0], wr_out.shape[1], wr_out.shape[2])\n\n            # Processing time steps\n            hiddens = []\n            ht = h_init\n\n            for k in range(x.shape[0]):\n\n                # gru equation\n                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht))\n                rt = torch.sigmoid(wr_out[k] + self.ur[i](ht))\n                at = wh_out[k] + self.uh[i](rt * ht)\n                hcand = self.act[i](at) * drop_mask\n                ht = zt * ht + (1 - zt) * hcand\n\n                if self.gru_use_laynorm[i]:\n                    ht = self.ln[i](ht)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h = torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f = h[:, 0 : int(x.shape[1] / 2)]\n                h_b = flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), 0)\n                h = torch.cat([h_f, h_b], 2)\n\n            # Setup x for the next hidden layer\n            x = h\n\n        return x\n\n\nclass logMelFb(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(logMelFb, self).__init__()\n        import torchaudio\n\n        self._sample_rate = int(options[""logmelfb_nr_sample_rate""])\n        self._nr_of_filters = int(options[""logmelfb_nr_filt""])\n        self._stft_window_size = int(options[""logmelfb_stft_window_size""])\n        self._stft_window_shift = int(options[""logmelfb_stft_window_shift""])\n        self._use_cuda = strtobool(options[""use_cuda""])\n        self.out_dim = self._nr_of_filters\n        self._mspec = torchaudio.transforms.MelSpectrogram(\n            sr=self._sample_rate,\n            n_fft=self._stft_window_size,\n            ws=self._stft_window_size,\n            hop=self._stft_window_shift,\n            n_mels=self._nr_of_filters,\n        )\n\n    def forward(self, x):\n        def _safe_log(inp, epsilon=1e-20):\n            eps = torch.FloatTensor([epsilon])\n            if self._use_cuda:\n                eps = eps.cuda()\n            log_inp = torch.log10(torch.max(inp, eps.expand_as(inp)))\n            return log_inp\n\n        assert x.shape[-1] == 1, ""Multi channel time signal processing not suppored yet""\n        x_reshape_for_stft = torch.squeeze(x, -1).transpose(0, 1)\n        if self._use_cuda:\n            window = self._mspec.window(self._stft_window_size).cuda()\n        else:\n            window = self._mspec.window(self._stft_window_size)\n        x_stft = torch.stft(\n            x_reshape_for_stft, self._stft_window_size, hop_length=self._stft_window_shift, center=False, window=window\n        )\n        x_power_stft = x_stft.pow(2).sum(-1)\n        x_power_stft_reshape_for_filterbank_mult = x_power_stft.transpose(1, 2)\n        mel_spec = self._mspec.fm(x_power_stft_reshape_for_filterbank_mult).transpose(0, 1)\n        log_mel_spec = _safe_log(mel_spec)\n        out = log_mel_spec\n        return out\n\n\nclass channel_averaging(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(channel_averaging, self).__init__()\n        self._use_cuda = strtobool(options[""use_cuda""])\n        channel_weights = [float(e) for e in options[""chAvg_channelWeights""].split("","")]\n        self._nr_of_channels = len(channel_weights)\n        numpy_weights = np.asarray(channel_weights, dtype=np.float32) * 1.0 / np.sum(channel_weights)\n        self._weights = torch.from_numpy(numpy_weights)\n        if self._use_cuda:\n            self._weights = self._weights.cuda()\n        self.out_dim = 1\n\n    def forward(self, x):\n        assert self._nr_of_channels == x.shape[-1]\n        out = torch.einsum(""tbc,c->tb"", x, self._weights).unsqueeze(-1)\n        return out\n\nclass fusionRNN_jit(torch.jit.ScriptModule):\n    def __init__(self, options, inp_dim):\n        super(fusionRNN_jit, self).__init__()\n\n        # Reading parameters\n        input_size = inp_dim\n        hidden_size = list(map(int, options[""fusionRNN_lay""].split("","")))[0]\n        dropout = list(map(float, options[""fusionRNN_drop""].split("","")))[0]\n        num_layers = len(list(map(int, options[""fusionRNN_lay""].split("",""))))\n        batch_size = int(options[""batches""])\n        self.do_fusion = map(strtobool, options[""fusionRNN_do_fusion""].split("",""))\n        self.act = str(options[""fusionRNN_fusion_act""])\n        self.reduce = str(options[""fusionRNN_fusion_reduce""])\n        self.fusion_layer_size = int(options[""fusionRNN_fusion_layer_size""])\n        self.to_do = options[""to_do""]\n        self.number_of_mic = int(options[""fusionRNN_number_of_mic""])\n        self.save_mic = self.number_of_mic\n        bidirectional = True\n\n        self.out_dim = 2 * hidden_size\n\n        current_dim = int(input_size)\n\n        self.model = torch.nn.ModuleList([])\n\n        if self.to_do == ""train"":\n            self.training = True\n        else:\n            self.training = False\n\n        for i in range(num_layers):\n            rnn_lay = liGRU_layer(\n                current_dim,\n                hidden_size,\n                num_layers,\n                batch_size,\n                dropout=dropout,\n                bidirectional=bidirectional,\n                device=""cuda"",\n                do_fusion=self.do_fusion,\n                fusion_layer_size=self.fusion_layer_size,\n                number_of_mic=self.number_of_mic,\n                act=self.act,\n                reduce=self.reduce\n            )\n            if i == 0:\n\n                if self.do_fusion:\n                    if bidirectional:\n                        current_dim = (self.fusion_layer_size // self.save_mic) * 2\n                    else:\n                        current_dim = self.fusion_layer_size // self.save_mic\n                    #We need to reset the number of mic for the next layers so it is divided by 1\n                    self.number_of_mic = 1\n                else:\n                    if bidirectional:\n                        current_dim = hidden_size * 2\n                    else:\n                        current_dim = hidden_size\n                self.do_fusion = False # DO NOT APPLY FUSION ON THE NEXT LAYERS\n            else:\n                if bidirectional:\n                    current_dim = hidden_size * 2\n                else:\n                    current_dim == hidden_size\n            self.model.append(rnn_lay)\n\n\n    @torch.jit.script_method\n    def forward(self, x):\n        # type: (Tensor) -> Tensor\n\n        for ligru_lay in self.model:\n\n            x = ligru_lay(x)\n\n        return x\n\n\nclass liGRU_layer(torch.jit.ScriptModule):\n    def __init__(\n        self,\n        input_size,\n        hidden_size,\n        num_layers,\n        batch_size,\n        dropout=0.0,\n        nonlinearity=""relu"",\n        bidirectional=True,\n        device=""cuda"",\n        do_fusion=False,\n        fusion_layer_size=64,\n        number_of_mic=1,\n        act=""relu"",\n        reduce=""mean"",\n    ):\n\n        super(liGRU_layer, self).__init__()\n\n        self.hidden_size = int(hidden_size)\n        self.input_size = int(input_size)\n        self.batch_size = batch_size\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n        self.device = device\n        self.do_fusion = do_fusion\n        self.fusion_layer_size = fusion_layer_size\n        self.number_of_mic = number_of_mic\n        self.act = act\n        self.reduce = reduce\n\n        if self.do_fusion:\n            self.hidden_size = self.fusion_layer_size //  self.number_of_mic\n\n        if self.do_fusion:\n            self.wz = FusionLinearConv(\n                self.input_size, self.hidden_size, bias=True, number_of_mic = self.number_of_mic, act=self.act, reduce=self.reduce\n            ).to(device)\n\n            self.wh = FusionLinearConv(\n                self.input_size, self.hidden_size, bias=True, number_of_mic = self.number_of_mic, act=self.act, reduce=self.reduce\n            ).to(device)\n        else:\n            self.wz = nn.Linear(\n                self.input_size, self.hidden_size, bias=True\n            ).to(device)\n\n            self.wh = nn.Linear(\n                self.input_size, self.hidden_size, bias=True\n            ).to(device)\n\n            self.wz.bias.data.fill_(0)\n            torch.nn.init.xavier_normal_(self.wz.weight.data)\n            self.wh.bias.data.fill_(0)\n            torch.nn.init.xavier_normal_(self.wh.weight.data)\n\n        self.u = nn.Linear(\n            self.hidden_size, 2 * self.hidden_size, bias=False\n        ).to(device)\n\n        # Adding orthogonal initialization for recurrent connection\n        nn.init.orthogonal_(self.u.weight)\n\n        self.bn_wh = nn.BatchNorm1d(self.hidden_size, momentum=0.05).to(\n            device\n        )\n\n        self.bn_wz = nn.BatchNorm1d(self.hidden_size, momentum=0.05).to(\n            device\n        )\n\n\n        self.drop = torch.nn.Dropout(p=self.dropout, inplace=False).to(device)\n        self.drop_mask_te = torch.tensor([1.0], device=device).float()\n        self.N_drop_masks = 100\n        self.drop_mask_cnt = 0\n\n        # Setting the activation function\n        self.act = torch.nn.ReLU().to(device)\n\n    @torch.jit.script_method\n    def forward(self, x):\n        # type: (Tensor) -> Tensor\n\n        if self.bidirectional:\n            x_flip = x.flip(0)\n            x = torch.cat([x, x_flip], dim=1)\n\n        # Feed-forward affine transformations (all steps in parallel)\n        wz = self.wz(x)\n        wh = self.wh(x)\n\n        # Apply batch normalization\n        wz_bn = self.bn_wz(wz.view(wz.shape[0] * wz.shape[1], wz.shape[2]))\n        wh_bn = self.bn_wh(wh.view(wh.shape[0] * wh.shape[1], wh.shape[2]))\n\n        wz = wz_bn.view(wz.shape[0], wz.shape[1], wz.shape[2])\n        wh = wh_bn.view(wh.shape[0], wh.shape[1], wh.shape[2])\n\n        # Processing time steps\n        h = self.ligru_cell(wz, wh)\n\n        if self.bidirectional:\n            h_f, h_b = h.chunk(2, dim=1)\n            h_b = h_b.flip(0)\n            h = torch.cat([h_f, h_b], dim=2)\n\n        return h\n\n    @torch.jit.script_method\n    def ligru_cell(self, wz, wh):\n        # type: (Tensor, Tensor) -> Tensor\n\n        if self.bidirectional:\n            h_init = torch.zeros(\n                2 * self.batch_size,\n                self.hidden_size,\n                device=""cuda"",\n            )\n            drop_masks_i = self.drop(\n                torch.ones(\n                    self.N_drop_masks,\n                    2 * self.batch_size,\n                    self.hidden_size,\n                    device=""cuda"",\n                )\n            ).data\n\n        else:\n            h_init = torch.zeros(\n                self.batch_size,\n                self.hidden_size,\n                device=""cuda"",\n            )\n            drop_masks_i = self.drop(\n                torch.ones(\n                    self.N_drop_masks,\n                    self.batch_size,\n                    self.hidden_size,\n                    device=""cuda"",\n                )\n            ).data\n\n        hiddens = []\n        ht = h_init\n\n        if self.training:\n\n            drop_mask = drop_masks_i[self.drop_mask_cnt]\n            self.drop_mask_cnt = self.drop_mask_cnt + 1\n\n            if self.drop_mask_cnt >= self.N_drop_masks:\n                self.drop_mask_cnt = 0\n                if self.bidirectional:\n                    drop_masks_i = (\n                        self.drop(\n                            torch.ones(\n                                self.N_drop_masks,\n                                2 * self.batch_size,\n                                self.hidden_size,\n                            )\n                        )\n                        .to(self.device)\n                        .data\n                    )\n                else:\n                    drop_masks_i = (\n                        self.drop(\n                            torch.ones(\n                                self.N_drop_masks,\n                                self.batch_size,\n                                self.hidden_size,\n                            )\n                        )\n                        .to(self.device)\n                        .data\n                    )\n\n        else:\n            drop_mask = self.drop_mask_te\n\n        for k in range(wh.shape[0]):\n\n            uz, uh = self.u(ht).chunk(2, 1)\n\n            at = wh[k] + uh\n            zt = wz[k] + uz\n\n            # ligru equation\n            zt = torch.sigmoid(zt)\n            hcand = self.act(at) * drop_mask\n            ht = zt * ht + (1 - zt) * hcand\n            hiddens.append(ht)\n\n        # Stacking hidden states\n        h = torch.stack(hiddens)\n        return h\n\nclass liGRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(liGRU, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.ligru_lay = list(map(int, options[""ligru_lay""].split("","")))\n        self.ligru_drop = list(map(float, options[""ligru_drop""].split("","")))\n        self.ligru_use_batchnorm = list(map(strtobool, options[""ligru_use_batchnorm""].split("","")))\n        self.ligru_use_laynorm = list(map(strtobool, options[""ligru_use_laynorm""].split("","")))\n        self.ligru_use_laynorm_inp = strtobool(options[""ligru_use_laynorm_inp""])\n        self.ligru_use_batchnorm_inp = strtobool(options[""ligru_use_batchnorm_inp""])\n        self.ligru_orthinit = strtobool(options[""ligru_orthinit""])\n        self.ligru_act = options[""ligru_act""].split("","")\n        self.bidir = strtobool(options[""ligru_bidir""])\n        self.use_cuda = strtobool(options[""use_cuda""])\n        self.to_do = options[""to_do""]\n\n        if self.to_do == ""train"":\n            self.test_flag = False\n        else:\n            self.test_flag = True\n\n        # List initialization\n        self.wh = nn.ModuleList([])\n        self.uh = nn.ModuleList([])\n\n        self.wz = nn.ModuleList([])  # Update Gate\n        self.uz = nn.ModuleList([])  # Update Gate\n\n        self.ln = nn.ModuleList([])  # Layer Norm\n        self.bn_wh = nn.ModuleList([])  # Batch Norm\n        self.bn_wz = nn.ModuleList([])  # Batch Norm\n\n        self.act = nn.ModuleList([])  # Activations\n\n        # Input layer normalization\n        if self.ligru_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # Input batch normalization\n        if self.ligru_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_ligru_lay = len(self.ligru_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_ligru_lay):\n\n            # Activations\n            self.act.append(act_fun(self.ligru_act[i]))\n\n            add_bias = True\n\n            if self.ligru_use_laynorm[i] or self.ligru_use_batchnorm[i]:\n                add_bias = False\n\n            # Feed-forward connections\n            self.wh.append(nn.Linear(current_input, self.ligru_lay[i], bias=add_bias))\n            self.wz.append(nn.Linear(current_input, self.ligru_lay[i], bias=add_bias))\n\n            # Recurrent connections\n            self.uh.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias=False))\n            self.uz.append(nn.Linear(self.ligru_lay[i], self.ligru_lay[i], bias=False))\n\n            if self.ligru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n\n            # batch norm initialization\n            self.bn_wh.append(nn.BatchNorm1d(self.ligru_lay[i], momentum=0.05))\n            self.bn_wz.append(nn.BatchNorm1d(self.ligru_lay[i], momentum=0.05))\n\n            self.ln.append(LayerNorm(self.ligru_lay[i]))\n\n            if self.bidir:\n                current_input = 2 * self.ligru_lay[i]\n            else:\n                current_input = self.ligru_lay[i]\n\n        self.out_dim = self.ligru_lay[i] + self.bidir * self.ligru_lay[i]\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.ligru_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.ligru_use_batchnorm_inp):\n            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n\n        for i in range(self.N_ligru_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2 * x.shape[1], self.ligru_lay[i])\n                x = torch.cat([x, flip(x, 0)], 1)\n            else:\n                h_init = torch.zeros(x.shape[1], self.ligru_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag == False:\n                drop_mask = torch.bernoulli(\n                    torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.ligru_drop[i])\n                )\n            else:\n                drop_mask = torch.FloatTensor([1 - self.ligru_drop[i]])\n\n            if self.use_cuda:\n                h_init = h_init.cuda()\n                drop_mask = drop_mask.cuda()\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out = self.wh[i](x)\n            wz_out = self.wz[i](x)\n\n            # Apply batch norm if needed (all steos in parallel)\n            if self.ligru_use_batchnorm[i]:\n\n                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n\n                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n\n            # Processing time steps\n            hiddens = []\n            ht = h_init\n\n            for k in range(x.shape[0]):\n\n                # ligru equation\n                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht))\n                at = wh_out[k] + self.uh[i](ht)\n                hcand = self.act[i](at) * drop_mask\n                ht = zt * ht + (1 - zt) * hcand\n\n                if self.ligru_use_laynorm[i]:\n                    ht = self.ln[i](ht)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h = torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f = h[:, 0 : int(x.shape[1] / 2)]\n                h_b = flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), 0)\n                h = torch.cat([h_f, h_b], 2)\n\n            # Setup x for the next hidden layer\n            x = h\n\n        return x\n\n\nclass minimalGRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(minimalGRU, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.minimalgru_lay = list(map(int, options[""minimalgru_lay""].split("","")))\n        self.minimalgru_drop = list(map(float, options[""minimalgru_drop""].split("","")))\n        self.minimalgru_use_batchnorm = list(map(strtobool, options[""minimalgru_use_batchnorm""].split("","")))\n        self.minimalgru_use_laynorm = list(map(strtobool, options[""minimalgru_use_laynorm""].split("","")))\n        self.minimalgru_use_laynorm_inp = strtobool(options[""minimalgru_use_laynorm_inp""])\n        self.minimalgru_use_batchnorm_inp = strtobool(options[""minimalgru_use_batchnorm_inp""])\n        self.minimalgru_orthinit = strtobool(options[""minimalgru_orthinit""])\n        self.minimalgru_act = options[""minimalgru_act""].split("","")\n        self.bidir = strtobool(options[""minimalgru_bidir""])\n        self.use_cuda = strtobool(options[""use_cuda""])\n        self.to_do = options[""to_do""]\n\n        if self.to_do == ""train"":\n            self.test_flag = False\n        else:\n            self.test_flag = True\n\n        # List initialization\n        self.wh = nn.ModuleList([])\n        self.uh = nn.ModuleList([])\n\n        self.wz = nn.ModuleList([])  # Update Gate\n        self.uz = nn.ModuleList([])  # Update Gate\n\n        self.ln = nn.ModuleList([])  # Layer Norm\n        self.bn_wh = nn.ModuleList([])  # Batch Norm\n        self.bn_wz = nn.ModuleList([])  # Batch Norm\n\n        self.act = nn.ModuleList([])  # Activations\n\n        # Input layer normalization\n        if self.minimalgru_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # Input batch normalization\n        if self.minimalgru_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_minimalgru_lay = len(self.minimalgru_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_minimalgru_lay):\n\n            # Activations\n            self.act.append(act_fun(self.minimalgru_act[i]))\n\n            add_bias = True\n\n            if self.minimalgru_use_laynorm[i] or self.minimalgru_use_batchnorm[i]:\n                add_bias = False\n\n            # Feed-forward connections\n            self.wh.append(nn.Linear(current_input, self.minimalgru_lay[i], bias=add_bias))\n            self.wz.append(nn.Linear(current_input, self.minimalgru_lay[i], bias=add_bias))\n\n            # Recurrent connections\n            self.uh.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i], bias=False))\n            self.uz.append(nn.Linear(self.minimalgru_lay[i], self.minimalgru_lay[i], bias=False))\n\n            if self.minimalgru_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n                nn.init.orthogonal_(self.uz[i].weight)\n\n            # batch norm initialization\n            self.bn_wh.append(nn.BatchNorm1d(self.minimalgru_lay[i], momentum=0.05))\n            self.bn_wz.append(nn.BatchNorm1d(self.minimalgru_lay[i], momentum=0.05))\n\n            self.ln.append(LayerNorm(self.minimalgru_lay[i]))\n\n            if self.bidir:\n                current_input = 2 * self.minimalgru_lay[i]\n            else:\n                current_input = self.minimalgru_lay[i]\n\n        self.out_dim = self.minimalgru_lay[i] + self.bidir * self.minimalgru_lay[i]\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.minimalgru_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.minimalgru_use_batchnorm_inp):\n            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n\n        for i in range(self.N_minimalgru_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2 * x.shape[1], self.minimalgru_lay[i])\n                x = torch.cat([x, flip(x, 0)], 1)\n            else:\n                h_init = torch.zeros(x.shape[1], self.minimalgru_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag == False:\n                drop_mask = torch.bernoulli(\n                    torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.minimalgru_drop[i])\n                )\n            else:\n                drop_mask = torch.FloatTensor([1 - self.minimalgru_drop[i]])\n\n            if self.use_cuda:\n                h_init = h_init.cuda()\n                drop_mask = drop_mask.cuda()\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out = self.wh[i](x)\n            wz_out = self.wz[i](x)\n\n            # Apply batch norm if needed (all steos in parallel)\n            if self.minimalgru_use_batchnorm[i]:\n\n                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n\n                wz_out_bn = self.bn_wz[i](wz_out.view(wz_out.shape[0] * wz_out.shape[1], wz_out.shape[2]))\n                wz_out = wz_out_bn.view(wz_out.shape[0], wz_out.shape[1], wz_out.shape[2])\n\n            # Processing time steps\n            hiddens = []\n            ht = h_init\n\n            for k in range(x.shape[0]):\n\n                # minimalgru equation\n                zt = torch.sigmoid(wz_out[k] + self.uz[i](ht))\n                at = wh_out[k] + self.uh[i](zt * ht)\n                hcand = self.act[i](at) * drop_mask\n                ht = zt * ht + (1 - zt) * hcand\n\n                if self.minimalgru_use_laynorm[i]:\n                    ht = self.ln[i](ht)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h = torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f = h[:, 0 : int(x.shape[1] / 2)]\n                h_b = flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), 0)\n                h = torch.cat([h_f, h_b], 2)\n\n            # Setup x for the next hidden layer\n            x = h\n\n        return x\n\n\nclass RNN(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(RNN, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.rnn_lay = list(map(int, options[""rnn_lay""].split("","")))\n        self.rnn_drop = list(map(float, options[""rnn_drop""].split("","")))\n        self.rnn_use_batchnorm = list(map(strtobool, options[""rnn_use_batchnorm""].split("","")))\n        self.rnn_use_laynorm = list(map(strtobool, options[""rnn_use_laynorm""].split("","")))\n        self.rnn_use_laynorm_inp = strtobool(options[""rnn_use_laynorm_inp""])\n        self.rnn_use_batchnorm_inp = strtobool(options[""rnn_use_batchnorm_inp""])\n        self.rnn_orthinit = strtobool(options[""rnn_orthinit""])\n        self.rnn_act = options[""rnn_act""].split("","")\n        self.bidir = strtobool(options[""rnn_bidir""])\n        self.use_cuda = strtobool(options[""use_cuda""])\n        self.to_do = options[""to_do""]\n\n        if self.to_do == ""train"":\n            self.test_flag = False\n        else:\n            self.test_flag = True\n\n        # List initialization\n        self.wh = nn.ModuleList([])\n        self.uh = nn.ModuleList([])\n\n        self.ln = nn.ModuleList([])  # Layer Norm\n        self.bn_wh = nn.ModuleList([])  # Batch Norm\n\n        self.act = nn.ModuleList([])  # Activations\n\n        # Input layer normalization\n        if self.rnn_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        # Input batch normalization\n        if self.rnn_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d(self.input_dim, momentum=0.05)\n\n        self.N_rnn_lay = len(self.rnn_lay)\n\n        current_input = self.input_dim\n\n        # Initialization of hidden layers\n\n        for i in range(self.N_rnn_lay):\n\n            # Activations\n            self.act.append(act_fun(self.rnn_act[i]))\n\n            add_bias = True\n\n            if self.rnn_use_laynorm[i] or self.rnn_use_batchnorm[i]:\n                add_bias = False\n\n            # Feed-forward connections\n            self.wh.append(nn.Linear(current_input, self.rnn_lay[i], bias=add_bias))\n\n            # Recurrent connections\n            self.uh.append(nn.Linear(self.rnn_lay[i], self.rnn_lay[i], bias=False))\n\n            if self.rnn_orthinit:\n                nn.init.orthogonal_(self.uh[i].weight)\n\n            # batch norm initialization\n            self.bn_wh.append(nn.BatchNorm1d(self.rnn_lay[i], momentum=0.05))\n\n            self.ln.append(LayerNorm(self.rnn_lay[i]))\n\n            if self.bidir:\n                current_input = 2 * self.rnn_lay[i]\n            else:\n                current_input = self.rnn_lay[i]\n\n        self.out_dim = self.rnn_lay[i] + self.bidir * self.rnn_lay[i]\n\n    def forward(self, x):\n\n        # Applying Layer/Batch Norm\n        if bool(self.rnn_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.rnn_use_batchnorm_inp):\n            x_bn = self.bn0(x.view(x.shape[0] * x.shape[1], x.shape[2]))\n            x = x_bn.view(x.shape[0], x.shape[1], x.shape[2])\n\n        for i in range(self.N_rnn_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2 * x.shape[1], self.rnn_lay[i])\n                x = torch.cat([x, flip(x, 0)], 1)\n            else:\n                h_init = torch.zeros(x.shape[1], self.rnn_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag == False:\n                drop_mask = torch.bernoulli(torch.Tensor(h_init.shape[0], h_init.shape[1]).fill_(1 - self.rnn_drop[i]))\n            else:\n                drop_mask = torch.FloatTensor([1 - self.rnn_drop[i]])\n\n            if self.use_cuda:\n                h_init = h_init.cuda()\n                drop_mask = drop_mask.cuda()\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wh_out = self.wh[i](x)\n\n            # Apply batch norm if needed (all steos in parallel)\n            if self.rnn_use_batchnorm[i]:\n\n                wh_out_bn = self.bn_wh[i](wh_out.view(wh_out.shape[0] * wh_out.shape[1], wh_out.shape[2]))\n                wh_out = wh_out_bn.view(wh_out.shape[0], wh_out.shape[1], wh_out.shape[2])\n\n            # Processing time steps\n            hiddens = []\n            ht = h_init\n\n            for k in range(x.shape[0]):\n\n                # rnn equation\n                at = wh_out[k] + self.uh[i](ht)\n                ht = self.act[i](at) * drop_mask\n\n                if self.rnn_use_laynorm[i]:\n                    ht = self.ln[i](ht)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h = torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f = h[:, 0 : int(x.shape[1] / 2)]\n                h_b = flip(h[:, int(x.shape[1] / 2) : x.shape[1]].contiguous(), 0)\n                h = torch.cat([h_f, h_b], 2)\n\n            # Setup x for the next hidden layer\n            x = h\n\n        return x\n\n\nclass CNN(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(CNN, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.cnn_N_filt = list(map(int, options[""cnn_N_filt""].split("","")))\n\n        self.cnn_len_filt = list(map(int, options[""cnn_len_filt""].split("","")))\n        self.cnn_max_pool_len = list(map(int, options[""cnn_max_pool_len""].split("","")))\n\n        self.cnn_act = options[""cnn_act""].split("","")\n        self.cnn_drop = list(map(float, options[""cnn_drop""].split("","")))\n\n        self.cnn_use_laynorm = list(map(strtobool, options[""cnn_use_laynorm""].split("","")))\n        self.cnn_use_batchnorm = list(map(strtobool, options[""cnn_use_batchnorm""].split("","")))\n        self.cnn_use_laynorm_inp = strtobool(options[""cnn_use_laynorm_inp""])\n        self.cnn_use_batchnorm_inp = strtobool(options[""cnn_use_batchnorm_inp""])\n\n        self.N_cnn_lay = len(self.cnn_N_filt)\n        self.conv = nn.ModuleList([])\n        self.bn = nn.ModuleList([])\n        self.ln = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n\n        if self.cnn_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        if self.cnn_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d([self.input_dim], momentum=0.05)\n\n        current_input = self.input_dim\n\n        for i in range(self.N_cnn_lay):\n\n            N_filt = int(self.cnn_N_filt[i])\n            len_filt = int(self.cnn_len_filt[i])\n\n            # dropout\n            self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n\n            # activation\n            self.act.append(act_fun(self.cnn_act[i]))\n\n            # layer norm initialization\n            self.ln.append(\n                LayerNorm([N_filt, int((current_input - self.cnn_len_filt[i] + 1) / self.cnn_max_pool_len[i])])\n            )\n\n            self.bn.append(\n                nn.BatchNorm1d(\n                    N_filt, int((current_input - self.cnn_len_filt[i] + 1) / self.cnn_max_pool_len[i]), momentum=0.05\n                )\n            )\n\n            if i == 0:\n                self.conv.append(nn.Conv1d(1, N_filt, len_filt))\n\n            else:\n                self.conv.append(nn.Conv1d(self.cnn_N_filt[i - 1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n\n            current_input = int((current_input - self.cnn_len_filt[i] + 1) / self.cnn_max_pool_len[i])\n\n        self.out_dim = current_input * N_filt\n\n    def forward(self, x):\n\n        batch = x.shape[0]\n        seq_len = x.shape[1]\n\n        if bool(self.cnn_use_laynorm_inp):\n            x = self.ln0((x))\n\n        if bool(self.cnn_use_batchnorm_inp):\n            x = self.bn0((x))\n\n        x = x.view(batch, 1, seq_len)\n\n        for i in range(self.N_cnn_lay):\n\n            if self.cnn_use_laynorm[i]:\n                x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n            if self.cnn_use_batchnorm[i]:\n                x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n\n            if self.cnn_use_batchnorm[i] == False and self.cnn_use_laynorm[i] == False:\n                x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n\n        x = x.view(batch, -1)\n\n        return x\n\n\nclass SincNet(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SincNet, self).__init__()\n\n        # Reading parameters\n        self.input_dim = inp_dim\n        self.sinc_N_filt = list(map(int, options[""sinc_N_filt""].split("","")))\n\n        self.sinc_len_filt = list(map(int, options[""sinc_len_filt""].split("","")))\n        self.sinc_max_pool_len = list(map(int, options[""sinc_max_pool_len""].split("","")))\n\n        self.sinc_act = options[""sinc_act""].split("","")\n        self.sinc_drop = list(map(float, options[""sinc_drop""].split("","")))\n\n        self.sinc_use_laynorm = list(map(strtobool, options[""sinc_use_laynorm""].split("","")))\n        self.sinc_use_batchnorm = list(map(strtobool, options[""sinc_use_batchnorm""].split("","")))\n        self.sinc_use_laynorm_inp = strtobool(options[""sinc_use_laynorm_inp""])\n        self.sinc_use_batchnorm_inp = strtobool(options[""sinc_use_batchnorm_inp""])\n\n        self.N_sinc_lay = len(self.sinc_N_filt)\n\n        self.sinc_sample_rate = int(options[""sinc_sample_rate""])\n        self.sinc_min_low_hz = int(options[""sinc_min_low_hz""])\n        self.sinc_min_band_hz = int(options[""sinc_min_band_hz""])\n\n        self.conv = nn.ModuleList([])\n        self.bn = nn.ModuleList([])\n        self.ln = nn.ModuleList([])\n        self.act = nn.ModuleList([])\n        self.drop = nn.ModuleList([])\n\n        if self.sinc_use_laynorm_inp:\n            self.ln0 = LayerNorm(self.input_dim)\n\n        if self.sinc_use_batchnorm_inp:\n            self.bn0 = nn.BatchNorm1d([self.input_dim], momentum=0.05)\n\n        current_input = self.input_dim\n\n        for i in range(self.N_sinc_lay):\n\n            N_filt = int(self.sinc_N_filt[i])\n            len_filt = int(self.sinc_len_filt[i])\n\n            # dropout\n            self.drop.append(nn.Dropout(p=self.sinc_drop[i]))\n\n            # activation\n            self.act.append(act_fun(self.sinc_act[i]))\n\n            # layer norm initialization\n            self.ln.append(\n                LayerNorm([N_filt, int((current_input - self.sinc_len_filt[i] + 1) / self.sinc_max_pool_len[i])])\n            )\n\n            self.bn.append(\n                nn.BatchNorm1d(\n                    N_filt, int((current_input - self.sinc_len_filt[i] + 1) / self.sinc_max_pool_len[i]), momentum=0.05\n                )\n            )\n\n            if i == 0:\n                self.conv.append(\n                    SincConv(\n                        1,\n                        N_filt,\n                        len_filt,\n                        sample_rate=self.sinc_sample_rate,\n                        min_low_hz=self.sinc_min_low_hz,\n                        min_band_hz=self.sinc_min_band_hz,\n                    )\n                )\n\n            else:\n                self.conv.append(nn.Conv1d(self.sinc_N_filt[i - 1], self.sinc_N_filt[i], self.sinc_len_filt[i]))\n\n            current_input = int((current_input - self.sinc_len_filt[i] + 1) / self.sinc_max_pool_len[i])\n\n        self.out_dim = current_input * N_filt\n\n    def forward(self, x):\n\n        batch = x.shape[0]\n        seq_len = x.shape[1]\n\n        if bool(self.sinc_use_laynorm_inp):\n            x = self.ln0(x)\n\n        if bool(self.sinc_use_batchnorm_inp):\n            x = self.bn0(x)\n\n        x = x.view(batch, 1, seq_len)\n\n        for i in range(self.N_sinc_lay):\n\n            if self.sinc_use_laynorm[i]:\n                x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n            if self.sinc_use_batchnorm[i]:\n                x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i]))))\n\n            if self.sinc_use_batchnorm[i] == False and self.sinc_use_laynorm[i] == False:\n                x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.sinc_max_pool_len[i])))\n\n        x = x.view(batch, -1)\n\n        return x\n\n\nclass SincConv(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=False,\n        groups=1,\n        sample_rate=16000,\n        min_low_hz=50,\n        min_band_hz=50,\n    ):\n\n        super(SincConv, self).__init__()\n\n        if in_channels != 1:\n            # msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n\n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size % 2 == 0:\n            self.kernel_size = self.kernel_size + 1\n\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(""SincConv does not support bias."")\n        if groups > 1:\n            raise ValueError(""SincConv does not support groups."")\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n        hz = self.to_hz(mel) / self.sample_rate\n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        # self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin = torch.linspace(0, self.kernel_size, steps=self.kernel_size)\n        self.window_ = 0.54 - 0.46 * torch.cos(2 * math.pi * n_lin / self.kernel_size)\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2\n        self.n_ = torch.arange(-n, n + 1).view(1, -1) / self.sample_rate\n\n    def sinc(self, x):\n        # Numerically stable definition\n        x_left = x[:, 0 : int((x.shape[1] - 1) / 2)]\n        y_left = torch.sin(x_left) / x_left\n        y_right = torch.flip(y_left, dims=[1])\n\n        sinc = torch.cat([y_left, torch.ones([x.shape[0], 1]).to(x.device), y_right], dim=1)\n\n        return sinc\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz / self.sample_rate + torch.abs(self.low_hz_)\n        high = low + self.min_band_hz / self.sample_rate + torch.abs(self.band_hz_)\n\n        f_times_t = torch.matmul(low, self.n_)\n\n        low_pass1 = 2 * low * self.sinc(2 * math.pi * f_times_t * self.sample_rate)\n\n        f_times_t = torch.matmul(high, self.n_)\n        low_pass2 = 2 * high * self.sinc(2 * math.pi * f_times_t * self.sample_rate)\n\n        band_pass = low_pass2 - low_pass1\n        max_, _ = torch.max(band_pass, dim=1, keepdim=True)\n        band_pass = band_pass / max_\n\n        self.filters = (band_pass * self.window_).view(self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(\n            waveforms,\n            self.filters,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            bias=None,\n            groups=1,\n        )\n\n\nclass SincConv_fast(nn.Module):\n    """"""Sinc-based convolution\n    Parameters\n    ----------\n    in_channels : `int`\n        Number of input channels. Must be 1.\n    out_channels : `int`\n        Number of filters.\n    kernel_size : `int`\n        Filter length.\n    sample_rate : `int`, optional\n        Sample rate. Defaults to 16000.\n    Usage\n    -----\n    See `torch.nn.Conv1d`\n    Reference\n    ---------\n    Mirco Ravanelli, Yoshua Bengio,\n    ""Speaker Recognition from raw waveform with SincNet"".\n    https://arxiv.org/abs/1808.00158\n    """"""\n\n    @staticmethod\n    def to_mel(hz):\n        return 2595 * np.log10(1 + hz / 700)\n\n    @staticmethod\n    def to_hz(mel):\n        return 700 * (10 ** (mel / 2595) - 1)\n\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride=1,\n        padding=0,\n        dilation=1,\n        bias=False,\n        groups=1,\n        sample_rate=16000,\n        min_low_hz=50,\n        min_band_hz=50,\n    ):\n\n        super(SincConv_fast, self).__init__()\n\n        if in_channels != 1:\n            # msg = (f\'SincConv only support one input channel \'\n            #       f\'(here, in_channels = {in_channels:d}).\')\n            msg = ""SincConv only support one input channel (here, in_channels = {%i})"" % (in_channels)\n            raise ValueError(msg)\n\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n\n        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n        if kernel_size % 2 == 0:\n            self.kernel_size = self.kernel_size + 1\n\n        self.stride = stride\n        self.padding = padding\n        self.dilation = dilation\n\n        if bias:\n            raise ValueError(""SincConv does not support bias."")\n        if groups > 1:\n            raise ValueError(""SincConv does not support groups."")\n\n        self.sample_rate = sample_rate\n        self.min_low_hz = min_low_hz\n        self.min_band_hz = min_band_hz\n\n        # initialize filterbanks such that they are equally spaced in Mel scale\n        low_hz = 30\n        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n\n        mel = np.linspace(self.to_mel(low_hz), self.to_mel(high_hz), self.out_channels + 1)\n        hz = self.to_hz(mel)\n\n        # filter lower frequency (out_channels, 1)\n        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n\n        # filter frequency band (out_channels, 1)\n        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n\n        # Hamming window\n        # self.window_ = torch.hamming_window(self.kernel_size)\n        n_lin = torch.linspace(\n            0, (self.kernel_size / 2) - 1, steps=int((self.kernel_size / 2))\n        )  # computing only half of the window\n        self.window_ = 0.54 - 0.46 * torch.cos(2 * math.pi * n_lin / self.kernel_size)\n\n        # (kernel_size, 1)\n        n = (self.kernel_size - 1) / 2.0\n        self.n_ = (\n            2 * math.pi * torch.arange(-n, 0).view(1, -1) / self.sample_rate\n        )  # Due to symmetry, I only need half of the time axes\n\n    def forward(self, waveforms):\n        """"""\n        Parameters\n        ----------\n        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n            Batch of waveforms.\n        Returns\n        -------\n        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n            Batch of sinc filters activations.\n        """"""\n\n        self.n_ = self.n_.to(waveforms.device)\n\n        self.window_ = self.window_.to(waveforms.device)\n\n        low = self.min_low_hz + torch.abs(self.low_hz_)\n\n        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_), self.min_low_hz, self.sample_rate / 2)\n        band = (high - low)[:, 0]\n\n        f_times_t_low = torch.matmul(low, self.n_)\n        f_times_t_high = torch.matmul(high, self.n_)\n\n        band_pass_left = (\n            (torch.sin(f_times_t_high) - torch.sin(f_times_t_low)) / (self.n_ / 2)\n        ) * self.window_  # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations.\n        band_pass_center = 2 * band.view(-1, 1)\n        band_pass_right = torch.flip(band_pass_left, dims=[1])\n\n        band_pass = torch.cat([band_pass_left, band_pass_center, band_pass_right], dim=1)\n\n        band_pass = band_pass / (2 * band[:, None])\n\n        self.filters = (band_pass).view(self.out_channels, 1, self.kernel_size)\n\n        return F.conv1d(\n            waveforms,\n            self.filters,\n            stride=self.stride,\n            padding=self.padding,\n            dilation=self.dilation,\n            bias=None,\n            groups=1,\n        )\n\n\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[\n        :, getattr(torch.arange(x.size(1) - 1, -1, -1), (""cpu"", ""cuda"")[x.is_cuda])().long(), :\n    ]\n    return x.view(xsize)\n\n\nclass SRU(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(SRU, self).__init__()\n        self.input_dim = inp_dim\n        self.hidden_size = int(options[""sru_hidden_size""])\n        self.num_layers = int(options[""sru_num_layers""])\n        self.dropout = float(options[""sru_dropout""])\n        self.rnn_dropout = float(options[""sru_rnn_dropout""])\n        self.use_tanh = bool(strtobool(options[""sru_use_tanh""]))\n        self.use_relu = bool(strtobool(options[""sru_use_relu""]))\n        self.use_selu = bool(strtobool(options[""sru_use_selu""]))\n        self.weight_norm = bool(strtobool(options[""sru_weight_norm""]))\n        self.layer_norm = bool(strtobool(options[""sru_layer_norm""]))\n        self.bidirectional = bool(strtobool(options[""sru_bidirectional""]))\n        self.is_input_normalized = bool(strtobool(options[""sru_is_input_normalized""]))\n        self.has_skip_term = bool(strtobool(options[""sru_has_skip_term""]))\n        self.rescale = bool(strtobool(options[""sru_rescale""]))\n        self.highway_bias = float(options[""sru_highway_bias""])\n        self.n_proj = int(options[""sru_n_proj""])\n        self.sru = sru.SRU(\n            self.input_dim,\n            self.hidden_size,\n            num_layers=self.num_layers,\n            dropout=self.dropout,\n            rnn_dropout=self.rnn_dropout,\n            bidirectional=self.bidirectional,\n            n_proj=self.n_proj,\n            use_tanh=self.use_tanh,\n            use_selu=self.use_selu,\n            use_relu=self.use_relu,\n            weight_norm=self.weight_norm,\n            layer_norm=self.layer_norm,\n            has_skip_term=self.has_skip_term,\n            is_input_normalized=self.is_input_normalized,\n            highway_bias=self.highway_bias,\n            rescale=self.rescale,\n        )\n        self.out_dim = self.hidden_size + self.bidirectional * self.hidden_size\n\n    def forward(self, x):\n        if self.bidirectional:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size * 2)\n        else:\n            h0 = torch.zeros(self.num_layers, x.shape[1], self.hidden_size)\n        if x.is_cuda:\n            h0 = h0.cuda()\n        output, hn = self.sru(x, c0=h0)\n        return output\n\n\nclass PASE(nn.Module):\n    def __init__(self, options, inp_dim):\n        super(PASE, self).__init__()\n\n        # To use PASE within PyTorch-Kaldi, please clone the current PASE repository: https://github.com/santi-pdp/pase\n        # Note that you have to clone the dev branch.\n        # Take a look into the requirements (requirements.txt) and install in your environment what is missing. An important requirement is QRNN (https://github.com/salesforce/pytorch-qrnn).\n        # Before starting working with PASE, it could make sense to a quick test  with QRNN independently (see \xe2\x80\x9cusage\xe2\x80\x9d section in the QRNN repository).\n        # Remember to install pase. This way it can be used outside the pase folder directory.  To do it, go into the pase folder and type:\n        # ""python setup.py install""\n\n        from pase.models.frontend import wf_builder\n\n        self.input_dim = inp_dim\n        self.pase_cfg = options[""pase_cfg""]\n        self.pase_model = options[""pase_model""]\n\n        self.pase = wf_builder(self.pase_cfg)\n\n        self.pase.load_pretrained(self.pase_model, load_last=True, verbose=True)\n\n        # Reading the out_dim from the config file:\n        with open(self.pase_cfg) as json_file:\n            config = json.load(json_file)\n\n        self.out_dim = int(config[""emb_dim""])\n\n    def forward(self, x):\n\n        x = x.unsqueeze(0).unsqueeze(0)\n        output = self.pase(x)\n\n        return output\n\nclass FusionLinearConv(nn.Module):\n    r""""""Applies a FusionLayer as described in:\n        \'FusionRNN: Shared Neural Parameters for\n        Multi-Channel Distant Speech Recognition\', Titouan P. et Al.\n\n        Input channels are supposed to be concatenated along the last dimension\n    """"""\n\n    def __init__(self, in_features, out_features, number_of_mic=1, bias=True,seed=None,act=""leaky"",reduce=""sum""):\n\n        super(FusionLinearConv, self).__init__()\n        self.in_features       = in_features // number_of_mic\n        self.out_features      = out_features\n        self.number_of_mic     = number_of_mic\n        self.reduce            = reduce\n\n        if act == ""leaky_relu"":\n            self.act_function = nn.LeakyReLU()\n        elif act == ""prelu"":\n            self.act_function = nn.PReLU()\n        elif act == ""relu"":\n            self.act_function = nn.ReLU()\n        else:\n            self.act_function = nn.Tanh()\n\n        self.conv = nn.Conv1d(1, self.out_features, kernel_size=self.in_features, stride=self.in_features, bias=True, padding=0)\n\n        self.conv.bias.data.fill_(0)\n        torch.nn.init.xavier_normal_(self.conv.weight.data)\n\n\n    def forward(self, input):\n\n        orig_shape = input.shape\n\n        out = self.act_function(self.conv(input.view(orig_shape[0]*orig_shape[1], 1, -1)))\n\n        if self.reduce == ""mean"":\n            out = torch.mean(out, dim=-1)\n        else:\n            out = torch.sum(out, dim=-1)\n\n        return out.view(orig_shape[0],orig_shape[1], -1)\n'"
plot_acc_and_loss.py,0,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport sys\nimport configparser\nimport os\nfrom utils import create_curves\n\n# Checking arguments\nif len(sys.argv) != 2:\n    print(""ERROR: Please provide only the path of the cfg_file as : python plot_acc_and_loss.py cfg/TIMIT_MLP_mfcc.cfg"")\n\n# Checking if the cfg_file exists and loading it\ncfg_file = sys.argv[1]\nif not (os.path.exists(cfg_file)):\n    sys.stderr.write(""ERROR: The config file %s does not exist !\\n"" % (cfg_file))\n    sys.exit(0)\nelse:\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n\n# Getting the parameters\nvalid_data_lst = config[""data_use""][""valid_with""].split("","")\nout_folder = config[""exp""][""out_folder""]\nN_ep = int(config[""exp""][""N_epochs_tr""])\n\n# Handling call without running run_exp.py before\nif not (os.path.exists(out_folder + ""res.res"")):\n    sys.stderr.write(""ERROR: Please run the experiment in order to get results to plot first !\\n"")\n    sys.exit(0)\n\n# Creating files and curves\ncreate_curves(out_folder, N_ep, valid_data_lst)\n'"
quaternion_neural_networks.py,62,"b'##########################################################\n# Quaternion Neural Networks\n# Titouan Parcollet, Xinchi Qiu, Mirco Ravanelli\n# University of Oxford and Mila, University of Montreal\n# May 2020\n##########################################################\n\nimport torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.nn.parameter import Parameter\nfrom torch.autograd import Variable\nfrom torch.nn.utils.rnn import PackedSequence\nfrom torch.nn import Module\nimport numpy as np\nfrom scipy.stats import chi\nfrom numpy.random import RandomState\nfrom distutils.util import strtobool\nimport math\n\nclass QLSTM(nn.Module):\n    """"""\n        This class implements a straightforward QLSTM as described\n        in ""Quaternion Recurrent Neural Networks"", Titouan P., ICLR 2019\n\n        Please note that the autograd parameter is usefull if you run out of\n        VRAM. Set it to False, and the model will use a custom QuaternionLinear\n        function that follows a custom backpropagation. The training will\n        be even slower but will consume 4 times less VRAM.\n    """"""\n    def __init__(self, options,inp_dim):\n        super(QLSTM, self).__init__()\n\n        # Reading parameters\n        self.input_dim=inp_dim\n        self.lstm_lay=list(map(int, options[\'lstm_lay\'].split(\',\')))\n        self.lstm_drop=list(map(float, options[\'lstm_drop\'].split(\',\')))\n        self.lstm_act=options[\'lstm_act\'].split(\',\')\n        self.bidir=strtobool(options[\'lstm_bidir\'])\n        self.use_cuda=strtobool(options[\'use_cuda\'])\n        self.autograd=strtobool(options[\'autograd\'])\n        self.to_do=options[\'to_do\']\n\n        if self.to_do==\'train\':\n            self.test_flag=False\n        else:\n            self.test_flag=True\n\n\n        # List initialization\n        self.wfx  = nn.ModuleList([]) # Forget\n        self.ufh  = nn.ModuleList([]) # Forget\n\n        self.wix  = nn.ModuleList([]) # Input\n        self.uih  = nn.ModuleList([]) # Input\n\n        self.wox  = nn.ModuleList([]) # Output\n        self.uoh  = nn.ModuleList([]) # Output\n\n        self.wcx  = nn.ModuleList([]) # Cell state\n        self.uch  = nn.ModuleList([])  # Cell state\n\n        self.act  = nn.ModuleList([]) # Activations\n\n        self.N_lstm_lay=len(self.lstm_lay)\n\n        # Initialization of hidden layers\n        current_input=self.input_dim\n        for i in range(self.N_lstm_lay):\n\n             # Activations\n             self.act.append(act_fun(self.lstm_act[i]))\n\n             add_bias=True\n\n             # QuaternionLinearAutograd = Autograd (High VRAM consumption but faster)\n             # QuaternionLinear = Custom Backward (Low VRAM consumption but slower)\n             if(self.autograd):\n\n                 # Feed-forward connections\n                 self.wfx.append(QuaternionLinearAutograd(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wix.append(QuaternionLinearAutograd(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wox.append(QuaternionLinearAutograd(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wcx.append(QuaternionLinearAutograd(current_input, self.lstm_lay[i],bias=add_bias))\n\n                 # Recurrent connections\n                 self.ufh.append(QuaternionLinearAutograd(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uih.append(QuaternionLinearAutograd(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uoh.append(QuaternionLinearAutograd(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uch.append(QuaternionLinearAutograd(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             else:\n\n                # Feed-forward connections\n                 self.wfx.append(QuaternionLinear(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wix.append(QuaternionLinear(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wox.append(QuaternionLinear(current_input, self.lstm_lay[i],bias=add_bias))\n                 self.wcx.append(QuaternionLinear(current_input, self.lstm_lay[i],bias=add_bias))\n\n                 # Recurrent connections\n                 self.ufh.append(QuaternionLinear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uih.append(QuaternionLinear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uoh.append(QuaternionLinear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n                 self.uch.append(QuaternionLinear(self.lstm_lay[i], self.lstm_lay[i],bias=False))\n             if self.bidir:\n                 current_input=2*self.lstm_lay[i]\n             else:\n                 current_input=self.lstm_lay[i]\n\n        self.out_dim=self.lstm_lay[i]+self.bidir*self.lstm_lay[i]\n\n    def forward(self, x):\n\n        for i in range(self.N_lstm_lay):\n\n            # Initial state and concatenation\n            if self.bidir:\n                h_init = torch.zeros(2*x.shape[1], self.lstm_lay[i])\n                x=torch.cat([x,flip(x,0)],1)\n            else:\n                h_init = torch.zeros(x.shape[1],self.lstm_lay[i])\n\n            # Drop mask initilization (same mask for all time steps)\n            if self.test_flag==False:\n                drop_mask=torch.bernoulli(torch.Tensor(h_init.shape[0],h_init.shape[1]).fill_(1-self.lstm_drop[i]))\n            else:\n                drop_mask=torch.FloatTensor([1-self.lstm_drop[i]])\n\n            if self.use_cuda:\n               h_init=h_init.cuda()\n               drop_mask=drop_mask.cuda()\n\n\n            # Feed-forward affine transformations (all steps in parallel)\n            wfx_out=self.wfx[i](x)\n            wix_out=self.wix[i](x)\n            wox_out=self.wox[i](x)\n            wcx_out=self.wcx[i](x)\n\n            # Processing time steps\n            hiddens = []\n            ct=h_init\n            ht=h_init\n\n            for k in range(x.shape[0]):\n\n                # LSTM equations\n                ft=torch.sigmoid(wfx_out[k]+self.ufh[i](ht))\n                it=torch.sigmoid(wix_out[k]+self.uih[i](ht))\n                ot=torch.sigmoid(wox_out[k]+self.uoh[i](ht))\n                ct=it*self.act[i](wcx_out[k]+self.uch[i](ht))*drop_mask+ft*ct\n                ht=ot*self.act[i](ct)\n\n                hiddens.append(ht)\n\n            # Stacking hidden states\n            h=torch.stack(hiddens)\n\n            # Bidirectional concatenations\n            if self.bidir:\n                h_f=h[:,0:int(x.shape[1]/2)]\n                h_b=flip(h[:,int(x.shape[1]/2):x.shape[1]].contiguous(),0)\n                h=torch.cat([h_f,h_b],2)\n\n            # Setup x for the next hidden layer\n            x=h\n\n\n        return x\n\n#\n# From this point, the defined functions are PyTorch modules extending\n# linear layers to the quaternion domain.\n#\n\nclass QuaternionLinearAutograd(Module):\n    r""""""Applies a quaternion linear transformation to the incoming data.\n    The backward process follows the Autograd scheme.\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True,\n                 init_criterion=\'glorot\', weight_init=\'quaternion\',\n                 seed=None):\n\n        super(QuaternionLinearAutograd, self).__init__()\n        self.in_features       = in_features//4\n        self.out_features      = out_features//4\n\n        self.r_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.i_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.j_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.k_weight = Parameter(torch.Tensor(self.in_features, self.out_features))\n\n        if bias:\n            self.bias = Parameter(torch.Tensor(self.out_features*4))\n        else:\n            self.bias = torch.zeros(self.out_features*4)\n\n        self.init_criterion = init_criterion\n        self.weight_init = weight_init\n        self.seed = seed if seed is not None else np.random.randint(0,1234)\n        self.rng = RandomState(self.seed)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        winit = {\'quaternion\': quaternion_init, \'unitary\': unitary_init, \'random\': random_init}[self.weight_init]\n        if self.bias is not None:\n            self.bias.data.fill_(0)\n        affect_init(self.r_weight, self.i_weight, self.j_weight, self.k_weight, winit,\n                    self.rng, self.init_criterion)\n\n    def forward(self, input):\n        return quaternion_linear(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n            + \'in_features=\' + str(self.in_features) \\\n            + \', out_features=\' + str(self.out_features) \\\n            + \', bias=\' + str(self.bias is not None) \\\n            + \', init_criterion=\' + str(self.init_criterion) \\\n            + \', weight_init=\' + str(self.weight_init) \\\n            + \', seed=\' + str(self.seed) + \')\'\n\nclass QuaternionLinear(Module):\n    r""""""A custom Autograd function is call to drastically reduce the VRAM consumption.\n    Nonetheless, computing time is increased compared to QuaternionLinearAutograd().\n    """"""\n\n    def __init__(self, in_features, out_features, bias=True,\n                 init_criterion=\'glorot\', weight_init=\'quaternion\',\n                 seed=None):\n\n        super(QuaternionLinear, self).__init__()\n        self.in_features  = in_features//4\n        self.out_features = out_features//4\n        self.r_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.i_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.j_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n        self.k_weight     = Parameter(torch.Tensor(self.in_features, self.out_features))\n\n        if bias:\n            self.bias     = Parameter(torch.Tensor(self.out_features*4))\n        else:\n            self.register_parameter(\'bias\', None)\n\n        self.init_criterion = init_criterion\n        self.weight_init    = weight_init\n        self.seed           = seed if seed is not None else np.random.randint(0,1234)\n        self.rng            = RandomState(self.seed)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        winit = {\'quaternion\': quaternion_init,\n                 \'unitary\': unitary_init}[self.weight_init]\n        if self.bias is not None:\n            self.bias.data.fill_(0)\n        affect_init(self.r_weight, self.i_weight, self.j_weight, self.k_weight, winit,\n                    self.rng, self.init_criterion)\n\n    def forward(self, input):\n        # See the autograd section for explanation of what happens here.\n        if input.dim() == 3:\n            T, N, C = input.size()\n            input = input.view(T * N, C)\n            output = QuaternionLinearFunction.apply(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n            output = output.view(T, N, output.size(1))\n        elif input.dim() == 2:\n            output = QuaternionLinearFunction.apply(input, self.r_weight, self.i_weight, self.j_weight, self.k_weight, self.bias)\n        else:\n            raise NotImplementedError\n\n        return output\n\n    def __repr__(self):\n        return self.__class__.__name__ + \'(\' \\\n            + \'in_features=\' + str(self.in_features) \\\n            + \', out_features=\' + str(self.out_features) \\\n            + \', bias=\' + str(self.bias is not None) \\\n            + \', init_criterion=\' + str(self.init_criterion) \\\n            + \', weight_init=\' + str(self.weight_init) \\\n            + \', seed=\' + str(self.seed) + \')\'\n\n#\n# Thereafter are utility functions needed by the above classes\n#\n\ndef flip(x, dim):\n    xsize = x.size()\n    dim = x.dim() + dim if dim < 0 else dim\n    x = x.contiguous()\n    x = x.view(-1, *xsize[dim:])\n    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, -1, -1), (\'cpu\',\'cuda\')[x.is_cuda])().long(), :]\n    return x.view(xsize)\n\ndef act_fun(act_type):\n\n if act_type==""relu"":\n    return nn.ReLU()\n\n if act_type==""prelu"":\n    return nn.PReLU()\n\n if act_type==""tanh"":\n    return nn.Tanh()\n\n if act_type==""sigmoid"":\n    return nn.Sigmoid()\n\n if act_type==""hardtanh"":\n    return nn.Hardtanh()\n\n if act_type==""leaky_relu"":\n    return nn.LeakyReLU(0.2)\n\n if act_type==""elu"":\n    return nn.ELU()\n\n if act_type==""softmax"":\n    return nn.LogSoftmax(dim=1)\n\n if act_type==""linear"":\n     return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n\n\n\ndef check_input(input):\n\n    if input.dim() not in {2, 3}:\n        raise RuntimeError(\n            ""quaternion linear accepts only input of dimension 2 or 3.""\n            "" input.dim = "" + str(input.dim())\n        )\n\n    nb_hidden = input.size()[-1]\n\n    if nb_hidden % 4 != 0:\n        raise RuntimeError(\n            ""Quaternion Tensors must be divisible by 4.""\n            "" input.size()[1] = "" + str(nb_hidden)\n        )\n\n#\n# Quaternion getters!\n#\ndef get_r(input):\n    check_input(input)\n    nb_hidden = input.size()[-1]\n    if input.dim() == 2:\n        return input.narrow(1, 0, nb_hidden // 4)\n    elif input.dim() == 3:\n        return input.narrow(2, 0, nb_hidden // 4)\n\n\ndef get_i(input):\n    check_input(input)\n    nb_hidden = input.size()[-1]\n    if input.dim() == 2:\n        return input.narrow(1, nb_hidden // 4, nb_hidden // 4)\n    if input.dim() == 3:\n        return input.narrow(2, nb_hidden // 4, nb_hidden // 4)\n\ndef get_j(input):\n    check_input(input)\n    nb_hidden = input.size()[-1]\n    if input.dim() == 2:\n        return input.narrow(1, nb_hidden // 2, nb_hidden // 4)\n    if input.dim() == 3:\n        return input.narrow(2, nb_hidden // 2, nb_hidden // 4)\n\ndef get_k(input):\n    check_input(input)\n    nb_hidden = input.size()[-1]\n    if input.dim() == 2:\n        return input.narrow(1, nb_hidden - nb_hidden // 4, nb_hidden // 4)\n    if input.dim() == 3:\n        return input.narrow(2, nb_hidden - nb_hidden // 4, nb_hidden // 4)\n\n\ndef quaternion_linear(input, r_weight, i_weight, j_weight, k_weight, bias):\n\n    """"""\n    Applies a quaternion linear transformation to the incoming data:\n    It is important to notice that the forward phase of a QNN is defined\n    as W * Inputs (with * equal to the Hamilton product). The constructed\n    cat_kernels_4_quaternion is a modified version of the quaternion representation\n    so when we do torch.mm(Input,W) it\'s equivalent to W * Inputs.\n    """"""\n\n    cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n    cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n    cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n    cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n    cat_kernels_4_quaternion   = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=1)\n\n    if input.dim() == 2 :\n\n        if bias is not None:\n            return torch.addmm(bias, input, cat_kernels_4_quaternion)\n        else:\n            return torch.mm(input, cat_kernels_4_quaternion)\n    else:\n        output = torch.matmul(input, cat_kernels_4_quaternion)\n        if bias is not None:\n            return output+bias\n        else:\n            return output\n\n# Custom AUTOGRAD for lower VRAM consumption\nclass QuaternionLinearFunction(torch.autograd.Function):\n\n    @staticmethod\n    def forward(ctx, input, r_weight, i_weight, j_weight, k_weight, bias=None):\n        ctx.save_for_backward(input, r_weight, i_weight, j_weight, k_weight, bias)\n        check_input(input)\n        cat_kernels_4_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n        cat_kernels_4_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n        cat_kernels_4_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n        cat_kernels_4_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n        cat_kernels_4_quaternion = torch.cat([cat_kernels_4_r, cat_kernels_4_i, cat_kernels_4_j, cat_kernels_4_k], dim=1)\n        if input.dim() == 2 :\n            if bias is not None:\n                return torch.addmm(bias, input, cat_kernels_4_quaternion)\n            else:\n                return torch.mm(input, cat_kernels_4_quaternion)\n        else:\n            output = torch.matmul(input, cat_kernels_4_quaternion)\n            if bias is not None:\n                return output+bias\n            else:\n                return output\n\n    # This function has only a single output, so it gets only one gradient\n    @staticmethod\n    def backward(ctx, grad_output):\n\n        input, r_weight, i_weight, j_weight, k_weight, bias = ctx.saved_tensors\n        grad_input = grad_weight_r = grad_weight_i = grad_weight_j = grad_weight_k = grad_bias = None\n\n        input_r = torch.cat([r_weight, -i_weight, -j_weight, -k_weight], dim=0)\n        input_i = torch.cat([i_weight,  r_weight, -k_weight, j_weight], dim=0)\n        input_j = torch.cat([j_weight,  k_weight, r_weight, -i_weight], dim=0)\n        input_k = torch.cat([k_weight,  -j_weight, i_weight, r_weight], dim=0)\n        cat_kernels_4_quaternion_T = Variable(torch.cat([input_r, input_i, input_j, input_k], dim=1).permute(1,0), requires_grad=False)\n\n        r = get_r(input)\n        i = get_i(input)\n        j = get_j(input)\n        k = get_k(input)\n        input_r = torch.cat([r, -i, -j, -k], dim=0)\n        input_i = torch.cat([i,  r, -k, j], dim=0)\n        input_j = torch.cat([j,  k, r, -i], dim=0)\n        input_k = torch.cat([k,  -j, i, r], dim=0)\n        input_mat = Variable(torch.cat([input_r, input_i, input_j, input_k], dim=1), requires_grad=False)\n\n        r = get_r(grad_output)\n        i = get_i(grad_output)\n        j = get_j(grad_output)\n        k = get_k(grad_output)\n        input_r = torch.cat([r, i, j, k], dim=1)\n        input_i = torch.cat([-i,  r, k, -j], dim=1)\n        input_j = torch.cat([-j,  -k, r, i], dim=1)\n        input_k = torch.cat([-k,  j, -i, r], dim=1)\n        grad_mat = torch.cat([input_r, input_i, input_j, input_k], dim=0)\n\n        if ctx.needs_input_grad[0]:\n            grad_input  = grad_output.mm(cat_kernels_4_quaternion_T)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_mat.permute(1,0).mm(input_mat).permute(1,0)\n            unit_size_x = r_weight.size(0)\n            unit_size_y = r_weight.size(1)\n            grad_weight_r = grad_weight.narrow(0,0,unit_size_x).narrow(1,0,unit_size_y)\n            grad_weight_i = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y,unit_size_y)\n            grad_weight_j = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y*2,unit_size_y)\n            grad_weight_k = grad_weight.narrow(0,0,unit_size_x).narrow(1,unit_size_y*3,unit_size_y)\n        if ctx.needs_input_grad[5]:\n            grad_bias   = grad_output.sum(0).squeeze(0)\n\n        return grad_input, grad_weight_r, grad_weight_i, grad_weight_j, grad_weight_k, grad_bias\n\n#\n# PARAMETERS INITIALIZATION\n#\n\ndef unitary_init(in_features, out_features, rng, kernel_size=None, criterion=\'he\'):\n\n    if kernel_size is not None:\n        receptive_field = np.prod(kernel_size)\n        fan_in          = in_features  * receptive_field\n        fan_out         = out_features * receptive_field\n    else:\n        fan_in          = in_features\n        fan_out         = out_features\n\n    if criterion == \'glorot\':\n        s = 1. / np.sqrt(2*(fan_in + fan_out))\n    elif criterion == \'he\':\n        s = 1. / np.sqrt(2*fan_in)\n    else:\n        raise ValueError(\'Invalid criterion: \' + criterion)\n\n    if kernel_size is None:\n        kernel_shape = (in_features, out_features)\n    else:\n        if type(kernel_size) is int:\n            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n        else:\n            kernel_shape = (out_features, in_features) + (*kernel_size,)\n\n    s = np.sqrt(3.0) * s\n\n    number_of_weights = np.prod(kernel_shape)\n    v_r = np.random.uniform(-s,s,number_of_weights)\n    v_i = np.random.uniform(-s,s,number_of_weights)\n    v_j = np.random.uniform(-s,s,number_of_weights)\n    v_k = np.random.uniform(-s,s,number_of_weights)\n\n    # Unitary quaternion\n    for i in range(0, number_of_weights):\n        norm = np.sqrt(v_r[i]**2 + v_i[i]**2 + v_j[i]**2 + v_k[i]**2)+0.0001\n        v_r[i]/= norm\n        v_i[i]/= norm\n        v_j[i]/= norm\n        v_k[i]/= norm\n\n    v_r = v_r.reshape(kernel_shape)\n    v_i = v_i.reshape(kernel_shape)\n    v_j = v_j.reshape(kernel_shape)\n    v_k = v_k.reshape(kernel_shape)\n\n    return (v_r, v_i, v_j, v_k)\n\ndef random_init(in_features, out_features, rng, kernel_size=None, criterion=\'glorot\'):\n\n    if kernel_size is not None:\n        receptive_field = np.prod(kernel_size)\n        fan_in          = in_features  * receptive_field\n        fan_out         = out_features * receptive_field\n    else:\n        fan_in          = in_features\n        fan_out         = out_features\n\n    if criterion == \'glorot\':\n        s = 1. / np.sqrt(2*(fan_in + fan_out))\n    elif criterion == \'he\':\n        s = 1. / np.sqrt(2*fan_in)\n    else:\n        raise ValueError(\'Invalid criterion: \' + criterion)\n\n    if kernel_size is None:\n        kernel_shape = (in_features, out_features)\n    else:\n        if type(kernel_size) is int:\n            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n        else:\n            kernel_shape = (out_features, in_features) + (*kernel_size,)\n\n    number_of_weights = np.prod(kernel_shape)\n    v_r = np.random.uniform(0.0,1.0,number_of_weights)\n    v_i = np.random.uniform(0.0,1.0,number_of_weights)\n    v_j = np.random.uniform(0.0,1.0,number_of_weights)\n    v_k = np.random.uniform(0.0,1.0,number_of_weights)\n\n    v_r = v_r.reshape(kernel_shape)\n    v_i = v_i.reshape(kernel_shape)\n    v_j = v_j.reshape(kernel_shape)\n    v_k = v_k.reshape(kernel_shape)\n\n    weight_r = v_r * s\n    weight_i = v_i * s\n    weight_j = v_j * s\n    weight_k = v_k * s\n    return (weight_r, weight_i, weight_j, weight_k)\n\n\ndef quaternion_init(in_features, out_features, rng, kernel_size=None, criterion=\'glorot\'):\n\n    if kernel_size is not None:\n        receptive_field = np.prod(kernel_size)\n        fan_in          = in_features  * receptive_field\n        fan_out         = out_features * receptive_field\n    else:\n        fan_in          = in_features\n        fan_out         = out_features\n\n    if criterion == \'glorot\':\n        s = 1. / np.sqrt(2*(fan_in + fan_out))\n    elif criterion == \'he\':\n        s = 1. / np.sqrt(2*fan_in)\n    else:\n        raise ValueError(\'Invalid criterion: \' + criterion)\n\n    rng = RandomState(np.random.randint(1,1234))\n\n\n    # Generating randoms and purely imaginary quaternions :\n    if kernel_size is None:\n        kernel_shape = (in_features, out_features)\n    else:\n        if type(kernel_size) is int:\n            kernel_shape = (out_features, in_features) + tuple((kernel_size,))\n        else:\n            kernel_shape = (out_features, in_features) + (*kernel_size,)\n\n    modulus = chi.rvs(4,loc=0,scale=s,size=kernel_shape)\n    number_of_weights = np.prod(kernel_shape)\n    v_i = np.random.normal(0,1.0,number_of_weights)\n    v_j = np.random.normal(0,1.0,number_of_weights)\n    v_k = np.random.normal(0,1.0,number_of_weights)\n\n    # Purely imaginary quaternions unitary\n    for i in range(0, number_of_weights):\n    \tnorm = np.sqrt(v_i[i]**2 + v_j[i]**2 + v_k[i]**2 +0.0001)\n    \tv_i[i]/= norm\n    \tv_j[i]/= norm\n    \tv_k[i]/= norm\n    v_i = v_i.reshape(kernel_shape)\n    v_j = v_j.reshape(kernel_shape)\n    v_k = v_k.reshape(kernel_shape)\n\n    phase = rng.uniform(low=-np.pi, high=np.pi, size=kernel_shape)\n\n    weight_r = modulus * np.cos(phase)\n    weight_i = modulus * v_i*np.sin(phase)\n    weight_j = modulus * v_j*np.sin(phase)\n    weight_k = modulus * v_k*np.sin(phase)\n\n    return (weight_r, weight_i, weight_j, weight_k)\n\ndef affect_init(r_weight, i_weight, j_weight, k_weight, init_func, rng, init_criterion):\n    if r_weight.size() != i_weight.size() or r_weight.size() != j_weight.size() or \\\n    r_weight.size() != k_weight.size() :\n         raise ValueError(\'The real and imaginary weights \'\n                 \'should have the same size . Found: r:\'\n                 + str(r_weight.size()) +\' i:\'\n                 + str(i_weight.size()) +\' j:\'\n                 + str(j_weight.size()) +\' k:\'\n                 + str(k_weight.size()))\n\n    elif r_weight.dim() != 2:\n        raise Exception(\'affect_init accepts only matrices. Found dimension = \'\n                        + str(r_weight.dim()))\n    kernel_size = None\n    r, i, j, k  = init_func(r_weight.size(0), r_weight.size(1), rng, kernel_size, init_criterion)\n    r, i, j, k  = torch.from_numpy(r), torch.from_numpy(i), torch.from_numpy(j), torch.from_numpy(k)\n    r_weight.data = r.type_as(r_weight.data)\n    i_weight.data = i.type_as(i_weight.data)\n    j_weight.data = j.type_as(j_weight.data)\n    k_weight.data = k.type_as(k_weight.data)\n'"
run_exp.py,0,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\n\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport glob\nimport configparser\nimport numpy as np\nfrom utils import (\n    check_cfg,\n    create_lists,\n    create_configs,\n    compute_avg_performance,\n    read_args_command_line,\n    run_shell,\n    compute_n_chunks,\n    get_all_archs,\n    cfg_item2sec,\n    dump_epoch_results,\n    create_curves,\n    change_lr_cfg,\n    expand_str_ep,\n    do_validation_after_chunk,\n    get_val_info_file_path,\n    get_val_cfg_file_path,\n    get_chunks_after_which_to_validate,\n)\nfrom data_io import read_lab_fea_refac01 as read_lab_fea\nfrom shutil import copyfile\nfrom core import read_next_chunk_into_shared_list_with_subprocess, extract_data_from_shared_list, convert_numpy_to_torch\nimport re\nfrom distutils.util import strtobool\nimport importlib\nimport math\nimport multiprocessing\n\n\ndef _run_forwarding_in_subprocesses(config):\n    use_cuda = strtobool(config[""exp""][""use_cuda""])\n    if use_cuda:\n        return False\n    else:\n        return True\n\n\ndef _is_first_validation(ep, ck, N_ck_tr, config):\n    def _get_nr_of_valid_per_epoch_from_config(config):\n        if not ""nr_of_valid_per_epoch"" in config[""exp""]:\n            return 1\n        return int(config[""exp""][""nr_of_valid_per_epoch""])\n    \n    if ep>0:\n        return False\n    \n    val_chunks = get_chunks_after_which_to_validate(N_ck_tr, _get_nr_of_valid_per_epoch_from_config(config))\n    if ck == val_chunks[0]:\n        return True\n\n    \n    return False\n\n\ndef _max_nr_of_parallel_forwarding_processes(config):\n    if ""max_nr_of_parallel_forwarding_processes"" in config[""forward""]:\n        return int(config[""forward""][""max_nr_of_parallel_forwarding_processes""])\n    return -1\n\n\n# Reading global cfg file (first argument-mandatory file)\ncfg_file = sys.argv[1]\nif not (os.path.exists(cfg_file)):\n    sys.stderr.write(""ERROR: The config file %s does not exist!\\n"" % (cfg_file))\n    sys.exit(0)\nelse:\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n\n\n# Reading and parsing optional arguments from command line (e.g.,--optimization,lr=0.002)\n[section_args, field_args, value_args] = read_args_command_line(sys.argv, config)\n\n\n# Output folder creation\nout_folder = config[""exp""][""out_folder""]\nif not os.path.exists(out_folder):\n    os.makedirs(out_folder + ""/exp_files"")\n\n# Log file path\nlog_file = config[""exp""][""out_folder""] + ""/log.log""\n\n\n# Read, parse, and check the config file\ncfg_file_proto = config[""cfg_proto""][""cfg_proto""]\n[config, name_data, name_arch] = check_cfg(cfg_file, config, cfg_file_proto)\n\n\n# Read cfg file options\nis_production = strtobool(config[""exp""][""production""])\ncfg_file_proto_chunk = config[""cfg_proto""][""cfg_proto_chunk""]\n\ncmd = config[""exp""][""cmd""]\nN_ep = int(config[""exp""][""N_epochs_tr""])\nN_ep_str_format = ""0"" + str(max(math.ceil(np.log10(N_ep)), 1)) + ""d""\ntr_data_lst = config[""data_use""][""train_with""].split("","")\nvalid_data_lst = config[""data_use""][""valid_with""].split("","")\nforward_data_lst = config[""data_use""][""forward_with""].split("","")\nmax_seq_length_train = config[""batches""][""max_seq_length_train""]\nforward_save_files = list(map(strtobool, config[""forward""][""save_out_file""].split("","")))\n\n\nprint(""- Reading config file......OK!"")\n\n\n# Copy the global cfg file into the output folder\ncfg_file = out_folder + ""/conf.cfg""\nwith open(cfg_file, ""w"") as configfile:\n    config.write(configfile)\n\n\n# Load the run_nn function from core libriary\n# The run_nn is a function that process a single chunk of data\nrun_nn_script = config[""exp""][""run_nn_script""].split("".py"")[0]\nmodule = importlib.import_module(""core"")\nrun_nn = getattr(module, run_nn_script)\n\n\n# Splitting data into chunks (see out_folder/additional_files)\ncreate_lists(config)\n\n# Writing the config files\ncreate_configs(config)\n\nprint(""- Chunk creation......OK!\\n"")\n\n# create res_file\nres_file_path = out_folder + ""/res.res""\nres_file = open(res_file_path, ""w"")\nres_file.close()\n\n\n# Learning rates and architecture-specific optimization parameters\narch_lst = get_all_archs(config)\nlr = {}\nauto_lr_annealing = {}\nimprovement_threshold = {}\nhalving_factor = {}\npt_files = {}\n\nfor arch in arch_lst:\n    lr[arch] = expand_str_ep(config[arch][""arch_lr""], ""float"", N_ep, ""|"", ""*"")\n    if len(config[arch][""arch_lr""].split(""|"")) > 1:\n        auto_lr_annealing[arch] = False\n    else:\n        auto_lr_annealing[arch] = True\n    improvement_threshold[arch] = float(config[arch][""arch_improvement_threshold""])\n    halving_factor[arch] = float(config[arch][""arch_halving_factor""])\n    pt_files[arch] = config[arch][""arch_pretrain_file""]\n\n\n# If production, skip training and forward directly from last saved models\nif is_production:\n    ep = N_ep - 1\n    N_ep = 0\n    model_files = {}\n\n    for arch in pt_files.keys():\n        model_files[arch] = out_folder + ""/exp_files/final_"" + arch + "".pkl""\n\n\nop_counter = 1  # used to dected the next configuration file from the list_chunks.txt\n\n# Reading the ordered list of config file to process\ncfg_file_list = [line.rstrip(""\\n"") for line in open(out_folder + ""/exp_files/list_chunks.txt"")]\ncfg_file_list.append(cfg_file_list[-1])\n\n\n# A variable that tells if the current chunk is the first one that is being processed:\nprocessed_first = True\n\ndata_name = []\ndata_set = []\ndata_end_index = []\nfea_dict = []\nlab_dict = []\narch_dict = []\n\n\n# --------TRAINING LOOP--------#\nfor ep in range(N_ep):\n\n    tr_loss_tot = 0\n    tr_error_tot = 0\n    tr_time_tot = 0\n    val_time_tot = 0\n\n    print(\n        ""------------------------------ Epoch %s / %s ------------------------------""\n        % (format(ep, N_ep_str_format), format(N_ep - 1, N_ep_str_format))\n    )\n\n    for tr_data in tr_data_lst:\n\n        # Compute the total number of chunks for each training epoch\n        N_ck_tr = compute_n_chunks(out_folder, tr_data, ep, N_ep_str_format, ""train"")\n        N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_ck_tr)), 1)) + ""d""\n\n        # ***Epoch training***\n        for ck in range(N_ck_tr):\n\n            # paths of the output files (info,model,chunk_specific cfg file)\n            info_file = (\n                out_folder\n                + ""/exp_files/train_""\n                + tr_data\n                + ""_ep""\n                + format(ep, N_ep_str_format)\n                + ""_ck""\n                + format(ck, N_ck_str_format)\n                + "".info""\n            )\n\n            if ep + ck == 0:\n                model_files_past = {}\n            else:\n                model_files_past = model_files\n\n            model_files = {}\n            for arch in pt_files.keys():\n                model_files[arch] = info_file.replace("".info"", ""_"" + arch + "".pkl"")\n\n            config_chunk_file = (\n                out_folder\n                + ""/exp_files/train_""\n                + tr_data\n                + ""_ep""\n                + format(ep, N_ep_str_format)\n                + ""_ck""\n                + format(ck, N_ck_str_format)\n                + "".cfg""\n            )\n\n            # update learning rate in the cfg file (if needed)\n            change_lr_cfg(config_chunk_file, lr, ep)\n\n            # if this chunk has not already been processed, do training...\n            if not (os.path.exists(info_file)):\n\n                print(""Training %s chunk = %i / %i"" % (tr_data, ck + 1, N_ck_tr))\n\n                # getting the next chunk\n                next_config_file = cfg_file_list[op_counter]\n\n                # run chunk processing\n                [data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict] = run_nn(\n                    data_name,\n                    data_set,\n                    data_end_index,\n                    fea_dict,\n                    lab_dict,\n                    arch_dict,\n                    config_chunk_file,\n                    processed_first,\n                    next_config_file,\n                )\n\n                # update the first_processed variable\n                processed_first = False\n\n                if not (os.path.exists(info_file)):\n                    sys.stderr.write(\n                        ""ERROR: training epoch %i, chunk %i not done! File %s does not exist.\\nSee %s \\n""\n                        % (ep, ck, info_file, log_file)\n                    )\n                    sys.exit(0)\n\n            # update the operation counter\n            op_counter += 1\n\n            # update pt_file (used to initialized the DNN for the next chunk)\n            for pt_arch in pt_files.keys():\n                pt_files[pt_arch] = (\n                    out_folder\n                    + ""/exp_files/train_""\n                    + tr_data\n                    + ""_ep""\n                    + format(ep, N_ep_str_format)\n                    + ""_ck""\n                    + format(ck, N_ck_str_format)\n                    + ""_""\n                    + pt_arch\n                    + "".pkl""\n                )\n\n            # remove previous pkl files\n            if len(model_files_past.keys()) > 0:\n                for pt_arch in pt_files.keys():\n                    if os.path.exists(model_files_past[pt_arch]):\n                        os.remove(model_files_past[pt_arch])\n\n            if do_validation_after_chunk(ck, N_ck_tr, config):\n                if not _is_first_validation(ep,ck, N_ck_tr, config):\n                    valid_peformance_dict_prev = valid_peformance_dict\n                valid_peformance_dict = {}\n                for valid_data in valid_data_lst:\n                    N_ck_valid = compute_n_chunks(out_folder, valid_data, ep, N_ep_str_format, ""valid"")\n                    N_ck_str_format_val = ""0"" + str(max(math.ceil(np.log10(N_ck_valid)), 1)) + ""d""\n                    for ck_val in range(N_ck_valid):\n                        info_file = get_val_info_file_path(\n                            out_folder,\n                            valid_data,\n                            ep,\n                            ck,\n                            ck_val,\n                            N_ep_str_format,\n                            N_ck_str_format,\n                            N_ck_str_format_val,\n                        )\n                        config_chunk_file = get_val_cfg_file_path(\n                            out_folder,\n                            valid_data,\n                            ep,\n                            ck,\n                            ck_val,\n                            N_ep_str_format,\n                            N_ck_str_format,\n                            N_ck_str_format_val,\n                        )\n                        if not (os.path.exists(info_file)):\n                            print(""Validating %s chunk = %i / %i"" % (valid_data, ck_val + 1, N_ck_valid))\n                            next_config_file = cfg_file_list[op_counter]\n                            data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict = run_nn(\n                                data_name,\n                                data_set,\n                                data_end_index,\n                                fea_dict,\n                                lab_dict,\n                                arch_dict,\n                                config_chunk_file,\n                                processed_first,\n                                next_config_file,\n                            )\n                            processed_first = False\n                            if not (os.path.exists(info_file)):\n                                sys.stderr.write(\n                                    ""ERROR: validation on epoch %i, chunk %i, valid chunk %i of dataset %s not done! File %s does not exist.\\nSee %s \\n""\n                                    % (ep, ck, ck_val, valid_data, info_file, log_file)\n                                )\n                                sys.exit(0)\n                        op_counter += 1\n                    valid_info_lst = sorted(\n                        glob.glob(\n                            get_val_info_file_path(\n                                out_folder,\n                                valid_data,\n                                ep,\n                                ck,\n                                None,\n                                N_ep_str_format,\n                                N_ck_str_format,\n                                N_ck_str_format_val,\n                            )\n                        )\n                    )\n                    valid_loss, valid_error, valid_time = compute_avg_performance(valid_info_lst)\n                    valid_peformance_dict[valid_data] = [valid_loss, valid_error, valid_time]\n                    val_time_tot += valid_time\n                if not _is_first_validation(ep,ck, N_ck_tr, config):\n                    err_valid_mean = np.mean(np.asarray(list(valid_peformance_dict.values()))[:, 1])\n                    err_valid_mean_prev = np.mean(np.asarray(list(valid_peformance_dict_prev.values()))[:, 1])\n                    for lr_arch in lr.keys():\n                        if ep < N_ep - 1 and auto_lr_annealing[lr_arch]:\n                            if ((err_valid_mean_prev - err_valid_mean) / err_valid_mean) < improvement_threshold[\n                                lr_arch\n                            ]:\n                                new_lr_value = float(lr[lr_arch][ep]) * halving_factor[lr_arch]\n                                for i in range(ep + 1, N_ep):\n                                    lr[lr_arch][i] = str(new_lr_value)\n\n        # Training Loss and Error\n        tr_info_lst = sorted(\n            glob.glob(out_folder + ""/exp_files/train_"" + tr_data + ""_ep"" + format(ep, N_ep_str_format) + ""*.info"")\n        )\n        [tr_loss, tr_error, tr_time] = compute_avg_performance(tr_info_lst)\n\n        tr_loss_tot = tr_loss_tot + tr_loss\n        tr_error_tot = tr_error_tot + tr_error\n        tr_time_tot = tr_time_tot + tr_time\n        tot_time = tr_time + val_time_tot\n\n    # Print results in both res_file and stdout\n    dump_epoch_results(\n        res_file_path,\n        ep,\n        tr_data_lst,\n        tr_loss_tot,\n        tr_error_tot,\n        tot_time,\n        valid_data_lst,\n        valid_peformance_dict,\n        lr,\n        N_ep,\n    )\n\n# Training has ended, copy the last .pkl to final_arch.pkl for production\nfor pt_arch in pt_files.keys():\n    if os.path.exists(model_files[pt_arch]) and not os.path.exists(out_folder + ""/exp_files/final_"" + pt_arch + "".pkl""):\n        copyfile(model_files[pt_arch], out_folder + ""/exp_files/final_"" + pt_arch + "".pkl"")\n\n\n# --------FORWARD--------#\nfor forward_data in forward_data_lst:\n\n    # Compute the number of chunks\n    N_ck_forward = compute_n_chunks(out_folder, forward_data, ep, N_ep_str_format, ""forward"")\n    N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_ck_forward)), 1)) + ""d""\n\n    processes = list()\n    info_files = list()\n    for ck in range(N_ck_forward):\n\n        if not is_production:\n            print(""Testing %s chunk = %i / %i"" % (forward_data, ck + 1, N_ck_forward))\n        else:\n            print(""Forwarding %s chunk = %i / %i"" % (forward_data, ck + 1, N_ck_forward))\n\n        # output file\n        info_file = (\n            out_folder\n            + ""/exp_files/forward_""\n            + forward_data\n            + ""_ep""\n            + format(ep, N_ep_str_format)\n            + ""_ck""\n            + format(ck, N_ck_str_format)\n            + "".info""\n        )\n        config_chunk_file = (\n            out_folder\n            + ""/exp_files/forward_""\n            + forward_data\n            + ""_ep""\n            + format(ep, N_ep_str_format)\n            + ""_ck""\n            + format(ck, N_ck_str_format)\n            + "".cfg""\n        )\n\n        # Do forward if the chunk was not already processed\n        if not (os.path.exists(info_file)):\n\n            # Doing forward\n\n            # getting the next chunk\n            next_config_file = cfg_file_list[op_counter]\n\n            # run chunk processing\n            if _run_forwarding_in_subprocesses(config):\n                shared_list = list()\n                output_folder = config[""exp""][""out_folder""]\n                save_gpumem = strtobool(config[""exp""][""save_gpumem""])\n                use_cuda = strtobool(config[""exp""][""use_cuda""])\n                p = read_next_chunk_into_shared_list_with_subprocess(\n                    read_lab_fea, shared_list, config_chunk_file, is_production, output_folder, wait_for_process=True\n                )\n                data_name, data_end_index_fea, data_end_index_lab, fea_dict, lab_dict, arch_dict, data_set_dict = extract_data_from_shared_list(\n                    shared_list\n                )\n                data_set_inp, data_set_ref = convert_numpy_to_torch(data_set_dict, save_gpumem, use_cuda)\n                data_set = {""input"": data_set_inp, ""ref"": data_set_ref}\n                data_end_index = {""fea"": data_end_index_fea, ""lab"": data_end_index_lab}\n                p = multiprocessing.Process(\n                    target=run_nn,\n                    kwargs={\n                        ""data_name"": data_name,\n                        ""data_set"": data_set,\n                        ""data_end_index"": data_end_index,\n                        ""fea_dict"": fea_dict,\n                        ""lab_dict"": lab_dict,\n                        ""arch_dict"": arch_dict,\n                        ""cfg_file"": config_chunk_file,\n                        ""processed_first"": False,\n                        ""next_config_file"": None,\n                    },\n                )\n                processes.append(p)\n                if _max_nr_of_parallel_forwarding_processes(config) != -1 and len(\n                    processes\n                ) > _max_nr_of_parallel_forwarding_processes(config):\n                    processes[0].join()\n                    del processes[0]\n                p.start()\n            else:\n                [data_name, data_set, data_end_index, fea_dict, lab_dict, arch_dict] = run_nn(\n                    data_name,\n                    data_set,\n                    data_end_index,\n                    fea_dict,\n                    lab_dict,\n                    arch_dict,\n                    config_chunk_file,\n                    processed_first,\n                    next_config_file,\n                )\n                processed_first = False\n                if not (os.path.exists(info_file)):\n                    sys.stderr.write(\n                        ""ERROR: forward chunk %i of dataset %s not done! File %s does not exist.\\nSee %s \\n""\n                        % (ck, forward_data, info_file, log_file)\n                    )\n                    sys.exit(0)\n\n            info_files.append(info_file)\n\n        # update the operation counter\n        op_counter += 1\n    if _run_forwarding_in_subprocesses(config):\n        for process in processes:\n            process.join()\n        for info_file in info_files:\n            if not (os.path.exists(info_file)):\n                sys.stderr.write(\n                    ""ERROR: File %s does not exist. Forwarding did not suceed.\\nSee %s \\n"" % (info_file, log_file)\n                )\n                sys.exit(0)\n\n\n# --------DECODING--------#\ndec_lst = glob.glob(out_folder + ""/exp_files/*_to_decode.ark"")\n\nforward_data_lst = config[""data_use""][""forward_with""].split("","")\nforward_outs = config[""forward""][""forward_out""].split("","")\nforward_dec_outs = list(map(strtobool, config[""forward""][""require_decoding""].split("","")))\n\n\nfor data in forward_data_lst:\n    for k in range(len(forward_outs)):\n        if forward_dec_outs[k]:\n\n            print(""Decoding %s output %s"" % (data, forward_outs[k]))\n\n            info_file = out_folder + ""/exp_files/decoding_"" + data + ""_"" + forward_outs[k] + "".info""\n\n            # create decode config file\n            config_dec_file = out_folder + ""/decoding_"" + data + ""_"" + forward_outs[k] + "".conf""\n            config_dec = configparser.ConfigParser()\n            config_dec.add_section(""decoding"")\n\n            for dec_key in config[""decoding""].keys():\n                config_dec.set(""decoding"", dec_key, config[""decoding""][dec_key])\n\n            # add graph_dir, datadir, alidir\n            lab_field = config[cfg_item2sec(config, ""data_name"", data)][""lab""]\n\n            # Production case, we don\'t have labels\n            if not is_production:\n                pattern = ""lab_folder=(.*)\\nlab_opts=(.*)\\nlab_count_file=(.*)\\nlab_data_folder=(.*)\\nlab_graph=(.*)""\n                alidir = re.findall(pattern, lab_field)[0][0]\n                config_dec.set(""decoding"", ""alidir"", os.path.abspath(alidir))\n\n                datadir = re.findall(pattern, lab_field)[0][3]\n                config_dec.set(""decoding"", ""data"", os.path.abspath(datadir))\n\n                graphdir = re.findall(pattern, lab_field)[0][4]\n                config_dec.set(""decoding"", ""graphdir"", os.path.abspath(graphdir))\n            else:\n                pattern = ""lab_data_folder=(.*)\\nlab_graph=(.*)""\n                datadir = re.findall(pattern, lab_field)[0][0]\n                config_dec.set(""decoding"", ""data"", os.path.abspath(datadir))\n\n                graphdir = re.findall(pattern, lab_field)[0][1]\n                config_dec.set(""decoding"", ""graphdir"", os.path.abspath(graphdir))\n\n                # The ali dir is supposed to be in exp/model/ which is one level ahead of graphdir\n                alidir = graphdir.split(""/"")[0 : len(graphdir.split(""/"")) - 1]\n                alidir = ""/"".join(alidir)\n                config_dec.set(""decoding"", ""alidir"", os.path.abspath(alidir))\n\n            with open(config_dec_file, ""w"") as configfile:\n                config_dec.write(configfile)\n\n            out_folder = os.path.abspath(out_folder)\n            files_dec = out_folder + ""/exp_files/forward_"" + data + ""_ep*_ck*_"" + forward_outs[k] + ""_to_decode.ark""\n            out_dec_folder = out_folder + ""/decode_"" + data + ""_"" + forward_outs[k]\n\n            if not (os.path.exists(info_file)):\n\n                # Run the decoder\n                cmd_decode = (\n                    cmd\n                    + config[""decoding""][""decoding_script_folder""]\n                    + ""/""\n                    + config[""decoding""][""decoding_script""]\n                    + "" ""\n                    + os.path.abspath(config_dec_file)\n                    + "" ""\n                    + out_dec_folder\n                    + \' ""\'\n                    + files_dec\n                    + \'""\'\n                )\n                run_shell(cmd_decode, log_file)\n\n                # remove ark files if needed\n                if not forward_save_files[k]:\n                    list_rem = glob.glob(files_dec)\n                    for rem_ark in list_rem:\n                        os.remove(rem_ark)\n\n            # Print WER results and write info file\n            cmd_res = ""./check_res_dec.sh "" + out_dec_folder\n            wers = run_shell(cmd_res, log_file).decode(""utf-8"")\n            res_file = open(res_file_path, ""a"")\n            res_file.write(""%s\\n"" % wers)\n            print(wers)\n\n# Saving Loss and Err as .txt and plotting curves\nif not is_production:\n    create_curves(out_folder, N_ep, valid_data_lst)\n'"
save_raw_fea.py,0,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n#\n# Description: This script generates kaldi ark files containing raw features.\n# The file list must be a file containing ""snt_id file.wav"".\n# Note that only wav files are supported here (sphere or other format are not supported)\n##########################################################\n\n\nimport scipy.io.wavfile\nimport math\nimport numpy as np\nimport os\nfrom data_io import read_vec_int_ark, write_mat\n\n\n# Run it for all the data chunks (e.g., train, dev, test) => uncomment\n\nlab_folder = ""/users/parcollet/KALDI/kaldi-trunk/egs/timit/s5/exp/dnn4_pretrain-dbn_dnn_ali_test""\nlab_opts = ""ali-to-pdf""\nout_folder = ""/users/parcollet/KALDI/kaldi-trunk/egs/timit/s5/data/raw_TIMIT_200ms/test""\nwav_lst = ""/users/parcollet/KALDI/kaldi-trunk/egs/timit/s5/data/test/wav.lst""\nscp_file_out = ""/users/parcollet/KALDI/kaldi-trunk/egs/timit/s5/data/raw_TIMIT_200ms/test/feats_raw.scp""\n\n# lab_folder=\'quick_test/dnn4_pretrain-dbn_dnn_ali_dev\'\n# lab_opts=\'ali-to-pdf\'\n# out_folder=\'raw_TIMIT_200ms/dev\'\n# wav_lst=\'/home/mirco/pytorch-kaldi-new/quick_test/data/dev/wav_lst.scp\'\n# scp_file_out=\'quick_test/data/dev/feats_raw.scp\'\n\n# lab_folder=\'quick_test/dnn4_pretrain-dbn_dnn_ali_test\'\n# lab_opts=\'ali-to-pdf\'\n# out_folder=\'raw_TIMIT_200ms/test\'\n# wav_lst=\'/home/mirco/pytorch-kaldi-new/quick_test/data/test/wav_lst.scp\'\n# scp_file_out=\'quick_test/data/test/feats_raw.scp\'\n\n\nsig_fs = 16000  # Hz\nsig_wlen = 200  # ms\n\nlab_fs = 16000  # Hz\nlab_wlen = 25  # ms\nlab_wshift = 10  # ms\n\nsig_wlen_samp = int((sig_fs * sig_wlen) / 1000)\nlab_wlen_samp = int((lab_fs * lab_wlen) / 1000)\nlab_wshift_samp = int((lab_fs * lab_wshift) / 1000)\n\n\n# Create the output folder\ntry:\n    os.stat(out_folder)\nexcept:\n    os.makedirs(out_folder)\n\n\n# Creare the scp file\nscp_file = open(scp_file_out, ""w"")\n\n# reading the labels\nlab = {\n    k: v\n    for k, v in read_vec_int_ark(\n        ""gunzip -c "" + lab_folder + ""/ali*.gz | "" + lab_opts + "" "" + lab_folder + ""/final.mdl ark:- ark:-|"", out_folder\n    )\n}\n\n# reading the list file\nwith open(wav_lst) as f:\n    sig_lst = f.readlines()\n\nsig_lst = [x.strip() for x in sig_lst]\n\nfor sig_file in sig_lst:\n    sig_id = sig_file.split("" "")[0]\n    sig_path = sig_file.split("" "")[1]\n    [fs, signal] = scipy.io.wavfile.read(sig_path)\n    signal = signal.astype(float) / 32768\n    signal = signal / np.max(np.abs(signal))\n\n    cnt_fr = 0\n    beg_samp = 0\n    frame_all = []\n\n    while beg_samp + lab_wlen_samp < signal.shape[0]:\n        sample_fr = np.zeros(sig_wlen_samp)\n        central_sample_lab = int(((beg_samp + lab_wlen_samp / 2) - 1))\n        central_fr_index = int(((sig_wlen_samp / 2) - 1))\n\n        beg_signal_fr = int(central_sample_lab - (sig_wlen_samp / 2))\n        end_signal_fr = int(central_sample_lab + (sig_wlen_samp / 2))\n\n        if beg_signal_fr >= 0 and end_signal_fr <= signal.shape[0]:\n            sample_fr = signal[beg_signal_fr:end_signal_fr]\n        else:\n            if beg_signal_fr < 0:\n                n_left_samples = central_sample_lab\n                sample_fr[central_fr_index - n_left_samples + 1 :] = signal[0:end_signal_fr]\n            if end_signal_fr > signal.shape[0]:\n                n_right_samples = signal.shape[0] - central_sample_lab\n                sample_fr[0 : central_fr_index + n_right_samples + 1] = signal[beg_signal_fr:]\n\n        frame_all.append(sample_fr)\n        cnt_fr = cnt_fr + 1\n        beg_samp = beg_samp + lab_wshift_samp\n\n    frame_all = np.asarray(frame_all)\n\n    # Save the matrix into a kaldi ark\n    out_file = out_folder + ""/"" + sig_id + "".ark""\n    write_mat(out_folder, out_file, frame_all, key=sig_id)\n    print(sig_id)\n    scp_file.write(sig_id + "" "" + out_folder + ""/"" + sig_id + "".ark:"" + str(len(sig_id) + 1) + ""\\n"")\n\n    N_fr_comp = 1 + math.floor((signal.shape[0] - 400) / 160)\n    # print(""%s %i %i ""%(lab[sig_id].shape[0],N_fr_comp,cnt_fr))\n\nscp_file.close()\n'"
tune_hyperparameters.py,0,"b'#!/usr/bin/env python\n##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n#\n# Description:\n# This scripts generates config files with the random hyperparamters specified by the user.\n# python tune_hyperparameters.py cfg_file out_folder N_exp hyperparameters_spec\n# e.g., python tune_hyperparameters.py cfg/TIMIT_MLP_mfcc.cfg exp/TIMIT_MLP_mfcc_tuning 10 arch_lr=randfloat(0.001,0.01) batch_size_train=randint(32,256) dnn_act=choose_str{relu,relu,relu,relu,softmax|tanh,tanh,tanh,tanh,softmax}\n##########################################################\n\n\nimport random\nimport re\nimport os\nimport sys\nfrom random import randint\n\nif __name__ == ""__main__"":\n    cfg_file = sys.argv[1]\n    output_folder = sys.argv[2]\n    N_exp = int(sys.argv[3])\n    hyperparam_list = sys.argv[4:]\n    seed = 1234\n\n    print(""Generating config file for hyperparameter tuning..."")\n\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    random.seed(seed)\n\n    for i in range(N_exp):\n\n        cfg_file_out = output_folder + ""/exp"" + str(i) + "".cfg""\n\n        with open(cfg_file_out, ""wt"") as cfg_out, open(cfg_file, ""rt"") as cfg_in:\n            for line in cfg_in:\n\n                key = line.split(""="")[0]\n\n                if key == ""out_folder"":\n                    line = ""out_folder="" + output_folder + ""/exp"" + str(i) + ""\\n""\n\n                hyper_found = False\n                for hyperparam in hyperparam_list:\n\n                    key_hyper = hyperparam.split(""="")[0]\n\n                    if key == key_hyper:\n\n                        if ""randint"" in hyperparam:\n                            lower, higher = re.search(""randint\\((.+?)\\)"", hyperparam).group(1).split("","")\n                            value_hyper = randint(int(lower), int(higher))\n                            hyper_found = True\n\n                        if ""randfloat"" in hyperparam:\n                            lower, higher = re.search(""randfloat\\((.+?)\\)"", hyperparam).group(1).split("","")\n                            value_hyper = random.uniform(float(lower), float(higher))\n                            hyper_found = True\n\n                        if ""choose_str"" in hyperparam:\n                            value_hyper = random.choice(re.search(""\\{(.+?)\\}"", hyperparam).group(1).split(""|""))\n                            hyper_found = True\n\n                        if ""choose_int"" in hyperparam:\n                            value_hyper = int(random.choice(re.search(""\\{(.+?)\\}"", hyperparam).group(1).split(""|"")))\n                            hyper_found = True\n\n                        if ""choose_float"" in hyperparam:\n                            value_hyper = float(random.choice(re.search(""\\{(.+?)\\}"", hyperparam).group(1).split(""|"")))\n                            hyper_found = True\n\n                        line_out = key + ""="" + str(value_hyper) + ""\\n""\n\n                if not hyper_found:\n                    line_out = line\n\n                cfg_out.write(line_out)\n\n            print(""Done %s"" % cfg_file_out)\n'"
utils.py,10,"b'##########################################################\n# pytorch-kaldi v.0.1\n# Mirco Ravanelli, Titouan Parcollet\n# Mila, University of Montreal\n# October 2018\n##########################################################\n\nimport configparser\nimport sys\nimport os.path\nimport random\nimport subprocess\nimport numpy as np\nimport re\nimport glob\nfrom distutils.util import strtobool\nimport importlib\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport math\n\n\ndef run_command(cmd):\n    """"""from http://blog.kagesenshi.org/2008/02/teeing-python-subprocesspopen-output.html\n    """"""\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)\n    stdout = []\n    while True:\n        line = p.stdout.readline()\n        stdout.append(line)\n        print(line.decode(""utf-8""))\n        if line == """" and p.poll() != None:\n            break\n    return """".join(stdout)\n\n\ndef run_shell_display(cmd):\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    while True:\n        out = p.stdout.read(1).decode(""utf-8"")\n        if out == """" and p.poll() != None:\n            break\n        if out != """":\n            sys.stdout.write(out)\n            sys.stdout.flush()\n    return\n\n\ndef run_shell(cmd, log_file):\n    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n\n    (output, err) = p.communicate()\n    p.wait()\n    with open(log_file, ""a+"") as logfile:\n        logfile.write(output.decode(""utf-8"") + ""\\n"")\n        logfile.write(err.decode(""utf-8"") + ""\\n"")\n\n    # print(output.decode(""utf-8""))\n    return output\n\n\ndef read_args_command_line(args, config):\n\n    sections = []\n    fields = []\n    values = []\n\n    for i in range(2, len(args)):\n\n        # check if the option is valid for second level\n        r2 = re.compile(""--.*,.*=.*"")\n\n        # check if the option is valid for 4 level\n        r4 = re.compile(\'--.*,.*,.*,.*="".*""\')\n        if r2.match(args[i]) is None and r4.match(args[i]) is None:\n            sys.stderr.write(\n                \'ERROR: option ""%s"" from command line is not valid! (the format must be ""--section,field=value"")\\n\'\n                % (args[i])\n            )\n            sys.exit(0)\n\n        sections.append(re.search(""--(.*),"", args[i]).group(1))\n        fields.append(re.search("",(.*)"", args[i].split(""="")[0]).group(1))\n        values.append(re.search(""=(.*)"", args[i]).group(1))\n\n    # parsing command line arguments\n    for i in range(len(sections)):\n\n        # Remove multi level is level >= 2\n        sections[i] = sections[i].split("","")[0]\n\n        if sections[i] in config.sections():\n\n            # Case of args level > than 2 like --sec,fields,0,field=""value""\n            if len(fields[i].split("","")) >= 2:\n\n                splitted = fields[i].split("","")\n\n                # Get the actual fields\n                field = splitted[0]\n                number = int(splitted[1])\n                f_name = splitted[2]\n                if field in list(config[sections[i]]):\n\n                    # Get the current string of the corresponding field\n                    current_config_field = config[sections[i]][field]\n\n                    # Count the number of occurence of the required field\n                    matching = re.findall(f_name + ""."", current_config_field)\n                    if number >= len(matching):\n                        sys.stderr.write(\n                            \'ERROR: the field number ""%s"" provided from command line is not valid, we found ""%s"" ""%s"" field(s) in section ""%s""!\\n\'\n                            % (number, len(matching), f_name, field)\n                        )\n                        sys.exit(0)\n                    else:\n\n                        # Now replace\n                        str_to_be_replaced = re.findall(f_name + "".*"", current_config_field)[number]\n                        new_str = str(f_name + ""="" + values[i])\n                        replaced = nth_replace_string(current_config_field, str_to_be_replaced, new_str, number + 1)\n                        config[sections[i]][field] = replaced\n\n                else:\n                    sys.stderr.write(\n                        \'ERROR: field ""%s"" of section ""%s"" from command line is not valid!"")\\n\' % (field, sections[i])\n                    )\n                    sys.exit(0)\n            else:\n                if fields[i] in list(config[sections[i]]):\n                    config[sections[i]][fields[i]] = values[i]\n                else:\n                    sys.stderr.write(\n                        \'ERROR: field ""%s"" of section ""%s"" from command line is not valid!"")\\n\'\n                        % (fields[i], sections[i])\n                    )\n                    sys.exit(0)\n        else:\n            sys.stderr.write(\'ERROR: section ""%s"" from command line is not valid!"")\\n\' % (sections[i]))\n            sys.exit(0)\n\n    return [sections, fields, values]\n\n\ndef compute_avg_performance(info_lst):\n\n    losses = []\n    errors = []\n    times = []\n\n    for tr_info_file in info_lst:\n        config_res = configparser.ConfigParser()\n        config_res.read(tr_info_file)\n        losses.append(float(config_res[""results""][""loss""]))\n        errors.append(float(config_res[""results""][""err""]))\n        times.append(float(config_res[""results""][""elapsed_time_chunk""]))\n\n    loss = np.mean(losses)\n    error = np.mean(errors)\n    time = np.sum(times)\n\n    return [loss, error, time]\n\n\ndef check_field(inp, type_inp, field):\n\n    valid_field = True\n\n    if inp == """" and field != ""cmd"":\n        sys.stderr.write(\'ERROR: The the field  ""%s"" of the config file is empty! \\n\' % (field))\n        valid_field = False\n        sys.exit(0)\n\n    if type_inp == ""path"":\n        if not (os.path.isfile(inp)) and not (os.path.isdir(inp)) and inp != ""none"":\n            sys.stderr.write(\n                \'ERROR: The path ""%s"" specified in the field  ""%s"" of the config file does not exists! \\n\'\n                % (inp, field)\n            )\n            valid_field = False\n            sys.exit(0)\n\n    if ""{"" and ""}"" in type_inp:\n        arg_list = type_inp[1:-1].split("","")\n        if inp not in arg_list:\n            sys.stderr.write(\'ERROR: The field ""%s"" can only contain %s  arguments \\n\' % (field, arg_list))\n            valid_field = False\n            sys.exit(0)\n\n    if ""int("" in type_inp:\n        try:\n            int(inp)\n        except ValueError:\n            sys.stderr.write(\'ERROR: The field ""%s"" can only contain an integer (got ""%s"") \\n\' % (field, inp))\n            valid_field = False\n            sys.exit(0)\n\n        # Check if the value if within the expected range\n        lower_bound = type_inp.split("","")[0][4:]\n        upper_bound = type_inp.split("","")[1][:-1]\n\n        if lower_bound != ""-inf"":\n            if int(inp) < int(lower_bound):\n                sys.stderr.write(\n                    \'ERROR: The field ""%s"" can only contain an integer greater than %s (got ""%s"") \\n\'\n                    % (field, lower_bound, inp)\n                )\n                valid_field = False\n                sys.exit(0)\n\n        if upper_bound != ""inf"":\n            if int(inp) > int(upper_bound):\n                sys.stderr.write(\n                    \'ERROR: The field ""%s"" can only contain an integer smaller than %s (got ""%s"") \\n\'\n                    % (field, upper_bound, inp)\n                )\n                valid_field = False\n                sys.exit(0)\n\n    if ""float("" in type_inp:\n        try:\n            float(inp)\n        except ValueError:\n            sys.stderr.write(\'ERROR: The field ""%s"" can only contain a float (got ""%s"") \\n\' % (field, inp))\n            valid_field = False\n            sys.exit(0)\n\n        # Check if the value if within the expected range\n        lower_bound = type_inp.split("","")[0][6:]\n        upper_bound = type_inp.split("","")[1][:-1]\n\n        if lower_bound != ""-inf"":\n            if float(inp) < float(lower_bound):\n                sys.stderr.write(\n                    \'ERROR: The field ""%s"" can only contain a float greater than %s (got ""%s"") \\n\'\n                    % (field, lower_bound, inp)\n                )\n                valid_field = False\n                sys.exit(0)\n\n        if upper_bound != ""inf"":\n            if float(inp) > float(upper_bound):\n                sys.stderr.write(\n                    \'ERROR: The field ""%s"" can only contain a float smaller than %s (got ""%s"") \\n\'\n                    % (field, upper_bound, inp)\n                )\n                valid_field = False\n                sys.exit(0)\n\n    if type_inp == ""bool"":\n        lst = {""True"", ""true"", ""1"", ""False"", ""false"", ""0""}\n        if not (inp in lst):\n            sys.stderr.write(\'ERROR: The field ""%s"" can only contain a boolean (got ""%s"") \\n\' % (field, inp))\n            valid_field = False\n            sys.exit(0)\n\n    if ""int_list("" in type_inp:\n        lst = inp.split("","")\n        try:\n            list(map(int, lst))\n        except ValueError:\n            sys.stderr.write(\n                \'ERROR: The field ""%s"" can only contain a list of integer (got ""%s"").  Make also sure there aren\\\'t white spaces between commas.\\n\'\n                % (field, inp)\n            )\n            valid_field = False\n            sys.exit(0)\n\n        # Check if the value if within the expected range\n        lower_bound = type_inp.split("","")[0][9:]\n        upper_bound = type_inp.split("","")[1][:-1]\n\n        for elem in lst:\n\n            if lower_bound != ""-inf"":\n                if int(elem) < int(lower_bound):\n                    sys.stderr.write(\n                        \'ERROR: The field ""%s"" can only contain an integer greater than %s (got ""%s"") \\n\'\n                        % (field, lower_bound, elem)\n                    )\n                    valid_field = False\n                    sys.exit(0)\n            if upper_bound != ""inf"":\n                if int(elem) > int(upper_bound):\n                    sys.stderr.write(\n                        \'ERROR: The field ""%s"" can only contain an integer smaller than %s (got ""%s"") \\n\'\n                        % (field, upper_bound, elem)\n                    )\n                    valid_field = False\n                    sys.exit(0)\n\n    if ""float_list("" in type_inp:\n        lst = inp.split("","")\n        try:\n            list(map(float, lst))\n        except ValueError:\n            sys.stderr.write(\n                \'ERROR: The field ""%s"" can only contain a list of floats (got ""%s""). Make also sure there aren\\\'t white spaces between commas. \\n\'\n                % (field, inp)\n            )\n            valid_field = False\n            sys.exit(0)\n        # Check if the value if within the expected range\n        lower_bound = type_inp.split("","")[0][11:]\n        upper_bound = type_inp.split("","")[1][:-1]\n\n        for elem in lst:\n\n            if lower_bound != ""-inf"":\n                if float(elem) < float(lower_bound):\n                    sys.stderr.write(\n                        \'ERROR: The field ""%s"" can only contain a float greater than %s (got ""%s"") \\n\'\n                        % (field, lower_bound, elem)\n                    )\n                    valid_field = False\n                    sys.exit(0)\n\n            if upper_bound != ""inf"":\n                if float(elem) > float(upper_bound):\n                    sys.stderr.write(\n                        \'ERROR: The field ""%s"" can only contain a float smaller than %s (got ""%s"") \\n\'\n                        % (field, upper_bound, elem)\n                    )\n                    valid_field = False\n                    sys.exit(0)\n\n    if type_inp == ""bool_list"":\n        lst = {""True"", ""true"", ""1"", ""False"", ""false"", ""0""}\n        inps = inp.split("","")\n        for elem in inps:\n            if not (elem in lst):\n                sys.stderr.write(\n                    \'ERROR: The field ""%s"" can only contain a list of boolean (got ""%s""). Make also sure there aren\\\'t white spaces between commas.\\n\'\n                    % (field, inp)\n                )\n                valid_field = False\n                sys.exit(0)\n\n    return valid_field\n\n\ndef get_all_archs(config):\n\n    arch_lst = []\n    for sec in config.sections():\n        if ""architecture"" in sec:\n            arch_lst.append(sec)\n    return arch_lst\n\n\ndef expand_section(config_proto, config):\n\n    # expands config_proto with fields in prototype files\n    name_data = []\n    name_arch = []\n    for sec in config.sections():\n\n        if ""dataset"" in sec:\n\n            config_proto.add_section(sec)\n            config_proto[sec] = config_proto[""dataset""]\n            name_data.append(config[sec][""data_name""])\n\n        if ""architecture"" in sec:\n            name_arch.append(config[sec][""arch_name""])\n            config_proto.add_section(sec)\n            config_proto[sec] = config_proto[""architecture""]\n            proto_file = config[sec][""arch_proto""]\n\n            # Reading proto file (architecture)\n            config_arch = configparser.ConfigParser()\n            config_arch.read(proto_file)\n\n            # Reading proto options\n            fields_arch = list(dict(config_arch.items(""proto"")).keys())\n            fields_arch_type = list(dict(config_arch.items(""proto"")).values())\n\n            for i in range(len(fields_arch)):\n                config_proto.set(sec, fields_arch[i], fields_arch_type[i])\n\n            # Reading proto file (architecture_optimizer)\n            opt_type = config[sec][""arch_opt""]\n            if opt_type == ""sgd"":\n                proto_file = ""proto/sgd.proto""\n\n            if opt_type == ""rmsprop"":\n                proto_file = ""proto/rmsprop.proto""\n\n            if opt_type == ""adam"":\n                proto_file = ""proto/adam.proto""\n\n            config_arch = configparser.ConfigParser()\n            config_arch.read(proto_file)\n\n            # Reading proto options\n            fields_arch = list(dict(config_arch.items(""proto"")).keys())\n            fields_arch_type = list(dict(config_arch.items(""proto"")).values())\n\n            for i in range(len(fields_arch)):\n                config_proto.set(sec, fields_arch[i], fields_arch_type[i])\n\n    config_proto.remove_section(""dataset"")\n    config_proto.remove_section(""architecture"")\n\n    return [config_proto, name_data, name_arch]\n\n\ndef expand_section_proto(config_proto, config):\n\n    # Read config proto file\n    config_proto_optim_file = config[""optimization""][""opt_proto""]\n    config_proto_optim = configparser.ConfigParser()\n    config_proto_optim.read(config_proto_optim_file)\n    for optim_par in list(config_proto_optim[""proto""]):\n        config_proto.set(""optimization"", optim_par, config_proto_optim[""proto""][optim_par])\n\n\ndef check_cfg_fields(config_proto, config, cfg_file):\n\n    # Check mandatory sections and fields\n    sec_parse = True\n\n    for sec in config_proto.sections():\n\n        if any(sec in s for s in config.sections()):\n\n            # Check fields\n            for field in list(dict(config_proto.items(sec)).keys()):\n\n                if not (field in config[sec]):\n                    sys.stderr.write(\n                        \'ERROR: The confg file %s does not contain the field ""%s="" in section  ""[%s]"" (mandatory)!\\n\'\n                        % (cfg_file, field, sec)\n                    )\n                    sec_parse = False\n                else:\n                    field_type = config_proto[sec][field]\n                    if not (check_field(config[sec][field], field_type, field)):\n                        sec_parse = False\n\n        # If a mandatory section doesn\'t exist...\n        else:\n            sys.stderr.write(\n                \'ERROR: The confg file %s does not contain ""[%s]"" section (mandatory)!\\n\' % (cfg_file, sec)\n            )\n            sec_parse = False\n\n    if sec_parse == False:\n        sys.stderr.write(""ERROR: Revise the confg file %s \\n"" % (cfg_file))\n        sys.exit(0)\n    return sec_parse\n\n\ndef check_consistency_with_proto(cfg_file, cfg_file_proto):\n\n    sec_parse = True\n\n    # Check if cfg file exists\n    try:\n        open(cfg_file, ""r"")\n    except IOError:\n        sys.stderr.write(""ERROR: The confg file %s does not exist!\\n"" % (cfg_file))\n        sys.exit(0)\n\n    # Check if cfg proto  file exists\n    try:\n        open(cfg_file_proto, ""r"")\n    except IOError:\n        sys.stderr.write(""ERROR: The confg file %s does not exist!\\n"" % (cfg_file_proto))\n        sys.exit(0)\n\n    # Parser Initialization\n    config = configparser.ConfigParser()\n\n    # Reading the cfg file\n    config.read(cfg_file)\n\n    # Reading proto cfg file\n    config_proto = configparser.ConfigParser()\n    config_proto.read(cfg_file_proto)\n\n    # Adding the multiple entries in data and architecture sections\n    [config_proto, name_data, name_arch] = expand_section(config_proto, config)\n\n    # Check mandatory sections and fields\n    sec_parse = check_cfg_fields(config_proto, config, cfg_file)\n\n    if sec_parse == False:\n        sys.exit(0)\n\n    return [config_proto, name_data, name_arch]\n\n\ndef check_cfg(cfg_file, config, cfg_file_proto):\n\n    # Check consistency between cfg_file and cfg_file_proto\n    [config_proto, name_data, name_arch] = check_consistency_with_proto(cfg_file, cfg_file_proto)\n\n    # Reload data_name because they might be altered by arguments\n    name_data = []\n    for sec in config.sections():\n        if ""dataset"" in sec:\n            name_data.append(config[sec][""data_name""])\n\n    # check consistency between [data_use] vs [data*]\n    sec_parse = True\n    data_use_with = []\n    for data in list(dict(config.items(""data_use"")).values()):\n        data_use_with.append(data.split("",""))\n\n    data_use_with = sum(data_use_with, [])\n\n    if not (set(data_use_with).issubset(name_data)):\n        sys.stderr.write(""ERROR: in [data_use] you are using a dataset not specified in [dataset*] %s \\n"" % (cfg_file))\n        sec_parse = False\n        sys.exit(0)\n\n    # Set to false the first layer norm layer if the architecture is sequential (to avoid numerical instabilities)\n    seq_model = False\n    for sec in config.sections():\n        if ""architecture"" in sec:\n            if strtobool(config[sec][""arch_seq_model""]):\n                seq_model = True\n                break\n\n    if seq_model:\n        for item in list(config[""architecture1""].items()):\n            if ""use_laynorm"" in item[0] and ""_inp"" not in item[0]:\n                ln_list = item[1].split("","")\n                if ln_list[0] == ""True"":\n                    ln_list[0] = ""False""\n                    config[""architecture1""][item[0]] = "","".join(ln_list)\n\n    # Production case (We don\'t have the alignement for the forward_with), by default the prod\n    # Flag is set to False, and the dataset prod number to 1, corresponding to no prod dataset\n    config[""exp""][""production""] = str(""False"")\n    prod_dataset_number = ""dataset1""\n\n    for data in name_data:\n\n        [lab_names, _, _] = parse_lab_field(config[cfg_item2sec(config, ""data_name"", data)][""lab""])\n        if ""none"" in lab_names and data == config[""data_use""][""forward_with""]:\n            config[""exp""][""production""] = str(""True"")\n            prod_data_name = data\n            for sec in config.sections():\n                if ""dataset"" in sec:\n                    if config[sec][""data_name""] == data:\n                        prod_dataset_number = sec\n        else:\n            continue\n\n    # If production case is detected, remove all the other datasets except production\n    if config[""exp""][""production""] == str(""True""):\n        name_data = [elem for elem in name_data if elem == prod_data_name]\n\n    # Parse fea and lab  fields in datasets*\n    cnt = 0\n    fea_names_lst = []\n    lab_names_lst = []\n    for data in name_data:\n\n        [lab_names, _, _] = parse_lab_field(config[cfg_item2sec(config, ""data_name"", data)][""lab""])\n        if ""none"" in lab_names:\n            continue\n\n        [fea_names, fea_lsts, fea_opts, cws_left, cws_right] = parse_fea_field(\n            config[cfg_item2sec(config, ""data_name"", data)][""fea""]\n        )\n        [lab_names, lab_folders, lab_opts] = parse_lab_field(config[cfg_item2sec(config, ""data_name"", data)][""lab""])\n\n        fea_names_lst.append(sorted(fea_names))\n        lab_names_lst.append(sorted(lab_names))\n\n        # Check that fea_name doesn\'t contain special characters\n        for name_features in fea_names_lst[cnt]:\n            if not (re.match(""^[a-zA-Z0-9]*$"", name_features)):\n                sys.stderr.write(\n                    \'ERROR: features names (fea_name=) must contain only letters or numbers (no special characters as ""_,$,.."") \\n\'\n                )\n                sec_parse = False\n                sys.exit(0)\n\n        if cnt > 0:\n            if fea_names_lst[cnt - 1] != fea_names_lst[cnt]:\n                sys.stderr.write(""ERROR: features name (fea_name) must be the same of all the datasets! \\n"")\n                sec_parse = False\n                sys.exit(0)\n            if lab_names_lst[cnt - 1] != lab_names_lst[cnt]:\n                sys.stderr.write(""ERROR: labels name (lab_name) must be the same of all the datasets! \\n"")\n                sec_parse = False\n                sys.exit(0)\n\n        cnt = cnt + 1\n\n    # Create the output folder\n    out_folder = config[""exp""][""out_folder""]\n\n    if not os.path.exists(out_folder) or not (os.path.exists(out_folder + ""/exp_files"")):\n        os.makedirs(out_folder + ""/exp_files"")\n\n    # Parsing forward field\n    model = config[""model""][""model""]\n    possible_outs = list(re.findall(""(.*)="", model.replace("" "", """")))\n    forward_out_lst = config[""forward""][""forward_out""].split("","")\n    forward_norm_lst = config[""forward""][""normalize_with_counts_from""].split("","")\n    forward_norm_bool_lst = config[""forward""][""normalize_posteriors""].split("","")\n\n    lab_lst = list(re.findall(""lab_name=(.*)\\n"", config[prod_dataset_number][""lab""].replace("" "", """")))\n    lab_folders = list(re.findall(""lab_folder=(.*)\\n"", config[prod_dataset_number][""lab""].replace("" "", """")))\n    N_out_lab = [""none""] * len(lab_lst)\n\n    if config[""exp""][""production""] == str(""False""):\n        for i in range(len(lab_opts)):\n\n            # Compute number of monophones if needed\n            if ""ali-to-phones"" in lab_opts[i]:\n\n                log_file = config[""exp""][""out_folder""] + ""/log.log""\n                folder_lab_count = lab_folders[i]\n                cmd = ""hmm-info "" + folder_lab_count + ""/final.mdl | awk \'/phones/{print $4}\'""\n                output = run_shell(cmd, log_file)\n                if output.decode().rstrip() == """":\n                    sys.stderr.write(\n                        ""ERROR: hmm-info command doesn\'t exist. Make sure your .bashrc contains the Kaldi paths and correctly exports it.\\n""\n                    )\n                    sys.exit(0)\n\n                N_out = int(output.decode().rstrip())\n                N_out_lab[i] = N_out\n\n        for i in range(len(forward_out_lst)):\n\n            if forward_out_lst[i] not in possible_outs:\n                sys.stderr.write(\n                    \'ERROR: the output ""%s"" in the section ""forward_out"" is not defined in section model)\\n\'\n                    % (forward_out_lst[i])\n                )\n                sys.exit(0)\n\n            if strtobool(forward_norm_bool_lst[i]):\n\n                if forward_norm_lst[i] not in lab_lst:\n                    if not os.path.exists(forward_norm_lst[i]):\n                        sys.stderr.write(\n                            \'ERROR: the count_file ""%s"" in the section ""forward_out"" does not exist)\\n\'\n                            % (forward_norm_lst[i])\n                        )\n                        sys.exit(0)\n                    else:\n                        # Check if the specified file is in the right format\n                        f = open(forward_norm_lst[i], ""r"")\n                        cnts = f.read()\n                        if not (bool(re.match(""(.*)\\[(.*)\\]"", cnts))):\n                            sys.stderr.write(\n                                \'ERROR: the count_file ""%s"" in the section ""forward_out"" not in the right format)\\n\'\n                                % (forward_norm_lst[i])\n                            )\n\n                else:\n                    # Try to automatically retrieve the count file from the config file\n\n                    # Compute the number of context-dependent phone states\n                    if ""ali-to-pdf"" in lab_opts[lab_lst.index(forward_norm_lst[i])]:\n                        log_file = config[""exp""][""out_folder""] + ""/log.log""\n                        folder_lab_count = lab_folders[lab_lst.index(forward_norm_lst[i])]\n                        cmd = ""hmm-info "" + folder_lab_count + ""/final.mdl | awk \'/pdfs/{print $4}\'""\n                        output = run_shell(cmd, log_file)\n                        if output.decode().rstrip() == """":\n                            sys.stderr.write(\n                                ""ERROR: hmm-info command doesn\'t exist. Make sure your .bashrc contains the Kaldi paths and correctly exports it.\\n""\n                            )\n                            sys.exit(0)\n                        N_out = int(output.decode().rstrip())\n                        N_out_lab[lab_lst.index(forward_norm_lst[i])] = N_out\n                        count_file_path = (\n                            out_folder\n                            + ""/exp_files/forward_""\n                            + forward_out_lst[i]\n                            + ""_""\n                            + forward_norm_lst[i]\n                            + "".count""\n                        )\n                        cmd = (\n                            ""analyze-counts --print-args=False --verbose=0 --binary=false --counts-dim=""\n                            + str(N_out)\n                            + \' ""ark:ali-to-pdf \'\n                            + folder_lab_count\n                            + \'/final.mdl \\\\""ark:gunzip -c \'\n                            + folder_lab_count\n                            + \'/ali.*.gz |\\\\"" ark:- |"" \'\n                            + count_file_path\n                        )\n                        run_shell(cmd, log_file)\n                        forward_norm_lst[i] = count_file_path\n\n                    else:\n                        sys.stderr.write(\n                            \'ERROR: Not able to automatically retrieve count file for the label ""%s"". Please add a valid count file path in ""normalize_with_counts_from"" or set normalize_posteriors=False \\n\'\n                            % (forward_norm_lst[i])\n                        )\n                        sys.exit(0)\n\n    # Update the config file with the count_file paths\n    config[""forward""][""normalize_with_counts_from""] = "","".join(forward_norm_lst)\n\n    # When possible replace the pattern ""N_out_lab*"" with the detected number of output\n    for sec in config.sections():\n        for field in list(config[sec]):\n            for i in range(len(lab_lst)):\n                pattern = ""N_out_"" + lab_lst[i]\n\n                if pattern in config[sec][field]:\n                    if N_out_lab[i] != ""none"":\n                        config[sec][field] = config[sec][field].replace(pattern, str(N_out_lab[i]))\n\n                    else:\n                        sys.stderr.write(\n                            ""ERROR: Cannot automatically retrieve the number of output in %s. Please, add manually the number of outputs \\n""\n                            % (pattern)\n                        )\n                        sys.exit(0)\n\n    # Check the model field\n    parse_model_field(cfg_file)\n\n    # Create block diagram picture of the model\n    create_block_diagram(cfg_file)\n\n    if sec_parse == False:\n        sys.exit(0)\n\n    return [config, name_data, name_arch]\n\n\ndef cfg_item2sec(config, field, value):\n\n    for sec in config.sections():\n        if field in list(dict(config.items(sec)).keys()):\n            if value in list(dict(config.items(sec)).values()):\n                return sec\n\n    sys.stderr.write(""ERROR: %s=%s not found in config file \\n"" % (field, value))\n    sys.exit(0)\n    return -1\n\n\ndef split_chunks(seq, size):\n    newseq = []\n    splitsize = 1.0 / size * len(seq)\n    for i in range(size):\n        newseq.append(seq[int(round(i * splitsize)) : int(round((i + 1) * splitsize))])\n    return newseq\n\n\ndef get_chunks_after_which_to_validate(N_ck_tr, nr_of_valid_per_epoch):\n    def _partition_chunks(N_ck_tr, nr_of_valid_per_epoch):\n        chunk_part = list()\n        chunk_size = int(np.ceil(N_ck_tr / float(nr_of_valid_per_epoch)))\n        for i1 in range(nr_of_valid_per_epoch):\n            chunk_part.append(range(0, N_ck_tr)[i1 * chunk_size : (i1 + 1) * chunk_size])\n        return chunk_part\n\n    part_chunk_ids = _partition_chunks(N_ck_tr, nr_of_valid_per_epoch)\n    chunk_ids = list()\n    for l in part_chunk_ids:\n        chunk_ids.append(l[-1])\n    return chunk_ids\n\n\ndef do_validation_after_chunk(ck, N_ck_tr, config):\n    def _get_nr_of_valid_per_epoch_from_config(config):\n        if not ""nr_of_valid_per_epoch"" in config[""exp""]:\n            return 1\n        return int(config[""exp""][""nr_of_valid_per_epoch""])\n\n    nr_of_valid_per_epoch = _get_nr_of_valid_per_epoch_from_config(config)\n    valid_chunks = get_chunks_after_which_to_validate(N_ck_tr, nr_of_valid_per_epoch)\n    if ck in valid_chunks:\n        return True\n    else:\n        return False\n\n\ndef _get_val_file_name_base(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n    file_name = ""valid_"" + dataset + ""_ep"" + format(ep, N_ep_str_format) + ""_trCk"" + format(ck, N_ck_str_format)\n    if ck_val is None:\n        file_name += ""*""\n    else:\n        file_name += ""_ck"" + format(ck_val, N_ck_str_format_val)\n\n    return file_name\n\n\ndef get_val_lst_file_path(\n    out_folder, valid_data, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n):\n    def _get_val_lst_file_name(\n        dataset, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n    ):\n        file_name = _get_val_file_name_base(\n            dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n        )\n        file_name += ""_""\n        if not fea_name is None:\n            file_name += fea_name\n        else:\n            file_name += ""*""\n        file_name += "".lst""\n        return file_name\n\n    lst_file_name = _get_val_lst_file_name(\n        valid_data, ep, ck, ck_val, fea_name, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n    )\n    lst_file = out_folder + ""/exp_files/"" + lst_file_name\n    return lst_file\n\n\ndef get_val_info_file_path(\n    out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n):\n    def _get_val_info_file_name(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n        file_name = _get_val_file_name_base(\n            dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n        )\n        file_name += "".info""\n        return file_name\n\n    info_file_name = _get_val_info_file_name(\n        valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n    )\n    info_file = out_folder + ""/exp_files/"" + info_file_name\n    return info_file\n\n\ndef get_val_cfg_file_path(\n    out_folder, valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n):\n    def _get_val_cfg_file_name(dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val):\n        file_name = _get_val_file_name_base(\n            dataset, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n        )\n        file_name += "".cfg""\n        return file_name\n\n    cfg_file_name = _get_val_cfg_file_name(\n        valid_data, ep, ck, ck_val, N_ep_str_format, N_ck_str_format, N_ck_str_format_val\n    )\n    config_chunk_file = out_folder + ""/exp_files/"" + cfg_file_name\n    return config_chunk_file\n\n\ndef create_configs(config):\n\n    # This function create the chunk-specific config files\n    cfg_file_proto_chunk = config[""cfg_proto""][""cfg_proto_chunk""]\n    N_ep = int(config[""exp""][""N_epochs_tr""])\n    N_ep_str_format = ""0"" + str(max(math.ceil(np.log10(N_ep)), 1)) + ""d""\n    tr_data_lst = config[""data_use""][""train_with""].split("","")\n    valid_data_lst = config[""data_use""][""valid_with""].split("","")\n    max_seq_length_train = config[""batches""][""max_seq_length_train""]\n    forward_data_lst = config[""data_use""][""forward_with""].split("","")\n    is_production = strtobool(config[""exp""][""production""])\n\n    out_folder = config[""exp""][""out_folder""]\n    cfg_file = out_folder + ""/conf.cfg""\n    chunk_lst = out_folder + ""/exp_files/list_chunks.txt""\n\n    lst_chunk_file = open(chunk_lst, ""w"")\n\n    # Read the batch size string\n    batch_size_tr_str = config[""batches""][""batch_size_train""]\n    batch_size_tr_arr = expand_str_ep(batch_size_tr_str, ""int"", N_ep, ""|"", ""*"")\n\n    # Read the max_seq_length_train\n    if len(max_seq_length_train.split("","")) == 1:\n        max_seq_length_tr_arr = expand_str_ep(max_seq_length_train, ""int"", N_ep, ""|"", ""*"")\n    else:\n        max_seq_length_tr_arr = [max_seq_length_train] * N_ep\n\n    cfg_file_proto = config[""cfg_proto""][""cfg_proto""]\n\n    [config, name_data, name_arch] = check_cfg(cfg_file, config, cfg_file_proto)\n\n    arch_lst = get_all_archs(config)\n    lr = {}\n    improvement_threshold = {}\n    halving_factor = {}\n    pt_files = {}\n    drop_rates = {}\n    for arch in arch_lst:\n        lr_arr = expand_str_ep(config[arch][""arch_lr""], ""float"", N_ep, ""|"", ""*"")\n        lr[arch] = lr_arr\n\n        improvement_threshold[arch] = float(config[arch][""arch_improvement_threshold""])\n        halving_factor[arch] = float(config[arch][""arch_halving_factor""])\n        pt_files[arch] = config[arch][""arch_pretrain_file""]\n\n        # Loop over all the sections and look for a ""_drop"" field (to perform dropout scheduling\n        for (field_key, field_val) in config.items(arch):\n            if ""_drop"" in field_key:\n                drop_lay = field_val.split("","")\n                N_lay = len(drop_lay)\n                drop_rates[arch] = []\n                for lay_id in range(N_lay):\n                    drop_rates[arch].append(expand_str_ep(drop_lay[lay_id], ""float"", N_ep, ""|"", ""*""))\n\n                # Check dropout factors\n                for dropout_factor in drop_rates[arch][0]:\n                    if float(dropout_factor) < 0.0 or float(dropout_factor) > 1.0:\n                        sys.stderr.write(\n                            ""The dropout rate should be between 0 and 1. Got %s in %s.\\n"" % (dropout_factor, field_key)\n                        )\n                        sys.exit(0)\n\n    # Production case, we don\'t want to train, only forward without labels\n    if is_production:\n        ep = N_ep - 1\n        N_ep = 0\n        model_files = {}\n        max_seq_length_train_curr = max_seq_length_train\n\n        for arch in pt_files.keys():\n            model_files[arch] = out_folder + ""/exp_files/final_"" + arch + "".pkl""\n\n    if strtobool(config[""batches""][""increase_seq_length_train""]):\n        max_seq_length_train_curr = config[""batches""][""start_seq_len_train""]\n        if len(max_seq_length_train.split("","")) == 1:\n            max_seq_length_train_curr = int(max_seq_length_train_curr)\n        else:\n            # TODO: add support for increasing seq length when fea and lab have different time dimensionality\n            pass\n\n    for ep in range(N_ep):\n\n        for tr_data in tr_data_lst:\n\n            # Compute the total number of chunks for each training epoch\n            N_ck_tr = compute_n_chunks(out_folder, tr_data, ep, N_ep_str_format, ""train"")\n            N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_ck_tr)), 1)) + ""d""\n\n            # ***Epoch training***\n            for ck in range(N_ck_tr):\n\n                # path of the list of features for this chunk\n                lst_file = (\n                    out_folder\n                    + ""/exp_files/train_""\n                    + tr_data\n                    + ""_ep""\n                    + format(ep, N_ep_str_format)\n                    + ""_ck""\n                    + format(ck, N_ck_str_format)\n                    + ""_*.lst""\n                )\n\n                # paths of the output files (info,model,chunk_specific cfg file)\n                info_file = (\n                    out_folder\n                    + ""/exp_files/train_""\n                    + tr_data\n                    + ""_ep""\n                    + format(ep, N_ep_str_format)\n                    + ""_ck""\n                    + format(ck, N_ck_str_format)\n                    + "".info""\n                )\n\n                if ep + ck == 0:\n                    model_files_past = {}\n                else:\n                    model_files_past = model_files\n\n                model_files = {}\n                for arch in pt_files.keys():\n                    model_files[arch] = info_file.replace("".info"", ""_"" + arch + "".pkl"")\n\n                config_chunk_file = (\n                    out_folder\n                    + ""/exp_files/train_""\n                    + tr_data\n                    + ""_ep""\n                    + format(ep, N_ep_str_format)\n                    + ""_ck""\n                    + format(ck, N_ck_str_format)\n                    + "".cfg""\n                )\n                lst_chunk_file.write(config_chunk_file + ""\\n"")\n\n                if strtobool(config[""batches""][""increase_seq_length_train""]) == False:\n                    if len(max_seq_length_train.split("","")) == 1:\n                        max_seq_length_train_curr = int(max_seq_length_tr_arr[ep])\n                    else:\n                        max_seq_length_train_curr = max_seq_length_tr_arr[ep]\n\n                # Write chunk-specific cfg file\n                write_cfg_chunk(\n                    cfg_file,\n                    config_chunk_file,\n                    cfg_file_proto_chunk,\n                    pt_files,\n                    lst_file,\n                    info_file,\n                    ""train"",\n                    tr_data,\n                    lr,\n                    max_seq_length_train_curr,\n                    name_data,\n                    ep,\n                    ck,\n                    batch_size_tr_arr[ep],\n                    drop_rates,\n                )\n\n                # update pt_file (used to initialized the DNN for the next chunk)\n                for pt_arch in pt_files.keys():\n                    pt_files[pt_arch] = (\n                        out_folder\n                        + ""/exp_files/train_""\n                        + tr_data\n                        + ""_ep""\n                        + format(ep, N_ep_str_format)\n                        + ""_ck""\n                        + format(ck, N_ck_str_format)\n                        + ""_""\n                        + pt_arch\n                        + "".pkl""\n                    )\n                if do_validation_after_chunk(ck, N_ck_tr, config):\n                    for valid_data in valid_data_lst:\n                        N_ck_valid = compute_n_chunks(out_folder, valid_data, ep, N_ep_str_format, ""valid"")\n                        N_ck_str_format_val = ""0"" + str(max(math.ceil(np.log10(N_ck_valid)), 1)) + ""d""\n                        for ck_val in range(N_ck_valid):\n                            lst_file = get_val_lst_file_path(\n                                out_folder,\n                                valid_data,\n                                ep,\n                                ck,\n                                ck_val,\n                                None,\n                                N_ep_str_format,\n                                N_ck_str_format,\n                                N_ck_str_format_val,\n                            )\n                            info_file = get_val_info_file_path(\n                                out_folder,\n                                valid_data,\n                                ep,\n                                ck,\n                                ck_val,\n                                N_ep_str_format,\n                                N_ck_str_format,\n                                N_ck_str_format_val,\n                            )\n                            config_chunk_file = get_val_cfg_file_path(\n                                out_folder,\n                                valid_data,\n                                ep,\n                                ck,\n                                ck_val,\n                                N_ep_str_format,\n                                N_ck_str_format,\n                                N_ck_str_format_val,\n                            )\n                            lst_chunk_file.write(config_chunk_file + ""\\n"")\n                            write_cfg_chunk(\n                                cfg_file,\n                                config_chunk_file,\n                                cfg_file_proto_chunk,\n                                model_files,\n                                lst_file,\n                                info_file,\n                                ""valid"",\n                                valid_data,\n                                lr,\n                                max_seq_length_train_curr,\n                                name_data,\n                                ep,\n                                ck_val,\n                                batch_size_tr_arr[ep],\n                                drop_rates,\n                            )\n                    if strtobool(config[""batches""][""increase_seq_length_train""]):\n                        if len(max_seq_length_train.split("","")) == 1:\n                            max_seq_length_train_curr = max_seq_length_train_curr * int(\n                                config[""batches""][""multply_factor_seq_len_train""]\n                            )\n                            if max_seq_length_train_curr > int(max_seq_length_tr_arr[ep]):\n                                max_seq_length_train_curr = int(max_seq_length_tr_arr[ep])\n                        else:\n                            # TODO: add support for increasing seq length when fea and lab have different time dimensionality\n                            pass\n\n    for forward_data in forward_data_lst:\n\n        # Compute the number of chunks\n        N_ck_forward = compute_n_chunks(out_folder, forward_data, ep, N_ep_str_format, ""forward"")\n        N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_ck_forward)), 1)) + ""d""\n\n        for ck in range(N_ck_forward):\n\n            # path of the list of features for this chunk\n            lst_file = (\n                out_folder\n                + ""/exp_files/forward_""\n                + forward_data\n                + ""_ep""\n                + format(ep, N_ep_str_format)\n                + ""_ck""\n                + format(ck, N_ck_str_format)\n                + ""_*.lst""\n            )\n\n            # output file\n            info_file = (\n                out_folder\n                + ""/exp_files/forward_""\n                + forward_data\n                + ""_ep""\n                + format(ep, N_ep_str_format)\n                + ""_ck""\n                + format(ck, N_ck_str_format)\n                + "".info""\n            )\n            config_chunk_file = (\n                out_folder\n                + ""/exp_files/forward_""\n                + forward_data\n                + ""_ep""\n                + format(ep, N_ep_str_format)\n                + ""_ck""\n                + format(ck, N_ck_str_format)\n                + "".cfg""\n            )\n            lst_chunk_file.write(config_chunk_file + ""\\n"")\n\n            # Write chunk-specific cfg file\n            write_cfg_chunk(\n                cfg_file,\n                config_chunk_file,\n                cfg_file_proto_chunk,\n                model_files,\n                lst_file,\n                info_file,\n                ""forward"",\n                forward_data,\n                lr,\n                max_seq_length_train_curr,\n                name_data,\n                ep,\n                ck,\n                batch_size_tr_arr[ep],\n                drop_rates,\n            )\n\n    lst_chunk_file.close()\n\n\ndef create_lists(config):\n    def _get_validation_data_for_chunks(fea_names, list_fea, N_chunks):\n        full_list = []\n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(""\\n"") + "","" for line in open(list_fea[i])])\n            full_list[i] = sorted(full_list[i])\n        full_list_fea_conc = full_list[0]\n        for i in range(1, len(full_list)):\n            full_list_fea_conc = list(map(str.__add__, full_list_fea_conc, full_list[i]))\n        random.shuffle(full_list_fea_conc)\n        valid_chunks_fea = list(split_chunks(full_list_fea_conc, N_chunks))\n        return valid_chunks_fea\n\n    def _shuffle_forward_data(config):\n        if ""shuffle_forwarding_data"" in config[""forward""]:\n            suffle_on_forwarding = strtobool(config[""forward""][""shuffle_forwarding_data""])\n            if not suffle_on_forwarding:\n                return False\n        return True\n\n    # splitting data into chunks (see out_folder/additional_files)\n    out_folder = config[""exp""][""out_folder""]\n    seed = int(config[""exp""][""seed""])\n    N_ep = int(config[""exp""][""N_epochs_tr""])\n    N_ep_str_format = ""0"" + str(max(math.ceil(np.log10(N_ep)), 1)) + ""d""\n\n    # Setting the random seed\n    random.seed(seed)\n\n    # training chunk lists creation\n    tr_data_name = config[""data_use""][""train_with""].split("","")\n\n    # Reading validation feature lists\n    for dataset in tr_data_name:\n        sec_data = cfg_item2sec(config, ""data_name"", dataset)\n        [fea_names, list_fea, fea_opts, cws_left, cws_right] = parse_fea_field(\n            config[cfg_item2sec(config, ""data_name"", dataset)][""fea""]\n        )\n\n        N_chunks = int(config[sec_data][""N_chunks""])\n        N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_chunks)), 1)) + ""d""\n\n        full_list = []\n\n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(""\\n"") + "","" for line in open(list_fea[i])])\n            full_list[i] = sorted(full_list[i])\n\n        # concatenating all the featues in a single file (useful for shuffling consistently)\n        full_list_fea_conc = full_list[0]\n        for i in range(1, len(full_list)):\n            full_list_fea_conc = list(map(str.__add__, full_list_fea_conc, full_list[i]))\n\n        for ep in range(N_ep):\n            #  randomize the list\n            random.shuffle(full_list_fea_conc)\n            tr_chunks_fea = list(split_chunks(full_list_fea_conc, N_chunks))\n            tr_chunks_fea.reverse()\n\n            for ck in range(N_chunks):\n                for i in range(len(fea_names)):\n\n                    tr_chunks_fea_split = []\n                    for snt in tr_chunks_fea[ck]:\n                        # print(snt.split(\',\')[i])\n                        tr_chunks_fea_split.append(snt.split("","")[i])\n                    output_lst_file = (\n                        out_folder\n                        + ""/exp_files/train_""\n                        + dataset\n                        + ""_ep""\n                        + format(ep, N_ep_str_format)\n                        + ""_ck""\n                        + format(ck, N_ck_str_format)\n                        + ""_""\n                        + fea_names[i]\n                        + "".lst""\n                    )\n                    f = open(output_lst_file, ""w"")\n                    tr_chunks_fea_wr = map(lambda x: x + ""\\n"", tr_chunks_fea_split)\n                    f.writelines(tr_chunks_fea_wr)\n                    f.close()\n                if do_validation_after_chunk(ck, N_chunks, config):\n                    valid_data_name = config[""data_use""][""valid_with""].split("","")\n                    for dataset_val in valid_data_name:\n                        sec_data = cfg_item2sec(config, ""data_name"", dataset_val)\n                        fea_names, list_fea, fea_opts, cws_left, cws_right = parse_fea_field(\n                            config[cfg_item2sec(config, ""data_name"", dataset_val)][""fea""]\n                        )\n                        N_chunks_val = int(config[sec_data][""N_chunks""])\n                        N_ck_str_format_val = ""0"" + str(max(math.ceil(np.log10(N_chunks_val)), 1)) + ""d""\n                        valid_chunks_fea = _get_validation_data_for_chunks(fea_names, list_fea, N_chunks_val)\n                        for ck_val in range(N_chunks_val):\n                            for fea_idx in range(len(fea_names)):\n                                valid_chunks_fea_split = []\n                                for snt in valid_chunks_fea[ck_val]:\n                                    valid_chunks_fea_split.append(snt.split("","")[fea_idx])\n                                output_lst_file = get_val_lst_file_path(\n                                    out_folder,\n                                    dataset_val,\n                                    ep,\n                                    ck,\n                                    ck_val,\n                                    fea_names[fea_idx],\n                                    N_ep_str_format,\n                                    N_ck_str_format,\n                                    N_ck_str_format_val,\n                                )\n                                f = open(output_lst_file, ""w"")\n                                valid_chunks_fea_wr = map(lambda x: x + ""\\n"", valid_chunks_fea_split)\n                                f.writelines(valid_chunks_fea_wr)\n                                f.close()\n\n    # forward chunk lists creation\n    forward_data_name = config[""data_use""][""forward_with""].split("","")\n\n    # Reading validation feature lists\n    for dataset in forward_data_name:\n        sec_data = cfg_item2sec(config, ""data_name"", dataset)\n        [fea_names, list_fea, fea_opts, cws_left, cws_right] = parse_fea_field(\n            config[cfg_item2sec(config, ""data_name"", dataset)][""fea""]\n        )\n\n        N_chunks = int(config[sec_data][""N_chunks""])\n        N_ck_str_format = ""0"" + str(max(math.ceil(np.log10(N_chunks)), 1)) + ""d""\n\n        full_list = []\n\n        for i in range(len(fea_names)):\n            full_list.append([line.rstrip(""\\n"") + "","" for line in open(list_fea[i])])\n            full_list[i] = sorted(full_list[i])\n\n        # concatenating all the featues in a single file (useful for shuffling consistently)\n        full_list_fea_conc = full_list[0]\n        for i in range(1, len(full_list)):\n            full_list_fea_conc = list(map(str.__add__, full_list_fea_conc, full_list[i]))\n\n        # randomize the list\n        if _shuffle_forward_data(config):\n            random.shuffle(full_list_fea_conc)\n        forward_chunks_fea = list(split_chunks(full_list_fea_conc, N_chunks))\n\n        for ck in range(N_chunks):\n            for i in range(len(fea_names)):\n\n                forward_chunks_fea_split = []\n                for snt in forward_chunks_fea[ck]:\n                    # print(snt.split(\',\')[i])\n                    forward_chunks_fea_split.append(snt.split("","")[i])\n\n                output_lst_file = (\n                    out_folder\n                    + ""/exp_files/forward_""\n                    + dataset\n                    + ""_ep""\n                    + format(ep, N_ep_str_format)\n                    + ""_ck""\n                    + format(ck, N_ck_str_format)\n                    + ""_""\n                    + fea_names[i]\n                    + "".lst""\n                )\n                f = open(output_lst_file, ""w"")\n                forward_chunks_fea_wr = map(lambda x: x + ""\\n"", forward_chunks_fea_split)\n                f.writelines(forward_chunks_fea_wr)\n                f.close()\n\n\ndef write_cfg_chunk(\n    cfg_file,\n    config_chunk_file,\n    cfg_file_proto_chunk,\n    pt_files,\n    lst_file,\n    info_file,\n    to_do,\n    data_set_name,\n    lr,\n    max_seq_length_train_curr,\n    name_data,\n    ep,\n    ck,\n    batch_size,\n    drop_rates,\n):\n\n    # writing the chunk-specific cfg file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n\n    config_chunk = configparser.ConfigParser()\n    config_chunk.read(cfg_file)\n\n    # Exp section\n    config_chunk[""exp""][""to_do""] = to_do\n    config_chunk[""exp""][""out_info""] = info_file\n\n    # change seed for randomness\n    config_chunk[""exp""][""seed""] = str(int(config_chunk[""exp""][""seed""]) + ep + ck)\n\n    config_chunk[""batches""][""batch_size_train""] = batch_size\n\n    for arch in pt_files.keys():\n        config_chunk[arch][""arch_pretrain_file""] = pt_files[arch]\n\n    # writing the current learning rate\n    for lr_arch in lr.keys():\n        config_chunk[lr_arch][""arch_lr""] = str(lr[lr_arch][ep])\n\n        for (field_key, field_val) in config.items(lr_arch):\n            if ""_drop"" in field_key:\n                N_lay = len(drop_rates[lr_arch])\n                drop_arr = []\n                for lay in range(N_lay):\n                    drop_arr.append(drop_rates[lr_arch][lay][ep])\n\n                config_chunk[lr_arch][field_key] = str("","".join(drop_arr))\n\n    # Data_chunk section\n    config_chunk.add_section(""data_chunk"")\n\n    config_chunk[""data_chunk""] = config[cfg_item2sec(config, ""data_name"", data_set_name)]\n\n    lst_files = sorted(glob.glob(lst_file))\n\n    current_fea = config_chunk[""data_chunk""][""fea""]\n\n    list_current_fea = re.findall(""fea_name=(.*)\\nfea_lst=(.*)\\n"", current_fea)\n\n    for (fea, path) in list_current_fea:\n        for path_cand in lst_files:\n            fea_type_cand = re.findall(""_(.*).lst"", path_cand)[0].split(""_"")[-1]\n            if fea_type_cand == fea:\n                config_chunk[""data_chunk""][""fea""] = config_chunk[""data_chunk""][""fea""].replace(path, path_cand)\n\n    config_chunk.remove_option(""data_chunk"", ""data_name"")\n    config_chunk.remove_option(""data_chunk"", ""N_chunks"")\n\n    config_chunk.remove_section(""decoding"")\n    config_chunk.remove_section(""data_use"")\n\n    data_to_del = []\n    for sec in config.sections():\n        if ""dataset"" in sec:\n            data_to_del.append(config[sec][""data_name""])\n\n    for dataset in data_to_del:\n        config_chunk.remove_section(cfg_item2sec(config_chunk, ""data_name"", dataset))\n\n    # Create batche section\n    config_chunk.remove_option(""batches"", ""increase_seq_length_train"")\n    config_chunk.remove_option(""batches"", ""start_seq_len_train"")\n    config_chunk.remove_option(""batches"", ""multply_factor_seq_len_train"")\n\n    config_chunk[""batches""][""max_seq_length_train""] = str(max_seq_length_train_curr)\n\n    # Write cfg_file_chunk\n    with open(config_chunk_file, ""w"") as configfile:\n        config_chunk.write(configfile)\n\n    # Check cfg_file_chunk\n    [config_proto_chunk, name_data_ck, name_arch_ck] = check_consistency_with_proto(\n        config_chunk_file, cfg_file_proto_chunk\n    )\n\n\ndef parse_fea_field(fea):\n\n    # Adding the required fields into a list\n    fea_names = []\n    fea_lsts = []\n    fea_opts = []\n    cws_left = []\n    cws_right = []\n\n    for line in fea.split(""\\n""):\n\n        line = re.sub("" +"", "" "", line)\n\n        if ""fea_name="" in line:\n            fea_names.append(line.split(""="")[1])\n\n        if ""fea_lst="" in line:\n            fea_lsts.append(line.split(""="")[1])\n\n        if ""fea_opts="" in line:\n            fea_opts.append(line.split(""fea_opts="")[1])\n\n        if ""cw_left="" in line:\n            cws_left.append(line.split(""="")[1])\n            if not (check_field(line.split(""="")[1], ""int(0,inf)"", ""cw_left"")):\n                sys.exit(0)\n\n        if ""cw_right="" in line:\n            cws_right.append(line.split(""="")[1])\n            if not (check_field(line.split(""="")[1], ""int(0,inf)"", ""cw_right"")):\n                sys.exit(0)\n\n    # Check features names\n    if not (sorted(fea_names) == sorted(list(set(fea_names)))):\n        sys.stderr.write(""ERROR fea_names must be different! (got %s)"" % (fea_names))\n        sys.exit(0)\n\n    snt_lst = []\n    cnt = 0\n\n    # Check consistency of feature lists\n    for fea_lst in fea_lsts:\n        if not (os.path.isfile(fea_lst)):\n            sys.stderr.write(\n                \'ERROR: The path ""%s"" specified in the field  ""fea_lst"" of the config file does not exists! \\n\'\n                % (fea_lst)\n            )\n            sys.exit(0)\n        else:\n            snts = sorted([line.rstrip(""\\n"").split("" "")[0] for line in open(fea_lst)])\n            snt_lst.append(snts)\n            # Check if all the sentences are present in all the list files\n            if cnt > 0:\n                if snt_lst[cnt - 1] != snt_lst[cnt]:\n                    sys.stderr.write(\n                        ""ERROR: the files %s in fea_lst contain a different set of sentences! \\n"" % (fea_lst)\n                    )\n                    sys.exit(0)\n            cnt = cnt + 1\n    return [fea_names, fea_lsts, fea_opts, cws_left, cws_right]\n\n\ndef parse_lab_field(lab):\n\n    # Adding the required fields into a list\n    lab_names = []\n    lab_folders = []\n    lab_opts = []\n\n    for line in lab.split(""\\n""):\n\n        line = re.sub("" +"", "" "", line)\n\n        if ""lab_name="" in line:\n            lab_names.append(line.split(""="")[1])\n\n        if ""lab_folder="" in line:\n            lab_folders.append(line.split(""="")[1])\n\n        if ""lab_opts="" in line:\n            lab_opts.append(line.split(""lab_opts="")[1])\n\n    # Check features names\n    if not (sorted(lab_names) == sorted(list(set(lab_names)))):\n        sys.stderr.write(""ERROR lab_names must be different! (got %s)"" % (lab_names))\n        sys.exit(0)\n\n    # Check consistency of feature lists\n    for lab_fold in lab_folders:\n        if not (os.path.isdir(lab_fold)):\n            sys.stderr.write(\n                \'ERROR: The path ""%s"" specified in the field  ""lab_folder"" of the config file does not exists! \\n\'\n                % (lab_fold)\n            )\n            sys.exit(0)\n\n    return [lab_names, lab_folders, lab_opts]\n\n\ndef compute_n_chunks(out_folder, data_list, ep, N_ep_str_format, step):\n    list_ck = sorted(\n        glob.glob(out_folder + ""/exp_files/"" + step + ""_"" + data_list + ""_ep"" + format(ep, N_ep_str_format) + ""*.lst"")\n    )\n    last_ck = list_ck[-1]\n    N_ck = int(re.findall(""_ck(.+)_"", last_ck)[-1].split(""_"")[0]) + 1\n    return N_ck\n\n\ndef parse_model_field(cfg_file):\n\n    # Reading the config file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n\n    # reading the proto file\n    model_proto_file = config[""model""][""model_proto""]\n    f = open(model_proto_file, ""r"")\n    proto_model = f.read()\n\n    # readiing the model string\n    model = config[""model""][""model""]\n\n    # Reading fea,lab arch architectures from the cfg file\n    fea_lst = list(re.findall(""fea_name=(.*)\\n"", config[""dataset1""][""fea""].replace("" "", """")))\n    lab_lst = list(re.findall(""lab_name=(.*)\\n"", config[""dataset1""][""lab""].replace("" "", """")))\n    arch_lst = list(re.findall(""arch_name=(.*)\\n"", open(cfg_file, ""r"").read().replace("" "", """")))\n    possible_operations = re.findall(""(.*)\\((.*),(.*)\\)\\n"", proto_model)\n\n    possible_inputs = fea_lst\n    model_arch = list(filter(None, model.replace("" "", """").split(""\\n"")))\n\n    # Reading the model field line by line\n    for line in model_arch:\n\n        pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n        if not re.match(pattern, line):\n            sys.stderr.write(\n                ""ERROR: all the entries must be of the following type: output=operation(str,str), got (%s)\\n"" % (line)\n            )\n            sys.exit(0)\n        else:\n\n            # Analyze line and chech if it is compliant with proto_model\n            [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n            inps = [inp1, inp2]\n\n            found = False\n            for i in range(len(possible_operations)):\n                if operation == possible_operations[i][0]:\n                    found = True\n\n                    for k in range(1, 3):\n                        if possible_operations[i][k] == ""architecture"":\n                            if inps[k - 1] not in arch_lst:\n                                sys.stderr.write(\n                                    \'ERROR: the architecture ""%s"" is not in the architecture lists of the config file (possible architectures are %s)\\n\'\n                                    % (inps[k - 1], arch_lst)\n                                )\n                                sys.exit(0)\n\n                        if possible_operations[i][k] == ""label"":\n                            if inps[k - 1] not in lab_lst:\n                                sys.stderr.write(\n                                    \'ERROR: the label ""%s"" is not in the label lists of the config file (possible labels are %s)\\n\'\n                                    % (inps[k - 1], lab_lst)\n                                )\n                                sys.exit(0)\n\n                        if possible_operations[i][k] == ""input"":\n                            if inps[k - 1] not in possible_inputs:\n                                sys.stderr.write(\n                                    \'ERROR: the input ""%s"" is not defined before (possible inputs are %s)\\n\'\n                                    % (inps[k - 1], possible_inputs)\n                                )\n                                sys.exit(0)\n\n                        if possible_operations[i][k] == ""float"":\n\n                            try:\n                                float(inps[k - 1])\n                            except ValueError:\n                                sys.stderr.write(\n                                    \'ERROR: the input ""%s"" must be a float, got %s\\n\' % (inps[k - 1], line)\n                                )\n                                sys.exit(0)\n\n                    # Update the list of possible inpus\n                    possible_inputs.append(out_name)\n                    break\n\n            if found == False:\n                sys.stderr.write(\n                    (\'ERROR: operation ""%s"" does not exists (not defined into the model proto file)\\n\' % (operation))\n                )\n                sys.exit(0)\n\n    # Check for the mandatory fiels\n    if ""loss_final"" not in """".join(model_arch):\n        sys.stderr.write(""ERROR: the variable loss_final should be defined in model\\n"")\n        sys.exit(0)\n\n    if ""err_final"" not in """".join(model_arch):\n        sys.stderr.write(""ERROR: the variable err_final should be defined in model\\n"")\n        sys.exit(0)\n\n\ndef terminal_node_detection(model_arch, node):\n\n    terminal = True\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    for line in model_arch:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n        if inp1 == node or inp2 == node:\n            terminal = False\n\n    return terminal\n\n\ndef create_block_connection(lst_inp, model_arch, diag_lines, cnt_names, arch_dict):\n\n    if lst_inp == []:\n        return [[], [], diag_lines]\n\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    arch_current = []\n    output_conn = []\n    current_inp = []\n\n    for input_element in lst_inp:\n\n        for l in range(len(model_arch)):\n            [out_name, operation, inp1, inp2] = list(re.findall(pattern, model_arch[l])[0])\n\n            if inp1 == input_element or inp2 == input_element:\n                if operation == ""compute"":\n                    arch_current.append(inp1)\n                    output_conn.append(out_name)\n                    current_inp.append(inp2)\n                    model_arch[l] = ""processed"" + ""="" + operation + ""("" + inp1 + "",processed)""\n                else:\n                    arch_current.append(out_name)\n                    output_conn.append(out_name)\n                    if inp1 == input_element:\n                        current_inp.append(inp1)\n                        model_arch[l] = out_name + ""="" + operation + ""(processed,"" + inp2 + "")""\n\n                    if inp2 == input_element:\n                        current_inp.append(inp2)\n                        model_arch[l] = out_name + ""="" + operation + ""("" + inp1 + "",processed)""\n\n    for i in range(len(arch_current)):\n        # Create connections\n        diag_lines = (\n            diag_lines\n            + str(cnt_names.index(arch_dict[current_inp[i]]))\n            + "" -> ""\n            + str(cnt_names.index(arch_current[i]))\n            + \' [label = ""\'\n            + current_inp[i]\n            + \'""]\\n\'\n        )\n\n    #  remove terminal nodes from output list\n    output_conn_pruned = []\n    for node in output_conn:\n        if not (terminal_node_detection(model_arch, node)):\n            output_conn_pruned.append(node)\n\n    [arch_current, output_conn, diag_lines] = create_block_connection(\n        output_conn, model_arch, diag_lines, cnt_names, arch_dict\n    )\n\n    return [arch_current, output_conn_pruned, diag_lines]\n\n\ndef create_block_diagram(cfg_file):\n\n    # Reading the config file\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n\n    # readiing the model string\n    model = config[""model""][""model""]\n\n    # Reading fea,lab arch architectures from the cfg file\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    fea_lst = list(re.findall(""fea_name=(.*)\\n"", config[""dataset1""][""fea""].replace("" "", """")))\n    lab_lst = list(re.findall(""lab_name=(.*)\\n"", config[""dataset1""][""lab""].replace("" "", """")))\n    arch_lst = list(re.findall(""arch_name=(.*)\\n"", open(cfg_file, ""r"").read().replace("" "", """")))\n\n    out_diag_file = config[""exp""][""out_folder""] + ""/model.diag""\n\n    model_arch = list(filter(None, model.replace("" "", """").split(""\\n"")))\n\n    diag_lines = ""blockdiag {\\n""\n\n    cnt = 0\n    cnt_names = []\n    arch_lst = []\n    fea_lst_used = []\n    lab_lst_used = []\n\n    for line in model_arch:\n        if ""err_final="" in line:\n            model_arch.remove(line)\n\n    # Initializations of the blocks\n    for line in model_arch:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n\n        if operation != ""compute"":\n\n            # node architecture\n            diag_lines = diag_lines + str(cnt) + \' [label=""\' + operation + \'"",shape = roundedbox];\\n\'\n            arch_lst.append(out_name)\n            cnt_names.append(out_name)\n            cnt = cnt + 1\n\n            # labels\n            if inp2 in lab_lst:\n                diag_lines = diag_lines + str(cnt) + \' [label=""\' + inp2 + \'"",shape = roundedbox];\\n\'\n                if inp2 not in lab_lst_used:\n                    lab_lst_used.append(inp2)\n                    cnt_names.append(inp2)\n                    cnt = cnt + 1\n\n            # features\n            if inp1 in fea_lst:\n                diag_lines = diag_lines + str(cnt) + \' [label=""\' + inp1 + \'"",shape = circle];\\n\'\n                if inp1 not in fea_lst_used:\n                    fea_lst_used.append(inp1)\n                    cnt_names.append(inp1)\n                    cnt = cnt + 1\n\n            if inp2 in fea_lst:\n                diag_lines = diag_lines + str(cnt) + \' [label=""\' + inp2 + \'"",shape = circle];\\n\'\n                if inp2 not in fea_lst_used:\n                    fea_lst_used.append(inp2)\n                    cnt_names.append(inp2)\n                    cnt = cnt + 1\n\n        else:\n            # architecture\n            diag_lines = diag_lines + str(cnt) + \' [label=""\' + inp1 + \'"",shape = box];\\n\'\n            arch_lst.append(inp1)\n            cnt_names.append(inp1)\n            cnt = cnt + 1\n\n            # feature\n            if inp2 in fea_lst:\n                diag_lines = diag_lines + str(cnt) + \' [label=""\' + inp2 + \'"",shape = circle];\\n\'\n                if inp2 not in fea_lst_used:\n                    fea_lst_used.append(inp2)\n                    cnt_names.append(inp2)\n                    cnt = cnt + 1\n\n    # Connections across blocks\n    lst_conc = fea_lst_used + lab_lst_used\n\n    arch_dict = {}\n\n    for elem in lst_conc:\n        arch_dict[elem] = elem\n\n    for model_line in model_arch:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, model_line)[0])\n        if operation == ""compute"":\n            arch_dict[out_name] = inp1\n        else:\n            arch_dict[out_name] = out_name\n\n    output_conn = lst_conc\n    [arch_current, output_conn, diag_lines] = create_block_connection(\n        output_conn, model_arch, diag_lines, cnt_names, arch_dict\n    )\n\n    diag_lines = diag_lines + ""}""\n\n    # Write the diag file describing the model\n    with open(out_diag_file, ""w"") as text_file:\n        text_file.write(""%s"" % diag_lines)\n\n    # Create image from the diag file\n    log_file = config[""exp""][""out_folder""] + ""/log.log""\n    cmd = ""blockdiag -Tsvg "" + out_diag_file + "" -o "" + config[""exp""][""out_folder""] + ""/model.svg""\n    run_shell(cmd, log_file)\n\n\ndef list_fea_lab_arch(config):  # cancel\n    model = config[""model""][""model""].split(""\\n"")\n    fea_lst = list(re.findall(""fea_name=(.*)\\n"", config[""data_chunk""][""fea""].replace("" "", """")))\n    lab_lst = list(re.findall(""lab_name=(.*)\\n"", config[""data_chunk""][""lab""].replace("" "", """")))\n\n    fea_lst_used = []\n    lab_lst_used = []\n    arch_lst_used = []\n\n    fea_dict_used = {}\n    lab_dict_used = {}\n    arch_dict_used = {}\n\n    fea_lst_used_name = []\n    lab_lst_used_name = []\n    arch_lst_used_name = []\n\n    fea_field = config[""data_chunk""][""fea""]\n    lab_field = config[""data_chunk""][""lab""]\n\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    for line in model:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n\n        if inp1 in fea_lst and inp1 not in fea_lst_used_name:\n            pattern_fea = ""fea_name="" + inp1 + ""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            fea_lst_used.append((inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("",""))\n            fea_dict_used[inp1] = (inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("","")\n\n            fea_lst_used_name.append(inp1)\n        if inp2 in fea_lst and inp2 not in fea_lst_used_name:\n            pattern_fea = ""fea_name="" + inp2 + ""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            fea_lst_used.append((inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("",""))\n            fea_dict_used[inp2] = (inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("","")\n\n            fea_lst_used_name.append(inp2)\n        if inp1 in lab_lst and inp1 not in lab_lst_used_name:\n            pattern_lab = ""lab_name="" + inp1 + ""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            lab_lst_used.append((inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("",""))\n            lab_dict_used[inp1] = (inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("","")\n\n            lab_lst_used_name.append(inp1)\n\n        if inp2 in lab_lst and inp2 not in lab_lst_used_name:\n            pattern_lab = ""lab_name="" + inp2 + ""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            lab_lst_used.append((inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("",""))\n            lab_dict_used[inp2] = (inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("","")\n\n            lab_lst_used_name.append(inp2)\n\n        if operation == ""compute"" and inp1 not in arch_lst_used_name:\n            arch_id = cfg_item2sec(config, ""arch_name"", inp1)\n            arch_seq_model = strtobool(config[arch_id][""arch_seq_model""])\n            arch_lst_used.append([arch_id, inp1, arch_seq_model])\n            arch_dict_used[inp1] = [arch_id, inp1, arch_seq_model]\n\n            arch_lst_used_name.append(inp1)\n\n    # convert to unicode (for python 2)\n    for i in range(len(fea_lst_used)):\n        fea_lst_used[i] = list(map(str, fea_lst_used[i]))\n\n    for i in range(len(lab_lst_used)):\n        lab_lst_used[i] = list(map(str, lab_lst_used[i]))\n\n    for i in range(len(arch_lst_used)):\n        arch_lst_used[i] = list(map(str, arch_lst_used[i]))\n\n    return [fea_lst_used, lab_lst_used, arch_lst_used]\n\n\ndef dict_fea_lab_arch(config, fea_only):\n    model = config[""model""][""model""].split(""\\n"")\n    fea_lst = list(re.findall(""fea_name=(.*)\\n"", config[""data_chunk""][""fea""].replace("" "", """")))\n    lab_lst = list(re.findall(""lab_name=(.*)\\n"", config[""data_chunk""][""lab""].replace("" "", """")))\n\n    fea_lst_used = []\n    lab_lst_used = []\n    arch_lst_used = []\n\n    fea_dict_used = {}\n    lab_dict_used = {}\n    arch_dict_used = {}\n\n    fea_lst_used_name = []\n    lab_lst_used_name = []\n    arch_lst_used_name = []\n\n    fea_field = config[""data_chunk""][""fea""]\n    lab_field = config[""data_chunk""][""lab""]\n\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    for line in model:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n\n        if inp1 in fea_lst and inp1 not in fea_lst_used_name:\n            pattern_fea = ""fea_name="" + inp1 + ""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            if sys.version_info[0] == 2:\n                fea_lst_used.append(\n                    (inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).encode(""utf8"").split("","")\n                )\n                fea_dict_used[inp1] = (\n                    (inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).encode(""utf8"").split("","")\n                )\n            else:\n                fea_lst_used.append((inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("",""))\n                fea_dict_used[inp1] = (inp1 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("","")\n\n            fea_lst_used_name.append(inp1)\n\n        if inp2 in fea_lst and inp2 not in fea_lst_used_name:\n            pattern_fea = ""fea_name="" + inp2 + ""\\nfea_lst=(.*)\\nfea_opts=(.*)\\ncw_left=(.*)\\ncw_right=(.*)""\n            if sys.version_info[0] == 2:\n                fea_lst_used.append(\n                    (inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).encode(""utf8"").split("","")\n                )\n                fea_dict_used[inp2] = (\n                    (inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).encode(""utf8"").split("","")\n                )\n            else:\n                fea_lst_used.append((inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("",""))\n                fea_dict_used[inp2] = (inp2 + "","" + "","".join(list(re.findall(pattern_fea, fea_field)[0]))).split("","")\n\n            fea_lst_used_name.append(inp2)\n        if inp1 in lab_lst and inp1 not in lab_lst_used_name and not fea_only:\n            pattern_lab = ""lab_name="" + inp1 + ""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n\n            if sys.version_info[0] == 2:\n                lab_lst_used.append(\n                    (inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).encode(""utf8"").split("","")\n                )\n                lab_dict_used[inp1] = (\n                    (inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).encode(""utf8"").split("","")\n                )\n            else:\n                lab_lst_used.append((inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("",""))\n                lab_dict_used[inp1] = (inp1 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("","")\n\n            lab_lst_used_name.append(inp1)\n\n        if inp2 in lab_lst and inp2 not in lab_lst_used_name and not fea_only:\n            # Testing production case (no labels)\n            pattern_lab = ""lab_name="" + inp2 + ""\\nlab_folder=(.*)\\nlab_opts=(.*)""\n            if sys.version_info[0] == 2:\n                lab_lst_used.append(\n                    (inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).encode(""utf8"").split("","")\n                )\n                lab_dict_used[inp2] = (\n                    (inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).encode(""utf8"").split("","")\n                )\n            else:\n                lab_lst_used.append((inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("",""))\n                lab_dict_used[inp2] = (inp2 + "","" + "","".join(list(re.findall(pattern_lab, lab_field)[0]))).split("","")\n\n            lab_lst_used_name.append(inp2)\n\n        if operation == ""compute"" and inp1 not in arch_lst_used_name:\n            arch_id = cfg_item2sec(config, ""arch_name"", inp1)\n            arch_seq_model = strtobool(config[arch_id][""arch_seq_model""])\n            arch_lst_used.append([arch_id, inp1, arch_seq_model])\n            arch_dict_used[inp1] = [arch_id, inp1, arch_seq_model]\n\n            arch_lst_used_name.append(inp1)\n\n    # convert to unicode (for python 2)\n    for i in range(len(fea_lst_used)):\n        fea_lst_used[i] = list(map(str, fea_lst_used[i]))\n\n    for i in range(len(lab_lst_used)):\n        lab_lst_used[i] = list(map(str, lab_lst_used[i]))\n\n    for i in range(len(arch_lst_used)):\n        arch_lst_used[i] = list(map(str, arch_lst_used[i]))\n\n    return [fea_dict_used, lab_dict_used, arch_dict_used]\n\n\ndef is_sequential(config, arch_lst):  # To cancel\n    seq_model = False\n\n    for [arch_id, arch_name, arch_seq] in arch_lst:\n        if strtobool(config[arch_id][""arch_seq_model""]):\n            seq_model = True\n            break\n    return seq_model\n\n\ndef is_sequential_dict(config, arch_dict):\n    seq_model = False\n\n    for arch in arch_dict.keys():\n        arch_id = arch_dict[arch][0]\n        if strtobool(config[arch_id][""arch_seq_model""]):\n            seq_model = True\n            break\n    return seq_model\n\n\ndef compute_cw_max(fea_dict):\n    cw_left_arr = []\n    cw_right_arr = []\n\n    for fea in fea_dict.keys():\n        cw_left_arr.append(int(fea_dict[fea][3]))\n        cw_right_arr.append(int(fea_dict[fea][4]))\n\n    cw_left_max = max(cw_left_arr)\n    cw_right_max = max(cw_right_arr)\n\n    return [cw_left_max, cw_right_max]\n\n\ndef model_init(inp_out_dict, model, config, arch_dict, use_cuda, multi_gpu, to_do):\n\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    nns = {}\n    costs = {}\n\n    for line in model:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n\n        if operation == ""compute"":\n\n            # computing input dim\n            inp_dim = inp_out_dict[inp2][-1]\n\n            # import the class\n            module = importlib.import_module(config[arch_dict[inp1][0]][""arch_library""])\n            nn_class = getattr(module, config[arch_dict[inp1][0]][""arch_class""])\n\n            # add use cuda and todo options\n            config.set(arch_dict[inp1][0], ""use_cuda"", config[""exp""][""use_cuda""])\n            config.set(arch_dict[inp1][0], ""to_do"", config[""exp""][""to_do""])\n\n            arch_freeze_flag = strtobool(config[arch_dict[inp1][0]][""arch_freeze""])\n\n            # initialize the neural network\n            net = nn_class(config[arch_dict[inp1][0]], inp_dim)\n\n            if use_cuda:\n                net.cuda()\n\n            if to_do == ""train"":\n                if not (arch_freeze_flag):\n                    net.train()\n                else:\n                    # Switch to eval modality if architecture is frozen (mainly for batch_norm/dropout functions)\n                    net.eval()\n            else:\n                net.eval()\n\n            # addigng nn into the nns dict\n            nns[arch_dict[inp1][1]] = net\n\n            out_dim = net.out_dim\n\n            # updating output dim\n            inp_out_dict[out_name] = [out_dim]\n\n        if operation == ""concatenate"":\n\n            inp_dim1 = inp_out_dict[inp1][-1]\n            inp_dim2 = inp_out_dict[inp2][-1]\n\n            inp_out_dict[out_name] = [inp_dim1 + inp_dim2]\n\n        if operation == ""cost_nll"":\n            costs[out_name] = nn.NLLLoss()\n            inp_out_dict[out_name] = [1]\n\n        if operation == ""cost_err"":\n            inp_out_dict[out_name] = [1]\n\n        if (\n            operation == ""mult""\n            or operation == ""sum""\n            or operation == ""mult_constant""\n            or operation == ""sum_constant""\n            or operation == ""avg""\n            or operation == ""mse""\n        ):\n            inp_out_dict[out_name] = inp_out_dict[inp1]\n\n    return [nns, costs]\n\n\ndef optimizer_init(nns, config, arch_dict):\n\n    # optimizer init\n    optimizers = {}\n    for net in nns.keys():\n\n        lr = float(config[arch_dict[net][0]][""arch_lr""])\n\n        if config[arch_dict[net][0]][""arch_opt""] == ""sgd"":\n\n            opt_momentum = float(config[arch_dict[net][0]][""opt_momentum""])\n            opt_weight_decay = float(config[arch_dict[net][0]][""opt_weight_decay""])\n            opt_dampening = float(config[arch_dict[net][0]][""opt_dampening""])\n            opt_nesterov = strtobool(config[arch_dict[net][0]][""opt_nesterov""])\n\n            optimizers[net] = optim.SGD(\n                nns[net].parameters(),\n                lr=lr,\n                momentum=opt_momentum,\n                weight_decay=opt_weight_decay,\n                dampening=opt_dampening,\n                nesterov=opt_nesterov,\n            )\n\n        if config[arch_dict[net][0]][""arch_opt""] == ""adam"":\n\n            opt_betas = list(map(float, (config[arch_dict[net][0]][""opt_betas""].split("",""))))\n            opt_eps = float(config[arch_dict[net][0]][""opt_eps""])\n            opt_weight_decay = float(config[arch_dict[net][0]][""opt_weight_decay""])\n            opt_amsgrad = strtobool(config[arch_dict[net][0]][""opt_amsgrad""])\n\n            optimizers[net] = optim.Adam(\n                nns[net].parameters(),\n                lr=lr,\n                betas=opt_betas,\n                eps=opt_eps,\n                weight_decay=opt_weight_decay,\n                amsgrad=opt_amsgrad,\n            )\n\n        if config[arch_dict[net][0]][""arch_opt""] == ""rmsprop"":\n\n            opt_momentum = float(config[arch_dict[net][0]][""opt_momentum""])\n            opt_alpha = float(config[arch_dict[net][0]][""opt_alpha""])\n            opt_eps = float(config[arch_dict[net][0]][""opt_eps""])\n            opt_centered = strtobool(config[arch_dict[net][0]][""opt_centered""])\n            opt_weight_decay = float(config[arch_dict[net][0]][""opt_weight_decay""])\n\n            optimizers[net] = optim.RMSprop(\n                nns[net].parameters(),\n                lr=lr,\n                momentum=opt_momentum,\n                alpha=opt_alpha,\n                eps=opt_eps,\n                centered=opt_centered,\n                weight_decay=opt_weight_decay,\n            )\n\n    return optimizers\n\n\ndef forward_model_refac01(\n    fea_dict,\n    lab_dict,\n    arch_dict,\n    model,\n    nns,\n    costs,\n    inp,\n    ref,\n    inp_out_dict,\n    max_len_fea,\n    max_len_lab,\n    batch_size,\n    to_do,\n    forward_outs,\n):\n    def _add_input_features_to_outs_dict(fea_dict, outs_dict, inp):\n        for fea in fea_dict.keys():\n            if len(inp.shape) == 3 and len(fea_dict[fea]) > 1:\n                outs_dict[fea] = inp[:, :, fea_dict[fea][5] : fea_dict[fea][6]]\n            if len(inp.shape) == 2 and len(fea_dict[fea]) > 1:\n                outs_dict[fea] = inp[:, fea_dict[fea][5] : fea_dict[fea][6]]\n        return outs_dict\n\n    def _compute_layer_values(\n        inp_out_dict, inp2, inp, inp1, max_len, batch_size, arch_dict, out_name, nns, outs_dict, to_do\n    ):\n        def _is_input_feature(inp_out_dict, inp2):\n            if len(inp_out_dict[inp2]) > 1:\n                return True\n            return False\n\n        def _extract_respective_feature_from_input(inp, inp_out_dict, inp2, arch_dict, inp1, max_len, batch_size):\n            if len(inp.shape) == 3:\n                inp_dnn = inp\n                if not (bool(arch_dict[inp1][2])):\n                    inp_dnn = inp_dnn.view(max_len * batch_size, -1)\n            if len(inp.shape) == 2:\n                inp_dnn = inp\n                if bool(arch_dict[inp1][2]):\n                    inp_dnn = inp_dnn.view(max_len, batch_size, -1)\n            return inp_dnn\n\n        do_break = False\n        if _is_input_feature(inp_out_dict, inp2):\n            inp_dnn = _extract_respective_feature_from_input(\n                inp, inp_out_dict, inp2, arch_dict, inp1, max_len, batch_size\n            )\n            outs_dict[out_name] = nns[inp1](inp_dnn)\n        else:\n            if not (bool(arch_dict[inp1][2])) and len(outs_dict[inp2].shape) == 3:\n                outs_dict[inp2] = outs_dict[inp2].view(outs_dict[inp2].shape[0] * outs_dict[inp2].shape[1], -1)\n            if bool(arch_dict[inp1][2]) and len(outs_dict[inp2].shape) == 2:\n                # TODO: This computation needs to be made independent of max_len in case the network is performing sub sampling in time\n                outs_dict[inp2] = outs_dict[inp2].view(max_len, batch_size, -1)\n            outs_dict[out_name] = nns[inp1](outs_dict[inp2])\n        if to_do == ""forward"" and out_name == forward_outs[-1]:\n            do_break = True\n        return outs_dict, do_break\n\n    def _get_labels_from_input(ref, inp2, lab_dict):\n        if len(inp.shape) == 3:\n            lab_dnn = ref\n        if len(inp.shape) == 2:\n            lab_dnn = ref\n        lab_dnn = lab_dnn.view(-1).long()\n        return lab_dnn\n\n    def _get_network_output(outs_dict, inp1, max_len, batch_size):\n        out = outs_dict[inp1]\n        if len(out.shape) == 3:\n            out = out.view(max_len * batch_size, -1)\n        return out\n\n    outs_dict = {}\n    _add_input_features_to_outs_dict(fea_dict, outs_dict, inp)\n    layer_string_pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n    for line in model:\n        out_name, operation, inp1, inp2 = list(re.findall(layer_string_pattern, line)[0])\n        if operation == ""compute"":\n            outs_dict, do_break = _compute_layer_values(\n                inp_out_dict, inp2, inp, inp1, max_len_fea, batch_size, arch_dict, out_name, nns, outs_dict, to_do\n            )\n            if do_break:\n                break\n        elif operation == ""cost_nll"":\n            lab_dnn = _get_labels_from_input(ref, inp2, lab_dict)\n            out = _get_network_output(outs_dict, inp1, max_len_lab, batch_size)\n            if to_do != ""forward"":\n                outs_dict[out_name] = costs[out_name](out, lab_dnn)\n        elif operation == ""cost_err"":\n            lab_dnn = _get_labels_from_input(ref, inp2, lab_dict)\n            out = _get_network_output(outs_dict, inp1, max_len_lab, batch_size)\n            if to_do != ""forward"":\n                pred = torch.max(out, dim=1)[1]\n                err = torch.mean((pred != lab_dnn).float())\n                outs_dict[out_name] = err\n        elif operation == ""concatenate"":\n            dim_conc = len(outs_dict[inp1].shape) - 1\n            outs_dict[out_name] = torch.cat((outs_dict[inp1], outs_dict[inp2]), dim_conc)  # check concat axis\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""mult"":\n            outs_dict[out_name] = outs_dict[inp1] * outs_dict[inp2]\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""sum"":\n            outs_dict[out_name] = outs_dict[inp1] + outs_dict[inp2]\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""mult_constant"":\n            outs_dict[out_name] = outs_dict[inp1] * float(inp2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""sum_constant"":\n            outs_dict[out_name] = outs_dict[inp1] + float(inp2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""avg"":\n            outs_dict[out_name] = (outs_dict[inp1] + outs_dict[inp2]) / 2\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n        elif operation == ""mse"":\n            outs_dict[out_name] = torch.mean((outs_dict[inp1] - outs_dict[inp2]) ** 2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n    return outs_dict\n\n\ndef forward_model(\n    fea_dict, lab_dict, arch_dict, model, nns, costs, inp, inp_out_dict, max_len, batch_size, to_do, forward_outs\n):\n\n    # Forward Step\n    outs_dict = {}\n    pattern = ""(.*)=(.*)\\((.*),(.*)\\)""\n\n    # adding input features to out_dict:\n    for fea in fea_dict.keys():\n        if len(inp.shape) == 3 and len(fea_dict[fea]) > 1:\n            outs_dict[fea] = inp[:, :, fea_dict[fea][5] : fea_dict[fea][6]]\n\n        if len(inp.shape) == 2 and len(fea_dict[fea]) > 1:\n            outs_dict[fea] = inp[:, fea_dict[fea][5] : fea_dict[fea][6]]\n\n    for line in model:\n        [out_name, operation, inp1, inp2] = list(re.findall(pattern, line)[0])\n\n        if operation == ""compute"":\n\n            if len(inp_out_dict[inp2]) > 1:  # if it is an input feature\n\n                # Selection of the right feature in the inp tensor\n                if len(inp.shape) == 3:\n                    inp_dnn = inp[:, :, inp_out_dict[inp2][-3] : inp_out_dict[inp2][-2]]\n                    if not (bool(arch_dict[inp1][2])):\n                        inp_dnn = inp_dnn.view(max_len * batch_size, -1)\n\n                if len(inp.shape) == 2:\n                    inp_dnn = inp[:, inp_out_dict[inp2][-3] : inp_out_dict[inp2][-2]]\n                    if bool(arch_dict[inp1][2]):\n                        inp_dnn = inp_dnn.view(max_len, batch_size, -1)\n\n                outs_dict[out_name] = nns[inp1](inp_dnn)\n\n            else:\n                if not (bool(arch_dict[inp1][2])) and len(outs_dict[inp2].shape) == 3:\n                    outs_dict[inp2] = outs_dict[inp2].view(max_len * batch_size, -1)\n\n                if bool(arch_dict[inp1][2]) and len(outs_dict[inp2].shape) == 2:\n                    outs_dict[inp2] = outs_dict[inp2].view(max_len, batch_size, -1)\n\n                outs_dict[out_name] = nns[inp1](outs_dict[inp2])\n\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""cost_nll"":\n\n            # Put labels in the right format\n            if len(inp.shape) == 3:\n                lab_dnn = inp[:, :, lab_dict[inp2][3]]\n            if len(inp.shape) == 2:\n                lab_dnn = inp[:, lab_dict[inp2][3]]\n\n            lab_dnn = lab_dnn.view(-1).long()\n\n            # put output in the right format\n            out = outs_dict[inp1]\n\n            if len(out.shape) == 3:\n                out = out.view(max_len * batch_size, -1)\n\n            if to_do != ""forward"":\n                outs_dict[out_name] = costs[out_name](out, lab_dnn)\n\n        if operation == ""cost_err"":\n\n            if len(inp.shape) == 3:\n                lab_dnn = inp[:, :, lab_dict[inp2][3]]\n            if len(inp.shape) == 2:\n                lab_dnn = inp[:, lab_dict[inp2][3]]\n\n            lab_dnn = lab_dnn.view(-1).long()\n\n            # put output in the right format\n            out = outs_dict[inp1]\n\n            if len(out.shape) == 3:\n                out = out.view(max_len * batch_size, -1)\n\n            if to_do != ""forward"":\n                pred = torch.max(out, dim=1)[1]\n                err = torch.mean((pred != lab_dnn).float())\n                outs_dict[out_name] = err\n                # print(err)\n\n        if operation == ""concatenate"":\n            dim_conc = len(outs_dict[inp1].shape) - 1\n            outs_dict[out_name] = torch.cat((outs_dict[inp1], outs_dict[inp2]), dim_conc)  # check concat axis\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""mult"":\n            outs_dict[out_name] = outs_dict[inp1] * outs_dict[inp2]\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""sum"":\n            outs_dict[out_name] = outs_dict[inp1] + outs_dict[inp2]\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""mult_constant"":\n            outs_dict[out_name] = outs_dict[inp1] * float(inp2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""sum_constant"":\n            outs_dict[out_name] = outs_dict[inp1] + float(inp2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""avg"":\n            outs_dict[out_name] = (outs_dict[inp1] + outs_dict[inp2]) / 2\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n        if operation == ""mse"":\n            outs_dict[out_name] = torch.mean((outs_dict[inp1] - outs_dict[inp2]) ** 2)\n            if to_do == ""forward"" and out_name == forward_outs[-1]:\n                break\n\n    return outs_dict\n\n\ndef dump_epoch_results(\n    res_file_path, ep, tr_data_lst, tr_loss_tot, tr_error_tot, tot_time, valid_data_lst, valid_peformance_dict, lr, N_ep\n):\n\n    #\n    # Default terminal line size is 80 characters, try new dispositions to fit this limit\n    #\n\n    N_ep_str_format = ""0"" + str(max(math.ceil(np.log10(N_ep)), 1)) + ""d""\n    res_file = open(res_file_path, ""a"")\n    res_file.write(\n        ""ep=%s tr=%s loss=%s err=%s ""\n        % (\n            format(ep, N_ep_str_format),\n            tr_data_lst,\n            format(tr_loss_tot / len(tr_data_lst), ""0.3f""),\n            format(tr_error_tot / len(tr_data_lst), ""0.3f""),\n        )\n    )\n    print("" "")\n    print(""----- Summary epoch %s / %s"" % (format(ep, N_ep_str_format), format(N_ep - 1, N_ep_str_format)))\n    print(""Training on %s"" % (tr_data_lst))\n    print(\n        ""Loss = %s | err = %s ""\n        % (format(tr_loss_tot / len(tr_data_lst), ""0.3f""), format(tr_error_tot / len(tr_data_lst), ""0.3f""))\n    )\n    print(""-----"")\n    for valid_data in valid_data_lst:\n        res_file.write(\n            ""valid=%s loss=%s err=%s ""\n            % (\n                valid_data,\n                format(valid_peformance_dict[valid_data][0], ""0.3f""),\n                format(valid_peformance_dict[valid_data][1], ""0.3f""),\n            )\n        )\n        print(""Validating on %s"" % (valid_data))\n        print(\n            ""Loss = %s | err = %s ""\n            % (\n                format(valid_peformance_dict[valid_data][0], ""0.3f""),\n                format(valid_peformance_dict[valid_data][1], ""0.3f""),\n            )\n        )\n\n    print(""-----"")\n    for lr_arch in lr.keys():\n        res_file.write(""lr_%s=%s "" % (lr_arch, lr[lr_arch][ep]))\n        print(""Learning rate on %s = %s "" % (lr_arch, lr[lr_arch][ep]))\n    print(""-----"")\n    res_file.write(""time(s)=%i\\n"" % (int(tot_time)))\n    print(""Elapsed time (s) = %i\\n"" % (int(tot_time)))\n    print("" "")\n    res_file.close()\n\n\ndef progress(count, total, status=""""):\n    bar_len = 40\n    filled_len = int(round(bar_len * count / float(total)))\n\n    percents = round(100.0 * count / float(total), 1)\n    bar = ""="" * filled_len + ""-"" * (bar_len - filled_len)\n\n    if count == total - 1:\n        sys.stdout.write(""[%s] %s%s %s\\r"" % (bar, 100, ""%"", status))\n        sys.stdout.write(""\\n"")\n    else:\n        sys.stdout.write(""[%s] %s%s %s\\r"" % (bar, percents, ""%"", status))\n\n    sys.stdout.flush()\n\n\ndef export_loss_acc_to_txt(out_folder, N_ep, val_lst):\n\n    if not os.path.exists(out_folder + ""/generated_outputs""):\n        os.makedirs(out_folder + ""/generated_outputs"")\n\n    nb_val = len(val_lst)\n    res = open(out_folder + ""/res.res"", ""r"").readlines()\n\n    tr_loss = []\n    tr_acc = []\n    val_loss = np.ndarray((nb_val, N_ep))\n    val_acc = np.ndarray((nb_val, N_ep))\n\n    line_cpt = 0\n    for i in range(N_ep):\n        splitted = res[i].split("" "")\n\n        # Getting uniq training loss and acc\n        tr_loss.append(float(splitted[2].split(""="")[1]))\n        tr_acc.append(1 - float(splitted[3].split(""="")[1]))\n\n        # Getting multiple or uniq val loss and acc\n        # +5 to avoird the 6 first columns of the res.res file\n        for i in range(nb_val):\n            val_loss[i][line_cpt] = float(splitted[(i * 3) + 5].split(""="")[1])\n            val_acc[i][line_cpt] = 1 - float(splitted[(i * 3) + 6].split(""="")[1])\n\n        line_cpt += 1\n\n    # Saving to files\n    np.savetxt(out_folder + ""/generated_outputs/tr_loss.txt"", np.asarray(tr_loss), ""%0.3f"", delimiter="","")\n    np.savetxt(out_folder + ""/generated_outputs/tr_acc.txt"", np.asarray(tr_acc), ""%0.3f"", delimiter="","")\n\n    for i in range(nb_val):\n        np.savetxt(out_folder + ""/generated_outputs/val_"" + str(i) + ""_loss.txt"", val_loss[i], ""%0.5f"", delimiter="","")\n        np.savetxt(out_folder + ""/generated_outputs/val_"" + str(i) + ""_acc.txt"", val_acc[i], ""%0.5f"", delimiter="","")\n\n\ndef create_curves(out_folder, N_ep, val_lst):\n\n    try:\n        import matplotlib as mpl\n\n        mpl.use(""Agg"")\n        import matplotlib.pyplot as plt\n\n    except ValueError:\n        print(""WARNING:  matplotlib is not installed. The plots of the training curves have not been created."")\n        sys.exit(0)\n\n    print("" "")\n    print(""-----"")\n    print(""Generating output files and plots ... "")\n    export_loss_acc_to_txt(out_folder, N_ep, val_lst)\n\n    if not os.path.exists(out_folder + ""/generated_outputs""):\n        sys.stdacc.write(""accOR: No results generated please call export_loss_err_to_txt() before"")\n        sys.exit(0)\n\n    nb_epoch = len(open(out_folder + ""/generated_outputs/tr_loss.txt"", ""r"").readlines())\n    x = np.arange(nb_epoch)\n    nb_val = len(val_lst)\n\n    # Loading train Loss and acc\n    tr_loss = np.loadtxt(out_folder + ""/generated_outputs/tr_loss.txt"")\n    tr_acc = np.loadtxt(out_folder + ""/generated_outputs/tr_acc.txt"")\n\n    # Loading val loss and acc\n    val_loss = []\n    val_acc = []\n    for i in range(nb_val):\n        val_loss.append(np.loadtxt(out_folder + ""/generated_outputs/val_"" + str(i) + ""_loss.txt""))\n        val_acc.append(np.loadtxt(out_folder + ""/generated_outputs/val_"" + str(i) + ""_acc.txt""))\n\n    #\n    # LOSS PLOT\n    #\n\n    # Getting maximum values\n    max_loss = np.amax(tr_loss)\n    for i in range(nb_val):\n        if np.amax(val_loss[i]) > max_loss:\n            max_loss = np.amax(val_loss[i])\n\n    # Plot train loss and acc\n    plt.plot(x, tr_loss, label=""train_loss"")\n\n    # Plot val loss and acc\n    for i in range(nb_val):\n        plt.plot(x, val_loss[i], label=""val_"" + str(i) + ""_loss"")\n\n    plt.ylabel(""Loss"")\n    plt.xlabel(""Epoch"")\n    plt.title(""Evolution of the loss function"")\n    plt.axis([0, nb_epoch - 1, 0, max_loss + 1])\n    plt.legend()\n    plt.savefig(out_folder + ""/generated_outputs/loss.png"")\n\n    # Clear plot\n    plt.gcf().clear()\n\n    #\n    # ACC PLOT\n    #\n\n    # Plot train loss and acc\n    plt.plot(x, tr_acc, label=""train_acc"")\n\n    # Plot val loss and acc\n    for i in range(nb_val):\n        plt.plot(x, val_acc[i], label=""val_"" + str(i) + ""_acc"")\n\n    plt.ylabel(""Accuracy"")\n    plt.xlabel(""Epoch"")\n    plt.title(""Evolution of the accuracy"")\n    plt.axis([0, nb_epoch - 1, 0, 1])\n    plt.legend()\n    plt.savefig(out_folder + ""/generated_outputs/acc.png"")\n\n    print(""OK"")\n\n\n# Replace the nth pattern in a string\ndef nth_replace_string(s, sub, repl, nth):\n    find = s.find(sub)\n\n    # if find is not p1 we have found at least one match for the substring\n    i = find != -1\n    # loop util we find the nth or we find no match\n    while find != -1 and i != nth:\n        # find + 1 means we start at the last match start index + 1\n        find = s.find(sub, find + 1)\n        i += 1\n    # if i  is equal to nth we found nth matches so replace\n    if i == nth:\n        return s[:find] + repl + s[find + len(sub) :]\n    return s\n\n\ndef change_lr_cfg(cfg_file, lr, ep):\n\n    config = configparser.ConfigParser()\n    config.read(cfg_file)\n    field = ""arch_lr""\n\n    for lr_arch in lr.keys():\n\n        config.set(lr_arch, field, str(lr[lr_arch][ep]))\n\n    # Write cfg_file_chunk\n    with open(cfg_file, ""w"") as configfile:\n        config.write(configfile)\n\n\ndef shift(arr, num, fill_value=np.nan):\n    if num >= 0:\n        return np.concatenate((np.full(num, fill_value), arr[:-num]))\n    else:\n        return np.concatenate((arr[-num:], np.full(-num, fill_value)))\n\n\ndef expand_str_ep(str_compact, type_inp, N_ep, split_elem, mult_elem):\n\n    lst_out = []\n\n    str_compact_lst = str_compact.split(split_elem)\n\n    for elem in str_compact_lst:\n        elements = elem.split(mult_elem)\n\n        if type_inp == ""int"":\n            try:\n                int(elements[0])\n            except ValueError:\n                sys.stderr.write(\'The string ""%s"" must contain integers. Got %s.\\n\' % (str_compact, elements[0]))\n                sys.exit(0)\n\n        if type_inp == ""float"":\n            try:\n                float(elements[0])\n            except ValueError:\n                sys.stderr.write(\'The string ""%s"" must contain floats. Got %s.\\n\' % (str_compact, elements[0]))\n                sys.exit(0)\n\n        if len(elements) == 2:\n            try:\n                int(elements[1])\n                lst_out.extend([elements[0] for i in range(int(elements[1]))])\n            except ValueError:\n                sys.stderr.write(\'The string ""%s"" must contain integers. Got %s\\n\' % (str_compact, elements[1]))\n                sys.exit(0)\n\n        if len(elements) == 1:\n            lst_out.append(elements[0])\n\n    if len(str_compact_lst) == 1 and len(elements) == 1:\n        lst_out.extend([elements[0] for i in range(N_ep - 1)])\n\n    # Final check\n    if len(lst_out) != N_ep:\n        sys.stderr.write(\n            \'The total number of elements specified in the string ""%s"" is equal to %i not equal to the total number of epochs %s.\\n\'\n            % (str_compact, len(lst_out), N_ep)\n        )\n        sys.exit(0)\n\n    return lst_out\n'"
kaldi_decoding_scripts/utils/filt.py,0,"b'#!/usr/bin/env python\n\n# Apache 2.0\nfrom __future__ import print_function\n\nimport sys\n\nvocab = set()\nwith open(sys.argv[1]) as vocabfile:\n    for line in vocabfile:\n        vocab.add(line.strip())\n\nwith open(sys.argv[2]) as textfile:\n    for line in textfile:\n        print("" "".join(map(lambda word: word if word in vocab else ""<UNK>"", line.strip().split())))\n'"
kaldi_decoding_scripts/utils/reverse_arpa.py,0,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n# Copyright 2012 Mirko Hannemann BUT, mirko.hannemann@gmail.com\n\nimport sys\nimport codecs # for UTF-8/unicode\n\nif len(sys.argv) != 2:\n    print \'usage: reverse_arpa arpa.in\'\n    sys.exit()\narpaname = sys.argv[1]\n\n#\\data\\\n#ngram 1=4\n#ngram 2=2\n#ngram 3=2\n#\n#\\1-grams:\n#-5.234679\ta -3.3\n#-3.456783\tb\n#0.0000000\t<s> -2.5\n#-4.333333\t</s>\n#\n#\\2-grams:\n#-1.45678\ta b -3.23\n#-1.30490\t<s> a -4.2\n#\n#\\3-grams:\n#-0.34958\t<s> a b\n#-0.23940\ta b </s>\n#\\end\\\n\n# read language model in ARPA format\ntry:\n  file = codecs.open(arpaname, ""r"", ""utf-8"")\nexcept IOError:\n  print \'file not found: \' + arpaname\n  sys.exit()\n\ntext=file.readline()\nwhile (text and text[:6] != ""\\\\data\\\\""): text=file.readline()\nif not text:\n  print ""invalid ARPA file""\n  sys.exit()\n#print text,\nwhile (text and text[:5] != ""ngram""): text=file.readline()\n\n# get ngram counts\ncngrams=[]\nn=0\nwhile (text and text[:5] == ""ngram""):\n  ind = text.split(""="")\n  counts = int(ind[1].strip())\n  r = ind[0].split()\n  read_n = int(r[1].strip())\n  if read_n != n+1:\n    print ""invalid ARPA file:"", text\n    sys.exit()\n  n = read_n\n  cngrams.append(counts)\n  #print text,\n  text=file.readline()\n\n# read all n-grams order by order\nsentprob = 0.0 # sentence begin unigram\nngrams=[]\ninf=float(""inf"")\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  while (text and ""-grams:"" not in text): text=file.readline()\n  if n != int(text[1]):\n    print ""invalid ARPA file:"", text\n    sys.exit()\n  #print text,cngrams[n-1]\n  this_ngrams={} # stores all read ngrams\n  for ng in range(cngrams[n-1]):\n    while (text and len(text.split())<2):\n      text=file.readline()\n      if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))): break\n    if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))):\n      break # to deal with incorrect ARPA files\n    entry = text.split()\n    prob = float(entry[0])\n    if len(entry)>n+1:\n      back = float(entry[-1])\n      words = entry[1:n+1]\n    else:\n      back = 0.0\n      words = entry[1:]\n    ngram = "" "".join(words)\n    if (n==1) and words[0]==""<s>"":\n      sentprob = prob\n      prob = 0.0\n    this_ngrams[ngram] = (prob,back)\n    #print prob,ngram.encode(""utf-8""),back\n\n    for x in range(n-1,0,-1):\n      # add all missing backoff ngrams for reversed lm\n      l_ngram = "" "".join(words[:x]) # shortened ngram\n      r_ngram = "" "".join(words[1:1+x]) # shortened ngram with offset one\n      if l_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][l_ngram] = (0.0,inf)\n        #print ngram, ""create 0.0"", l_ngram, ""inf""\n      if r_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][r_ngram] = (0.0,inf)\n        #print ngram, ""create 0.0"", r_ngram, ""inf"",x,n,h_ngram\n\n      # add all missing backoff ngrams for forward lm\n      h_ngram = "" "".join(words[n-x:]) # shortened history\n      if h_ngram not in ngrams[x-1]: # create missing ngram\n        ngrams[x-1][h_ngram] = (0.0,inf)\n        #print ""create inf"", h_ngram, ""0.0""\n    text=file.readline()\n    if (not text) or ((len(text.split())==1) and ((""-grams:"" in text) or (text[:5] == ""\\\\end\\\\""))): break\n  ngrams.append(this_ngrams)\n\nwhile (text and text[:5] != ""\\\\end\\\\""): text=file.readline()\nif not text:\n  print ""invalid ARPA file""\n  sys.exit()\nfile.close()\n#print text,\n\n#fourgram ""maxent"" model (b(ABCD)=0):\n#p(A)+b(A) A 0\n#p(AB)+b(AB)-b(A)-p(B) AB 0\n#p(ABC)+b(ABC)-b(AB)-p(BC) ABC 0\n#p(ABCD)+b(ABCD)-b(ABC)-p(BCD) ABCD 0\n\n#fourgram reverse ARPA model (b(ABCD)=0):\n#p(A)+b(A) A 0\n#p(AB)+b(AB)-p(B)+p(A) BA 0\n#p(ABC)+b(ABC)-p(BC)+p(AB)-p(B)+p(A) CBA 0\n#p(ABCD)+b(ABCD)-p(BCD)+p(ABC)-p(BC)+p(AB)-p(B)+p(A) DCBA 0\n\n# compute new reversed ARPA model\nprint ""\\\\data\\\\""\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  print ""ngram ""+str(n)+""=""+str(len(ngrams[n-1].keys()))\noffset = 0.0\nfor n in range(1,len(cngrams)+1): # unigrams, bigrams, trigrams\n  print ""\\\\""+str(n)+""-grams:""\n  keys = ngrams[n-1].keys()\n  keys.sort()\n  for ngram in keys:\n    prob = ngrams[n-1][ngram]\n    # reverse word order\n    words = ngram.split()\n    rstr = "" "".join(reversed(words))\n    # swap <s> and </s>\n    rev_ngram = rstr.replace(""<s>"",""<temp>"").replace(""</s>"",""<s>"").replace(""<temp>"",""</s>"")\n\n    revprob = prob[0]\n    if (prob[1] != inf): # only backoff weights from not newly created ngrams\n      revprob = revprob + prob[1]\n    #print prob[0],prob[1]\n    # sum all missing terms in decreasing ngram order\n    for x in range(n-1,0,-1): \n      l_ngram = "" "".join(words[:x]) # shortened ngram\n      if l_ngram not in ngrams[x-1]:\n        sys.stderr.write(rev_ngram+"": not found ""+l_ngram+""\\n"")\n      p_l = ngrams[x-1][l_ngram][0]\n      #print p_l,l_ngram\n      revprob = revprob + p_l\n\n      r_ngram = "" "".join(words[1:1+x]) # shortened ngram with offset one\n      if r_ngram not in ngrams[x-1]:\n        sys.stderr.write(rev_ngram+"": not found ""+r_ngram+""\\n"")\n      p_r = ngrams[x-1][r_ngram][0]\n      #print -p_r,r_ngram\n      revprob = revprob - p_r\n\n    if n != len(cngrams): #not highest order\n      back = 0.0\n      if rev_ngram[:3] == ""<s>"": # special handling since arpa2fst ignores <s> weight\n        if n == 1:\n          offset = revprob # remember <s> weight\n          revprob = sentprob # apply <s> weight from forward model\n          back = offset\n        elif n == 2:\n          revprob = revprob + offset # add <s> weight to bigrams starting with <s>\n      if (prob[1] != inf): # only backoff weights from not newly created ngrams\n        print revprob,rev_ngram.encode(""utf-8""),back\n      else:\n        print revprob,rev_ngram.encode(""utf-8""),""-100000.0""\n    else: # highest order - no backoff weights\n      if (n==2) and (rev_ngram[:3] == ""<s>""): revprob = revprob + offset\n      print revprob,rev_ngram.encode(""utf-8"")\nprint ""\\\\end\\\\""\n'"
kaldi_decoding_scripts/utils/nnet/gen_dct_mat.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_dct_mat.py\n# script generates matrix with DCT transform, which is sparse\n# and takes into account that data-layout is along frequency axis,\n# while DCT is done along temporal axis.\nfrom __future__ import print_function\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(""--fea-dim"", dest=""dim"", help=""feature dimension"")\nparser.add_option(""--splice"", dest=""splice"", help=""applied splice value"")\nparser.add_option(""--dct-basis"", dest=""dct_basis"", help=""number of DCT basis"")\n(options, args) = parser.parse_args()\n\nif options.dim == None:\n    parser.print_help()\n    sys.exit(1)\n\ndim = int(options.dim)\nsplice = int(options.splice)\ndct_basis = int(options.dct_basis)\n\ntimeContext = 2 * splice + 1\n\n\n# generate the DCT matrix\nM_PI = 3.1415926535897932384626433832795\nM_SQRT2 = 1.4142135623730950488016887\n\n\n# generate sparse DCT matrix\nprint(""["")\nfor k in range(dct_basis):\n    for m in range(dim):\n        for n in range(timeContext):\n            if n == 0:\n                print(m * ""0 "", end="" "")\n            else:\n                print((dim - 1) * ""0 "", end="" "")\n            print(str(sqrt(2.0 / timeContext) * cos(M_PI / timeContext * k * (n + 0.5))), end="" "")\n            if n == timeContext - 1:\n                print((dim - m - 1) * ""0 "", end="" "")\n        print()\n    print()\n\nprint(""]"")\n'"
kaldi_decoding_scripts/utils/nnet/gen_hamm_mat.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_hamm_mat.py\n# script generates diagonal matrix with hamming window values\nfrom __future__ import print_function\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(""--fea-dim"", dest=""dim"", help=""feature dimension"")\nparser.add_option(""--splice"", dest=""splice"", help=""applied splice value"")\n(options, args) = parser.parse_args()\n\nif options.dim == None:\n    parser.print_help()\n    sys.exit(1)\n\ndim = int(options.dim)\nsplice = int(options.splice)\n\n\n# generate the diagonal matrix with hammings\nM_2PI = 6.283185307179586476925286766559005\n\ndim_mat = (2 * splice + 1) * dim\ntimeContext = 2 * splice + 1\nprint(""["")\nfor row in range(dim_mat):\n    for col in range(dim_mat):\n        if col != row:\n            print(""0"", end="" "")\n        else:\n            i = int(row / dim)\n            print(str(0.54 - 0.46 * cos((M_2PI * i) / (timeContext - 1))), end="" "")\n    print()\n\nprint(""]"")\n'"
kaldi_decoding_scripts/utils/nnet/gen_splice.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2012  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# ./gen_splice.py\n# generates <splice> Component\nfrom __future__ import print_function\n\nfrom math import *\nimport sys\n\n\nfrom optparse import OptionParser\n\nparser = OptionParser()\nparser.add_option(""--fea-dim"", dest=""dim_in"", help=""feature dimension"")\nparser.add_option(""--splice"", dest=""splice"", help=""number of frames to concatenate with the central frame"")\nparser.add_option(\n    ""--splice-step"",\n    dest=""splice_step"",\n    help=""splicing step (frames dont need to be consecutive, --splice 3 --splice-step 2 will select offsets: -6 -4 -2 0 2 4 6)"",\n    default=""1"",\n)\n(options, args) = parser.parse_args()\n\nif options.dim_in == None:\n    parser.print_help()\n    sys.exit(1)\n\ndim_in = int(options.dim_in)\nsplice = int(options.splice)\nsplice_step = int(options.splice_step)\n\ndim_out = (2 * splice + 1) * dim_in\n\nprint(""<splice>"", dim_out, dim_in)\nprint(""["", end="" "")\n\nsplice_vec = range(-splice * splice_step, splice * splice_step + 1, splice_step)\nfor idx in range(len(splice_vec)):\n    print(splice_vec[idx], end="" "")\n\nprint(""]"")\n'"
kaldi_decoding_scripts/utils/nnet/make_blstm_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\nfrom __future__ import print_function\n\nimport sys\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\n\nusage = ""%prog [options] <feat-dim> <num-leaves> >nnet-proto-file""\nparser = OptionParser(usage)\n#\nparser.add_option(\n    ""--num-cells"", dest=""num_cells"", type=""int"", default=800, help=""Number of LSTM cells [default: %default]""\n)\nparser.add_option(\n    ""--num-recurrent"",\n    dest=""num_recurrent"",\n    type=""int"",\n    default=512,\n    help=""Number of LSTM recurrent units [default: %default]"",\n)\nparser.add_option(\n    ""--num-layers"", dest=""num_layers"", type=""int"", default=2, help=""Number of LSTM layers [default: %default]""\n)\nparser.add_option(\n    ""--lstm-stddev-factor"",\n    dest=""lstm_stddev_factor"",\n    type=""float"",\n    default=0.01,\n    help=""Standard deviation of initialization [default: %default]"",\n)\nparser.add_option(\n    ""--param-stddev-factor"",\n    dest=""param_stddev_factor"",\n    type=""float"",\n    default=0.04,\n    help=""Standard deviation in output layer [default: %default]"",\n)\nparser.add_option(\n    ""--clip-gradient"",\n    dest=""clip_gradient"",\n    type=""float"",\n    default=5.0,\n    help=""Clipping constant applied to gradients [default: %default]"",\n)\n#\n(o, args) = parser.parse_args()\nif len(args) != 2:\n    parser.print_help()\n    sys.exit(1)\n\n(feat_dim, num_leaves) = list(map(int, args))\n\n# Original prototype from Jiayu,\n# <NnetProto>\n# <Transmit> <InputDim> 40 <OutputDim> 40\n# <LstmProjectedStreams> <InputDim> 40 <OutputDim> 512 <CellDim> 800 <ParamScale> 0.01 <NumStream> 4\n# <AffineTransform> <InputDim> 512 <OutputDim> 8000 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.04\n# <Softmax> <InputDim> 8000 <OutputDim> 8000\n# </NnetProto>\n\nprint(""<NnetProto>"")\n# normally we won\'t use more than 2 layers of LSTM\nif o.num_layers == 1:\n    print(\n        ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (feat_dim, 2 * o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\nelif o.num_layers == 2:\n    print(\n        ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (feat_dim, 2 * o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\n    print(\n        ""<BLstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (2 * o.num_recurrent, 2 * o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\nelse:\n    sys.stderr.write(""make_lstm_proto.py ERROR: more than 2 layers of LSTM, not supported yet.\\n"")\n    sys.exit(1)\nprint(\n    ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> 0.0 <BiasRange> 0.0 <ParamStddev> %f""\n    % (2 * o.num_recurrent, num_leaves, o.param_stddev_factor)\n)\nprint(""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves))\nprint(""</NnetProto>"")\n'"
kaldi_decoding_scripts/utils/nnet/make_cnn2d_proto.py,0,"b'#!/usr/bin/python\n\n# Copyright 2014  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\nfrom __future__ import print_function\n\nimport math, random, sys, warnings\nfrom optparse import OptionParser\n\n###\n### Parse options\n###\nusage = ""%prog [options] <feat-dim> <num-leaves> <num-hidden-layers> <num-hidden-neurons>  >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\n    ""--activation-type"",\n    dest=""activation_type"",\n    help=""Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]"",\n    default=""<Sigmoid>"",\n    type=""string"",\n)\n\nparser.add_option(\n    ""--cnn1-num-filters"",\n    dest=""cnn1_num_filters"",\n    help=""Number of filters in first convolutional layer [default: %default]"",\n    default=128,\n    type=""int"",\n)\n# this is given by splice\n# parser.add_option(\'--cnn1-fmap-x-len\', dest=\'cnn1_fmap_x_len\',\n# \t  \t   help=\'Size of cnn1-fmap-x-len [default: %default]\',\n# \t\t   default=11, type=\'int\')\n\n# this should be equal to feat_raw_dim\n# parser.add_option(\'--cnn1-fmap-y-len\', dest=\'cnn1_fmap_y_len\',\n# \t  \t   help=\'Size of cnn1-fmap-y-len [default: %default]\',\n# \t\t   default=32, type=\'int\')\n\nparser.add_option(\n    ""--cnn1-filt-x-len"",\n    dest=""cnn1_filt_x_len"",\n    help=""Size of cnn1-filt-x-len [default: %default]"",\n    default=9,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn1-filt-y-len"",\n    dest=""cnn1_filt_y_len"",\n    help=""Size of cnn1-filt-y-len [default: %default]"",\n    default=9,\n    type=""int"",\n)\n\nparser.add_option(\n    ""--cnn1-filt-x-step"",\n    dest=""cnn1_filt_x_step"",\n    help=""Size of cnn1-filt-x-step [default: %default]"",\n    default=1,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn1-filt-y-step"",\n    dest=""cnn1_filt_y_step"",\n    help=""Size of cnn1-filt-y-step [default: %default]"",\n    default=1,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn1-connect-fmap"",\n    dest=""cnn1_connect_fmap"",\n    help=""Size of cnn1-connect-fmap [default: %default]"",\n    default=0,\n    type=""int"",\n)\n\nparser.add_option(\n    ""--pool1-x-len"", dest=""pool1_x_len"", help=""Size of pool1-filt-x-len [default: %default]"", default=1, type=""int""\n)\nparser.add_option(\n    ""--pool1-x-step"", dest=""pool1_x_step"", help=""Size of pool1-x-step [default: %default]"", default=1, type=""int""\n)\n\n\n#\nparser.add_option(\n    ""--pool1-y-len"", dest=""pool1_y_len"", help=""Size of pool1-y-len [default: %default]"", default=3, type=""int""\n)\nparser.add_option(\n    ""--pool1-y-step"", dest=""pool1_y_step"", help=""Size of pool1-y-step [default: %default]"", default=3, type=""int""\n)\n\nparser.add_option(\n    ""--pool1-type"",\n    dest=""pool1_type"",\n    help=""Type of pooling (Max || Average) [default: %default]"",\n    default=""Max"",\n    type=""string"",\n)\n\nparser.add_option(\n    ""--cnn2-num-filters"",\n    dest=""cnn2_num_filters"",\n    help=""Number of filters in first convolutional layer [default: %default]"",\n    default=256,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn2-filt-x-len"",\n    dest=""cnn2_filt_x_len"",\n    help=""Size of cnn2-filt-x-len [default: %default]"",\n    default=3,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn2-filt-y-len"",\n    dest=""cnn2_filt_y_len"",\n    help=""Size of cnn2-filt-y-len [default: %default]"",\n    default=4,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn2-filt-x-step"",\n    dest=""cnn2_filt_x_step"",\n    help=""Size of cnn2-filt-x-step [default: %default]"",\n    default=1,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn2-filt-y-step"",\n    dest=""cnn2_filt_y_step"",\n    help=""Size of cnn2-filt-y-step [default: %default]"",\n    default=1,\n    type=""int"",\n)\nparser.add_option(\n    ""--cnn2-connect-fmap"",\n    dest=""cnn2_connect_fmap"",\n    help=""Size of cnn2-connect-fmap [default: %default]"",\n    default=1,\n    type=""int"",\n)\n\nparser.add_option(\n    ""--pitch-dim"",\n    dest=""pitch_dim"",\n    help=""Number of features representing pitch [default: %default]"",\n    default=0,\n    type=""int"",\n)\nparser.add_option(\n    ""--delta-order"", dest=""delta_order"", help=""Order of delta features [default: %default]"", default=2, type=""int""\n)\nparser.add_option(""--splice"", dest=""splice"", help=""Length of splice [default: %default]"", default=5, type=""int"")\nparser.add_option(\n    ""--dir"",\n    dest=""dirct"",\n    help=""Directory, where network prototypes will be saved [default: %default]"",\n    default=""."",\n    type=""string"",\n)\nparser.add_option(\n    ""--num-pitch-neurons"",\n    dest=""num_pitch_neurons"",\n    help=""Number of neurons in layers processing pitch features [default: %default]"",\n    default=""200"",\n    type=""int"",\n)\n\n\n(o, args) = parser.parse_args()\nif len(args) != 1:\n    parser.print_help()\n    sys.exit(1)\n\nfeat_dim = int(args[0])\n### End parse options\n\nfeat_raw_dim = (\n    feat_dim / (o.delta_order + 1) / (o.splice * 2 + 1) - o.pitch_dim\n)  # we need number of feats without deltas and splice and pitch\no.cnn1_fmap_y_len = feat_raw_dim\no.cnn1_fmap_x_len = o.splice * 2 + 1\n\n# Check\nassert feat_dim > 0\nassert o.pool1_type == ""Max"" or o.pool1_type == ""Average""\n\n## Extra checks if dimensions are matching, if not match them by\n## producing a warning\n# cnn1\nassert (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) % o.cnn1_filt_y_step == 0\nassert (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) % o.cnn1_filt_x_step == 0\n\n# subsample1\ncnn1_out_fmap_y_len = 1 + (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) / o.cnn1_filt_y_step\ncnn1_out_fmap_x_len = 1 + (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) / o.cnn1_filt_x_step\n\n# fix filt_len and filt_step\ndef fix_filt_step(inp_len, filt_len, filt_step):\n\n    if (inp_len - filt_len) % filt_step == 0:\n        return filt_step\n    else:\n        # filt_step <= filt_len\n        for filt_step in range(filt_len, 0, -1):\n            if (inp_len - filt_len) % filt_step == 0:\n                return filt_step\n\n\no.pool1_y_step = fix_filt_step(cnn1_out_fmap_y_len, o.pool1_y_len, o.pool1_y_step)\nif o.pool1_y_step == 1 and o.pool1_y_len != 1:\n    warnings.warn(""WARNING: Choose different pool1_y_len as subsampling is not happening"")\n\no.pool1_x_step = fix_filt_step(cnn1_out_fmap_x_len, o.pool1_x_len, o.pool1_x_step)\nif o.pool1_x_step == 1 and o.pool1_x_len != 1:\n    warnings.warn(""WARNING: Choose different pool1_x_len as subsampling is not happening"")\n\n\n###\n### Print prototype of the network\n###\n\n# Begin the prototype\nprint(""<NnetProto>"")\n\n# Convolutional part of network\n""""""1st CNN layer""""""\ncnn1_input_dim = feat_raw_dim * (o.delta_order + 1) * (o.splice * 2 + 1)\ncnn1_out_fmap_x_len = 1 + (o.cnn1_fmap_x_len - o.cnn1_filt_x_len) / o.cnn1_filt_x_step\ncnn1_out_fmap_y_len = 1 + (o.cnn1_fmap_y_len - o.cnn1_filt_y_len) / o.cnn1_filt_y_step\ncnn1_output_dim = o.cnn1_num_filters * cnn1_out_fmap_x_len * cnn1_out_fmap_y_len\n\n""""""1st Pooling layer""""""\npool1_input_dim = cnn1_output_dim\npool1_fmap_x_len = cnn1_out_fmap_x_len\npool1_out_fmap_x_len = 1 + (pool1_fmap_x_len - o.pool1_x_len) / o.pool1_x_step\npool1_fmap_y_len = cnn1_out_fmap_y_len\npool1_out_fmap_y_len = 1 + (pool1_fmap_y_len - o.pool1_y_len) / o.pool1_y_step\npool1_output_dim = o.cnn1_num_filters * pool1_out_fmap_x_len * pool1_out_fmap_y_len\n\n""""""2nd CNN layer""""""\ncnn2_input_dim = pool1_output_dim\ncnn2_fmap_x_len = pool1_out_fmap_x_len\ncnn2_out_fmap_x_len = 1 + (cnn2_fmap_x_len - o.cnn2_filt_x_len) / o.cnn2_filt_x_step\ncnn2_fmap_y_len = pool1_out_fmap_y_len\ncnn2_out_fmap_y_len = 1 + (cnn2_fmap_y_len - o.cnn2_filt_y_len) / o.cnn2_filt_y_step\ncnn2_output_dim = o.cnn2_num_filters * cnn2_out_fmap_x_len * cnn2_out_fmap_y_len\n\n\nconvolution_proto = """"\n\nconvolution_proto += (\n    ""<Convolutional2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <FiltXLen> %d <FiltYLen> %d <FiltXStep> %d <FiltYStep> %d <ConnectFmap> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n    % (\n        cnn1_input_dim,\n        cnn1_output_dim,\n        o.cnn1_fmap_x_len,\n        o.cnn1_fmap_y_len,\n        o.cnn1_filt_x_len,\n        o.cnn1_filt_y_len,\n        o.cnn1_filt_x_step,\n        o.cnn1_filt_y_step,\n        o.cnn1_connect_fmap,\n        0.0,\n        0.0,\n        0.01,\n    )\n)\nconvolution_proto += (\n    ""<%sPooling2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <PoolXLen> %d <PoolYLen> %d <PoolXStep> %d <PoolYStep> %d\\n""\n    % (\n        o.pool1_type,\n        pool1_input_dim,\n        pool1_output_dim,\n        pool1_fmap_x_len,\n        pool1_fmap_y_len,\n        o.pool1_x_len,\n        o.pool1_y_len,\n        o.pool1_x_step,\n        o.pool1_y_step,\n    )\n)\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    pool1_output_dim,\n    pool1_output_dim,\n    1.0,\n)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    pool1_output_dim,\n    pool1_output_dim,\n    0.0,\n)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, pool1_output_dim, pool1_output_dim)\nconvolution_proto += (\n    ""<Convolutional2DComponent> <InputDim> %d <OutputDim> %d <FmapXLen> %d <FmapYLen> %d <FiltXLen> %d <FiltYLen> %d <FiltXStep> %d <FiltYStep> %d <ConnectFmap> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n    % (\n        cnn2_input_dim,\n        cnn2_output_dim,\n        cnn2_fmap_x_len,\n        cnn2_fmap_y_len,\n        o.cnn2_filt_x_len,\n        o.cnn2_filt_y_len,\n        o.cnn2_filt_x_step,\n        o.cnn2_filt_y_step,\n        o.cnn2_connect_fmap,\n        -2.0,\n        4.0,\n        0.1,\n    )\n)\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (cnn2_output_dim, cnn2_output_dim, 1.0)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    cnn2_output_dim,\n    cnn2_output_dim,\n    0.0,\n)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, cnn2_output_dim, cnn2_output_dim)\n\nif o.pitch_dim > 0:\n    # convolutional part\n    f_conv = open(""%s/nnet.proto.convolution"" % o.dirct, ""w"")\n    f_conv.write(""<NnetProto>\\n"")\n    f_conv.write(convolution_proto)\n    f_conv.write(""</NnetProto>\\n"")\n    f_conv.close()\n\n    # pitch part\n    f_pitch = open(""%s/nnet.proto.pitch"" % o.dirct, ""w"")\n    f_pitch.write(""<NnetProto>\\n"")\n    f_pitch.write(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n        % ((o.pitch_dim * (o.delta_order + 1) * (o.splice * 2 + 1)), o.num_pitch_neurons, -2.0, 4.0, 0.109375)\n    )\n    f_pitch.write(""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n    f_pitch.write(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n        % (o.num_pitch_neurons, o.num_pitch_neurons, -2.0, 4.0, 0.109375)\n    )\n    f_pitch.write(""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n    f_pitch.write(""</NnetProto>\\n"")\n    f_pitch.close()\n\n    # paralell part\n    vector = """"\n    for i in range(\n        1, (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1), feat_raw_dim + o.pitch_dim\n    ):\n        vector += ""%d:1:%d "" % (i, i + feat_raw_dim - 1)\n    for i in range(\n        feat_raw_dim + 1,\n        (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n        feat_raw_dim + o.pitch_dim,\n    ):\n        vector += ""%d:1:%d "" % (i, i + o.pitch_dim - 1)\n    print(\n        ""<Copy> <InputDim> %d <OutputDim> %d <BuildVector>  %s </BuildVector> ""\n        % (\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            vector,\n        )\n    )\n    print(\n        ""<ParallelComponent> <InputDim> %d <OutputDim> %d <NestedNnetProto> %s %s </NestedNnetProto>""\n        % (\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            o.num_pitch_neurons + cnn2_output_dim,\n            ""%s/nnet.proto.convolution"" % o.dirct,\n            ""%s/nnet.proto.pitch"" % o.dirct,\n        )\n    )\n\n    num_convolution_output = o.num_pitch_neurons + cnn2_output_dim\nelse:  # no pitch\n    print(convolution_proto)\n\n# We are done!\nsys.exit(0)\n'"
kaldi_decoding_scripts/utils/nnet/make_cnn_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014  Brno University of Technology (author: Katerina Zmolikova, Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\nfrom __future__ import print_function\n\nimport math, random, sys\nfrom optparse import OptionParser\n\n###\n### Parse options\n###\nusage = ""%prog [options] <feat-dim> <num-leaves> <num-hidden-layers> <num-hidden-neurons>  >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\n    ""--activation-type"",\n    dest=""activation_type"",\n    help=""Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]"",\n    default=""<Sigmoid>"",\n    type=""string"",\n)\nparser.add_option(\n    ""--num-filters1"",\n    dest=""num_filters1"",\n    help=""Number of filters in first convolutional layer [default: %default]"",\n    default=128,\n    type=""int"",\n)\nparser.add_option(\n    ""--num-filters2"",\n    dest=""num_filters2"",\n    help=""Number of filters in second convolutional layer [default: %default]"",\n    default=256,\n    type=""int"",\n)\nparser.add_option(""--pool-size"", dest=""pool_size"", help=""Size of pooling [default: %default]"", default=3, type=""int"")\nparser.add_option(""--pool-step"", dest=""pool_step"", help=""Step of pooling [default: %default]"", default=3, type=""int"")\nparser.add_option(\n    ""--pool-type"",\n    dest=""pool_type"",\n    help=""Type of pooling (Max || Average) [default: %default]"",\n    default=""Max"",\n    type=""string"",\n)\nparser.add_option(\n    ""--pitch-dim"",\n    dest=""pitch_dim"",\n    help=""Number of features representing pitch [default: %default]"",\n    default=0,\n    type=""int"",\n)\nparser.add_option(\n    ""--delta-order"", dest=""delta_order"", help=""Order of delta features [default: %default]"", default=2, type=""int""\n)\nparser.add_option(""--splice"", dest=""splice"", help=""Length of splice [default: %default]"", default=5, type=""int"")\nparser.add_option(\n    ""--patch-step1"",\n    dest=""patch_step1"",\n    help=""Patch step of first convolutional layer [default: %default]"",\n    default=1,\n    type=""int"",\n)\nparser.add_option(\n    ""--patch-dim1"",\n    dest=""patch_dim1"",\n    help=""Dim of convolutional kernel in 1st layer (freq. axis) [default: %default]"",\n    default=8,\n    type=""int"",\n)\nparser.add_option(\n    ""--patch-dim2"",\n    dest=""patch_dim2"",\n    help=""Dim of convolutional kernel in 2nd layer (freq. axis) [default: %default]"",\n    default=4,\n    type=""int"",\n)\nparser.add_option(\n    ""--dir"",\n    dest=""protodir"",\n    help=""Directory, where network prototypes will be saved [default: %default]"",\n    default=""."",\n    type=""string"",\n)\nparser.add_option(\n    ""--num-pitch-neurons"",\n    dest=""num_pitch_neurons"",\n    help=""Number of neurons in layers processing pitch features [default: %default]"",\n    default=""200"",\n    type=""int"",\n)\n\n(o, args) = parser.parse_args()\nif len(args) != 1:\n    parser.print_help()\n    sys.exit(1)\n\nfeat_dim = int(args[0])\n### End parse options\n\nfeat_raw_dim = (\n    feat_dim / (o.delta_order + 1) / (o.splice * 2 + 1) - o.pitch_dim\n)  # we need number of feats without deltas and splice and pitch\n\n# Check\nassert feat_dim > 0\nassert o.pool_type == ""Max"" or o.pool_type == ""Average""\n\n###\n### Print prototype of the network\n###\n\n# Begin the prototype\nprint(""<NnetProto>"")\n\n# Convolutional part of network\nnum_patch1 = 1 + (feat_raw_dim - o.patch_dim1) / o.patch_step1\nnum_pool = 1 + (num_patch1 - o.pool_size) / o.pool_step\npatch_dim2 = o.patch_dim2 * o.num_filters1\npatch_step2 = o.num_filters1\npatch_stride2 = num_pool * o.num_filters1  # same as layer1 outputs\nnum_patch2 = 1 + (num_pool * o.num_filters1 - patch_dim2) / patch_step2\n\nconvolution_proto = """"\n\nconvolution_proto += (\n    ""<ConvolutionalComponent> <InputDim> %d <OutputDim> %d <PatchDim> %d <PatchStep> %d <PatchStride> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f\\n""\n    % (\n        feat_raw_dim * (o.delta_order + 1) * (o.splice * 2 + 1),\n        o.num_filters1 * num_patch1,\n        o.patch_dim1,\n        o.patch_step1,\n        feat_raw_dim,\n        -1.0,\n        2.0,\n        0.02,\n        30,\n    )\n)  # ~8x11x3 = 264 inputs\nconvolution_proto += (\n    ""<%sPoolingComponent> <InputDim> %d <OutputDim> %d <PoolSize> %d <PoolStep> %d <PoolStride> %d\\n""\n    % (o.pool_type, o.num_filters1 * num_patch1, o.num_filters1 * num_pool, o.pool_size, o.pool_step, o.num_filters1)\n)\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    o.num_filters1 * num_pool,\n    o.num_filters1 * num_pool,\n    1,\n)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    o.num_filters1 * num_pool,\n    o.num_filters1 * num_pool,\n    0,\n)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % (\n    o.activation_type,\n    o.num_filters1 * num_pool,\n    o.num_filters1 * num_pool,\n)\nconvolution_proto += (\n    ""<ConvolutionalComponent> <InputDim> %d <OutputDim> %d <PatchDim> %d <PatchStep> %d <PatchStride> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f\\n""\n    % (\n        o.num_filters1 * num_pool,\n        o.num_filters2 * num_patch2,\n        patch_dim2,\n        patch_step2,\n        patch_stride2,\n        -2.0,\n        4.0,\n        0.1,\n        50,\n    )\n)  # ~4x128 = 512 inputs\nconvolution_proto += ""<Rescale> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    o.num_filters2 * num_patch2,\n    o.num_filters2 * num_patch2,\n    1,\n)\nconvolution_proto += ""<AddShift> <InputDim> %d <OutputDim> %d <InitParam> %f\\n"" % (\n    o.num_filters2 * num_patch2,\n    o.num_filters2 * num_patch2,\n    0,\n)\nconvolution_proto += ""%s <InputDim> %d <OutputDim> %d\\n"" % (\n    o.activation_type,\n    o.num_filters2 * num_patch2,\n    o.num_filters2 * num_patch2,\n)\n\nif o.pitch_dim > 0:\n    # convolutional part\n    f_conv = open(""%s/nnet.proto.convolution"" % o.protodir, ""w"")\n    f_conv.write(""<NnetProto>\\n"")\n    f_conv.write(convolution_proto)\n    f_conv.write(""</NnetProto>\\n"")\n    f_conv.close()\n\n    # pitch part\n    f_pitch = open(""%s/nnet.proto.pitch"" % o.protodir, ""w"")\n    f_pitch.write(""<NnetProto>\\n"")\n    f_pitch.write(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n        % ((o.pitch_dim * (o.delta_order + 1) * (o.splice * 2 + 1)), o.num_pitch_neurons, -2, 4, 0.02)\n    )\n    f_pitch.write(""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n    f_pitch.write(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f\\n""\n        % (o.num_pitch_neurons, o.num_pitch_neurons, -2, 4, 0.1)\n    )\n    f_pitch.write(""%s <InputDim> %d <OutputDim> %d\\n"" % (o.activation_type, o.num_pitch_neurons, o.num_pitch_neurons))\n    f_pitch.write(""</NnetProto>\\n"")\n    f_pitch.close()\n\n    # parallel part\n    vector = """"\n    for i in range(\n        1, (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1), feat_raw_dim + o.pitch_dim\n    ):\n        vector += ""%d:1:%d "" % (i, i + feat_raw_dim - 1)\n    for i in range(\n        feat_raw_dim + 1,\n        (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n        feat_raw_dim + o.pitch_dim,\n    ):\n        vector += ""%d:1:%d "" % (i, i + o.pitch_dim - 1)\n    print(\n        ""<Copy> <InputDim> %d <OutputDim> %d <BuildVector> %s </BuildVector>""\n        % (\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            vector,\n        )\n    )\n    print(\n        ""<ParallelComponent> <InputDim> %d <OutputDim> %d <NestedNnetProto> %s %s </NestedNnetProto>""\n        % (\n            (feat_raw_dim + o.pitch_dim) * (o.delta_order + 1) * (o.splice * 2 + 1),\n            o.num_pitch_neurons + o.num_filters2 * num_patch2,\n            ""%s/nnet.proto.convolution"" % o.protodir,\n            ""%s/nnet.proto.pitch"" % o.protodir,\n        )\n    )\n\nelse:  # no pitch\n    print(convolution_proto)\n\n# We are done!\nsys.exit(0)\n'"
kaldi_decoding_scripts/utils/nnet/make_lstm_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2015  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\nfrom __future__ import print_function\n\nimport sys\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\n\nusage = ""%prog [options] <feat-dim> <num-leaves> >nnet-proto-file""\nparser = OptionParser(usage)\n#\nparser.add_option(\n    ""--num-cells"", dest=""num_cells"", type=""int"", default=800, help=""Number of LSTM cells [default: %default]""\n)\nparser.add_option(\n    ""--num-recurrent"",\n    dest=""num_recurrent"",\n    type=""int"",\n    default=512,\n    help=""Number of LSTM recurrent units [default: %default]"",\n)\nparser.add_option(\n    ""--num-layers"", dest=""num_layers"", type=""int"", default=2, help=""Number of LSTM layers [default: %default]""\n)\nparser.add_option(\n    ""--lstm-stddev-factor"",\n    dest=""lstm_stddev_factor"",\n    type=""float"",\n    default=0.01,\n    help=""Standard deviation of initialization [default: %default]"",\n)\nparser.add_option(\n    ""--param-stddev-factor"",\n    dest=""param_stddev_factor"",\n    type=""float"",\n    default=0.04,\n    help=""Standard deviation in output layer [default: %default]"",\n)\nparser.add_option(\n    ""--clip-gradient"",\n    dest=""clip_gradient"",\n    type=""float"",\n    default=5.0,\n    help=""Clipping constant applied to gradients [default: %default]"",\n)\n#\n(o, args) = parser.parse_args()\nif len(args) != 2:\n    parser.print_help()\n    sys.exit(1)\n\n(feat_dim, num_leaves) = list(map(int, args))\n\n# Original prototype from Jiayu,\n# <NnetProto>\n# <Transmit> <InputDim> 40 <OutputDim> 40\n# <LstmProjectedStreams> <InputDim> 40 <OutputDim> 512 <CellDim> 800 <ParamScale> 0.01 <NumStream> 4\n# <AffineTransform> <InputDim> 512 <OutputDim> 8000 <BiasMean> 0.000000 <BiasRange> 0.000000 <ParamStddev> 0.04\n# <Softmax> <InputDim> 8000 <OutputDim> 8000\n# </NnetProto>\n\nprint(""<NnetProto>"")\n# normally we won\'t use more than 2 layers of LSTM\nif o.num_layers == 1:\n    print(\n        ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\nelif o.num_layers == 2:\n    print(\n        ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (feat_dim, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\n    print(\n        ""<LstmProjectedStreams> <InputDim> %d <OutputDim> %d <CellDim> %s <ParamScale> %f <ClipGradient> %f""\n        % (o.num_recurrent, o.num_recurrent, o.num_cells, o.lstm_stddev_factor, o.clip_gradient)\n    )\nelse:\n    sys.stderr.write(""make_lstm_proto.py ERROR: more than 2 layers of LSTM, not supported yet.\\n"")\n    sys.exit(1)\nprint(\n    ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> 0.0 <BiasRange> 0.0 <ParamStddev> %f""\n    % (o.num_recurrent, num_leaves, o.param_stddev_factor)\n)\nprint(""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves))\nprint(""</NnetProto>"")\n'"
kaldi_decoding_scripts/utils/nnet/make_nnet_proto.py,0,"b'#!/usr/bin/env python\n\n# Copyright 2014  Brno University of Technology (author: Karel Vesely)\n\n# Licensed under the Apache License, Version 2.0 (the ""License"");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#  http://www.apache.org/licenses/LICENSE-2.0\n#\n# THIS CODE IS PROVIDED *AS IS* BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, EITHER EXPRESS OR IMPLIED, INCLUDING WITHOUT LIMITATION ANY IMPLIED\n# WARRANTIES OR CONDITIONS OF TITLE, FITNESS FOR A PARTICULAR PURPOSE,\n# MERCHANTABLITY OR NON-INFRINGEMENT.\n# See the Apache 2 License for the specific language governing permissions and\n# limitations under the License.\n\n# Generated Nnet prototype, to be initialized by \'nnet-initialize\'.\nfrom __future__ import print_function\n\nimport math, random, sys, re\n\n###\n### Parse options\n###\nfrom optparse import OptionParser\n\nusage = ""%prog [options] <feat-dim> <num-leaves> <num-hid-layers> <num-hid-neurons> >nnet-proto-file""\nparser = OptionParser(usage)\n\nparser.add_option(\n    ""--no-proto-head"",\n    dest=""with_proto_head"",\n    help=""Do not put <NnetProto> head-tag in the prototype [default: %default]"",\n    default=True,\n    action=""store_false"",\n)\nparser.add_option(\n    ""--no-softmax"",\n    dest=""with_softmax"",\n    help=""Do not put <SoftMax> in the prototype [default: %default]"",\n    default=True,\n    action=""store_false"",\n)\nparser.add_option(\n    ""--block-softmax-dims"",\n    dest=""block_softmax_dims"",\n    help=""Generate <BlockSoftmax> with dims D1:D2:D3 [default: %default]"",\n    default="""",\n    type=""string"",\n)\nparser.add_option(\n    ""--activation-type"",\n    dest=""activation_type"",\n    help=""Select type of activation function : (<Sigmoid>|<Tanh>) [default: %default]"",\n    default=""<Sigmoid>"",\n    type=""string"",\n)\nparser.add_option(\n    ""--hid-bias-mean"",\n    dest=""hid_bias_mean"",\n    help=""Set bias for hidden activations [default: %default]"",\n    default=-2.0,\n    type=""float"",\n)\nparser.add_option(\n    ""--hid-bias-range"",\n    dest=""hid_bias_range"",\n    help=""Set bias range for hidden activations (+/- 1/2 range around mean) [default: %default]"",\n    default=4.0,\n    type=""float"",\n)\nparser.add_option(\n    ""--param-stddev-factor"",\n    dest=""param_stddev_factor"",\n    help=""Factor to rescale Normal distriburtion for initalizing weight matrices [default: %default]"",\n    default=0.1,\n    type=""float"",\n)\nparser.add_option(\n    ""--bottleneck-dim"",\n    dest=""bottleneck_dim"",\n    help=""Make bottleneck network with desired bn-dim (0 = no bottleneck) [default: %default]"",\n    default=0,\n    type=""int"",\n)\nparser.add_option(\n    ""--no-glorot-scaled-stddev"",\n    dest=""with_glorot"",\n    help=""Generate normalized weights according to X.Glorot paper, but mapping U->N with same variance (factor sqrt(x/(dim_in+dim_out)))"",\n    action=""store_false"",\n    default=True,\n)\nparser.add_option(\n    ""--no-smaller-input-weights"",\n    dest=""smaller_input_weights"",\n    help=""Disable 1/12 reduction of stddef in input layer [default: %default]"",\n    action=""store_false"",\n    default=True,\n)\nparser.add_option(\n    ""--no-bottleneck-trick"",\n    dest=""bottleneck_trick"",\n    help=""Disable smaller initial weights and learning rate around bottleneck"",\n    action=""store_false"",\n    default=True,\n)\nparser.add_option(\n    ""--max-norm"",\n    dest=""max_norm"",\n    help=""Max radius of neuron-weights in L2 space (if longer weights get shrinked, not applied to last layer, 0.0 = disable) [default: %default]"",\n    default=0.0,\n    type=""float"",\n)\n\n\n(o, args) = parser.parse_args()\nif len(args) != 4:\n    parser.print_help()\n    sys.exit(1)\n\n(feat_dim, num_leaves, num_hid_layers, num_hid_neurons) = list(map(int, args))\n### End parse options\n\n\n# Check\nassert feat_dim > 0\nassert num_leaves > 0\nassert num_hid_layers >= 0\nassert num_hid_neurons > 0\nif o.block_softmax_dims:\n    assert sum(map(int, re.split(""[,:]"", o.block_softmax_dims))) == num_leaves  # posible separators : \',\' \':\'\n\n# Optionaly scale\ndef Glorot(dim1, dim2):\n    if o.with_glorot:\n        # 35.0 = magic number, gives ~1.0 in inner layers for hid-dim 1024dim,\n        return 35.0 * math.sqrt(2.0 / (dim1 + dim2))\n    else:\n        return 1.0\n\n\n###\n### Print prototype of the network\n###\n\n# NO HIDDEN LAYER, ADDING BOTTLENECK!\n# No hidden layer while adding bottleneck means:\n# - add bottleneck layer + hidden layer + output layer\nif num_hid_layers == 0 and o.bottleneck_dim != 0:\n    assert o.bottleneck_dim > 0\n    assert num_hid_layers == 0\n    if o.with_proto_head:\n        print(""<NnetProto>"")\n    if o.bottleneck_trick:\n        # 25% smaller stddev -> small bottleneck range, 10x smaller learning rate\n        print(\n            ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f <LearnRateCoef> %f""\n            % (feat_dim, o.bottleneck_dim, (o.param_stddev_factor * Glorot(feat_dim, o.bottleneck_dim) * 0.75), 0.1)\n        )\n        # 25% smaller stddev -> smaller gradient in prev. layer, 10x smaller learning rate for weigts & biases\n        print(\n            ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f <MaxNorm> %f""\n            % (\n                o.bottleneck_dim,\n                num_hid_neurons,\n                o.hid_bias_mean,\n                o.hid_bias_range,\n                (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons) * 0.75),\n                0.1,\n                0.1,\n                o.max_norm,\n            )\n        )\n    else:\n        print(\n            ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f""\n            % (feat_dim, o.bottleneck_dim, (o.param_stddev_factor * Glorot(feat_dim, o.bottleneck_dim)))\n        )\n        print(\n            ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f""\n            % (\n                o.bottleneck_dim,\n                num_hid_neurons,\n                o.hid_bias_mean,\n                o.hid_bias_range,\n                (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons)),\n                o.max_norm,\n            )\n        )\n    print(""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons))  # Non-linearity\n    # Last AffineTransform (10x smaller learning rate on bias)\n    print(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f""\n        % (\n            num_hid_neurons,\n            num_leaves,\n            0.0,\n            0.0,\n            (o.param_stddev_factor * Glorot(num_hid_neurons, num_leaves)),\n            1.0,\n            0.1,\n        )\n    )\n    # Optionaly append softmax\n    if o.with_softmax:\n        if o.block_softmax_dims == """":\n            print(""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves))\n        else:\n            print(\n                ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s""\n                % (num_leaves, num_leaves, o.block_softmax_dims)\n            )\n    print(""</NnetProto>"")\n    # We are done!\n    sys.exit(0)\n\n# NO HIDDEN LAYERS!\n# Add only last layer (logistic regression)\nif num_hid_layers == 0:\n    if o.with_proto_head:\n        print(""<NnetProto>"")\n    print(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f""\n        % (feat_dim, num_leaves, 0.0, 0.0, (o.param_stddev_factor * Glorot(feat_dim, num_leaves)))\n    )\n    if o.with_softmax:\n        if o.block_softmax_dims == """":\n            print(""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves))\n        else:\n            print(\n                ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s""\n                % (num_leaves, num_leaves, o.block_softmax_dims)\n            )\n    print(""</NnetProto>"")\n    # We are done!\n    sys.exit(0)\n\n\n# THE USUAL DNN PROTOTYPE STARTS HERE!\n# Assuming we have >0 hidden layers,\nassert num_hid_layers > 0\n\n# Begin the prototype,\nif o.with_proto_head:\n    print(""<NnetProto>"")\n\n# First AffineTranform,\nprint(\n    ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f""\n    % (\n        feat_dim,\n        num_hid_neurons,\n        o.hid_bias_mean,\n        o.hid_bias_range,\n        (\n            o.param_stddev_factor\n            * Glorot(feat_dim, num_hid_neurons)\n            * (math.sqrt(1.0 / 12.0) if o.smaller_input_weights else 1.0)\n        ),\n        o.max_norm,\n    )\n)\n# Note.: compensating dynamic range mismatch between input features and Sigmoid-hidden layers,\n# i.e. mapping the std-dev of N(0,1) (input features) to std-dev of U[0,1] (sigmoid-outputs).\n# This is done by multiplying with stddev(U[0,1]) = sqrt(1/12).\n# The stddev of weights is consequently reduced by 0.29x.\nprint(""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons))\n\n# Internal AffineTransforms,\nfor i in range(num_hid_layers - 1):\n    print(\n        ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f""\n        % (\n            num_hid_neurons,\n            num_hid_neurons,\n            o.hid_bias_mean,\n            o.hid_bias_range,\n            (o.param_stddev_factor * Glorot(num_hid_neurons, num_hid_neurons)),\n            o.max_norm,\n        )\n    )\n    print(""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons))\n\n# Optionaly add bottleneck,\nif o.bottleneck_dim != 0:\n    assert o.bottleneck_dim > 0\n    if o.bottleneck_trick:\n        # 25% smaller stddev -> small bottleneck range, 10x smaller learning rate\n        print(\n            ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f <LearnRateCoef> %f""\n            % (\n                num_hid_neurons,\n                o.bottleneck_dim,\n                (o.param_stddev_factor * Glorot(num_hid_neurons, o.bottleneck_dim) * 0.75),\n                0.1,\n            )\n        )\n        # 25% smaller stddev -> smaller gradient in prev. layer, 10x smaller learning rate for weigts & biases\n        print(\n            ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f <MaxNorm> %f""\n            % (\n                o.bottleneck_dim,\n                num_hid_neurons,\n                o.hid_bias_mean,\n                o.hid_bias_range,\n                (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons) * 0.75),\n                0.1,\n                0.1,\n                o.max_norm,\n            )\n        )\n    else:\n        # Same learninig-rate and stddev-formula everywhere,\n        print(\n            ""<LinearTransform> <InputDim> %d <OutputDim> %d <ParamStddev> %f""\n            % (num_hid_neurons, o.bottleneck_dim, (o.param_stddev_factor * Glorot(num_hid_neurons, o.bottleneck_dim)))\n        )\n        print(\n            ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <MaxNorm> %f""\n            % (\n                o.bottleneck_dim,\n                num_hid_neurons,\n                o.hid_bias_mean,\n                o.hid_bias_range,\n                (o.param_stddev_factor * Glorot(o.bottleneck_dim, num_hid_neurons)),\n                o.max_norm,\n            )\n        )\n    print(""%s <InputDim> %d <OutputDim> %d"" % (o.activation_type, num_hid_neurons, num_hid_neurons))\n\n# Last AffineTransform (10x smaller learning rate on bias)\nprint(\n    ""<AffineTransform> <InputDim> %d <OutputDim> %d <BiasMean> %f <BiasRange> %f <ParamStddev> %f <LearnRateCoef> %f <BiasLearnRateCoef> %f""\n    % (num_hid_neurons, num_leaves, 0.0, 0.0, (o.param_stddev_factor * Glorot(num_hid_neurons, num_leaves)), 1.0, 0.1)\n)\n\n# Optionaly append softmax\nif o.with_softmax:\n    if o.block_softmax_dims == """":\n        print(""<Softmax> <InputDim> %d <OutputDim> %d"" % (num_leaves, num_leaves))\n    else:\n        print(\n            ""<BlockSoftmax> <InputDim> %d <OutputDim> %d <BlockDims> %s""\n            % (num_leaves, num_leaves, o.block_softmax_dims)\n        )\n\n# End the prototype\nprint(""</NnetProto>"")\n\n# We are done!\nsys.exit(0)\n'"
