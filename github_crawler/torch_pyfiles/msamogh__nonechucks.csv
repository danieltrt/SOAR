file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\n\nsetup(\n    name=""nonechucks"",\n    version=""0.4.0"",\n    url=""https://github.com/msamogh/nonechucks"",\n    license=""MIT"",\n    author=""Amogh Mannekote"",\n    author_email=""msamogh@gmail.com"",\n    description=""""""nonechucks is a library that provides wrappers for """"""\n    + """"""PyTorch\'s datasets, samplers, and transforms to """"""\n    + """"""allow for dropping unwanted or invalid samples """"""\n    + """"""dynamically."""""",\n    install_requires=[""future""],\n    packages=find_packages(),\n    long_description=open(""README.md"").read(),\n    long_description_content_type=""text/markdown"",\n    zip_safe=False,\n)\n'"
nonechucks/__init__.py,6,"b'import logging\n\nimport torch\nimport torch.utils.data\n\n\nlogger = logging.getLogger(__name__)\n\n\ndef _get_pytorch_version():\n    version = torch.__version__\n    major, minor = [int(x) for x in version.split(""."")[:2]]\n    if major != 1:\n        raise RuntimeError(\n            ""nonechucks only supports PyTorch major version 1 at the moment.""\n        )\n    if minor > 2:\n        logger.warn(\n            ""nonechucks may not work properly with this version of PyTorch ({}). ""\n            ""It has only been tested on PyTorch versions 1.0, 1.1, and 1.2"".format(\n                version\n            )\n        )\n    return major, minor\n\n\nMAJOR, MINOR = _get_pytorch_version()\n\nif MINOR > 1:\n    SingleProcessDataLoaderIter = (\n        torch.utils.data.dataloader._SingleProcessDataLoaderIter\n    )\n    MultiProcessingDataLoaderIter = (\n        torch.utils.data.dataloader._MultiProcessingDataLoaderIter\n    )\nelse:\n    SingleProcessDataLoaderIter = torch.utils.data.dataloader._DataLoaderIter\n    MultiProcessingDataLoaderIter = torch.utils.data.dataloader._DataLoaderIter\n\n\nfrom nonechucks.dataset import SafeDataset\nfrom nonechucks.sampler import SafeSampler\nfrom nonechucks.dataloader import SafeDataLoader\n'"
nonechucks/dataloader.py,3,"b'from functools import partial\nfrom future.utils import with_metaclass\n\nimport torch.utils.data as data\n\ntry:\n    from torch.utils.data.dataloader import default_collate\nexcept ImportError:\n    from torch.utils.data._utils.collate import default_collate\n\nfrom nonechucks import SingleProcessDataLoaderIter, MultiProcessingDataLoaderIter\nfrom nonechucks.dataset import SafeDataset\nfrom nonechucks.sampler import SafeSampler\nfrom nonechucks.utils import batch_len, collate_batches, slice_batch\n\n\nclass _SafeDataLoaderCaller(type):\n    """"""Metaclass that overrides the __call__ method to replace\n    `SequentialSampler` and `RandomSampler` with their corresponding\n    `SafeSampler`s in DataLoader\'s namespace.\n    """"""\n\n    def __call__(cls, *args, **kwargs):\n        cls._replace_default_samplers()\n        obj = type.__call__(cls, *args, **kwargs)\n        cls._restore_default_samplers()\n        return obj\n\n    def _replace_default_samplers(cls):\n        cls.sequential = data.dataloader.SequentialSampler\n        cls.random = data.dataloader.RandomSampler\n\n        def safe_sampler_callable(sampler_cls, dataset):\n            return SafeSampler(dataset, sampler_cls(dataset))\n\n        data.dataloader.SequentialSampler = partial(\n            safe_sampler_callable, data.SequentialSampler\n        )\n        data.dataloader.RandomSampler = partial(\n            safe_sampler_callable, data.RandomSampler\n        )\n\n    def _restore_default_samplers(cls):\n        data.dataloader.SequentialSampler = cls.sequential\n        data.dataloader.RandomSampler = cls.random\n\n\nclass _SafeDataLoaderIter(MultiProcessingDataLoaderIter):\n    def __init__(self, loader):\n        super().__init__(loader)\n        self.batch_size = loader.batch_size\n        self.coalescing_in_progress = False\n        self.drop_last = loader.drop_last_original\n        if isinstance(loader.sampler, SafeSampler):\n            self.step_to_index_fn = loader.sampler.step_to_index_fn\n        else:\n            self.step_to_index_fn = SafeSampler.default_step_to_index_fn\n\n    def _process_next_batch(self, curr_batch):\n        """"""Fills an incomplete batch if necessary before processing it.""""""\n        super()._process_next_batch(curr_batch)\n        if self.coalescing_in_progress:\n            return curr_batch\n\n        # When set to True, _process_next_batch simply returns an incomplete\n        # batch instead of trying to fill it up to a length of batch_size.\n        #\n        # Stays True until the current batch is filled so that nested calls for\n        # _process_next_batch from within the loop simply call the parent\'s\n        # version of the _process_next_batch.\n        self.coalescing_in_progress = True\n        n_empty_slots = self.batch_size - batch_len(curr_batch)\n        while n_empty_slots > 0:\n            # check if curr_batch is the final batch\n            if self.batches_outstanding == 0 and not self.reorder_dict:\n                if (not self.drop_last) or (batch_len(curr_batch) == self.batch_size):\n                    return curr_batch\n\n            # raises StopIteration if no more elements left, which exits the\n            # loop\n            next_batch = next(self)\n            if len(next_batch) == 0:\n                super()._process_next_batch(next_batch)\n                continue\n            elif len(next_batch) > n_empty_slots:\n                # Take only n_empty_slots number of samples from next_batch.\n                # The remaining elements of next_batch are added back into the\n                # dict for future consumption.\n                self.rcvd_idx -= 1\n                curr_batch = collate_batches(\n                    [curr_batch, slice_batch(next_batch, end=n_empty_slots)]\n                )\n                self.reorder_dict[self.rcvd_idx] = slice_batch(\n                    next_batch, start=n_empty_slots\n                )\n            else:\n                curr_batch = collate_batches([curr_batch, next_batch])\n\n            n_empty_slots -= min(n_empty_slots, batch_len(next_batch))\n        self.coalescing_in_progress = False\n        return curr_batch\n\n\nclass _OriginalDataset(SafeDataset):\n    """"""Wraps a SafeDataset to return None for invalid samples.""""""\n\n    def __init__(self, safe_dataset):\n        self.safe_dataset = safe_dataset\n\n    def __getitem__(self, idx):\n        return self.safe_dataset._safe_get_item(idx)\n\n\nclass SafeDataLoader(with_metaclass(_SafeDataLoaderCaller, data.DataLoader)):\n    """"""A DataLoader that reverts to safe versions of `SequentialSampler` and\n    `RandomSampler` when no default sampler is specified.\n    """"""\n\n    @staticmethod\n    def _safe_default_collate(batch):\n        filtered_batch = [x for x in batch if x is not None]\n        if len(filtered_batch) == 0:\n            return []\n        return default_collate(filtered_batch)\n\n    def __init__(self, dataset, **kwargs):\n        # drop_last is handled transparently by _SafeDataLoaderIter (bypassing\n        # DataLoader). Since drop_last cannot be changed after initializing the\n        # DataLoader instance, it needs to be intercepted here.\n        assert isinstance(\n            dataset, SafeDataset\n        ), ""dataset must be an instance of SafeDataset.""\n\n        self.drop_last_original = False\n        if ""drop_last"" in kwargs:\n            self.drop_last_original = kwargs[""drop_last""]\n            kwargs[""drop_last""] = False\n        super(SafeDataLoader, self).__init__(dataset, **kwargs)\n\n        self.safe_dataset = self.dataset\n        self.dataset = _OriginalDataset(self.safe_dataset)\n\n        if self.collate_fn is default_collate:\n            self.collate_fn = SafeDataLoader._safe_default_collate\n\n    def __iter__(self):\n        if self.num_workers > 0:\n            return _SafeDataLoaderIter(self)\n        return SingleProcessDataLoaderIter(self)\n'"
nonechucks/dataset.py,3,"b'import torch\nimport torch.utils.data\n\nfrom nonechucks.utils import memoize\n\n\nclass SafeDataset(torch.utils.data.Dataset):\n    """"""A wrapper around a torch.utils.data.Dataset that allows dropping\n    samples dynamically.\n    """"""\n\n    def __init__(self, dataset, eager_eval=False):\n        """"""Creates a `SafeDataset` wrapper around `dataset`.""""""\n        self.dataset = dataset\n        self.eager_eval = eager_eval\n        # These will contain indices over the original dataset. The indices of\n        # the safe samples will go into _safe_indices and similarly for unsafe\n        # samples.\n        self._safe_indices = []\n        self._unsafe_indices = []\n\n        # If eager_eval is True, we can simply go ahead and build the index\n        # by attempting to access every sample in self.dataset.\n        if self.eager_eval is True:\n            self._build_index()\n\n    def _safe_get_item(self, idx):\n        """"""Returns None instead of throwing an error when dealing with an\n        unsafe sample, and also builds an index of safe and unsafe samples as\n        and when they get accessed.\n        """"""\n        try:\n            # differentiates IndexError occuring here from one occuring during\n            # sample loading\n            invalid_idx = False\n            if idx >= len(self.dataset):\n                invalid_idx = True\n                raise IndexError\n            sample = self.dataset[idx]\n            if idx not in self._safe_indices:\n                self._safe_indices.append(idx)\n            return sample\n        except Exception as e:\n            if isinstance(e, IndexError):\n                if invalid_idx:\n                    raise\n            if idx not in self._unsafe_indices:\n                self._unsafe_indices.append(idx)\n            return None\n\n    def _build_index(self):\n        for idx in range(len(self.dataset)):\n            # The returned sample is deliberately discarded because\n            # self._safe_get_item(idx) is called only to classify every index\n            # into either safe_samples_indices or _unsafe_samples_indices.\n            _ = self._safe_get_item(idx)\n\n    def _reset_index(self):\n        """"""Resets the safe and unsafe samples indices.""""""\n        self._safe_indices = self._unsafe_indices = []\n\n    @property\n    def is_index_built(self):\n        """"""Returns True if all indices of the original dataset have been\n        classified into safe_samples_indices or _unsafe_samples_indices.\n        """"""\n        return len(self.dataset) == len(self._safe_indices) + len(self._unsafe_indices)\n\n    @property\n    def num_samples_examined(self):\n        return len(self._safe_indices) + len(self._unsafe_indices)\n\n    def __len__(self):\n        """"""Returns the length of the original dataset.\n        NOTE: This is different from the number of actually valid samples.\n        """"""\n        return len(self.dataset)\n\n    def __iter__(self):\n        return (\n            self._safe_get_item(i)\n            for i in range(len(self))\n            if self._safe_get_item(i) is not None\n        )\n\n    @memoize\n    def __getitem__(self, idx):\n        """"""Behaves like the standard __getitem__ for Dataset when the index\n        has been built.\n        """"""\n        while idx < len(self.dataset):\n            sample = self._safe_get_item(idx)\n            if sample is not None:\n                return sample\n            idx += 1\n        raise IndexError\n\n    def __getattr__(self, key):\n        """"""Delegates to original dataset object if an attribute is not\n        found in this class.\n        """"""\n        return getattr(self.dataset, key)\n'"
nonechucks/sampler.py,3,"b'import torch\nimport torch.utils.data\n\nfrom nonechucks.dataset import SafeDataset\n\n\nclass SafeSampler(torch.utils.data.sampler.Sampler):\n    """"""SafeSampler can be used both as a standard Sampler (over a Dataset),\n    or as a wrapper around an existing `Sampler` instance. It allows you to\n    drop unwanted samples while sampling.\n    """"""\n\n    @staticmethod\n    def default_step_to_index_fn(original_idx, actual_idx):\n        return actual_idx\n\n    def __init__(self, dataset, sampler=None, step_to_index_fn=None):\n        """"""Create a `SafeSampler` instance that performs sampling over either\n        another sampler object or directly over a dataset. `step_to_index_fn`\n        will define the `SafeSampler` instance\'s behavior when it encounters\n        an unsafe sample.\n\n        When a `Sampler` object is passed to `sampler\', the returned\n        `SafeSampler` instance performs sampling over the list of indices\n        sampled by `sampler`, acting as a wrapper over it. If `sampler` is\n        `None`, the `SafeSampler` instance that is returned performs sampling\n        directly over the `dataset` object.\n\n        Arguments:\n            dataset (SafeDataset): The dataset to be sampled.\n            sampler (Sampler, optional): If `sampler` is `None`, the sampling\n                is performed directly over the `dataset`, otherwise it\'s done\n                over the list of indices returned by `sampler`\'s `__iter__`\n                method.\n                If `sampler` takes a `Dataset` object as a parameter,\n                `dataset` should ideally be the same as the one passed to\n                `sampler`.\n            step_to_index_fn (function, optional): Function that takes in 2\n                arguments - (`num_valid_samples` and `num_samples_examined`),\n                and returns the next index to be sampled. If None or not\n                specified, the default function returns the\n                `num_samples_examined` as the output.\n        """"""\n        assert isinstance(\n            dataset, SafeDataset\n        ), ""dataset must be an instance of SafeDataset.""\n        self.dataset = dataset\n\n        if sampler is None:\n            sampler = torch.utils.data.sampler.SequentialSampler(dataset)\n        self.sampler = sampler\n\n        if step_to_index_fn is None:\n            step_to_index_fn = SafeSampler.default_step_to_index_fn\n        self.step_to_index_fn = step_to_index_fn\n\n    def __iter__(self):\n        """"""Return iterator over sampled indices.""""""\n        if self.sampler is not None:\n            self.sampler_indices = list(iter(self.sampler))\n\n        self.num_valid_samples = self.num_samples_examined = 0\n        return self\n\n    def _get_next_index(self):\n        """"""Helper function that calls `step_to_index_fn` and decides\n        whether to sample directly from `dataset` or through `sampler`.""""""\n        index = self.step_to_index_fn(self.num_valid_samples, self.num_samples_examined)\n        if self.sampler is not None:\n            index = self.sampler_indices[index]\n        return index\n\n    def __next__(self):\n        """"""Returns next index to sample over `dataset`.""""""\n        while True:\n            try:\n                index = self._get_next_index()\n                self.num_samples_examined += 1\n                if self.dataset._safe_get_item(index) is not None:\n                    self.num_valid_samples += 1\n                    return index\n            except IndexError:\n                raise StopIteration\n\n    # For Python2 compatibility\n    next = __next__\n'"
nonechucks/utils.py,5,"b'import collections\n\nfrom itertools import chain\nfrom functools import partial\n\nimport torch\n\ntry:\n    from torch.utils.data.dataloader import default_collate\nexcept ImportError:\n    from torch.utils.data._utils.collate import default_collate\nfrom torch._six import string_classes\n\n\nclass memoize(object):\n    """"""cache the return value of a method\n\n    This class is meant to be used as a decorator of methods. The return value\n    from a given method invocation will be cached on the instance whose method\n    was invoked. All arguments passed to a method decorated with memoize must\n    be hashable.\n\n    If a memoized method is invoked directly on its class the result will not\n    be cached. Instead the method will be invoked like a static method:\n    class Obj(object):\n        @memoize\n        def add_to(self, arg):\n            return self + arg\n    Obj.add_to(1) # not enough arguments\n    Obj.add_to(1, 2) # returns 3, result is not cached\n    """"""\n\n    def __init__(self, func):\n        self.func = func\n\n    def __get__(self, obj, objtype=None):\n        if obj is None:\n            return self.func\n        return partial(self, obj)\n\n    def __call__(self, *args, **kw):\n        obj = args[0]\n        try:\n            cache = obj.__cache\n        except AttributeError:\n            cache = obj.__cache = {}\n        key = (self.func, args[1:], frozenset(kw.items()))\n        try:\n            res = cache[key]\n        except KeyError:\n            res = cache[key] = self.func(*args, **kw)\n        return res\n\n\ndef collate_batches(batches, collate_fn=default_collate):\n    """"""Collate multiple batches.""""""\n    error_msg = ""batches must be tensors, dicts, or lists; found {}""\n    if isinstance(batches[0], torch.Tensor):\n        return torch.cat(batches, 0)\n    elif isinstance(batches[0], collections.Sequence):\n        return list(chain(*batches))\n    elif isinstance(batches[0], collections.Mapping):\n        return {key: default_collate([d[key] for d in batches]) for key in batches[0]}\n    raise TypeError((error_msg.format(type(batches[0]))))\n\n\ndef batch_len(batch):\n    # error_msg = ""batch must be tensor, dict, or list: found {}""\n    if isinstance(batch, list):\n        if isinstance(batch[0], string_classes):\n            return len(batch)\n        else:\n            return len(batch[0])\n    elif isinstance(batch, collections.Mapping):\n        first_key = list(batch.keys())[0]\n        return len(batch[first_key])\n    return len(batch)\n\n\ndef slice_batch(batch, start=None, end=None):\n    if isinstance(batch, list):\n        if isinstance(batch[0], string_classes):\n            return batch[start:end]\n        else:\n            return [sample[start:end] for sample in batch]\n    elif isinstance(batch[0], collections.Mapping):\n        return {key: batch[key][start:end] for key in batch}\n    else:\n        return batch[start:end]\n'"
tests/test_dataset.py,6,"b'import collections\nimport unittest\n\ntry:\n    import unittest.mock as mock\nexcept ImportError:\n    import mock\n\nimport torch\nimport torch.utils.data as data\n\nfrom nonechucks import *\nimport nonechucks\n\n\nclass SafeDatasetTest(unittest.TestCase):\n    """"""Unit tests for `SafeDataset`.""""""\n\n    SafeDatasetPair = collections.namedtuple(""SafeDatasetPair"", [""unsafe"", ""safe""])\n\n    @classmethod\n    def get_safe_dataset_pair(cls, dataset, **kwargs):\n        """"""Returns a `SafeDatasetPair` (a tuple of size 2), which contains\n            both the unsafe and safe versions of the dataset.\n        """"""\n        return SafeDatasetTest.SafeDatasetPair(\n            dataset, nonechucks.SafeDataset(dataset, **kwargs)\n        )\n\n    def setUp(self):\n        tensor_data = data.TensorDataset(torch.arange(0, 10))\n        self._dataset = self.get_safe_dataset_pair(tensor_data)\n\n    @property\n    def dataset(self):\n        self._dataset.safe._reset_index()\n        return self._dataset\n\n    def test_build_index(self):\n        dataset = data.TensorDataset(torch.arange(0, 10))\n        dataset = self.get_safe_dataset_pair(dataset, eager_eval=True)\n\n        self.assertTrue(dataset.safe.is_index_built)\n        self.assertEqual(len(dataset.safe), len(dataset.unsafe))\n\n    def test_dataset_iterator(self):\n        counter = 0\n        for i in self.dataset.safe:\n            self.assertEqual(i[0].tolist(), counter)\n            counter += 1\n\n    def test_iter_calls_safe_get_item(self):\n        dataset = data.TensorDataset(torch.arange(0, 10))\n        dataset = self.get_safe_dataset_pair(dataset).safe\n        for sample in dataset:\n            pass\n        self.assertTrue(dataset.is_index_built)\n\n    # @mock.patch(\'torch.utils.data.TensorDataset.__getitem__\')\n    # def test_default_map(self, mock_get_item):\n    #     def side_effect(idx):\n    #         return [10, 11, 12, 13, None, 14, None, None, 15, 16][idx]\n    #     mock_get_item.side_effect = side_effect\n\n    #     dataset = data.TensorDataset(torch.arange(0, 10))\n    #     dataset = self.get_safe_dataset_pair(dataset)\n    #     self.assertEqual(dataset.safe[4], 14)\n    #     self.assertEqual(dataset.safe[5], 15)\n    #     self.assertEqual(dataset.safe[4], 14)\n\n    def test_memoization(self):\n        pass\n\n    def test_import(self):\n        self.assertIsNotNone(SafeDataset)\n        self.assertIsNotNone(SafeSampler)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
tests/test_sampler.py,6,"b'import unittest\n\ntry:\n    import unittest.mock as mock\nexcept ImportError:\n    import mock\n\nimport torch\nimport torch.utils.data as data\n\nimport nonechucks\n\n\nclass SafeSamplerTest(unittest.TestCase):\n    def test_sequential_sampler(self):\n        dataset = data.TensorDataset(torch.arange(0, 10))\n        dataset = nonechucks.SafeDataset(dataset)\n        dataloader = data.DataLoader(\n            dataset, sampler=nonechucks.SafeSequentialSampler(dataset)\n        )\n        for i_batch, sample_batched in enumerate(dataloader):\n            print(""Sample {}: {}"".format(i_batch, sample_batched))\n\n    def test_first_last_sampler(self):\n        dataset = data.TensorDataset(torch.arange(0, 10))\n        dataset = nonechucks.SafeDataset(dataset)\n        dataloader = data.DataLoader(\n            dataset, sampler=nonechucks.SafeFirstAndLastSampler(dataset)\n        )\n        for i_batch, sample_batched in enumerate(dataloader):\n            print(""Sample {}: {}"".format(i_batch, sample_batched))\n\n    @mock.patch(""torch.utils.data.TensorDataset.__getitem__"")\n    @mock.patch(""torch.utils.data.TensorDataset.__len__"")\n    def test_sampler_wrapper(self, mock_len, mock_get_item):\n        def side_effect(idx):\n            return [0, 1, None, 3, 4, 5][idx]\n\n        mock_get_item.side_effect = side_effect\n        mock_len.return_value = 6\n        dataset = data.TensorDataset(torch.arange(0, 10))\n        dataset = nonechucks.SafeDataset(dataset)\n        self.assertEqual(len(dataset), 6)\n        sequential_sampler = data.SequentialSampler(dataset)\n        dataloader = data.DataLoader(\n            dataset, sampler=nonechucks.SafeSampler(dataset, sequential_sampler)\n        )\n        for i_batch, sample_batched in enumerate(dataloader):\n            print(""Sample {}: {}"".format(i_batch, sample_batched))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
