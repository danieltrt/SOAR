file_path,api_count,code
setup.py,0,"b""from setuptools import setup\n\nsetup(name='vq_vae',\n      version='0.0.1',\n      install_requires=['torch>=0.4.0',\n                        'torchvision>=0.2.0',\n                        'numpy']\n)\n"""
vq_vae/__init__.py,0,b''
vq_vae/auto_encoder.py,15,"b'from __future__ import print_function\nimport abc\n\nimport numpy as np\nimport logging\nimport torch\nimport torch.utils.data\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .nearest_embed import NearestEmbed, NearestEmbedEMA\n\n\nclass AbstractAutoEncoder(nn.Module):\n    __metaclass__ = abc.ABCMeta\n\n    @abc.abstractmethod\n    def encode(self, x):\n        return\n\n    @abc.abstractmethod\n    def decode(self, z):\n        return\n\n    @abc.abstractmethod\n    def forward(self, x):\n        """"""model return (reconstructed_x, *)""""""\n        return\n\n    @abc.abstractmethod\n    def sample(self, size):\n        """"""sample new images from model""""""\n        return\n\n    @abc.abstractmethod\n    def loss_function(self, **kwargs):\n        """"""accepts (original images, *) where * is the same as returned from forward()""""""\n        return\n\n    @abc.abstractmethod\n    def latest_losses(self):\n        """"""returns the latest losses in a dictionary. Useful for logging.""""""\n        return\n\n\nclass VAE(nn.Module):\n    """"""Variational AutoEncoder for MNIST\n       Taken from pytorch/examples: https://github.com/pytorch/examples/tree/master/vae""""""\n    def __init__(self, kl_coef=1, **kwargs):\n        super(VAE, self).__init__()\n\n        self.fc1 = nn.Linear(784, 400)\n        self.fc21 = nn.Linear(400, 20)\n        self.fc22 = nn.Linear(400, 20)\n        self.fc3 = nn.Linear(20, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.kl_coef = kl_coef\n        self.bce = 0\n        self.kl = 0\n\n    def encode(self, x):\n        h1 = self.relu(self.fc1(x))\n        return self.fc21(h1), self.fc22(h1)\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = std.new(std.size()).normal_()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def decode(self, z):\n        h3 = self.relu(self.fc3(z))\n        return self.tanh(self.fc4(h3))\n\n    def forward(self, x):\n        mu, logvar = self.encode(x.view(-1, 784))\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\n    def sample(self, size):\n        sample = torch.randn(size, 20)\n        if self.cuda():\n            sample = sample.cuda()\n        sample = self.decode(sample).cpu()\n        return sample\n\n    def loss_function(self, x, recon_x, mu, logvar):\n        self.bce = F.binary_cross_entropy(recon_x, x.view(-1, 784), size_average=False)\n        batch_size = x.size(0)\n\n        # see Appendix B from VAE paper:\n        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n        # https://arxiv.org/abs/1312.6114\n        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n        self.kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n\n        return self.bce + self.kl_coef*self.kl\n\n    def latest_losses(self):\n        return {\'bce\': self.bce, \'kl\': self.kl}\n\n\nclass VQ_VAE(nn.Module):\n    """"""Vector Quantized AutoEncoder for mnist""""""\n    def __init__(self, hidden=200, k=10, vq_coef=0.2, comit_coef=0.4, **kwargs):\n        super(VQ_VAE, self).__init__()\n\n        self.emb_size = k\n        self.fc1 = nn.Linear(784, 400)\n        self.fc2 = nn.Linear(400, hidden)\n        self.fc3 = nn.Linear(hidden, 400)\n        self.fc4 = nn.Linear(400, 784)\n\n        self.emb = NearestEmbed(k, self.emb_size)\n\n        self.relu = nn.ReLU()\n        self.sigmoid = nn.Sigmoid()\n        self.vq_coef = vq_coef\n        self.comit_coef = comit_coef\n        self.hidden = hidden\n        self.ce_loss = 0\n        self.vq_loss = 0\n        self.commit_loss = 0\n\n    def encode(self, x):\n        h1 = self.relu(self.fc1(x))\n        h2 = self.fc2(h1)\n        return h2.view(-1, self.emb_size, int(self.hidden / self.emb_size))\n\n    def decode(self, z):\n        h3 = self.relu(self.fc3(z))\n        return self.tanh(self.fc4(h3))\n\n    def forward(self, x):\n        z_e = self.encode(x.view(-1, 784))\n        z_q, _ = self.emb(z_e, weight_sg=True).view(-1, self.hidden)\n        emb, _ = self.emb(z_e.detach()).view(-1, self.hidden)\n        return self.decode(z_q), z_e, emb\n\n    def sample(self, size):\n        sample = torch.randn(size, self.emb_size, int(self.hidden / self.emb_size))\n        if self.cuda():\n            sample = sample.cuda()\n        emb, _ = self.emb(sample)\n        sample = self.decode(emb(sample).view(-1, self.hidden)).cpu()\n        return sample\n\n    def loss_function(self, x, recon_x, z_e, emb):\n        self.ce_loss = F.binary_cross_entropy(recon_x, x.view(-1, 784))\n        self.vq_loss = F.mse_loss(emb, z_e.detach())\n        self.commit_loss = F.mse_loss(z_e, emb.detach())\n\n        return self.ce_loss + self.vq_coef*self.vq_loss + self.comit_coef*self.commit_loss\n\n    def latest_losses(self):\n        return {\'cross_entropy\': self.ce_loss, \'vq\': self.vq_loss, \'commitment\': self.commit_loss}\n\n\nclass ResBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, mid_channels=None, bn=False):\n        super(ResBlock, self).__init__()\n\n        if mid_channels is None:\n            mid_channels = out_channels\n\n        layers = [\n            nn.ReLU(),\n            nn.Conv2d(in_channels, mid_channels, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(mid_channels, out_channels, kernel_size=1, stride=1, padding=0)]\n        if bn:\n            layers.insert(2, nn.BatchNorm2d(out_channels))\n        self.convs = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return x + self.convs(x)\n\n\nclass CVAE(AbstractAutoEncoder):\n    def __init__(self, d, kl_coef=0.1, **kwargs):\n        super(CVAE, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, d // 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(d // 2),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(d // 2, d, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(d),\n            nn.ReLU(inplace=True),\n            ResBlock(d, d, bn=True),\n            nn.BatchNorm2d(d),\n            ResBlock(d, d, bn=True),\n        )\n        self.decoder = nn.Sequential(\n            ResBlock(d, d, bn=True),\n            nn.BatchNorm2d(d),\n            ResBlock(d, d, bn=True),\n            nn.BatchNorm2d(d),\n\n            nn.ConvTranspose2d(d, d // 2, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(d//2),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(d // 2, 3, kernel_size=4, stride=2, padding=1, bias=False),\n        )\n        self.f = 8\n        self.d = d\n        self.fc11 = nn.Linear(d * self.f ** 2, d * self.f ** 2)\n        self.fc12 = nn.Linear(d * self.f ** 2, d * self.f ** 2)\n        self.kl_coef = kl_coef\n        self.kl_loss = 0\n        self.mse = 0\n\n    def encode(self, x):\n        h1 = self.encoder(x)\n        h1 = h1.view(-1, self.d * self.f ** 2)\n        return self.fc11(h1), self.fc12(h1)\n\n    def reparameterize(self, mu, logvar):\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = std.new(std.size()).normal_()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def decode(self, z):\n        z = z.view(-1, self.d, self.f, self.f)\n        h3 = self.decoder(z)\n        return torch.tanh(h3)\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        return self.decode(z), mu, logvar\n\n    def sample(self, size):\n        sample = torch.randn(size, self.d * self.f ** 2, requires_grad=False)\n        if self.cuda():\n            sample = sample.cuda()\n        return self.decode(sample).cpu()\n\n    def loss_function(self, x, recon_x, mu, logvar):\n        self.mse = F.mse_loss(recon_x, x)\n        batch_size = x.size(0)\n\n        # see Appendix B from VAE paper:\n        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n        # https://arxiv.org/abs/1312.6114\n        # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n        self.kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n        # Normalise by same number of elements as in reconstruction\n        self.kl_loss /= batch_size * 3 * 1024\n\n        # return mse\n        return self.mse + self.kl_coef * self.kl_loss\n\n    def latest_losses(self):\n        return {\'mse\': self.mse, \'kl\': self.kl_loss}\n\n\nclass VQ_CVAE(nn.Module):\n    def __init__(self, d, k=10, bn=True, vq_coef=1, commit_coef=0.5, num_channels=3, **kwargs):\n        super(VQ_CVAE, self).__init__()\n\n        self.encoder = nn.Sequential(\n            nn.Conv2d(num_channels, d, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(d),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(d, d, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(d),\n            nn.ReLU(inplace=True),\n            ResBlock(d, d, bn),\n            nn.BatchNorm2d(d),\n            ResBlock(d, d, bn),\n            nn.BatchNorm2d(d),\n        )\n        self.decoder = nn.Sequential(\n            ResBlock(d, d),\n            nn.BatchNorm2d(d),\n            ResBlock(d, d),\n            nn.ConvTranspose2d(d, d, kernel_size=4, stride=2, padding=1),\n            nn.BatchNorm2d(d),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(d, num_channels, kernel_size=4, stride=2, padding=1),\n        )\n        self.d = d\n        self.emb = NearestEmbed(k, d)\n        self.vq_coef = vq_coef\n        self.commit_coef = commit_coef\n        self.mse = 0\n        self.vq_loss = torch.zeros(1)\n        self.commit_loss = 0\n\n        for l in self.modules():\n            if isinstance(l, nn.Linear) or isinstance(l, nn.Conv2d):\n                l.weight.detach().normal_(0, 0.02)\n                torch.fmod(l.weight, 0.04)\n                nn.init.constant_(l.bias, 0)\n\n        self.encoder[-1].weight.detach().fill_(1 / 40)\n\n        self.emb.weight.detach().normal_(0, 0.02)\n        torch.fmod(self.emb.weight, 0.04)\n\n    def encode(self, x):\n        return self.encoder(x)\n\n    def decode(self, x):\n        return torch.tanh(self.decoder(x))\n\n    def forward(self, x):\n        z_e = self.encode(x)\n        self.f = z_e.shape[-1]\n        z_q, argmin = self.emb(z_e, weight_sg=True)\n        emb, _ = self.emb(z_e.detach())\n        return self.decode(z_q), z_e, emb, argmin\n\n    def sample(self, size):\n        sample = torch.randn(size, self.d, self.f, self.f, requires_grad=False),\n        if self.cuda():\n            sample = sample.cuda()\n        emb, _ = self.emb(sample)\n        return self.decode(emb.view(size, self.d, self.f, self.f)).cpu()\n\n    def loss_function(self, x, recon_x, z_e, emb, argmin):\n        self.mse = F.mse_loss(recon_x, x)\n\n        self.vq_loss = torch.mean(torch.norm((emb - z_e.detach())**2, 2, 1))\n        self.commit_loss = torch.mean(torch.norm((emb.detach() - z_e)**2, 2, 1))\n\n        return self.mse + self.vq_coef*self.vq_loss + self.commit_coef*self.commit_loss\n\n    def latest_losses(self):\n        return {\'mse\': self.mse, \'vq\': self.vq_loss, \'commitment\': self.commit_loss}\n\n    def print_atom_hist(self, argmin):\n\n        argmin = argmin.detach().cpu().numpy()\n        unique, counts = np.unique(argmin, return_counts=True)\n        logging.info(counts)\n        logging.info(unique)\n'"
vq_vae/main.py,15,"b'import os\nimport sys\nimport time\nimport logging\nimport argparse\n\nimport torch.utils.data\nfrom torch import optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image, make_grid\n\nfrom vq_vae.util import setup_logging_from_args\nfrom vq_vae.auto_encoder import *\n\nmodels = {\n    \'custom\': {\'vqvae\': VQ_CVAE,\n               \'vqvae2\': VQ_CVAE2},\n    \'imagenet\': {\'vqvae\': VQ_CVAE,\n                 \'vqvae2\': VQ_CVAE2},\n    \'cifar10\': {\'vae\': CVAE,\n                \'vqvae\': VQ_CVAE,\n                \'vqvae2\': VQ_CVAE2},\n    \'mnist\': {\'vae\': VAE,\n              \'vqvae\': VQ_CVAE},\n}\ndatasets_classes = {\n    \'custom\': datasets.ImageFolder,\n    \'imagenet\': datasets.ImageFolder,\n    \'cifar10\': datasets.CIFAR10,\n    \'mnist\': datasets.MNIST\n}\ndataset_train_args = {\n    \'custom\': {},\n    \'imagenet\': {},\n    \'cifar10\': {\'train\': True, \'download\': True},\n    \'mnist\': {\'train\': True, \'download\': True},\n}\ndataset_test_args = {\n    \'custom\': {},\n    \'imagenet\': {},\n    \'cifar10\': {\'train\': False, \'download\': True},\n    \'mnist\': {\'train\': False, \'download\': True},\n}\ndataset_n_channels = {\n    \'custom\': 3,\n    \'imagenet\': 3,\n    \'cifar10\': 3,\n    \'mnist\': 1,\n}\n\ndataset_transforms = {\n    \'custom\': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \'imagenet\': transforms.Compose([transforms.Resize(256), transforms.CenterCrop(256),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \'cifar10\': transforms.Compose([transforms.ToTensor(),\n                                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]),\n    \'mnist\': transforms.ToTensor()\n}\ndefault_hyperparams = {\n    \'custom\': {\'lr\': 2e-4, \'k\': 512, \'hidden\': 128},\n    \'imagenet\': {\'lr\': 2e-4, \'k\': 512, \'hidden\': 128},\n    \'cifar10\': {\'lr\': 2e-4, \'k\': 10, \'hidden\': 256},\n    \'mnist\': {\'lr\': 1e-4, \'k\': 10, \'hidden\': 64}\n}\n\n\ndef main(args):\n    parser = argparse.ArgumentParser(description=\'Variational AutoEncoders\')\n\n    model_parser = parser.add_argument_group(\'Model Parameters\')\n    model_parser.add_argument(\'--model\', default=\'vae\', choices=[\'vae\', \'vqvae\'],\n                              help=\'autoencoder variant to use: vae | vqvae\')\n    model_parser.add_argument(\'--batch-size\', type=int, default=128, metavar=\'N\',\n                              help=\'input batch size for training (default: 128)\')\n    model_parser.add_argument(\'--hidden\', type=int, metavar=\'N\',\n                              help=\'number of hidden channels\')\n    model_parser.add_argument(\'-k\', \'--dict-size\', type=int, dest=\'k\', metavar=\'K\',\n                              help=\'number of atoms in dictionary\')\n    model_parser.add_argument(\'--lr\', type=float, default=None,\n                              help=\'learning rate\')\n    model_parser.add_argument(\'--vq_coef\', type=float, default=None,\n                              help=\'vq coefficient in loss\')\n    model_parser.add_argument(\'--commit_coef\', type=float, default=None,\n                              help=\'commitment coefficient in loss\')\n    model_parser.add_argument(\'--kl_coef\', type=float, default=None,\n                              help=\'kl-divergence coefficient in loss\')\n\n    training_parser = parser.add_argument_group(\'Training Parameters\')\n    training_parser.add_argument(\'--dataset\', default=\'cifar10\', choices=[\'mnist\', \'cifar10\', \'imagenet\',\n                                                                          \'custom\'],\n                                 help=\'dataset to use: mnist | cifar10 | imagenet | custom\')\n    training_parser.add_argument(\'--dataset_dir_name\', default=\'\',\n                                 help=\'name of the dir containing the dataset if dataset == custom\')\n    training_parser.add_argument(\'--data-dir\', default=\'/media/ssd/Datasets\',\n                                 help=\'directory containing the dataset\')\n    training_parser.add_argument(\'--epochs\', type=int, default=20, metavar=\'N\',\n                                 help=\'number of epochs to train (default: 10)\')\n    training_parser.add_argument(\'--max-epoch-samples\', type=int, default=50000,\n                                 help=\'max num of samples per epoch\')\n    training_parser.add_argument(\'--no-cuda\', action=\'store_true\', default=False,\n                                 help=\'enables CUDA training\')\n    training_parser.add_argument(\'--seed\', type=int, default=1, metavar=\'S\',\n                                 help=\'random seed (default: 1)\')\n    training_parser.add_argument(\'--gpus\', default=\'0\',\n                                 help=\'gpus used for training - e.g 0,1,3\')\n\n    logging_parser = parser.add_argument_group(\'Logging Parameters\')\n    logging_parser.add_argument(\'--log-interval\', type=int, default=10, metavar=\'N\',\n                                help=\'how many batches to wait before logging training status\')\n    logging_parser.add_argument(\'--results-dir\', metavar=\'RESULTS_DIR\', default=\'./results\',\n                                help=\'results dir\')\n    logging_parser.add_argument(\'--save-name\', default=\'\',\n                                help=\'saved folder\')\n    logging_parser.add_argument(\'--data-format\', default=\'json\',\n                                help=\'in which format to save the data\')\n    args = parser.parse_args(args)\n    args.cuda = not args.no_cuda and torch.cuda.is_available()\n    dataset_dir_name = args.dataset if args.dataset != \'custom\' else args.dataset_dir_name\n\n    lr = args.lr or default_hyperparams[args.dataset][\'lr\']\n    k = args.k or default_hyperparams[args.dataset][\'k\']\n    hidden = args.hidden or default_hyperparams[args.dataset][\'hidden\']\n    num_channels = dataset_n_channels[args.dataset]\n\n    save_path = setup_logging_from_args(args)\n    writer = SummaryWriter(save_path)\n\n    torch.manual_seed(args.seed)\n    if args.cuda:\n        torch.cuda.manual_seed_all(args.seed)\n        args.gpus = [int(i) for i in args.gpus.split(\',\')]\n        torch.cuda.set_device(args.gpus[0])\n        cudnn.benchmark = True\n        torch.cuda.manual_seed(args.seed)\n\n    model = models[args.dataset][args.model](hidden, k=k, num_channels=num_channels)\n    if args.cuda:\n        model.cuda()\n\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, 10 if args.dataset == \'imagenet\' else 30, 0.5,)\n\n    kwargs = {\'num_workers\': 8, \'pin_memory\': True} if args.cuda else {}\n    dataset_train_dir = os.path.join(args.data_dir, dataset_dir_name)\n    dataset_test_dir = os.path.join(args.data_dir, dataset_dir_name)\n    if args.dataset in [\'imagenet\', \'custom\']:\n        dataset_train_dir = os.path.join(dataset_train_dir, \'train\')\n        dataset_test_dir = os.path.join(dataset_test_dir, \'val\')\n    train_loader = torch.utils.data.DataLoader(\n        datasets_classes[args.dataset](dataset_train_dir,\n                                       transform=dataset_transforms[args.dataset],\n                                       **dataset_train_args[args.dataset]),\n        batch_size=args.batch_size, shuffle=True, **kwargs)\n    test_loader = torch.utils.data.DataLoader(\n        datasets_classes[args.dataset](dataset_test_dir,\n                                       transform=dataset_transforms[args.dataset],\n                                       **dataset_test_args[args.dataset]),\n        batch_size=args.batch_size, shuffle=False, **kwargs)\n\n    for epoch in range(1, args.epochs + 1):\n        train_losses = train(epoch, model, train_loader, optimizer, args.cuda,\n                             args.log_interval, save_path, args, writer)\n        test_losses = test_net(epoch, model, test_loader, args.cuda, save_path, args, writer)\n\n        for k in train_losses.keys():\n            name = k.replace(\'_train\', \'\')\n            train_name = k\n            test_name = k.replace(\'train\', \'test\')\n            writer.add_scalars(name, {\'train\': train_losses[train_name],\n                                      \'test\': test_losses[test_name],\n                                      })\n        scheduler.step()\n\n\ndef train(epoch, model, train_loader, optimizer, cuda, log_interval, save_path, args, writer):\n    model.train()\n    loss_dict = model.latest_losses()\n    losses = {k + \'_train\': 0 for k, v in loss_dict.items()}\n    epoch_losses = {k + \'_train\': 0 for k, v in loss_dict.items()}\n    start_time = time.time()\n    batch_idx, data = None, None\n    for batch_idx, (data, _) in enumerate(train_loader):\n        if cuda:\n            data = data.cuda()\n        optimizer.zero_grad()\n        outputs = model(data)\n        loss = model.loss_function(data, *outputs)\n        loss.backward()\n        optimizer.step()\n        latest_losses = model.latest_losses()\n        for key in latest_losses:\n            losses[key + \'_train\'] += float(latest_losses[key])\n            epoch_losses[key + \'_train\'] += float(latest_losses[key])\n\n        if batch_idx % log_interval == 0:\n            for key in latest_losses:\n                losses[key + \'_train\'] /= log_interval\n            loss_string = \' \'.join([\'{}: {:.6f}\'.format(k, v) for k, v in losses.items()])\n            logging.info(\'Train Epoch: {epoch} [{batch:5d}/{total_batch} ({percent:2d}%)]   time:\'\n                         \' {time:3.2f}   {loss}\'\n                         .format(epoch=epoch, batch=batch_idx * len(data), total_batch=len(train_loader) * len(data),\n                                 percent=int(100. * batch_idx / len(train_loader)),\n                                 time=time.time() - start_time,\n                                 loss=loss_string))\n            start_time = time.time()\n            # logging.info(\'z_e norm: {:.2f}\'.format(float(torch.mean(torch.norm(outputs[1][0].contiguous().view(256,-1),2,0)))))\n            # logging.info(\'z_q norm: {:.2f}\'.format(float(torch.mean(torch.norm(outputs[2][0].contiguous().view(256,-1),2,0)))))\n            for key in latest_losses:\n                losses[key + \'_train\'] = 0\n        if batch_idx == (len(train_loader) - 1):\n            save_reconstructed_images(data, epoch, outputs[0], save_path, \'reconstruction_train\')\n\n            write_images(data, outputs, writer, \'train\')\n\n        if args.dataset in [\'imagenet\', \'custom\'] and batch_idx * len(data) > args.max_epoch_samples:\n            break\n\n    for key in epoch_losses:\n        if args.dataset != \'imagenet\':\n            epoch_losses[key] /= (len(train_loader.dataset) / train_loader.batch_size)\n        else:\n            epoch_losses[key] /= (len(train_loader.dataset) / train_loader.batch_size)\n    loss_string = \'\\t\'.join([\'{}: {:.6f}\'.format(k, v) for k, v in epoch_losses.items()])\n    logging.info(\'====> Epoch: {} {}\'.format(epoch, loss_string))\n    writer.add_histogram(\'dict frequency\', outputs[3], bins=range(args.k + 1))\n    model.print_atom_hist(outputs[3])\n    return epoch_losses\n\n\ndef test_net(epoch, model, test_loader, cuda, save_path, args, writer):\n    model.eval()\n    loss_dict = model.latest_losses()\n    losses = {k + \'_test\': 0 for k, v in loss_dict.items()}\n    i, data = None, None\n    with torch.no_grad():\n        for i, (data, _) in enumerate(test_loader):\n            if cuda:\n                data = data.cuda()\n            outputs = model(data)\n            model.loss_function(data, *outputs)\n            latest_losses = model.latest_losses()\n            for key in latest_losses:\n                losses[key + \'_test\'] += float(latest_losses[key])\n            if i == 0:\n                write_images(data, outputs, writer, \'test\')\n\n                save_reconstructed_images(data, epoch, outputs[0], save_path, \'reconstruction_test\')\n                save_checkpoint(model, epoch, save_path)\n            if args.dataset == \'imagenet\' and i * len(data) > 1000:\n                break\n\n    for key in losses:\n        if args.dataset not in [\'imagenet\', \'custom\']:\n            losses[key] /= (len(test_loader.dataset) / test_loader.batch_size)\n        else:\n            losses[key] /= (i * len(data))\n    loss_string = \' \'.join([\'{}: {:.6f}\'.format(k, v) for k, v in losses.items()])\n    logging.info(\'====> Test set losses: {}\'.format(loss_string))\n    return losses\n\n\ndef write_images(data, outputs, writer, suffix):\n    original = data.mul(0.5).add(0.5)\n    original_grid = make_grid(original[:6])\n    writer.add_image(f\'original/{suffix}\', original_grid)\n    reconstructed = outputs[0].mul(0.5).add(0.5)\n    reconstructed_grid = make_grid(reconstructed[:6])\n    writer.add_image(f\'reconstructed/{suffix}\', reconstructed_grid)\n\n\ndef save_reconstructed_images(data, epoch, outputs, save_path, name):\n    size = data.size()\n    n = min(data.size(0), 8)\n    batch_size = data.size(0)\n    comparison = torch.cat([data[:n],\n                            outputs.view(batch_size, size[1], size[2], size[3])[:n]])\n    save_image(comparison.cpu(),\n               os.path.join(save_path, name + \'_\' + str(epoch) + \'.png\'), nrow=n, normalize=True)\n\n\ndef save_checkpoint(model, epoch, save_path):\n    os.makedirs(os.path.join(save_path, \'checkpoints\'), exist_ok=True)\n    checkpoint_path = os.path.join(save_path, \'checkpoints\', f\'model_{epoch}.pth\')\n    torch.save(model.state_dict(), checkpoint_path)\n\n\nif __name__ == ""__main__"":\n    main(sys.argv[1:])\n'"
vq_vae/nearest_embed.py,10,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Function, Variable\nimport torch.nn.functional as F\n\n\nclass NearestEmbedFunc(Function):\n    """"""\n    Input:\n    ------\n    x - (batch_size, emb_dim, *)\n        Last dimensions may be arbitrary\n    emb - (emb_dim, num_emb)\n    """"""\n    @staticmethod\n    def forward(ctx, input, emb):\n        if input.size(1) != emb.size(0):\n            raise RuntimeError(\'invalid argument: input.size(1) ({}) must be equal to emb.size(0) ({})\'.\n                               format(input.size(1), emb.size(0)))\n\n        # save sizes for backward\n        ctx.batch_size = input.size(0)\n        ctx.num_latents = int(np.prod(np.array(input.size()[2:])))\n        ctx.emb_dim = emb.size(0)\n        ctx.num_emb = emb.size(1)\n        ctx.input_type = type(input)\n        ctx.dims = list(range(len(input.size())))\n\n        # expand to be broadcast-able\n        x_expanded = input.unsqueeze(-1)\n        num_arbitrary_dims = len(ctx.dims) - 2\n        if num_arbitrary_dims:\n            emb_expanded = emb.view(emb.shape[0], *([1] * num_arbitrary_dims), emb.shape[1])\n        else:\n            emb_expanded = emb\n\n        # find nearest neighbors\n        dist = torch.norm(x_expanded - emb_expanded, 2, 1)\n        _, argmin = dist.min(-1)\n        shifted_shape = [input.shape[0], *list(input.shape[2:]) ,input.shape[1]]\n        result = emb.t().index_select(0, argmin.view(-1)).view(shifted_shape).permute(0, ctx.dims[-1], *ctx.dims[1:-1])\n\n        ctx.save_for_backward(argmin)\n        return result.contiguous(), argmin\n\n    @staticmethod\n    def backward(ctx, grad_output, argmin=None):\n        grad_input = grad_emb = None\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output\n\n        if ctx.needs_input_grad[1]:\n            argmin, = ctx.saved_variables\n            latent_indices = torch.arange(ctx.num_emb).type_as(argmin)\n            idx_choices = (argmin.view(-1, 1) == latent_indices.view(1, -1)).type_as(grad_output.data)\n            n_idx_choice = idx_choices.sum(0)\n            n_idx_choice[n_idx_choice == 0] = 1\n            idx_avg_choices = idx_choices / n_idx_choice\n            grad_output = grad_output.permute(0, *ctx.dims[2:], 1).contiguous()\n            grad_output = grad_output.view(ctx.batch_size * ctx.num_latents, ctx.emb_dim)\n            grad_emb = torch.sum(grad_output.data.view(-1, ctx.emb_dim, 1) *\n                                 idx_avg_choices.view(-1, 1, ctx.num_emb), 0)\n        return grad_input, grad_emb, None, None\n\n\ndef nearest_embed(x, emb):\n    return NearestEmbedFunc().apply(x, emb)\n\n\nclass NearestEmbed(nn.Module):\n    def __init__(self, num_embeddings, embeddings_dim):\n        super(NearestEmbed, self).__init__()\n        self.weight = nn.Parameter(torch.rand(embeddings_dim, num_embeddings))\n\n    def forward(self, x, weight_sg=False):\n        """"""Input:\n        ---------\n        x - (batch_size, emb_size, *)\n        """"""\n        return nearest_embed(x, self.weight.detach() if weight_sg else self.weight)\n\n\n# adapted from https://github.com/rosinality/vq-vae-2-pytorch/blob/master/vqvae.py#L25\n# that adapted from https://github.com/deepmind/sonnet\n\n\nclass NearestEmbedEMA(nn.Module):\n    def __init__(self, n_emb, emb_dim, decay=0.99, eps=1e-5):\n        super(NearestEmbedEMA, self).__init__()\n        self.decay = decay\n        self.eps = eps\n        self.embeddings_dim = emb_dim\n        self.n_emb = n_emb\n        self.emb_dim = emb_dim\n        embed = torch.rand(emb_dim, n_emb)\n        self.register_buffer(\'weight\', embed)\n        self.register_buffer(\'cluster_size\', torch.zeros(n_emb))\n        self.register_buffer(\'embed_avg\', embed.clone())\n\n    def forward(self, x):\n        """"""Input:\n        ---------\n        x - (batch_size, emb_size, *)\n        """"""\n\n        dims = list(range(len(x.size())))\n        x_expanded = x.unsqueeze(-1)\n        num_arbitrary_dims = len(dims) - 2\n        if num_arbitrary_dims:\n            emb_expanded = self.weight.view(self.emb_dim, *([1] * num_arbitrary_dims), self.n_emb)\n        else:\n            emb_expanded = self.weight\n\n        # find nearest neighbors\n        dist = torch.norm(x_expanded - emb_expanded, 2, 1)\n        _, argmin = dist.min(-1)\n        shifted_shape = [x.shape[0], *list(x.shape[2:]), x.shape[1]]\n        result = self.weight.t().index_select(0, argmin.view(-1)).view(shifted_shape).permute(0, dims[-1], *dims[1:-1])\n\n        if self.training:\n            latent_indices = torch.arange(self.n_emb).type_as(argmin)\n            emb_onehot = (argmin.view(-1, 1) == latent_indices.view(1, -1)).type_as(x.data)\n            n_idx_choice = emb_onehot.sum(0)\n            n_idx_choice[n_idx_choice == 0] = 1\n            flatten = x.permute(1, 0, *dims[-2:]).contiguous().view(x.shape[1], -1)\n\n            self.cluster_size.data.mul_(self.decay).add_(\n                1 - self.decay, n_idx_choice\n            )\n            embed_sum = flatten @ emb_onehot\n            self.embed_avg.data.mul_(self.decay).add_(1 - self.decay, embed_sum)\n\n            n = self.cluster_size.sum()\n            cluster_size = (\n                (self.cluster_size + self.eps) / (n + self.n_emb * self.eps) * n\n            )\n            embed_normalized = self.embed_avg / cluster_size.unsqueeze(0)\n            self.weight.data.copy_(embed_normalized)\n\n        return result, argmin\n'"
vq_vae/test_nearest_embed.py,12,"b""import unittest\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\n\nfrom vq_vae.nearest_embed import nearest_embed\n\n\n# class NearestEmbedTest(unittest.TestCase):\n#     def test_something(self):\n#\n#         emb = Variable(torch.eye(10, 10).double())\n#         a = np.array(([1,0,0,0,0,0,0,0,0,0],\n#                       [0,1,0,0,0,0,0,0,0,0]), dtype=np.double)\n#         input = Variable(torch.from_numpy(a))\n#         z_q = nearest_embed(input, emb, dim=1)\n#         self.assertEqual(True, torch.equal(z_q.data, input.data))\n\n\nclass NearestEmbed2dTest(unittest.TestCase):\n\n    def test_single_embedding(self):\n        # inputs\n        emb = Variable(torch.eye(5, 7).float(), requires_grad=True)\n\n        a = np.array([[[0.9, 0. ],\n                       [0. , 0. ],\n                       [0. , 2. ],\n                       [0. , 0. ],\n                       [0. , 0. ]],\n\n                      [[0. , 0.7],\n                       [0. , 0. ],\n                       [0. , 0. ],\n                       [0.6, 0. ],\n                       [0. , 0. ]]], dtype=np.float32)\n\n        # expected results\n        out = np.array([[[1. , 0.],\n                         [0. , 0.],\n                         [0. , 1.],\n                         [0. , 0.],\n                         [0. , 0.]],\n\n                        [[0. , 1.],\n                         [0. , 0.],\n                         [0. , 0.],\n                         [1. , 0.],\n                         [0. , 0.]]], dtype=np.float32)\n\n        grad_input = np.array([[[1. , 0.],\n                                [0. , 0.],\n                                [0. , 1.],\n                                [0. , 0.],\n                                [0. , 0.]],\n\n                               [[0. , 1.],\n                                [0. , 0.],\n                                [0. , 0.],\n                                [1. , 0.],\n                                [0. , 0.]]], dtype=np.float32)\n\n        grad_emb = np.array([[1. , 0., 0., 0. , 0., 0., 0.],\n                             [0. , 0., 0., 0. , 0., 0., 0.],\n                             [0. , 0., 1., 0. , 0., 0., 0.],\n                             [0. , 0., 0., 1. , 0., 0., 0.],\n                             [0. , 0., 0., 0. , 0., 0., 0.]], dtype=np.float32)\n\n        grad_input = torch.from_numpy(grad_input).float()\n        grad_emb = torch.from_numpy(grad_emb).float()\n\n        input = Variable(torch.from_numpy(a).float(), requires_grad=True)\n        z_q, _ = nearest_embed(input, emb)\n\n        (0.5 * z_q.pow(2)).sum().backward(retain_graph=True)\n        out = torch.from_numpy(out)\n\n        self.assertEqual(True, torch.equal(z_q.data, out))\n        self.assertEqual(True, torch.equal(input.grad.data, grad_input))\n        self.assertEqual(True, torch.equal(emb.grad.data, grad_emb))\n\n\nif __name__ == '__main__':\n    unittest.main()\n"""
vq_vae/util.py,1,"b'import shutil\nimport os\nfrom itertools import cycle\nimport torch\nimport logging.config\nfrom datetime import datetime\nimport json\n\n\ndef setup_logging_from_args(args):\n    """"""\n    Calls setup_logging, exports args and creates a ResultsLog class.\n    Can resume training/logging if args.resume is set\n    """"""\n    def set_args_default(field_name, value):\n        if hasattr(args, field_name):\n            return eval(\'args.\' + field_name)\n        else:\n            return value\n\n    # Set default args in case they don\'t exist in args\n    resume = set_args_default(\'resume\', False)\n    save_name = set_args_default(\'save_name\', \'\')\n    results_dir = set_args_default(\'results_dir\', \'./results\')\n\n    if save_name is \'\':\n        save_name = datetime.now().strftime(\'%Y-%m-%d_%H-%M-%S\')\n    save_path = os.path.join(results_dir, save_name)\n    if os.path.exists(save_path):\n        shutil.rmtree(save_path)\n    os.makedirs(save_path, exist_ok=True)\n    log_file = os.path.join(save_path, \'log.txt\')\n\n    setup_logging(log_file, resume)\n    export_args(args, save_path)\n    return save_path\n\n\ndef setup_logging(log_file=\'log.txt\', resume=False):\n    """"""\n    Setup logging configuration\n    """"""\n    if os.path.isfile(log_file) and resume:\n        file_mode = \'a\'\n    else:\n        file_mode = \'w\'\n\n    root_logger = logging.getLogger()\n    if root_logger.handlers:\n        root_logger.removeHandler(root_logger.handlers[0])\n    logging.basicConfig(level=logging.INFO,\n                        format=""%(asctime)s - %(levelname)s - %(message)s"",\n                        datefmt=""%Y-%m-%d %H:%M:%S"",\n                        filename=log_file,\n                        filemode=file_mode)\n    console = logging.StreamHandler()\n    console.setLevel(logging.INFO)\n    formatter = logging.Formatter(\'%(message)s\')\n    console.setFormatter(formatter)\n    logging.getLogger(\'\').addHandler(console)\n\n\ndef export_args(args, save_path):\n    """"""\n    args: argparse.Namespace\n        arguments to save\n    save_path: string\n        path to directory to save at\n    """"""\n    os.makedirs(save_path, exist_ok=True)\n    json_file_name = os.path.join(save_path, \'args.json\')\n    with open(json_file_name, \'w\') as fp:\n        json.dump(dict(args._get_kwargs()), fp, sort_keys=True, indent=4)\n\n\ndef save_checkpoint(state, is_best, path=\'.\', filename=\'checkpoint.pth.tar\', save_all=False):\n    filename = os.path.join(path, filename)\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, os.path.join(path, \'model_best.pth.tar\'))\n    if save_all:\n        shutil.copyfile(filename, os.path.join(\n            path, \'checkpoint_epoch_%s.pth.tar\' % state[\'epoch\']))\n'"
