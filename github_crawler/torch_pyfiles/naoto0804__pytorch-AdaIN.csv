file_path,api_count,code
function.py,7,"b'import torch\n\n\ndef calc_mean_std(feat, eps=1e-5):\n    # eps is a small value added to the variance to avoid divide-by-zero.\n    size = feat.size()\n    assert (len(size) == 4)\n    N, C = size[:2]\n    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n    return feat_mean, feat_std\n\n\ndef adaptive_instance_normalization(content_feat, style_feat):\n    assert (content_feat.size()[:2] == style_feat.size()[:2])\n    size = content_feat.size()\n    style_mean, style_std = calc_mean_std(style_feat)\n    content_mean, content_std = calc_mean_std(content_feat)\n\n    normalized_feat = (content_feat - content_mean.expand(\n        size)) / content_std.expand(size)\n    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n\n\ndef _calc_feat_flatten_mean_std(feat):\n    # takes 3D feat (C, H, W), return mean and std of array within channels\n    assert (feat.size()[0] == 3)\n    assert (isinstance(feat, torch.FloatTensor))\n    feat_flatten = feat.view(3, -1)\n    mean = feat_flatten.mean(dim=-1, keepdim=True)\n    std = feat_flatten.std(dim=-1, keepdim=True)\n    return feat_flatten, mean, std\n\n\ndef _mat_sqrt(x):\n    U, D, V = torch.svd(x)\n    return torch.mm(torch.mm(U, D.pow(0.5).diag()), V.t())\n\n\ndef coral(source, target):\n    # assume both source and target are 3D array (C, H, W)\n    # Note: flatten -> f\n\n    source_f, source_f_mean, source_f_std = _calc_feat_flatten_mean_std(source)\n    source_f_norm = (source_f - source_f_mean.expand_as(\n        source_f)) / source_f_std.expand_as(source_f)\n    source_f_cov_eye = \\\n        torch.mm(source_f_norm, source_f_norm.t()) + torch.eye(3)\n\n    target_f, target_f_mean, target_f_std = _calc_feat_flatten_mean_std(target)\n    target_f_norm = (target_f - target_f_mean.expand_as(\n        target_f)) / target_f_std.expand_as(target_f)\n    target_f_cov_eye = \\\n        torch.mm(target_f_norm, target_f_norm.t()) + torch.eye(3)\n\n    source_f_norm_transfer = torch.mm(\n        _mat_sqrt(target_f_cov_eye),\n        torch.mm(torch.inverse(_mat_sqrt(source_f_cov_eye)),\n                 source_f_norm)\n    )\n\n    source_f_transfer = source_f_norm_transfer * \\\n                        target_f_std.expand_as(source_f_norm) + \\\n                        target_f_mean.expand_as(source_f_norm)\n\n    return source_f_transfer.view(source.size())\n'"
net.py,1,"b""import torch.nn as nn\n\nfrom function import adaptive_instance_normalization as adain\nfrom function import calc_mean_std\n\ndecoder = nn.Sequential(\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 256, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 128, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 128, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 64, (3, 3)),\n    nn.ReLU(),\n    nn.Upsample(scale_factor=2, mode='nearest'),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 64, (3, 3)),\n    nn.ReLU(),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 3, (3, 3)),\n)\n\nvgg = nn.Sequential(\n    nn.Conv2d(3, 3, (1, 1)),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(3, 64, (3, 3)),\n    nn.ReLU(),  # relu1-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 64, (3, 3)),\n    nn.ReLU(),  # relu1-2\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(64, 128, (3, 3)),\n    nn.ReLU(),  # relu2-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 128, (3, 3)),\n    nn.ReLU(),  # relu2-2\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(128, 256, (3, 3)),\n    nn.ReLU(),  # relu3-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 256, (3, 3)),\n    nn.ReLU(),  # relu3-4\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(256, 512, (3, 3)),\n    nn.ReLU(),  # relu4-1, this is the last layer used\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu4-4\n    nn.MaxPool2d((2, 2), (2, 2), (0, 0), ceil_mode=True),\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-1\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-2\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU(),  # relu5-3\n    nn.ReflectionPad2d((1, 1, 1, 1)),\n    nn.Conv2d(512, 512, (3, 3)),\n    nn.ReLU()  # relu5-4\n)\n\n\nclass Net(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(Net, self).__init__()\n        enc_layers = list(encoder.children())\n        self.enc_1 = nn.Sequential(*enc_layers[:4])  # input -> relu1_1\n        self.enc_2 = nn.Sequential(*enc_layers[4:11])  # relu1_1 -> relu2_1\n        self.enc_3 = nn.Sequential(*enc_layers[11:18])  # relu2_1 -> relu3_1\n        self.enc_4 = nn.Sequential(*enc_layers[18:31])  # relu3_1 -> relu4_1\n        self.decoder = decoder\n        self.mse_loss = nn.MSELoss()\n\n        # fix the encoder\n        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n            for param in getattr(self, name).parameters():\n                param.requires_grad = False\n\n    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n    def encode_with_intermediate(self, input):\n        results = [input]\n        for i in range(4):\n            func = getattr(self, 'enc_{:d}'.format(i + 1))\n            results.append(func(results[-1]))\n        return results[1:]\n\n    # extract relu4_1 from input image\n    def encode(self, input):\n        for i in range(4):\n            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n        return input\n\n    def calc_content_loss(self, input, target):\n        assert (input.size() == target.size())\n        assert (target.requires_grad is False)\n        return self.mse_loss(input, target)\n\n    def calc_style_loss(self, input, target):\n        assert (input.size() == target.size())\n        assert (target.requires_grad is False)\n        input_mean, input_std = calc_mean_std(input)\n        target_mean, target_std = calc_mean_std(target)\n        return self.mse_loss(input_mean, target_mean) + \\\n               self.mse_loss(input_std, target_std)\n\n    def forward(self, content, style, alpha=1.0):\n        assert 0 <= alpha <= 1\n        style_feats = self.encode_with_intermediate(style)\n        content_feat = self.encode(content)\n        t = adain(content_feat, style_feats[-1])\n        t = alpha * t + (1 - alpha) * content_feat\n\n        g_t = self.decoder(t)\n        g_t_feats = self.encode_with_intermediate(g_t)\n\n        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n        for i in range(1, 4):\n            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n        return loss_c, loss_s\n"""
sampler.py,1,"b'import numpy as np\nfrom torch.utils import data\n\n\ndef InfiniteSampler(n):\n    # i = 0\n    i = n - 1\n    order = np.random.permutation(n)\n    while True:\n        yield order[i]\n        i += 1\n        if i >= n:\n            np.random.seed()\n            order = np.random.permutation(n)\n            i = 0\n\n\nclass InfiniteSamplerWrapper(data.sampler.Sampler):\n    def __init__(self, data_source):\n        self.num_samples = len(data_source)\n\n    def __iter__(self):\n        return iter(InfiniteSampler(self.num_samples))\n\n    def __len__(self):\n        return 2 ** 31\n'"
test.py,8,"b'import argparse\nfrom pathlib import Path\n\nimport torch\nimport torch.nn as nn\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\n\nimport net\nfrom function import adaptive_instance_normalization, coral\n\n\ndef test_transform(size, crop):\n    transform_list = []\n    if size != 0:\n        transform_list.append(transforms.Resize(size))\n    if crop:\n        transform_list.append(transforms.CenterCrop(size))\n    transform_list.append(transforms.ToTensor())\n    transform = transforms.Compose(transform_list)\n    return transform\n\n\ndef style_transfer(vgg, decoder, content, style, alpha=1.0,\n                   interpolation_weights=None):\n    assert (0.0 <= alpha <= 1.0)\n    content_f = vgg(content)\n    style_f = vgg(style)\n    if interpolation_weights:\n        _, C, H, W = content_f.size()\n        feat = torch.FloatTensor(1, C, H, W).zero_().to(device)\n        base_feat = adaptive_instance_normalization(content_f, style_f)\n        for i, w in enumerate(interpolation_weights):\n            feat = feat + w * base_feat[i:i + 1]\n        content_f = content_f[0:1]\n    else:\n        feat = adaptive_instance_normalization(content_f, style_f)\n    feat = feat * alpha + content_f * (1 - alpha)\n    return decoder(feat)\n\n\nparser = argparse.ArgumentParser()\n# Basic options\nparser.add_argument(\'--content\', type=str,\n                    help=\'File path to the content image\')\nparser.add_argument(\'--content_dir\', type=str,\n                    help=\'Directory path to a batch of content images\')\nparser.add_argument(\'--style\', type=str,\n                    help=\'File path to the style image, or multiple style \\\n                    images separated by commas if you want to do style \\\n                    interpolation or spatial control\')\nparser.add_argument(\'--style_dir\', type=str,\n                    help=\'Directory path to a batch of style images\')\nparser.add_argument(\'--vgg\', type=str, default=\'models/vgg_normalised.pth\')\nparser.add_argument(\'--decoder\', type=str, default=\'models/decoder.pth\')\n\n# Additional options\nparser.add_argument(\'--content_size\', type=int, default=512,\n                    help=\'New (minimum) size for the content image, \\\n                    keeping the original size if set to 0\')\nparser.add_argument(\'--style_size\', type=int, default=512,\n                    help=\'New (minimum) size for the style image, \\\n                    keeping the original size if set to 0\')\nparser.add_argument(\'--crop\', action=\'store_true\',\n                    help=\'do center crop to create squared image\')\nparser.add_argument(\'--save_ext\', default=\'.jpg\',\n                    help=\'The extension name of the output image\')\nparser.add_argument(\'--output\', type=str, default=\'output\',\n                    help=\'Directory to save the output image(s)\')\n\n# Advanced options\nparser.add_argument(\'--preserve_color\', action=\'store_true\',\n                    help=\'If specified, preserve color of the content image\')\nparser.add_argument(\'--alpha\', type=float, default=1.0,\n                    help=\'The weight that controls the degree of \\\n                             stylization. Should be between 0 and 1\')\nparser.add_argument(\n    \'--style_interpolation_weights\', type=str, default=\'\',\n    help=\'The weight for blending the style of multiple style images\')\n\nargs = parser.parse_args()\n\ndo_interpolation = False\n\ndevice = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\noutput_dir = Path(args.output)\noutput_dir.mkdir(exist_ok=True, parents=True)\n\n# Either --content or --contentDir should be given.\nassert (args.content or args.content_dir)\nif args.content:\n    content_paths = [Path(args.content)]\nelse:\n    content_dir = Path(args.content_dir)\n    content_paths = [f for f in content_dir.glob(\'*\')]\n\n# Either --style or --styleDir should be given.\nassert (args.style or args.style_dir)\nif args.style:\n    style_paths = args.style.split(\',\')\n    if len(style_paths) == 1:\n        style_paths = [Path(args.style)]\n    else:\n        do_interpolation = True\n        assert (args.style_interpolation_weights != \'\'), \\\n            \'Please specify interpolation weights\'\n        weights = [int(i) for i in args.style_interpolation_weights.split(\',\')]\n        interpolation_weights = [w / sum(weights) for w in weights]\nelse:\n    style_dir = Path(args.style_dir)\n    style_paths = [f for f in style_dir.glob(\'*\')]\n\ndecoder = net.decoder\nvgg = net.vgg\n\ndecoder.eval()\nvgg.eval()\n\ndecoder.load_state_dict(torch.load(args.decoder))\nvgg.load_state_dict(torch.load(args.vgg))\nvgg = nn.Sequential(*list(vgg.children())[:31])\n\nvgg.to(device)\ndecoder.to(device)\n\ncontent_tf = test_transform(args.content_size, args.crop)\nstyle_tf = test_transform(args.style_size, args.crop)\n\nfor content_path in content_paths:\n    if do_interpolation:  # one content image, N style image\n        style = torch.stack([style_tf(Image.open(str(p))) for p in style_paths])\n        content = content_tf(Image.open(str(content_path))) \\\n            .unsqueeze(0).expand_as(style)\n        style = style.to(device)\n        content = content.to(device)\n        with torch.no_grad():\n            output = style_transfer(vgg, decoder, content, style,\n                                    args.alpha, interpolation_weights)\n        output = output.cpu()\n        output_name = output_dir / \'{:s}_interpolation{:s}\'.format(\n            content_path.stem, args.save_ext)\n        save_image(output, str(output_name))\n\n    else:  # process one content and one style\n        for style_path in style_paths:\n            content = content_tf(Image.open(str(content_path)))\n            style = style_tf(Image.open(str(style_path)))\n            if args.preserve_color:\n                style = coral(style, content)\n            style = style.to(device).unsqueeze(0)\n            content = content.to(device).unsqueeze(0)\n            with torch.no_grad():\n                output = style_transfer(vgg, decoder, content, style,\n                                        args.alpha)\n            output = output.cpu()\n\n            output_name = output_dir / \'{:s}_stylized_{:s}{:s}\'.format(\n                content_path.stem, style_path.stem, args.save_ext)\n            save_image(output, str(output_name))\n'"
torch_to_pytorch.py,12,"b'from __future__ import print_function\n\nimport argparse\nfrom functools import reduce\n\nimport torch\nassert torch.__version__.split(\'.\')[0] == \'0\', \'Only working on PyTorch 0.x.x\'\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.serialization import load_lua\n\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        # result is Variables list [Variable1, Variable2, ...]\n        return list(map(self.lambda_func, self.forward_prepare(input)))\n\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        # result is a Variable\n        return reduce(self.lambda_func, self.forward_prepare(input))\n\n\ndef copy_param(m, n):\n    if m.weight is not None: n.weight.data.copy_(m.weight)\n    if m.bias is not None: n.bias.data.copy_(m.bias)\n    if hasattr(n, \'running_mean\'): n.running_mean.copy_(m.running_mean)\n    if hasattr(n, \'running_var\'): n.running_var.copy_(m.running_var)\n\n\ndef add_submodule(seq, *args):\n    for n in args:\n        seq.add_module(str(len(seq._modules)), n)\n\n\ndef lua_recursive_model(module, seq):\n    for m in module.modules:\n        name = type(m).__name__\n        real = m\n        if name == \'TorchObject\':\n            name = m._typename.replace(\'cudnn.\', \'\')\n            m = m._obj\n\n        if name == \'SpatialConvolution\':\n            if not hasattr(m, \'groups\'): m.groups = 1\n            n = nn.Conv2d(m.nInputPlane, m.nOutputPlane, (m.kW, m.kH),\n                          (m.dW, m.dH), (m.padW, m.padH), 1, m.groups,\n                          bias=(m.bias is not None))\n            copy_param(m, n)\n            add_submodule(seq, n)\n        elif name == \'SpatialBatchNormalization\':\n            n = nn.BatchNorm2d(m.running_mean.size(0), m.eps, m.momentum,\n                               m.affine)\n            copy_param(m, n)\n            add_submodule(seq, n)\n        elif name == \'ReLU\':\n            n = nn.ReLU()\n            add_submodule(seq, n)\n        elif name == \'SpatialMaxPooling\':\n            n = nn.MaxPool2d((m.kW, m.kH), (m.dW, m.dH), (m.padW, m.padH),\n                             ceil_mode=m.ceil_mode)\n            add_submodule(seq, n)\n        elif name == \'SpatialAveragePooling\':\n            n = nn.AvgPool2d((m.kW, m.kH), (m.dW, m.dH), (m.padW, m.padH),\n                             ceil_mode=m.ceil_mode)\n            add_submodule(seq, n)\n        elif name == \'SpatialUpSamplingNearest\':\n            n = nn.UpsamplingNearest2d(scale_factor=m.scale_factor)\n            add_submodule(seq, n)\n        elif name == \'View\':\n            n = Lambda(lambda x: x.view(x.size(0), -1))\n            add_submodule(seq, n)\n        elif name == \'Linear\':\n            # Linear in pytorch only accept 2D input\n            n1 = Lambda(lambda x: x.view(1, -1) if 1 == len(x.size()) else x)\n            n2 = nn.Linear(m.weight.size(1), m.weight.size(0),\n                           bias=(m.bias is not None))\n            copy_param(m, n2)\n            n = nn.Sequential(n1, n2)\n            add_submodule(seq, n)\n        elif name == \'Dropout\':\n            m.inplace = False\n            n = nn.Dropout(m.p)\n            add_submodule(seq, n)\n        elif name == \'SoftMax\':\n            n = nn.Softmax()\n            add_submodule(seq, n)\n        elif name == \'Identity\':\n            n = Lambda(lambda x: x)  # do nothing\n            add_submodule(seq, n)\n        elif name == \'SpatialFullConvolution\':\n            n = nn.ConvTranspose2d(m.nInputPlane, m.nOutputPlane, (m.kW, m.kH),\n                                   (m.dW, m.dH), (m.padW, m.padH))\n            add_submodule(seq, n)\n        elif name == \'SpatialReplicationPadding\':\n            n = nn.ReplicationPad2d((m.pad_l, m.pad_r, m.pad_t, m.pad_b))\n            add_submodule(seq, n)\n        elif name == \'SpatialReflectionPadding\':\n            n = nn.ReflectionPad2d((m.pad_l, m.pad_r, m.pad_t, m.pad_b))\n            add_submodule(seq, n)\n        elif name == \'Copy\':\n            n = Lambda(lambda x: x)  # do nothing\n            add_submodule(seq, n)\n        elif name == \'Narrow\':\n            n = Lambda(\n                lambda x, a=(m.dimension, m.index, m.length): x.narrow(*a))\n            add_submodule(seq, n)\n        elif name == \'SpatialCrossMapLRN\':\n            lrn = torch.legacy.nn.SpatialCrossMapLRN(m.size, m.alpha, m.beta,\n                                                     m.k)\n            n = Lambda(lambda x, lrn=lrn: lrn.forward(x))\n            add_submodule(seq, n)\n        elif name == \'Sequential\':\n            n = nn.Sequential()\n            lua_recursive_model(m, n)\n            add_submodule(seq, n)\n        elif name == \'ConcatTable\':  # output is list\n            n = LambdaMap(lambda x: x)\n            lua_recursive_model(m, n)\n            add_submodule(seq, n)\n        elif name == \'CAddTable\':  # input is list\n            n = LambdaReduce(lambda x, y: x + y)\n            add_submodule(seq, n)\n        elif name == \'Concat\':\n            dim = m.dimension\n            n = LambdaReduce(lambda x, y, dim=dim: torch.cat((x, y), dim))\n            lua_recursive_model(m, n)\n            add_submodule(seq, n)\n        elif name == \'TorchObject\':\n            print(\'Not Implement\', name, real._typename)\n        else:\n            print(\'Not Implement\', name)\n\n\ndef lua_recursive_source(module):\n    s = []\n    for m in module.modules:\n        name = type(m).__name__\n        real = m\n        if name == \'TorchObject\':\n            name = m._typename.replace(\'cudnn.\', \'\')\n            m = m._obj\n\n        if name == \'SpatialConvolution\':\n            if not hasattr(m, \'groups\'): m.groups = 1\n            s += [\'nn.Conv2d({},{},{},{},{},{},{},bias={}),#Conv2d\'.format(\n                m.nInputPlane,\n                m.nOutputPlane, (m.kW, m.kH), (m.dW, m.dH), (m.padW, m.padH),\n                1, m.groups, m.bias is not None)]\n        elif name == \'SpatialBatchNormalization\':\n            s += [\'nn.BatchNorm2d({},{},{},{}),#BatchNorm2d\'.format(\n                m.running_mean.size(0), m.eps, m.momentum, m.affine)]\n        elif name == \'ReLU\':\n            s += [\'nn.ReLU()\']\n        elif name == \'SpatialMaxPooling\':\n            s += [\'nn.MaxPool2d({},{},{},ceil_mode={}),#MaxPool2d\'.format(\n                (m.kW, m.kH), (m.dW, m.dH), (m.padW, m.padH), m.ceil_mode)]\n        elif name == \'SpatialAveragePooling\':\n            s += [\'nn.AvgPool2d({},{},{},ceil_mode={}),#AvgPool2d\'.format(\n                (m.kW, m.kH), (m.dW, m.dH), (m.padW, m.padH), m.ceil_mode)]\n        elif name == \'SpatialUpSamplingNearest\':\n            s += [\'nn.UpsamplingNearest2d(scale_factor={})\'.format(\n                m.scale_factor)]\n        elif name == \'View\':\n            s += [\'Lambda(lambda x: x.view(x.size(0),-1)), # View\']\n        elif name == \'Linear\':\n            s1 = \'Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x )\'\n            s2 = \'nn.Linear({},{},bias={})\'.format(m.weight.size(1),\n                                                   m.weight.size(0),\n                                                   (m.bias is not None))\n            s += [\'nn.Sequential({},{}),#Linear\'.format(s1, s2)]\n        elif name == \'Dropout\':\n            s += [\'nn.Dropout({})\'.format(m.p)]\n        elif name == \'SoftMax\':\n            s += [\'nn.Softmax()\']\n        elif name == \'Identity\':\n            s += [\'Lambda(lambda x: x), # Identity\']\n        elif name == \'SpatialFullConvolution\':\n            s += [\'nn.ConvTranspose2d({},{},{},{},{})\'.format(m.nInputPlane,\n                                                              m.nOutputPlane,\n                                                              (m.kW, m.kH),\n                                                              (m.dW, m.dH), (\n                                                              m.padW, m.padH))]\n        elif name == \'SpatialReplicationPadding\':\n            s += [\'nn.ReplicationPad2d({})\'.format(\n                (m.pad_l, m.pad_r, m.pad_t, m.pad_b))]\n        elif name == \'SpatialReflectionPadding\':\n            s += [\'nn.ReflectionPad2d({})\'.format(\n                (m.pad_l, m.pad_r, m.pad_t, m.pad_b))]\n        elif name == \'Copy\':\n            s += [\'Lambda(lambda x: x), # Copy\']\n        elif name == \'Narrow\':\n            s += [\'Lambda(lambda x,a={}: x.narrow(*a))\'.format(\n                (m.dimension, m.index, m.length))]\n        elif name == \'SpatialCrossMapLRN\':\n            lrn = \'torch.legacy.nn.SpatialCrossMapLRN(*{})\'.format(\n                (m.size, m.alpha, m.beta, m.k))\n            s += [\n                \'Lambda(lambda x,lrn={}: Variable(lrn.forward(x)))\'.format(\n                    lrn)]\n\n        elif name == \'Sequential\':\n            s += [\'nn.Sequential( # Sequential\']\n            s += lua_recursive_source(m)\n            s += [\')\']\n        elif name == \'ConcatTable\':\n            s += [\'LambdaMap(lambda x: x, # ConcatTable\']\n            s += lua_recursive_source(m)\n            s += [\')\']\n        elif name == \'CAddTable\':\n            s += [\'LambdaReduce(lambda x,y: x+y), # CAddTable\']\n        elif name == \'Concat\':\n            dim = m.dimension\n            s += [\n                \'LambdaReduce(lambda x,y,dim={}: torch.cat((x,y),dim), # Concat\'.format(\n                    m.dimension)]\n            s += lua_recursive_source(m)\n            s += [\')\']\n        else:\n            s += \'# \' + name + \' Not Implement,\\n\'\n    s = map(lambda x: \'\\t{}\'.format(x), s)\n    return s\n\n\ndef simplify_source(s):\n    s = map(lambda x: x.replace(\',(1, 1),(0, 0),1,1,bias=True),#Conv2d\', \')\'),\n            s)\n    s = map(lambda x: x.replace(\',(0, 0),1,1,bias=True),#Conv2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',1,1,bias=True),#Conv2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',bias=True),#Conv2d\', \')\'), s)\n    s = map(lambda x: x.replace(\'),#Conv2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',1e-05,0.1,True),#BatchNorm2d\', \')\'), s)\n    s = map(lambda x: x.replace(\'),#BatchNorm2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',(0, 0),ceil_mode=False),#MaxPool2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',ceil_mode=False),#MaxPool2d\', \')\'), s)\n    s = map(lambda x: x.replace(\'),#MaxPool2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',(0, 0),ceil_mode=False),#AvgPool2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',ceil_mode=False),#AvgPool2d\', \')\'), s)\n    s = map(lambda x: x.replace(\',bias=True)),#Linear\', \')), # Linear\'), s)\n    s = map(lambda x: x.replace(\')),#Linear\', \')), # Linear\'), s)\n\n    s = map(lambda x: \'{},\\n\'.format(x), s)\n    s = map(lambda x: x[1:], s)\n    s = reduce(lambda x, y: x + y, s)\n    return s\n\n\ndef torch_to_pytorch(t7_filename, outputname=None):\n    model = load_lua(t7_filename, unknown_classes=True)\n    if type(model).__name__ == \'hashable_uniq_dict\': model = model.model\n    model.gradInput = None\n    slist = lua_recursive_source(torch.legacy.nn.Sequential().add(model))\n    s = simplify_source(slist)\n    header = \'\'\'\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom functools import reduce\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nclass LambdaMap(LambdaBase):\n    def forward(self, input):\n        return list(map(self.lambda_func,self.forward_prepare(input)))\n\nclass LambdaReduce(LambdaBase):\n    def forward(self, input):\n        return reduce(self.lambda_func,self.forward_prepare(input))\n\'\'\'\n    varname = t7_filename.replace(\'.t7\', \'\').replace(\'.\', \'_\').replace(\'-\',\n                                                                       \'_\')\n    s = \'{}\\n\\n{} = {}\'.format(header, varname, s[:-2])\n\n    if outputname is None: outputname = varname\n    with open(outputname + \'.py\', ""w"") as pyfile:\n        pyfile.write(s)\n\n    n = nn.Sequential()\n    lua_recursive_model(model, n)\n    torch.save(n.state_dict(), outputname + \'.pth\')\n\n\nparser = argparse.ArgumentParser(\n    description=\'Convert torch t7 model to pytorch\')\nparser.add_argument(\'--model\', \'-m\', type=str, required=True,\n                    help=\'torch model file in t7 format\')\nparser.add_argument(\'--output\', \'-o\', type=str, default=None,\n                    help=\'output file name prefix, xxx.py xxx.pth\')\nargs = parser.parse_args()\n\ntorch_to_pytorch(args.model, args.output)\n'"
train.py,8,"b'import argparse\nfrom pathlib import Path\n\nimport torch\nimport torch.backends.cudnn as cudnn\nimport torch.nn as nn\nimport torch.utils.data as data\nfrom PIL import Image, ImageFile\nfrom tensorboardX import SummaryWriter\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nimport net\nfrom sampler import InfiniteSamplerWrapper\n\ncudnn.benchmark = True\nImage.MAX_IMAGE_PIXELS = None  # Disable DecompressionBombError\n# Disable OSError: image file is truncated\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\n\ndef train_transform():\n    transform_list = [\n        transforms.Resize(size=(512, 512)),\n        transforms.RandomCrop(256),\n        transforms.ToTensor()\n    ]\n    return transforms.Compose(transform_list)\n\n\nclass FlatFolderDataset(data.Dataset):\n    def __init__(self, root, transform):\n        super(FlatFolderDataset, self).__init__()\n        self.root = root\n        self.paths = list(Path(self.root).glob(\'*\'))\n        self.transform = transform\n\n    def __getitem__(self, index):\n        path = self.paths[index]\n        img = Image.open(str(path)).convert(\'RGB\')\n        img = self.transform(img)\n        return img\n\n    def __len__(self):\n        return len(self.paths)\n\n    def name(self):\n        return \'FlatFolderDataset\'\n\n\ndef adjust_learning_rate(optimizer, iteration_count):\n    """"""Imitating the original implementation""""""\n    lr = args.lr / (1.0 + args.lr_decay * iteration_count)\n    for param_group in optimizer.param_groups:\n        param_group[\'lr\'] = lr\n\n\nparser = argparse.ArgumentParser()\n# Basic options\nparser.add_argument(\'--content_dir\', type=str, required=True,\n                    help=\'Directory path to a batch of content images\')\nparser.add_argument(\'--style_dir\', type=str, required=True,\n                    help=\'Directory path to a batch of style images\')\nparser.add_argument(\'--vgg\', type=str, default=\'models/vgg_normalised.pth\')\n\n# training options\nparser.add_argument(\'--save_dir\', default=\'./experiments\',\n                    help=\'Directory to save the model\')\nparser.add_argument(\'--log_dir\', default=\'./logs\',\n                    help=\'Directory to save the log\')\nparser.add_argument(\'--lr\', type=float, default=1e-4)\nparser.add_argument(\'--lr_decay\', type=float, default=5e-5)\nparser.add_argument(\'--max_iter\', type=int, default=160000)\nparser.add_argument(\'--batch_size\', type=int, default=8)\nparser.add_argument(\'--style_weight\', type=float, default=10.0)\nparser.add_argument(\'--content_weight\', type=float, default=1.0)\nparser.add_argument(\'--n_threads\', type=int, default=16)\nparser.add_argument(\'--save_model_interval\', type=int, default=10000)\nargs = parser.parse_args()\n\ndevice = torch.device(\'cuda\')\nsave_dir = Path(args.save_dir)\nsave_dir.mkdir(exist_ok=True, parents=True)\nlog_dir = Path(args.log_dir)\nlog_dir.mkdir(exist_ok=True, parents=True)\nwriter = SummaryWriter(log_dir=str(log_dir))\n\ndecoder = net.decoder\nvgg = net.vgg\n\nvgg.load_state_dict(torch.load(args.vgg))\nvgg = nn.Sequential(*list(vgg.children())[:31])\nnetwork = net.Net(vgg, decoder)\nnetwork.train()\nnetwork.to(device)\n\ncontent_tf = train_transform()\nstyle_tf = train_transform()\n\ncontent_dataset = FlatFolderDataset(args.content_dir, content_tf)\nstyle_dataset = FlatFolderDataset(args.style_dir, style_tf)\n\ncontent_iter = iter(data.DataLoader(\n    content_dataset, batch_size=args.batch_size,\n    sampler=InfiniteSamplerWrapper(content_dataset),\n    num_workers=args.n_threads))\nstyle_iter = iter(data.DataLoader(\n    style_dataset, batch_size=args.batch_size,\n    sampler=InfiniteSamplerWrapper(style_dataset),\n    num_workers=args.n_threads))\n\noptimizer = torch.optim.Adam(network.decoder.parameters(), lr=args.lr)\n\nfor i in tqdm(range(args.max_iter)):\n    adjust_learning_rate(optimizer, iteration_count=i)\n    content_images = next(content_iter).to(device)\n    style_images = next(style_iter).to(device)\n    loss_c, loss_s = network(content_images, style_images)\n    loss_c = args.content_weight * loss_c\n    loss_s = args.style_weight * loss_s\n    loss = loss_c + loss_s\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    writer.add_scalar(\'loss_content\', loss_c.item(), i + 1)\n    writer.add_scalar(\'loss_style\', loss_s.item(), i + 1)\n\n    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n        state_dict = net.decoder.state_dict()\n        for key in state_dict.keys():\n            state_dict[key] = state_dict[key].to(torch.device(\'cpu\'))\n        torch.save(state_dict, save_dir /\n                   \'decoder_iter_{:d}.pth.tar\'.format(i + 1))\nwriter.close()\n'"
