file_path,api_count,code
evaluation.py,6,"b""import torch\nfrom torchvision.utils import make_grid\nfrom torchvision.utils import save_image\n\nfrom util.image import unnormalize\n\n\ndef evaluate(model, dataset, device, filename):\n    image, mask, gt = zip(*[dataset[i] for i in range(8)])\n    image = torch.stack(image)\n    mask = torch.stack(mask)\n    gt = torch.stack(gt)\n    with torch.no_grad():\n        output, _ = model(image.to(device), mask.to(device))\n    output = output.to(torch.device('cpu'))\n    output_comp = mask * image + (1 - mask) * output\n\n    grid = make_grid(\n        torch.cat((unnormalize(image), mask, unnormalize(output),\n                   unnormalize(output_comp), unnormalize(gt)), dim=0))\n    save_image(grid, filename)\n"""
generate_data.py,0,"b'import argparse\nimport numpy as np\nimport random\nfrom PIL import Image\n\naction_list = [[0, 1], [0, -1], [1, 0], [-1, 0]]\n\n\ndef random_walk(canvas, ini_x, ini_y, length):\n    x = ini_x\n    y = ini_y\n    img_size = canvas.shape[-1]\n    x_list = []\n    y_list = []\n    for i in range(length):\n        r = random.randint(0, len(action_list) - 1)\n        x = np.clip(x + action_list[r][0], a_min=0, a_max=img_size - 1)\n        y = np.clip(y + action_list[r][1], a_min=0, a_max=img_size - 1)\n        x_list.append(x)\n        y_list.append(y)\n    canvas[np.array(x_list), np.array(y_list)] = 0\n    return canvas\n\n\nif __name__ == \'__main__\':\n    import os\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\'--image_size\', type=int, default=512)\n    parser.add_argument(\'--N\', type=int, default=10000)\n    parser.add_argument(\'--save_dir\', type=str, default=\'mask\')\n    args = parser.parse_args()\n\n    if not os.path.exists(args.save_dir):\n        os.makedirs(args.save_dir)\n\n    for i in range(args.N):\n        canvas = np.ones((args.image_size, args.image_size)).astype(""i"")\n        ini_x = random.randint(0, args.image_size - 1)\n        ini_y = random.randint(0, args.image_size - 1)\n        mask = random_walk(canvas, ini_x, ini_y, args.image_size ** 2)\n        print(""save:"", i, np.sum(mask))\n\n        img = Image.fromarray(mask * 255).convert(\'1\')\n        img.save(\'{:s}/{:06d}.jpg\'.format(args.save_dir, i))\n'"
loss.py,7,"b""import torch\nimport torch.nn as nn\n\n\ndef gram_matrix(feat):\n    # https://github.com/pytorch/examples/blob/master/fast_neural_style/neural_style/utils.py\n    (b, ch, h, w) = feat.size()\n    feat = feat.view(b, ch, h * w)\n    feat_t = feat.transpose(1, 2)\n    gram = torch.bmm(feat, feat_t) / (ch * h * w)\n    return gram\n\n\ndef total_variation_loss(image):\n    # shift one pixel and get difference (for both x and y direction)\n    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n        torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\n    return loss\n\n\nclass InpaintingLoss(nn.Module):\n    def __init__(self, extractor):\n        super().__init__()\n        self.l1 = nn.L1Loss()\n        self.extractor = extractor\n\n    def forward(self, input, mask, output, gt):\n        loss_dict = {}\n        output_comp = mask * input + (1 - mask) * output\n\n        loss_dict['hole'] = self.l1((1 - mask) * output, (1 - mask) * gt)\n        loss_dict['valid'] = self.l1(mask * output, mask * gt)\n\n        if output.shape[1] == 3:\n            feat_output_comp = self.extractor(output_comp)\n            feat_output = self.extractor(output)\n            feat_gt = self.extractor(gt)\n        elif output.shape[1] == 1:\n            feat_output_comp = self.extractor(torch.cat([output_comp]*3, 1))\n            feat_output = self.extractor(torch.cat([output]*3, 1))\n            feat_gt = self.extractor(torch.cat([gt]*3, 1))\n        else:\n            raise ValueError('only gray an')\n\n        loss_dict['prc'] = 0.0\n        for i in range(3):\n            loss_dict['prc'] += self.l1(feat_output[i], feat_gt[i])\n            loss_dict['prc'] += self.l1(feat_output_comp[i], feat_gt[i])\n\n        loss_dict['style'] = 0.0\n        for i in range(3):\n            loss_dict['style'] += self.l1(gram_matrix(feat_output[i]),\n                                          gram_matrix(feat_gt[i]))\n            loss_dict['style'] += self.l1(gram_matrix(feat_output_comp[i]),\n                                          gram_matrix(feat_gt[i]))\n\n        loss_dict['tv'] = total_variation_loss(output_comp)\n\n        return loss_dict\n"""
net.py,14,"b'import math\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\n\n\ndef weights_init(init_type=\'gaussian\'):\n    def init_fun(m):\n        classname = m.__class__.__name__\n        if (classname.find(\'Conv\') == 0 or classname.find(\n                \'Linear\') == 0) and hasattr(m, \'weight\'):\n            if init_type == \'gaussian\':\n                nn.init.normal_(m.weight, 0.0, 0.02)\n            elif init_type == \'xavier\':\n                nn.init.xavier_normal_(m.weight, gain=math.sqrt(2))\n            elif init_type == \'kaiming\':\n                nn.init.kaiming_normal_(m.weight, a=0, mode=\'fan_in\')\n            elif init_type == \'orthogonal\':\n                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))\n            elif init_type == \'default\':\n                pass\n            else:\n                assert 0, ""Unsupported initialization: {}"".format(init_type)\n            if hasattr(m, \'bias\') and m.bias is not None:\n                nn.init.constant_(m.bias, 0.0)\n\n    return init_fun\n\n\nclass VGG16FeatureExtractor(nn.Module):\n    def __init__(self):\n        super().__init__()\n        vgg16 = models.vgg16(pretrained=True)\n        self.enc_1 = nn.Sequential(*vgg16.features[:5])\n        self.enc_2 = nn.Sequential(*vgg16.features[5:10])\n        self.enc_3 = nn.Sequential(*vgg16.features[10:17])\n\n        # fix the encoder\n        for i in range(3):\n            for param in getattr(self, \'enc_{:d}\'.format(i + 1)).parameters():\n                param.requires_grad = False\n\n    def forward(self, image):\n        results = [image]\n        for i in range(3):\n            func = getattr(self, \'enc_{:d}\'.format(i + 1))\n            results.append(func(results[-1]))\n        return results[1:]\n\n\nclass PartialConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1,\n                 padding=0, dilation=1, groups=1, bias=True):\n        super().__init__()\n        self.input_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                                    stride, padding, dilation, groups, bias)\n        self.mask_conv = nn.Conv2d(in_channels, out_channels, kernel_size,\n                                   stride, padding, dilation, groups, False)\n        self.input_conv.apply(weights_init(\'kaiming\'))\n\n        torch.nn.init.constant_(self.mask_conv.weight, 1.0)\n\n        # mask is not updated\n        for param in self.mask_conv.parameters():\n            param.requires_grad = False\n\n    def forward(self, input, mask):\n        # http://masc.cs.gmu.edu/wiki/partialconv\n        # C(X) = W^T * X + b, C(0) = b, D(M) = 1 * M + 0 = sum(M)\n        # W^T* (M .* X) / sum(M) + b = [C(M .* X) \xe2\x80\x93 C(0)] / D(M) + C(0)\n\n        output = self.input_conv(input * mask)\n        if self.input_conv.bias is not None:\n            output_bias = self.input_conv.bias.view(1, -1, 1, 1).expand_as(\n                output)\n        else:\n            output_bias = torch.zeros_like(output)\n\n        with torch.no_grad():\n            output_mask = self.mask_conv(mask)\n\n        no_update_holes = output_mask == 0\n        mask_sum = output_mask.masked_fill_(no_update_holes, 1.0)\n\n        output_pre = (output - output_bias) / mask_sum + output_bias\n        output = output_pre.masked_fill_(no_update_holes, 0.0)\n\n        new_mask = torch.ones_like(output)\n        new_mask = new_mask.masked_fill_(no_update_holes, 0.0)\n\n        return output, new_mask\n\n\nclass PCBActiv(nn.Module):\n    def __init__(self, in_ch, out_ch, bn=True, sample=\'none-3\', activ=\'relu\',\n                 conv_bias=False):\n        super().__init__()\n        if sample == \'down-5\':\n            self.conv = PartialConv(in_ch, out_ch, 5, 2, 2, bias=conv_bias)\n        elif sample == \'down-7\':\n            self.conv = PartialConv(in_ch, out_ch, 7, 2, 3, bias=conv_bias)\n        elif sample == \'down-3\':\n            self.conv = PartialConv(in_ch, out_ch, 3, 2, 1, bias=conv_bias)\n        else:\n            self.conv = PartialConv(in_ch, out_ch, 3, 1, 1, bias=conv_bias)\n\n        if bn:\n            self.bn = nn.BatchNorm2d(out_ch)\n        if activ == \'relu\':\n            self.activation = nn.ReLU()\n        elif activ == \'leaky\':\n            self.activation = nn.LeakyReLU(negative_slope=0.2)\n\n    def forward(self, input, input_mask):\n        h, h_mask = self.conv(input, input_mask)\n        if hasattr(self, \'bn\'):\n            h = self.bn(h)\n        if hasattr(self, \'activation\'):\n            h = self.activation(h)\n        return h, h_mask\n\n\nclass PConvUNet(nn.Module):\n    def __init__(self, layer_size=7, input_channels=3, upsampling_mode=\'nearest\'):\n        super().__init__()\n        self.freeze_enc_bn = False\n        self.upsampling_mode = upsampling_mode\n        self.layer_size = layer_size\n        self.enc_1 = PCBActiv(input_channels, 64, bn=False, sample=\'down-7\')\n        self.enc_2 = PCBActiv(64, 128, sample=\'down-5\')\n        self.enc_3 = PCBActiv(128, 256, sample=\'down-5\')\n        self.enc_4 = PCBActiv(256, 512, sample=\'down-3\')\n        for i in range(4, self.layer_size):\n            name = \'enc_{:d}\'.format(i + 1)\n            setattr(self, name, PCBActiv(512, 512, sample=\'down-3\'))\n\n        for i in range(4, self.layer_size):\n            name = \'dec_{:d}\'.format(i + 1)\n            setattr(self, name, PCBActiv(512 + 512, 512, activ=\'leaky\'))\n        self.dec_4 = PCBActiv(512 + 256, 256, activ=\'leaky\')\n        self.dec_3 = PCBActiv(256 + 128, 128, activ=\'leaky\')\n        self.dec_2 = PCBActiv(128 + 64, 64, activ=\'leaky\')\n        self.dec_1 = PCBActiv(64 + input_channels, input_channels,\n                              bn=False, activ=None, conv_bias=True)\n\n    def forward(self, input, input_mask):\n        h_dict = {}  # for the output of enc_N\n        h_mask_dict = {}  # for the output of enc_N\n\n        h_dict[\'h_0\'], h_mask_dict[\'h_0\'] = input, input_mask\n\n        h_key_prev = \'h_0\'\n        for i in range(1, self.layer_size + 1):\n            l_key = \'enc_{:d}\'.format(i)\n            h_key = \'h_{:d}\'.format(i)\n            h_dict[h_key], h_mask_dict[h_key] = getattr(self, l_key)(\n                h_dict[h_key_prev], h_mask_dict[h_key_prev])\n            h_key_prev = h_key\n\n        h_key = \'h_{:d}\'.format(self.layer_size)\n        h, h_mask = h_dict[h_key], h_mask_dict[h_key]\n\n        # concat upsampled output of h_enc_N-1 and dec_N+1, then do dec_N\n        # (exception)\n        #                            input         dec_2            dec_1\n        #                            h_enc_7       h_enc_8          dec_8\n\n        for i in range(self.layer_size, 0, -1):\n            enc_h_key = \'h_{:d}\'.format(i - 1)\n            dec_l_key = \'dec_{:d}\'.format(i)\n\n            h = F.interpolate(h, scale_factor=2, mode=self.upsampling_mode)\n            h_mask = F.interpolate(\n                h_mask, scale_factor=2, mode=\'nearest\')\n\n            h = torch.cat([h, h_dict[enc_h_key]], dim=1)\n            h_mask = torch.cat([h_mask, h_mask_dict[enc_h_key]], dim=1)\n            h, h_mask = getattr(self, dec_l_key)(h, h_mask)\n\n        return h, h_mask\n\n    def train(self, mode=True):\n        """"""\n        Override the default train() to freeze the BN parameters\n        """"""\n        super().train(mode)\n        if self.freeze_enc_bn:\n            for name, module in self.named_modules():\n                if isinstance(module, nn.BatchNorm2d) and \'enc\' in name:\n                    module.eval()\n\n\nif __name__ == \'__main__\':\n    size = (1, 3, 5, 5)\n    input = torch.ones(size)\n    input_mask = torch.ones(size)\n    input_mask[:, :, 2:, :][:, :, :, 2:] = 0\n\n    conv = PartialConv(3, 3, 3, 1, 1)\n    l1 = nn.L1Loss()\n    input.requires_grad = True\n\n    output, output_mask = conv(input, input_mask)\n    loss = l1(output, torch.randn(1, 3, 5, 5))\n    loss.backward()\n\n    assert (torch.sum(input.grad != input.grad).item() == 0)\n    assert (torch.sum(torch.isnan(conv.input_conv.weight.grad)).item() == 0)\n    assert (torch.sum(torch.isnan(conv.input_conv.bias.grad)).item() == 0)\n\n    # model = PConvUNet()\n    # output, output_mask = model(input, input_mask)\n'"
opt.py,0,"b""MEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]\n\nLAMBDA_DICT = {\n    'valid': 1.0, 'hole': 6.0, 'tv': 0.1, 'prc': 0.05, 'style': 120.0}\n"""
places2.py,1,"b""import random\nimport torch\nfrom PIL import Image\nfrom glob import glob\n\n\nclass Places2(torch.utils.data.Dataset):\n    def __init__(self, img_root, mask_root, img_transform, mask_transform,\n                 split='train'):\n        super(Places2, self).__init__()\n        self.img_transform = img_transform\n        self.mask_transform = mask_transform\n\n        # use about 8M images in the challenge dataset\n        if split == 'train':\n            self.paths = glob('{:s}/data_large/**/*.jpg'.format(img_root),\n                              recursive=True)\n        else:\n            self.paths = glob('{:s}/{:s}_large/*'.format(img_root, split))\n\n        self.mask_paths = glob('{:s}/*.jpg'.format(mask_root))\n        self.N_mask = len(self.mask_paths)\n\n    def __getitem__(self, index):\n        gt_img = Image.open(self.paths[index])\n        gt_img = self.img_transform(gt_img.convert('RGB'))\n\n        mask = Image.open(self.mask_paths[random.randint(0, self.N_mask - 1)])\n        mask = self.mask_transform(mask.convert('RGB'))\n        return gt_img * mask, mask, gt_img\n\n    def __len__(self):\n        return len(self.paths)\n"""
test.py,1,"b""import argparse\nimport torch\nfrom torchvision import transforms\n\nimport opt\nfrom places2 import Places2\nfrom evaluation import evaluate\nfrom net import PConvUNet\nfrom util.io import load_ckpt\n\nparser = argparse.ArgumentParser()\n# training options\nparser.add_argument('--root', type=str, default='./data')\nparser.add_argument('--snapshot', type=str, default='')\nparser.add_argument('--image_size', type=int, default=256)\nargs = parser.parse_args()\n\ndevice = torch.device('cuda')\n\nsize = (args.image_size, args.image_size)\nimg_transform = transforms.Compose(\n    [transforms.Resize(size=size), transforms.ToTensor(),\n     transforms.Normalize(mean=opt.MEAN, std=opt.STD)])\nmask_transform = transforms.Compose(\n    [transforms.Resize(size=size), transforms.ToTensor()])\n\ndataset_val = Places2(args.root, img_transform, mask_transform, 'val')\n\nmodel = PConvUNet().to(device)\nload_ckpt(args.snapshot, [('model', model)])\n\nmodel.eval()\nevaluate(model, dataset_val, device, 'result.jpg')\n"""
train.py,4,"b""import argparse\nimport numpy as np\nimport os\nimport torch\nfrom tensorboardX import SummaryWriter\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom tqdm import tqdm\n\nimport opt\nfrom evaluation import evaluate\nfrom loss import InpaintingLoss\nfrom net import PConvUNet\nfrom net import VGG16FeatureExtractor\nfrom places2 import Places2\nfrom util.io import load_ckpt\nfrom util.io import save_ckpt\n\n\nclass InfiniteSampler(data.sampler.Sampler):\n    def __init__(self, num_samples):\n        self.num_samples = num_samples\n\n    def __iter__(self):\n        return iter(self.loop())\n\n    def __len__(self):\n        return 2 ** 31\n\n    def loop(self):\n        i = 0\n        order = np.random.permutation(self.num_samples)\n        while True:\n            yield order[i]\n            i += 1\n            if i >= self.num_samples:\n                np.random.seed()\n                order = np.random.permutation(self.num_samples)\n                i = 0\n\n\nparser = argparse.ArgumentParser()\n# training options\nparser.add_argument('--root', type=str, default='/srv/datasets/Places2')\nparser.add_argument('--mask_root', type=str, default='./masks')\nparser.add_argument('--save_dir', type=str, default='./snapshots/default')\nparser.add_argument('--log_dir', type=str, default='./logs/default')\nparser.add_argument('--lr', type=float, default=2e-4)\nparser.add_argument('--lr_finetune', type=float, default=5e-5)\nparser.add_argument('--max_iter', type=int, default=1000000)\nparser.add_argument('--batch_size', type=int, default=16)\nparser.add_argument('--n_threads', type=int, default=16)\nparser.add_argument('--save_model_interval', type=int, default=50000)\nparser.add_argument('--vis_interval', type=int, default=5000)\nparser.add_argument('--log_interval', type=int, default=10)\nparser.add_argument('--image_size', type=int, default=256)\nparser.add_argument('--resume', type=str)\nparser.add_argument('--finetune', action='store_true')\nargs = parser.parse_args()\n\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device('cuda')\n\nif not os.path.exists(args.save_dir):\n    os.makedirs('{:s}/images'.format(args.save_dir))\n    os.makedirs('{:s}/ckpt'.format(args.save_dir))\n\nif not os.path.exists(args.log_dir):\n    os.makedirs(args.log_dir)\nwriter = SummaryWriter(log_dir=args.log_dir)\n\nsize = (args.image_size, args.image_size)\nimg_tf = transforms.Compose(\n    [transforms.Resize(size=size), transforms.ToTensor(),\n     transforms.Normalize(mean=opt.MEAN, std=opt.STD)])\nmask_tf = transforms.Compose(\n    [transforms.Resize(size=size), transforms.ToTensor()])\n\ndataset_train = Places2(args.root, args.mask_root, img_tf, mask_tf, 'train')\ndataset_val = Places2(args.root, args.mask_root, img_tf, mask_tf, 'val')\n\niterator_train = iter(data.DataLoader(\n    dataset_train, batch_size=args.batch_size,\n    sampler=InfiniteSampler(len(dataset_train)),\n    num_workers=args.n_threads))\nprint(len(dataset_train))\nmodel = PConvUNet().to(device)\n\nif args.finetune:\n    lr = args.lr_finetune\n    model.freeze_enc_bn = True\nelse:\n    lr = args.lr\n\nstart_iter = 0\noptimizer = torch.optim.Adam(\n    filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\ncriterion = InpaintingLoss(VGG16FeatureExtractor()).to(device)\n\nif args.resume:\n    start_iter = load_ckpt(\n        args.resume, [('model', model)], [('optimizer', optimizer)])\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    print('Starting from iter ', start_iter)\n\nfor i in tqdm(range(start_iter, args.max_iter)):\n    model.train()\n\n    image, mask, gt = [x.to(device) for x in next(iterator_train)]\n    output, _ = model(image, mask)\n    loss_dict = criterion(image, mask, output, gt)\n\n    loss = 0.0\n    for key, coef in opt.LAMBDA_DICT.items():\n        value = coef * loss_dict[key]\n        loss += value\n        if (i + 1) % args.log_interval == 0:\n            writer.add_scalar('loss_{:s}'.format(key), value.item(), i + 1)\n\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter:\n        save_ckpt('{:s}/ckpt/{:d}.pth'.format(args.save_dir, i + 1),\n                  [('model', model)], [('optimizer', optimizer)], i + 1)\n\n    if (i + 1) % args.vis_interval == 0:\n        model.eval()\n        evaluate(model, dataset_val, device,\n                 '{:s}/images/test_{:d}.jpg'.format(args.save_dir, i + 1))\n\nwriter.close()\n"""
util/image.py,1,"b'import torch\nimport opt\n\n\ndef unnormalize(x):\n    x = x.transpose(1, 3)\n    x = x * torch.Tensor(opt.STD) + torch.Tensor(opt.MEAN)\n    x = x.transpose(1, 3)\n    return x\n'"
util/io.py,4,"b""import torch\nimport torch.nn as nn\n\n\ndef get_state_dict_on_cpu(obj):\n    cpu_device = torch.device('cpu')\n    state_dict = obj.state_dict()\n    for key in state_dict.keys():\n        state_dict[key] = state_dict[key].to(cpu_device)\n    return state_dict\n\n\ndef save_ckpt(ckpt_name, models, optimizers, n_iter):\n    ckpt_dict = {'n_iter': n_iter}\n    for prefix, model in models:\n        ckpt_dict[prefix] = get_state_dict_on_cpu(model)\n\n    for prefix, optimizer in optimizers:\n        ckpt_dict[prefix] = optimizer.state_dict()\n    torch.save(ckpt_dict, ckpt_name)\n\n\ndef load_ckpt(ckpt_name, models, optimizers=None):\n    ckpt_dict = torch.load(ckpt_name)\n    for prefix, model in models:\n        assert isinstance(model, nn.Module)\n        model.load_state_dict(ckpt_dict[prefix], strict=False)\n    if optimizers is not None:\n        for prefix, optimizer in optimizers:\n            optimizer.load_state_dict(ckpt_dict[prefix])\n    return ckpt_dict['n_iter']\n"""
