file_path,api_count,code
PyTorch CONLL 2000 Chunking.py,16,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\nimport numpy as np\n\nfrom tqdm import tqdm\n\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pytorch_utils import *\nfrom pytorch_models import *\nfrom utils import load_sequences, conll_classification_report_to_df\nfrom conlleval import main as conll_eval\nimport re\n\nimport io\nfrom pathlib import Path\n\nsns.set_context(""poster"")\nsns.set_style(""ticks"")\n\n\n# In[2]:\n\nTRAIN_CORPUS=""data/conll2000/train.txt""\nTEST_CORPUS=""data/conll2000/test.txt""\n\n\n# In[3]:\n\ntrain_corpus = load_sequences(TRAIN_CORPUS, sep="" "", col_ids=(0, -1))\ntrain_corpus, dev_corpus = train_corpus[100:], train_corpus[:100]\nprint(""Total items in train corpus: %s"" % len(train_corpus))\nprint(""Total items in dev corpus: %s"" % len(dev_corpus))\ntest_corpus = load_sequences(TEST_CORPUS, sep="" "", col_ids=(0, -1))\nprint(""Total items in test corpus: %s"" % len(test_corpus))\n\n\n# In[4]:\n\ntrain_corpus[0]\n\n\n# In[5]:\n\ndef create_vocab(data, vocabs, char_vocab, word_idx=0):\n    n_vocabs = len(vocabs)\n    for sent in data:\n        for token_tags in sent:\n            for vocab_id in range(n_vocabs):\n                vocabs[vocab_id].add(token_tags[vocab_id])\n            char_vocab.batch_add(token_tags[word_idx])\n    print(""Created vocabs: %s, chars[%s]"" % ("", "".join(\n        ""{}[{}]"".format(vocab.name, vocab.size)\n        for vocab in vocabs\n    ), char_vocab.size))\n\n\n# In[6]:\n\nword_vocab = Vocab(""words"", UNK=""UNK"", lower=True)\nchar_vocab = Vocab(""chars"", UNK=""<U>"", lower=False)\nchunk_vocab = Vocab(""chunk_tags"", lower=False)\n\ncreate_vocab(train_corpus+dev_corpus+test_corpus, [word_vocab, chunk_vocab], char_vocab)\n\n\n# In[7]:\n\ndef data2tensors(data, vocabs, char_vocab, word_idx=0, column_ids=(0, -1)):\n    vocabs = [vocabs[idx] for idx in column_ids]\n    n_vocabs = len(vocabs)\n    tensors = []\n    char_tensors = []\n    for sent in data:\n        sent_vecs = [[] for i in range(n_vocabs+1)] # Last is for char vecs\n        char_vecs = []\n        for token_tags in sent:\n            vocab_id = 0 # First column is the word\n            # lowercase the word\n            sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id].lower())\n                )\n            for vocab_id in range(1, n_vocabs):\n                sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id])\n                )\n            sent_vecs[-1].append(\n                [char_vocab.getidx(c) for c in token_tags[word_idx]]\n            )\n        tensors.append(sent_vecs)\n    return tensors\n\n\n# In[8]:\n\ntrain_tensors = data2tensors(train_corpus, [word_vocab, chunk_vocab], char_vocab)\ndev_tensors = data2tensors(dev_corpus, [word_vocab, chunk_vocab], char_vocab)\ntest_tensors = data2tensors(test_corpus, [word_vocab, chunk_vocab], char_vocab)\nprint(""Train: {}, Dev: {}, Test: {}"".format(\n    len(train_tensors),\n    len(dev_tensors),\n    len(test_tensors),\n))\n\n\n# In[9]:\n\ndef load_word_vectors(vector_file, ndims, vocab, cache_file, override_cache=False):\n    W = np.zeros((vocab.size, ndims), dtype=""float32"")\n    # Check for cached file and return vectors\n    cache_file = Path(cache_file)\n    if cache_file.is_file() and not override_cache:\n        W = np.load(cache_file)\n        return W\n    # Else load vectors from the vector file\n    total, found = 0, 0\n    with open(vector_file) as fp:\n        for line in fp:\n            line = line.strip().split()\n            if line:\n                total += 1\n                assert len(line) == ndims+1,(\n                    ""{} vector dims {} doesn\'t match ndims={}"".format(line[0], len(line)-1, ndims)\n                )\n                word = line[0]\n                idx = vocab.getidx(word) \n                if idx >= vocab.offset:\n                    found += 1\n                    vecs = np.array(list(map(float, line[1:])))\n                    W[idx, :] += vecs\n    # Write to cache file\n    print(""Found {} [{:.2f}%] vectors from {} vectors in {} with ndims={}"".format(\n        found, found * 100/vocab.size, total, vector_file, ndims))\n    norm_W = np.sqrt((W*W).sum(axis=1, keepdims=True))\n    valid_idx = norm_W.squeeze() != 0\n    W[valid_idx, :] /= norm_W[valid_idx]\n    print(""Caching embedding with shape {} to {}"".format(W.shape, cache_file.as_posix()))\n    np.save(cache_file, W)\n    return W\n        \n                    \n                \n\n\n# In[10]:\n\nget_ipython().run_cell_magic(u\'time\', u\'\', u\'embedding_file=""/home/napsternxg/datadrive/Downloads/Glove/glove.6B.100d.txt""\\ncache_file=""conll2000.glove.100.npy""\\nndims=100\\npretrained_embeddings = load_word_vectors(embedding_file, ndims, word_vocab, cache_file)\')\n\n\n# In[11]:\n\ndef plot_losses(train_losses, eval_losses=None, plot_std=False, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    for losses, color, label in zip(\n        [train_losses, eval_losses],\n        [""0.5"", ""r""],\n        [""Train"", ""Eval""],\n    ):\n        mean_loss, std_loss = zip(*losses)\n        mean_loss = np.array(mean_loss)\n        std_loss = np.array(std_loss)\n        ax.plot(\n            mean_loss, color=color, label=label,\n            linestyle=""-"", \n        )\n        if plot_std:\n            ax.fill_between(\n                np.arange(mean_loss.shape[0]),\n                mean_loss-std_loss,\n                mean_loss+std_loss,\n                color=color,\n                alpha=0.3\n            )\n    ax.set_xlabel(""Epochs"")\n    ax.set_ylabel(""Mean Loss ($\\pm$ S.D.)"")\n    \n    \ndef print_predictions(corpus, predictions, filename, label_vocab):\n    with open(filename, ""w+"") as fp:\n        for seq, pred in zip(corpus, predictions):\n            for (token, true_label), pred_label in zip(seq, pred):\n                pred_label = label_vocab.idx2item[pred_label]\n                print(""{}\\t{}\\t{}"".format(token, true_label, pred_label), file=fp)\n            print(file=fp) # Add new line after each sequence\n\n\n# In[12]:\n\nchar_emb_size=10\noutput_channels=50\nkernel_sizes=[2, 3]\nchar_embedding = CharEmbedding(char_vocab.size, char_emb_size, output_channels, kernel_sizes)\n\n\n# In[13]:\n\nchar_embedding(Variable(torch.LongTensor([[1,1,2,3]]), requires_grad=False)).size()\n\n\n# In[14]:\n\nword_emb_size=100\nchar_embed_kwargs=dict(\n    vocab_size=char_vocab.size,\n    embedding_size=char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\nword_char_embedding = WordCharEmbedding(\n    word_vocab.size, word_emb_size, char_embed_kwargs, dropout=0.2)\n\n\n# In[15]:\n\ndef charseq2varlist(X_chars):\n    return [Variable(torch.LongTensor([x]), requires_grad=False) for x in X_chars]\n\n\n# In[16]:\n\nprint(len(train_tensors[0][0]))\nprint(len(train_tensors[0][-1]))\n\n\n# In[17]:\n\ntrain_corpus[0]\n\n\n# In[18]:\n\ncharseq2varlist(train_tensors[0][-1])\n\n\n# In[19]:\n\nword_char_embedding(\n    Variable(torch.LongTensor([train_tensors[0][0]]), requires_grad=False),\n    charseq2varlist(train_tensors[0][-1])\n).size()\n\n\n# In[20]:\n\ndef assign_embeddings(embedding_module, pretrained_embeddings, fix_embedding=False):\n    embedding_module.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n    if fix_embedding:\n        embedding_module.weight.requires_grad = False\n\n\n# In[21]:\n\nassign_embeddings(word_char_embedding.word_embeddings, pretrained_embeddings, fix_embedding=True)\n\n\n# ## Class based\n\n# In[22]:\n\nclass ModelWrapper(object):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False\n                ):\n        self.model = model\n        self.loss_function = loss_function\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            self.model.cuda()\n        \n    def _process_instance_tensors(self, instance_tensors):\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def zero_grad(self):\n        self.model.zero_grad()\n        \n    def get_parameters(self):\n        return self.model.paramerters()\n    \n    def set_model_mode(self, training_mode=True):\n        if training_mode:\n            self.model.train()\n        else:\n            self.model.eval()\n            \n    def save(self, filename):\n        torch.save(self.model, filename)\n        print(""{} model saved to {}"".format(self.model.__class__, filename))\n        \n    def load(self, filename):\n        self.model = torch.load(filename)\n        if self.use_cuda:\n            self.model.cuda()\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grads:\n        ## Clear gradients before every update else memory runs out\n            self.zero_grad()\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def predict(self, instance_tensors):\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def predict_batch(self, batch_tensors):\n        predictions = []\n        for instance_tensors in batch_tensors:\n            predictions.append(self.predict(instance_tensors))\n        return predictions\n        \n        \ndef get_epoch_function(model_wrapper, optimizer,\n                       use_cuda=False):\n    def perform_epoch(data_tensors, training_mode=True, batch_size=1):\n        model_wrapper.set_model_mode(training_mode)\n        step_losses = []\n        data_tensors = np.random.permutation(data_tensors)\n        n_splits = data_tensors.shape[0]//batch_size\n        for batch_tensors in np.array_split(data_tensors, n_splits):\n            #from IPython.core.debugger import Tracer; Tracer()()\n            model_wrapper.zero_grad()\n            loss = Variable(torch.FloatTensor([0.]))\n            if use_cuda:\n                loss = loss.cuda()\n            for instance_tensors in batch_tensors:\n                loss += model_wrapper.get_instance_loss(instance_tensors, zero_grad=False)\n            loss = loss/batch_tensors.shape[0] # Mean loss\n            step_losses.append(loss.data[0])\n            if training_mode:\n                ## Get gradients of model params wrt. loss\n                loss.backward()\n                ## Optimize the loss by one step\n                optimizer.step()\n        return step_losses\n    return perform_epoch\n\ndef write_losses(losses, fp, title=""train"", epoch=0):\n    for i, loss in enumerate(losses):\n        print(""{:<10} epoch={:<3} batch={:<5} loss={:<10}"".format(\n            title, epoch, i, loss\n        ), file=fp)\n    print(""{:<10} epoch={:<3} {:<11} mean={:<10.3f} std={:<10.3f}"".format(\n        title, epoch, ""overall"", np.mean(losses), np.std(losses)\n    ), file=fp)\n\n\ndef training_wrapper(\n    model_wrapper, data_tensors,\n    eval_tensors=None,\n    optimizer=optim.SGD,\n    optimizer_kwargs=None,\n    n_epochs=10,\n    batch_size=1,\n    use_cuda=False,\n    log_file=""training_output.log""\n):\n    """"""Wrapper to train the model\n    """"""\n    if optimizer_kwargs is None:\n        optimizer_kwargs = {}\n    # Fileter out parameters which don\'t require a gradient\n    parameters = filter(lambda p: p.requires_grad, model_wrapper.model.parameters())\n    optimizer=optimizer(parameters, **optimizer_kwargs)\n    # Start training\n    losses = []\n    eval_losses = []\n    data_tensors = np.array(data_tensors)\n    if eval_tensors is not None:\n        eval_tensors = np.array(eval_tensors)\n    perform_epoch = get_epoch_function(\n        model_wrapper,\n        optimizer,\n        use_cuda=use_cuda)\n    with open(log_file, ""w+"") as fp:\n        for epoch in tqdm(range(n_epochs)):\n            i = epoch\n            step_losses = perform_epoch(data_tensors, batch_size=batch_size)\n            mean_loss, std_loss = np.mean(step_losses), np.std(step_losses)\n            losses.append((mean_loss, std_loss))\n            write_losses(step_losses, fp, title=""train"", epoch=i)\n            if eval_tensors is not None:\n                step_losses = perform_epoch(eval_tensors, training_mode=False)\n                mean_loss, std_loss = np.mean(step_losses), np.std(step_losses)\n                eval_losses.append((mean_loss, std_loss))\n                write_losses(step_losses, fp, title=""eval"", epoch=i)\n    return {\n        ""training_loss"": losses,\n        ""evaluation_loss"": eval_losses\n    }\n\n\n# In[23]:\n\nclass LSTMTaggerModel(ModelWrapper):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False):\n        self.model = model\n        self.loss_function = loss_function\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            #[k.cuda() for k in self.model.modules()]\n            self.model.cuda()\n        \n    def _process_instance_tensors(self, instance_tensors):\n        X, Y, X_char = instance_tensors\n        X = Variable(torch.LongTensor([X]), requires_grad=False)\n        Y = Variable(torch.LongTensor(Y), requires_grad=False)\n        X_char = charseq2varlist(X_char)\n        if self.use_cuda:\n            X = X.cuda()\n            Y = Y.cuda()\n            X_char = [t.cuda() for t in X_char]\n        return X, X_char, Y\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grad:\n            ## Clear gradients before every update else memory runs out\n            self.model.zero_grad()\n        X, X_char, Y = self._process_instance_tensors(instance_tensors)\n        #print(X.get_device(), [t.get_device() for t in X_char])\n        return self.loss_function(self.model.forward(X, X_char), Y)\n        \n    def predict(self, instance_tensors):\n        X, X_char, Y = self._process_instance_tensors(instance_tensors)\n        prediction = self.model.forward(X, X_char)\n        return prediction.data.cpu().max(1)[1].numpy().ravel()\n\n\n# In[24]:\n\nuse_cuda=True\nn_embed=100\nhidden_size=100\nbatch_size=10\n\nchar_emb_size=50\noutput_channels=50\nkernel_sizes=[2, 3]\n\nword_emb_size=100\nchar_embed_kwargs=dict(\n    vocab_size=char_vocab.size,\n    embedding_size=char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\n\nword_char_embedding = WordCharEmbedding(\n        word_vocab.size, word_emb_size,\n        char_embed_kwargs, dropout=0)\n# Assign glove embeddings\nassign_embeddings(word_char_embedding.word_embeddings, pretrained_embeddings, fix_embedding=True)\n\nmodel_wrapper = LSTMTaggerModel(\n    LSTMTaggerWordChar(word_char_embedding, n_embed, hidden_size, chunk_vocab.size),\n    nn.NLLLoss(), use_cuda=use_cuda)\n\n\n# In[25]:\n\nmodel_wrapper.get_instance_loss(train_tensors[0])\n\n\n# In[26]:\n\nlen(list(model_wrapper.model.parameters()))\n\n\n# In[27]:\n\nn_epochs=5\ntraining_history = training_wrapper(\n    model_wrapper, train_tensors, \n    eval_tensors=dev_tensors,\n    optimizer=optim.Adam,\n    optimizer_kwargs={\n        #""lr"": 0.01,\n        ""weight_decay"": 0.0\n    },\n    n_epochs=n_epochs,\n    batch_size=batch_size,\n    use_cuda=use_cuda,\n    log_file=""LSTMTaggerModel_CONLL2000.log""\n)\nmodel_wrapper.save(""LSTMTaggerModel_CONLL2000"")\n\n\n# In[28]:\n\npreds = model_wrapper.predict(train_tensors[0])\npreds\n\n\n# In[29]:\n\nfig, ax = plt.subplots(1,1)\nplot_losses(training_history[""training_loss""],\n            training_history[""evaluation_loss""],\n            plot_std=True,\n            ax=ax)\nax.legend()\nsns.despine(offset=5)\nplt.savefig(""LSTMTaggerModel_CONLL2000.pdf"")\n\n# In[30]:\n\nfor title, tensors, corpus in zip(\n    [""train"", ""dev"", ""test""],\n    [train_tensors, dev_tensors, test_tensors],\n    [train_corpus, dev_corpus, test_corpus],\n                         ):\n    get_ipython().magic(u\'time predictions = model_wrapper.predict_batch(tensors)\')\n    print_predictions(corpus, predictions, ""%s.chunking.conll"" % title, chunk_vocab)\n    conll_eval([""conlleval"", ""%s.chunking.conll"" % title])\n\n\n# ## CRF model\n\n# In[31]:\n\nclass BiLSTMTaggerWordCRFModel(ModelWrapper):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False):\n        self.model = model\n        self.loss_function = None\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            #[k.cuda() for k in self.model.modules()]\n            self.model.cuda()\n        \n    def _process_instance_tensors(self, instance_tensors):\n        X, Y, X_char = instance_tensors\n        X = Variable(torch.LongTensor([X]), requires_grad=False)\n        Y = torch.LongTensor(Y)\n        X_char = charseq2varlist(X_char)\n        if self.use_cuda:\n            X = X.cuda()\n            Y = Y.cuda()\n            X_char = [t.cuda() for t in X_char]\n        return X, X_char, Y\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grad:\n            ## Clear gradients before every update else memory runs out\n            self.model.zero_grad()\n        X, X_char, Y = self._process_instance_tensors(instance_tensors)\n        #print(X.get_device(), [t.get_device() for t in X_char])\n        return self.model.loss(X, X_char, Y)\n        \n    def predict(self, instance_tensors):\n        X, X_char, Y = self._process_instance_tensors(instance_tensors)\n        emissions = self.model.forward(X, X_char)\n        return self.model.crf.forward(emissions)[1]\n\n\n# In[32]:\n\nuse_cuda=True\nn_embed=100\nhidden_size=128\nbatch_size=64\n\nchar_emb_size=50\noutput_channels=50\nkernel_sizes=[2, 3]\n\nword_emb_size=100\nchar_embed_kwargs=dict(\n    vocab_size=char_vocab.size,\n    embedding_size=char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\n\nword_char_embedding = WordCharEmbedding(\n        word_vocab.size, word_emb_size,\n        char_embed_kwargs, dropout=0)\n# Assign glove embeddings\nassign_embeddings(word_char_embedding.word_embeddings, pretrained_embeddings, fix_embedding=True)\n\nmodel_wrapper = BiLSTMTaggerWordCRFModel(\n    LSTMTaggerWordCharCRF(word_char_embedding, n_embed, hidden_size, chunk_vocab.size),\n    None, use_cuda=use_cuda)\n\n\n# In[33]:\n\nn_epochs=50\ntraining_history = training_wrapper(\n    model_wrapper, train_tensors, \n    eval_tensors=dev_tensors,\n    optimizer=optim.Adam,\n    optimizer_kwargs={\n        #""lr"": 0.01,\n        ""weight_decay"": 0\n    },\n    n_epochs=n_epochs,\n    batch_size=batch_size,\n    use_cuda=use_cuda,\n    log_file=""BiLSTMTaggerWordCRFModel_CONLL2000.log""\n)\nmodel_wrapper.save(""BiLSTMTaggerWordCRFModel_CONLL2000"")\n\n\n# In[34]:\n\nfig, ax = plt.subplots(1,1)\nplot_losses(training_history[""training_loss""],\n            training_history[""evaluation_loss""],\n            plot_std=True,\n            ax=ax)\nax.legend()\nsns.despine(offset=5)\nplt.savefig(""BiLSTMTaggerWordCRFModel_CONLL2000.pdf"")\n\n# Performance may improve by creating all the torch tensors upfront and then pinning them to memory\n\n\n# In[35]:\n\nfor title, tensors, corpus in zip(\n    [""train"", ""dev"", ""test""],\n    [train_tensors, dev_tensors, test_tensors],\n    [train_corpus, dev_corpus, test_corpus],\n                         ):\n    get_ipython().magic(u\'time predictions = model_wrapper.predict_batch(tensors)\')\n    print_predictions(corpus, predictions, ""%s.chunking.conll"" % title, chunk_vocab)\n    conll_eval([""conlleval"", ""%s.chunking.conll"" % title]) \n\n\n'"
chunking_bilstm_crf_char_concat.py,7,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pytorch_utils import *\nfrom pytorch_models import *\nfrom utils import load_sequences, conll_classification_report_to_df\nfrom conlleval import main as conll_eval\nimport re\n\nsns.set_context(""poster"")\nsns.set_style(""ticks"")\n\n\n# In[2]:\n\nTRAIN_CORPUS=""data/conll2000/train.txt""\nTEST_CORPUS=""data/conll2000/test.txt""\n\n\n# In[3]:\n\ntrain_corpus = load_sequences(TRAIN_CORPUS, sep="" "", col_ids=(0, -1))\ntrain_corpus, dev_corpus = train_corpus[100:], train_corpus[:100]\nprint(""Total items in train corpus: %s"" % len(train_corpus))\nprint(""Total items in dev corpus: %s"" % len(dev_corpus))\ntest_corpus = load_sequences(TEST_CORPUS, sep="" "", col_ids=(0, -1))\nprint(""Total items in test corpus: %s"" % len(test_corpus))\n\n\n# In[5]:\n\ndef create_vocab(data, vocabs, char_vocab, word_idx=0):\n    n_vocabs = len(vocabs)\n    for sent in data:\n        for token_tags in sent:\n            for vocab_id in range(n_vocabs):\n                vocabs[vocab_id].add(token_tags[vocab_id])\n            char_vocab.batch_add(token_tags[word_idx])\n    print(""Created vocabs: %s, chars[%s]"" % ("", "".join(\n        ""{}[{}]"".format(vocab.name, vocab.size)\n        for vocab in vocabs\n    ), char_vocab.size))\n\n\n# In[6]:\n\nword_vocab = Vocab(""words"", UNK=""UNK"", lower=True)\nchar_vocab = Vocab(""chars"", UNK=""<U>"", lower=False)\nchunk_vocab = Vocab(""chunk_tags"", lower=False)\n\ncreate_vocab(train_corpus+dev_corpus+test_corpus, [word_vocab, chunk_vocab], char_vocab)\n\n\n# In[7]:\n\ndef data2tensors(data, vocabs, char_vocab, word_idx=0, column_ids=(0, -1)):\n    vocabs = [vocabs[idx] for idx in column_ids]\n    n_vocabs = len(vocabs)\n    tensors = []\n    char_tensors = []\n    for sent in data:\n        sent_vecs = [[] for i in range(n_vocabs+1)] # Last is for char vecs\n        char_vecs = []\n        for token_tags in sent:\n            vocab_id = 0 # First column is the word\n            # lowercase the word\n            sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id].lower())\n                )\n            for vocab_id in range(1, n_vocabs):\n                sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id])\n                )\n            sent_vecs[-1].append(\n                [char_vocab.getidx(c) for c in token_tags[word_idx]]\n            )\n        tensors.append(sent_vecs)\n    return tensors\n\n\n# In[8]:\n\ntrain_tensors = data2tensors(train_corpus, [word_vocab, chunk_vocab], char_vocab)\ndev_tensors = data2tensors(dev_corpus, [word_vocab, chunk_vocab], char_vocab)\ntest_tensors = data2tensors(test_corpus, [word_vocab, chunk_vocab], char_vocab)\nprint(""Train: {}, Dev: {}, Test: {}"".format(\n    len(train_tensors),\n    len(dev_tensors),\n    len(test_tensors),\n))\n\n\n# In[9]:\n\nembedding_file=""/home/napsternxg/datadrive/Downloads/Glove/glove.6B.100d.txt""\ncache_file=""conll2000.glove.100.npy""\nndims=100\npretrained_embeddings = load_word_vectors(embedding_file, ndims, word_vocab, cache_file)\n\n\n# In[10]:\n\ndef plot_losses(train_losses, eval_losses=None, plot_std=False, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    for losses, color, label in zip(\n        [train_losses, eval_losses],\n        [""0.5"", ""r""],\n        [""Train"", ""Eval""],\n    ):\n        mean_loss, std_loss = zip(*losses)\n        mean_loss = np.array(mean_loss)\n        std_loss = np.array(std_loss)\n        ax.plot(\n            mean_loss, color=color, label=label,\n            linestyle=""-"", \n        )\n        if plot_std:\n            ax.fill_between(\n                np.arange(mean_loss.shape[0]),\n                mean_loss-std_loss,\n                mean_loss+std_loss,\n                color=color,\n                alpha=0.3\n            )\n    ax.set_xlabel(""Epochs"")\n    ax.set_ylabel(""Mean Loss ($\\pm$ S.D.)"")\n    \n    \ndef print_predictions(corpus, predictions, filename, label_vocab):\n    with open(filename, ""w+"") as fp:\n        for seq, pred in zip(corpus, predictions):\n            for (token, true_label), pred_label in zip(seq, pred):\n                pred_label = label_vocab.idx2item[pred_label]\n                print(""{}\\t{}\\t{}"".format(token, true_label, pred_label), file=fp)\n            print(file=fp) # Add new line after each sequence\n\n\n# In[11]:\n\n# ## Class based\n\n# In[19]:\n\nclass BiLSTMTaggerWordCRFModel(ModelWrapper):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False):\n        self.model = model\n        self.loss_function = None\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            #[k.cuda() for k in self.model.modules()]\n            self.model.cuda()\n        \n    def _process_instance_tensors(self, instance_tensors, volatile=False):\n        X, Y, X_char = instance_tensors\n        X = Variable(torch.LongTensor([X]), requires_grad=False, volatile=volatile)\n        Y = torch.LongTensor(Y)\n        X_char = charseq2varlist(X_char, volatile=volatile)\n        return X, X_char, Y\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grad:\n            ## Clear gradients before every update else memory runs out\n            self.model.zero_grad()\n        X, X_char, Y = instance_tensors\n        if self.use_cuda:\n            X = X.cuda(async=True)\n            Y = Y.cuda(async=True)\n            X_char = [t.cuda(async=True) for t in X_char]\n        #print(X.get_device(), [t.get_device() for t in X_char])\n        return self.model.loss(X, X_char, Y)\n        \n    def predict(self, instance_tensors):\n        X, X_char, Y = self._process_instance_tensors(instance_tensors, volatile=True)\n        if self.use_cuda:\n            X = X.cuda(async=True)\n            Y = Y.cuda(async=True)\n            X_char = [t.cuda(async=True) for t in X_char]\n        emissions = self.model.forward(X, X_char)\n        return self.model.crf.forward(emissions)[1]\n\n\nuse_cuda=True\nhidden_size=128\nbatch_size=64\n\nchar_emb_size=50\noutput_channels=25\nkernel_sizes=[2, 3]\n\nword_emb_size=100\nn_embed=150 # Get this using char embedding and word embed\nchar_embed_kwargs=dict(\n    vocab_size=char_vocab.size,\n    embedding_size=char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\n\nword_char_embedding = WordCharEmbedding(\n        word_vocab.size, word_emb_size,\n        char_embed_kwargs, dropout=0, concat=True)\n# Assign glove embeddings\nassign_embeddings(word_char_embedding.word_embeddings, pretrained_embeddings, fix_embedding=True)\n\nmodel_wrapper = BiLSTMTaggerWordCRFModel(\n    LSTMTaggerWordCharCRF(word_char_embedding, n_embed, hidden_size, chunk_vocab.size),\n    None, use_cuda=use_cuda)\n\n\n# In[33]:\nmodel_prefix=""BiLSTMCharConcatCRF_CONLL2000""\nn_epochs=50\ntraining_history = training_wrapper(\n    model_wrapper, train_tensors, \n    eval_tensors=dev_tensors,\n    optimizer=optim.Adam,\n    optimizer_kwargs={\n        #""lr"": 0.01,\n        ""weight_decay"": 0\n    },\n    n_epochs=n_epochs,\n    batch_size=batch_size,\n    use_cuda=use_cuda,\n    log_file=""{}.log"".format(model_prefix)\n)\nmodel_wrapper.save(""{}.pth"".format(model_prefix))\n\n\n# In[34]:\n\nfig, ax = plt.subplots(1,1)\nplot_losses(training_history[""training_loss""],\n            training_history[""evaluation_loss""],\n            plot_std=True,\n            ax=ax)\nax.legend()\nsns.despine(offset=5)\nplt.savefig(""{}.pdf"".format(model_prefix))\n\nfor title, tensors, corpus in zip(\n    [""train"", ""dev"", ""test""],\n    [train_tensors, dev_tensors, test_tensors],\n    [train_corpus, dev_corpus, test_corpus],\n                         ):\n    predictions = model_wrapper.predict_batch(tensors, title=title)\n    print_predictions(corpus, predictions, ""%s.chunking.conll"" % title, chunk_vocab)\n    conll_eval([""conlleval"", ""%s.chunking.conll"" % title]) \n\n\n'"
conlleval.py,0,"b""#!/usr/bin/env python\n\n## Original script taken from https://github.com/spyysalo/conlleval.py\n## Modifications made by Shubhanshu Mishra to support notypes argument and functional api\n\n# Python version of the evaluation script from CoNLL'00-\n\n# Intentional differences:\n# - accept any space as delimiter by default\n# - optional file argument (default STDIN)\n# - option to set boundary (-b argument)\n# - LaTeX output (-l argument) not supported\n# - raw tags (-r argument) not supported\n\nimport sys\nimport re\n\nfrom collections import defaultdict, namedtuple\n\nANY_SPACE = '<SPACE>'\n\nclass FormatError(Exception):\n    pass\n\nMetrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n\nclass EvalCounts(object):\n    def __init__(self):\n        self.correct_chunk = 0    # number of correctly identified chunks\n        self.correct_tags = 0     # number of correct chunk tags\n        self.found_correct = 0    # number of chunks in corpus\n        self.found_guessed = 0    # number of identified chunks\n        self.token_counter = 0    # token counter (ignores sentence breaks)\n\n        # counts by type\n        self.t_correct_chunk = defaultdict(int)\n        self.t_found_correct = defaultdict(int)\n        self.t_found_guessed = defaultdict(int)\n\ndef parse_args(argv):\n    import argparse\n    parser = argparse.ArgumentParser(\n        description='evaluate tagging results using CoNLL criteria',\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    arg = parser.add_argument\n    arg('-b', '--boundary', metavar='STR', default='-X-',\n        help='sentence boundary')\n    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n        help='character delimiting items in input')\n    arg('-o', '--otag', metavar='CHAR', default='O',\n        help='alternative outside tag')\n    arg('-t', '--no-types', action='store_const', const=True, default=False,\n        help='evaluate without entity types')\n    arg('file', nargs='?', default=None)\n    arg('--outstream', default=None,\n        help='output file for storing report')\n    return parser.parse_args(argv)\n\ndef parse_tag(t):\n    m = re.match(r'^([^-]*)-(.*)$', t)\n    return m.groups() if m else (t, '')\n\ndef evaluate(iterable, options=None):\n    if options is None:\n        options = parse_args([])    # use defaults\n    counts = EvalCounts()\n    num_features = None       # number of features per line\n    in_correct = False        # currently processed chunks is correct until now\n    last_correct = 'O'        # previous chunk tag in corpus\n    last_correct_type = ''    # type of previously identified chunk tag\n    last_guessed = 'O'        # previously identified chunk tag\n    last_guessed_type = ''    # type of previous chunk tag in corpus\n    new_sent=True\n\n    for line in iterable:\n        line = line.rstrip('\\r\\n')\n\n        if options.delimiter == ANY_SPACE:\n            features = line.split()\n        else:\n            features = line.split(options.delimiter)[-2:]\n\n        if num_features is None:\n            num_features = len(features)\n        elif num_features != len(features) and len(features) != 0:\n            raise FormatError('unexpected number of features: %d (%d)' %\n                              (len(features), num_features))\n\n        if len(features) == 0 or features[0] == options.boundary:\n            features = ['O', 'O']\n            new_sent=True\n        else:\n            new_sent=False\n        if len(features) < 2:\n            raise FormatError('unexpected number of features in line %s' % line)\n\n        guessed, guessed_type = parse_tag(features.pop())\n        correct, correct_type = parse_tag(features.pop())\n        if options.no_types:\n            guessed_type = ''\n            correct_type = ''\n\n        if new_sent:\n            guessed = 'O'\n\n        end_correct = end_of_chunk(last_correct, correct,\n                                   last_correct_type, correct_type)\n        end_guessed = end_of_chunk(last_guessed, guessed,\n                                   last_guessed_type, guessed_type)\n        start_correct = start_of_chunk(last_correct, correct,\n                                       last_correct_type, correct_type)\n        start_guessed = start_of_chunk(last_guessed, guessed,\n                                       last_guessed_type, guessed_type)\n\n        if in_correct:\n            if (end_correct and end_guessed and\n                last_guessed_type == last_correct_type):\n                in_correct = False\n                counts.correct_chunk += 1\n                counts.t_correct_chunk[last_correct_type] += 1\n            elif (end_correct != end_guessed or guessed_type != correct_type):\n                in_correct = False\n\n        if start_correct and start_guessed and guessed_type == correct_type:\n            in_correct = True\n\n        if start_correct:\n            counts.found_correct += 1\n            counts.t_found_correct[correct_type] += 1\n        if start_guessed:\n            counts.found_guessed += 1\n            counts.t_found_guessed[guessed_type] += 1\n        if not new_sent:\n            if correct == guessed and guessed_type == correct_type:\n                counts.correct_tags += 1\n            counts.token_counter += 1\n\n        last_guessed = guessed\n        last_correct = correct\n        last_guessed_type = guessed_type\n        last_correct_type = correct_type\n\n    if in_correct:\n        counts.correct_chunk += 1\n        counts.t_correct_chunk[last_correct_type] += 1\n\n    return counts\n\ndef uniq(iterable):\n    seen = set()\n    return [i for i in iterable if not (i in seen or seen.add(i))]\n\ndef calculate_metrics(correct, guessed, total):\n    tp, fp, fn = correct, guessed-correct, total-correct\n    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n    return Metrics(tp, fp, fn, p, r, f)\n\ndef metrics(counts):\n    c = counts\n    overall = calculate_metrics(\n        c.correct_chunk, c.found_guessed, c.found_correct\n    )\n    by_type = {}\n    for t in uniq(list(c.t_found_correct.keys()) + list(c.t_found_guessed.keys())):\n        by_type[t] = calculate_metrics(\n            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n        )\n    return overall, by_type\n\ndef report(counts, out=None):\n    if out is None:\n        out = sys.stdout\n\n    overall, by_type = metrics(counts)\n\n    c = counts\n    out.write('processed %d tokens with %d phrases; ' %\n              (c.token_counter, c.found_correct))\n    out.write('found: %d phrases; correct: %d.\\n' %\n              (c.found_guessed, c.correct_chunk))\n\n    if c.token_counter > 0:\n        out.write('accuracy: %6.2f%%; ' %\n                  (100.*c.correct_tags/c.token_counter))\n        out.write('precision: %6.2f%%; ' % (100.*overall.prec))\n        out.write('recall: %6.2f%%; ' % (100.*overall.rec))\n        out.write('FB1: %6.2f\\n' % (100.*overall.fscore))\n\n    for i, m in sorted(by_type.items()):\n        out.write('%17s: ' % i)\n        out.write('precision: %6.2f%%; ' % (100.*m.prec))\n        out.write('recall: %6.2f%%; ' % (100.*m.rec))\n        out.write('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n\ndef end_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk ended between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    chunk_end = False\n\n    if prev_tag == 'E': chunk_end = True\n    if prev_tag == 'U': chunk_end = True\n\n    if prev_tag == 'B' and tag == 'B': chunk_end = True\n    if prev_tag == 'B' and tag == 'U': chunk_end = True\n    if prev_tag == 'B' and tag == 'O': chunk_end = True\n    if prev_tag == 'I' and tag == 'B': chunk_end = True\n    if prev_tag == 'I' and tag == 'U': chunk_end = True\n    if prev_tag == 'I' and tag == 'O': chunk_end = True\n\n    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n        chunk_end = True\n\n    # these chunks are assumed to have length 1\n    if prev_tag == ']': chunk_end = True\n    if prev_tag == '[': chunk_end = True\n\n    return chunk_end\n\ndef start_of_chunk(prev_tag, tag, prev_type, type_):\n    # check if a chunk started between the previous and current word\n    # arguments: previous and current chunk tags, previous and current types\n    chunk_start = False\n\n    if tag == 'B': chunk_start = True\n    if tag == 'U': chunk_start = True\n\n    if prev_tag == 'E' and tag == 'E': chunk_start = True\n    if prev_tag == 'E' and tag == 'I': chunk_start = True\n    if prev_tag == 'U' and tag == 'E': chunk_start = True\n    if prev_tag == 'U' and tag == 'I': chunk_start = True\n    if prev_tag == 'O' and tag == 'E': chunk_start = True\n    if prev_tag == 'O' and tag == 'I': chunk_start = True\n\n    if tag != 'O' and tag != '.' and prev_type != type_:\n        chunk_start = True\n\n    # these chunks are assumed to have length 1\n    if tag == '[': chunk_start = True\n    if tag == ']': chunk_start = True\n\n    return chunk_start\n\ndef evaluate_from_file(filename, argv, outstream=None):\n    args = parse_args(argv[1:])\n    with open(filename) as f:\n        counts = evaluate(f, args)\n    report(counts, outstream)\n    \ndef main(argv, outstream=None):\n    args = parse_args(argv[1:])\n\n    if args.file is None:\n        counts = evaluate(sys.stdin, args)\n    else:\n        with open(args.file) as f:\n            counts = evaluate(f, args)\n    if outstream is not None:\n        args.outstream = outstream\n    report(counts, args.outstream)\n\nif __name__ == '__main__':\n    sys.exit(main(sys.argv))\n"""
pytorch_models.py,24,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport numpy as np\nfrom tqdm import tqdm\n\n\ndef to_scalar(var):\n    # returns a python float\n    return var.view(-1).data.tolist()[0]\n\n\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return to_scalar(idx)\n\n\ndef log_sum_exp_torch(vecs, axis=None):\n    ## Use help from: http://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html#sphx-glr-beginner-nlp-advanced-tutorial-py\n    if axis < 0:\n        axis = vecs.ndimension()+axis\n    max_val, _ = vecs.max(axis)\n    vecs = vecs - max_val.expand_as(vecs)\n    out_val = torch.log(torch.exp(vecs).sum(axis))\n    #print(max_val, out_val)\n    return max_val + out_val\n\n\ndef charseq2varlist(X_chars, volatile=False):\n    return [Variable(torch.LongTensor([x]).pin_memory(), requires_grad=False, volatile=volatile) for x in X_chars]\n\n\ndef assign_embeddings(embedding_module, pretrained_embeddings, fix_embedding=False):\n    embedding_module.weight.data.copy_(torch.from_numpy(pretrained_embeddings))\n    if fix_embedding:\n        embedding_module.weight.requires_grad = False\n\n\nclass ModelWrapper(object):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False\n                ):\n        self.model = model\n        self.loss_function = loss_function\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            self.model.cuda()\n            \n    def batch_process_tensors(self, data_tensors):\n        for instance_tensors in data_tensors:\n            yield self._process_instance_tensors(instance_tensors)\n        \n    def _process_instance_tensors(self, instance_tensors, volatile=False):\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def zero_grad(self):\n        self.model.zero_grad()\n\n    def post_backward(self):\n        ## Implement things like grad clipping or grad norm\n        pass\n        \n    def get_parameters(self):\n        return self.model.paramerters()\n    \n    def set_model_mode(self, training_mode=True):\n        if training_mode:\n            self.model.train()\n        else:\n            self.model.eval()\n            \n    def save(self, filename, verbose=True):\n        torch.save(self.model, filename)\n        if verbose:\n            print(""{} model saved to {}"".format(self.model.__class__, filename))\n        \n    def load(self, filename):\n        self.model = torch.load(filename)\n        if self.use_cuda:\n            self.model.cuda()\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grad:\n        ## Clear gradients before every update else memory runs out\n            self.zero_grad()\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def predict(self, instance_tensors):\n        raise NotImplementedError(""Please define this function explicitly"")\n        \n    def predict_batch(self, batch_tensors, title=""train""):\n        self.model.eval() # Set model to eval mode\n        predictions = []\n        for instance_tensors in tqdm(batch_tensors,\n                desc=""%s predict"" % title, unit=""instance""):\n            predictions.append(self.predict(instance_tensors))\n        return predictions\n        \n        \ndef get_epoch_function(model_wrapper, optimizer,\n                       use_cuda=False):\n    def perform_epoch(data_tensors, training_mode=True, batch_size=1, pbar=None):\n        model_wrapper.set_model_mode(training_mode)\n        step_losses = []\n        len_data_tensors = len(data_tensors)\n        data_tensor_idxs = np.random.permutation(np.arange(len_data_tensors, dtype=""int""))\n        n_splits = data_tensor_idxs.shape[0]//batch_size\n        title = ""train"" if training_mode else ""eval""\n        for batch_tensors_idxs in np.array_split(data_tensor_idxs, n_splits):\n            #from IPython.core.debugger import Tracer; Tracer()()\n            optimizer.zero_grad()\n            #loss = Variable(torch.FloatTensor([0.]))\n            losses = []\n            for instance_tensors_idx in batch_tensors_idxs:\n                instance_tensors = data_tensors[instance_tensors_idx]\n                loss = model_wrapper.get_instance_loss(instance_tensors, zero_grad=False)\n                losses.append(loss)\n                if pbar is not None:\n                    pbar.update(1)\n            loss = torch.mean(torch.cat(losses))\n            #loss = loss/batch_tensors_idxs.shape[0] # Mean loss\n            step_losses.append(loss.data[0])\n            if training_mode:\n                ## Get gradients of model params wrt. loss\n                loss.backward()\n                ## Model grad specific steps like clipping or norm\n                model_wrapper.post_backward()\n                ## Optimize the loss by one step\n                optimizer.step()\n        return step_losses\n    return perform_epoch\n\ndef write_losses(losses, fp, title=""train"", epoch=0):\n    for i, loss in enumerate(losses):\n        print(""{:<10} epoch={:<3} batch={:<5} loss={:<10}"".format(\n            title, epoch, i, loss\n        ), file=fp)\n    print(""{:<10} epoch={:<3} {:<11} mean={:<10.3f} std={:<10.3f}"".format(\n        title, epoch, ""overall"", np.mean(losses), np.std(losses)\n    ), file=fp)\n\n\ndef training_wrapper(\n    model_wrapper, data_tensors,\n    eval_tensors=None,\n    optimizer=optim.SGD,\n    optimizer_kwargs=None,\n    n_epochs=10,\n    batch_size=1,\n    use_cuda=False,\n    log_file=""training_output.log"",\n    early_stopping=None,\n    save_best=False,\n    save_path=""best_model.pth"",\n    reduce_lr_every=5,\n    lr_reduce_factor=0.5\n):\n    """"""Wrapper to train the model\n    """"""\n    if optimizer_kwargs is None:\n        optimizer_kwargs = {}\n    # Fileter out parameters which don\'t require a gradient\n    parameters = filter(lambda p: p.requires_grad, model_wrapper.model.parameters())\n    optimizer=optimizer(parameters, **optimizer_kwargs)\n    # Start training\n    losses = []\n    eval_losses = []\n    ## Covert data tensors to torch tensors\n    data_tensors = list(\n        tqdm(\n            model_wrapper.batch_process_tensors(data_tensors),\n            total=len(data_tensors),\n            desc=""Proc. train tensors"",\n            #leave=False,\n        )\n    )\n    if eval_tensors is not None:\n        eval_tensors = list(\n            tqdm(\n                model_wrapper.batch_process_tensors(eval_tensors),\n                total=len(eval_tensors),\n                desc=""Proc. eval tensors"",\n                #leave=False,\n            )\n        )\n    ## \n    #data_tensors = np.array(data_tensors)\n    #if eval_tensors is not None:\n    #    eval_tensors = np.array(eval_tensors)\n    perform_epoch = get_epoch_function(\n        model_wrapper,\n        optimizer,\n        use_cuda=use_cuda)\n    with open(log_file, ""w+"") as fp:\n        with tqdm(total=n_epochs, desc=""Epochs"", unit=""epochs"") as epoch_progress_bar:\n            for epoch in range(n_epochs):\n                with tqdm(\n                    total=len(data_tensors),\n                    desc=""Train"", unit=""instance"", leave=False\n                    ) as train_progress_bar:\n                    step_losses = perform_epoch(data_tensors, batch_size=batch_size, pbar=train_progress_bar)\n                    mean_loss, std_loss = np.mean(step_losses), np.std(step_losses)\n                    losses.append((mean_loss, std_loss))\n                    write_losses(step_losses, fp, title=""train"", epoch=epoch)\n                if eval_tensors is not None:\n                    with tqdm(\n                        total=len(eval_tensors),\n                        desc=""Eval"", unit=""instance"", leave=False) as eval_progress_bar:\n                        step_losses = perform_epoch(eval_tensors, training_mode=False, pbar=eval_progress_bar)\n                        mean_loss, std_loss = np.mean(step_losses), np.std(step_losses)\n                        eval_losses.append((mean_loss, std_loss))\n                        write_losses(step_losses, fp, title=""eval"", epoch=epoch)\n                epoch_progress_bar.update(1)\n                if early_stopping is not None and epoch > 1:\n                    assert isinstance(early_stopping, float), ""early_stopping should be either None or float value. Got {}"".format(early_stopping)\n                    eval_loss_diff = np.abs(eval_losses[-2][0] - eval_losses[-1][0])\n                    if eval_loss_diff < early_stopping:\n                        epoch_progress_bar.write(""Evaluation loss stopped decreased less than {}. Early stopping at epoch {}."".format(early_stopping, epoch))\n                        break\n                if save_best and save_path is not None:\n                    if epoch == 0:\n                        best_eval_loss = eval_losses[-1][0]\n                        best_epoch = epoch\n                        model_wrapper.save(save_path, verbose=False)\n                        continue\n                    # Save the best model\n                    if eval_losses[-1][0] < best_eval_loss:\n                        best_eval_loss = eval_losses[-1][0]\n                        best_epoch = epoch\n                        model_wrapper.save(save_path, verbose=False)\n                    if epoch == n_epochs -1:\n                        epoch_progress_bar.write(""Best model from {} epoch with {:3f} loss"".format(best_epoch, best_eval_loss))\n\n                if reduce_lr_every > 0 and lr_reduce_factor > 0 and ((epoch + 1) % reduce_lr_every) == 0:\n                    for param_group in optimizer.param_groups:\n                        param_group[\'lr\'] = param_group[\'lr\']*lr_reduce_factor\n\n\n    return {\n        ""training_loss"": losses,\n        ""evaluation_loss"": eval_losses\n    }\n\n\n\n\nclass BoWModule(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(BoWModule, self).__init__()\n        self.W = nn.Linear(input_size, output_size)\n        \n    def forward(self, X):\n        return F.log_softmax(self.W(X))\n\n\nclass BoEmbeddingsModule(nn.Module):\n    def __init__(self, vocab_size, embedding_size, output_size):\n        super(BoEmbeddingsModule, self).__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.W = nn.Linear(embedding_size, output_size)\n        \n    def forward(self, X):\n        hidden_layer = self.word_embeddings(X).mean(1).view(-1,self.word_embeddings.embedding_dim)\n        return F.log_softmax(self.W(hidden_layer))\n    \n\n    \nclass LSTMPredictor(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, output_size):\n        super(LSTMPredictor, self).__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size)\n        self.output = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, X):\n        seq_embed = self.word_embeddings(X).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        output = self.output(out[-1, :, :])\n        return F.log_softmax(output)    \n\n    \nclass LSTMTagger(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, output_size):\n        super(LSTMTagger, self).__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size)\n        self.output = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, X):\n        seq_embed = self.word_embeddings(X).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        # Reshape the output to be a tensor of shape seq_len*label_size\n        output = self.output(out.view(X.data.size(1), -1))\n        return F.log_softmax(output)\n    \n    \nclass CharEmbedding(nn.Module):\n    def __init__(self, vocab_size, embedding_size,\n                 out_channels, kernel_sizes, dropout=0.5):\n        super(CharEmbedding, self).__init__()\n        self.char_embeddings = nn.Embedding(vocab_size, embedding_size)\n        # Usage of nn.ModuleList is important\n        ## See: https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219/6\n        self.convs1 = nn.ModuleList([nn.Conv2d(1, out_channels, (K, embedding_size), padding=(K-1, 0)) \n                       for K in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, X):\n        x = self.char_embeddings(X)\n        x = self.dropout(x)\n        # Ref: https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py\n        x = x.unsqueeze(1) # (N,Ci,W,D)\n        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n        x = torch.cat(x, 1)\n        return self.dropout(x)\n    \n    \nclass WordCharEmbedding(nn.Module):\n    def __init__(self,\n            vocab_size, embedding_size,\n            char_embed_kwargs, dropout=0.5,\n            aux_embedding_size=None,\n            concat=False\n            ):\n        super(WordCharEmbedding, self).__init__()\n        self.char_embeddings = CharEmbedding(**char_embed_kwargs)\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        if concat and aux_embedding_size is not None:\n            ## Only allow aux embedding in concat mode\n            self.aux_word_embeddings = nn.Embedding(vocab_size, aux_embedding_size)\n        self.concat = concat\n        \n    def forward(self, X, X_char=None):\n        # Ref: https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py\n        word_vecs = self.word_embeddings(X)\n        if X_char is not None:\n            char_vecs = torch.cat([\n                self.char_embeddings(x).unsqueeze(0)\n                for x in X_char\n            ], 1)\n            if self.concat:\n                embedding_list = [char_vecs, word_vecs]\n                if hasattr(self, ""aux_word_embeddings""):\n                    aux_vecs = self.aux_word_embeddings(X)\n                    embedding_list.append(aux_vecs)\n                word_vecs = torch.cat(embedding_list, 2)\n            else:\n                word_vecs = char_vecs + word_vecs\n        return self.dropout(word_vecs)\n\nclass WordCharEmbedding_tuple(nn.Module):\n    def __init__(self,\n            vocab_size, embedding_size,\n            char_embed_kwargs, dropout=0.5,\n            aux_embedding_size=None,\n            concat=False\n            ):\n        super(WordCharEmbedding_tuple, self).__init__()\n        self.char_embeddings = CharEmbedding(**char_embed_kwargs)\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.dropout = nn.Dropout(dropout)\n        self.concat = concat\n        if concat and aux_embedding_size is not None:\n            ## Only allow aux embedding in concat mode\n            self.aux_word_embeddings = nn.Embedding(vocab_size, aux_embedding_size)\n        \n    def forward(self, X):\n        if isinstance(X, tuple):\n            X, X_char = X\n        # Ref: https://github.com/Shawn1993/cnn-text-classification-pytorch/blob/master/model.py\n        word_vecs = self.word_embeddings(X)\n        if X_char is not None:\n            char_vecs = torch.cat([\n                self.char_embeddings(x).unsqueeze(0)\n                for x in X_char\n            ], 1)\n            if self.concat:\n                embedding_list = [char_vecs, word_vecs]\n                if hasattr(self, ""aux_word_embeddings""):\n                    aux_vecs = self.aux_word_embeddings(X)\n                    embedding_list.append(aux_vecs)\n                word_vecs = torch.cat(embedding_list, 2)\n            else:\n                word_vecs = char_vecs + word_vecs\n        return self.dropout(word_vecs)\n\nclass ConcatInputs(nn.Module):\n    def __init__(self, input_modules, dim=2):\n        super(ConcatInputs, self).__init__()\n        assert isinstance(input_modules, list), ""Modules should be a list of input modules""\n        self.input_modules = nn.ModuleList(input_modules)\n        self.dim = dim\n\n    def forward(self, X):\n        assert isinstance(X, list), ""X should be a list of input variables""\n        concat_vecs = torch.cat([self.input_modules[i](x) for i,x in enumerate(X)], self.dim)\n        return concat_vecs\n\n\n    \nclass LSTMTaggerWordChar(nn.Module):\n    def __init__(self, word_char_embedding, embedding_size, hidden_size, output_size):\n        super(LSTMTaggerWordChar, self).__init__()\n        self.word_embeddings = word_char_embedding\n        self.lstm = nn.LSTM(embedding_size, hidden_size//2, bidirectional=True)\n        self.output = nn.Linear(hidden_size, output_size)\n        \n    def forward(self, X, X_char):\n        seq_embed = self.word_embeddings(X, X_char).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        # Reshape the output to be a tensor of shape seq_len*label_size\n        output = self.output(out.view(X.data.size(1), -1))\n        return F.log_softmax(output)\n    \n    \n    \n    \nclass CRFLayer(nn.Module):\n    def __init__(self, num_labels):\n        super(CRFLayer, self).__init__()\n        self.num_labels = num_labels\n        self.transitions = nn.Parameter(torch.randn(self.num_labels, self.num_labels))\n        \n    def _forward_alg(self, emissions):\n        scores = emissions[0]\n        # Get the log sum exp score\n        transitions = self.transitions.transpose(-1,-2)\n        for i in range(1, emissions.size(0)):\n            scores = emissions[i] + log_sum_exp_torch(\n                scores.expand_as(transitions) + transitions,\n                axis=1)\n        return log_sum_exp_torch(scores, axis=-1)\n        \n    def _score_sentence(self, emissions, tags):\n        score = emissions[0][tags[0]]\n        if emissions.size()[0] < 2:\n            return score\n        for i, emission in enumerate(emissions[1:]):\n            score = score + self.transitions[tags[i], tags[i+1]] + emission[tags[i+1]]\n        return score\n    \n    def _viterbi_decode(self, emissions):\n        emissions = emissions.data.cpu()\n        scores = torch.zeros(emissions.size(1))\n        back_pointers = torch.zeros(emissions.size()).int()\n        scores = scores + emissions[0]\n        transitions = self.transitions.data.cpu()\n        # Generate most likely scores and paths for each step in sequence\n        for i in range(1, emissions.size(0)):\n            scores_with_transitions = scores.unsqueeze(1).expand_as(transitions) + transitions\n            max_scores, back_pointers[i] = torch.max(scores_with_transitions, 0)\n            scores = emissions[i] + max_scores\n        # Generate the most likely path\n        viterbi = [scores.numpy().argmax()]\n        back_pointers = back_pointers.numpy()\n        for bp in reversed(back_pointers[1:]):\n            viterbi.append(bp[viterbi[-1]])\n        viterbi.reverse()\n        viterbi_score = scores.numpy().max()\n        return viterbi_score, viterbi\n        \n    def neg_log_likelihood(self, feats, tags):\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n        \n    def forward(self, feats):\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(feats)\n        return score, tag_seq\n    \n    \nclass BiLSTMTaggerWordCRF(nn.Module):\n    def __init__(self, vocab_size, embedding_size, hidden_size, output_size):\n        super(BiLSTMTaggerWordCRF, self).__init__()\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_size)\n        self.lstm = nn.LSTM(embedding_size, hidden_size//2, bidirectional=True)\n        self.output = nn.Linear(hidden_size, output_size)\n        self.crf = CRFLayer(output_size)\n        \n    def forward(self, X):\n        seq_embed = self.word_embeddings(X).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        # Reshape the output to be a tensor of shape seq_len*label_size\n        output = self.output(out.view(X.data.size(1), -1))\n        return output\n    \n    def loss(self, X, Y):\n        feats = self.forward(X)\n        return self.crf.neg_log_likelihood(feats, Y)\n    \n    \nclass LSTMTaggerWordCharCRF(nn.Module):\n    def __init__(self, word_char_embedding, embedding_size, hidden_size, output_size):\n        super(LSTMTaggerWordCharCRF, self).__init__()\n        self.word_embeddings = word_char_embedding\n        self.lstm = nn.LSTM(embedding_size, hidden_size//2, bidirectional=True)\n        self.output = nn.Linear(hidden_size, output_size)\n        self.crf = CRFLayer(output_size)\n        \n    def forward(self, X, X_char):\n        seq_embed = self.word_embeddings(X, X_char).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        # Reshape the output to be a tensor of shape seq_len*label_size\n        output = self.output(out.view(X.data.size(1), -1))\n        return output\n    \n    def loss(self, X, X_char, Y):\n        feats = self.forward(X, X_char)\n        return self.crf.neg_log_likelihood(feats, Y)\n    \nclass BiLSTMTaggerWordCharCRF(nn.Module):\n    def __init__(self, input_embedding, embedding_size, hidden_size, output_size):\n        super(BiLSTMTaggerWordCharCRF, self).__init__()\n        self.input_embedding = input_embedding\n        self.lstm = nn.LSTM(embedding_size, hidden_size//2, bidirectional=True)\n        self.output = nn.Linear(hidden_size, output_size)\n        self.crf = CRFLayer(output_size)\n        \n    def forward(self, X):\n        seq_embed = self.input_embedding(X).permute(1, 0, 2)\n        out, hidden = self.lstm(seq_embed)\n        # Reshape the output to be a tensor of shape seq_len*label_size\n        output = self.output(out.view(out.data.size(0), -1))\n        return output\n    \n    def loss(self, X, Y):\n        feats = self.forward(X)\n        return self.crf.neg_log_likelihood(feats, Y)\n    \n'"
pytorch_utils.py,5,"b'import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom pathlib import Path\n\n\nclass Vocab(object):\n    def __init__(self, name=""vocab"",\n                 offset_items=tuple([]),\n                 UNK=None, lower=True):\n        self.name = name\n        self.item2idx = {}\n        self.idx2item = []\n        self.size = 0\n        self.UNK = UNK\n        self.lower=lower\n        \n        self.batch_add(offset_items, lower=False)\n        if UNK is not None:\n            self.add(UNK, lower=False)\n            self.UNK_ID = self.item2idx[self.UNK]\n        self.offset = self.size\n        \n    def add(self, item, lower=True):\n        if self.lower and lower:\n            item = item.lower()\n        if item not in self.item2idx:\n            self.item2idx[item] = self.size\n            self.size += 1\n            self.idx2item.append(item)\n            \n    def batch_add(self, items, lower=True):\n        for item in items:\n            self.add(item, lower=lower)\n            \n    def in_vocab(self, item, lower=True):\n        if self.lower and lower:\n            item = item.lower()\n        return item in self.item2idx\n        \n    def getidx(self, item, lower=True):\n        if self.lower and lower:\n            item = item.lower()\n        if item not in self.item2idx:\n            if self.UNK is None:\n                raise RuntimeError(""UNK is not defined. %s not in vocab."" % item)\n            return self.UNK_ID\n        return self.item2idx[item]\n            \n    def __repr__(self):\n        return ""Vocab(name={}, size={:d}, UNK={}, offset={:d}, lower={})"".format(\n            self.name, self.size,\n            self.UNK, self.offset,\n            self.lower\n        )\n    \n    \ndef load_word_vectors(vector_file, ndims, vocab, cache_file, override_cache=False):\n    W = np.zeros((vocab.size, ndims), dtype=""float32"")\n    # Check for cached file and return vectors\n    cache_file = Path(cache_file)\n    if cache_file.is_file() and not override_cache:\n        W = np.load(cache_file)\n        return W\n    # Else load vectors from the vector file\n    total, found = 0, 0\n    with open(vector_file) as fp:\n        for i, line in enumerate(fp):\n            line = line.rstrip().split()\n            if line:\n                total += 1\n                try:\n                    assert len(line) == ndims+1,(\n                        ""Line[{}] {} vector dims {} doesn\'t match ndims={}"".format(i, line[0], len(line)-1, ndims)\n                    )\n                except AssertionError as e:\n                    print(e)\n                    continue\n                word = line[0]\n                idx = vocab.getidx(word) \n                if idx >= vocab.offset:\n                    found += 1\n                    vecs = np.array(list(map(float, line[1:])))\n                    W[idx, :] += vecs\n    # Write to cache file\n    print(""Found {} [{:.2f}%] vectors from {} vectors in {} with ndims={}"".format(\n        found, found * 100/vocab.size, total, vector_file, ndims))\n    norm_W = np.sqrt((W*W).sum(axis=1, keepdims=True))\n    valid_idx = norm_W.squeeze() != 0\n    W[valid_idx, :] /= norm_W[valid_idx]\n    print(""Caching embedding with shape {} to {}"".format(W.shape, cache_file.as_posix()))\n    np.save(cache_file, W)\n    return W    \n    \nclass Seq2Vec(object):\n    def __init__(self, vocab):\n        self.vocab = vocab\n        \n    def encode(self, seq):\n        vec = []\n        for item in seq:\n            vec.append(self.vocab.getidx(item))\n        return vec\n    \n    def batch_encode(self, seq_batch):\n        vecs = [self.encode(seq) for seq in seq_batch]\n        return vecs\n        \n        \nclass Seq2OneHot(object):\n    def __init__(self, size):\n        self.size = size\n    \n    def encode(self, x, as_variable=False):\n        one_hot = torch.zeros(self.size)\n        for i in x:\n            one_hot[i] += 1\n        one_hot = one_hot.view(1, -1)\n        if as_variable:\n            return Variable(one_hot)\n        return one_hot\n    \n    \ndef print_log_probs(log_probs, label_vocab, label_true=None):\n    for i, label_probs in enumerate(log_probs.data.tolist()):\n        prob_string = "", "".join([\n            ""{}: {:.3f}"".format(label_vocab.idx2item[j], val)\n            for j, val in enumerate(label_probs)\n        ])\n        true_string = ""?""\n        if label_true is not None:\n            true_string = label_vocab.idx2item[label_true[i]]\n            \n        print(prob_string, ""True label: "", true_string)    \n'"
utils.py,0,"b'import numpy as np\nfrom collections import Counter\nimport pandas as pd\nimport re\n\n#import tensorflow as tf\n\nfrom sklearn.cluster import KMeans\n\n\ndef get_clusters(W_word, n_clusters=10, **kwargs):\n    clusterer = KMeans(n_clusters=n_clusters,\n            n_jobs=-1, **kwargs)\n    cluster_labels = clusterer.fit_predict(W_word)\n    return cluster_labels\n\n\ndef read_glove(filename,\n               ndims=50):\n    vocab = []\n    char_vocab = Counter()\n    W = []\n    with open(filename) as fp:\n        for line in fp:\n            line = line.rstrip().split()\n            word = line[0]\n            embed = list(map(float, line[1:]))\n            vocab.append(word)\n            W.append(embed)\n            char_vocab.update(list(word))\n    return vocab, char_vocab, np.array(W)\n\n\ndef crf_loss(y_true, y_pred):\n    y_true = tf.cast(tf.squeeze(y_true), tf.int32)\n    seq_lengths_t = tf.reduce_sum(\n            tf.cast(tf.not_equal(y_true, 0),\n                tf.int32), axis=-1)\n    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(\n            y_pred, y_true, seq_lengths_t)\n    return tf.reduce_mean(-log_likelihood, axis=-1)\n\n\ndef load_sequences(filenames, sep="" "", col_ids=None):\n    sequences = []\n    if isinstance(filenames, str):\n        filenames = [filenames]\n    for filename in filenames:\n        with open(filename, encoding=\'utf-8\') as fp:\n            seq = []\n            for line in fp:\n                line = line.rstrip()\n                if line:\n                    line = line.split(sep)\n                    if col_ids is not None:\n                        line = [line[idx] for idx in col_ids]\n                    seq.append(tuple(line))\n                else:\n                    if seq:\n                        sequences.append(seq)\n                    seq = []\n            if seq:\n                sequences.append(seq)\n    return sequences\n\n\ndef classification_report_to_df(report):\n    report_list = []\n    for i, line in enumerate(report.split(""\\n"")):\n        if i == 0:\n            report_list.append([""class"", ""precision"", ""recall"", ""f1-score"", ""support""])\n        else:\n            line = line.strip()\n            if line:\n                if line.startswith(""avg""):\n                    line = line.replace(""avg / total"", ""avg/total"")\n                line = re.split(r\'\\s+\', line)\n                line = [line[0]] + list(map(float, line[1:-1])) + [int(line[-1])]\n                report_list.append(tuple(line))\n    return pd.DataFrame(report_list[1:], columns=report_list[0])  \n\n\ndef conll_classification_report_to_df(report):\n    report_list = []\n    report_list.append([""class"", ""accuracy"", ""precision"", ""recall"", ""f1-score"", ""support""])\n    for i, line in enumerate(report.split(""\\n"")):\n        line = line.strip()\n        if not line:\n            continue\n        if i == 0:\n            continue\n        if i == 1:\n            line = re.findall(\n                \'accuracy:\\s*([0-9\\.]{4,5})%; precision:\\s+([0-9\\.]{4,5})%; recall:\\s+([0-9\\.]{4,5})%; FB1:\\s+([0-9\\.]{4,5})\',\n                line)[0]\n            line = (""overall"",) + tuple(map(float, line)) + (0,)\n        else:\n            line = re.findall(\n                \'\\s*(.+?): precision:\\s+([0-9\\.]{4,5})%; recall:\\s+([0-9\\.]{4,5})%; FB1:\\s+([0-9\\.]{4,5})\\s+([0-9]+)\',\n                line)[0]\n            line = (line[0], 0.0) + tuple(map(float, line[1:-1])) + (int(line[-1]),)\n        report_list.append(line)\n    return pd.DataFrame(report_list[1:], columns=report_list[0])\n\n\ndef get_labels(y_arr):\n    return np.expand_dims(\n        np.array([\n            np.zeros(max_len)\n            if y is None else y\n            for y in y_arr],\n            dtype=\'int\'),\n        -1)\n\n\n\ndef create_tagged_sequence(seq, task2col, default_tag):\n    seq_tags = []\n    for t in seq:\n        try:\n            tag = default_tag._replace(token=t[0], **{ti: t[ci] for ti, ci in task2col.items()})\n        except:\n            print(""Error processing tag:"", t)\n            print(""Error in sequence: "", seq)\n            raise\n        seq_tags.append(tag)\n    return seq_tags        \n\n\ndef get_tagged_corpus(corpus, *args):\n    max_len = 0\n    for seq in corpus:\n        if seq:\n            max_len = max(len(seq), max_len)\n            yield create_tagged_sequence(seq, *args)\n    print(""Max sequence length in the corpus is: %s"" % max_len)\n\ndef gen_vocab_counts(corpus, tasks, include_chars=False, token_counts=None):\n    task_counts = {k: Counter() for k in tasks}\n    if token_counts is None:\n        token_counts = Counter()\n    max_seq_len = 0\n    max_word_len = 0\n    if include_chars:\n        char_counts = Counter()\n    for seq in corpus:\n        max_seq_len = max(len(seq), max_seq_len)\n        for t in seq:\n            token_counts[t.token] += 1\n            if include_chars:\n                char_counts.update(list(t.token))\n                max_word_len = max(len(t.token), max_word_len)\n            for k in task_counts:\n                v = getattr(t, k)\n                if v is not None:\n                    task_counts[k][v] += 1\n    if include_chars:\n        return token_counts, task_counts, max_seq_len, char_counts, max_word_len\n    return token_counts, task_counts, max_seq_len\n\ndef print_predictions(tagged_seq, predictions, filename, label_id=0, task_id=0):\n    from sklearn.metrics import classification_report, accuracy_score\n    y_true, y_pred = [], []\n    with open(filename, ""w+"") as fp:\n        for seq, pred in zip(tagged_seq, predictions[label_id]):\n            for tag, label in zip(seq, pred):\n                true_label = tag[task_id+1]\n                print(u""%s\\t%s\\t%s"" % (tag[0], true_label, label), file=fp)\n                y_true.append(true_label)\n                y_pred.append(label)\n            print(u"""", file=fp) \n    \n    report = classification_report(y_true, y_pred)\n    print(report)\n    print(""Accuracy: %s"" % accuracy_score(y_true, y_pred))\n    return classification_report_to_df(report)\n\n\n\n'"
wnut_bilstm_crf_char_concat.py,9,"b'\n# coding: utf-8\n\n# In[1]:\n\nimport matplotlib\nmatplotlib.use(""Agg"")\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)\n\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom pytorch_utils import *\nfrom pytorch_models import *\nfrom utils import load_sequences, conll_classification_report_to_df\nfrom conlleval import main as conll_eval\nimport re\n\nsns.set_context(""poster"")\nsns.set_style(""ticks"")\n\n\n# In[2]:\n\nTRAIN_CORPUS=""data/WNUT_NER/train.tsv""\nDEV_CORPUS=""data/WNUT_NER/dev.tsv""\nTEST_CORPUS=""data/WNUT_NER/test.tsv""\n\n\n# In[3]:\n\ntrain_corpus = load_sequences(TRAIN_CORPUS, sep=""\\t"", col_ids=(0, -1))\nprint(""Total items in train corpus: %s"" % len(train_corpus))\ndev_corpus = load_sequences(DEV_CORPUS, sep=""\\t"", col_ids=(0, -1))\nprint(""Total items in dev corpus: %s"" % len(dev_corpus))\ntest_corpus = load_sequences(TEST_CORPUS, sep=""\\t"", col_ids=(0, -1))\nprint(""Total items in test corpus: %s"" % len(test_corpus))\n\n\n# In[5]:\nCAP_LETTERS=re.compile(r\'[A-Z]\')\nSMALL_LETTERS=re.compile(r\'[a-z]\')\nNUMBERS=re.compile(r\'[0-9]\')\nPUNCT=re.compile(r\'[\\.,\\""\\\'!\\?;:]\')\nOTHERS=re.compile(r\'[^A-Za-z0-9\\.,\\""\\\'!\\?;:]\')\n\ndef get_ortho_feature(word):\n    word = CAP_LETTERS.sub(""A"", word)\n    word = SMALL_LETTERS.sub(""a"", word)\n    word = NUMBERS.sub(""0"", word)\n    word = PUNCT.sub(""."", word)\n    word = OTHERS.sub(""%"", word)\n    return word\n\ndef create_vocab(data, vocabs, char_vocab, ortho_word_vocab, ortho_char_vocab, word_idx=0):\n    n_vocabs = len(vocabs)\n    for sent in data:\n        for token_tags in sent:\n            for vocab_id in range(n_vocabs):\n                vocabs[vocab_id].add(token_tags[vocab_id])\n            char_vocab.batch_add(token_tags[word_idx])\n            ortho_word = get_ortho_feature(token_tags[word_idx])\n            ortho_word_vocab.add(ortho_word)\n            ortho_char_vocab.batch_add(ortho_word)\n    print(""Created vocabs: %s"" % ("", "".join(\n        ""{}[{}]"".format(vocab.name, vocab.size)\n        for vocab in vocabs + [char_vocab, ortho_word_vocab, ortho_char_vocab]\n    )))\n\n\n# In[6]:\n\nword_vocab = Vocab(""words"", UNK=""UNK"", lower=True)\nchar_vocab = Vocab(""chars"", UNK=""<U>"", lower=False)\northo_word_vocab = Vocab(""ortho_words"", UNK=""UNK"", lower=True)\northo_char_vocab = Vocab(""ortho_chars"", UNK=""<U>"", lower=False)\nner_vocab = Vocab(""ner_tags"", lower=False)\n\ncreate_vocab(train_corpus+dev_corpus+test_corpus, [word_vocab, ner_vocab], char_vocab, ortho_word_vocab, ortho_char_vocab)\n\n\n# In[7]:\n\ndef data2tensors(data, vocabs, char_vocab, ortho_word_vocab, ortho_char_vocab, word_idx=0, column_ids=(0, -1)):\n    vocabs = [vocabs[idx] for idx in column_ids]\n    n_vocabs = len(vocabs)\n    tensors = []\n    char_tensors = []\n    for sent in data:\n        sent_vecs = [[] for i in range(n_vocabs+3)] # Last 3 are for char vecs, ortho_word and ortho_char\n        char_vecs = []\n        for token_tags in sent:\n            vocab_id = 0 # First column is the word\n            ortho_word = get_ortho_feature(token_tags[vocab_id])\n            # lowercase the word\n            sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id].lower())\n                )\n            for vocab_id in range(1, n_vocabs):\n                sent_vecs[vocab_id].append(\n                    vocabs[vocab_id].getidx(token_tags[vocab_id])\n                )\n            sent_vecs[-3].append(\n                [char_vocab.getidx(c) for c in token_tags[word_idx]]\n            )\n            sent_vecs[-2].append(\n                    ortho_word_vocab.getidx(ortho_word)\n                )\n            sent_vecs[-1].append(\n                [ortho_char_vocab.getidx(c) for c in ortho_word]\n            )\n        tensors.append(sent_vecs)\n    return tensors\n\n\n# In[8]:\n\ntrain_tensors = data2tensors(train_corpus, [word_vocab, ner_vocab], char_vocab, ortho_word_vocab, ortho_char_vocab)\ndev_tensors = data2tensors(dev_corpus, [word_vocab, ner_vocab], char_vocab, ortho_word_vocab, ortho_char_vocab)\ntest_tensors = data2tensors(test_corpus, [word_vocab, ner_vocab], char_vocab, ortho_word_vocab, ortho_char_vocab)\nprint(""Train: ({}, {}), Dev: ({}, {}), Test: ({}, {})"".format(\n    len(train_tensors), len(train_tensors[0]),\n    len(dev_tensors), len(dev_tensors[0]),\n    len(test_tensors), len(test_tensors[0])\n))\n\n\n# In[9]:\n\nembedding_file=""data/WNUT_NER/wnut_vecs.txt""\ncache_file=""wnut_ner.twitter.400.npy""\nndims=400\npretrained_embeddings = load_word_vectors(embedding_file, ndims, word_vocab, cache_file)\n\n\n# In[10]:\n\ndef plot_losses(train_losses, eval_losses=None, plot_std=False, ax=None):\n    if ax is None:\n        ax = plt.gca()\n    for losses, color, label in zip(\n        [train_losses, eval_losses],\n        [""0.5"", ""r""],\n        [""Train"", ""Eval""],\n    ):\n        mean_loss, std_loss = zip(*losses)\n        mean_loss = np.array(mean_loss)\n        std_loss = np.array(std_loss)\n        ax.plot(\n            mean_loss, color=color, label=label,\n            linestyle=""-"", \n        )\n        if plot_std:\n            ax.fill_between(\n                np.arange(mean_loss.shape[0]),\n                mean_loss-std_loss,\n                mean_loss+std_loss,\n                color=color,\n                alpha=0.3\n            )\n    ax.set_xlabel(""Epochs"")\n    ax.set_ylabel(""Mean Loss ($\\pm$ S.D.)"")\n    \n    \ndef print_predictions(corpus, predictions, filename, label_vocab):\n    with open(filename, ""w+"") as fp:\n        for seq, pred in zip(corpus, predictions):\n            for (token, true_label), pred_label in zip(seq, pred):\n                pred_label = label_vocab.idx2item[pred_label]\n                print(""{}\\t{}\\t{}"".format(token, true_label, pred_label), file=fp)\n            print(file=fp) # Add new line after each sequence\n\n\n# In[11]:\n\n# ## Class based\n\n# In[19]:\n\nclass BiLSTMTaggerWordCRFModel(ModelWrapper):\n    def __init__(self, model,\n                 loss_function,\n                 use_cuda=False, grad_max_norm=5):\n        self.model = model\n        self.loss_function = None\n        self.grad_max_norm=grad_max_norm\n\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            #[k.cuda() for k in self.model.modules()]\n            self.model.cuda()\n\n    def post_backward(self):\n        torch.nn.utils.clip_grad_norm(self.model.parameters(), self.grad_max_norm)\n\n    def _process_instance_tensors(self, instance_tensors, volatile=False):\n        X, Y, X_char, X_ortho, X_char_ortho = instance_tensors\n        X = Variable(torch.LongTensor([X]), requires_grad=False, volatile=volatile)\n        X_char = charseq2varlist(X_char, volatile=volatile)\n        X_ortho = Variable(torch.LongTensor([X_ortho]), requires_grad=False, volatile=volatile)\n        X_char_ortho = charseq2varlist(X_char_ortho, volatile=volatile)\n        Y = torch.LongTensor(Y)\n        return X, X_char, X_ortho, X_char_ortho, Y\n\n    def get_instance_loss(self, instance_tensors, zero_grad=True):\n        if zero_grad:\n            ## Clear gradients before every update else memory runs out\n            self.model.zero_grad()\n        X, X_char, X_ortho, X_char_ortho, Y = instance_tensors\n        if self.use_cuda:\n            X = X.cuda(async=True)\n            X_char = [t.cuda(async=True) for t in X_char]\n            X_ortho = X_ortho.cuda(async=True)\n            X_char_ortho = [t.cuda(async=True) for t in X_char_ortho]\n            Y = Y.cuda(async=True)\n        return self.model.loss([(X, X_char), (X_ortho, X_char_ortho)], Y)\n        \n    def predict(self, instance_tensors):\n        X, X_char, X_ortho, X_char_ortho, Y = self._process_instance_tensors(instance_tensors, volatile=True)\n        if self.use_cuda:\n            X = X.cuda(async=True)\n            X_char = [t.cuda(async=True) for t in X_char]\n            X_ortho = X_ortho.cuda(async=True)\n            X_char_ortho = [t.cuda(async=True) for t in X_char_ortho]\n            Y = Y.cuda(async=True)\n        emissions = self.model.forward([(X, X_char), (X_ortho, X_char_ortho)])\n        return self.model.crf.forward(emissions)[1]\n\n\nuse_cuda=True\nhidden_size=128\nbatch_size=64\n\nchar_emb_size=30\noutput_channels=200\nkernel_sizes=[3]\n\nword_emb_size=400\naux_emb_size=100\n\nmain_total_emb_dims=700\nchar_embed_kwargs=dict(\n    vocab_size=char_vocab.size,\n    embedding_size=char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\n\nword_char_embedding = WordCharEmbedding_tuple(\n        word_vocab.size, word_emb_size,\n        char_embed_kwargs, dropout=0.5,\n        aux_embedding_size=aux_emb_size,\n        concat=True)\n\n\northo_char_emb_size=30\noutput_channels=200\nkernel_sizes=[3]\northo_word_emb_size=200\northo_total_emb_dims=400\n\northo_char_embed_kwargs=dict(\n    vocab_size=ortho_char_vocab.size,\n    embedding_size=ortho_char_emb_size,\n    out_channels=output_channels,\n    kernel_sizes=kernel_sizes\n)\n\northo_word_char_embedding = WordCharEmbedding_tuple(\n        ortho_word_vocab.size, ortho_word_emb_size,\n        ortho_char_embed_kwargs, dropout=0.5, concat=True)\n\n\nconcat_embeddings = ConcatInputs([word_char_embedding, ortho_word_char_embedding])\n\n# Assign glove embeddings\nassign_embeddings(word_char_embedding.word_embeddings, pretrained_embeddings, fix_embedding=True)\n\nn_embed=main_total_emb_dims + ortho_total_emb_dims # Get this using char embedding and word embed and ortho embeddings\nmodel_wrapper = BiLSTMTaggerWordCRFModel(\n    BiLSTMTaggerWordCharCRF(concat_embeddings, n_embed, hidden_size, ner_vocab.size),\n    None, use_cuda=use_cuda, grad_max_norm=5)\n\n\n# In[33]:\nmodel_prefix=""BiLSTMCharConcatCRF_WNUT_NER_ortho""\nn_epochs=50\n\nload_model = True\n\nif load_model:\n    model_wrapper.load(""{}.pth"".format(model_prefix))\n    print(""Loaded model from {}.pth"".format(model_prefix))\n\ntraining_history = training_wrapper(\n    model_wrapper, train_tensors, \n    eval_tensors=dev_tensors,\n    optimizer=optim.Adam,\n    optimizer_kwargs={\n        ""lr"": 0.1,\n        ""weight_decay"": 1e-2\n    },\n    n_epochs=n_epochs,\n    batch_size=batch_size,\n    use_cuda=use_cuda,\n    log_file=""{}.log"".format(model_prefix),\n    #early_stopping=0.001,\n    save_best=True,\n    save_path=""{}.pth"".format(model_prefix)\n)\n#model_wrapper.save(""{}.pth"".format(model_prefix))\nmodel_wrapper.load(""{}.pth"".format(model_prefix))\n\n# In[34]:\n\nfig, ax = plt.subplots(1,1)\nplot_losses(training_history[""training_loss""],\n            training_history[""evaluation_loss""],\n            plot_std=True,\n            ax=ax)\nax.legend()\nsns.despine(offset=5)\nplt.savefig(""{}.pdf"".format(model_prefix))\n\nfor title, tensors, corpus in zip(\n    [""train"", ""dev"", ""test""],\n    [train_tensors, dev_tensors, test_tensors],\n    [train_corpus, dev_corpus, test_corpus],\n                         ):\n    predictions = model_wrapper.predict_batch(tensors, title=title)\n    print_predictions(corpus, predictions, ""%s.wnut.conll"" % title, ner_vocab)\n    conll_eval([""conlleval"", ""%s.wnut.conll"" % title]) \n\n\n'"
