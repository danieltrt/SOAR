file_path,api_count,code
extract/extractor.py,7,"b'\n\'\'\'extractor.py\nextract DeLF local features\n\'\'\'\n\nimport os, sys, time\nsys.path.append(\'../\')\nsys.path.append(\'../train\')\nimport argparse\n\nimport torch\nimport torch.nn\nimport torch\nfrom torch.autograd import Variable\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport numpy as np\nimport h5py\nimport pickle\nimport copy\n\nimport delf_helper\nfrom train.delf import Delf_V1\nfrom pca import DelfPCA\nfrom folder import ImageFolder\nfrom utils import mkdir_p, Bar, AverageMeter\n\n__DEBUG__ = False\n\n\'\'\'helper functions.\n\'\'\'\ndef __cuda__(x):\n    if torch.cuda.is_available():\n        return x.cuda()\n    else:\n        return x\n\ndef __is_cuda__():\n    return torch.cuda.is_available()\n\ndef __to_var__(x, volatile=False):\n    return Variable(x, volatile=volatile)\n\ndef __to_tensor__(x):\n    return x.data\n\ndef __build_delf_config__(data):\n    parser = argparse.ArgumentParser(\'delf-config\')\n    parser.add_argument(\'--stage\', type=str, default=\'inference\')\n    parser.add_argument(\'--expr\', type=str, default=\'dummy\')\n    parser.add_argument(\'--ncls\', type=str, default=\'dummy\')\n    parser.add_argument(\'--use_random_gamma_rescale\', type=str, default=False)\n    parser.add_argument(\'--arch\', type=str, default=data[\'ARCH\'])\n    parser.add_argument(\'--load_from\', type=str, default=data[\'LOAD_FROM\'])\n    parser.add_argument(\'--target_layer\', type=str, default=data[\'TARGET_LAYER\'])\n    delf_config, _ = parser.parse_known_args()\n    \n    # print config.\n    state = {k: v for k, v in delf_config._get_kwargs()}\n    print(state)\n    return delf_config\n\n\nclass FeatureExtractor():\n    def __init__(self,\n                 extractor_config):\n        \n        # environment setting.\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(extractor_config.get(\'GPU_ID\'))\n       \n        # parameters.\n        self.title = \'DeLF-Inference\'\n        self.mode = extractor_config.get(\'MODE\')\n        self.ncls = extractor_config.get(\'NCLS\')\n        self.iou_thres = extractor_config.get(\'IOU_THRES\')\n        self.attn_thres = extractor_config.get(\'ATTN_THRES\')\n        self.top_k = extractor_config.get(\'TOP_K\')\n        self.target_layer = extractor_config.get(\'TARGET_LAYER\')\n        self.scale_list = extractor_config.get(\'SCALE_LIST\')\n        self.use_pca = extractor_config.get(\'USE_PCA\')\n        self.input_path = extractor_config.get(\'INPUT_PATH\')\n        self.output_path = extractor_config.get(\'OUTPUT_PATH\')\n\n        \n        # load pytorch model\n        print(\'load DeLF pytorch model...\')\n        delf_config = __build_delf_config__(extractor_config) \n        self.model = Delf_V1(\n            ncls = delf_config.ncls,\n            load_from = delf_config.load_from,\n            arch = delf_config.arch,\n            stage = delf_config.stage,\n            target_layer = delf_config.target_layer,\n            use_random_gamma_rescale = False)\n        self.model.eval()\n        self.model = __cuda__(self.model)\n        \n        # load pca matrix\n        if self.mode.lower() in [\'delf\']:\n            if self.use_pca:\n                print(\'load PCA parameters...\')\n                h5file = h5py.File(extractor_config.get(\'PCA_PARAMETERS_PATH\'), \'r\')\n                self.pca_mean = h5file[\'.\'][\'pca_mean\'].value\n                self.pca_vars = h5file[\'.\'][\'pca_vars\'].value\n                self.pca_matrix = h5file[\'.\'][\'pca_matrix\'].value\n                self.pca_dims = extractor_config.get(\'PCA_DIMS\')\n                self.use_pca = extractor_config.get(\'USE_PCA\')\n            else:\n                print(\'PCA will not be applied...\')\n                self.pca_mean = None\n                self.pca_vars = None\n                self.pca_matrix = None\n                self.pca_dims = None\n                    \n        # PCA.\n        if self.mode.lower() in [\'pca\']:\n            self.pca = DelfPCA(\n                pca_n_components = extractor_config.get(\'PCA_DIMS\'),\n                pca_whitening = True,\n                pca_parameters_path = extractor_config.get(\'PCA_PARAMETERS_PATH\'))\n\n        # set receptive field, stride, padding.\n        if self.target_layer in [\'layer3\']:\n            self.rf = 291.0\n            self.stride = 16.0\n            self.padding = 145.0\n        elif self.target_layer in [\'layer4\']:\n            self.rf = 483.0\n            self.stride = 32.0\n            self.padding = 241.0\n        else:\n            raise ValueError(\'Unsupported target_layer: {}\'.format(self.target_layer))\n\n    def __adjust_pixel_range__(self, \n                             x,\n                             range_from=[0,1],\n                             range_to=[-1,1]):\n        \'\'\'\n        adjust pixel range from <range_from> to <range_to>.\n        \'\'\'\n        if not range_from == range_to:\n            scale = float(range_to[1]-range_to[0])/float(range_from[1]-range_from[0])\n            bias = range_to[0]-range_from[0]*scale\n            x = x.mul(scale).add(bias)\n            return x\n\n\n    def __extract_delf_feature__(self, x, filename, mode=\'pca\'):\n        \'\'\'extract raw features from image batch.\n        x: Input FloatTensor, [b x c x w x h]\n        output: Output FloatTensor, [b x c x dim x dim]\n        \'\'\'\n        if mode == \'pca\':\n            use_pca = False\n            pca_mean = \'dummy_pca_mean\',\n            pca_vars = \'dummy_pca_vars\',\n            pca_matrix = \'dummy_pca_matrix\',\n            pca_dims = \'dummy_pca_dims\',\n            workers = 4\n        else:\n            assert mode == \'delf\', \'mode must be either pca or delf\'\n            use_pca = copy.deepcopy(self.use_pca)\n            pca_mean = copy.deepcopy(self.pca_mean)\n            pca_vars = copy.deepcopy(self.pca_vars)\n            pca_matrix = copy.deepcopy(self.pca_matrix)\n            pca_dims = copy.deepcopy(self.pca_dims)\n            workers = 4\n        try:\n            output = delf_helper.GetDelfFeatureFromMultiScale(\n                x = x,\n                model = self.model,\n                filename = filename,\n                pca_mean = pca_mean,\n                pca_vars = pca_vars,\n                pca_matrix = pca_matrix,\n                pca_dims = pca_dims,\n                rf = self.rf,\n                stride = self.stride,\n                padding = self.padding,\n                top_k = self.top_k,\n                scale_list = self.scale_list,\n                iou_thres = self.iou_thres,\n                attn_thres = self.attn_thres,\n                use_pca = use_pca,\n                workers = workers)\n            if mode == \'pca\':\n                descriptor_np_list = output[\'descriptor_np_list\']\n                descriptor = [descriptor_np_list[i,:] for i in range(descriptor_np_list.shape[0])]\n                return descriptor\n            else:\n                return output\n        except Exception as e:\n            print(\'\\n[Error] filename:{}, error message:{}\'.format(filename, e))\n            return None\n\n\n    def __save_delf_features_to_file__(self,\n                                       data,\n                                       filename):\n        \'\'\'save final local features after delf-postprocessing(PCA, NMS)\n        use pickle to save features.\n        Args:\n            data = [{\n                filename:\n                location_np_list:\n                descriptor_np_list:\n                feature_scale_np_list:\n                attention_score_np_list:\n                attention_np_list:\n            }, ... ]\n        \'\'\'\n        with open(filename, \'wb\') as handle:\n            pickle.dump(data, handle, protocol=2)       # use protocol <= 2 for python2 compatibility.\n        print(\'\\nsaved DeLF feature at {}\'.format(filename))\n\n\n    def __save_raw_features_to_file__(self,\n                                      feature_maps,\n                                      filename):\n        \'\'\'save feature to local file.\n        feature_maps: list of descriptor tensors in batch. [x1, x2, x3, x4 ...], x1 = Tensor([c x w x h])\n        output_path: path to save file.\n\n        save: \n        list of descriptors converted to numpy array. [d1, d2, d3, ...]\n        \'\'\'\n        np_feature_maps = []\n        np_feature_maps = [x.numpy() for _, x in enumerate(feature_maps)]\n        np_feature_maps = np.asarray(np_feature_maps)\n        \n        # save features, labels to h5 file.\n        h5file = h5py.File(filename, \'w\')\n        h5file.create_dataset(\'feature_maps\', data=np_feature_maps)\n        h5file.close()\n\n    \n    def extract(self, input_path, output_path):\n        \'\'\'extract features from single image without batch process.\n        \'\'\'\n        assert self.mode.lower() in [\'pca\', \'delf\']\n        batch_timer = AverageMeter()\n        data_timer = AverageMeter()\n        since = time.time()\n\n        # dataloader.\n        dataset = ImageFolder(\n            root = input_path,\n            transform = transforms.ToTensor())\n        self.dataloader = torch.utils.data.DataLoader(\n            dataset = dataset,\n            batch_size = 1,\n            shuffle = True,\n            num_workers = 0)\n        feature_maps = []\n        if self.mode.lower() in [\'pca\']:\n            bar = Bar(\'[{}]{}\'.format(self.mode.upper(), self.title), max=len(self.dataloader))\n            for batch_idx, (inputs, _, filename) in enumerate(self.dataloader):\n                # image size upper limit.\n                if not (len(inputs.size()) == 4):\n                    if __DEBUG__:\n                        print(\'wrong input dimenstion! ({},{})\'.format(filename, input.size()))\n                    continue;\n                if not (inputs.size(2)*inputs.size(3) <= 1200*1200):\n                    if __DEBUG__:\n                        print(\'passed: image size too large! ({},{})\'.format(filename, inputs.size()))\n                    continue;\n                if not (inputs.size(2) >= 112 and inputs.size(3) >= 112):\n                    if __DEBUG__:\n                        print(\'passed: image size too small! ({},{})\'.format(filename, inputs.size()))\n                    continue;\n                \n                data_timer.update(time.time() - since)\n                # prepare inputs\n                if __is_cuda__():\n                    inputs = __cuda__(inputs)\n                inputs = __to_var__(inputs)\n                \n                # get delf feature only for pca calculation.\n                pca_feature = self.__extract_delf_feature__(inputs.data, filename, mode=\'pca\')\n                if pca_feature is not None:\n                    feature_maps.extend(pca_feature)\n               \n                batch_timer.update(time.time() - since)\n                since = time.time()\n            \n                # progress\n                log_msg  = (\'\\n[Extract][Processing:({batch}/{size})] \'+ \\\n                            \'eta: (data:{data:.3f}s),(batch:{bt:.3f}s),(total:{tt:})\') \\\n                .format(\n                    batch=batch_idx + 1,\n                    size=len(self.dataloader),\n                    data=data_timer.val,\n                    bt=batch_timer.val,\n                    tt=bar.elapsed_td)\n                print(log_msg)\n                bar.next()\n                print(\'\\nnumber of selected features so far: {}\'.format(len(feature_maps)))\n                if len(feature_maps) >= 10000000:        # UPPER LIMIT.\n                    break;\n                \n                # free GPU cache every.\n                if batch_idx % 10 == 0:\n                    torch.cuda.empty_cache()\n                    if __DEBUG__:\n                        print(\'GPU Memory flushed !!!!!!!!!\')\n\n            # trian PCA.\n            self.pca(feature_maps)\n        \n        else:\n            bar = Bar(\'[{}]{}\'.format(self.mode.upper(), self.title), max=len(self.dataloader))\n            assert self.mode.lower() in [\'delf\']\n            feature_maps = []\n            for batch_idx, (inputs, labels, filename) in enumerate(self.dataloader):\n                # image size upper limit.\n                if not (len(inputs.size()) == 4):\n                    if __DEBUG__:\n                        print(\'wrong input dimenstion! ({},{})\'.format(filename, input.size()))\n                    continue;\n                if not (inputs.size(2)*inputs.size(3) <= 1200*1200):\n                    if __DEBUG__:\n                        print(\'passed: image size too large! ({},{})\'.format(filename, inputs.size()))\n                    continue;\n                if not (inputs.size(2) >= 112 and inputs.size(3) >= 112):\n                    if __DEBUG__:\n                        print(\'passed: image size too small! ({},{})\'.format(filename, inputs.size()))\n                    continue;\n                \n                data_timer.update(time.time() - since)\n                # prepare inputs\n                if __is_cuda__():\n                    inputs = __cuda__(inputs)\n                inputs = __to_var__(inputs)\n                    \n                # get delf everything (score, feature, etc.)\n                delf_feature = self.__extract_delf_feature__(inputs.data, filename, mode=\'delf\')\n                if delf_feature is not None:\n                    feature_maps.append(delf_feature)\n               \n                # log.\n                batch_timer.update(time.time() - since)\n                since = time.time()\n                log_msg  = (\'\\n[Extract][Processing:({batch}/{size})] \'+ \\\n                            \'eta: (data:{data:.3f}s),(batch:{bt:.3f}s),(total:{tt:})\') \\\n                .format(\n                    batch=batch_idx + 1,\n                    size=len(self.dataloader),\n                    data=data_timer.val,\n                    bt=batch_timer.val,\n                    tt=bar.elapsed_td)\n                print(log_msg)\n                bar.next()\n                \n                # free GPU cache every.\n                if batch_idx % 10 == 0:\n                    torch.cuda.empty_cache()\n                    if __DEBUG__:\n                        print(\'GPU Memory flushed !!!!!!!!!\')\n                \n            # use pickle to save DeLF features.\n            self.__save_delf_features_to_file__(feature_maps, output_path)\n                \n\nif __name__ == ""__main__"":\n    MODE = \'delf\'           # either ""delf"" or ""pca""\n    GPU_ID = 7\n    IOU_THRES = 0.98\n    ATTN_THRES = 0.17\n    TOP_K = 1000\n    USE_PCA = True\n    PCA_DIMS = 40\n    SCALE_LIST = [0.25, 0.3535, 0.5, 0.7071, 1.0, 1.4142, 2.0]\n    ARCH = \'resnet50\'\n    EXPR = \'dummy\'\n    TARGET_LAYER = \'layer3\'\n    \n    MODEL_NAME = \'ldmk\'\n    \n    LOAD_FROM = \'archive/model/{}/keypoint/ckpt/fix.pth.tar\'.format(MODEL_NAME)\n    PCA_PARAMETERS_PATH = \'archive/pca/{}/pca.h5\'.format(MODEL_NAME)\n\n    extractor_config = {\n        # params for feature extraction.\n        \'MODE\': MODE,\n        \'GPU_ID\': GPU_ID,\n        \'IOU_THRES\': IOU_THRES,\n        \'ATTN_THRES\': ATTN_THRES,\n        \'TOP_K\': TOP_K,\n        \'PCA_PARAMETERS_PATH\': PCA_PARAMETERS_PATH,\n        \'PCA_DIMS\': PCA_DIMS,\n        \'USE_PCA\': USE_PCA,\n        \'SCALE_LIST\': SCALE_LIST,\n        \n        # params for model load.\n        \'LOAD_FROM\': LOAD_FROM,\n        \'ARCH\': ARCH,\n        \'EXPR\': EXPR,\n        \'TARGET_LAYER\': TARGET_LAYER,\n    }\n\n    \n    extractor = FeatureExtractor(extractor_config)\n    if MODE.lower() in [\'pca\']:\n        OUTPUT_PATH = \'dummy\'\n        INPUT_PATH = \'your_path_to_dataset\'\n        extractor.extract(INPUT_PATH, OUTPUT_PATH)\n    \n    elif MODE.lower() in [\'delf\']:\n        # query\n        INPUT_PATH = \'your_path_to_dataset\'\n        OUTPUT_PATH = \'archive/delf.batch/{}/oxf5k_query.delf\'.format(MODEL_NAME)\n        extractor.extract(INPUT_PATH, OUTPUT_PATH)\n        # index\n        INPUT_PATH = \'data/oxf5k/index\'\n        OUTPUT_PATH = \'archive/delf.batch/{}/oxf5k_index.delf\'.format(MODEL_NAME)\n        extractor.extract(INPUT_PATH, OUTPUT_PATH)\n\n\n\n\n\n'"
extract/folder.py,1,"b'import torch.utils.data as data\n\nfrom PIL import Image\n\nimport os\nimport os.path\n\n\ndef has_file_allowed_extension(filename, extensions):\n    """"""Checks if a file is an allowed extension.\n\n    Args:\n        filename (string): path to a file\n\n    Returns:\n        bool: True if the filename ends with a known image extension\n    """"""\n    filename_lower = filename.lower()\n    return any(filename_lower.endswith(ext) for ext in extensions)\n\n\ndef find_classes(dir):\n    classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n    classes.sort()\n    class_to_idx = {classes[i]: i for i in range(len(classes))}\n    return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx, extensions):\n    images = []\n    dir = os.path.expanduser(dir)\n    for target in sorted(os.listdir(dir)):\n        d = os.path.join(dir, target)\n        if not os.path.isdir(d):\n            continue\n\n        for root, _, fnames in sorted(os.walk(d)):\n            for fname in sorted(fnames):\n                if has_file_allowed_extension(fname, extensions):\n                    path = os.path.join(root, fname)\n                    item = (path, class_to_idx[target])\n                    images.append(item)\n\n    return images\n\n\nclass DatasetFolder(data.Dataset):\n    """"""A generic data loader where the samples are arranged in this way: ::\n\n        root/class_x/xxx.ext\n        root/class_x/xxy.ext\n        root/class_x/xxz.ext\n\n        root/class_y/123.ext\n        root/class_y/nsdf3.ext\n        root/class_y/asd932_.ext\n\n    Args:\n        root (string): Root directory path.\n        loader (callable): A function to load a sample given its path.\n        extensions (list[string]): A list of allowed extensions.\n        transform (callable, optional): A function/transform that takes in\n            a sample and returns a transformed version.\n            E.g, ``transforms.RandomCrop`` for images.\n        target_transform (callable, optional): A function/transform that takes\n            in the target and transforms it.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        samples (list): List of (sample path, class_index) tuples\n    """"""\n\n    def __init__(self, root, loader, extensions, transform=None, target_transform=None):\n        classes, class_to_idx = find_classes(root)\n        samples = make_dataset(root, class_to_idx, extensions)\n        if len(samples) == 0:\n            raise(RuntimeError(""Found 0 files in subfolders of: "" + root + ""\\n""\n                               ""Supported extensions are: "" + "","".join(extensions)))\n\n        self.root = root\n        self.loader = loader\n        self.extensions = extensions\n\n        self.classes = classes\n        self.class_to_idx = class_to_idx\n        self.samples = samples\n\n        self.transform = transform\n        self.target_transform = target_transform\n\n    def __getitem__(self, index):\n        """"""\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (sample, target) where target is class_index of the target class.\n        """"""\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        return sample, target, str(os.path.basename(path))\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __repr__(self):\n        fmt_str = \'Dataset \' + self.__class__.__name__ + \'\\n\'\n        fmt_str += \'    Number of datapoints: {}\\n\'.format(self.__len__())\n        fmt_str += \'    Root Location: {}\\n\'.format(self.root)\n        tmp = \'    Transforms (if any): \'\n        fmt_str += \'{0}{1}\\n\'.format(tmp, self.transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        tmp = \'    Target Transforms (if any): \'\n        fmt_str += \'{0}{1}\'.format(tmp, self.target_transform.__repr__().replace(\'\\n\', \'\\n\' + \' \' * len(tmp)))\n        return fmt_str\n\n\nIMG_EXTENSIONS = [\'.jpg\', \'.jpeg\', \'.png\', \'.ppm\', \'.bmp\', \'.pgm\', \'.tif\']\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        img = Image.open(f)\n        return img.convert(\'RGB\')\n\n\ndef accimage_loader(path):\n    import accimage\n    try:\n        return accimage.Image(path)\n    except IOError:\n        # Potentially a decoding problem, fall back to PIL.Image\n        return pil_loader(path)\n\n\ndef default_loader(path):\n    from torchvision import get_image_backend\n    if get_image_backend() == \'accimage\':\n        return accimage_loader(path)\n    else:\n        return pil_loader(path)\n\n\nclass ImageFolder(DatasetFolder):\n    """"""A generic data loader where the images are arranged in this way: ::\n\n        root/dog/xxx.png\n        root/dog/xxy.png\n        root/dog/xxz.png\n\n        root/cat/123.png\n        root/cat/nsdf3.png\n        root/cat/asd932_.png\n\n    Args:\n        root (string): Root directory path.\n        transform (callable, optional): A function/transform that  takes in an PIL image\n            and returns a transformed version. E.g, ``transforms.RandomCrop``\n        target_transform (callable, optional): A function/transform that takes in the\n            target and transforms it.\n        loader (callable, optional): A function to load an image given its path.\n\n     Attributes:\n        classes (list): List of the class names.\n        class_to_idx (dict): Dict with items (class_name, class_index).\n        imgs (list): List of (image path, class_index) tuples\n    """"""\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=default_loader):\n        super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n                                          transform=transform,\n                                          target_transform=target_transform)\n        self.imgs = self.samples\n'"
extract/pca.py,0,"b""\n'''pca.py\ncalculate PCA for given features, and save output into file.\n'''\n\nimport os\nimport sys\nimport time\nimport glob\n\nimport numpy as np\nimport h5py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n\nclass DelfPCA():\n    def __init__(self,\n                 pca_n_components,\n                 pca_whitening=True,\n                 pca_parameters_path='./output/pca/pca.h5'):\n        self.pca_n_components = pca_n_components\n        self.pca_whitening = pca_whitening\n        self.pca_parameters_path = pca_parameters_path\n\n    def __call__(self,\n                 feature_maps):\n        '''training pca.\n        Args:\n            feature_maps: list of feature tensorsm,\n                          feature_maps = [f1, f2, f3 ...],\n                          f1 = FloatTensor(fmap_depth)\n        Returns:\n            pca_matrix,\n            pca_means,\n            pca_vars\n        '''\n\n        # calculate pca.\n        pca = PCA(whiten=self.pca_whitening)\n        pca.fit(np.array(feature_maps))\n        pca_matrix = pca.components_\n        pca_mean = pca.mean_\n        pca_vars = pca.explained_variance_\n        \n        # save as h5 file.\n        print('================= PCA RESULT ==================')\n        print('pca_matrix: {}'.format(pca_matrix.shape))\n        print('pca_mean: {}'.format(pca_mean.shape))\n        print('pca_vars: {}'.format(pca_vars.shape))\n        print('===============================================')\n        \n        # save features, labels to h5 file.\n        filename = os.path.join(self.pca_parameters_path)\n        h5file = h5py.File(filename, 'w')\n        h5file.create_dataset('pca_matrix', data=pca_matrix)\n        h5file.create_dataset('pca_mean', data=pca_mean)\n        h5file.create_dataset('pca_vars', data=pca_vars)\n        h5file.close()\n\n"""
helper/__init__.py,0,b'\n\n\nfrom .matcher import *\nfrom .feeder import *\nfrom .delf_helper import *\n\n\n\n'
helper/delf_helper.py,39,"b'#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n""""""\ndelf_helper.py\nhelper functions to extract DeLF functions.\n""""""\n\n\nimport os, sys, time\n\nimport numpy as np\nimport h5py\nimport torch\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\nfrom concurrent.futures import ThreadPoolExecutor, as_completed     # for use of multi-threads\n\n__DEBUG__ = False\n\ndef GenerateCoordinates(h,w):\n    \'\'\'generate coorinates\n    Returns: [h*w, 2] FloatTensor\n    \'\'\'\n    x = torch.floor(torch.arange(0, w*h) / w)\n    y = torch.arange(0, w).repeat(h)\n\n    coord = torch.stack([x,y], dim=1)\n    return coord\n\ndef CalculateReceptiveBoxes(height,\n                            width,\n                            rf,\n                            stride,\n                            padding):\n\n    \'\'\'\n    caculate receptive boxes from original image for each feature point.\n    Args:\n        height: The height of feature map.\n        width: The width of feature map.\n        rf: The receptive field size.\n        stride: The effective stride between two adjacent feature points.\n        padding: The effective padding size.\n\n    Returns:\n        rf_boxes: [N, 4] recpetive boxes tensor. (N = height x weight).\n        each box is represented by [ymin, xmin, ymax, xmax].\n    \'\'\'\n    coordinates = GenerateCoordinates(h=height,\n                                      w=width)\n    # create [ymin, xmin, ymax, xmax]\n    point_boxes = torch.cat([coordinates, coordinates], dim=1)\n    bias = torch.FloatTensor([-padding, -padding, -padding + rf - 1, -padding + rf - 1])\n    rf_boxes = stride * point_boxes + bias\n    return rf_boxes \n\ndef CalculateKeypointCenters(rf_boxes):\n    \'\'\'compute feature centers, from receptive field boxes (rf_boxes).\n    Args:\n        rf_boxes: [N, 4] FloatTensor.\n    Returns:\n        centers: [N, 2] FloatTensor.\n    \'\'\'\n    xymin = torch.index_select(rf_boxes, dim=1, index=torch.LongTensor([0,1]))\n    xymax = torch.index_select(rf_boxes, dim=1, index=torch.LongTensor([2,3]))\n    return (xymax + xymin) / 2.0\n\ndef ApplyPcaAndWhitening(data,\n                         pca_matrix,\n                         pca_mean,\n                         pca_vars,\n                         pca_dims,\n                         use_whitening=False):\n    \'\'\'apply PCA/Whitening to data.\n    Args: \n        data: [N, dim] FloatTensor containing data which undergoes PCA/Whitening.\n        pca_matrix: [dim, dim] numpy array PCA matrix, row-major.\n        pca_mean: [dim] numpy array mean to subtract before projection.\n        pca_dims: # of dimenstions to use in output data, of type int.\n        pca_vars: [dim] numpy array containing PCA variances. \n                   Only used if use_whitening is True.\n        use_whitening: Whether whitening is to be used. usually recommended.\n    Returns:\n        output: [N, output_dim] FloatTensor with output of PCA/Whitening operation.\n    (Warning: element 0 in pca_variances might produce nan/inf value.) \n    \'\'\'\n    pca_mean = torch.from_numpy(pca_mean).float()\n    pca_vars = torch.from_numpy(pca_vars).float()\n    pca_matrix = torch.from_numpy(pca_matrix).float()\n\n    data = data - pca_mean\n    output = data.matmul(pca_matrix.narrow(0, 0, pca_dims).transpose(0,1))\n    \n    if use_whitening:\n        output = output.div((pca_vars.narrow(0, 0, pca_dims) ** 0.5))\n    return output\n\ndef GetDelfFeatureFromMultiScale(\n    x,\n    model,\n    filename,\n    pca_mean,\n    pca_vars,\n    pca_matrix,\n    pca_dims,\n    rf,\n    stride,\n    padding,\n    top_k,\n    scale_list,\n    iou_thres,\n    attn_thres,\n    use_pca=False,\n    workers=8):\n    \'\'\'GetDelfFeatureFromMultiScale\n    warning: use workers = 1 for serving otherwise out of memory error could occurs.\n    (because uwsgi uses multi-threads by itself.)\n    \'\'\'\n\n    # helper func.\n    def __concat_tensors_in_list__(tensor_list, dim):\n        res = None\n        tensor_list = [x for x in tensor_list if x is not None]\n        for tensor in tensor_list:\n            if res is None:\n                res = tensor\n            else:\n                res = torch.cat((res, tensor), dim=dim)\n        return res\n\n    # extract features for each scale, and concat.\n    output_boxes = []\n    output_features = []\n    output_scores = []\n    output_scales = []\n    output_original_scale_attn = None\n\n    # multi-threaded feature extraction from different scales.\n    with ThreadPoolExecutor(max_workers=workers) as pool:\n        # assign jobs.\n        futures = {\n            pool.submit(\n                GetDelfFeatureFromSingleScale,\n                    x,\n                    model,\n                    scale,\n                    pca_mean,\n                    pca_vars,\n                    pca_matrix,\n                    pca_dims,\n                    rf,\n                    stride,\n                    padding,\n                    attn_thres,\n                    use_pca):\n            scale for scale in scale_list\n        }\n        for future in as_completed(futures):\n            (selected_boxes, selected_features, \n            selected_scales, selected_scores, \n            selected_original_scale_attn) = future.result()\n            # append to list.  \n            output_boxes.append(selected_boxes) if selected_boxes is not None else output_boxes\n            output_features.append(selected_features) if selected_features is not None else output_features\n            output_scales.append(selected_scales) if selected_scales is not None else output_scales\n            output_scores.append(selected_scores) if selected_scores is not None else output_scores\n            if selected_original_scale_attn is not None:\n                output_original_scale_attn = selected_original_scale_attn\n\n    # if scale == 1.0 is not included in scale list, just show noisy attention image.\n    if output_original_scale_attn is None:\n        output_original_scale_attn = x.clone().uniform()\n\n    # concat tensors precessed from different scales.\n    output_boxes = __concat_tensors_in_list__(output_boxes, dim=0)\n    output_features = __concat_tensors_in_list__(output_features, dim=0)\n    output_scales = __concat_tensors_in_list__(output_scales, dim=0)\n    output_scores = __concat_tensors_in_list__(output_scores, dim=0)\n\n    # perform Non Max Suppression(NMS) to select top-k bboxes arrcoding to the attn_score.\n    keep_indices, count = nms(boxes = output_boxes,\n                              scores = output_scores,\n                              overlap = iou_thres,\n                              top_k = top_k)\n    keep_indices = keep_indices[:top_k]\n    output_boxes = torch.index_select(output_boxes, dim=0, index=keep_indices)\n    output_features = torch.index_select(output_features, dim=0, index=keep_indices)\n    output_scales = torch.index_select(output_scales, dim=0, index=keep_indices)\n    output_scores = torch.index_select(output_scores, dim=0, index=keep_indices)\n    output_locations = CalculateKeypointCenters(output_boxes)\n    \n    data = {\n        \'filename\':filename,\n        \'location_np_list\':output_locations.cpu().numpy(),\n        \'descriptor_np_list\':output_features.cpu().numpy(),\n        \'feature_scale_np_list\':output_scales.cpu().numpy(),\n        \'attention_score_np_list\':output_scores.cpu().numpy(),\n        \'attention_np_list\':output_original_scale_attn.cpu().numpy()\n    }\n    \n    # free GPU memory.\n    del output_locations\n    del output_boxes, selected_boxes\n    del output_features, selected_features\n    del output_scales, selected_scales\n    del output_scores, selected_scores\n    del output_original_scale_attn, selected_original_scale_attn \n    #torch.cuda.empty_cache()            # it releases all unoccupied cached memory!! (but it makes process slow)\n\n    if __DEBUG__:\n        #PrintGpuMemoryStats() \n        PrintResult(data)\n    return data\n\ndef PrintGpuMemoryStats():\n    \'\'\'PyTorch >= 0.5.0\n    \'\'\'\n    print\n    print(\'\\n----------------------------------------------------------\')\n    print(\'[Monitor] max GPU Memory Used by Tensor: {}\'.format(torch.cuda.max_memory_allocated()))\n    print(\'[Monitor] max GPU Memory Used by Cache: {}\'.format(torch.cuda.max_memory_cached()))\n    print(\'----------------------------------------------------------\')\n\ndef PrintResult(data): \n    print(\'\\n----------------------------------------------------------\')\n    print(\'filename: \', data[\'filename\'])\n    print(""location_np_list shape: "", data[\'location_np_list\'].shape)\n    print(""descriptor_np_list shape: "", data[\'descriptor_np_list\'].shape)\n    print(""feature_scale_np_list shape: "", data[\'feature_scale_np_list\'].shape)\n    print(""attention_score_np_list shape: "", data[\'attention_score_np_list\'].shape)\n    print(""attention_np_list shape: "", data[\'attention_np_list\'].shape)\n    print(\'----------------------------------------------------------\\n\')\n\ndef GetDelfFeatureFromSingleScale(\n    x,\n    model,\n    scale,\n    pca_mean,\n    pca_vars,\n    pca_matrix,\n    pca_dims,\n    rf,\n    stride,\n    padding,\n    attn_thres,\n    use_pca):\n\n    # scale image then get features and attention.\n    new_h = int(round(x.size(2)*scale))\n    new_w = int(round(x.size(3)*scale))\n    scaled_x = F.upsample(x, size=(new_h, new_w), mode=\'bilinear\')\n    scaled_features, scaled_scores = model.forward_for_serving(scaled_x)\n\n    # save original size attention (used for attention visualization.)\n    selected_original_scale_attn = None\n    if scale == 1.0:\n        selected_original_scale_attn = torch.clamp(scaled_scores*255, 0, 255) # 1 1 h w\n        \n    # calculate receptive field boxes.\n    rf_boxes = CalculateReceptiveBoxes(\n        height=scaled_features.size(2),\n        width=scaled_features.size(3),\n        rf=rf,\n        stride=stride,\n        padding=padding)\n    \n    # re-projection back to original image space.\n    rf_boxes = rf_boxes / scale\n    scaled_scores = scaled_scores.view(-1)\n    scaled_features = scaled_features.view(scaled_features.size(1), -1).t()\n\n    # do post-processing for dimension reduction by PCA.\n    scaled_features = DelfFeaturePostProcessing(\n        rf_boxes, \n        scaled_features,\n        pca_mean,\n        pca_vars,\n        pca_matrix,\n        pca_dims,\n        use_pca)\n\n    # use attention score to select feature.\n    indices = None\n    while(indices is None or len(indices) == 0):\n        indices = torch.gt(scaled_scores, attn_thres).nonzero().squeeze()\n        attn_thres = attn_thres * 0.5   # use lower threshold if no indexes are found.\n        if attn_thres < 0.001:\n            break;\n   \n    try:\n        selected_boxes = torch.index_select(rf_boxes, dim=0, index=indices)\n        selected_features = torch.index_select(scaled_features, dim=0, index=indices)\n        selected_scores = torch.index_select(scaled_scores, dim=0, index=indices)\n        selected_scales = torch.ones_like(selected_scores) * scale\n    except Exception as e:\n        selected_boxes = None\n        selected_features = None\n        selected_scores = None\n        selected_scales = None\n        print(e)\n        pass;\n        \n    return selected_boxes, selected_features, selected_scales, selected_scores, selected_original_scale_attn\n\n\ndef DelfFeaturePostProcessing(\n    boxes,\n    descriptors,\n    pca_mean,\n    pca_vars,\n    pca_matrix,\n    pca_dims,\n    use_pca):\n\n    \'\'\' Delf feature post-processing.\n    (1) apply L2 Normalization.\n    (2) apply PCA and Whitening.\n    (3) apply L2 Normalization once again.\n    Args:\n        descriptors: (w x h, fmap_depth) descriptor Tensor.\n    Retturn:\n        descriptors: (w x h, pca_dims) desciptor Tensor.\n    \'\'\'\n\n    locations = CalculateKeypointCenters(boxes)\n\n    # L2 Normalization.\n    descriptors = descriptors.squeeze()\n    l2_norm = descriptors.norm(p=2, dim=1, keepdim=True)        # (1, w x h)\n    descriptors = descriptors.div(l2_norm.expand_as(descriptors))  # (N, w x h)\n\n    if use_pca:\n        # apply PCA and Whitening.\n        descriptors = ApplyPcaAndWhitening(\n            descriptors,\n            pca_matrix,\n            pca_mean,\n            pca_vars,\n            pca_dims,\n            True)\n        # L2 Normalization (we found L2 Norm is not helpful. DO NOT UNCOMMENT THIS.)\n        #descriptors = descriptors.view(descriptors.size(0), -1)     # (N, w x h)\n        #l2_norm = descriptors.norm(p=2, dim=0, keepdim=True)        # (1, w x h)\n        #descriptors = descriptors.div(l2_norm.expand_as(descriptors))  # (N, w x h)\n    \n    return descriptors\n\n\n# Original author: Francisco Massa:\n# https://github.com/fmassa/object-detection.torch\n# Ported to PyTorch by Max deGroot (02/01/2017)\ndef nms(boxes, scores, overlap=0.5, top_k=200):\n    """"""Apply non-maximum suppression at test time to avoid detecting too many\n    overlapping bounding boxes for a given object.\n    Args:\n        boxes: (tensor) The location preds for the img, Shape: [num_priors,4].\n        scores: (tensor) The class predscores for the img, Shape:[num_priors].\n        overlap: (float) The overlap thresh for suppressing unnecessary boxes.\n        top_k: (int) The Maximum number of box preds to consider.\n    Returns:\n        The indices of the kept boxes with respect to num_priors.\n    """"""\n\n    keep = scores.new(scores.size(0)).zero_().long()\n    if boxes.numel() == 0:\n        return keep\n    y1 = boxes[:, 0]\n    x1 = boxes[:, 1]\n    y2 = boxes[:, 2]\n    x2 = boxes[:, 3]\n    area = torch.mul(x2 - x1, y2 - y1)\n    v, idx = scores.sort(0)  # sort in ascending order\n    # I = I[v >= 0.01]\n    idx = idx[-top_k:]  # indices of the top-k largest vals\n    xx1 = boxes.new()\n    yy1 = boxes.new()\n    xx2 = boxes.new()\n    yy2 = boxes.new()\n    w = boxes.new()\n    h = boxes.new()\n\n    # keep = torch.Tensor()\n    count = 0\n    while idx.numel() > 0:\n        i = idx[-1]  # index of current largest val\n        # keep.append(i)\n        keep[count] = i\n        count += 1\n        if idx.size(0) == 1:\n            break\n        idx = idx[:-1]  # remove kept element from view\n        # load bboxes of next highest vals\n        torch.index_select(x1, 0, idx, out=xx1)\n        torch.index_select(y1, 0, idx, out=yy1)\n        torch.index_select(x2, 0, idx, out=xx2)\n        torch.index_select(y2, 0, idx, out=yy2)\n        # store element-wise max with next highest score\n        xx1 = torch.clamp(xx1, min=x1[i])\n        yy1 = torch.clamp(yy1, min=y1[i])\n        xx2 = torch.clamp(xx2, max=x2[i])\n        yy2 = torch.clamp(yy2, max=y2[i])\n        w.resize_as_(xx2)\n        h.resize_as_(yy2)\n        w = xx2 - xx1\n        h = yy2 - yy1\n        # check sizes of xx1 and xx2.. after each iteration\n        w = torch.clamp(w, min=0.0)\n        h = torch.clamp(h, min=0.0)\n        inter = w*h\n        # IoU = i / (area(a) + area(b) - i)\n        rem_areas = torch.index_select(area, 0, idx)  # load remaining areas)\n        union = (rem_areas - inter) + area[i]\n        IoU = inter/union  # store result in iou\n        # keep only elements with an IoU <= overlap\n        idx = idx[IoU.le(overlap)]\n    return keep, count\n\n\n\n'"
helper/feeder.py,1,"b'\'\'\'feeder.py\n\'\'\'\n\nimport os, sys, time\nsys.path.append(\'../\')\nimport argparse\n\nfrom PIL import Image\nimport h5py\nimport torch\nimport torchvision.transforms as transforms\n\nimport helper.delf_helper as delf_helper\nfrom train.delf import Delf_V1\n\n__DEBUG__ = False\n\ndef __cuda__(x):\n    if torch.cuda.is_available():\n        return x.cuda()\n    else:\n        return x\n\ndef __build_delf_config__(data):\n    parser = argparse.ArgumentParser(\'delf-config\')\n    parser.add_argument(\'--stage\', type=str, default=\'inference\')\n    parser.add_argument(\'--expr\', type=str, default=\'dummy\')\n    parser.add_argument(\'--ncls\', type=str, default=\'dummy\')\n    parser.add_argument(\'--use_random_gamma_rescale\', type=str, default=False)\n    parser.add_argument(\'--arch\', type=str, default=data[\'ARCH\'])\n    parser.add_argument(\'--load_from\', type=str, default=data[\'LOAD_FROM\'])\n    parser.add_argument(\'--target_layer\', type=str, default=data[\'TARGET_LAYER\'])\n    delf_config, _ = parser.parse_known_args()\n    \n    # print config.\n    state = {k: v for k, v in delf_config._get_kwargs()}\n    print(state)\n    return delf_config\n\n\nclass Feeder():\n    def __init__(self,\n                 feeder_config):\n        # environment setting.\n        os.environ[\'CUDA_VISIBLE_DEVICES\'] = str(feeder_config.get(\'GPU_ID\'))\n        \n        # parameters.\n        self.iou_thres = feeder_config.get(\'IOU_THRES\')\n        self.attn_thres = feeder_config.get(\'ATTN_THRES\')\n        self.top_k = feeder_config.get(\'TOP_K\')\n        self.target_layer = feeder_config.get(\'TARGET_LAYER\')\n        self.scale_list = feeder_config.get(\'SCALE_LIST\')\n        self.workers = feeder_config.get(\'WORKERS\')\n\n        # load pytorch model\n        print(\'load DeLF pytorch model...\')\n        delf_config = __build_delf_config__(feeder_config) \n        self.model = Delf_V1(\n            ncls = delf_config.ncls,\n            load_from = delf_config.load_from,\n            arch = delf_config.arch,\n            stage = delf_config.stage,\n            target_layer = delf_config.target_layer,\n            use_random_gamma_rescale = False)\n        self.model.eval()\n        self.model = __cuda__(self.model)\n        \n        # load pca matrix\n        print(\'load PCA parameters...\')\n        h5file = h5py.File(feeder_config.get(\'PCA_PARAMETERS_PATH\'), \'r\')\n        self.pca_mean = h5file[\'.\'][\'pca_mean\'].value\n        self.pca_vars = h5file[\'.\'][\'pca_vars\'].value\n        self.pca_matrix = h5file[\'.\'][\'pca_matrix\'].value\n        self.pca_dims = feeder_config.get(\'PCA_DIMS\')\n        self.use_pca = feeder_config.get(\'USE_PCA\')\n\n        # !!! stride value in tensorflow inference code is not applicable for pytorch, because pytorch works differently.\n        # !!! make sure to use stride=16 for target_layer==\'layer3\'.\n        if self.target_layer in [\'layer3\']:\n            self.fmap_depth = 1024\n            self.rf = 291.0\n            self.stride = 16.0\n            self.padding = 145.0\n        elif self.target_layer in [\'layer4\']:\n            self.fmap_depth = 2048\n            self.rf = 483.0\n            self.stride = 32.0\n            self.padding = 241.0\n        else:\n            raise ValueError(\'Unsupported target_layer: {}\'.format(self.target_layer))\n        \n\n    def __resize_image__(self, image, target_size):\n        return \'resize image.\'\n\n    def __transform__(self, image):\n        transform = transforms.ToTensor()\n        return transform(image)\n\n    def __print_result__(self, data):\n        print(\'----------------------------------------------------------\')\n        print(\'filename: \', data[\'filename\'])\n        print(""location_np_list shape: "", data[\'location_np_list\'].shape)\n        print(""descriptor_np_list shape: "", data[\'descriptor_np_list\'].shape)\n        print(""feature_scale_np_list shape: "", data[\'feature_scale_np_list\'].shape)\n        print(""attention_score_np_list shape: "", data[\'attention_score_np_list\'].shape)\n        print(""attention_np_list shape: "", data[\'attention_np_list\'].shape)\n        print(\'----------------------------------------------------------\')\n\n    def __get_result__(self,\n                       path,\n                       image):\n        # load tensor image\n        x = __cuda__(self.__transform__(image))\n        x = x.unsqueeze(0)\n\n        # extract feature.\n        data = delf_helper.GetDelfFeatureFromMultiScale(\n            x = x,\n            model = self.model,\n            filename = path,\n            pca_mean = self.pca_mean,\n            pca_vars = self.pca_vars,\n            pca_matrix = self.pca_matrix,\n            pca_dims = self.pca_dims,\n            rf = self.rf,\n            stride = self.stride,\n            padding = self.padding,\n            top_k = self.top_k,\n            scale_list = self.scale_list,\n            iou_thres = self.iou_thres,\n            attn_thres = self.attn_thres,\n            use_pca = self.use_pca,\n            workers = self.workers)\n        \n        if __DEBUG__:\n            self.__print_result__(data)\n        return data \n\n    def feed(self, pil_image, filename=\'dummy\'):\n        return self.__get_result__(filename, pil_image)\n\n    def feed_to_compare(self, query_path, pil_image):\n        \'\'\'feed_to_compare\n        used to visualize mathcing between two query images.\n        \'\'\'\n        assert len(pil_image)==2, \'length of query list should be 2.\'\n        outputs = []\n        for i in range(2):\n            outputs.append(self.__get_result__(query_path[i], pil_image[i]))\n        return outputs\n\n'"
helper/matcher.py,0,"b'\'\'\'matcher.py\nMatches two images using their DELF features.\nThe matching is done using feature-based nearest-neighbor search, followed by\ngeometric verification using RANSAC.\nThe DELF features can be extracted using the extract_features.py script.\n\'\'\'\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys, time\n\nimport numpy as np\nfrom PIL import Image\nimport io\nfrom io import BytesIO\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nfrom scipy.spatial import cKDTree\nfrom skimage.feature import plot_matches\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\n\nimport cv2\n\n_DISTANCE_THRESHOLD = 3.4           # Adjust this value depending on your dataset. \n                                    # This value needs to be engineered for optimized result.\nIMAGE_SIZE = (16, 12)\n\ndef load_image_into_numpy_array(image):\n    if image.mode == ""P"": # PNG palette mode\n        image = image.convert(\'RGBA\')\n        # image.palette = None # PIL Bug Workaround\n\n    (im_width, im_height) = image.size\n    imgarray = np.asarray(image).reshape(\n        (im_height, im_width, -1)).astype(np.uint8)\n\n    return imgarray[:, :, :3] # truncate alpha channel if exists. \n\ndef read_image(image_path):\n    with open(image_path, \'rb\') as image_fp:\n        image = Image.open(image_fp)\n        # the array based representation of the image will be used later in order to prepare the\n        # result image with boxes and labels on it.\n        image_np = load_image_into_numpy_array(image)\n    return image_np\n\ndef get_inliers(locations_1, descriptors_1, locations_2, descriptors_2):  \n\n    num_features_1 = locations_1.shape[0]\n    num_features_2 = locations_2.shape[0]\n\n    # Find nearest-neighbor matches using a KD tree.\n    d1_tree = cKDTree(descriptors_1)\n    distances, indices = d1_tree.query(\n        descriptors_2, distance_upper_bound=_DISTANCE_THRESHOLD)\n\n    # Select feature locations for putative matches.\n    locations_2_to_use = np.array([\n        locations_2[i,] for i in range(num_features_2)\n        if indices[i] != num_features_1\n    ])\n    locations_1_to_use = np.array([\n        locations_1[indices[i],] for i in range(num_features_2)\n        if indices[i] != num_features_1\n    ])\n\n    # Perform geometric verification using RANSAC.\n    model_robust, inliers = ransac(\n        (locations_1_to_use, locations_2_to_use),\n        AffineTransform,\n        min_samples=3,\n        residual_threshold=20,\n        max_trials=1000)\n    return inliers, locations_1_to_use, locations_2_to_use\n\n\ndef get_attention_image_byte(att_score):\n    print(\'attn_score shape: {}\'.format(att_score.shape))\n    attention_np = np.squeeze(att_score, (0, 1)).astype(np.uint8)\n\n    im = Image.fromarray(np.dstack((attention_np, attention_np, attention_np)))\n    buf = io.BytesIO()\n    im.save(buf, \'PNG\')\n    return buf.getvalue()\n\n    \ndef get_ransac_image_byte(img_1, locations_1, descriptors_1, img_2, locations_2, descriptors_2, save_path=None, use_opencv_match_vis=True):\n    """"""\n    Args:\n        img_1: image bytes. JPEG, PNG\n        img_2: image bytes. JPEG, PNG\n    Return:\n        ransac result PNG image as byte\n        score: number of matching inlier\n    """"""\n\n    # Convert image byte to 3 channel numpy array\n    with Image.open(BytesIO(img_1)) as img:\n        img_1 = load_image_into_numpy_array(img)\n    with Image.open(BytesIO(img_2)) as img:\n        img_2 = load_image_into_numpy_array(img)\n\n    inliers, locations_1_to_use, locations_2_to_use = get_inliers(\n        locations_1,\n        descriptors_1,\n        locations_2,\n        descriptors_2)\n\n    # Visualize correspondences, and save to file.\n    #fig, ax = plt.subplots(figsize=IMAGE_SIZE)\n    inlier_idxs = np.nonzero(inliers)[0]\n    score = sum(inliers)\n    if score is None:\n        score = 0\n\n    \n    if use_opencv_match_vis:\n        inlier_matches = []\n        for idx in inlier_idxs:\n            inlier_matches.append(cv2.DMatch(idx, idx, 0))\n        \n        kp1 =[]\n        for point in locations_1_to_use:\n            kp = cv2.KeyPoint(point[1], point[0], _size=1)\n            kp1.append(kp)\n\n        kp2 =[]\n        for point in locations_2_to_use:\n            kp = cv2.KeyPoint(point[1], point[0], _size=1)\n            kp2.append(kp)\n\n\n        ransac_img = cv2.drawMatches(img_1, kp1, img_2, kp2, inlier_matches, None, flags=0)\n        ransac_img = cv2.cvtColor(ransac_img, cv2.COLOR_BGR2RGB)    \n        image_byte = cv2.imencode(\'.png\', ransac_img)[1].tostring()\n\n    else:\n        plot_matches(\n            ax,\n            img_1,\n            img_2,\n            locations_1_to_use,\n            locations_2_to_use,\n            np.column_stack((inlier_idxs, inlier_idxs)),\n            matches_color=\'b\')\n        ax.axis(\'off\')\n        extent = ax.get_window_extent().transformed(fig.dpi_scale_trans.inverted())      \n        buf = io.BytesIO()\n        fig.savefig(buf, bbox_inches=extent, format=\'png\')\n        plt.close(\'all\') # close resources. \n        image_byte = buf.getvalue()\n    \n    return image_byte, score\n\n\n'"
train/config.py,0,"b'""""""\nconfig.py\n""""""\n\nimport argparse\nimport time\nimport torchvision.models as models\n\n# helper func.\ndef str2bool(v):\n    return v.lower() in (\'true\', \'1\')\n\n\n# Parser\nparser = argparse.ArgumentParser(\'delf\')\n\n# Common options.\nparser.add_argument(\'--gpu_id\', \n                    default=\'4\', \n                    type=str, \n                    help=\'id(s) for CUDA_VISIBLE_DEVICES\')\nparser.add_argument(\'--manualSeed\', \n                    type=int, \n                    default=int(time.time()), \n                    help=\'manual seed\')\n# Experiment\nparser.add_argument(\'--expr\', \n                    default=\'devel\', \n                    type=str, \n                    help=\'experiment name\')\nparser.add_argument(\'--load_from\', \n                    default=\'dummy\',\n                    type=str, \n                    help=\'from which experiment the model be loaded\')\n# Datasets\nparser.add_argument(\'--stage\', \n                    default=\'finetune\', \n                    type=str, \n                    help=\'target stage: finetune | keypoint\')\nparser.add_argument(\'--train_path_for_pretraining\', \n                    default=\'../../data/landmarks/landmarks_full_train\', \n                    type=str)\nparser.add_argument(\'--val_path_for_pretraining\', \n                    default=\'../../data/landmarks/landmarks_full_val\', \n                    type=str)\nparser.add_argument(\'--train_path_for_finetuning\', \n                    default=\'../../data/landmarks/landmarks_clean_train\', \n                    type=str)\nparser.add_argument(\'--val_path_for_finetuning\', \n                    default=\'../../data/landmarks/landmarks_clean_val\', \n                    type=str)\nparser.add_argument(\'--workers\', \n                    default=20, \n                    type=int,\n                    help=\'number of data loading workers (default: 4)\')\n# preprocessing\nparser.add_argument(\'--finetune_sample_size\',\n                    default=256,\n                    type=int,\n                    help=\'finetune resize (default: 256)\')\nparser.add_argument(\'--finetune_crop_size\',\n                    default=224,\n                    type=int,\n                    help=\'finetune crop (default: 224)\')\nparser.add_argument(\'--keypoint_sample_size\', \n                    default=900, \n                    type=int,\n                    help=\'keypoint (default: 900)\')\nparser.add_argument(\'--keypoint_crop_size\', \n                    default=720, \n                    type=int,\n                    help=\'keypoint (default: 720)\')\nparser.add_argument(\'--target_layer\', \n                    default=\'layer3\', \n                    type=str,\n                    help=\'target layer you wish to extract local features from: layer3 | layer4\')\nparser.add_argument(\'--use_random_gamma_rescale\',\n                    type=str2bool,\n                    default=True,\n                    help=\'apply gamma rescaling in range of [0.3535, 1.0]\')\n# training parameters\nparser.add_argument(\'--finetune_epoch\',\n                    default=30,\n                    type=int,\n                    help=\'number of total epochs for finetune stage.\')\nparser.add_argument(\'--keypoint_epoch\',\n                    default=30,\n                    type=int,\n                    help=\'number of total epochs for keypoint stage.\')\nparser.add_argument(\'--lr\',\n                    default=0.008,\n                    type=float,\n                    help=\'initial learning rate\')\nparser.add_argument(\'--lr_gamma\',\n                    default=0.5,\n                    type=float,\n                    help=\'decay factor of learning rate\')\nparser.add_argument(\'--lr_stepsize\',\n                    default=10,\n                    type=int,\n                    help=\'decay learning rate at every specified epoch.\')\nparser.add_argument(\'--weight_decay\',\n                    default=0.0001,\n                    type=float,\n                    help=\'weight decay (l2 penalty)\')\nparser.add_argument(\'--optim\',\n                    default=\'sgd\',\n                    type=str,\n                    help=\'optimizer: rmsprop | sgd | adam\')\nparser.add_argument(\'--train_batch_size\',\n                    default=8,\n                    type=int,\n                    help=\'train batchsize (default: 16)\')\nparser.add_argument(\'--val_batch_size\',\n                    default=8,\n                    type=int,\n                    help=\'val batchsize (default: 16)\')\nparser.add_argument(\'--ncls\',\n                    default=586,\n                    type=int,\n                    help=\'number of classes\')\nparser.add_argument(\'--lr_decay\',\n                    default=0.5,\n                    type=float,\n                    help=\'lr decay factor\')\nparser.add_argument(\'--arch\',\n                    metavar=\'ARCH\',\n                    default=\'resnet50\',\n                    choices=[\'resnet50, resnet101, resnet152\'],\n                    help=\'only support resnet50 at the moment.\')\n\n## parse and save config.\nconfig, _ = parser.parse_known_args()\n\n\n'"
train/dataloader.py,2,"b'\n#-*- coding: utf-8 -*-\n\n\'\'\'\ndataloader.py\n\'\'\'\n\nimport sys, os, time\n\nimport torch\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom PIL import Image, ImageFile\nImage.MAX_IMAGE_PIXELS = 1000000000     # to avoid error ""https://github.com/zimeon/iiif/issues/11""\nImage.warnings.simplefilter(\'error\', Image.DecompressionBombWarning)\nImageFile.LOAD_TRUNCATED_IMAGES = True  # to avoid error ""https://github.com/python-pillow/Pillow/issues/1510""\n\ndef get_loader(\n    train_path,\n    val_path,\n    stage,\n    train_batch_size,\n    val_batch_size,\n    sample_size,\n    crop_size,\n    workers):\n\n    if stage in [\'finetune\']:\n        # for train\n        prepro = []\n        prepro.append(transforms.Resize(size=sample_size))\n        prepro.append(transforms.CenterCrop(size=sample_size))\n        prepro.append(transforms.RandomCrop(size=crop_size, padding=0))\n        prepro.append(transforms.RandomHorizontalFlip())\n        #prepro.append(transforms.RandomRotation((-15, 15)))        # experimental.\n        prepro.append(transforms.ToTensor())\n        train_transform = transforms.Compose(prepro)\n        train_path = train_path\n        \n        # for val\n        prepro = []\n        prepro.append(transforms.Resize(size=sample_size))\n        prepro.append(transforms.CenterCrop(size=crop_size))\n        prepro.append(transforms.ToTensor())\n        val_transform = transforms.Compose(prepro)\n        val_path = val_path\n\n    elif stage in [\'keypoint\']:\n        # for train\n        prepro = []\n        prepro.append(transforms.Resize(size=sample_size))\n        prepro.append(transforms.CenterCrop(size=sample_size))\n        prepro.append(transforms.RandomCrop(size=crop_size, padding=0))\n        prepro.append(transforms.RandomHorizontalFlip())\n        #prepro.append(transforms.RandomRotation((-15, 15)))        # experimental.\n        prepro.append(transforms.ToTensor())\n        train_transform = transforms.Compose(prepro)\n        train_path = train_path\n        \n        # for val\n        prepro = []\n        prepro.append(transforms.Resize(size=sample_size))\n        prepro.append(transforms.CenterCrop(size=crop_size))\n        prepro.append(transforms.ToTensor())\n        val_transform = transforms.Compose(prepro)\n        val_path = val_path\n    \n    # image folder dataset.\n    train_dataset = datasets.ImageFolder(root = train_path,\n                                         transform = train_transform)\n    val_dataset = datasets.ImageFolder(root = val_path,\n                                       transform = val_transform)\n\n    # return train/val dataloader\n    train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n                                               batch_size = train_batch_size,\n                                               shuffle = True,\n                                               num_workers = workers)\n    val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\n                                             batch_size = val_batch_size,\n                                             shuffle = False,\n                                             num_workers = workers)\n\n    return train_loader, val_loader\n\n\n\n'"
train/delf.py,6,"b'\n# -*- coding: utf-8 -*-\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport os, sys, time\nsys.path.append(\'../\')\nimport random\nimport logging\nimport copy\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\nfrom train.layers import (\n    CMul, \n    Flatten, \n    ConcatTable, \n    Identity, \n    Reshape, \n    SpatialAttention2d, \n    WeightedSum2d)\n\n\n\'\'\' helper functions\n\'\'\'\n\ndef __unfreeze_weights__(module_dict, freeze=[]):\n    for _, v in enumerate(freeze):\n        module = module_dict[v]\n        for param in module.parameters():\n            param.requires_grad = True\n    \ndef __freeze_weights__(module_dict, freeze=[]):\n    for _, v in enumerate(freeze):\n        module = module_dict[v]\n        for param in module.parameters():\n            param.requires_grad = False\n\ndef __print_freeze_status__(model):\n    \'\'\'print freeze stagus. only for debugging purpose.\n    \'\'\'\n    for i, module in enumerate(model.named_children()):\n        for param in module[1].parameters():\n            print(\'{}:{}\'.format(module[0], str(param.requires_grad)))\n\ndef __load_weights_from__(module_dict, load_dict, modulenames):\n    for modulename in modulenames:\n        module = module_dict[modulename]\n        print(\'loaded weights from module ""{}"" ...\'.format(modulename))\n        module.load_state_dict(load_dict[modulename])\n\ndef __deep_copy_module__(module, exclude=[]):\n    modules = {}\n    for name, m in module.named_children():\n        if name not in exclude:\n            modules[name] = copy.deepcopy(m)\n            print(\'deep copied weights from layer ""{}"" ...\'.format(name))\n    return modules\n\ndef __cuda__(model):\n    if torch.cuda.is_available():\n        model.cuda()\n    return model\n\n\n\'\'\'Delf\n\'\'\'\n\nclass Delf_V1(nn.Module):\n    def __init__(\n        self,\n        ncls=None,\n        load_from=None,\n        arch=\'resnet50\',\n        stage=\'inference\',\n        target_layer=\'layer3\',\n        use_random_gamma_rescale=False):\n\n        super(Delf_V1, self).__init__()\n\n        self.arch = arch\n        self.stage = stage\n        self.target_layer = target_layer\n        self.load_from = load_from\n        self.use_random_gamma_rescale = use_random_gamma_rescale\n\n        self.module_list = nn.ModuleList()\n        self.module_dict = {}\n        self.end_points = {}\n\n        in_c = self.__get_in_c__()\n        if self.stage in [\'finetune\']:\n            use_pretrained_base = True\n            exclude = [\'avgpool\', \'fc\']\n\n        elif self.stage in [\'keypoint\']:\n            use_pretrained_base = False\n            self.use_l2_normalized_feature = True\n            if self.target_layer in [\'layer3\']:\n                exclude = [\'layer4\', \'avgpool\', \'fc\']\n            if self.target_layer in [\'layer4\']:\n                exclude = [\'avgpool\', \'fc\']\n\n        else:\n            assert self.stage in [\'inference\']\n            use_pretrained_base = False\n            self.use_l2_normalized_feature = True\n            if self.target_layer in [\'layer3\']:\n                exclude = [\'layer4\', \'avgpool\', \'fc\']\n            if self.target_layer in [\'layer4\']:\n                exclude = [\'avgpool\', \'fc\']\n\n        if self.arch in [\'resnet18\', \'resnet34\', \'resnet50\', \'resnet101\', \'resnet152\']:\n            print(\'[{}] loading {} pretrained ImageNet weights ... It may take few seconds...\'\n                    .format(self.stage, self.arch))\n            module = models.__dict__[self.arch](pretrained=use_pretrained_base)\n            module_state_dict = __deep_copy_module__(module, exclude=exclude)\n            module = None\n\n            # endpoint: base\n            submodules = []\n            submodules.append(module_state_dict[\'conv1\'])\n            submodules.append(module_state_dict[\'bn1\'])\n            submodules.append(module_state_dict[\'relu\'])\n            submodules.append(module_state_dict[\'maxpool\'])\n            submodules.append(module_state_dict[\'layer1\'])\n            submodules.append(module_state_dict[\'layer2\'])\n            submodules.append(module_state_dict[\'layer3\'])\n            self.__register_module__(\'base\', submodules)\n\n            # build structure.\n            if self.stage in [\'finetune\']:\n                # endpoint: layer4, pool\n                self.__register_module__(\'layer4\', module_state_dict[\'layer4\'])\n                self.__register_module__(\'pool\', nn.AvgPool2d(\n                    kernel_size=7, stride=1, padding=0,\n                    ceil_mode=False, count_include_pad=True))\n            elif self.stage in [\'keypoint\', \'inference\']:\n                # endpoint: attn, pool\n                self.__register_module__(\'attn\', SpatialAttention2d(in_c=in_c, act_fn=\'relu\'))\n                self.__register_module__(\'pool\', WeightedSum2d())\n\n\n            if self.stage not in [\'inference\']:\n                # endpoint: logit\n                submodules = []\n                submodules.append(nn.Conv2d(in_c, ncls, 1))\n                submodules.append(Flatten())\n                self.__register_module__(\'logits\', submodules)\n\n            # load weights.\n            if self.stage in [\'keypoint\']:\n                load_dict = torch.load(self.load_from)\n                __load_weights_from__(self.module_dict, load_dict, modulenames=[\'base\'])\n                __freeze_weights__(self.module_dict, freeze=[\'base\'])\n                print(\'load model from ""{}""\'.format(load_from))\n            elif self.stage in [\'inference\']:\n                load_dict = torch.load(self.load_from)\n                __load_weights_from__(self.module_dict, load_dict, modulenames=[\'base\',\'attn\',\'pool\'])\n                print(\'load model from ""{}""\'.format(load_from))\n                \n\n    def __register_module__(self, modulename, module):\n        if isinstance(module, list) or isinstance(module, tuple):\n            module = nn.Sequential(*module)\n        self.module_list.append(module)\n        self.module_dict[modulename] = module\n\n    def __get_in_c__(self):\n        # adjust input channels according to arch.\n        if self.arch in [\'resnet18\', \'resnet34\']:\n            in_c = 512\n        elif self.arch in [\'resnet50\', \'resnet101\', \'resnet152\']:\n            if self.stage in [\'finetune\']:\n                in_c = 2048\n            elif self.stage in [\'keypoint\', \'inference\']:\n                if self.target_layer in [\'layer3\']:\n                    in_c = 1024\n                elif self.target_layer in [\'layer4\']:\n                    in_c = 2048\n        return in_c\n\n    def __forward_and_save__(self, x, modulename):\n        module = self.module_dict[modulename]\n        x = module(x)\n        self.end_points[modulename] = x\n        return x\n\n    def __forward_and_save_feature__(self, x, model, name):\n        x = model(x)\n        self.end_points[name] = x.data\n        return x\n\n    def __gamma_rescale__(self, x, min_scale=0.3535, max_scale=1.0):\n        \'\'\'max_scale > 1.0 may cause training failure.\n        \'\'\'\n        h, w = x.size(2), x.size(3)\n        assert w == h, \'input must be square image.\'\n        gamma = random.uniform(min_scale, max_scale)\n        new_h, new_w = int(h*gamma), int(w*gamma)\n        x = F.upsample(x, size=(new_h, new_w), mode=\'bilinear\')\n        return x\n\n    def get_endpoints(self):\n        return self.end_points\n\n    def get_feature_at(self, modulename):\n        return copy.deepcopy(self.end_points[modulename].data.cpu())\n\n    def write_to(self, state):\n        if self.stage in [\'finetune\']:\n            state[\'base\'] = self.module_dict[\'base\'].state_dict()\n            state[\'layer4\'] = self.module_dict[\'layer4\'].state_dict()\n            state[\'pool\'] = self.module_dict[\'pool\'].state_dict()\n            state[\'logits\'] = self.module_dict[\'logits\'].state_dict()\n        elif self.stage in [\'keypoint\']:\n            state[\'base\'] = self.module_dict[\'base\'].state_dict()\n            state[\'attn\'] = self.module_dict[\'attn\'].state_dict()\n            state[\'pool\'] = self.module_dict[\'pool\'].state_dict()\n            state[\'logits\'] = self.module_dict[\'logits\'].state_dict()\n        else:\n            assert self.stage in [\'inference\']\n            raise ValueError(\'inference does not support model saving!\')\n\n    def forward_for_serving(self, x):\n        \'\'\'\n        This function directly returns attention score and raw features\n        without saving to endpoint dict.\n        \'\'\'\n        x = self.__forward_and_save__(x, \'base\')\n        if self.target_layer in [\'layer4\']:\n            x = self.__forward_and_save__(x, \'layer4\')\n        ret_x = x\n        if self.use_l2_normalized_feature:\n            attn_x = F.normalize(x, p=2, dim=1)\n        else:\n            attn_x = x\n        attn_score = self.__forward_and_save__(x, \'attn\')\n        ret_s = attn_score\n        return ret_x.data.cpu(), ret_s.data.cpu()\n\n    def forward(self, x):\n        if self.stage in [\'finetune\']:\n            x = self.__forward_and_save__(x, \'base\')\n            x = self.__forward_and_save__(x, \'layer4\')\n            x = self.__forward_and_save__(x, \'pool\')\n            x = self.__forward_and_save__(x, \'logits\')\n        elif self.stage in [\'keypoint\']:\n            if self.use_random_gamma_rescale:\n                x = self.__gamma_rescale__(x)\n            x = self.__forward_and_save__(x, \'base\')\n            if self.target_layer in [\'layer4\']:\n                x = self.__forward_and_save__(x, \'layer4\')\n            if self.use_l2_normalized_feature:\n                attn_x = F.normalize(x, p=2, dim=1)\n            else:\n                attn_x = x\n            attn_score = self.__forward_and_save__(x, \'attn\')\n            x = self.__forward_and_save__([attn_x, attn_score], \'pool\')\n            x = self.__forward_and_save__(x, \'logits\')\n        \n        elif self.stage in [\'inference\']:\n            x = self.__forward_and_save__(x, \'base\')\n            if self.target_layer in [\'layer4\']:\n                x = self.__forward_and_save__(x, \'layer4\')\n            if self.use_l2_normalized_feature:\n                attn_x = F.normalize(x, p=2, dim=1)\n            else:\n                attn_x = x\n            attn_score = self.__forward_and_save__(x, \'attn\')\n            x = self.__forward_and_save__([attn_x, attn_score], \'pool\')\n\n        else:\n            raise ValueError(\'unsupported stage parameter: {}\'.format(self.stage))\n        return x\n\nif __name__==""__main__"":\n    pass;\n\n\n\n\n\n\n\n\n\n'"
train/layers.py,3,"b'# -*- coding: utf-8 -*-\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n\n\'\'\'custom layers\n\'\'\'\nclass Flatten(nn.Module):\n    def __init__(self):\n        super(Flatten, self).__init__()\n\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n    \n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass ConcatTable(nn.Module):\n    \'\'\'ConcatTable container in Torch7.\n    \'\'\'\n    def __init__(self, layer1, layer2):\n        super(ConcatTable, self).__init__()\n        self.layer1 = layer1\n        self.layer2 = layer2\n        \n    def forward(self,x):\n        return [self.layer1(x), self.layer2(x)]\n\n\nclass Identity(nn.Module):\n    \'\'\'\n    nn.Identity in Torch7.\n    \'\'\'\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\n    def __repr__(self):\n        return self.__class__.__name__ + \' (skip connection)\'\n\n\nclass Reshape(nn.Module):\n    \'\'\'\n    nn.Reshape in Torch7.\n    \'\'\'\n    def __init__(self, shape):\n        super(Reshape, self).__init__()\n        self.shape = shape\n    def forward(self, x):\n        return x.view(self.shape)\n    def __repr__(self):\n        return self.__class__.__name__ + \' (reshape to size: {})\'.format("" "".join(str(x) for x in self.shape))\n\n\nclass CMul(nn.Module):\n    \'\'\'\n    nn.CMul in Torch7.\n    \'\'\'\n    def __init__(self):\n        super(CMul, self).__init__()\n    def forward(self, x):\n        return x[0]*x[1]\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass WeightedSum2d(nn.Module):\n    def __init__(self):\n        super(WeightedSum2d, self).__init__()\n    def forward(self, x):\n        x, weights = x\n        assert x.size(2) == weights.size(2) and x.size(3) == weights.size(3),\\\n                \'err: h, w of tensors x({}) and weights({}) must be the same.\'\\\n                .format(x.size, weights.size)\n        y = x * weights                                       # element-wise multiplication\n        y = y.view(-1, x.size(1), x.size(2) * x.size(3))      # b x c x hw\n        return torch.sum(y, dim=2).view(-1, x.size(1), 1, 1)  # b x c x 1 x 1\n    def __repr__(self):\n        return self.__class__.__name__\n\n\nclass SpatialAttention2d(nn.Module):\n    \'\'\'\n    SpatialAttention2d\n    2-layer 1x1 conv network with softplus activation.\n    <!!!> attention score normalization will be added for experiment.\n    \'\'\'\n    def __init__(self, in_c, act_fn=\'relu\'):\n        super(SpatialAttention2d, self).__init__()\n        self.conv1 = nn.Conv2d(in_c, 512, 1, 1)                 # 1x1 conv\n        if act_fn.lower() in [\'relu\']:\n            self.act1 = nn.ReLU()\n        elif act_fn.lower() in [\'leakyrelu\', \'leaky\', \'leaky_relu\']:\n            self.act1 = nn.LeakyReLU()\n        self.conv2 = nn.Conv2d(512, 1, 1, 1)                    # 1x1 conv\n        self.softplus = nn.Softplus(beta=1, threshold=20)       # use default setting.\n\n    def forward(self, x):\n        \'\'\'\n        x : spatial feature map. (b x c x w x h)\n        s : softplus attention score \n        \'\'\'\n        x = self.conv1(x)\n        x = self.act1(x)\n        x = self.conv2(x)\n        x = self.softplus(x)\n        return x\n    \n    def __repr__(self):\n        return self.__class__.__name__\n\n'"
train/main.py,9,"b""'''\nmain.py\n'''\n\nimport os, sys, time\nsys.path.append('../')\nimport shutil\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.backends.cudnn as cudnn\nimport torch.optim as optim\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport torchvision.models as models\n\nfrom config import config\n\n\ndef main():\n    # print config.\n    state = {k: v for k, v in config._get_kwargs()}\n    print(state)\n\n    # if use cuda.\n    os.environ['CUDA_VISIBLE_DEVICES'] = config.gpu_id\n    use_cuda = torch.cuda.is_available()\n\n    # Random seed\n    if config.manualSeed is None:\n        config.manualSeed = random.randint(1, 10000)\n    random.seed(config.manualSeed)\n    torch.manual_seed(config.manualSeed)\n    if use_cuda:\n        torch.cuda.manual_seed_all(config.manualSeed)\n        torch.backends.cudnn.benchmark = True           # speed up training.\n    \n    # data loader\n    from dataloader import get_loader\n    if config.stage in ['finetune']:\n        sample_size = config.finetune_sample_size\n        crop_size = config.finetune_crop_size\n    elif config.stage in ['keypoint']:\n        sample_size = config.keypoint_sample_size\n        crop_size = config.keypoint_crop_size\n   \n    # dataloader for pretrain\n    train_loader_pt, val_loader_pt = get_loader(\n        train_path = config.train_path_for_pretraining,\n        val_path = config.val_path_for_pretraining,\n        stage = config.stage,\n        train_batch_size = config.train_batch_size,\n        val_batch_size = config.val_batch_size,\n        sample_size = sample_size,\n        crop_size = crop_size,\n        workers = config.workers)\n    # dataloader for finetune\n    train_loader_ft, val_loader_ft = get_loader(\n        train_path = config.train_path_for_finetuning,\n        val_path = config.val_path_for_finetuning,\n        stage = config.stage,\n        train_batch_size = config.train_batch_size,\n        val_batch_size = config.val_batch_size,\n        sample_size = sample_size,\n        crop_size = crop_size,\n        workers = config.workers)\n    \n\n    # load model\n    from delf import Delf_V1\n    model = Delf_V1(\n        ncls = config.ncls,\n        load_from = config.load_from,\n        arch = config.arch,\n        stage = config.stage,\n        target_layer = config.target_layer,\n        use_random_gamma_rescale = config.use_random_gamma_rescale)\n\n    # solver\n    from solver import Solver\n    solver = Solver(config=config, model=model)\n    if config.stage in ['finetune']:\n        epochs = config.finetune_epoch\n    elif config.stage in ['keypoint']:\n        epochs = config.keypoint_epoch\n\n    # train/test for N-epochs. (50%: pretain with datasetA, 50%: finetune with datasetB)\n    for epoch in range(epochs):\n        if epoch < int(epochs * 0.5):\n            print('[{:.1f}] load pretrain dataset: {}'.format(\n                float(epoch) / epochs,\n                config.train_path_for_pretraining))\n            train_loader = train_loader_pt\n            val_loader = val_loader_pt\n        else:\n            print('[{:.1f}] load finetune dataset: {}'.format(\n                float(epoch) / epochs,\n                config.train_path_for_finetuning))\n            train_loader = train_loader_ft\n            val_loader = val_loader_ft\n\n        solver.train('train', epoch, train_loader, val_loader)\n        solver.train('val', epoch, train_loader, val_loader)\n\n    print('Congrats! You just finished DeLF training.')\n\n\nif __name__ == '__main__':\n    main()\n"""
train/solver.py,6,"b""#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n'''\nPyTorch Implementation of training DeLF feature.\nSolver for step 1 (finetune local descriptor)\nnashory, 2018.04\n'''\nimport os, sys, time\nimport shutil\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\nfrom utils import Bar, Logger, AverageMeter, compute_precision_top_k, mkdir_p\n    \n'''helper functions.\n'''\ndef __cuda__(x):\n    if torch.cuda.is_available():\n        return x.cuda()\n    else:\n        return x\n\ndef __is_cuda__():\n    return torch.cuda.is_available()\n\ndef __to_var__(x, volatile=False):\n    return Variable(x, volatile=volatile)\n\ndef __to_tensor__(x):\n    return x.data\n\n\nclass Solver(object):\n    def __init__(self, config, model):\n        self.state = {k: v for k, v in config._get_kwargs()} \n        self.config = config\n        self.epoch = 0          # global epoch.\n        self.best_acc = 0       # global best accuracy.\n        self.prefix = os.path.join('repo', config.expr)\n        \n        # ship model to cuda\n        self.model = __cuda__(model)\n\n        # define criterion and optimizer\n        self.criterion = nn.CrossEntropyLoss()\n        if config.optim.lower() in ['rmsprop']:\n            self.optimizer = optim.RMSprop(filter(lambda p: p.requires_grad, self.model.parameters()),\n                                           lr=config.lr,\n                                           weight_decay=config.weight_decay)\n        elif config.optim.lower() in ['sgd']:\n            self.optimizer = optim.SGD(filter(lambda p: p.requires_grad, self.model.parameters()),\n                                       lr=config.lr,\n                                       weight_decay=config.weight_decay)\n        elif config.optim.lower() in ['adam']:\n            self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.model.parameters()),\n                                        lr=config.lr,\n                                        weight_decay=config.weight_decay)\n        \n        # decay learning rate by a factor of 0.5 every 10 epochs\n        self.lr_scheduler = optim.lr_scheduler.StepLR(\n            self.optimizer,\n            step_size=config.lr_stepsize, \n            gamma=config.lr_gamma)\n\n        # create directory to save result if not exist.\n        self.ckpt_path = os.path.join(self.prefix, config.stage, 'ckpt')\n        self.log_path = os.path.join(self.prefix, config.stage, 'log')\n        self.image_path = os.path.join(self.prefix, config.stage, 'image')\n        mkdir_p(self.ckpt_path)\n        mkdir_p(self.log_path)\n        mkdir_p(self.image_path)\n\n        # set logger.\n        self.logger = {}\n        self.title = 'DeLF-{}'.format(config.stage.upper())\n        self.logger['train'] = Logger(os.path.join(self.prefix, config.stage, 'log/train.log'))\n        self.logger['val'] = Logger(os.path.join(self.prefix, config.stage, 'log/val.log'))\n        self.logger['train'].set_names(\n            ['epoch','lr', 'loss', 'top1_accu', 'top3_accu', 'top5_accu'])\n        self.logger['val'].set_names(\n            ['epoch','lr', 'loss', 'top1_accu', 'top3_accu', 'top5_accu'])\n        \n    def __exit__(self):\n        self.train_logger.close()\n        self.val_logger.close()\n\n\n    def __adjust_pixel_range__(self, \n                             x,\n                             range_from=[0,1],\n                             range_to=[-1,1]):\n        '''\n        adjust pixel range from <range_from> to <range_to>.\n        '''\n        if not range_from == range_to:\n            scale = float(range_to[1]-range_to[0])/float(range_from[1]-range_from[0])\n            bias = range_to[0]-range_from[0]*scale\n            x = x.mul(scale).add(bias)\n            return x\n\n    def __save_checkpoint__(self, state, ckpt='ckpt', filename='checkpoint.pth.tar'):\n        filepath = os.path.join(ckpt, filename)\n        torch.save(state, filepath)\n    \n    def __solve__(self, mode, epoch, dataloader):\n        '''solve\n        mode: train / val\n        '''\n        batch_timer = AverageMeter()\n        data_timer = AverageMeter()\n        prec_losses = AverageMeter()\n        prec_top1 = AverageMeter()\n        prec_top3 = AverageMeter()\n        prec_top5 = AverageMeter()\n        \n        if mode in ['val']:\n            pass;\n            #confusion_matrix = ConusionMeter()\n        \n        since = time.time()\n        bar = Bar('[{}]{}'.format(mode.upper(), self.title), max=len(dataloader))\n        for batch_idx, (inputs, labels) in enumerate(dataloader):\n            # measure data loading time\n            data_timer.update(time.time() - since)\n            \n            # wrap inputs in variable\n            if mode in ['train']:\n                if __is_cuda__():\n                    inputs = inputs.cuda()\n                    labels = labels.cuda(async=True)\n                inputs = __to_var__(inputs)\n                labels = __to_var__(labels)\n            elif mode in ['val']:\n                if __is_cuda__():\n                    inputs = inputs.cuda()\n                    labels = labels.cuda(async=True)\n                inputs = __to_var__(inputs, volatile=True)\n                labels = __to_var__(labels, volatile=False)\n            \n            # forward\n            outputs = self.model(inputs)\n            loss = self.criterion(outputs, labels)\n            \n            # backward + optimize\n            if mode in ['train']:\n                self.optimizer.zero_grad()\n                loss.backward()\n                self.optimizer.step()\n            \n            # statistics\n            prec_1, prec_3, prec_5 = compute_precision_top_k(\n                __to_tensor__(outputs),\n                __to_tensor__(labels),\n                top_k=(1,3,5))\n            batch_size = inputs.size(0)\n            prec_losses.update(__to_tensor__(loss)[0], batch_size)\n            prec_top1.update(prec_1[0], batch_size)\n            prec_top3.update(prec_3[0], batch_size)\n            prec_top5.update(prec_5[0], batch_size)\n            \n            # measure elapsed time\n            batch_timer.update(time.time() - since)\n            since = time.time()\n            \n            # progress\n            log_msg = ('\\n[{mode}][epoch:{epoch}][iter:({batch}/{size})]'+\n                        '[lr:{lr}] loss: {loss:.4f} | top1: {top1:.4f} | ' +\n                        'top3: {top3:.4f} | top5: {top5:.4f} | eta: ' +\n                        '(data:{dt:.3f}s),(batch:{bt:.3f}s),(total:{tt:})') \\\n                        .format(\n                            mode=mode,\n                            epoch=self.epoch+1,\n                            batch=batch_idx+1,\n                            size=len(dataloader),\n                            lr=self.lr_scheduler.get_lr()[0],\n                            loss=prec_losses.avg,\n                            top1=prec_top1.avg,\n                            top3=prec_top3.avg,\n                            top5=prec_top5.avg,\n                            dt=data_timer.val,\n                            bt=batch_timer.val,\n                            tt=bar.elapsed_td)\n            print(log_msg)\n            bar.next()\n        bar.finish()\n\n        # write to logger\n        self.logger[mode].append([self.epoch+1,\n                                  self.lr_scheduler.get_lr()[0],\n                                  prec_losses.avg,\n                                  prec_top1.avg,\n                                  prec_top3.avg,\n                                  prec_top5.avg])\n        \n        # save model\n        if mode == 'val' and prec_top1.avg > self.best_acc:\n            print('best_acc={}, new_best_acc={}'.format(self.best_acc, prec_top1.avg))\n            self.best_acc = prec_top1.avg\n            state = {\n                'epoch': self.epoch,\n                'acc': self.best_acc,\n                'optimizer': self.optimizer.state_dict(),\n            }\n            self.model.write_to(state)\n            filename = 'bestshot.pth.tar'\n            self.__save_checkpoint__(state, ckpt=self.ckpt_path, filename=filename)\n\n\n    def train(self, mode, epoch, train_loader, val_loader):\n        self.epoch = epoch\n        if mode in ['train']:\n            self.model.train()\n            self.lr_scheduler.step()\n            dataloader = train_loader\n        else:\n            assert mode == 'val'\n            self.model.eval()\n            dataloader = val_loader\n        self.__solve__(mode, epoch, dataloader)\n\n\n\n\n"""
utils/__init__.py,0,"b'\n""""""\nuseful utils.\n""""""\nfrom .misc import *\nfrom .logger import *\nfrom .confusionmeter import *\n\n# progress bar\nimport os, sys\nsys.path.append(os.path.join(os.path.dirname(__file__), ""progress""))\nfrom progress.bar import Bar as Bar\n'"
utils/logger.py,0,"b'# A simple torch style logger\n# (C) Wei YANG 2017\nfrom __future__ import absolute_import\nimport matplotlib.pyplot as plt\nimport os\nimport sys\nimport numpy as np\n\n__all__ = [\'Logger\', \'LoggerMonitor\', \'savefig\']\n\ndef savefig(fname, dpi=None):\n    dpi = 150 if dpi == None else dpi\n    plt.savefig(fname, dpi=dpi)\n    \ndef plot_overlap(logger, names=None):\n    names = logger.names if names == None else names\n    numbers = logger.numbers\n    for _, name in enumerate(names):\n        x = np.arange(len(numbers[name]))\n        plt.plot(x, np.asarray(numbers[name]))\n    return [logger.title + \'(\' + name + \')\' for name in names]\n\nclass Logger(object):\n    \'\'\'Save training process to log file with simple plot function.\'\'\'\n    def __init__(self, fpath, title=None, resume=False): \n        self.file = None\n        self.resume = resume\n        self.title = \'\' if title == None else title\n        if fpath is not None:\n            if resume: \n                self.file = open(fpath, \'r\') \n                name = self.file.readline()\n                self.names = name.rstrip().split(\'\\t\')\n                self.numbers = {}\n                for _, name in enumerate(self.names):\n                    self.numbers[name] = []\n\n                for numbers in self.file:\n                    numbers = numbers.rstrip().split(\'\\t\')\n                    for i in range(0, len(numbers)):\n                        self.numbers[self.names[i]].append(numbers[i])\n                self.file.close()\n                self.file = open(fpath, \'a\')  \n            else:\n                self.file = open(fpath, \'w\')\n\n    def set_names(self, names):\n        if self.resume: \n            pass\n        # initialize numbers as empty list\n        self.numbers = {}\n        self.names = names\n        for _, name in enumerate(self.names):\n            self.file.write(name)\n            self.file.write(\'\\t\')\n            self.numbers[name] = []\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n\n    def append(self, numbers):\n        assert len(self.names) == len(numbers), \'Numbers do not match names\'\n        for index, num in enumerate(numbers):\n            self.file.write(""{0:.6f}"".format(num))\n            self.file.write(\'\\t\')\n            self.numbers[self.names[index]].append(num)\n        self.file.write(\'\\n\')\n        self.file.flush()\n\n    def plot(self, names=None):   \n        names = self.names if names == None else names\n        numbers = self.numbers\n        for _, name in enumerate(names):\n            x = np.arange(len(numbers[name]))\n            plt.plot(x, np.asarray(numbers[name]))\n        plt.legend([self.title + \'(\' + name + \')\' for name in names])\n        plt.grid(True)\n\n    def close(self):\n        if self.file is not None:\n            self.file.close()\n\nclass LoggerMonitor(object):\n    \'\'\'Load and visualize multiple logs.\'\'\'\n    def __init__ (self, paths):\n        \'\'\'paths is a distionary with {name:filepath} pair\'\'\'\n        self.loggers = []\n        for title, path in paths.items():\n            logger = Logger(path, title=title, resume=True)\n            self.loggers.append(logger)\n\n    def plot(self, names=None):\n        plt.figure()\n        plt.subplot(121)\n        legend_text = []\n        for logger in self.loggers:\n            legend_text += plot_overlap(logger, names)\n        plt.legend(legend_text, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n        plt.grid(True)\n                    \nif __name__ == \'__main__\':\n    # # Example\n    # logger = Logger(\'test.txt\')\n    # logger.set_names([\'Train loss\', \'Valid loss\',\'Test loss\'])\n\n    # length = 100\n    # t = np.arange(length)\n    # train_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # valid_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n    # test_loss = np.exp(-t / 10.0) + np.random.rand(length) * 0.1\n\n    # for i in range(0, length):\n    #     logger.append([train_loss[i], valid_loss[i], test_loss[i]])\n    # logger.plot()\n\n    # Example: logger monitor\n    paths = {\n    \'resadvnet20\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet20/log.txt\', \n    \'resadvnet32\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet32/log.txt\',\n    \'resadvnet44\':\'/home/wyang/code/pytorch-classification/checkpoint/cifar10/resadvnet44/log.txt\',\n    }\n\n    field = [\'Valid Acc.\']\n\n    monitor = LoggerMonitor(paths)\n    monitor.plot(names=field)\n    savefig(\'test.eps\')'"
utils/misc.py,3,"b'\'\'\'Some helper functions for PyTorch, including:\n    - get_mean_and_std: calculate the mean and std value of dataset.\n    - msr_init: net parameter initialization.\n    - progress_bar: progress bar mimic xlua.progress.\n\'\'\'\n\nfrom __future__ import print_function, absolute_import\n\nimport errno\nimport os\nimport sys\nimport time\nimport math\n\nimport torch.nn as nn\nimport torch.nn.init as init\nfrom torch.autograd import Variable\n\n__all__ = [\'get_mean_and_std\', \'init_params\', \'mkdir_p\', \'AverageMeter\', \'compute_precision_top_k\']\n\ndef compute_precision_top_k(output, target, top_k=(1,)):\n    """"""Computes the precision@k for the specified values of k""""""\n    maxk = max(top_k)\n    batch_size = target.size(0)\n\n    _, pred = output.topk(maxk, 1, True, True)\n    pred = pred.t()\n    correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n    res = []\n    for k in top_k:\n        correct_k = correct[:k].view(-1).float().sum(0)\n        res.append(correct_k.mul_(100.0 / batch_size))\n    return res\n\ndef mkdir_p(path):\n    \'\'\'make dir if not exist\'\'\'\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\nclass AverageMeter(object):\n    """"""Computes and stores the average and current value\n       Imported from https://github.com/pytorch/examples/blob/master/imagenet/main.py#L247-L262\n    """"""\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\n\n\n'"
