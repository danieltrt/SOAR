file_path,api_count,code
setup.py,0,"b'from setuptools import setup\nimport setuptools\n\nwith open(""README.md"", ""r"") as fh:\n    long_description = fh.read()\n\nsetup(name=\'pytorch-policy\',\n      version=\'0.1.1\',\n      description=\'Reinforcement Learning in Pytorch\',\n      url=\'https://github.com/navneet-nmk/pytorch-rl\',\n      author=\'Navneet M Kumar\',\n      author_email=\'navneet.nmk@gmail.com\',\n      long_description=long_description,\n      long_description_content_type=""text/markdown"",\n      packages=setuptools.find_packages(),\n      license=\'MIT\',\n      zip_safe=False)'"
train_ddpg.py,4,"b""# Training script for the DDPG\n\nimport torch\n# Add this line to get better performance\ntorch.backends.cudnn.benchmark=True\nfrom Utils import utils\nimport torch.optim as optim\nfrom models.DDPG import DDPG\nimport torch.nn.functional as F\nuse_cuda = torch.cuda.is_available()\nfrom Training.trainer import Trainer\nimport os\n\nif __name__ == '__main__':\n    # Specify the environment name and create the appropriate environment\n    seed = 4240\n    env = utils.EnvGenerator(name='FetchReach-v1', goal_based=False, seed=seed)\n    eval_env = utils.EnvGenerator(name='FetchReach-v1', goal_based=False,seed=seed)\n    action_dim = env.get_action_dim()\n    observation_dim = env.get_observation_dim()\n    goal_dim =  env.get_goal_dim()\n    env= env.get_environment()\n    eval_env = eval_env.get_environment()\n\n    # Training constants\n    her_training=True\n    # Future framnes to look at\n    future= 4\n\n    buffer_capacity = int(1e3)\n    q_dim = 1\n    batch_size = 128\n    hidden_units = 256\n    gamma = 0.98  # Discount Factor for future rewards\n    num_epochs = 50\n    learning_rate = 0.001\n    critic_learning_rate = 0.001\n    polyak_factor = 0.05\n    # Huber loss to aid small gradients\n    criterion = F.smooth_l1_loss\n    # Adam Optimizer\n    opt = optim.Adam\n\n    # Output Folder\n    output_folder = os.getcwd() + '/output_ddpg/'\n\n    # Convert the observation and action dimension to int\n    print(observation_dim)\n    observation_dim = int(observation_dim)\n    action_dim = int(action_dim)\n    print(action_dim)\n    goal_dim= int(goal_dim)\n\n    # Create the agent\n    agent = DDPG(num_hidden_units=hidden_units, input_dim=observation_dim+goal_dim,\n                      num_actions=action_dim, num_q_val=q_dim, batch_size=batch_size, random_seed=seed,\n                      use_cuda=use_cuda, gamma=gamma, actor_optimizer=opt, critic_optimizer=optim,\n                      actor_learning_rate=learning_rate, critic_learning_rate=critic_learning_rate,\n                      loss_function=criterion, polyak_constant=polyak_factor, buffer_capacity=buffer_capacity,\n                 goal_dim=goal_dim, observation_dim=observation_dim)\n\n    # Train the agent\n    trainer = Trainer(agent=agent, num_epochs=50, num_rollouts=19*50, num_eval_rollouts=100,\n                      max_episodes_per_epoch=50, env=env, eval_env=None,\n                      nb_train_steps=19*50, multi_gpu_training=False, random_seed=seed, future=future)\n\n    if her_training:\n        trainer.her_training()\n    else:\n        trainer.train()\n\n\n\n"""
train_dqn.py,18,"b'# Training file for the DQN\n\nimport gym\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom models import DQN\nimport random\nimport numpy as np\nimport torch.optim as optim\nfrom itertools import count\nimport math\nimport torch.nn.functional as F\n# Constants for training\nuse_cuda = torch.cuda.is_available()\nEPS_START = 0.9\nEPS_END = 0.05\nEPS_DECAY = 200\n\n# Preprocessing\nsteps_done = 0\n\n\ndef downsample(img):\n    return img[::2, ::2]\n\n\ndef preprocess(img):\n    img = downsample(img)\n    return img.astype(np.float)\n\n\ndef choose_best_action(model, state):\n    state = Variable(torch.FloatTensor(state))\n    if use_cuda:\n        state = state.cuda()\n        model = model.cuda()\n    state = state.unsqueeze(0)\n    state = torch.transpose(state, 1, 3)\n    state = torch.transpose(state, 2, 3)\n    Q_values = model(state)\n    value, indice = Q_values.max(1)\n    action = indice.data[0]\n    return action\n\n\ndef get_epsilon_iteration(steps_done):\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n                              math.exp(-1. * steps_done / EPS_DECAY)\n    return eps_threshold\n\n\ndef fit_batch(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, criterion,\n              iteration, learning_rate, use_polyak_averaging=True, polyak_constant=0.001):\n\n    # Step 1: Sample mini batch from B uniformly\n    if buffer.get_buffer_size() < batch_size:\n        return 0, 0\n    batch = buffer.sample_batch(batch_size)\n    states = []\n    new_states = []\n    actions = []\n    rewards = []\n    for k in batch:\n        state, action, new_state, reward = k\n        states.append(state)\n        actions.append(action)\n        new_states.append(new_state)\n        rewards.append(reward)\n\n    states = torch.FloatTensor(states)\n    states = torch.transpose(states, 1, 3)\n    states = torch.transpose(states, 2, 3)\n    states = Variable(states)\n    new_states = torch.FloatTensor(new_states)\n    new_states = torch.transpose(new_states, 1, 3)\n    new_states = torch.transpose(new_states, 2, 3)\n    new_states = Variable(new_states)\n\n    rewards = torch.FloatTensor(rewards)\n    rewards = Variable(rewards)\n\n    actions = torch.LongTensor(actions)\n    actions = actions.view(-1, 1)\n    actions = Variable(actions)\n\n    if use_cuda:\n        states = states.cuda()\n        actions = actions.cuda()\n        rewards = rewards.cuda()\n        new_states = new_states.cuda()\n        target_dqn_model = target_dqn_model.cuda()\n        dqn_model = dqn_model.cuda()\n\n    for p in target_dqn_model.parameters():\n        p.requires_grad = False\n\n    # Step 2: Compute the target values using the target network\n    Q_values = target_dqn_model(new_states)\n    next_Q_values, indice = Q_values.max(1)\n    y = rewards + gamma*next_Q_values\n    y = y.detach()\n\n    model_parameters = dqn_model.parameters()\n\n    optimizer = optim.Adam(model_parameters, lr=learning_rate)\n\n    # Zero the optimizer gradients\n    optimizer.zero_grad()\n\n    # Forward pass\n    outputs = dqn_model(states)\n    outputs = outputs.gather(1, actions)\n    loss = criterion(outputs, y)\n    loss.backward()\n    # Gradient clipping\n    for p in dqn_model.parameters():\n        p.grad.data.clamp(-1,1)\n    optimizer.step()\n    # Stabilizes training as proposed in the DDPG paper\n    if use_polyak_averaging:\n        t = polyak_constant\n        target_dqn_model.conv1.weight.data = t*(dqn_model.conv1.weight.data) + \\\n                                             (1-t)*(target_dqn_model.conv1.weight.data)\n        target_dqn_model.bn1.weight.data = t * (dqn_model.bn1.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.bn1.weight.data)\n        target_dqn_model.conv2.weight.data = t * (dqn_model.conv2.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.conv2.weight.data)\n        target_dqn_model.bn2.weight.data = t * (dqn_model.bn2.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.bn2.weight.data)\n        target_dqn_model.conv3.weight.data = t * (dqn_model.conv3.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.conv3.weight.data)\n        target_dqn_model.bn3.weight.data = t * (dqn_model.bn3.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.bn3.weight.data)\n        target_dqn_model.fully_connected_layer.weight.data = t * (dqn_model.fully_connected_layer.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.fully_connected_layer.weight.data)\n        target_dqn_model.output_layer.weight.data = t * (dqn_model.output_layer.weight.data) + \\\n                                             (1 - t) * (target_dqn_model.output_layer.weight.data)\n    else:\n        if n == iteration:\n            target_dqn_model.load_state_dict(dqn_model.state_dict())\n\n    return loss, torch.sum(rewards)\n\n\ndef train(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, num_epochs, criterion, learning_rate,\n          use_double_q_learning = False):\n    for iteration in range(num_epochs):\n        print(""Epoch "", iteration)\n        state = env.reset()\n        state = preprocess(state)\n        loss = 0\n        re = 0\n        # Populate the buffer\n        for t in count():\n            global steps_done\n            epsilon = get_epsilon_iteration(steps_done)\n            steps_done +=1\n            # Choose a random action\n            if random.random() < epsilon:\n                action = env.action_space.sample()\n                new_state, reward, done, info = env.step(action)\n            else:\n                if use_double_q_learning:\n                    action = choose_best_action(dqn_model, state)\n                else:\n                    action = choose_best_action(target_dqn_model, state)\n                new_state, reward, done, info = env.step(action)\n\n            new_state = preprocess(new_state)\n            buffer.add((state, action, new_state, reward))\n            state = new_state\n            # Fit the model on a batch of data\n            loss_n, r = fit_batch(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, criterion, iteration, learning_rate)\n            #print(loss)\n            loss += loss_n\n            re += r\n            if done:\n                break\n        print(""Loss for episode"", iteration, "" is "", loss.data/t)\n        print(""Reward for episode"", iteration, "" is "", re)\n\n    return target_dqn_model, dqn_model\n\n\nif __name__ == \'__main__\':\n    env = gym.make(\'BreakoutDeterministic-v4\')\n    input_shape = env.observation_space.shape\n    img_height, img_width, img_channels = input_shape\n    num_actions = env.action_space.n\n\n    dqn_model = DQN.ActionPredictionNetwork(num_conv_layers=16, input_channels=img_channels,\n                                            output_q_value=num_actions, pool_kernel_size=3,\n                                            kernel_size=3, dense_layer_features=256,\n                                            IM_HEIGHT=img_height//2, IM_WIDTH=img_width//2)\n\n    target_dqn_model = DQN.ActionPredictionNetwork(num_conv_layers=16, input_channels=img_channels,\n                                            output_q_value=num_actions, pool_kernel_size=3,\n                                            kernel_size=3, dense_layer_features=256,\n                                            IM_HEIGHT=img_height//2, IM_WIDTH=img_width//2)\n\n    buffer = DQN.ReplayBuffer(size_of_buffer=10000) # Experience Replay\n    batch_size= 32\n    gamma = 0.99 # Discount factor\n    num_epochs = 1000\n    learning_rate = 0.01\n    # Huber loss to aid small gradients\n    criterion = F.smooth_l1_loss\n    n = 10 # Target network parameter update\n    if use_cuda:\n        target_dqn_model = target_dqn_model.cuda()\n        dqn_model = dqn_model.cuda()\n    model, _ = train(target_dqn_model, dqn_model, buffer, batch_size, gamma, n, num_epochs, criterion, learning_rate,\n                     use_double_q_learning=True)\n    # Saving the model\n    path = \'/home/kumar/anaconda3/bin/python /home/kumar/PycharmProjects/Deep-Q-Learning/\'\n    torch.save(model.state_dict(), path)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
train_infogan.py,4,"b'# Training Script for the InfoGAN\n\nimport torch\n# Add this line to get better performance\ntorch.backends.cudnn.benchmark=True\nimport numpy as np\nfrom skimage import io, transform\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nimport os\nfrom models import infogan\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass MontezumaRevengeFramesDataset(Dataset):\n    """"""\n\n    Dataset consisting of the frames of the Atari Game-\n    Montezuma Revenge\n\n    """"""\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.list_files()\n\n    def __len__(self):\n        return len(self.images)\n\n    def list_files(self):\n        for m in os.listdir(self.root_dir):\n            if m.endswith(\'.jpg\'):\n                self.images.append(m)\n\n    def __getitem__(self, idx):\n        m = self.images[idx]\n        image = io.imread(os.path.join( self.root_dir, m))\n        sample = {\'image\': image}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n# Transformations\nclass Rescale(object):\n    """"""Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size * h / w, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        return {\'image\': img}\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        return {\'image\': torch.from_numpy(image)}\n\n\nif __name__ == \'__main__\':\n    dataset = MontezumaRevengeFramesDataset(root_dir=\'/mr\', transform=transforms.Compose([Rescale(256), ToTensor()]))\n    dataloader = DataLoader(dataset, batch_size=4,\n                            shuffle=True, num_workers=4)\n    model = infogan.InfoGAN(conv_layers=32,\n                              conv_kernel_size=3,\n                              generator_input_channels=1,\n                              generator_output_channels=3,\n                              batch_size=4, categorical_dim=10, continuous_dim=2,\n                              pool_kernel_size=3, height=256, width=256, discriminator_input_channels=3,\n                              discriminator_lr=1e-4, generator_lr=1e-4, discriminator_output_dim=1,\n                              output_dim=12, hidden_dim=256, num_epochs=100)\n\n    model.train(dataloader=dataloader)\n\n'"
Distributions/distributions.py,7,"b'import torch\nimport torch.nn.functional as F\n\ntry:\n    from torch.distributions import Distribution, Normal\nexcept ImportError:\n    print(""You should use a PyTorch version that has torch.distributions."")\n\n    import math\n    from numbers import Number\n\n    # Base Class distribution\n    class Distribution(object):\n        r""""""\n        Distribution is the abstract base class for probability distributions.\n        """"""\n\n        def sample(self):\n            """"""\n            Generates a single sample or single batch of samples if the distribution\n            parameters are batched.\n            """"""\n            raise NotImplementedError\n\n        def sample_n(self, n):\n            """"""\n            Generates n samples or n batches of samples if the distribution parameters\n            are batched.\n            """"""\n            raise NotImplementedError\n\n        def log_prob(self, value):\n            """"""\n            Returns the log of the probability density/mass function evaluated at\n            `value`.\n            Args:\n                value (Tensor or Variable):\n            """"""\n            raise NotImplementedError\n\n\n# Sigmoid Normal Distribution\nclass SigmoidNormal(Distribution):\n    """"""\n        Represent distribution of X where\n            X ~ sigmoid(Z)\n            Z ~ N(mean, std)\n        Note: this is not very numerically stable.\n        """"""\n\n    def __init__(self, normal_mean, normal_std, epsilon=1e-6):\n        super(SigmoidNormal, self).__init__()\n        """"""\n        :param normal_mean: Mean of the normal distribution\n        :param normal_std: Std of the normal distribution\n        :param epsilon: Numerical stability epsilon when computing log-prob.\n        """"""\n        self.normal = Normal(normal_mean, normal_std)\n        self.epsilon = epsilon\n\n    def sample_n(self, n, return_pre_sigmoid_value=False):\n        z = self.normal.sample_n(n)\n        if return_pre_sigmoid_value:\n            return F.sigmoid(z), z\n        else:\n            return F.sigmoid(z)\n\n    def log_prob(self, value, pre_sigmoid_value=None):\n        """"""\n        :param value: some value, x\n        :param pre_sigmoid_value: arcsigmoid(x)\n        :return:\n        """"""\n        if pre_sigmoid_value is None:\n            pre_sigmoid_value = torch.log(\n                    (value) / (1 - value)\n                )\n        return self.normal.log_prob(pre_sigmoid_value) - torch.log(\n                value*(1-value) + self.epsilon\n        )\n\n    def sample(self, return_pre_sigmoid_value=False):\n        z = self.normal.sample()\n        if return_pre_sigmoid_value:\n            return F.sigmoid(z), z\n        else:\n            return F.sigmoid(z)\n\n\n# Tanh Normal Distribution\nclass TanhNormal(Distribution):\n    """"""\n        Represent distribution of X where\n            X ~ tanh(Z)\n            Z ~ N(mean, std)\n        Note: this is not very numerically stable.\n        """"""\n\n    def __init__(self, normal_mean, normal_std, epsilon=1e-6):\n        super(TanhNormal, self).__init__()\n        """"""\n        :param normal_mean: Mean of the normal distribution\n        :param normal_std: Std of the normal distribution\n        :param epsilon: Numerical stability epsilon when computing log-prob.\n        """"""\n        self.normal = Normal(normal_mean, normal_std)\n        self.epsilon = epsilon\n\n    def sample_n(self, n, return_pre_tanh_value=False):\n        z = self.normal.sample_n(n)\n        if return_pre_tanh_value:\n            return F.tanh(z), z\n        else:\n            return F.tanh(z)\n\n    def log_prob(self, value, pre_tanh_value=None):\n        """"""\n        :param value: some value, x\n        :param pre_sigmoid_value: arcsigmoid(x)\n        :return:\n        """"""\n        if pre_tanh_value is None:\n            pre_sigmoid_value = torch.log(\n                    (1+value) / (1 - value)\n                )/2\n        return self.normal.log_prob(pre_tanh_value) - torch.log(\n                1- value*value + self.epsilon\n        )\n\n    def sample(self, return_pre_tanh_value=False):\n        z = self.normal.sample()\n        if return_pre_tanh_value:\n            return F.tanh(z), z\n        else:\n            return F.tanh(z)\n\n\n'"
Environments/OpensimEnv.py,0,b'import opensim\nfrom osim.env import ProstheticsEnv\nenv = ProstheticsEnv()\n\n\n\n'
Environments/env_wrappers.py,0,"b'""""""\n\nEnvironment wrappers to help the training - This is specifically used for training on the Mario\nenvironment used for the Empowerment driven models.\n\n""""""\n\nimport numpy as np\nfrom collections import deque\nfrom PIL import Image\nfrom gym.spaces.box import Box\nimport gym\nimport time, sys\nfrom multiprocessing import Process, Pipe\nfrom gym import spaces\nimport cv2\ncv2.ocl.setUseOpenCL(False)\n\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4, grid_env=False):\n        """"""Return only every `skip`-th frame""""""\n        super(MaxAndSkipEnv, self).__init__(env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = deque(maxlen=2)\n        self._skip = skip\n        self.grid_env = grid_env\n\n    def step(self, action):\n        """"""Repeat action, sum reward, and max over last observations.""""""\n        total_reward = 0.0\n        done = None\n        combined_info = {}\n        for _ in range(self._skip):\n            obs, reward, done, info = self.env.step(action)\n            if self.grid_env:\n                obs =  obs[\'image\']\n            self._obs_buffer.append(obs)\n            total_reward += reward\n            combined_info.update(info)\n            if done:\n                break\n        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n\n        return max_frame, total_reward, done, combined_info\n\n    def reset(self):\n        """"""Clear past frame buffer and init. to first obs. from inner env.""""""\n        self._obs_buffer.clear()\n        obs = self.env.reset()\n        self._obs_buffer.append(obs)\n        return obs\n\ndef flatten_lists(listoflists):\n    return [el for list_ in listoflists for el in list_]\n\ndef worker(remote, env_fn_wrapper):\n    env = env_fn_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.action_space, env.observation_space))\n        elif cmd == \'get_history\':\n            senv = env\n            while not hasattr(senv, \'get_history\'):\n                senv = senv.env\n            remote.send(senv.get_history(data))\n        elif cmd == \'recursive_getattr\':\n            remote.send(env.recursive_getattr(data))\n        elif cmd == \'decrement_starting_point\':\n            env.decrement_starting_point(data)\n        else:\n            raise NotImplementedError\n\n\nclass CloudpickleWrapper(object):\n    """"""\n    Uses cloudpickle to serialize contents (otherwise multiprocessing tries to use pickle)\n    """"""\n    def __init__(self, x):\n        self.x = x\n    def __getstate__(self):\n        import cloudpickle\n        return cloudpickle.dumps(self.x)\n    def __setstate__(self, ob):\n        import pickle\n        self.x = pickle.loads(ob)\n    \nclass SubprocVecEnv(gym.Wrapper):\n    def __init__(self, env_fns):\n        """"""\n        envs: list of gym environments to run in subprocesses\n        """"""\n        \n        super(SubprocVecEnv, self).__init__(env=env_fns)\n        nenvs = len(env_fns)\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(nenvs)])\n        self.ps = [Process(target=worker, args=(work_remote, CloudpickleWrapper(env_fn)))\n            for (work_remote, env_fn) in zip(self.work_remotes, env_fns)]\n        for p in self.ps:\n            p.start()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        self.action_space, self.observation_space = self.remotes[0].recv()\n\n    def step(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        results = [remote.recv() for remote in self.remotes]\n        obs, rews, dones, infos = zip(*results)\n\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def step_async(self, actions):\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        self.waiting = True\n\n    def step_wait(self):\n        results = [remote.recv() for remote in self.remotes]\n        self.waiting = False\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    def get_history(self, nsteps):\n        for remote in self.remotes:\n            remote.send((\'get_history\', nsteps))\n        results = [remote.recv() for remote in self.remotes]\n        obs, acts, dones = zip(*results)\n        obs = np.stack(obs)\n        acts = np.stack(acts)\n        dones = np.stack(dones)\n        return obs, acts, dones\n\n    def recursive_getattr(self, name):\n        for remote in self.remotes:\n            remote.send((\'recursive_getattr\',name))\n        return [remote.recv() for remote in self.remotes]\n\n    def decrement_starting_point(self, n):\n        for remote in self.remotes:\n            remote.send((\'decrement_starting_point\', n))\n\n    def close(self):\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n\n    @property\n    def num_envs(self):\n        return len(self.remotes)\n\n\nclass ReshapeObsEnv(gym.ObservationWrapper):\n    """"""\n    Reshape the gym environment observation,\n    and changes the input to grayscale.\n    """"""\n\n    def __init__(self, env=None, shape=(84, 84),\n                 channel_last=True):\n        super(ReshapeObsEnv, self).__init__(env)\n        self.env = env\n        self.obs_shape = shape\n        self.observation_space = Box(0.0, 255.0, shape)\n        self.ch_axis = -1 if channel_last else 0\n        self.scale = 1.0 / 255\n        self.observation_space.high[...] = 1.0\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return self._observation(obs), reward, done, info\n\n    def _observation(self, obs):\n        obs = self._convert(obs)\n        return obs.astype(np.float32) * self.scale\n\n    def _convert(self, obs):\n        small_frame = np.array(Image.fromarray(obs).resize(\n            self.obs_shape, resample=Image.BILINEAR), dtype=np.uint8)\n        return small_frame\n\n    def _rgb2y(self, im):\n        """"""Converts an RGB image to a Y image (as in YUV).\n        These coefficients are taken from the torch/image library.\n        Beware: these are more critical than you might think, as the\n        monochromatic contrast can be surprisingly low.\n        """"""\n        if len(im.shape) < 3:\n            return im\n        return np.sum(im * [0.299, 0.587, 0.114], axis=2)\n\n\nclass BufferedObsEnv(gym.ObservationWrapper):\n    """"""Buffer observations and stack e.g. for frame skipping.\n    n is the length of the buffer, and number of observations stacked.\n    skip is the number of steps between buffered observations (min=1).\n    n.b. first obs is the oldest, last obs is the newest.\n         the buffer is zeroed out on reset.\n         *must* call reset() for init!\n    """"""\n    def __init__(self, env=None, n=4, skip=4, shape=(84, 84),\n                    channel_last=True, maxFrames=True):\n        super(BufferedObsEnv, self).__init__(env)\n        self.obs_shape = shape\n        # most recent raw observations (for max pooling across time steps)\n        self.obs_buffer = deque(maxlen=2)\n        self.maxFrames = maxFrames\n        self.n = n\n        self.skip = skip\n        self.buffer = deque(maxlen=self.n)\n        self.counter = 0  # init and reset should agree on this\n        shape = shape + (n,) if channel_last else (n,) + shape\n        self.observation_space = Box(0.0, 255.0, shape)\n        self.ch_axis = -1 if channel_last else 0\n        self.scale = 1.0 / 255\n        self.observation_space.high[...] = 1.0\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        return self._observation(obs), reward, done, info\n\n    def _observation(self, obs):\n        obs = self._convert(obs)\n        self.counter += 1\n        if self.counter % self.skip == 0:\n            self.buffer.append(obs)\n        obsNew = np.stack(self.buffer, axis=self.ch_axis)\n        return obsNew.astype(np.float32) * self.scale\n\n    def _reset(self):\n        """"""Clear buffer and re-fill by duplicating the first observation.""""""\n        self.obs_buffer.clear()\n        obs = self._convert(self.env.reset())\n        self.buffer.clear()\n        self.counter = 0\n        for _ in range(self.n - 1):\n            self.buffer.append(np.zeros_like(obs))\n        self.buffer.append(obs)\n        obsNew = np.stack(self.buffer, axis=self.ch_axis)\n        return obsNew.astype(np.float32) * self.scale\n\n    def _convert(self, obs):\n        self.obs_buffer.append(obs)\n        if self.maxFrames:\n            max_frame = np.max(np.stack(self.obs_buffer), axis=0)\n        else:\n            max_frame = obs\n        intensity_frame = self._rgb2y(max_frame).astype(np.uint8)\n        small_frame = np.array(Image.fromarray(intensity_frame).resize(\n            self.obs_shape, resample=Image.BILINEAR), dtype=np.uint8)\n        return small_frame\n\n    def _rgb2y(self, im):\n        """"""Converts an RGB image to a Y image (as in YUV).\n        These coefficients are taken from the torch/image library.\n        Beware: these are more critical than you might think, as the\n        monochromatic contrast can be surprisingly low.\n        """"""\n        if len(im.shape) < 3:\n            return im\n        return np.sum(im * [0.299, 0.587, 0.114], axis=2)\n\n\nclass NoNegativeRewardEnv(gym.RewardWrapper):\n    """"""Clip reward in negative direction.""""""\n    def __init__(self, env=None, neg_clip=0.0):\n        super(NoNegativeRewardEnv, self).__init__(env)\n        self.neg_clip = neg_clip\n\n    def _reward(self, reward):\n        new_reward = self.neg_clip if reward < self.neg_clip else reward\n        return new_reward\n\n\nclass SkipEnv(gym.Wrapper):\n    """"""Skip timesteps: repeat action, accumulate reward, take last obs.""""""\n    def __init__(self, env=None, skip=4):\n        super(SkipEnv, self).__init__(env)\n        self.skip = skip\n\n    def _step(self, action):\n        total_reward = 0\n        for i in range(0, self.skip):\n            obs, reward, done, info = self.env.step(action)\n            total_reward += reward\n            info[\'steps\'] = i + 1\n            if done:\n                break\n        return obs, total_reward, done, info\n\nclass MarioEnv(gym.Wrapper):\n    """"""\n        Reset mario environment without actually restarting fceux everytime.\n        This speeds up unrolling by approximately 10 times.\n    """"""\n\n    def __init__(self, env=None, tilesEnv=None):\n        super(MarioEnv, self).__init__(env)\n        self.resetCount = -1\n        self.maxDistance = 3000\n        self.tilesEnv = tilesEnv\n\n    def _reset(self):\n        if self.resetCount < 0:\n            print(\'\\nDoing hard mario fceux reset (40 seconds wait) !\')\n            sys.stdout.flush()\n            self.env.reset()\n            time.sleep(40)\n        obs, _, _, info = self.env.step(7)  # take right once to start game\n        if info.get(\'ignore\', False):  # assuming this happens only in beginning\n            self.resetCount = -1\n            self.env.close()\n            return self._reset()\n        self.resetCount = info.get(\'iteration\', -1)\n        if self.tilesEnv:\n            return obs\n        return obs[24:-12, 8:-8, :]\n\n    def _step(self, action):\n        obs, reward, done, info = self.env.step(action)\n        # print(\'info:\', info)\n        done = info[\'iteration\'] > self.resetCount\n        reward = float(reward) / self.maxDistance  # note: we do not use this rewards at all.\n        if self.tilesEnv:\n            return obs, reward, done, info\n        return obs[24:-12, 8:-8, :], reward, done, info\n\n    def _close(self):\n        self.resetCount = -1\n        return self.env.close()\n\n\nclass WarpFrame(gym.ObservationWrapper):\n    def __init__(self, env, width, height, grid_env=False):\n        """"""Warp frames to wxh as done in the Nature paper and later work.""""""\n        gym.ObservationWrapper.__init__(self, env)\n        self.width = width\n        self.height = height\n        self.grid_env = grid_env\n        self.observation_space = spaces.Box(low=0, high=255,\n            shape=(self.height, self.width, 1), dtype=np.uint8)\n\n    def observation(self, frame):\n        if self.grid_env:\n            frame = frame[\'image\']\n        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n        return frame[:, :, None]\n\n\n\nclass FrameStack(gym.Wrapper):\n    def __init__(self, env, k):\n        """"""Stack k last frames.\n        Returns lazy array, which is much more memory efficient.\n        See Also\n        --------\n        baselines.common.atari_wrappers.LazyFrames\n        """"""\n        gym.Wrapper.__init__(self, env)\n        self.k = k\n        self.frames = deque([], maxlen=k)\n        shp = env.observation_space.shape\n        self.observation_space = spaces.Box(low=0, high=255, shape=(shp[0], shp[1], shp[2] * k), dtype=np.uint8)\n\n    def reset(self):\n        ob = self.env.reset()\n        for _ in range(self.k):\n            self.frames.append(ob)\n        return self._get_ob()\n\n    def step(self, action):\n        ob, reward, done, info = self.env.step(action)\n        self.frames.append(ob)\n        return self._get_ob(), reward, done, info\n\n    def _get_ob(self):\n        assert len(self.frames) == self.k\n        return LazyFrames(list(self.frames))\n\n    \nclass LazyFrames(object):\n    def __init__(self, frames):\n        """"""This object ensures that common frames between the observations are only stored once.\n        It exists purely to optimize memory usage which can be huge for DQN\'s 1M frames replay\n        buffers.\n        This object should only be converted to numpy array before being passed to the model.\n        You\'d not believe how complex the previous solution was.""""""\n        self._frames = frames\n        self._out = None\n\n    def _force(self):\n        if self._out is None:\n            self._out = np.concatenate(self._frames, axis=2)\n            self._frames = None\n        return self._out\n\n    def __array__(self, dtype=None):\n        out = self._force()\n        if dtype is not None:\n            out = out.astype(dtype)\n        return out\n\n    def __len__(self):\n        return len(self._force())\n\n    def __getitem__(self, i):\n        return self._force()[i]\n\n\nclass ImageToPyTorch(gym.ObservationWrapper):\n    """"""\n    Image shape to num_channels x weight x height\n    """"""\n\n    def __init__(self, env):\n        super(ImageToPyTorch, self).__init__(env)\n        old_shape = self.observation_space.shape\n        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]),\n                                                dtype=np.uint8)\n\n    def observation(self, observation):\n        return np.swapaxes(observation, 2, 0)\n\n\ndef wrap_pytorch(env):\n    return ImageToPyTorch(env)\n\n\ndef warp_wrap(env, height, width, max_skip=False, grid_env=False):\n    if max_skip:\n        env = MaxAndSkipEnv(env, grid_env=grid_env)\n    env = WarpFrame(env, height=height, width=width, grid_env=grid_env)\n    env = FrameStack(env, 4)\n    return env\n\n\n'"
Layers/LayerNorm.py,3,"b'import torch\nimport torch.nn as nn\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.gamma = nn.Parameter(torch.ones(features))\n        self.beta = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True)\n        std = x.std(-1, keepdim=True)\n        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n'"
Layers/NoisyLinearNet.py,2,"b'""""""\n\nThis script contains the implementation of the NoisyNets paper.\nThis is a Noisy Linear Net which aids exploration by learning perturbations\nto the weights of the individual layers.\n\n""""""\n\nimport torch\nimport torch.nn as nn\n\nUSE_CUDA = torch.cuda.is_available()\n\nclass NoisyLinearLayer(nn.Module):\n\n    def __init__(self):\n        super(NoisyLinearLayer, self).__init__()\n        '"
Layers/Spectral_norm.py,5,"b'""""""\n\nImplementation of the spectral normalization for the weight matrix\n\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter\n\n\nclass SpectralNorm(nn.Module):\n\n    def __init__(self, module, name_module=\'weight\', power_iterations=1):\n        super(SpectralNorm, self).__init__()\n        self.module= module\n        self.name = name_module\n        self.power_iter = power_iterations\n\n        if not self._made_params():\n            self._make_params()\n\n\n    def update_u_v(self):\n        """"""\n        Updating the parameters u and v which would be used for the\n        calculation of the spectral norm.\n        :return:\n        """"""\n\n        u = getattr(self.module, self.name + ""_u"")\n        v = getattr(self.module, self.name + ""_v"")\n        w = getattr(self.module, self.name + ""_bar"")\n\n        height = w.data.shape[0]\n        for _ in range(self.power_iterations):\n            v.data = self.l2normalize(torch.mv(torch.t(w.view(height, -1).data), u.data))\n            u.data = self.l2normalize(torch.mv(w.view(height, -1).data, v.data))\n\n        # sigma = torch.dot(u.data, torch.mv(w.view(height,-1).data, v.data))\n        sigma = u.dot(w.view(height, -1).mv(v))\n\n        setattr(self.module, self.name, w / sigma.expand_as(w))\n\n\n    def l2normalize(self, v, eps=1e-12):\n        return v / (v.norm() + eps)\n\n    def _made_params(self):\n        try:\n            u = getattr(self.module, self.name + ""_u"")\n            v = getattr(self.module, self.name + ""_v"")\n            w = getattr(self.module, self.name + ""_bar"")\n            return True\n        except AttributeError:\n            return False\n\n    def _make_params(self):\n        w = getattr(self.module, self.name)\n\n        height = w.data.shape[0]\n        width = w.view(height, -1).data.shape[1]\n\n        u = Parameter(w.data.new(height).normal_(0, 1), requires_grad=False)\n        v = Parameter(w.data.new(width).normal_(0, 1), requires_grad=False)\n        u.data = self.l2normalize(u.data)\n        v.data = self.l2normalize(v.data)\n        w_bar = Parameter(w.data)\n\n        del self.module._parameters[self.name]\n        self.module.register_parameter(self.name + ""_u"", u)\n        self.module.register_parameter(self.name + ""_v"", v)\n        self.module.register_parameter(self.name + ""_bar"", w_bar)\n\n    def forward(self, *args):\n        self.update_u_v()\n        return self.module.forward(*args)'"
Layers/self_attention.py,5,"b'""""""\n\nThis script contains the implementation for the self attention module\n, first introduced in the Attention is all you need, Vaswani et al.\n\nThis self attention module is now being used in Convolutional GANs\n(Self attention GAN) to account for global dependencies in images.\n\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass Self_Attn(nn.Module):\n    """""" Self attention Layer""""""\n\n    def __init__(self, in_dim, activation):\n        super(Self_Attn, self).__init__()\n        self.chanel_in = in_dim\n        self.activation = activation\n\n        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim // 8, kernel_size=1)\n        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n        # Tunable parameter (Start with zero to make training easier iniitially)\n        self.gamma = nn.Parameter(torch.zeros(1))\n\n        self.softmax = nn.Softmax(dim=-1)  #\n\n    def forward(self, x):\n        """"""\n            inputs :\n                x : input feature maps( B X C X W X H)\n            returns :\n                out : self attention value + input feature\n                attention: B X N X N (N is Width*Height)\n        """"""\n        m_batchsize, C, width, height = x.size()\n        # The projected query vector\n        proj_query = self.query_conv(x).view(m_batchsize, -1, width * height).permute(0, 2, 1)  # B X CX(N)\n        # The projected key vector\n        proj_key = self.key_conv(x).view(m_batchsize, -1, width * height)  # B X C x (*W*H)\n        energy = torch.bmm(proj_query, proj_key)  # transpose check\n        attention = self.softmax(energy)  # BX (N) X (N)\n        proj_value = self.value_conv(x).view(m_batchsize, -1, width * height)  # B X C X N\n\n        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n        out = out.view(m_batchsize, C, width, height)\n\n        out = self.gamma * out + x\n\n        return out, attention'"
Memory/Buffer.py,1,"b'from collections import  namedtuple, deque\nfrom torch.utils.data import Dataset\nimport random\nimport numpy as np\n\nTransition = namedtuple(\'Transition\',\n                        (\'state\', \'action\', \'next_state\', \'reward\', \'done\'))\n\n\n# Segment tree data structure where parent node values are sum/max of children node values\nclass SegmentTree():\n  def __init__(self, size):\n    self.index = 0\n    self.size = size\n    self.full = False  # Used to track actual capacity\n    self.sum_tree = [0] * (2 * size - 1)  # Initialise fixed size tree with all (priority) zeros\n    self.data = [None] * size  # Wrap-around cyclic buffer\n    self.max = 1  # Initial max value to return (1 = 1^\xcf\x89)\n\n  # Propagates value up tree given a tree index\n  def _propagate(self, index, value):\n    parent = (index - 1) // 2\n    left, right = 2 * parent + 1, 2 * parent + 2\n    self.sum_tree[parent] = self.sum_tree[left] + self.sum_tree[right]\n    if parent != 0:\n      self._propagate(parent, value)\n\n  # Updates value given a tree index\n  def update(self, index, value):\n    self.sum_tree[index] = value  # Set new value\n    self._propagate(index, value)  # Propagate value\n    self.max = max(value, self.max)\n\n  def append(self, data, value):\n    self.data[self.index] = data  # Store data in underlying data structure\n    self.update(self.index + self.size - 1, value)  # Update tree\n    self.index = (self.index + 1) % self.size  # Update index\n    self.full = self.full or self.index == 0  # Save when capacity reached\n    self.max = max(value, self.max)\n\n  # Searches for the location of a value in sum tree\n  def _retrieve(self, index, value):\n    left, right = 2 * index + 1, 2 * index + 2\n    if left >= len(self.sum_tree):\n      return index\n    elif value <= self.sum_tree[left]:\n      return self._retrieve(left, value)\n    else:\n      return self._retrieve(right, value - self.sum_tree[left])\n\n  # Searches for a value in sum tree and returns value, data index and tree index\n  def find(self, value):\n    index = self._retrieve(0, value)  # Search for index of item from root\n    data_index = index - self.size + 1\n    return (self.sum_tree[index], data_index, index)  # Return value, data index, tree index\n\n  # Returns data given a data index\n  def get(self, data_index):\n    return self.data[data_index % self.size]\n\n  def total(self):\n    return self.sum_tree[0]\n\n\nclass ReplayBufferDataset(Dataset):\n    """"""\n    Dataset implementation of the experience replay\n    This class helps in the case of using multi gpu training\n    """"""\n\n    def __init__(self, capacity):\n        self.capacity = capacity\n        self.memory = []\n        self.position = 0\n\n    def push(self, *args):\n        """"""Saves a transition.""""""\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample_batch(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n    def get_buffer_size(self):\n        return len(self.memory)\n\n    def __getitem__(self, index):\n        return self.memory[index]\n\n\nclass ReplayBuffer(object):\n\n    def __init__(self, capacity, seed,\n                 priority_weight=None, priority_exponent=None,\n                 priotirized_experience=False):\n        self.capacity = capacity\n        self.position = 0\n        self.prioritize = priotirized_experience\n        self.priority_weight = priority_weight  # Initial importance sampling weight \xce\xb2, annealed to 1 over course of training\n        self.priority_exponent = priority_exponent\n        if self.prioritize:\n            self.memory = []\n        else:\n            self.memory = []\n        # Seed for reproducible results\n        np.random.seed(seed)\n\n    def push(self, *args):\n        """"""Saves a transition.""""""\n        if len(self.memory) < self.capacity:\n            self.memory.append(None)\n        self.memory[self.position] = Transition(*args)\n        self.position = (self.position + 1) % self.capacity\n\n    def sample_batch(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)\n\n    def get_buffer_size(self):\n        return len(self.memory)\n\n\n# Use this replay buffer for non goal environments\nclass ReplayBufferDeque(object):\n\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n\n    def push(self, state, action, reward, next_state, done):\n        state = np.expand_dims(state, 0)\n        next_state = np.expand_dims(next_state, 0)\n\n        self.buffer.append((state, action, reward, next_state, done))\n\n    def sample(self, batch_size):\n        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n        return np.concatenate(state), action, reward, np.concatenate(next_state), done\n\n    def __len__(self):\n        return len(self.buffer)\n\n\n# Long term memory that uses resevoir sampling for adding items\nclass SelectiveExperienceReplayBuffer(object):\n    pass\n\n\n\n\n\n\n\n'"
Memory/data_structures.py,0,"b'""""""\nThis file contains all the necessary data structures used for training\n\nFor example, Priority Queue to be used for Prioritized Experience Replay and Selective Experience Replay\n""""""\n\n\nclass PriorityQueue(object):\n    """"""\n    Implementation of priority queue using a max binary heap\n    Fibonacci Heap (Coming Soon)\n    """"""\n    def __init__(self):\n        pass'"
Memory/rollout_storage.py,10,"b'import torch\nfrom torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass RolloutStorage(object):\n    def __init__(self, num_steps, num_processes, obs_shape,\n                 action_space, use_cuda, action_shape):\n        """"""\n        A storage class for storing the episode rollouts across various environments\n        :param num_steps: Steps into the environment\n        :param num_processes: Parallel workers collecting the experiences\n        :param obs_shape: Shape of the observation\n        :param action_space: Action Shape\n        :param state_size: Shape of the state (Maybe similar to the observation)\n        :param use_cuda: Use GPU\n        """"""\n        self.observations = torch.zeros(num_steps + 1, num_processes, *obs_shape)\n        # Rewards given by the environment - Extrinsic Rewards\n        self.rewards = torch.zeros(num_steps, num_processes, 1)\n        # Rewards generated by the intrinsic curiosity module\n        self.intrinsic_rewards = torch.zeros(num_steps, num_processes, 1)\n        self.value_preds = torch.zeros(num_steps + 1, num_processes, 1)\n        # Cumulative returns (calculated using the rewards and the value predictions)\n        self.returns = torch.zeros(num_steps + 1, num_processes, 1)\n        # Log probabilities of the actions by the previous policy\n        self.action_log_probs = torch.zeros(num_steps, num_processes, 1)\n\n        self.num_steps = num_steps\n        self.num_processes = num_processes\n        self.obs_shape = obs_shape\n        self.action_space = action_space\n        self.action_shape = action_shape\n        self.use_cuda = use_cuda\n\n        action_shape = self.action_shape\n\n        self.actions = torch.zeros(num_steps, num_processes, action_shape)\n\n        self.actions = self.actions.long()\n        self.masks = torch.ones(num_steps + 1, num_processes, 1)\n        self.use_cuda = use_cuda\n        if self.use_cuda:\n            self.cuda()\n\n    def cuda(self):\n        self.observations = self.observations.cuda()\n        self.rewards = self.rewards.cuda()\n        self.intrinsic_rewards = self.intrinsic_rewards.cuda()\n        self.value_preds = self.value_preds.cuda()\n        self.returns = self.returns.cuda()\n        self.action_log_probs = self.action_log_probs.cuda()\n        self.actions = self.actions.cuda()\n        self.masks = self.masks.cuda()\n\n    def insert(self, step,\n               current_obs, action, action_log_prob,\n               value_pred, reward, intrinsic_reward):\n        """"""\n\n        :param step:\n        :param current_obs:\n        :param state:\n        :param action:\n        :param action_log_prob:\n        :param value_pred:\n        :param reward:\n        :param mask:\n        :return:\n        """"""\n        self.observations[step + 1].copy_(current_obs)\n        self.actions[step].copy_(action)\n        self.action_log_probs[step].copy_(action_log_prob)\n        self.value_preds[step].copy_(value_pred)\n        self.rewards[step].copy_(reward)\n        #self.masks[step + 1].copy_(mask)\n        self.intrinsic_rewards[step].copy_(intrinsic_reward)\n\n    def after_update(self):\n        self.observations[0].copy_(self.observations[-1])\n        self.masks[0].copy_(self.masks[-1])\n\n    def compute_returns(self, next_value, use_gae, gamma, tau):\n        """"""\n        This function is being used to compute the true state values using a bootstrapped\n        estimate and backtracking.\n\n        :param next_value:\n        :param use_gae: Use generalized advantage estimation\n        :param gamma: Discount factor\n        :param tau:\n        :return:\n        """"""\n        # Returns defines the possible sum of rewards/returns from a given state\n\n        if use_gae:\n            self.value_preds[-1] = next_value\n            # Initialize the GAE to 0\n            gae = 0\n            # Starting from the back\n            for step in reversed(range(self.rewards.size(0))):\n                # Delta = Reward + discount*Value_next_step - Value_current_step\n                delta = (self.rewards[step]+self.intrinsic_rewards[step]) + \\\n                        gamma*self.value_preds[step+1] - self.value_preds[step]\n                # Advantage = delta + gamma*tau*previous_advantage\n                gae = delta + gamma*tau*gae\n                # Final return = gae + value\n                self.returns[step] = gae + self.value_preds[step]\n        else:\n            # Initialize the returns vector with the next predicted value of the state\n            # (Value of the last state of the rollout)\n            self.returns[-1] = next_value\n            for step in reversed(range(self.rewards.size(0))):\n                # Returns at current step = gamma*Returns at next step + rewards_at_current_step\n                self.returns[step] = self.returns[step + 1] * \\\n                    gamma * self.masks[step + 1] + (self.rewards[step]+self.intrinsic_rewards[step])\n\n    def feed_forward_generator(self, advantages, mini_batches):\n        num_steps, num_processes = self.rewards.size()[0:2]\n        total_batch_size = num_processes*num_steps\n        assert total_batch_size >= mini_batches, (\n            f""PPO requires the number processes ({num_processes}) ""\n            f""* number of steps ({num_steps}) = {num_processes * num_steps} ""\n            f""to be greater than or equal to the number of PPO mini batches ({mini_batches})."")\n\n        mini_batch_size = total_batch_size // mini_batches\n        # Create a random batch sampler\n        sampler = BatchSampler(SubsetRandomSampler(range(total_batch_size)), mini_batch_size, drop_last=False)\n        for i in sampler:\n            observations_batch = self.observations[:-1].view(-1,\n                                                             *self.observations.size()[2:])[i]\n            actions_batch = self.actions.view(-1, self.actions.size(-1))[i]\n            return_batch = self.returns[:-1].view(-1, 1)[i]\n            old_action_log_probs_batch = self.action_log_probs.view(-1, 1)[i]\n            adv_targ = advantages.view(-1, 1)[i]\n\n            yield observations_batch, actions_batch, \\\n                  return_batch, old_action_log_probs_batch, adv_targ\n\n\n\n'"
Training/trainer.py,23,"b'""""""\nClass for a generic trainer used for training all the different reinforcement learning models\n""""""\nimport torch\nimport torch.nn as nn\nfrom Utils.utils import *\nfrom collections import deque, defaultdict\nfrom models.attention import *\nimport time\nimport numpy as np\nimport random\nimport torch.optim as optim\nfrom torch.autograd import Variable\n\n\nclass Trainer(object):\n\n    def __init__(self, agent, num_epochs,\n                 num_rollouts, num_eval_rollouts, env, eval_env, nb_train_steps,\n                 max_episodes_per_epoch, random_seed,\n                 output_folder=None,\n                 her_training=False,\n                 multi_gpu_training=False,\n                 use_cuda=True, verbose=True,\n                 save_model=False, plot_stats=True, future=None):\n\n        """"""\n\n        :param ddpg: The ddpg network\n        :param num_rollouts: number of experience gathering rollouts per episode\n        :param num_eval_rollouts: number of evaluation rollouts\n        :param num_episodes: number of episodes per epoch\n        :param env: Gym environment to train on\n        :param eval_env: Gym environment to evaluate on\n        :param nb_train_steps: training steps to take\n        :param max_episodes_per_epoch: maximum number of episodes per epoch\n        :param her_training: use hindsight experience replay\n        :param multi_gpu_training: train on multiple gpus\n        """"""\n\n        self.ddpg = agent\n        self.num_epochs = num_epochs\n        self.num_rollouts = num_rollouts\n        self.num_eval_rollouts = num_eval_rollouts\n        self.env = env\n        self.eval_env = eval_env\n        self.nb_train_steps = nb_train_steps\n        self.max_episodes = max_episodes_per_epoch\n        self.seed(random_seed)\n        self.her = her_training\n        self.multi_gpu = multi_gpu_training\n        self.cuda = use_cuda\n        self.verbose = verbose\n        self.plot_stats = plot_stats\n        self.save_model = save_model\n        self.output_folder = output_folder\n        self.future = future\n\n        self.all_rewards = []\n        self.successes = []\n\n        # Get the target  and standard networks\n        self.target_actor = self.ddpg.get_actors()[\'target\']\n        self.actor = self.ddpg.get_actors()[\'actor\']\n        self.target_critic  = self.ddpg.get_critics()[\'target\']\n        self.critic = self.ddpg.get_critics()[\'critic\']\n        self.statistics = defaultdict(float)\n        self.combined_statistics = defaultdict(list)\n\n        if self.multi_gpu:\n            if torch.cuda.device_count() > 1:\n                print(""Training on "", torch.cuda.device_count() , "" GPUs "")\n                self.target_critic = nn.DataParallel(self.target_critic)\n                self.critic = nn.DataParallel(self.critic)\n                self.target_actor = nn.DataParallel(self.target_actor)\n                self.actor = nn.DataParallel(self.actor)\n            else:\n                print(""Only 1 gpu available for training ....."")\n\n    def train_on_policy(self):\n        pass\n\n    def train(self):\n\n        # Starting time\n        start_time = time.time()\n\n        # Initialize the statistics dictionary\n        statistics = self.statistics\n\n        episode_rewards_history = deque(maxlen=100)\n        eval_episode_rewards_history = deque(maxlen=100)\n        episode_success_history = deque(maxlen=100)\n        eval_episode_success_history = deque(maxlen=100)\n\n        epoch_episode_rewards = []\n        epoch_episode_success = []\n        epoch_episode_steps = []\n\n        # Epoch Rewards and success\n        epoch_rewards = []\n        epoch_success = []\n\n        # Initialize the training with an initial state\n        state = self.env.reset()\n        # If eval, initialize the evaluation with an initial state\n        if self.eval_env is not None:\n            eval_state = self.eval_env.reset()\n            eval_state = to_tensor(eval_state, use_cuda=self.cuda)\n            eval_state = torch.unsqueeze(eval_state, dim=0)\n\n        # Initialize the losses\n        loss = 0\n        episode_reward =  0\n        episode_success = 0\n        episode_step = 0\n        epoch_actions = []\n        t = 0\n\n        # Check whether to use cuda or not\n        state = to_tensor(state, use_cuda=self.cuda)\n        state = torch.unsqueeze(state, dim=0)\n\n        # Main training loop\n        for epoch in range(self.num_epochs):\n            epoch_actor_losses = []\n            epoch_critic_losses = []\n            for episode in range(self.max_episodes):\n\n                # Rollout of trajectory to fill the replay buffer before training\n                for rollout in range(self.num_rollouts):\n                    # Sample an action from behavioural policy pi\n                    action = self.ddpg.get_action(state=state, noise=True)\n                    assert action.shape == self.env.get_action_shape\n\n                    # Execute next action\n                    new_state, reward, done, success = self.env.step(action)\n                    success = success[\'is_success\']\n                    done_bool = done * 1\n\n                    t+=1\n                    episode_reward += reward\n                    episode_step += 1\n                    episode_success += success\n\n                    # Book keeping\n                    epoch_actions.append(action)\n                    # Store the transition in the replay buffer of the agent\n                    self.ddpg.store_transition(state=state, new_state=new_state,\n                                               action=action, done=done_bool, reward=reward,\n                                               success=success)\n                    # Set the current state as the next state\n                    state = to_tensor(new_state, use_cuda=self.cuda)\n                    state = torch.unsqueeze(state, dim=0)\n\n                    # End of the episode\n                    if done:\n                        epoch_episode_rewards.append(episode_reward)\n                        episode_rewards_history.append(episode_reward)\n                        episode_success_history.append(episode_success)\n                        epoch_episode_success.append(episode_success)\n                        epoch_episode_steps.append(episode_step)\n                        episode_reward = 0\n                        episode_step = 0\n                        episode_success = 0\n\n                        # Reset the agent\n                        self.ddpg.reset()\n                        # Get a new initial state to start from\n                        state = self.env.reset()\n                        state = to_tensor(state, use_cuda=self.cuda)\n\n                # Train\n                for train_steps in range(self.nb_train_steps):\n                    critic_loss, actor_loss = self.ddpg.fit_batch()\n                    if critic_loss is not None and actor_loss is not None:\n                        epoch_critic_losses.append(critic_loss)\n                        epoch_actor_losses.append(actor_loss)\n\n                    # Update the target networks using polyak averaging\n                    self.ddpg.update_target_networks()\n\n                eval_episode_rewards = []\n                eval_episode_successes = []\n                if self.eval_env is not None:\n                    eval_episode_reward = 0\n                    eval_episode_success = 0\n                    for t_rollout in range(self.num_eval_rollouts):\n                        if eval_state is not None:\n                            eval_action = self.ddpg.get_action(state=eval_state, noise=False)\n                        eval_new_state, eval_reward, eval_done, eval_success = self.eval_env.step(eval_action)\n                        eval_episode_reward += eval_reward\n                        eval_episode_success += eval_success\n\n                        if eval_done:\n                            eval_state = self.eval_env.reset()\n                            eval_state = to_tensor(eval_state, use_cuda=self.cuda)\n                            eval_state = torch.unsqueeze(eval_state, dim=0)\n                            eval_episode_rewards.append(eval_episode_reward)\n                            eval_episode_rewards_history.append(eval_episode_reward)\n                            eval_episode_successes.append(eval_episode_success)\n                            eval_episode_success_history.append(eval_episode_success)\n                            eval_episode_reward = 0\n                            eval_episode_success = 0\n\n            # Log stats\n            duration = time.time() - start_time\n            statistics[\'rollout/rewards\'] = np.mean(epoch_episode_rewards)\n            statistics[\'rollout/rewards_history\'] = np.mean(episode_rewards_history)\n            statistics[\'rollout/successes\'] = np.mean(epoch_episode_success)\n            statistics[\'rollout/successes_history\'] = np.mean(episode_success_history)\n            statistics[\'rollout/actions_mean\'] = np.mean(epoch_actions)\n            statistics[\'train/loss_actor\'] = np.mean(epoch_actor_losses)\n            statistics[\'train/loss_critic\'] = np.mean(epoch_critic_losses)\n            statistics[\'total/duration\'] = duration\n\n            # Evaluation statistics\n            if self.eval_env is not None:\n                statistics[\'eval/rewards\'] = np.mean(eval_episode_rewards)\n                statistics[\'eval/rewards_history\'] = np.mean(eval_episode_rewards_history)\n                statistics[\'eval/successes\'] = np.mean(eval_episode_successes)\n                statistics[\'eval/success_history\'] = np.mean(eval_episode_success_history)\n\n            # Print the statistics\n            if self.verbose:\n                if epoch % 5 == 0:\n                    print(""Actor Loss: "", statistics[\'train/loss_actor\'])\n                    print(""Critic Loss: "", statistics[\'train/loss_critic\'])\n                    print(""Reward "", statistics[\'rollout/rewards\'])\n                    print(""Successes "", statistics[\'rollout/successes\'])\n\n                    if self.eval_env is not None:\n                        print(""Evaluation Reward "", statistics[\'eval/rewards\'])\n                        print(""Evaluation Successes "", statistics[\'eval/successes\'])\n\n            # Log the combined statistics for all epochs\n            for key in sorted(statistics.keys()):\n                self.combined_statistics[key].append(statistics[key])\n\n            # Log the epoch rewards and successes\n            epoch_rewards.append(np.mean(epoch_episode_rewards))\n            epoch_success.append(np.mean(epoch_episode_success))\n\n        # Plot the statistics calculated\n        if self.plot_stats:\n            # Plot the rewards and successes\n            rewards_fname = self.output_folder + \'/rewards.jpg\'\n            success_fname = self.output_folder + \'/success.jpg\'\n            plot(epoch_rewards, f_name=rewards_fname, save_fig=True, show_fig=False)\n            plot(epoch_success, f_name=success_fname, save_fig=True, show_fig=False)\n\n        # Save the models on the disk\n        if self.save_model:\n            self.ddpg.save_model(self.output_folder)\n\n        return self.combined_statistics\n\n    def seed(self, s):\n        # Seed everything to make things reproducible\n        self.env.seed(s)\n        np.random.seed(seed=s)\n        random.seed = s\n        if self.eval_env is not None:\n            self.eval_env.seed(s)\n\n    def get_frames(self, transition, sample_experience, k):\n        """"""\n\n        :param transition: Current transition -> Goal substitution\n        :param sample_experience: The Future episode experiences\n        :param k: The number of transitions to consider\n        :return:\n        """"""\n        # Get the frames predicted by our self attention network\n        seq_length = len(sample_experience)\n        states = []\n        new_states= []\n        rewards = []\n        successes = []\n        actions = []\n        dones = []\n        for t in sample_experience:\n            state, new_state, reward, success, action, done_bool = t\n            state = np.concatenate(state[:self.ddpg.obs_dim], state[\'achieved_goal\'])\n            new_state = np.concatenate(new_state[:self.ddpg.obs_dim], new_state[\'achieved_goal\'])\n            states.append(state)\n            new_states.append(new_state[:self.ddpg.obs_dim])\n            rewards.append(reward)\n            successes.append(success)\n            actions.append(action)\n            dones.append(done_bool)\n\n        # Input Sequence consists of n embeddings of states||achieved_goals\n        input_sequence = Variable(torch.cat(states))\n        # The Query vector is the current state || desired goal\n        state, new_state, reward, success, action, done_bool = transition\n        query = Variable(state)\n\n        # The Goal Network\n        gn = GoalNetwork(input_dim=seq_length, embedding_dim=self.ddpg.input_dim,\n                         query_dim=self.ddpg.input_dim, num_hidden=self.ddpg.num_hidden_units,\n                         output_features=1, use_additive=True, use_self_attn=True, use_token2token=True,\n                         activation=nn.ReLU)\n\n        if self.cuda:\n            input_sequence = input_sequence.cuda()\n            query = query.cuda()\n            gn = gn.cuda()\n\n        scores = gn(input_sequence, query)\n        optimizer_gn = optim.Adam(gn.parameters(), lr=self.ddpg.actor_lr)\n        optimizer_gn.zero_grad()\n        # Dimension of the scores vector is 1 x n\n        # Find the top 5 maximum values from the scores vector and their indexes\n        values, indices = torch.topk(scores, k, largest=True)\n        # Now we have the indices -> Get the corresponding experiences\n        top_experiences = []\n        for m in indices:\n            top_experiences.append(sample_experience[m])\n\n        # Training Step\n        TD_error = 0\n        for t in top_experiences:\n            TD_error += self.ddpg.calc_td_error(t)\n        loss = -1 * (TD_error.mean())\n        loss.backward()\n        # Clamp the gradients to avoid the vanishing gradient problem\n        for param in gn.parameters():\n            param.grad.data.clamp_(-1, 1)\n        optimizer_gn.step()\n\n        return top_experiences\n\n    def sample_goals(self,sampling_strategy, experience, future=None, transition=None):\n        g = []\n        if sampling_strategy == \'final\':\n            n_s = experience[len(experience)-1]\n            g.append(n_s[\'achieved_goal\'])\n\n        elif sampling_strategy == \'self_attention\':\n            if transition is not None:\n                index_of_t = experience.index(transition)\n                sample_experience = experience[index_of_t:]\n                if future is None:\n                    future = 5\n                frames = self.get_frames(transition, sample_experience, k=future)\n                for f in frames:\n                    g.append(f[\'achieved_goal\'])\n\n        elif sampling_strategy == \'future\':\n            if transition is not None and future is not None:\n                index_of_t = transition\n                if index_of_t < len(experience)-2:\n                    sample_experience = experience[index_of_t+1:]\n                    random_transitions = random.sample(population=sample_experience,\n                                              k=future)\n                    for f in random_transitions:\n                        observation, new_observation, state, new_state, reward, success, action, done_bool, achieved_goal, desired_goal = f\n                        g.append(achieved_goal)\n\n        elif sampling_strategy == \'prioritized\':\n            pass\n\n        return g\n\n    def her_training(self):\n\n        # Starting Time\n        start_time = time.time()\n\n        # Initialize the statistics dictionary\n        statistics = self.statistics\n\n        episode_rewards_history = deque(maxlen=100)\n        episode_revised_rewards_history  =  deque(maxlen=100)\n        eval_episode_rewards_history = deque(maxlen=100)\n        episode_success_history = deque(maxlen=100)\n        eval_episode_success_history = deque(maxlen=100)\n        episode_goals_history = deque(maxlen=100)\n        eval_episode_goals_history = deque(maxlen=100)\n        all_goals_history = deque(maxlen=100)\n\n        epoch_episode_rewards = []\n        epoch_episode_success = []\n        epoch_episode_steps = []\n\n\n        episode_states_history = deque(maxlen=100)\n        episode_new_states_history = deque(maxlen=100)\n\n        # Rewards and success for each epoch\n        epoch_rewards = []\n        epoch_success = []\n\n        # Sample a goal g and an initial state s0\n        state = self.env.reset() # The state space includes the observation, achieved_goal and the desired_goal\n        observation = state[\'observation\']\n        achieved_goal = state[\'achieved_goal\']\n        desired_goal = state[\'desired_goal\']\n        state = np.concatenate((observation, desired_goal))\n\n        # If eval, initialize the evaluation with an initial state\n        if self.eval_env is not None:\n            eval_state = self.eval_env.reset()\n            eval_observation = eval_state[\'observation\']\n            eval_achieved_goal = eval_state[\'achieved_goal\']\n            eval_desired_goal = eval_state[\'desired_goal\']\n            eval_state = np.concatenate((eval_observation, eval_desired_goal))\n            eval_state = to_tensor(eval_state, use_cuda=self.cuda)\n            eval_state = torch.unsqueeze(eval_state, dim=0)\n\n        # Initialize the losses\n        loss = 0\n        episode_reward = 0\n        episode_success = 0\n        episode_step = 0\n        epoch_actions = []\n        t = 0\n\n        # Check whether to use cuda or not\n        state = to_tensor(state, use_cuda=self.cuda)\n        state = torch.unsqueeze(state, dim=0)\n\n        for epoch in range(self.num_epochs):\n            epoch_actor_losses = []\n            epoch_critic_losses = []\n\n            for cycle in range(self.max_episodes):\n\n                # States and new states for the hindsight experience replay\n                episode_states = []\n                episode_achieved_goals = []\n                episode_desired_goals = []\n                episode_new_states = []\n                episode_rewards = []\n                episode_successes = []\n                episode_actions = []\n                episode_dones = []\n                episode_experience = []\n                episode_observations = []\n                episode_new_observations = []\n\n                # Rollout of trajectory to fill the replay buffer before the training\n                for rollout in range(self.num_rollouts):\n                    # Sample an action from behavioural policy pi\n                    action = self.ddpg.get_action(state=state, noise=True)\n                    #assert action.shape == self.env.get_action_shape\n\n                    # Execute the action and observe the new state\n                    new_state, reward, done, success = self.env.step(action)\n\n                    # The following has to hold\n                    assert reward == self.env.compute_reward(\n                        new_state[\'achieved_goal\'], new_state[\'desired_goal\'],\n                        info=success\n                    )\n\n                    new_observation = new_state[\'observation\']\n                    new_achieved_goal = new_state[\'achieved_goal\']\n                    new_desired_goal = new_state[\'desired_goal\']\n                    new_state = np.concatenate((new_observation, new_desired_goal))\n                    new_state = to_tensor(new_state, self.cuda)\n                    #new_state = torch.unsqueeze(new_state, dim=0)\n                    success = success[\'is_success\']\n                    done_bool = done * 1\n\n                    episode_states.append(state)\n                    episode_new_states.append(new_state)\n                    episode_rewards.append(reward)\n                    episode_successes.append(success)\n                    episode_actions.append(action)\n                    episode_dones.append(done_bool)\n                    episode_achieved_goals.append(new_achieved_goal)\n                    episode_desired_goals.append(new_desired_goal)\n                    episode_observations.append(observation)\n                    episode_new_observations.append(new_observation)\n                    episode_experience.append(\n                        (observation, new_observation, state, new_state, reward, success, action, done_bool, new_achieved_goal, desired_goal)\n                    )\n\n                    t += 1\n                    episode_reward += reward\n                    episode_step += 1\n                    episode_success += success\n\n                    # Set the current state as the next state\n                    state = to_tensor(new_state, use_cuda=self.cuda)\n                    state = torch.unsqueeze(state, dim=0)\n                    observation = new_observation\n\n                    # End of the episode\n                    if done:\n                        # Get the episode goal\n                        #episode_goal = new_state[:self.ddpg.obs_dim]\n                        #episode_goals_history.append(episode_goal)\n                        epoch_episode_rewards.append(episode_reward)\n                        episode_goals_history.append(achieved_goal)\n                        episode_rewards_history.append(episode_reward)\n                        episode_success_history.append(episode_success)\n                        epoch_episode_success.append(episode_success)\n                        epoch_episode_steps.append(episode_step)\n                        episode_reward = 0\n                        episode_step = 0\n                        episode_success = 0\n\n                        # Reset the agent\n                        self.ddpg.reset()\n                        # Get a new initial state to start from\n                        state = self.env.reset()\n                        observation = state[\'observation\']\n                        achieved_goal = state[\'achieved_goal\']\n                        desired_goal = state[\'desired_goal\']\n                        state = np.concatenate((observation, desired_goal))\n                        state = to_tensor(state, use_cuda=self.cuda)\n                        state = torch.unsqueeze(state, dim=0)\n\n                # Standard Experience Replay\n                i = 0\n                for tr in episode_experience:\n                    observation, new_observation, state, new_state, reward, success, action, done_bool, achieved_goal, desired_goal = tr\n                    new_state = torch.unsqueeze(new_state, dim=0)\n                    action = to_tensor(action, use_cuda=self.cuda)\n                    action = torch.unsqueeze(action, dim=0)\n                    reward = to_tensor([np.asscalar(reward)], use_cuda=self.cuda)\n                    done_bool = to_tensor([done_bool], use_cuda=self.cuda)\n                    #success = to_tensor([np.asscalar(success)], use_cuda=self.cuda)\n\n                    # Store the transition in the experience replay\n                    self.ddpg.store_transition(\n                        state=state, new_state=new_state, reward=reward,\n                        success=success, action=action, done=done_bool\n                    )\n\n                    # Hindsight Experience Replay\n                    # Sample a set of additional goals for replay G: S\n                    additional_goals = self.sample_goals(sampling_strategy=\'future\',\n                                                         experience=episode_experience,\n                                                         future=self.future, transition=i)\n\n                    for g in additional_goals:\n                        # Recalculate the reward\n                        substitute_goal = g\n\n                        # Recalculate the reward now when the desired goal is the substituted goal\n                        # which is the achieved goal sampled using the sampling strategy\n                        reward_revised = self.env.compute_reward(\n                            achieved_goal, substitute_goal, info=success\n                        )\n                        # Book Keeping\n                        #episode_revised_rewards_history.append(reward_revised)\n                        # Store the transition with the new goal and reward in the replay buffer\n                        # Get the observation and new observation from the concatenated value\n\n                        # Currently, the env on resetting returns a concatenated vector of\n                        # Observation and the desired goal. Therefore, we need to extract the\n                        # Observation for this step.\n                        observation = to_tensor(observation, use_cuda=self.cuda)\n                        new_observation = to_tensor(new_observation, use_cuda=self.cuda)\n\n                        g = to_tensor(g, use_cuda=self.cuda)\n                        #reward_revised = to_tensor(reward_revised, use_cuda=self.cuda)\n                        #print(observation)\n                        augmented_state = torch.cat([observation, g])\n                        augmented_new_state = torch.cat([new_observation, g])\n                        augmented_state = torch.unsqueeze(augmented_state, dim=0)\n                        augmented_new_state = torch.unsqueeze(augmented_new_state, dim=0)\n                        reward_revised = to_tensor([np.asscalar(reward_revised)], use_cuda=self.cuda)\n\n                        # Store the transition in the buffer\n                        self.ddpg.store_transition(state=augmented_state, new_state=augmented_new_state,\n                                                   action=action, done=done_bool, reward=reward_revised,\n                                                   success=success)\n\n                # Train the network\n                for train_steps in range(self.nb_train_steps):\n                    critic_loss, actor_loss = self.ddpg.fit_batch()\n                    if critic_loss is not None and actor_loss is not None:\n                        epoch_critic_losses.append(critic_loss)\n                        epoch_actor_losses.append(actor_loss)\n\n                    # Update the target networks using polyak averaging\n                    self.ddpg.update_target_networks()\n\n                eval_episode_rewards = []\n                eval_episode_successes = []\n                if self.eval_env is not None:\n                    eval_episode_reward = 0\n                    eval_episode_success = 0\n                    for t_rollout in range(self.num_eval_rollouts):\n                        if eval_state is not None:\n                            eval_action = self.ddpg.get_action(state=eval_state, noise=False)\n                        eval_new_state, eval_reward, eval_done, eval_success = self.eval_env.step(eval_action)\n                        eval_episode_reward += eval_reward\n                        eval_episode_success += eval_success[\'is_success\']\n\n                        if eval_done:\n                            # Get the episode goal\n                            #eval_episode_goal = eval_new_state[:self.ddpg.obs_dim]\n                            #eval_episode_goals_history.append(eval_episode_goal)\n                            eval_state = self.eval_env.reset()\n                            eval_state = to_tensor(eval_state, use_cuda=self.cuda)\n                            eval_state = torch.unsqueeze(eval_state, dim=0)\n                            eval_episode_rewards.append(eval_episode_reward)\n                            eval_episode_rewards_history.append(eval_episode_reward)\n                            eval_episode_successes.append(eval_episode_success)\n                            eval_episode_success_history.append(eval_episode_success)\n                            eval_episode_reward = 0\n                            eval_episode_success = 0\n\n                # Log stats\n                duration = time.time() - start_time\n                statistics[\'rollout/rewards\'] = np.mean(epoch_episode_rewards)\n                statistics[\'rollout/rewards_history\'] = np.mean(episode_rewards_history)\n                statistics[\'rollout/successes\'] = np.mean(epoch_episode_success)\n                statistics[\'rollout/successes_history\'] = np.mean(episode_success_history)\n                statistics[\'rollout/actions_mean\'] = np.mean(epoch_actions)\n                statistics[\'rollout/goals_mean\'] = np.mean(episode_goals_history)\n                statistics[\'train/loss_actor\'] = np.mean(epoch_actor_losses)\n                statistics[\'train/loss_critic\'] = np.mean(epoch_critic_losses)\n                statistics[\'total/duration\'] = duration\n\n                # Evaluation statistics\n                if self.eval_env is not None:\n                    statistics[\'eval/rewards\'] = np.mean(eval_episode_rewards)\n                    statistics[\'eval/rewards_history\'] = np.mean(eval_episode_rewards_history)\n                    statistics[\'eval/successes\'] = np.mean(eval_episode_successes)\n                    statistics[\'eval/success_history\'] = np.mean(eval_episode_success_history)\n                    statistics[\'eval/goals_history\'] = np.mean(eval_episode_goals_history)\n\n            # Print the statistics\n            if self.verbose:\n                if epoch % 5 == 0:\n                    print(epoch)\n                    print(""Reward "", statistics[\'rollout/rewards\'])\n                    print(""Successes "", statistics[\'rollout/successes\'])\n\n                    if self.eval_env is not None:\n                        print(""Evaluation Reward "", statistics[\'eval/rewards\'])\n                        print(""Evaluation Successes "", statistics[\'eval/successes\'])\n\n            # Log the combined statistics for all epochs\n            for key in sorted(statistics.keys()):\n                self.combined_statistics[key].append(statistics[key])\n\n            # Log the epoch rewards and successes\n            epoch_rewards.append(np.mean(epoch_episode_rewards))\n            epoch_success.append(np.mean(epoch_episode_success))\n\n        # Plot the statistics calculated\n        if self.plot_stats:\n            # Plot the rewards and successes\n            rewards_fname = self.output_folder + \'/rewards.jpg\'\n            success_fname = self.output_folder + \'/success.jpg\'\n            plot(epoch_rewards, f_name=rewards_fname, save_fig=True, show_fig=False)\n            plot(epoch_success, f_name=success_fname, save_fig=True, show_fig=False)\n\n        # Save the models on the disk\n        if self.save_model:\n            self.ddpg.save_model(self.output_folder)\n\n        return self.combined_statistics\n\n\n\n\n\n'"
Training/trainer_infogan.py,4,"b'""""""\nClass for a generic trainer used for training all the different generative models\n""""""\nfrom models import infogan\nfrom models.attention import *\nfrom torch.utils.data import Dataset\nimport os\nfrom skimage import io, transform\nfrom torchvision import transforms\nfrom Utils import tensorboard_writer\nimport torch.nn.functional as F\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass StatesDataset(Dataset):\n    """"""\n\n    Dataset consisting of the frames of the Atari Game-\n    Montezuma Revenge\n\n    """"""\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.list_files()\n\n    def __len__(self):\n        return len(self.images)\n\n    def list_files(self):\n        for m in os.listdir(self.root_dir):\n            if m.endswith(\'.jpg\'):\n                self.images.append(m)\n\n    def __getitem__(self, idx):\n        m = self.images[idx]\n        image = io.imread(os.path.join( self.root_dir, m))\n        sample = {\'image\': image}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n# Transformations\nclass Rescale(object):\n    """"""Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        return {\'image\': img}\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n        image = torch.FloatTensor(torch.from_numpy(image).float())\n        return {\'image\': image}\n\n\nif __name__ == \'__main__\':\n\n    image_size = 128\n    height_img = 128\n    width_img = 128\n    seed = 100\n    input_images = \'montezuma_resources\'\n\n    dataset = StatesDataset(root_dir=input_images, transform=\n        transforms.Compose([Rescale(image_size),ToTensor()]))\n    generator = infogan.Generator(conv_layers=32, conv_kernel_size=2, latent_space_dimension=128,\n                                   height=height_img, width=width_img, hidden_dim=128, input_channels=3)\n    discriminator = infogan.Discriminator_recognizer(input_channels=3, conv_layers=32, conv_kernel_size=3, pool_kernel_size=2,\n                                           hidden=64, height=height_img, width=width_img, cat_dim=10, cont_dim=2)\n\n    if USE_CUDA:\n        generator = generator.cuda()\n        discriminator = discriminator.cuda()\n\n    # Tensorboard writer for visualizing the training curves\n    tb_writer = tensorboard_writer.TensorboardWriter()\n\n    infogan_model = infogan.InfoGAN(generator=generator, discriminator=discriminator,\n                                    dataset=dataset, batch_size=16, generator_lr=1e-4,\n                                    discriminator_lr=4e-4, num_epochs=500, random_seed=seed,\n                                    shuffle=True, tensorboard_summary_writer=tensorboard_writer,\n                                    use_cuda=USE_CUDA, output_folder=\'infogan/inference/\')\n    infogan_model.train()\n\n\n\n'"
Training/trainer_sac.py,4,"b""# Training script for SAC\n\nimport torch\n# Add this line to get better performance\ntorch.backends.cudnn.benchmark=True\nfrom Utils import utils\nimport torch.optim as optim\nfrom models.SAC import SAC, StochasticActor, Critic, ValueNetwork\nimport torch.nn.functional as F\nuse_cuda = torch.cuda.is_available()\nfrom osim.env import ProstheticsEnv\nimport os\n\nif __name__ == '__main__':\n    # Specify the environment name and create the appropriate environment\n    seed = 4240\n    env = ProstheticsEnv(visualize=False)\n    eval_env = ProstheticsEnv(visualize=False)\n\n    action_dim = env.get_action_space_size()\n    observation_dim = env.get_observation_space_size()\n\n    buffer_capacity = int(1e3)\n    q_dim = 1\n    v_dim = 1\n    batch_size = 128\n    hidden_units = 256\n    gamma = 0.98  # Discount Factor for future rewards\n    num_epochs = 50\n    learning_rate = 1e-2\n    critic_learning_rate = 1e-2\n    value_learning_rate = 1e-2\n    polyak_factor = 0.05\n    # Adam Optimizer\n    opt = optim.Adam\n\n    # Output Folder\n    output_folder = os.getcwd() + '/output_sac/'\n\n    # Convert the observation and action dimension to int\n    print(observation_dim)\n    observation_dim = int(observation_dim)\n    action_dim = int(action_dim)\n    print(action_dim)\n\n    # Agent definition\n    actor = StochasticActor(state_dim=observation_dim, action_dim=action_dim,\n                            hidden_dim=hidden_units, use_sigmoid=True)\n    critic = Critic(state_dim=observation_dim, action_dim=action_dim,\n                    output_dim=q_dim, hidden_dim=hidden_units)\n    value = ValueNetwork(state_dim=observation_dim, hidden_dim=hidden_units,\n                         output_dim=v_dim)\n    target_value = ValueNetwork(state_dim=observation_dim, hidden_dim=hidden_units,\n                         output_dim=v_dim)\n    sac = SAC(state_dim=observation_dim, action_dim=action_dim,\n              hidden_dim=hidden_units, actor=actor, critic=critic,\n              value_network=value, actor_learning_rate=learning_rate,\n              critic_learning_rate=critic_learning_rate, value_learning_rate=value_learning_rate,\n              batch_size=batch_size, buffer_capacity=buffer_capacity, env=env, eval_env=eval_env,\n              gamma=gamma, max_episodes_per_epoch=50, nb_train_steps=50, num_epochs=num_epochs,\n              num_eval_rollouts=50, num_q_value=q_dim, num_v_value=v_dim, num_rollouts=100,\n              output_folder=output_folder, polyak_constant=polyak_factor, random_seed=seed,\n              target_value_network=target_value, use_cuda=use_cuda)\n\n    # Train\n    sac.train()\n"""
Training/trainer_vae.py,12,"b'""""""\nClass for a generic trainer used for training all the different generative models\n""""""\nimport torch\nimport torch.nn as nn\nfrom Utils.utils import *\nfrom collections import deque, defaultdict\nfrom models import vae\nfrom models.attention import *\nimport time\nimport numpy as np\nimport random\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import DataLoader, Dataset\nimport os\nfrom skimage import io, transform\nimport scipy.misc as m\n\n\nclass StatesDataset(Dataset):\n    """"""\n\n    Dataset consisting of the frames of the Atari Game-\n    Montezuma Revenge\n\n    """"""\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.list_files()\n\n    def __len__(self):\n        return len(self.images)\n\n    def list_files(self):\n        for m in os.listdir(self.root_dir):\n            if m.endswith(\'.jpg\'):\n                self.images.append(m)\n\n    def __getitem__(self, idx):\n        m = self.images[idx]\n        image = io.imread(os.path.join( self.root_dir, m))\n        sample = {\'image\': image}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n# Transformations\nclass Rescale(object):\n    """"""Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        return {\'image\': img}\n\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n\n        return {\'image\': torch.FloatTensor(torch.from_numpy(image).float())}\n\n\nclass Trainer(object):\n\n    """"""\n    Training class for training a beta vae\n    with the option of training it with a denoising autoencoder\n    similar to how it was trained in DARLA\n    """"""\n\n    def __init__(self, beta, num_d_epochs,\n                 generative_model, learning_rate, num_epochs,\n                 input_images_folder, batch_size, image_size,\n                 random_seed, output_folder, multi_gpu_training=False,\n                 use_cuda=True, save_model=True, verbose=True,\n                 plot_stats=True, shuffle=True, model_path=None,\n                 denoising_autoencoder=None, d_output_folder=None,\n                 dae_weights=None):\n\n        """"""\n\n        :param generative_model: The generative model (eg VAE) to train\n        :param learning_rate: learning rate for the optimizer\n        :param num_epochs: total number of training steps\n        :param random_seed: set the random seed for reproduction of the results\n        :param output_folder: output folder for the saved model\n        :param use_cuda: use cuda in case of availability of gpu\n        :param save_model: save the generative model weights\n        :param verbose: print the training statements\n        :param plot_stats: plot the stats of training\n        :param beta: This hyperparameter decides the disentanglement factor of the vae\n        :param model_path: The path to the saved weights of the model\n        """"""\n\n        self.model = generative_model\n        self.lr = learning_rate\n        self.seed = random_seed\n        self.input_images = input_images_folder\n        self.num_epochs = num_epochs\n        self.output_folder = output_folder\n        self.use_cuda  = use_cuda\n        self.multi_gpu = multi_gpu_training\n        self.save_model_bool = save_model\n        self.verbose = verbose\n        self.plot_stats= plot_stats\n        self.batch = batch_size\n        self.shuffle = shuffle\n        self.dataset = StatesDataset(root_dir=self.input_images, transform=\n        transforms.Compose([Rescale(image_size), ToTensor()]))\n        self.optimizer = optim.Adam(lr=learning_rate, params=self.model.parameters())\n        self.beta = beta\n        self.model_weights = model_path\n        self.latents = []\n        self.d_autoencoder = denoising_autoencoder\n        self.d_output_folder = d_output_folder\n        self.d_optim = optim.Adam(lr=learning_rate, params=self.d_autoencoder.parameters())\n        self.dae_weights = dae_weights\n        self.num_d_epochs = num_d_epochs\n\n\n    def get_dataloader(self):\n        # Generates the dataloader for the images for training\n\n        dataset_loader = DataLoader(self.dataset,\n                                    batch_size=self.batch,\n                                    shuffle=self.shuffle)\n\n        return dataset_loader\n\n    # Definition of the loss function -> Defining beta which is used in beta-vae\n    def loss_function(self, recon_x, x, mu, logvar, beta, BATCH_SIZE):\n        # This is the log p(x|z) defined as the mean squared loss between the\n        # reconstruction and the original image\n        x.detach_()\n        MSE = nn.MSELoss()(recon_x, x)\n\n\n        # KLD - Kullback liebler divergence -- how much one learned distribution\n        # deviate from one another, in this case the learned distribution\n        # from the unit Gaussian.\n\n        # see Appendix B from VAE paper:\n        # Kingma and Welling. Auto-Encoding Variational Bayes. ICLR, 2014\n        # https://arxiv.org/abs/1312.6114\n        # - D_{KL} = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n\n        mu_sum_sq = (mu * mu).sum(dim=1)\n        sigma = logvar.mul(0.5).exp_()\n        sig_sum_sq = (sigma * sigma).sum(dim=1)\n        log_term = (1 + torch.log(sigma ** 2)).sum(dim=1)\n        kldiv = -0.5 * (log_term - mu_sum_sq - sig_sum_sq)\n        # Normalize by the same number of elements in reconstruction\n        #KLD = KLD / BATCH_SIZE\n\n        # BCE tries to make our reconstruction as accurate as possible\n        # KLD tries to push the distributions as close as possible to unit Gaussian\n\n        # To learn disentangled representations, we use the beta parameter\n        # as in the beta-vae\n        loss = MSE + beta * kldiv.mean()\n\n        return loss\n\n    def mse_loss(self, input, target):\n        return torch.sum((input - target) ^ 2) / input.data.nelement()\n\n    def train_dae(self):\n\n        if self.d_autoencoder is not None:\n\n            for epoch in range(self.num_d_epochs):\n                cummulative_loss = 0\n                for i_batch, sampled_batch in enumerate(self.get_dataloader()):\n                    image = sampled_batch[\'image\']\n                    image = Variable(image)\n                    self.d_optim.zero_grad()\n                    decoded_image, encoded = self.d_autoencoder(image)\n                    loss = nn.MSELoss()(decoded_image, image)\n                    loss.backward()\n                    cummulative_loss += loss.data[0]\n\n                    self.d_optim.step()\n\n                print(cummulative_loss)\n\n            self.save_model(output=self.d_output_folder, model=self.d_autoencoder)\n\n    def train_bvae(self):\n        #dae_state_dict = torch.load(self.dae_weights)\n        #dae = vae.DAE(conv_layers=16, conv_kernel_size=3, pool_kernel_size=2,\n        #                            height=96, width=96, input_channels=3, hidden_dim=64, noise_scale=0)\n        #dae.load_state_dict(dae_state_dict)\n        # Freeze the weights of the denoising autoencoder\n        #dae.eval()\n\n        for epoch in range(self.num_epochs):\n            cummulative_loss = 0\n            for i_batch, sampled_batch in enumerate(self.get_dataloader()):\n                image = sampled_batch[\'image\']\n                image = Variable(image)\n\n                if self.use_cuda:\n                    image = image.cuda()\n\n                self.optimizer.zero_grad()\n\n                decoded_image, mu, logvar, z = self.model(image)\n\n                loss = self.loss_function(decoded_image, image, mu, logvar,\n                                              self.beta, self.batch)\n                loss.backward()\n                cummulative_loss += loss.data[0]\n\n                self.optimizer.step()\n\n            print(cummulative_loss)\n\n        self.save_model(output=self.output_folder, model=self.model)\n\n    def seed(self, s):\n        # Seed everything to make things reproducible\n        random.seed = s\n        np.random.seed(seed=s)\n\n    def save_model(self, output, model):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the generative model"")\n        torch.save(\n            model.state_dict(),\n            \'{}/generative_model.pt\'.format(output)\n        )\n\n    def load_model(self):\n        # Load the model from the saved weights file\n        if self.model_weights is not None:\n            model_state_dict = torch.load(self.model_weights)\n            self.model.load_state_dict(model_state_dict)\n        else:\n            print(""Train a model for loading later"")\n\n    def inference(self):\n        # Do inference on the images\n        self.load_model()\n        self.model.eval()\n\n        for j, batch in enumerate(self.get_dataloader()):\n            images = batch[\'image\']\n            index = random.randint(0, self.batch-1)\n            i = images[index]\n            i = Variable(torch.unsqueeze(i, 0))\n            if self.use_cuda:\n                i = i.cuda()\n            decoded_image, mu, logvar, z = self.model(i)\n            self.latents.append(z)\n            decoded_image = decoded_image.data.cpu().numpy()\n            decoded_image = np.squeeze(decoded_image, 0)\n            decoded_image = np.transpose(decoded_image, (1, 2, 0))\n            path = os.path.join(\'\', str(j) + \'decoded.jpg\')\n            m.imsave(path, decoded_image)\n\n    def interpolate(self):\n        z = self.latents[0]\n        self.interpolated_latents = []\n        add = np.zeros(shape=z.shape)\n        for i in range(-3, 3):\n            add[0] = i\n            z_new = z + add\n            self.interpolated_latents.append(z_new)\n\n\nif __name__ == \'__main__\':\n    image_size = 96\n    seed = 100\n    USE_CUDA = torch.cuda.is_available()\n    generative_model = vae.VAE(conv_layers=32, z_dimension=64,\n                               pool_kernel_size=2, conv_kernel_size=4,\n                               input_channels=3, height=96, width=96, hidden_dim=128, use_cuda=USE_CUDA)\n    denoising_autoencoder = vae.DAE(conv_layers=16, conv_kernel_size=3, pool_kernel_size=2,\n                                    height=96, width=96, input_channels=3, hidden_dim=64, noise_scale=0.3, use_cuda=USE_CUDA)\n\n    if USE_CUDA:\n        generative_model = generative_model.cuda()\n        denoising_autoencoder = denoising_autoencoder.cuda()\n\n    trainer = Trainer(beta=0.1, generative_model=generative_model, learning_rate=1e-3,\n                      num_epochs=200, input_images_folder=\'montezuma_resources\',\n                      image_size=image_size, batch_size=32, output_folder=\'vae_output/\',\n                      random_seed=seed, model_path=\'vae_output/generative_model.pt\',\n                      denoising_autoencoder=denoising_autoencoder,\n                      d_output_folder=\'denoising_output/\', dae_weights=\'denoising_output/generative_model.pt\',\n                      num_d_epochs=60)\n    #trainer.train_dae()\n    trainer.train_bvae()\n\n    #Inference\n    trainer.inference()'"
Training/trainer_vaegan.py,3,"b'""""""\nClass for a generic trainer used for training all the different generative models\n""""""\nfrom models import cvae_gan\nfrom models.attention import *\nfrom torch.utils.data import Dataset\nimport os\nfrom skimage import io, transform\nfrom torchvision import transforms\nfrom Utils import tensorboard_writer\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass StatesDataset(Dataset):\n    """"""\n\n    Dataset consisting of the frames of the Atari Game-\n    Montezuma Revenge\n\n    """"""\n\n    def __init__(self, root_dir, transform=None):\n        self.root_dir = root_dir\n        self.transform = transform\n        self.images = []\n        self.list_files()\n\n    def __len__(self):\n        return len(self.images)\n\n    def list_files(self):\n        for m in os.listdir(self.root_dir):\n            if m.endswith(\'.jpg\'):\n                self.images.append(m)\n\n    def __getitem__(self, idx):\n        m = self.images[idx]\n        image = io.imread(os.path.join( self.root_dir, m))\n        sample = {\'image\': image}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n\n# Transformations\nclass Rescale(object):\n    """"""Rescale the image in a sample to a given size.\n\n    Args:\n        output_size (tuple or int): Desired output size. If tuple, output is\n            matched to output_size. If int, smaller of image edges is matched\n            to output_size keeping aspect ratio the same.\n    """"""\n\n    def __init__(self, output_size):\n        assert isinstance(output_size, (int, tuple))\n        self.output_size = output_size\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        h, w = image.shape[:2]\n        if isinstance(self.output_size, int):\n            if h > w:\n                new_h, new_w = self.output_size, self.output_size\n            else:\n                new_h, new_w = self.output_size, self.output_size * w / h\n        else:\n            new_h, new_w = self.output_size\n\n        new_h, new_w = int(new_h), int(new_w)\n\n        img = transform.resize(image, (new_h, new_w))\n\n        return {\'image\': img}\n\nclass ToTensor(object):\n    """"""Convert ndarrays in sample to Tensors.""""""\n\n    def __call__(self, sample):\n        image = sample[\'image\']\n\n        # swap color axis because\n        # numpy image: H x W x C\n        # torch image: C X H X W\n        image = image.transpose((2, 0, 1))\n\n        return {\'image\': torch.FloatTensor(torch.from_numpy(image).float())}\n\n\nif __name__ == \'__main__\':\n\n    image_size = 128\n    height_img = 128\n    width_img = 128\n    seed = 100\n    input_images = \'montezuma_resources\'\n\n    lambda_1 = 3\n    lambda_2 = 1\n\n    dataset = StatesDataset(root_dir=input_images, transform=\n        transforms.Compose([Rescale(image_size), ToTensor()]))\n\n    encoder = cvae_gan.Encoder(conv_layers=32, conv_kernel_size=3, latent_space_dim=256,\n                               hidden_dim=128, use_cuda=USE_CUDA, height=height_img, width=width_img,\n                               input_channels=3, pool_kernel_size=2)\n    generator = cvae_gan.Generator(conv_layers=32, conv_kernel_size=2, latent_space_dimension=256,\n                                   height=height_img, width=width_img, hidden_dim=128, input_channels=3)\n    discriminator = cvae_gan.Discriminator(input_channels=3, conv_layers=16, conv_kernel_size=3, pool_kernel_size=2,\n                                           hidden=128, height=height_img, width=width_img)\n\n    if USE_CUDA:\n        generator = generator.cuda()\n        discriminator = discriminator.cuda()\n        encoder = encoder.cuda()\n\n    # Tensorboard writer for visualizing the training curves\n    tb_writer = tensorboard_writer.TensorboardWriter()\n\n    cvae_gan = cvae_gan.CVAEGAN(encoder=encoder, batch_size=32, num_epochs=400,\n                                random_seed=seed, dataset=dataset, discriminator=discriminator,\n                                generator=generator, discriminator_lr=0.000001, encoder_lr=0.000001,\n                                generator_lr=0.000001, use_cuda=USE_CUDA, output_folder=\'cvaegan_output/\',\n                                inference_output_folder=\'cvaegan_output/inference/\', test_dataset=dataset,\n                                tensorboard_summary_writer=tb_writer,\n                                encoder_weights=\'cvaegan_output/encoder/cvaegan.pt\',\n                                generator_weights=\'cvaegan_output/generator/cvaegan.pt\',\n                                discriminator_weights=\'cvaegan_output/discriminator/cvaegan.pt\')\n\n    cvae_gan.train(lambda_1=lambda_1, lambda_2=lambda_2)\n    #cvae_gan.inference()\n\n'"
Utils/parallel_process.py,1,"b'from torch.multiprocessing import Process, Pipe\nimport pickle\nimport cloudpickle\nimport numpy as np\n\ndef worker(remote, parent_remote, env_function_wrapper):\n    parent_remote.close()\n    env = env_function_wrapper.x()\n    while True:\n        cmd, data = remote.recv()\n        if cmd == \'step\':\n            ob, reward, done, info = env.step(data)\n            if done:\n                ob = env.reset()\n            remote.send((ob, reward, done, info))\n        elif cmd == \'reset\':\n            ob = env.reset()\n            remote.send(ob)\n        elif cmd == \'reset_task\':\n            ob = env.reset_task()\n            remote.send(ob)\n        elif cmd == \'close\':\n            remote.close()\n            break\n        elif cmd == \'get_spaces\':\n            remote.send((env.action_space, env.observation_space))\n        else:\n            raise NotImplementedError\n\n\n\nclass CloudPickleWrapper(object):\n    """"""\n    Use CloudPickle to serialize contents otherwise multiprocessing uses pickle to serialize\n    """"""\n    def __init__(self, x):\n        self.x = x\n    def __getstate__(self):\n        return cloudpickle.dumps(self.x)\n    def __setstate__(self, ob):\n        self.x = pickle.loads(ob)\n\n\nclass SubProcVecEnv(object):\n\n    def __init__(self, envs):\n        """"""\n\n        :param envs: List of gym environments to run in subprocess\n        """"""\n        self.envs = envs\n        self.closed = False\n\n        # Get the number of envs to run\n        num_envs = len(envs)\n        # Create the remotes and work remotes accordingly\n        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(num_envs)])\n        # Now create the different processes accordingly\n        ps = [Process(target=worker, args=(worker_remote, remote, CloudPickleWrapper(env)))\n              for (worker_remote, remote, env) in zip(self.work_remotes, self.remotes, envs)]\n        # Iterate through the processes and start them\n        for p in ps:\n            p.daemon = True  # if the main process crashes, we should not cause things to hang\n            p.start()\n\n        # Close the work remotes\n        for remote in self.work_remotes:\n            remote.close()\n\n        self.remotes[0].send((\'get_spaces\', None))\n        self.action_space, self.observation_space = self.remotes[0].recv()\n\n\n\n    # Stepping into multiple environments and aggregating the results\n    def step(self, actions):\n        # Send the corresponding action step for the remote\n        for remote, action in zip(self.remotes, actions):\n            remote.send((\'step\', action))\n        # Get the results from the remotes\n        results = [remote.recv() for remote in self.remotes]\n        obs, rews, dones, infos = zip(*results)\n        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n\n    # Reset the multiple enviroments\n    def reset(self):\n        for remote in self.remotes:\n            remote.send((\'reset\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    # Reset the tasks in the multiple environments\n    def reset_task(self):\n        for remote in self.remotes:\n            remote.send((\'reset_task\', None))\n        return np.stack([remote.recv() for remote in self.remotes])\n\n    # Close the environment\n    def close(self):\n        if self.closed:\n            return\n\n        for remote in self.remotes:\n            remote.send((\'close\', None))\n        for p in self.ps:\n            p.join()\n        self.closed = True\n\n    @property\n    def num_envs(self):\n        return len(self.remotes)\n\n'"
Utils/parameter_perturbations.py,2,"b'""""""\n\nThis script contains the implementation of the parameter perturbations as suggested in\nPARAMETER SPACE NOISE FOR EXPLORATION, Plappert et al.\n\n""""""\n\nimport torch\nfrom copy import copy\n\n\nclass ParameterNoise(object):\n\n    def __init__(self, actor, param_noise_stddev,\n                 param_noise=None,\n                 normalized_observation=None):\n\n        self.actor = actor\n        self.param_noise_stddev = param_noise_stddev\n        self.param_noise = param_noise\n        self.normalized_observation= normalized_observation\n\n    def get_perturbable_parameters(self, model):\n        # Removing parameters that don\'t require parameter noise\n        parameters = []\n        for name, params in model.named_parameters():\n            if \'ln\' not in name:\n                parameters.append(params)\n\n        return parameters\n\n    def set_perturbed_actor_updates(self, model):\n        """"""\n\n        Update the perturbed actor parameters\n\n        :return:\n        """"""\n        assert len(self.actor.parameters()) == len(model.parameters())\n        actor_perturbable_parameters = self.get_perturbable_parameters(self.actor)\n        perturbed_actor_perturbable_parameters = self.get_perturbable_parameters(model)\n        assert len(actor_perturbable_parameters) == len(perturbed_actor_perturbable_parameters)\n\n        for params, perturbed_params in zip(actor_perturbable_parameters, perturbed_actor_perturbable_parameters):\n            # Update the parameters\n            perturbed_params.data.copy_(params + torch.normal(mean=torch.zeros(params.shape),\n                                                              std=self.param_noise_stddev))\n\n    def setup_param_noise(self, normalized_observation):\n\n        assert self.param_noise_stddev is not None\n        # Configure perturbed actor\n        self.perturbed_actor = copy(self.actor)\n        # Perturb the perturbed actor weights\n        self.set_perturbed_actor_updates(self.perturbed_actor)\n        # Configure separate copy for stddev adoption\n        self.adaptive_perturbed_actor = copy(self.actor)\n        # Perturb the adaptive actor weights\n        self.set_perturbed_actor_updates(self.adaptive_perturbed_actor)\n        # Refer to https://arxiv.org/pdf/1706.01905.pdf for details on the distance used specifically for DDPG\n        self.adaptive_policy_distance = torch.pow(torch.mean(torch.pow(self.actor(normalized_observation) -\n                                                                       self.adaptive_perturbed_actor(normalized_observation)\n                                                                       , 2)), 0.5)\n\n        return self.adaptive_policy_distance\n\n    def adapt_param_noise(self, state):\n        if self.param_noise_stddev is None:\n            return 0.\n\n        # Perturb a separate copy of the policy to adjust the scale for the next ""real"" perturbation.\n        self.set_perturbed_actor_updates(self.perturbed_actor)\n        adaptive_noise_distance = self.setup_param_noise(state)\n        self.param_noise.adapt(adaptive_noise_distance)\n\n    def reset(self):\n        # Reset internal state after an episode is complete\n        if self.param_noise is not None:\n            self.param_noise_stddev = self.param_noise.current_stddev\n            self.set_perturbed_actor_updates(self.perturbed_actor)'"
Utils/random_process.py,0,"b""import numpy as np\n\n\n# Base Random Process Class\nclass RandomProcess(object):\n\n    def reset_states(self):\n        pass\n\n\n# Annealed Gaussian\nclass AnnealedGaussianProcess(RandomProcess):\n\n    def __init__(self, mu, sigma, sigma_min, n_steps_annealing):\n        self.mu = mu\n        self.sigma = sigma\n        self.n = 0\n\n        if sigma_min is not None:\n            self.m = - float(sigma - sigma_min)/ float(n_steps_annealing)\n            self.c = sigma\n            self.sigma_min = sigma_min\n        else:\n            self.m = 0\n            self.c = sigma\n            self.sigma_min = sigma\n\n    @property\n    def current_sigma(self):\n        sigma = max(self.sigma_min, self.m*float(self.n) + self.c)\n        return sigma\n\n\nclass OrnsteinUhlenbeckActionNoise:\n\n    def __init__(self, action_dim, mu = 0, theta = 0.15, sigma = 0.2):\n        self.action_dim = action_dim\n        self.mu = mu\n        self.theta = theta\n        self.sigma = sigma\n        self.X = np.ones(self.action_dim) * self.mu\n\n    def reset(self):\n        self.X = np.ones(self.action_dim) * self.mu\n\n    def sample(self):\n        dx = self.theta * (self.mu - self.X)\n        dx = dx + self.sigma * np.random.randn(len(self.X))\n        self.X = self.X + dx\n        return self.X\n\n\n# Adaptive param noise for parameter perturbations\nclass AdaptiveParamNoise(object):\n\n    def __init__(self, initial_stddev=0.1, desired_action_stddev=0.1, adoption_coefficient=1.01):\n        self.initial_stddev = initial_stddev\n        self.desired_action_stddev = desired_action_stddev\n        self.adoption_coefficient = adoption_coefficient\n\n        self.current_stddev = initial_stddev\n\n    def adapt(self, distance):\n        if distance > self.desired_action_stddev:\n            # Decrease stddev.\n            self.current_stddev /= self.adoption_coefficient\n        else:\n            # Increase stddev.\n            self.current_stddev *= self.adoption_coefficient\n\n    def get_stats(self):\n        stats = {\n            'param_noise_stddev': self.current_stddev,\n        }\n        return stats\n\n    def __repr__(self):\n        fmt = 'AdaptiveParamNoiseSpec(initial_stddev={}, desired_action_stddev={}, adoption_coefficient={})'\n        return fmt.format(self.initial_stddev, self.desired_action_stddev, self.adoption_coefficient)"""
Utils/tensorboard_writer.py,0,"b'""""""\n\nA Tensorboard Summary writer for showing the training curves\nin a pytorch training loop.\n\n""""""\n\n\nfrom tensorboardX import SummaryWriter\n\n\nclass TensorboardWriter(object):\n\n    def __init__(self):\n        self.writer = SummaryWriter()\n\n    def write(self, scalar, scalar_name, epoch):\n        self.writer.add_scalar(scalar_name, scalar, epoch)\n\n    def write_scalars(self, scalars, scalar_grp_name, epoch):\n        assert type(scalars) == \'dict\'\n        self.writer.add_scalars(scalar_grp_name, scalars, epoch)\n\n    def export(self, path, close_writer=True):\n        self.writer.export_scalars_to_json(path)\n        if close_writer:\n            self.writer.close()'"
Utils/utils.py,3,"b'import matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport gym\nimport torch\nfrom torch.autograd import Variable\n\n\nclass EnvGenerator(object):\n    """"""\n    Class for generating the required gym environment creator\n    """"""\n\n    def __init__(self, name, seed, goal_based=True):\n        self.name = name\n        self.goal_based = goal_based\n        self.seed = seed\n        # Create the suitable environment\n\n        self.env = gym.make(name)\n        if goal_based:\n            self.env = gym.wrappers.FlattenDictWrapper(\n                self.env, [\'observation\', \'desired_goal\']\n            )\n\n    def make_env_goal_based(self, keys_to_concatenate):\n        self.env = gym.wrappers.FlattenDictWrapper(\n            self.env, keys_to_concatenate\n        )\n\n    def seed_env(self):\n        self.env.seed(self.seed)\n\n    def get_environment(self):\n        return self.env\n\n    def get_observation_space(self):\n        return self.env.observation_space\n\n    def get_action_space(self):\n        return self.env.action_space\n\n    def get_observation_dim(self):\n        observation = self.env.reset()[\'observation\']\n        return observation.shape[0]\n\n    def get_action_dim(self):\n        return self.get_action_space().shape[0]\n\n    def get_goal_dim(self):\n        d = self.env.reset()\n        d = d[\'desired_goal\']\n        return d.shape[0]\n\n    def get_action_shape(self):\n        return self.get_action_space().shape\n\n    def get_observation_shape(self):\n        return self.get_observation_space().shape\n\n    def take_random_action(self):\n        return self.get_action_space().sample()\n\n    def render(self):\n        self.env.render()\n\n\ndef plot_goals(rewards, suc):\n    plt.figure(figsize=(20,5))\n    plt.subplot(131)\n    plt.title(\'reward: %s\' % (np.mean(rewards[-10:])))\n    plt.plot(rewards)\n    plt.subplot(132)\n    plt.title(\'success\')\n    plt.plot(suc)\n    plt.show()\n\n\ndef plot(x, f_name=None, save_fig=True, show_fig=True):\n    fig = plt.figure(figsize=(20, 5))\n    plt.plot(x)\n    if show_fig:\n        plt.show()\n    if save_fig:\n        if f_name is not None:\n            fig.savefig(f_name)\n\n\ndef to_tensor(v, use_cuda=True):\n    if use_cuda:\n        v = torch.cuda.FloatTensor(v)\n    else:\n        v = torch.FloatTensor(v)\n    return v\n\n\ndef soft_update(polyak_factor, target_network, network):\n    """"""\n    Soft update of the parameters using Polyak averaging\n    :param polyak_factor: The factor by which to move the averages\n    :param target_network: The network to load the weights INTO\n    :param network: The network to load weights FROM\n    """"""\n    for target_param, param in zip(target_network.parameters(), network.parameters()):\n        target_param.data.copy_(polyak_factor*param.data + target_param.data*(1.0 - polyak_factor))\n\n\n\ndef hard_update(target_network, network):\n    """"""\n    Hard Update of the networks\n    :param target_network: The network to load the weights INTO\n    :param network: The network to load weights FROM\n    """"""\n    target_network.load_state_dict(network.state_dict())\n\n\ndef get_epsilon_iteration(steps_done, EPS_END, EPS_START, EPS_DECAY):\n    """"""\n    Used for the epsilon greedy policy (Used in DQN)\n    :param steps_done:\n    :param EPS_END:\n    :param EPS_START:\n    :param EPS_DECAY:\n    :return:\n    """"""\n    eps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n                              math.exp(-1. * steps_done / EPS_DECAY)\n    return eps_threshold'"
Utils/visualize.py,0,"b'""""""\nThe script contains the helper functions required for plotting the graphs\n""""""\n\n# Copied from https://github.com/emansim/baselines-mansimov/blob/master/baselines/a2c/visualize_atari.py\n# and https://github.com/emansim/baselines-mansimov/blob/master/baselines/a2c/load.py\n# Thanks to the author and OpenAI team!\n\nimport glob\nimport json\nimport os\n\nimport matplotlib\nmatplotlib.use(\'Agg\')\nimport matplotlib.pyplot as plt\nplt.switch_backend(\'agg\')\nimport numpy as np\nfrom scipy.signal import medfilt\nmatplotlib.rcParams.update({\'font.size\': 8})\n\n\ndef smooth_reward_curve(x, y):\n    # Halfwidth of our smoothing convolution\n    halfwidth = min(31, int(np.ceil(len(x) / 30)))\n    k = halfwidth\n    xsmoo = x[k:-k]\n    ysmoo = np.convolve(y, np.ones(2 * k + 1), mode=\'valid\') / \\\n        np.convolve(np.ones_like(y), np.ones(2 * k + 1), mode=\'valid\')\n    downsample = max(int(np.floor(len(xsmoo) / 1e3)), 1)\n    return xsmoo[::downsample], ysmoo[::downsample]\n\n\ndef fix_point(x, y, interval):\n    np.insert(x, 0, 0)\n    np.insert(y, 0, 0)\n\n    fx, fy = [], []\n    pointer = 0\n\n    ninterval = int(max(x) / interval + 1)\n\n    for i in range(ninterval):\n        tmpx = interval * i\n\n        while pointer + 1 < len(x) and tmpx > x[pointer + 1]:\n            pointer += 1\n\n        if pointer + 1 < len(x):\n            alpha = (y[pointer + 1] - y[pointer]) / \\\n                (x[pointer + 1] - x[pointer])\n            tmpy = y[pointer] + alpha * (tmpx - x[pointer])\n            fx.append(tmpx)\n            fy.append(tmpy)\n\n    return fx, fy\n\n\ndef load_data(indir, smooth, bin_size):\n    datas = []\n    infiles = glob.glob(os.path.join(indir, \'*.monitor.csv\'))\n\n    for inf in infiles:\n        with open(inf, \'r\') as f:\n            f.readline()\n            f.readline()\n            for line in f:\n                tmp = line.split(\',\')\n                t_time = float(tmp[2])\n                tmp = [t_time, int(tmp[1]), float(tmp[0])]\n                datas.append(tmp)\n\n    datas = sorted(datas, key=lambda d_entry: d_entry[0])\n    result = []\n    timesteps = 0\n    for i in range(len(datas)):\n        result.append([timesteps, datas[i][-1]])\n        timesteps += datas[i][1]\n\n    if len(result) < bin_size:\n        return [None, None]\n\n    x, y = np.array(result)[:, 0], np.array(result)[:, 1]\n\n    if smooth == 1:\n        x, y = smooth_reward_curve(x, y)\n\n    if smooth == 2:\n        y = medfilt(y, kernel_size=9)\n\n    x, y = fix_point(x, y, bin_size)\n    return [x, y]\n\n\ncolor_defaults = [\n    \'#1f77b4\',  # muted blue\n    \'#ff7f0e\',  # safety orange\n    \'#2ca02c\',  # cooked asparagus green\n    \'#d62728\',  # brick red\n    \'#9467bd\',  # muted purple\n    \'#8c564b\',  # chestnut brown\n    \'#e377c2\',  # raspberry yogurt pink\n    \'#7f7f7f\',  # middle gray\n    \'#bcbd22\',  # curry yellow-green\n    \'#17becf\'   # blue-teal\n]\n\n\ndef visdom_plot(viz, win, folder, game, name, num_steps, bin_size=100, smooth=1):\n    tx, ty = load_data(folder, smooth, bin_size)\n    if tx is None or ty is None:\n        return win\n\n    fig = plt.figure()\n    plt.plot(tx, ty, label=""{}"".format(name))\n\n    tick_fractions = np.array([0.1, 0.2, 0.4, 0.6, 0.8, 1.0])\n    ticks = tick_fractions * num_steps\n    tick_names = [""{:.0e}"".format(tick) for tick in ticks]\n    plt.xticks(ticks, tick_names)\n    plt.xlim(0, num_steps * 1.01)\n\n    plt.xlabel(\'Number of Timesteps\')\n    plt.ylabel(\'Rewards\')\n\n    plt.title(game)\n    plt.legend(loc=4)\n    plt.show()\n    plt.draw()\n\n    image = np.fromstring(fig.canvas.tostring_rgb(), dtype=np.uint8, sep=\'\')\n    image = image.reshape(fig.canvas.get_width_height()[::-1] + (3, ))\n    plt.close(fig)\n\n    # Show it in visdom\n    image = np.transpose(image, (2, 0, 1))\n    return viz.image(image, win=win)\n\n\nif __name__ == ""__main__"":\n    from visdom import Visdom\n    viz = Visdom()\n    visdom_plot(viz, None, \'/tmp/gym/\', \'BreakOut\', \'a2c\', bin_size=100, smooth=1)'"
distributed/parallel_process.py,5,"b'""""""\nThis script spawns 2 processes who will each setup the distributed environments ,\ninitialize the process group and finally execute the given run function\n""""""\n\n\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.multiprocessing import Process\n\n\n"""""" Blocking point to point communication. """"""\n\n\ndef run_p2p(rank, size):\n    tensor = torch.zeros(1)\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        dist.send(tensor=tensor, dst=1)\n    else:\n        # Receive tensor from process 0\n        dist.recv(tensor=tensor, src=0)\n\n    print(\'Rank \', rank, \' has data \', tensor[0])\n\n\n"""""" Non Blocking point to point communication""""""\n\n\ndef run_non_p2p(rank, size):\n    tensor = torch.zeros(1)\n    req = None\n    if rank == 0:\n        tensor += 1\n        # Send the tensor to process 1\n        req = dist.isend(tensor=tensor, dst=1)\n        print(\'Rank 0 started sending\')\n    else:\n        # Receive tensor from process 0\n        req = dist.irecv(tensor=tensor, src=0)\n        print(\'Rank 1 started receiving\')\n\n    # We should not modify the sent tensor nor access the received tensor before req.wait()\n    req.wait()\n    print(\'Rank \', rank, \' has data \', tensor[0])\n\n\n"""""" Collective Communication - All reduce example """"""\n\n\ndef run_all_reduce(rank, size):\n    """"""\n    As opposed to point to point communication, collective communication\n    allow for communication patterns across all processes in a group.\n    :param rank:\n    :param size:\n    :return:\n    """"""\n\n    """""" Simple point to point communication """"""\n    group = dist.new_group([0, 1])\n    tensor = torch.ones(1)\n    dist.all_reduce(tensor, op=dist.reduce_op.SUM, group=group)\n    print(\'Rank \', rank, \' has data \', tensor[0])\n\n\n\ndef run(rank, size):\n    """"""\n    Distributed function to be run\n    :param rank:\n    :param size:\n    :return:\n    """"""\n\n    pass\n\n\ndef init_processes(rank, size, fn, backend=\'tcp\'):\n    """"""\n    Initialize the distributed environment\n    :param rank:\n    :param size:\n    :param fn:\n    :param backend:\n    :return:\n    """"""\n\n    os.environ[\'MASTER_ADDR\'] = \'127.0.0.1\'\n    os.environ[\'MASTER_PORT\'] = \'29500\'\n    # This method essentially allows processes to communicate with each other by sharing their positions\n    dist.init_process_group(backend=backend, rank=rank, world_size=size)\n    fn(rank, size)\n\n\nif __name__ == ""__main__"":\n    size = 2\n    processes = []\n    for rank in range(size):\n        p = Process(target=init_processes, args=(rank, size, run))\n        p.start()\n        processes.append(p)\n\n    for p in processes:\n        p.join()\n\n'"
doomFiles/action_space.py,0,"b'import gym\n\n# Constants\nNUM_ACTIONS = 43\nALLOWED_ACTIONS = [\n    [0, 10, 11],                                # 0 - Basic\n    [0, 10, 11, 13, 14, 15],                    # 1 - Corridor\n    [0, 14, 15],                                # 2 - DefendCenter\n    [0, 14, 15],                                # 3 - DefendLine\n    [13, 14, 15],                               # 4 - HealthGathering\n    [13, 14, 15],                               # 5 - MyWayHome\n    [0, 14, 15],                                # 6 - PredictPosition\n    [10, 11],                                   # 7 - TakeCover\n    [x for x in range(NUM_ACTIONS) if x != 33], # 8 - Deathmatch\n    [13, 14, 15],                               # 9 - MyWayHomeFixed\n    [13, 14, 15],                               # 10 - MyWayHomeFixed15\n]\n\n__all__ = [ \'ToDiscrete\', \'ToBox\' ]\n\ndef ToDiscrete(config):\n    # Config can be \'minimal\', \'constant-7\', \'constant-17\', \'full\'\n\n    class ToDiscreteWrapper(gym.Wrapper):\n        """"""\n            Doom wrapper to convert MultiDiscrete action space to Discrete\n            config:\n                - minimal - Will only use the levels\' allowed actions (+ NOOP)\n                - constant-7 - Will use the 7 minimum actions (+NOOP) to complete all levels\n                - constant-17 - Will use the 17 most common actions (+NOOP) to complete all levels\n                - full - Will use all available actions (+ NOOP)\n            list of commands:\n                - minimal:\n                    Basic:              NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT\n                    Corridor:           NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    DefendCenter        NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    DefendLine:         NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    HealthGathering:    NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    MyWayHome:          NOOP, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    PredictPosition:    NOOP, ATTACK, TURN_RIGHT, TURN_LEFT\n                    TakeCover:          NOOP, MOVE_RIGHT, MOVE_LEFT\n                    Deathmatch:         NOOP, ALL COMMANDS (Deltas are limited to [0,1] range and will not work properly)\n                - constant-7: NOOP, ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, SELECT_NEXT_WEAPON\n                - constant-17: NOOP, ATTACK, JUMP, CROUCH, TURN180, RELOAD, SPEED, STRAFE, MOVE_RIGHT, MOVE_LEFT, MOVE_BACKWARD\n                                MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, LOOK_UP, LOOK_DOWN, SELECT_NEXT_WEAPON, SELECT_PREV_WEAPON\n        """"""\n        def __init__(self, env):\n            super(ToDiscreteWrapper, self).__init__(env)\n            if config == \'minimal\':\n                allowed_actions = ALLOWED_ACTIONS[self.unwrapped.level]\n            elif config == \'constant-7\':\n                allowed_actions = [0, 10, 11, 13, 14, 15, 31]\n            elif config == \'constant-17\':\n                allowed_actions = [0, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 31, 32]\n            elif config == \'full\':\n                allowed_actions = None\n            else:\n                raise gym.error.Error(\'Invalid configuration. Valid options are ""minimal"", ""constant-7"", ""constant-17"", ""full""\')\n            self.action_space = gym.spaces.multi_discrete.DiscreteToMultiDiscrete(self.action_space, allowed_actions)\n        def _step(self, action):\n            return self.env._step(self.action_space(action))\n\n    return ToDiscreteWrapper\n\ndef ToBox(config):\n    # Config can be \'minimal\', \'constant-7\', \'constant-17\', \'full\'\n\n    class ToBoxWrapper(gym.Wrapper):\n        """"""\n            Doom wrapper to convert MultiDiscrete action space to Box\n            config:\n                - minimal - Will only use the levels\' allowed actions\n                - constant-7 - Will use the 7 minimum actions to complete all levels\n                - constant-17 - Will use the 17 most common actions to complete all levels\n                - full - Will use all available actions\n            list of commands:\n                - minimal:\n                    Basic:              ATTACK, MOVE_RIGHT, MOVE_LEFT\n                    Corridor:           ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    DefendCenter        ATTACK, TURN_RIGHT, TURN_LEFT\n                    DefendLine:         ATTACK, TURN_RIGHT, TURN_LEFT\n                    HealthGathering:    MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    MyWayHome:          MOVE_FORWARD, TURN_RIGHT, TURN_LEFT\n                    PredictPosition:    ATTACK, TURN_RIGHT, TURN_LEFT\n                    TakeCover:          MOVE_RIGHT, MOVE_LEFT\n                    Deathmatch:         ALL COMMANDS\n                - constant-7: ATTACK, MOVE_RIGHT, MOVE_LEFT, MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, SELECT_NEXT_WEAPON\n                - constant-17:  ATTACK, JUMP, CROUCH, TURN180, RELOAD, SPEED, STRAFE, MOVE_RIGHT, MOVE_LEFT, MOVE_BACKWARD\n                                MOVE_FORWARD, TURN_RIGHT, TURN_LEFT, LOOK_UP, LOOK_DOWN, SELECT_NEXT_WEAPON, SELECT_PREV_WEAPON\n        """"""\n        def __init__(self, env):\n            super(ToBoxWrapper, self).__init__(env)\n            if config == \'minimal\':\n                allowed_actions = ALLOWED_ACTIONS[self.unwrapped.level]\n            elif config == \'constant-7\':\n                allowed_actions = [0, 10, 11, 13, 14, 15, 31]\n            elif config == \'constant-17\':\n                allowed_actions = [0, 2, 3, 4, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 31, 32]\n            elif config == \'full\':\n                allowed_actions = None\n            else:\n                raise gym.error.Error(\'Invalid configuration. Valid options are ""minimal"", ""constant-7"", ""constant-17"", ""full""\')\n            self.action_space = gym.spaces.multi_discrete.BoxToMultiDiscrete(self.action_space, allowed_actions)\n        def _step(self, action):\n            return self.env._step(self.action_space(action))\n\n    return ToBoxWrapper'"
doomFiles/doom_env.py,0,"b'import os\nimport multiprocessing\n\nimport logging\n\nimport numpy as np\n\nimport gym\nfrom gym import spaces, error\nfrom gym.utils import seeding\nfrom time import sleep\n\nimport vizdoom\nfrom vizdoom import DoomGame, Mode, Button, GameVariable, ScreenFormat, ScreenResolution, Loader, doom_fixed_to_double\nfrom vizdoom import ViZDoomUnexpectedExitException, ViZDoomErrorException\n\nlogger = logging.getLogger(__name__)\n\n# Arguments:\nRANDOMIZE_MAPS = 80  # 0 means load default, otherwise randomly load in the id mentioned\nNO_MONSTERS = True  # remove monster spawning\n\n# Constants\nNUM_ACTIONS = 43\nNUM_LEVELS = 9\nCONFIG = 0\nSCENARIO = 1\nMAP = 2\nDIFFICULTY = 3\nACTIONS = 4\nMIN_SCORE = 5\nTARGET_SCORE = 6\n\n# Format (config, scenario, map, difficulty, actions, min, target)\nDOOM_SETTINGS = [\n    [\'basic.cfg\', \'basic.wad\', \'map01\', 5, [0, 10, 11], -485, 10],                               # 0 - Basic\n    [\'deadly_corridor.cfg\', \'deadly_corridor.wad\', \'\', 1, [0, 10, 11, 13, 14, 15], -120, 1000],  # 1 - Corridor\n    [\'defend_the_center.cfg\', \'defend_the_center.wad\', \'\', 5, [0, 14, 15], -1, 10],              # 2 - DefendCenter\n    [\'defend_the_line.cfg\', \'defend_the_line.wad\', \'\', 5, [0, 14, 15], -1, 15],                  # 3 - DefendLine\n    [\'health_gathering.cfg\', \'health_gathering.wad\', \'map01\', 5, [13, 14, 15], 0, 1000],         # 4 - HealthGathering\n    [\'my_way_home.cfg\', \'my_way_home_dense.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],                     # 5 - MyWayHome\n    [\'predict_position.cfg\', \'predict_position.wad\', \'map01\', 3, [0, 14, 15], -0.075, 0.5],      # 6 - PredictPosition\n    [\'take_cover.cfg\', \'take_cover.wad\', \'map01\', 5, [10, 11], 0, 750],                          # 7 - TakeCover\n    [\'deathmatch.cfg\', \'deathmatch.wad\', \'\', 5, [x for x in range(NUM_ACTIONS) if x != 33], 0, 20], # 8 - Deathmatch\n    [\'my_way_home.cfg\', \'my_way_home_sparse.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],               # 9 - MyWayHomeFixed\n    [\'my_way_home.cfg\', \'my_way_home_verySparse.wad\', \'\', 5, [13, 14, 15], -0.22, 0.5],               # 10 - MyWayHomeFixed15\n]\n\n\n# Singleton pattern\nclass DoomLock:\n    class __DoomLock:\n        def __init__(self):\n            self.lock = multiprocessing.Lock()\n    instance = None\n    def __init__(self):\n        if not DoomLock.instance:\n            DoomLock.instance = DoomLock.__DoomLock()\n    def get_lock(self):\n        return DoomLock.instance.lock\n\n\n\nclass DoomEnv(gym.Env):\n    metadata = {\'render.modes\': [\'human\', \'rgb_array\'], \'video.frames_per_second\': 35}\n\n    def __init__(self, level):\n        self.previous_level = -1\n        self.level = level\n        self.game = DoomGame()\n        self.loader = Loader()\n        self.doom_dir = os.path.dirname(os.path.abspath(__file__))\n        self._mode = \'algo\'  # \'algo\' or \'human\'\n        self.no_render = False  # To disable double rendering in human mode\n        self.viewer = None\n        self.is_initialized = False  # Indicates that reset() has been called\n        self.curr_seed = 0\n        self.lock = (DoomLock()).get_lock()\n        self.action_space = spaces.MultiDiscrete([[0, 1]] * 38 + [[-10, 10]] * 2 + [[-100, 100]] * 3)\n        self.allowed_actions = list(range(NUM_ACTIONS))\n        self.screen_height = 480\n        self.screen_width = 640\n        self.screen_resolution = ScreenResolution.RES_640X480\n        self.observation_space = spaces.Box(low=0, high=255, shape=(self.screen_height, self.screen_width, 3))\n        self._seed()\n        self._configure()\n\n    def _configure(self, lock=None, **kwargs):\n        if \'screen_resolution\' in kwargs:\n            logger.warn(\n                \'Deprecated - Screen resolution must now be set using a wrapper. See documentation for details.\')\n        # Multiprocessing lock\n        if lock is not None:\n            self.lock = lock\n\n    # Loading a level\n    def _load_level(self):\n        # Closing the level if it is initialized\n        if self.is_initialized:\n            self.is_initialized = False\n            self.game.close()\n            self.game = DoomGame()\n\n        # Customizing level\n        if getattr(self, \'_customize_game\', None) is not None and callable(self._customize_game):\n            self.level = -1\n            self._customize_game()\n\n\n        else:\n            # Loading Paths\n            if not self.is_initialized:\n                self.game.set_vizdoom_path(self.loader.get_vizdoom_path())\n                self.game.set_doom_game_path(self.loader.get_freedoom_path())\n\n\n            # Common Settings\n            self.game.load_config(os.path.join(self.doom_dir, \'assets/%s\' % DOOM_SETTINGS[self.level][CONFIG]))\n            self.game.set_doom_scenario_path(self.loader.get_scenario_path(DOOM_SETTINGS[self.level][SCENARIO]))\n\n            # Random Map Selection\n            if DOOM_SETTINGS[self.level][MAP] != \'\':\n                if RANDOMIZE_MAPS > 0 and \'labyrinth\' in DOOM_SETTINGS[self.level][CONFIG].lower():\n                    if \'fix\' in DOOM_SETTINGS[self.level][SCENARIO].lower():\n                        # mapId = \'map%02d\'%np.random.randint(1, 23)\n                        mapId = \'map%02d\' % np.random.randint(4, 8)\n                    else:\n                        mapId = \'map%02d\' % np.random.randint(1, RANDOMIZE_MAPS + 1)\n                    print(\'\\t=> Special Config: Randomly Loading Maps. MapID = \' + mapId)\n                    self.game.set_doom_map(mapId)\n                else:\n                    print(\'\\t=> Default map loaded. MapID = \' + DOOM_SETTINGS[self.level][MAP])\n                    self.game.set_doom_map(DOOM_SETTINGS[self.level][MAP])\n\n            # Setting Vizdoom map settings\n\n            self.game.set_doom_skill(DOOM_SETTINGS[self.level][DIFFICULTY])\n            self.allowed_actions = DOOM_SETTINGS[self.level][ACTIONS]\n            self.game.set_screen_resolution(self.screen_resolution)\n\n        self.previous_level = self.level\n        self._closed = False\n\n        # Algo mode\n        if \'human\' != self._mode:\n            if NO_MONSTERS:\n                print(\'\\t=> Special Config: Monsters Removed.\')\n                self.game.add_game_args(\'-nomonsters 1\')\n            self.game\n            self.game.set_window_visible(False)\n            self.game.set_mode(Mode.PLAYER)\n            self.no_render = False\n            try:\n                with self.lock:\n                    self.game.init()\n            except (ViZDoomUnexpectedExitException, ViZDoomErrorException):\n                raise error.Error(\n                    \'VizDoom exited unexpectedly. This is likely caused by a missing multiprocessing lock. \' +\n                    \'To run VizDoom across multiple processes, you need to pass a lock when you configure the env \' +\n                    \'[e.g. env.configure(lock=my_multiprocessing_lock)], or create and close an env \' +\n                    \'before starting your processes [e.g. env = gym.make(""DoomBasic-v0""); env.close()] to cache a \' +\n                    \'singleton lock in memory.\')\n            self._start_episode()\n            self.is_initialized = True\n            return self.game.get_state().image_buffer.copy()\n\n        # Human mode\n        else:\n            if NO_MONSTERS:\n                print(\'\\t=> Special Config: Monsters Removed.\')\n                self.game.add_game_args(\'-nomonsters 1\')\n            self.game.add_game_args(\'+freelook 1\')\n            self.game.set_window_visible(True)\n            self.game.set_mode(Mode.SPECTATOR)\n            self.no_render = True\n            with self.lock:\n                self.game.init()\n            self._start_episode()\n            self.is_initialized = True\n            self._play_human_mode()\n            return np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n\n    def _start_episode(self):\n        if self.curr_seed > 0:\n            self.game.set_seed(self.curr_seed)\n            self.curr_seed = 0\n        self.game.new_episode()\n        return\n\n    def _play_human_mode(self):\n        while not self.game.is_episode_finished():\n            self.game.advance_action()\n            state = self.game.get_state()\n            total_reward = self.game.get_total_reward()\n            info = self._get_game_variables(state.game_variables)\n            info[""TOTAL_REWARD""] = round(total_reward, 4)\n            print(\'===============================\')\n            print(\'State: #\' + str(state.number))\n            print(\'Action: \\t\' + str(self.game.get_last_action()) + \'\\t (=> only allowed actions)\')\n            print(\'Reward: \\t\' + str(self.game.get_last_reward()))\n            print(\'Total Reward: \\t\' + str(total_reward))\n            print(\'Variables: \\n\' + str(info))\n            sleep(0.02857)  # 35 fps = 0.02857 sleep between frames\n        print(\'===============================\')\n        print(\'Done\')\n        return\n\n    # Environment step function according to the action\n    def _step(self, action):\n        if NUM_ACTIONS != len(action):\n            logger.warn(\'Doom action list must contain %d items. Padding missing items with 0\' % NUM_ACTIONS)\n            old_action = action\n            action = [0] * NUM_ACTIONS\n            for i in range(len(old_action)):\n                action[i] = old_action[i]\n\n        # action is a list of numbers but DoomGame.make_action expects a list of ints\n        if len(self.allowed_actions) > 0:\n            list_action = [int(action[action_idx]) for action_idx in self.allowed_actions]\n        else:\n            list_action = [int(x) for x in action]\n\n        # Try, except block\n        try:\n            reward = self.game.make_action(list_action)\n            state = self.game.get_state()\n            info = self._get_game_variables(state.game_variables)\n            info[""TOTAL_REWARD""] = round(self.game.get_total_reward(), 4)\n\n            if self.game.is_episode_finished():\n                is_finished = True\n                return np.zeros(shape=self.observation_space.shape, dtype=np.uint8), reward, is_finished, info\n            else:\n                is_finished = False\n                return state.image_buffer.copy(), reward, is_finished, info\n\n        except vizdoom.ViZDoomIsNotRunningException:\n            return np.zeros(shape=self.observation_space.shape, dtype=np.uint8), 0, True, {}\n\n    # State Reset\n    def _reset(self):\n        if self.is_initialized and not self._closed:\n            self._start_episode()\n            image_buffer = self.game.get_state().image_buffer\n            if image_buffer is None:\n                raise error.Error(\n                    \'VizDoom incorrectly initiated. This is likely caused by a missing multiprocessing lock. \' +\n                    \'To run VizDoom across multiple processes, you need to pass a lock when you configure the env \' +\n                    \'[e.g. env.configure(lock=my_multiprocessing_lock)], or create and close an env \' +\n                    \'before starting your processes [e.g. env = gym.make(""DoomBasic-v0""); env.close()] to cache a \' +\n                    \'singleton lock in memory.\')\n            return image_buffer.copy()\n        else:\n            return self._load_level()\n\n    # Game State Rendering\n    def _render(self, mode=\'human\', close=False):\n        if close:\n            if self.viewer is not None:\n                self.viewer.close()\n                self.viewer = None      # If we don\'t None out this reference pyglet becomes unhappy\n            return\n        try:\n            if \'human\' == mode and self.no_render:\n                return\n            state = self.game.get_state()\n            img = state.image_buffer\n            # VizDoom returns None if the episode is finished, let\'s make it\n            # an empty image so the recorder doesn\'t stop\n            if img is None:\n                img = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n            if mode == \'rgb_array\':\n                return img\n            elif mode is \'human\':\n                from gym.envs.classic_control import rendering\n                if self.viewer is None:\n                    self.viewer = rendering.SimpleImageViewer()\n                self.viewer.imshow(img)\n        except vizdoom.ViZDoomIsNotRunningException:\n            pass  # Doom has been closed\n\n    # Close game processes\n    def _close(self):\n        # Lock required for VizDoom to close processes properly\n        with self.lock:\n            self.game.close()\n\n    # Get the random seed\n    def _seed(self, seed=None):\n        self.curr_seed = seeding.hash_seed(seed) % 2 ** 32\n        return [self.curr_seed]\n\n    # Get the game state variables\n    def _get_game_variables(self, state_variables):\n        info = {\n            ""LEVEL"": self.level\n        }\n        if state_variables is None:\n            return info\n        info[\'KILLCOUNT\'] = state_variables[0]\n        info[\'ITEMCOUNT\'] = state_variables[1]\n        info[\'SECRETCOUNT\'] = state_variables[2]\n        info[\'FRAGCOUNT\'] = state_variables[3]\n        info[\'HEALTH\'] = state_variables[4]\n        info[\'ARMOR\'] = state_variables[5]\n        info[\'DEAD\'] = state_variables[6]\n        info[\'ON_GROUND\'] = state_variables[7]\n        info[\'ATTACK_READY\'] = state_variables[8]\n        info[\'ALTATTACK_READY\'] = state_variables[9]\n        info[\'SELECTED_WEAPON\'] = state_variables[10]\n        info[\'SELECTED_WEAPON_AMMO\'] = state_variables[11]\n        info[\'AMMO1\'] = state_variables[12]\n        info[\'AMMO2\'] = state_variables[13]\n        info[\'AMMO3\'] = state_variables[14]\n        info[\'AMMO4\'] = state_variables[15]\n        info[\'AMMO5\'] = state_variables[16]\n        info[\'AMMO6\'] = state_variables[17]\n        info[\'AMMO7\'] = state_variables[18]\n        info[\'AMMO8\'] = state_variables[19]\n        info[\'AMMO9\'] = state_variables[20]\n        info[\'AMMO0\'] = state_variables[21]\n        info[\'POSITION_X\'] = doom_fixed_to_double(self.game.get_game_variable(GameVariable.USER1))\n        info[\'POSITION_Y\'] = doom_fixed_to_double(self.game.get_game_variable(GameVariable.USER2))\n        return info\n\n\nclass MetaDoomEnv(DoomEnv):\n\n    def __init__(self, average_over=10, passing_grade=600, min_tries_for_avg=5):\n        super(MetaDoomEnv, self).__init__(0)\n        self.average_over = average_over\n        self.passing_grade = passing_grade\n        self.min_tries_for_avg = min_tries_for_avg              # Need to use at least this number of tries to calc avg\n        self.scores = [[]] * NUM_LEVELS\n        self.locked_levels = [True] * NUM_LEVELS                # Locking all levels but the first\n        self.locked_levels[0] = False\n        self.total_reward = 0\n        self.find_new_level = False                             # Indicates that we need a level change\n        self._unlock_levels()\n\n    def _play_human_mode(self):\n        while not self.game.is_episode_finished():\n            self.game.advance_action()\n            state = self.game.get_state()\n            episode_reward = self.game.get_total_reward()\n            (reward, self.total_reward) = self._calculate_reward(episode_reward, self.total_reward)\n            info = self._get_game_variables(state.game_variables)\n            info[""SCORES""] = self.get_scores()\n            info[""TOTAL_REWARD""] = round(self.total_reward, 4)\n            info[""LOCKED_LEVELS""] = self.locked_levels\n            print(\'===============================\')\n            print(\'State: #\' + str(state.number))\n            print(\'Action: \\t\' + str(self.game.get_last_action()) + \'\\t (=> only allowed actions)\')\n            print(\'Reward: \\t\' + str(reward))\n            print(\'Total Reward: \\t\' + str(self.total_reward))\n            print(\'Variables: \\n\' + str(info))\n            sleep(0.02857)  # 35 fps = 0.02857 sleep between frames\n        print(\'===============================\')\n        print(\'Done\')\n        return\n\n    def _get_next_level(self):\n        # Finds the unlocked level with the lowest average\n        averages = self.get_scores()\n        lowest_level = 0                          # Defaulting to first level\n        lowest_score = 1001\n        for i in range(NUM_LEVELS):\n            if not self.locked_levels[i]:\n                if averages[i] < lowest_score:\n                    lowest_level = i\n                    lowest_score = averages[i]\n        return lowest_level\n\n    def _unlock_levels(self):\n        averages = self.get_scores()\n        for i in range(NUM_LEVELS - 2, -1, -1):\n            if self.locked_levels[i + 1] and averages[i] >= self.passing_grade:\n                self.locked_levels[i + 1] = False\n        return\n\n    def _start_episode(self):\n        if 0 == len(self.scores[self.level]):\n            self.scores[self.level] = [0] * self.min_tries_for_avg\n        else:\n            self.scores[self.level].insert(0, 0)\n            self.scores[self.level] = self.scores[self.level][:self.min_tries_for_avg]\n        self.is_new_episode = True\n        return super(MetaDoomEnv, self)._start_episode()\n\n    def change_level(self, new_level=None):\n        if new_level is not None and self.locked_levels[new_level] == False:\n            self.find_new_level = False\n            self.level = new_level\n            self.reset()\n        else:\n            self.find_new_level = False\n            self.level = self._get_next_level()\n            self.reset()\n        return\n\n    def _get_standard_reward(self, episode_reward):\n        # Returns a standardized reward for an episode (i.e. between 0 and 1,000)\n        min_score = float(DOOM_SETTINGS[self.level][MIN_SCORE])\n        target_score = float(DOOM_SETTINGS[self.level][TARGET_SCORE])\n        max_score = min_score + (target_score - min_score) / 0.99           # Target is 99th percentile (Scale 0-1000)\n        std_reward = round(1000 * (episode_reward - min_score) / (max_score - min_score), 4)\n        std_reward = min(1000, std_reward)                                  # Cannot be more than 1,000\n        std_reward = max(0, std_reward)                                     # Cannot be less than 0\n        return std_reward\n\n    def get_total_reward(self):\n        # Returns the sum of the average of all levels\n        total_score = 0\n        passed_levels = 0\n        for i in range(NUM_LEVELS):\n            if len(self.scores[i]) > 0:\n                level_total = 0\n                level_count = min(len(self.scores[i]), self.average_over)\n                for j in range(level_count):\n                    level_total += self.scores[i][j]\n                level_average = level_total / level_count\n                if level_average >= 990:\n                    passed_levels += 1\n                total_score += level_average\n        # Bonus for passing all levels (50 * num of levels)\n        if NUM_LEVELS == passed_levels:\n            total_score += NUM_LEVELS * 50\n        return round(total_score, 4)\n\n    def _calculate_reward(self, episode_reward, prev_total_reward):\n        # Calculates the action reward and the new total reward\n        std_reward = self._get_standard_reward(episode_reward)\n        self.scores[self.level][0] = std_reward\n        total_reward = self.get_total_reward()\n        reward = total_reward - prev_total_reward\n        return reward, total_reward\n\n    def get_scores(self):\n        # Returns a list with the averages per level\n        averages = [0] * NUM_LEVELS\n        for i in range(NUM_LEVELS):\n            if len(self.scores[i]) > 0:\n                level_total = 0\n                level_count = min(len(self.scores[i]), self.average_over)\n                for j in range(level_count):\n                    level_total += self.scores[i][j]\n                level_average = level_total / level_count\n                averages[i] = round(level_average, 4)\n        return averages\n\n    def _reset(self):\n        # Reset is called on first step() after level is finished\n        # or when change_level() is called. Returning if neither have been called to\n        # avoid resetting the level twice\n        if self.find_new_level:\n            return\n\n        if self.is_initialized and not self._closed and self.previous_level == self.level:\n            self._start_episode()\n            return self.game.get_state().image_buffer.copy()\n        else:\n            return self._load_level()\n\n    def _step(self, action):\n        # Changing level\n        if self.find_new_level:\n            self.change_level()\n\n        if \'human\' == self._mode:\n            self._play_human_mode()\n            obs = np.zeros(shape=self.observation_space.shape, dtype=np.uint8)\n            reward = 0\n            is_finished = True\n            info = self._get_game_variables(None)\n        else:\n            obs, step_reward, is_finished, info = super(MetaDoomEnv, self)._step(action)\n            reward, self.total_reward = self._calculate_reward(self.game.get_total_reward(), self.total_reward)\n            # First step() after new episode returns the entire total reward\n            # because stats_recorder resets the episode score to 0 after reset() is called\n            if self.is_new_episode:\n                reward = self.total_reward\n\n        self.is_new_episode = False\n        info[""SCORES""] = self.get_scores()\n        info[""TOTAL_REWARD""] = round(self.total_reward, 4)\n        info[""LOCKED_LEVELS""] = self.locked_levels\n\n        # Indicating new level required\n        if is_finished:\n            self._unlock_levels()\n            self.find_new_level = True\n\n        return obs, reward, is_finished, info\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
models/A2C.py,15,"b'""""""\nThis class consists of the implementation of advantage actor critic network\n""""""\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport numpy as np\nimport math\nimport torch.optim as opt\nfrom Utils import utils\n\nclass A2C(object):\n\n    """"""\n    Advantage actor critic\n    """"""\n\n    def __init__(self, num_hidden_units, input_dim, num_actions, num_q_val,\n                 observation_dim, goal_dim,\n                 batch_size, use_cuda, gamma, random_seed,\n                 actor_optimizer, critic_optimizer,\n                 actor_learning_rate, critic_learning_rate,\n                 n_games, n_steps, env,\n                 loss_function, non_conv=True,\n                 num_conv_layers=None, num_pool_layers=None,\n                 conv_kernel_size=None, img_height=None, img_width=None,\n                 input_channels=None):\n\n\n        """"""\n\n        :param num_hidden_units:\n        :param input_dim:\n        :param num_actions:\n        :param num_q_val:\n        :param observation_dim:\n        :param goal_dim:\n        :param batch_size:\n        :param use_cuda:\n        :param gamma:\n        :param random_seed:\n        :param actor_optimizer:\n        :param critic_optimizer:\n        :param actor_learning_rate:\n        :param critic_learning_rate:\n        :param n_games:\n        :param n_steps:\n        :param env:\n        :param loss_function:\n        :param non_conv:\n        :param num_conv_layers:\n        :param num_pool_layers:\n        :param conv_kernel_size:\n        :param img_height:\n        :param img_width:\n        :param input_channels:\n        """"""\n\n        self.num_hidden_units = num_hidden_units\n        self.input_dim = input_dim\n        self.non_conv = non_conv\n        self.num_actions = num_actions\n        self.num_q = num_q_val\n        self.obs_dim = observation_dim\n        self.goal_dim = goal_dim\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.cuda = use_cuda\n        self.env = env\n        self.gamma = gamma\n        self.n_games = n_games\n        self.n_steps = n_steps\n        self.random_seed = random_seed\n        self.actor_optim = actor_optimizer\n        self.critic_optim = critic_optimizer\n        self.actor_lr = actor_learning_rate\n        self.critic_lr = critic_learning_rate\n        self.criterion = loss_function\n\n        if self.random_seed is not None:\n            self.seed(self.random_seed)\n\n        # Convolution Parameters\n        self.num_conv = num_conv_layers\n        self.pool = num_pool_layers\n        self.im_height = img_height\n        self.im_width = img_width\n        self.conv_kernel_size = conv_kernel_size\n        self.input_channels = input_channels\n\n\n        if non_conv:\n            self.actor = ActorNonConvNetwork(num_hidden_layers=self.num_hidden_units, output_action=self.num_actions,\n                                             input=self.input_dim)\n\n            self.critic = CriticNonConvNetwork(num_hidden_layers=self.num_hidden_units, output_q_value=self.num_q,\n                                               input=self.input_dim, action_dim=self.num_actions)\n\n        else:\n            self.actor = ActorNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                      input_channels=self.input_channels, output_action=self.num_actions,\n                                      dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                      IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width)\n            self.critic = CriticNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                      input_channels=self.input_channels, action_dim=self.num_actions,\n                                      dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                      IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width, output_q_value=self.num_q)\n\n\n        if self.cuda:\n            self.to_cuda()\n\n        # Create the optimizers for the actor and critic using the corresponding learning rate\n        actor_parameters = self.actor.parameters()\n        critic_parameters = self.critic.parameters()\n\n        self.actor_optim = opt.Adam(actor_parameters, lr=self.actor_lr)\n        self.critic_optim = opt.Adam(critic_parameters, lr=self.critic_lr)\n\n    def to_cuda(self):\n        self.target_actor = self.target_actor.cuda()\n        self.target_critic = self.target_critic.cuda()\n        self.actor = self.actor.cuda()\n        self.critic = self.critic.cuda()\n\n    def save_model(self, output):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the actor and critic"")\n        torch.save(\n            self.actor.state_dict(),\n            \'{}/actor.pkl\'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            \'{}/critic.pkl\'.format(output)\n        )\n\n    def seed(self, s):\n        """"""\n        Setting the random seed for a particular training iteration\n        :param s:\n        :return:\n        """"""\n        torch.manual_seed(s)\n        if self.cuda:\n            torch.cuda.manual_seed(s)\n\n\n    def collect_minibatch(self, finished_games):\n        """"""\n        Collect a mini-batch of data by moving around in the environment\n        :return: A minibatch of simulations in the open ai gym environment\n        """"""\n        state = self.env.reset()\n\n        states, actions, rewards, dones  = [], [], [], []\n\n        # Gather training data\n        for i in range(self.n_steps):\n            state = Variable(torch.from_numpy(state).float().unsqueeze(0))\n            state = utils.to_tensor(state, use_cuda=self.cuda)\n\n            action_probs = self.actor(state)\n            action = action_probs.multinomial().data[0][0]\n            next_state, reward, done, _ = self.env.step(action)\n\n            states.append(states)\n            actions.append(actions)\n            rewards.append(rewards)\n            dones.append(done)\n\n            if done:\n                # Game over\n                state = self.env.reset()\n                finished_games += 1\n            else:\n                state = next_state\n\n        return states, actions, rewards, dones, finished_games\n\n\n    def calc_true_state_values(self, states, rewards, dones):\n        """"""\n        Calculate true state values working backwards\n\n        :param rewards: Collected rewards (sampled from the current batch)\n        :param dones: Collected dones (sampled from the current batch)\n        :return: True state values\n        """"""\n\n        R = []\n        rewards.reverse()\n\n        # If we happen to end at the terminal state, set next return to zero\n        if dones[-1] == True:\n            next_return = 0\n\n        # If not terminal state, bootstrap v(s) using our critic\n        else:\n            s = torch.from_numpy(states[-1]).float().unsqueeze(0)\n            s = utils.to_tensor(s, use_cuda=self.cuda)\n            next_return = self.critic(Variable(s)).data[0][0]\n\n        # Backup from last state to calculate ""true"" returns for each state in the set\n        R.append(next_return)\n        dones.reverse()\n\n        # Iterate from the second last state\n        for r in range(1, len(rewards)):\n            if not dones[r]:\n                # If this is not the final state for the episode, then calculate the expected reward\n                # for this state using the bootstrapped value calculated by the critic\n                current_return = rewards[r] + next_return * self.gamma\n            else:\n                # This is the final state so the current return must be zero\n                current_return = 0\n            R.append(current_return)\n            # Update the next return with the current return and backtrack\n            next_return = current_return\n\n        # Reverse the R vector\n        R.reverse()\n        R = utils.to_tensor(R, use_cuda=self.cuda)\n        state_values_true = Variable(torch.FloatTensor(R)).unsqueeze(1)\n\n        return state_values_true\n\n    # Training procedure\n    def train(self):\n        finished_games = 0\n        actor_losses = []\n        critic_losses = []\n        while finished_games < self.n_games:\n\n            # Collect a minibatch of data\n            states, actions, rewards, dones, finished_games  = \\\n                self.collect_minibatch(finished_games=finished_games)\n\n            # Calculate the ground truth labels\n            true_state_values = self.calc_true_state_values(states, rewards, dones)\n            states = utils.to_tensor(states, use_cuda=self.cuda)\n            s = Variable(torch.FloatTensor(states))\n            action_probs = self.actor(s)\n            state_values = self.critic(s)\n            action_log_probs = action_probs.log()\n\n            # Choose the actions with maximum log probability\n            actions = utils.to_tensor(actions, use_cuda=self.cuda)\n            a = Variable(torch.LongTensor(actions).view(-1, 1))\n            chosen_action_log_probs = action_log_probs.gather(1, a)\n\n\n            # Compute the TD error\n            advantages = true_state_values - state_values\n\n            # Compute the entropy - (This is used for exploration)\n            entropy = (action_probs * action_log_probs).sum(1).mean()\n            action_gain = (chosen_action_log_probs * advantages).mean()\n            self.critic_optim.zero_grad()\n            value_loss = advantages.pow(2).mean()\n            value_loss.backward()\n            # Clip the gradient to avoid exploding gradients\n            nn.utils.clip_grad_norm(self.critic.parameters(), 0.5)\n            self.critic_optim.step()\n\n            self.actor_optim.zero_grad()\n            # Maximize the log probability for the best possible actions\n            actor_loss = -action_gain - 0.0001*entropy\n            actor_loss.backward()\n            # Clip the gradient to avoid exploding gradients\n            nn.utils.clip_grad_norm(self.actor.parameters(), 0.5)\n            self.actor_optim.step()\n\n            # Book Keeping\n            actor_losses.append(actor_loss)\n            critic_losses.append(value_loss)\n\n        return actor_losses, critic_losses\n\n\n    #Test the model\n    def test(self):\n        score = 0\n        done = False\n        state = self.env.reset()\n        global action_probs\n        while not done:\n            score += 1\n            s = torch.from_numpy(state).float().unsqueeze(0)\n            s = utils.to_tensor(s, use_cuda=self.cuda)\n            action_probs = self.actor(Variable(s))\n\n            _, action_index = action_probs.max(1)\n            action = action_index.data[0]\n            next_state, reward, done, thing = self.env.step(action)\n            state = next_state\n        return score\n\n\n\n\ndef fanin_init(size, fanin=None):\n    fanin = fanin or size[0]\n    v = 1. / np.sqrt(fanin)\n    return torch.Tensor(size).uniform_(-v, v)\n\n\nclass ActorNetwork(nn.Module):\n    # The actor network takes the state as input and outputs an action\n    # The actor network is used to approximate the argmax action in a continous action space\n    # The actor network in the case of a discrete action space is just argmax_a(Q(s,a))\n\n    def __init__(self, num_conv_layers, conv_kernel_size, input_channels, output_action, dense_layer,\n                 pool_kernel_size, IMG_HEIGHT, IMG_WIDTH):\n        super(ActorNetwork, self).__init__()\n        self.num_conv_layers = num_conv_layers\n        self.conv_kernel = conv_kernel_size\n        self.input_channels = input_channels\n        self.output_action = output_action\n        self.dense_layer = dense_layer\n        self.pool_kernel_size = pool_kernel_size\n        self.im_height = IMG_HEIGHT\n        self.im_width = IMG_WIDTH\n\n        # Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn3 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        # Fully connected layer\n        self.fully_connected_layer = nn.Linear(234432, self.dense_layer)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.output_layer = nn.Linear(self.dense_layer, output_action)\n\n        # Weight initialization from a uniform gaussian distribution\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n        x = x.view(x.size(0), -1)\n        x = self.fully_connected_layer(x)\n        x = self.relu4(x)\n        out = self.output_layer(x)\n        return out\n\n\n# For non image state space\nclass ActorNonConvNetwork(nn.Module):\n    def __init__(self, num_hidden_layers, output_action, input):\n        super(ActorNonConvNetwork, self).__init__()\n        self.num_hidden_layers = num_hidden_layers\n        self.input = input\n        self.output_action = output_action\n        self.init_w = 3e-3\n\n        #Dense Block\n        self.dense_1 = nn.Linear(self.input, self.num_hidden_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.dense_2 = nn.Linear(self.num_hidden_layers, self.num_hidden_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.num_hidden_layers, self.output_action)\n        self.softmax = nn.Softmax()\n\n    def init_weights(self, init_w):\n        self.dense_1.weight.data = fanin_init(self.dense_1.weight.data.size())\n        self.dense_2.weight.data = fanin_init(self.dense_2.weight.data.size())\n        self.output.weight.data.uniform_(-init_w, init_w)\n\n    def forward(self, input):\n        x = self.dense_1(input)\n        x = self.relu1(x)\n        x = self.dense_2(x)\n        x = self.relu2(x)\n        output = self.output(x)\n        output = self.softmax(output)\n        return output\n\n\nclass CriticNetwork(nn.Module):\n\n    # The Critic Network basically takes the state and action as the input and outputs a q value\n    def __init__(self, num_conv_layers, conv_kernel_size, input_channels, output_q_value, dense_layer,\n                pool_kernel_size, IMG_HEIGHT, IMG_WIDTH, action_dim):\n        super(CriticNetwork, self).__init__()\n\n        self.num_conv_layers = num_conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.input_channels = input_channels\n        self.output_q_value = output_q_value\n        self.dense_layer = dense_layer\n        self.pool_kernel_size = pool_kernel_size\n        self.img_height = IMG_HEIGHT\n        self.img_width = IMG_WIDTH\n        self.action_dim = action_dim\n\n        # Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.pool_kernel_size)\n        self.bn1 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.pool_kernel_size)\n        self.bn2 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.fully_connected_layer = nn.Linear(234432, self.dense_layer)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.dense_layer, output_q_value)\n\n        # Weight initialization from a uniform gaussian distribution\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n\n    def forward(self, states, actions):\n        x = self.conv1(states)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fully_connected_layer(x)\n        x = x + actions # Adding the action input\n        x = self.relu3(x)\n        output = self.output(x)\n        return output\n\n\nclass CriticNonConvNetwork(nn.Module):\n\n    def __init__(self, num_hidden_layers, output_q_value, input,\n                 action_dim):\n        super(CriticNonConvNetwork, self).__init__()\n        # Initialize the variables\n        self.num_hidden = num_hidden_layers\n        self.output_dim = output_q_value\n        self.input = input\n        self.action_dim = action_dim\n        self.init_w = 3e-3\n\n        # Dense Block\n        self.dense1 = nn.Linear(self.input, self.num_hidden)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.hidden2 = nn.Linear(self.num_hidden + self.action_dim, self.num_hidden)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.num_hidden, self.output_dim)\n\n    def init_weights(self, init_w):\n        self.dense1.weight.data = fanin_init(self.dense1.weight.data.size())\n        self.hidden2.weight.data = fanin_init(self.hidden2.weight.data.size())\n        self.output.weight.data.uniform_(-init_w, init_w)\n\n    def forward(self, states, actions):\n        #print(states)\n        x = self.dense1(states)\n        x = self.relu1(x)\n        x = torch.cat((x, actions), dim=1)\n        x = self.hidden2(x)\n        x = self.relu3(x)\n        out = self.output(x)\n        return out\n\n\n# Combined Actor Critic\nclass ActorCritic(nn.Module):\n    def __init__(self, input_dim, num_actions, num_hidden, num_q_values):\n        super(ActorCritic, self).__init__()\n        self.linear1 = nn.Linear(input_dim, num_hidden)\n        self.linear2 = nn.Linear(num_hidden, num_hidden*2)\n        self.linear3 = nn.Linear(num_hidden*2, num_hidden)\n\n        self.actor = nn.Linear(num_hidden, num_actions)\n        self.critic = nn.Linear(num_hidden, num_q_values)\n        self.softmax = nn.Softmax()\n        self.relu = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.linear1(x)\n        x = self.relu(x)\n        x = self.linear2(x)\n        x = self.relu(x)\n        x = self.linear3(x)\n        x = self.relu(x)\n        return x\n\n    # Only the Actor head\n    def get_action_probs(self, x):\n        x = self(x)\n        action_probs = self.softmax(self.actor(x))\n        return action_probs\n\n    # Only the Critic head\n    def get_state_value(self, x):\n        x = self(x)\n        state_value = self.critic(x)\n        return state_value\n\n    # Both heads\n    def evaluate_actions(self, x):\n        x = self(x)\n        action_probs = self.softmax(self.actor(x))\n        state_values = self.critic(x)\n        return action_probs, state_values'"
models/CuriosityDrivenExploration.py,5,"b'""""""\n\nThis script contains the implementation of the model\npresented in the paper - \xef\xbb\xbfCuriosity-driven Exploration by Self-supervised Prediction\n\n\nThe agent is composed of 2 subsystems -\n1. A reward generator which outputs a curiosity driven intrinsic reward\n2. A Policy network that outputs a sequence of actions to maximize the reward\n\nReward Generator Network\n\nIntrinsic Curiosity Module\n\nThe reward generator network consists of 2 parts\n1. Inverse Dynamics Model\n2. Forward Dynamics Model\n\nThe inverse dynamics models takes in the current state and\nthe next state and tries to predict the plausible action taken.\n\nThe forward dynamics model takes in the feature representation of a state and the\naction and tries to predict the feature representation of the next state.\n\n\xef\xbb\xbfThe inverse model learns a feature space that encodes information\nrelevant for predicting the agent\xe2\x80\x99s actions only and the forward model\nmakes predictions in this feature space.\n\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nUSE_CUDA = torch.cuda.is_available()\n\n\n# Encoder for the states\nclass Encoder(nn.Module):\n\n    def __init__(self, conv_layers, conv_kernel_size,\n                 input_channels, height,\n                 width, use_batchnorm=False,\n                 ):\n        super(Encoder, self).__init__()\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.input_channels = input_channels\n        self.height = height\n        self.width = width\n        self.use_batchnorm = use_batchnorm\n\n        # Encoder Architecture\n\n        self.conv1 = nn.Conv2d(in_channels=self.input_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers)\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*2)\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Weight initialization\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n\n    def forward(self, input):\n        batch_size, _ ,_, _ = input.shape\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.lrelu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.lrelu(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.lrelu(x)\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.lrelu(x)\n\n        # Flatten the output\n        x = x.view((batch_size, -1))\n        return x\n\n\n# Inverse Dynamics model\nclass InverseModel(nn.Module):\n\n    def __init__(self, latent_dimension, action_dimension,\n                 hidden_dim):\n\n        super(InverseModel, self).__init__()\n        self.input_dim = latent_dimension\n        self.output_dim = action_dimension\n        self.hidden = hidden_dim\n\n        # Inverse Model architecture\n\n        self.linear_1 = nn.Linear(in_features=self.input_dim*2, out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Output Activation\n        self.softmax = nn.Softmax()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.linear_1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state, next_state):\n\n        # Concatenate the state and the next state\n        input = torch.cat([state, next_state], dim=-1)\n        x = self.linear_1(input)\n        x = self.lrelu(x)\n        x = self.output(x)\n\n        output = self.softmax(x)\n        return output\n\n\n# Forward Dynamics Model\nclass ForwardDynamicsModel(nn.Module):\n\n    def __init__(self, state_dim, action_dim,\n                 hidden_dim):\n\n        super(ForwardDynamicsModel, self).__init__()\n\n        self.input_dim = state_dim+action_dim\n        self.output_dim= state_dim\n        self.hidden = hidden_dim\n\n\n        # Forward Model Architecture\n\n        self.linear_1 = nn.Linear(in_features=self.input_dim, out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)\n\n        # Leaky Relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.linear_1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state, action):\n\n        # Concatenate the state and the action\n\n        # Note that the state in this case is the feature representation of the state\n\n        input = torch.cat([state, action], dim=-1)\n        x = self.linear_1(input)\n        x = self.lrelu(x)\n        output = self.output(x)\n\n        return output\n\n\nclass IntrinsicCuriosityModule(object):\n\n    def __init__(self,\n                 inverse_model,\n                 forward_dynamics_model,\n                 inverse_lr,\n                 forward_lr,\n                 num_epochs,\n                 save_path):\n\n        self.inverse_model = inverse_model\n        self.forward_dynamics_model = forward_dynamics_model\n        self.inverse_lr = inverse_lr\n        self.forward_lr = forward_lr\n        self.save_path = save_path\n\n        self.inverse_optim = optim.Adam(lr=self.inverse_lr, params=self.inverse_model.parameters())\n        self.forward_optim = optim.Adam(lr=self.forward_lr, params=self.forward_dynamics_model.parameters())\n        self.num_epochs = num_epochs\n\n    def get_inverse_dynamics_loss(self):\n        criterionID = nn.BCELoss()\n        return criterionID\n\n    def get_forward_dynamics_loss(self):\n        criterionFD = nn.MSELoss()\n        return criterionFD\n\n    def fit_batch(self, state, action, next_state, train=True):\n        # Predict the action from the current state and the next state\n        pred_action = self.inverse_model(state, next_state)\n        criterionID = self.get_inverse_dynamics_loss()\n        inverse_loss = criterionID(pred_action, action)\n        if train:\n            self.inverse_optim.zero_grad()\n            inverse_loss.backward()\n            self.inverse_optim.step()\n\n        # Predict the next state from the current state and the action\n        pred_next_state = self.forward_dynamics_model(state, action)\n        criterionFD = self.get_forward_dynamics_loss()\n        forward_loss = criterionFD(pred_next_state, next_state)\n        if train:\n            self.forward_optim.zero_grad()\n            forward_loss.backward()\n            self.forward_optim.step()\n\n        return inverse_loss, forward_loss'"
models/DDPG.py,17,"b'# This script contains the Actor and Critic classes\nimport torch.nn as nn\nimport math\nimport torch\nimport numpy as np\nimport Utils.random_process as random_process\nfrom torch.autograd import Variable\nfrom Memory import Buffer\nimport torch.optim as opt\n\n\nclass DDPG(object):\n    """"""\n    The Deep Deterministic policy gradient network\n    """"""\n\n    def __init__(self, num_hidden_units, input_dim, num_actions, num_q_val,\n                 observation_dim, goal_dim,\n                 batch_size, use_cuda, gamma, random_seed,\n                 actor_optimizer, critic_optimizer,\n                 actor_learning_rate, critic_learning_rate,\n                 loss_function, polyak_constant,\n                 buffer_capacity,non_conv=True,\n                 num_conv_layers=None, num_pool_layers=None,\n                 conv_kernel_size=None, img_height=None, img_width=None,\n                 input_channels=None):\n\n        self.num_hidden_units = num_hidden_units\n        self.non_conv = non_conv\n        self.num_actions = num_actions\n        self.num_q = num_q_val\n        self.obs_dim = observation_dim\n        self.goal_dim = goal_dim\n        self.input_dim = input_dim\n        self.batch_size = batch_size\n        self.cuda = use_cuda\n        self.gamma = gamma\n        self.seed(random_seed)\n        self.actor_optim = actor_optimizer\n        self.critic_optim = critic_optimizer\n        self.actor_lr = actor_learning_rate\n        self.critic_lr = critic_learning_rate\n        self.criterion = loss_function\n        self.tau = polyak_constant\n        self.buffer = Buffer.ReplayBuffer(capacity=buffer_capacity, seed=random_seed)\n\n        # Convolution Parameters\n        self.num_conv = num_conv_layers\n        self.pool = num_pool_layers\n        self.im_height = img_height\n        self.im_width = img_width\n        self.conv_kernel_size = conv_kernel_size\n        self.input_channels = input_channels\n\n        if non_conv:\n            self.target_actor = ActorDDPGNonConvNetwork(num_hidden_layers=num_hidden_units,\n                                                        output_action=num_actions, input=input_dim)\n\n            self.actor = ActorDDPGNonConvNetwork(num_hidden_layers=num_hidden_units,\n                                                        output_action=num_actions, input=input_dim)\n\n            self.target_critic = CriticDDPGNonConvNetwork(num_hidden_layers=num_hidden_units,\n                                                          output_q_value=num_q_val, input=input_dim, action_dim=num_actions,\n                                                          goal_dim=self.goal_dim)\n            self.critic = CriticDDPGNonConvNetwork(num_hidden_layers=num_hidden_units,\n                                                          output_q_value=num_q_val, input=input_dim, action_dim=num_actions,\n                                                   goal_dim=self.goal_dim)\n\n        else:\n            self.target_actor = ActorDDPGNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                                 input_channels=self.input_channels, output_action=self.num_actions,\n                                                 dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                                 IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width)\n\n            self.actor = ActorDDPGNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                                 input_channels=self.input_channels, output_action=self.num_actions,\n                                                 dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                                 IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width)\n\n            self.target_critic = CriticDDPGNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                                 input_channels=self.input_channels, output_q_value=self.num_q,\n                                                 dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                                 IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width)\n            self.critic = CriticDDPGNetwork(num_conv_layers=self.num_conv, conv_kernel_size=self.conv_kernel_size,\n                                                 input_channels=self.input_channels, output_q_value=self.num_q,\n                                                 dense_layer=self.num_hidden_units, pool_kernel_size=self.pool,\n                                                 IMG_HEIGHT=self.im_height, IMG_WIDTH=self.im_width)\n        if self.cuda:\n            self.target_actor = self.target_actor.cuda()\n            self.actor = self.actor.cuda()\n            self.target_critic = self.target_critic.cuda()\n            self.critic = self.critic.cuda()\n\n        # Initializing the target networks with the standard network weights\n        self.target_actor.load_state_dict(self.actor.state_dict())\n        self.target_critic.load_state_dict(self.critic.state_dict())\n\n        # Create the optimizers for the actor and critic using the corresponding learning rate\n        actor_parameters = self.actor.parameters()\n        critic_parameters = self.critic.parameters()\n\n        self.actor_optim = opt.Adam(actor_parameters, lr=self.actor_lr)\n        self.critic_optim = opt.Adam(critic_parameters, lr=self.critic_lr)\n\n        # Initialize a random exploration noise\n        self.random_noise = random_process.OrnsteinUhlenbeckActionNoise(self.num_actions)\n\n    def to_cuda(self):\n        self.target_actor = self.target_actor.cuda()\n        self.target_critic = self.target_critic.cuda()\n        self.actor = self.actor.cuda()\n        self.critic = self.critic.cuda()\n\n    def save_model(self, output):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the actor and critic"")\n        torch.save(\n            self.actor.state_dict(),\n            \'{}/actor.pkl\'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            \'{}/critic.pkl\'.format(output)\n        )\n\n    def random_action(self):\n        """"""\n        Take a random action bounded between min and max values of the action space\n        :return:\n        """"""\n        action = np.random.uniform(-1., 1., self.num_actions)\n        self.a_t = action\n\n        return action\n\n    def seed(self, s):\n        """"""\n        Setting the random seed for a particular training iteration\n        :param s:\n        :return:\n        """"""\n        torch.manual_seed(s)\n        if self.cuda:\n            torch.cuda.manual_seed(s)\n\n    def get_actors(self):\n        return {\'target\': self.target_actor, \'actor\': self.actor}\n\n    def get_critics(self):\n        return {\'target\': self.target_critic, \'critic\': self.critic}\n\n    # Get the action with an option for random exploration noise\n    def get_action(self, state, noise=True):\n        state_v = Variable(state)\n        action = self.actor(state_v)\n        if noise:\n            noise = self.random_noise\n            action = action.data.cpu().numpy()[0] + noise.sample()\n        else:\n            action = action.data.cpu().numpy()[0]\n        action = np.clip(action, -1., 1.)\n        return action\n\n    # Reset the noise\n    def reset(self):\n        self.random_noise.reset()\n\n    # Store the transition into the replay buffer\n    def store_transition(self, state, new_state, action, reward, done, success):\n        self.buffer.push(state, action, new_state, reward, done, success)\n\n    # Update the target networks using polyak averaging\n    def update_target_networks(self):\n        for target_param, param in zip(self.target_critic.parameters(), self.critic.parameters()):\n            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n\n        for target_param, param in zip(self.target_actor.parameters(), self.actor.parameters()):\n            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n\n    # Calculate the Temporal Difference Error\n    def calc_td_error(self, transition):\n        """"""\n        Calculates the td error against the bellman target\n        :return:\n        """"""\n        # Calculate the TD error only for the particular transition\n\n        # Get the separate values from the named tuple\n        state, new_state, reward, success, action, done = transition\n\n        state = Variable(state)\n        new_state = Variable(new_state)\n        reward = Variable(reward)\n        action = Variable(action)\n        done = Variable(done)\n\n        if self.cuda:   \n            state = state.cuda()\n            action = action.cuda()\n            reward = reward.cuda()\n            new_state = new_state.cuda()\n            done = done.cuda()\n\n        new_action = self.target_actor(new_state)\n        next_Q_value = self.target_critic(new_state, new_action)\n        # Find the Q-value for the action according to the target actior network\n        # We do this because calculating max over a continuous action space is intractable\n        next_Q_value.volatile = False\n        next_Q_value = torch.squeeze(next_Q_value, dim=1)\n        next_Q_value = next_Q_value * (1 - done)\n        y = reward + self.gamma * next_Q_value\n\n        outputs = self.critic(state, action)\n        td_loss = self.criterion(outputs, y)\n        return td_loss\n\n    # Train the networks\n    def fit_batch(self):\n        # Sample mini-batch from the buffer uniformly or using prioritized experience replay\n\n        # If the size of the buffer is less than batch size then return\n        if self.buffer.get_buffer_size() < self.batch_size:\n            return None, None\n\n        transitions = self.buffer.sample_batch(self.batch_size)\n        batch = Buffer.Transition(*zip(*transitions))\n\n        # Get the separate values from the named tuple\n        states = batch.state\n        new_states = batch.next_state\n        actions = batch.action\n        rewards = batch.reward\n        dones = batch.done\n\n        #actions = list(actions)\n        rewards = list(rewards)\n        dones = list(dones)\n\n        states = Variable(torch.cat(states))\n        new_states = Variable(torch.cat(new_states), volatile=True)\n        actions = Variable(torch.cat(actions))\n        rewards = Variable(torch.cat(rewards))\n        dones = Variable(torch.cat(dones))\n\n        if self.cuda:\n            states = states.cuda()\n            actions = actions.cuda()\n            rewards = rewards.cuda()\n            new_states = new_states.cuda()\n            dones = dones.cuda()\n\n        # Step 2: Compute the target values using the target actor network and target critic network\n        # Compute the Q-values given the current state ( in this case it is the new_states)\n        #with torch.no_grad():\n\n        new_action = self.target_actor(new_states)\n        new_action.volatile = True\n        next_Q_values = self.target_critic(new_states, new_action)\n        # Find the Q-value for the action according to the target actior network\n        # We do this because calculating max over a continuous action space is intractable\n        # next_Q_values.volatile = False\n        next_Q_values = torch.squeeze(next_Q_values, dim=1)\n        next_Q_values = next_Q_values * (1 - dones)\n        next_Q_values.volatile = False\n        y = rewards + self.gamma*next_Q_values\n\n        # Zero the optimizer gradients\n        self.actor_optim.zero_grad()\n        self.critic_optim.zero_grad()\n\n        # Forward pass\n        outputs = self.critic(states, actions)\n        loss = self.criterion(outputs, y)\n        loss.backward()\n        # Clamp the gradients to avoid vanishing gradient problem\n        for param in self.critic.parameters():\n            param.grad.data.clamp_(-1, 1)\n        self.critic_optim.step()\n\n        # Updating the actor policy\n        policy_loss = -1 * self.critic(states, self.actor(states))\n        policy_loss = policy_loss.mean()\n        policy_loss.backward()\n        # Clamp the gradients to avoid the vanishing gradient problem\n        for param in self.actor.parameters():\n            param.grad.data.clamp_(-1, 1)\n        self.actor_optim.step()\n\n        return loss, policy_loss\n\n\ndef fanin_init(size, fanin=None):\n    fanin = fanin or size[0]\n    v = 1. / np.sqrt(fanin)\n    return torch.Tensor(size).uniform_(-v, v)\n\n\nclass ActorDDPGNetwork(nn.Module):\n    # The actor network takes the state as input and outputs an action\n    # The actor network is used to approximate the argmax action in a continous action space\n    # The actor network in the case of a discrete action space is just argmax_a(Q(s,a))\n\n    def __init__(self, num_conv_layers, conv_kernel_size, input_channels, output_action, dense_layer,\n                 pool_kernel_size, IMG_HEIGHT, IMG_WIDTH):\n        super(ActorDDPGNetwork, self).__init__()\n        self.num_conv_layers = num_conv_layers\n        self.conv_kernel = conv_kernel_size\n        self.input_channels = input_channels\n        self.output_action = output_action\n        self.dense_layer = dense_layer\n        self.pool_kernel_size = pool_kernel_size\n        self.im_height = IMG_HEIGHT\n        self.im_width = IMG_WIDTH\n\n        # Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn1 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn3 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        # Fully connected layer\n        self.fully_connected_layer = nn.Linear(234432, self.dense_layer)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.output_layer = nn.Linear(self.dense_layer, output_action)\n\n        # Weight initialization from a uniform gaussian distribution\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n        x = x.view(x.size(0), -1)\n        x = self.fully_connected_layer(x)\n        x = self.relu4(x)\n        out = self.output_layer(x)\n        return out\n\n\n# For non image state space\nclass ActorDDPGNonConvNetwork(nn.Module):\n    def __init__(self, num_hidden_layers, output_action, input):\n        super(ActorDDPGNonConvNetwork, self).__init__()\n        self.num_hidden_layers = num_hidden_layers\n        self.input = input\n        self.output_action = output_action\n        self.init_w = 3e-3\n\n        #Dense Block\n        self.dense_1 = nn.Linear(self.input, self.num_hidden_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.dense_2 = nn.Linear(self.num_hidden_layers, self.num_hidden_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.num_hidden_layers, self.output_action)\n        self.tanh = nn.Tanh()\n\n    def init_weights(self, init_w):\n        self.dense_1.weight.data = fanin_init(self.dense_1.weight.data.size())\n        self.dense_2.weight.data = fanin_init(self.dense_2.weight.data.size())\n        self.output.weight.data.uniform_(-init_w, init_w)\n\n    def forward(self, input):\n        x = self.dense_1(input)\n        x = self.relu1(x)\n        x = self.dense_2(x)\n        x = self.relu2(x)\n        output = self.output(x)\n        output = self.tanh(output)\n        return output\n\n\nclass CriticDDPGNetwork(nn.Module):\n\n    # The Critic Network basically takes the state and action as the input and outputs a q value\n    def __init__(self, num_conv_layers, conv_kernel_size, input_channels, output_q_value, dense_layer,\n                pool_kernel_size, IMG_HEIGHT, IMG_WIDTH):\n        super(CriticDDPGNetwork, self).__init__()\n\n        self.num_conv_layers = num_conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.input_channels = input_channels\n        self.output_q_value = output_q_value\n        self.dense_layer = dense_layer\n        self.pool_kernel_size = pool_kernel_size\n        self.img_height = IMG_HEIGHT\n        self.img_width = IMG_WIDTH\n\n        # Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.pool_kernel_size)\n        self.bn1 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.pool_kernel_size)\n        self.bn2 = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.fully_connected_layer = nn.Linear(234432, self.dense_layer)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.dense_layer, output_q_value)\n\n        # Weight initialization from a uniform gaussian distribution\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n\n    def forward(self, states, actions):\n        x = self.conv1(states)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = x.view(x.size(0), -1)\n        x = self.fully_connected_layer(x)\n        x = x + actions # Adding the action input\n        x = self.relu3(x)\n        output = self.output(x)\n        return output\n\n\nclass CriticDDPGNonConvNetwork(nn.Module):\n\n    def __init__(self, num_hidden_layers, output_q_value, input,\n                 action_dim, goal_dim):\n        super(CriticDDPGNonConvNetwork, self).__init__()\n        # Initialize the variables\n        self.num_hidden = num_hidden_layers\n        self.output_dim = output_q_value\n        self.input = input\n        self.action_dim = action_dim\n        self.init_w = 3e-3\n\n        # Dense Block\n        self.dense1 = nn.Linear(self.input, self.num_hidden)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.hidden2 = nn.Linear(self.num_hidden + self.action_dim, self.num_hidden)\n        self.relu3 = nn.ReLU(inplace=True)\n        self.output = nn.Linear(self.num_hidden, self.output_dim)\n\n    def init_weights(self, init_w):\n        self.dense1.weight.data = fanin_init(self.dense1.weight.data.size())\n        self.hidden2.weight.data = fanin_init(self.hidden2.weight.data.size())\n        self.output.weight.data.uniform_(-init_w, init_w)\n\n    def forward(self, states, actions):\n        #print(states)\n        x = self.dense1(states)\n        x = self.relu1(x)\n        x = torch.cat((x, actions), dim=1)\n        x = self.hidden2(x)\n        x = self.relu3(x)\n        out = self.output(x)\n        return out'"
models/DQN.py,15,"b'# The file contains the Convolutional neural network as well as the replay buffer\nimport torch\nimport torch.nn as nn\nimport random\nimport math\nfrom torch.autograd import Variable\nfrom Memory.Buffer import  ReplayBuffer\nfrom Memory import Buffer\nimport torch.optim as optim\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport Environments.super_mario_bros_env as me\nimport Environments.env_wrappers as env_wrappers\nfrom Utils.utils import to_tensor\n\nUSE_CUDA = torch.cuda.is_available()\n\n\n# Random Encoder\nclass Encoder(nn.Module):\n\n    def __init__(self,\n                 state_space,\n                 conv_kernel_size,\n                 conv_layers,\n                 hidden,\n                 input_channels,\n                 height,\n                 width\n                 ):\n        super(Encoder, self).__init__()\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.hidden = hidden\n        self.state_space = state_space\n        self.input_channels = input_channels\n        self.height = height\n        self.width = width\n\n        # Random Encoder Architecture\n        self.conv1 = nn.Conv2d(in_channels=self.input_channels,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2, padding=1)\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU(inplace=True)\n\n        # Hidden Layers\n        self.hidden_1 = nn.Linear(in_features=self.height // 4 * self.width // 4 * self.conv_layers,\n                                  out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.state_space)\n\n        # Initialize the weights of the network (Since this is a random encoder, these weights will\n        # remain static during the training of other networks).\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state):\n        state = torch.unsqueeze(state, dim=0)\n        x = self.conv1(state)\n        x = self.lrelu(x)\n        x = self.conv2(x)\n        x = self.lrelu(x)\n        x = x.view((-1, self.height//4*self.width//4*self.conv_layers))\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n        encoded_state = self.output(x)\n        return encoded_state\n\n\ndef epsilon_greedy_exploration():\n    epsilon_start = 1.0\n    epsilon_final = 0.01\n    epsilon_decay = 500\n    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n        -1. * frame_idx / epsilon_decay)\n\n    return epsilon_by_frame\n\n\nclass DQN(object):\n    """"""\n    The Deep Q Network\n    """"""\n\n    def __init__(self,\n                 env,\n                 num_hidden_units,\n                 num_epochs,\n                 learning_rate,\n                 buffer_size,\n                 discount_factor,\n                 num_rollouts,\n                 num_training_steps,\n                 random_seed,\n                 state_space,\n                 action_space,\n                 num_frames,\n                 batch_size,\n                 height_img,\n                 width_img,\n                 train_limit_buffer,\n                 use_cuda=False,\n                 ):\n        self.env = env\n        self.num_hidden_units = num_hidden_units\n        self.num_epochs = num_epochs\n        self.num_rollouts = num_rollouts\n        self.num_training_steps = num_training_steps\n        self.lr = learning_rate\n        self.seed = random_seed\n        self.use_cuda = use_cuda\n        self.gamma = discount_factor\n        self.num_frames = num_frames\n        self.batch_size = batch_size\n        self.height = height_img\n        self.width = width_img\n        self.train_limit_buffer = train_limit_buffer\n\n        self.buffer = ReplayBuffer(capacity=buffer_size, seed=random_seed)\n\n        self.current_model = QNetwork(env=self.env, state_space=state_space,\n                                      action_space=action_space, hidden=num_hidden_units)\n        self.target_model = QNetwork(env=self.env, state_space=state_space,\n                                      action_space=action_space, hidden=num_hidden_units)\n        self.encoder = Encoder(state_space=state_space, conv_kernel_size=3, conv_layers=32,\n                               hidden=64, input_channels=1, height=self.height,\n                               width=self.width)\n\n        if self.use_cuda:\n            self.current_model = self.current_model.cuda()\n            self.target_model = self.target_model.cuda()\n\n        self.optim = optim.Adam(lr=self.lr, params=self.current_model.parameters())\n\n        self.update_target_network()\n\n    def update_target_network(self):\n        self.target_model.load_state_dict(self.current_model.state_dict())\n\n    # Calculate the Temporal Difference Error\n    def calc_td_error(self):\n        """"""\n        Calculates the td error against the bellman target\n        :return:\n        """"""\n        # Calculate the TD error only for the particular transition\n\n        # Get the separate values from the named tuple\n        transitions = self.buffer.sample_batch(self.batch_size)\n        batch = Buffer.Transition(*zip(*transitions))\n\n        state = batch.state\n        new_state = batch.next_state\n        action = batch.action\n        reward = batch.reward\n        done = batch.done\n\n        #reward = list(reward)\n        #done = list(done)\n\n        state = Variable(torch.cat(state), volatile=True)\n        new_state = Variable(torch.cat(new_state), volatile=True)\n        action = Variable(torch.cat(action))\n        reward = Variable(torch.cat(reward))\n        done = Variable(torch.cat(done))\n\n        if self.use_cuda:\n            state = state.cuda()\n            action = action.cuda()\n            reward = reward.cuda()\n            new_state = new_state.cuda()\n            done = done.cuda()\n\n        q_values = self.current_model(state)\n        next_q_values = self.current_model(new_state)\n        next_q_state_values = self.target_model(new_state)\n\n        q_value = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\n        next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n        expected_q_value = reward + self.gamma * next_q_value * (1 - done)\n\n        loss = (q_value - Variable(expected_q_value.data)).pow(2).mean()\n\n        self.optim.zero_grad()\n        loss.backward()\n        self.optim.step()\n\n        return loss\n\n    def plot(self, frame_idx, rewards, losses):\n        fig = plt.figure(figsize=(20, 5))\n        plt.subplot(131)\n        plt.title(\'frame %s. reward: %s\' % (frame_idx, np.mean(rewards[-10:])))\n        plt.plot(rewards)\n        plt.subplot(132)\n        plt.title(\'loss\')\n        plt.plot(losses)\n        fig.savefig(\'DQN-SuperMarioBros\'+str(frame_idx)+\'.jpg\')\n\n    # Main training loop\n    def train(self):\n        losses = []\n        all_rewards = []\n        episode_reward = 0\n\n        state = self.env.reset()\n        state = to_tensor(state, use_cuda=self.use_cuda)\n        state = self.encoder(state)\n\n        for frame_idx in range(1, self.num_frames+1):\n            epsilon_by_frame = epsilon_greedy_exploration()\n            epsilon = epsilon_by_frame(frame_idx)\n            action = self.current_model.act(state, epsilon)\n            next_state, reward, done, success = self.env.step(action.item())\n            reward = reward/100\n            episode_reward += reward\n\n            next_state = to_tensor(next_state, use_cuda=self.use_cuda)\n            next_state = self.encoder(next_state)\n\n            reward = torch.tensor([reward], dtype=torch.float)\n\n            done_bool = done * 1\n            done_bool = torch.tensor([done_bool], dtype=torch.float)\n\n            self.buffer.push(state, action, next_state, reward, done_bool, success)\n\n            state = next_state\n\n            if done:\n                state = self.env.reset()\n                state = to_tensor(state, use_cuda=self.use_cuda)\n                state = self.encoder(state)\n                all_rewards.append(episode_reward)\n                episode_reward = 0\n\n            if len(self.buffer) > self.train_limit_buffer:\n                for t in range(self.num_training_steps):\n                    loss = self.calc_td_error()\n                    losses.append(loss.data[0])\n\n            if frame_idx % 2000 == 0:\n                self.plot(frame_idx, all_rewards, losses)\n                print(\'Reward \', str(np.mean(all_rewards)))\n                print(\'Loss\', str(np.mean(losses)))\n\n            if frame_idx % 1000 == 0:\n                self.update_target_network()\n\n\nclass ConvQNetwork(nn.Module):\n\n    def __init__(self, num_conv_layers, input_channels, output_q_value, pool_kernel_size,\n                 kernel_size, dense_layer_features, IM_HEIGHT, IM_WIDTH):\n        super(ConvQNetwork, self).__init__()\n        self.num_conv_layers = num_conv_layers\n        self.input_channels = input_channels\n        self.output = output_q_value\n        self.pool_kernel_size = pool_kernel_size\n        self.kernel_size =  kernel_size\n        self.dense_features = dense_layer_features\n        self.height = IM_HEIGHT\n        self.width = IM_WIDTH\n\n        # Convolutional Block\n        self.conv1 = nn.Conv2d(in_channels=input_channels, out_channels=num_conv_layers, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn1  = nn.BatchNorm2d(num_features=num_conv_layers)\n        self.relu1 = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(in_channels=num_conv_layers, out_channels=num_conv_layers*2, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn2 = nn.BatchNorm2d(num_features=num_conv_layers*2)\n        self.relu2 = nn.ReLU(inplace=True)\n        self.conv3 = nn.Conv2d(in_channels=num_conv_layers*2, out_channels=num_conv_layers*2, padding=0,\n                               kernel_size=self.kernel_size)\n        self.bn3 = nn.BatchNorm2d(num_features=num_conv_layers*2)\n        self.relu3 = nn.ReLU(inplace=True)\n\n        #Fully connected layer\n        self.fully_connected_layer = nn.Linear(234432, self.dense_features)\n        self.relu4 = nn.ReLU(inplace=True)\n        self.output_layer = nn.Linear(256, output_q_value)\n\n        # Weight initialization using Xavier initialization\n\n    def forward(self, input):\n        x = self.conv1(input)\n        x = self.bn1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.relu3(x)\n        x = x.view(x.size(0), -1)\n        x = self.fully_connected_layer(x)\n        x = self.relu4(x)\n        out = self.output_layer(x)\n        return out\n\n\nclass QNetwork(nn.Module):\n    def __init__(self,\n                 env,\n                 state_space,\n                 action_space,\n                 hidden):\n        super(QNetwork, self).__init__()\n\n        self.state_space = state_space\n        self.action_space = action_space\n        self.hidden = hidden\n        self.env = env\n\n        self.layers = nn.Sequential(\n            nn.Linear(self.state_space, self.hidden),\n            nn.ReLU(),\n            nn.Linear(self.hidden, self.hidden),\n            nn.ReLU(),\n            nn.Linear(self.hidden, self.action_space)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n    def act(self, state, epsilon):\n        if random.random() > epsilon:\n            state = Variable(torch.FloatTensor(state), volatile=True)\n            q_value = self.forward(state)\n            # Action corresponding to the max Q Value for the state action pairs\n\n            action = q_value.max(1)[1].view(1)\n        else:\n            action = torch.tensor([random.randrange(self.env.action_space.n)], dtype=torch.long)\n        return action\n\n\nif __name__ == \'__main__\':\n    # Create the mario environment\n    env = me.get_mario_bros_env()\n    # Add the required environment wrappers\n    env = env_wrappers.wrap_wrap(env, height=84, width=84)\n    env = env_wrappers.wrap_pytorch(env)\n\n    # Create the DQN Model\n    dqn = DQN(env=env, num_hidden_units=128,\n              num_epochs=10, learning_rate=1e-3,\n              action_space=env.action_space.n, state_space=64,\n              batch_size=16, buffer_size=50000, discount_factor=0.99,\n              height_img=84, width_img=84, num_frames=100000,\n              num_rollouts=10, num_training_steps=10, random_seed=1000000, train_limit_buffer=10000)\n    dqn.train()\n\n\n\n'"
models/PPO.py,13,"b'""""""\nImplementation of the Proximal Policy Optimization algorithm\n\nBased on the Trust Region Policy optimization by John Schulman\n\n""""""\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom Distributions.distributions import init, Categorical\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom Utils.utils import *\nfrom Memory.rollout_storage import RolloutStorage\n\n\nclass Flatten(nn.Module):\n    def forward(self, x):\n        return x.view(x.size(0), -1)\n\n\nclass CNNBase(nn.Module):\n\n    def __init__(self,\n                 conv_layers,\n                 conv_kernel_size,\n                 input_channels,\n                 height,\n                 width,\n                 hidden,\n                 use_gru,\n                 gru_hidden,\n                 value,\n                 ):\n        super(CNNBase, self).__init__()\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.in_channels = input_channels\n        self.height = height\n        self.width = width\n        self.hidden = hidden\n        self.use_gru = use_gru\n        self.gru_hidden = gru_hidden\n        self.value_dim = value\n\n        init_ = lambda m: init(m,\n                               nn.init.orthogonal_,\n                               lambda x: nn.init.constant_(x, 0),\n                               nn.init.calculate_gain(\'relu\'))\n\n        self.main = nn.Sequential(\n            init_(nn.Conv2d(self.in_channels, self.conv_layers,self.conv_kernel_size, stride=4)),\n            nn.ReLU(),\n            init_(nn.Conv2d(self.conv_layers, self.conv_layers*2, self.conv_kernel_size//2, stride=2)),\n            nn.ReLU(),\n            init_(nn.Conv2d(self.conv_layers*2, self.conv_layers, self.conv_kernel_size//2-1, stride=1)),\n            nn.ReLU(),\n            Flatten(),\n            init_(nn.Linear(self.conv_layers * self.height//8 * self.width//8, self.hidden)),\n            nn.ReLU()\n        )\n\n        if self.use_gru:\n            self.gru = nn.GRUCell(self.gru_hidden, self.gru_hidden)\n            nn.init.orthogonal_(self.gru.weight_ih.data)\n            nn.init.orthogonal_(self.gru.weight_hh.data)\n            self.gru.bias_ih.data.fill_(0)\n            self.gru.bias_hh.data.fill_(0)\n\n        init_ = lambda m: init(m,\n                               nn.init.orthogonal_,\n                               lambda x: nn.init.constant_(x, 0))\n\n        self.critic_linear = init_(nn.Linear(self.hidden, self.value_dim))\n\n    def forward(self, input):\n        x = self.main(input)\n        value = self.critic_linear(x)\n\n        return value, x\n\n\nclass ActorCritic(nn.Module):\n    """"""\n    Implementation of the actor critic architecture in the PPO\n\n    This network outputs the action (Policy Network) and criticizes\n    the policy (Critic Network)\n    """"""\n\n    def __init__(self, num_obs,\n                 num_actions,\n                 action_space,\n                 input_channels,\n                 num_value, hidden,\n                 height, width,\n                 use_cnn=True):\n        super(ActorCritic, self).__init__()\n        self.obs = num_obs\n        self.action_dim = num_actions\n        self.value = num_value\n        self.hidden = hidden\n        self.action_space = action_space\n        self.use_cnn = use_cnn\n        self.in_channels = input_channels\n        self.height = height\n        self.width = width\n\n        # Common architecture\n        self.linear1 = nn.Linear(num_obs, hidden)\n        self.hidden1 = nn.Linear(hidden, hidden)\n\n        # Critic Head\n        self.value_head = nn.Linear(hidden, self.value)\n\n        # Actor Features\n        self.actions = nn.Linear(hidden, self.action_dim)\n\n        # Activation function\n        self.activation = nn.Tanh()\n\n        if action_space.__class__.__name__ == ""Discrete"":\n            num_outputs = action_space.n\n            self.dist = Categorical(self.base.output_size, num_outputs)\n\n        if self.use_cnn:\n            self.base = CNNBase(conv_layers=32, conv_kernel_size=8,\n                                input_channels=self.in_channels,\n                                use_gru=False, gru_hidden=512, hidden=512,\n                                height=self.height, width=self.width,\n                                value=1)\n\n        else:\n\n            self.base = nn.Sequential(\n                self.linear1,\n                self.activation,\n                self.hidden1,\n                self.activation,\n            )\n\n        # Old and New Module list for comparison from the previous policy\n        # This forms the update step for PPO and helps in the\n        # reduction of the variance\n\n        self.module_list_current = [self.linear1, self.hidden1, self.action_mean, self.action_log_std]\n        self.module_list_old = [None] * len(self.module_list_current)\n\n    def forward(self, state):\n        raise NotImplementedError\n\n    # The function that actually acts\n    def act(self, input, deterministic=False):\n        value, actor_features = self.base(input)\n        dist = self.dist(actor_features)\n        if deterministic:\n            action = dist.mode()\n        else:\n            action = dist.sample()\n\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n\n        return value, action, action_log_probs, dist_entropy\n\n    def kl_div_p_q(self, p_mean, p_std, q_mean, q_std, eps=1e-12):\n        """"""KL divergence D_{KL}[p(x)||q(x)] for a fully factorized Gaussian""""""\n        # print (type(p_mean), type(p_std), type(q_mean), type(q_std))\n        # q_mean = Variable(torch.DoubleTensor([q_mean])).expand_as(p_mean)\n        # q_std = Variable(torch.DoubleTensor([q_std])).expand_as(p_std)\n        numerator = torch.pow((p_mean - q_mean), 2.) + \\\n            torch.pow(p_std, 2.) - torch.pow(q_std, 2.) #.expand_as(p_std)\n        denominator = 2. * torch.pow(q_std, 2.) + eps\n        return torch.sum(numerator / denominator + torch.log(q_std) - torch.log(p_std))\n\n    def entropy(self):\n        """"""Gives entropy of current defined prob dist""""""\n        ent = torch.sum(self.action_log_std + .5 * torch.log(2.0 * np.pi * np.e))\n        return ent\n\n    def kl_old_new(self):\n        """"""Gives kld from old params to new params""""""\n        kl_div = self.kl_div_p_q(self.module_list_old[-2], self.module_list_old[-1], self.action_mean,\n                                 self.action_log_std)\n        return kl_div\n\n    def get_value(self, inputs):\n        value, _ = self.base(inputs)\n        return value\n\n    def evaluate_actions(self, inputs, action):\n        value, actor_features, states = self.base(inputs)\n        dist = self.dist(actor_features)\n\n        action_log_probs = dist.log_probs(action)\n        dist_entropy = dist.entropy().mean()\n\n        return value, action_log_probs, dist_entropy\n\n\nclass PPO(object):\n\n    def __init__(self,\n                 actor_critic,\n                 clip_param,\n                 num_epochs,\n                 batch_size,\n                 value_loss_param,\n                 entropy_param,\n                 max_grad_norm,\n                 learning_rate, use_cuda):\n        self.model = actor_critic\n        self.num_epochs = num_epochs\n        self.clip_param = clip_param\n        self.batch_size = batch_size\n        self.value_loss_param = value_loss_param\n        self.entropy_loss_param = entropy_param\n        self.lr= learning_rate\n        self.use_cuda = use_cuda\n        self.max_grad_norm = max_grad_norm\n\n        self.optimizer = optim.Adam(lr=self.lr, params=self.model.parameters())\n\n    def train(self, rollouts):\n        advantages = rollouts.returns[:-1] - rollouts.value_preds[:-1]\n        # Normalize the advantages\n        advantages = (advantages-advantages.mean())/(advantages.std() + 1e-5)\n\n        value_loss_epoch = 0\n        action_loss_epoch = 0\n        dist_entropy_epoch = 0\n\n        for epoch in range(self.num_epochs):\n            data_generator = rollouts.feed_forward_generator(\n                advantages, self.batch_size\n            )\n\n            for i, sample in enumerate(data_generator):\n                observations, actions, \\\n                rewards, old_action_log_probs, advantage_targets = sample\n\n                values, action_log_probs, dist_entropy = self.model.evaluate_actions(observations, actions)\n\n                ratio = torch.exp(action_log_probs - old_action_log_probs)\n                surr1 = ratio * advantage_targets\n                surr2 = torch.clamp(ratio, 1.0 - self.clip_param,\n                                    1.0 + self.clip_param) * advantage_targets\n                action_loss = -torch.min(surr1, surr2).mean()\n\n                value_loss = F.mse_loss(rewards, values)\n\n                self.optimizer.zero_grad()\n                (value_loss * self.value_loss_param + action_loss -\n                 dist_entropy * self.entropy_loss_param).backward()\n                nn.utils.clip_grad_norm_(self.model.parameters(),\n                                         self.max_grad_norm)\n                self.optimizer.step()\n\n                value_loss_epoch += value_loss.item()\n                action_loss_epoch += action_loss.item()\n                dist_entropy_epoch += dist_entropy.item()\n\n        num_updates = self.num_epochs * self.batch_size\n\n        value_loss_epoch /= num_updates\n        action_loss_epoch /= num_updates\n        dist_entropy_epoch /= num_updates\n\n        return value_loss_epoch, action_loss_epoch, dist_entropy_epoch\n\n\n# The training class for PPO\nclass PPOTrainer(object):\n\n    def __init__(self,\n                 environment,\n                 ppo,\n                 forward_dynamics_model,\n                 inverse_dynamics_model,\n                 forward_dynamics_learning_rate,\n                 inverse_dynamics_learning_rate,\n                 num_epochs,\n                 num_rollouts,\n                 num_processes,\n                 random_seed,\n                 use_cuda, model_save_path):\n\n        self.agent = ppo\n        self.fwd_model = forward_dynamics_model\n        self.invd_model = inverse_dynamics_model\n        self.num_epochs = num_epochs\n        self.num_rollouts = num_rollouts\n        self.fwd_lr = forward_dynamics_learning_rate\n        self.use_cuda = use_cuda\n        self.save_path = model_save_path\n        self.inv_lr = inverse_dynamics_learning_rate\n        self.env = environment\n        self.num_processes = num_processes\n        self.random_seed = random_seed\n\n        # Environment Details\n        self.action_space = self.env.action_space\n        self.action_shape = self.env.action_space.n\n        self.obs_shape = self.env.observation_space.n\n\n        # Create the rollout storage\n        self.rollout_storage = RolloutStorage(num_steps=self.num_rollouts,\n                                              action_shape=self.action_shape,\n                                              action_space=self.action_space,\n                                              num_processes=self.num_processes,\n                                              obs_shape=self.obs_shape,\n                                              use_cuda=self.use_cuda)\n\n        # Define the optimizers for the forward dynamics and inverse dynamics models\n        self.fwd_optim = optim.Adam(lr=self.fwd_lr, params=self.fwd_model.parameters())\n        self.inverse_optim = optim.Adam(lr=self.inv_lr, params=self.invd_model.parameters())\n\n    # Calculation of the curiosity reward\n    def calculate_intrinsic_reward(self, obs, action, new_obs):\n        # Encode the obs and new_obs\n        obs_encoding = self.invd_model.encode(obs)\n        new_obs_encoding = self.invd_model.encode(new_obs)\n\n        # Pass the action and obs encoding to forward dynamic model\n        pred_new_obs_encoding = self.fwd_model(obs_encoding, action)\n        reward = F.mse_loss(pred_new_obs_encoding, new_obs_encoding)\n\n        return reward\n\n    # Rollout Collection function\n    def collect_rollouts(self):\n        observation = self.env.reset()\n        observation = to_tensor(observation, use_cuda=self.use_cuda)\n        for r in range(self.num_rollouts):\n            value, action, action_log_prob, dist_entropy = self.agent.act(observation)\n            next_observation, reward, done, _ = self.env.step(action)\n            intrinsic_reward = self.calculate_intrinsic_reward(obs=observation,\n                                                               action=action,\n                                                               new_obs=next_observation)\n            # Store in the rollout storage\n            self.rollout_storage.insert(step=r,\n                                        current_obs=observation,\n                                        action=action,\n                                        action_log_prob=action_log_prob,\n                                        intrinsic_reward=intrinsic_reward,\n                                        reward=reward,\n                                        value_pred=value)\n\n            if done:\n                break\n\n    def train(self):\n        # Update the agent\n        self.agent.train(self.rollout_storage)\n        # Update the inverse dynamics model\n\n        # Update the forward dynamics model\n        self.fwd_model.train()\n\n        # Update the inverse dynamics model\n        self.invd_model.train()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'"
models/RandomExplorationPolicy.py,0,"b'import gym\nimport pickle\nimport os\nimport scipy.misc as m\nfrom random import randint\nimport random\nimport numpy as np\nfrom tqdm import tqdm\n\n\nclass RandomExplorationPolicy(object):\n\n    def __init__(self, env, states_to_save, seed,\n                 ram_env=None,\n                 demo_file=None,\n                 save_obs=False):\n\n        self.env = env\n        self.action = self.env.action_space\n        self.obs = self.env.observation_space\n        self.seed = seed\n        self.save_obs = save_obs\n        self.num_actions = self.env.action_space.n\n        self.num_states_to_save = states_to_save\n        self.demonstrations = demo_file\n        self.demonstrated_actions = []\n        self.ram_env = ram_env\n        self.ram_states = []\n        self.states = []\n\n    def set_seed(self):\n        random.seed = self.seed\n\n    def get_demonstrations(self):\n        # Open the demonstration file and store the actions\n        if self.demonstrations is not None:\n            with open(self.demonstrations, \'rb\') as f:\n                dat = pickle.load(f)\n            self.demonstrated_actions = dat[\'actions\']\n\n\n    def calc_mean_std(self, images):\n        mean_image = np.mean(images, axis=(0, 1))\n        std_image = np.std(images, axis=(0, 1))\n        return mean_image, std_image\n\n    def step(self, use_demonstrations=False):\n\n        self.set_seed()\n        """"""\n\n        Executing a random action in the environment\n\n        :return:\n        """"""\n\n        state = self.env.reset()\n        state_ram = self.ram_env.reset()\n\n        if use_demonstrations:\n            self.get_demonstrations()\n\n            for i, a in tqdm(enumerate(self.demonstrated_actions)):\n                state, reward, done, success = self.env.step(action=a)\n                state_ram, reward_ram, done_ram, success_ram = self.ram_env.step(a)\n                if i % 10 == 0:\n                    file_name = str(i) + \'.jpg\'\n                    path =  os.path.join(\'montezuma_resources\', file_name)\n                    m.imsave(path, state)\n                    # Try learning an infogan on the ram states\n                    self.ram_states.append(state_ram)\n                    self.states.append(state)\n            # Save the ram states\n            file_name = \'states_ram.npy\'\n            path = os.path.join(\'montezuma_resources\', file_name)\n            np.save(path, self.ram_states)\n\n        else:\n            for i in range(self.num_states_to_save):\n                action = randint(0, self.num_actions-1)\n                state, reward, done, success  = self.env.step(action=action)\n                if i % 4 ==0 :\n                    file_name = str(i) + \'.jpg\'\n                    path = os.path.join(\'montezuma_resources\', file_name)\n                    m.imsave(path, state)\n\n                if done:\n                    self.env.reset()\n        self.calc_mean_std(self.states)\n\n\nif __name__ == \'__main__\':\n    env  = gym.make(\'MontezumaRevenge-v0\')\n    # Using the RAM Model to get the state representation in the latent vector form\n    env_ram = gym.make(\'MontezumaRevenge-ram-v0\')\n\n    re = RandomExplorationPolicy(env=env, states_to_save=16000, seed=100,\n                                 ram_env=env_ram,\n                                 demo_file=\'montezuma_resources/MontezumaRevenge.demo\')\n    re.step(use_demonstrations=True)'"
models/SAC.py,23,"b'""""""\n\nThis script contains an implementation of the Soft Actor Critic.\n\nThis is an off policy Actor critic algorithm with the entropy of\nthe current policy added to the reward.\n\nThe maximization of the augmented reward enables the agent to discover multimodal actions\n(Multiple actions that results in reward). It promotes exploration\nand is considerably more stable to random seeds variation and hyperparameter\ntuning compared to Deep Deterministic Policy Gradient.\n\n""""""\n\nLOG_SIG_MAX = 2\nLOG_SIG_MIN = -20\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom Memory import Buffer\nimport Utils.random_process as random_process\nfrom Distributions.distributions import SigmoidNormal\nfrom collections import defaultdict, deque\nimport time\nfrom Utils.utils import *\nfrom Layers.LayerNorm import LayerNorm\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass SAC(object):\n\n    def __init__(self, state_dim,\n                 action_dim,\n                 hidden_dim,\n                 actor, critic, value_network,\n                 target_value_network,\n                 polyak_constant,\n                 actor_learning_rate,\n                 critic_learning_rate,\n                 value_learning_rate,\n                 num_q_value,\n                 num_v_value,\n                 batch_size, gamma,\n                 random_seed,\n                 num_epochs,\n                 num_rollouts, num_eval_rollouts,\n                 env, eval_env, nb_train_steps,\n                 max_episodes_per_epoch,\n                 output_folder,\n                 use_cuda, buffer_capacity,\n                 policy_reg_mean_weight=1e-3,\n                 policy_reg_std_weight=1e-3,\n                 policy_preactivation_weight=0,\n                 verbose=True,\n                 plot_stats=False,\n                 ):\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.hidden = hidden_dim\n        self.q_dim = num_q_value\n        self.v_dim = num_v_value\n        self.actor = actor\n        self.critic = critic\n        self.value = value_network\n        self.tau = polyak_constant\n        self.bs = batch_size\n        self.gamma = gamma\n        self.seed = random_seed\n        self.use_cuda = use_cuda\n        self.buffer = Buffer.ReplayBuffer(capacity=buffer_capacity, seed=self.seed)\n        self.policy_reg_mean_weight = policy_reg_mean_weight\n        self.policy_reg_std_weight = policy_reg_std_weight\n        self.policy_pre_activation_weight = policy_preactivation_weight\n\n        # Training specific parameters\n        self.num_epochs = num_epochs\n        self.num_rollouts = num_rollouts\n        self.num_eval_rollouts = num_eval_rollouts\n        self.env = env\n        self.eval_env = eval_env\n        self.nb_train_steps = nb_train_steps\n        self.max_episodes_per_epoch = max_episodes_per_epoch\n        self.statistics = defaultdict(float)\n        self.combined_statistics = defaultdict(list)\n        self.verbose = verbose\n        self.output_folder = output_folder\n        self.plot_stats = plot_stats\n\n\n        self.actor_optim = optim.Adam(lr=actor_learning_rate, params=self.actor.parameters())\n        self.critic_optim = optim.Adam(lr=critic_learning_rate, params=self.critic.parameters())\n        self.value_optim = optim.Adam(lr=value_learning_rate, params=self.value.parameters())\n\n        self.target_value = target_value_network\n\n        if self.use_cuda:\n            self.actor  = self.actor.cuda()\n            self.critic = self.critic.cuda()\n            self.value = self.value.cuda()\n            self.target_value = self.target_value.cuda()\n\n        # Initializing the target networks with the standard network weights\n        self.target_value.load_state_dict(self.value.state_dict())\n\n        # Initialize a random exploration noise\n        self.random_noise = random_process.OrnsteinUhlenbeckActionNoise(self.action_dim)\n\n    def save_model(self, output):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the actor, critic and value networks"")\n        torch.save(\n            self.actor.state_dict(),\n            \'{}/actor.pt\'.format(output)\n        )\n        torch.save(\n            self.critic.state_dict(),\n            \'{}/critic.pt\'.format(output)\n        )\n\n        torch.save(\n            self.value.state_dict(),\n            \'{}/value.pt\'.format(output)\n        )\n\n    # Get the action with an option for random exploration noise\n    def get_action(self, state, noise=True):\n        state_v = Variable(state)\n        action = self.actor(state_v)\n        if noise:\n            noise = self.random_noise\n            action = action.data.cpu().numpy()[0] + noise.sample()\n        else:\n            action = action.data.cpu().numpy()[0]\n        action = np.clip(action, -1., 1.)\n        return action\n\n    # Reset the noise\n    def reset(self):\n        self.random_noise.reset()\n\n    # Store the transition into the replay buffer\n    def store_transition(self, state, new_state, action, reward, done, success):\n        self.buffer.push(state, action, new_state, reward, done, success)\n\n    # Update the target networks using polyak averaging\n    def update_target_networks(self):\n        for target_param, param in zip(self.target_value.parameters(), self.value.parameters()):\n            target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n\n    def random_action(self):\n        """"""\n        Take a random action bounded between min and max values of the action space\n        :return:\n        """"""\n        action = np.random.uniform(-1., 1., self.action_dim)\n        self.a_t = action\n\n        return action\n\n    def seed(self, s):\n        """"""\n        Setting the random seed for a particular training iteration\n        :param s:\n        :return:\n        """"""\n        np.random.seed(s)\n        torch.manual_seed(s)\n        if self.use_cuda:\n            torch.cuda.manual_seed(s)\n\n    # Calculate the Value function error\n    def calc_soft_value_function_error(self, states):\n        # The values for the states\n        values = self.value(states)\n        actions, means, log_probs, stds, log_stds, pre_sigmoid_value = self.actor(states)\n        q_values = self.critic(states, actions)\n        real_values = q_values - log_probs\n        real_values.detach()\n        loss = nn.MSELoss()(values, real_values)\n        return loss, values\n\n    # Calculate the Q function error\n    def calc_soft_q_function_error(self, states, actions, next_states, rewards, dones):\n        r = rewards\n        value_next_states = self.target_value(next_states)\n        value_next_states = value_next_states * (1-dones)\n\n        y = r + self.gamma*value_next_states\n        y.detach()\n\n        outputs = self.critic(states, actions)\n        temporal_difference_loss = nn.MSELoss()(outputs, y)\n\n        return temporal_difference_loss, outputs\n\n    # Calculate the policy loss\n    def calc_policy_loss(self, states, q_values, value_predictions):\n        actions, means, log_probs, stds, log_stds, pre_sigmoid_value = self.actor(states)\n        log_policy_target = q_values - value_predictions\n        policy_loss = (\n            log_probs * (log_probs - log_policy_target).detach()\n        ).mean()\n        mean_reg_loss = self.policy_reg_mean_weight * (means**2).mean()\n        std_reg_loss = self.policy_reg_std_weight * (log_stds ** 2).mean()\n\n        pre_activation_reg_loss = self.policy_pre_activation_weight * (\n            (pre_sigmoid_value ** 2).sum(dim=1).mean()\n        )\n\n        policy_reg_loss = mean_reg_loss + std_reg_loss + pre_activation_reg_loss\n\n        policy_loss = policy_loss + policy_reg_loss\n\n        return policy_loss\n\n    # Fitting one batch of data from the replay buffer\n    def fit_batch(self):\n        transitions = self.buffer.sample_batch(self.bs)\n        batch = Buffer.Transition(*zip(*transitions))\n        # Get the separate values from the named tuple\n        states = batch.state\n        new_states = batch.next_state\n        actions = batch.action\n        rewards = batch.reward\n        dones = batch.done\n\n        states = Variable(torch.cat(states))\n        with torch.no_grad():\n            new_states = Variable(torch.cat(new_states))\n        actions = Variable(torch.cat(actions))\n        rewards = Variable(torch.cat(rewards))\n        dones = Variable(torch.cat(dones))\n\n        if self.use_cuda:\n            states = states.cuda()\n            actions = actions.cuda()\n            rewards = rewards.cuda()\n            new_states = new_states.cuda()\n            dones = dones.cuda()\n\n        value_loss, values = self.calc_soft_value_function_error(states)\n        q_loss, q_values = self.calc_soft_q_function_error(states, actions, new_states, rewards, dones)\n        policy_loss = self.calc_policy_loss(states, q_values, values)\n\n        """"""\n        Update the networks\n        """"""\n        self.value_optim.zero_grad()\n        value_loss.backward()\n        self.value_optim.step()\n\n        self.critic_optim.zero_grad()\n        q_loss.backward()\n        self.critic_optim.zero_grad()\n\n        self.actor_optim.zero_grad()\n        policy_loss.backward()\n        self.actor_optim.step()\n\n        # Update the target networks\n        self.update_target_networks()\n\n        return value_loss, q_loss, policy_loss\n\n    # The main training loop\n    def train(self):\n\n        for name, param in self.actor.named_parameters():\n            print(name, param)\n\n        # Starting time\n        start_time = time.time()\n\n        # Initialize the statistics dictionary\n        statistics = self.statistics\n\n        episode_rewards_history = deque(maxlen=100)\n        eval_episode_rewards_history = deque(maxlen=100)\n        episode_success_history = deque(maxlen=100)\n        eval_episode_success_history = deque(maxlen=100)\n\n        epoch_episode_rewards = []\n        epoch_episode_success = []\n        epoch_episode_steps = []\n\n        # Epoch Rewards and success\n        epoch_rewards = []\n        epoch_success = []\n\n        # Initialize the training with an initial state\n        state = self.env.reset()\n        state = to_tensor(state, use_cuda=self.use_cuda)\n        state = torch.unsqueeze(state, dim=0)\n\n        # If eval, initialize the evaluation with an initial state\n        if self.eval_env is not None:\n            eval_state = self.eval_env.reset()\n            eval_state = to_tensor(eval_state, use_cuda=self.use_cuda)\n            eval_state = torch.unsqueeze(eval_state, dim=0)\n\n        # Initialize the losses\n        loss = 0\n        episode_reward =  0\n        episode_success = 0\n        episode_step = 0\n        epoch_actions = []\n        t = 0\n\n        # Main training loop\n        for epoch in range(self.num_epochs):\n            epoch_actor_losses = []\n            epoch_critic_losses = []\n            epoch_value_losses = []\n            for episode in range(self.max_episodes_per_epoch):\n                # Rollout of trajectory to fill the replay buffer before training\n                for rollout in range(self.num_rollouts):\n                    # Sample an action from behavioural policy pi\n                    action, means, log_probs, stds, log_stds, pre_sigmoid_value = self.actor(state)\n                    action = action.data.cpu().numpy()[0]\n                    # Execute next action\n                    new_state, reward, done, success = self.env.step(action)\n                    done_bool = done * 1\n\n                    # Convert new state to a tensor\n                    new_state = to_tensor(new_state, use_cuda=self.use_cuda)\n                    new_state = torch.unsqueeze(new_state, dim=0)\n                    action = to_tensor(action, use_cuda=self.use_cuda)\n                    action = torch.unsqueeze(action, dim=0)\n\n                    t += 1\n                    episode_reward += reward\n                    episode_step += 1\n\n                    reward = [reward]\n                    done_bool = [done_bool]\n\n                    reward = to_tensor(reward, use_cuda=self.use_cuda)\n                    done_bool = to_tensor(done_bool, use_cuda=self.use_cuda)\n\n\n                    # Book keeping\n                    epoch_actions.append(action)\n\n                    # Store the transition in the replay buffer of the agent\n                    self.store_transition(state=state, new_state=new_state,\n                                               action=action, done=done_bool, reward=reward, success=success)\n\n                    # Set the current state as the next state\n                    state = to_tensor(new_state, use_cuda=self.use_cuda)\n\n                    # End of the episode\n                    if done:\n                        epoch_episode_rewards.append(episode_reward)\n                        episode_rewards_history.append(episode_reward)\n                        episode_success_history.append(episode_success)\n                        epoch_episode_success.append(episode_success)\n                        epoch_episode_steps.append(episode_step)\n                        episode_reward = 0\n                        episode_step = 0\n                        episode_success = 0\n\n                        # Reset the agent\n                        self.reset()\n                        # Get a new initial state to start from\n                        state = self.env.reset()\n                        state = to_tensor(state, use_cuda=self.use_cuda)\n                        state = torch.unsqueeze(state, dim=0)\n\n                # Train\n                for train_steps in range(self.nb_train_steps):\n                    value_loss, critic_loss, actor_loss = self.fit_batch()\n                    if critic_loss is not None and actor_loss is not None and value_loss is not None:\n                        epoch_critic_losses.append(critic_loss.data.numpy())\n                        epoch_actor_losses.append(actor_loss.data.numpy())\n                        epoch_value_losses.append(value_loss.data.numpy())\n\n                # Evaluation Step\n                eval_episode_rewards = []\n                eval_episode_successes = []\n                if self.eval_env is not None:\n                    eval_episode_reward = 0\n                    eval_episode_success = 0\n                    for t_rollout in range(self.num_eval_rollouts):\n                        if eval_state is not None:\n                            action, means, log_probs, stds, log_stds, pre_sigmoid_value = self.actor(eval_state)\n                            eval_new_state, eval_reward, eval_done, eval_success = self.eval_env.step(action)\n                            eval_episode_reward += eval_reward\n                            eval_episode_success += eval_success\n\n                            if eval_done:\n                                eval_state = self.eval_env.reset()\n                                eval_state = to_tensor(eval_state, use_cuda=self.use_cuda)\n                                eval_episode_rewards.append(eval_episode_reward)\n                                eval_episode_rewards_history.append(eval_episode_reward)\n                                eval_episode_successes.append(eval_episode_success)\n                                eval_episode_success_history.append(eval_episode_success)\n                                eval_episode_reward = 0\n                                eval_episode_success = 0\n\n            # Log stats\n            duration = time.time() - start_time\n            statistics[\'rollout/rewards\'] = np.mean(epoch_episode_rewards)\n            statistics[\'rollout/rewards_history\'] = np.mean(episode_rewards_history)\n            statistics[\'rollout/successes\'] = np.mean(epoch_episode_success)\n            statistics[\'rollout/successes_history\'] = np.mean(episode_success_history)\n            statistics[\'rollout/actions_mean\'] = np.mean(epoch_actions)\n            statistics[\'train/loss_actor\'] = np.mean(epoch_actor_losses)\n            statistics[\'train/loss_critic\'] = np.mean(epoch_critic_losses)\n            statistics[\'train/loss_value\'] = np.mean(epoch_value_losses)\n            statistics[\'total/duration\'] = duration\n\n            # Evaluation statistics\n            if self.eval_env is not None:\n                statistics[\'eval/rewards\'] = np.mean(eval_episode_rewards)\n                statistics[\'eval/rewards_history\'] = np.mean(eval_episode_rewards_history)\n                statistics[\'eval/successes\'] = np.mean(eval_episode_successes)\n                statistics[\'eval/success_history\'] = np.mean(eval_episode_success_history)\n\n            # Print the statistics\n            if self.verbose:\n                if epoch % 1 == 0:\n                    print(""Actor Loss: "", statistics[\'train/loss_actor\'])\n                    print(""Critic Loss: "", statistics[\'train/loss_critic\'])\n                    print(""Value Loss: "", statistics[\'train/loss_value\'])\n                    print(""Reward "", statistics[\'rollout/rewards\'])\n                    print(""Successes "", statistics[\'rollout/successes\'])\n\n                    if self.eval_env is not None:\n                        print(""Evaluation Reward "", statistics[\'eval/rewards\'])\n                        print(""Evaluation Successes "", statistics[\'eval/successes\'])\n\n            # Log the combined statistics for all epochs\n            for key in sorted(statistics.keys()):\n                self.combined_statistics[key].append(statistics[key])\n\n            # Log the epoch rewards and successes\n            epoch_rewards.append(np.mean(epoch_episode_rewards))\n            epoch_success.append(np.mean(epoch_episode_success))\n\n        # Plot the statistics calculated\n        if self.plot_stats:\n            # Plot the rewards and successes\n            rewards_fname = self.output_folder + \'/rewards.jpg\'\n            success_fname = self.output_folder + \'/success.jpg\'\n            plot(epoch_rewards, f_name=rewards_fname, save_fig=True, show_fig=False)\n            plot(epoch_success, f_name=success_fname, save_fig=True, show_fig=False)\n\n        # Save the models on the disk\n        if self.save_model:\n            self.save_model(self.output_folder)\n\n        return self.combined_statistics\n\n\n# The Policy Network\nclass StochasticActor(nn.Module):\n\n    def __init__(self, state_dim, action_dim,\n                 hidden_dim, use_tanh=False,\n                 use_sigmoid=False, deterministic=False, use_layernorm=False):\n        super(StochasticActor, self).__init__()\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.hidden = hidden_dim\n        self.use_tanh = use_tanh\n        self.use_sigmoid = use_sigmoid\n        self.deterministic = deterministic\n        self.use_layernorm = use_layernorm\n\n        # Architecture\n        self.input = nn.Linear(in_features=self.state_dim, out_features=self.hidden)\n        self.hidden_1 = nn.Linear(in_features=self.hidden, out_features=self.hidden*2)\n        self.hidden_2 = nn.Linear(in_features=self.hidden*2, out_features=self.hidden*2)\n        self.output_mu = nn.Linear(in_features=self.hidden*2, out_features=self.action_dim)\n        self.output_logstd = nn.Linear(in_features=self.hidden*2, out_features=self.action_dim)\n\n        if self.use_layernorm:\n            self.ln1 =  LayerNorm(self.hidden)\n            self.ln2 = LayerNorm(self.hidden*2)\n            self.ln3 = LayerNorm(self.hidden*2)\n\n        # Leaky Relu activation function\n        self.lrelu = nn.LeakyReLU()\n\n        #Output Activation function\n        self.tanh = nn.Tanh()\n\n        # Output Activation sigmoid function\n        self.sigmoid = nn.Sigmoid()\n\n        # Initialize the weights with xavier initialization\n        nn.init.xavier_uniform_(self.input.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.hidden_2.weight)\n        nn.init.xavier_uniform_(self.output_mu.weight)\n        nn.init.xavier_uniform_(self.output_logstd.weight)\n\n    def reparameterize(self, mu, logvar):\n        # Reparameterization trick as shown in the auto encoding variational bayes paper\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            if self.use_cuda:\n                eps = eps.cuda()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def forward(self, state, deterministic=False, return_log_prob=True):\n\n        """"""\n                :param state: state\n                :param deterministic: If True, do not sample\n                :param return_log_prob: If True, return a sample and its log probability\n        """"""\n\n        x = self.input(state)\n        if self.use_layernorm:\n            x = self.ln1(x)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        if self.use_layernorm:\n            x = self.ln2(x)\n        x = self.lrelu(x)\n        x = self.hidden_2(x)\n        if self.use_layernorm:\n            x = self.ln3(x)\n        x = self.lrelu(x)\n\n        mu = self.output_mu(x)\n        logstd = self.output_logstd(x)\n        # Clamp the log of the standard deviation\n        logstd = torch.clamp(logstd, LOG_SIG_MIN, LOG_SIG_MAX)\n        std = torch.exp(logstd)\n\n        log_prob = None\n        pre_sigmoid_value = None\n\n        if deterministic:\n            output = nn.Sigmoid()(mu)\n        else:\n            sigmoid_normal = SigmoidNormal(normal_mean=mu, normal_std=std)\n            if return_log_prob:\n                output, pre_sigmoid_value = sigmoid_normal.sample(\n                    return_pre_sigmoid_value=True\n                )\n                log_prob = sigmoid_normal.log_prob(\n                    output,\n                    pre_sigmoid_value=pre_sigmoid_value\n                )\n                log_prob = log_prob.sum(dim=1, keepdim=True)\n            else:\n                output = sigmoid_normal.sample()\n\n        #output = self.reparameterize(mu, logstd)\n\n        if self.use_tanh:\n            output = self.tanh(output)\n\n        return output, mu, log_prob, std, logstd, pre_sigmoid_value\n\n\n# Estimates the Q value of a state action pair\nclass Critic(nn.Module):\n\n    def __init__(self, state_dim, action_dim,\n                 hidden_dim, output_dim, use_layer_norm=False):\n        super(Critic, self).__init__()\n\n        self.state_dim = state_dim\n        self.action_dim = action_dim\n        self.hidden = hidden_dim\n        self.output_dim = output_dim\n        self.use_layer_norm = use_layer_norm\n\n        # Architecture\n        self.input = nn.Linear(in_features=self.state_dim+self.action_dim, out_features=self.hidden)\n        self.hidden_1 = nn.Linear(in_features=self.hidden, out_features=self.hidden*2)\n        self.hidden_2 = nn.Linear(in_features=self.hidden*2, out_features=self.hidden*2)\n        self.output = nn.Linear(in_features=self.hidden*2, out_features=self.output_dim)\n\n        if self.use_layer_norm:\n            self.ln1 = LayerNorm(self.hidden)\n            self.ln2 = LayerNorm(self.hidden*2)\n            self.ln3 = LayerNorm(self.hidden*2)\n\n        # Leaky Relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights with xavier initialization\n        nn.init.xavier_uniform_(self.input.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.hidden_2.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state, action):\n        #x = torch.cat([state, action], dim=-1)\n        x = self.input(state)\n        if self.use_layer_norm:\n            x = self.ln1(x)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        if self.use_layer_norm:\n            x = self.ln2(x)\n        x = self.lrelu(x)\n        # For the critic, actions are not included until the second hidden layer\n        x = torch.cat([x, action], dim=-1)\n        x = self.hidden_2(x)\n        if self.use_layer_norm:\n            x = self.ln3(x)\n        x = self.lrelu(x)\n        output = self.output(x)\n        return output\n\n\n# Estimates the value of a state\nclass ValueNetwork(nn.Module):\n\n    def __init__(self, state_dim,\n                 hidden_dim, output_dim):\n        super(ValueNetwork, self).__init__()\n\n        self.state_dim = state_dim\n        self.hidden = hidden_dim\n        self.output_dim = output_dim\n\n        # Architecture\n        self.input = nn.Linear(in_features=self.state_dim, out_features=self.hidden)\n        self.hidden_1 = nn.Linear(in_features=self.hidden, out_features=self.hidden * 2)\n        self.hidden_2 = nn.Linear(in_features=self.hidden * 2, out_features=self.hidden * 2)\n        self.output = nn.Linear(in_features=self.hidden * 2, out_features=self.output_dim)\n\n        # Leaky Relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights with xavier initialization\n        nn.init.xavier_uniform_(self.input.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.hidden_2.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state):\n        x = self.input(state)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n        x = self.hidden_2(x)\n        x = self.lrelu(x)\n        output = self.output(x)\n        return output'"
models/attention.py,5,"b'""""""\nThis file contains the self attention and the multi attention modules\n\nDescription :\nThe Network being implemented here produces context aware representation by applying attention to\neach pair of tokens from the input sequence\n\nThe idea is to use this network to predict the top n goals to use\nfor resubstitution in the Hindsight experience replay\n\n""""""\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef multiplicative_fn( x, q, linear_x, linear_q):\n    """"""\n\n    :param x: Input token\n    :param q: Query vector\n    :param linear_x: Weight matrix for x\n    :param linear_q: Weight matrix for query\n    :return: compatibility function\n    """"""\n\n    # Return the cosine similarity\n    x_ = linear_x(x)\n    q_ = linear_q(q)\n    f = F.cosine_similarity(x_, q_)\n    return f\n\n\ndef additive_fn( x, q, linear_x, linear_q, out_linear, num_hidden, activation):\n    """"""\n\n    :param x: Input token\n    :param q: Query vector\n    :param linear_x: Weight matrix for x\n    :param linear_q: Weight matrix for query\n    :param num_hidden: Number of hidden layers\n    :param activation: The Non linear activation function to use\n    :return: compatibility function\n    """"""\n\n    # Additive attention achieves better empirical performance\n    x_ = linear_x(x)\n    q_ = linear_q(q)\n    o = out_linear(activation(x_ + q_))\n    return o\n\n\nclass VanillaAttention(nn.Module):\n    """"""\n    The standard attention module\n    Vanilla Attention computes alignments scores of the query to each\n    of the input tokens in an input sequence.\n    """"""\n    def __init__(self, input_dim, query_dim, num_hidden, embedding_dim,\n                 activation, output_features,\n                 use_additive=True, save_attention=False,\n                 attention_dict=None, name=None):\n        """"""\n\n        :param input_dim: The input dimension of the sequence or length of the sequence\n        :param query_dim: The dimension of the query embedding - Current State || Desired Goal Vector\n        :param num_hidden: The number of hidden layers in the network\n        :param embedding_dim: The dimension of each embedding - State || Achieved Goal Vector\n        :param activation: The non-linear activation function to use\n        :param use_additive_fn: Boolean to determine whether to use additive or multiplicative attention\n        """"""\n        super(VanillaAttention, self).__init__()\n        self.input_dim = input_dim # n\n        self.query_dim = query_dim # q\n        self.num_hidden = num_hidden # h\n        self.embedding_dim = embedding_dim # e\n        self.output = output_features\n        self.additive = use_additive\n        self.activation = activation\n        self.save_attention = save_attention\n        self.attention_dict = attention_dict\n        self.name = name\n\n        # Define the linear layers\n        self.linear_x = nn.Linear(in_features=embedding_dim, out_features=num_hidden)\n        self.linear_q = nn.Linear(in_features=query_dim, out_features=num_hidden)\n        self.out_linear = nn.Linear(in_features=num_hidden, out_features=output_features)\n        self.output_softmax_scores = nn.Softmax()\n\n    def forward(self, input_sequence, query_vector):\n        # Returns the alignment scores\n        if self.additive:\n            s = additive_fn(x=input_sequence, q=query_vector, linear_x=self.linear_x,\n                        linear_q=self.linear_q, num_hidden=self.num_hidden, activation=self.activation,\n                            out_linear=self.out_linear)\n        else:\n            s = multiplicative_fn(x=input_sequence, q=query_vector, linear_x=self.linear_x,\n                              linear_q=self.linear_q)\n\n        scores = self.output_softmax_scores(s)\n        # A large score here for a particular x (embedding) means that it contributes\n        # important information to the given query vector\n        # Dimension of scores -> b x n\n        expectations_of_sampling = torch.sum(torch.mul(scores, input_sequence))\n        if self.save_attention:\n            if self.name is not None and self.attention_dict is not None:\n                self.attention_dict[self.name] = scores\n        return scores, expectations_of_sampling\n\n    # Weights Initialization\n    def init_weights(self, init_range=0.1):\n        self.linear_x.weight.data.uniform_(-init_range, init_range)\n        self.linear_q.weight.data.uniform_(-init_range, init_range)\n        self.out_linear.weight.data.uniform(-init_range, init_range)\n\n\nclass MultiAttention(nn.Module):\n    """"""\n    The Mutli Attention module\n\n    Alignment Score computed for each feature\n\n    Score of a token pair is vector rather than a scalar\n\n    Has embedding_dim indicators for embedding_dim features\n    Each indicator has a probability\n    distribution that is generated by applying softmax to\n    the n alignment scores of the corresponding feature.\n\n\n    """"""\n    def __init__(self, input_dim, embedding_dim, query_dim, num_hidden,\n                 activation, output_features,\n                 use_additive=True, save_attention=False,\n                 attention_dict=None, name=None):\n        """"""\n        :param input_dim: The input dimension of the sequence or length of the sequence\n        :param query_dim: The dimension of the query embedding - Current State || Desired Goal Vector\n        :param num_hidden: The number of hidden layers in the network\n        :param embedding_dim: The dimension of each embedding - State || Achieved Goal Vector\n        :param activation: The non-linear activation function to use\n        :param use_additive_fn: Boolean to determine whether to use additive or multiplicative attention\n        """"""\n        super(MultiAttention, self).__init__()\n        self.input_dim = input_dim\n        self.query_dim = query_dim\n        self.embedding_dim = embedding_dim\n        self.num_hidden = num_hidden\n        self.activation = activation\n        self.additive = use_additive\n        self.out_features = output_features\n        self.save_attention = save_attention\n        self.attention_dict = attention_dict\n        self.name = name\n\n        # Define the linear layers\n        self.linear_x = nn.Linear(in_features=embedding_dim, out_features=num_hidden)\n        self.linear_q = nn.Linear(in_features=query_dim, out_features=num_hidden)\n        self.out_linear = nn.Linear(in_features=num_hidden, out_features=embedding_dim)\n        self.output_softmax_scores = nn.Softmax()\n\n    def forward(self, input_sequence, query_vector):\n        if self.additive:\n            s = additive_fn(x=input_sequence, q=query_vector, linear_x=self.linear_x,\n                            linear_q=self.linear_q, num_hidden=self.num_hidden, activation=self.activation,\n                            out_linear=self.out_linear)\n        else:\n            s = multiplicative_fn(x=input_sequence, q=query_vector, linear_x=self.linear_x,\n                                  linear_q=self.linear_q)\n\n        prob_distribution = self.output_softmax_scores(s)\n        score_vectors = torch.sum(torch.bmm(prob_distribution, input_sequence))\n        if self.save_attention:\n            if self.attention_dict is not None and self.name is not None:\n                self.attention_dict[self.name] = prob_distribution\n        return score_vectors, prob_distribution\n\n    # Weights Initialization\n    def init_weights(self, init_range=0.1):\n        self.linear_x.weight.data.uniform_(-init_range, init_range)\n        self.linear_q.weight.data.uniform_(-init_range, init_range)\n        self.out_linear.weight.data.uniform(-init_range, init_range)\n\n\nclass SelfAttention(nn.Module):\n    """"""\n    The Self Attention module produces context-aware representations by\n    exploring the dependency between two tokens xi and xj from the same sequence x.\n\n    """"""\n    def __init__(self, input_dim, embedding_dim, query_dim, num_hidden,\n                 output_features, activation, use_additive=True,\n                 token2token=True, seq2token=False, attention_dict=None,\n                 save_attention=False, name=None):\n        """"""\n\n        :param input_dim:\n        :param embedding_dim:\n        :param query_dim:\n        :param num_hidden:\n        :param activation:\n        :param use_additive:\n        :param token2token:\n        :param seq2token:\n        """"""\n        super(SelfAttention, self).__init__()\n        self.token2token = token2token\n        self.seq2token = seq2token\n        self.input_dim = input_dim\n        self.embedding_dim = embedding_dim\n        self.query_dim = query_dim\n        self.num_hidden = num_hidden\n        self.out_features = output_features\n        self.activation = activation\n        self.additive = use_additive\n        self.attention_dict = attention_dict\n        self.save_attention = save_attention\n        self.name = name\n\n        # Define the linear layers\n        self.linear_x = nn.Linear(in_features=self.embedding_dim, out_features=self.num_hidden)\n        self.linear_q = nn.Linear(in_features=self.query_dim, out_features=self.num_hidden)\n        self.out_linear = nn.Linear(in_features=self.num_hidden, out_features=self.out_features)\n        self.softmax_probs = nn.Softmax()\n\n    def forward(self, input_sequence):\n        # In this case the query vector itself is the input sequence\n\n        # Dimensions of the vectors\n        # Input Dimension -> B x Seq Length x embedding_dim\n        # Query Vector Dimension -> B x Seq Length x embedding_dim\n\n        x = input_sequence\n        query_vector = input_sequence\n\n        # Dimension of x and query after linear_x and linear_q respectively\n        # B x Seq Length x hidden_dim\n        x_ = self.linear_x(x)\n        q_ = self.linear_q(query_vector)\n\n        t = nn.ReLU(x_ + q_)\n\n        # Dimension of t after out_linear\n        # B x Seq Length x embedding_dim\n        o = self.out_linear(t)\n\n        # Softmax Scores\n        scores = self.softmax_probs(o)\n        # Dimension of expectation of the sampling\n        # B x seq length (Contrary to the paper, we are summing along the embedding dimension)\n        expectation_of_sampling = torch.sum(torch.mul(scores), dim=-1)\n\n        if self.save_attention:\n            if self.attention_dict is not None and self.name is not None:\n                self.attention_dict[self.name] = scores\n\n        return scores, expectation_of_sampling\n\n    # Weights Initialization\n    def init_weights(self, init_range=0.1):\n        self.linear_x.weight.data.uniform_(-init_range, init_range)\n        self.linear_q.weight.data.uniform_(-init_range, init_range)\n        self.out_linear.weight.data.uniform(-init_range, init_range)\n\n\nclass GoalNetwork(nn.Module):\n    """"""\n    This network uses the self, multi and vanilla attention modules and returns the\n    top n vectors according to the softmax probabilities.\n    """"""\n\n    def __init__(self, input_dim, query_dim, embedding_dim, num_hidden,\n                 output_features, activation, use_additive, use_token2token,\n                 use_self_attn, save_attention=False, attention_dict=None,\n                 use_multi_attn=False):\n        """"""\n\n        :param input_dim:\n        :param query_dim:\n        :param embedding_dim:\n        :param num_hidden:\n        :param output_features:\n        :param activation:\n        :param use_additive:\n        :param use_token2token:\n        :param use_self_attn:\n        :param save_attention:\n        :param attention_dict:\n        """"""\n        super(GoalNetwork, self).__init__()\n        self.input_dim = input_dim\n        self.query_dim = query_dim\n        self.embedding_dim = embedding_dim\n        self.num_hidden = num_hidden\n        self.output_features = output_features\n        self.activation = activation\n        self.additive = use_additive\n        self.use_self_attn = use_self_attn\n        self.token2token = use_token2token\n        self.save_attn = save_attention\n        self.attention_dict = attention_dict\n        self.multi_attn = use_multi_attn\n        self.self_attn = None\n        self.self_linear = None\n\n        if self.use_self_attn:\n            self.self_attn = SelfAttention(input_dim=self.input_dim, embedding_dim=self.embedding_dim,\n                                           query_dim=self.query_dim, num_hidden=self.num_hidden,\n                                           output_features=self.embedding_dim, activation=self.activation,\n                                           use_additive=self.additive, token2token=self.token2token,\n                                           save_attention=self.save_attn, attention_dict=self.attention_dict,\n                                           name=\'SelfAttention\')\n            self.self_linear = nn.Linear(in_features=1, out_features=self.num_hidden)\n\n        if self.multi_attn:\n            # Use multi dimensional attention\n            self.attn = MultiAttention(input_dim=self.input_dim, embedding_dim=self.embedding_dim,\n                                           query_dim=self.query_dim, num_hidden=self.num_hidden,\n                                           output_features=self.embedding_dim, activation=self.activation,\n                                           use_additive=self.additive,\n                                           save_attention=self.save_attn, attention_dict=self.attention_dict,\n                                           name=\'MultiAttention\')\n\n        else:\n            # Use the traditional vanilla attention\n            self.attn = VanillaAttention(input_dim=self.input_dim, embedding_dim=self.embedding_dim,\n                                           query_dim=self.query_dim, num_hidden=self.num_hidden,\n                                           output_features=1, activation=self.activation,\n                                           use_additive=self.additive,\n                                           save_attention=self.save_attn, attention_dict=self.attention_dict,\n                                           name=\'VanillaAttention\')\n\n        self.linear_attn = nn.Linear(in_features=1, out_features=self.num_hidden)\n        self.output_linear = nn.Linear(in_features=num_hidden, out_features=1)\n\n    def forward(self, input_sequence, current_embedding):\n        scores_self, expectations_self = None, None\n        if self.self_attn is not None:\n            scores_self, expectations_self = self.self_attn(input_sequence)\n        scores, expectation = self.attn(input_sequence, current_embedding)\n        if expectations_self is not None and self.self_linear is not None:\n            expectations_self = self.self_linear(expectations_self)\n        expectation = self.linear_attn(expectation)\n        if expectations_self is not None:\n            e = expectation + expectations_self\n        else:\n            e = expectation\n        e = self.activation(e)\n        o = self.output_linear(e)\n        return o\n\n\n\n\n\n'"
models/cvae_gan.py,26,"b'""""""\nThis script contains an implementation of the CVAEGAN paper.\n\nI have not used class conditional gan and vae for this implementation.\nI have just used the the ideas from the paper for stable training of the GAN\nand VAE parts of the network.\n\n\nFor the GAN stability tricks used please refer to\nhttps://github.com/soumith/ganhacks\n\n""""""\n\nimport torch\nprint(torch.__version__)\nimport torch.nn as nn\nimport os\nimport scipy.misc as m\nimport numpy as np\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass Encoder(nn.Module):\n\n    """"""\n    The encoder network in the cvae-gan pipeline\n\n    Given an image, this network returns the latent encoding\n    for the image.\n\n    """"""\n\n    def __init__(self, conv_layers, conv_kernel_size,\n                 latent_space_dim, hidden_dim, use_cuda,\n                 height, width, input_channels,pool_kernel_size):\n        super(Encoder, self).__init__()\n\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.z_dim = latent_space_dim\n        self.hidden_dim = hidden_dim\n        self.use_cuda = use_cuda\n        self.height = height\n        self.width = width\n        self.in_channels = input_channels\n        self.pool_size = pool_kernel_size\n\n        # Encoder Architecture\n\n        # 1st Stage\n        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers*2)\n        # Use strided convolution instead of maxpooling for generative models.\n        self.pool = nn.MaxPool2d(kernel_size=pool_kernel_size)\n\n        # 2nd Stage\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*4,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*4)\n        # Use strided convolution instead of maxpooling for generative models.\n        self.pool2  = nn.MaxPool2d(kernel_size=pool_kernel_size)\n\n        # Linear Layer\n        self.linear1 = nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*4, out_features=self.hidden_dim)\n        self.latent_mu = nn.Linear(in_features=self.hidden_dim, out_features=self.z_dim)\n        self.latent_logvar = nn.Linear(in_features=self.hidden_dim, out_features=self.z_dim)\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.leaky_relu = nn.LeakyReLU(inplace=True)\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        nn.init.xavier_uniform_(self.linear1.weight)\n        nn.init.xavier_uniform_(self.latent_mu.weight)\n        nn.init.xavier_uniform_(self.latent_logvar.weight)\n\n\n    def encode(self, x):\n        # Encoding the input image to the mean and var of the latent distribution\n        bs, _, _, _ = x.shape\n\n        conv1 = self.conv1(x)\n        conv1 = self.bn1(conv1)\n        conv1 = self.relu(conv1)\n        conv2 = self.conv2(conv1)\n        conv2 = self.bn2(conv2)\n        conv2 = self.relu(conv2)\n        #pool = self.pool(conv2)\n\n        conv3 = self.conv3(conv2)\n        conv3 = self.bn3(conv3)\n        conv3 = self.relu(conv3)\n        conv4 = self.conv4(conv3)\n        conv4 = self.bn4(conv4)\n        conv4 = self.relu(conv4)\n        #pool2 = self.pool2(conv4)\n\n        pool2 = conv4.view((bs, -1))\n\n        linear = self.linear1(pool2)\n        linear = self.relu(linear)\n        mu = self.latent_mu(linear)\n        logvar = self.latent_logvar(linear)\n\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Reparameterization trick as shown in the auto encoding variational bayes paper\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def forward(self, input):\n        mu, logvar = self.encode(input)\n        z = self.reparameterize(mu, logvar)\n        return z, mu, logvar\n\n\nclass Generator(nn.Module):\n\n    """"""\n    The generator/decoder in the CVAE-GAN pipeline\n\n    Given a latent encoding or a noise vector, this network outputs an image.\n\n    """"""\n\n    def __init__(self, latent_space_dimension, conv_kernel_size,\n                 conv_layers, hidden_dim, height, width, input_channels):\n        super(Generator, self).__init__()\n\n        self.z_dimension = latent_space_dimension\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.hidden = hidden_dim\n        self.height = height\n        self.width = width\n        self.input_channels = input_channels\n\n        # Decoder/Generator Architecture\n        self.linear_decoder = nn.Linear(in_features=self.z_dimension,\n                                        out_features=self.height//16 * self.width//16 * self.conv_layers*4)\n        #self.bnl = nn.BatchNorm2d(se)\n\n        # Deconvolution layers\n        self.conv1 = nn.ConvTranspose2d(in_channels=self.conv_layers*4,\n                                        out_channels=self.conv_layers*4, kernel_size=self.conv_kernel_size,\n                                        stride=2)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers*4)\n\n        self.conv2 = nn.ConvTranspose2d(in_channels=self.conv_layers*4, out_channels=self.conv_layers*2,\n                                        kernel_size=self.conv_kernel_size, stride=2)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers*2)\n\n        self.conv3 = nn.ConvTranspose2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                                        kernel_size=self.conv_kernel_size, stride=2)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n\n        self.conv4 = nn.ConvTranspose2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers,\n                                        kernel_size=self.conv_kernel_size, stride=2)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers)\n\n        self.output = nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.input_channels,\n                                kernel_size=self.conv_kernel_size-1, stride=1)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.leaky_relu = nn.LeakyReLU(inplace=True)\n\n        # Use dropouts in the generator to stabilize the training\n        self.dropout = nn.Dropout()\n\n        self.sigmoid_output = nn.Sigmoid()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        nn.init.xavier_uniform_(self.linear_decoder.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, z):\n        z  = self.linear_decoder(z)\n        z = self.leaky_relu(z)\n\n        z =  z.view((-1, self.conv_layers*4, self.height//16, self.width//16))\n\n        z = self.conv1(z)\n        z = self.bn1(z)\n        z = self.leaky_relu(z)\n        z = self.conv2(z)\n        z = self.bn2(z)\n        z = self.leaky_relu(z)\n        #z = self.dropout(z)\n\n        z = self.conv3(z)\n        z = self.bn3(z)\n        z = self.leaky_relu(z)\n        z = self.conv4(z)\n        z = self.bn4(z)\n        z = self.leaky_relu(z)\n        #z = self.dropout(z)\n\n        output = self.output(z)\n        output = self.sigmoid_output(output)\n\n        return output\n\n\nclass Discriminator(nn.Module):\n\n    """"""\n    The discriminator network in the CVAEGAN pipeline\n\n    This network distinguishes the fake images from the real\n\n    """"""\n\n    def __init__(self, input_channels, conv_layers,\n                 pool_kernel_size, conv_kernel_size,\n                 height, width, hidden):\n\n        super(Discriminator, self).__init__()\n\n        self.in_channels = input_channels\n        self.conv_layers = conv_layers\n        self.pool = pool_kernel_size\n        self.conv_kernel_size = conv_kernel_size\n        self.height = height\n        self.width = width\n        self.hidden = hidden\n\n        # Discriminator architecture\n        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers*2)\n        # Use strided convolution in place of max pooling\n        self.pool_1 = nn.MaxPool2d(kernel_size=self.pool)\n\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*4,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*4)\n        # Use strided convolution in place of max pooling\n        self.pool_2 = nn.MaxPool2d(kernel_size=self.pool)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.leaky_relu = nn.LeakyReLU(inplace=True)\n\n        # Fully Connected Layer\n        self.hidden_layer1 = nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*4,\n                                       out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=1)\n        self.sigmoid_output = nn.Sigmoid()\n\n        # Weight initialization\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n\n        nn.init.xavier_uniform_(self.hidden_layer1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n        # Dropout layer\n        self.dropout = nn.Dropout()\n\n    def forward(self, input):\n\n        conv1 = self.conv1(input)\n        conv1 = self.bn1(conv1)\n        conv1 = self.leaky_relu(conv1)\n        conv2 = self.conv2(conv1)\n        conv2 = self.bn2(conv2)\n        conv2 = self.leaky_relu(conv2)\n        #pool1 = self.pool_1(conv2)\n\n        conv3 = self.conv3(conv2)\n        conv3 = self.bn3(conv3)\n        conv3 = self.leaky_relu(conv3)\n        conv4 = self.conv4(conv3)\n        conv4 = self.bn4(conv4)\n        conv4 = self.leaky_relu(conv4)\n        #pool2 = self.pool_2(conv4)\n\n        pool2 = conv4.view((-1, self.height//16*self.width//16*self.conv_layers*4))\n\n        feature_mean = pool2\n\n        hidden = self.hidden_layer1(pool2)\n        hidden = self.leaky_relu(hidden)\n\n        #feature_mean = hidden\n\n        output = self.output(hidden)\n        output = self.sigmoid_output(output)\n\n        return output, feature_mean\n\n\nclass CVAEGAN(object):\n\n    """"""\n\n    The complete CVAEGAN Class containing the following models\n\n    1. Encoder\n    2. Generator/Decoder\n    3. Discriminator\n\n    """"""\n\n    def __init__(self, encoder,\n                 batch_size,\n                 num_epochs,\n                 random_seed, dataset,\n                 generator, discriminator,\n                 encoder_lr, generator_lr,\n                 discriminator_lr, use_cuda,\n                 output_folder, test_dataset,\n                 inference_output_folder,\n                 tensorboard_summary_writer,\n                 encoder_weights=None, generator_weights=None,\n                 shuffle=True,\n                 discriminator_weights=None):\n\n        self.encoder = encoder\n        self.generator = generator\n        self.discriminator = discriminator\n\n        self.shuffle = shuffle\n        self.dataset = dataset\n        self.test_dataset = test_dataset\n        self.e_lr = encoder_lr\n        self.g_lr = generator_lr\n        self.d_lr = discriminator_lr\n        self.seed = random_seed\n        self.batch = batch_size\n        self.num_epochs = num_epochs\n        self.output_folder = output_folder\n        self.inference_output_folder = inference_output_folder\n\n        self.e_optim = optim.Adam(lr=self.e_lr, params=self.encoder.parameters())\n        self.g_optim = optim.Adam(lr=self.g_lr, params=self.generator.parameters())\n        # GAN stability trick\n        self.d_optim = optim.Adam(lr=self.d_lr, params=self.discriminator.parameters())\n\n        self.encoder_weights = encoder_weights\n        self.generator_weights = generator_weights\n        self.discriminator_weights = discriminator_weights\n        self.use_cuda = use_cuda\n\n        if use_cuda:\n            # Use cuda for GPU utilization\n            self.encoder = self.encoder.cuda()\n            self.generator = self.generator.cuda()\n            self.discriminator = self.discriminator.cuda()\n\n        # Tensorboard logger\n        self.tb = tensorboard_summary_writer\n\n    def set_seed(self):\n        # Set the seed for reproducible results\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n    def get_dataloader(self):\n        # Generates the dataloader for the images for training\n\n        dataset_loader = DataLoader(self.dataset,\n                                    batch_size=self.batch,\n                                    shuffle=self.shuffle)\n\n        return dataset_loader\n\n    def get_test_dataloader(self):\n        # Generates the dataloader for the images for testing\n        dataset_loader = DataLoader(self.test_dataset,\n                                    batch_size=self.batch)\n        return dataset_loader\n\n    def save_model(self, output, model):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the cvaegan model"")\n        torch.save(\n            model.state_dict(),\n            \'{}/cvaegan.pt\'.format(output)\n        )\n\n    def klloss(self, mu, logvar):\n        # Kullback Liebler divergence loss for the VAE\n        mu_sum_sq = (mu*mu).sum(dim=1)\n        sigma = logvar.mul(0.5).exp_()\n        sig_sum_sq = (sigma * sigma).sum(dim=1)\n        log_term = (1 + torch.log(sigma ** 2)).sum(dim=1)\n        kldiv = -0.5 * (log_term - mu_sum_sq - sig_sum_sq)\n\n        return kldiv.mean()\n\n    def discriminator_loss(self, x, recon_x, recon_x_noise, std):\n        labels_x = torch.FloatTensor(x.shape[0])\n        labels_recon_x = torch.FloatTensor(recon_x.shape[0])\n        labels_recon_x_noise = torch.FloatTensor(recon_x_noise.shape[0])\n\n        # Labels for the real images are 1 and for the fake are 0\n        labels_x.data.fill_(1)\n        labels_recon_x.data.fill_(0)\n        labels_recon_x_noise.data.fill_(0)\n\n        if self.use_cuda:\n            labels_x = labels_x.cuda()\n            labels_recon_x = labels_recon_x.cuda()\n            labels_recon_x_noise = labels_recon_x_noise.cuda()\n\n        # Adding instance noise to improve the stability of the\n        # Discriminator\n\n        mean = torch.zeros(x.shape)\n        noise_x = torch.normal(mean=mean, std=std)\n        noise_recon = torch.normal(mean=mean, std=std)\n        noise_recon_noise = torch.normal(mean=mean, std=std)\n\n        if self.use_cuda:\n            noise_x = noise_x.cuda()\n            noise_recon = noise_recon.cuda()\n            noise_recon_noise = noise_recon_noise.cuda()\n\n        x = x+noise_x\n        recon_x = recon_x + noise_x\n        recon_x_noise = recon_x_noise + noise_x\n\n        o_x, _ = self.discriminator(x)\n        o_x_recon, _ = self.discriminator(recon_x.detach())\n        o_x_recon_noise, _ = self.discriminator(recon_x_noise.detach())\n\n        loss_real = nn.BCELoss()(o_x, labels_x)\n        loss_fake = nn.BCELoss()(o_x_recon, labels_recon_x)\n        loss_fake_noise = nn.BCELoss()(o_x_recon_noise, labels_recon_x_noise)\n        loss = torch.mean(loss_fake+loss_fake_noise+loss_real)\n        return loss\n\n    def generator_discriminator_loss(self, x,\n                                     recon_x_noise, recon_x,\n                                     lambda_1, lambda_2, std):\n\n        mean = torch.zeros(x.shape)\n        noise_x = torch.normal(mean=mean, std=std)\n        noise_recon = torch.normal(mean=mean, std=std)\n        noise_recon_noise = torch.normal(mean=mean, std=std)\n\n        if self.use_cuda:\n            noise_x = noise_x.cuda()\n            noise_recon = noise_recon.cuda()\n            noise_recon_noise = noise_recon_noise.cuda()\n\n        x = x + noise_x\n        recon_x = recon_x + noise_x\n        recon_x_noise = recon_x_noise + noise_x\n\n        # Generator Discriminator loss\n        _, fd_x = self.discriminator(x)\n        _, fd_x_noise = self.discriminator(recon_x_noise)\n\n        fd_x = torch.mean(fd_x, 0)\n        fd_x_noise = torch.mean(fd_x_noise, 0)\n\n        loss_g_d = nn.MSELoss()(fd_x_noise.detach(), fd_x.detach())\n\n        # Generator Loss\n        reconstruction_loss = nn.MSELoss()(recon_x, x)\n        _, fd_x_r = self.discriminator(x)\n        _, fd_x_f = self.discriminator(recon_x)\n        feature_matching_reconstruction_loss = nn.MSELoss()(fd_x_f.detach(), fd_x_r.detach())\n\n        loss_g = reconstruction_loss + feature_matching_reconstruction_loss\n\n        loss = lambda_1*loss_g_d +  lambda_2*loss_g\n\n        return loss, loss_g, loss_g_d\n\n    def sample_random_noise(self, z):\n        # Sample a random noise vector for the Generator input\n        # Sample from a normal distribution with mean 0 and std 1 (similar to P(z) optimized by the VAE)\n        noise = torch.randn(z.shape)\n        noise = Variable(noise)\n        #noise.data.uniform_(-1.0, 1.0)\n        if self.use_cuda:\n            noise = noise.cuda()\n        return noise\n\n    def linear_annealing_variance(self, std, epoch):\n        # Reduce the standard deviation over the epochs\n        if std > 0:\n            std -= epoch*0.1\n        else:\n            std = 0\n        return std\n\n    def train(self, lambda_1, lambda_2):\n        std = 1\n        for epoch in range(self.num_epochs):\n            cummulative_loss_enocder = 0\n            cummulative_loss_discriminator = 0\n            cummulative_loss_generator = 0\n            std = self.linear_annealing_variance(std, epoch)\n            for i_batch, sampled_batch in enumerate(self.get_dataloader()):\n                images = sampled_batch[\'image\']\n                images = Variable(images)\n                if self.use_cuda:\n                    images = images.cuda()\n\n                latent_vectors, mus, logvars = self.encoder(images)\n                loss_kl = self.klloss(mus, logvar=logvars)\n\n                # Reconstruct images from latent vectors - x_f\n                recon_images = self.generator(latent_vectors.detach())\n\n                # Reconstruct images from random noise - x_p\n                random_noise = self.sample_random_noise(latent_vectors)\n                recon_images_noise = self.generator(random_noise)\n\n\n                if epoch%5 ==0:\n                    self.save_image_tensor(reconstructed_images=recon_images, output=self.inference_output_folder,\n                                           batch_number=str(epoch)+ \'_\'+str(i_batch))\n\n                # Discriminator Loss with standard deviation\n                loss_d = self.discriminator_loss(x=images, recon_x=recon_images,\n                                                 recon_x_noise=recon_images_noise,\n                                                 std=std)\n\n                cummulative_loss_discriminator += loss_d\n\n\n                # Generator Loss\n                loss_g, l_g, l_g_d = self.generator_discriminator_loss(x=images, recon_x_noise=recon_images_noise,\n                                                           recon_x=recon_images, lambda_1=2, lambda_2=1, std=std)\n\n\n                cummulative_loss_generator += loss_g\n\n                # Encoder Loss\n                loss_e = lambda_1*loss_kl + lambda_2*l_g.detach()\n\n                cummulative_loss_enocder += loss_e\n\n                # Make the gradient updates\n                self.d_optim.zero_grad()\n                loss_d.backward()\n                self.d_optim.step()\n\n                self.g_optim.zero_grad()\n                loss_g.backward()\n                self.g_optim.step()\n\n                self.e_optim.zero_grad()\n                loss_e.backward()\n                self.e_optim.step()\n\n            print(\'Loss Encoder for \', str(epoch), \' is \',\n                  cummulative_loss_enocder/575)\n            print(\'Loss Generator for \', str(epoch), \' is \',\n                  cummulative_loss_generator/575)\n            print(\'Loss Discriminator for \', str(epoch), \' is \',\n                  cummulative_loss_discriminator/575)\n\n            # Log the data onto tensorboard\n#            self.tb.write(\'encoder/loss\', cummulative_loss_enocder/len(self.get_dataloader()), epoch)\n #           self.tb.write(\'generator/loss\', cummulative_loss_generator / len(self.get_dataloader()), epoch)\n  #          self.tb.write(\'discriminator/loss\', cummulative_loss_discriminator / len(self.get_dataloader()), epoch)\n\n        # Save the models\n        self.save_model(output=self.output_folder+\'encoder/\', model=self.encoder)\n        self.save_model(output=self.output_folder+\'generator/\', model=self.generator)\n        self.save_model(output=self.output_folder+\'discriminator/\', model=self.discriminator)\n\n        # Export and close the Tb Writer\n        tb_json = \'tb.json\'\n        path = os.path.join(self.output_folder, tb_json)\n        self.tb.export(path, close_writer=True)\n\n    def load_model(self, weights, model):\n        # Load the model from the saved weights file\n        if self.use_cuda:\n            model_state_dict = torch.load(weights)\n        else:\n            model_state_dict = torch.load(weights, map_location=\'cpu\')\n        model.load_state_dict(model_state_dict)\n        return model\n\n    def save_image_tensor(self, reconstructed_images, output, batch_number):\n        for i, r_i in enumerate(reconstructed_images):\n            decoded_image = r_i.data.cpu().numpy()\n            #decoded_image = np.squeeze(decoded_image, 0)\n            decoded_image = np.transpose(decoded_image, (1, 2, 0))\n            path = os.path.join(output, str(batch_number)+ \'_\' +str(i) + \'.jpg\')\n            m.imsave(path, decoded_image)\n\n    def inference(self):\n        if self.encoder_weights is not None:\n            self.encoder = self.load_model(weights=self.encoder_weights, model=self.encoder)\n        if self.generator_weights is not None:\n            self.generator = self.load_model(weights=self.generator_weights, model=self.generator)\n        if self.discriminator is not None:\n            self.discriminator = self.load_model(weights=self.discriminator_weights, model=self.discriminator)\n\n        if self.use_cuda:\n            self.encoder = self.encoder.cuda()\n            self.generator = self.generator.cuda()\n            self.discriminator = self.discriminator.cuda()\n\n        # Set the models in evaluation mode\n        self.encoder.eval()\n        self.generator.eval()\n        self.discriminator.eval()\n\n        for i_batch, sampled_batch in tqdm(enumerate(self.get_test_dataloader())):\n            images = sampled_batch[\'image\']\n            images = Variable(images)\n            if self.use_cuda:\n                images = images.cuda()\n            latent_vectors, mus, logvars = self.encoder(images)\n            z  = self.sample_random_noise(latent_vectors)\n            # Reconstruct images from latent vectors\n            reconstructed_images = self.generator(z)\n            t, _ = self.discriminator(reconstructed_images)\n            t, _ = self.discriminator(images)\n            # Save the reconstructed images\n            self.save_image_tensor(reconstructed_images=reconstructed_images,\n                                   output=self.inference_output_folder, batch_number=i_batch)\n            #self.save_image_tensor(reconstructed_images=images, output=self.inference_output_folder, batch_number=i_batch)\n\n        print(""Saved the images to "", self.inference_output_folder)\n'"
models/empowerment_models.py,60,"b'""""""\n\nThis script contains an implementation of the models for learning an intrinsically motivated\nagent trained empowerment. The policy is trained using deep q learning.\n\n""""""\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom Utils.utils import *\nfrom Memory import Buffer\nimport torch.optim as optim\nimport torch.nn.functional as F\nUSE_CUDA = torch.cuda.is_available()\nfrom collections import deque, defaultdict\nimport time\nimport numpy as np\nfrom torch.distributions import Normal\nfrom torch.distributions.categorical import Categorical\nimport Environments.env_wrappers as env_wrappers\nimport random\nimport math\n#from tensorboardX import SummaryWriter\nimport torch.nn.functional as F\nimport torch.multiprocessing as mp\ntorch.backends.cudnn.enabled = False\nimport gym_minigrid\n\ndef epsilon_greedy_exploration():\n    epsilon_start = 1.0\n    epsilon_final = 0.01\n    epsilon_decay = 50000\n    epsilon_by_frame = lambda frame_idx: epsilon_final + (epsilon_start - epsilon_final) * math.exp(\n        -1. * frame_idx / epsilon_decay)\n\n    return epsilon_by_frame\n\n# L2 normalize the vector\ndef l2_normalize(tensor):\n    l2_norm = torch.sqrt(torch.sum(torch.pow(tensor, 2)))\n    return tensor/l2_norm\n\n# Random Encoder\nclass Encoder(nn.Module):\n\n    def __init__(self,\n                 state_space,\n                 conv_kernel_size,\n                 conv_layers,\n                 hidden,\n                 input_channels,\n                 height,\n                 width\n                 ):\n        super(Encoder, self).__init__()\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.hidden = hidden\n        self.state_space = state_space\n        self.input_channels = input_channels\n        self.height = height\n        self.width = width\n\n        # Random Encoder Architecture\n        self.conv1 = nn.Conv2d(in_channels=self.input_channels,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2, padding=1)\n\n        # Use batchnormalization in encoder to stabilize the training\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers)\n\n        # Relu activation\n        self.relu = nn.ReLU(inplace=True)\n\n        # Hidden Layers\n        self.hidden_1 = nn.Linear(in_features=self.height // 4 * self.width // 4 * self.conv_layers,\n                                  out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.state_space)\n        self.tanh_activ = nn.Tanh()\n\n        # Initialize the weights of the network (Since this is a random encoder, these weights will\n        # remain static during the training of other networks).\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        #nn.init.xavier_uniform_(self.bn1.weight)\n        #nn.init.xavier_uniform_(self.bn2.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state):\n        state = torch.unsqueeze(state, dim=0)\n        x = self.conv1(state)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.relu(x)\n        x = x.view((-1, self.height//4*self.width//4*self.conv_layers))\n        x = self.hidden_1(x)\n        x = self.relu(x)\n        encoded_state = self.output(x)\n        #encoded_state = self.tanh_activ(encoded_state)\n        # L2 Normalize the output of the encoder\n        #encoded_state = l2_normalize(encoded_state)\n        return encoded_state\n\nclass inverse_dynamics_distribution(nn.Module):\n    \n    def __init__(self, state_space,\n                 action_space,\n                 height, width,\n                 conv_kernel_size,\n                 conv_layers, hidden,\n                 use_encoding=True):\n        super(inverse_dynamics_distribution, self).__init__()\n        self.state_space = state_space\n        self.action_space = action_space\n        self.height = height\n        self.width = width\n        self.conv_kernel_size = conv_kernel_size\n        self.hidden = hidden\n        self.conv_layers = conv_layers\n        self.use_encoding = use_encoding\n\n        # Inverse Dynamics Architecture\n\n        # Given the current state and the next state, this network predicts the action\n\n        self.layer1 = nn.Linear(in_features=self.state_space*2, out_features=self.hidden)\n        self.layer2 = nn.Linear(in_features=self.hidden, out_features=self.hidden)\n        self.layer3 = nn.Linear(in_features=self.hidden, out_features=self.hidden * 2)\n        self.layer4 = nn.Linear(in_features=self.hidden * 2, out_features=self.hidden * 2)\n        self.hidden_1 = nn.Linear(in_features=self.hidden * 2, out_features=self.hidden)\n\n        self.conv1 = nn.Conv2d(in_channels=self.input_channels*2,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers,\n                               out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers,\n                               out_channels=self.conv_layers * 2,\n                               kernel_size=self.conv_kernel_size, stride=2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers * 2,\n                               out_channels=self.conv_layers * 2,\n                               kernel_size=self.conv_kernel_size, stride=2)\n\n        # Relu activation\n        self.relu = nn.ReLU(inplace=True)\n\n        # Hidden Layers\n        self.hidden_1 = nn.Linear(in_features=self.height // 16 * self.width // 16 * self.conv_layers * 2,\n                                  out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.action_space)\n\n        # Output activation function\n        self.output_activ = nn.Softmax()\n\n    def forward(self, current_state, next_state):\n        state = torch.cat([current_state, next_state], dim=-1)\n        if self.use_encoding:\n            x = self.layer1(state)\n            x = self.relu(x)\n            x = self.layer2(x)\n            x = self.relu(x)\n            x = self.layer3(x)\n            x = self.relu(x)\n            x = self.layer4(x)\n            x = self.relu(x)\n            x = self.hidden_1(x)\n        else:\n            x = self.conv1(state)\n            x = self.relu(x)\n            x = self.conv2(x)\n            x = self.relu(x)\n            x = self.conv3(x)\n            x = self.relu(x)\n            x = self.conv4(x)\n            x = self.relu(x)\n            x = x.view((-1, self.height // 16 * self.width // 16 * self.conv_layers * 2))\n            x = self.hidden_1(x)\n\n        x = self.relu(x)\n        x = self.output(x)\n        output = self.output_activ(x)\n\n        return output\n\n\nclass MDN_LSTM(nn.Module):\n\n    """"""\n\n    This follows the Mixture Density Network LSTM defined in the paper\n    World Models, Ha et al.\n\n    """"""\n\n    def __init__(self, state_space,\n                 action_space,\n                 lstm_hidden,\n                 gaussians,\n                 num_lstm_layers):\n        super(MDN_LSTM, self).__init__()\n        self.state_space = state_space\n        self.action_space = action_space\n\n        self.lstm_hidden = lstm_hidden\n        self.gaussians = gaussians\n        self.num_lstm_layers = num_lstm_layers\n\n        #self.sequence_length = sequence_length\n\n        self.hidden = self.init_hidden(self.sequence_length)\n\n        # Define the RNN\n        self.rnn = nn.LSTM(self.state_space+self.action_space, self.lstm_hidden, self.num_lstm_layers)\n\n        # Define the fully connected layer\n        self.s_pi = nn.Linear(self.lstm_hidden, self.state_space*self.gaussians)\n        self.s_sigma = nn.Linear(self.lstm_hidden, self.state_space*self.gaussians)\n        self.s_mean = nn.Linear(self.lstm_hidden, self.state_space*self.gaussians)\n\n    def init_hidden(self, sequence):\n        hidden = torch.zeros(self.num_lstm_layers, sequence, self.lstm_hidden)\n        cell = torch.zeros(self.num_lstm_layers, sequence, self.lstm_hidden)\n        return hidden, cell\n\n    def forward(self, states, actions):\n        self.rnn.flatten_parameters()\n        seq_length = actions.size()[1]\n        self.hidden = self.init_hidden(seq_length)\n\n        inputs = torch.cat([states, actions], dim=-1)\n        s, self.hidden = self.rnn(inputs, self.hidden)\n\n        pi = self.s_pi(s).view(-1, seq_length, self.gaussians, self.state_space)\n        pi = F.softmax(pi, dim=2)\n\n        sigma = torch.exp(self.s_sigma(s)).view(-1, seq_length,\n                                                self.gaussians, self.state_space)\n\n        mu = self.s_mean(s).view(-1, seq_length, self.gaussians, self.state_space)\n\n        return pi, sigma, mu\n\n\nclass forward_dynamics_lstm(object):\n\n    def __init__(self,\n                 sequence_length,\n                 state_space,\n                 epsilon,\n                 mdn_lstm,\n                 num_epochs,\n                 learning_rate,\n                 print_epoch=5\n                 ):\n        self.seq = sequence_length\n        self.state_space = state_space\n        self.eps = epsilon\n        self.model = mdn_lstm\n        self.num_epochs = num_epochs\n        self.lr = learning_rate\n        self.optimizer = optim.Adam(lr=self.lr, params=self.model.parameters())\n        self.p_epoch = print_epoch\n\n    def mdn_loss_function(self, out_pi, out_sigma, out_mu, target_next_states):\n        y = target_next_states.view(-1, self.seq, 1, self.state_space)\n        result = Normal(loc=out_mu, scale=out_sigma)\n        result = torch.exp(result.log_prob(y))\n        result = torch.sum(result * out_pi, dim=2)\n        result = -torch.log(self.eps + result)\n        return torch.mean(result)\n\n    def train_on_batch(self, states, actions, next_states):\n        for epoch in range(self.num_epochs):\n            pis, sigmas, mus = self.model(states, actions)\n            loss = self.mdn_loss_function(out_pi=pis, out_sigma=sigmas, out_mu=mus, target_next_states=next_states)\n            self.optimizer.zero_grad()\n            loss.backward()\n            self.optimizer.step()\n\n            if epoch % self.p_epoch == 0:\n                print(\'LSTM Loss: \', loss)\n\n    def sample_next_state(self, state, action):\n        pis, sigmas, mus = self.model(state, action)\n        mixt = Categorical(torch.exp(pis)).sample().item()\n\n        next_state = mus[:, mixt, :]\n\n        return next_state\n\nclass forward_dynamics_model(nn.Module):\n\n    def __init__(self,\n                 state_space,\n                 action_space,\n                 hidden,\n                 use_encoding=True,\n                 return_gaussians=False):\n        super(forward_dynamics_model, self).__init__()\n        self.state_space = state_space\n        self.action_space = action_space\n        self.hidden = hidden\n        self.use_encoding = use_encoding\n        self.return_gaussians = return_gaussians\n\n        # Forward Dynamics Model Architecture\n\n        # Given the current state and the action, this network predicts the next state\n\n        self.layer1 = nn.Linear(in_features=self.state_space, out_features=self.hidden)\n        self.layer2 = nn.Linear(in_features=self.hidden, out_features=self.hidden*2)\n        self.layer3 = nn.Linear(in_features=self.hidden*2+self.action_space, out_features=self.hidden*2)\n        if self.return_gaussians:\n            self.output_mu = nn.Linear(in_features=self.hidden, out_features=self.state_space)\n            self.output_logvar = nn.Linear(in_features=self.hidden, out_features=self.state_space)\n        self.output = nn.Linear(in_features=self.hidden*2, out_features=self.state_space)\n\n        # Relu activation\n        self.relu = nn.ReLU(inplace=True)\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.layer1.weight)\n        nn.init.xavier_uniform_(self.layer2.weight)\n        nn.init.xavier_uniform_(self.layer3.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def one_hot_action(self, batch_size, action):\n        ac = torch.zeros(batch_size, self.action_space)\n        for i in range(batch_size):\n            ac[i, action[i]] = 1\n        return ac\n\n    def forward(self, current_state, action):\n        bs, _ = current_state.shape\n        x = self.layer1(current_state)\n        x = self.relu(x)\n        x = self.layer2(x)\n        x = self.relu(x)\n        action = action.unsqueeze(1)\n        ac = self.one_hot_action(batch_size=bs, action=action)\n        x = torch.cat([x, ac], dim=-1)\n        x = self.layer3(x)\n        x = self.relu(x)\n        if self.return_gaussians:\n            output_mu, output_logvar = self.output_mu(x), self.output_logvar(x)\n            return output_mu, output_logvar\n        output = self.output(x)\n        return output\n\n\nclass QNetwork(nn.Module):\n    def __init__(self,\n                 env,\n                 state_space,\n                 action_space,\n                 hidden):\n        super(QNetwork, self).__init__()\n\n        self.state_space = state_space\n        self.action_space = action_space\n        self.hidden = hidden\n        self.env = env\n\n        self.layers = nn.Sequential(\n            nn.Linear(self.state_space, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.action_space)\n        )\n\n    def forward(self, x):\n        return self.layers(x)\n\n    def act(self, state, epsilon):\n        if random.random() > epsilon:\n            state = Variable(torch.FloatTensor(state), requires_grad=False)\n            q_value = self.forward(state)\n            # Action corresponding to the max Q Value for the state action pairs\n\n            action = q_value.max(1)[1].view(1)\n        else:\n            action = torch.tensor([random.randrange(self.env.action_space.n)], dtype=torch.long)\n\n        return action\n\n# Convolutional Policy Network\nclass ConvolutionalQNetwork(nn.Module):\n    def __init__(self, env, state_height,\n                 state_width,\n                 action_space,\n                 input_channels,\n                 hidden):\n        super(ConvolutionalQNetwork, self).__init__()\n\n        self.height = state_height\n        self.width = state_width\n        self.hidden = hidden\n        self.env = env\n        self.action_space = action_space\n        self.in_channels = input_channels\n\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(self.in_channels, hidden, kernel_size=self.conv_kernel_size, stride=2, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(hidden, hidden, kernel_size=self.conv_kernel_size, stride=2, padding=1),\n            nn.ReLU(),\n        )\n\n        self.layers = nn.Sequential(\n            nn.Linear(self.height//4*self.width//4*self.hidden, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.action_space)\n        )\n\n        # Separate head for intrinsic rewards\n        self.intrinsic_layers = nn.Sequential(\n            nn.Linear(self.height // 4 * self.width // 4 * self.hidden, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.hidden),\n            nn.ReLU(inplace=True),\n            nn.Linear(self.hidden, self.action_space)\n        )\n\n    def forward(self, state):\n        conv = self.conv_layers(state)\n        # Return the Q values for all the actions\n        output = self.layers(conv)\n        return output\n\n    def act(self, state, epsilon):\n        if random.random() > epsilon:\n            state = Variable(torch.FloatTensor(state), requires_grad=False)\n            q_value = self.forward(state)\n            # Action corresponding to the max Q Value for the state action pairs\n\n            action = q_value.max(1)[1].view(1)\n        else:\n            action = torch.tensor([random.randrange(self.env.action_space.n)], dtype=torch.long)\n\n        return action\n\n\n\nclass StatisticsNetwork(nn.Module):\n\n    def __init__(self, state_space,\n                 action_space,\n                 hidden, output_dim):\n        super(StatisticsNetwork, self).__init__()\n\n        self.state_space = state_space\n        self.action_space = action_space\n        self.hidden = hidden\n        self.output_dim = output_dim\n\n        # Statistics Network Architecture\n        self.layer1 = nn.Linear(in_features=self.state_space+self.action_space,\n                                out_features=self.hidden)\n        self.layer2 = nn.Linear(in_features=self.hidden, out_features=self.hidden)\n        self.output = nn.Linear(in_features=self.hidden, out_features=self.output_dim)\n\n        # Relu activation\n        self.relu = nn.ReLU(inplace=True)\n\n    def one_hot_action(self, batch_size, action):\n        ac = torch.zeros(batch_size, self.action_space)\n        for i in range(batch_size):\n            ac[i, action[i]] = 1\n        return ac\n\n    def forward(self, next_state, action):\n        bs, _ = next_state.shape\n        action = action.unsqueeze(1)\n        ac = self.one_hot_action(batch_size=bs, action=action)\n        s = torch.cat([next_state, ac], dim=-1)\n        x = self.layer1(s)\n        x = self.relu(x)\n        x = self.layer2(x)\n        x = self.relu(x)\n        output = self.output(x)\n        return output\n\n\nclass EmpowermentTrainer(object):\n\n    def __init__(self,\n                 env,\n                 encoder,\n                 forward_dynamics,\n                 statistics_network,\n                 target_policy_network,\n                 policy_network,\n                 forward_dynamics_lr,\n                 stats_lr,\n                 policy_lr,\n                 num_train_epochs,\n                 num_frames,\n                 num_fwd_train_steps,\n                 num_stats_train_steps,\n                 fwd_dynamics_limit,\n                 stats_network_limit,\n                 policy_limit,\n                 size_replay_buffer,\n                 random_seed,\n                 polyak_constant,\n                 discount_factor,\n                 batch_size,\n                 action_space,\n                 model_output_folder,\n                 save_epoch,\n                 target_stats_network=None,\n                 target_fwd_dynamics_network=None,\n                 clip_rewards=True,\n                 clip_augmented_rewards=False,\n                 print_every=2000,\n                 update_network_every=2000,\n                 plot_every=5000,\n                 intrinsic_param=0.01,\n                 non_episodic_intrinsic=True,\n                 use_mine_formulation=True,\n                 use_cuda=False,\n                 save_models=True,\n                 plot_stats=False,\n                 verbose=True):\n\n        self.encoder = encoder\n        self.fwd = forward_dynamics\n        self.stats = statistics_network\n        self.use_cuda = use_cuda\n        self.policy_network = policy_network\n        self.target_policy_network = target_policy_network\n        self.output_folder = model_output_folder\n        self.use_mine_formulation = use_mine_formulation\n        self.env = env\n        self.train_epochs = num_train_epochs\n        self.num_frames = num_frames\n        self.num_fwd_train_steps = num_fwd_train_steps\n        self.num_stats_train_steps = num_stats_train_steps\n        self.fwd_lr = forward_dynamics_lr\n        self.stats_lr = stats_lr\n        self.policy_lr = policy_lr\n        self.random_seed = random_seed\n        self.save_models = save_models\n        self.plot_stats = plot_stats\n        self.verbose = verbose\n        self.intrinsic_param = intrinsic_param\n        self.save_epoch = save_epoch\n        self.clip_rewards = clip_rewards\n        self.clip_augmented_rewards = clip_augmented_rewards\n        self.max = torch.zeros(1)\n        self.min = torch.zeros(1)\n\n        self.fwd_limit = fwd_dynamics_limit\n        self.stats_limit = stats_network_limit\n        self.policy_limit = policy_limit\n\n        self.print_every = print_every\n        self.update_every = update_network_every\n        self.plot_every = plot_every\n        self.non_episodic = non_episodic_intrinsic\n\n        self.statistics = defaultdict(float)\n        self.combined_statistics = defaultdict(list)\n\n        self.target_stats_network = target_stats_network\n        self.target_fwd_dynamics_network = target_fwd_dynamics_network\n\n        # Fix the encoder weights\n        for param in self.encoder.parameters():\n            param.requires_grad = False\n\n        self.replay_buffer = Buffer.ReplayBuffer(capacity=size_replay_buffer,\n                                                 seed=self.random_seed)\n\n        self.tau = polyak_constant\n        self.gamma = discount_factor\n        self.batch_size = batch_size\n        self.action_space = action_space\n\n        torch.manual_seed(self.random_seed)\n        if self.use_cuda:\n            torch.cuda.manual_seed(self.random_seed)\n\n        if self.use_cuda:\n            self.encoder = self.encoder.cuda()\n            self.invd = self.invd.cuda()\n            self.fwd = self.fwd.cuda()\n            self.policy_network = self.policy_network.cuda()\n            self.source_distribution = self.source_distribution.cuda()\n\n        self.fwd_optim = optim.Adam(params=self.fwd.parameters(), lr=self.fwd_lr)\n        self.policy_optim = optim.Adam(params=self.policy_network.parameters(), lr=self.policy_lr)\n        self.stats_optim = optim.Adam(params=self.stats.parameters(), lr=self.stats_lr)\n        # Update the policy and target policy networks\n        self.update_networks()\n\n    def get_all_actions(self, action_space):\n        all_actions = []\n        for i in range(action_space):\n            all_actions.append(torch.LongTensor([i]))\n        return all_actions\n\n    def softplus(self, z):\n        return torch.log(1+ torch.exp(z))\n\n    # Store the transition into the replay buffer\n    def store_transition(self, state, new_state, action, reward, done):\n        self.replay_buffer.push(state, action, new_state, reward, done)\n\n    # Update the networks using polyak averaging\n    def update_networks(self, hard_update=True):\n        if hard_update:\n            for target_param, param in zip(self.target_policy_network.parameters(), self.policy_network.parameters()):\n                target_param.data.copy_(param.data)\n            for target_param, param in zip(self.target_stats_network.parameters(), self.stats.parameters()):\n                target_param.data.copy_(param.data)\n            for target_param, param in zip(self.target_fwd_dynamics_network.parameters(), self.fwd.parameters()):\n                target_param.data.copy_(param.data)\n        else:\n            for target_param, param in zip(self.target_policy_network.parameters(), self.policy_network.parameters()):\n                target_param.data.copy_(self.tau * param.data + target_param.data * (1.0 - self.tau))\n\n    # Train the policy network\n    def train_policy(self, batch, rewards, clip_gradients=True):\n\n        states = batch[\'states\']\n        new_states = batch[\'new_states\']\n        actions = batch[\'actions\']\n        dones = batch[\'dones\']\n\n        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n        # columns of actions taken\n\n        q_values = self.policy_network(states)\n        next_q_values = self.policy_network(new_states)\n        with torch.no_grad():\n            next_q_state_values = self.target_policy_network(new_states).detach()\n\n        q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n        next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n        expected_q_value = rewards + self.gamma * next_q_value * (1 - dones)\n        expected_q_value = expected_q_value.detach()\n        # Use smooth l1 loss with caution. refer to https://jaromiru.com/2017/05/27/on-using-huber-loss-in-deep-q-learning/\n        td_loss = F.smooth_l1_loss(q_value, expected_q_value)\n\n        self.policy_optim.zero_grad()\n        td_loss.backward()\n        if clip_gradients:\n            for param in self.policy_network.parameters():\n                param.grad.data.clamp_(-1, 1)\n        self.policy_optim.step()\n\n        return td_loss\n\n    def train_forward_dynamics(self, batch, clamp_gradients=False, use_difference_representation=True):\n\n        states = batch[\'states\']\n        new_states = batch[\'new_states\']\n        actions = batch[\'actions\']\n\n        if use_difference_representation:\n            # Under this representation, the model predicts the difference between the current state and the next state.\n            diff_new_states  = self.fwd(states, actions)\n            predicted_new_states = states + diff_new_states\n        else:\n            predicted_new_states = self.fwd(states, actions)\n\n        mse_error = F.mse_loss(predicted_new_states, new_states)\n        self.fwd_optim.zero_grad()\n        mse_error.backward()\n        # Clamp the gradients\n        self.fwd_optim.step()\n\n        return mse_error\n\n    def update_intrinsic_param(self, param, rewards):\n        t = torch.max(torch.FloatTensor([1]), torch.mean(rewards) )\n        new_param_val = param/t\n        return new_param_val\n\n    def train_statistics_network(self, batch, use_jenson_shannon_divergence=True):\n\n        states = batch[\'states\']\n        new_states = batch[\'new_states\']\n        actions = batch[\'actions\']\n        rewards = batch[\'rewards\']\n\n        all_actions = self.get_all_actions(self.action_space)\n        all_actions = Variable(torch.cat(all_actions))\n\n        new_state_marginals = []\n        for state in states:\n            state = state.expand(self.action_space, -1)\n            with torch.no_grad():\n                n_s = self.fwd(state, all_actions)\n            n_s = n_s.detach()\n            n_s = n_s + state\n            n_s = torch.mean(n_s, dim=0)\n            n_s = torch.unsqueeze(n_s, dim=0)\n            new_state_marginals.append(n_s)\n\n        new_state_marginals = tuple(new_state_marginals)\n        new_state_marginals = Variable(torch.cat(new_state_marginals), requires_grad=False)\n\n        p_sa = self.stats(new_states, actions)\n        p_s_a = self.stats(new_state_marginals, actions)\n\n        if use_jenson_shannon_divergence:\n            # Improves stability and gradients are unbiased\n            # But use the kl divergence representation for the reward\n            mutual_information =-F.softplus(-p_sa) - F.softplus(p_s_a)\n            lower_bound = torch.mean(-F.softplus(-p_sa)) - torch.mean(F.softplus(p_s_a))\n        else:\n            # Use KL Divergence\n            mutual_information = -F.softplus(-p_sa) - F.softplus(p_s_a)\n\n        # Maximize the mutual information\n        loss = -lower_bound\n        self.stats_optim.zero_grad()\n        loss.backward()\n        self.stats_optim.step()\n\n\n        mutual_information = mutual_information.squeeze(-1)\n        mutual_information = mutual_information.detach()\n\n        #mutual_information = torch.clamp(input=mutual_information, min=-1., max=1.)\n\n        augmented_rewards = rewards + self.intrinsic_param*mutual_information\n        #augmented_rewards = mutual_information\n        augmented_rewards.detach()\n\n\n        return loss, augmented_rewards\n\n    def plot(self, frame_idx, rewards, placeholder_name, output_folder, mean_rewards):\n        fig = plt.figure(figsize=(20, 5))\n        plt.subplot(131)\n        plt.title(\'frame %s. reward: %s\' % (frame_idx, np.mean(rewards)))\n        plt.plot(rewards)\n        plt.subplot(132)\n        plt.title(\'frame %s. reward: %s\' % (frame_idx, np.mean(rewards)))\n        plt.plot(mean_rewards)\n        file_name_pre = output_folder+placeholder_name\n        fig.savefig(file_name_pre+str(frame_idx)+\'.jpg\')\n        plt.close(fig)\n\n    def save_rewards(self, ep_rewards, mean_rewards):\n        np.save(file=\'epoch_rewards.npy\', arr=ep_rewards)\n        np.save(file=\'mean_rewards.npy\', arr=mean_rewards)\n\n\n    def save_m(self):\n        torch.save(\n            self.encoder.state_dict(),\n            \'{}/encoder.pt\'.format(self.output_folder)\n        )\n        torch.save(\n            self.stats.state_dict(),\n            \'{}/statistics_network.pt\'.format(self.output_folder)\n        )\n        torch.save(\n            self.policy_network.state_dict(),\n            \'{}/policy_network.pt\'.format(self.output_folder)\n        )\n        torch.save(\n            self.fwd.state_dict(),\n            \'{}/forward_dynamics_network.pt\'.format(self.output_folder)\n        )\n\n    def get_train_variables(self, batch):\n\n        states = batch.state\n        new_states = batch.next_state\n        actions = batch.action\n        rewards = batch.reward\n        dones = batch.done\n\n        states = Variable(torch.cat(states))\n        new_states = Variable(torch.cat(new_states), requires_grad=False)\n        actions = Variable(torch.cat(actions))\n        rewards = Variable(torch.cat(rewards))\n        dones = Variable(torch.cat(dones))\n\n        if self.use_cuda:\n            states = states.cuda()\n            actions = actions.cuda()\n            rewards = rewards.cuda()\n            new_states = new_states.cuda()\n            dones = dones.cuda()\n\n        b = defaultdict()\n        b[\'states\'] = states\n        b[\'actions\'] = actions\n        b[\'rewards\'] = rewards\n        b[\'new_states\'] = new_states\n        b[\'dones\'] = dones\n\n        return b\n\n    def normalize(self, r):\n        normalized = (r - torch.mean(r))/(torch.std(r) + 1e-10)\n        return normalized\n\n    def train(self):\n        epoch_episode_rewards = []\n\n        # Initialize the training with an initial state\n        state = self.env.reset()\n\n        # Initialize the losses\n        episode_reward = 0\n        # Check whether to use cuda or not\n        state = to_tensor(state, use_cuda=self.use_cuda)\n\n        fwd_loss = 0\n        stats_loss = 0\n        policy_loss = 0\n\n\n        # Mean rewards\n        mean_rewards = []\n        with torch.no_grad():\n            state = self.encoder(state)\n        state = state.detach()\n\n        for frame_idx in range(1, self.num_frames+1):\n            epsilon_by_frame = epsilon_greedy_exploration()\n            epsilon = epsilon_by_frame(frame_idx)\n            action = self.policy_network.act(state, epsilon)\n\n            # Execute the action\n            next_state, reward, done, success = self.env.step(action.item())\n            episode_reward += reward\n\n            reward = np.sign(reward)\n\n            next_state = to_tensor(next_state, use_cuda=self.use_cuda)\n            with torch.no_grad():\n                next_state = self.encoder(next_state)\n\n            next_state = next_state.detach()\n\n            reward = torch.tensor([reward], dtype=torch.float)\n\n            done_bool = done * 1\n            done_bool = torch.tensor([done_bool], dtype=torch.float)\n\n            # Store in the replay buffer\n            self.store_transition(state=state, new_state=next_state,\n                                  action=action, done=done_bool,reward=reward)\n\n            state = next_state\n\n            if done:\n                epoch_episode_rewards.append(episode_reward)\n                # Add episode reward to tensorboard\n                episode_reward = 0\n                state = self.env.reset()\n                state = to_tensor(state, use_cuda=self.use_cuda)\n                state = self.encoder(state)\n\n            # Train the forward dynamics model\n            if len(self.replay_buffer) > self.fwd_limit:\n                # Sample a minibatch from the replay buffer\n                transitions = self.replay_buffer.sample_batch(self.batch_size)\n                batch = Buffer.Transition(*zip(*transitions))\n                batch = self.get_train_variables(batch)\n                mse_loss = self.train_forward_dynamics(batch=batch)\n                fwd_loss += mse_loss.item()\n                if frame_idx % self.print_every == 0:\n                    print(\'Forward Dynamics Loss :\', fwd_loss/(frame_idx-self.fwd_limit))\n\n            # Train the statistics network and the policy\n            if len(self.replay_buffer) > self.policy_limit:\n                transitions = self.replay_buffer.sample_batch(self.batch_size)\n                batch = Buffer.Transition(*zip(*transitions))\n                batch = self.get_train_variables(batch)\n                loss, aug_rewards = self.train_statistics_network(batch=batch)\n\n                p_loss = self.train_policy(batch=batch, rewards=aug_rewards)\n\n                stats_loss += loss.item()\n                policy_loss += p_loss.item()\n\n                if frame_idx % self.print_every == 0:\n                    print(\'Statistics Loss: \', stats_loss/(frame_idx-self.policy_limit))\n                    print(\'Policy Loss: \', policy_loss/(frame_idx - self.policy_limit))\n\n\n            # Print the statistics\n            if self.verbose:\n                if frame_idx % self.print_every == 0:\n                    print(\'Mean Reward \', str(np.mean(epoch_episode_rewards)))\n                    print(\'Sum of Rewards \', str(np.sum(epoch_episode_rewards)))\n                    mean_rewards.append(np.mean(epoch_episode_rewards))\n\n            if self.plot_stats:\n                if frame_idx % self.plot_every == 0:\n                # Plot the statistics calculated\n                    self.plot(frame_idx=frame_idx, rewards=epoch_episode_rewards,\n                              mean_rewards=mean_rewards,\n                            output_folder=self.output_folder, placeholder_name=\'/DQN_montezuma_intrinsic\')\n\n            # Update the target network\n            if frame_idx % self.update_every == 0:\n                self.update_networks()\n\n            # Save the models and the rewards file\n            if frame_idx % self.save_epoch == 0:\n                self.save_m()\n                self.save_rewards(ep_rewards=epoch_episode_rewards, mean_rewards=mean_rewards)\n\n        self.save_m()\n\nif __name__ == \'__main__\':\n    grid_env = gym.make(\'MiniGrid-DoorKey-5x5-v0\')\n\n    # Setup the environment\n    # Frame skipping is added in the wrapper\n    #env = gym.make(\'MontezumaRevengeNoFrameskip-v4\')\n    # Add the required environment wrappers\n    env = env_wrappers.warp_wrap(grid_env, height=8, width=8, grid_env=True)\n    env = env_wrappers.wrap_pytorch(env)\n\n    action_space = env.action_space.n\n    state_space = env.observation_space\n    height = 8\n    width = 8\n    num_hidden_units = 32\n    # The input to the encoder is the stack of the last 4 frames of the environment.\n    encoder = Encoder(state_space=num_hidden_units, conv_kernel_size=3, conv_layers=16,\n                      hidden=32, input_channels=4, height=height,\n                      width=width)\n    policy_model = QNetwork(env=env, state_space=num_hidden_units,\n                             action_space=action_space, hidden=num_hidden_units)\n    target_policy_model = QNetwork(env=env, state_space=num_hidden_units,\n                             action_space=action_space, hidden=num_hidden_units)\n    stats_network = StatisticsNetwork(action_space=action_space, state_space=num_hidden_units,\n                                      hidden=32, output_dim=1)\n    forward_dynamics_network = forward_dynamics_model(action_space=action_space, hidden=32,\n                                                      state_space=num_hidden_units)\n\n    # Defining targets networks to possibily improve the stability of the algorithm\n    target_stats_network = StatisticsNetwork(action_space=action_space, state_space=num_hidden_units,\n                                             hidden=32, output_dim=1)\n    target_fwd_dynamics_network = forward_dynamics_model(action_space=action_space, hidden=32,\n                                                         state_space=num_hidden_units)\n\n    # Define the model\n    empowerment_model = EmpowermentTrainer(\n        action_space=action_space,\n        batch_size=32,\n        discount_factor=0.99,\n        encoder=encoder,\n        statistics_network=stats_network,\n        forward_dynamics=forward_dynamics_network,\n        policy_network=policy_model,\n        target_policy_network=target_policy_model,\n        env=env,\n        forward_dynamics_lr=1e-3,\n        stats_lr=1e-4,\n        policy_lr=1e-4,\n        fwd_dynamics_limit=100,\n        stats_network_limit=500,\n        model_output_folder=\'minigrid_world_16x16\',\n        num_frames=1000000,\n        num_fwd_train_steps=1,\n        num_stats_train_steps=1,\n        num_train_epochs=1,\n        policy_limit=1000,\n        polyak_constant=0.99,\n        random_seed=2450,\n        size_replay_buffer=1000000,\n        plot_stats=True,\n        print_every=400,\n        plot_every=10000,\n        intrinsic_param=0.025,\n        save_epoch=2000,\n        update_network_every=200,\n        target_stats_network=target_stats_network,\n        target_fwd_dynamics_network=target_fwd_dynamics_network\n    )\n\n    # Train\n    empowerment_model.train()\n\n'"
models/infogan.py,23,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nfrom torchvision.utils import save_image\nfrom Layers.Spectral_norm import SpectralNorm\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass InfoGAN(object):\n    # The InfoGAN class consisting of the Generator, Discriminator and the Recognizer\n    def __init__(self, generator, discriminator,\n                 dataset, num_epochs,\n                 random_seed, shuffle, use_cuda,\n                 tensorboard_summary_writer,\n                 output_folder, image_size,\n                 image_channels,\n                 noise_dim, cat_dim, cont_dim,\n                 generator_lr, discriminator_lr, batch_size,\n                 cont_lambda, cat_lambda,\n                 noise_uniform_val,\n                 images_dir,\n                 save_iter=100,\n                 save_image_rows=8):\n\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n        self.dataset = dataset\n        self.seed = random_seed\n        self.batch = batch_size\n        self.shuffle = shuffle\n        self.use_cuda = use_cuda\n        self.generator = generator\n        self.discriminator = discriminator\n        self.tb_writer = tensorboard_summary_writer\n        self.output_folder = output_folder\n        self.image_size = image_size\n        self.cat_dim = cat_dim\n        self.cont_dim = cont_dim\n        self.noise_dim = noise_dim\n        self.img_channels = image_channels\n        self.img_rows = save_image_rows\n        self.cont_lambda = cont_lambda\n        self.cat_lambda = cat_lambda\n        self.noise_uniform = noise_uniform_val\n        self.images_dir = images_dir\n        self.save_iter = save_iter\n\n        if self.use_cuda:\n            self.generator = self.generator.cuda()\n            self.discriminator = self.discriminator.cuda()\n\n        self.gen_optim = Adam(filter(lambda p: p.requires_grad,self.generator.parameters()), lr=generator_lr)\n        self.dis_optim = Adam(filter(lambda p: p.requires_grad,self.discriminator.parameters()), lr=discriminator_lr)\n\n    def set_seed(self):\n        # Set the seed for reproducible results\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n    def get_dataloader(self):\n        # Generates the dataloader for the images for training\n\n        dataset_loader = DataLoader(self.dataset,\n                                    batch_size=self.batch,\n                                    shuffle=self.shuffle)\n\n        return dataset_loader\n\n    # Loss Function\n    def loss(self):\n\n        # Discriminator loss\n        criterionD = nn.BCELoss()\n        criterionQ_categorical = nn.CrossEntropyLoss()\n        criterionQ_continuos  = nn.MSELoss()\n\n        return criterionD, criterionQ_categorical, criterionQ_continuos\n\n    # Noise Sample Generator\n    def _noise_sample(self, cat_c, con_c, noise, bs):\n        idx = np.random.randint(10, size=bs)\n        c = np.zeros((bs, 10))\n        c[range(bs), idx] = 1.0\n\n        cat_c.data.copy_(torch.Tensor(c))\n        con_c.data.uniform_(-self.noise_uniform, self.noise_uniform)\n        noise.data.uniform_(-self.noise_uniform, self.noise_uniform)\n        z = torch.cat([noise, cat_c, con_c], 1).view(-1, (self.noise_dim+self.cat_dim+self.cont_dim))\n\n        return z, idx\n\n    def linear_annealing_variance(self, std, epoch):\n        # Reduce the standard deviation over the epochs\n        if std > 0:\n            std -= epoch*0.1\n        else:\n            std = 0\n        return std\n\n    def train(self):\n        real_x = torch.FloatTensor(self.batch_size, self.img_channels,\n                                   self.image_size, self.image_size)\n        labels = torch.FloatTensor(self.batch_size)\n        cat_c = torch.FloatTensor(self.batch_size, self.cat_dim)\n        con_c = torch.FloatTensor(self.batch_size, self.cont_dim)\n        noise = torch.FloatTensor(self.batch_size, self.noise_dim)\n\n        cat_c = Variable(cat_c)\n        con_c = Variable(con_c)\n        noise = Variable(noise)\n\n        labels = Variable(labels)\n        labels.requires_grad = False\n\n        criterionD, criterion_cat, criterion_cont = self.loss()\n\n        # fixed random variables for inference\n        c = np.linspace(-1, 1, 10).reshape(1, -1)\n        c = np.repeat(c, 10, 0).reshape(-1, 1)\n\n        c1 = np.hstack([c, np.zeros_like(c)])\n        c2 = np.hstack([np.zeros_like(c), c])\n\n        #print(c1.shape)\n\n        idx = np.arange(10).repeat(self.batch_size)\n        one_hot = np.zeros((10))\n        one_hot[1] = 1\n        fix_noise = torch.Tensor(self.noise_dim).uniform_(-self.noise_uniform, self.noise_uniform)\n\n        for epoch in range(self.num_epochs):\n            std = 1.0\n            for num_iters, batch_data in enumerate(self.get_dataloader()):\n\n                # Real Part\n                self.dis_optim.zero_grad()\n\n                x = batch_data[\'image\']\n                bs = x.size(0)\n\n                x = Variable(x)\n\n                if self.use_cuda:\n                    x = x.cuda()\n                    real_x = real_x.cuda()\n                    labels = labels.cuda()\n                    cat_c = cat_c.cuda()\n                    con_c = con_c.cuda()\n                    noise = noise.cuda()\n\n                real_x.data.resize_(x.size())\n                labels.data.resize(bs)\n                cat_c.data.resize_(bs, self.cat_dim)\n                con_c.data.resize_(bs, self.cont_dim)\n                noise.data.resize_(bs, self.noise_dim)\n\n                real_x.data.copy_(x)\n                # Add noise to the inputs of the discriminator\n                noise_data = torch.zeros(x.shape)\n    #            print(noise.shape)\n                noise_data = torch.normal(mean=noise_data, std=std)\n                if self.use_cuda:\n                    noise_data = noise_data.cuda()\n\n                x += noise_data\n                d_output, recog_cat, recog_cont = self.discriminator(x)\n                labels.data.fill_(1)\n                loss_real = criterionD(d_output, labels)\n                loss_real.backward()\n\n                # Fake Part\n                z, idx = self._noise_sample(cat_c, con_c, noise, bs)\n                fake_x = self.generator(z)\n\n                fake_x = fake_x + noise_data\n                d_output, recog_cat, recog_cont = self.discriminator(fake_x.detach())\n                labels.data.fill_(0)\n                loss_fake = criterionD(d_output, labels)\n                loss_fake.backward()\n\n                D_loss = loss_real+loss_fake\n                self.dis_optim.step()\n\n                # Generator and Recognizer Part\n                d_output, recog_cat, recog_cont = self.discriminator(fake_x)\n                labels.data.fill_(1.0)\n                reconstruct_loss = criterionD(d_output, labels)\n\n                class_ = torch.LongTensor(idx)\n                target = Variable(class_)\n\n                if self.use_cuda:\n                    target = target.cuda()\n\n                self.gen_optim.zero_grad()\n\n                cont_loss = criterion_cont(recog_cont, con_c)*self.cont_lambda\n                cat_loss = criterion_cat(recog_cat, target)*self.cat_lambda  # Refer to the paper for the values of lambda\n\n                G_loss = reconstruct_loss + cont_loss + cat_loss\n                G_loss.backward()\n\n                self.gen_optim.step()\n\n                if num_iters % self.save_iter == 0:\n                    print(\'Epoch/Iter:{0}/{1}, Dloss: {2}, Gloss: {3}\'.format(\n                        epoch, num_iters, D_loss.data.cpu().numpy(),\n                        G_loss.data.cpu().numpy())\n                    )\n\n                    # Anneal the noise standard deviation\n                    std = self.linear_annealing_variance(std=std, epoch=epoch)\n\n                    noise.data.copy_(fix_noise)\n                    cat_c.data.copy_(torch.Tensor(one_hot))\n\n                    con_c.data.uniform_(-self.noise_uniform, self.noise_uniform)\n                    z = torch.cat([noise, cat_c, con_c], 1).view(-1, (self.noise_dim+self.cont_dim+self.cat_dim))\n                    x_save = self.generator(z)\n                    save_image(x_save.data.cpu(), self.images_dir+ str(epoch)+\'c1.png\', nrow=self.img_rows)\n\n                    #con_c.data.copy_(torch.from_numpy(c2))\n                    con_c.data.uniform_(-self.noise_uniform, self.noise_uniform)\n                    z = torch.cat([noise, cat_c, con_c], 1).view(-1, (self.noise_dim+self.cont_dim+self.cat_dim))\n                    x_save = self.generator(z)\n                    save_image(x_save.data.cpu(), self.images_dir+ str(epoch)+\'c2.png\', nrow=self.img_rows)\n\n            self.save_model(output=self.output_folder)\n\n    def to_cuda(self):\n        self.generator = self.generator.cuda()\n        self.discriminator = self.discriminator.cuda()\n\n    def save_model(self, output):\n        """"""\n        Saving the models\n        :param output:\n        :return:\n        """"""\n        print(""Saving the generator and discriminator"")\n        torch.save(\n            self.generator.state_dict(),\n            \'{}/generator.pt\'.format(output)\n        )\n        torch.save(\n            self.discriminator.state_dict(),\n            \'{}/discriminator.pt\'.format(output)\n        )\n\n\nclass Generator(nn.Module):\n\n    """"""\n    The generator/decoder in the CVAE-GAN pipeline\n\n    Given a latent encoding or a noise vector, this network outputs an image.\n\n    """"""\n\n    def __init__(self, latent_space_dimension, conv_kernel_size,\n                 conv_layers, hidden_dim, height, width, input_channels):\n        super(Generator, self).__init__()\n\n        self.z_dimension = latent_space_dimension\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.hidden = hidden_dim\n        self.height = height\n        self.width = width\n        self.input_channels = input_channels\n\n        # We will be using spectral norm in both the generator as well as the discriminator\n        # since this improves the training dynamics (https://arxiv.org/abs/1805.08318)\n\n\n        # Decoder/Generator Architecture\n\n        # Deconvolution layers\n        self.conv1 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers*4,\n                                        out_channels=self.conv_layers*4, kernel_size=self.conv_kernel_size,\n                                        stride=2))\n        self.bn1 = nn.BatchNorm2d(self.conv_layers*4)\n\n        self.conv2 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers*4, out_channels=self.conv_layers*3,\n                                        kernel_size=self.conv_kernel_size, stride=2))\n        self.bn2 = nn.BatchNorm2d(self.conv_layers*3)\n\n        self.conv3 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers*3, out_channels=self.conv_layers*3,\n                                        kernel_size=self.conv_kernel_size, stride=2))\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*3)\n\n        self.conv4 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers*3, out_channels=self.conv_layers*2,\n                                        kernel_size=self.conv_kernel_size, stride=2))\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*2)\n\n        self.conv5 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers * 2, out_channels=self.conv_layers*2,\n                                                     kernel_size=self.conv_kernel_size, stride=2))\n        self.bn5 = nn.BatchNorm2d(self.conv_layers*2)\n\n        self.conv6 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers * 2, out_channels=self.conv_layers,\n                                                     kernel_size=self.conv_kernel_size, stride=2))\n        self.bn6 = nn.BatchNorm2d(self.conv_layers)\n\n        self.conv7 = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                                                     kernel_size=self.conv_kernel_size, stride=2))\n        self.bn7 = nn.BatchNorm2d(self.conv_layers)\n\n        self.output = SpectralNorm(nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.input_channels,\n                                kernel_size=self.conv_kernel_size-1, stride=1))\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.leaky_relu = nn.LeakyReLU(inplace=True)\n\n        # Use dropouts in the generator to stabilize the training\n        self.dropout = nn.Dropout()\n\n        self.sigmoid_output = nn.Sigmoid()\n\n        # Initialize the weights using xavier initialization\n        #nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, z):\n\n        z =  z.view((z.shape[0],z.shape[1], 1, 1))\n\n        # Use spectral norm to improve training dynamics\n        z = self.conv1(z)\n        z = self.bn1(z)\n        z = self.leaky_relu(z)\n        z = self.conv2(z)\n        z = self.bn2(z)\n        z = self.leaky_relu(z)\n        #z = self.dropout(z)\n\n\n        z = self.conv3(z)\n        z = self.bn3(z)\n        z = self.leaky_relu(z)\n        z = self.conv4(z)\n        z = self.bn4(z)\n        z = self.leaky_relu(z)\n        #z = self.dropout(z)\n\n        z = self.conv5(z)\n        z = self.bn5(z)\n        z = self.leaky_relu(z)\n        z = self.conv6(z)\n        z = self.bn6(z)\n        z = self.leaky_relu(z)\n        z = self.conv7(z)\n        z = self.bn7(z)\n        z = self.leaky_relu(z)\n\n        output = self.output(z)\n        output = self.sigmoid_output(output)\n\n        return output\n\n\nclass Discriminator_recognizer(nn.Module):\n\n    """"""\n    The discriminator and the recognizer network for the infogan\n\n    This network distinguishes the fake images from the real\n\n    """"""\n\n    def __init__(self, input_channels, conv_layers,\n                 pool_kernel_size, conv_kernel_size,\n                 height, width, hidden, cat_dim, cont_dim):\n\n        super(Discriminator_recognizer, self).__init__()\n\n        self.in_channels = input_channels\n        self.conv_layers = conv_layers\n        self.pool = pool_kernel_size\n        self.conv_kernel_size = conv_kernel_size\n        self.height = height\n        self.width = width\n        self.hidden = hidden\n        self.cat_dim = cat_dim\n        self.cont_dim = cont_dim\n\n        # Discriminator architecture\n        self.conv1 = SpectralNorm(nn.Conv2d(in_channels=self.in_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2))\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = SpectralNorm(nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2))\n        self.bn2 = nn.BatchNorm2d(self.conv_layers*2)\n        # Use strided convolution in place of max pooling\n        self.pool_1 = nn.MaxPool2d(kernel_size=self.pool)\n\n        self.conv3 = SpectralNorm(nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2))\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = SpectralNorm(nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*4,\n                               kernel_size=self.conv_kernel_size, padding=1, stride=2))\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*4)\n        # Use strided convolution in place of max pooling\n        self.pool_2 = nn.MaxPool2d(kernel_size=self.pool)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.leaky_relu = nn.LeakyReLU(inplace=True)\n\n        # Fully Connected Layer\n        self.output = SpectralNorm(nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*4,\n                                             out_features=1))\n        self.sigmoid_output = nn.Sigmoid()\n\n        self.recognizer_output_cont = SpectralNorm(nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*4,\n                                                             out_features=self.cont_dim))\n        self.recognizer_output_cat = SpectralNorm(nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*4,\n                                                            out_features=self.cat_dim))\n\n        self.softmax_output = nn.Softmax()\n\n        # Dropout layer\n        self.dropout = nn.Dropout()\n\n    def forward(self, input):\n\n        conv1 = self.conv1(input)\n\n        #conv1 = self.bn1(conv1)\n        conv1 = self.leaky_relu(conv1)\n        conv2 = self.conv2(conv1)\n\n        #conv2 = self.bn2(conv2)\n        conv2 = self.leaky_relu(conv2)\n        #pool1 = self.pool_1(conv2)\n\n        conv3 = self.conv3(conv2)\n\n        # conv3 = self.bn3(conv3)\n        conv3 = self.leaky_relu(conv3)\n        conv4 = self.conv4(conv3)\n\n        #conv4 = self.bn4(conv4)\n        conv4 = self.leaky_relu(conv4)\n        #pool2 = self.pool_2(conv4)\n\n        pool2 = conv4.view((-1, self.height//16*self.width//16*self.conv_layers*4))\n\n        #hidden = self.hidden_layer1(pool2)\n        #hidden = self.leaky_relu(hidden)\n\n        #feature_mean = hidden\n\n        output = self.output(pool2)\n        output = self.sigmoid_output(output)\n\n        cat_output = self.recognizer_output_cat(pool2)\n        cat_output = self.softmax_output(cat_output)\n\n        cont_output = self.recognizer_output_cont(pool2)\n\n        return output, cat_output, cont_output\n\n\n'"
models/inverse_dynamics_vae.py,5,"b'""""""\n\nThis script contains an implementation of the inverse dynamics variational autoencoder.\nThis basically combines the inverse dynamics model (predicting the action from the\ncurrent and the next state) and the variational autoencoder trained with reconstruction error.\nAs an added parameter, beta is included with the KL divergence inorder to encourage\ndisentangled representations.\n\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom Layers.Spectral_norm import SpectralNorm\n\n# The encoder for the INVAE\nclass Encoder(nn.Module):\n\n    def __init__(self, conv_layers, conv_kernel_size,\n                 latent_dimension, in_channels, height, width):\n        super(Encoder, self).__init__()\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.latent_dim = latent_dimension\n        self.in_channels = in_channels\n        self.height = height\n        self.width = width\n\n        # Encoder Architecture\n        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.conv_layers,\n                               stride=2, kernel_size=self.conv_kernel_size)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                               stride=2, kernel_size=self.conv_kernel_size)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers)\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               stride=2, kernel_size=self.conv_kernel_size)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               stride=2, kernel_size=self.conv_kernel_size)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*2)\n\n        #Latent variable mean and logvariance\n        self.mu = nn.Linear(in_features=self.height//16*self.width/16*self.conv_layers*2, out_features=self.latent_dim)\n        self.logvar = nn.Linear(in_features=self.height//16*self.width/16*self.conv_layers*2, out_features=self.latent_dim)\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights using xavier initialization\n\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        nn.init.xavier_uniform_(self.mu.weight)\n        nn.init.xavier_uniform_(self.logvar.weight)\n\n    def forward(self, state):\n        x = self.conv1(state)\n        x = self.bn1(x)\n        x = self.lrelu(x)\n\n        x = self.conv2(x)\n        x = self.bn2(x)\n        x = self.lrelu(x)\n\n        x = self.conv3(x)\n        x = self.bn3(x)\n        x = self.lrelu(x)\n\n        x = self.conv4(x)\n        x = self.bn4(x)\n        x = self.lrelu(x)\n\n        x = x.view((-1, self.height//16*self.width//16*self.conv_layers*2))\n\n        mean = self.mu(x)\n        logvar = self.logvar(x)\n\n        return mean, logvar\n\n\n# The decoder for the INVAE\nclass Decoder(nn.Module):\n\n    def __init__(self, height, width,\n                 image_channels, conv_layers,\n                 hidden,\n                 conv_kernel_size, latent_dimension):\n        super(Decoder, self).__init__()\n        self.height = height\n        self.width = width\n        self.out_channels = image_channels\n        self.conv_layers = conv_layers\n        self.conv_kernel_size = conv_kernel_size\n        self.in_dimension = latent_dimension\n        self.hidden = hidden\n\n        # Decoder Architecture\n        self.linear1_decoder = nn.Linear(in_features=self.in_dimension,\n                                         out_features=self.hidden)\n        self.bn_l_d = nn.BatchNorm1d(self.hidden)\n        self.linear = nn.Linear(in_features=self.hidden,\n                                out_features=self.height // 16 * self.width // 16 * self.conv_layers * 2)\n        self.bn_l_2_d = nn.BatchNorm1d(self.height // 16 * self.width * 16 * self.conv_layers * 2)\n        self.conv5 = nn.ConvTranspose2d(in_channels=self.conv_layers * 2, out_channels=self.conv_layers * 2,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn5 = nn.BatchNorm2d(self.conv_layers * 2)\n        self.conv6 = nn.ConvTranspose2d(in_channels=self.conv_layers * 2, out_channels=self.conv_layers * 2,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn6 = nn.BatchNorm2d(self.conv_layers * 2)\n        self.conv7 = nn.ConvTranspose2d(in_channels=self.conv_layers * 2, out_channels=self.conv_layers,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn7 = nn.BatchNorm2d(self.conv_layers)\n        self.conv8 = nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.output = nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.in_channels,\n                                         kernel_size=self.conv_kernel_shape - 3, )\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Output activation\n        self.output_sigmoid = nn.Sigmoid()\n\n        # Initialize weights using xavier initialization\n        nn.init.xavier_uniform_(self.conv5.weight)\n        nn.init.xavier_uniform_(self.conv6.weight)\n        nn.init.xavier_uniform_(self.conv7.weight)\n        nn.init.xavier_uniform_(self.conv8.weight)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.xavier_uniform_(self.linear1_decoder.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, z):\n        z = self.linear1_decoder(z)\n        z = self.l_relu(z)\n        z = self.linear(z)\n        z = self.l_relu(z)\n        z = z.view((-1, self.conv_layers * 2, self.height // 16, self.width // 16))\n\n        z = self.conv5(z)\n        z = self.l_relu(z)\n\n        z = self.conv6(z)\n        z = self.l_relu(z)\n\n        z = self.conv7(z)\n        z = self.l_relu(z)\n\n        z = self.conv8(z)\n        z = self.l_relu(z)\n\n        output = self.output(z)\n        output = self.sigmoid_output(output)\n\n        return output\n\n\n# The inverse dynamic module\nclass InverseDM(nn.Module):\n\n    def __init__(self, latent_dim, action_dim, hidden_dim):\n        super(InverseDM, self).__init__()\n        self.latent_dim = latent_dim\n        self.action_dim = action_dim\n        self.hidden_dim = hidden_dim\n\n        # Inverse Dynamics Architecture\n        self.input_linear = nn.Linear(in_features=self.latent_dim*2, out_features=self.hidden_dim)\n        self.hidden_1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim)\n        self.hidden_2 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim*2)\n        self.hidden_3 = nn.Linear(in_features=self.hidden_dim*2, out_features=self.hidden_dim*2)\n        self.output = nn.Linear(in_features=self.hidden_dim*2, out_features=self.action_dim)\n\n        # Output Activation function\n        self.output_softmax = nn.Softmax()\n\n        # Leaky Relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.input_linear.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.hidden_2.weight)\n        nn.init.xavier_uniform_(self.hidden_3.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, current_state, next_state):\n        x = torch.cat([current_state, next_state])\n        x = self.input_linear(x)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n        x = self.hidden_2(x)\n        x = self.lrelu(x)\n        x = self.hidden_3(x)\n        x = self.lrelu(x)\n\n        output = self.output(x)\n        output = self.output_softmax(output)\n\n        return output\n\n\nclass INVAE(nn.Module):\n\n    def __init__(self, conv_layers,\n                 conv_kernel_size, height,\n                 width, latent_dim, hidden_dim,\n                 input_dim, action_dim,\n                 ):\n        super(INVAE, self).__init__()\n\n        self.encoder = Encoder(conv_kernel_size=conv_kernel_size, conv_layers=conv_layers,\n                               height=height, width=width, in_channels=input_dim,\n                               latent_dimension=latent_dim)\n\n        self.decoder = Decoder(conv_layers=conv_layers, conv_kernel_size=conv_kernel_size,\n                               latent_dimension=latent_dim, height=height,\n                               width=width, hidden=hidden_dim, image_channels=input_dim)\n\n        self.inverse_dm = InverseDM(latent_dim=latent_dim, action_dim=action_dim,\n                                    hidden_dim=hidden_dim)\n\n    def reparameterize(self, mu, logvar):\n        # Reparameterization trick as shown in the auto encoding variational bayes paper\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            if self.use_cuda:\n                eps = eps.cuda()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def forward(self, current_state, next_state):\n\n        mu_current, logvar_current = self.encoder(current_state)\n        mu_next, logvar_next = self.encoder(next_state)\n\n        z_current = self.reparameterize(mu_current, logvar_current)\n        z_next = self.reparameterize(mu_next, logvar_next)\n\n        reconstructed_current_state = self.decoder(z_current)\n        reconstructed_next_state = self.decoder(z_next)\n\n        action = self.inverse_dm(z_current, z_next)\n\n        return action, reconstructed_current_state, \\\n               reconstructed_next_state, mu_current, mu_next, \\\n               logvar_current, logvar_next, z_current, z_next\n\n\n# This model takes as input the current state and the action and predicts the next state\nclass StandardForwardDynamics(nn.Module):\n\n    def __init__(self, action_dim, state_dim, hidden_dim):\n        super(StandardForwardDynamics, self).__init__()\n        self.action_dim = action_dim\n        self.state_dim = state_dim\n        self.hidden_dim = hidden_dim\n\n        # Forward dynamics architecture\n        self.input_linear = nn.Linear(in_features=self.action_dim+self.state_dim,\n                                      out_features=self.hidden_dim)\n        self.hidden_1 = nn.Linear(in_features=self.hidden_dim, out_features=self.hidden_dim*2)\n        self.output = nn.Linear(in_features=self.hidden_dim*2, out_features=self.state_dim)\n\n        # Leaky relu activation\n        self.lrelu = nn.LeakyReLU()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.input_linear.weight)\n        nn.init.xavier_uniform_(self.hidden_1.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def forward(self, state, action):\n        # Concatenate the state and the action\n\n        # Note that the state in this case is the feature representation of the state\n\n        input = torch.cat([state, action], dim=-1)\n        x = self.input_linear(input)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n        output = self.output(x)\n\n        return output\n\n\n# This model takes as input the current state and the action and predicts the next state\n# using a infogan which maximizes the information from the action.\n\n# This model consists of 2 networks - Generator and the Discriminator\n\nclass Generator(nn.Module):\n\n    """"""\n    The generator/decoder in the CVAE-GAN pipeline\n\n    Given a latent encoding or a noise vector, this network outputs an image.\n\n    """"""\n\n    def __init__(self, latent_space_dimension,\n                 hidden_dim, action_dim):\n        super(Generator, self).__init__()\n\n        self.z_dimension = latent_space_dimension\n        self.hidden = hidden_dim\n        self.action_dim = action_dim\n\n        # We will be using spectral norm in both the generator as well as the discriminator\n        # since this improves the training dynamics (https://arxiv.org/abs/1805.08318)\n\n        # Decoder/Generator Architecture\n\n        self.input_linear = SpectralNorm(nn.Linear(in_features=self.action_dim+self.z_dimension,\n                                                   out_features=self.hidden))\n        self.hidden_1  = SpectralNorm(nn.Linear(in_features=self.hidden, out_features=self.hidden*2))\n        self.output = SpectralNorm(nn.Linear(in_features=self.hidden*2, out_features=self.z_dimension))\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.lrelu = nn.LeakyReLU(inplace=True)\n\n        # Use dropouts in the generator to stabilize the training\n        self.dropout = nn.Dropout()\n\n        self.sigmoid_output = nn.Sigmoid()\n\n    def forward(self, state, action):\n        # Concatenate the state and the action\n\n        # Note that the state in this case is the feature representation of the state\n\n        input = torch.cat([state, action], dim=-1)\n        x = self.input_linear(input)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n        output = self.output(x)\n\n        return output\n\n\nclass Discriminator_recognizer(nn.Module):\n\n    def __init__(self, latent_space_dimension,\n                 hidden_dim, action_dim):\n        super(Discriminator_recognizer, self).__init__()\n\n        self.z_dimension = latent_space_dimension\n        self.hidden = hidden_dim\n        self.action_dim = action_dim\n\n        # Discriminator architecture\n        self.input_linear = SpectralNorm(nn.Linear(in_features=self.z_dimension, out_features=self.hidden))\n        self.hidden_1 = SpectralNorm(nn.Linear(self.hidden, self.hidden*2))\n        self.output = SpectralNorm(nn.Linear(self.hidden*2, 1))\n        self.action_output = SpectralNorm(nn.Linear(self.hidden*2, self.action_dim))\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # The stability of the GAN Game suffers from the problem of sparse gradients\n        # Therefore, try to use LeakyRelu instead of relu\n        self.lrelu = nn.LeakyReLU(inplace=True)\n\n        self.sigmoid_output = nn.Sigmoid()\n        self.softmax_output = nn.Softmax()\n\n    def forward(self, state):\n\n        x = self.input_linear(state)\n        x = self.lrelu(x)\n        x = self.hidden_1(x)\n        x = self.lrelu(x)\n\n        output = self.output(x)\n        output = self.sigmoid_output(output)\n\n        actions = self.action_output(x)\n        actions = self.softmax_output(actions)\n\n        return output, actions\n\n'"
models/sagan.py,15,"b'""""""\n\nThis file contains the implementation of the Self attention GAN\n\nhttps://arxiv.org/abs/1805.08318\n\n""""""\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader\nimport torch.multiprocessing as mp\nfrom multiprocessing import Value\n\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torchvision.utils import save_image\n\nUSE_CUDA = torch.cuda.is_available()\n\n\nclass SAGAN(object):\n\n    def __init__(self, generator, discriminator,\n                 dataset, num_epochs,\n                 random_seed, shuffle, use_cuda,\n                 tensorboard_summary_writer,\n                 output_folder, image_size,\n                 image_channels, noise_dim,\n                 images_dir, save_iter,\n                 generator_lr, discriminator_lr, batch_size,\n                 num_threads, num_train_threads,\n                 save_images_row=8):\n\n        self.generator = generator\n        self.discriminator = discriminator\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.num_epochs = num_epochs\n        self.dataset = dataset\n        self.seed = random_seed\n        self.batch = batch_size\n        self.shuffle = shuffle\n        self.use_cuda = use_cuda\n        self.generator = generator\n        self.discriminator = discriminator\n        self.tb_writer = tensorboard_summary_writer\n        self.output_folder = output_folder\n        self.image_size = image_size\n        self.img_channels = image_channels\n        self.noise_dim = noise_dim\n        self.images_dir = images_dir\n        self.save_iter = save_iter\n        self.save_images_row = save_images_row\n        self.num_threads = num_threads\n        self.num_train_threads = num_train_threads\n\n        if self.use_cuda:\n            self.generator = self.generator.cuda()\n            self.discriminator = self.discriminator.cuda()\n\n        # Use of Lambda function since the Generator and Discriminator uses spectral norm\n        self.gen_optim = Adam(filter(lambda p: p.requires_grad, self.generator.parameters()), lr=generator_lr)\n        self.dis_optim = Adam(filter(lambda p: p.requires_grad, self.discriminator.parameters()), lr=discriminator_lr)\n\n    def set_seed(self):\n        # Set the seed for reproducible results\n        torch.manual_seed(self.seed)\n        np.random.seed(self.seed)\n\n    def get_dataloader(self):\n        # Generates the dataloader for the images for training\n\n        dataset_loader = DataLoader(self.dataset,\n                                    batch_size=self.batch,\n                                    shuffle=self.shuffle)\n\n        return dataset_loader\n\n    # Discriminator Loss function\n    def disc_loss(self):\n\n        criterionD = nn.BCELoss()\n        return criterionD\n\n    # Noise sample generator\n    def _noise_sample(self, noise, bs):\n        noise.data.uniform_(-1.0, 1.0)\n        z = noise\n        return z\n\n    def linear_annealing_variance(self, std, epoch):\n        # Reduce the standard deviation over the epochs\n        if std > 0:\n            std -= epoch*0.1\n        else:\n            std = 0\n        return std\n\n    def train_multithreaded(self):\n        pass\n\n    # The main training loop function\n    def train(self):\n        real_x = torch.FloatTensor(self.batch_size, self.img_channels,\n                                   self.image_size, self.image_size)\n        labels = torch.FloatTensor(self.batch_size)\n        noise = torch.FloatTensor(self.batch_size, self.noise_dim)\n\n        noise = Variable(noise)\n        labels = Variable(labels)\n        labels.requires_grad = False\n\n        criterionD = self.disc_loss()\n\n        # For inference\n        fix_noise = torch.Tensor(self.noise_dim).uniform_(-1, 1)\n\n        # Main training loop\n        for epoch in range(self.num_epochs):\n            std = 1.0\n            for num_iters, batch_data in enumerate(self.get_dataloader()):\n                # Real Part\n                self.dis_optim.zero_grad()\n\n                x = batch_data[\'image\']\n                bs = x.size(0)\n\n                x = Variable(x)\n\n                if self.use_cuda:\n                    x = x.cuda()\n                    real_x = real_x.cuda()\n                    noise = noise.cuda()\n                    labels = labels.cuda()\n\n                real_x.data.copy_(x)\n                # Add noise to the inputs of the discriminator\n\n                # This is an auxillary noise added to the discriminator to stabilize the training\n                noise_data = torch.zeros(x.shape)\n                #            print(noise.shape)\n                noise_data = torch.normal(mean=noise_data, std=std)\n                if self.use_cuda:\n                    noise_data = noise_data.cuda()\n\n                x += noise_data\n                d_output = self.discriminator(x)\n                labels.data.fill_(1)\n                loss_real = criterionD(d_output, labels)\n                loss_real.backward()\n\n                # Fake Part\n                z = self._noise_sample(noise, bs)\n                fake_x = self.generator(z)\n\n                fake_x = fake_x + noise_data\n                d_output = self.discriminator(fake_x.detach())\n                labels.data.fill_(0)\n                loss_fake = criterionD(d_output, labels)\n                loss_fake.backward()\n\n                D_loss = loss_real + loss_fake\n                self.dis_optim.step()\n\n                # Generator Loss update\n                d_output = self.discriminator(fake_x)\n                labels.data.fill_(1.0)\n                reconstruct_loss = criterionD(d_output, labels)\n\n                self.gen_optim.zero_grad()\n\n                G_loss = reconstruct_loss\n                G_loss.backward()\n\n                self.gen_optim.step()\n\n                if num_iters % self.save_iter == 0:\n                    print(\'Epoch/Iter:{0}/{1}, Dloss: {2}, Gloss: {3}\'.format(\n                        epoch, num_iters, D_loss.data.cpu().numpy(),\n                        G_loss.data.cpu().numpy())\n                    )\n                    # Anneal the standard deviation of the noise vector\n                    std = self.linear_annealing_variance(std=std, epoch=epoch)\n\n                    noise.data.copy_(fix_noise)\n\n                    z = noise\n                    x_save = self.generator(z)\n                    save_image(x_save.data.cpu(), self.images_dir + str(epoch)+\'c1.png\', nrow=self.save_images_row)\n\n            # Save the model at the end of each epoch\n            self.save_model(output=self.output_folder)\n\n    def to_cuda(self):\n        self.generator = self.generator.cuda()\n        self.discriminator = self.discriminator.cuda()\n\n    def save_model(self, output):\n        print(""Saving the generator and discriminator"")\n        torch.save(\n                self.generator.state_dict(),\n                \'{}/generator.pt\'.format(output)\n            )\n        torch.save(\n                    self.discriminator.state_dict(),\n                    \'{}/discriminator.pt\'.format(output)\n                )'"
models/vae.py,9,"b""import torch\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Variational Autoencoder with the option for tuning the disentaglement- Refer to the paper - beta VAE\nclass VAE(nn.Module):\n    def __init__(self, conv_layers, z_dimension, pool_kernel_size,\n                 conv_kernel_size, input_channels, height, width, hidden_dim, use_cuda, use_skip_connections=True):\n        super(VAE, self).__init__()\n\n        self.conv_layers = conv_layers\n        self.conv_kernel_shape = conv_kernel_size\n        self.pool = pool_kernel_size\n        self.z_dimension = z_dimension\n        self.in_channels = input_channels\n        self.height = height\n        self.width = width\n        self.hidden = hidden_dim\n        self.use_cuda = use_cuda\n        self.use_skip = use_skip_connections\n\n        # Intialize a list of skip values to be used for the decoder\n        skip_layers  = ['conv1', 'conv2', 'conv3', 'conv4']\n        self.skip_values = dict(skip_layers)\n\n\n        # Encoder Architecture\n        self.conv1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_shape, padding=1, stride=2)\n        self.bn1 = nn.BatchNorm2d(self.conv_layers)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_shape, padding=1, stride=2)\n        self.bn2 = nn.BatchNorm2d(self.conv_layers)\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_shape, padding=1, stride=2)\n        self.bn3 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_shape, padding=1, stride=2)\n        self.bn4 = nn.BatchNorm2d(self.conv_layers*2)\n        # Size of input features = HxWx2C\n        self.linear1 = nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*2, out_features=self.hidden)\n        self.bn_l = nn.BatchNorm1d(self.hidden)\n        self.latent_mu = nn.Linear(in_features=self.hidden, out_features=self.z_dimension)\n        self.latent_logvar = nn.Linear(in_features=self.hidden, out_features=self.z_dimension)\n        self.relu = nn.ReLU(inplace=True)\n\n        # Decoder Architecture\n        self.linear1_decoder = nn.Linear(in_features=self.z_dimension,\n                                         out_features=self.hidden)\n        self.bn_l_d = nn.BatchNorm1d(self.hidden)\n        self.linear = nn.Linear(in_features=self.hidden, out_features=self.height//16*self.width//16*self.conv_layers*2)\n        self.bn_l_2_d  =nn.BatchNorm1d(self.height//16*self.width*16*self.conv_layers*2)\n        self.conv5 = nn.ConvTranspose2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn5 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv6  = nn.ConvTranspose2d(in_channels=self.conv_layers*2,  out_channels=self.conv_layers*2,\n                                         kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn6 = nn.BatchNorm2d(self.conv_layers*2)\n        self.conv7 = nn.ConvTranspose2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.bn7 = nn.BatchNorm2d(self.conv_layers)\n        self.conv8 = nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                                        kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.output = nn.ConvTranspose2d(in_channels=self.conv_layers, out_channels=self.in_channels,\n                                        kernel_size=self.conv_kernel_shape-3,)\n\n        # Define the leaky relu activation function\n        self.l_relu = nn.LeakyReLU(0.1)\n\n        # Output Activation function\n        self.sigmoid_output = nn.Sigmoid()\n\n        # Initialize the weights using xavier initialization\n        nn.init.xavier_uniform_(self.conv1.weight)\n        nn.init.xavier_uniform_(self.conv2.weight)\n        nn.init.xavier_uniform_(self.conv3.weight)\n        nn.init.xavier_uniform_(self.conv4.weight)\n        nn.init.xavier_uniform_(self.conv5.weight)\n        nn.init.xavier_uniform_(self.conv6.weight)\n        nn.init.xavier_uniform_(self.conv7.weight)\n        nn.init.xavier_uniform_(self.conv8.weight)\n        nn.init.xavier_uniform_(self.linear.weight)\n        nn.init.xavier_uniform_(self.linear1.weight)\n        nn.init.xavier_uniform_(self.linear1_decoder.weight)\n        nn.init.xavier_uniform_(self.latent_mu.weight)\n        nn.init.xavier_uniform_(self.latent_logvar.weight)\n        nn.init.xavier_uniform_(self.output.weight)\n\n    def encode(self, x):\n        # Encoding the input image to the mean and var of the latent distribution\n        bs, _, _, _ = x.shape\n        conv1 = self.conv1(x)\n        conv1 = self.bn1(conv1)\n        conv1 = self.l_relu(conv1)\n        conv2 = self.conv2(conv1)\n        conv2 = self.bn2(conv2)\n        conv2 = self.l_relu(conv2)\n        conv3 = self.conv3(conv2)\n        conv3 = self.bn3(conv3)\n        conv3 = self.l_relu(conv3)\n        conv4 = self.conv4(conv3)\n        conv4 = self.bn4(conv4)\n        conv4 = self.l_relu(conv4)\n\n        fl = conv4.view((bs, -1))\n\n        linear = self.linear1(fl)\n        linear = self.bn_l(linear)\n        linear = self.l_relu(linear)\n        mu = self.latent_mu(linear)\n        logvar = self.latent_logvar(linear)\n\n        self.skip_values['conv1'] = conv1\n        self.skip_values['conv2'] = conv2\n        self.skip_values['conv3'] = conv3\n        self.skip_values['conv4'] = conv4\n\n        return mu, logvar\n\n    def reparameterize(self, mu, logvar):\n        # Reparameterization trick as shown in the auto encoding variational bayes paper\n        if self.training:\n            std = logvar.mul(0.5).exp_()\n            eps = Variable(std.data.new(std.size()).normal_())\n            if self.use_cuda:\n                eps = eps.cuda()\n            return eps.mul(std).add_(mu)\n        else:\n            return mu\n\n    def decode(self, z):\n        # Decoding the image from the latent vector\n        z = self.linear1_decoder(z)\n        z = self.l_relu(z)\n        z = self.linear(z)\n        z = self.l_relu(z)\n        z = z.view((-1, self.conv_layers*2, self.height//16, self.width//16))\n        z = self.conv5(z)\n        z = self.l_relu(z)\n        # Add skip connections\n        z = torch.cat([z, self.skip_values['conv3']])\n\n        z = self.conv6(z)\n        z = self.l_relu(z)\n        # Add skip connections\n        z = torch.cat([z, self.skip_values['conv2']])\n\n        z = self.conv7(z)\n        z = self.l_relu(z)\n        # Add skip connections\n        z = torch.cat([z, self.skip_values['conv1']])\n\n        z = self.conv8(z)\n        z = self.l_relu(z)\n\n        output = self.output(z)\n        output = self.sigmoid_output(output)\n\n        return output\n\n    def forward(self, x):\n        mu, logvar = self.encode(x)\n        z = self.reparameterize(mu, logvar)\n        output = self.decode(z)\n        return output, mu, logvar, z\n\n\n# Denoising Autoencoder\nclass DAE(nn.Module):\n    def __init__(self, conv_layers,\n                 conv_kernel_size, pool_kernel_size, use_cuda,\n                 height, width, input_channels, hidden_dim,\n                 noise_scale=0.1):\n        super(DAE, self).__init__()\n\n        self.conv_layers = conv_layers\n        self.conv_kernel_shape = conv_kernel_size\n        self.pool = pool_kernel_size\n        self.height = height\n        self.width = width\n        self.input_channels = input_channels\n        self.hidden = hidden_dim\n        self.noise_scale = noise_scale\n        self.cuda_avbl = use_cuda\n\n\n        # Encoder\n        # \xef\xbb\xbffour convolutional layers, each with kernel size 4 and stride 2 in both the height and width dimensions.\n        self.conv1 = nn.Conv2d(in_channels=self.input_channels, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers,\n                               kernel_size=self.conv_kernel_shape,  stride=2, padding=1)\n        self.conv3 = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n\n        self.conv4 = nn.Conv2d(in_channels=self.conv_layers*2, out_channels=self.conv_layers*2,\n                               kernel_size=self.conv_kernel_shape, stride=2, padding=1)\n\n        self.relu = nn.ReLU(inplace=True)\n\n        # Bottleneck Layer\n        self.bottleneck = nn.Linear(in_features=self.height//16*self.width//16*self.conv_layers*2,\n                                    out_features=self.hidden)\n\n\n        # Decoder\n        self.linear_decoder = nn.Linear(in_features=hidden_dim, out_features=self.height//16*self.width//16*self.conv_layers*2)\n        self.conv5 = nn.ConvTranspose2d(in_channels=self.conv_layers*2,\n                                        out_channels=self.conv_layers*2, stride=2, kernel_size=self.conv_kernel_shape-1\n                                        )\n\n        self.conv6 = nn.ConvTranspose2d(in_channels=self.conv_layers*2,\n                                        out_channels=self.conv_layers * 2, stride=2, kernel_size=self.conv_kernel_shape-1\n                                        )\n\n        self.conv7 = nn.ConvTranspose2d(in_channels=self.conv_layers * 2,\n                                        out_channels=self.conv_layers, stride=2, kernel_size=self.conv_kernel_shape-1\n                                        )\n\n        self.conv8 = nn.ConvTranspose2d(in_channels=self.conv_layers,\n                                        out_channels=self.conv_layers, stride=2, kernel_size=self.conv_kernel_shape-1\n                                        )\n        # Decoder output\n        self.output = nn.Conv2d(in_channels=self.conv_layers, out_channels=self.input_channels,\n                                kernel_size=self.conv_kernel_shape-2)\n\n    def encode(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n\n        x = self.conv2(x)\n        x = self.relu(x)\n\n        x = self.conv3(x)\n        x = self.relu(x)\n\n        x = self.conv4(x)\n        x = self.relu(x)\n\n        x = x.view((-1, self.height//16*self.width//16*self.conv_layers*2))\n\n        out = self.bottleneck(x)\n        return out\n    \n    def decode(self, encoded):\n        x = self.linear_decoder(encoded)\n        x  = x.view((-1, self.conv_layers*2, self.height//16, self.width//16))\n        x = self.conv5(x)\n        x = self.relu(x)\n\n        x = self.conv6(x)\n        x = self.relu(x)\n\n        x = self.conv7(x)\n        x = self.relu(x)\n\n        x = self.conv8(x)\n        x = self.relu(x)\n\n        out = self.output(x)\n        return out\n\n    def forward(self, image):\n        # Adding noise\n        n, _, _, _ = image.shape\n        noise = Variable(torch.randn(n, 3, self.height, self.width))\n        if self.cuda_avbl:\n            noise = noise.cuda()\n        #image = torch.mul(image + 0.25, 0.1 * noise)\n        image = torch.add(image, self.noise_scale*noise)\n        encoded = self.encode(image)\n        decoded = self.decode(encoded)\n        return decoded, encoded"""
models/vae_gan.py,3,"b'""""""\n\nThis script contains the implementation of the VAE-GAN model\nthat combines a vartiational autoencoder and a GAN.\n\n""""""\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.autograd import Variable'"
models/vq_vae.py,3,"b'""""""\n\nThis script contains an implementation of the VQ-VAE ie\nVector Quantised Variational Autoencoder.\n\n""""""\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom torch.autograd import Variable\n\nUSE_CUDA = torch.cuda.is_available()\n\nclass VQ_VAE(nn.Module):\n\n    def __init__(self):\n        super(VQ_VAE, self).__init__()\n'"
