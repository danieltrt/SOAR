file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\nfrom os import path\n\nVERSION = ""0.1.0""\n\nhere = path.abspath(path.dirname(__file__))\nwith open(path.join(here, \'README.md\'), encoding=\'utf-8\') as f:\n    long_description = f.read()\n\n\nsetup(\n    name=\'torchfold\',\n    version=VERSION,\n    description=\'Dynamic Batching with PyTorch\',\n    long_description=long_description,\n    long_description_content_type=\'text/markdown\',\n    packages=find_packages(exclude=[""*_test.py""]),\n    license=\'Apache License, Version 2.0\',\n    author=\'Illia Polosukhin, NEAR Inc\',\n    author_email=""illia@near.ai"",\n    project_urls={\n        \'Blog Post\': ""http://near.ai/articles/2017-09-06-PyTorch-Dynamic-Batching/"",\n        \'Source\': ""https://github.com/nearai/torchfold"",\n    },\n)\n\n'"
torchfold/__init__.py,0,"b'from .torchfold import Fold, Unfold\n'"
torchfold/torchfold.py,13,"b'import collections\n\nimport torch\nfrom torch.autograd import Variable\n\n\nclass Fold(object):\n\n    class Node(object):\n        def __init__(self, op, step, index, *args):\n            self.op = op\n            self.step = step\n            self.index = index\n            self.args = args\n            self.split_idx = -1\n            self.batch = True\n\n        def split(self, num):\n            """"""Split resulting node, if function returns multiple values.""""""\n            nodes = []\n            for idx in range(num):\n                nodes.append(Fold.Node(\n                    self.op, self.step, self.index, *self.args))\n                nodes[-1].split_idx = idx\n            return tuple(nodes)\n\n        def nobatch(self):\n            self.batch = False\n            return self\n\n        def get(self, values):\n            return values[self.step][self.op].get(self.index, self.split_idx)\n\n        def __repr__(self):\n            return ""[%d:%d]%s"" % (\n                self.step, self.index, self.op)\n\n    class ComputedResult(object):\n        def __init__(self, batch_size, batched_result):\n            self.batch_size = batch_size\n            self.result = batched_result\n            if isinstance(self.result, tuple):\n                self.result = list(self.result)\n\n        def try_get_batched(self, nodes):\n            all_are_nodes = all(isinstance(n, Fold.Node) for n in nodes)\n            num_nodes_is_equal = len(nodes) == self.batch_size\n            if not all_are_nodes or not num_nodes_is_equal:\n                return None\n\n            valid_node_sequence = all(\n                nodes[i].index < nodes[i + 1].index  # Indices are ordered\n                and nodes[i].split_idx == nodes[i + 1].split_idx  # Same split index\n                and nodes[i].step == nodes[i + 1].step  # Same step\n                and nodes[i].op == nodes[i + 1].op  # Same op\n                for i in range(len(nodes) - 1))\n            if not valid_node_sequence:\n                return None\n\n            if nodes[0].split_idx == -1 and not isinstance(self.result, tuple):\n                return self.result\n            elif nodes[0].split_idx >= 0 and not isinstance(self.result[nodes[0].split_idx], tuple):\n                return self.result[nodes[0].split_idx]\n            else:\n                # This result was already chunked.\n                return None\n\n        def get(self, index, split_idx=-1):\n            if split_idx == -1:\n                if not isinstance(self.result, tuple):\n                    self.result = torch.chunk(self.result, self.batch_size)\n                return self.result[index]\n            else:\n                if not isinstance(self.result[split_idx], tuple):\n                    self.result[split_idx] = torch.chunk(self.result[split_idx], self.batch_size)\n                return self.result[split_idx][index]\n\n    def __init__(self, volatile=False, cuda=False):\n        self.steps = collections.defaultdict(\n            lambda: collections.defaultdict(list))\n        self.cached_nodes = collections.defaultdict(dict)\n        self.total_nodes = 0\n        self.volatile = volatile\n        self._cuda = cuda\n\n    def cuda(self):\n        self._cuda = True\n        return self\n\n    def add(self, op, *args):\n        """"""Add op to the fold.""""""\n        self.total_nodes += 1\n        if not all([isinstance(arg, (\n                Fold.Node, int, torch.tensor._TensorBase, Variable)) for arg in args]):\n            raise ValueError(\n                ""All args should be Tensor, Variable, int or Node, got: %s"" % str(args))\n        if args not in self.cached_nodes[op]:\n            step = max([0] + [arg.step + 1 for arg in args\n                              if isinstance(arg, Fold.Node)])\n            node = Fold.Node(op, step, len(self.steps[step][op]), *args)\n            self.steps[step][op].append(args)\n            self.cached_nodes[op][args] = node\n        return self.cached_nodes[op][args]\n\n    def _batch_args(self, arg_lists, values):\n        res = []\n        for arg in arg_lists:\n            r = []\n            if all(isinstance(arg_item, Fold.Node) for arg_item in arg):\n                assert all(arg[0].batch == arg_item.batch\n                           for arg_item in arg[1:])\n\n                if arg[0].batch:\n                    batched_arg = values[arg[0].step][arg[0].op].try_get_batched(arg)\n                    if batched_arg is not None:\n                        res.append(batched_arg)\n                    else:\n                        res.append(\n                            torch.cat([arg_item.get(values)\n                                       for arg_item in arg], 0))\n                else:\n                    for arg_item in arg[1:]:\n                        if arg_item != arg[0]:\n                            raise ValueError(""Can not use more then one of nobatch argument, got: %s."" % str(arg_item))\n                    res.append(arg[0].get(values))\n            elif all(isinstance(arg_item, int) for arg_item in arg):\n                if self._cuda:\n                    var = Variable(\n                        torch.cuda.LongTensor(arg), volatile=self.volatile)\n                else:\n                    var = Variable(\n                        torch.LongTensor(arg), volatile=self.volatile)\n                res.append(var)\n            else:\n                for arg_item in arg:\n                    if isinstance(arg_item, Fold.Node):\n                        assert arg_item.batch\n                        r.append(arg_item.get(values))\n                    elif isinstance(arg_item, (torch.tensor._TensorBase, Variable)):\n                        r.append(arg_item)\n                    else:\n                        raise ValueError(\n                            \'Not allowed to mix Fold.Node/Tensor with int\')\n                res.append(torch.cat(r, 0))\n        return res\n\n    def apply(self, nn, nodes):\n        """"""Apply current fold to given neural module.""""""\n        values = {}\n        for step in sorted(self.steps.keys()):\n            values[step] = {}\n            for op in self.steps[step]:\n                func = getattr(nn, op)\n                try:\n                    batched_args = self._batch_args(\n                        zip(*self.steps[step][op]), values)\n                except Exception:\n                    print(""Error while executing node %s[%d] with args: %s"" % (\n                        op, step, self.steps[step][op][0]))\n                    raise\n                if batched_args:\n                    arg_size = batched_args[0].size()[0]\n                else:\n                    arg_size = 1\n                res = func(*batched_args)\n                values[step][op] = Fold.ComputedResult(arg_size, res)\n        try:\n            return self._batch_args(nodes, values)\n        except Exception:\n            print(""Retrieving %s"" % nodes)\n            for lst in nodes:\n                if isinstance(lst[0], Fold.Node):\n                    print(\', \'.join([str(x.get(values).size()) for x in lst]))\n            raise\n\n    def __str__(self):\n        result = \'\'\n        for step in sorted(self.steps.keys()):\n            result += \'%d step:\\n\' % step\n            for op in self.steps[step]:\n                first_el = \'\'\n                for arg in self.steps[step][op][0]:\n                    if first_el: first_el += \', \'\n                    if isinstance(arg, (torch.tensor._TensorBase, Variable)):\n                        first_el += str(arg.size())\n                    else:\n                        first_el += str(arg)\n                result += \'\\t%s = %d x (%s)\\n\' % (\n                    op, len(self.steps[step][op]), first_el)\n        return result\n\n    def __repr__(self):\n        return str(self)\n\n\nclass Unfold(object):\n    """"""Replacement of Fold for debugging, where it does computation right away.""""""\n\n    class Node(object):\n\n        def __init__(self, tensor):\n            self.tensor = tensor\n\n        def __repr__(self):\n            return str(self.tensor)\n\n        def nobatch(self):\n            return self\n\n        def split(self, num):\n            return [Unfold.Node(self.tensor[i]) for i in range(num)]\n\n    def __init__(self, nn, volatile=False, cuda=False):\n        self.nn = nn\n        self.volatile = volatile\n        self._cuda = cuda\n\n    def cuda(self):\n        self._cuda = True\n        return self\n\n    def _arg(self, arg):\n        if isinstance(arg, Unfold.Node):\n            return arg.tensor\n        elif isinstance(arg, int):\n            if self._cuda:\n                return Variable(torch.cuda.LongTensor([arg]), volatile=self.volatile)\n            else:\n                return Variable(torch.LongTensor([arg]), volatile=self.volatile)\n        else:\n            return arg\n\n    def add(self, op, *args):\n        values = []\n        for arg in args:\n            values.append(self._arg(arg))\n        res = getattr(self.nn, op)(*values)\n        return Unfold.Node(res)\n\n    def apply(self, nn, nodes):\n        if nn != self.nn:\n            raise ValueError(""Expected that nn argument passed to constructor and passed to apply would match."")\n        result = []\n        for n in nodes:\n            result.append(torch.cat([self._arg(a) for a in n]))\n        return result\n'"
torchfold/torchfold_test.py,9,"b'import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\n\nimport torchfold\n\nimport mock\nimport unittest\n\n\nclass TestEncoder(nn.Module):\n\n    def __init__(self):\n        super(TestEncoder, self).__init__()\n        self.embed = nn.Embedding(10, 10)\n        self.out = nn.Linear(20, 10)\n\n    def concat(self, *nodes):\n        return torch.cat(nodes, 0)\n\n    def value(self, idx):\n        return self.embed(idx)\n\n    def value2(self, idx):\n        return self.embed(idx), self.embed(idx)\n\n    def attr(self, left, right):\n        return self.out(torch.cat([left, right], 1))\n\n    def logits(self, enc, embed):\n        return torch.mm(enc, embed.t())\n\n\nclass TorchFoldTest(unittest.TestCase):\n\n    def test_rnn(self):\n        f = torchfold.Fold()\n        v1, _ = f.add(\'value2\', 1).split(2)\n        v2, _ = f.add(\'value2\', 2).split(2)\n        r = v1\n        for i in range(1000):\n            r = f.add(\'attr\', v1, v2)\n            r = f.add(\'attr\', r, v2)\n\n        te = TestEncoder()\n        enc = f.apply(te, [[r]])\n        self.assertEqual(enc[0].size(), (1, 10))\n\n    def test_nobatch(self):\n        f = torchfold.Fold()\n        v = []\n        for i in range(15):\n            v.append(f.add(\'value\', i % 10))\n        d = f.add(\'concat\', *v).nobatch()\n        res = []\n        for i in range(100):\n            res.append(f.add(\'logits\', v[i % 10], d))\n\n        te = TestEncoder()\n        enc = f.apply(te, [res])\n        self.assertEqual(len(enc), 1)\n        self.assertEqual(enc[0].size(), (100, 15))\n\n\nclass RNNEncoder(nn.Module):\n\n    def __init__(self, num_units, input_size):\n        super(RNNEncoder, self).__init__()\n        self.num_units = num_units\n        self.input_size = input_size\n        self.encoder = nn.GRUCell(self.input_size, self.num_units)\n\n    def encode(self, input_, state):\n        return self.encoder(input_, state)\n\n\nclass TestRNNBatching(unittest.TestCase):\n\n    def setUp(self):\n        torch.manual_seed(42)\n        self.input_size = 5\n        self.num_units = 4\n\n    def _generate_variable(self, dim):\n        t = torch.Tensor(1, dim).uniform_(0, 1)\n        return Variable(t)\n\n    def test_rnn_optimized_chunking(self):\n        seq_lengths = [2, 3, 5]\n\n        states = []\n        for _ in xrange(len(seq_lengths)):\n            states.append(self._generate_variable(self.num_units))\n\n        f = torchfold.Fold()\n        for seq_ind in xrange(len(seq_lengths)):\n            for _ in xrange(seq_lengths[seq_ind]):\n                states[seq_ind] = f.add(\'encode\', self._generate_variable(self.input_size), states[seq_ind])\n\n        enc = RNNEncoder(self.num_units, self.input_size)\n        with mock.patch.object(torch, \'chunk\', wraps=torch.chunk) as wrapped_chunk:\n            result = f.apply(enc, [states])\n            # torch.chunk is called 3 times instead of max(seq_lengths)=5.\n            self.assertEquals(3, wrapped_chunk.call_count)\n        self.assertEqual(len(result), 1)\n        self.assertEqual(result[0].size(), (len(seq_lengths), self.num_units))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
examples/snli/spinn-example.py,6,"b'""""""This is just example model, not real implementation !!!""""""\n\nimport time\nimport argparse\nimport sys\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nfrom torch import optim\n\nfrom torchtext import data\nfrom torchtext import datasets\n\nimport torchfold\n\n\nparser = argparse.ArgumentParser(description=\'SPINN\')\nparser.add_argument(\'--fold\', action=\'store_true\', default=False)\nparser.add_argument(\'--no-cuda\', action=\'store_true\', default=False)\nparser.add_argument(\'--batch_size\', type=int, default=128)\nargs, _ = parser.parse_known_args(sys.argv)\nargs.cuda = not args.no_cuda and torch.cuda.is_available()\n\n\nclass TreeLSTM(nn.Module):\n    def __init__(self, num_units):\n        super(TreeLSTM, self).__init__()\n        self.num_units = num_units\n        self.left = nn.Linear(num_units, 5 * num_units)\n        self.right = nn.Linear(num_units, 5 * num_units)\n\n    def forward(self, left_in, right_in):\n        lstm_in = self.left(left_in[0])\n        lstm_in += self.right(right_in[0])\n        a, i, f1, f2, o = lstm_in.chunk(5, 1)\n        c = (a.tanh() * i.sigmoid() + f1.sigmoid() * left_in[1] +\n             f2.sigmoid() * right_in[1])\n        h = o.sigmoid() * c.tanh()\n        return h, c\n\n\nclass SPINN(nn.Module):\n\n    def __init__(self, n_classes, size, n_words):\n        super(SPINN, self).__init__()\n        self.size = size\n        self.tree_lstm = TreeLSTM(size)\n        self.embeddings = nn.Embedding(n_words, size)\n        self.out = nn.Linear(size, n_classes)\n\n    def leaf(self, word_id):\n        return self.embeddings(word_id), Variable(torch.FloatTensor(word_id.size()[0], self.size))\n\n    def children(self, left_h, left_c, right_h, right_c):\n        return self.tree_lstm((left_h, left_c), (right_h, right_c))\n\n    def logits(self, encoding):\n        return self.out(encoding)\n\n\ndef encode_tree_regular(model, tree):\n    def encode_node(node):\n        if node.is_leaf():\n            return model.leaf(Variable(torch.LongTensor([node.id])))\n        else:\n            left_h, left_c = encode_node(node.left)\n            right_h, right_c = encode_node(node.right)\n            return model.children(left_h, left_c, right_h, right_c)\n    encoding, _ = encode_node(tree.root)\n    return model.logits(encoding)\n\n\ndef encode_tree_fold(fold, tree):\n    def encode_node(node):\n        if node.is_leaf():\n            return fold.add(\'leaf\', node.id).split(2)\n        else:\n            left_h, left_c = encode_node(node.left)\n            right_h, right_c = encode_node(node.right)\n            return fold.add(\'children\', left_h, left_c, right_h, right_c).split(2)\n    encoding, _ = encode_node(tree.root)\n    return fold.add(\'logits\', encoding)\n\n\nclass Tree(object):\n    class Node(object):\n        def __init__(self, leaf=None, left=None, right=None):\n            self.id = leaf\n            self.left = left\n            self.right = right\n\n        def is_leaf(self):\n            return self.id is not None\n\n        def __repr__(self):\n            return str(self.id) if self.is_leaf() else ""(%s, %s)"" % (self.left, self.right)\n\n    def __init__(self, example, inputs_vocab, answer_vocab):\n        self.label = answer_vocab.stoi[example.label] - 1\n        queue = []\n        idx, transition_idx = 0, 0\n        while transition_idx < len(example.premise_transitions):\n            t = example.premise_transitions[transition_idx]\n            transition_idx += 1\n            if t == \'shift\':\n                queue.append(Tree.Node(leaf=inputs_vocab.stoi[example.premise[idx]]))\n                idx += 1\n            else:\n                n_left = queue.pop()\n                n_right = queue.pop()\n                queue.append(Tree.Node(left=n_left, right=n_right))\n        assert len(queue) == 1\n        self.root = queue[0]\n\n\ndef main():\n    inputs = datasets.snli.ParsedTextField(lower=True)\n    transitions = datasets.snli.ShiftReduceField()\n    answers = data.Field(sequential=False)\n\n    train, dev, test = datasets.SNLI.splits(inputs, answers, transitions)\n    inputs.build_vocab(train, dev, test)\n    answers.build_vocab(train)\n    train_iter, dev_iter, test_iter = data.BucketIterator.splits(\n        (train, dev, test), batch_size=args.batch_size, device=0 if args.cuda else -1)\n\n    model = SPINN(3, 500, 1000)\n    criterion = nn.CrossEntropyLoss()\n    opt = optim.Adam(model.parameters(), lr=0.01)\n\n    for epoch in range(10):\n        start = time.time()\n        iteration = 0\n        for batch_idx, batch in enumerate(train_iter):\n            opt.zero_grad()\n\n            all_logits, all_labels = [], []\n            fold = torchfold.Fold(cuda=args.cuda)\n            for example in batch.dataset:\n                tree = Tree(example, inputs.vocab, answers.vocab)\n                if args.fold:\n                    all_logits.append(encode_tree_fold(fold, tree))\n                else:\n                    all_logits.append(encode_tree_regular(model, tree))\n                all_labels.append(tree.label)\n\n            if args.fold:\n                res = fold.apply(model, [all_logits, all_labels])\n                loss = criterion(res[0], res[1])\n            else:\n                loss = criterion(torch.cat(all_logits, 0), Variable(torch.LongTensor(all_labels)))\n            loss.backward(); opt.step()\n\n            iteration += 1\n            if iteration % 10 == 0:\n                print(""Avg. Time: %fs"" % ((time.time() - start) / iteration))\n                # iteration = 0\n                # start = time.time()\n\n\nif __name__ == ""__main__"":\n    main()\n'"
