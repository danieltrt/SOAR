file_path,api_count,code
setup.py,0,"b'from setuptools import setup, find_packages\n\n\ndef parse_requirements(filename):\n    """""" load requirements from a pip requirements file """"""\n    lineiter = (line.strip() for line in open(filename))\n    return [line for line in lineiter if line and not line.startswith(""#"")]\n\n\nreqs = parse_requirements(\'requirements.txt\')\n\n\nwith open(\'README.md\') as f:\n    long_description = f.read()\n\n\nsetup(name=\'pytorch2keras\',\n      version=\'0.2.4\',\n      description=\'The deep learning models converter\',\n      long_description=long_description,\n      long_description_content_type=\'text/markdown\',\n      url=\'https://github.com/nerox8664/pytorch2keras\',\n      author=\'Grigory Malivenko\',\n      author_email=\'nerox8664@gmail.com\',\n      classifiers=[\n        \'Development Status :: 3 - Alpha\',\n        \'Intended Audience :: Science/Research\',\n        \'License :: OSI Approved :: MIT License\',\n        \'Operating System :: OS Independent\',\n        \'Programming Language :: Python\',\n        \'Topic :: Scientific/Engineering :: Image Recognition\',\n      ],\n      keywords=\'machine-learning deep-learning pytorch keras neuralnetwork vgg resnet \'\n               \'densenet drn dpn darknet squeezenet mobilenet\',\n      license=\'MIT\',\n      packages=find_packages(),\n      install_requires=reqs,\n      zip_safe=False)\n'"
pytorch2keras/__init__.py,0,"b""from .converter import pytorch_to_keras\n\n__all__ = ['pytorch_to_keras']"""
pytorch2keras/converter.py,2,"b'""""""\nThe PyTorch2Keras converter interface\n""""""\n\nfrom onnx2keras import onnx_to_keras\nimport torch\nimport onnx\nfrom onnx import optimizer\nimport io\nimport logging\n\n\ndef pytorch_to_keras(\n    model, args, input_shapes=None,\n    change_ordering=False, verbose=False, name_policy=None,\n    use_optimizer=False, do_constant_folding=False\n):\n    """"""\n    By given PyTorch model convert layers with ONNX.\n\n    Args:\n        model: pytorch model\n        args: pytorch model arguments\n        input_shapes: keras input shapes (using for each InputLayer)\n        change_ordering: change CHW to HWC\n        verbose: verbose output\n        name_policy: use short names, use random-suffix or keep original names for keras layers\n\n    Returns:\n        model: created keras model.\n    """"""\n    logger = logging.getLogger(\'pytorch2keras\')\n\n    if verbose:\n        logging.basicConfig(level=logging.DEBUG)\n\n    logger.info(\'Converter is called.\')\n\n    if name_policy:\n        logger.warning(\'Name policy isn\\\'t supported now.\')\n\n    if input_shapes:\n        logger.warning(\'Custom shapes isn\\\'t supported now.\')\n\n    if input_shapes and not isinstance(input_shapes, list):\n        input_shapes = [input_shapes]\n\n    if not isinstance(args, list):\n        args = [args]\n\n    args = tuple(args)\n\n    dummy_output = model(*args)\n\n    if isinstance(dummy_output, torch.autograd.Variable):\n        dummy_output = [dummy_output]\n\n    input_names = [\'input_{0}\'.format(i) for i in range(len(args))]\n    output_names = [\'output_{0}\'.format(i) for i in range(len(dummy_output))]\n\n    logger.debug(\'Input_names:\')\n    logger.debug(input_names)\n\n    logger.debug(\'Output_names:\')\n    logger.debug(output_names)\n\n    stream = io.BytesIO()\n    torch.onnx.export(model, args, stream, do_constant_folding=do_constant_folding, verbose=verbose, input_names=input_names, output_names=output_names)\n\n    stream.seek(0)\n    onnx_model = onnx.load(stream)\n    if use_optimizer:\n        if use_optimizer is True:\n            optimizer2run = optimizer.get_available_passes()\n        else:\n            use_optimizer = set(use_optimizer)\n            optimizer2run = [x for x in optimizer.get_available_passes() if x in use_optimizer]\n        logger.info(""Running optimizer:\\n%s"", ""\\n"".join(optimizer2run))\n        onnx_model = optimizer.optimize(onnx_model, optimizer2run)\n\n    k_model = onnx_to_keras(onnx_model=onnx_model, input_names=input_names,\n                            input_shapes=input_shapes, name_policy=name_policy,\n                            verbose=verbose, change_ordering=change_ordering)\n\n    return k_model\n'"
tests/__init__.py,0,b''
tests/layers/__init__.py,0,b''
tests/layers/multiple_inputs.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x, y, z):\n        from torch.nn import functional as F\n        return F.relu(x) + F.relu(y) + F.relu(z)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np1 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var1 = Variable(torch.FloatTensor(input_np1))\n\n        input_np2 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var2 = Variable(torch.FloatTensor(input_np2))\n        output = model(input_var1, input_var2, input_var2)\n\n        k_model = pytorch_to_keras(model, [input_var1, input_var2, input_var2], [(3, 224, 224,), (3, 224, 224,), (3, 224, 224,)], verbose=True)\n\n        error = check_error(output, k_model, [input_np1, input_np2, input_np2])\n        if max_error < error:\n            max_error = error\n\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/upsample_nearest.py,4,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torch.nn.functional as F\n\n\nclass TestUpsampleNearest2d(nn.Module):\n    """"""Module for UpsampleNearest2d conversion testing\n    """"""\n    def __init__(self, inp=10, out=16, kernel_size=3, bias=True):\n        super(TestUpsampleNearest2d, self).__init__()\n        self.conv2d = nn.Conv2d(inp, out, kernel_size=kernel_size, bias=bias)\n        self.up = nn.UpsamplingNearest2d(scale_factor=2)\n\n    def forward(self, x):\n        x = self.conv2d(x)\n        x = F.upsample(x, scale_factor=2)\n        x = self.up(x)\n        return x\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(100):\n        kernel_size = np.random.randint(1, 7)\n        inp = np.random.randint(kernel_size + 1, 100)\n        out = np.random.randint(1, 100)\n\n        model = TestUpsampleNearest2d(inp, out, kernel_size, inp % 2)\n\n        input_np = np.random.uniform(0, 1, (1, inp, inp, inp))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (inp, inp, inp,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/__init__.py,0,b''
tests/models/alexnet.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass AlexNet(torchvision.models.AlexNet):\n    def __init__(self):\n        super(AlexNet, self).__init__()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view([int(x.size(0)), 256 * 6 * 6])  # << important fix\n        x = self.classifier(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = AlexNet()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/drn.py,3,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torch.nn as nn\n\n\n# Code below copied from DRN repo\nimport math\nBatchNorm = nn.BatchNorm2d\n\n\ndef conv3x3(in_planes, out_planes, stride=1, padding=1, dilation=1):\n    return nn.Conv2d(\n        in_planes, out_planes, kernel_size=3, stride=stride, padding=padding, bias=False, dilation=dilation\n    )\n\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride,\n                             padding=dilation[0], dilation=dilation[0])\n        self.bn1 = BatchNorm(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes,\n                             padding=dilation[1], dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.residual = residual\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n        if self.residual:\n            out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None,\n                 dilation=(1, 1), residual=True):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = BatchNorm(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=dilation[1], bias=False,\n                               dilation=dilation[1])\n        self.bn2 = BatchNorm(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = BatchNorm(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass DRN(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000,\n                 channels=(16, 32, 64, 128, 256, 512, 512, 512),\n                 out_map=False, out_middle=False, pool_size=28, arch='D'):\n        super(DRN, self).__init__()\n        self.inplanes = channels[0]\n        self.out_map = out_map\n        self.out_dim = channels[-1]\n        self.out_middle = out_middle\n        self.arch = arch\n\n        if arch == 'C':\n            self.conv1 = nn.Conv2d(3, channels[0], kernel_size=7, stride=1,\n                                   padding=3, bias=False)\n            self.bn1 = BatchNorm(channels[0])\n            self.relu = nn.ReLU(inplace=True)\n\n            self.layer1 = self._make_layer(\n                BasicBlock, channels[0], layers[0], stride=1)\n            self.layer2 = self._make_layer(\n                BasicBlock, channels[1], layers[1], stride=2)\n        elif arch == 'D':\n            self.layer0 = nn.Sequential(\n                nn.Conv2d(3, channels[0], kernel_size=7, stride=1, padding=3,\n                          bias=False),\n                BatchNorm(channels[0]),\n                nn.ReLU(inplace=True)\n            )\n\n            self.layer1 = self._make_conv_layers(\n                channels[0], layers[0], stride=1)\n            self.layer2 = self._make_conv_layers(\n                channels[1], layers[1], stride=2)\n\n        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=2)\n        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=2)\n        self.layer5 = self._make_layer(block, channels[4], layers[4],\n                                       dilation=2, new_level=False)\n        self.layer6 = None if layers[5] == 0 else \\\n            self._make_layer(block, channels[5], layers[5], dilation=4,\n                             new_level=False)\n\n        if arch == 'C':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_layer(BasicBlock, channels[6], layers[6], dilation=2,\n                                 new_level=False, residual=False)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_layer(BasicBlock, channels[7], layers[7], dilation=1,\n                                 new_level=False, residual=False)\n        elif arch == 'D':\n            self.layer7 = None if layers[6] == 0 else \\\n                self._make_conv_layers(channels[6], layers[6], dilation=2)\n            self.layer8 = None if layers[7] == 0 else \\\n                self._make_conv_layers(channels[7], layers[7], dilation=1)\n\n        if num_classes > 0:\n            self.avgpool = nn.AvgPool2d(pool_size)\n            self.fc = nn.Conv2d(self.out_dim, num_classes, kernel_size=1,\n                                stride=1, padding=0, bias=True)\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1,\n                    new_level=True, residual=True):\n        assert dilation == 1 or dilation % 2 == 0\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                BatchNorm(planes * block.expansion),\n            )\n\n        layers = list()\n        layers.append(block(\n            self.inplanes, planes, stride, downsample,\n            dilation=(1, 1) if dilation == 1 else (\n                dilation // 2 if new_level else dilation, dilation),\n            residual=residual))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, residual=residual,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def _make_conv_layers(self, channels, convs, stride=1, dilation=1):\n        modules = []\n        for i in range(convs):\n            modules.extend([\n                nn.Conv2d(self.inplanes, channels, kernel_size=3,\n                          stride=stride if i == 0 else 1,\n                          padding=dilation, bias=False, dilation=dilation),\n                BatchNorm(channels),\n                nn.ReLU(inplace=True)])\n            self.inplanes = channels\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        y = list()\n\n        if self.arch == 'C':\n            x = self.conv1(x)\n            x = self.bn1(x)\n            x = self.relu(x)\n        elif self.arch == 'D':\n            x = self.layer0(x)\n\n        x = self.layer1(x)\n        y.append(x)\n        x = self.layer2(x)\n        y.append(x)\n\n        x = self.layer3(x)\n        y.append(x)\n\n        x = self.layer4(x)\n        y.append(x)\n\n        x = self.layer5(x)\n        y.append(x)\n\n        if self.layer6 is not None:\n            x = self.layer6(x)\n            y.append(x)\n\n        if self.layer7 is not None:\n            x = self.layer7(x)\n            y.append(x)\n\n        if self.layer8 is not None:\n            x = self.layer8(x)\n            y.append(x)\n\n        if self.out_map:\n            x = self.fc(x)\n        else:\n            x = self.avgpool(x)\n            x = self.fc(x)\n            x = x.view(x.size(0), -1)\n\n        if self.out_middle:\n            return x, y\n        else:\n            return x\n\n\nclass DRN_A(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        self.inplanes = 64\n        super(DRN_A, self).__init__()\n        self.out_dim = 512 * block.expansion\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=1,\n                                       dilation=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=1,\n                                       dilation=4)\n        self.avgpool = nn.AvgPool2d(28, stride=1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n            elif isinstance(m, BatchNorm):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n        # for m in self.modules():\n        #     if isinstance(m, nn.Conv2d):\n        #         nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n        #     elif isinstance(m, nn.BatchNorm2d):\n        #         nn.init.constant_(m.weight, 1)\n        #         nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes,\n                                dilation=(dilation, dilation)))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = DRN(BasicBlock, [1, 1, 2, 2, 2, 2, 1, 1], arch='C')\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/menet.py,7,"b'import numpy as np\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n""""""\n    MENet, implemented in PyTorch.\n    Original paper: \'Merging and Evolution: Improving Convolutional Neural Networks for Mobile Applications\'\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\n\n\ndef depthwise_conv3x3(channels,\n                      stride):\n    return nn.Conv2d(\n        in_channels=channels,\n        out_channels=channels,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        groups=channels,\n        bias=False)\n\n\ndef group_conv1x1(in_channels,\n                  out_channels,\n                  groups):\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        groups=groups,\n        bias=False)\n\n\ndef channel_shuffle(x,\n                    groups):\n    """"""Channel Shuffle operation from ShuffleNet [arxiv: 1707.01083]\n    Arguments:\n        x (Tensor): tensor to shuffle.\n        groups (int): groups to be split\n    """"""\n    batch, channels, height, width = x.size()\n    channels_per_group = channels // groups\n    x = x.view(batch, groups, channels_per_group, height, width)\n    x = torch.transpose(x, 1, 2).contiguous()\n    x = x.view(batch, channels, height, width)\n    return x\n\n\nclass ChannelShuffle(nn.Module):\n\n    def __init__(self,\n                 channels,\n                 groups):\n        super(ChannelShuffle, self).__init__()\n        if channels % groups != 0:\n            raise ValueError(\'channels must be divisible by groups\')\n        self.groups = groups\n\n    def forward(self, x):\n        return channel_shuffle(x, self.groups)\n\n\nclass ShuffleInitBlock(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n        super(ShuffleInitBlock, self).__init__()\n\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=3,\n            stride=2,\n            padding=1,\n            bias=False)\n        self.bn = nn.BatchNorm2d(num_features=out_channels)\n        self.activ = nn.ReLU(inplace=True)\n        self.pool = nn.MaxPool2d(\n            kernel_size=3,\n            stride=2,\n            padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.activ(x)\n        x = self.pool(x)\n        return x\n\n\ndef conv1x1(in_channels,\n            out_channels):\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        bias=False)\n\n\ndef conv3x3(in_channels,\n            out_channels,\n            stride):\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=1,\n        bias=False)\n\n\nclass MEModule(nn.Module):\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 side_channels,\n                 groups,\n                 downsample,\n                 ignore_group):\n        super(MEModule, self).__init__()\n        self.downsample = downsample\n        mid_channels = out_channels // 4\n\n        if downsample:\n            out_channels -= in_channels\n\n        # residual branch\n        self.compress_conv1 = group_conv1x1(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            groups=(1 if ignore_group else groups))\n        self.compress_bn1 = nn.BatchNorm2d(num_features=mid_channels)\n        self.c_shuffle = ChannelShuffle(\n            channels=mid_channels,\n            groups=(1 if ignore_group else groups))\n        self.dw_conv2 = depthwise_conv3x3(\n            channels=mid_channels,\n            stride=(2 if self.downsample else 1))\n        self.dw_bn2 = nn.BatchNorm2d(num_features=mid_channels)\n        self.expand_conv3 = group_conv1x1(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            groups=groups)\n        self.expand_bn3 = nn.BatchNorm2d(num_features=out_channels)\n        if downsample:\n            self.avgpool = nn.AvgPool2d(kernel_size=3, stride=2, padding=1)\n        self.activ = nn.ReLU(inplace=True)\n\n        # fusion branch\n        self.s_merge_conv = conv1x1(\n            in_channels=mid_channels,\n            out_channels=side_channels)\n        self.s_merge_bn = nn.BatchNorm2d(num_features=side_channels)\n        self.s_conv = conv3x3(\n            in_channels=side_channels,\n            out_channels=side_channels,\n            stride=(2 if self.downsample else 1))\n        self.s_conv_bn = nn.BatchNorm2d(num_features=side_channels)\n        self.s_evolve_conv = conv1x1(\n            in_channels=side_channels,\n            out_channels=mid_channels)\n        self.s_evolve_bn = nn.BatchNorm2d(num_features=mid_channels)\n\n    def forward(self, x):\n        identity = x\n        # pointwise group convolution 1\n        x = self.activ(self.compress_bn1(self.compress_conv1(x)))\n        x = self.c_shuffle(x)\n        # merging\n        y = self.s_merge_conv(x)\n        y = self.s_merge_bn(y)\n        y = self.activ(y)\n        # depthwise convolution (bottleneck)\n        x = self.dw_bn2(self.dw_conv2(x))\n        # evolution\n        y = self.s_conv(y)\n        y = self.s_conv_bn(y)\n        y = self.activ(y)\n        y = self.s_evolve_conv(y)\n        y = self.s_evolve_bn(y)\n        y = F.sigmoid(y)\n        x = x * y\n        # pointwise group convolution 2\n        x = self.expand_bn3(self.expand_conv3(x))\n        # identity branch\n        if self.downsample:\n            identity = self.avgpool(identity)\n            x = torch.cat((x, identity), dim=1)\n        else:\n            x = x + identity\n        x = self.activ(x)\n        return x\n\n\nclass MENet(nn.Module):\n\n    def __init__(self,\n                 block_channels,\n                 side_channels,\n                 groups,\n                 num_classes=1000):\n        super(MENet, self).__init__()\n        input_channels = 3\n        block_layers = [4, 8, 4]\n\n        self.features = nn.Sequential()\n        self.features.add_module(""init_block"", ShuffleInitBlock(\n            in_channels=input_channels,\n            out_channels=block_channels[0]))\n\n        for i in range(len(block_channels) - 1):\n            stage = nn.Sequential()\n            in_channels_i = block_channels[i]\n            out_channels_i = block_channels[i + 1]\n            for j in range(block_layers[i]):\n                stage.add_module(""unit_{}"".format(j + 1), MEModule(\n                    in_channels=(in_channels_i if j == 0 else out_channels_i),\n                    out_channels=out_channels_i,\n                    side_channels=side_channels,\n                    groups=groups,\n                    downsample=(j == 0),\n                    ignore_group=(i == 0 and j == 0)))\n            self.features.add_module(""stage_{}"".format(i + 1), stage)\n\n        self.features.add_module(\'final_pool\', nn.AvgPool2d(kernel_size=7))\n\n        self.output = nn.Linear(\n            in_features=block_channels[-1],\n            out_features=num_classes)\n\n        self._init_params()\n\n    def _init_params(self):\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Conv2d):\n                init.kaiming_uniform_(module.weight)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.output(x)\n        return x\n\n\ndef get_menet(first_block_channels,\n              side_channels,\n              groups,\n              pretrained=False,\n              **kwargs):\n    if first_block_channels == 108:\n        block_channels = [12, 108, 216, 432]\n    elif first_block_channels == 128:\n        block_channels = [12, 128, 256, 512]\n    elif first_block_channels == 160:\n        block_channels = [16, 160, 320, 640]\n    elif first_block_channels == 228:\n        block_channels = [24, 228, 456, 912]\n    elif first_block_channels == 256:\n        block_channels = [24, 256, 512, 1024]\n    elif first_block_channels == 348:\n        block_channels = [24, 348, 696, 1392]\n    elif first_block_channels == 352:\n        block_channels = [24, 352, 704, 1408]\n    elif first_block_channels == 456:\n        block_channels = [48, 456, 912, 1824]\n    else:\n        raise ValueError(""The {} of `first_block_channels` is not supported"".format(first_block_channels))\n\n    if pretrained:\n        raise ValueError(""Pretrained model is not supported"")\n\n    net = MENet(\n        block_channels=block_channels,\n        side_channels=side_channels,\n        groups=groups,\n        **kwargs)\n    return net\n\n\ndef menet108_8x1_g3(**kwargs):\n    return get_menet(108, 8, 3, **kwargs)\n\n\ndef menet128_8x1_g4(**kwargs):\n    return get_menet(128, 8, 4, **kwargs)\n\n\ndef menet160_8x1_g8(**kwargs):\n    return get_menet(160, 8, 8, **kwargs)\n\n\ndef menet228_12x1_g3(**kwargs):\n    return get_menet(228, 12, 3, **kwargs)\n\n\ndef menet256_12x1_g4(**kwargs):\n    return get_menet(256, 12, 4, **kwargs)\n\n\ndef menet348_12x1_g3(**kwargs):\n    return get_menet(348, 12, 3, **kwargs)\n\n\ndef menet352_12x1_g8(**kwargs):\n    return get_menet(352, 12, 8, **kwargs)\n\n\ndef menet456_24x1_g3(**kwargs):\n    return get_menet(456, 24, 3, **kwargs)\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(10):\n        model = menet228_12x1_g3()\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/mobilinet.py,3,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torch.nn as nn\nimport math\n\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        self.conv = nn.Sequential(\n            # pw\n            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # dw\n            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, 3, stride, 1, groups=inp * expand_ratio, bias=False),\n            nn.BatchNorm2d(inp * expand_ratio),\n            nn.ReLU6(inplace=True),\n            # pw-linear\n            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),\n            nn.BatchNorm2d(oup),\n        )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        # setting of inverted residual blocks\n        self.interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(32 * width_mult)\n        self.last_channel = int(1280 * width_mult) if width_mult > 1.0 else 1280\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in self.interverted_residual_setting:\n            output_channel = int(c * width_mult)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(InvertedResidual(input_channel, output_channel, s, t))\n                else:\n                    self.features.append(InvertedResidual(input_channel, output_channel, 1, t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        self.features.append(nn.AvgPool2d(input_size//32))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        # self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(-1, self.last_channel)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = MobileNetV2()\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/preresnet18.py,5,"b'""""""\nModel from https://github.com/osmr/imgclsmob/tree/master/pytorch/models\n""""""\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\nimport os\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass PreResConv(nn.Module):\n    """"""\n    PreResNet specific convolution block, with pre-activation.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int\n        Padding value for convolution layer.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding):\n        super(PreResConv, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=in_channels)\n        self.activ = nn.ReLU(inplace=True)\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            bias=False)\n\n    def forward(self, x):\n        x = self.bn(x)\n        x = self.activ(x)\n        x_pre_activ = x\n        x = self.conv(x)\n        return x, x_pre_activ\n\n\ndef conv1x1(in_channels,\n            out_channels,\n            stride):\n    """"""\n    Convolution 1x1 layer.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    """"""\n    return nn.Conv2d(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=stride,\n        padding=0,\n        bias=False)\n\n\ndef preres_conv1x1(in_channels,\n                   out_channels,\n                   stride):\n    """"""\n    1x1 version of the PreResNet specific convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    """"""\n    return PreResConv(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=1,\n        stride=stride,\n        padding=0)\n\n\ndef preres_conv3x3(in_channels,\n                   out_channels,\n                   stride):\n    """"""\n    3x3 version of the PreResNet specific convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    bn_use_global_stats : bool\n        Whether global moving statistics is used instead of local batch-norm for BatchNorm layers.\n    """"""\n    return PreResConv(\n        in_channels=in_channels,\n        out_channels=out_channels,\n        kernel_size=3,\n        stride=stride,\n        padding=1)\n\n\nclass PreResBlock(nn.Module):\n    """"""\n    Simple PreResNet block for residual path in ResNet unit.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride):\n        super(PreResBlock, self).__init__()\n        self.conv1 = preres_conv3x3(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            stride=stride)\n        self.conv2 = preres_conv3x3(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            stride=1)\n\n    def forward(self, x):\n        x, x_pre_activ = self.conv1(x)\n        x, _ = self.conv2(x)\n        return x, x_pre_activ\n\n\nclass PreResBottleneck(nn.Module):\n    """"""\n    PreResNet bottleneck block for residual path in PreResNet unit.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    conv1_stride : bool\n        Whether to use stride in the first or the second convolution layer of the block.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 conv1_stride):\n        super(PreResBottleneck, self).__init__()\n        mid_channels = out_channels // 4\n\n        self.conv1 = preres_conv1x1(\n            in_channels=in_channels,\n            out_channels=mid_channels,\n            stride=(stride if conv1_stride else 1))\n        self.conv2 = preres_conv3x3(\n            in_channels=mid_channels,\n            out_channels=mid_channels,\n            stride=(1 if conv1_stride else stride))\n        self.conv3 = preres_conv1x1(\n            in_channels=mid_channels,\n            out_channels=out_channels,\n            stride=1)\n\n    def forward(self, x):\n        x, x_pre_activ = self.conv1(x)\n        x, _ = self.conv2(x)\n        x, _ = self.conv3(x)\n        return x, x_pre_activ\n\n\nclass PreResUnit(nn.Module):\n    """"""\n    PreResNet unit with residual connection.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    bottleneck : bool\n        Whether to use a bottleneck or simple block in units.\n    conv1_stride : bool\n        Whether to use stride in the first or the second convolution layer of the block.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride,\n                 bottleneck,\n                 conv1_stride):\n        super(PreResUnit, self).__init__()\n        self.resize_identity = (in_channels != out_channels) or (stride != 1)\n\n        if bottleneck:\n            self.body = PreResBottleneck(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                stride=stride,\n                conv1_stride=conv1_stride)\n        else:\n            self.body = PreResBlock(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                stride=stride)\n        if self.resize_identity:\n            self.identity_conv = conv1x1(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                stride=stride)\n\n    def forward(self, x):\n        identity = x\n        x, x_pre_activ = self.body(x)\n        if self.resize_identity:\n            identity = self.identity_conv(x_pre_activ)\n        x = x + identity\n        return x\n\n\nclass PreResInitBlock(nn.Module):\n    """"""\n    PreResNet specific initial block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n        super(PreResInitBlock, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=3,\n            bias=False)\n        self.bn = nn.BatchNorm2d(num_features=out_channels)\n        self.activ = nn.ReLU(inplace=True)\n        self.pool = nn.MaxPool2d(\n            kernel_size=3,\n            stride=2,\n            padding=1)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.activ(x)\n        x = self.pool(x)\n        return x\n\n\nclass PreResActivation(nn.Module):\n    """"""\n    PreResNet pure pre-activation block without convolution layer. It\'s used by itself as the final block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    """"""\n    def __init__(self,\n                 in_channels):\n        super(PreResActivation, self).__init__()\n        self.bn = nn.BatchNorm2d(num_features=in_channels)\n        self.activ = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.bn(x)\n        x = self.activ(x)\n        return x\n\n\nclass PreResNet(nn.Module):\n    """"""\n    PreResNet model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    channels : list of list of int\n        Number of output channels for each unit.\n    init_block_channels : int\n        Number of output channels for the initial unit.\n    bottleneck : bool\n        Whether to use a bottleneck or simple block in units.\n    conv1_stride : bool\n        Whether to use stride in the first or the second convolution layer in units.\n    in_channels : int, default 3\n        Number of input channels.\n    num_classes : int, default 1000\n        Number of classification classes.\n    """"""\n    def __init__(self,\n                 channels,\n                 init_block_channels,\n                 bottleneck,\n                 conv1_stride,\n                 in_channels=3,\n                 num_classes=1000):\n        super(PreResNet, self).__init__()\n\n        self.features = nn.Sequential()\n        self.features.add_module(""init_block"", PreResInitBlock(\n            in_channels=in_channels,\n            out_channels=init_block_channels))\n        in_channels = init_block_channels\n        for i, channels_per_stage in enumerate(channels):\n            stage = nn.Sequential()\n            for j, out_channels in enumerate(channels_per_stage):\n                stride = 1 if (i == 0) or (j != 0) else 2\n                stage.add_module(""unit{}"".format(j + 1), PreResUnit(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    stride=stride,\n                    bottleneck=bottleneck,\n                    conv1_stride=conv1_stride))\n                in_channels = out_channels\n            self.features.add_module(""stage{}"".format(i + 1), stage)\n        self.features.add_module(\'post_activ\', PreResActivation(in_channels=in_channels))\n        self.features.add_module(\'final_pool\', nn.AvgPool2d(\n            kernel_size=7,\n            stride=1))\n\n        self.output = nn.Linear(\n            in_features=in_channels,\n            out_features=num_classes)\n\n        self._init_params()\n\n    def _init_params(self):\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Conv2d):\n                init.kaiming_uniform_(module.weight)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(int(x.size(0)), -1)\n        x = self.output(x)\n        return x\n\n\ndef get_preresnet(blocks,\n                  conv1_stride=True,\n                  width_scale=1.0,\n                  model_name=None,\n                  pretrained=False,\n                  root=os.path.join(\'~\', \'.torch\', \'models\'),\n                  **kwargs):\n    """"""\n    Create PreResNet model with specific parameters.\n\n    Parameters:\n    ----------\n    blocks : int\n        Number of blocks.\n    conv1_stride : bool\n        Whether to use stride in the first or the second convolution layer in units.\n    width_scale : float\n        Scale factor for width of layers.\n    model_name : str or None, default None\n        Model name for loading pretrained model.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n\n    if blocks == 10:\n        layers = [1, 1, 1, 1]\n    elif blocks == 12:\n        layers = [2, 1, 1, 1]\n    elif blocks == 14:\n        layers = [2, 2, 1, 1]\n    elif blocks == 16:\n        layers = [2, 2, 2, 1]\n    elif blocks == 18:\n        layers = [2, 2, 2, 2]\n    elif blocks == 34:\n        layers = [3, 4, 6, 3]\n    elif blocks == 50:\n        layers = [3, 4, 6, 3]\n    elif blocks == 101:\n        layers = [3, 4, 23, 3]\n    elif blocks == 152:\n        layers = [3, 8, 36, 3]\n    elif blocks == 200:\n        layers = [3, 24, 36, 3]\n    else:\n        raise ValueError(""Unsupported ResNet with number of blocks: {}"".format(blocks))\n\n    init_block_channels = 64\n\n    if blocks < 50:\n        channels_per_layers = [64, 128, 256, 512]\n        bottleneck = False\n    else:\n        channels_per_layers = [256, 512, 1024, 2048]\n        bottleneck = True\n\n    channels = [[ci] * li for (ci, li) in zip(channels_per_layers, layers)]\n\n    if width_scale != 1.0:\n        channels = [[int(cij * width_scale) for cij in ci] for ci in channels]\n        init_block_channels = int(init_block_channels * width_scale)\n\n    net = PreResNet(\n        channels=channels,\n        init_block_channels=init_block_channels,\n        bottleneck=bottleneck,\n        conv1_stride=conv1_stride,\n        **kwargs)\n\n    if pretrained:\n        if (model_name is None) or (not model_name):\n            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")\n        import torch\n        from .model_store import get_model_file\n        net.load_state_dict(torch.load(get_model_file(\n            model_name=model_name,\n            local_model_store_dir_path=root)))\n\n    return net\n\n\ndef preresnet10(**kwargs):\n    """"""\n    PreResNet-10 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n    It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=10, model_name=""preresnet10"", **kwargs)\n\n\ndef preresnet12(**kwargs):\n    """"""\n    PreResNet-12 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n    It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=12, model_name=""preresnet12"", **kwargs)\n\n\ndef preresnet14(**kwargs):\n    """"""\n    PreResNet-14 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n    It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=14, model_name=""preresnet14"", **kwargs)\n\n\ndef preresnet16(**kwargs):\n    """"""\n    PreResNet-16 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n    It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=16, model_name=""preresnet16"", **kwargs)\n\n\ndef preresnet18_wd4(**kwargs):\n    """"""\n    PreResNet-18 model with 0.25 width scale from \'Identity Mappings in Deep Residual Networks,\'\n    https://arxiv.org/abs/1603.05027. It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=18, width_scale=0.25, model_name=""preresnet18_wd4"", **kwargs)\n\n\ndef preresnet18_wd2(**kwargs):\n    """"""\n    PreResNet-18 model with 0.5 width scale from \'Identity Mappings in Deep Residual Networks,\'\n    https://arxiv.org/abs/1603.05027. It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=18, width_scale=0.5, model_name=""preresnet18_wd2"", **kwargs)\n\n\ndef preresnet18_w3d4(**kwargs):\n    """"""\n    PreResNet-18 model with 0.75 width scale from \'Identity Mappings in Deep Residual Networks,\'\n    https://arxiv.org/abs/1603.05027. It\'s an experimental model.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=18, width_scale=0.75, model_name=""preresnet18_w3d4"", **kwargs)\n\n\ndef preresnet18(**kwargs):\n    """"""\n    PreResNet-18 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=18, model_name=""preresnet18"", **kwargs)\n\n\ndef preresnet34(**kwargs):\n    """"""\n    PreResNet-34 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=34, model_name=""preresnet34"", **kwargs)\n\n\ndef preresnet50(**kwargs):\n    """"""\n    PreResNet-50 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=50, model_name=""preresnet50"", **kwargs)\n\n\ndef preresnet50b(**kwargs):\n    """"""\n    PreResNet-50 model with stride at the second convolution in bottleneck block from \'Identity Mappings in Deep\n    Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=50, conv1_stride=False, model_name=""preresnet50b"", **kwargs)\n\n\ndef preresnet101(**kwargs):\n    """"""\n    PreResNet-101 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=101, model_name=""preresnet101"", **kwargs)\n\n\ndef preresnet101b(**kwargs):\n    """"""\n    PreResNet-101 model with stride at the second convolution in bottleneck block from \'Identity Mappings in Deep\n    Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=101, conv1_stride=False, model_name=""preresnet101b"", **kwargs)\n\n\ndef preresnet152(**kwargs):\n    """"""\n    PreResNet-152 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=152, model_name=""preresnet152"", **kwargs)\n\n\ndef preresnet152b(**kwargs):\n    """"""\n    PreResNet-152 model with stride at the second convolution in bottleneck block from \'Identity Mappings in Deep\n    Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=152, conv1_stride=False, model_name=""preresnet152b"", **kwargs)\n\n\ndef preresnet200(**kwargs):\n    """"""\n    PreResNet-200 model from \'Identity Mappings in Deep Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=200, model_name=""preresnet200"", **kwargs)\n\n\ndef preresnet200b(**kwargs):\n    """"""\n    PreResNet-200 model with stride at the second convolution in bottleneck block from \'Identity Mappings in Deep\n    Residual Networks,\' https://arxiv.org/abs/1603.05027.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_preresnet(blocks=200, conv1_stride=False, model_name=""preresnet200b"", **kwargs)\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(10):\n        model = preresnet18()\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/resnet18.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass ResNet(torchvision.models.resnet.ResNet):\n    def __init__(self, *args, **kwargs):\n        super(ResNet, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(int(x.size(0)), -1)  # << This fix again\n        x = self.fc(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/resnet18_channels_last.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass ResNet(torchvision.models.resnet.ResNet):\n    def __init__(self, *args, **kwargs):\n        super(ResNet, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(int(x.size(0)), -1)  # << This fix again\n        x = self.fc(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = ResNet(torchvision.models.resnet.BasicBlock, [2, 2, 2, 2])\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True,  change_ordering=True)\n\n        error = check_error(output, k_model, input_np.transpose(0, 2, 3, 1))\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/resnet34.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass ResNet(torchvision.models.resnet.ResNet):\n    def __init__(self, *args, **kwargs):\n        super(ResNet, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(int(x.size(0)), -1)  # << This fix again\n        x = self.fc(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-3):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = ResNet(torchvision.models.resnet.BasicBlock, [3, 4, 6, 3])\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/resnet50.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass ResNet(torchvision.models.resnet.ResNet):\n    def __init__(self, *args, **kwargs):\n        super(ResNet, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.avgpool(x)\n        x = x.view(int(x.size(0)), -1)  # << This fix again\n        x = self.fc(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-3):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = ResNet(torchvision.models.resnet.Bottleneck, [3, 4, 6, 3])\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/models/senet.py,2,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torchvision.models import ResNet\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel // reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel // reduction, channel),\n                nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x)\n        y = y.view([int(b), -1])\n        y = self.fc(y)\n        y = y.view([int(b), int(c), 1, 1])\n        return x * y\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass SEBasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes, 1)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(nn.Module):\n    expansion = 4\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=16):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride,\n                               padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes * 4, reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\ndef se_resnet18(num_classes):\n    """"""Constructs a ResNet-18 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [2, 2, 2, 2], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet34(num_classes):\n    """"""Constructs a ResNet-34 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBasicBlock, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet50(num_classes):\n    """"""Constructs a ResNet-50 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 6, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet101(num_classes):\n    """"""Constructs a ResNet-101 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 4, 23, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\ndef se_resnet152(num_classes):\n    """"""Constructs a ResNet-152 model.\n\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    """"""\n    model = ResNet(SEBottleneck, [3, 8, 36, 3], num_classes=num_classes)\n    model.avgpool = nn.AdaptiveAvgPool2d(1)\n    return model\n\n\nclass CifarSEBasicBlock(nn.Module):\n    def __init__(self, inplanes, planes, stride=1, reduction=16):\n        super(CifarSEBasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.se = SELayer(planes, reduction)\n        if inplanes != planes:\n            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),\n                                            nn.BatchNorm2d(planes))\n        else:\n            self.downsample = lambda x: x\n        self.stride = stride\n\n    def forward(self, x):\n        residual = self.downsample(x)\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.se(out)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass CifarSEResNet(nn.Module):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEResNet, self).__init__()\n        self.inplane = 16\n        self.conv1 = nn.Conv2d(3, self.inplane, kernel_size=3, stride=1, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.relu = nn.ReLU(inplace=True)\n        self.layer1 = self._make_layer(block, 16, blocks=n_size, stride=1, reduction=reduction)\n        self.layer2 = self._make_layer(block, 32, blocks=n_size, stride=2, reduction=reduction)\n        self.layer3 = self._make_layer(block, 64, blocks=n_size, stride=2, reduction=reduction)\n        self.avgpool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Linear(64, num_classes)\n        self.initialize()\n\n    def initialize(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant(m.weight, 1)\n                nn.init.constant(m.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride, reduction):\n        strides = [stride] + [1] * (blocks - 1)\n        layers = []\n        for stride in strides:\n            layers.append(block(self.inplane, planes, stride, reduction))\n            self.inplane = planes\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.avgpool(x)\n        x = x.view([int(x.size(0)), -1])\n        x = self.fc(x)\n\n        return x\n\n\nclass CifarSEPreActResNet(CifarSEResNet):\n    def __init__(self, block, n_size, num_classes=10, reduction=16):\n        super(CifarSEPreActResNet, self).__init__(block, n_size, num_classes, reduction)\n        self.bn1 = nn.BatchNorm2d(self.inplane)\n        self.initialize()\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n\n        x = self.bn1(x)\n        x = self.relu(x)\n\n        x = self.avgpool(x)\n        x = x.view([int(x.size(0)), -1])\n        x = self.fc(x)\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(10):\n        model = CifarSEResNet(CifarSEBasicBlock, 3)\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/squeezenet.py,6,"b'# flake8: noqa\n\nimport keras  # work around segfault\nimport sys\nimport numpy as np\nimport math\n\nimport torch\nimport torchvision\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nfrom pytorch2keras.converter import pytorch_to_keras\n\n# The code from torchvision\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass Fire(nn.Module):\n\n    def __init__(self, inplanes, squeeze_planes,\n                 expand1x1_planes, expand3x3_planes):\n        super(Fire, self).__init__()\n        self.inplanes = inplanes\n        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1)\n        self.squeeze_activation = nn.ReLU(inplace=True)\n        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,\n                                   kernel_size=1)\n        self.expand1x1_activation = nn.ReLU(inplace=True)\n        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,\n                                   kernel_size=3, padding=1)\n        self.expand3x3_activation = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.squeeze_activation(self.squeeze(x))\n        return torch.cat([\n            self.expand1x1_activation(self.expand1x1(x)),\n            self.expand3x3_activation(self.expand3x3(x))\n        ], 1)\n\n\nclass SqueezeNet(nn.Module):\n\n    def __init__(self, version=1.0, num_classes=1000):\n        super(SqueezeNet, self).__init__()\n        if version not in [1.0, 1.1]:\n            raise ValueError(""Unsupported SqueezeNet version {version}:""\n                             ""1.0 or 1.1 expected"".format(version=version))\n        self.num_classes = num_classes\n        if version == 1.0:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(96, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                Fire(128, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(256, 32, 128, 128),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(512, 64, 256, 256),\n            )\n        else:\n            self.features = nn.Sequential(\n                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(64, 16, 64, 64),\n                Fire(128, 16, 64, 64),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(128, 32, 128, 128),\n                Fire(256, 32, 128, 128),\n                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=False),\n                Fire(256, 48, 192, 192),\n                Fire(384, 48, 192, 192),\n                Fire(384, 64, 256, 256),\n                Fire(512, 64, 256, 256),\n            )\n        # Final convolution is initialized differently form the rest\n        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.5),\n            final_conv,\n            nn.ReLU(inplace=True),\n            nn.AvgPool2d(13, stride=1)\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                if m is final_conv:\n                    init.normal(m.weight.data, mean=0.0, std=0.01)\n                else:\n                    init.kaiming_uniform(m.weight.data)\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.classifier(x)\n        return x.view([int(x.size(0)), self.num_classes])\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(10):\n        model = SqueezeNet(version=1.1)\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/squeezenext.py,5,"b'""""""\nModel from https://github.com/osmr/imgclsmob/tree/master/pytorch/models\n""""""\n\n\nimport numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\nimport os\nimport torch.nn as nn\nimport torch.nn.init as init\n\n\nclass SqnxtConv(nn.Module):\n    """"""\n    SqueezeNext specific convolution block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    kernel_size : int or tuple/list of 2 int\n        Convolution window size.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    padding : int or tuple/list of 2 int, default (0, 0)\n        Padding value for convolution layer.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride,\n                 padding=(0, 0)):\n        super(SqnxtConv, self).__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding)\n        self.bn = nn.BatchNorm2d(num_features=out_channels)\n        self.activ = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.bn(x)\n        x = self.activ(x)\n        return x\n\n\nclass SqnxtUnit(nn.Module):\n    """"""\n    SqueezeNext unit.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    stride : int or tuple/list of 2 int\n        Strides of the convolution.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 stride):\n        super(SqnxtUnit, self).__init__()\n        if stride == 2:\n            reduction_den = 1\n            self.resize_identity = True\n        elif in_channels > out_channels:\n            reduction_den = 4\n            self.resize_identity = True\n        else:\n            reduction_den = 2\n            self.resize_identity = False\n\n        self.conv1 = SqnxtConv(\n            in_channels=in_channels,\n            out_channels=(in_channels // reduction_den),\n            kernel_size=1,\n            stride=stride)\n        self.conv2 = SqnxtConv(\n            in_channels=(in_channels // reduction_den),\n            out_channels=(in_channels // (2 * reduction_den)),\n            kernel_size=1,\n            stride=1)\n        self.conv3 = SqnxtConv(\n            in_channels=(in_channels // (2 * reduction_den)),\n            out_channels=(in_channels // reduction_den),\n            kernel_size=(1, 3),\n            stride=1,\n            padding=(0, 1))\n        self.conv4 = SqnxtConv(\n            in_channels=(in_channels // reduction_den),\n            out_channels=(in_channels // reduction_den),\n            kernel_size=(3, 1),\n            stride=1,\n            padding=(1, 0))\n        self.conv5 = SqnxtConv(\n            in_channels=(in_channels // reduction_den),\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1)\n\n        if self.resize_identity:\n            self.identity_conv = SqnxtConv(\n                in_channels=in_channels,\n                out_channels=out_channels,\n                kernel_size=1,\n                stride=stride)\n        self.activ = nn.ReLU(inplace=True)\n\n    def forward(self, x):\n        if self.resize_identity:\n            identity = self.identity_conv(x)\n        else:\n            identity = x\n        identity = self.activ(identity)\n        x = self.conv1(x)\n        x = self.conv2(x)\n        x = self.conv3(x)\n        x = self.conv4(x)\n        x = self.conv5(x)\n        x = x + identity\n        x = self.activ(x)\n        return x\n\n\nclass SqnxtInitBlock(nn.Module):\n    """"""\n    SqueezeNext specific initial block.\n\n    Parameters:\n    ----------\n    in_channels : int\n        Number of input channels.\n    out_channels : int\n        Number of output channels.\n    """"""\n    def __init__(self,\n                 in_channels,\n                 out_channels):\n        super(SqnxtInitBlock, self).__init__()\n        self.conv = SqnxtConv(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=7,\n            stride=2,\n            padding=1)\n        self.pool = nn.MaxPool2d(\n            kernel_size=3,\n            stride=2,\n            ceil_mode=True)\n\n    def forward(self, x):\n        x = self.conv(x)\n        x = self.pool(x)\n        return x\n\n\nclass SqueezeNext(nn.Module):\n    """"""\n    SqueezeNext model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    channels : list of list of int\n        Number of output channels for each unit.\n    init_block_channels : int\n        Number of output channels for the initial unit.\n    final_block_channels : int\n        Number of output channels for the final block of the feature extractor.\n    in_channels : int, default 3\n        Number of input channels.\n    num_classes : int, default 1000\n        Number of classification classes.\n    """"""\n    def __init__(self,\n                 channels,\n                 init_block_channels,\n                 final_block_channels,\n                 in_channels=3,\n                 num_classes=1000):\n        super(SqueezeNext, self).__init__()\n\n        self.features = nn.Sequential()\n        self.features.add_module(""init_block"", SqnxtInitBlock(\n            in_channels=in_channels,\n            out_channels=init_block_channels))\n        in_channels = init_block_channels\n        for i, channels_per_stage in enumerate(channels):\n            stage = nn.Sequential()\n            for j, out_channels in enumerate(channels_per_stage):\n                stride = 2 if (j == 0) and (i != 0) else 1\n                stage.add_module(""unit{}"".format(j + 1), SqnxtUnit(\n                    in_channels=in_channels,\n                    out_channels=out_channels,\n                    stride=stride))\n                in_channels = out_channels\n            self.features.add_module(""stage{}"".format(i + 1), stage)\n        self.features.add_module(\'final_block\', SqnxtConv(\n            in_channels=in_channels,\n            out_channels=final_block_channels,\n            kernel_size=1,\n            stride=1))\n        in_channels = final_block_channels\n        self.features.add_module(\'final_pool\', nn.AvgPool2d(\n            kernel_size=7,\n            stride=1))\n\n        self.output = nn.Linear(\n            in_features=in_channels,\n            out_features=num_classes)\n\n        self._init_params()\n\n    def _init_params(self):\n        for name, module in self.named_modules():\n            if isinstance(module, nn.Conv2d):\n                init.kaiming_uniform_(module.weight)\n                if module.bias is not None:\n                    init.constant_(module.bias, 0)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view(x.size(0), -1)\n        x = self.output(x)\n        return x\n\n\ndef get_squeezenext(version,\n                    width_scale,\n                    model_name=None,\n                    pretrained=False,\n                    root=os.path.join(\'~\', \'.torch\', \'models\'),\n                    **kwargs):\n    """"""\n    Create SqueezeNext model with specific parameters.\n\n    Parameters:\n    ----------\n    version : str\n        Version of SqueezeNet (\'23\' or \'23v5\').\n    width_scale : float\n        Scale factor for width of layers.\n    model_name : str or None, default None\n        Model name for loading pretrained model.\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    ctx : Context, default CPU\n        The context in which to load the pretrained weights.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n\n    init_block_channels = 64\n    final_block_channels = 128\n    channels_per_layers = [32, 64, 128, 256]\n\n    if version == \'23\':\n        layers = [6, 6, 8, 1]\n    elif version == \'23v5\':\n        layers = [2, 4, 14, 1]\n    else:\n        raise ValueError(""Unsupported SqueezeNet version {}"".format(version))\n\n    channels = [[ci] * li for (ci, li) in zip(channels_per_layers, layers)]\n\n    if width_scale != 1:\n        channels = [[int(cij * width_scale) for cij in ci] for ci in channels]\n        init_block_channels = int(init_block_channels * width_scale)\n        final_block_channels = int(final_block_channels * width_scale)\n\n    net = SqueezeNext(\n        channels=channels,\n        init_block_channels=init_block_channels,\n        final_block_channels=final_block_channels,\n        **kwargs)\n\n    if pretrained:\n        if (model_name is None) or (not model_name):\n            raise ValueError(""Parameter `model_name` should be properly initialized for loading pretrained model."")\n        import torch\n        from .model_store import get_model_file\n        net.load_state_dict(torch.load(get_model_file(\n            model_name=model_name,\n            local_model_store_dir_path=root)))\n\n    return net\n\n\ndef sqnxt23_w1(**kwargs):\n    """"""\n    1.0-SqNxt-23 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23"", width_scale=1.0, model_name=""sqnxt23_w1"", **kwargs)\n\n\ndef sqnxt23_w3d2(**kwargs):\n    """"""\n    0.75-SqNxt-23 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23"", width_scale=1.5, model_name=""sqnxt23_w3d2"", **kwargs)\n\n\ndef sqnxt23_w2(**kwargs):\n    """"""\n    0.5-SqNxt-23 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23"", width_scale=2.0, model_name=""sqnxt23_w2"", **kwargs)\n\n\ndef sqnxt23v5_w1(**kwargs):\n    """"""\n    1.0-SqNxt-23v5 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23v5"", width_scale=1.0, model_name=""sqnxt23v5_w1"", **kwargs)\n\n\ndef sqnxt23v5_w3d2(**kwargs):\n    """"""\n    0.75-SqNxt-23v5 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23v5"", width_scale=1.5, model_name=""sqnxt23v5_w3d2"", **kwargs)\n\n\ndef sqnxt23v5_w2(**kwargs):\n    """"""\n    0.5-SqNxt-23v5 model from \'SqueezeNext: Hardware-Aware Neural Network Design,\' https://arxiv.org/abs/1803.10615.\n\n    Parameters:\n    ----------\n    pretrained : bool, default False\n        Whether to load the pretrained weights for model.\n    root : str, default \'~/.torch/models\'\n        Location for keeping the model parameters.\n    """"""\n    return get_squeezenext(version=""23v5"", width_scale=2.0, model_name=""sqnxt23v5_w2"", **kwargs)\n\n\nif __name__ == \'__main__\':\n    max_error = 0\n    for i in range(10):\n        model = sqnxt23_w1()\n        for m in model.modules():\n            m.training = False\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        pytorch_output = output.data.numpy()\n        keras_output = k_model.predict(input_np)\n\n        error = np.max(pytorch_output - keras_output)\n        print(error)\n        if max_error < error:\n            max_error = error\n\n    print(\'Max error: {0}\'.format(max_error))\n'"
tests/models/vgg11.py,2,"b""import numpy as np\nimport torch\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport torchvision\n\n\nclass VGG(torchvision.models.vgg.VGG):\n    def __init__(self, *args, **kwargs):\n        super(VGG, self).__init__(*args, **kwargs)\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.view([int(x.size(0)), -1])\n        x = self.classifier(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(100):\n        model = VGG(torchvision.models.vgg.make_layers(torchvision.models.vgg.cfg['A'], batch_norm=True))\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/hard_tanh.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, min_val, max_val):\n        super(LayerTest, self).__init__()\n        self.htanh = nn.Hardtanh(min_val=min_val, max_val=max_val)\n\n    def forward(self, x):\n        x = self.htanh(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self, min_val, max_val):\n        super(FTest, self).__init__()\n        self.min_val = min_val\n        self.max_val = max_val\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.hardtanh(x, min_val=self.min_val, max_val=self.max_val)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        import random\n        vmin = random.random() - 1.0\n        vmax = vmin + 2.0 * random.random()\n        model = LayerTest(vmin, vmax)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        vmin = random.random() - 1.0\n        vmax = vmin + 2.0 * random.random()\n        model = FTest(vmin, vmax)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/lrelu.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, negative_slope):\n        super(LayerTest, self).__init__()\n        self.relu = nn.LeakyReLU(negative_slope=negative_slope)\n\n    def forward(self, x):\n        x = self.relu(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self, negative_slope):\n        super(FTest, self).__init__()\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.leaky_relu(x, self.negative_slope)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        import random\n        model = LayerTest(negative_slope=random.random() - 0.5)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest(negative_slope=random.random() - 0.5)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/relu.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        x = self.relu(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.relu(x)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/selu.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.selu = nn.SELU()\n\n    def forward(self, x):\n        x = self.selu(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.selu(x)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/sigmoid.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        x = self.sigmoid(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.sigmoid(x)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/softmax.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, dim):\n        super(LayerTest, self).__init__()\n        self.softmax = nn.Softmax(dim=dim)\n\n    def forward(self, x):\n        x = self.softmax(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self, dim):\n        super(FTest, self).__init__()\n        self.dim = dim\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.softmax(x, dim=self.dim)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(50):\n        import random\n        model = LayerTest(dim=np.random.randint(0, 3))\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(50):\n        model = FTest(dim=np.random.randint(0, 3))\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/activations/tanh.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.tanh(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.tanh(x)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/constants/constant.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        return x + torch.FloatTensor([1.0])\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/convolutions/conv1d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, inp, out, kernel_size=3, padding=1, stride=1, bias=False, dilation=1):\n        super(LayerTest, self).__init__()\n        self.conv = nn.Conv1d(inp, out, kernel_size=kernel_size, padding=padding, \\\n            stride=stride, bias=bias, dilation=dilation)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for kernel_size in [1, 3, 5]:\n        for padding in [0, 1, 3]:\n            for stride in [1, 2]:\n                for bias in [True, False]:\n                    for dilation in [1, 2, 3]:\n                        # ValueError: strides > 1 not supported in conjunction with dilation_rate > 1\n                        if stride > 1 and dilation > 1:\n                            continue\n\n                        ins = np.random.choice([1, 3, 7])\n                        model = LayerTest(ins, np.random.choice([1, 3, 7]), \\\n                            kernel_size=kernel_size, padding=padding, stride=stride, bias=bias, dilation=dilation)\n                        model.eval()\n\n                        input_np = np.random.uniform(0, 1, (1, ins, 224))\n                        input_var = Variable(torch.FloatTensor(input_np))\n                        output = model(input_var)\n                        print(output.size())\n                        k_model = pytorch_to_keras(model, input_var, (ins, 224,), verbose=True)\n\n                        error = check_error(output, k_model, input_np)\n                        if max_error < error:\n                            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/convolutions/conv2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, inp, out, kernel_size=3, padding=1, stride=1, bias=False, dilation=1, groups=1):\n        super(LayerTest, self).__init__()\n        self.conv = nn.Conv2d(inp, out, kernel_size=kernel_size, padding=padding, \\\n            stride=stride, bias=bias, dilation=dilation, groups=groups)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for kernel_size in [1, 3, 5]:\n        for padding in [0, 1, 3]:\n            for stride in [1, 2]:\n                for bias in [True, False]:\n                    for dilation in [1, 2, 3]:\n                        for groups in [1, 3]:\n                            # ValueError: strides > 1 not supported in conjunction with dilation_rate > 1\n                            if stride > 1 and dilation > 1:\n                                continue\n\n                            model = LayerTest(3, groups, \\\n                                kernel_size=kernel_size, padding=padding, stride=stride, bias=bias, dilation=dilation, groups=groups)\n                            model.eval()\n\n                            input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n                            input_var = Variable(torch.FloatTensor(input_np))\n                            output = model(input_var)\n\n                            k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n                            error = check_error(output, k_model, input_np)\n                            if max_error < error:\n                                max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/convolutions/convtranspose2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, inp, out, kernel_size=3, padding=1, stride=1, bias=False):\n        super(LayerTest, self).__init__()\n        self.conv = nn.ConvTranspose2d(inp, out, kernel_size=kernel_size, padding=padding, \\\n            stride=stride, bias=bias)\n\n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for kernel_size in [1, 3, 5]:\n        for padding in [0, 1, 3]:\n            for stride in [1, 2]:\n                for bias in [True, False]:\n                    outs = np.random.choice([1, 3, 7])\n\n                    model = LayerTest(3, outs, \\\n                        kernel_size=kernel_size, padding=padding, stride=stride, bias=bias)\n                    model.eval()\n\n                    input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n                    input_var = Variable(torch.FloatTensor(input_np))\n                    output = model(input_var)\n\n                    k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n                    error = check_error(output, k_model, input_np)\n                    if max_error < error:\n                        max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/elementwise/add.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n    def forward(self, x, y):\n        x = x + y\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np1 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_np2 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var1 = Variable(torch.FloatTensor(input_np1))\n        input_var2 = Variable(torch.FloatTensor(input_np2))\n        output = model(input_var1, input_var2)\n\n        k_model = pytorch_to_keras(model, [input_var1, input_var2], [(3, 224, 224,), (3, 224, 224,)], verbose=True)\n\n        error = check_error(output, k_model, [input_np1, input_np2])\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/elementwise/div.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n    def forward(self, x, y):\n        x = x / y\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np1 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_np2 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var1 = Variable(torch.FloatTensor(input_np1))\n        input_var2 = Variable(torch.FloatTensor(input_np2))\n        output = model(input_var1, input_var2)\n\n        k_model = pytorch_to_keras(model, [input_var1, input_var2], [(3, 224, 224,), (3, 224, 224,)], verbose=True)\n\n        error = check_error(output, k_model, [input_np1, input_np2])\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/elementwise/mul.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n    def forward(self, x, y):\n        x = x * y\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np1 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_np2 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var1 = Variable(torch.FloatTensor(input_np1))\n        input_var2 = Variable(torch.FloatTensor(input_np2))\n        output = model(input_var1, input_var2)\n\n        k_model = pytorch_to_keras(model, [input_var1, input_var2], [(3, 224, 224,), (3, 224, 224,)], verbose=True)\n\n        error = check_error(output, k_model, [input_np1, input_np2])\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/elementwise/sub.py,4,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n    def forward(self, x, y):\n        x = x - y\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np1 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_np2 = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var1 = Variable(torch.FloatTensor(input_np1))\n        input_var2 = Variable(torch.FloatTensor(input_np2))\n        output = model(input_var1, input_var2)\n\n        k_model = pytorch_to_keras(model, [input_var1, input_var2], [(3, 224, 224,), (3, 224, 224,)], verbose=True)\n\n        error = check_error(output, k_model, [input_np1, input_np2])\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/embeddings/embedding.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, input_size, embedd_size):\n        super(LayerTest, self).__init__()\n        self.embedd = nn.Embedding(input_size, embedd_size)\n\n    def forward(self, x):\n        x = self.embedd(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n\n    for i in range(10):\n        emb_size = np.random.randint(10, 1000)\n        inp_size = np.random.randint(10, 1000)\n\n        model = LayerTest(inp_size, emb_size)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 1, inp_size))\n        input_var = Variable(torch.LongTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, [(1, inp_size)], verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/linears/linear.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, inp, out, bias=False):\n        super(LayerTest, self).__init__()\n        self.fc = nn.Linear(inp, out, bias=bias)\n\n    def forward(self, x):\n        x = self.fc(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for bias in [True, False]:\n        ins = np.random.choice([1, 3, 7])\n        model = LayerTest(ins, np.random.choice([1, 3, 7]), bias=bias)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, ins))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n        print(output.size())\n        k_model = pytorch_to_keras(model, input_var, (ins,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/normalizations/bn2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport random\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, out, eps, momentum):\n        super(LayerTest, self).__init__()\n        self.bn = nn.BatchNorm2d(out, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.bn(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        inp_size = np.random.randint(10, 100)\n\n        model = LayerTest(inp_size, random.random(), random.random())\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, inp_size, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (inp_size, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/normalizations/do.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport random\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, p):\n        super(LayerTest, self).__init__()\n        self.do = nn.Dropout2d(p=p)\n\n    def forward(self, x):\n        x = x + 0  # To keep the graph\n        x = self.do(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        inp_size = np.random.randint(10, 100)\n\n        model = LayerTest(random.random())\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, inp_size, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (inp_size, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/normalizations/in2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\nimport random\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, out, eps, momentum):\n        super(LayerTest, self).__init__()\n        self.in2d = nn.InstanceNorm2d(out, eps=eps, momentum=momentum)\n\n    def forward(self, x):\n        x = self.in2d(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n        inp_size = np.random.randint(10, 100)\n\n        model = LayerTest(inp_size, random.random(), random.random())\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, inp_size, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (inp_size, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/poolings/avgpool2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self,  kernel_size=3, padding=1, stride=1):\n        super(LayerTest, self).__init__()\n        self.pool = nn.AvgPool2d(kernel_size=kernel_size, padding=padding, stride=stride)\n\n    def forward(self, x):\n        x = self.pool(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for kernel_size in [1, 3, 5, 7]:\n        for padding in [0, 1, 3]:\n            for stride in [1, 2, 3, 4]:\n                # RuntimeError: invalid argument 2: pad should be smaller than half of kernel size, but got padW = 1, padH = 1, kW = 1,\n                if padding > kernel_size / 2:\n                    continue\n\n                model = LayerTest(kernel_size=kernel_size, padding=padding, stride=stride)\n                model.eval()\n\n                input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n                input_var = Variable(torch.FloatTensor(input_np))\n                output = model(input_var)\n\n                k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n                error = check_error(output, k_model, input_np)\n                if max_error < error:\n                    max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/poolings/global_avgpool2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.pool = nn.AdaptiveAvgPool2d((1,1))\n\n    def forward(self, x):\n        x = self.pool(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/poolings/global_maxpool2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self):\n        super(LayerTest, self).__init__()\n        self.pool = nn.AdaptiveMaxPool2d((1,1))\n\n    def forward(self, x):\n        x = self.pool(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for i in range(10):\n\n        model = LayerTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/poolings/maxpool2d.py,3,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self,  kernel_size=3, padding=1, stride=1):\n        super(LayerTest, self).__init__()\n        self.pool = nn.MaxPool2d(kernel_size=kernel_size, padding=padding, stride=stride)\n\n    def forward(self, x):\n        x = self.pool(x)\n        return x\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-5):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for kernel_size in [1, 3, 5, 7]:\n        for padding in [0, 1, 3]:\n            for stride in [1, 2, 3, 4]:\n                # RuntimeError: invalid argument 2: pad should be smaller than half of kernel size, but got padW = 1, padH = 1, kW = 1,\n                if padding > kernel_size / 2:\n                    continue\n\n                model = LayerTest(kernel_size=kernel_size, padding=padding, stride=stride)\n                model.eval()\n\n                input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n                input_var = Variable(torch.FloatTensor(input_np))\n                output = model(input_var)\n\n                k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n                error = check_error(output, k_model, input_np)\n                if max_error < error:\n                    max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/upsamplings/upsampling_bilinear.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, scale_factor=2):\n        super(LayerTest, self).__init__()\n        self.up = nn.UpsamplingBilinear2d(scale_factor=scale_factor)\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.upsample_bilinear(x, scale_factor=2)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-4):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for scale_factor in [1, 2, 3, 4]:\n        model = LayerTest(scale_factor)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
tests/layers/upsamplings/upsampling_nearest.py,5,"b""import numpy as np\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom pytorch2keras.converter import pytorch_to_keras\n\n\nclass LayerTest(nn.Module):\n    def __init__(self, scale_factor=2):\n        super(LayerTest, self).__init__()\n        self.up = nn.UpsamplingNearest2d(scale_factor=scale_factor)\n\n    def forward(self, x):\n        x = self.up(x)\n        return x\n\n\nclass FTest(nn.Module):\n    def __init__(self):\n        super(FTest, self).__init__()\n\n    def forward(self, x):\n        from torch.nn import functional as F\n        return F.upsample_nearest(x, scale_factor=2)\n\n\ndef check_error(output, k_model, input_np, epsilon=1e-4):\n    pytorch_output = output.data.numpy()\n    keras_output = k_model.predict(input_np)\n\n    error = np.max(pytorch_output - keras_output)\n    print('Error:', error)\n\n    assert error < epsilon\n    return error\n\n\nif __name__ == '__main__':\n    max_error = 0\n    for scale_factor in [1, 2, 3, 4]:\n        model = LayerTest(scale_factor)\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    for i in range(10):\n        model = FTest()\n        model.eval()\n\n        input_np = np.random.uniform(0, 1, (1, 3, 224, 224))\n        input_var = Variable(torch.FloatTensor(input_np))\n        output = model(input_var)\n\n        k_model = pytorch_to_keras(model, input_var, (3, 224, 224,), verbose=True)\n\n        error = check_error(output, k_model, input_np)\n        if max_error < error:\n            max_error = error\n\n    print('Max error: {0}'.format(max_error))\n"""
