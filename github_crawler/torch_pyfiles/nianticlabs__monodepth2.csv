file_path,api_count,code
evaluate_depth.py,5,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport cv2\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom layers import disp_to_depth\nfrom utils import readlines\nfrom options import MonodepthOptions\nimport datasets\nimport networks\n\ncv2.setNumThreads(0)  # This speeds up evaluation 5x on our unix systems (OpenCV 3.3.1)\n\n\nsplits_dir = os.path.join(os.path.dirname(__file__), ""splits"")\n\n# Models which were trained with stereo supervision were trained with a nominal\n# baseline of 0.1 units. The KITTI rig has a baseline of 54cm. Therefore,\n# to convert our stereo predictions to real-world scale we multiply our depths by 5.4.\nSTEREO_SCALE_FACTOR = 5.4\n\n\ndef compute_errors(gt, pred):\n    """"""Computation of error metrics between predicted and ground truth depths\n    """"""\n    thresh = np.maximum((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).mean()\n    a2 = (thresh < 1.25 ** 2).mean()\n    a3 = (thresh < 1.25 ** 3).mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = np.sqrt(rmse.mean())\n\n    rmse_log = (np.log(gt) - np.log(pred)) ** 2\n    rmse_log = np.sqrt(rmse_log.mean())\n\n    abs_rel = np.mean(np.abs(gt - pred) / gt)\n\n    sq_rel = np.mean(((gt - pred) ** 2) / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n\n\ndef batch_post_process_disparity(l_disp, r_disp):\n    """"""Apply the disparity post-processing method as introduced in Monodepthv1\n    """"""\n    _, h, w = l_disp.shape\n    m_disp = 0.5 * (l_disp + r_disp)\n    l, _ = np.meshgrid(np.linspace(0, 1, w), np.linspace(0, 1, h))\n    l_mask = (1.0 - np.clip(20 * (l - 0.05), 0, 1))[None, ...]\n    r_mask = l_mask[:, :, ::-1]\n    return r_mask * l_disp + l_mask * r_disp + (1.0 - l_mask - r_mask) * m_disp\n\n\ndef evaluate(opt):\n    """"""Evaluates a pretrained model using a specified test set\n    """"""\n    MIN_DEPTH = 1e-3\n    MAX_DEPTH = 80\n\n    assert sum((opt.eval_mono, opt.eval_stereo)) == 1, \\\n        ""Please choose mono or stereo evaluation by setting either --eval_mono or --eval_stereo""\n\n    if opt.ext_disp_to_eval is None:\n\n        opt.load_weights_folder = os.path.expanduser(opt.load_weights_folder)\n\n        assert os.path.isdir(opt.load_weights_folder), \\\n            ""Cannot find a folder at {}"".format(opt.load_weights_folder)\n\n        print(""-> Loading weights from {}"".format(opt.load_weights_folder))\n\n        filenames = readlines(os.path.join(splits_dir, opt.eval_split, ""test_files.txt""))\n        encoder_path = os.path.join(opt.load_weights_folder, ""encoder.pth"")\n        decoder_path = os.path.join(opt.load_weights_folder, ""depth.pth"")\n\n        encoder_dict = torch.load(encoder_path)\n\n        dataset = datasets.KITTIRAWDataset(opt.data_path, filenames,\n                                           encoder_dict[\'height\'], encoder_dict[\'width\'],\n                                           [0], 4, is_train=False)\n        dataloader = DataLoader(dataset, 16, shuffle=False, num_workers=opt.num_workers,\n                                pin_memory=True, drop_last=False)\n\n        encoder = networks.ResnetEncoder(opt.num_layers, False)\n        depth_decoder = networks.DepthDecoder(encoder.num_ch_enc)\n\n        model_dict = encoder.state_dict()\n        encoder.load_state_dict({k: v for k, v in encoder_dict.items() if k in model_dict})\n        depth_decoder.load_state_dict(torch.load(decoder_path))\n\n        encoder.cuda()\n        encoder.eval()\n        depth_decoder.cuda()\n        depth_decoder.eval()\n\n        pred_disps = []\n\n        print(""-> Computing predictions with size {}x{}"".format(\n            encoder_dict[\'width\'], encoder_dict[\'height\']))\n\n        with torch.no_grad():\n            for data in dataloader:\n                input_color = data[(""color"", 0, 0)].cuda()\n\n                if opt.post_process:\n                    # Post-processed results require each image to have two forward passes\n                    input_color = torch.cat((input_color, torch.flip(input_color, [3])), 0)\n\n                output = depth_decoder(encoder(input_color))\n\n                pred_disp, _ = disp_to_depth(output[(""disp"", 0)], opt.min_depth, opt.max_depth)\n                pred_disp = pred_disp.cpu()[:, 0].numpy()\n\n                if opt.post_process:\n                    N = pred_disp.shape[0] // 2\n                    pred_disp = batch_post_process_disparity(pred_disp[:N], pred_disp[N:, :, ::-1])\n\n                pred_disps.append(pred_disp)\n\n        pred_disps = np.concatenate(pred_disps)\n\n    else:\n        # Load predictions from file\n        print(""-> Loading predictions from {}"".format(opt.ext_disp_to_eval))\n        pred_disps = np.load(opt.ext_disp_to_eval)\n\n        if opt.eval_eigen_to_benchmark:\n            eigen_to_benchmark_ids = np.load(\n                os.path.join(splits_dir, ""benchmark"", ""eigen_to_benchmark_ids.npy""))\n\n            pred_disps = pred_disps[eigen_to_benchmark_ids]\n\n    if opt.save_pred_disps:\n        output_path = os.path.join(\n            opt.load_weights_folder, ""disps_{}_split.npy"".format(opt.eval_split))\n        print(""-> Saving predicted disparities to "", output_path)\n        np.save(output_path, pred_disps)\n\n    if opt.no_eval:\n        print(""-> Evaluation disabled. Done."")\n        quit()\n\n    elif opt.eval_split == \'benchmark\':\n        save_dir = os.path.join(opt.load_weights_folder, ""benchmark_predictions"")\n        print(""-> Saving out benchmark predictions to {}"".format(save_dir))\n        if not os.path.exists(save_dir):\n            os.makedirs(save_dir)\n\n        for idx in range(len(pred_disps)):\n            disp_resized = cv2.resize(pred_disps[idx], (1216, 352))\n            depth = STEREO_SCALE_FACTOR / disp_resized\n            depth = np.clip(depth, 0, 80)\n            depth = np.uint16(depth * 256)\n            save_path = os.path.join(save_dir, ""{:010d}.png"".format(idx))\n            cv2.imwrite(save_path, depth)\n\n        print(""-> No ground truth is available for the KITTI benchmark, so not evaluating. Done."")\n        quit()\n\n    gt_path = os.path.join(splits_dir, opt.eval_split, ""gt_depths.npz"")\n    gt_depths = np.load(gt_path, fix_imports=True, encoding=\'latin1\')[""data""]\n\n    print(""-> Evaluating"")\n\n    if opt.eval_stereo:\n        print(""   Stereo evaluation - ""\n              ""disabling median scaling, scaling by {}"".format(STEREO_SCALE_FACTOR))\n        opt.disable_median_scaling = True\n        opt.pred_depth_scale_factor = STEREO_SCALE_FACTOR\n    else:\n        print(""   Mono evaluation - using median scaling"")\n\n    errors = []\n    ratios = []\n\n    for i in range(pred_disps.shape[0]):\n\n        gt_depth = gt_depths[i]\n        gt_height, gt_width = gt_depth.shape[:2]\n\n        pred_disp = pred_disps[i]\n        pred_disp = cv2.resize(pred_disp, (gt_width, gt_height))\n        pred_depth = 1 / pred_disp\n\n        if opt.eval_split == ""eigen"":\n            mask = np.logical_and(gt_depth > MIN_DEPTH, gt_depth < MAX_DEPTH)\n\n            crop = np.array([0.40810811 * gt_height, 0.99189189 * gt_height,\n                             0.03594771 * gt_width,  0.96405229 * gt_width]).astype(np.int32)\n            crop_mask = np.zeros(mask.shape)\n            crop_mask[crop[0]:crop[1], crop[2]:crop[3]] = 1\n            mask = np.logical_and(mask, crop_mask)\n\n        else:\n            mask = gt_depth > 0\n\n        pred_depth = pred_depth[mask]\n        gt_depth = gt_depth[mask]\n\n        pred_depth *= opt.pred_depth_scale_factor\n        if not opt.disable_median_scaling:\n            ratio = np.median(gt_depth) / np.median(pred_depth)\n            ratios.append(ratio)\n            pred_depth *= ratio\n\n        pred_depth[pred_depth < MIN_DEPTH] = MIN_DEPTH\n        pred_depth[pred_depth > MAX_DEPTH] = MAX_DEPTH\n\n        errors.append(compute_errors(gt_depth, pred_depth))\n\n    if not opt.disable_median_scaling:\n        ratios = np.array(ratios)\n        med = np.median(ratios)\n        print("" Scaling ratios | med: {:0.3f} | std: {:0.3f}"".format(med, np.std(ratios / med)))\n\n    mean_errors = np.array(errors).mean(0)\n\n    print(""\\n  "" + (""{:>8} | "" * 7).format(""abs_rel"", ""sq_rel"", ""rmse"", ""rmse_log"", ""a1"", ""a2"", ""a3""))\n    print((""&{: 8.3f}  "" * 7).format(*mean_errors.tolist()) + ""\\\\\\\\"")\n    print(""\\n-> Done!"")\n\n\nif __name__ == ""__main__"":\n    options = MonodepthOptions()\n    evaluate(options.parse())\n'"
evaluate_pose.py,5,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport numpy as np\n\nimport torch\nfrom torch.utils.data import DataLoader\n\nfrom layers import transformation_from_parameters\nfrom utils import readlines\nfrom options import MonodepthOptions\nfrom datasets import KITTIOdomDataset\nimport networks\n\n\n# from https://github.com/tinghuiz/SfMLearner\ndef dump_xyz(source_to_target_transformations):\n    xyzs = []\n    cam_to_world = np.eye(4)\n    xyzs.append(cam_to_world[:3, 3])\n    for source_to_target_transformation in source_to_target_transformations:\n        cam_to_world = np.dot(cam_to_world, source_to_target_transformation)\n        xyzs.append(cam_to_world[:3, 3])\n    return xyzs\n\n\n# from https://github.com/tinghuiz/SfMLearner\ndef compute_ate(gtruth_xyz, pred_xyz_o):\n\n    # Make sure that the first matched frames align (no need for rotational alignment as\n    # all the predicted/ground-truth snippets have been converted to use the same coordinate\n    # system with the first frame of the snippet being the origin).\n    offset = gtruth_xyz[0] - pred_xyz_o[0]\n    pred_xyz = pred_xyz_o + offset[None, :]\n\n    # Optimize the scaling factor\n    scale = np.sum(gtruth_xyz * pred_xyz) / np.sum(pred_xyz ** 2)\n    alignment_error = pred_xyz * scale - gtruth_xyz\n    rmse = np.sqrt(np.sum(alignment_error ** 2)) / gtruth_xyz.shape[0]\n    return rmse\n\n\ndef evaluate(opt):\n    """"""Evaluate odometry on the KITTI dataset\n    """"""\n    assert os.path.isdir(opt.load_weights_folder), \\\n        ""Cannot find a folder at {}"".format(opt.load_weights_folder)\n\n    assert opt.eval_split == ""odom_9"" or opt.eval_split == ""odom_10"", \\\n        ""eval_split should be either odom_9 or odom_10""\n\n    sequence_id = int(opt.eval_split.split(""_"")[1])\n\n    filenames = readlines(\n        os.path.join(os.path.dirname(__file__), ""splits"", ""odom"",\n                     ""test_files_{:02d}.txt"".format(sequence_id)))\n\n    dataset = KITTIOdomDataset(opt.data_path, filenames, opt.height, opt.width,\n                               [0, 1], 4, is_train=False)\n    dataloader = DataLoader(dataset, opt.batch_size, shuffle=False,\n                            num_workers=opt.num_workers, pin_memory=True, drop_last=False)\n\n    pose_encoder_path = os.path.join(opt.load_weights_folder, ""pose_encoder.pth"")\n    pose_decoder_path = os.path.join(opt.load_weights_folder, ""pose.pth"")\n\n    pose_encoder = networks.ResnetEncoder(opt.num_layers, False, 2)\n    pose_encoder.load_state_dict(torch.load(pose_encoder_path))\n\n    pose_decoder = networks.PoseDecoder(pose_encoder.num_ch_enc, 1, 2)\n    pose_decoder.load_state_dict(torch.load(pose_decoder_path))\n\n    pose_encoder.cuda()\n    pose_encoder.eval()\n    pose_decoder.cuda()\n    pose_decoder.eval()\n\n    pred_poses = []\n\n    print(""-> Computing pose predictions"")\n\n    opt.frame_ids = [0, 1]  # pose network only takes two frames as input\n\n    with torch.no_grad():\n        for inputs in dataloader:\n            for key, ipt in inputs.items():\n                inputs[key] = ipt.cuda()\n\n            all_color_aug = torch.cat([inputs[(""color_aug"", i, 0)] for i in opt.frame_ids], 1)\n\n            features = [pose_encoder(all_color_aug)]\n            axisangle, translation = pose_decoder(features)\n\n            pred_poses.append(\n                transformation_from_parameters(axisangle[:, 0], translation[:, 0]).cpu().numpy())\n\n    pred_poses = np.concatenate(pred_poses)\n\n    gt_poses_path = os.path.join(opt.data_path, ""poses"", ""{:02d}.txt"".format(sequence_id))\n    gt_global_poses = np.loadtxt(gt_poses_path).reshape(-1, 3, 4)\n    gt_global_poses = np.concatenate(\n        (gt_global_poses, np.zeros((gt_global_poses.shape[0], 1, 4))), 1)\n    gt_global_poses[:, 3, 3] = 1\n    gt_xyzs = gt_global_poses[:, :3, 3]\n\n    gt_local_poses = []\n    for i in range(1, len(gt_global_poses)):\n        gt_local_poses.append(\n            np.linalg.inv(np.dot(np.linalg.inv(gt_global_poses[i - 1]), gt_global_poses[i])))\n\n    ates = []\n    num_frames = gt_xyzs.shape[0]\n    track_length = 5\n    for i in range(0, num_frames - 1):\n        local_xyzs = np.array(dump_xyz(pred_poses[i:i + track_length - 1]))\n        gt_local_xyzs = np.array(dump_xyz(gt_local_poses[i:i + track_length - 1]))\n\n        ates.append(compute_ate(gt_local_xyzs, local_xyzs))\n\n    print(""\\n   Trajectory error: {:0.3f}, std: {:0.3f}\\n"".format(np.mean(ates), np.std(ates)))\n\n    save_path = os.path.join(opt.load_weights_folder, ""poses.npy"")\n    np.save(save_path, pred_poses)\n    print(""-> Predictions saved to"", save_path)\n\n\nif __name__ == ""__main__"":\n    options = MonodepthOptions()\n    evaluate(options.parse())\n'"
export_gt_depth.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\n\nimport argparse\nimport numpy as np\nimport PIL.Image as pil\n\nfrom utils import readlines\nfrom kitti_utils import generate_depth_map\n\n\ndef export_gt_depths_kitti():\n\n    parser = argparse.ArgumentParser(description=\'export_gt_depth\')\n\n    parser.add_argument(\'--data_path\',\n                        type=str,\n                        help=\'path to the root of the KITTI data\',\n                        required=True)\n    parser.add_argument(\'--split\',\n                        type=str,\n                        help=\'which split to export gt from\',\n                        required=True,\n                        choices=[""eigen"", ""eigen_benchmark""])\n    opt = parser.parse_args()\n\n    split_folder = os.path.join(os.path.dirname(__file__), ""splits"", opt.split)\n    lines = readlines(os.path.join(split_folder, ""test_files.txt""))\n\n    print(""Exporting ground truth depths for {}"".format(opt.split))\n\n    gt_depths = []\n    for line in lines:\n\n        folder, frame_id, _ = line.split()\n        frame_id = int(frame_id)\n\n        if opt.split == ""eigen"":\n            calib_dir = os.path.join(opt.data_path, folder.split(""/"")[0])\n            velo_filename = os.path.join(opt.data_path, folder,\n                                         ""velodyne_points/data"", ""{:010d}.bin"".format(frame_id))\n            gt_depth = generate_depth_map(calib_dir, velo_filename, 2, True)\n        elif opt.split == ""eigen_benchmark"":\n            gt_depth_path = os.path.join(opt.data_path, folder, ""proj_depth"",\n                                         ""groundtruth"", ""image_02"", ""{:010d}.png"".format(frame_id))\n            gt_depth = np.array(pil.open(gt_depth_path)).astype(np.float32) / 256\n\n        gt_depths.append(gt_depth.astype(np.float32))\n\n    output_path = os.path.join(split_folder, ""gt_depths.npz"")\n\n    print(""Saving to {}"".format(opt.split))\n\n    np.savez_compressed(output_path, data=np.array(gt_depths))\n\n\nif __name__ == ""__main__"":\n    export_gt_depths_kitti()\n'"
kitti_utils.py,0,"b'from __future__ import absolute_import, division, print_function\n\nimport os\nimport numpy as np\nfrom collections import Counter\n\n\ndef load_velodyne_points(filename):\n    """"""Load 3D point cloud from KITTI file format\n    (adapted from https://github.com/hunse/kitti)\n    """"""\n    points = np.fromfile(filename, dtype=np.float32).reshape(-1, 4)\n    points[:, 3] = 1.0  # homogeneous\n    return points\n\n\ndef read_calib_file(path):\n    """"""Read KITTI calibration file\n    (from https://github.com/hunse/kitti)\n    """"""\n    float_chars = set(""0123456789.e+- "")\n    data = {}\n    with open(path, \'r\') as f:\n        for line in f.readlines():\n            key, value = line.split(\':\', 1)\n            value = value.strip()\n            data[key] = value\n            if float_chars.issuperset(value):\n                # try to cast to float array\n                try:\n                    data[key] = np.array(list(map(float, value.split(\' \'))))\n                except ValueError:\n                    # casting error: data[key] already eq. value, so pass\n                    pass\n\n    return data\n\n\ndef sub2ind(matrixSize, rowSub, colSub):\n    """"""Convert row, col matrix subscripts to linear indices\n    """"""\n    m, n = matrixSize\n    return rowSub * (n-1) + colSub - 1\n\n\ndef generate_depth_map(calib_dir, velo_filename, cam=2, vel_depth=False):\n    """"""Generate a depth map from velodyne data\n    """"""\n    # load calibration files\n    cam2cam = read_calib_file(os.path.join(calib_dir, \'calib_cam_to_cam.txt\'))\n    velo2cam = read_calib_file(os.path.join(calib_dir, \'calib_velo_to_cam.txt\'))\n    velo2cam = np.hstack((velo2cam[\'R\'].reshape(3, 3), velo2cam[\'T\'][..., np.newaxis]))\n    velo2cam = np.vstack((velo2cam, np.array([0, 0, 0, 1.0])))\n\n    # get image shape\n    im_shape = cam2cam[""S_rect_02""][::-1].astype(np.int32)\n\n    # compute projection matrix velodyne->image plane\n    R_cam2rect = np.eye(4)\n    R_cam2rect[:3, :3] = cam2cam[\'R_rect_00\'].reshape(3, 3)\n    P_rect = cam2cam[\'P_rect_0\'+str(cam)].reshape(3, 4)\n    P_velo2im = np.dot(np.dot(P_rect, R_cam2rect), velo2cam)\n\n    # load velodyne points and remove all behind image plane (approximation)\n    # each row of the velodyne data is forward, left, up, reflectance\n    velo = load_velodyne_points(velo_filename)\n    velo = velo[velo[:, 0] >= 0, :]\n\n    # project the points to the camera\n    velo_pts_im = np.dot(P_velo2im, velo.T).T\n    velo_pts_im[:, :2] = velo_pts_im[:, :2] / velo_pts_im[:, 2][..., np.newaxis]\n\n    if vel_depth:\n        velo_pts_im[:, 2] = velo[:, 0]\n\n    # check if in bounds\n    # use minus 1 to get the exact same value as KITTI matlab code\n    velo_pts_im[:, 0] = np.round(velo_pts_im[:, 0]) - 1\n    velo_pts_im[:, 1] = np.round(velo_pts_im[:, 1]) - 1\n    val_inds = (velo_pts_im[:, 0] >= 0) & (velo_pts_im[:, 1] >= 0)\n    val_inds = val_inds & (velo_pts_im[:, 0] < im_shape[1]) & (velo_pts_im[:, 1] < im_shape[0])\n    velo_pts_im = velo_pts_im[val_inds, :]\n\n    # project to image\n    depth = np.zeros((im_shape[:2]))\n    depth[velo_pts_im[:, 1].astype(np.int), velo_pts_im[:, 0].astype(np.int)] = velo_pts_im[:, 2]\n\n    # find the duplicate points and choose the closest depth\n    inds = sub2ind(depth.shape, velo_pts_im[:, 1], velo_pts_im[:, 0])\n    dupe_inds = [item for item, count in Counter(inds).items() if count > 1]\n    for dd in dupe_inds:\n        pts = np.where(inds == dd)[0]\n        x_loc = int(velo_pts_im[pts[0], 0])\n        y_loc = int(velo_pts_im[pts[0], 1])\n        depth[y_loc, x_loc] = velo_pts_im[pts, 2].min()\n    depth[depth < 0] = 0\n\n    return depth\n'"
layers.py,39,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef disp_to_depth(disp, min_depth, max_depth):\n    """"""Convert network\'s sigmoid output into depth prediction\n    The formula for this conversion is given in the \'additional considerations\'\n    section of the paper.\n    """"""\n    min_disp = 1 / max_depth\n    max_disp = 1 / min_depth\n    scaled_disp = min_disp + (max_disp - min_disp) * disp\n    depth = 1 / scaled_disp\n    return scaled_disp, depth\n\n\ndef transformation_from_parameters(axisangle, translation, invert=False):\n    """"""Convert the network\'s (axisangle, translation) output into a 4x4 matrix\n    """"""\n    R = rot_from_axisangle(axisangle)\n    t = translation.clone()\n\n    if invert:\n        R = R.transpose(1, 2)\n        t *= -1\n\n    T = get_translation_matrix(t)\n\n    if invert:\n        M = torch.matmul(R, T)\n    else:\n        M = torch.matmul(T, R)\n\n    return M\n\n\ndef get_translation_matrix(translation_vector):\n    """"""Convert a translation vector into a 4x4 transformation matrix\n    """"""\n    T = torch.zeros(translation_vector.shape[0], 4, 4).to(device=translation_vector.device)\n\n    t = translation_vector.contiguous().view(-1, 3, 1)\n\n    T[:, 0, 0] = 1\n    T[:, 1, 1] = 1\n    T[:, 2, 2] = 1\n    T[:, 3, 3] = 1\n    T[:, :3, 3, None] = t\n\n    return T\n\n\ndef rot_from_axisangle(vec):\n    """"""Convert an axisangle rotation into a 4x4 transformation matrix\n    (adapted from https://github.com/Wallacoloo/printipi)\n    Input \'vec\' has to be Bx1x3\n    """"""\n    angle = torch.norm(vec, 2, 2, True)\n    axis = vec / (angle + 1e-7)\n\n    ca = torch.cos(angle)\n    sa = torch.sin(angle)\n    C = 1 - ca\n\n    x = axis[..., 0].unsqueeze(1)\n    y = axis[..., 1].unsqueeze(1)\n    z = axis[..., 2].unsqueeze(1)\n\n    xs = x * sa\n    ys = y * sa\n    zs = z * sa\n    xC = x * C\n    yC = y * C\n    zC = z * C\n    xyC = x * yC\n    yzC = y * zC\n    zxC = z * xC\n\n    rot = torch.zeros((vec.shape[0], 4, 4)).to(device=vec.device)\n\n    rot[:, 0, 0] = torch.squeeze(x * xC + ca)\n    rot[:, 0, 1] = torch.squeeze(xyC - zs)\n    rot[:, 0, 2] = torch.squeeze(zxC + ys)\n    rot[:, 1, 0] = torch.squeeze(xyC + zs)\n    rot[:, 1, 1] = torch.squeeze(y * yC + ca)\n    rot[:, 1, 2] = torch.squeeze(yzC - xs)\n    rot[:, 2, 0] = torch.squeeze(zxC - ys)\n    rot[:, 2, 1] = torch.squeeze(yzC + xs)\n    rot[:, 2, 2] = torch.squeeze(z * zC + ca)\n    rot[:, 3, 3] = 1\n\n    return rot\n\n\nclass ConvBlock(nn.Module):\n    """"""Layer to perform a convolution followed by ELU\n    """"""\n    def __init__(self, in_channels, out_channels):\n        super(ConvBlock, self).__init__()\n\n        self.conv = Conv3x3(in_channels, out_channels)\n        self.nonlin = nn.ELU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv(x)\n        out = self.nonlin(out)\n        return out\n\n\nclass Conv3x3(nn.Module):\n    """"""Layer to pad and convolve input\n    """"""\n    def __init__(self, in_channels, out_channels, use_refl=True):\n        super(Conv3x3, self).__init__()\n\n        if use_refl:\n            self.pad = nn.ReflectionPad2d(1)\n        else:\n            self.pad = nn.ZeroPad2d(1)\n        self.conv = nn.Conv2d(int(in_channels), int(out_channels), 3)\n\n    def forward(self, x):\n        out = self.pad(x)\n        out = self.conv(out)\n        return out\n\n\nclass BackprojectDepth(nn.Module):\n    """"""Layer to transform a depth image into a point cloud\n    """"""\n    def __init__(self, batch_size, height, width):\n        super(BackprojectDepth, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n\n        meshgrid = np.meshgrid(range(self.width), range(self.height), indexing=\'xy\')\n        self.id_coords = np.stack(meshgrid, axis=0).astype(np.float32)\n        self.id_coords = nn.Parameter(torch.from_numpy(self.id_coords),\n                                      requires_grad=False)\n\n        self.ones = nn.Parameter(torch.ones(self.batch_size, 1, self.height * self.width),\n                                 requires_grad=False)\n\n        self.pix_coords = torch.unsqueeze(torch.stack(\n            [self.id_coords[0].view(-1), self.id_coords[1].view(-1)], 0), 0)\n        self.pix_coords = self.pix_coords.repeat(batch_size, 1, 1)\n        self.pix_coords = nn.Parameter(torch.cat([self.pix_coords, self.ones], 1),\n                                       requires_grad=False)\n\n    def forward(self, depth, inv_K):\n        cam_points = torch.matmul(inv_K[:, :3, :3], self.pix_coords)\n        cam_points = depth.view(self.batch_size, 1, -1) * cam_points\n        cam_points = torch.cat([cam_points, self.ones], 1)\n\n        return cam_points\n\n\nclass Project3D(nn.Module):\n    """"""Layer which projects 3D points into a camera with intrinsics K and at position T\n    """"""\n    def __init__(self, batch_size, height, width, eps=1e-7):\n        super(Project3D, self).__init__()\n\n        self.batch_size = batch_size\n        self.height = height\n        self.width = width\n        self.eps = eps\n\n    def forward(self, points, K, T):\n        P = torch.matmul(K, T)[:, :3, :]\n\n        cam_points = torch.matmul(P, points)\n\n        pix_coords = cam_points[:, :2, :] / (cam_points[:, 2, :].unsqueeze(1) + self.eps)\n        pix_coords = pix_coords.view(self.batch_size, 2, self.height, self.width)\n        pix_coords = pix_coords.permute(0, 2, 3, 1)\n        pix_coords[..., 0] /= self.width - 1\n        pix_coords[..., 1] /= self.height - 1\n        pix_coords = (pix_coords - 0.5) * 2\n        return pix_coords\n\n\ndef upsample(x):\n    """"""Upsample input tensor by a factor of 2\n    """"""\n    return F.interpolate(x, scale_factor=2, mode=""nearest"")\n\n\ndef get_smooth_loss(disp, img):\n    """"""Computes the smoothness loss for a disparity image\n    The color image is used for edge-aware smoothness\n    """"""\n    grad_disp_x = torch.abs(disp[:, :, :, :-1] - disp[:, :, :, 1:])\n    grad_disp_y = torch.abs(disp[:, :, :-1, :] - disp[:, :, 1:, :])\n\n    grad_img_x = torch.mean(torch.abs(img[:, :, :, :-1] - img[:, :, :, 1:]), 1, keepdim=True)\n    grad_img_y = torch.mean(torch.abs(img[:, :, :-1, :] - img[:, :, 1:, :]), 1, keepdim=True)\n\n    grad_disp_x *= torch.exp(-grad_img_x)\n    grad_disp_y *= torch.exp(-grad_img_y)\n\n    return grad_disp_x.mean() + grad_disp_y.mean()\n\n\nclass SSIM(nn.Module):\n    """"""Layer to compute the SSIM loss between a pair of images\n    """"""\n    def __init__(self):\n        super(SSIM, self).__init__()\n        self.mu_x_pool   = nn.AvgPool2d(3, 1)\n        self.mu_y_pool   = nn.AvgPool2d(3, 1)\n        self.sig_x_pool  = nn.AvgPool2d(3, 1)\n        self.sig_y_pool  = nn.AvgPool2d(3, 1)\n        self.sig_xy_pool = nn.AvgPool2d(3, 1)\n\n        self.refl = nn.ReflectionPad2d(1)\n\n        self.C1 = 0.01 ** 2\n        self.C2 = 0.03 ** 2\n\n    def forward(self, x, y):\n        x = self.refl(x)\n        y = self.refl(y)\n\n        mu_x = self.mu_x_pool(x)\n        mu_y = self.mu_y_pool(y)\n\n        sigma_x  = self.sig_x_pool(x ** 2) - mu_x ** 2\n        sigma_y  = self.sig_y_pool(y ** 2) - mu_y ** 2\n        sigma_xy = self.sig_xy_pool(x * y) - mu_x * mu_y\n\n        SSIM_n = (2 * mu_x * mu_y + self.C1) * (2 * sigma_xy + self.C2)\n        SSIM_d = (mu_x ** 2 + mu_y ** 2 + self.C1) * (sigma_x + sigma_y + self.C2)\n\n        return torch.clamp((1 - SSIM_n / SSIM_d) / 2, 0, 1)\n\n\ndef compute_depth_errors(gt, pred):\n    """"""Computation of error metrics between predicted and ground truth depths\n    """"""\n    thresh = torch.max((gt / pred), (pred / gt))\n    a1 = (thresh < 1.25     ).float().mean()\n    a2 = (thresh < 1.25 ** 2).float().mean()\n    a3 = (thresh < 1.25 ** 3).float().mean()\n\n    rmse = (gt - pred) ** 2\n    rmse = torch.sqrt(rmse.mean())\n\n    rmse_log = (torch.log(gt) - torch.log(pred)) ** 2\n    rmse_log = torch.sqrt(rmse_log.mean())\n\n    abs_rel = torch.mean(torch.abs(gt - pred) / gt)\n\n    sq_rel = torch.mean((gt - pred) ** 2 / gt)\n\n    return abs_rel, sq_rel, rmse, rmse_log, a1, a2, a3\n'"
options.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport argparse\n\nfile_dir = os.path.dirname(__file__)  # the directory that options.py resides in\n\n\nclass MonodepthOptions:\n    def __init__(self):\n        self.parser = argparse.ArgumentParser(description=""Monodepthv2 options"")\n\n        # PATHS\n        self.parser.add_argument(""--data_path"",\n                                 type=str,\n                                 help=""path to the training data"",\n                                 default=os.path.join(file_dir, ""kitti_data""))\n        self.parser.add_argument(""--log_dir"",\n                                 type=str,\n                                 help=""log directory"",\n                                 default=os.path.join(os.path.expanduser(""~""), ""tmp""))\n\n        # TRAINING options\n        self.parser.add_argument(""--model_name"",\n                                 type=str,\n                                 help=""the name of the folder to save the model in"",\n                                 default=""mdp"")\n        self.parser.add_argument(""--split"",\n                                 type=str,\n                                 help=""which training split to use"",\n                                 choices=[""eigen_zhou"", ""eigen_full"", ""odom"", ""benchmark""],\n                                 default=""eigen_zhou"")\n        self.parser.add_argument(""--num_layers"",\n                                 type=int,\n                                 help=""number of resnet layers"",\n                                 default=18,\n                                 choices=[18, 34, 50, 101, 152])\n        self.parser.add_argument(""--dataset"",\n                                 type=str,\n                                 help=""dataset to train on"",\n                                 default=""kitti"",\n                                 choices=[""kitti"", ""kitti_odom"", ""kitti_depth"", ""kitti_test""])\n        self.parser.add_argument(""--png"",\n                                 help=""if set, trains from raw KITTI png files (instead of jpgs)"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--height"",\n                                 type=int,\n                                 help=""input image height"",\n                                 default=192)\n        self.parser.add_argument(""--width"",\n                                 type=int,\n                                 help=""input image width"",\n                                 default=640)\n        self.parser.add_argument(""--disparity_smoothness"",\n                                 type=float,\n                                 help=""disparity smoothness weight"",\n                                 default=1e-3)\n        self.parser.add_argument(""--scales"",\n                                 nargs=""+"",\n                                 type=int,\n                                 help=""scales used in the loss"",\n                                 default=[0, 1, 2, 3])\n        self.parser.add_argument(""--min_depth"",\n                                 type=float,\n                                 help=""minimum depth"",\n                                 default=0.1)\n        self.parser.add_argument(""--max_depth"",\n                                 type=float,\n                                 help=""maximum depth"",\n                                 default=100.0)\n        self.parser.add_argument(""--use_stereo"",\n                                 help=""if set, uses stereo pair for training"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--frame_ids"",\n                                 nargs=""+"",\n                                 type=int,\n                                 help=""frames to load"",\n                                 default=[0, -1, 1])\n\n        # OPTIMIZATION options\n        self.parser.add_argument(""--batch_size"",\n                                 type=int,\n                                 help=""batch size"",\n                                 default=12)\n        self.parser.add_argument(""--learning_rate"",\n                                 type=float,\n                                 help=""learning rate"",\n                                 default=1e-4)\n        self.parser.add_argument(""--num_epochs"",\n                                 type=int,\n                                 help=""number of epochs"",\n                                 default=20)\n        self.parser.add_argument(""--scheduler_step_size"",\n                                 type=int,\n                                 help=""step size of the scheduler"",\n                                 default=15)\n\n        # ABLATION options\n        self.parser.add_argument(""--v1_multiscale"",\n                                 help=""if set, uses monodepth v1 multiscale"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--avg_reprojection"",\n                                 help=""if set, uses average reprojection loss"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--disable_automasking"",\n                                 help=""if set, doesn\'t do auto-masking"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--predictive_mask"",\n                                 help=""if set, uses a predictive masking scheme as in Zhou et al"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--no_ssim"",\n                                 help=""if set, disables ssim in the loss"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--weights_init"",\n                                 type=str,\n                                 help=""pretrained or scratch"",\n                                 default=""pretrained"",\n                                 choices=[""pretrained"", ""scratch""])\n        self.parser.add_argument(""--pose_model_input"",\n                                 type=str,\n                                 help=""how many images the pose network gets"",\n                                 default=""pairs"",\n                                 choices=[""pairs"", ""all""])\n        self.parser.add_argument(""--pose_model_type"",\n                                 type=str,\n                                 help=""normal or shared"",\n                                 default=""separate_resnet"",\n                                 choices=[""posecnn"", ""separate_resnet"", ""shared""])\n\n        # SYSTEM options\n        self.parser.add_argument(""--no_cuda"",\n                                 help=""if set disables CUDA"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--num_workers"",\n                                 type=int,\n                                 help=""number of dataloader workers"",\n                                 default=12)\n\n        # LOADING options\n        self.parser.add_argument(""--load_weights_folder"",\n                                 type=str,\n                                 help=""name of model to load"")\n        self.parser.add_argument(""--models_to_load"",\n                                 nargs=""+"",\n                                 type=str,\n                                 help=""models to load"",\n                                 default=[""encoder"", ""depth"", ""pose_encoder"", ""pose""])\n\n        # LOGGING options\n        self.parser.add_argument(""--log_frequency"",\n                                 type=int,\n                                 help=""number of batches between each tensorboard log"",\n                                 default=250)\n        self.parser.add_argument(""--save_frequency"",\n                                 type=int,\n                                 help=""number of epochs between each save"",\n                                 default=1)\n\n        # EVALUATION options\n        self.parser.add_argument(""--eval_stereo"",\n                                 help=""if set evaluates in stereo mode"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_mono"",\n                                 help=""if set evaluates in mono mode"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--disable_median_scaling"",\n                                 help=""if set disables median scaling in evaluation"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--pred_depth_scale_factor"",\n                                 help=""if set multiplies predictions by this number"",\n                                 type=float,\n                                 default=1)\n        self.parser.add_argument(""--ext_disp_to_eval"",\n                                 type=str,\n                                 help=""optional path to a .npy disparities file to evaluate"")\n        self.parser.add_argument(""--eval_split"",\n                                 type=str,\n                                 default=""eigen"",\n                                 choices=[\n                                    ""eigen"", ""eigen_benchmark"", ""benchmark"", ""odom_9"", ""odom_10""],\n                                 help=""which split to run eval on"")\n        self.parser.add_argument(""--save_pred_disps"",\n                                 help=""if set saves predicted disparities"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--no_eval"",\n                                 help=""if set disables evaluation"",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_eigen_to_benchmark"",\n                                 help=""if set assume we are loading eigen results from npy but ""\n                                      ""we want to evaluate using the new benchmark."",\n                                 action=""store_true"")\n        self.parser.add_argument(""--eval_out_dir"",\n                                 help=""if set will output the disparities to this folder"",\n                                 type=str)\n        self.parser.add_argument(""--post_process"",\n                                 help=""if set will perform the flipping post processing ""\n                                      ""from the original monodepth paper"",\n                                 action=""store_true"")\n\n    def parse(self):\n        self.options = self.parser.parse_args()\n        return self.options\n'"
test_simple.py,7,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport sys\nimport glob\nimport argparse\nimport numpy as np\nimport PIL.Image as pil\nimport matplotlib as mpl\nimport matplotlib.cm as cm\n\nimport torch\nfrom torchvision import transforms, datasets\n\nimport networks\nfrom layers import disp_to_depth\nfrom utils import download_model_if_doesnt_exist\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        description=\'Simple testing funtion for Monodepthv2 models.\')\n\n    parser.add_argument(\'--image_path\', type=str,\n                        help=\'path to a test image or folder of images\', required=True)\n    parser.add_argument(\'--model_name\', type=str,\n                        help=\'name of a pretrained model to use\',\n                        choices=[\n                            ""mono_640x192"",\n                            ""stereo_640x192"",\n                            ""mono+stereo_640x192"",\n                            ""mono_no_pt_640x192"",\n                            ""stereo_no_pt_640x192"",\n                            ""mono+stereo_no_pt_640x192"",\n                            ""mono_1024x320"",\n                            ""stereo_1024x320"",\n                            ""mono+stereo_1024x320""])\n    parser.add_argument(\'--ext\', type=str,\n                        help=\'image extension to search for in folder\', default=""jpg"")\n    parser.add_argument(""--no_cuda"",\n                        help=\'if set, disables CUDA\',\n                        action=\'store_true\')\n\n    return parser.parse_args()\n\n\ndef test_simple(args):\n    """"""Function to predict for a single image or folder of images\n    """"""\n    assert args.model_name is not None, \\\n        ""You must specify the --model_name parameter; see README.md for an example""\n\n    if torch.cuda.is_available() and not args.no_cuda:\n        device = torch.device(""cuda"")\n    else:\n        device = torch.device(""cpu"")\n\n    download_model_if_doesnt_exist(args.model_name)\n    model_path = os.path.join(""models"", args.model_name)\n    print(""-> Loading model from "", model_path)\n    encoder_path = os.path.join(model_path, ""encoder.pth"")\n    depth_decoder_path = os.path.join(model_path, ""depth.pth"")\n\n    # LOADING PRETRAINED MODEL\n    print(""   Loading pretrained encoder"")\n    encoder = networks.ResnetEncoder(18, False)\n    loaded_dict_enc = torch.load(encoder_path, map_location=device)\n\n    # extract the height and width of image that this model was trained with\n    feed_height = loaded_dict_enc[\'height\']\n    feed_width = loaded_dict_enc[\'width\']\n    filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n    encoder.load_state_dict(filtered_dict_enc)\n    encoder.to(device)\n    encoder.eval()\n\n    print(""   Loading pretrained decoder"")\n    depth_decoder = networks.DepthDecoder(\n        num_ch_enc=encoder.num_ch_enc, scales=range(4))\n\n    loaded_dict = torch.load(depth_decoder_path, map_location=device)\n    depth_decoder.load_state_dict(loaded_dict)\n\n    depth_decoder.to(device)\n    depth_decoder.eval()\n\n    # FINDING INPUT IMAGES\n    if os.path.isfile(args.image_path):\n        # Only testing on a single image\n        paths = [args.image_path]\n        output_directory = os.path.dirname(args.image_path)\n    elif os.path.isdir(args.image_path):\n        # Searching folder for images\n        paths = glob.glob(os.path.join(args.image_path, \'*.{}\'.format(args.ext)))\n        output_directory = args.image_path\n    else:\n        raise Exception(""Can not find args.image_path: {}"".format(args.image_path))\n\n    print(""-> Predicting on {:d} test images"".format(len(paths)))\n\n    # PREDICTING ON EACH IMAGE IN TURN\n    with torch.no_grad():\n        for idx, image_path in enumerate(paths):\n\n            if image_path.endswith(""_disp.jpg""):\n                # don\'t try to predict disparity for a disparity image!\n                continue\n\n            # Load image and preprocess\n            input_image = pil.open(image_path).convert(\'RGB\')\n            original_width, original_height = input_image.size\n            input_image = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n            input_image = transforms.ToTensor()(input_image).unsqueeze(0)\n\n            # PREDICTION\n            input_image = input_image.to(device)\n            features = encoder(input_image)\n            outputs = depth_decoder(features)\n\n            disp = outputs[(""disp"", 0)]\n            disp_resized = torch.nn.functional.interpolate(\n                disp, (original_height, original_width), mode=""bilinear"", align_corners=False)\n\n            # Saving numpy file\n            output_name = os.path.splitext(os.path.basename(image_path))[0]\n            name_dest_npy = os.path.join(output_directory, ""{}_disp.npy"".format(output_name))\n            scaled_disp, _ = disp_to_depth(disp, 0.1, 100)\n            np.save(name_dest_npy, scaled_disp.cpu().numpy())\n\n            # Saving colormapped depth image\n            disp_resized_np = disp_resized.squeeze().cpu().numpy()\n            vmax = np.percentile(disp_resized_np, 95)\n            normalizer = mpl.colors.Normalize(vmin=disp_resized_np.min(), vmax=vmax)\n            mapper = cm.ScalarMappable(norm=normalizer, cmap=\'magma\')\n            colormapped_im = (mapper.to_rgba(disp_resized_np)[:, :, :3] * 255).astype(np.uint8)\n            im = pil.fromarray(colormapped_im)\n\n            name_dest_im = os.path.join(output_directory, ""{}_disp.jpeg"".format(output_name))\n            im.save(name_dest_im)\n\n            print(""   Processed {:d} of {:d} images - saved prediction to {}"".format(\n                idx + 1, len(paths), name_dest_im))\n\n    print(\'-> Done!\')\n\n\nif __name__ == \'__main__\':\n    args = parse_args()\n    test_simple(args)\n'"
train.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nfrom trainer import Trainer\nfrom options import MonodepthOptions\n\noptions = MonodepthOptions()\nopts = options.parse()\n\n\nif __name__ == ""__main__"":\n    trainer = Trainer(opts)\n    trainer.train()\n'"
trainer.py,25,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport time\n\nimport torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tensorboardX import SummaryWriter\n\nimport json\n\nfrom utils import *\nfrom kitti_utils import *\nfrom layers import *\n\nimport datasets\nimport networks\nfrom IPython import embed\n\n\nclass Trainer:\n    def __init__(self, options):\n        self.opt = options\n        self.log_path = os.path.join(self.opt.log_dir, self.opt.model_name)\n\n        # checking height and width are multiples of 32\n        assert self.opt.height % 32 == 0, ""\'height\' must be a multiple of 32""\n        assert self.opt.width % 32 == 0, ""\'width\' must be a multiple of 32""\n\n        self.models = {}\n        self.parameters_to_train = []\n\n        self.device = torch.device(""cpu"" if self.opt.no_cuda else ""cuda"")\n\n        self.num_scales = len(self.opt.scales)\n        self.num_input_frames = len(self.opt.frame_ids)\n        self.num_pose_frames = 2 if self.opt.pose_model_input == ""pairs"" else self.num_input_frames\n\n        assert self.opt.frame_ids[0] == 0, ""frame_ids must start with 0""\n\n        self.use_pose_net = not (self.opt.use_stereo and self.opt.frame_ids == [0])\n\n        if self.opt.use_stereo:\n            self.opt.frame_ids.append(""s"")\n\n        self.models[""encoder""] = networks.ResnetEncoder(\n            self.opt.num_layers, self.opt.weights_init == ""pretrained"")\n        self.models[""encoder""].to(self.device)\n        self.parameters_to_train += list(self.models[""encoder""].parameters())\n\n        self.models[""depth""] = networks.DepthDecoder(\n            self.models[""encoder""].num_ch_enc, self.opt.scales)\n        self.models[""depth""].to(self.device)\n        self.parameters_to_train += list(self.models[""depth""].parameters())\n\n        if self.use_pose_net:\n            if self.opt.pose_model_type == ""separate_resnet"":\n                self.models[""pose_encoder""] = networks.ResnetEncoder(\n                    self.opt.num_layers,\n                    self.opt.weights_init == ""pretrained"",\n                    num_input_images=self.num_pose_frames)\n\n                self.models[""pose_encoder""].to(self.device)\n                self.parameters_to_train += list(self.models[""pose_encoder""].parameters())\n\n                self.models[""pose""] = networks.PoseDecoder(\n                    self.models[""pose_encoder""].num_ch_enc,\n                    num_input_features=1,\n                    num_frames_to_predict_for=2)\n\n            elif self.opt.pose_model_type == ""shared"":\n                self.models[""pose""] = networks.PoseDecoder(\n                    self.models[""encoder""].num_ch_enc, self.num_pose_frames)\n\n            elif self.opt.pose_model_type == ""posecnn"":\n                self.models[""pose""] = networks.PoseCNN(\n                    self.num_input_frames if self.opt.pose_model_input == ""all"" else 2)\n\n            self.models[""pose""].to(self.device)\n            self.parameters_to_train += list(self.models[""pose""].parameters())\n\n        if self.opt.predictive_mask:\n            assert self.opt.disable_automasking, \\\n                ""When using predictive_mask, please disable automasking with --disable_automasking""\n\n            # Our implementation of the predictive masking baseline has the the same architecture\n            # as our depth decoder. We predict a separate mask for each source frame.\n            self.models[""predictive_mask""] = networks.DepthDecoder(\n                self.models[""encoder""].num_ch_enc, self.opt.scales,\n                num_output_channels=(len(self.opt.frame_ids) - 1))\n            self.models[""predictive_mask""].to(self.device)\n            self.parameters_to_train += list(self.models[""predictive_mask""].parameters())\n\n        self.model_optimizer = optim.Adam(self.parameters_to_train, self.opt.learning_rate)\n        self.model_lr_scheduler = optim.lr_scheduler.StepLR(\n            self.model_optimizer, self.opt.scheduler_step_size, 0.1)\n\n        if self.opt.load_weights_folder is not None:\n            self.load_model()\n\n        print(""Training model named:\\n  "", self.opt.model_name)\n        print(""Models and tensorboard events files are saved to:\\n  "", self.opt.log_dir)\n        print(""Training is using:\\n  "", self.device)\n\n        # data\n        datasets_dict = {""kitti"": datasets.KITTIRAWDataset,\n                         ""kitti_odom"": datasets.KITTIOdomDataset}\n        self.dataset = datasets_dict[self.opt.dataset]\n\n        fpath = os.path.join(os.path.dirname(__file__), ""splits"", self.opt.split, ""{}_files.txt"")\n\n        train_filenames = readlines(fpath.format(""train""))\n        val_filenames = readlines(fpath.format(""val""))\n        img_ext = \'.png\' if self.opt.png else \'.jpg\'\n\n        num_train_samples = len(train_filenames)\n        self.num_total_steps = num_train_samples // self.opt.batch_size * self.opt.num_epochs\n\n        train_dataset = self.dataset(\n            self.opt.data_path, train_filenames, self.opt.height, self.opt.width,\n            self.opt.frame_ids, 4, is_train=True, img_ext=img_ext)\n        self.train_loader = DataLoader(\n            train_dataset, self.opt.batch_size, True,\n            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n        val_dataset = self.dataset(\n            self.opt.data_path, val_filenames, self.opt.height, self.opt.width,\n            self.opt.frame_ids, 4, is_train=False, img_ext=img_ext)\n        self.val_loader = DataLoader(\n            val_dataset, self.opt.batch_size, True,\n            num_workers=self.opt.num_workers, pin_memory=True, drop_last=True)\n        self.val_iter = iter(self.val_loader)\n\n        self.writers = {}\n        for mode in [""train"", ""val""]:\n            self.writers[mode] = SummaryWriter(os.path.join(self.log_path, mode))\n\n        if not self.opt.no_ssim:\n            self.ssim = SSIM()\n            self.ssim.to(self.device)\n\n        self.backproject_depth = {}\n        self.project_3d = {}\n        for scale in self.opt.scales:\n            h = self.opt.height // (2 ** scale)\n            w = self.opt.width // (2 ** scale)\n\n            self.backproject_depth[scale] = BackprojectDepth(self.opt.batch_size, h, w)\n            self.backproject_depth[scale].to(self.device)\n\n            self.project_3d[scale] = Project3D(self.opt.batch_size, h, w)\n            self.project_3d[scale].to(self.device)\n\n        self.depth_metric_names = [\n            ""de/abs_rel"", ""de/sq_rel"", ""de/rms"", ""de/log_rms"", ""da/a1"", ""da/a2"", ""da/a3""]\n\n        print(""Using split:\\n  "", self.opt.split)\n        print(""There are {:d} training items and {:d} validation items\\n"".format(\n            len(train_dataset), len(val_dataset)))\n\n        self.save_opts()\n\n    def set_train(self):\n        """"""Convert all models to training mode\n        """"""\n        for m in self.models.values():\n            m.train()\n\n    def set_eval(self):\n        """"""Convert all models to testing/evaluation mode\n        """"""\n        for m in self.models.values():\n            m.eval()\n\n    def train(self):\n        """"""Run the entire training pipeline\n        """"""\n        self.epoch = 0\n        self.step = 0\n        self.start_time = time.time()\n        for self.epoch in range(self.opt.num_epochs):\n            self.run_epoch()\n            if (self.epoch + 1) % self.opt.save_frequency == 0:\n                self.save_model()\n\n    def run_epoch(self):\n        """"""Run a single epoch of training and validation\n        """"""\n        self.model_lr_scheduler.step()\n\n        print(""Training"")\n        self.set_train()\n\n        for batch_idx, inputs in enumerate(self.train_loader):\n\n            before_op_time = time.time()\n\n            outputs, losses = self.process_batch(inputs)\n\n            self.model_optimizer.zero_grad()\n            losses[""loss""].backward()\n            self.model_optimizer.step()\n\n            duration = time.time() - before_op_time\n\n            # log less frequently after the first 2000 steps to save time & disk space\n            early_phase = batch_idx % self.opt.log_frequency == 0 and self.step < 2000\n            late_phase = self.step % 2000 == 0\n\n            if early_phase or late_phase:\n                self.log_time(batch_idx, duration, losses[""loss""].cpu().data)\n\n                if ""depth_gt"" in inputs:\n                    self.compute_depth_losses(inputs, outputs, losses)\n\n                self.log(""train"", inputs, outputs, losses)\n                self.val()\n\n            self.step += 1\n\n    def process_batch(self, inputs):\n        """"""Pass a minibatch through the network and generate images and losses\n        """"""\n        for key, ipt in inputs.items():\n            inputs[key] = ipt.to(self.device)\n\n        if self.opt.pose_model_type == ""shared"":\n            # If we are using a shared encoder for both depth and pose (as advocated\n            # in monodepthv1), then all images are fed separately through the depth encoder.\n            all_color_aug = torch.cat([inputs[(""color_aug"", i, 0)] for i in self.opt.frame_ids])\n            all_features = self.models[""encoder""](all_color_aug)\n            all_features = [torch.split(f, self.opt.batch_size) for f in all_features]\n\n            features = {}\n            for i, k in enumerate(self.opt.frame_ids):\n                features[k] = [f[i] for f in all_features]\n\n            outputs = self.models[""depth""](features[0])\n        else:\n            # Otherwise, we only feed the image with frame_id 0 through the depth encoder\n            features = self.models[""encoder""](inputs[""color_aug"", 0, 0])\n            outputs = self.models[""depth""](features)\n\n        if self.opt.predictive_mask:\n            outputs[""predictive_mask""] = self.models[""predictive_mask""](features)\n\n        if self.use_pose_net:\n            outputs.update(self.predict_poses(inputs, features))\n\n        self.generate_images_pred(inputs, outputs)\n        losses = self.compute_losses(inputs, outputs)\n\n        return outputs, losses\n\n    def predict_poses(self, inputs, features):\n        """"""Predict poses between input frames for monocular sequences.\n        """"""\n        outputs = {}\n        if self.num_pose_frames == 2:\n            # In this setting, we compute the pose to each source frame via a\n            # separate forward pass through the pose network.\n\n            # select what features the pose network takes as input\n            if self.opt.pose_model_type == ""shared"":\n                pose_feats = {f_i: features[f_i] for f_i in self.opt.frame_ids}\n            else:\n                pose_feats = {f_i: inputs[""color_aug"", f_i, 0] for f_i in self.opt.frame_ids}\n\n            for f_i in self.opt.frame_ids[1:]:\n                if f_i != ""s"":\n                    # To maintain ordering we always pass frames in temporal order\n                    if f_i < 0:\n                        pose_inputs = [pose_feats[f_i], pose_feats[0]]\n                    else:\n                        pose_inputs = [pose_feats[0], pose_feats[f_i]]\n\n                    if self.opt.pose_model_type == ""separate_resnet"":\n                        pose_inputs = [self.models[""pose_encoder""](torch.cat(pose_inputs, 1))]\n                    elif self.opt.pose_model_type == ""posecnn"":\n                        pose_inputs = torch.cat(pose_inputs, 1)\n\n                    axisangle, translation = self.models[""pose""](pose_inputs)\n                    outputs[(""axisangle"", 0, f_i)] = axisangle\n                    outputs[(""translation"", 0, f_i)] = translation\n\n                    # Invert the matrix if the frame id is negative\n                    outputs[(""cam_T_cam"", 0, f_i)] = transformation_from_parameters(\n                        axisangle[:, 0], translation[:, 0], invert=(f_i < 0))\n\n        else:\n            # Here we input all frames to the pose net (and predict all poses) together\n            if self.opt.pose_model_type in [""separate_resnet"", ""posecnn""]:\n                pose_inputs = torch.cat(\n                    [inputs[(""color_aug"", i, 0)] for i in self.opt.frame_ids if i != ""s""], 1)\n\n                if self.opt.pose_model_type == ""separate_resnet"":\n                    pose_inputs = [self.models[""pose_encoder""](pose_inputs)]\n\n            elif self.opt.pose_model_type == ""shared"":\n                pose_inputs = [features[i] for i in self.opt.frame_ids if i != ""s""]\n\n            axisangle, translation = self.models[""pose""](pose_inputs)\n\n            for i, f_i in enumerate(self.opt.frame_ids[1:]):\n                if f_i != ""s"":\n                    outputs[(""axisangle"", 0, f_i)] = axisangle\n                    outputs[(""translation"", 0, f_i)] = translation\n                    outputs[(""cam_T_cam"", 0, f_i)] = transformation_from_parameters(\n                        axisangle[:, i], translation[:, i])\n\n        return outputs\n\n    def val(self):\n        """"""Validate the model on a single minibatch\n        """"""\n        self.set_eval()\n        try:\n            inputs = self.val_iter.next()\n        except StopIteration:\n            self.val_iter = iter(self.val_loader)\n            inputs = self.val_iter.next()\n\n        with torch.no_grad():\n            outputs, losses = self.process_batch(inputs)\n\n            if ""depth_gt"" in inputs:\n                self.compute_depth_losses(inputs, outputs, losses)\n\n            self.log(""val"", inputs, outputs, losses)\n            del inputs, outputs, losses\n\n        self.set_train()\n\n    def generate_images_pred(self, inputs, outputs):\n        """"""Generate the warped (reprojected) color images for a minibatch.\n        Generated images are saved into the `outputs` dictionary.\n        """"""\n        for scale in self.opt.scales:\n            disp = outputs[(""disp"", scale)]\n            if self.opt.v1_multiscale:\n                source_scale = scale\n            else:\n                disp = F.interpolate(\n                    disp, [self.opt.height, self.opt.width], mode=""bilinear"", align_corners=False)\n                source_scale = 0\n\n            _, depth = disp_to_depth(disp, self.opt.min_depth, self.opt.max_depth)\n\n            outputs[(""depth"", 0, scale)] = depth\n\n            for i, frame_id in enumerate(self.opt.frame_ids[1:]):\n\n                if frame_id == ""s"":\n                    T = inputs[""stereo_T""]\n                else:\n                    T = outputs[(""cam_T_cam"", 0, frame_id)]\n\n                # from the authors of https://arxiv.org/abs/1712.00175\n                if self.opt.pose_model_type == ""posecnn"":\n\n                    axisangle = outputs[(""axisangle"", 0, frame_id)]\n                    translation = outputs[(""translation"", 0, frame_id)]\n\n                    inv_depth = 1 / depth\n                    mean_inv_depth = inv_depth.mean(3, True).mean(2, True)\n\n                    T = transformation_from_parameters(\n                        axisangle[:, 0], translation[:, 0] * mean_inv_depth[:, 0], frame_id < 0)\n\n                cam_points = self.backproject_depth[source_scale](\n                    depth, inputs[(""inv_K"", source_scale)])\n                pix_coords = self.project_3d[source_scale](\n                    cam_points, inputs[(""K"", source_scale)], T)\n\n                outputs[(""sample"", frame_id, scale)] = pix_coords\n\n                outputs[(""color"", frame_id, scale)] = F.grid_sample(\n                    inputs[(""color"", frame_id, source_scale)],\n                    outputs[(""sample"", frame_id, scale)],\n                    padding_mode=""border"")\n\n                if not self.opt.disable_automasking:\n                    outputs[(""color_identity"", frame_id, scale)] = \\\n                        inputs[(""color"", frame_id, source_scale)]\n\n    def compute_reprojection_loss(self, pred, target):\n        """"""Computes reprojection loss between a batch of predicted and target images\n        """"""\n        abs_diff = torch.abs(target - pred)\n        l1_loss = abs_diff.mean(1, True)\n\n        if self.opt.no_ssim:\n            reprojection_loss = l1_loss\n        else:\n            ssim_loss = self.ssim(pred, target).mean(1, True)\n            reprojection_loss = 0.85 * ssim_loss + 0.15 * l1_loss\n\n        return reprojection_loss\n\n    def compute_losses(self, inputs, outputs):\n        """"""Compute the reprojection and smoothness losses for a minibatch\n        """"""\n        losses = {}\n        total_loss = 0\n\n        for scale in self.opt.scales:\n            loss = 0\n            reprojection_losses = []\n\n            if self.opt.v1_multiscale:\n                source_scale = scale\n            else:\n                source_scale = 0\n\n            disp = outputs[(""disp"", scale)]\n            color = inputs[(""color"", 0, scale)]\n            target = inputs[(""color"", 0, source_scale)]\n\n            for frame_id in self.opt.frame_ids[1:]:\n                pred = outputs[(""color"", frame_id, scale)]\n                reprojection_losses.append(self.compute_reprojection_loss(pred, target))\n\n            reprojection_losses = torch.cat(reprojection_losses, 1)\n\n            if not self.opt.disable_automasking:\n                identity_reprojection_losses = []\n                for frame_id in self.opt.frame_ids[1:]:\n                    pred = inputs[(""color"", frame_id, source_scale)]\n                    identity_reprojection_losses.append(\n                        self.compute_reprojection_loss(pred, target))\n\n                identity_reprojection_losses = torch.cat(identity_reprojection_losses, 1)\n\n                if self.opt.avg_reprojection:\n                    identity_reprojection_loss = identity_reprojection_losses.mean(1, keepdim=True)\n                else:\n                    # save both images, and do min all at once below\n                    identity_reprojection_loss = identity_reprojection_losses\n\n            elif self.opt.predictive_mask:\n                # use the predicted mask\n                mask = outputs[""predictive_mask""][""disp"", scale]\n                if not self.opt.v1_multiscale:\n                    mask = F.interpolate(\n                        mask, [self.opt.height, self.opt.width],\n                        mode=""bilinear"", align_corners=False)\n\n                reprojection_losses *= mask\n\n                # add a loss pushing mask to 1 (using nn.BCELoss for stability)\n                weighting_loss = 0.2 * nn.BCELoss()(mask, torch.ones(mask.shape).cuda())\n                loss += weighting_loss.mean()\n\n            if self.opt.avg_reprojection:\n                reprojection_loss = reprojection_losses.mean(1, keepdim=True)\n            else:\n                reprojection_loss = reprojection_losses\n\n            if not self.opt.disable_automasking:\n                # add random numbers to break ties\n                identity_reprojection_loss += torch.randn(\n                    identity_reprojection_loss.shape).cuda() * 0.00001\n\n                combined = torch.cat((identity_reprojection_loss, reprojection_loss), dim=1)\n            else:\n                combined = reprojection_loss\n\n            if combined.shape[1] == 1:\n                to_optimise = combined\n            else:\n                to_optimise, idxs = torch.min(combined, dim=1)\n\n            if not self.opt.disable_automasking:\n                outputs[""identity_selection/{}"".format(scale)] = (\n                    idxs > identity_reprojection_loss.shape[1] - 1).float()\n\n            loss += to_optimise.mean()\n\n            mean_disp = disp.mean(2, True).mean(3, True)\n            norm_disp = disp / (mean_disp + 1e-7)\n            smooth_loss = get_smooth_loss(norm_disp, color)\n\n            loss += self.opt.disparity_smoothness * smooth_loss / (2 ** scale)\n            total_loss += loss\n            losses[""loss/{}"".format(scale)] = loss\n\n        total_loss /= self.num_scales\n        losses[""loss""] = total_loss\n        return losses\n\n    def compute_depth_losses(self, inputs, outputs, losses):\n        """"""Compute depth metrics, to allow monitoring during training\n\n        This isn\'t particularly accurate as it averages over the entire batch,\n        so is only used to give an indication of validation performance\n        """"""\n        depth_pred = outputs[(""depth"", 0, 0)]\n        depth_pred = torch.clamp(F.interpolate(\n            depth_pred, [375, 1242], mode=""bilinear"", align_corners=False), 1e-3, 80)\n        depth_pred = depth_pred.detach()\n\n        depth_gt = inputs[""depth_gt""]\n        mask = depth_gt > 0\n\n        # garg/eigen crop\n        crop_mask = torch.zeros_like(mask)\n        crop_mask[:, :, 153:371, 44:1197] = 1\n        mask = mask * crop_mask\n\n        depth_gt = depth_gt[mask]\n        depth_pred = depth_pred[mask]\n        depth_pred *= torch.median(depth_gt) / torch.median(depth_pred)\n\n        depth_pred = torch.clamp(depth_pred, min=1e-3, max=80)\n\n        depth_errors = compute_depth_errors(depth_gt, depth_pred)\n\n        for i, metric in enumerate(self.depth_metric_names):\n            losses[metric] = np.array(depth_errors[i].cpu())\n\n    def log_time(self, batch_idx, duration, loss):\n        """"""Print a logging statement to the terminal\n        """"""\n        samples_per_sec = self.opt.batch_size / duration\n        time_sofar = time.time() - self.start_time\n        training_time_left = (\n            self.num_total_steps / self.step - 1.0) * time_sofar if self.step > 0 else 0\n        print_string = ""epoch {:>3} | batch {:>6} | examples/s: {:5.1f}"" + \\\n            "" | loss: {:.5f} | time elapsed: {} | time left: {}""\n        print(print_string.format(self.epoch, batch_idx, samples_per_sec, loss,\n                                  sec_to_hm_str(time_sofar), sec_to_hm_str(training_time_left)))\n\n    def log(self, mode, inputs, outputs, losses):\n        """"""Write an event to the tensorboard events file\n        """"""\n        writer = self.writers[mode]\n        for l, v in losses.items():\n            writer.add_scalar(""{}"".format(l), v, self.step)\n\n        for j in range(min(4, self.opt.batch_size)):  # write a maxmimum of four images\n            for s in self.opt.scales:\n                for frame_id in self.opt.frame_ids:\n                    writer.add_image(\n                        ""color_{}_{}/{}"".format(frame_id, s, j),\n                        inputs[(""color"", frame_id, s)][j].data, self.step)\n                    if s == 0 and frame_id != 0:\n                        writer.add_image(\n                            ""color_pred_{}_{}/{}"".format(frame_id, s, j),\n                            outputs[(""color"", frame_id, s)][j].data, self.step)\n\n                writer.add_image(\n                    ""disp_{}/{}"".format(s, j),\n                    normalize_image(outputs[(""disp"", s)][j]), self.step)\n\n                if self.opt.predictive_mask:\n                    for f_idx, frame_id in enumerate(self.opt.frame_ids[1:]):\n                        writer.add_image(\n                            ""predictive_mask_{}_{}/{}"".format(frame_id, s, j),\n                            outputs[""predictive_mask""][(""disp"", s)][j, f_idx][None, ...],\n                            self.step)\n\n                elif not self.opt.disable_automasking:\n                    writer.add_image(\n                        ""automask_{}/{}"".format(s, j),\n                        outputs[""identity_selection/{}"".format(s)][j][None, ...], self.step)\n\n    def save_opts(self):\n        """"""Save options to disk so we know what we ran this experiment with\n        """"""\n        models_dir = os.path.join(self.log_path, ""models"")\n        if not os.path.exists(models_dir):\n            os.makedirs(models_dir)\n        to_save = self.opt.__dict__.copy()\n\n        with open(os.path.join(models_dir, \'opt.json\'), \'w\') as f:\n            json.dump(to_save, f, indent=2)\n\n    def save_model(self):\n        """"""Save model weights to disk\n        """"""\n        save_folder = os.path.join(self.log_path, ""models"", ""weights_{}"".format(self.epoch))\n        if not os.path.exists(save_folder):\n            os.makedirs(save_folder)\n\n        for model_name, model in self.models.items():\n            save_path = os.path.join(save_folder, ""{}.pth"".format(model_name))\n            to_save = model.state_dict()\n            if model_name == \'encoder\':\n                # save the sizes - these are needed at prediction time\n                to_save[\'height\'] = self.opt.height\n                to_save[\'width\'] = self.opt.width\n                to_save[\'use_stereo\'] = self.opt.use_stereo\n            torch.save(to_save, save_path)\n\n        save_path = os.path.join(save_folder, ""{}.pth"".format(""adam""))\n        torch.save(self.model_optimizer.state_dict(), save_path)\n\n    def load_model(self):\n        """"""Load model(s) from disk\n        """"""\n        self.opt.load_weights_folder = os.path.expanduser(self.opt.load_weights_folder)\n\n        assert os.path.isdir(self.opt.load_weights_folder), \\\n            ""Cannot find folder {}"".format(self.opt.load_weights_folder)\n        print(""loading model from folder {}"".format(self.opt.load_weights_folder))\n\n        for n in self.opt.models_to_load:\n            print(""Loading {} weights..."".format(n))\n            path = os.path.join(self.opt.load_weights_folder, ""{}.pth"".format(n))\n            model_dict = self.models[n].state_dict()\n            pretrained_dict = torch.load(path)\n            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n            model_dict.update(pretrained_dict)\n            self.models[n].load_state_dict(model_dict)\n\n        # loading adam state\n        optimizer_load_path = os.path.join(self.opt.load_weights_folder, ""adam.pth"")\n        if os.path.isfile(optimizer_load_path):\n            print(""Loading Adam weights"")\n            optimizer_dict = torch.load(optimizer_load_path)\n            self.model_optimizer.load_state_dict(optimizer_dict)\n        else:\n            print(""Cannot find Adam weights so Adam is randomly initialized"")\n'"
utils.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\nimport os\nimport hashlib\nimport zipfile\nfrom six.moves import urllib\n\n\ndef readlines(filename):\n    """"""Read all the lines in a text file and return as a list\n    """"""\n    with open(filename, \'r\') as f:\n        lines = f.read().splitlines()\n    return lines\n\n\ndef normalize_image(x):\n    """"""Rescale image pixels to span range [0, 1]\n    """"""\n    ma = float(x.max().cpu().data)\n    mi = float(x.min().cpu().data)\n    d = ma - mi if ma != mi else 1e5\n    return (x - mi) / d\n\n\ndef sec_to_hm(t):\n    """"""Convert time in seconds to time in hours, minutes and seconds\n    e.g. 10239 -> (2, 50, 39)\n    """"""\n    t = int(t)\n    s = t % 60\n    t //= 60\n    m = t % 60\n    t //= 60\n    return t, m, s\n\n\ndef sec_to_hm_str(t):\n    """"""Convert time in seconds to a nice string\n    e.g. 10239 -> \'02h50m39s\'\n    """"""\n    h, m, s = sec_to_hm(t)\n    return ""{:02d}h{:02d}m{:02d}s"".format(h, m, s)\n\n\ndef download_model_if_doesnt_exist(model_name):\n    """"""If pretrained kitti model doesn\'t exist, download and unzip it\n    """"""\n    # values are tuples of (<google cloud URL>, <md5 checksum>)\n    download_paths = {\n        ""mono_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip"",\n             ""a964b8356e08a02d009609d9e3928f7c""),\n        ""stereo_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip"",\n             ""3dfb76bcff0786e4ec07ac00f658dd07""),\n        ""mono+stereo_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip"",\n             ""c024d69012485ed05d7eaa9617a96b81""),\n        ""mono_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip"",\n             ""9c2f071e35027c895a4728358ffc913a""),\n        ""stereo_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip"",\n             ""41ec2de112905f85541ac33a854742d1""),\n        ""mono+stereo_no_pt_640x192"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip"",\n             ""46c3b824f541d143a45c37df65fbab0a""),\n        ""mono_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip"",\n             ""0ab0766efdfeea89a0d9ea8ba90e1e63""),\n        ""stereo_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip"",\n             ""afc2f2126d70cf3fdf26b550898b501a""),\n        ""mono+stereo_1024x320"":\n            (""https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip"",\n             ""cdc5fc9b23513c07d5b19235d9ef08f7""),\n        }\n\n    if not os.path.exists(""models""):\n        os.makedirs(""models"")\n\n    model_path = os.path.join(""models"", model_name)\n\n    def check_file_matches_md5(checksum, fpath):\n        if not os.path.exists(fpath):\n            return False\n        with open(fpath, \'rb\') as f:\n            current_md5checksum = hashlib.md5(f.read()).hexdigest()\n        return current_md5checksum == checksum\n\n    # see if we have the model already downloaded...\n    if not os.path.exists(os.path.join(model_path, ""encoder.pth"")):\n\n        model_url, required_md5checksum = download_paths[model_name]\n\n        if not check_file_matches_md5(required_md5checksum, model_path + "".zip""):\n            print(""-> Downloading pretrained model to {}"".format(model_path + "".zip""))\n            urllib.request.urlretrieve(model_url, model_path + "".zip"")\n\n        if not check_file_matches_md5(required_md5checksum, model_path + "".zip""):\n            print(""   Failed to download a file which matches the checksum - quitting"")\n            quit()\n\n        print(""   Unzipping model..."")\n        with zipfile.ZipFile(model_path + "".zip"", \'r\') as f:\n            f.extractall(model_path)\n\n        print(""   Model unzipped to {}"".format(model_path))\n'"
datasets/__init__.py,0,"b'from .kitti_dataset import KITTIRAWDataset, KITTIOdomDataset, KITTIDepthDataset\n'"
datasets/kitti_dataset.py,0,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport skimage.transform\nimport numpy as np\nimport PIL.Image as pil\n\nfrom kitti_utils import generate_depth_map\nfrom .mono_dataset import MonoDataset\n\n\nclass KITTIDataset(MonoDataset):\n    """"""Superclass for different types of KITTI dataset loaders\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(KITTIDataset, self).__init__(*args, **kwargs)\n\n        self.K = np.array([[0.58, 0, 0.5, 0],\n                           [0, 1.92, 0.5, 0],\n                           [0, 0, 1, 0],\n                           [0, 0, 0, 1]], dtype=np.float32)\n\n        self.full_res_shape = (1242, 375)\n        self.side_map = {""2"": 2, ""3"": 3, ""l"": 2, ""r"": 3}\n\n    def check_depth(self):\n        line = self.filenames[0].split()\n        scene_name = line[0]\n        frame_index = int(line[1])\n\n        velo_filename = os.path.join(\n            self.data_path,\n            scene_name,\n            ""velodyne_points/data/{:010d}.bin"".format(int(frame_index)))\n\n        return os.path.isfile(velo_filename)\n\n    def get_color(self, folder, frame_index, side, do_flip):\n        color = self.loader(self.get_image_path(folder, frame_index, side))\n\n        if do_flip:\n            color = color.transpose(pil.FLIP_LEFT_RIGHT)\n\n        return color\n\n\nclass KITTIRAWDataset(KITTIDataset):\n    """"""KITTI dataset which loads the original velodyne depth maps for ground truth\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(KITTIRAWDataset, self).__init__(*args, **kwargs)\n\n    def get_image_path(self, folder, frame_index, side):\n        f_str = ""{:010d}{}"".format(frame_index, self.img_ext)\n        image_path = os.path.join(\n            self.data_path, folder, ""image_0{}/data"".format(self.side_map[side]), f_str)\n        return image_path\n\n    def get_depth(self, folder, frame_index, side, do_flip):\n        calib_path = os.path.join(self.data_path, folder.split(""/"")[0])\n\n        velo_filename = os.path.join(\n            self.data_path,\n            folder,\n            ""velodyne_points/data/{:010d}.bin"".format(int(frame_index)))\n\n        depth_gt = generate_depth_map(calib_path, velo_filename, self.side_map[side])\n        depth_gt = skimage.transform.resize(\n            depth_gt, self.full_res_shape[::-1], order=0, preserve_range=True, mode=\'constant\')\n\n        if do_flip:\n            depth_gt = np.fliplr(depth_gt)\n\n        return depth_gt\n\n\nclass KITTIOdomDataset(KITTIDataset):\n    """"""KITTI dataset for odometry training and testing\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(KITTIOdomDataset, self).__init__(*args, **kwargs)\n\n    def get_image_path(self, folder, frame_index, side):\n        f_str = ""{:06d}{}"".format(frame_index, self.img_ext)\n        image_path = os.path.join(\n            self.data_path,\n            ""sequences/{:02d}"".format(int(folder)),\n            ""image_{}"".format(self.side_map[side]),\n            f_str)\n        return image_path\n\n\nclass KITTIDepthDataset(KITTIDataset):\n    """"""KITTI dataset which uses the updated ground truth depth maps\n    """"""\n    def __init__(self, *args, **kwargs):\n        super(KITTIDepthDataset, self).__init__(*args, **kwargs)\n\n    def get_image_path(self, folder, frame_index, side):\n        f_str = ""{:010d}{}"".format(frame_index, self.img_ext)\n        image_path = os.path.join(\n            self.data_path,\n            folder,\n            ""image_0{}/data"".format(self.side_map[side]),\n            f_str)\n        return image_path\n\n    def get_depth(self, folder, frame_index, side, do_flip):\n        f_str = ""{:010d}.png"".format(frame_index)\n        depth_path = os.path.join(\n            self.data_path,\n            folder,\n            ""proj_depth/groundtruth/image_0{}"".format(self.side_map[side]),\n            f_str)\n\n        depth_gt = pil.open(depth_path)\n        depth_gt = depth_gt.resize(self.full_res_shape, pil.NEAREST)\n        depth_gt = np.array(depth_gt).astype(np.float32) / 256\n\n        if do_flip:\n            depth_gt = np.fliplr(depth_gt)\n\n        return depth_gt\n'"
datasets/mono_dataset.py,5,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport os\nimport random\nimport numpy as np\nimport copy\nfrom PIL import Image  # using pillow-simd for increased speed\n\nimport torch\nimport torch.utils.data as data\nfrom torchvision import transforms\n\n\ndef pil_loader(path):\n    # open path as file to avoid ResourceWarning\n    # (https://github.com/python-pillow/Pillow/issues/835)\n    with open(path, \'rb\') as f:\n        with Image.open(f) as img:\n            return img.convert(\'RGB\')\n\n\nclass MonoDataset(data.Dataset):\n    """"""Superclass for monocular dataloaders\n\n    Args:\n        data_path\n        filenames\n        height\n        width\n        frame_idxs\n        num_scales\n        is_train\n        img_ext\n    """"""\n    def __init__(self,\n                 data_path,\n                 filenames,\n                 height,\n                 width,\n                 frame_idxs,\n                 num_scales,\n                 is_train=False,\n                 img_ext=\'.jpg\'):\n        super(MonoDataset, self).__init__()\n\n        self.data_path = data_path\n        self.filenames = filenames\n        self.height = height\n        self.width = width\n        self.num_scales = num_scales\n        self.interp = Image.ANTIALIAS\n\n        self.frame_idxs = frame_idxs\n\n        self.is_train = is_train\n        self.img_ext = img_ext\n\n        self.loader = pil_loader\n        self.to_tensor = transforms.ToTensor()\n\n        # We need to specify augmentations differently in newer versions of torchvision.\n        # We first try the newer tuple version; if this fails we fall back to scalars\n        try:\n            self.brightness = (0.8, 1.2)\n            self.contrast = (0.8, 1.2)\n            self.saturation = (0.8, 1.2)\n            self.hue = (-0.1, 0.1)\n            transforms.ColorJitter.get_params(\n                self.brightness, self.contrast, self.saturation, self.hue)\n        except TypeError:\n            self.brightness = 0.2\n            self.contrast = 0.2\n            self.saturation = 0.2\n            self.hue = 0.1\n\n        self.resize = {}\n        for i in range(self.num_scales):\n            s = 2 ** i\n            self.resize[i] = transforms.Resize((self.height // s, self.width // s),\n                                               interpolation=self.interp)\n\n        self.load_depth = self.check_depth()\n\n    def preprocess(self, inputs, color_aug):\n        """"""Resize colour images to the required scales and augment if required\n\n        We create the color_aug object in advance and apply the same augmentation to all\n        images in this item. This ensures that all images input to the pose network receive the\n        same augmentation.\n        """"""\n        for k in list(inputs):\n            frame = inputs[k]\n            if ""color"" in k:\n                n, im, i = k\n                for i in range(self.num_scales):\n                    inputs[(n, im, i)] = self.resize[i](inputs[(n, im, i - 1)])\n\n        for k in list(inputs):\n            f = inputs[k]\n            if ""color"" in k:\n                n, im, i = k\n                inputs[(n, im, i)] = self.to_tensor(f)\n                inputs[(n + ""_aug"", im, i)] = self.to_tensor(color_aug(f))\n\n    def __len__(self):\n        return len(self.filenames)\n\n    def __getitem__(self, index):\n        """"""Returns a single training item from the dataset as a dictionary.\n\n        Values correspond to torch tensors.\n        Keys in the dictionary are either strings or tuples:\n\n            (""color"", <frame_id>, <scale>)          for raw colour images,\n            (""color_aug"", <frame_id>, <scale>)      for augmented colour images,\n            (""K"", scale) or (""inv_K"", scale)        for camera intrinsics,\n            ""stereo_T""                              for camera extrinsics, and\n            ""depth_gt""                              for ground truth depth maps.\n\n        <frame_id> is either:\n            an integer (e.g. 0, -1, or 1) representing the temporal step relative to \'index\',\n        or\n            ""s"" for the opposite image in the stereo pair.\n\n        <scale> is an integer representing the scale of the image relative to the fullsize image:\n            -1      images at native resolution as loaded from disk\n            0       images resized to (self.width,      self.height     )\n            1       images resized to (self.width // 2, self.height // 2)\n            2       images resized to (self.width // 4, self.height // 4)\n            3       images resized to (self.width // 8, self.height // 8)\n        """"""\n        inputs = {}\n\n        do_color_aug = self.is_train and random.random() > 0.5\n        do_flip = self.is_train and random.random() > 0.5\n\n        line = self.filenames[index].split()\n        folder = line[0]\n\n        if len(line) == 3:\n            frame_index = int(line[1])\n        else:\n            frame_index = 0\n\n        if len(line) == 3:\n            side = line[2]\n        else:\n            side = None\n\n        for i in self.frame_idxs:\n            if i == ""s"":\n                other_side = {""r"": ""l"", ""l"": ""r""}[side]\n                inputs[(""color"", i, -1)] = self.get_color(folder, frame_index, other_side, do_flip)\n            else:\n                inputs[(""color"", i, -1)] = self.get_color(folder, frame_index + i, side, do_flip)\n\n        # adjusting intrinsics to match each scale in the pyramid\n        for scale in range(self.num_scales):\n            K = self.K.copy()\n\n            K[0, :] *= self.width // (2 ** scale)\n            K[1, :] *= self.height // (2 ** scale)\n\n            inv_K = np.linalg.pinv(K)\n\n            inputs[(""K"", scale)] = torch.from_numpy(K)\n            inputs[(""inv_K"", scale)] = torch.from_numpy(inv_K)\n\n        if do_color_aug:\n            color_aug = transforms.ColorJitter.get_params(\n                self.brightness, self.contrast, self.saturation, self.hue)\n        else:\n            color_aug = (lambda x: x)\n\n        self.preprocess(inputs, color_aug)\n\n        for i in self.frame_idxs:\n            del inputs[(""color"", i, -1)]\n            del inputs[(""color_aug"", i, -1)]\n\n        if self.load_depth:\n            depth_gt = self.get_depth(folder, frame_index, side, do_flip)\n            inputs[""depth_gt""] = np.expand_dims(depth_gt, 0)\n            inputs[""depth_gt""] = torch.from_numpy(inputs[""depth_gt""].astype(np.float32))\n\n        if ""s"" in self.frame_idxs:\n            stereo_T = np.eye(4, dtype=np.float32)\n            baseline_sign = -1 if do_flip else 1\n            side_sign = -1 if side == ""l"" else 1\n            stereo_T[0, 3] = side_sign * baseline_sign * 0.1\n\n            inputs[""stereo_T""] = torch.from_numpy(stereo_T)\n\n        return inputs\n\n    def get_color(self, folder, frame_index, side, do_flip):\n        raise NotImplementedError\n\n    def check_depth(self):\n        raise NotImplementedError\n\n    def get_depth(self, folder, frame_index, side, do_flip):\n        raise NotImplementedError\n'"
networks/__init__.py,0,b'from .resnet_encoder import ResnetEncoder\nfrom .depth_decoder import DepthDecoder\nfrom .pose_decoder import PoseDecoder\nfrom .pose_cnn import PoseCNN\n'
networks/depth_decoder.py,2,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\nimport torch\nimport torch.nn as nn\n\nfrom collections import OrderedDict\nfrom layers import *\n\n\nclass DepthDecoder(nn.Module):\n    def __init__(self, num_ch_enc, scales=range(4), num_output_channels=1, use_skips=True):\n        super(DepthDecoder, self).__init__()\n\n        self.num_output_channels = num_output_channels\n        self.use_skips = use_skips\n        self.upsample_mode = \'nearest\'\n        self.scales = scales\n\n        self.num_ch_enc = num_ch_enc\n        self.num_ch_dec = np.array([16, 32, 64, 128, 256])\n\n        # decoder\n        self.convs = OrderedDict()\n        for i in range(4, -1, -1):\n            # upconv_0\n            num_ch_in = self.num_ch_enc[-1] if i == 4 else self.num_ch_dec[i + 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(""upconv"", i, 0)] = ConvBlock(num_ch_in, num_ch_out)\n\n            # upconv_1\n            num_ch_in = self.num_ch_dec[i]\n            if self.use_skips and i > 0:\n                num_ch_in += self.num_ch_enc[i - 1]\n            num_ch_out = self.num_ch_dec[i]\n            self.convs[(""upconv"", i, 1)] = ConvBlock(num_ch_in, num_ch_out)\n\n        for s in self.scales:\n            self.convs[(""dispconv"", s)] = Conv3x3(self.num_ch_dec[s], self.num_output_channels)\n\n        self.decoder = nn.ModuleList(list(self.convs.values()))\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, input_features):\n        self.outputs = {}\n\n        # decoder\n        x = input_features[-1]\n        for i in range(4, -1, -1):\n            x = self.convs[(""upconv"", i, 0)](x)\n            x = [upsample(x)]\n            if self.use_skips and i > 0:\n                x += [input_features[i - 1]]\n            x = torch.cat(x, 1)\n            x = self.convs[(""upconv"", i, 1)](x)\n            if i in self.scales:\n                self.outputs[(""disp"", i)] = self.sigmoid(self.convs[(""dispconv"", i)](x))\n\n        return self.outputs\n'"
networks/pose_cnn.py,1,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\n\n\nclass PoseCNN(nn.Module):\n    def __init__(self, num_input_frames):\n        super(PoseCNN, self).__init__()\n\n        self.num_input_frames = num_input_frames\n\n        self.convs = {}\n        self.convs[0] = nn.Conv2d(3 * num_input_frames, 16, 7, 2, 3)\n        self.convs[1] = nn.Conv2d(16, 32, 5, 2, 2)\n        self.convs[2] = nn.Conv2d(32, 64, 3, 2, 1)\n        self.convs[3] = nn.Conv2d(64, 128, 3, 2, 1)\n        self.convs[4] = nn.Conv2d(128, 256, 3, 2, 1)\n        self.convs[5] = nn.Conv2d(256, 256, 3, 2, 1)\n        self.convs[6] = nn.Conv2d(256, 256, 3, 2, 1)\n\n        self.pose_conv = nn.Conv2d(256, 6 * (num_input_frames - 1), 1)\n\n        self.num_convs = len(self.convs)\n\n        self.relu = nn.ReLU(True)\n\n        self.net = nn.ModuleList(list(self.convs.values()))\n\n    def forward(self, out):\n\n        for i in range(self.num_convs):\n            out = self.convs[i](out)\n            out = self.relu(out)\n\n        out = self.pose_conv(out)\n        out = out.mean(3).mean(2)\n\n        out = 0.01 * out.view(-1, self.num_input_frames - 1, 1, 6)\n\n        axisangle = out[..., :3]\n        translation = out[..., 3:]\n\n        return axisangle, translation\n'"
networks/pose_decoder.py,2,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport torch\nimport torch.nn as nn\nfrom collections import OrderedDict\n\n\nclass PoseDecoder(nn.Module):\n    def __init__(self, num_ch_enc, num_input_features, num_frames_to_predict_for=None, stride=1):\n        super(PoseDecoder, self).__init__()\n\n        self.num_ch_enc = num_ch_enc\n        self.num_input_features = num_input_features\n\n        if num_frames_to_predict_for is None:\n            num_frames_to_predict_for = num_input_features - 1\n        self.num_frames_to_predict_for = num_frames_to_predict_for\n\n        self.convs = OrderedDict()\n        self.convs[(""squeeze"")] = nn.Conv2d(self.num_ch_enc[-1], 256, 1)\n        self.convs[(""pose"", 0)] = nn.Conv2d(num_input_features * 256, 256, 3, stride, 1)\n        self.convs[(""pose"", 1)] = nn.Conv2d(256, 256, 3, stride, 1)\n        self.convs[(""pose"", 2)] = nn.Conv2d(256, 6 * num_frames_to_predict_for, 1)\n\n        self.relu = nn.ReLU()\n\n        self.net = nn.ModuleList(list(self.convs.values()))\n\n    def forward(self, input_features):\n        last_features = [f[-1] for f in input_features]\n\n        cat_features = [self.relu(self.convs[""squeeze""](f)) for f in last_features]\n        cat_features = torch.cat(cat_features, 1)\n\n        out = cat_features\n        for i in range(3):\n            out = self.convs[(""pose"", i)](out)\n            if i != 2:\n                out = self.relu(out)\n\n        out = out.mean(3).mean(2)\n\n        out = 0.01 * out.view(-1, self.num_frames_to_predict_for, 1, 6)\n\n        axisangle = out[..., :3]\n        translation = out[..., 3:]\n\n        return axisangle, translation\n'"
networks/resnet_encoder.py,3,"b'# Copyright Niantic 2019. Patent Pending. All rights reserved.\n#\n# This software is licensed under the terms of the Monodepth2 licence\n# which allows for non-commercial use only, the full terms of which are made\n# available in the LICENSE file.\n\nfrom __future__ import absolute_import, division, print_function\n\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.utils.model_zoo as model_zoo\n\n\nclass ResNetMultiImageInput(models.ResNet):\n    """"""Constructs a resnet model with varying number of input images.\n    Adapted from https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n    """"""\n    def __init__(self, block, layers, num_classes=1000, num_input_images=1):\n        super(ResNetMultiImageInput, self).__init__(block, layers)\n        self.inplanes = 64\n        self.conv1 = nn.Conv2d(\n            num_input_images * 3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode=\'fan_out\', nonlinearity=\'relu\')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n\ndef resnet_multiimage_input(num_layers, pretrained=False, num_input_images=1):\n    """"""Constructs a ResNet model.\n    Args:\n        num_layers (int): Number of resnet layers. Must be 18 or 50\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n        num_input_images (int): Number of frames stacked as input\n    """"""\n    assert num_layers in [18, 50], ""Can only run with 18 or 50 layer resnet""\n    blocks = {18: [2, 2, 2, 2], 50: [3, 4, 6, 3]}[num_layers]\n    block_type = {18: models.resnet.BasicBlock, 50: models.resnet.Bottleneck}[num_layers]\n    model = ResNetMultiImageInput(block_type, blocks, num_input_images=num_input_images)\n\n    if pretrained:\n        loaded = model_zoo.load_url(models.resnet.model_urls[\'resnet{}\'.format(num_layers)])\n        loaded[\'conv1.weight\'] = torch.cat(\n            [loaded[\'conv1.weight\']] * num_input_images, 1) / num_input_images\n        model.load_state_dict(loaded)\n    return model\n\n\nclass ResnetEncoder(nn.Module):\n    """"""Pytorch module for a resnet encoder\n    """"""\n    def __init__(self, num_layers, pretrained, num_input_images=1):\n        super(ResnetEncoder, self).__init__()\n\n        self.num_ch_enc = np.array([64, 64, 128, 256, 512])\n\n        resnets = {18: models.resnet18,\n                   34: models.resnet34,\n                   50: models.resnet50,\n                   101: models.resnet101,\n                   152: models.resnet152}\n\n        if num_layers not in resnets:\n            raise ValueError(""{} is not a valid number of resnet layers"".format(num_layers))\n\n        if num_input_images > 1:\n            self.encoder = resnet_multiimage_input(num_layers, pretrained, num_input_images)\n        else:\n            self.encoder = resnets[num_layers](pretrained)\n\n        if num_layers > 34:\n            self.num_ch_enc[1:] *= 4\n\n    def forward(self, input_image):\n        self.features = []\n        x = (input_image - 0.45) / 0.225\n        x = self.encoder.conv1(x)\n        x = self.encoder.bn1(x)\n        self.features.append(self.encoder.relu(x))\n        self.features.append(self.encoder.layer1(self.encoder.maxpool(self.features[-1])))\n        self.features.append(self.encoder.layer2(self.features[-1]))\n        self.features.append(self.encoder.layer3(self.features[-1]))\n        self.features.append(self.encoder.layer4(self.features[-1]))\n\n        return self.features\n'"
