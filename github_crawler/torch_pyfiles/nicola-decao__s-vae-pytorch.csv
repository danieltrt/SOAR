file_path,api_count,code
setup.py,0,"b""\nimport os\nfrom setuptools import setup\nfrom setuptools import find_packages\n\nsetup(\n    name='hyperspherical_vae',\n    version='0.1.1',\n    author='Nicola De Cao, Tim R. Davidson, Luca Falorsi',\n    author_email='nicola.decao@gmail.com',\n    description='Pytorch implementation of Hyperspherical Variational Auto-Encoders',\n    license='MIT',\n    keywords='pytorch vae variational-auto-encoder von-mises-fisher  machine-learning deep-learning manifold-learning',\n    url='https://nicola-decao.github.io/s-vae-pytorch/',\n    download_url='https://github.com/nicola-decao/SVAE',\n    long_description=open(os.path.join(os.path.dirname(__file__), 'README.md')).read(),\n    install_requires=['numpy', 'torch>=0.4.1', 'scipy>=1.0.0', 'numpy'],\n    packages=find_packages()\n)\n"""
examples/mnist.py,16,"b'\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torchvision import datasets, transforms\nfrom collections import defaultdict\n\nfrom hyperspherical_vae.distributions import VonMisesFisher\nfrom hyperspherical_vae.distributions import HypersphericalUniform\n\n\ntrain_loader = torch.utils.data.DataLoader(datasets.MNIST(\'./data\', train=True, download=True,\n    transform=transforms.ToTensor()), batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(datasets.MNIST(\'./data\', train=False, download=True,\n    transform=transforms.ToTensor()), batch_size=64)\n\n\nclass ModelVAE(torch.nn.Module):\n    \n    def __init__(self, h_dim, z_dim, activation=F.relu, distribution=\'normal\'):\n        """"""\n        ModelVAE initializer\n        :param h_dim: dimension of the hidden layers\n        :param z_dim: dimension of the latent representation\n        :param activation: callable activation function\n        :param distribution: string either `normal` or `vmf`, indicates which distribution to use\n        """"""\n        super(ModelVAE, self).__init__()\n        \n        self.z_dim, self.activation, self.distribution = z_dim, activation, distribution\n        \n        # 2 hidden layers encoder\n        self.fc_e0 = nn.Linear(784, h_dim * 2)\n        self.fc_e1 = nn.Linear(h_dim * 2, h_dim)\n\n        if self.distribution == \'normal\':\n            # compute mean and std of the normal distribution\n            self.fc_mean = nn.Linear(h_dim, z_dim)\n            self.fc_var =  nn.Linear(h_dim, z_dim)\n        elif self.distribution == \'vmf\':\n            # compute mean and concentration of the von Mises-Fisher\n            self.fc_mean = nn.Linear(h_dim, z_dim)\n            self.fc_var = nn.Linear(h_dim, 1)\n        else:\n            raise NotImplemented\n            \n        # 2 hidden layers decoder\n        self.fc_d0 = nn.Linear(z_dim, h_dim)\n        self.fc_d1 = nn.Linear(h_dim, h_dim * 2)\n        self.fc_logits = nn.Linear(h_dim * 2, 784)\n\n    def encode(self, x):\n        # 2 hidden layers encoder\n        x = self.activation(self.fc_e0(x))\n        x = self.activation(self.fc_e1(x))\n        \n        if self.distribution == \'normal\':\n            # compute mean and std of the normal distribution\n            z_mean = self.fc_mean(x)\n            z_var = F.softplus(self.fc_var(x))\n        elif self.distribution == \'vmf\':\n            # compute mean and concentration of the von Mises-Fisher\n            z_mean = self.fc_mean(x)\n            z_mean = z_mean / z_mean.norm(dim=-1, keepdim=True)\n            # the `+ 1` prevent collapsing behaviors\n            z_var = F.softplus(self.fc_var(x)) + 1\n        else:\n            raise NotImplemented\n        \n        return z_mean, z_var\n        \n    def decode(self, z):\n        \n        x = self.activation(self.fc_d0(z))\n        x = self.activation(self.fc_d1(x))\n        x = self.fc_logits(x)\n        \n        return x\n        \n    def reparameterize(self, z_mean, z_var):\n        if self.distribution == \'normal\':\n            q_z = torch.distributions.normal.Normal(z_mean, z_var)\n            p_z = torch.distributions.normal.Normal(torch.zeros_like(z_mean), torch.ones_like(z_var))\n        elif self.distribution == \'vmf\':\n            q_z = VonMisesFisher(z_mean, z_var)\n            p_z = HypersphericalUniform(self.z_dim - 1)\n        else:\n            raise NotImplemented\n\n        return q_z, p_z\n        \n    def forward(self, x): \n        z_mean, z_var = self.encode(x)\n        q_z, p_z = self.reparameterize(z_mean, z_var)\n        z = q_z.rsample()\n        x_ = self.decode(z)\n        \n        return (z_mean, z_var), (q_z, p_z), z, x_\n    \n    \ndef log_likelihood(model, x, n=10):\n    """"""\n    :param model: model object\n    :param optimizer: optimizer object\n    :param n: number of MC samples\n    :return: MC estimate of log-likelihood\n    """"""\n\n    z_mean, z_var = model.encode(x.reshape(-1, 784))\n    q_z, p_z = model.reparameterize(z_mean, z_var)\n    z = q_z.rsample(torch.Size([n]))\n    x_mb_ = model.decode(z)\n\n    log_p_z = p_z.log_prob(z)\n\n    if model.distribution == \'normal\':\n        log_p_z = log_p_z.sum(-1)\n\n    log_p_x_z = -nn.BCEWithLogitsLoss(reduction=\'none\')(x_mb_, x.reshape(-1, 784).repeat((n, 1, 1))).sum(-1)\n\n    log_q_z_x = q_z.log_prob(z)\n\n    if model.distribution == \'normal\':\n        log_q_z_x = log_q_z_x.sum(-1)\n\n    return ((log_p_x_z + log_p_z - log_q_z_x).t().logsumexp(-1) - np.log(n)).mean()\n\n\ndef train(model, optimizer):\n    for i, (x_mb, y_mb) in enumerate(train_loader):\n\n            optimizer.zero_grad()\n            \n            # dynamic binarization\n            x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n\n            _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n\n            loss_recon = nn.BCEWithLogitsLoss(reduction=\'none\')(x_mb_, x_mb.reshape(-1, 784)).sum(-1).mean()\n\n            if model.distribution == \'normal\':\n                loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean()\n            elif model.distribution == \'vmf\':\n                loss_KL = torch.distributions.kl.kl_divergence(q_z, p_z).mean()\n            else:\n                raise NotImplemented\n\n            loss = loss_recon + loss_KL\n\n            loss.backward()\n            optimizer.step()\n            \n            \ndef test(model, optimizer):\n    print_ = defaultdict(list)\n    for x_mb, y_mb in test_loader:\n        \n        # dynamic binarization\n        x_mb = (x_mb > torch.distributions.Uniform(0, 1).sample(x_mb.shape)).float()\n        \n        _, (q_z, p_z), _, x_mb_ = model(x_mb.reshape(-1, 784))\n        \n        print_[\'recon loss\'].append(float(nn.BCEWithLogitsLoss(reduction=\'none\')(x_mb_,\n            x_mb.reshape(-1, 784)).sum(-1).mean().data))\n        \n        if model.distribution == \'normal\':\n            print_[\'KL\'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).sum(-1).mean().data))\n        elif model.distribution == \'vmf\':\n            print_[\'KL\'].append(float(torch.distributions.kl.kl_divergence(q_z, p_z).mean().data))\n        else:\n            raise NotImplemented\n        \n        print_[\'ELBO\'].append(- print_[\'recon loss\'][-1] - print_[\'KL\'][-1])\n        print_[\'LL\'].append(float(log_likelihood(model, x_mb).data))\n    \n    print({k: np.mean(v) for k, v in print_.items()})\n\n\n# hidden dimension and dimension of latent space\nH_DIM = 128\nZ_DIM = 5\n\n# normal VAE\nmodelN = ModelVAE(h_dim=H_DIM, z_dim=Z_DIM, distribution=\'normal\')\noptimizerN = optim.Adam(modelN.parameters(), lr=1e-3)\n\nprint(\'##### Normal VAE #####\')\n\n# training for 1 epoch\ntrain(modelN, optimizerN)\n\n# test\ntest(modelN, optimizerN)\n\nprint()\n\n# hyper-spherical  VAE\nmodelS = ModelVAE(h_dim=H_DIM, z_dim=Z_DIM + 1, distribution=\'vmf\')\noptimizerS = optim.Adam(modelS.parameters(), lr=1e-3)\n\nprint(\'##### Hyper-spherical VAE #####\')\n\n# training for 1 epoch\ntrain(modelS, optimizerS)\n\n# test\ntest(modelS, optimizerS)\n'"
hyperspherical_vae/__init__.py,0,b'import hyperspherical_vae.ops\nimport hyperspherical_vae.distributions\n'
hyperspherical_vae/distributions/__init__.py,0,"b'from hyperspherical_vae.distributions.von_mises_fisher import VonMisesFisher\nfrom hyperspherical_vae.distributions.hyperspherical_uniform import (\n    HypersphericalUniform,\n)\n'"
hyperspherical_vae/distributions/hyperspherical_uniform.py,13,"b'import math\nimport torch\n\n\nclass HypersphericalUniform(torch.distributions.Distribution):\n\n    support = torch.distributions.constraints.real\n    has_rsample = False\n    _mean_carrier_measure = 0\n\n    @property\n    def dim(self):\n        return self._dim\n\n    @property\n    def device(self):\n        return self._device\n\n    @device.setter\n    def device(self, val):\n        self._device = val if isinstance(val, torch.device) else torch.device(val)\n\n    def __init__(self, dim, validate_args=None, device=""cpu""):\n        super(HypersphericalUniform, self).__init__(\n            torch.Size([dim]), validate_args=validate_args\n        )\n        self._dim = dim\n        self.device = device\n\n    def sample(self, shape=torch.Size()):\n        output = (\n            torch.distributions.Normal(0, 1)\n            .sample(\n                (shape if isinstance(shape, torch.Size) else torch.Size([shape]))\n                + torch.Size([self._dim + 1])\n            )\n            .to(self.device)\n        )\n\n        return output / output.norm(dim=-1, keepdim=True)\n\n    def entropy(self):\n        return self.__log_surface_area()\n\n    def log_prob(self, x):\n        return -torch.ones(x.shape[:-1], device=self.device) * self.__log_surface_area()\n\n    def __log_surface_area(self):\n        if torch.__version__ >= ""1.0.0"":\n            lgamma = torch.lgamma(torch.tensor([(self._dim + 1) / 2]).to(self.device))\n        else:\n            lgamma = torch.lgamma(\n                torch.Tensor([(self._dim + 1) / 2], device=self.device)\n            )\n        return math.log(2) + ((self._dim + 1) / 2) * math.log(math.pi) - lgamma\n'"
hyperspherical_vae/distributions/von_mises_fisher.py,42,"b'import math\nimport torch\nfrom torch.distributions.kl import register_kl\n\nfrom hyperspherical_vae.ops.ive import ive, ive_fraction_approx, ive_fraction_approx2\nfrom hyperspherical_vae.distributions.hyperspherical_uniform import (\n    HypersphericalUniform,\n)\n\n\nclass VonMisesFisher(torch.distributions.Distribution):\n\n    arg_constraints = {\n        ""loc"": torch.distributions.constraints.real,\n        ""scale"": torch.distributions.constraints.positive,\n    }\n    support = torch.distributions.constraints.real\n    has_rsample = True\n    _mean_carrier_measure = 0\n\n    @property\n    def mean(self):\n        # option 1:\n        return self.loc * (\n            ive(self.__m / 2, self.scale) / ive(self.__m / 2 - 1, self.scale)\n        )\n        # option 2:\n        # return self.loc * ive_fraction_approx(torch.tensor(self.__m / 2), self.scale)\n        # options 3:\n        # return self.loc * ive_fraction_approx2(torch.tensor(self.__m / 2), self.scale)\n\n    @property\n    def stddev(self):\n        return self.scale\n\n    def __init__(self, loc, scale, validate_args=None, k=1):\n        self.dtype = loc.dtype\n        self.loc = loc\n        self.scale = scale\n        self.device = loc.device\n        self.__m = loc.shape[-1]\n        self.__e1 = (torch.Tensor([1.0] + [0] * (loc.shape[-1] - 1))).to(self.device)\n        self.k = k\n\n        super().__init__(self.loc.size(), validate_args=validate_args)\n\n    def sample(self, shape=torch.Size()):\n        with torch.no_grad():\n            return self.rsample(shape)\n\n    def rsample(self, shape=torch.Size()):\n        shape = shape if isinstance(shape, torch.Size) else torch.Size([shape])\n\n        w = (\n            self.__sample_w3(shape=shape)\n            if self.__m == 3\n            else self.__sample_w_rej(shape=shape)\n        )\n\n        v = (\n            torch.distributions.Normal(0, 1)\n            .sample(shape + torch.Size(self.loc.shape))\n            .to(self.device)\n            .transpose(0, -1)[1:]\n        ).transpose(0, -1)\n        v = v / v.norm(dim=-1, keepdim=True)\n\n        w_ = torch.sqrt(torch.clamp(1 - (w ** 2), 1e-10))\n        x = torch.cat((w, w_ * v), -1)\n        z = self.__householder_rotation(x)\n\n        return z.type(self.dtype)\n\n    def __sample_w3(self, shape):\n        shape = shape + torch.Size(self.scale.shape)\n        u = torch.distributions.Uniform(0, 1).sample(shape).to(self.device)\n        self.__w = (\n            1\n            + torch.stack(\n                [torch.log(u), torch.log(1 - u) - 2 * self.scale], dim=0\n            ).logsumexp(0)\n            / self.scale\n        )\n        return self.__w\n\n    def __sample_w_rej(self, shape):\n        c = torch.sqrt((4 * (self.scale ** 2)) + (self.__m - 1) ** 2)\n        b_true = (-2 * self.scale + c) / (self.__m - 1)\n\n        # using Taylor approximation with a smooth swift from 10 < scale < 11\n        # to avoid numerical errors for large scale\n        b_app = (self.__m - 1) / (4 * self.scale)\n        s = torch.min(\n            torch.max(\n                torch.tensor([0.0], dtype=self.dtype, device=self.device),\n                self.scale - 10,\n            ),\n            torch.tensor([1.0], dtype=self.dtype, device=self.device),\n        )\n        b = b_app * s + b_true * (1 - s)\n\n        a = (self.__m - 1 + 2 * self.scale + c) / 4\n        d = (4 * a * b) / (1 + b) - (self.__m - 1) * math.log(self.__m - 1)\n\n        self.__b, (self.__e, self.__w) = b, self.__while_loop(b, a, d, shape, k=self.k)\n        return self.__w\n\n    @staticmethod\n    def first_nonzero(x, dim, invalid_val=-1):\n        mask = x > 0\n        idx = torch.where(\n            mask.any(dim=dim),\n            mask.float().argmax(dim=1).squeeze(),\n            torch.tensor(invalid_val, device=x.device),\n        )\n        return idx\n\n    def __while_loop(self, b, a, d, shape, k=20, eps=1e-20):\n        #  matrix while loop: samples a matrix of [A, k] samples, to avoid looping all together\n        b, a, d = [\n            e.repeat(*shape, *([1] * len(self.scale.shape))).reshape(-1, 1)\n            for e in (b, a, d)\n        ]\n        w, e, bool_mask = (\n            torch.zeros_like(b).to(self.device),\n            torch.zeros_like(b).to(self.device),\n            (torch.ones_like(b) == 1).to(self.device),\n        )\n\n        sample_shape = torch.Size([b.shape[0], k])\n        shape = shape + torch.Size(self.scale.shape)\n\n        while bool_mask.sum() != 0:\n            con1 = torch.tensor((self.__m - 1) / 2, dtype=torch.float64)\n            con2 = torch.tensor((self.__m - 1) / 2, dtype=torch.float64)\n            e_ = (\n                torch.distributions.Beta(con1, con2)\n                .sample(sample_shape)\n                .to(self.device)\n                .type(self.dtype)\n            )\n\n            u = (\n                torch.distributions.Uniform(0 + eps, 1 - eps)\n                .sample(sample_shape)\n                .to(self.device)\n                .type(self.dtype)\n            )\n\n            w_ = (1 - (1 + b) * e_) / (1 - (1 - b) * e_)\n            t = (2 * a * b) / (1 - (1 - b) * e_)\n\n            accept = ((self.__m - 1.0) * t.log() - t + d) > torch.log(u)\n            accept_idx = self.first_nonzero(accept, dim=-1, invalid_val=-1).unsqueeze(1)\n            accept_idx_clamped = accept_idx.clamp(0)\n            # we use .abs(), in order to not get -1 index issues, the -1 is still used afterwards\n            w_ = w_.gather(1, accept_idx_clamped.view(-1, 1))\n            e_ = e_.gather(1, accept_idx_clamped.view(-1, 1))\n\n            reject = accept_idx < 0\n            accept = ~reject if torch.__version__ >= ""1.2.0"" else 1 - reject\n\n            w[bool_mask * accept] = w_[bool_mask * accept]\n            e[bool_mask * accept] = e_[bool_mask * accept]\n\n            bool_mask[bool_mask * accept] = reject[bool_mask * accept]\n\n        return e.reshape(shape), w.reshape(shape)\n\n    def __householder_rotation(self, x):\n        u = self.__e1 - self.loc\n        u = u / (u.norm(dim=-1, keepdim=True) + 1e-5)\n        z = x - 2 * (x * u).sum(-1, keepdim=True) * u\n        return z\n\n    def entropy(self):\n        # option 1:\n        output = (\n            -self.scale\n            * ive(self.__m / 2, self.scale)\n            / ive((self.__m / 2) - 1, self.scale)\n        )\n        # option 2:\n        # output = - self.scale * ive_fraction_approx(torch.tensor(self.__m / 2), self.scale)\n        # option 3:\n        # output = - self.scale * ive_fraction_approx2(torch.tensor(self.__m / 2), self.scale)\n\n        return output.view(*(output.shape[:-1])) + self._log_normalization()\n\n    def log_prob(self, x):\n        return self._log_unnormalized_prob(x) - self._log_normalization()\n\n    def _log_unnormalized_prob(self, x):\n        output = self.scale * (self.loc * x).sum(-1, keepdim=True)\n\n        return output.view(*(output.shape[:-1]))\n\n    def _log_normalization(self):\n        output = -(\n            (self.__m / 2 - 1) * torch.log(self.scale)\n            - (self.__m / 2) * math.log(2 * math.pi)\n            - (self.scale + torch.log(ive(self.__m / 2 - 1, self.scale)))\n        )\n\n        return output.view(*(output.shape[:-1]))\n\n\n@register_kl(VonMisesFisher, HypersphericalUniform)\ndef _kl_vmf_uniform(vmf, hyu):\n    return -vmf.entropy() + hyu.entropy()\n'"
hyperspherical_vae/ops/__init__.py,0,b'from hyperspherical_vae.ops.ive import ive\n'
hyperspherical_vae/ops/ive.py,7,"b'import torch\nimport numpy as np\nimport scipy.special\nfrom numbers import Number\n\n\nclass IveFunction(torch.autograd.Function):\n    @staticmethod\n    def forward(self, v, z):\n\n        assert isinstance(v, Number), ""v must be a scalar""\n\n        self.save_for_backward(z)\n        self.v = v\n        z_cpu = z.data.cpu().numpy()\n\n        if np.isclose(v, 0):\n            output = scipy.special.i0e(z_cpu, dtype=z_cpu.dtype)\n        elif np.isclose(v, 1):\n            output = scipy.special.i1e(z_cpu, dtype=z_cpu.dtype)\n        else:  #  v > 0\n            output = scipy.special.ive(v, z_cpu, dtype=z_cpu.dtype)\n        #         else:\n        #             print(v, type(v), np.isclose(v, 0))\n        #             raise RuntimeError(\'v must be >= 0, it is {}\'.format(v))\n\n        return torch.Tensor(output).to(z.device)\n\n    @staticmethod\n    def backward(self, grad_output):\n        z = self.saved_tensors[-1]\n        return (\n            None,\n            grad_output * (ive(self.v - 1, z) - ive(self.v, z) * (self.v + z) / z),\n        )\n\n\nclass Ive(torch.nn.Module):\n    def __init__(self, v):\n        super(Ive, self).__init__()\n        self.v = v\n\n    def forward(self, z):\n        return ive(self.v, z)\n\n\nive = IveFunction.apply\n\n\n##########\n# The below provided approximations were provided in the\n# respective source papers, to improve the stability of\n# the Bessel fractions.\n# I_(v/2)(k) / I_(v/2 - 1)(k)\n\n# source: https://arxiv.org/pdf/1606.02008.pdf\ndef ive_fraction_approx(v, z):\n    # I_(v/2)(k) / I_(v/2 - 1)(k) >= z / (v-1 + ((v+1)^2 + z^2)^0.5\n    return z / (v - 1 + torch.pow(torch.pow(v + 1, 2) + torch.pow(z, 2), 0.5))\n\n\n# source: https://arxiv.org/pdf/1902.02603.pdf\ndef ive_fraction_approx2(v, z, eps=1e-20):\n    def delta_a(a):\n        lamb = v + (a - 1.0) / 2.0\n        return (v - 0.5) + lamb / (\n            2 * torch.sqrt((torch.pow(lamb, 2) + torch.pow(z, 2)).clamp(eps))\n        )\n\n    delta_0 = delta_a(0.0)\n    delta_2 = delta_a(2.0)\n    B_0 = z / (\n        delta_0 + torch.sqrt((torch.pow(delta_0, 2) + torch.pow(z, 2))).clamp(eps)\n    )\n    B_2 = z / (\n        delta_2 + torch.sqrt((torch.pow(delta_2, 2) + torch.pow(z, 2))).clamp(eps)\n    )\n\n    return (B_0 + B_2) / 2.0\n'"
