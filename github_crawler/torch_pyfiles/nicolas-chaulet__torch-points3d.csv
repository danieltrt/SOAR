file_path,api_count,code
eval.py,4,"b'import torch\nimport hydra\nimport logging\nfrom omegaconf import OmegaConf\n\n# Import building function for model and dataset\nfrom torch_points3d.datasets.dataset_factory import instantiate_dataset\nfrom torch_points3d.models.model_factory import instantiate_model\n\n# Import BaseModel / BaseDataset for type checking\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n# Import from metrics\nfrom torch_points3d.metrics.base_tracker import BaseTracker\nfrom torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n# Utils import\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\nfrom torch_points3d.utils.colors import COLORS\n\nlog = logging.getLogger(__name__)\n\n\ndef eval_epoch(\n    model: BaseModel,\n    dataset,\n    device,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    voting_runs=1,\n    tracker_options={},\n):\n    tracker.reset(""val"")\n    loader = dataset.val_dataloader\n    for i in range(voting_runs):\n        with Ctq(loader) as tq_val_loader:\n            for data in tq_val_loader:\n                with torch.no_grad():\n                    model.set_input(data, device)\n                    model.forward()\n\n                tracker.track(model, data=data, **tracker_options)\n                tq_val_loader.set_postfix(**tracker.get_metrics(), color=COLORS.VAL_COLOR)\n\n    tracker.finalise(**tracker_options)\n    tracker.print_summary()\n\n\ndef test_epoch(\n    model: BaseModel,\n    dataset,\n    device,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    voting_runs=1,\n    tracker_options={},\n):\n\n    loaders = dataset.test_dataloaders\n\n    for loader in loaders:\n        stage_name = loader.dataset.name\n        if not loader.has_labels and not tracker_options[""make_submission""]:  # No label, no submission -> do nothing\n            log.warning(""No forward will be run on dataset %s."" % stage_name)\n            continue\n\n        tracker.reset(stage_name)\n        for i in range(voting_runs):\n            with Ctq(loader) as tq_test_loader:\n                for data in tq_test_loader:\n                    with torch.no_grad():\n                        model.set_input(data, device)\n                        model.forward()\n\n                    tracker.track(model, data=data, **tracker_options)\n                    tq_test_loader.set_postfix(**tracker.get_metrics(), color=COLORS.TEST_COLOR)\n\n        tracker.finalise(**tracker_options)\n        tracker.print_summary()\n\n\ndef run(\n    cfg,\n    model,\n    dataset: BaseDataset,\n    device,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    voting_runs=1,\n    tracker_options={},\n):\n    if dataset.has_val_loader:\n        eval_epoch(\n            model, dataset, device, tracker, checkpoint, voting_runs=voting_runs, tracker_options=tracker_options\n        )\n\n    if dataset.has_test_loaders:\n        test_epoch(\n            model, dataset, device, tracker, checkpoint, voting_runs=voting_runs, tracker_options=tracker_options,\n        )\n\n\n@hydra.main(config_path=""conf/eval.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)\n\n    # Get device\n    device = torch.device(""cuda"" if (torch.cuda.is_available() and cfg.cuda) else ""cpu"")\n    log.info(""DEVICE : {}"".format(device))\n\n    # Enable CUDNN BACKEND\n    torch.backends.cudnn.enabled = cfg.enable_cudnn\n\n    # Checkpoint\n    checkpoint = ModelCheckpoint(cfg.checkpoint_dir, cfg.model_name, cfg.weight_name, strict=True)\n\n    # Create model and datasets\n    dataset = instantiate_dataset(checkpoint.data_config)\n    model = checkpoint.create_model(dataset, weight_name=cfg.weight_name)\n    log.info(model)\n    log.info(""Model size = %i"", sum(param.numel() for param in model.parameters() if param.requires_grad))\n\n    # Set dataloaders\n    dataset.create_dataloaders(\n        model, cfg.batch_size, cfg.shuffle, cfg.num_workers, cfg.precompute_multi_scale,\n    )\n    log.info(dataset)\n\n    model.eval()\n    if cfg.enable_dropout:\n        model.enable_dropout_in_eval()\n    model = model.to(device)\n\n    tracker: BaseTracker = dataset.get_tracker(False, False)\n\n    # Run training / evaluation\n    run(\n        cfg,\n        model,\n        dataset,\n        device,\n        tracker,\n        checkpoint,\n        voting_runs=cfg.voting_runs,\n        tracker_options=cfg.tracker_options,\n    )\n\n\nif __name__ == ""__main__"":\n    main()\n'"
find_neighbour_dist.py,3,"b'import os\nimport torch\nimport hydra\nimport logging\nimport numpy as np\nfrom omegaconf import OmegaConf\nimport pickle\n\n# Import building function for model and dataset\nfrom torch_points3d.datasets.dataset_factory import instantiate_dataset\nfrom torch_points3d.models.model_factory import instantiate_model\n\n# Import BaseModel / BaseDataset for type checking\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n# Import from metrics\nfrom torch_points3d.metrics.base_tracker import BaseTracker\nfrom torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n# Utils import\nfrom torch_points3d.utils.colors import COLORS\nfrom torch_points3d.utils.config import determine_stage, launch_wandb\nfrom torch_points3d.visualization import Visualizer\nfrom torch_points3d.utils.config import set_debugging_vars_to_global\nfrom torch_points3d.utils.debugging_vars import extract_histogram\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nlog = logging.getLogger(__name__)\n\n\ndef process(model, data, device):\n    with torch.no_grad():\n        model.set_input(data, device)\n        model.forward()\n\n\ndef run_epoch(model: BaseModel, loader, device: str, num_batches: int):\n    model.eval()\n    with Ctq(loader) as tq_loader:\n        for batch_idx, data in enumerate(tq_loader):\n            if batch_idx < num_batches:\n                process(model, data, device)\n            else:\n                break\n\n\ndef run(cfg, model: BaseModel, dataset: BaseDataset, device, measurement_name: str):\n    measurements = {}\n\n    num_batches = getattr(cfg.debugging, ""num_batches"", np.inf)\n\n    run_epoch(model, dataset.train_dataloader, device, num_batches)\n    measurements[""train""] = extract_histogram(model.get_spatial_ops(), normalize=False)\n\n    if dataset.has_val_loader:\n        run_epoch(model, dataset.val_dataloader, device, num_batches)\n        measurements[""val""] = extract_histogram(model.get_spatial_ops(), normalize=False)\n\n    for loader in dataset.test_dataloaders:\n        run_epoch(model, dataset.test_dataloaders, device, num_batches)\n        measurements[loader.dataset.name] = extract_histogram(model.get_spatial_ops(), normalize=False)\n\n    with open(os.path.join(DIR, ""measurements/{}.pickle"".format(measurement_name)), ""wb"") as f:\n        pickle.dump(measurements, f)\n\n\n@hydra.main(config_path=""conf/config.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n    if cfg.pretty_print:\n        print(cfg.pretty())\n\n    set_debugging_vars_to_global(cfg.debugging)\n\n    # Get device\n    device = torch.device(""cuda"" if (torch.cuda.is_available() and cfg.training.cuda) else ""cpu"")\n    log.info(""DEVICE : {}"".format(device))\n\n    # Enable CUDNN BACKEND\n    torch.backends.cudnn.enabled = cfg.training.enable_cudnn\n\n    dataset = instantiate_dataset(cfg.data)\n    model = instantiate_model(cfg, dataset)\n\n    log.info(model)\n    log.info(""Model size = %i"", sum(param.numel() for param in model.parameters() if param.requires_grad))\n\n    # Set dataloaders\n    dataset.create_dataloaders(\n        model,\n        cfg.training.batch_size,\n        cfg.training.shuffle,\n        cfg.training.num_workers,\n        cfg.training.precompute_multi_scale,\n    )\n    log.info(dataset)\n\n    # Run training / evaluation\n    model = model.to(device)\n\n    measurement_name = ""{}_{}"".format(cfg.model_name, dataset.__class__.__name__)\n    run(cfg, model, dataset, device, measurement_name)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
train.py,5,"b'import os\nimport torch\nimport hydra\nimport time\nimport logging\nfrom omegaconf import OmegaConf\n\n# Import building function for model and dataset\nfrom torch_points3d.datasets.dataset_factory import instantiate_dataset\nfrom torch_points3d.models.model_factory import instantiate_model\n\n# Import BaseModel / BaseDataset for type checking\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n# Import from metrics\nfrom torch_points3d.metrics.base_tracker import BaseTracker\nfrom torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n# Utils import\nfrom torch_points3d.utils.colors import COLORS\nfrom torch_points3d.utils.config import launch_wandb\nfrom torch_points3d.visualization import Visualizer\n\nlog = logging.getLogger(__name__)\n\n\ndef train_epoch(\n    epoch: int,\n    model: BaseModel,\n    dataset,\n    device: str,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    visualizer: Visualizer,\n    debugging,\n):\n\n    early_break = getattr(debugging, ""early_break"", False)\n    profiling = getattr(debugging, ""profiling"", False)\n\n    model.train()\n    tracker.reset(""train"")\n    visualizer.reset(epoch, ""train"")\n    train_loader = dataset.train_dataloader\n\n    iter_data_time = time.time()\n    with Ctq(train_loader) as tq_train_loader:\n        for i, data in enumerate(tq_train_loader):\n            t_data = time.time() - iter_data_time\n            iter_start_time = time.time()\n            model.set_input(data, device)\n            model.optimize_parameters(epoch, dataset.batch_size)\n            if i % 10 == 0:\n                tracker.track(model, data=data)\n\n            tq_train_loader.set_postfix(\n                **tracker.get_metrics(),\n                data_loading=float(t_data),\n                iteration=float(time.time() - iter_start_time),\n                color=COLORS.TRAIN_COLOR\n            )\n\n            if visualizer.is_active:\n                visualizer.save_visuals(model.get_current_visuals())\n\n            iter_data_time = time.time()\n\n            if early_break:\n                break\n\n            if profiling:\n                if i > getattr(debugging, ""num_batches"", 50):\n                    return 0\n\n    tracker.finalise()\n    metrics = tracker.publish(epoch)\n    checkpoint.save_best_models_under_current_metrics(model, metrics, tracker.metric_func)\n    log.info(""Learning rate = %f"" % model.learning_rate)\n\n\ndef eval_epoch(\n    epoch: int,\n    model: BaseModel,\n    dataset,\n    device,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    visualizer: Visualizer,\n    debugging,\n):\n\n    early_break = getattr(debugging, ""early_break"", False)\n\n    model.eval()\n    tracker.reset(""val"")\n    visualizer.reset(epoch, ""val"")\n    loader = dataset.val_dataloader\n    with Ctq(loader) as tq_val_loader:\n        for data in tq_val_loader:\n            with torch.no_grad():\n                model.set_input(data, device)\n                model.forward()\n\n            tracker.track(model, data=data)\n            tq_val_loader.set_postfix(**tracker.get_metrics(), color=COLORS.VAL_COLOR)\n\n            if visualizer.is_active:\n                visualizer.save_visuals(model.get_current_visuals())\n\n            if early_break:\n                break\n\n    tracker.finalise()\n    metrics = tracker.publish(epoch)\n    tracker.print_summary()\n    checkpoint.save_best_models_under_current_metrics(model, metrics, tracker.metric_func)\n\n\ndef test_epoch(\n    epoch: int,\n    model: BaseModel,\n    dataset,\n    device,\n    tracker: BaseTracker,\n    checkpoint: ModelCheckpoint,\n    visualizer: Visualizer,\n    debugging,\n):\n    early_break = getattr(debugging, ""early_break"", False)\n    model.eval()\n\n    loaders = dataset.test_dataloaders\n\n    for loader in loaders:\n        if not loader.has_labels:\n            continue\n        stage_name = loader.dataset.name\n        tracker.reset(stage_name)\n        visualizer.reset(epoch, stage_name)\n        with Ctq(loader) as tq_test_loader:\n            for data in tq_test_loader:\n                with torch.no_grad():\n                    model.set_input(data, device)\n                    model.forward()\n\n                tracker.track(model, data=data)\n                tq_test_loader.set_postfix(**tracker.get_metrics(), color=COLORS.TEST_COLOR)\n\n                if visualizer.is_active:\n                    visualizer.save_visuals(model.get_current_visuals())\n\n                if early_break:\n                    break\n\n        tracker.finalise()\n        metrics = tracker.publish(epoch)\n        tracker.print_summary()\n        checkpoint.save_best_models_under_current_metrics(model, metrics, tracker.metric_func)\n\n\ndef run(\n    cfg, model, dataset: BaseDataset, device, tracker: BaseTracker, checkpoint: ModelCheckpoint, visualizer: Visualizer\n):\n\n    profiling = getattr(cfg.debugging, ""profiling"", False)\n\n    for epoch in range(checkpoint.start_epoch, cfg.training.epochs):\n        log.info(""EPOCH %i / %i"", epoch, cfg.training.epochs)\n        train_epoch(epoch, model, dataset, device, tracker, checkpoint, visualizer, cfg.debugging)\n        if profiling:\n            return 0\n        if dataset.has_val_loader:\n            eval_epoch(epoch, model, dataset, device, tracker, checkpoint, visualizer, cfg.debugging)\n\n        if dataset.has_test_loaders:\n            test_epoch(epoch, model, dataset, device, tracker, checkpoint, visualizer, cfg.debugging)\n\n    # Single test evaluation in resume case\n    if checkpoint.start_epoch > cfg.training.epochs:\n        if dataset.has_test_loaders:\n            test_epoch(epoch, model, dataset, device, tracker, checkpoint, visualizer, cfg.debugging)\n\n\n@hydra.main(config_path=""conf/config.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)  # This allows getattr and hasattr methods to function correctly\n    if cfg.pretty_print:\n        print(cfg.pretty())\n\n    # Get device\n    device = torch.device(""cuda"" if (torch.cuda.is_available() and cfg.training.cuda) else ""cpu"")\n    log.info(""DEVICE : {}"".format(device))\n\n    # Enable CUDNN BACKEND\n    torch.backends.cudnn.enabled = cfg.training.enable_cudnn\n\n    # Profiling\n    profiling = getattr(cfg.debugging, ""profiling"", False)\n    if profiling:\n        # Set the num_workers as torch.utils.bottleneck doesn\'t work well with it\n        cfg.training.num_workers = 0\n\n    # Start Wandb if public\n    launch_wandb(cfg, cfg.wandb.public and cfg.wandb.log)\n\n    # Checkpoint\n    checkpoint = ModelCheckpoint(\n        cfg.training.checkpoint_dir,\n        cfg.model_name,\n        cfg.training.weight_name,\n        run_config=cfg,\n        resume=bool(cfg.training.checkpoint_dir),\n    )\n\n    # Create model and datasets\n    if not checkpoint.is_empty:\n        dataset = instantiate_dataset(checkpoint.data_config)\n        model = checkpoint.create_model(dataset, weight_name=cfg.training.weight_name)\n    else:\n        dataset = instantiate_dataset(cfg.data)\n        model = instantiate_model(cfg, dataset)\n        model.instantiate_optimizers(cfg)\n    log.info(model)\n    model.log_optimizers()\n    log.info(""Model size = %i"", sum(param.numel() for param in model.parameters() if param.requires_grad))\n\n    # Set dataloaders\n    dataset.create_dataloaders(\n        model,\n        cfg.training.batch_size,\n        cfg.training.shuffle,\n        cfg.training.num_workers,\n        cfg.training.precompute_multi_scale,\n    )\n    log.info(dataset)\n\n    # Choose selection stage\n    selection_stage = getattr(cfg, ""selection_stage"", """")\n    checkpoint.selection_stage = dataset.resolve_saving_stage(selection_stage)\n    tracker: BaseTracker = dataset.get_tracker(cfg.wandb.log, cfg.tensorboard.log)\n\n    launch_wandb(cfg, not cfg.wandb.public and cfg.wandb.log)\n\n    # Run training / evaluation\n    model = model.to(device)\n    visualizer = Visualizer(cfg.visualization, dataset.num_batches, dataset.batch_size, os.getcwd())\n    run(cfg, model, dataset, device, tracker, checkpoint, visualizer)\n\n    # https://github.com/facebookresearch/hydra/issues/440\n    hydra._internal.hydra.GlobalHydra.get_state().clear()\n    return 0\n\n\nif __name__ == ""__main__"":\n    main()\n'"
docs/conf.py,0,"b'# Configuration file for the Sphinx documentation builder.\n#\n# This file only contains a selection of the most common options. For a full\n# list see the documentation:\n# https://www.sphinx-doc.org/en/master/usage/configuration.html\n\n# -- Path setup --------------------------------------------------------------\n\n# If extensions (or modules to document with autodoc) are in another directory,\n# add these directories to sys.path here. If the directory is relative to the\n# documentation root, use os.path.abspath to make it absolute, like shown here.\n#\nimport os\nimport sys\n\nsys.path.insert(0, os.path.abspath(""./..""))\nimport sphinx_rtd_theme\n\n# -- Project information -----------------------------------------------------\n\nproject = ""Torch Points 3D""\ncopyright = ""2020, Thomas Chaton and Nicolas Chaulet""\nauthor = ""Thomas Chaton and Nicolas Chaulet""\n\n\n# -- General configuration ---------------------------------------------------\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named \'sphinx.ext.*\') or your custom\n# ones.\nextensions = [\n    ""sphinx_rtd_theme"",\n    ""sphinx.ext.autosectionlabel"",\n    ""sphinx.ext.autodoc"",\n    ""sphinx.ext.mathjax"",\n    ""sphinx.ext.githubpages"",\n    ""sphinx.ext.viewcode"",\n    ""sphinx.ext.napoleon"",\n]\nautosectionlabel_prefix_document = True\nautodoc_mock_imports = [\n    ""torch_scatter"",\n    ""torch_sparse"",\n    ""torch_cluster"",\n    ""torch_points_kernels"",\n    ""torch"",\n    ""torch_geometric"",\n    ""sklearn"",\n    ""omegaconf"",\n    ""tqdm"",\n    ""hydra"",\n    ""matplotlib"",\n    ""pytorch_metric_learning"",\n    ""scipy"",\n    ""MinkowskiEngine"",\n    ""pandas"",\n    ""numpy"",\n    ""torchnet"",\n    ""h5py"",\n    ""plyfile"",\n    ""wandb"",\n]\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [""_templates""]\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = [""_build"", ""Thumbs.db"", "".DS_Store""]\n\n\n# -- Options for HTML output -------------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = ""sphinx_rtd_theme""\nhtml_theme_options = {\n    ""display_version"": True,\n    ""prev_next_buttons_location"": ""bottom"",\n    # Toc options\n    ""collapse_navigation"": False,\n    ""navigation_depth"": 3,\n}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named ""default.css"" will overwrite the builtin ""default.css"".\nhtml_static_path = [""_static""]\n'"
forward_scripts/__init__.py,0,b''
forward_scripts/forward.py,3,"b'import torch\nimport hydra\nimport logging\nfrom omegaconf import OmegaConf\nimport os\nimport sys\nimport numpy as np\n\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\n# Import building function for model and dataset\nfrom torch_points3d.datasets.dataset_factory import instantiate_dataset, get_dataset_class\nfrom torch_points3d.models.model_factory import instantiate_model\n\n# Import BaseModel / BaseDataset for type checking\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n# Import from metrics\nfrom torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n# Utils import\nfrom torch_points3d.utils.colors import COLORS\n\nlog = logging.getLogger(__name__)\n\n\ndef save(prefix, predicted):\n    for key, value in predicted.items():\n        filename = os.path.splitext(key)[0]\n        out_file = filename + ""_pred""\n        np.save(os.path.join(prefix, out_file), value)\n\n\ndef run(model: BaseModel, dataset: BaseDataset, device, output_path):\n    loaders = dataset.test_dataloaders\n    predicted = {}\n    for loader in loaders:\n        loader.dataset.name\n        with Ctq(loader) as tq_test_loader:\n            for data in tq_test_loader:\n                with torch.no_grad():\n                    model.set_input(data, device)\n                    model.forward()\n                predicted = {**predicted, **dataset.predict_original_samples(data, model.conv_type, model.get_output())}\n\n    save(output_path, predicted)\n\n\n@hydra.main(config_path=""conf/config.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)\n\n    # Get device\n    device = torch.device(""cuda"" if (torch.cuda.is_available() and cfg.cuda) else ""cpu"")\n    log.info(""DEVICE : {}"".format(device))\n\n    # Enable CUDNN BACKEND\n    torch.backends.cudnn.enabled = cfg.enable_cudnn\n\n    # Checkpoint\n    checkpoint = ModelCheckpoint(cfg.checkpoint_dir, cfg.model_name, cfg.weight_name, strict=True)\n\n    # Setup the dataset config\n    # Generic config\n    train_dataset_cls = get_dataset_class(checkpoint.data_config)\n    setattr(checkpoint.data_config, ""class"", train_dataset_cls.FORWARD_CLASS)\n    setattr(checkpoint.data_config, ""dataroot"", cfg.input_path)\n\n    # Datset specific configs\n    if cfg.data:\n        for key, value in cfg.data.items():\n            checkpoint.data_config.update(key, value)\n\n    # Create dataset and mdoel\n    dataset = instantiate_dataset(checkpoint.data_config)\n    model = checkpoint.create_model(dataset, weight_name=cfg.weight_name)\n    log.info(model)\n    log.info(""Model size = %i"", sum(param.numel() for param in model.parameters() if param.requires_grad))\n\n    # Set dataloaders\n    dataset.create_dataloaders(\n        model, cfg.batch_size, cfg.shuffle, cfg.num_workers, False,\n    )\n    log.info(dataset)\n\n    model.eval()\n    if cfg.enable_dropout:\n        model.enable_dropout_in_eval()\n    model = model.to(device)\n\n    # Run training / evaluation\n    if not os.path.exists(cfg.output_path):\n        os.makedirs(cfg.output_path)\n\n    run(model, dataset, device, cfg.output_path)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/find_env.py,7,"b'# This script outputs relevant system environment info\n# Run it with `python collect_env.py`.\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport locale\nimport re\nimport subprocess\nimport sys\nimport os\nfrom collections import namedtuple\nimport logging\n\nlog = logging.getLogger(__name__)\ntry:\n    import torch\n\n    TORCH_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError):\n    TORCH_AVAILABLE = False\n\nPY3 = sys.version_info >= (3, 0)\n\n# System Environment Information\nSystemEnv = namedtuple(\n    ""SystemEnv"",\n    [\n        ""torch_version"",\n        ""is_debug_build"",\n        ""cuda_compiled_version"",\n        ""gcc_version"",\n        ""cmake_version"",\n        ""os"",\n        ""python_version"",\n        ""is_cuda_available"",\n        ""cuda_runtime_version"",\n        ""nvidia_driver_version"",\n        ""nvidia_gpu_models"",\n        ""cudnn_version"",\n        ""pip_version"",  # \'pip\' or \'pip3\'\n        ""pip_packages"",\n        ""conda_packages"",\n    ],\n)\n\n\ndef run(command):\n    """"""Returns (return-code, stdout, stderr)""""""\n    p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True)\n    output, err = p.communicate()\n    rc = p.returncode\n    if PY3:\n        enc = locale.getpreferredencoding()\n        output = output.decode(enc)\n        err = err.decode(enc)\n    return rc, output.strip(), err.strip()\n\n\ndef run_and_read_all(run_lambda, command):\n    """"""Runs command using run_lambda; reads and returns entire output if rc is 0""""""\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    return out\n\n\ndef run_and_parse_first_match(run_lambda, command, regex):\n    """"""Runs command using run_lambda, returns the first regex match if it exists""""""\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    match = re.search(regex, out)\n    if match is None:\n        return None\n    return match.group(1)\n\n\ndef get_conda_packages(run_lambda):\n    if get_platform() == ""win32"":\n        grep_cmd = r\'findstr /R ""torch soumith mkl magma""\'\n    else:\n        grep_cmd = r\'grep ""torch\\|soumith\\|mkl\\|magma""\'\n    conda = os.environ.get(""CONDA_EXE"", ""conda"")\n    out = run_and_read_all(run_lambda, conda + "" list | "" + grep_cmd)\n    if out is None:\n        return out\n    # Comment starting at beginning of line\n    comment_regex = re.compile(r""^#.*\\n"")\n    return re.sub(comment_regex, """", out)\n\n\ndef get_gcc_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""gcc --version"", r""gcc (.*)"")\n\n\ndef get_cmake_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""cmake --version"", r""cmake (.*)"")\n\n\ndef get_nvidia_driver_version(run_lambda):\n    if get_platform() == ""darwin"":\n        cmd = ""kextstat | grep -i cuda""\n        return run_and_parse_first_match(run_lambda, cmd, r""com[.]nvidia[.]CUDA [(](.*?)[)]"")\n    smi = get_nvidia_smi()\n    return run_and_parse_first_match(run_lambda, smi, r""Driver Version: (.*?) "")\n\n\ndef get_gpu_info(run_lambda):\n    if get_platform() == ""darwin"":\n        if TORCH_AVAILABLE and torch.cuda.is_available():\n            return torch.cuda.get_device_name(None)\n        return None\n    smi = get_nvidia_smi()\n    uuid_regex = re.compile(r"" \\(UUID: .+?\\)"")\n    rc, out, _ = run_lambda(smi + "" -L"")\n    if rc != 0:\n        return None\n    # Anonymize GPUs by removing their UUID\n    return re.sub(uuid_regex, """", out)\n\n\ndef get_running_cuda_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""nvcc --version"", r""V(.*)$"")\n\n\ndef get_cudnn_version(run_lambda):\n    """"""This will return a list of libcudnn.so; it\'s hard to tell which one is being used""""""\n    if get_platform() == ""win32"":\n        cudnn_cmd = \'where /R ""%CUDA_PATH%\\\\bin"" cudnn*.dll\'\n    elif get_platform() == ""darwin"":\n        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n        cudnn_cmd = ""ls /usr/local/cuda/lib/libcudnn*""\n    else:\n        cudnn_cmd = \'ldconfig -p | grep libcudnn | rev | cut -d"" "" -f1 | rev\'\n    rc, out, _ = run_lambda(cudnn_cmd)\n    # find will return 1 if there are permission errors or if not found\n    if len(out) == 0 or (rc != 1 and rc != 0):\n        l = os.environ.get(""CUDNN_LIBRARY"")\n        if l is not None and os.path.isfile(l):\n            return os.path.realpath(l)\n        return None\n    files = set()\n    for fn in out.split(""\\n""):\n        fn = os.path.realpath(fn)  # eliminate symbolic links\n        if os.path.isfile(fn):\n            files.add(fn)\n    if not files:\n        return None\n    # Alphabetize the result because the order is non-deterministic otherwise\n    files = list(sorted(files))\n    if len(files) == 1:\n        return files[0]\n    result = ""\\n"".join(files)\n    return ""Probably one of the following:\\n{}"".format(result)\n\n\ndef get_nvidia_smi():\n    # Note: nvidia-smi is currently available only on Windows and Linux\n    smi = ""nvidia-smi""\n    if get_platform() == ""win32"":\n        smi = \'""C:\\\\Program Files\\\\NVIDIA Corporation\\\\NVSMI\\\\%s""\' % smi\n    return smi\n\n\ndef get_platform():\n    if sys.platform.startswith(""linux""):\n        return ""linux""\n    elif sys.platform.startswith(""win32""):\n        return ""win32""\n    elif sys.platform.startswith(""cygwin""):\n        return ""cygwin""\n    elif sys.platform.startswith(""darwin""):\n        return ""darwin""\n    else:\n        return sys.platform\n\n\ndef get_mac_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""sw_vers -productVersion"", r""(.*)"")\n\n\ndef get_windows_version(run_lambda):\n    return run_and_read_all(run_lambda, ""wmic os get Caption | findstr /v Caption"")\n\n\ndef get_lsb_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""lsb_release -a"", r""Description:\\t(.*)"")\n\n\ndef check_release_file(run_lambda):\n    return run_and_parse_first_match(run_lambda, ""cat /etc/*-release"", r\'PRETTY_NAME=""(.*)""\')\n\n\ndef get_os(run_lambda):\n    platform = get_platform()\n\n    if platform == ""win32"" or platform == ""cygwin"":\n        return get_windows_version(run_lambda)\n\n    if platform == ""darwin"":\n        version = get_mac_version(run_lambda)\n        if version is None:\n            return None\n        return ""Mac OSX {}"".format(version)\n\n    if platform == ""linux"":\n        # Ubuntu/Debian based\n        desc = get_lsb_version(run_lambda)\n        if desc is not None:\n            return desc\n\n        # Try reading /etc/*-release\n        desc = check_release_file(run_lambda)\n        if desc is not None:\n            return desc\n\n        return platform\n\n    # Unknown platform\n    return platform\n\n\ndef get_pip_packages(run_lambda):\n    # People generally have `pip` as `pip` or `pip3`\n    def run_with_pip(pip):\n        if get_platform() == ""win32"":\n            grep_cmd = r\'findstr /R ""numpy torch""\'\n        else:\n            grep_cmd = r\'grep ""torch\\|numpy""\'\n        return run_and_read_all(run_lambda, pip + "" list --format=freeze | "" + grep_cmd)\n\n    if not PY3:\n        return ""pip"", run_with_pip(""pip"")\n\n    # Try to figure out if the user is running pip or pip3.\n    out2 = run_with_pip(""pip"")\n    out3 = run_with_pip(""pip3"")\n\n    num_pips = len([x for x in [out2, out3] if x is not None])\n    if num_pips == 0:\n        return ""pip"", out2\n\n    if num_pips == 1:\n        if out2 is not None:\n            return ""pip"", out2\n        return ""pip3"", out3\n\n    # num_pips is 2. Return pip3 by default b/c that most likely\n    # is the one associated with Python 3\n    return ""pip3"", out3\n\n\ndef get_env_info():\n    run_lambda = run\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\n\n    if TORCH_AVAILABLE:\n        version_str = torch.__version__\n        debug_mode_str = torch.version.debug\n        cuda_available_str = torch.cuda.is_available()\n        cuda_version_str = torch.version.cuda\n    else:\n        version_str = debug_mode_str = cuda_available_str = cuda_version_str = ""N/A""\n\n    return SystemEnv(\n        torch_version=version_str,\n        is_debug_build=debug_mode_str,\n        python_version=""{}.{}"".format(sys.version_info[0], sys.version_info[1]),\n        is_cuda_available=cuda_available_str,\n        cuda_compiled_version=cuda_version_str,\n        cuda_runtime_version=get_running_cuda_version(run_lambda),\n        nvidia_gpu_models=get_gpu_info(run_lambda),\n        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n        cudnn_version=get_cudnn_version(run_lambda),\n        pip_version=pip_version,\n        pip_packages=pip_list_output,\n        conda_packages=get_conda_packages(run_lambda),\n        os=get_os(run_lambda),\n        gcc_version=get_gcc_version(run_lambda),\n        cmake_version=get_cmake_version(run_lambda),\n    )\n\n\nenv_info_fmt = """"""\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nCMake version: {cmake_version}\n\nPython version: {python_version}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n"""""".strip()\n\n\ndef pretty_str(envinfo):\n    def replace_nones(dct, replacement=""Could not collect""):\n        for key in dct.keys():\n            if dct[key] is not None:\n                continue\n            dct[key] = replacement\n        return dct\n\n    def replace_bools(dct, true=""Yes"", false=""No""):\n        for key in dct.keys():\n            if dct[key] is True:\n                dct[key] = true\n            elif dct[key] is False:\n                dct[key] = false\n        return dct\n\n    def prepend(text, tag=""[prepend]""):\n        lines = text.split(""\\n"")\n        updated_lines = [tag + line for line in lines]\n        return ""\\n"".join(updated_lines)\n\n    def replace_if_empty(text, replacement=""No relevant packages""):\n        if text is not None and len(text) == 0:\n            return replacement\n        return text\n\n    def maybe_start_on_next_line(string):\n        # If `string` is multiline, prepend a \\n to it.\n        if string is not None and len(string.split(""\\n"")) > 1:\n            return ""\\n{}\\n"".format(string)\n        return string\n\n    mutable_dict = envinfo._asdict()\n\n    # If nvidia_gpu_models is multiline, start on the next line\n    mutable_dict[""nvidia_gpu_models""] = maybe_start_on_next_line(envinfo.nvidia_gpu_models)\n\n    # If the machine doesn\'t have CUDA, report some fields as \'No CUDA\'\n    dynamic_cuda_fields = [\n        ""cuda_runtime_version"",\n        ""nvidia_gpu_models"",\n        ""nvidia_driver_version"",\n    ]\n    all_cuda_fields = dynamic_cuda_fields + [""cudnn_version""]\n    all_dynamic_cuda_fields_missing = all(mutable_dict[field] is None for field in dynamic_cuda_fields)\n    if TORCH_AVAILABLE and not torch.cuda.is_available() and all_dynamic_cuda_fields_missing:\n        for field in all_cuda_fields:\n            mutable_dict[field] = ""No CUDA""\n        if envinfo.cuda_compiled_version is None:\n            mutable_dict[""cuda_compiled_version""] = ""None""\n\n    # Replace True with Yes, False with No\n    mutable_dict = replace_bools(mutable_dict)\n\n    # Replace all None objects with \'Could not collect\'\n    mutable_dict = replace_nones(mutable_dict)\n\n    # If either of these are \'\', replace with \'No relevant packages\'\n    mutable_dict[""pip_packages""] = replace_if_empty(mutable_dict[""pip_packages""])\n    mutable_dict[""conda_packages""] = replace_if_empty(mutable_dict[""conda_packages""])\n\n    # Tag conda and pip packages with a prefix\n    # If they were previously None, they\'ll show up as ie \'[conda] Could not collect\'\n    if mutable_dict[""pip_packages""]:\n        mutable_dict[""pip_packages""] = prepend(mutable_dict[""pip_packages""], ""[{}] "".format(envinfo.pip_version))\n    if mutable_dict[""conda_packages""]:\n        mutable_dict[""conda_packages""] = prepend(mutable_dict[""conda_packages""], ""[conda] "")\n    return env_info_fmt.format(**mutable_dict)\n\n\ndef get_pretty_env_info():\n    return pretty_str(get_env_info())\n\n\ndef main():\n    print(""Collecting environment information..."")\n    output = get_pretty_env_info()\n    print(output)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/find_runs.py,1,"b'import os\nimport sys\nimport argparse\nfrom glob import glob\nfrom collections import defaultdict\nimport torch\nimport shutil\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.utils.colors import COLORS\n\n\ndef colored_print(color, msg):\n    print(color + msg + COLORS.END_NO_TOKEN)\n\n\nclass ExperimentFolder:\n    def __init__(self, run_path):\n        self._run_path = run_path\n        self._model_name = None\n        self._stats = None\n        self._find_files()\n\n    def _find_files(self):\n        self._files = os.listdir(self._run_path)\n\n    def __repr__(self):\n        return self._run_path.split(""outputs"")[1]\n\n    @property\n    def model_name(self):\n        return self._model_name\n\n    @property\n    def contains_trained_model(self):\n        if not hasattr(self, ""_contains_trained_model""):\n            for f in self._files:\n                if "".pt"" in f:\n                    self._contains_trained_model = True\n                    self._model_name = f\n                    return self._contains_trained_model\n            self._contains_trained_model = False\n            return self._contains_trained_model\n        else:\n            return self._contains_trained_model\n\n    def extract_stats(self):\n        path_to_checkpoint = os.path.join(self._run_path, self.model_name)\n        stats = torch.load(path_to_checkpoint)[""stats""]\n        self._stats = stats\n        num_epoch = len(stats[""train""])\n        stats_dict = defaultdict(dict)\n        for split_name in stats.keys():\n            if len(stats[split_name]) > 0:\n                latest_epoch = stats[split_name][-1]\n                for metric_name in latest_epoch.keys():\n                    if ""best"" in metric_name:\n                        stats_dict[metric_name][split_name] = latest_epoch[metric_name]\n        return num_epoch, stats_dict\n\n    @property\n    def stats(self):\n        return self._stats\n\n\ndef main(args):\n\n    experiment_with_models = defaultdict(list)\n    run_paths = glob(os.path.join(ROOT, ""outputs"", ""*"", ""*""))\n    for run_path in run_paths:\n        experiment = ExperimentFolder(run_path)\n        if experiment.contains_trained_model:\n            experiment_with_models[experiment.model_name].append(experiment)\n        else:\n            if args.d:\n                shutil.rmtree(run_path)\n\n    print("""")\n    for model_name in experiment_with_models.keys():\n        colored_print(COLORS.Green, str(model_name))\n        for experiment in experiment_with_models[model_name]:\n            print(experiment)\n            num_epoch, stats = experiment.extract_stats()\n            colored_print(COLORS.Red, ""Epoch: {}"".format(num_epoch))\n            for metric_name in stats:\n                sentence = """"\n                for split_name in stats[metric_name].keys():\n                    sentence += ""{}: {}, "".format(split_name, stats[metric_name][split_name])\n                metric_sentence = metric_name + ""({})"".format(sentence[:-2])\n                colored_print(COLORS.BBlue, metric_sentence)\n            print("""")\n        print("""")\n\n    if args.pdb:\n        import pdb\n\n        pdb.set_trace()\n\n\nif __name__ == ""__main__"":\n    parser = argparse.ArgumentParser(description=""Find experiments"")\n    parser.add_argument(""-d"", action=""store_true"", default=False, help=""Delete empty folders"")\n    parser.add_argument(""-pdb"", action=""store_true"", default=False, help=""Activate pdb for explore Experiment Folder"")\n    args = parser.parse_args()\n    main(args)\n'"
test/__init__.py,1,"b'import torch\n\n\ndef run_if_cuda(func):\n    def wrapped_func(*args, **kwargs):\n        if torch.cuda.is_available():\n            return func(*args, **kwargs)\n        else:\n            return\n\n    return wrapped_func\n'"
test/mock_models.py,0,"b'from torch_points3d.models.base_model import BaseModel\nfrom torch import nn\n\n\nclass MockModelConfig(object):\n    def __init__(self):\n        pass\n\n    def keys(self):\n        return []\n\n\nclass MockModel(BaseModel):\n    def __init__(self, opt):\n        super(MockModel, self).__init__(opt)\n\n    def set_input(self, data, device):\n        pass\n\n    def forward(self):\n        pass\n\n    def backward(self):\n        pass\n\n\nclass DifferentiableMockModel(BaseModel):\n    def __init__(self, opt):\n        super(DifferentiableMockModel, self).__init__(opt)\n\n        self.nn = nn.Linear(3, 3)\n\n    def set_input(self, data, device):\n        self.pos = data.pos\n\n    def forward(self):\n        self.output = self.nn(self.pos)\n        self.loss = self.output.sum()\n\n    def backward(self):\n        self.loss.backward()\n'"
test/mockdatasets.py,22,"b'import numpy as np\nimport torch\nfrom torch_geometric.data import Data, Batch\n\nfrom torch_points3d.datasets.batch import SimpleBatch\nfrom torch_points3d.core.data_transform import MultiScaleTransform\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.datasets.registration.pair import Pair, PairBatch, PairMultiScaleBatch, DensePairBatch\n\n\nclass MockDatasetConfig(object):\n    def __init__(self):\n        pass\n\n    def keys(self):\n        return []\n\n    def get(self, dataset_name, default):\n        return None\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, feature_size=0, transform=None, num_points=100, include_box=False, batch_size=2):\n        self.feature_dimension = feature_size\n        self.num_classes = 10\n        self.num_points = num_points\n        self.batch_size = batch_size\n        self.weight_classes = None\n        self.feature_size = feature_size\n        if feature_size > 0:\n            self._feature = torch.tensor([range(feature_size) for i in range(self.num_points)], dtype=torch.float,)\n        else:\n            self._feature = None\n        self._y = torch.randint(0, 10, (self.num_points,))\n        self._category = torch.ones((self.num_points,), dtype=torch.long)\n        self._ms_transform = None\n        self._transform = transform\n        self.mean_size_arr = np.array([1])\n        self.include_box = include_box\n\n    def __len__(self):\n        return self.num_points\n\n    def _generate_data(self):\n        data = Data(\n            pos=torch.randn((self.num_points, 3)),\n            x=torch.randn((self.num_points, self.feature_size)) if self.feature_size else None,\n            y=torch.randint(0, 10, (self.num_points,)),\n            category=self._category,\n        )\n        if self.include_box:\n            num_boxes = 10\n            data.center_label = torch.randn(num_boxes, 3)\n            data.heading_class_label = torch.zeros((num_boxes,))\n            data.heading_residual_label = torch.randn((num_boxes,))\n            data.size_class_label = torch.randint(0, 10, (num_boxes,))\n            data.size_residual_label = torch.randn(num_boxes, 3)\n            data.sem_cls_label = torch.randint(0, 10, (num_boxes,))\n            data.box_label_mask = torch.randint(0, 1, (num_boxes,)).bool()\n            data.vote_label = torch.randn(self.num_points, 9)\n            data.vote_label_mask = torch.randint(0, 1, (self.num_points,)).bool()\n        return data\n\n    @property\n    def datalist(self):\n        datalist = [self._generate_data() for i in range(self.batch_size)]\n        if self._transform:\n            datalist = [self._transform(d.clone()) for d in datalist]\n        if self._ms_transform:\n            datalist = [self._ms_transform(d.clone()) for d in datalist]\n        return datalist\n\n    def __getitem__(self, index):\n        return SimpleBatch.from_data_list(self.datalist)\n\n    @property\n    def class_to_segments(self):\n        return {""class1"": [0, 1, 2, 3, 4, 5], ""class2"": [6, 7, 8, 9]}\n\n    def set_strategies(self, model):\n        strategies = model.get_spatial_ops()\n        transform = MultiScaleTransform(strategies)\n        self._ms_transform = transform\n\n\nclass MockDatasetGeometric(MockDataset):\n    def __getitem__(self, index):\n        if self._ms_transform:\n            return MultiScaleBatch.from_data_list(self.datalist)\n        else:\n            return Batch.from_data_list(self.datalist)\n\n\nclass PairMockDataset(MockDataset):\n    def __init__(self, feature_size=0, transform=None, num_points=100, is_pair_ind=True, batch_size=2):\n        super(PairMockDataset, self).__init__(feature_size, transform, num_points, batch_size=batch_size)\n        if is_pair_ind:\n            self._pair_ind = torch.tensor([[0, 1], [1, 0]])\n        else:\n            self._pair_ind = None\n\n    @property\n    def datalist(self):\n        torch.manual_seed(0)\n        datalist_source = [\n            Data(\n                pos=torch.randn((self.num_points, 3)),\n                x=self._feature,\n                pair_ind=self._pair_ind,\n                size_pair_ind=torch.tensor([len(self._pair_ind)]),\n            )\n            for i in range(self.batch_size)\n        ]\n        datalist_target = [\n            Data(\n                pos=torch.randn((self.num_points, 3)),\n                x=self._feature,\n                pair_ind=self._pair_ind,\n                size_pair_ind=torch.tensor([len(self._pair_ind)]),\n            )\n            for i in range(self.batch_size)\n        ]\n        if self._transform:\n            datalist_source = [self._transform(d.clone()) for d in datalist_source]\n            datalist_target = [self._transform(d.clone()) for d in datalist_target]\n        if self._ms_transform:\n            datalist_source = [self._ms_transform(d.clone()) for d in datalist_source]\n            datalist_target = [self._ms_transform(d.clone()) for d in datalist_target]\n        datalist = [Pair.make_pair(datalist_source[i], datalist_target[i]) for i in range(self.batch_size)]\n        return datalist\n\n    def __getitem__(self, index):\n        return DensePairBatch.from_data_list(self.datalist)\n\n\nclass PairMockDatasetGeometric(PairMockDataset):\n    def __getitem__(self, index):\n\n        if self._ms_transform:\n            return PairMultiScaleBatch.from_data_list(self.datalist)\n        else:\n            return PairBatch.from_data_list(self.datalist)\n'"
test/test_api.py,1,"b'import os\nimport sys\nimport unittest\nimport torch\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\n\nfrom test.mockdatasets import MockDatasetGeometric, MockDataset\nfrom torch_points3d.core.data_transform import GridSampling3D\n\nseed = 0\ntorch.manual_seed(seed)\ndevice = ""cpu""\n\n\nclass TestAPIUnet(unittest.TestCase):\n    def test_kpconv(self):\n        from torch_points3d.applications.kpconv import KPConv\n\n        input_nc = 3\n        num_layers = 4\n        grid_sampling = 0.02\n        in_feat = 32\n        model = KPConv(\n            architecture=""unet"",\n            input_nc=input_nc,\n            in_feat=in_feat,\n            in_grid_size=grid_sampling,\n            num_layers=num_layers,\n            config=None,\n        )\n        dataset = MockDatasetGeometric(input_nc + 1, transform=GridSampling3D(0.01), num_points=128)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers + 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n        self.assertEqual(len(model._modules[""up_modules""]), 4)\n        self.assertFalse(model.has_mlp_head)\n        self.assertEqual(model.output_nc, in_feat)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], in_feat)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n        input_nc = 3\n        num_layers = 4\n        grid_sampling = 0.02\n        in_feat = 32\n        output_nc = 5\n        model = KPConv(\n            architecture=""unet"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            in_feat=in_feat,\n            in_grid_size=grid_sampling,\n            num_layers=num_layers,\n            config=None,\n        )\n        dataset = MockDatasetGeometric(input_nc + 1, transform=GridSampling3D(0.01), num_points=128)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers + 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n        self.assertEqual(len(model._modules[""up_modules""]), 4)\n        self.assertTrue(model.has_mlp_head)\n        self.assertEqual(model.output_nc, output_nc)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n    def test_pn2(self):\n        from torch_points3d.applications.pointnet2 import PointNet2\n\n        input_nc = 2\n        num_layers = 3\n        output_nc = 5\n        model = PointNet2(\n            architecture=""unet"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            num_layers=num_layers,\n            multiscale=True,\n            config=None,\n        )\n        dataset = MockDataset(input_nc, num_points=512)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers - 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n        self.assertEqual(len(model._modules[""up_modules""]), num_layers)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n    def test_rsconv(self):\n        from torch_points3d.applications.rsconv import RSConv\n\n        input_nc = 2\n        num_layers = 4\n        output_nc = 5\n        model = RSConv(\n            architecture=""unet"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            num_layers=num_layers,\n            multiscale=True,\n            config=None,\n        )\n        dataset = MockDataset(input_nc, num_points=1024)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers)\n        self.assertEqual(len(model._modules[""inner_modules""]), 2)\n        self.assertEqual(len(model._modules[""up_modules""]), num_layers)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n\nclass TestAPIEncoder(unittest.TestCase):\n    def test_kpconv(self):\n        from torch_points3d.applications.kpconv import KPConv\n\n        input_nc = 3\n        num_layers = 4\n        grid_sampling = 0.02\n        in_feat = 16\n        model = KPConv(\n            architecture=""encoder"",\n            input_nc=input_nc,\n            in_feat=in_feat,\n            in_grid_size=grid_sampling,\n            num_layers=num_layers,\n            config=None,\n        )\n        dataset = MockDatasetGeometric(input_nc + 1, transform=GridSampling3D(0.01), num_points=128)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers + 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n        self.assertFalse(model.has_mlp_head)\n        self.assertEqual(model.output_nc, 32 * in_feat)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], 32 * in_feat)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n        input_nc = 3\n        num_layers = 4\n        grid_sampling = 0.02\n        in_feat = 32\n        output_nc = 5\n        model = KPConv(\n            architecture=""encoder"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            in_feat=in_feat,\n            in_grid_size=grid_sampling,\n            num_layers=num_layers,\n            config=None,\n        )\n        dataset = MockDatasetGeometric(input_nc + 1, transform=GridSampling3D(0.01), num_points=128)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers + 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n        self.assertTrue(model.has_mlp_head)\n        self.assertEqual(model.output_nc, output_nc)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n    def test_pn2(self):\n        from torch_points3d.applications.pointnet2 import PointNet2\n\n        input_nc = 2\n        num_layers = 3\n        output_nc = 5\n        model = PointNet2(\n            architecture=""encoder"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            num_layers=num_layers,\n            multiscale=True,\n            config=None,\n        )\n        dataset = MockDataset(input_nc, num_points=512)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers - 1)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n    def test_rsconv(self):\n        from torch_points3d.applications.rsconv import RSConv\n\n        input_nc = 2\n        num_layers = 4\n        output_nc = 5\n        model = RSConv(\n            architecture=""encoder"",\n            input_nc=input_nc,\n            output_nc=output_nc,\n            num_layers=num_layers,\n            multiscale=True,\n            config=None,\n        )\n        dataset = MockDataset(input_nc, num_points=1024)\n        self.assertEqual(len(model._modules[""down_modules""]), num_layers)\n        self.assertEqual(len(model._modules[""inner_modules""]), 1)\n\n        try:\n            data_out = model.forward(dataset[0])\n            self.assertEqual(data_out.x.shape[1], output_nc)\n        except Exception as e:\n            print(""Model failing:"")\n            print(model)\n            raise e\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_basedataset.py,9,"b'import unittest\nfrom omegaconf import DictConfig, OmegaConf\nimport os\nimport sys\nfrom glob import glob\nimport torch\nimport numpy as np\nfrom torch_geometric.data.data import Data\nimport torch_geometric.transforms as T\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom test.mockdatasets import MockDataset, MockDatasetConfig\nfrom test.mock_models import MockModel, MockModelConfig\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.models.model_factory import instantiate_model\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\nfrom torch_points3d.utils.enums import ConvolutionFormat\nimport torch_points3d.core.data_transform as T3d\n\n\nclass Options:\n    def __init__(self):\n        pass\n\n    def get(self, key, default):\n        if hasattr(self, key):\n            return getattr(self, key, default)\n        else:\n            return default\n\n    def keys(self):\n        return self.__dict__.keys()\n\n\nclass CustomMockDataset:\n    def __init__(self, num_points, input_nc, output_nc, num_samples, conv_type=""dense""):\n        self.num_points = num_points\n        self.input_nc = input_nc\n        self.output_nc = output_nc\n        self.num_samples = num_samples\n        self.conv_type = conv_type\n\n    def __len__(self):\n        return self.num_samples\n\n    @property\n    def num_classes(self):\n        return self.output_nc\n\n    @property\n    def num_features(self):\n        return self.input_nc\n\n    def __getitem__(self, idx):\n        pos = torch.from_numpy(np.random.normal(0, 1, (self.num_points, 3)))\n        y = torch.from_numpy(np.random.normal(0, 1, (self.num_points, self.output_nc)))\n        x = torch.from_numpy(np.random.normal(0, 1, (self.num_points, self.input_nc)))\n        return Data(x=x, pos=pos, y=y)\n\n\nclass MockBaseDataset(BaseDataset):\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n        self._data_path = dataset_opt.dataroot\n        self.train_dataset = MockDataset()\n        self.val_dataset = MockDataset()\n\n\nclass TestDataset(unittest.TestCase):\n    def test_empty_dataset(self):\n        opt = Options()\n        opt.dataset_name = os.path.join(os.getcwd(), ""test"")\n        opt.dataroot = os.path.join(os.getcwd(), ""test"")\n        opt.pre_transform = [DictConfig({""transform"": ""RandomNoise""})]\n        opt.test_transform = [DictConfig({""transform"": ""AddOnes""})]\n        opt.val_transform = [DictConfig({""transform"": ""Jitter""})]\n        opt.train_transform = [DictConfig({""transform"": ""RandomSymmetry""})]\n        dataset = BaseDataset(opt)\n\n        self.assertEqual(str(dataset.pre_transform), str(T.Compose([T3d.RandomNoise()])))\n        self.assertEqual(str(dataset.test_transform), str(T.Compose([T3d.AddOnes()])))\n        self.assertEqual(str(dataset.train_transform), str(T.Compose([T3d.RandomSymmetry()])))\n        self.assertEqual(str(dataset.val_transform), str(T.Compose([T3d.Jitter()])))\n        self.assertEqual(str(dataset.inference_transform), str(T.Compose([T3d.RandomNoise(), T3d.AddOnes()])))\n        self.assertEqual(dataset.train_dataset, None)\n        self.assertEqual(dataset.test_dataset, None)\n        self.assertEqual(dataset.val_dataset, None)\n\n    def test_simple_datasets(self):\n        opt = Options()\n        opt.dataset_name = os.path.join(os.getcwd(), ""test"")\n        opt.dataroot = os.path.join(os.getcwd(), ""test"")\n\n        class SimpleDataset(BaseDataset):\n            def __init__(self, dataset_opt):\n                super(SimpleDataset, self).__init__(dataset_opt)\n\n                self.train_dataset = CustomMockDataset(10, 1, 3, 10)\n                self.test_dataset = CustomMockDataset(10, 1, 3, 10)\n\n        dataset = SimpleDataset(opt)\n\n        model_config = MockModelConfig()\n        model_config.conv_type = ""dense""\n        model = MockModel(model_config)\n        dataset.create_dataloaders(model, 5, True, 0, False)\n\n        self.assertEqual(dataset.pre_transform, None)\n        self.assertEqual(dataset.test_transform, None)\n        self.assertEqual(dataset.train_transform, None)\n        self.assertEqual(dataset.val_transform, None)\n        self.assertNotEqual(dataset.train_dataset, None)\n        self.assertNotEqual(dataset.test_dataset, None)\n        self.assertTrue(dataset.has_test_loaders)\n        self.assertFalse(dataset.has_val_loader)\n\n    def test_multiple_test_datasets(self):\n        opt = Options()\n        opt.dataset_name = os.path.join(os.getcwd(), ""test"")\n        opt.dataroot = os.path.join(os.getcwd(), ""test"")\n\n        class MultiTestDataset(BaseDataset):\n            def __init__(self, dataset_opt):\n                super(MultiTestDataset, self).__init__(dataset_opt)\n\n                self.train_dataset = CustomMockDataset(10, 1, 3, 10)\n                self.val_dataset = CustomMockDataset(10, 1, 3, 10)\n                self.test_dataset = [CustomMockDataset(10, 1, 3, 10), CustomMockDataset(10, 1, 3, 20)]\n\n        dataset = MultiTestDataset(opt)\n\n        model_config = MockModelConfig()\n        model_config.conv_type = ""dense""\n        model = MockModel(model_config)\n        dataset.create_dataloaders(model, 5, True, 0, False)\n\n        loaders = dataset.test_dataloaders\n        self.assertEqual(len(loaders), 2)\n        self.assertEqual(len(loaders[0].dataset), 10)\n        self.assertEqual(len(loaders[1].dataset), 20)\n        self.assertEqual(dataset.num_classes, 3)\n        self.assertEqual(dataset.is_hierarchical, False)\n        self.assertEqual(dataset.has_fixed_points_transform, False)\n        self.assertEqual(dataset.has_val_loader, True)\n        self.assertEqual(dataset.class_to_segments, None)\n        self.assertEqual(dataset.feature_dimension, 1)\n\n        batch = next(iter(loaders[0]))\n        num_samples = BaseDataset.get_num_samples(batch, ""dense"")\n        self.assertEqual(num_samples, 5)\n\n        sample = BaseDataset.get_sample(batch, ""pos"", 1, ""dense"")\n        self.assertEqual(sample.shape, (10, 3))\n        sample = BaseDataset.get_sample(batch, ""x"", 1, ""dense"")\n        self.assertEqual(sample.shape, (10, 1))\n        self.assertEqual(dataset.num_batches, {""train"": 2, ""val"": 2, ""test_0"": 2, ""test_1"": 4})\n\n        repr = ""Dataset: MultiTestDataset \\n\\x1b[0;95mpre_transform \\x1b[0m= None\\n\\x1b[0;95mtest_transform \\x1b[0m= None\\n\\x1b[0;95mtrain_transform \\x1b[0m= None\\n\\x1b[0;95mval_transform \\x1b[0m= None\\n\\x1b[0;95minference_transform \\x1b[0m= None\\nSize of \\x1b[0;95mtrain_dataset \\x1b[0m= 10\\nSize of \\x1b[0;95mtest_dataset \\x1b[0m= 10, 20\\nSize of \\x1b[0;95mval_dataset \\x1b[0m= 10\\n\\x1b[0;95mBatch size =\\x1b[0m 5""\n        self.assertEqual(dataset.__repr__(), repr)\n\n    def test_normal(self):\n        dataset_opt = MockDatasetConfig()\n        setattr(dataset_opt, ""dataroot"", os.path.join(DIR, ""temp_dataset""))\n\n        mock_base_dataset = MockBaseDataset(dataset_opt)\n        mock_base_dataset.test_dataset = MockDataset()\n        model_config = MockModelConfig()\n        setattr(model_config, ""conv_type"", ""dense"")\n        model = MockModel(model_config)\n\n        mock_base_dataset.create_dataloaders(model, 2, True, 0, False)\n        datasets = mock_base_dataset.test_dataloaders\n\n        self.assertEqual(len(datasets), 1)\n\n    def test_get_by_name(self):\n        dataset_opt = MockDatasetConfig()\n        setattr(dataset_opt, ""dataroot"", os.path.join(DIR, ""temp_dataset""))\n\n        mock_base_dataset = MockBaseDataset(dataset_opt)\n        mock_base_dataset.test_dataset = [MockDataset(), MockDataset()]\n        mock_base_dataset.train_dataset = MockDataset()\n        mock_base_dataset.val_dataset = MockDataset()\n\n        for name in [""train"", ""val"", ""test_0"", ""test_1""]:\n            self.assertEqual(mock_base_dataset.get_dataset(name).name, name)\n\n        test_with_name = MockDataset()\n        setattr(test_with_name, ""name"", ""testos"")\n        mock_base_dataset.test_dataset = test_with_name\n        with self.assertRaises(ValueError):\n            mock_base_dataset.get_dataset(""test_1"")\n        mock_base_dataset.get_dataset(""testos"")\n\n        with self.assertRaises(ValueError):\n            mock_base_dataset.test_dataset = [test_with_name, test_with_name]\n\n\nclass TestBatchCollate(unittest.TestCase):\n    def test_num_batches(self):\n        data = Data(pos=torch.randn((2, 3, 3)))\n        self.assertEqual(MockBaseDataset.get_num_samples(data, ConvolutionFormat.DENSE.value), 2)\n\n        data = Data(pos=torch.randn((3, 3)), batch=torch.tensor([0, 1, 2]))\n        self.assertEqual(MockBaseDataset.get_num_samples(data, ConvolutionFormat.PARTIAL_DENSE.value), 3)\n\n    def test_get_sample(self):\n        data = Data(pos=torch.randn((2, 3, 3)))\n        torch.testing.assert_allclose(\n            MockBaseDataset.get_sample(data, ""pos"", 1, ConvolutionFormat.DENSE.value), data.pos[1]\n        )\n\n        data = Data(pos=torch.randn((3, 3)), batch=torch.tensor([0, 1, 2]))\n        torch.testing.assert_allclose(\n            MockBaseDataset.get_sample(data, ""pos"", 1, ConvolutionFormat.PARTIAL_DENSE.value), data.pos[1]\n        )\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_basemodel.py,1,"b'import unittest\nfrom omegaconf import OmegaConf, DictConfig\nfrom torch.nn import (\n    Sequential,\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nimport os\nimport sys\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.models.base_model import BaseModel\nfrom test.mockdatasets import MockDatasetGeometric\nfrom torch_points3d.models.model_factory import instantiate_model\n\n\ndef load_model_config(task, model_type, model_name):\n    models_conf = os.path.join(ROOT, ""conf/models/{}/{}.yaml"".format(task, model_type))\n    config = OmegaConf.load(models_conf)\n    config.update(""model_name"", model_name)\n    config.update(""data.task"", task)\n    return config\n\n\ndef MLP(channels):\n    return Sequential(\n        *[Sequential(Lin(channels[i - 1], channels[i]), Dropout(0.5), BN(channels[i])) for i in range(1, len(channels))]\n    )\n\n\nclass MockModel(BaseModel):\n    def __init__(self):\n        super(MockModel, self).__init__(DictConfig({""conv_type"": ""Dummy""}))\n\n        self._channels = [12, 12, 12, 12]\n        self.nn = MLP(self._channels)\n\n    def set_input(self, a):\n        self.input = a\n\n\nclass TestSimpleBatch(unittest.TestCase):\n    def test_enable_dropout_eval(self):\n        model = MockModel()\n        model.eval()\n\n        for i in range(len(model._channels) - 1):\n            self.assertEqual(model.nn[i][1].training, False)\n            self.assertEqual(model.nn[i][2].training, False)\n\n        model.enable_dropout_in_eval()\n        for i in range(len(model._channels) - 1):\n            self.assertEqual(model.nn[i][1].training, True)\n            self.assertEqual(model.nn[i][2].training, False)\n\n    def test_accumulated_gradient(self):\n        params = load_model_config(""segmentation"", ""pointnet2"", ""pointnet2ms"")\n        config_training = OmegaConf.load(os.path.join(DIR, ""test_config/training_config.yaml""))\n        dataset = MockDatasetGeometric(5)\n        model = instantiate_model(params, dataset)\n        model.instantiate_optimizers(config_training)\n        model.set_input(dataset[0], ""cpu"")\n        expected_make_optimizer_step = [False, False, True, False, False, True, False, False, True, False]\n        expected_contains_grads = [False, True, True, False, True, True, False, True, True, False]\n        make_optimizer_steps = []\n        contains_grads = []\n        for epoch in range(10):\n            model.forward()\n\n            make_optimizer_step = model._manage_optimizer_zero_grad()  # Accumulate gradient if option is up\n            make_optimizer_steps.append(make_optimizer_step)\n            grad_ = model._modules[""lin1""].weight.grad\n            if grad_ is not None:\n                contains_grads.append((grad_.sum() != 0).item())\n            else:\n                contains_grads.append(False)\n\n            model.backward()  # calculate gradients\n\n            if make_optimizer_step:\n                model._optimizer.step()  # update parameters\n\n        self.assertEqual(contains_grads, expected_contains_grads)\n        self.assertEqual(make_optimizer_steps, expected_make_optimizer_step)\n\n\nclass TestBaseModel(unittest.TestCase):\n    def test_getinput(self):\n        model = MockModel()\n        with self.assertRaises(AttributeError):\n            model.get_input()\n\n        model.set_input(1)\n        self.assertEqual(model.get_input(), 1)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_batch.py,2,"b'import unittest\nimport torch\nfrom torch_geometric.data import Data\n\nimport os\nimport sys\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.datasets.batch import SimpleBatch\n\n\nclass TestSimpleBatch(unittest.TestCase):\n    def test_fromlist(self):\n        nb_points = 100\n        pos = torch.randn((nb_points, 3))\n        y = torch.tensor([range(10) for i in range(pos.shape[0])], dtype=torch.float)\n        d = Data(pos=pos, y=y)\n\n        b = SimpleBatch.from_data_list([d, d])\n        self.assertEqual(b.pos.size(), (2, 100, 3))\n        self.assertEqual(b.y.size(), (2, 100, 10))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_bn_scheduler.py,0,"b'import unittest\nfrom omegaconf import OmegaConf\n\nimport os\nimport sys\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.core.schedulers import instantiate_bn_scheduler\nfrom torch_points3d.core.common_modules import MLP\nfrom torch_points3d.core.common_modules.dense_modules import MLP2D\n\n\nclass TestBNMomentumScheduler(unittest.TestCase):\n    def test_scheduler(self):\n        bn_scheduler_config = OmegaConf.load(os.path.join(DIR, ""test_config/bn_scheduler_config.yaml""))\n        bn_momentum = bn_scheduler_config.bn_scheduler.params.bn_momentum\n        bn_scheduler_params = bn_scheduler_config.bn_scheduler.params\n        bn_lambda = lambda e: max(\n            bn_scheduler_params.bn_momentum\n            * bn_scheduler_params.bn_decay ** (int(e // bn_scheduler_params.decay_step)),\n            bn_scheduler_params.bn_clip,\n        )\n        model = MLP([3, 3, 3], bn_momentum=10)\n        bn_scheduler = instantiate_bn_scheduler(model, bn_scheduler_config.bn_scheduler)\n        self.assertEqual(model[0][1].batch_norm.momentum, bn_momentum)\n        for epoch in range(100):\n            bn_scheduler.step(epoch)\n            self.assertEqual(model[0][1].batch_norm.momentum, bn_lambda(epoch))\n\n        model = MLP2D([3, 3, 3], bn=True)\n        bn_scheduler = instantiate_bn_scheduler(model, bn_scheduler_config.bn_scheduler)\n        self.assertEqual(model[0][1].momentum, bn_momentum)\n        for epoch in range(100):\n            bn_scheduler.step(epoch)\n            self.assertEqual(model[0][1].momentum, bn_lambda(epoch))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_confusionMatrix.py,0,"b'import os\nimport sys\nimport numpy as np\nimport unittest\nimport tempfile\nimport h5py\nimport numpy.testing as npt\nimport numpy.matlib\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\n\n\nclass TestConfusionMatrix(unittest.TestCase):\n    def setUp(self):\n        matrix = np.asarray([[4, 1], [2, 10]])\n        self._confusion = ConfusionMatrix.create_from_matrix(matrix)\n\n    def test_getCount(self):\n        self.assertEqual(self._confusion.get_count(0, 0), 4)\n\n    def test_getIoU(self):\n        iou = self._confusion.get_intersection_union_per_class()[0]\n        self.assertAlmostEqual(iou[0], 4 / (4.0 + 1.0 + 2.0))\n        self.assertAlmostEqual(iou[1], 10 / (10.0 + 1.0 + 2.0))\n\n    def test_getIoUMissing(self):\n        matrix = np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 0]])\n        confusion = ConfusionMatrix.create_from_matrix(matrix)\n        iou, mask = confusion.get_intersection_union_per_class()\n        self.assertAlmostEqual(iou[0], 1)\n        self.assertAlmostEqual(iou[1], 1)\n        self.assertAlmostEqual(iou[2], 0)\n        npt.assert_array_equal(mask, np.array([True, True, False]))\n\n    def test_getMeanIoU(self):\n        iou = self._confusion.get_intersection_union_per_class()[0]\n        self.assertAlmostEqual(iou[0], 4 / (4.0 + 1.0 + 2.0))\n        self.assertAlmostEqual(iou[1], 10 / (10.0 + 1.0 + 2.0))\n\n    def test_test_getMeanIoUMissing(self):\n        matrix = np.asarray([[1, 1, 0], [0, 1, 0], [0, 0, 0]])\n        confusion = ConfusionMatrix.create_from_matrix(matrix)\n        self.assertAlmostEqual(confusion.get_average_intersection_union(missing_as_one=False), (0.5 + 0.5) / 2)\n        self.assertAlmostEqual(confusion.get_average_intersection_union(missing_as_one=True), (0.5 + 1 + 0.5) / 3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_dataset_factory.py,0,"b'import os\nimport sys\nimport unittest\nimport numpy as np\nfrom omegaconf import OmegaConf\n\nimport torch\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)))\nsys.path.append(ROOT)\n\nfrom torch_points3d.datasets.segmentation.shapenet import ShapeNetDataset\nfrom torch_points3d.datasets.dataset_factory import get_dataset_class\n\ndef load_dataconfig(task, dataset):\n    data_conf = os.path.join(ROOT, "".."", ""conf/data/{}/{}.yaml"".format(task, dataset))\n    config = OmegaConf.load(data_conf)\n    config.update(""data.task"", ""segmentation"")\n    config.update(""data.dataroot"", ""data"")\n    return config\n\nclass TestBaseFactory(unittest.TestCase):\n    def test_simple(self):\n        data_config = load_dataconfig(""segmentation"", ""shapenet"")\n        dataset_cls = get_dataset_class(data_config.data)\n        self.assertEqual(""<class \'torch_points3d.datasets.segmentation.shapenet.ShapeNetDataset\'>"", str(dataset_cls))\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_filter.py,9,"b'import unittest\nimport sys\nimport os\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom torch_points3d.core.data_transform import FCompose, PlanarityFilter, euler_angles_to_rotation_matrix\n\n\nclass TestFilter(unittest.TestCase):\n    def test_planarity_filter(self):\n\n        # Plane with high planarity\n        U = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        plane = torch.rand(1000, 3) @ U @ torch.diag(torch.tensor([1, 1, 0.001])) @ U\n        data1 = Data(pos=plane)\n        # random isotropic gaussian\n        data2 = Data(pos=torch.randn(100, 3))\n        plane_filter = PlanarityFilter(0.3)\n        self.assertTrue(plane_filter(data2).item())\n        self.assertFalse(plane_filter(data1).item())\n\n    def test_composition(self):\n\n        U_1 = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        U_2 = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        U_3 = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        p_1 = torch.rand(1000, 3) @ U_1.T @ torch.diag(torch.tensor([1, 0.7, 0.5])) @ U_1\n        p_2 = torch.rand(1000, 3) @ U_2.T @ torch.diag(torch.tensor([1, 0.9, 0.001])) @ U_2\n        p_3 = torch.rand(1000, 3) @ U_3.T @ torch.diag(torch.tensor([1, 0.0001, 0.000001])) @ U_3\n\n        data_1 = Data(pos=p_1)\n        data_2 = Data(pos=p_2)\n        data_3 = Data(pos=p_3)\n\n        compose_filter = FCompose([PlanarityFilter(0.5, is_leq=True), PlanarityFilter(0.1, is_leq=False)])\n\n        self.assertTrue(compose_filter(data_1).item())\n        self.assertFalse(compose_filter(data_2).item())\n        self.assertFalse(compose_filter(data_3).item())\n'"
test/test_fps.py,2,"b'import torch\nimport numpy as np\nfrom torch_geometric.nn import fps\nimport unittest\nimport logging\nimport os\nimport sys\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom test import run_if_cuda\n\nlog = logging.getLogger(__name__)\n\n\nclass TestPytorchClusterFPS(unittest.TestCase):\n    @run_if_cuda\n    def test_simple(self):\n        num_points = 2048\n        pos = torch.randn((num_points, 3)).cuda()\n        batch = torch.zeros((num_points)).cuda().long()\n        idx = fps(pos, batch, 0.25)\n\n        idx = idx.detach().cpu().numpy()\n\n        cnd_1 = np.sum(idx) > 0\n        cnd_2 = np.sum(idx) < num_points * idx.shape[0]\n\n        assert (\n            cnd_1 and cnd_2\n        ), ""Your Pytorch Cluster FPS doesn\'t seem to return the correct value. It shouldn\'t be used to perform sampling""\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_grid_sampling.py,7,"b'import os\nimport sys\nimport numpy as np\nimport torch\nimport unittest\nimport h5py\nimport numpy.testing as npt\nimport numpy.matlib\nfrom torch_geometric.data import Data\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nimport torch_points3d.core.data_transform as cT\n\n\nclass TestGridSampling3D(unittest.TestCase):\n    def setUp(self):\n        self.sampler = cT.GridSampling3D(0.04)\n        num_points = 5\n        pos = torch.from_numpy(np.array([[0, 0, 0.01], [0.01, 0, 0], [0, 0.01, 0], [0, 0.01, 0], [0.01, 0, 0.01]]))\n        batch = torch.from_numpy(np.zeros(num_points)).long()\n        y = np.asarray(np.random.randint(0, 2, 5))\n        uniq, counts = np.unique(y, return_counts=True)\n        self.answer = uniq[np.argmax(counts)]\n        y = torch.from_numpy(y)\n        self.data = Data(pos=pos, batch=batch, y=y)\n\n    def test_simple(self):\n        """"""\n        This test verifies that the class output is correct and corresponds to the maximun vote from sub_part\n        """"""\n        out = self.sampler(self.data)\n        y = out.y.detach().cpu().numpy()\n        npt.assert_array_almost_equal(self.answer, y)\n\n    def loop(self, data, gr, sparse, msg):\n        shapes = []\n        u = data.clone()\n        for i in range(2):\n            u = gr(u)\n            shapes.append(u.pos.shape[0])\n\n        data = sparse(u)\n        num_points = np.unique(data.pos, axis=0).shape[0]\n\n        shapes.append(num_points)\n        self.assertEqual(shapes, [shapes[0] for _ in range(len(shapes))])\n\n    def test_double_grid_sampling(self):\n        data_random = Data(pos=torch.randn(1000, 3) * 0.1, x=torch.ones((1000, 1)))\n        data_fragment = torch.load(os.path.join(DIR_PATH, ""test_data/fragment_000003.pt""))\n\n        sparse = cT.ToSparseInput(0.02)\n        gr = cT.GridSampling3D(0.02)\n\n        self.loop(data_random, gr, sparse, ""random"")\n        self.loop(data_fragment, gr, sparse, ""fragment"")\n\n    def test_quantize(self):\n        data_random = Data(pos=torch.randn(100, 3) * 0.1, x=torch.ones((100, 1)))\n        gr = cT.GridSampling3D(0.2, quantize_coords=True)\n        quantized = gr(data_random)\n        self.assertEqual(quantized.x.shape[0], quantized.pos.shape[0])\n        self.assertEqual(quantized.num_nodes, quantized.pos.shape[0])\n        self.assertEqual(quantized.pos.dtype, torch.int)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_ind_tracker.py,5,"b'import os\nimport sys\nimport numpy as np\nimport torch\nimport unittest\nimport h5py\nimport numpy.testing as npt\nimport numpy.matlib\nfrom torch_geometric.data import Data\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom torch_points3d.datasets.registration.utils import tracked_matches\n\n\nclass TestTrackedMatches(unittest.TestCase):\n    def test_simple(self):\n\n        ind_source = torch.tensor([1, 2, 5])\n        ind_target = torch.tensor([0, 5, 6])\n        data_s = Data(pos=torch.randn(3, 3), origin_id=ind_source)\n        data_t = Data(pos=torch.randn(3, 3), origin_id=ind_target)\n        pair = torch.tensor([[0, 2], [1, 3], [2, 0], [3, 1]])\n\n        res = tracked_matches(data_s, data_t, pair)\n        expected = np.array([[1, 0]])\n        npt.assert_array_almost_equal(res.detach().cpu().numpy(), expected)\n'"
test/test_interpolateop.py,6,"b'import unittest\nimport torch\nfrom torch_geometric.data import Data\n\nfrom torch_points3d.core.spatial_ops import KNNInterpolate\nfrom torch_points3d.core.data_transform import GridSampling3D\n\n\nclass TestInterpolate(unittest.TestCase):\n    def test_precompute(self):\n        pos = torch.tensor([[1, 0, 0], [0, 1, 0], [1, 1, 0], [0, 0, 0], [0.1, 0, 0]])\n        x = torch.tensor([0, 0, 0, 0, 1]).unsqueeze(-1)\n        support = Data(x=x, pos=pos)\n\n        query = GridSampling3D(1)(support.clone())\n\n        interpolate = KNNInterpolate(1)\n        up = interpolate.precompute(query, support)\n        self.assertEqual(up.num_nodes, 5)\n        self.assertEqual(up.x_idx[4], up.x_idx[3])\n\n    def test_compute(self):\n        npoints = 100\n        pos = torch.randn((npoints, 3))\n        x = torch.randn((npoints, 4))\n        support = Data(x=x, pos=pos)\n        query = Data(x=torch.randn((npoints // 2, 4)), pos=torch.randn((npoints // 2, 3)))\n\n        interpolate = KNNInterpolate(3)\n        precomputed = interpolate.precompute(query, support)\n\n        gt = interpolate(query, support)\n        pre = interpolate(query, support, precomputed=precomputed)\n\n        torch.testing.assert_allclose(gt, pre)\n'"
test/test_kpconv.py,5,"b'import os\nimport sys\nimport unittest\nimport numpy as np\nimport numpy.testing as npt\nimport torch\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.modules.KPConv.losses import repulsion_loss, fitting_loss, permissive_loss\n\n\nclass TestKPConvLosses(unittest.TestCase):\n    def test_permissive_loss(self):\n        pos_n = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0]]).astype(np.float)\n        pos_t = torch.from_numpy(pos_n)\n        loss = permissive_loss(pos_t, 1).item()\n        assert loss == np.sqrt(2)\n\n    def test_fitting_loss(self):\n        pos_n = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0]]).astype(np.float)\n        target = np.asarray([[0.5, 0.5, 0]])\n        K_points = torch.from_numpy(pos_n)\n        neighbors = torch.from_numpy(target)\n        neighbors = neighbors\n        neighbors = neighbors.repeat([4, 1])\n        differences = neighbors - K_points\n        sq_distances = torch.sum(differences ** 2, dim=-1).unsqueeze(0)\n        loss = fitting_loss(sq_distances, 1).item()\n        assert loss == 0.5\n\n    def test_repulsion_loss(self):\n        pos_n = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [1, 1, 0]]).astype(np.float64)\n        K_points = torch.from_numpy(pos_n)\n        loss = repulsion_loss(K_points.unsqueeze(0), 1).item()\n        arr_ = np.asarray([0.25, 0.25, 0.0074]).astype(np.float64)\n        # Pytorch losses precision from decimal 4\n        npt.assert_almost_equal(loss, 4 * np.sum(arr_), decimal=3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_losses.py,7,"b'import unittest\nimport torch\nimport os\nimport sys\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\nfrom torch_points3d.core.losses.dirichlet_loss import (\n    _variance_estimator_dense,\n    dirichlet_loss,\n    _variance_estimator_sparse,\n)\n\n\nclass TestDirichletLoss(unittest.TestCase):\n    def test_loss_dense(self):\n        pos = torch.tensor([[[0, 0, 0], [1, 0, 0], [1.1, 0, 0]]], dtype=torch.float)\n        f = torch.tensor([[1, 1, 3]], dtype=torch.float)\n\n        var = _variance_estimator_dense(1.01, pos, f)\n        torch.testing.assert_allclose(var, [[0, 4, 4]])\n\n        loss = dirichlet_loss(1.01, pos, f)\n        self.assertAlmostEqual(loss.item(), 4 / 3.0)\n\n    def test_loss_sparse(self):\n        pos = torch.tensor([[0, 0, 0], [1, 0, 0], [1.1, 0, 0], [0, 0, 0], [1, 0, 0], [1.1, 0, 0]], dtype=torch.float)\n        f = torch.tensor([1, 1, 3, 0, 1, 0], dtype=torch.float)\n        batch_idx = torch.tensor([0, 0, 0, 1, 1, 1])\n\n        var = _variance_estimator_sparse(1.01, pos, f, batch_idx)\n        torch.testing.assert_allclose(var, [0, 4, 4, 1, 2, 1])\n\n        loss = dirichlet_loss(1.01, pos, f, batch_idx)\n        self.assertAlmostEqual(loss.item(), sum([0, 4, 4, 1, 2, 1]) / (2 * 6))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_lr_scheduler.py,6,"b'import os\nimport sys\nfrom omegaconf import OmegaConf\nimport torch\nimport unittest\nimport logging\nfrom torch_geometric.data import Data\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.models.base_model import BaseModel\nfrom mock_models import DifferentiableMockModel\n\nlog = logging.getLogger(__name__)\n\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[""lr""]\n\n\nclass TestLrScheduler(unittest.TestCase):\n    def test_update_scheduler_on_epoch(self):\n\n        base_lr = 0.1\n        gamma = 0.9\n        conf = os.path.join(DIR, ""test_config/lr_scheduler.yaml"")\n        opt = OmegaConf.load(conf)\n        opt.update_lr_scheduler_on = ""on_epoch""\n        model = DifferentiableMockModel(opt)\n        model.instantiate_optimizers(opt)\n        model.schedulers.__repr__()\n\n        data = Data(pos=torch.randn((1, 3)))\n        model.set_input(data, torch.device(""cpu""))\n\n        num_epochs = 5\n        num_samples_epoch = 32\n        batch_size = 4\n        steps = num_samples_epoch // batch_size\n\n        for epoch in range(num_epochs):\n            for step in range(steps):\n                model.optimize_parameters(epoch, batch_size)\n        self.assertEqual(get_lr(model._optimizer), base_lr * gamma ** (num_epochs - 1))\n\n    def test_update_scheduler_on_sample(self):\n        base_lr = 0.1\n        gamma = 0.9\n        conf = os.path.join(DIR, ""test_config/lr_scheduler.yaml"")\n        opt = OmegaConf.load(conf)\n        opt.update_lr_scheduler_on = ""on_num_sample""\n        model = DifferentiableMockModel(opt)\n        model.instantiate_optimizers(opt)\n\n        data = Data(pos=torch.randn((1, 3)))\n        model.set_input(data, torch.device(""cpu""))\n\n        num_epochs = 5\n        num_samples_epoch = 32\n        batch_size = 4\n        steps = num_samples_epoch // batch_size\n\n        for epoch in range(num_epochs):\n            for step in range(steps):\n                model.optimize_parameters(epoch, batch_size)\n\n        self.assertEqual(get_lr(model._optimizer), base_lr * gamma ** (num_epochs - 1))\n\n    def test_update_scheduler_on_batch(self):\n        base_lr = 0.1\n        gamma = 0.9\n        conf = os.path.join(DIR, ""test_config/lr_scheduler.yaml"")\n        opt = OmegaConf.load(conf)\n        opt.update_lr_scheduler_on = ""on_num_batch""\n        model = DifferentiableMockModel(opt)\n        model.instantiate_optimizers(opt)\n\n        data = Data(pos=torch.randn((1, 3)))\n        model.set_input(data, torch.device(""cpu""))\n\n        num_epochs = 5\n        num_samples_epoch = 32\n        batch_size = 4\n        steps = num_samples_epoch // batch_size\n\n        count_batch = 0\n        for epoch in range(num_epochs):\n            for step in range(steps):\n                count_batch += 1\n                model.optimize_parameters(epoch, batch_size)\n        self.assertEqual(get_lr(model._optimizer), base_lr * gamma ** (count_batch))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_make_pair.py,8,"b'import unittest\nimport torch\nfrom torch_points3d.datasets.registration.pair import Pair\n\n\nclass TestMakePair(unittest.TestCase):\n    def simple_test(self):\n        nb_points_1 = 101\n        data_source = Pair(\n            pos=torch.randn((nb_points_1, 3)),\n            x=torch.randn((nb_points_1, 9)),\n            norm=torch.randn((nb_points_1, 3)),\n            random_feat=torch.randn((nb_points_1, 15)),\n        )\n        nb_points_2 = 105\n        data_target = Pair(\n            pos=torch.randn((nb_points_2, 3)),\n            x=torch.randn((nb_points_2, 9)),\n            norm=torch.randn((nb_points_2, 3)),\n            random_feat=torch.randn((nb_points_2, 15)),\n        )\n\n        b = Pair.make_pair(data_source, data_target)\n        self.assertEqual(b.pos.size(), (nb_points_1 + nb_points_2, 3))\n        self.assertEqual(b.x.size(), (nb_points_1 + nb_points_2, 9))\n        print(""pair:"", b.pair)\n        assert getattr(b, ""pair"", None) is not None\n        self.assertEqual(b.pos_source.size(), (nb_points_1, 3))\n        self.assertEqual(b.x_source.size(), (nb_points_1, 9))\n        self.assertEqual(b.pos_target.size(), (nb_points_2, 3))\n        self.assertEqual(b.x_target.size(), (nb_points_2, 9))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_model_checkpoint.py,4,"b'import unittest\nfrom omegaconf import OmegaConf, DictConfig\nimport os\nimport sys\nimport hydra\nimport shutil\nimport torch\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom mockdatasets import MockDatasetGeometric\nfrom torch_points3d.models.model_factory import instantiate_model\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n\ndef load_config(task, model_type) -> DictConfig:\n    models_conf = os.path.join(ROOT, ""conf/models/{}/{}.yaml"".format(task, model_type))\n    config = OmegaConf.load(models_conf)\n    config.update(""model_name"", ""pointnet2"")\n    config.update(""data.task"", ""segmentation"")\n    return config\n\n\ndef remove(path):\n    try:\n        os.remove(path)\n    except:\n        pass\n\n\nclass MockModel(torch.nn.Module):\n    """""" Mock mdoel that does literaly nothing but holds a state\n    """"""\n\n    def __init__(self):\n        super().__init__()\n        self.state = torch.nn.parameter.Parameter(torch.tensor([1.0]))\n        self.optimizer = torch.nn.Module()\n        self.schedulers = {}\n\n\nclass TestModelCheckpoint(unittest.TestCase):\n    def setUp(self):\n        self.data_config = OmegaConf.load(os.path.join(DIR, ""test_config/data_config.yaml""))\n        training_config = OmegaConf.load(os.path.join(DIR, ""test_config/training_config.yaml""))\n        scheduler_config = OmegaConf.load(os.path.join(DIR, ""test_config/scheduler_config.yaml""))\n        params = load_config(""segmentation"", ""pointnet2"")\n        self.config = OmegaConf.merge(training_config, scheduler_config, params)\n        self.model_name = ""model""\n\n    def test_model_ckpt_using_pointnet2ms(self,):\n        # Create a checkpt\n\n        self.run_path = os.path.join(DIR, ""checkpt"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        model_checkpoint = ModelCheckpoint(self.run_path, self.model_name, ""test"", run_config=self.config, resume=False)\n        dataset = MockDatasetGeometric(5)\n        model = instantiate_model(self.config, dataset)\n        model.set_input(dataset[0], ""cpu"")\n        model.instantiate_optimizers(self.config)\n\n        mock_metrics = {""current_metrics"": {""acc"": 12}, ""stage"": ""test"", ""epoch"": 10}\n        metric_func = {""acc"": max}\n        model_checkpoint.save_best_models_under_current_metrics(model, mock_metrics, metric_func)\n\n        # Load checkpoint and initialize model\n        model_checkpoint = ModelCheckpoint(self.run_path, self.model_name, ""test"", self.config, resume=True)\n        model2 = model_checkpoint.create_model(dataset, weight_name=""acc"")\n\n        self.assertEqual(str(model.optimizer.__class__.__name__), str(model2.optimizer.__class__.__name__))\n        self.assertEqual(model.optimizer.defaults, model2.optimizer.defaults)\n        self.assertEqual(model.schedulers[""lr_scheduler""].state_dict(), model2.schedulers[""lr_scheduler""].state_dict())\n        self.assertEqual(model.schedulers[""bn_scheduler""].state_dict(), model2.schedulers[""bn_scheduler""].state_dict())\n\n        remove(os.path.join(ROOT, ""{}.pt"".format(self.model_name)))\n        remove(os.path.join(DIR, ""{}.pt"".format(self.model_name)))\n\n    def test_best_metric(self):\n        self.run_path = os.path.join(DIR, ""checkpt"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        model_checkpoint = ModelCheckpoint(self.run_path, self.model_name, ""test"", run_config=self.config, resume=False)\n        model = MockModel()\n        optimal_state = model.state.item()\n        metric_func = {""acc"": max}\n        mock_metrics = {""current_metrics"": {""acc"": 12}, ""stage"": ""test"", ""epoch"": 10}\n        model_checkpoint.save_best_models_under_current_metrics(model, mock_metrics, metric_func)\n        model.state[0] = 2\n        mock_metrics = {""current_metrics"": {""acc"": 0}, ""stage"": ""test"", ""epoch"": 11}\n        model_checkpoint.save_best_models_under_current_metrics(model, mock_metrics, metric_func)\n        mock_metrics = {""current_metrics"": {""acc"": 10}, ""stage"": ""train"", ""epoch"": 11}\n        model_checkpoint.save_best_models_under_current_metrics(model, mock_metrics, metric_func)\n        mock_metrics = {""current_metrics"": {""acc"": 15}, ""stage"": ""train"", ""epoch"": 11}\n        model_checkpoint.save_best_models_under_current_metrics(model, mock_metrics, metric_func)\n\n        ckp = torch.load(os.path.join(self.run_path, self.model_name + "".pt""))\n\n        self.assertEqual(ckp[""models""][""best_acc""][""state""].item(), optimal_state)\n        self.assertEqual(ckp[""models""][""latest""][""state""].item(), model.state.item())\n\n    def tearDown(self):\n        if os.path.exists(self.run_path):\n            shutil.rmtree(self.run_path)\n            # os.remove(os.path.join(DIR, ""{}.pt"".format(self.model_name)))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_models.py,1,"b'import unittest\nfrom omegaconf import OmegaConf\nimport os\nimport sys\nfrom glob import glob\nimport torch\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom test.mockdatasets import MockDatasetGeometric, MockDataset\nfrom test.mockdatasets import PairMockDatasetGeometric, PairMockDataset\nfrom test.utils import test_hasgrad\n\nfrom torch_points3d.models.model_factory import instantiate_model\nfrom torch_points3d.core.data_transform import ToSparseInput, XYZFeature, GridSampling3D\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\nfrom torch_points3d.datasets.registration.pair import Pair, PairBatch, PairMultiScaleBatch, DensePairBatch\nfrom torch_geometric.transforms import Compose\n\n\nHAS_MINKOWSKI = True\ntry:\n    import MinkowskiEngine\nexcept:\n    HAS_MINKOWSKI = False\n    print(""=============== Skipping tests that require Minkowski Engine ============="")\n\nseed = 0\ntorch.manual_seed(seed)\ndevice = ""cpu""\n\n\ndef load_model_config(task, model_type, model_name):\n    models_conf = os.path.join(ROOT, ""conf/models/{}/{}.yaml"".format(task, model_type))\n    config = OmegaConf.load(models_conf)\n    config.update(""model_name"", model_name)\n    config.update(""data.task"", task)\n    return config\n\n\ndef get_dataset(conv_type, task):\n    num_points = 1024\n    features = 2\n    batch_size = 2\n    if task == ""object_detection"":\n        include_box = True\n    else:\n        include_box = False\n\n    if conv_type.lower() == ""dense"":\n        num_points = 2048\n        batch_size = 1\n\n    if task == ""registration"":\n        if conv_type.lower() == ""dense"":\n            return PairMockDataset(features, num_points=num_points, batch_size=batch_size)\n        if conv_type.lower() == ""sparse"":\n            tr = Compose([XYZFeature(True, True, True), GridSampling3D(size=0.01, quantize_coords=True, mode=""last"")])\n            return PairMockDatasetGeometric(features, transform=tr, num_points=num_points, batch_size=batch_size)\n        return PairMockDatasetGeometric(features, batch_size=batch_size)\n    else:\n        if conv_type.lower() == ""dense"":\n            num_points = 2048\n            return MockDataset(features, num_points=num_points, include_box=include_box, batch_size=batch_size)\n        if conv_type.lower() == ""sparse"":\n            return MockDatasetGeometric(\n                features,\n                include_box=include_box,\n                transform=GridSampling3D(size=0.01, quantize_coords=True, mode=""last""),\n                num_points=num_points,\n                batch_size=batch_size,\n            )\n        return MockDatasetGeometric(features, batch_size=batch_size)\n\n\nclass TestModels(unittest.TestCase):\n    def setUp(self):\n        self.data_config = OmegaConf.load(os.path.join(DIR, ""test_config/data_config.yaml""))\n        self.model_type_files = glob(os.path.join(ROOT, ""conf/models/*/*.yaml""))\n\n    def test_runall(self):\n        def is_known_to_fail(model_name):\n            forward_failing = [""MinkUNet_WIP"", ""pointcnn"", ""RSConv_4LD"", ""RSConv_2LD"", ""randlanet""]\n            if not HAS_MINKOWSKI:\n                forward_failing += [""Res16"", ""MinkUNet"", ""ResUNetBN2B""]\n            for failing in forward_failing:\n                if failing.lower() in model_name.lower():\n                    return True\n            return False\n\n        for type_file in self.model_type_files:\n            associated_task = type_file.split(""/"")[-2]\n            models_config = OmegaConf.load(type_file)\n            models_config = OmegaConf.merge(models_config, self.data_config)\n            models_config.update(""data.task"", associated_task)\n            for model_name in models_config.models.keys():\n                with self.subTest(model_name):\n                    if not is_known_to_fail(model_name):\n                        models_config.update(""model_name"", model_name)\n                        dataset = get_dataset(models_config.models[model_name].conv_type, associated_task)\n                        model = instantiate_model(models_config, dataset)\n                        model.set_input(dataset[0], device)\n                        try:\n                            model.forward()\n                            model.backward()\n                        except Exception as e:\n                            print(""Forward or backward failing"")\n                            raise e\n                        try:\n                            ratio = test_hasgrad(model)\n                            if ratio < 1:\n                                print(\n                                    ""Model %s.%s.%s has %i%% of parameters with 0 gradient""\n                                    % (associated_task, type_file.split(""/"")[-1][:-5], model_name, 100 * ratio)\n                                )\n                        except Exception as e:\n                            print(""Model with zero gradient %s: %s"" % (type_file, model_name))\n                            raise e\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_modules.py,7,"b'import unittest\nimport numpy as np\nimport numpy.testing as npt\nimport torch\nimport os\nimport sys\nfrom collections import defaultdict\nfrom omegaconf import DictConfig\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.models.base_model import BaseModel, BaseInternalLossModule\nfrom torch_points3d.modules.PointNet.modules import PointNetSTN3D\n\n\nclass TestPointnetModules(unittest.TestCase):\n\n    # test that stn forward works and is initialised with the identity\n    def test_stn(self):\n        pos = torch.tensor([[1, 1, 2], [-1, 0, 1], [10, 12, 13], [-18, 15, 16]]).to(torch.float32)\n        batch = torch.tensor([0, 0, 1, 1])\n\n        stn = PointNetSTN3D(batch_size=2)\n\n        trans_pos = stn(pos, batch)\n\n        npt.assert_array_equal(np.asarray(pos.detach()), np.asarray(trans_pos.detach()))\n\n\nclass MockLossModule(BaseInternalLossModule):\n    def __init__(self, internal_losses):\n        super().__init__()\n        self.internal_losses = internal_losses\n\n    def get_internal_losses(self):\n        return self.internal_losses\n\n\nclass MockModel(BaseModel):\n    def __init__(self):\n        super().__init__(DictConfig({""conv_type"": ""dummy""}))\n\n        self.model1 = MockLossModule({""mock_loss_1"": torch.tensor(0.5), ""mock_loss_2"": torch.tensor(0.3),})\n\n        self.model2 = MockLossModule({""mock_loss_3"": torch.tensor(1.0),})\n\n\nclass TestInternalLosses(unittest.TestCase):\n    def setUp(self):\n        self.model = MockModel()\n\n    def test_get_named_internal_losses(self):\n\n        d = defaultdict(list)\n        d[""mock_loss_1""].append(torch.tensor(0.5))\n        d[""mock_loss_2""].append(torch.tensor(0.3))\n        d[""mock_loss_3""].append(torch.tensor(1.0))\n\n        lossDict = self.model.get_named_internal_losses()\n        self.assertEqual(lossDict, d)\n\n    def test_get_internal_loss(self):\n\n        loss = self.model.get_internal_loss()\n        self.assertAlmostEqual(loss.item(), 0.6)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_msdata.py,10,"b'import unittest\nimport torch\nimport torch.testing as tt\nfrom torch_geometric.data import Data\n\n\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch, MultiScaleData\n\n\nclass TestMSData(unittest.TestCase):\n    def test_apply(self):\n        x = torch.tensor([1])\n        pos = torch.tensor([1])\n        d1 = Data(x=2 * x, pos=2 * pos)\n        d2 = Data(x=3 * x, pos=3 * pos)\n        data = MultiScaleData(x=x, pos=pos, multiscale=[d1, d2])\n        data.apply(lambda x: 2 * x)\n        self.assertEqual(data.x[0], 2)\n        self.assertEqual(data.pos[0], 2)\n        self.assertEqual(data.multiscale[0].pos[0], 4)\n        self.assertEqual(data.multiscale[0].x[0], 4)\n        self.assertEqual(data.multiscale[1].pos[0], 6)\n        self.assertEqual(data.multiscale[1].x[0], 6)\n\n    def test_batch(self):\n        x = torch.tensor([1])\n        pos = x\n        d1 = Data(x=x, pos=pos)\n        d2 = Data(x=4 * x, pos=4 * pos)\n        data1 = MultiScaleData(x=x, pos=pos, multiscale=[d1, d2])\n\n        x = torch.tensor([2])\n        pos = x\n        d1 = Data(x=x, pos=pos)\n        d2 = Data(x=4 * x, pos=4 * pos)\n        data2 = MultiScaleData(x=x, pos=pos, multiscale=[d1, d2])\n\n        batch = MultiScaleBatch.from_data_list([data1, data2])\n        tt.assert_allclose(batch.x, torch.tensor([1, 2]))\n        tt.assert_allclose(batch.batch, torch.tensor([0, 1]))\n\n        ms_batches = batch.multiscale\n        tt.assert_allclose(ms_batches[0].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].x, torch.tensor([4, 8]))\n'"
test/test_msdatapair.py,47,"b'import unittest\nimport torch\nimport torch.testing as tt\nfrom torch_geometric.data import Data\nimport numpy.testing as npt\n\nfrom torch_points3d.datasets.registration.pair import Pair, MultiScalePair, PairBatch, PairMultiScaleBatch\n\n\nclass TestMSPair(unittest.TestCase):\n    """"""\n\n    """"""\n    def test_apply(self):\n        x = torch.tensor([1])\n        pos = torch.tensor([1])\n        x_target = torch.tensor([2])\n        pos_target = torch.tensor([2])\n\n        ms = [Data(x=2 * x, pos=2 * pos), Data(x=3 * x, pos=3 * pos)]\n        ms_target = [Data(x=2 * x_target, pos=2 * pos_target),\n                     Data(x=3 * x_target, pos=3 * pos_target)]\n        data = MultiScalePair(x=x, pos=pos, multiscale=ms,\n                              x_target=x_target, pos_target=pos_target,\n                              multiscale_target=ms_target)\n        data.apply(lambda x: 2 * x)\n        self.assertEqual(data.x[0], 2)\n        self.assertEqual(data.pos[0], 2)\n        self.assertEqual(data.multiscale[0].pos[0], 4)\n        self.assertEqual(data.multiscale[0].x[0], 4)\n        self.assertEqual(data.multiscale[1].pos[0], 6)\n        self.assertEqual(data.multiscale[1].x[0], 6)\n\n        self.assertEqual(data.x_target[0], 4)\n        self.assertEqual(data.pos_target[0], 4)\n        self.assertEqual(data.multiscale_target[0].pos[0], 8)\n        self.assertEqual(data.multiscale_target[0].x[0], 8)\n        self.assertEqual(data.multiscale_target[1].pos[0], 12)\n        self.assertEqual(data.multiscale_target[1].x[0], 12)\n\n    def test_pair_batch(self):\n        d1 = Data(x=torch.tensor([1]), pos=torch.tensor([1]))\n        d2 = Data(x=torch.tensor([2]), pos=torch.tensor([4]))\n        d3 = Data(x=torch.tensor([3]), pos=torch.tensor([9]))\n        d4 = Data(x=torch.tensor([4]), pos=torch.tensor([16]))\n        p1 = Pair.make_pair(d1, d2)\n        p2 = Pair.make_pair(d3, d4)\n        batch = PairBatch.from_data_list([p1, p2])\n        tt.assert_allclose(batch.x, torch.tensor([1, 3]))\n        tt.assert_allclose(batch.pos, torch.tensor([1, 9]))\n        tt.assert_allclose(batch.batch, torch.tensor([0, 1]))\n        tt.assert_allclose(batch.x_target, torch.tensor([2, 4]))\n        tt.assert_allclose(batch.pos_target, torch.tensor([4, 16]))\n        tt.assert_allclose(batch.batch_target, torch.tensor([0, 1]))\n\n\n    def test_ms_pair_batch(self):\n        x = torch.tensor([1])\n        pos = x\n        x_target = torch.tensor([2])\n        pos_target = x\n        ms = [Data(x=x, pos=pos), Data(x=4 * x, pos=4 * pos)]\n        ms_target = [Data(x=x_target, pos=pos_target),\n                     Data(x=4 * x_target, pos=4 * pos_target)]\n        data1 = MultiScalePair(x=x, pos=pos, multiscale=ms,\n                               x_target=x_target, pos_target=pos_target,\n                               multiscale_target=ms_target)\n\n        x = torch.tensor([3])\n        pos = x\n        x_target = torch.tensor([4])\n        pos_target = x\n        ms = [Data(x=x, pos=pos), Data(x=4 * x, pos=4 * pos)]\n        ms_target = [Data(x=x_target, pos=pos_target),\n                     Data(x=4 * x_target, pos=4 * pos_target)]\n        data2 = MultiScalePair(x=x, pos=pos, multiscale=ms,\n                               x_target=x_target, pos_target=pos_target,\n                               multiscale_target=ms_target)\n\n        batch = PairMultiScaleBatch.from_data_list([data1, data2])\n        tt.assert_allclose(batch.x, torch.tensor([1, 3]))\n        tt.assert_allclose(batch.x_target, torch.tensor([2, 4]))\n        tt.assert_allclose(batch.batch, torch.tensor([0, 1]))\n        tt.assert_allclose(batch.batch_target, torch.tensor([0, 1]))\n\n        ms_batches = batch.multiscale\n        tt.assert_allclose(ms_batches[0].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].x, torch.tensor([4, 12]))\n\n        ms_batches = batch.multiscale_target\n        tt.assert_allclose(ms_batches[0].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].batch, torch.tensor([0, 1]))\n        tt.assert_allclose(ms_batches[1].x, torch.tensor([8, 16]))\n\n\n    def test_pair_ind(self):\n        data1 = Data(pos=torch.randn(100, 3))\n        data2 = Data(pos=torch.randn(114, 3))\n        pair1 = Pair.make_pair(data1, data2)\n        pair1.pair_ind = torch.tensor([[0, 1], [99, 36], [98, 113], [54, 29], [10, 110], [1, 0]])\n        data3 = Data(pos=torch.randn(102, 3))\n        data4 = Data(pos=torch.randn(104, 3))\n        pair2 = Pair.make_pair(data3, data4)\n        pair2.pair_ind = torch.tensor([[0, 1], [45, 28], [101, 36], [98, 1], [14, 99], [34, 52], [1, 0]])\n        data5 = Data(pos=torch.randn(128, 3))\n        data6 = Data(pos=torch.randn(2102, 3))\n        pair3 = Pair.make_pair(data5, data6)\n        pair3.pair_ind = torch.tensor([[0, 1], [100, 1000], [1, 0]])\n\n        batch = PairBatch.from_data_list([pair1, pair2, pair3])\n        expected_pair_ind = torch.tensor([[0, 1],\n                                          [99, 36],\n                                          [98, 113],\n                                          [54, 29],\n                                          [10, 110],\n                                          [1, 0],\n                                          [0 + 100, 1 + 114],\n                                          [45 + 100, 28 + 114],\n                                          [101 + 100, 36 + 114],\n                                          [98 + 100, 1 + 114],\n                                          [14 + 100, 99 + 114],\n                                          [34 + 100, 52 + 114],\n                                          [1 + 100, 0 + 114],\n                                          [0 + 100 + 102, 1 + 114 + 104],\n                                          [100 + 100 + 102, 1000 + 114 + 104],\n                                          [1 + 100 + 102, 0 + 114 + 104]]).to(torch.long)\n        npt.assert_almost_equal(batch.pair_ind.numpy(), expected_pair_ind.numpy())\n\n    def test_ms_pair_ind(self):\n\n        x = torch.randn(1001, 3)\n        pos = x\n        x_target = torch.randn(1452, 3)\n        pos_target = x_target\n        ms = [Data(x=x, pos=pos), Data(x=4 * x, pos=4 * pos)]\n        ms_target = [Data(x=x_target, pos=pos_target),\n                     Data(x=4 * x_target, pos=4 * pos_target)]\n        data1 = MultiScalePair(x=x, pos=pos, multiscale=ms,\n                               x_target=x_target, pos_target=pos_target,\n                               multiscale_target=ms_target)\n        data1.pair_ind = torch.tensor([[0, 1], [99, 36], [98, 113], [54, 29], [10, 110], [1, 0]])\n        x = torch.randn(300, 3)\n        pos = x\n        x_target = torch.randn(154, 3)\n        pos_target = x_target\n        ms = [Data(x=x, pos=pos), Data(x=4 * x, pos=4 * pos)]\n        ms_target = [Data(x=x_target, pos=pos_target),\n                     Data(x=4 * x_target, pos=4 * pos_target)]\n        data2 = MultiScalePair(x=x, pos=pos, multiscale=ms,\n                               x_target=x_target, pos_target=pos_target,\n                               multiscale_target=ms_target)\n        data2.pair_ind = torch.tensor([[0, 1], [100, 1000], [1, 0]])\n\n        batch = PairMultiScaleBatch.from_data_list([data1, data2])\n\n        expected_pair_ind = torch.tensor([[0, 1],\n                                          [99, 36],\n                                          [98, 113],\n                                          [54, 29],\n                                          [10, 110],\n                                          [1, 0],\n                                          [0 + 1001, 1 + 1452],\n                                          [100 + 1001, 1000 + 1452],\n                                          [1 + 1001, 0 + 1452]])\n\n        npt.assert_almost_equal(batch.pair_ind.numpy(), expected_pair_ind.numpy())\n'"
test/test_normal.py,1,"b'import os\nimport unittest\nimport numpy as np\nimport numpy.testing as npt\nimport torch\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\n\nfrom torch_points3d.core.data_transform.transforms import MeshToNormal\n\n\nclass TestModelUtils(unittest.TestCase):\n    def setUp(self):\n\n        pos = np.array(\n            [[0, 1, 0], [0, 0, 0], [1, 0, 0], [0, 0, 0], [0, 0, 1], [0, 1, 0]]\n        )  # Should be [0, 0, 1], [1, 0, 0]\n        face = np.array([[0, 1, 2], [3, 4, 5]]).T\n\n        self.data = Data(pos=torch.from_numpy(pos).float(), face=torch.from_numpy(face))\n\n    def test_mesh_to_normal(self):\n        mesh_transform = MeshToNormal()\n        data = mesh_transform(self.data)\n        normals = data.normals.numpy()\n        npt.assert_array_equal(normals[0], [0, 0, 1])\n        npt.assert_array_equal(normals[1], [-1, 0, 0])\n'"
test/test_random_sphere.py,1,"b'import os\nimport sys\nimport unittest\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.core.data_transform.transforms import RandomSphere\n\n\nclass TestRandmSphere(unittest.TestCase):\n    def setUp(self):\n\n        pos = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 1]])\n        labels = np.array([0, 0, 0, 0, 0, 0])\n\n        self.data = Data(pos=torch.from_numpy(pos).float(), labels=torch.from_numpy(labels))\n\n    def test_neighbour_found_under_random_sampling(self):\n        random_sphere = RandomSphere(0.1, strategy=""RANDOM"")\n        data = random_sphere(self.data.clone())\n        assert data.labels.shape[0] == 1\n\n        random_sphere = RandomSphere(3, strategy=""RANDOM"")\n        data = random_sphere(self.data.clone())\n        assert data.labels.shape[0] == 6\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_registration_metrics.py,18,"b'import numpy as np\nimport torch\nimport unittest\nimport numpy.testing as npt\n\nfrom torch_points3d.metrics.registration_metrics import estimate_transfo\nfrom torch_points3d.metrics.registration_metrics import fast_global_registration\nfrom torch_points3d.metrics.registration_metrics import compute_hit_ratio\nfrom torch_points3d.metrics.registration_metrics import compute_transfo_error\nfrom torch_points3d.metrics.registration_metrics import rodrigues\nfrom torch_points3d.core.data_transform import euler_angles_to_rotation_matrix\n\n\nclass TestRegistrationMetrics(unittest.TestCase):\n    def test_estimate_transfo(self):\n\n        a = torch.randn(100, 3)\n\n        R_gt = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        t_gt = torch.rand(3)\n        T_gt = torch.eye(4)\n        T_gt[:3, :3] = R_gt\n        T_gt[:3, 3] = t_gt\n        b = a.mm(R_gt.T) + t_gt\n        T_pred = estimate_transfo(a, b)\n\n        npt.assert_allclose(T_pred.numpy(), T_gt.numpy(), rtol=1e-3)\n\n    def test_fast_global_registration(self):\n        a = torch.randn(100, 3)\n\n        R_gt = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        t_gt = torch.rand(3)\n        T_gt = torch.eye(4)\n        T_gt[:3, :3] = R_gt\n        T_gt[:3, 3] = t_gt\n        b = a.mm(R_gt.T) + t_gt\n        T_pred = fast_global_registration(a, b, mu_init=1, num_iter=20)\n        npt.assert_allclose(T_pred.numpy(), T_gt.numpy(), rtol=1e-3)\n\n    def test_fast_global_registration_with_outliers(self):\n        a = torch.randn(100, 3)\n        R_gt = euler_angles_to_rotation_matrix(torch.rand(3) * np.pi)\n        t_gt = torch.rand(3)\n        T_gt = torch.eye(4)\n        T_gt[:3, :3] = R_gt\n        T_gt[:3, 3] = t_gt\n        b = a.mm(R_gt.T) + t_gt\n        b[[1, 5, 20, 32, 74, 17, 27, 77, 88, 89]] *= 42\n        T_pred = fast_global_registration(a, b, mu_init=1, num_iter=20)\n        # T_pred = estimate_transfo(a, b)\n        npt.assert_allclose(T_pred.numpy(), T_gt.numpy(), rtol=1e-3)\n\n    def test_compute_hit_ratio(self):\n        xyz = torch.randn(100, 3)\n        xyz_target = xyz.clone()\n        xyz[[1, 5, 20, 32, 74, 17, 27, 77, 88, 89]] += 42\n\n        hit = compute_hit_ratio(xyz, xyz_target, torch.eye(4), 0.1)\n\n        self.assertAlmostEqual(hit.item(), 0.9)\n\n    def test_compute_transfo_error(self):\n\n        axis = torch.randn(3)\n        axis = axis / torch.norm(axis)\n        theta = 30 * np.pi / 180\n\n        R = rodrigues(axis, theta)\n\n        T = torch.eye(4)\n        T[:3, :3] = R\n        T[0, 3] = 1\n\n        rte, rre = compute_transfo_error(torch.eye(4), T)\n        npt.assert_allclose(rte.item(), 1)\n        npt.assert_allclose(rre.item(), 30, rtol=1e-3)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_registration_tracker.py,24,"b'import unittest\nimport torch\nimport os\nimport sys\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\nfrom torch_points3d.metrics.registration_metrics import rodrigues\nfrom torch_points3d.metrics.registration_tracker import FragmentRegistrationTracker\n\n\ndef rand_T():\n\n    t = torch.randn(3)\n    axis = torch.randn(3)\n    theta = torch.norm(axis)\n    axis = axis / theta\n    R = rodrigues(axis, theta)\n    T = torch.eye(4)\n    T[:3, :3] = R\n    T[:3, 3] = t\n    return T\n\n\nclass MockDataset:\n    def __init__(self):\n        self.num_classes = 2\n\n\nclass MockModel:\n    def __init__(self):\n        self.iter = 0\n        self.losses = [\n            {""loss_1"": 1, ""loss_2"": 2},\n            {""loss_1"": 2, ""loss_2"": 2},\n            {""loss_1"": 1, ""loss_2"": 2},\n            {""loss_1"": 1, ""loss_2"": 2},\n        ]\n\n        list_xyz = [torch.randn(100, 3) for _ in range(16)]\n        trans = [rand_T() for _ in range(16)]\n        list_xyz_target = [list_xyz[i].mm(trans[i][:3, :3]) + trans[i][:3, 3] for i in range(16)]\n        self.ind = [torch.arange(400) for _ in range(4)]\n        self.ind_target = [torch.arange(400) for _ in range(4)]\n        self.ind_size = [torch.tensor([100, 100, 100, 100]) for _ in range(4)]\n        self.xyz = [torch.cat(list_xyz[4 * i : 4 * (i + 1)], 0) for i in range(4)]\n        self.xyz_target = [torch.cat(list_xyz_target[4 * i : 4 * (i + 1)], 0) for i in range(4)]\n        self.feat = self.xyz\n        self.feat_target = self.xyz\n        self.batch_idx = [torch.cat(tuple(torch.arange(4).repeat(100, 1).T)) for i in range(4)]\n        self.batch_idx_target = [torch.cat(tuple(torch.arange(4).repeat(100, 1).T)) for i in range(4)]\n\n        rang1 = torch.cat((torch.arange(10), torch.arange(100, 110), torch.arange(200, 210), torch.arange(300, 310)))\n        rang2 = torch.cat((torch.arange(20), torch.arange(100, 120), torch.arange(200, 220), torch.arange(300, 320)))\n        rang3 = torch.cat((torch.arange(10), torch.arange(100, 116), torch.arange(200, 220), torch.arange(300, 330)))\n\n        inv1 = torch.cat(\n            (torch.arange(9, -1, -1), torch.arange(109, 99, -1), torch.arange(209, 199, -1), torch.arange(309, 299, -1))\n        )\n        inv2 = torch.cat(\n            (\n                torch.arange(19, -1, -1),\n                torch.arange(119, 99, -1),\n                torch.arange(219, 199, -1),\n                torch.arange(319, 299, -1),\n            )\n        )\n        inv3 = torch.cat(\n            (torch.arange(9, -1, -1), torch.arange(115, 99, -1), torch.arange(219, 199, -1), torch.arange(329, 299, -1))\n        )\n\n        self.feat[1][rang1] = self.feat[1][inv1]\n        self.feat[2][rang2] = self.feat[2][inv2]\n        self.feat[3][rang3] = self.feat[3][inv3]\n\n    def get_output(self):\n        return self.feat[self.iter], self.feat_target[self.iter]\n\n    def get_input(self):\n        input = Data(pos=self.xyz[self.iter], ind=self.ind[self.iter], size=self.ind_size[self.iter])\n        input_target = Data(\n            pos=self.xyz_target[self.iter], ind=self.ind_target[self.iter], size=self.ind_size[self.iter]\n        )\n        return input, input_target\n\n    def get_current_losses(self):\n        return self.losses[self.iter]\n\n    def get_batch(self):\n        return self.batch_idx[self.iter], self.batch_idx_target[self.iter]\n\n\nclass TestSegmentationTracker(unittest.TestCase):\n    def test_track_batch(self):\n        tracker = FragmentRegistrationTracker(MockDataset(), stage=""test"", tau_2=0.83, num_points=100)\n        model = MockModel()\n        list_hit_ratio = [1.0, 0.9, 0.8, (0.9 + 0.84 + 0.8 + 0.7) / 4]\n        list_feat_match_ratio = [1.0, 1.0, 0.0, 0.5]\n        for i in range(4):\n            tracker.track(model)\n            metrics = tracker.get_metrics()\n            # the most important metrics in registration\n            self.assertAlmostEqual(metrics[""test_hit_ratio""], list_hit_ratio[i])\n            self.assertAlmostEqual(metrics[""test_feat_match_ratio""], list_feat_match_ratio[i])\n            tracker.reset(""test"")\n            model.iter += 1\n\n    def test_track_all(self):\n        tracker = FragmentRegistrationTracker(MockDataset(), stage=""test"", tau_2=0.83, num_points=100)\n        model = MockModel()\n        tracker.reset(""test"")\n        model.iter = 0\n        for i in range(4):\n            tracker.track(model)\n            model.iter += 1\n        metrics = tracker.get_metrics()\n        self.assertAlmostEqual(metrics[""test_hit_ratio""], (4 * 1.0 + 4 * 0.9 + 4 * 0.8 + 0.9 + 0.84 + 0.8 + 0.7) / 16)\n        self.assertAlmostEqual(metrics[""test_feat_match_ratio""], (4 * 1 + 4 * 1 + 4 * 0 + 2 * 1 + 2 * 0) / 16)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_resolver.py,5,"b'import unittest\n\nfrom omegaconf import OmegaConf\nimport torch\nfrom torch_geometric.data import Batch\nimport os\nimport sys\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, feature_size=6):\n        self.feature_dimension = feature_size\n        self.num_classes = 10\n        self.weight_classes = None\n        nb_points = 100\n        self._pos = torch.randn((nb_points, 3))\n        if feature_size > 0:\n            self._feature = torch.tensor([range(feature_size) for i in range(self._pos.shape[0])], dtype=torch.float,)\n        else:\n            self._feature = None\n        self._y = torch.tensor([range(10) for i in range(self._pos.shape[0])], dtype=torch.float)\n        self._batch = torch.tensor([0 for i in range(self._pos.shape[0])])\n\n    def __getitem__(self, index):\n        return Batch(pos=self._pos, x=self._feature, y=self._y, batch=self._batch)\n\n\nclass TestModelDefinitionResolver(unittest.TestCase):\n    def test_resolve_1(self):\n        model_conf = os.path.join(ROOT, ""test/test_config/test_resolver_in.yaml"")\n        config = OmegaConf.load(model_conf)\n        dataset = MockDataset(6)\n        tested_task = ""segmentation""\n\n        resolve_model(config, dataset, tested_task)\n\n        expected = OmegaConf.load(os.path.join(ROOT, ""test/test_config/test_resolver_out.yaml""))\n\n        self.assertEqual(dict(config), dict(expected))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_sampler.py,24,"b'import unittest\nimport torch\n\nfrom torch_points3d.core.spatial_ops import (\n    FPSSampler,\n    RandomSampler,\n    RadiusNeighbourFinder,\n    MultiscaleRadiusNeighbourFinder,\n)\n\n\nclass TestSampler(unittest.TestCase):\n    def test_fps(self):\n        pos = torch.randn((10, 3))\n        batch = torch.zeros(10).to(torch.long)\n        sampler = FPSSampler(ratio=0.5)\n        self.assertEqual(sampler(pos, batch).shape[0], 5)\n\n        sampler = FPSSampler(num_to_sample=5)\n        self.assertEqual(sampler(pos, batch).shape[0], 5)\n\n    def test_random(self):\n        pos = torch.randn((10, 3))\n        batch = torch.zeros(10).to(torch.long)\n        sampler = RandomSampler(ratio=0.5)\n        self.assertEqual(sampler(pos, batch).shape[0], 5)\n\n        sampler = RandomSampler(num_to_sample=5)\n        self.assertEqual(sampler(pos, batch).shape[0], 5)\n\n\nclass TestNeighboorhoodSearch(unittest.TestCase):\n    def test_single_search(self):\n        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        batch_x = torch.tensor([0, 0, 0, 0])\n        y = torch.Tensor([[-1, 0], [1, 0]])\n        batch_y = torch.tensor([0, 0])\n\n        nei_finder = MultiscaleRadiusNeighbourFinder(1, 4)\n        self.assertEqual(nei_finder(x, y, batch_x, batch_y, 0)[1, :].shape[0], 4)\n\n    def test_multi_radius_search(self):\n        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        batch_x = torch.tensor([0, 0, 0, 0])\n        y = torch.Tensor([[-1, 0], [1, 0]])\n        batch_y = torch.tensor([0, 0])\n        nei_finder = MultiscaleRadiusNeighbourFinder([1, 10], 4)\n        multiscale = []\n        for i in range(2):\n            multiscale.append(nei_finder(x, y, batch_x, batch_y, i))\n\n        self.assertEqual(len(multiscale), 2)\n        self.assertEqual(multiscale[0][1, :].shape[0], 4)\n        self.assertEqual(multiscale[1][1, :].shape[0], 8)\n\n    def test_multi_num_search(self):\n        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        batch_x = torch.tensor([0, 0, 0, 0])\n        y = torch.Tensor([[-1, 0], [1, 0]])\n        batch_y = torch.tensor([0, 0])\n        nei_finder = MultiscaleRadiusNeighbourFinder(10, [3, 4])\n        multiscale = []\n        for i in range(2):\n            multiscale.append(nei_finder(x, y, batch_x, batch_y, i))\n\n        self.assertEqual(len(multiscale), 2)\n        self.assertEqual(multiscale[0][1, :].shape[0], 6)\n        self.assertEqual(multiscale[1][1, :].shape[0], 8)\n\n    def test_multiall(self):\n        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        batch_x = torch.tensor([0, 0, 0, 0])\n        y = torch.Tensor([[-1, 0], [1, 0]])\n        batch_y = torch.tensor([0, 0])\n\n        nei_finder = MultiscaleRadiusNeighbourFinder([1, 10], [3, 4])\n        multiscale = []\n        for i in range(2):\n            multiscale.append(nei_finder(x, y, batch_x, batch_y, i))\n\n        self.assertEqual(len(multiscale), 2)\n        self.assertEqual(multiscale[0][1, :].shape[0], 4)\n        self.assertEqual(multiscale[1][1, :].shape[0], 8)\n\n    def test_raises(self):\n        with self.assertRaises(ValueError):\n            nei_finder = MultiscaleRadiusNeighbourFinder([1], [3, 4])\n\n        x = torch.Tensor([[-1, -1], [-1, 1], [1, -1], [1, 1]])\n        batch_x = torch.tensor([0, 0, 0, 0])\n        y = torch.Tensor([[-1, 0], [1, 0]])\n        batch_y = torch.tensor([0, 0])\n        nei_finder = MultiscaleRadiusNeighbourFinder([1, 2], [3, 4])\n        with self.assertRaises(ValueError):\n            nei_finder(x, y, batch_x, batch_y, 10)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_samplers.py,0,"b'import os\nimport sys\nimport unittest\nimport numpy as np\n\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)))\nsys.path.append(ROOT)\n\nfrom torch_points3d.datasets.samplers import BalancedRandomSampler\n\n\nclass TestBalancedRandomSampler(unittest.TestCase):\n    def test_simple(self):\n\n        num_classes = 10\n        num_samples = 10000\n\n        p = np.asarray([2 ** i for i in range(num_classes)]).astype(float)\n        p /= p.sum()\n\n        labels = np.random.choice(range(num_classes), num_samples, p=p)\n\n        sampler = BalancedRandomSampler(labels)\n\n        indices = iter(sampler)\n        _, c = np.unique(labels[list(indices)], return_counts=True)\n        self.assertGreater(0.005, np.std(c) / num_samples)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_sampling_strategy.py,1,"b'import os\nimport sys\nimport unittest\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.utils.transform_utils import SamplingStrategy\n\n\nclass TestSamplingStrategy(unittest.TestCase):\n    def setUp(self):\n\n        pos = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 1]])\n        self.labels = np.array([0, 1, 2, 3, 4, 5])\n\n        self.data = Data(pos=torch.from_numpy(pos).float(), labels=torch.from_numpy(self.labels))\n\n    def test_random_sampling_strategy(self):\n        random_sphere = SamplingStrategy(strategy=""RANDOM"")\n\n        np.random.seed(42)\n\n        random_labels = []\n        for i in range(50):\n            random_center = random_sphere(self.data.clone())\n            random_labels.append(self.labels[random_center])\n\n        assert len(np.unique(random_labels)) == len(self.labels)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_segmentationtracker.py,13,"b'import unittest\nimport os\nimport torch\nimport sys\nimport numpy as np\nimport numpy.testing as npt\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.metrics.segmentation_tracker import SegmentationTracker\nfrom torch_points3d.metrics.segmentation_helpers import SegmentationVoter\nfrom torch_points3d.metrics.classification_tracker import ClassificationTracker\nfrom torch_points3d.metrics.s3dis_tracker import S3DISTracker\n\n\nclass MockDataset:\n    INV_OBJECT_LABEL = {0: ""first"", 1: ""wall"", 2: ""not"", 3: ""here"", 4: ""hoy""}\n    pos = torch.tensor([[1, 0, 0], [2, 0, 0], [3, 0, 0], [-1, 0, 0]]).float()\n    test_label = torch.tensor([1, 1, 0, 0])\n\n    def __init__(self):\n        self.num_classes = 2\n\n    @property\n    def test_data(self):\n        return Data(pos=self.pos, y=self.test_label)\n\n    def has_labels(self, stage):\n        return True\n\n\nclass MockModel:\n    def __init__(self):\n        self.iter = 0\n        self.losses = [\n            {""loss_1"": 1, ""loss_2"": 2},\n            {""loss_1"": 2, ""loss_2"": 2},\n            {""loss_1"": 1, ""loss_2"": 2},\n            {""loss_1"": 1, ""loss_2"": 2},\n        ]\n        self.outputs = [\n            torch.tensor([[0, 1], [0, 1]]),\n            torch.tensor([[1, 0], [1, 0]]),\n            torch.tensor([[1, 0], [1, 0]]),\n            torch.tensor([[1, 0], [1, 0], [1, 0]]),\n        ]\n        self.labels = [torch.tensor([1, 1]), torch.tensor([1, 1]), torch.tensor([1, 1]), torch.tensor([0, 0, -100])]\n        self.batch_idx = [torch.tensor([0, 1]), torch.tensor([0, 1]), torch.tensor([0, 1]), torch.tensor([0, 0, 1])]\n\n    def get_input(self):\n        return Data(pos=MockDataset.pos[:2, :], origin_id=torch.tensor([0, 1]))\n\n    def get_output(self):\n        return self.outputs[self.iter].float()\n\n    def get_labels(self):\n        return self.labels[self.iter]\n\n    def get_current_losses(self):\n        return self.losses[self.iter]\n\n    def get_batch(self):\n        return self.batch_idx[self.iter]\n\n    @property\n    def device(self):\n        return ""cpu""\n\n\nclass TestSegmentationTracker(unittest.TestCase):\n    def test_track(self):\n        tracker = SegmentationTracker(MockDataset())\n        model = MockModel()\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        for k in [""train_acc"", ""train_miou"", ""train_macc""]:\n            self.assertAlmostEqual(metrics[k], 100, 5)\n\n        model.iter += 1\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        for k in [""train_acc"", ""train_macc""]:\n            self.assertEqual(metrics[k], 50)\n        self.assertAlmostEqual(metrics[""train_miou""], 25, 5)\n        self.assertEqual(metrics[""train_loss_1""], 1.5)\n\n        tracker.reset(""test"")\n        model.iter += 1\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        for k in [""test_acc"", ""test_miou"", ""test_macc""]:\n            self.assertAlmostEqual(metrics[k], 0, 5)\n\n    def test_ignore_label(self):\n        tracker = SegmentationTracker(MockDataset(), ignore_label=-100)\n        tracker.reset(""test"")\n        model = MockModel()\n        model.iter = 3\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        for k in [""test_acc"", ""test_miou"", ""test_macc""]:\n            self.assertAlmostEqual(metrics[k], 100, 5)\n\n    def test_finalise(self):\n        tracker = SegmentationTracker(MockDataset(), ignore_label=-100)\n        tracker.reset(""test"")\n        model = MockModel()\n        model.iter = 3\n        tracker.track(model)\n        tracker.finalise()\n        with self.assertRaises(RuntimeError):\n            tracker.track(model)\n\n    def test_seg_full_res_helpers(self):\n\n        raw_pos = torch.from_numpy(np.asarray([[0, 0, 0], [0, 0.5, 0], [0.5, 1, 0], [1, 1, 0]]))\n        raw_y = torch.from_numpy(np.asarray([0, 0, 1, 1]))\n        preds = torch.from_numpy(np.asarray([[1, 0], [1, 0], [0, 1], [0, 1]]))\n        idx = torch.arange(0, 4)\n        raw_data = Data(pos=raw_pos, y=raw_y)\n\n        np.asarray([1, 1, 0, 0])\n        left_pred = np.asarray([0, 0, 0, 0])\n        np.asarray([0, 0, 1, 1])\n        right_pred = np.asarray([1, 1, 1, 1])\n\n        for _ in range(25):\n            segmentation_resolver = SegmentationVoter(raw_data, 2, ""dense"")\n\n            for _ in range(np.random.randint(1, 10)):\n                slice_ = np.random.choice(range(4), 2)\n                data = Data(pos=raw_pos[slice_], y=raw_y[slice_])\n                setattr(data, SaveOriginalPosId.KEY, [idx[slice_]])\n                output = preds[slice_]\n                segmentation_resolver.add_vote(data, output, 0)\n\n            mask = segmentation_resolver._vote_counts.numpy() > 0\n            if np.sum(mask > 0) > 2:\n                npt.assert_array_almost_equal(segmentation_resolver.full_res_preds.numpy(), raw_y)\n            else:\n                has_left = np.sum(mask[:2]) > 0\n                has_right = np.sum(mask[2:]) > 0\n\n                if has_left and has_right:\n                    npt.assert_array_almost_equal(segmentation_resolver.full_res_preds.numpy(), raw_y)\n\n                elif has_left and not has_right:\n                    npt.assert_array_almost_equal(segmentation_resolver.full_res_preds.numpy(), left_pred)\n                else:\n                    npt.assert_array_almost_equal(segmentation_resolver.full_res_preds.numpy(), right_pred)\n\n        segmentation_resolver.k = 5\n        self.assertEqual(segmentation_resolver.k, 5)\n\n\nclass TestS3DISTarcker(unittest.TestCase):\n    def test_fullres(self):\n        tracker = S3DISTracker(MockDataset())\n        tracker.reset(""test"")\n        model = MockModel()\n        tracker.track(model, full_res=True)\n        tracker.track(model, full_res=True)\n        tracker.finalise(full_res=True)\n        metrics = tracker.get_metrics(verbose=True)\n        self.assertAlmostEqual(metrics[""test_full_vote_miou""], 25, 5)\n        self.assertAlmostEqual(metrics[""test_vote_miou""], 100, 5)\n\n\nclass TestClassificationTracker(unittest.TestCase):\n    from torch_points3d.metrics.classification_tracker import ClassificationTracker\n\n    def test_classification(self):\n        tracker = ClassificationTracker(MockDataset())\n        tracker.reset(""test"")\n        model = MockModel()\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        self.assertAlmostEqual(metrics[""test_acc""], 100, 5)\n\n        model.iter += 1\n        tracker.track(model)\n        metrics = tracker.get_metrics()\n        self.assertAlmostEqual(metrics[""test_acc""], 50, 5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_shapenetforward.py,5,"b'import unittest\nimport os\nimport sys\nimport torch\nfrom omegaconf.dictconfig import DictConfig\nimport numpy.testing as npt\nimport numpy as np\nimport copy\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR, ""..""))\n\nfrom test.mock_models import MockModel\nfrom torch_points3d.datasets.segmentation.forward.shapenet import _ForwardShapenet, ForwardShapenetDataset\n\n\nclass TestForwardData(unittest.TestCase):\n    def setUp(self):\n        self.datadir = os.path.join(DIR, ""test_dataset"")\n        self.config = DictConfig(\n            {\n                ""dataroot"": self.datadir,\n                ""test_transforms"": [{""transform"": ""FixedPoints"", ""lparams"": [2]}],\n                ""category"": [""Airplane"", ""Cap""],\n                ""forward_category"": ""Airplane"",\n            }\n        )\n\n    def test_fileList(self):\n        test = _ForwardShapenet(self.datadir, 0)\n        self.assertEqual(len(test), 2)\n\n    def test_load(self):\n        test = _ForwardShapenet(self.datadir, 10)\n        data = test[0]\n        self.assertEqual(data.sampleid, torch.tensor([0]))\n        self.assertEqual(data.category[0], 10)\n\n    def test_break(self):\n        config = copy.deepcopy(self.config)\n        config.forward_category = ""Other""\n        with self.assertRaises(ValueError):\n            ForwardShapenetDataset(config)\n\n    def test_dataloaders(self):\n        dataset = ForwardShapenetDataset(self.config)\n        dataset.create_dataloaders(MockModel(DictConfig({""conv_type"": ""DENSE""})), 2, False, 1, False)\n        forward_set = dataset.test_dataloaders[0]\n        for b in forward_set:\n            self.assertEqual(b.origin_id.shape, (2, 2))\n\n        sparseconfig = DictConfig({""dataroot"": self.datadir, ""category"": ""Airplane"", ""forward_category"": ""Airplane""})\n        dataset = ForwardShapenetDataset(sparseconfig)\n        dataset.create_dataloaders(MockModel(DictConfig({""conv_type"": ""PARTIAL_DENSE""})), 2, False, 1, False)\n        forward_set = dataset.test_dataloaders[0]\n        for b in forward_set:\n            torch.testing.assert_allclose(b.origin_id, torch.tensor([0, 1, 2, 0, 1, 2, 3]))\n            torch.testing.assert_allclose(b.sampleid, torch.tensor([0, 1]))\n\n    def test_predictupsampledense(self):\n        dataset = ForwardShapenetDataset(self.config)\n        dataset.create_dataloaders(MockModel(DictConfig({""conv_type"": ""DENSE""})), 2, False, 1, False)\n        forward_set = dataset.test_dataloaders[0]\n        for b in forward_set:\n            output = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]])\n            predicted = dataset.predict_original_samples(b, ""DENSE"", output)\n            self.assertEqual(len(predicted), 2)\n            self.assertEqual(predicted[""example1.txt""].shape, (3, 4))\n            self.assertEqual(predicted[""example2.txt""].shape, (4, 4))\n            npt.assert_allclose(predicted[""example1.txt""][:, -1], np.asarray([0, 0, 0]))\n            npt.assert_allclose(predicted[""example2.txt""][:, -1], np.asarray([1, 1, 1, 1]))\n\n    def test_predictupsamplepartialdense(self):\n        dataset = ForwardShapenetDataset(self.config)\n        dataset.create_dataloaders(MockModel(DictConfig({""conv_type"": ""PARTIAL_DENSE""})), 2, False, 1, False)\n        forward_set = dataset.test_dataloaders[0]\n        for b in forward_set:\n            output = torch.tensor([[1, 0], [1, 0], [0, 1], [0, 1]])\n            predicted = dataset.predict_original_samples(b, ""PARTIAL_DENSE"", output)\n            self.assertEqual(len(predicted), 2)\n            self.assertEqual(predicted[""example1.txt""].shape, (3, 4))\n            self.assertEqual(predicted[""example2.txt""].shape, (4, 4))\n            npt.assert_allclose(predicted[""example1.txt""][:, -1], np.asarray([0, 0, 0]))\n            npt.assert_allclose(predicted[""example2.txt""][:, -1], np.asarray([1, 1, 1, 1]))\n\n    def test_numclasses(self):\n        dataset = ForwardShapenetDataset(self.config)\n        self.assertEqual(dataset.num_classes, 8)\n\n    def test_classtosegments(self):\n        dataset = ForwardShapenetDataset(self.config)\n        self.assertEqual(dataset.class_to_segments, {""Airplane"": [0, 1, 2, 3], ""Cap"": [6, 7]})\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_shapenetparttracker.py,0,"b'import unittest\nimport numpy as np\nimport os\nimport sys\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.metrics.shapenet_part_tracker import ShapenetPartTracker\n\n\nclass MockDataset:\n    def __init__(self):\n        self.num_classes = 5\n        self.class_to_segments = {""class1"": [0, 1], ""class2"": [2, 3]}\n        self.is_hierarchical = True\n\n\nclass MockModel:\n    def __init__(self):\n        self.iter = 0\n        self.losses = [{""loss_1"": 1, ""loss_2"": 2}, {""loss_1"": 1, ""loss_2"": 2}, {""loss_1"": 1, ""loss_2"": 2}]\n        self.outputs = [\n            np.asarray([[0, 1, 0, 0], [0, 1, 0, 0]]),\n            np.asarray([[0, 0, 1, 0], [0, 0, 1, 0]]),\n            np.asarray([[0, 0, 1, 0]]),\n        ]\n        self.labels = [np.asarray([1, 1]), np.asarray([2, 2]), np.asarray([3])]\n        self.batch_idx = [np.asarray([0, 1]), np.asarray([0, 1]), np.asarray([0])]\n        self.conv_type = ""DENSE""\n\n    def get_output(self):\n        return self.outputs[self.iter]\n\n    def get_labels(self):\n        return self.labels[self.iter]\n\n    def get_current_losses(self):\n        return self.losses[self.iter]\n\n    def get_batch(self):\n        return self.batch_idx[self.iter]\n\n\nclass TestSegmentationTracker(unittest.TestCase):\n    def test_track(self):\n        tracker = ShapenetPartTracker(MockDataset())\n        model = MockModel()\n        tracker.track(model)\n        metrics = tracker.get_metrics(verbose=True)\n        for k in [""train_Cmiou"", ""train_Imiou""]:\n            self.assertAlmostEqual(metrics[k], 100, 5)\n\n        model.iter += 1\n        tracker.track(model)\n        metrics = tracker.get_metrics(verbose=True)\n        for k in [""train_Cmiou"", ""train_Imiou""]:\n            self.assertAlmostEqual(metrics[k], 100, 5)\n\n        model.iter += 1\n        tracker.track(model)\n        metrics = tracker.get_metrics(verbose=True)\n        self.assertAlmostEqual(metrics[""train_Imiou""], 4 * 100 / 5)\n        self.assertAlmostEqual(metrics[""train_Cmiou""], (100 + 200 / 3.0) / 2.0)\n        # for k in [""train_Cmiou"", ""train_Imiou""]:\n        #     self.assertAlmostEqual(metrics[k], 100, 5)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_sphere_sampling.py,5,"b'import os\nimport sys\nimport unittest\nimport numpy as np\nimport torch\nfrom torch_geometric.data import Data\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.insert(0, ROOT)\nfrom torch_points3d.core.data_transform.transforms import RandomSphere, SphereSampling\n\n\nclass TestRandomSphere(unittest.TestCase):\n    def setUp(self):\n\n        pos = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 1]])\n        labels = np.array([0, 0, 0, 0, 0, 0])\n\n        self.data = Data(pos=torch.from_numpy(pos).float(), labels=torch.from_numpy(labels))\n\n    def test_neighbour_found_under_random_sampling(self):\n        random_sphere = RandomSphere(0.1, strategy=""RANDOM"")\n        data = random_sphere(self.data.clone())\n        assert data.labels.shape[0] == 1\n\n        random_sphere = RandomSphere(3, strategy=""RANDOM"")\n        data = random_sphere(self.data.clone())\n        assert data.labels.shape[0] == 6\n\n\nclass TestSphereSampling(unittest.TestCase):\n    def setUp(self):\n        pos = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 0], [0, 0, 1], [1, 1, 1], [0, 1, 1]])\n        labels = torch.tensor([0, 1, 2, 0, 0, 0])\n        self.data = Data(pos=pos.float(), labels=labels)\n\n    def test_sphere(self):\n        sphere_sampling = SphereSampling(0.1, [0, 0, 0])\n        sampled = sphere_sampling(self.data)\n\n        self.assertIn(SphereSampling.KDTREE_KEY, self.data)\n        self.assertEqual(len(sampled.labels), 1)\n        self.assertEqual(sampled.labels[0], 2)\n\n    def test_align(self):\n        sphere_sampling = SphereSampling(0.1, [1, 0, 0])\n        sampled = sphere_sampling(self.data)\n        torch.testing.assert_allclose(sampled.pos, torch.tensor([[0.0, 0, 0]]))\n        self.assertEqual(sampled.labels[0], 0)\n\n        sphere_sampling = SphereSampling(0.1, [1, 0, 0], align_origin=False)\n        sampled = sphere_sampling(self.data)\n        torch.testing.assert_allclose(sampled.pos, torch.tensor([[1.0, 0, 0]]))\n        self.assertEqual(sampled.labels[0], 0)\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_to_sparse.py,9,"b'import unittest\nimport sys\nimport os\nimport numpy as np\nimport torch\nfrom itertools import combinations\nfrom torch_geometric.data import Data\nfrom omegaconf.dictconfig import DictConfig\nfrom omegaconf.listconfig import ListConfig\nfrom omegaconf import OmegaConf\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom torch_points3d.core.data_transform import ToSparseInput\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\n\n\nclass TestSparse(unittest.TestCase):\n    def test_to_sparse_input(self):\n        arr = np.asarray([[0, 0, 0], [0, 1, 0], [0, 1, 0.25], [0.25, 0.25, 0]])\n        np_feat = [0, 1, 2, 3]\n        feat = torch.tensor(np_feat)\n        data = Data(pos=torch.from_numpy(arr), x=feat)\n        transform = ToSparseInput(grid_size=1, mode=""last"")\n        data_out = transform(data.clone())\n\n        self.assertEqual(data_out.pos.dtype, torch.int)\n        self.assertEqual(2, data_out.pos.shape[0])\n\n        combi = list(combinations(np_feat, 2))\n        tensors = [torch.tensor(t) for t in combi] + [torch.tensor(t[::-1]) for t in combi]\n\n        is_in = False\n        for t in tensors:\n            if torch.eq(data_out.x, t).sum().item() == 2:\n                is_in = True\n        self.assertEqual(is_in, True)\n\n    def test_to_sparse_input_mean(self):\n        arr = np.asarray([[0, 0, 0], [0, 1, 0], [0, 1, 0.25], [0.25, 0.25, 0]])\n        feat = torch.tensor([0, 1.0, 2.0, 4.0])\n        data = Data(pos=torch.from_numpy(arr), x=feat)\n        transform = ToSparseInput(grid_size=1, mode=""mean"")\n        data_out = transform(data.clone())\n\n        self.assertEqual(data_out.pos.dtype, torch.int)\n        self.assertEqual(2, data_out.pos.shape[0])\n        torch.testing.assert_allclose(data_out.x, torch.tensor([2, 1.5]))\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_transform.py,35,"b'import unittest\nimport sys\nimport os\nimport torch_geometric.transforms as T\nimport numpy as np\nimport numpy.testing as npt\nimport torch\nfrom torch_geometric.data import Data\nfrom omegaconf.dictconfig import DictConfig\nfrom omegaconf.listconfig import ListConfig\nfrom omegaconf import OmegaConf\n\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nsys.path.insert(0, os.path.join(DIR_PATH, ""..""))\n\nfrom torch_points3d.core.data_transform import (\n    instantiate_transform,\n    instantiate_transforms,\n    GridSampling3D,\n    MultiScaleTransform,\n    Random3AxisRotation,\n    AddFeatByKey,\n    AddFeatsByKeys,\n    RemoveAttributes,\n    RandomDropout,\n    ShiftVoxels,\n    PCACompute,\n    RandomCoordsFlip,\n    RemoveDuplicateCoords,\n    XYZFeature,\n    ScalePos,\n)\nfrom torch_points3d.core.spatial_ops import RadiusNeighbourFinder, KNNInterpolate\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\n\nnp.random.seed(0)\n\n\nclass Testhelpers(unittest.TestCase):\n    def test_Instantiate(self):\n        conf = DictConfig({""transform"": ""GridSampling3D"", ""params"": {""size"": 0.1}})\n        t = instantiate_transform(conf)\n        self.assertIsInstance(t, GridSampling3D)\n\n        conf = DictConfig({""transform"": ""None"", ""params"": {""size"": 0.1}})\n        with self.assertRaises(ValueError):\n            t = instantiate_transform(conf)\n\n    def test_InstantiateTransforms(self):\n        conf = ListConfig([{""transform"": ""GridSampling3D"", ""params"": {""size"": 0.1}}, {""transform"": ""Center""},])\n        t = instantiate_transforms(conf)\n        self.assertIsInstance(t.transforms[0], GridSampling3D)\n        self.assertIsInstance(t.transforms[1], T.Center)\n\n    def test_multiscaleTransforms(self):\n        samplers = [GridSampling3D(0.25), None, GridSampling3D(0.5)]\n        search = [\n            RadiusNeighbourFinder(0.5, 100, ConvolutionFormat.PARTIAL_DENSE.value),\n            RadiusNeighbourFinder(0.5, 150, ConvolutionFormat.PARTIAL_DENSE.value),\n            RadiusNeighbourFinder(1, 200, ConvolutionFormat.PARTIAL_DENSE.value),\n        ]\n        upsampler = [KNNInterpolate(1), KNNInterpolate(1)]\n\n        N = 10\n        x = np.linspace(0, 1, N)\n        y = np.linspace(0, 1, N)\n        xv, yv = np.meshgrid(x, y)\n\n        pos = torch.tensor([xv.flatten(), yv.flatten(), np.zeros(N * N)]).T\n        x = torch.ones_like(pos)\n        d = Data(pos=pos, x=x).contiguous()\n        ms_transform = MultiScaleTransform({""sampler"": samplers, ""neighbour_finder"": search, ""upsample_op"": upsampler})\n\n        transformed = ms_transform(d.clone())\n        npt.assert_almost_equal(transformed.x.numpy(), x.numpy())\n        npt.assert_almost_equal(transformed.pos.numpy(), pos.numpy())\n\n        ms = transformed.multiscale\n        npt.assert_almost_equal(ms[0].pos.numpy(), ms[1].pos.numpy())\n        npt.assert_almost_equal(ms[0].pos.numpy(), samplers[0](d.clone()).pos.numpy())\n        npt.assert_almost_equal(ms[2].pos.numpy(), samplers[2](ms[0].clone()).pos.numpy())\n\n        self.assertEqual(ms[0].__inc__(""idx_neighboors"", 0), pos.shape[0])\n        idx = search[0](\n            d.pos,\n            ms[0].pos,\n            torch.zeros((d.pos.shape[0]), dtype=torch.long),\n            torch.zeros((ms[0].pos.shape[0]), dtype=torch.long),\n        )\n        for i in range(len(ms[0].idx_neighboors)):\n            self.assertEqual(set(ms[0].idx_neighboors[i].tolist()), set(idx[i].tolist()))\n        self.assertEqual(ms[1].idx_neighboors.shape[1], 150)\n        self.assertEqual(ms[2].idx_neighboors.shape[1], 200)\n\n        upsample = transformed.upsample\n        self.assertEqual(upsample[0].num_nodes, ms[1].num_nodes)\n        self.assertEqual(upsample[1].num_nodes, pos.shape[0])\n        self.assertEqual(upsample[1].x_idx.max(), ms[0].num_nodes - 1)\n        self.assertEqual(upsample[1].y_idx.max(), pos.shape[0] - 1)\n        self.assertEqual(upsample[1].__inc__(""x_idx"", 0), ms[0].num_nodes)\n        self.assertEqual(upsample[1].__inc__(""y_idx"", 0), pos.shape[0])\n\n    def test_AddFeatByKey(self):\n\n        add_to_x = [False, True]\n        feat_name = [""y"", ""none""]\n        strict = [False, True]\n        input_nc_feat = [None, 1, 2]\n\n        c = 0\n        for atx in add_to_x:\n            for fn in feat_name:\n                for ine in input_nc_feat:\n                    for s in strict:\n                        fn_none = False\n                        ine_2 = False\n                        try:\n                            data = Data(x=torch.randn((10)), pos=torch.randn((10)), y=torch.randn((10)))\n                            transform = AddFeatByKey(atx, fn, input_nc_feat=ine, strict=s)\n                            data = transform(data)\n                        except Exception:\n                            if fn == ""none"":\n                                fn_none = True\n                            if ine == 2:\n                                ine_2 = True\n\n                        if fn_none or ine_2:\n                            c += 1\n                            continue\n\n                        if not atx:\n                            self.assertEqual(data.x.shape, torch.Size([10]))\n                        else:\n                            if fn == ""none"":\n                                self.assertEqual(data.x.shape, torch.Size([10]))\n                            else:\n                                self.assertEqual(data.x.shape, torch.Size([10, 2]))\n\n                        c += 1\n\n    def test_AddFeatsByKeys(self):\n        N = 10\n        mapping = {""a"": 1, ""b"": 2, ""c"": 3, ""d"": 4}\n        keys, values = np.asarray(list(mapping.keys())), np.asarray(list(mapping.values()))\n        data = Data(\n            a=torch.randn((N, 1)),\n            b=torch.randn((N, 2)),\n            c=torch.randn((N, 3)),\n            d=torch.randn((N, 4)),\n            pos=torch.randn((N)),\n        )\n        mask = np.random.uniform(0, 1, (4)) > 0.1\n        transform = AddFeatsByKeys(mask, keys)\n        data_out = transform(data)\n        self.assertEqual(data_out.x.shape[-1], np.sum(values[mask]))\n\n    def test_RemoveAttributes(self):\n        N = 10\n        mapping = {""a"": 1, ""b"": 2, ""c"": 3, ""d"": 4}\n        keys = np.asarray(list(mapping.keys()))\n        data = Data(\n            a=torch.randn((N, 1)),\n            b=torch.randn((N, 2)),\n            c=torch.randn((N, 3)),\n            d=torch.randn((N, 4)),\n            pos=torch.randn((N)),\n        )\n        mask = np.random.uniform(0, 1, (4)) > 0.5\n        transform = RemoveAttributes(keys[mask])\n        data_out = transform(data)\n        for key in keys[mask]:\n            self.assertNotIn(key, list(data_out.keys))\n\n    def test_RemoveDuplicateCoords(self):\n        indices = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 0, 0]])\n        data = Data(pos=torch.from_numpy(indices))\n        transform = RemoveDuplicateCoords()\n        data_out = transform(data)\n        self.assertEqual(data_out.pos.shape[0], 5)\n\n    def test_dropout(self):\n        indices = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 0, 0]])\n        data = Data(pos=torch.from_numpy(indices))\n        tr = RandomDropout(dropout_ratio=0.5, dropout_application_ratio=1.1)\n        data = tr(data)\n        self.assertEqual(len(data.pos), 3)\n\n    def test_shiftvoxels(self):\n        indices = np.asarray([[0, 0, 0], [1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 0], [0, 0, 0]])\n        data = Data(pos=torch.from_numpy(indices).int())\n        tr = ShiftVoxels()\n        tr_data = tr(data.clone())\n        self.assertGreaterEqual(tr_data.pos[0][0], data.pos[0][0])\n\n    def test_PCACompute(self):\n        vec1 = torch.randn(3)\n        vec1 = vec1 / torch.norm(vec1)\n        vec2 = torch.randn(3)\n        vec2 = vec2 / torch.norm(vec2)\n        norm = vec1.cross(vec2) / torch.norm(vec1.cross(vec2))\n        plane = torch.randn(100, 1) * vec1.view(1, 3) + torch.randn(100, 1) * vec2.view(1, 3)\n        data = Data(pos=plane)\n        pca = PCACompute()\n        data = pca(data)\n        npt.assert_allclose(np.abs(data.eigenvectors[:, 0].dot(norm).item()), 1.0, atol=1e-5)\n\n    def test_Random3AxisRotation(self):\n\n        pos = np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1]]).astype(np.float)\n        data = Data(pos=torch.from_numpy(pos).float())\n        t = Random3AxisRotation(apply_rotation=True, rot_x=0, rot_y=0, rot_z=180)\n\n        u, v, w = data.pos\n        u2, v2, w2 = t(data.clone()).pos\n\n        self.assertEqual(np.array_equal(u, u2), False)\n        self.assertEqual(np.array_equal(v, v2), False)\n        npt.assert_array_equal(w, w2)\n\n        t = Random3AxisRotation(apply_rotation=True, rot_x=180, rot_y=180, rot_z=180)\n\n        u2, v2, w2 = t(data.clone()).pos\n\n        self.assertEqual(np.array_equal(u, u2), False)\n        self.assertEqual(np.array_equal(v, v2), False)\n        self.assertEqual(np.array_equal(w, w2), False)\n\n        with self.assertRaises(Exception):\n            t = Random3AxisRotation(apply_rotation=True, rot_x=None, rot_y=None, rot_z=None)\n\n        pos = np.asarray([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 1, 1]]).astype(np.float)\n        data = Data(pos=torch.from_numpy(pos).float())\n        t = Random3AxisRotation(apply_rotation=True, rot_x=0, rot_y=0, rot_z=180)\n\n        self.assertEqual(t(data.clone()).pos.shape, torch.Size([4, 3]))\n\n    def test_RandomCoordsFlip(self):\n\n        pos = torch.from_numpy(np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n        pos_target = torch.from_numpy(np.asarray([[6, 2, 3], [3, 5, 6], [0, 8, 9]]))\n        data = Data(pos=pos)\n\n        upright_axis = [""y"", ""z""]\n        t = RandomCoordsFlip(upright_axis, p=1)\n\n        pos_out = t(data.clone()).pos\n\n        self.assertEqual(np.array_equal(pos_out, pos_target), True)\n\n    def test_XYZFeature(self):\n\n        pos = torch.from_numpy(np.asarray([[1, 2, 3], [4, 5, 6], [7, 8, 9]]))\n\n        data = Data(pos=pos)\n        t = XYZFeature()\n\n        data_out = t(data.clone())\n\n        x = data_out.pos_z\n\n        self.assertEqual(np.array_equal(x, pos[:, -1]), True)\n\n        pos += 1\n\n        self.assertEqual(np.array_equal(x, pos[:, -1]), False)\n\n        self.assertIn(""pos_z"", data_out.keys)\n        self.assertIn(""pos"", data_out.keys)\n        self.assertNotIn(""pos_x"", data_out.keys)\n        self.assertNotIn(""pos_y"", data_out.keys)\n\n    def test_scalePos(self):\n        tr = ScalePos(scale=2.0)\n        d = Data(pos=torch.tensor([[1, 0, 0], [0, 1, 1]]).float())\n        d = tr(d)\n        torch.testing.assert_allclose(d.pos, torch.tensor([[2, 0, 0], [0, 2, 2]]).float())\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_unwrapped_unet_base.py,3,"b'import unittest\nfrom omegaconf import OmegaConf\nimport torch\nimport os\nimport sys\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."")\nsys.path.append(ROOT)\n\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\nfrom torch_points3d.models.base_architectures import UnwrappedUnetBasedModel\n\n\nclass MockModel(UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        # call the initialization method of UnwrappedUnetBasedModel\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n\n\nclass ConvMockDown(torch.nn.Module):\n    def __init__(self, test_precompute=False, *args, **kwargs):\n        super().__init__()\n        self.kwargs = kwargs\n        self.test_precompute = test_precompute\n\n    def forward(self, data, *args, **kwargs):\n        data.append(self.kwargs[""down_conv_nn""])\n        if self.test_precompute:\n            assert kwargs[""precomputed""] is not None\n        return data\n\n\nclass InnerMock(torch.nn.Module):\n    def __init__(self, test_precompute=False, *args, **kwargs):\n        super().__init__()\n        self.kwargs = kwargs\n\n    def forward(self, data, *args, **kwargs):\n        data.append(""inner"")\n        return data\n\n\nclass ConvMockUp(torch.nn.Module):\n    def __init__(self, test_precompute=False, *args, **kwargs):\n        super().__init__()\n        self.kwargs = kwargs\n        self.test_precompute = test_precompute\n\n    def forward(self, data, *args, **kwargs):\n        data = data[0].copy()\n        data.append(self.kwargs[""up_conv_nn""])\n        if self.test_precompute:\n            assert kwargs[""precomputed""] is not None\n        return data\n\n\nclass MockModelLib:\n    ConvMockUp = ConvMockUp\n    ConvMockDown = ConvMockDown\n    InnerMock = InnerMock\n\n\nclass TestUnwrapperUnet(unittest.TestCase):\n    def test_forward(self):\n        models_conf = os.path.join(ROOT, ""test/config_unwrapped_unet_base/test_models.yaml"")\n        models_conf = OmegaConf.load(models_conf).models\n        modellib = MockModelLib()\n        model = MockModel(models_conf[""TestUnwrapper""], """", None, modellib)\n        data = []\n        d = model(data)\n        self.assertEqual(d, [0, 1, 2, 3, ""inner"", 4, 5, 6, 7])\n\n    def test_forwardprecompute(self):\n        models_conf = os.path.join(ROOT, ""test/config_unwrapped_unet_base/test_models.yaml"")\n        models_conf = OmegaConf.load(models_conf).models\n        modellib = MockModelLib()\n        model = MockModel(models_conf[""TestPrecompute""], """", None, modellib)\n        data = []\n        d = model(data, precomputed_up=""Hey"", precomputed_down=""Yay"")\n        self.assertEqual(d, [0, 1, 2, 3, ""inner"", 4, 5, 6, 7])\n\n    def test_noinnermost(self):\n        models_conf = os.path.join(ROOT, ""test/config_unwrapped_unet_base/test_models.yaml"")\n        models_conf = OmegaConf.load(models_conf).models\n        modellib = MockModelLib()\n        model = MockModel(models_conf[""TestNoInnermost""], """", None, modellib)\n        data = []\n        d = model(data, precomputed_up=""Hey"", precomputed_down=""Yay"")\n        self.assertEqual(d, [0, 1, 2, 3, 4, 5, 6])\n\n    def test_unbalanced(self):\n        models_conf = os.path.join(ROOT, ""test/config_unwrapped_unet_base/test_models.yaml"")\n        models_conf = OmegaConf.load(models_conf).models\n        modellib = MockModelLib()\n        model = MockModel(models_conf[""TestUnbalanced""], """", None, modellib)\n        data = []\n        d = model(data, precomputed_up=""Hey"", precomputed_down=""Yay"")\n        self.assertEqual(d, [0, 1, 2, 3, 4])\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/test_visualization.py,17,"b'import unittest\nimport os\nimport shutil\nimport sys\nfrom omegaconf import OmegaConf\nimport torch\nfrom torch_geometric.data import Data\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom torch_points3d.visualization import Visualizer\n\nbatch_size = 2\nepochs = 5\nnum_points = 20\n\n\ndef run(iter, visualizer, epoch, stage, data):\n    visualizer.reset(epoch, stage)\n    for i in range(iter):\n        visualizer.save_visuals(data)\n\n\nclass TestVisualizer(unittest.TestCase):\n    def test_empty(self):\n        mock_data = Data()\n        mock_data.pos = torch.zeros((batch_size, num_points, 3))\n        mock_data.y = torch.zeros((batch_size, num_points, 1))\n        mock_data.pred = torch.zeros((batch_size, num_points, 1))\n        data = {}\n\n        self.run_path = os.path.join(DIR, ""test_viz"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        mock_num_batches = {""train"": 9, ""test"": 3, ""val"": 0}\n        config = OmegaConf.load(os.path.join(DIR, ""test_config/viz/viz_config_indices.yaml""))\n        visualizer = Visualizer(config.visualization, mock_num_batches, batch_size, self.run_path)\n\n        for epoch in range(epochs):\n            run(9, visualizer, epoch, ""train"", data)\n            run(3, visualizer, epoch, ""test"", data)\n            run(2, visualizer, epoch, ""val"", data)\n\n        self.assertEqual(len(os.listdir(os.path.join(self.run_path, ""viz""))), 0)\n        shutil.rmtree(self.run_path)\n\n    def test_indices(self):\n        mock_data = Data()\n        mock_data.pos = torch.zeros((batch_size, num_points, 3))\n        mock_data.y = torch.zeros((batch_size, num_points, 1))\n        mock_data.pred = torch.zeros((batch_size, num_points, 1))\n        data = {""mock_date"": mock_data}\n\n        self.run_path = os.path.join(DIR, ""test_viz"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        mock_num_batches = {""train"": 9, ""test"": 3, ""val"": 0}\n        config = OmegaConf.load(os.path.join(DIR, ""test_config/viz/viz_config_indices.yaml""))\n        visualizer = Visualizer(config.visualization, mock_num_batches, batch_size, self.run_path)\n\n        for epoch in range(epochs):\n            run(9, visualizer, epoch, ""train"", data)\n            run(3, visualizer, epoch, ""test"", data)\n            run(0, visualizer, epoch, ""val"", data)\n\n        targets = set([""1_1.ply"", ""0_0.ply""])\n        for split in [""train""]:\n            for epoch in range(epochs):\n                self.assertEqual(targets, set(os.listdir(os.path.join(self.run_path, ""viz"", str(epoch), split))))\n        shutil.rmtree(self.run_path)\n\n    def test_save_all(self):\n        mock_data = Data()\n        mock_data.pos = torch.zeros((num_points * batch_size, 3))\n        mock_data.y = torch.zeros((num_points * batch_size, 1))\n        mock_data.pred = torch.zeros((num_points * batch_size, 1))\n        mock_data.batch = torch.zeros((num_points * batch_size))\n        mock_data.batch[:num_points] = 1\n        data = {""mock_date"": mock_data}\n\n        self.run_path = os.path.join(DIR, ""test_viz"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        epochs = 2\n        num_samples = 100\n        mock_num_batches = {""train"": num_samples}\n\n        config = OmegaConf.load(os.path.join(DIR, ""test_config/viz/viz_config_save_all.yaml""))\n        visualizer = Visualizer(config.visualization, mock_num_batches, batch_size, self.run_path)\n\n        for epoch in range(epochs):\n            run(num_samples // batch_size, visualizer, epoch, ""train"", data)\n\n        for split in [""train""]:\n            for epoch in range(epochs):\n                current = set(os.listdir(os.path.join(self.run_path, ""viz"", str(epoch), split)))\n                self.assertGreaterEqual(len(current), num_samples)\n        shutil.rmtree(self.run_path)\n\n    def test_pyg_data(self):\n        mock_data = Data()\n        mock_data.pos = torch.zeros((num_points * batch_size, 3))\n        mock_data.y = torch.zeros((num_points * batch_size, 1))\n        mock_data.pred = torch.zeros((num_points * batch_size, 1))\n        mock_data.batch = torch.zeros((num_points * batch_size))\n        mock_data.batch[:num_points] = 1\n        data = {""mock_date"": mock_data}\n\n        self.run_path = os.path.join(DIR, ""test_viz"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        epochs = 10\n        num_batches = 100\n        mock_num_batches = {""train"": num_batches}\n\n        config = OmegaConf.load(os.path.join(DIR, ""test_config/viz/viz_config_non_deterministic.yaml""))\n        visualizer = Visualizer(config.visualization, mock_num_batches, batch_size, self.run_path)\n\n        for epoch in range(epochs):\n            run(num_batches, visualizer, epoch, ""train"", data)\n\n        count = 0\n        for split in [""train""]:\n            target = set(os.listdir(os.path.join(self.run_path, ""viz"", ""0"", split)))\n            for epoch in range(1, epochs):\n                current = set(os.listdir(os.path.join(self.run_path, ""viz"", str(epoch), split)))\n                count += 1 if len(target & current) == 0 else 0\n        self.assertGreaterEqual(count, 4)\n        shutil.rmtree(self.run_path)\n\n    def test_dense_data(self):\n        mock_data = Data()\n        mock_data.pos = torch.zeros((batch_size, num_points, 3))\n        mock_data.y = torch.zeros((batch_size, num_points, 1))\n        mock_data.pred = torch.zeros((batch_size, num_points, 1))\n        data = {""mock_date"": mock_data}\n\n        self.run_path = os.path.join(DIR, ""test_viz"")\n        if not os.path.exists(self.run_path):\n            os.makedirs(self.run_path)\n\n        mock_num_batches = {""train"": 9, ""test"": 3, ""val"": 0}\n        config = OmegaConf.load(os.path.join(DIR, ""test_config/viz/viz_config_deterministic.yaml""))\n        visualizer = Visualizer(config.visualization, mock_num_batches, batch_size, self.run_path)\n\n        for epoch in range(epochs):\n            run(9, visualizer, epoch, ""train"", data)\n            run(3, visualizer, epoch, ""test"", data)\n            run(0, visualizer, epoch, ""val"", data)\n\n        for split in [""train"", ""test""]:\n            targets = os.listdir(os.path.join(self.run_path, ""viz"", ""0"", split))\n            for epoch in range(1, epochs):\n                self.assertEqual(targets, os.listdir(os.path.join(self.run_path, ""viz"", str(epoch), split)))\n        shutil.rmtree(self.run_path)\n\n    def tearDown(self):\n        try:\n            shutil.rmtree(self.run_path)\n        except:\n            pass\n\n\nif __name__ == ""__main__"":\n    unittest.main()\n'"
test/utils.py,0,"b'def test_hasgrad(model, strict=False, verbose=False):\n    """""" Tests if a pytorch module has got parameters with gradient equal to 0. Returns the\n    ratio of parameters with 0 gradient against the total number of parameters\n\n    Parameters\n    ----------\n    strict:\n        If True then raises an error\n    verbose:\n        If True then displays all the parameter\'s names that have 0 gradient\n    """"""\n    count = 0\n    total_params = 0\n    for name, p in model.named_parameters():\n        if p.requires_grad:\n            total_params += 1\n            assert p.grad is not None\n            if p.grad.nonzero().sum() == 0:\n                if verbose:\n                    print(""Param with name %s has 0 grad"" % name)\n                count += 1\n    if count > 0:\n        msg = ""Model has %.2f%% of parameters with 0 gradient"" % (count / (1.0 * total_params))\n        if strict:\n            raise ValueError(msg)\n        else:\n            return count / (1.0 * total_params)\n    return 1\n'"
torch_points3d/__init__.py,0,b''
scripts/datasets/download-scannet.py,0,"b'#!/usr/bin/env python\n# Downloads ScanNet public data release\n# Run with ./download-scannet.py (or python download-scannet.py on Windows)\n# -*- coding: utf-8 -*-\nimport argparse\nimport os\n#import urllib.request (for python3)\nimport tempfile\n\nimport sys\n\nimport urllib\n\nif sys.version_info[0] == 3:\n    from urllib.request import urlopen\nelse:\n    # Not Python 3 - today, it is most likely to be Python 2\n    # But note that this might need an update when Python 4\n    # might be around one day\n    from urllib import urlopen\n\nBASE_URL = \'http://kaldir.vc.in.tum.de/scannet/\'\nTOS_URL = BASE_URL + \'ScanNet_TOS.pdf\'\nFILETYPES = [\'.aggregation.json\', \'.sens\', \'.txt\', \'_vh_clean.ply\', \'_vh_clean_2.0.010000.segs.json\', \'_vh_clean_2.ply\', \'_vh_clean.segs.json\', \'_vh_clean.aggregation.json\', \'_vh_clean_2.labels.ply\', \'_2d-instance.zip\', \'_2d-instance-filt.zip\', \'_2d-label.zip\', \'_2d-label-filt.zip\']\nFILETYPES_TEST = [\'.sens\', \'.txt\', \'_vh_clean.ply\', \'_vh_clean_2.ply\']\nPREPROCESSED_FRAMES_FILE = [\'scannet_frames_25k.zip\', \'5.6GB\']\nTEST_FRAMES_FILE = [\'scannet_frames_test.zip\', \'610MB\']\nLABEL_MAP_FILES = [\'scannetv2-labels.combined.tsv\', \'scannet-labels.combined.tsv\']\nRELEASES = [\'v2/scans\', \'v1/scans\']\nRELEASES_TASKS = [\'v2/tasks\', \'v1/tasks\']\nRELEASES_NAMES = [\'v2\', \'v1\']\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = \'1.2TB\'\nV1_IDX = 1\n\nFAILED_DOWNLOAD = []\n\ndef get_release_scans(release_file):\n    scan_lines = urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode(\'utf8\').rstrip(\'\\n\')\n        scans.append(scan_id)\n    return scans\n\n\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:\n        return\n    print(\'Downloading ScanNet \' + RELEASE_NAME + \' release to \' + out_dir + \'...\')\n    for scan_id in release_scans:\n        scan_out_dir = os.path.join(out_dir, scan_id)\n        download_scan(scan_id, scan_out_dir, file_types, use_v1_sens)\n    print(\'Downloaded ScanNet \' + RELEASE_NAME + \' release.\')\n\n\ndef download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if not os.path.isfile(out_file):\n        print(\'\\t\' + url + \' > \' + out_file)\n        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n        f = os.fdopen(fh, \'w\')\n        f.close()\n        urllib.request.urlretrieve(url, out_file_tmp)\n        #urllib.urlretrieve(url, out_file_tmp)\n        os.rename(out_file_tmp, out_file)\n    else:\n        print(\'WARNING: skipping download of existing file \' + out_file)\n\ndef download_scan(scan_id, out_dir, file_types, use_v1_sens):\n    try:\n        print(\'Downloading ScanNet \' + RELEASE_NAME + \' scan \' + scan_id + \' ...\')\n        if not os.path.isdir(out_dir):\n            os.makedirs(out_dir)\n        for ft in file_types:\n            v1_sens = use_v1_sens and ft == \'.sens\'\n            url = BASE_URL + RELEASE + \'/\' + scan_id + \'/\' + scan_id + ft if not v1_sens else BASE_URL + RELEASES[V1_IDX] + \'/\' + scan_id + \'/\' + scan_id + ft\n            out_file = out_dir + \'/\' + scan_id + ft\n            download_file(url, out_file)\n        print(\'Downloaded scan \' + scan_id)\n    except:\n        FAILED_DOWNLOAD.append(scan_id)\n\n\ndef download_task_data(out_dir):\n    print(\'Downloading ScanNet v1 task data...\')\n    files = [\n        LABEL_MAP_FILES[V1_IDX], \'obj_classification/data.zip\',\n        \'obj_classification/trained_models.zip\', \'voxel_labeling/data.zip\',\n        \'voxel_labeling/trained_models.zip\'\n    ]\n    for file in files:\n        url = BASE_URL + RELEASES_TASKS[V1_IDX] + \'/\' + file\n        localpath = os.path.join(out_dir, file)\n        localdir = os.path.dirname(localpath)\n        if not os.path.isdir(localdir):\n          os.makedirs(localdir)\n        download_file(url, localpath)\n    print(\'Downloaded task data.\')\n\n\ndef download_label_map(out_dir):\n    print(\'Downloading ScanNet \' + RELEASE_NAME + \' label mapping file...\')\n    files = [ LABEL_MAP_FILE ]\n    for file in files:\n        url = BASE_URL + RELEASE_TASKS + \'/\' + file\n        localpath = os.path.join(out_dir, file)\n        localdir = os.path.dirname(localpath)\n        if not os.path.isdir(localdir):\n          os.makedirs(localdir)\n        download_file(url, localpath)\n    print(\'Downloaded ScanNet \' + RELEASE_NAME + \' label mapping file.\')\n\n\ndef main():\n    parser = argparse.ArgumentParser(description=\'Downloads ScanNet public data release.\')\n    parser.add_argument(\'-o\', \'--out_dir\', required=True, help=\'directory in which to download\')\n    parser.add_argument(\'--task_data\', action=\'store_true\', help=\'download task data (v1)\')\n    parser.add_argument(\'--label_map\', action=\'store_true\', help=\'download label map file\')\n    parser.add_argument(\'--v1\', action=\'store_true\', help=\'download ScanNet v1 instead of v2\')\n    parser.add_argument(\'--id\', help=\'specific scan id to download\')\n    parser.add_argument(\'--preprocessed_frames\', action=\'store_true\', help=\'download preprocessed subset of ScanNet frames (\' + PREPROCESSED_FRAMES_FILE[1] + \')\')\n    parser.add_argument(\'--test_frames_2d\', action=\'store_true\', help=\'download 2D test frames (\' + TEST_FRAMES_FILE[1] + \'; also included with whole dataset download)\')\n    parser.add_argument(\'--types\', nargs=\'+\', help=\'specific file type to download (.aggregation.json, .sens, .txt, _vh_clean.ply, _vh_clean_2.0.010000.segs.json, _vh_clean_2.ply, _vh_clean.segs.json, _vh_clean.aggregation.json, _vh_clean_2.labels.ply, _2d-instance.zip, _2d-instance-filt.zip, _2d-label.zip, _2d-label-filt.zip)\')\n    args = parser.parse_args()\n\n    print(\'By pressing any key to continue you confirm that you have agreed to the ScanNet terms of use as described at:\')\n    print(TOS_URL)\n    print(\'***\')\n    print(\'Press any key to continue, or CTRL-C to exit.\')\n    key = input(\'\')\n\n    if args.v1:\n        global RELEASE\n        global RELEASE_TASKS\n        global RELEASE_NAME\n        global LABEL_MAP_FILE\n        RELEASE = RELEASES[V1_IDX]\n        RELEASE_TASKS = RELEASES_TASKS[V1_IDX]\n        RELEASE_NAME = RELEASES_NAMES[V1_IDX]\n        LABEL_MAP_FILE = LABEL_MAP_FILES[V1_IDX]\n\n    release_file = BASE_URL + RELEASE + \'.txt\'\n    release_scans = get_release_scans(release_file)\n    file_types = FILETYPES;\n    release_test_file = BASE_URL + RELEASE + \'_test.txt\'\n    release_test_scans = get_release_scans(release_test_file)\n    file_types_test = FILETYPES_TEST;\n    out_dir_scans = os.path.join(args.out_dir, \'scans\')\n    out_dir_test_scans = os.path.join(args.out_dir, \'scans_test\')\n    out_dir_tasks = os.path.join(args.out_dir, \'tasks\')\n\n    if args.types:  # download file type\n        file_types = args.types\n        for file_type in file_types:\n            if file_type not in FILETYPES:\n                print(\'ERROR: Invalid file type: \' + file_type)\n                return\n        file_types_test = []\n        for file_type in file_types:\n            if file_type not in FILETYPES_TEST:\n                file_types_test.append(file_type)\n    if args.task_data:  # download task data\n        download_task_data(out_dir_tasks)\n    elif args.label_map:  # download label map file\n        download_label_map(args.out_dir)\n    elif args.preprocessed_frames:  # download preprocessed scannet_frames_25k.zip file\n        if args.v1:\n            print(\'ERROR: Preprocessed frames only available for ScanNet v2\')\n        print(\'You are downloading the preprocessed subset of frames \' + PREPROCESSED_FRAMES_FILE[0] + \' which requires \' + PREPROCESSED_FRAMES_FILE[1] + \' of space.\')\n        download_file(os.path.join(BASE_URL, RELEASE_TASKS, PREPROCESSED_FRAMES_FILE[0]), os.path.join(out_dir_tasks, PREPROCESSED_FRAMES_FILE[0]))\n    elif args.test_frames_2d:  # download test scannet_frames_test.zip file\n        if args.v1:\n            print(\'ERROR: 2D test frames only available for ScanNet v2\')\n        print(\'You are downloading the 2D test set \' + TEST_FRAMES_FILE[0] + \' which requires \' + TEST_FRAMES_FILE[1] + \' of space.\')\n        download_file(os.path.join(BASE_URL, RELEASE_TASKS, TEST_FRAMES_FILE[0]), os.path.join(out_dir_tasks, TEST_FRAMES_FILE[0]))\n    elif args.id:  # download single scan\n        scan_id = args.id\n        is_test_scan = scan_id in release_test_scans\n        if scan_id not in release_scans and (not is_test_scan or args.v1):\n            print(\'ERROR: Invalid scan id: \' + scan_id)\n        else:\n            out_dir = os.path.join(out_dir_scans, scan_id) if not is_test_scan else os.path.join(out_dir_test_scans, scan_id)\n            scan_file_types = file_types if not is_test_scan else file_types_test\n            use_v1_sens = not is_test_scan\n            if not is_test_scan and not args.v1 and \'.sens\' in scan_file_types:\n                print(\'Note: ScanNet v2 uses the same .sens files as ScanNet v1: Press \\\'n\\\' to exclude downloading .sens files for each scan\')\n                key = input(\'\')\n                if key.strip().lower() == \'n\':\n                    scan_file_types.remove(\'.sens\')\n            download_scan(scan_id, out_dir, scan_file_types, use_v1_sens)\n    else:  # download entire release\n        if len(file_types) == len(FILETYPES):\n            print(\'WARNING: You are downloading the entire ScanNet \' + RELEASE_NAME + \' release which requires \' + RELEASE_SIZE + \' of space.\')\n        else:\n            print(\'WARNING: You are downloading all ScanNet \' + RELEASE_NAME + \' scans of type \' + file_types[0])\n        print(\'Note that existing scan directories will be skipped. Delete partially downloaded directories to re-download.\')\n        print(\'***\')\n        print(\'Press any key to continue, or CTRL-C to exit.\')\n        key = input(\'\')\n        if not args.v1 and \'.sens\' in file_types:\n            print(\'Note: ScanNet v2 uses the same .sens files as ScanNet v1: Press \\\'n\\\' to exclude downloading .sens files for each scan\')\n            key = input(\'\')\n            if key.strip().lower() == \'n\':\n                file_types.remove(\'.sens\')\n        download_release(release_scans, out_dir_scans, file_types, use_v1_sens=True)\n        if not args.v1:\n            download_label_map(args.out_dir)\n            download_release(release_test_scans, out_dir_test_scans, file_types_test, use_v1_sens=False)\n            download_file(os.path.join(BASE_URL, RELEASE_TASKS, TEST_FRAMES_FILE[0]), os.path.join(out_dir_tasks, TEST_FRAMES_FILE[0]))\n\n        print(""FAILED DOWNLOADING"")\n        print(FAILED_DOWNLOAD)\n\nif __name__ == ""__main__"": main()'"
scripts/test_registration_scripts/descriptor_matcher.py,0,"b'import pandas as pd\nimport hydra\nimport numpy as np\nfrom sklearn.neighbors import KDTree\nimport os\nimport os.path as osp\nfrom omegaconf import OmegaConf\nimport sys\nimport matplotlib.pyplot as plt\n\n# Import building function for model and dataset\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."", "".."")\nsys.path.insert(0, ROOT)\n\n\ndef read_gt_log(path):\n    """"""\n    read the gt.log of evaluation set of 3DMatch or ETH Dataset and parse it.\n    """"""\n    list_pair = []\n    list_mat = []\n    with open(path, ""r"") as f:\n        all_mat = f.readlines()\n    mat = np.zeros((4, 4))\n    for i in range(len(all_mat)):\n        if i % 5 == 0:\n            if i != 0:\n                list_mat.append(mat)\n            mat = np.zeros((4, 4))\n            list_pair.append(list(map(int, all_mat[i].split(""\\t"")[:-1])))\n        else:\n            line = all_mat[i].split(""\\t"")\n\n            mat[i % 5 - 1] = np.asarray(line[:4], dtype=np.float)\n    list_mat.append(mat)\n    return list_pair, list_mat\n\n\ndef compute_matches(feature_source, feature_target, kp_source, kp_target, ratio=False, sym=False):\n    """"""\n    compute matches between features\n    """"""\n\n    tree_source = KDTree(feature_source)\n    tree_target = KDTree(feature_target)\n    _, nn_ind_source = tree_target.query(feature_source, k=1)\n    _, nn_ind_target = tree_source.query(feature_target, k=1)\n\n    # symetric test\n    if sym:\n        indmatch = np.where(nn_ind_source.T[0][nn_ind_target.T[0]] == np.arange(len(feature_source)))[0]\n    else:\n        indmatch = np.arange(len(feature_target))\n    new_kp_source = np.copy(kp_source[nn_ind_target.T[0][indmatch]])\n    new_kp_target = np.copy(kp_target[indmatch])\n\n    return new_kp_source, new_kp_target\n\n\ndef compute_dists(kp_source, kp_target, trans):\n    """"""\n    compute distance between points that are matches\n    """"""\n    kp_target_t = kp_target.dot(trans[:3, :3].T) + trans[:3, 3]\n    dist = np.linalg.norm(kp_source - kp_target_t, axis=1)\n    return dist\n\n\ndef compute_mean_correct_matches(dist, list_tau, is_leq=True):\n    """"""\n    for each pair, save the name of the source,\n    """"""\n    res = []\n    for tau in list_tau:\n        if is_leq:\n            res.append(np.mean(dist < tau))\n        else:\n            res.append(np.mean(dist > tau))\n    return res\n\n\ndef pair_evaluation(path_descr_source, path_descr_target, gt_trans, list_tau, res_path):\n    """"""\n    save matches (indices)\n    """"""\n\n    data_source = np.load(path_descr_source)\n    data_target = np.load(path_descr_target)\n\n    feat_s = data_source[""feat""]\n    feat_t = data_target[""feat""]\n\n    if len(data_source[""feat""]) != len(data_source[""keypoints""]):\n        # Sampled features using keypoints.\n        feat_s = feat_s[data_source[""keypoints""]]\n        feat_t = feat_t[data_target[""keypoints""]]\n\n    kp_source, kp_target = compute_matches(\n        feat_s, feat_t, data_source[""pcd""][data_source[""keypoints""]], data_target[""pcd""][data_target[""keypoints""]],\n    )\n\n    dist = compute_dists(kp_source, kp_target, gt_trans)\n\n    n_s = osp.split(path_descr_source)[-1].split(""."")[0]\n    n_t = osp.split(path_descr_target)[-1].split(""."")[0]\n\n    frac_correct = compute_mean_correct_matches(dist, list_tau)\n\n    dico = dict(\n        kp_source=kp_source,\n        kp_target=kp_target,\n        dist=dist,\n        list_tau1=list_tau,\n        frac_correct=frac_correct,\n        name_source=n_s,\n        name_target=n_t,\n    )\n    print(n_s, n_t, frac_correct)\n    return dico\n\n\ndef compute_recall_scene(scene_name, list_pair, list_trans, list_tau1, list_tau2, res_path):\n    """"""\n    evaluate the recall for each scene\n    """"""\n    list_frac_correct = []\n    list_dico = []\n    for i, pair in enumerate(list_pair):\n        dico = pair_evaluation(pair[0], pair[1], list_trans[i], list_tau1, res_path)\n        list_frac_correct.append(dico[""frac_correct""])\n        list_dico.append(dico)\n\n    list_recall = compute_mean_correct_matches(np.asarray(list_frac_correct), list_tau2, is_leq=False)\n    print(""Save the matches"")\n    df_matches = pd.DataFrame(list_dico)\n    df_matches.to_csv(osp.join(res_path, ""matches.csv""))\n    dico = dict(scene_name=scene_name, list_tau2=list(list_tau2), list_recall=list(list_recall))\n    df_res = pd.DataFrame([dico])\n    df_res.to_csv(osp.join(res_path, ""res_recall.csv""))\n    return dico\n\n\ndef evaluate(path_raw_fragment, path_results, list_tau1, list_tau2):\n\n    """"""\n    launch the evaluation procedure\n    """"""\n\n    list_scene = os.listdir(path_raw_fragment)\n    list_total_res = []\n    for scene in list_scene:\n        print(scene)\n        path_log = osp.join(path_raw_fragment, scene, ""gt.log"")\n        list_pair_num, list_mat = read_gt_log(path_log)\n        list_pair = []\n        for pair in list_pair_num:\n            name0 = ""{}_{}_desc.npz"".format(""cloud_bin"", pair[0])\n            name1 = ""{}_{}_desc.npz"".format(""cloud_bin"", pair[1])\n            list_pair.append(\n                [osp.join(path_results, ""features"", scene, name0), osp.join(path_results, ""features"", scene, name1)]\n            )\n        res_path = osp.join(path_results, ""matches"", scene)\n        if not osp.exists(res_path):\n            os.makedirs(res_path, exist_ok=True)\n        dico = compute_recall_scene(scene, list_pair, list_mat, list_tau1, list_tau2, res_path)\n        list_total_res.append(dico)\n    total_recall = np.mean([d[""list_recall""] for d in list_total_res], axis=0)\n    list_total_res.append(dict(scene_name=""total"", list_tau2=list_tau2, list_recall=list(total_recall)))\n    df = pd.DataFrame(list_total_res)\n    df.to_csv(osp.join(path_results, ""matches"", ""total_res.csv""))\n\n\n@hydra.main(config_path=""conf/config.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)\n    print(cfg)\n    evaluate(cfg.path_raw_fragment, cfg.path_results, cfg.list_tau1, cfg.list_tau2)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/test_registration_scripts/fpfh.py,1,"b'import open3d\nimport torch\nimport numpy as np\nimport hydra\nimport os\nimport os.path as osp\nimport sys\nimport json\nfrom omegaconf import OmegaConf\n\n# Import building function for model and dataset\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."")\nsys.path.insert(0, ROOT)\n\nfrom test_registration_scripts.save_feature import save\n\n\nclass FPFH(object):\n    def __init__(self, radius=0.3, max_nn=128, radius_normal=0.3, max_nn_normal=17):\n        """"""\n        given a fragment, compute FPFH descriptor for keypoints\n        """"""\n        self.kdtree = open3d.geometry.KDTreeSearchParamHybrid(radius, max_nn)\n        self.kdtree_normal = open3d.geometry.KDTreeSearchParamHybrid(radius_normal, max_nn_normal)\n\n    def __call__(self, data):\n        pcd = open3d.geometry.PointCloud()\n        pcd.points = open3d.utility.Vector3dVector(data.pos.numpy())\n        pcd.estimate_normals(self.kdtree_normal)\n        fpfh_feature = open3d.registration.compute_fpfh_feature(pcd, self.kdtree)\n        return np.asarray(fpfh_feature.data).T[data.keypoints.numpy()]\n\n\n@hydra.main(config_path=""conf/fpfh.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)\n    print(cfg)\n    input_path = cfg.input_path\n    output_path = cfg.output_path\n    radius = cfg.radius\n    max_nn = cfg.max_nn\n    radius_normal = cfg.radius_normal\n    max_nn_normal = cfg.max_nn_normal\n\n    fpfh = FPFH(radius, max_nn, radius_normal, max_nn_normal)\n\n    list_frag = sorted([f for f in os.listdir(input_path) if ""fragment"" in f])\n    path_table = osp.join(input_path, ""table.json"")\n    with open(path_table, ""r"") as f:\n        table = json.load(f)\n\n    for i in range(len(list_frag)):\n        print(i, table[str(i)], list_frag[i])\n        data = torch.load(osp.join(input_path, list_frag[i]))\n        feat = fpfh(data)\n        save(osp.join(output_path, ""features""), table[str(i)][""scene_path""], table[str(i)][""fragment_name""], data, feat)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
scripts/test_registration_scripts/misc.py,0,"b'import numpy as np\n\n\ndef read_gt_log(path):\n    """"""\n    read the gt.log of evaluation set of 3DMatch or ETH Dataset and parse it.\n    """"""\n    list_pair = []\n    list_mat = []\n    with open(path, ""r"") as f:\n        all_mat = f.readlines()\n    mat = np.zeros((4, 4))\n    for i in range(len(all_mat)):\n        if i % 5 == 0:\n            if i != 0:\n                list_mat.append(mat)\n            mat = np.zeros((4, 4))\n            list_pair.append(list(map(int, all_mat[i].split(""\\t"")[:-1])))\n        else:\n            line = all_mat[i].split(""\\t"")\n\n            mat[i % 5 - 1] = np.asarray(line[:4], dtype=np.float)\n    list_mat.append(mat)\n    return list_pair, list_mat\n'"
scripts/test_registration_scripts/save_feature.py,7,"b'import open3d\nimport torch\nimport hydra\nimport logging\nfrom omegaconf import OmegaConf\nimport os\nimport os.path as osp\nimport sys\nimport numpy as np\n\nDIR = os.path.dirname(os.path.realpath(__file__))\nROOT = os.path.join(DIR, "".."", "".."")\nsys.path.insert(0, ROOT)\n\n# Import building function for model and dataset\nfrom torch_points3d.datasets.dataset_factory import instantiate_dataset, get_dataset_class\nfrom torch_points3d.models.model_factory import instantiate_model\n\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n# Import from metrics\nfrom torch_points3d.metrics.colored_tqdm import Coloredtqdm as Ctq\nfrom torch_points3d.metrics.model_checkpoint import ModelCheckpoint\n\n# Utils import\nfrom torch_points3d.utils.colors import COLORS\n\nlog = logging.getLogger(__name__)\n\n\ndef save(out_path, scene_name, pc_name, data, feature):\n    """"""\n    save pointcloud, feature and keypoint if it is asked\n    """"""\n\n    kp = None\n    # it must contain keypoints\n    if hasattr(data, ""keypoints""):\n        kp = data.keypoints\n    else:\n        kp = None\n    out_dir = osp.join(out_path, scene_name)\n    if not osp.exists(out_dir):\n        os.makedirs(out_dir)\n    out_file = osp.join(out_dir, pc_name.split(""."")[0] + ""_desc.npz"")\n    if hasattr(data, ""pos_x""):\n        pcd = torch.stack((data.pos_x, data.pos_y, data.pos_z)).T.numpy()\n    else:\n        pcd = data.pos.numpy()\n    np.savez(out_file, pcd=pcd, feat=feature, keypoints=kp)\n\n\ndef run(model: BaseModel, dataset: BaseDataset, device, output_path, cfg):\n    # Set dataloaders\n    num_fragment = dataset.num_fragment\n    if cfg.data.is_patch:\n        for i in range(num_fragment):\n            dataset.set_patches(i)\n            dataset.create_dataloaders(\n                model, cfg.training.batch_size, False, cfg.training.num_workers, False,\n            )\n            loader = dataset.test_dataloaders[0]\n            features = []\n            scene_name, pc_name = dataset.get_name(i)\n\n            with Ctq(loader) as tq_test_loader:\n                for data in tq_test_loader:\n                    # pcd = open3d.geometry.PointCloud()\n                    # pcd.points = open3d.utility.Vector3dVector(data.pos[0].numpy())\n                    # open3d.visualization.draw_geometries([pcd])\n                    with torch.no_grad():\n                        model.set_input(data, device)\n                        model.forward()\n                        features.append(model.get_output().cpu())\n            features = torch.cat(features, 0).numpy()\n            log.info(""save {} from {} in  {}"".format(pc_name, scene_name, output_path))\n            save(output_path, scene_name, pc_name, dataset.base_dataset[i].to(""cpu""), features)\n    else:\n        dataset.create_dataloaders(\n            model, 1, False, cfg.training.num_workers, False,\n        )\n        loader = dataset.test_dataloaders[0]\n        with Ctq(loader) as tq_test_loader:\n            for i, data in enumerate(tq_test_loader):\n                scene_name, pc_name = dataset.get_name(i)\n                # pcd = open3d.geometry.PointCloud()\n                # pcd.points = open3d.utility.Vector3dVector(data.pos.to(torch.float).numpy())\n                # open3d.visualization.draw_geometries([pcd])\n                with torch.no_grad():\n                    model.set_input(data, device)\n                    model.forward()\n                    features = model.get_output()  # batch of 1\n                    save(output_path, scene_name, pc_name, data.to(""cpu""), features.to(""cpu""))\n\n\n@hydra.main(config_path=""../../conf/config.yaml"")\ndef main(cfg):\n    OmegaConf.set_struct(cfg, False)\n\n    # Get device\n    device = torch.device(""cuda"" if (torch.cuda.is_available() and cfg.training.cuda) else ""cpu"")\n    log.info(""DEVICE : {}"".format(device))\n\n    # Enable CUDNN BACKEND\n    torch.backends.cudnn.enabled = cfg.training.enable_cudnn\n\n    # Checkpoint\n    checkpoint = ModelCheckpoint(cfg.training.checkpoint_dir, cfg.model_name, cfg.training.weight_name, strict=True)\n\n    # Setup the dataset config\n    # Generic config\n\n    dataset = instantiate_dataset(cfg.data)\n    model = checkpoint.create_model(dataset, weight_name=cfg.training.weight_name)\n    log.info(model)\n    log.info(""Model size = %i"", sum(param.numel() for param in model.parameters() if param.requires_grad))\n\n    log.info(dataset)\n\n    model.eval()\n    if cfg.enable_dropout:\n        model.enable_dropout_in_eval()\n    model = model.to(device)\n\n    # Run training / evaluation\n    output_path = os.path.join(cfg.training.checkpoint_dir, cfg.data.name, ""features"")\n    if not os.path.exists(output_path):\n        os.makedirs(output_path, exist_ok=True)\n\n    run(model, dataset, device, output_path, cfg)\n\n\nif __name__ == ""__main__"":\n    main()\n'"
torch_points3d/applications/__init__.py,0,b''
torch_points3d/applications/kpconv.py,1,"b'import os\nfrom omegaconf import DictConfig, OmegaConf\nimport logging\n\nfrom torch_points3d.applications.modelfactory import ModelFactory\nfrom torch_points3d.core.common_modules import FastBatchNorm1d\nfrom torch_points3d.modules.KPConv import *\nfrom torch_points3d.core.base_conv.partial_dense import *\nfrom torch_points3d.models.base_architectures.unet import UnwrappedUnetBasedModel\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.core.common_modules.base_modules import MLP\nfrom .utils import extract_output_nc\n\n\nCUR_FILE = os.path.realpath(__file__)\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nPATH_TO_CONFIG = os.path.join(DIR_PATH, ""conf/kpconv"")\n\nlog = logging.getLogger(__name__)\n\n\ndef KPConv(\n    architecture: str = None, input_nc: int = None, num_layers: int = None, config: DictConfig = None, *args, **kwargs\n):\n    """""" Create a KPConv backbone model based on the architecture proposed in\n    https://arxiv.org/abs/1904.08889\n\n    Parameters\n    ----------\n    architecture : str, optional\n        Architecture of the model, choose from unet, encoder and decoder\n    input_nc : int, optional\n        Number of channels for the input\n    output_nc : int, optional\n        If specified, then we add a fully connected head at the end of the network to provide the requested dimension\n    num_layers : int, optional\n        Depth of the network\n    in_grid_size : float, optional\n        Size of the grid at the entry of the network. It is divided by two at each layer\n    in_feat : int, optional\n        Number of channels after the first convolution. Doubles at each layer\n    config : DictConfig, optional\n        Custom config, overrides the num_layers and architecture parameters\n    """"""\n    factory = KPConvFactory(\n        architecture=architecture, num_layers=num_layers, input_nc=input_nc, config=config, **kwargs\n    )\n    return factory.build()\n\n\nclass KPConvFactory(ModelFactory):\n    def _build_unet(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(PATH_TO_CONFIG, ""unet_{}.yaml"".format(self.num_layers))\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return KPConvUnet(model_config, None, None, modules_lib, **self.kwargs)\n\n    def _build_encoder(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(PATH_TO_CONFIG, ""encoder_{}.yaml"".format(self.num_layers))\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return KPConvEncoder(model_config, None, None, modules_lib, **self.kwargs)\n\n\nclass BaseKPConv(UnwrappedUnetBasedModel):\n    CONV_TYPE = ""partial_dense""\n\n    def __init__(self, model_config, model_type, dataset, modules, *args, **kwargs):\n        super(BaseKPConv, self).__init__(model_config, model_type, dataset, modules)\n        try:\n            default_output_nc = extract_output_nc(model_config)\n        except:\n            default_output_nc = -1\n            log.warning(""Could not resolve number of output channels"")\n\n        self._output_nc = default_output_nc\n        self._has_mlp_head = False\n        if ""output_nc"" in kwargs:\n            self._has_mlp_head = True\n            self._output_nc = kwargs[""output_nc""]\n            self.mlp = MLP([default_output_nc, self.output_nc], activation=torch.nn.LeakyReLU(0.2), bias=False)\n\n    @property\n    def has_mlp_head(self):\n        return self._has_mlp_head\n\n    @property\n    def output_nc(self):\n        return self._output_nc\n\n    def _set_input(self, data):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n\n        Parameters\n        -----------\n        data:\n            a dictionary that contains the data itself and its metadata information.\n        """"""\n        data = data.to(self.device)\n        if isinstance(data, MultiScaleBatch):\n            self.pre_computed = data.multiscale\n            self.upsample = data.upsample\n            del data.upsample\n            del data.multiscale\n        else:\n            self.upsample = None\n            self.pre_computed = None\n\n        self.input = data\n\n\nclass KPConvEncoder(BaseKPConv):\n    def forward(self, data):\n        """"""\n        Parameters\n        -----------\n        data:\n            A dictionary that contains the data itself and its metadata information. Should contain\n            - pos [N, 3]\n            - x [N, C]\n            - multiscale (optional) precomputed data for the down convolutions\n            - upsample (optional) precomputed data for the up convolutions\n\n        Returns\n        --------\n        data:\n            - pos [1, 3] - Dummy pos\n            - x [1, output_nc]\n        """"""\n        self._set_input(data)\n        data = self.input\n        stack_down = [data]\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n        data = self.down_modules[-1](data)\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n\n        if self.has_mlp_head:\n            data.x = self.mlp(data.x)\n        return data\n\n\nclass KPConvUnet(BaseKPConv):\n    def forward(self, data):\n        """"""Run forward pass.\n        Input --- D1 -- D2 -- D3 -- U1 -- U2 -- output\n                   |      |_________|     |\n                   |______________________|\n\n        Parameters\n        -----------\n        data:\n            A dictionary that contains the data itself and its metadata information. Should contain\n            - pos [N, 3]\n            - x [N, C]\n            - multiscale (optional) precomputed data for the down convolutions\n            - upsample (optional) precomputed data for the up convolutions\n\n        Returns\n        --------\n        data:\n            - pos [N, 3]\n            - x [N, output_nc]\n        """"""\n        self._set_input(data)\n        data = super().forward(self.input, precomputed_down=self.pre_computed, precomputed_up=self.upsample)\n        if self.has_mlp_head:\n            data.x = self.mlp(data.x)\n        return data\n'"
torch_points3d/applications/modelfactory.py,0,"b'from enum import Enum\nfrom omegaconf import DictConfig\nimport logging\n\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve\n\nlog = logging.getLogger(__name__)\n\n\nclass ModelArchitectures(Enum):\n    UNET = ""unet""\n    ENCODER = ""encoder""\n    DECODER = ""decoder""\n\n\nclass ModelFactory:\n    MODEL_ARCHITECTURES = [e.value for e in ModelArchitectures]\n\n    @staticmethod\n    def raise_enum_error(arg_name, arg_value, options):\n        raise Exception(""The provided argument {} with value {} isn\'t within {}"".format(arg_name, arg_value, options))\n\n    def __init__(\n        self,\n        architecture: str = None,\n        input_nc: int = None,\n        num_layers: int = None,\n        config: DictConfig = None,\n        **kwargs\n    ):\n        if not architecture:\n            raise ValueError()\n        self._architecture = architecture.lower()\n        assert self._architecture in self.MODEL_ARCHITECTURES, ModelFactory.raise_enum_error(\n            ""model_architecture"", self._architecture, self.MODEL_ARCHITECTURES\n        )\n\n        self._input_nc = input_nc\n        self._num_layers = num_layers\n        self._config = config\n        self._kwargs = kwargs\n\n        if self._config:\n            log.info(""The config will be used to build the model"")\n\n    @property\n    def modules_lib(self):\n        raise NotImplementedError\n\n    @property\n    def kwargs(self):\n        return self._kwargs\n\n    @property\n    def num_layers(self):\n        return self._num_layers\n\n    @property\n    def num_features(self):\n        return self._input_nc\n\n    def _build_unet(self):\n        raise NotImplementedError\n\n    def _build_encoder(self):\n        raise NotImplementedError\n\n    def _build_decoder(self):\n        raise NotImplementedError\n\n    def build(self):\n        if self._architecture == ModelArchitectures.UNET.value:\n            return self._build_unet()\n        elif self._architecture == ModelArchitectures.ENCODER.value:\n            return self._build_encoder()\n        elif self._architecture == ModelArchitectures.DECODER.value:\n            return self._build_decoder()\n        else:\n            raise NotImplementedError\n\n    def resolve_model(self, model_config):\n        """""" Parses the model config and evaluates any expression that may contain constants\n        Overrides any argument in the `define_constants` with keywords wrgument to the constructor\n        """"""\n        # placeholders to subsitute\n        constants = {\n            ""FEAT"": max(self.num_features, 0),\n            ""TASK"": ""segmentation"",\n        }\n\n        # user defined contants to subsitute\n        if ""define_constants"" in model_config.keys():\n            constants.update(dict(model_config.define_constants))\n            define_constants = model_config.define_constants\n            for key in define_constants.keys():\n                value = self.kwargs.get(key)\n                if value:\n                    constants[key] = value\n        resolve(model_config, constants)\n'"
torch_points3d/applications/models.py,0,b'from .kpconv import KPConv\nfrom .pointnet2 import PointNet2\nfrom .rsconv import RSConv\n'
torch_points3d/applications/pointnet2.py,0,"b'import os\nimport sys\nfrom omegaconf import DictConfig, OmegaConf\nimport logging\n\nfrom torch_points3d.applications.modelfactory import ModelFactory\nfrom torch_points3d.modules.pointnet2 import *\nfrom torch_points3d.core.base_conv.dense import DenseFPModule\nfrom torch_points3d.models.base_architectures.unet import UnwrappedUnetBasedModel\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom .utils import extract_output_nc\n\nCUR_FILE = os.path.realpath(__file__)\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nPATH_TO_CONFIG = os.path.join(DIR_PATH, ""conf/pointnet2"")\n\nlog = logging.getLogger(__name__)\n\n\ndef PointNet2(\n    architecture: str = None,\n    input_nc: int = None,\n    num_layers: int = None,\n    config: DictConfig = None,\n    multiscale=True,\n    *args,\n    **kwargs\n):\n    """""" Create a PointNet2 backbone model based on the architecture proposed in\n    https://arxiv.org/abs/1706.02413\n\n    Parameters\n    ----------\n    architecture : str, optional\n        Architecture of the model, choose from unet, encoder and decoder\n    input_nc : int, optional\n        Number of channels for the input\n   output_nc : int, optional\n        If specified, then we add a fully connected head at the end of the network to provide the requested dimension\n    num_layers : int, optional\n        Depth of the network\n    config : DictConfig, optional\n        Custom config, overrides the num_layers and architecture parameters\n    """"""\n    factory = PointNet2Factory(\n        architecture=architecture,\n        num_layers=num_layers,\n        input_nc=input_nc,\n        multiscale=multiscale,\n        config=config,\n        **kwargs\n    )\n    return factory.build()\n\n\nclass PointNet2Factory(ModelFactory):\n    def _build_unet(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(\n                PATH_TO_CONFIG, ""unet_{}_{}.yaml"".format(self.num_layers, ""ms"" if self.kwargs[""multiscale""] else ""ss"")\n            )\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return PointNet2Unet(model_config, None, None, modules_lib, **self.kwargs)\n\n    def _build_encoder(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(\n                PATH_TO_CONFIG,\n                ""encoder_{}_{}.yaml"".format(self.num_layers, ""ms"" if self.kwargs[""multiscale""] else ""ss""),\n            )\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return PointNet2Encoder(model_config, None, None, modules_lib, **self.kwargs)\n\n\nclass BasePointnet2(UnwrappedUnetBasedModel):\n\n    CONV_TYPE = ""dense""\n\n    def __init__(self, model_config, model_type, dataset, modules, *args, **kwargs):\n        super(BasePointnet2, self).__init__(model_config, model_type, dataset, modules)\n\n        try:\n            default_output_nc = extract_output_nc(model_config)\n        except:\n            default_output_nc = -1\n            log.warning(""Could not resolve number of output channels"")\n\n        self._has_mlp_head = False\n        self._output_nc = default_output_nc\n        if ""output_nc"" in kwargs:\n            self._has_mlp_head = True\n            self._output_nc = kwargs[""output_nc""]\n            self.mlp = Seq()\n            self.mlp.append(Conv1D(default_output_nc, self._output_nc, bn=True, bias=False))\n\n    @property\n    def has_mlp_head(self):\n        return self._has_mlp_head\n\n    @property\n    def output_nc(self):\n        return self._output_nc\n\n    def _set_input(self, data):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        """"""\n        assert len(data.pos.shape) == 3\n        data = data.to(self.device)\n        if data.x is not None:\n            data.x = data.x.transpose(1, 2).contiguous()\n        else:\n            data.x = None\n        self.input = data\n\n\nclass PointNet2Encoder(BasePointnet2):\n    def forward(self, data):\n        """"""\n        Parameters:\n        -----------\n        data\n            A dictionary that contains the data itself and its metadata information. Should contain\n                x -- Features [B, N, C]\n                pos -- Points [B, N, 3]\n        """"""\n        self._set_input(data)\n        data = self.input\n        stack_down = [data]\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n        data = self.down_modules[-1](data)\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n\n        if self.has_mlp_head:\n            data.x = self.mlp(data.x)\n        return data\n\n\nclass PointNet2Unet(BasePointnet2):\n    def forward(self, data):\n        """""" This method does a forward on the Unet assuming symmetrical skip connections\n        Input --- D1 -- D2 -- I -- U1 -- U2 -- U3 -- output\n           |       |      |________|      |    |\n           |       |______________________|    |\n           |___________________________________|\n\n        Parameters:\n        -----------\n        data\n            A dictionary that contains the data itself and its metadata information. Should contain\n                x -- Features [B, N, C]\n                pos -- Points [B, N, 3]\n        """"""\n        self._set_input(data)\n        data = self.input\n        stack_down = [data]\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n        data = self.down_modules[-1](data)\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n\n        sampling_ids = self._collect_sampling_ids(stack_down)\n\n        for i in range(len(self.up_modules)):\n            data = self.up_modules[i]((data, stack_down.pop()))\n\n        for key, value in sampling_ids.items():\n            setattr(data, key, value)\n\n        if self.has_mlp_head:\n            data.x = self.mlp(data.x)\n\n        return data\n'"
torch_points3d/applications/rsconv.py,1,"b'import os\nimport sys\nimport queue\nfrom omegaconf import DictConfig, OmegaConf\nimport logging\n\nfrom torch_points3d.applications.modelfactory import ModelFactory\nfrom torch_points3d.modules.RSConv import *\nfrom torch_points3d.core.base_conv.dense import DenseFPModule\nfrom torch_points3d.models.base_architectures.unet import UnwrappedUnetBasedModel\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom .utils import extract_output_nc\n\nCUR_FILE = os.path.realpath(__file__)\nDIR_PATH = os.path.dirname(os.path.realpath(__file__))\nPATH_TO_CONFIG = os.path.join(DIR_PATH, ""conf/rsconv"")\n\nlog = logging.getLogger(__name__)\n\n\ndef RSConv(\n    architecture: str = None, input_nc: int = None, num_layers: int = None, config: DictConfig = None, *args, **kwargs\n):\n    """""" Create a RSConv backbone model based on the architecture proposed in\n    https://arxiv.org/abs/1904.07601\n\n    Parameters\n    ----------\n    architecture : str, optional\n        Architecture of the model, choose from unet, encoder and decoder\n    input_nc : int, optional\n        Number of channels for the input\n    output_nc : int, optional\n        If specified, then we add a fully connected head at the end of the network to provide the requested dimension\n    num_layers : int, optional\n        Depth of the network\n    config : DictConfig, optional\n        Custom config, overrides the num_layers and architecture parameters\n    """"""\n    factory = RSConvFactory(\n        architecture=architecture, num_layers=num_layers, input_nc=input_nc, config=config, **kwargs\n    )\n    return factory.build()\n\n\nclass RSConvFactory(ModelFactory):\n    def _build_unet(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(PATH_TO_CONFIG, ""unet_{}.yaml"".format(self.num_layers))\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return RSConvUnet(model_config, None, None, modules_lib, **self.kwargs)\n\n    def _build_encoder(self):\n        if self._config:\n            model_config = self._config\n        else:\n            path_to_model = os.path.join(PATH_TO_CONFIG, ""encoder_{}.yaml"".format(self.num_layers))\n            model_config = OmegaConf.load(path_to_model)\n        self.resolve_model(model_config)\n        modules_lib = sys.modules[__name__]\n        return RSConvEncoder(model_config, None, None, modules_lib, **self.kwargs)\n\n\nclass RSConvBase(UnwrappedUnetBasedModel):\n    CONV_TYPE = ""dense""\n\n    def __init__(self, model_config, model_type, dataset, modules, *args, **kwargs):\n        super(RSConvBase, self).__init__(model_config, model_type, dataset, modules)\n\n        default_output_nc = kwargs.get(""default_output_nc"", 384)\n        self._has_mlp_head = False\n        self._output_nc = default_output_nc\n        if ""output_nc"" in kwargs:\n            self._has_mlp_head = True\n            self._output_nc = kwargs[""output_nc""]\n            self.mlp = Seq()\n            self.mlp.append(Conv1D(default_output_nc, self._output_nc, bn=True, bias=False))\n\n    @property\n    def has_mlp_head(self):\n        return self._has_mlp_head\n\n    @property\n    def output_nc(self):\n        return self._output_nc\n\n    def _set_input(self, data):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        Sets:\n            self.input:\n                x -- Features [B, C, N]\n                pos -- Points [B, N, 3]\n        """"""\n        assert len(data.pos.shape) == 3\n        data = data.to(self.device)\n        if data.x is not None:\n            data.x = data.x.transpose(1, 2).contiguous()\n        else:\n            data.x = None\n        self.input = data\n\n\nclass RSConvEncoder(RSConvBase):\n    def __init__(self, model_config, model_type, dataset, modules, *args, **kwargs):\n        try:\n            default_output_nc = extract_output_nc(model_config)\n        except:\n            default_output_nc = -1\n            log.warning(""Could not resolve number of output channels"")\n        super().__init__(\n            model_config, model_type, dataset, modules, default_output_nc=default_output_nc, *args, **kwargs\n        )\n\n    def forward(self, data):\n        """""" This method does a forward on the Unet\n\n        Parameters:\n        -----------\n        data\n            A dictionary that contains the data itself and its metadata information. Should contain\n                x -- Features [B, N, C]\n                pos -- Points [B, N, 3]\n        """"""\n        self._set_input(data)\n        data = self.input\n        stack_down = [data]\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n        data = self.down_modules[-1](data)\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n\n        if self.has_mlp_head:\n            data.x = self.mlp(data.x)\n        return data\n\n\nclass RSConvUnet(RSConvBase):\n    def __init__(self, model_config, model_type, dataset, modules, *args, **kwargs):\n        try:\n            default_output_nc = (\n                model_config.innermost[0].nn[-1]\n                + model_config.innermost[1].nn[-1]\n                + model_config.up_conv.up_conv_nn[-1][-1]\n            )\n        except:\n            default_output_nc = -1\n            log.warning(""Could not resolve number of output channels"")\n        super().__init__(\n            model_config, model_type, dataset, modules, default_output_nc=default_output_nc, *args, **kwargs\n        )\n\n    def forward(self, data):\n        """""" This method does a forward on the Unet\n\n        Parameters:\n        -----------\n        data\n            A dictionary that contains the data itself and its metadata information. Should contain\n                x -- Features [B, N, C]\n                pos -- Points [B, N, 3]\n        """"""\n        self._set_input(data)\n        stack_down = []\n        queue_up = queue.Queue()\n\n        data = self.input\n        stack_down.append(data)\n\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n\n        data = self.down_modules[-1](data)\n        queue_up.put(data)\n\n        assert len(self.inner_modules) == 2, ""For this segmentation model, we except 2 distinct inner""\n        data_inner = self.inner_modules[0](data)\n        data_inner_2 = self.inner_modules[1](stack_down[3])\n\n        for i in range(len(self.up_modules)):\n            data = self.up_modules[i]((queue_up.get(), stack_down.pop()))\n            queue_up.put(data)\n\n        last_feature = torch.cat(\n            [data.x, data_inner.x.repeat(1, 1, data.x.shape[-1]), data_inner_2.x.repeat(1, 1, data.x.shape[-1])], dim=1\n        )\n\n        if self.has_mlp_head:\n            data.x = self.mlp(last_feature)\n        return data\n'"
torch_points3d/applications/utils.py,0,"b'def extract_output_nc(model_config):\n    """""" Extracts the number of channels at the output of the network form the model config\n    """"""\n    if model_config.up_conv is not None:\n        output_nc = model_config.up_conv.up_conv_nn[-1][-1]\n    elif model_config.innermost is not None:\n        output_nc = model_config.innermost.nn[-1]\n    else:\n        raise ValueError(""Input model_config does not match expected pattern"")\n    return output_nc\n'"
torch_points3d/core/__init__.py,0,b''
torch_points3d/datasets/__init__.py,0,b''
torch_points3d/datasets/base_dataset.py,2,"b'import os\nfrom abc import ABC, abstractmethod\nimport logging\nfrom functools import partial\nimport numpy as np\nimport torch\nimport torch_geometric\nfrom torch_geometric.transforms import Compose, FixedPoints\nimport copy\n\nfrom torch_points3d.models import model_interface\nfrom torch_points3d.core.data_transform import instantiate_transforms, MultiScaleTransform\nfrom torch_points3d.core.data_transform import instantiate_filters\nfrom torch_points3d.datasets.batch import SimpleBatch\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.utils.config import ConvolutionFormatFactory\nfrom torch_points3d.utils.colors import COLORS, colored_print\n\n# A logger for this file\nlog = logging.getLogger(__name__)\n\n\ndef explode_transform(transforms):\n    """""" Returns a flattened list of transform\n    Arguments:\n        transforms {[list | T.Compose]} -- Contains list of transform to be added\n\n    Returns:\n        [list] -- [List of transforms]\n    """"""\n    out = []\n    if transforms is not None:\n        if isinstance(transforms, Compose):\n            out = copy.deepcopy(transforms.transforms)\n        elif isinstance(transforms, list):\n            out = copy.deepcopy(transforms)\n        else:\n            raise Exception(""transforms should be provided either within a list or a Compose"")\n    return out\n\n\nclass BaseDataset:\n    def __init__(self, dataset_opt):\n        self.dataset_opt = dataset_opt\n\n        # Default dataset path\n        dataset_name = dataset_opt.get(""dataset_name"", None)\n        if dataset_name:\n            self._data_path = os.path.join(dataset_opt.dataroot, dataset_name)\n        else:\n            class_name = self.__class__.__name__.lower().replace(""dataset"", """")\n            self._data_path = os.path.join(dataset_opt.dataroot, class_name)\n        self._batch_size = None\n        self.strategies = {}\n        self._contains_dataset_name = False\n\n        self.train_sampler = None\n        self.test_sampler = None\n        self.val_sampler = None\n\n        self._train_dataset = None\n        self._test_dataset = None\n        self._val_dataset = None\n\n        BaseDataset.set_transform(self, dataset_opt)\n        self.set_filter(dataset_opt)\n\n    @staticmethod\n    def remove_transform(transform_in, list_transform_class):\n\n        """""" Remove a transform if within list_transform_class\n\n        Arguments:\n            transform_in {[type]} -- [Compose | List of transform]\n            list_transform_class {[type]} -- [List of transform class to be removed]\n\n        Returns:\n            [type] -- [description]\n        """"""\n        if isinstance(transform_in, Compose) or isinstance(transform_in, list):\n            if len(list_transform_class) > 0:\n                transform_out = []\n                transforms = transform_in.transforms if isinstance(transform_in, Compose) else transform_in\n                for t in transforms:\n                    if not isinstance(t, tuple(list_transform_class)):\n                        transform_out.append(t)\n                transform_out = Compose(transform_out)\n        else:\n            transform_out = transform_in\n        return transform_out\n\n    @staticmethod\n    def set_transform(obj, dataset_opt):\n        """"""This function create and set the transform to the obj as attributes\n        """"""\n        obj.pre_transform = None\n        obj.test_transform = None\n        obj.train_transform = None\n        obj.val_transform = None\n        obj.inference_transform = None\n\n        for key_name in dataset_opt.keys():\n            if ""transform"" in key_name:\n                new_name = key_name.replace(""transforms"", ""transform"")\n                try:\n                    transform = instantiate_transforms(getattr(dataset_opt, key_name))\n                except Exception:\n                    log.exception(""Error trying to create {}, {}"".format(new_name, getattr(dataset_opt, key_name)))\n                    continue\n                setattr(obj, new_name, transform)\n\n        inference_transform = explode_transform(obj.pre_transform)\n        inference_transform += explode_transform(obj.test_transform)\n        obj.inference_transform = Compose(inference_transform) if len(inference_transform) > 0 else None\n\n    def set_filter(self, dataset_opt):\n        """"""This function create and set the pre_filter to the obj as attributes\n        """"""\n        self.pre_filter = None\n        for key_name in dataset_opt.keys():\n            if ""filter"" in key_name:\n                new_name = key_name.replace(""filters"", ""filter"")\n                try:\n                    filt = instantiate_filters(getattr(dataset_opt, key_name))\n                except Exception:\n                    log.exception(""Error trying to create {}, {}"".format(new_name, getattr(dataset_opt, key_name)))\n                    continue\n                setattr(self, new_name, filt)\n\n    @staticmethod\n    def _get_collate_function(conv_type, is_multiscale):\n        if is_multiscale:\n            if conv_type.lower() == ConvolutionFormat.PARTIAL_DENSE.value.lower():\n                return lambda datalist: MultiScaleBatch.from_data_list(datalist)\n            else:\n                raise NotImplementedError(\n                    ""MultiscaleTransform is activated and supported only for partial_dense format""\n                )\n\n        is_dense = ConvolutionFormatFactory.check_is_dense_format(conv_type)\n        if is_dense:\n            return lambda datalist: SimpleBatch.from_data_list(datalist)\n        else:\n            return lambda datalist: torch_geometric.data.batch.Batch.from_data_list(datalist)\n\n    @staticmethod\n    def get_num_samples(batch, conv_type):\n        is_dense = ConvolutionFormatFactory.check_is_dense_format(conv_type)\n        if is_dense:\n            return batch.pos.shape[0]\n        else:\n            return batch.batch.max() + 1\n\n    @staticmethod\n    def get_sample(batch, key, index, conv_type):\n        assert hasattr(batch, key)\n        is_dense = ConvolutionFormatFactory.check_is_dense_format(conv_type)\n        if is_dense:\n            return batch[key][index]\n        else:\n            return batch[key][batch.batch == index]\n\n    def create_dataloaders(\n        self,\n        model: model_interface.DatasetInterface,\n        batch_size: int,\n        shuffle: bool,\n        num_workers: int,\n        precompute_multi_scale: bool,\n    ):\n        """""" Creates the data loaders. Must be called in order to complete the setup of the Dataset\n        """"""\n        conv_type = model.conv_type\n        self._batch_size = batch_size\n\n        batch_collate_function = self.__class__._get_collate_function(conv_type, precompute_multi_scale)\n        dataloader = partial(\n            torch.utils.data.DataLoader, collate_fn=batch_collate_function, worker_init_fn=lambda _: np.random.seed()\n        )\n\n        if self.train_sampler:\n            log.info(self.train_sampler)\n        if self.train_dataset:\n            self._train_loader = dataloader(\n                self.train_dataset,\n                batch_size=batch_size,\n                shuffle=shuffle and not self.train_sampler,\n                num_workers=num_workers,\n                sampler=self.train_sampler,\n            )\n\n        if self.test_dataset:\n            self._test_loaders = [\n                dataloader(\n                    dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, sampler=self.test_sampler,\n                )\n                for dataset in self.test_dataset\n            ]\n\n        if self.val_dataset:\n            self._val_loader = dataloader(\n                self.val_dataset,\n                batch_size=batch_size,\n                shuffle=False,\n                num_workers=num_workers,\n                sampler=self.val_sampler,\n            )\n\n        if precompute_multi_scale:\n            self.set_strategies(model)\n\n        self._set_has_labels()\n\n    @property\n    def has_train_loader(self):\n        return hasattr(self, ""_train_loader"")\n\n    @property\n    def has_val_loader(self):\n        return hasattr(self, ""_val_loader"")\n\n    def _set_has_labels(self):\n        for loader in self._loaders:\n            sample = loader.dataset[0]\n            has_labels = False\n            if hasattr(sample, ""y""):\n                has_labels = sample.y is not None\n            setattr(loader, ""has_labels"", has_labels)\n            setattr(loader.dataset, ""has_labels"", has_labels)\n\n    @property\n    def has_test_loaders(self):\n        return hasattr(self, ""_test_loaders"")\n\n    @property\n    def train_dataset(self):\n        return self._train_dataset\n\n    @train_dataset.setter\n    def train_dataset(self, value):\n        self._train_dataset = value\n        if not hasattr(self._train_dataset, ""name""):\n            setattr(self._train_dataset, ""name"", ""train"")\n\n    @property\n    def val_dataset(self):\n        return self._val_dataset\n\n    @val_dataset.setter\n    def val_dataset(self, value):\n        self._val_dataset = value\n        if not hasattr(self._val_dataset, ""name""):\n            setattr(self._val_dataset, ""name"", ""val"")\n\n    @property\n    def test_dataset(self):\n        return self._test_dataset\n\n    @test_dataset.setter\n    def test_dataset(self, value):\n        if isinstance(value, list):\n            self._test_dataset = value\n        else:\n            self._test_dataset = [value]\n\n        for i, dataset in enumerate(self._test_dataset):\n            if not hasattr(dataset, ""name""):\n                if self.num_test_datasets > 1:\n                    setattr(dataset, ""name"", ""test_%i"" % i)\n                else:\n                    setattr(dataset, ""name"", ""test"")\n            else:\n                self._contains_dataset_name = True\n\n        # Check for uniqueness\n        all_names = [d.name for d in self.test_dataset]\n        if len(set(all_names)) != len(all_names):\n            raise ValueError(""Datasets need to have unique names. Current names are {}"".format(all_names))\n\n    @property\n    def train_dataloader(self):\n        return self._train_loader\n\n    @property\n    def val_dataloader(self):\n        return self._val_loader\n\n    @property\n    def test_dataloaders(self):\n        return self._test_loaders\n\n    @property\n    def _loaders(self):\n        loaders = []\n        if self.has_train_loader:\n            loaders += [self.train_dataloader]\n        if self.has_val_loader:\n            loaders += [self.val_dataloader]\n        if self.has_test_loaders:\n            loaders += self.test_dataloaders\n        return loaders\n\n    @property\n    def num_test_datasets(self):\n        return len(self._test_dataset) if self._test_dataset else 0\n\n    @property\n    def _test_datatset_names(self):\n        if self.test_dataset:\n            return [d.name for d in self.test_dataset]\n        else:\n            return []\n\n    @property\n    def available_stage_names(self):\n        out = self._test_datatset_names\n        if self.has_val_loader:\n            out += [self._val_dataset.name]\n        return out\n\n    @property\n    def available_dataset_names(self):\n        return [""train""] + self.available_stage_names\n\n    def get_raw_data(self, stage, idx, **kwargs):\n        assert stage in self.available_dataset_names\n        dataset = self.get_dataset(stage)\n        if hasattr(dataset, ""get_raw_data""):\n            return dataset.get_raw_data(idx, **kwargs)\n        else:\n            raise Exception(""Dataset {} doesn t have a get_raw_data function implemented"".format(dataset))\n\n    def has_labels(self, stage: str) -> bool:\n        """""" Tests if a given dataset has labels or not\n\n        Parameters\n        ----------\n        stage : str\n            name of the dataset to test\n        """"""\n        assert stage in self.available_dataset_names\n        dataset = self.get_dataset(stage)\n        return dataset.has_labels\n\n    @property\n    def has_fixed_points_transform(self):\n        """"""\n        This property checks if the dataset contains T.FixedPoints transform, meaning the number of points is fixed\n        """"""\n        transform_train = self.train_transform\n        transform_test = self.test_transform\n\n        if transform_train is None or transform_test is None:\n            return False\n\n        if not isinstance(transform_train, Compose):\n            transform_train = Compose([transform_train])\n\n        if not isinstance(transform_test, Compose):\n            transform_test = Compose([transform_test])\n\n        train_bool = False\n        test_bool = False\n\n        for transform in transform_train.transforms:\n            if isinstance(transform, FixedPoints):\n                train_bool = True\n        for transform in transform_test.transforms:\n            if isinstance(transform, FixedPoints):\n                test_bool = True\n        return train_bool and test_bool\n\n    @property\n    def is_hierarchical(self):\n        """""" Used by the metric trackers to log hierarchical metrics\n        """"""\n        return False\n\n    @property\n    def class_to_segments(self):\n        """""" Use this property to return the hierarchical map between classes and segment ids, example:\n        {\n            \'Airplaine\': [0,1,2],\n            \'Boat\': [3,4,5]\n        }\n        """"""\n        return None\n\n    @property\n    def num_classes(self):\n        return self.train_dataset.num_classes\n\n    @property\n    def weight_classes(self):\n        return getattr(self.train_dataset, ""weight_classes"", None)\n\n    @property\n    def feature_dimension(self):\n        if self.train_dataset:\n            return self.train_dataset.num_features\n        elif self.test_dataset is not None:\n            if isinstance(self.test_dataset, list):\n                return self.test_dataset[0].num_features\n            else:\n                return self.test_dataset.num_features\n        elif self.val_dataset is not None:\n            return self.val_dataset.num_features\n        else:\n            raise NotImplementedError()\n\n    @property\n    def batch_size(self):\n        return self._batch_size\n\n    @property\n    def num_batches(self):\n        out = {\n            self.train_dataset.name: len(self._train_loader),\n            ""val"": len(self._val_loader) if self.has_val_loader else 0,\n        }\n        if self.test_dataset:\n            for loader in self._test_loaders:\n                stage_name = loader.dataset.name\n                out[stage_name] = len(loader)\n        return out\n\n    def get_dataset(self, name):\n        """""" Get a dataset by name. Raises an exception if no dataset was found\n\n        Parameters\n        ----------\n        name : str\n        """"""\n        all_datasets = [self.train_dataset, self.val_dataset] + self.test_dataset\n        for dataset in all_datasets:\n            if dataset is not None and dataset.name == name:\n                return dataset\n        raise ValueError(""No dataset with name %s was found."" % name)\n\n    def _set_composed_multiscale_transform(self, attr, transform):\n        current_transform = getattr(attr.dataset, ""transform"", None)\n        if current_transform is None:\n            setattr(attr.dataset, ""transform"", transform)\n        else:\n            if (\n                isinstance(current_transform, Compose) and transform not in current_transform.transforms\n            ):  # The transform contains several transformations\n                current_transform.transforms += [transform]\n            elif current_transform != transform:\n                setattr(\n                    attr.dataset, ""transform"", Compose([current_transform, transform]),\n                )\n\n    def _set_multiscale_transform(self, transform):\n        for _, attr in self.__dict__.items():\n            if isinstance(attr, torch.utils.data.DataLoader):\n                self._set_composed_multiscale_transform(attr, transform)\n\n        for loader in self._test_loaders:\n            self._set_composed_multiscale_transform(loader, transform)\n\n    def set_strategies(self, model):\n        strategies = model.get_spatial_ops()\n        transform = MultiScaleTransform(strategies)\n        self._set_multiscale_transform(transform)\n\n    @abstractmethod\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        pass\n\n    def resolve_saving_stage(self, selection_stage):\n        """"""This function is responsible to determine if the best model selection\n        is going to be on the validation or test datasets\n        """"""\n        log.info(\n            ""Available stage selection datasets: {} {} {}"".format(\n                COLORS.IPurple, self.available_stage_names, COLORS.END_NO_TOKEN\n            )\n        )\n\n        if self.num_test_datasets > 1 and not self._contains_dataset_name:\n            msg = ""If you want to have better trackable names for your test datasets, add a ""\n            msg += COLORS.IPurple + ""name"" + COLORS.END_NO_TOKEN\n            msg += "" attribute to them""\n            log.info(msg)\n\n        if selection_stage == """":\n            if self.has_val_loader:\n                selection_stage = self.val_dataset.name\n            else:\n                selection_stage = self.test_dataset[0].name\n        log.info(\n            ""The models will be selected using the metrics on following dataset: {} {} {}"".format(\n                COLORS.IPurple, selection_stage, COLORS.END_NO_TOKEN\n            )\n        )\n        return selection_stage\n\n    def __repr__(self):\n        message = ""Dataset: %s \\n"" % self.__class__.__name__\n        for attr in self.__dict__:\n            if ""transform"" in attr:\n                message += ""{}{} {}= {}\\n"".format(COLORS.IPurple, attr, COLORS.END_NO_TOKEN, getattr(self, attr))\n        for attr in self.__dict__:\n            if attr.endswith(""_dataset""):\n                dataset = getattr(self, attr)\n                if isinstance(dataset, list):\n                    if len(dataset) > 1:\n                        size = "", "".join([str(len(d)) for d in dataset])\n                    else:\n                        size = len(dataset[0])\n                elif dataset:\n                    size = len(dataset)\n                else:\n                    size = 0\n                if attr.startswith(""_""):\n                    attr = attr[1:]\n                message += ""Size of {}{} {}= {}\\n"".format(COLORS.IPurple, attr, COLORS.END_NO_TOKEN, size)\n        for key, attr in self.__dict__.items():\n            if key.endswith(""_sampler"") and attr:\n                message += ""{}{} {}= {}\\n"".format(COLORS.IPurple, key, COLORS.END_NO_TOKEN, attr)\n        message += ""{}Batch size ={} {}"".format(COLORS.IPurple, COLORS.END_NO_TOKEN, self.batch_size)\n        return message\n'"
torch_points3d/datasets/batch.py,2,"b'import torch\nfrom torch_geometric.data import Data\n\n\nclass SimpleBatch(Data):\n    r"""""" A classic batch object wrapper with :class:`torch_geometric.data.Data` being the\n    base class, all its methods can also be used here.\n    """"""\n\n    def __init__(self, batch=None, **kwargs):\n        super(SimpleBatch, self).__init__(**kwargs)\n\n        self.batch = batch\n        self.__data_class__ = Data\n\n    @staticmethod\n    def from_data_list(data_list):\n        r""""""Constructs a batch object from a python list holding\n        :class:`torch_geometric.data.Data` objects. \n        """"""\n        keys = [set(data.keys) for data in data_list]\n        keys = list(set.union(*keys))\n\n        # Check if all dimensions matches and we can concatenate data\n        # if len(data_list) > 0:\n        #    for data in data_list[1:]:\n        #        for key in keys:\n        #            assert data_list[0][key].shape == data[key].shape\n\n        batch = SimpleBatch()\n        batch.__data_class__ = data_list[0].__class__\n\n        for key in keys:\n            batch[key] = []\n\n        for _, data in enumerate(data_list):\n            for key in data.keys:\n                item = data[key]\n                batch[key].append(item)\n\n        for key in batch.keys:\n            item = batch[key][0]\n            if (\n                torch.is_tensor(item)\n                or isinstance(item, int)\n                or isinstance(item, float)\n            ):\n                batch[key] = torch.stack(batch[key])\n            else:\n                raise ValueError(""Unsupported attribute type"")\n\n        return batch.contiguous()\n        # return [batch.x.transpose(1, 2).contiguous(), batch.pos, batch.y.view(-1)]\n\n    @property\n    def num_graphs(self):\n        """"""Returns the number of graphs in the batch.""""""\n        return self.batch[-1].item() + 1\n'"
torch_points3d/datasets/dataset_factory.py,0,"b'import importlib\nimport copy\nimport hydra\nimport logging\n\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\nlog = logging.getLogger(__name__)\n\n\ndef get_dataset_class(dataset_config):\n    task = dataset_config.task\n    # Find and create associated dataset\n    try:\n        dataset_config.dataroot = hydra.utils.to_absolute_path(dataset_config.dataroot)\n    except Exception:\n        log.error(""This should happen only during testing"")\n    dataset_class = getattr(dataset_config, ""class"")\n    dataset_paths = dataset_class.split(""."")\n    module = ""."".join(dataset_paths[:-1])\n    class_name = dataset_paths[-1]\n    dataset_module = ""."".join([""torch_points3d.datasets"", task, module])\n    datasetlib = importlib.import_module(dataset_module)\n\n    target_dataset_name = class_name\n    for name, cls in datasetlib.__dict__.items():\n        if name.lower() == target_dataset_name.lower() and issubclass(cls, BaseDataset):\n            dataset_cls = cls\n\n    if dataset_cls is None:\n        raise NotImplementedError(\n            ""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase.""\n            % (module, class_name)\n        )\n    return dataset_cls\n\n\ndef instantiate_dataset(dataset_config) -> BaseDataset:\n    """"""Import the module ""data/[module].py"".\n    In the file, the class called {class_name}() will\n    be instantiated. It has to be a subclass of BaseDataset,\n    and it is case-insensitive.\n    """"""\n    dataset_cls = get_dataset_class(dataset_config)\n    dataset = dataset_cls(dataset_config)\n    return dataset\n'"
torch_points3d/datasets/multiscale_data.py,8,"b'from typing import List, Optional\nimport torch\nimport copy\nimport torch_geometric\nfrom torch_geometric.data import Data\nfrom torch_geometric.data import Batch\n\n\nclass MultiScaleData(Data):\n    def __init__(\n        self,\n        x=None,\n        y=None,\n        pos=None,\n        multiscale: Optional[List[Data]] = None,\n        upsample: Optional[List[Data]] = None,\n        **kwargs,\n    ):\n        super().__init__(x=x, pos=pos, multiscale=multiscale, upsample=upsample, **kwargs)\n\n    def apply(self, func, *keys):\n        r""""""Applies the function :obj:`func` to all tensor and Data attributes\n        :obj:`*keys`. If :obj:`*keys` is not given, :obj:`func` is applied to\n        all present attributes.\n        """"""\n        for key, item in self(*keys):\n            if torch.is_tensor(item):\n                self[key] = func(item)\n        for scale in range(self.num_scales):\n            self.multiscale[scale] = self.multiscale[scale].apply(func)\n\n        for up in range(self.num_upsample):\n            self.upsample[up] = self.upsample[up].apply(func)\n        return self\n\n    @property\n    def num_scales(self):\n        """""" Number of scales in the multiscale array\n        """"""\n        return len(self.multiscale) if self.multiscale else 0\n\n    @property\n    def num_upsample(self):\n        """""" Number of upsample operations\n        """"""\n        return len(self.upsample) if self.upsample else 0\n\n    @classmethod\n    def from_data(cls, data):\n        ms_data = cls()\n        for k, item in data:\n            ms_data[k] = item\n        return ms_data\n\n\nclass MultiScaleBatch(MultiScaleData):\n    @staticmethod\n    def from_data_list(data_list, follow_batch=[]):\n        r""""""Constructs a batch object from a python list holding\n        :class:`torch_geometric.data.Data` objects.\n        The assignment vector :obj:`batch` is created on the fly.\n        Additionally, creates assignment batch vectors for each key in\n        :obj:`follow_batch`.""""""\n        for data in data_list:\n            assert isinstance(data, MultiScaleData)\n        num_scales = data_list[0].num_scales\n        for data_entry in data_list:\n            assert data_entry.num_scales == num_scales, ""All data objects should contain the same number of scales""\n        num_upsample = data_list[0].num_upsample\n        for data_entry in data_list:\n            assert data_entry.num_upsample == num_upsample, ""All data objects should contain the same number of scales""\n\n        # Build multiscale batches\n        multiscale = []\n        for scale in range(num_scales):\n            ms_scale = []\n            for data_entry in data_list:\n                ms_scale.append(data_entry.multiscale[scale])\n            multiscale.append(from_data_list_token(ms_scale))\n\n        # Build upsample batches\n        upsample = []\n        for scale in range(num_upsample):\n            upsample_scale = []\n            for data_entry in data_list:\n                upsample_scale.append(data_entry.upsample[scale])\n            upsample.append(from_data_list_token(upsample_scale))\n\n        # Create batch from non multiscale data\n        for data_entry in data_list:\n            del data_entry.multiscale\n            del data_entry.upsample\n        batch = Batch.from_data_list(data_list)\n        batch = MultiScaleBatch.from_data(batch)\n        batch.multiscale = multiscale\n        batch.upsample = upsample\n\n        if torch_geometric.is_debug_enabled():\n            batch.debug()\n\n        return batch\n\n\ndef from_data_list_token(data_list, follow_batch=[]):\n    """""" This is pretty a copy paste of the from data list of pytorch geometric\n    batch object with the difference that indexes that are negative are not incremented\n    """"""\n\n    keys = [set(data.keys) for data in data_list]\n    keys = list(set.union(*keys))\n    assert ""batch"" not in keys\n\n    batch = Batch()\n    batch.__data_class__ = data_list[0].__class__\n    batch.__slices__ = {key: [0] for key in keys}\n\n    for key in keys:\n        batch[key] = []\n\n    for key in follow_batch:\n        batch[""{}_batch"".format(key)] = []\n\n    cumsum = {key: 0 for key in keys}\n    batch.batch = []\n    for i, data in enumerate(data_list):\n        for key in data.keys:\n            item = data[key]\n            if torch.is_tensor(item) and item.dtype != torch.bool and cumsum[key] > 0:\n                mask = item >= 0\n                item[mask] = item[mask] + cumsum[key]\n            if torch.is_tensor(item):\n                size = item.size(data.__cat_dim__(key, data[key]))\n            else:\n                size = 1\n            batch.__slices__[key].append(size + batch.__slices__[key][-1])\n            cumsum[key] += data.__inc__(key, item)\n            batch[key].append(item)\n\n            if key in follow_batch:\n                item = torch.full((size,), i, dtype=torch.long)\n                batch[""{}_batch"".format(key)].append(item)\n\n        num_nodes = data.num_nodes\n        if num_nodes is not None:\n            item = torch.full((num_nodes,), i, dtype=torch.long)\n            batch.batch.append(item)\n\n    if num_nodes is None:\n        batch.batch = None\n\n    for key in batch.keys:\n        item = batch[key][0]\n        if torch.is_tensor(item):\n            batch[key] = torch.cat(batch[key], dim=data_list[0].__cat_dim__(key, item))\n        elif isinstance(item, int) or isinstance(item, float):\n            batch[key] = torch.tensor(batch[key])\n        else:\n            raise ValueError(""Unsupported attribute type {} : {}"".format(type(item), item))\n\n    if torch_geometric.is_debug_enabled():\n        batch.debug()\n\n    return batch.contiguous()\n'"
torch_points3d/datasets/samplers.py,1,"b'import torch\nimport numpy as np\nfrom torch.utils.data import Sampler\n\nclass BalancedRandomSampler(Sampler):\n    r""""""This sampler is responsible for creating balanced batch based on the class distribution.\n    It is implementing a replacement=True strategy for indices selection\n    """"""\n    def __init__(self, labels, replacement=True):\n\n        self.num_samples = len(labels)\n\n        self.idx_classes, self.counts = np.unique(labels, return_counts=True)\n        self.indices = {\n           idx: np.argwhere(labels == idx).flatten() for idx in self.idx_classes\n        }\n\n    def __iter__(self):\n        indices = []\n        for _ in range(self.num_samples):\n            idx = np.random.choice(self.idx_classes)\n            indice = int(np.random.choice(self.indices[idx]))\n            indices.append(indice)\n        return iter(indices)\n\n    def __len__(self):\n        return self.num_samples\n\n    def __repr__(self):\n        return ""{}(num_samples={})"".format(self.__class__.__name__, self.num_samples)\n\n'"
torch_points3d/metrics/__init__.py,0,b''
torch_points3d/metrics/base_tracker.py,2,"b'import os\nimport torchnet as tnt\nimport torch\nfrom typing import Dict\nimport wandb\nfrom torch.utils.tensorboard import SummaryWriter\nimport logging\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\nfrom torch_points3d.models import model_interface\n\nlog = logging.getLogger(__name__)\n\n\ndef meter_value(meter, dim=0):\n    return float(meter.value()[dim]) if meter.n > 0 else 0.0\n\n\nclass BaseTracker:\n    def __init__(self, stage: str, wandb_log: bool, use_tensorboard: bool):\n        self._wandb = wandb_log\n        self._use_tensorboard = use_tensorboard\n        self._tensorboard_dir = os.path.join(os.getcwd(), ""tensorboard"")\n        self._n_iter = 0\n        self._finalised = False\n        self._conv_type = None\n\n        if self._use_tensorboard:\n            log.info(\n                ""Access tensorboard with the following command <tensorboard --logdir={}>"".format(self._tensorboard_dir)\n            )\n            self._writer = SummaryWriter(log_dir=self._tensorboard_dir)\n\n    def reset(self, stage=""train""):\n        self._stage = stage\n        self._loss_meters = {}\n        self._finalised = False\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        metrics = {}\n        for key, loss_meter in self._loss_meters.items():\n            value = meter_value(loss_meter, dim=0)\n            if value:\n                metrics[key] = meter_value(loss_meter, dim=0)\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {""loss"": min}\n        return self._metric_func\n\n    def track(self, model: model_interface.TrackerInterface, **kwargs):\n        if self._finalised:\n            raise RuntimeError(""Cannot track new values with a finalised tracker, you need to reset it first"")\n        losses = self._convert(model.get_current_losses())\n        self._append_losses(losses)\n\n    def finalise(self, *args, **kwargs):\n        """""" Lifcycle method that is called at the end of an epoch. Use this to compute\n        end of epoch metrics.\n        """"""\n        self._finalised = True\n\n    def _append_losses(self, losses):\n        for key, loss in losses.items():\n            if loss is None:\n                continue\n            loss_key = ""%s_%s"" % (self._stage, key)\n            if loss_key not in self._loss_meters:\n                self._loss_meters[loss_key] = tnt.meter.AverageValueMeter()\n            self._loss_meters[loss_key].add(loss)\n\n    @staticmethod\n    def _convert(x):\n        if torch.is_tensor(x):\n            return x.detach().cpu().numpy()\n        else:\n            return x\n\n    def publish_to_tensorboard(self, metrics, step):\n        for metric_name, metric_value in metrics.items():\n            metric_name = ""{}/{}"".format(metric_name.replace(self._stage + ""_"", """"), self._stage)\n            self._writer.add_scalar(metric_name, metric_value, step)\n\n    @staticmethod\n    def _remove_stage_from_metric_keys(stage, metrics):\n        new_metrics = {}\n        for metric_name, metric_value in metrics.items():\n            new_metrics[metric_name.replace(stage + ""_"", """")] = metric_value\n        return new_metrics\n\n    def publish(self, step):\n        """""" Publishes the current metrics to wandb and tensorboard\n        Arguments:\n            step: current epoch\n        """"""\n        metrics = self.get_metrics()\n\n        if self._wandb:\n            wandb.log(metrics, step=step)\n\n        if self._use_tensorboard:\n            self.publish_to_tensorboard(metrics, step)\n\n        return {\n            ""stage"": self._stage,\n            ""epoch"": step,\n            ""current_metrics"": self._remove_stage_from_metric_keys(self._stage, metrics),\n        }\n\n    def print_summary(self):\n        metrics = self.get_metrics(verbose=True)\n        log.info("""".join([""="" for i in range(50)]))\n        for key, value in metrics.items():\n            log.info(""    {} = {}"".format(key, value))\n        log.info("""".join([""="" for i in range(50)]))\n'"
torch_points3d/metrics/classification_tracker.py,3,"b'from typing import Dict\nimport torch\nimport torchnet as tnt\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\nfrom torch_points3d.metrics.base_tracker import BaseTracker, meter_value\nfrom torch_points3d.models import model_interface\n\n\nclass ClassificationTracker(BaseTracker):\n    def __init__(self, dataset, stage=""train"", wandb_log=False, use_tensorboard: bool = False):\n        """""" This is a generic tracker for segmentation tasks.\n        It uses a confusion matrix in the back-end to track results.\n        Use the tracker to track an epoch.\n        You can use the reset function before you start a new epoch\n        Arguments:\n            dataset  -- dataset to track (used for the number of classes)\n        Keyword Arguments:\n            stage {str} -- current stage. (train, validation, test, etc...) (default: {""train""})\n            wandb_log {str} --  Log using weight and biases\n        """"""\n        super(ClassificationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n        self.reset(stage)\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n        self._acc = tnt.meter.AverageValueMeter()\n\n    @staticmethod\n    def detach_tensor(tensor):\n        if torch.torch.is_tensor(tensor):\n            tensor = tensor.detach()\n        return tensor\n\n    @staticmethod\n    def compute_acc(y_hat, y):\n        labels_hat = torch.argmax(y_hat, dim=1)\n        acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0)\n        return acc\n\n    def track(self, model: model_interface.TrackerInterface, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        super().track(model)\n\n        outputs = model.get_output()\n        targets = model.get_labels().flatten()\n\n        self._acc.add(100 * self.compute_acc(outputs, targets))\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n        metrics[""{}_acc"".format(self._stage)] = meter_value(self._acc)\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {\n            ""acc"": max,\n        }  # Those map subsentences to their optimization functions\n        return self._metric_func\n'"
torch_points3d/metrics/colored_tqdm.py,0,"b'from tqdm.auto import tqdm\nfrom tqdm import std\nimport numpy as np\n\nfrom torch_points3d.utils.colors import COLORS\n\n\nclass Coloredtqdm(tqdm):\n    def set_postfix(self, ordered_dict=None, refresh=True, color=None, round=4, **kwargs):\n        postfix = std._OrderedDict([] if ordered_dict is None else ordered_dict)\n\n        for key in sorted(kwargs.keys()):\n            postfix[key] = kwargs[key]\n\n        for key in postfix.keys():\n            if isinstance(postfix[key], std.Number):\n                postfix[key] = self.format_num_to_k(np.round(postfix[key], round), k=round + 1)\n            if isinstance(postfix[key], std._basestring):\n                postfix[key] = str(postfix[key])\n            if len(postfix[key]) != round:\n                postfix[key] += (round - len(postfix[key])) * "" ""\n\n        if color is not None:\n            self.postfix = color\n        else:\n            self.postfix = """"\n\n        self.postfix += "", "".join(key + ""="" + postfix[key] for key in postfix.keys())\n        if color is not None:\n            self.postfix += COLORS.END_TOKEN\n\n        if refresh:\n            self.refresh()\n\n    def format_num_to_k(self, seq, k=4):\n        seq = str(seq)\n        length = len(seq)\n        out = seq + "" "" * (k - length) if length < k else seq\n        return out if length < k else seq[:k]\n'"
torch_points3d/metrics/confusion_matrix.py,0,"b'import numpy as np\nimport sklearn.metrics as sk\nimport os\n\n\nclass ConfusionMatrix:\n    """"""Streaming interface to allow for any source of predictions. \n    Initialize it, count predictions one by one, then print confusion matrix and intersection-union score""""""\n\n    def __init__(self, number_of_labels=2):\n        self.number_of_labels = number_of_labels\n        self.confusion_matrix = None\n\n    @staticmethod\n    def create_from_matrix(confusion_matrix):\n        assert confusion_matrix.shape[0] == confusion_matrix.shape[1]\n        matrix = ConfusionMatrix(confusion_matrix.shape[0])\n        matrix.confusion_matrix = confusion_matrix\n        return matrix\n\n    def count_predicted_batch(self, ground_truth_vec, predicted):\n        assert np.max(predicted) < self.number_of_labels\n        batch_confusion = sk.confusion_matrix(ground_truth_vec, predicted, labels=range(self.number_of_labels))\n        if self.confusion_matrix is None:\n            self.confusion_matrix = batch_confusion\n        else:\n            self.confusion_matrix += batch_confusion\n\n    def get_count(self, ground_truth, predicted):\n        """"""labels are integers from 0 to number_of_labels-1""""""\n        return self.confusion_matrix[ground_truth][predicted]\n\n    def get_confusion_matrix(self):\n        """"""returns list of lists of integers; use it as result[ground_truth][predicted]\n            to know how many samples of class ground_truth were reported as class predicted""""""\n        return self.confusion_matrix\n\n    def get_intersection_union_per_class(self):\n        """""" Computes the intersection over union of each class in the \n        confusion matrix\n        Return:\n            (iou, missing_class_mask) - iou for class as well as a mask highlighting existing classes\n        """"""\n        TP_plus_FN = np.sum(self.confusion_matrix, axis=0)\n        TP_plus_FP = np.sum(self.confusion_matrix, axis=1)\n        TP = np.diagonal(self.confusion_matrix)\n        union = TP_plus_FN + TP_plus_FP - TP\n        iou = 1e-8 + TP / (union + 1e-8)\n        existing_class_mask = union > 1e-3\n        return iou, existing_class_mask\n\n    def get_overall_accuracy(self):\n        """"""returns 64-bit float""""""\n        confusion_matrix = self.confusion_matrix\n        matrix_diagonal = 0\n        all_values = 0\n        for row in range(self.number_of_labels):\n            for column in range(self.number_of_labels):\n                all_values += confusion_matrix[row][column]\n                if row == column:\n                    matrix_diagonal += confusion_matrix[row][column]\n        if all_values == 0:\n            all_values = 1\n        return float(matrix_diagonal) / all_values\n\n    def get_average_intersection_union(self, missing_as_one=False):\n        """""" Get the mIoU metric by ignoring missing labels. \n        If missing_as_one is True then treats missing classes in the IoU as 1\n        """"""\n        values, existing_classes_mask = self.get_intersection_union_per_class()\n        if np.sum(existing_classes_mask) == 0:\n            return 0\n        if missing_as_one:\n            values[~existing_classes_mask] = 1\n            existing_classes_mask[:] = True\n        return np.sum(values[existing_classes_mask]) / np.sum(existing_classes_mask)\n\n    def get_mean_class_accuracy(self):  # added\n        re = 0\n        label_presents = 0\n        for i in range(self.number_of_labels):\n            total_gt = np.sum(self.confusion_matrix[i, :])\n            if total_gt:\n                label_presents += 1\n                re = re + self.confusion_matrix[i][i] / max(1, total_gt)\n        if label_presents == 0:\n            return 0\n        return re / label_presents\n\n    def count_gt(self, ground_truth):\n        return self.confusion_matrix[ground_truth, :].sum()\n\n\ndef save_confusion_matrix(cm, path2save, ordered_names):\n    import seaborn as sns\n    import matplotlib.pyplot as plt\n\n    sns.set(font_scale=5)\n\n    template_path = os.path.join(path2save, ""{}.svg"")\n    # PRECISION\n    cmn = cm.astype(""float"") / cm.sum(axis=-1)[:, np.newaxis]\n    cmn[np.isnan(cmn) | np.isinf(cmn)] = 0\n    fig, ax = plt.subplots(figsize=(31, 31))\n    sns.heatmap(\n        cmn, annot=True, fmt="".2f"", xticklabels=ordered_names, yticklabels=ordered_names, annot_kws={""size"": 20}\n    )\n    # g.set_xticklabels(g.get_xticklabels(), rotation = 35, fontsize = 20)\n    plt.ylabel(""Actual"")\n    plt.xlabel(""Predicted"")\n    path_precision = template_path.format(""precision"")\n    plt.savefig(path_precision, format=""svg"")\n\n    # RECALL\n    cmn = cm.astype(""float"") / cm.sum(axis=0)[np.newaxis, :]\n    cmn[np.isnan(cmn) | np.isinf(cmn)] = 0\n    fig, ax = plt.subplots(figsize=(31, 31))\n    sns.heatmap(\n        cmn, annot=True, fmt="".2f"", xticklabels=ordered_names, yticklabels=ordered_names, annot_kws={""size"": 20}\n    )\n    # g.set_xticklabels(g.get_xticklabels(), rotation = 35, fontsize = 20)\n    plt.ylabel(""Actual"")\n    plt.xlabel(""Predicted"")\n    path_recall = template_path.format(""recall"")\n    plt.savefig(path_recall, format=""svg"")\n'"
torch_points3d/metrics/meters.py,14,"b'import math\nimport torch\n\n\nclass Meter(object):\n    """"""Meters provide a way to keep track of important statistics in an online manner.\n    This class is abstract, but provides a standard interface for all meters to follow.\n    """"""\n\n    def reset(self):\n        """"""Resets the meter to default settings.""""""\n\n    def add(self, value):\n        """"""Log a new value to the meter\n        Args:\n            value: Next restult to include.\n        """"""\n\n    def value(self):\n        """"""Get the value of the meter in the current state.""""""\n\n\n# This code has been taken from Pytorch Torchnet has it contains a bug with an assert\n# https://github.com/pytorch/tnt/issues/131\nclass APMeter(Meter):\n    """"""\n    The APMeter measures the average precision per class.\n    The APMeter is designed to operate on `NxK` Tensors `output` and\n    `target`, and optionally a `Nx1` Tensor weight where (1) the `output`\n    contains model output scores for `N` examples and `K` classes that ought to\n    be higher when the model is more convinced that the example should be\n    positively labeled, and smaller when the model believes the example should\n    be negatively labeled (for instance, the output of a sigmoid function); (2)\n    the `target` contains only values 0 (for negative examples) and 1\n    (for positive examples); and (3) the `weight` ( > 0) represents weight for\n    each sample.\n    """"""\n\n    def __init__(self):\n        super(APMeter, self).__init__()\n        self.reset()\n\n    def reset(self):\n        """"""Resets the meter with empty member variables""""""\n        self.scores = torch.FloatTensor(torch.FloatStorage())\n        self.targets = torch.LongTensor(torch.LongStorage())\n        self.weights = torch.FloatTensor(torch.FloatStorage())\n\n    def add(self, output, target, weight=None):\n        """"""Add a new observation\n        Args:\n            output (Tensor): NxK tensor that for each of the N examples\n                indicates the probability of the example belonging to each of\n                the K classes, according to the model. The probabilities should\n                sum to one over all classes\n            target (Tensor): binary NxK tensort that encodes which of the K\n                classes are associated with the N-th input\n                (eg: a row [0, 1, 0, 1] indicates that the example is\n                associated with classes 2 and 4)\n            weight (optional, Tensor): Nx1 tensor representing the weight for\n                each example (each weight > 0)\n        """"""\n        if not torch.is_tensor(output):\n            output = torch.from_numpy(output)\n        if not torch.is_tensor(target):\n            target = torch.from_numpy(target)\n\n        if weight is not None:\n            if not torch.is_tensor(weight):\n                weight = torch.from_numpy(weight)\n            weight = weight.squeeze()\n        if output.dim() == 1:\n            output = output.view(-1, 1)\n        else:\n            assert (\n                output.dim() == 2\n            ), ""wrong output size (should be 1D or 2D with one column \\\n                per class)""\n        if target.dim() == 1:\n            target = target.view(-1, 1)\n        else:\n            assert (\n                target.dim() == 2\n            ), ""wrong target size (should be 1D or 2D with one column \\\n                per class)""\n        if weight is not None:\n            assert weight.dim() == 1, ""Weight dimension should be 1""\n            assert weight.numel() == target.size(0), ""Weight dimension 1 should be the same as that of target""\n            assert torch.min(weight) >= 0, ""Weight should be non-negative only""\n        if self.scores.numel() > 0:\n            assert target.size(1) == self.targets.size(\n                1\n            ), ""dimensions for output should match previously added examples.""\n\n        # make sure storage is of sufficient size\n        if self.scores.storage().size() < self.scores.numel() + output.numel():\n            new_size = math.ceil(self.scores.storage().size() * 1.5)\n            new_weight_size = math.ceil(self.weights.storage().size() * 1.5)\n            self.scores.storage().resize_(int(new_size + output.numel()))\n            self.targets.storage().resize_(int(new_size + output.numel()))\n            if weight is not None:\n                self.weights.storage().resize_(int(new_weight_size + output.size(0)))\n\n        # store scores and targets\n        offset = self.scores.size(0) if self.scores.dim() > 0 else 0\n        self.scores.resize_(offset + output.size(0), output.size(1))\n        self.targets.resize_(offset + target.size(0), target.size(1))\n        self.scores.narrow(0, offset, output.size(0)).copy_(output)\n        self.targets.narrow(0, offset, target.size(0)).copy_(target)\n\n        if weight is not None:\n            self.weights.resize_(offset + weight.size(0))\n            self.weights.narrow(0, offset, weight.size(0)).copy_(weight)\n\n    def value(self):\n        """"""Returns the model\'s average precision for each class\n        Return:\n            ap (FloatTensor): 1xK tensor, with avg precision for each class k\n        """"""\n\n        if self.scores.numel() == 0:\n            return 0\n        ap = torch.zeros(self.scores.size(1))\n        if hasattr(torch, ""arange""):\n            rg = torch.arange(1, self.scores.size(0) + 1).float()\n        else:\n            rg = torch.range(1, self.scores.size(0)).float()\n        if self.weights.numel() > 0:\n            weight = self.weights.new(self.weights.size())\n            weighted_truth = self.weights.new(self.weights.size())\n\n        # compute average precision for each class\n        for k in range(self.scores.size(1)):\n            # sort scores\n            scores = self.scores[:, k]\n            targets = self.targets[:, k]\n            _, sortind = torch.sort(scores, 0, True)\n            truth = targets[sortind]\n            if self.weights.numel() > 0:\n                weight = self.weights[sortind]\n                weighted_truth = truth.float() * weight\n                rg = weight.cumsum(0)\n\n            # compute true positive sums\n            if self.weights.numel() > 0:\n                tp = weighted_truth.cumsum(0)\n            else:\n                tp = truth.float().cumsum(0)\n\n            # compute precision curve\n            precision = tp.div(rg)\n\n            # compute average precision\n            ap[k] = precision[truth.bool()].sum() / max(float(truth.sum()), 1)\n        return ap\n'"
torch_points3d/metrics/model_checkpoint.py,3,"b'from typing import Dict, Any, List, Optional, Tuple\nimport os\nimport torch\nimport logging\nimport copy\nimport glob\nimport shutil\nfrom omegaconf import OmegaConf, DictConfig\n\nfrom torch_points3d.models import model_interface\nfrom torch_points3d.utils.colors import COLORS, colored_print\nfrom torch_points3d.core.schedulers.lr_schedulers import instantiate_scheduler\nfrom torch_points3d.core.schedulers.bn_schedulers import instantiate_bn_scheduler\nfrom torch_points3d.models.model_factory import instantiate_model\n\nlog = logging.getLogger(__name__)\n\n\nclass Checkpoint:\n    _LATEST = ""latest""\n\n    def __init__(self, checkpoint_file: str, save_every_iter: bool = True):\n        """""" Checkpoint manager. Saves to working directory with check_name\n        Arguments\n            checkpoint_file {str} -- Path to the checkpoint\n            save_every_iter {bool} -- [description] (default: {True})\n        """"""\n        self._check_path = checkpoint_file\n        self._filled = False\n        self.run_config = None\n        self.models: Dict[str, Any] = {}\n        self.stats: Dict[str, List[Any]] = {""train"": [], ""test"": [], ""val"": []}\n        self.optimizer: Optional[Tuple[str, Any]] = None\n        self.schedulers: Dict[str, Any] = {}\n\n    def save_objects(self, models_to_save: Dict[str, Any], stage, current_stat, optimizer, schedulers, **kwargs):\n        """""" Saves checkpoint with updated mdoels for the given stage\n        """"""\n        self.models = models_to_save\n        self.optimizer = (optimizer.__class__.__name__, optimizer.state_dict())\n        self.schedulers = {\n            scheduler_name: [scheduler.scheduler_opt, scheduler.state_dict()]\n            for scheduler_name, scheduler in schedulers.items()\n        }\n        to_save = kwargs\n        for key, value in self.__dict__.items():\n            if not key.startswith(""_""):\n                to_save[key] = value\n        torch.save(to_save, self._check_path)\n\n    @staticmethod\n    def load(checkpoint_dir: str, checkpoint_name: str, run_config: DictConfig, strict=False, resume=True):\n        """""" Creates a new checkpoint object in the current working directory by loading the\n        checkpoint located at [checkpointdir]/[checkpoint_name].pt\n        """"""\n        checkpoint_file = os.path.join(checkpoint_dir, checkpoint_name) + "".pt""\n        if not os.path.exists(checkpoint_file):\n            ckp = Checkpoint(checkpoint_file)\n            if strict or resume:\n                available_checkpoints = glob.glob(os.path.join(checkpoint_dir, ""*.pt""))\n                message = ""The provided path {} didn\'t contain the checkpoint_file {}"".format(\n                    checkpoint_dir, checkpoint_name + "".pt""\n                )\n                if available_checkpoints:\n                    message += ""\\nDid you mean {}?"".format(os.path.basename(available_checkpoints[0]))\n                raise ValueError(message)\n            ckp.run_config = run_config\n            return ckp\n        else:\n            chkp_name = os.path.basename(checkpoint_file)\n            if resume:\n                shutil.copyfile(\n                    checkpoint_file, chkp_name\n                )  # Copy checkpoint to new run directory to make sure we don\'t override\n            ckp = Checkpoint(chkp_name)\n            log.info(""Loading checkpoint from {}"".format(checkpoint_file))\n            objects = torch.load(checkpoint_file, map_location=""cpu"")\n            for key, value in objects.items():\n                setattr(ckp, key, value)\n            ckp._filled = True\n        return ckp\n\n    @property\n    def is_empty(self):\n        return not self._filled\n\n    def get_optimizer(self, model):\n        if not self.is_empty:\n            try:\n                optimizer_config = self.optimizer\n                optimizer_cls = getattr(torch.optim, optimizer_config[0])\n                optimizer_params = {}\n                try:\n                    optimizer_params = self.run_config.training.optim.optimizer.params\n                except:\n                    pass\n                optimizer = optimizer_cls(model.parameters(), **optimizer_params)\n                optimizer.load_state_dict(optimizer_config[1])\n                return optimizer\n            except:\n                raise KeyError(""The checkpoint doesn t contain an optimizer"")\n\n    def get_schedulers(self, model):\n        if not self.is_empty:\n            try:\n                schedulers_out = {}\n                schedulers_config = self.schedulers\n                for scheduler_type, (scheduler_opt, scheduler_state) in schedulers_config.items():\n                    if scheduler_type == ""lr_scheduler"":\n                        optimizer = model.optimizer\n                        scheduler = instantiate_scheduler(optimizer, scheduler_opt)\n                        scheduler.load_state_dict(scheduler_state)\n                        schedulers_out[""lr_scheduler""] = scheduler\n                    elif scheduler_type == ""bn_scheduler"":\n                        scheduler = instantiate_bn_scheduler(model, scheduler_opt)\n                        scheduler.load_state_dict(scheduler_state)\n                        schedulers_out[""bn_scheduler""] = scheduler\n                    else:\n                        raise NotImplementedError\n                return schedulers_out\n            except:\n                log.warn(""The checkpoint doesn t contain schedulers"")\n                return None\n\n    def get_state_dict(self, weight_name):\n        if not self.is_empty:\n            try:\n                models = self.models\n                keys = [key.replace(""best_"", """") for key in models.keys()]\n                log.info(""Available weights : {}"".format(keys))\n                try:\n                    key_name = ""best_{}"".format(weight_name)\n                    model = models[key_name]\n                    log.info(""Model loaded from {}:{}."".format(self._check_path, key_name))\n                    return model\n                except:\n                    key_name = Checkpoint._LATEST\n                    model = models[Checkpoint._LATEST]\n                    log.info(""Model loaded from {}:{}"".format(self._check_path, key_name))\n                    return model\n            except:\n                raise Exception(""This weight name isn\'t within the checkpoint "")\n\n\nclass ModelCheckpoint(object):\n    """""" Create a checkpoint for a given model\n\n    Argumemnts:\n        - load_dir: directory where to load the checkpoint from (if exists)\n        - check_name: Name of the checkpoint (without the .pt extension)\n        - selection_stage: Stage that is used for selecting the best model\n        - run_config: Config of the run. In resume mode, this gets discarded\n        - resume: Resume a previous training - this creates optimizers\n        - strict: If strict and checkpoint is empty then it raises a ValueError. Being in resume mode forces strict\n    """"""\n\n    def __init__(\n        self,\n        load_dir: str,\n        check_name: str,\n        selection_stage: str,\n        run_config: DictConfig = DictConfig({}),\n        resume=False,\n        strict=False,\n    ):\n        self._checkpoint = Checkpoint.load(\n            load_dir, check_name, copy.deepcopy(run_config), strict=strict, resume=resume\n        )\n        self._resume = resume\n        self._selection_stage = selection_stage\n\n    def create_model(self, dataset, weight_name=Checkpoint._LATEST):\n        if not self.is_empty:\n            run_config = copy.deepcopy(self._checkpoint.run_config)\n            model = instantiate_model(run_config, dataset)\n            self._initialize_model(model, weight_name)\n            return model\n        else:\n            raise ValueError(""Checkpoint is empty"")\n\n    @property\n    def start_epoch(self):\n        if self._resume:\n            return self.get_starting_epoch()\n        else:\n            return 1\n\n    @property\n    def data_config(self):\n        return self._checkpoint.run_config.data\n\n    @property\n    def selection_stage(self):\n        return self._selection_stage\n\n    @selection_stage.setter\n    def selection_stage(self, value):\n        self._selection_stage = value\n\n    @property\n    def is_empty(self):\n        return self._checkpoint.is_empty\n\n    def get_starting_epoch(self):\n        return len(self._checkpoint.stats[""train""]) + 1\n\n    def _initialize_model(self, model: model_interface.CheckpointInterface, weight_name):\n        if not self._checkpoint.is_empty:\n            state_dict = self._checkpoint.get_state_dict(weight_name)\n            model.load_state_dict(state_dict)\n            if self._resume:\n                model.optimizer = self._checkpoint.get_optimizer(model)\n                model.schedulers = self._checkpoint.get_schedulers(model)\n\n    def find_func_from_metric_name(self, metric_name, default_metrics_func):\n        for token_name, func in default_metrics_func.items():\n            if token_name in metric_name:\n                return func\n        raise Exception(\n            \'The metric name {} doesn t have a func to measure which one is best in {}. Example: For best_train_iou, {{""iou"":max}}\'.format(\n                metric_name, default_metrics_func\n            )\n        )\n\n    def save_best_models_under_current_metrics(\n        self, model: model_interface.CheckpointInterface, metrics_holder: dict, metric_func_dict: dict, **kwargs\n    ):\n        """"""[This function is responsible to save checkpoint under the current metrics and their associated DEFAULT_METRICS_FUNC]\n        Arguments:\n            model {[CheckpointInterface]} -- [Model]\n            metrics_holder {[Dict]} -- [Need to contain stage, epoch, current_metrics]\n        """"""\n        metrics = metrics_holder[""current_metrics""]\n        stage = metrics_holder[""stage""]\n        epoch = metrics_holder[""epoch""]\n\n        stats = self._checkpoint.stats\n        state_dict = copy.deepcopy(model.state_dict())\n\n        current_stat = {}\n        current_stat[""epoch""] = epoch\n\n        models_to_save = self._checkpoint.models\n        if stage not in stats:\n            stats[stage] = []\n\n        if stage == ""train"":\n            models_to_save[Checkpoint._LATEST] = state_dict\n        else:\n            if len(stats[stage]) > 0:\n                latest_stats = stats[stage][-1]\n\n                msg = """"\n                improved_metric = 0\n\n                for metric_name, current_metric_value in metrics.items():\n                    current_stat[metric_name] = current_metric_value\n\n                    metric_func = self.find_func_from_metric_name(metric_name, metric_func_dict)\n                    best_metric_from_stats = latest_stats.get(""best_{}"".format(metric_name), current_metric_value)\n                    best_value = metric_func(best_metric_from_stats, current_metric_value)\n                    current_stat[""best_{}"".format(metric_name)] = best_value\n\n                    # This new value seems to be better under metric_func\n                    if (self._selection_stage == stage) and (\n                        current_metric_value == best_value\n                    ):  # Update the model weights\n                        models_to_save[""best_{}"".format(metric_name)] = state_dict\n\n                        msg += ""{}: {} -> {}, "".format(metric_name, best_metric_from_stats, best_value)\n                        improved_metric += 1\n\n                if improved_metric > 0:\n                    colored_print(COLORS.VAL_COLOR, msg[:-2])\n            else:\n                # stats[stage] is empty.\n                for metric_name, metric_value in metrics.items():\n                    current_stat[metric_name] = metric_value\n                    current_stat[""best_{}"".format(metric_name)] = metric_value\n                    models_to_save[""best_{}"".format(metric_name)] = state_dict\n\n        self._checkpoint.stats[stage].append(current_stat)\n        self._checkpoint.save_objects(models_to_save, stage, current_stat, model.optimizer, model.schedulers, **kwargs)\n'"
torch_points3d/metrics/object_detection_tracker.py,4,"b'from typing import Dict\nimport torchnet as tnt\nimport torch\n\nfrom torch_points3d.models.model_interface import TrackerInterface\nfrom torch_points3d.metrics.base_tracker import BaseTracker, meter_value\nfrom torch_points3d.metrics.meters import APMeter\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\n\nclass ObjectDetectionTracker(BaseTracker):\n    def __init__(self, dataset, stage=""train"", wandb_log=False, use_tensorboard: bool = False):\n        super(ObjectDetectionTracker, self).__init__(stage, wandb_log, use_tensorboard)\n        self._num_classes = dataset.num_classes\n        self._dataset = dataset\n        self.reset(stage)\n        self._metric_func = {""loss"": min, ""acc"": max}\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n\n    @staticmethod\n    def detach_tensor(tensor):\n        if torch.torch.is_tensor(tensor):\n            tensor = tensor.detach()\n        return tensor\n\n    @property\n    def confusion_matrix(self):\n        return self._confusion_matrix.confusion_matrix\n\n    def track(self, model: TrackerInterface, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        super().track(model)\n\n        outputs = model.get_output()\n        targets = model.get_labels()\n\n        obj_pred_val = torch.argmax(outputs[""objectness_scores""], 2)  # B,K\n        self._obj_acc = torch.sum(\n            (obj_pred_val == targets[""objectness_label""].long()).float() * targets[""objectness_mask""]\n        ) / (torch.sum(targets[""objectness_mask""]) + 1e-6)\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n\n        metrics[""{}_acc"".format(self._stage)] = self._obj_acc.item()\n\n        return metrics\n\n    @property\n    def metric_func(self):\n        return self._metric_func\n'"
torch_points3d/metrics/registration_metrics.py,24,"b'import torch\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors\nfrom torch_geometric.nn import knn\n\n\ndef compute_accuracy(embedded_ref_features, embedded_val_features):\n\n    """"""\n    accuracy for metric learning tasks in case descriptor learning\n    Args:\n        embedded_ref_feature(numpy array): size N x D the features computed by one head of the siamese\n        embedded_val_features(array): N x D the features computed by the other head\n    return:\n        each line of the matrix are supposed to be equal, this function counts when it\'s the case\n    """"""\n    number_of_test_points = embedded_ref_features.shape[0]\n    neigh = NearestNeighbors(n_neighbors=1, algorithm=""kd_tree"", metric=""euclidean"")\n    neigh.fit(embedded_ref_features)\n    dist_neigh_normal, ind_neigh_normal = neigh.kneighbors(embedded_val_features)\n    reference_neighbors = np.reshape(np.arange(number_of_test_points), newshape=(-1, 1))\n\n    wrong_matches = np.count_nonzero(ind_neigh_normal - reference_neighbors)\n    accuracy = (1 - wrong_matches / number_of_test_points) * 100\n    return accuracy\n\n\ndef estimate_transfo(xyz, xyz_target):\n    """"""\n    estimate the rotation and translation using Kabsch algorithm\n    Parameters:\n    xyz :\n    xyz_target:\n    """"""\n    assert xyz.shape == xyz.shape\n    xyz_c = xyz - xyz.mean(0)\n    xyz_target_c = xyz_target - xyz_target.mean(0)\n    Q = xyz_c.T.mm(xyz_target_c) / len(xyz)\n    U, S, V = torch.svd(Q)\n    d = torch.det(V.mm(U.T))\n    diag = torch.diag(torch.tensor([1, 1, d], device=xyz.device))\n    R = V.mm(diag).mm(U.T)\n    t = xyz_target.mean(0) - R @ xyz.mean(0)\n    T = torch.eye(4, device=xyz.device)\n    T[:3, :3] = R\n    T[:3, 3] = t\n    return T\n\n\ndef get_geman_mclure_weight(xyz, xyz_target, mu):\n    """"""\n    compute the weights defined here for the iterative reweighted least square.\n    http://vladlen.info/papers/fast-global-registration.pdf\n    """"""\n    norm2 = torch.norm(xyz_target - xyz, dim=1) ** 2\n    return (mu / (mu + norm2)).view(-1, 1)\n\n\ndef get_cross_product_matrix(k):\n    return torch.tensor([[0, -k[2], k[1]], [k[2], 0, -k[0]], [-k[1], k[0], 0]], device=k.device)\n\n\ndef rodrigues(axis, theta):\n    """"""\n    given an axis of norm one and an angle, compute the rotation matrix using rodrigues formula\n    source : https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula\n    """"""\n    K = get_cross_product_matrix(axis)\n    t = torch.tensor([theta], device=axis.device)\n    R = torch.eye(3, device=axis.device) + torch.sin(t) * K + (1 - torch.cos(t)) * K.mm(K)\n    return R\n\n\ndef get_matrix_system(xyz, xyz_target, weight):\n    """"""\n    Build matrix of size 3N x 6 and b of size 3N\n\n    xyz size N x 3\n    xyz_target size N x 3\n    weight size N\n    the matrix is minus cross product matrix concatenate with the identity (rearanged).\n    """"""\n    assert xyz.shape == xyz_target.shape\n    A_x = torch.zeros(xyz.shape[0], 6, device=xyz.device)\n    A_y = torch.zeros(xyz.shape[0], 6, device=xyz.device)\n    A_z = torch.zeros(xyz.shape[0], 6, device=xyz.device)\n    b_x = weight.view(-1) * (xyz_target[:, 0] - xyz[:, 0])\n    b_y = weight.view(-1) * (xyz_target[:, 1] - xyz[:, 1])\n    b_z = weight.view(-1) * (xyz_target[:, 2] - xyz[:, 2])\n    A_x[:, 1] = weight.view(-1) * xyz[:, 2]\n    A_x[:, 2] = -weight.view(-1) * xyz[:, 1]\n    A_x[:, 3] = weight.view(-1) * 1\n    A_y[:, 0] = -weight.view(-1) * xyz[:, 2]\n    A_y[:, 2] = weight.view(-1) * xyz[:, 0]\n    A_y[:, 4] = weight.view(-1) * 1\n    A_z[:, 0] = weight.view(-1) * xyz[:, 1]\n    A_z[:, 1] = -weight.view(-1) * xyz[:, 0]\n    A_z[:, 5] = weight.view(-1) * 1\n    return torch.cat([A_x, A_y, A_z], 0), torch.cat([b_x, b_y, b_z], 0).view(-1, 1)\n\n\ndef get_trans(x):\n    """"""\n    get the matrix\n    """"""\n    T = torch.eye(4, device=x.device)\n    T[:3, 3] = x[3:]\n    axis = x[:3]\n    theta = torch.norm(axis)\n    if theta > 0:\n        axis = axis / theta\n    T[:3, :3] = rodrigues(axis, theta)\n    return T\n\n\ndef fast_global_registration(xyz, xyz_target, mu_init=1, num_iter=20):\n    """"""\n    estimate the rotation and translation using Fast Global Registration algorithm (M estimator for robust estimation)\n    http://vladlen.info/papers/fast-global-registration.pdf\n    """"""\n    assert xyz.shape == xyz_target.shape\n\n    T_res = torch.eye(4, device=xyz.device)\n    mu = mu_init\n    source = xyz.clone()\n    weight = torch.ones(len(source), 1, device=xyz.device)\n    for i in range(num_iter):\n        if i > 0 and i % 5 == 0:\n            mu /= 2.0\n        A, b = get_matrix_system(source, xyz_target, weight)\n        x, _ = torch.solve(A.T @ b, A.T.mm(A))\n        T = get_trans(x.view(-1))\n        source = source.mm(T[:3, :3].T) + T[:3, 3]\n        T_res = T @ T_res\n        weight = get_geman_mclure_weight(source, xyz_target, mu)\n    return T_res\n\n\ndef compute_hit_ratio(xyz, xyz_target, T_gt, tau_1):\n    """"""\n    compute proportion of point which are close.\n    """"""\n    assert xyz.shape == xyz.shape\n    dist = torch.norm(xyz.mm(T_gt[:3, :3].T) + T_gt[:3, 3] - xyz_target, dim=1)\n\n    return torch.mean((dist < 0.1).to(torch.float))\n\n\ndef compute_transfo_error(T_gt, T_pred):\n    """"""\n    compute the translation error (the unit depends on the unit of the point cloud)\n    and compute the rotation error in degree using the formula (norm of antisymetr):\n    http://jlyang.org/tpami16_go-icp_preprint.pdf\n    """"""\n    rte = torch.norm(T_gt[:3, 3] - T_pred[:3, 3])\n    cos_theta = (torch.trace(T_gt[:3, :3].mm(T_pred[:3, :3].T)) - 1) * 0.5\n    cos_theta = torch.clamp(cos_theta, -1.0, 1.0)\n    rre = torch.acos(cos_theta) * 180 / np.pi\n    return rte, rre\n\n\ndef get_matches(feat_source, feat_target, sym=False):\n\n    matches = knn(feat_target, feat_source, k=1).T\n    if sym:\n        match_inv = knn(feat_source, feat_target, k=1).T\n        mask = match_inv[matches[:, 1], 1] == torch.arange(matches.shape[0])\n        return matches[mask]\n    else:\n        return matches\n'"
torch_points3d/metrics/registration_tracker.py,3,"b'from typing import Dict\nimport torchnet as tnt\nimport torch\n\nfrom .base_tracker import BaseTracker\nfrom .registration_metrics import compute_accuracy\nfrom .registration_metrics import estimate_transfo\nfrom .registration_metrics import fast_global_registration\nfrom .registration_metrics import compute_hit_ratio\nfrom .registration_metrics import compute_transfo_error\nfrom .registration_metrics import get_matches\nfrom torch_points3d.models import model_interface\n\n\nclass PatchRegistrationTracker(BaseTracker):\n    def __init__(self, dataset, stage=""train"", wandb_log=False, use_tensorboard: bool = False):\n        """"""\n        generic tracker for registration task.\n        to track results, it measures the loss, and the accuracy.\n        only useful for the training.\n        """"""\n\n        super(PatchRegistrationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n\n        self.reset(stage)\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n\n    def track(self, model: model_interface.TrackerInterface, **kwargs):\n        """""" Add model predictions (accuracy)\n        """"""\n        super().track(model)\n\n        outputs = self._convert(model.get_output())\n        N = len(outputs) // 2\n\n        self._acc = compute_accuracy(outputs[:N], outputs[N:])\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n\n        metrics[""{}_acc"".format(self._stage)] = self._acc\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {""loss"": min, ""acc"": max}\n        return self._metric_func\n\n\nclass FragmentRegistrationTracker(BaseTracker):\n    def __init__(\n        self,\n        dataset,\n        num_points=5000,\n        tau_1=0.1,\n        tau_2=0.05,\n        stage=""train"",\n        wandb_log=False,\n        use_tensorboard: bool = False,\n    ):\n\n        """"""\n        tracker for registration tasks (we learn feature for each fragments like segmentation network)\nit measures loss, feature match recall, hit ratio, rotation error, translation error.\n        """"""\n        super(FragmentRegistrationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n        self.reset(stage)\n        self.num_points = num_points\n        self.tau_1 = tau_1\n        self.tau_2 = tau_2\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n        self._rot_error = tnt.meter.AverageValueMeter()\n        self._trans_error = tnt.meter.AverageValueMeter()\n        self._hit_ratio = tnt.meter.AverageValueMeter()\n        self._feat_match_ratio = tnt.meter.AverageValueMeter()\n\n    def track(self, model: model_interface.TrackerInterface, **kwargs):\n        super().track(model)\n        if self._stage != ""train"":\n            batch_idx, batch_idx_target = model.get_batch()\n            # batch_xyz, batch_xyz_target = model.get_xyz()  # type: ignore\n            # batch_ind, batch_ind_target, batch_size_ind = model.get_ind()  # type: ignore\n            input, input_target = model.get_input()\n            batch_xyz, batch_xyz_target = input.pos, input_target.pos\n            batch_ind, batch_ind_target, batch_size_ind = input.ind, input_target.ind, input.size\n            batch_feat, batch_feat_target = model.get_output()\n\n            nb_batches = batch_idx.max() + 1\n            cum_sum = 0\n            cum_sum_target = 0\n            begin = 0\n            end = batch_size_ind[0].item()\n            for b in range(nb_batches):\n                xyz = batch_xyz[batch_idx == b]\n                xyz_target = batch_xyz_target[batch_idx_target == b]\n                feat = batch_feat[batch_idx == b]\n                feat_target = batch_feat_target[batch_idx_target == b]\n                # as we have concatenated ind,\n                # we need to substract the cum_sum because we deal\n                # with each batch independently\n                # ind = batch_ind[b * len(batch_ind) / nb_batches : (b + 1) * len(batch_ind) / nb_batches] - cum_sum\n                # ind_target = (batch_ind_target[b * len(batch_ind_target) / nb_batches : (b + 1) * len(batch_ind_target) / nb_batches]- cum_sum_target)\n                ind = batch_ind[begin:end] - cum_sum\n                ind_target = batch_ind_target[begin:end] - cum_sum_target\n                # print(begin, end)\n                if b < nb_batches - 1:\n                    begin = end\n                    end = begin + batch_size_ind[b + 1].item()\n                cum_sum += len(xyz)\n                cum_sum_target += len(xyz_target)\n                rand = torch.randperm(len(feat))[: self.num_points]\n                rand_target = torch.randperm(len(feat_target))[: self.num_points]\n\n                matches_gt = torch.stack([ind, ind_target]).transpose(0, 1)\n\n                # print(matches_gt.max(0), len(xyz), len(xyz_target), len(matches_gt))\n                # print(batch_ind.shape, nb_batches)\n                T_gt = estimate_transfo(xyz[matches_gt[:, 0]], xyz_target[matches_gt[:, 1]])\n\n                matches_pred = get_matches(feat[rand], feat_target[rand_target])\n                T_pred = fast_global_registration(\n                    xyz[rand][matches_pred[:, 0]], xyz_target[rand_target][matches_pred[:, 1]]\n                )\n\n                hit_ratio = compute_hit_ratio(\n                    xyz[rand][matches_pred[:, 0]], xyz_target[rand_target][matches_pred[:, 1]], T_gt, self.tau_1\n                )\n\n                trans_error, rot_error = compute_transfo_error(T_pred, T_gt)\n                self._hit_ratio.add(hit_ratio.item())\n                self._feat_match_ratio.add(float(hit_ratio.item() > self.tau_2))\n                self._trans_error.add(trans_error.item())\n                self._rot_error.add(rot_error.item())\n\n    def get_metrics(self, verbose=False):\n        metrics = super().get_metrics(verbose)\n        if self._stage != ""train"":\n            metrics[""{}_hit_ratio"".format(self._stage)] = float(self._hit_ratio.value()[0])\n            metrics[""{}_feat_match_ratio"".format(self._stage)] = float(self._feat_match_ratio.value()[0])\n            metrics[""{}_trans_error"".format(self._stage)] = float(self._trans_error.value()[0])\n            metrics[""{}_rot_error"".format(self._stage)] = float(self._rot_error.value()[0])\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {\n            ""loss"": min,\n            ""hit_ratio"": max,\n            ""feat_match_ratio"": max,\n            ""trans_error"": min,\n            ""rot_error"": min,\n        }\n        return self._metric_func\n'"
torch_points3d/metrics/s3dis_tracker.py,6,"b'from typing import Dict\nimport logging\nimport torch\nfrom torch_geometric.nn.unpool import knn_interpolate\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\nfrom torch_points3d.metrics.segmentation_tracker import SegmentationTracker\nfrom torch_points3d.metrics.base_tracker import BaseTracker, meter_value\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.models import model_interface\n\nlog = logging.getLogger(__name__)\n\n\nclass S3DISTracker(SegmentationTracker):\n    def reset(self, *args, **kwargs):\n        super().reset(*args, **kwargs)\n        self._test_area = None\n        self._full_vote_miou = None\n        self._vote_miou = None\n        self._iou_per_class = {}\n\n    def track(self, model: model_interface.TrackerInterface, full_res=False, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        super().track(model)\n\n        # Train mode or low res, nothing special to do\n        if self._stage == ""train"" or not full_res:\n            return\n\n        # Test mode, compute votes in order to get full res predictions\n        if self._test_area is None:\n            self._test_area = self._dataset.test_data.clone()\n            if self._test_area.y is None:\n                raise ValueError(""It seems that the test area data does not have labels (attribute y)."")\n            self._test_area.prediction_count = torch.zeros(self._test_area.y.shape[0], dtype=torch.int)\n            self._test_area.votes = torch.zeros((self._test_area.y.shape[0], self._num_classes), dtype=torch.float)\n            self._test_area.to(model.device)\n\n        # Gather origin ids and check that it fits with the test set\n        inputs = model.get_input()\n        if inputs[SaveOriginalPosId.KEY] is None:\n            raise ValueError(""The inputs given to the model do not have a %s attribute."" % SaveOriginalPosId.KEY)\n\n        originids = inputs[SaveOriginalPosId.KEY]\n        if originids.dim() == 2:\n            originids = originids.flatten()\n        if originids.max() >= self._test_area.pos.shape[0]:\n            raise ValueError(""Origin ids are larger than the number of points in the original point cloud."")\n\n        # Set predictions\n        outputs = model.get_output()\n        self._test_area.votes[originids] += outputs\n        self._test_area.prediction_count[originids] += 1\n\n    def finalise(self, full_res=False, vote_miou=True, ply_output="""", **kwargs):\n        per_class_iou = self._confusion_matrix.get_intersection_union_per_class()[0]\n        self._iou_per_class = {self._dataset.INV_OBJECT_LABEL[k]: v for k, v in enumerate(per_class_iou)}\n\n        if vote_miou and self._test_area:\n            # Complete for points that have a prediction\n            self._test_area = self._test_area.to(""cpu"")\n            c = ConfusionMatrix(self._num_classes)\n            has_prediction = self._test_area.prediction_count > 0\n            gt = self._test_area.y[has_prediction].numpy()\n            pred = torch.argmax(self._test_area.votes[has_prediction], 1).numpy()\n            c.count_predicted_batch(gt, pred)\n            self._vote_miou = c.get_average_intersection_union() * 100\n\n        if full_res:\n            self._compute_full_miou()\n\n        if ply_output:\n            has_prediction = self._test_area.prediction_count > 0\n            self._dataset.to_ply(\n                self._test_area.pos[has_prediction].cpu(),\n                torch.argmax(self._test_area.votes[has_prediction], 1).cpu().numpy(),\n                ply_output,\n            )\n\n    def _compute_full_miou(self):\n        if self._full_vote_miou is not None:\n            return\n\n        has_prediction = self._test_area.prediction_count > 0\n        log.info(\n            ""Computing full res mIoU, we have predictions for %.2f%% of the points.""\n            % (torch.sum(has_prediction) / (1.0 * has_prediction.shape[0]) * 100)\n        )\n\n        self._test_area = self._test_area.to(""cpu"")\n\n        # Full res interpolation\n        full_pred = knn_interpolate(\n            self._test_area.votes[has_prediction], self._test_area.pos[has_prediction], self._test_area.pos, k=1,\n        )\n\n        # Full res pred\n        c = ConfusionMatrix(self._num_classes)\n        c.count_predicted_batch(self._test_area.y.numpy(), torch.argmax(full_pred, 1).numpy())\n        self._full_vote_miou = c.get_average_intersection_union() * 100\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n\n        if verbose:\n            metrics[""{}_iou_per_class"".format(self._stage)] = self._iou_per_class\n            if self._vote_miou:\n                metrics[""{}_full_vote_miou"".format(self._stage)] = self._full_vote_miou\n                metrics[""{}_vote_miou"".format(self._stage)] = self._vote_miou\n        return metrics\n'"
torch_points3d/metrics/scannet_segmentation_tracker.py,3,"b'import os.path as osp\nfrom typing import Dict\nimport logging\nimport numpy as np\nimport torch\nfrom torch_geometric.nn.unpool import knn_interpolate\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\nfrom torch_points3d.metrics.segmentation_tracker import SegmentationTracker\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.models import model_interface\n\nlog = logging.getLogger(__name__)\n\n\nclass ScannetSegmentationTracker(SegmentationTracker):\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n        self._full_confusion_matrix = ConfusionMatrix(self._num_classes)\n        self._raw_datas = {}\n        self._votes = {}\n        self._vote_counts = {}\n        self._full_preds = {}\n        self._full_acc = None\n\n    def track(self, model: model_interface.TrackerInterface, full_res=False, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        super().track(model)\n\n        # Set conv type\n        self._conv_type = model.conv_type\n\n        # Train mode or low res, nothing special to do\n        if not full_res or self._stage == ""train"" or kwargs.get(""data"") is None:\n            return\n\n        self._vote(kwargs.get(""data""), model.get_output())\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n        if self._full_acc:\n            metrics[""{}_full_acc"".format(self._stage)] = self._full_acc\n            metrics[""{}_full_macc"".format(self._stage)] = self._full_macc\n            metrics[""{}_full_miou"".format(self._stage)] = self._full_miou\n        return metrics\n\n    def finalise(self, full_res=False, make_submission=False, **kwargs):\n        if not full_res and not make_submission:\n            return\n\n        self._predict_full_res()\n\n        # Compute full res metrics\n        if self._dataset.has_labels(self._stage):\n            for scan_id in self._full_preds:\n                full_labels = self._raw_datas[scan_id].y\n                # Mask ignored labels\n                mask = full_labels != self._ignore_label\n                full_labels = full_labels[mask]\n                full_preds = self._full_preds[scan_id].cpu()[mask].numpy()\n                self._full_confusion_matrix.count_predicted_batch(full_labels, full_preds)\n\n            self._full_acc = 100 * self._full_confusion_matrix.get_overall_accuracy()\n            self._full_macc = 100 * self._full_confusion_matrix.get_mean_class_accuracy()\n            self._full_miou = 100 * self._full_confusion_matrix.get_average_intersection_union()\n\n        # Save files to disk\n        if make_submission and self._stage == ""test"":\n            self._make_submission()\n\n    def _make_submission(self):\n        orginal_class_ids = np.asarray(self._dataset.train_dataset.valid_class_idx)\n        path_to_submission = self._dataset.path_to_submission\n        for scan_id in self._full_preds:\n            full_pred = self._full_preds[scan_id].cpu().numpy().astype(np.int8)\n            full_pred = orginal_class_ids[full_pred]  # remap labels to original labels between 0 and 40\n            scan_name = self._raw_datas[scan_id].scan_name\n            path_file = osp.join(path_to_submission, ""{}.txt"".format(scan_name))\n            np.savetxt(path_file, full_pred, delimiter=""/n"", fmt=""%d"")\n\n    def _vote(self, data, output):\n        """""" Populates scores for the points in data\n\n        Parameters\n        ----------\n        data : Data\n            should contain `pos` and `SaveOriginalPosId.KEY` keys\n        output : torch.Tensor\n            probablities out of the model, shape: [N,nb_classes]\n        """"""\n        id_scans = data.id_scan.squeeze()\n        if self._conv_type == ""DENSE"":\n            batch_size = len(id_scans)\n            output = output.view(batch_size, -1, output.shape[-1])\n\n        for idx_batch, id_scan in enumerate(id_scans):\n            # First time we see this scan\n            if id_scan not in self._raw_datas:\n                raw_data = self._dataset.get_raw_data(self._stage, id_scan, remap_labels=True)\n                self._raw_datas[id_scan] = raw_data\n                self._vote_counts[id_scan] = torch.zeros(raw_data.pos.shape[0], dtype=torch.int)\n                self._votes[id_scan] = torch.zeros((raw_data.pos.shape[0], self._num_classes), dtype=torch.float)\n            else:\n                raw_data = self._raw_datas[id_scan]\n\n            batch_mask = idx_batch\n            if self._conv_type != ""DENSE"":\n                batch_mask = data.batch == idx_batch\n            idx = data[SaveOriginalPosId.KEY][batch_mask]\n\n            self._votes[id_scan][idx] += output[batch_mask]\n            self._vote_counts[id_scan][idx] += 1\n\n    def _predict_full_res(self):\n        """""" Predict full resolution results based on votes """"""\n        for id_scan in self._votes:\n            has_prediction = self._vote_counts[id_scan] > 0\n            self._votes[id_scan][has_prediction] /= self._vote_counts[id_scan][has_prediction].unsqueeze(-1)\n\n            # Upsample and predict\n            full_pred = knn_interpolate(\n                self._votes[id_scan][has_prediction],\n                self._raw_datas[id_scan].pos[has_prediction],\n                self._raw_datas[id_scan].pos,\n                k=1,\n            )\n            self._full_preds[id_scan] = full_pred.argmax(-1)\n'"
torch_points3d/metrics/segmentation_helpers.py,4,"b'import numpy as np\nimport torch\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_geometric.nn.unpool import knn_interpolate\n\n\nclass SegmentationVoter:\n    """"""\n    This class is a helper to perform full point cloud prediction by having votes interpolated using knn\n    """"""\n\n    def __init__(self, raw_data, num_classes, conv_type, class_seg_map=None, k: int = 1):\n        assert k > 0\n        self._raw_data = raw_data\n        self._num_pos = raw_data.pos.shape[0]\n        self._votes = torch.zeros((self._num_pos, num_classes), dtype=torch.float)\n        self._vote_counts = torch.zeros(self._num_pos, dtype=torch.float)\n        self._full_res_preds = None\n        self._conv_type = conv_type\n        self._class_seg_map = class_seg_map\n        self._k = k\n        self._num_votes = 0\n\n    @property\n    def k(self):\n        return self._k\n\n    @k.setter\n    def k(self, k):\n        if isinstance(k, int):\n            if k > 0:\n                self._k = k\n            else:\n                raise Exception(""k should be >= 1"")\n        else:\n            raise Exception(""k used for knn_interpolate should be an int"")\n\n    @property\n    def num_votes(self):\n        return self._num_votes\n\n    @property\n    def coverage(self):\n        num = np.sum((self._vote_counts > 0).numpy())\n        return float(num) / self._num_pos\n\n    @property\n    def full_res_labels(self):\n        return self._raw_data.y\n\n    @property\n    def full_res_preds(self):\n        self._predict_full_res()\n        if self._class_seg_map:\n            return self._full_res_preds[:, self._class_seg_map].argmax(1) + self._class_seg_map[0]\n        else:\n            return self._full_res_preds.argmax(-1)\n\n    def add_vote(self, data, output, batch_mask):\n        """""" Populates scores for the points in data\n\n        Parameters\n        ----------\n        data : Data\n            should contain `pos` and `SaveOriginalPosId.KEY` keys\n        output : torch.Tensor\n            probablities out of the model, shape: [N,nb_classes]\n        batch_mask: torch.Tensor | int\n            mask to access the associated element\n        """"""\n        idx = data[SaveOriginalPosId.KEY][batch_mask]\n        self._votes[idx] += output\n        self._vote_counts[idx] += 1\n        self._num_votes += 1\n\n    def _predict_full_res(self):\n        """""" Predict full resolution results based on votes """"""\n        has_prediction = self._vote_counts > 0\n        votes = self._votes[has_prediction].div(self._vote_counts[has_prediction].unsqueeze(-1))\n\n        # Upsample and predict\n        full_pred = knn_interpolate(votes, self._raw_data.pos[has_prediction], self._raw_data.pos, k=self._k)\n        self._full_res_preds = full_pred\n\n    def __repr__(self):\n        return ""{}(num_pos={})"".format(self.__class__.__name__, self._num_pos)\n'"
torch_points3d/metrics/segmentation_tracker.py,1,"b'from typing import Dict\nimport torch\nimport numpy as np\n\nfrom torch_points3d.metrics.confusion_matrix import ConfusionMatrix\nfrom torch_points3d.metrics.base_tracker import BaseTracker, meter_value\nfrom torch_points3d.metrics.meters import APMeter\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\nfrom torch_points3d.models import model_interface\n\n\nclass SegmentationTracker(BaseTracker):\n    def __init__(\n        self, dataset, stage=""train"", wandb_log=False, use_tensorboard: bool = False, ignore_label: int = IGNORE_LABEL\n    ):\n        """""" This is a generic tracker for segmentation tasks.\n        It uses a confusion matrix in the back-end to track results.\n        Use the tracker to track an epoch.\n        You can use the reset function before you start a new epoch\n\n        Arguments:\n            dataset  -- dataset to track (used for the number of classes)\n\n        Keyword Arguments:\n            stage {str} -- current stage. (train, validation, test, etc...) (default: {""train""})\n            wandb_log {str} --  Log using weight and biases\n        """"""\n        super(SegmentationTracker, self).__init__(stage, wandb_log, use_tensorboard)\n        self._num_classes = dataset.num_classes\n        self._ignore_label = ignore_label\n        self._dataset = dataset\n        self.reset(stage)\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n        self._confusion_matrix = ConfusionMatrix(self._num_classes)\n        self._acc = 0\n        self._macc = 0\n        self._miou = 0\n\n    @staticmethod\n    def detach_tensor(tensor):\n        if torch.torch.is_tensor(tensor):\n            tensor = tensor.detach()\n        return tensor\n\n    @property\n    def confusion_matrix(self):\n        return self._confusion_matrix.confusion_matrix\n\n    def track(self, model: model_interface.TrackerInterface, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        if not self._dataset.has_labels(self._stage):\n            return\n\n        super().track(model)\n\n        outputs = model.get_output()\n        targets = model.get_labels()\n\n        # Mask ignored label\n        mask = targets != self._ignore_label\n        outputs = outputs[mask]\n        targets = targets[mask]\n\n        outputs = self._convert(outputs)\n        targets = self._convert(targets)\n\n        if len(targets) == 0:\n            return\n\n        assert outputs.shape[0] == len(targets)\n        self._confusion_matrix.count_predicted_batch(targets, np.argmax(outputs, 1))\n\n        self._acc = 100 * self._confusion_matrix.get_overall_accuracy()\n        self._macc = 100 * self._confusion_matrix.get_mean_class_accuracy()\n        self._miou = 100 * self._confusion_matrix.get_average_intersection_union()\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n\n        metrics[""{}_acc"".format(self._stage)] = self._acc\n        metrics[""{}_macc"".format(self._stage)] = self._macc\n        metrics[""{}_miou"".format(self._stage)] = self._miou\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {\n            ""miou"": max,\n            ""macc"": max,\n            ""acc"": max,\n            ""loss"": min,\n            ""map"": max,\n        }  # Those map subsentences to their optimization functions\n        return self._metric_func\n'"
torch_points3d/metrics/shapenet_part_tracker.py,0,"b'from typing import Dict\nimport numpy as np\nfrom .confusion_matrix import ConfusionMatrix\nfrom .base_tracker import meter_value, BaseTracker\nfrom torch_geometric.data import Data\n\nfrom torch_points3d.models import model_interface\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.metrics.segmentation_helpers import SegmentationVoter\n\n\nclass ShapenetPartTracker(BaseTracker):\n    def __init__(self, dataset, stage: str = ""train"", wandb_log: bool = False, use_tensorboard: bool = False):\n        """""" Segmentation tracker shapenet part seg problem. The dataset needs to have a\n        class_to_segment member that defines how metrics get computed and agregated.\n        It follows shapenet official formula for computing miou which treats missing part as having an iou of 1\n        See https://github.com/charlesq34/pointnet2/blob/42926632a3c33461aebfbee2d829098b30a23aaa/part_seg/evaluate.py#L166-L176\n\n        Arguments:\n            dataset {[type]}\n\n        Keyword Arguments:\n            stage {str} -- current stage (default: {""train""})\n            wandb_log {bool} -- Log to Wanndb (default: {False})\n            use_tensorboard {bool} -- Log to tensorboard (default: {False})\n        """"""\n        super(ShapenetPartTracker, self).__init__(stage, wandb_log, use_tensorboard)\n        self._dataset = dataset\n        self._num_classes = dataset.num_classes\n        self._class_seg_map = dataset.class_to_segments\n        self._seg_to_class = {}\n        for cat, segments in self._class_seg_map.items():\n            for label in segments:\n                self._seg_to_class[label] = cat\n        self.reset(stage=stage)\n\n    def reset(self, stage=""train""):\n        super().reset(stage=stage)\n        self._shape_ious = {cat: [] for cat in self._class_seg_map.keys()}\n        self._Cmiou = 0\n        self._Imiou = 0\n        self._miou_per_class = {}\n        self._full_res_scans = {cat: {} for cat in self._class_seg_map.keys()}\n        self._full_shape_ious = {cat: [] for cat in self._class_seg_map.keys()}\n        self._full_miou_per_class = None\n        self._full_Cmiou = None\n        self._full_Imiou = None\n        self._full_res = False\n\n    def track(self, model: model_interface.TrackerInterface, full_res: bool = True, data: Data = None, **kwargs):\n        """""" Add current model predictions (usually the result of a batch) to the tracking\n        """"""\n        super().track(model)\n        self._conv_type = model.conv_type\n        outputs = self._convert(model.get_output())\n        targets = self._convert(model.get_labels())\n        batch_idx = self._convert(model.get_batch())\n        if batch_idx is None:\n            raise ValueError(""Your model need to set the batch_idx variable in its set_input function."")\n\n        nb_batches = batch_idx.max() + 1\n\n        if self._stage != ""train"" and full_res:\n            self._add_votes(data, outputs, batch_idx)\n\n        # pred to the groundtruth classes (selected by seg_classes[cat])\n        for b in range(nb_batches):\n            segl = targets[batch_idx == b]\n            cat = self._seg_to_class[segl[0]]\n            logits = outputs[batch_idx == b, :]  # (num_points, num_classes)\n            segp = logits[:, self._class_seg_map[cat]].argmax(1) + self._class_seg_map[cat][0]\n            part_ious = self._compute_part_ious(segl, segp, cat)\n            self._shape_ious[cat].append(np.mean(part_ious))\n\n        self._miou_per_class, self._Cmiou, self._Imiou = ShapenetPartTracker._get_metrics_per_class(self._shape_ious)\n\n    def _add_votes(self, data, outputs, batch_idx):\n        nb_batches = batch_idx.max() + 1\n        for b in range(nb_batches):\n            batch_mask = b\n            if self._conv_type != ""DENSE"":\n                batch_mask = batch_idx == b\n            segl = data.y[batch_mask][0].item()\n\n            cat = self._seg_to_class[segl]\n            logits = outputs[batch_idx == b, :]  # (num_points, num_classes)\n\n            id_scan = data.id_scan[b].item()\n            if id_scan not in self._full_res_scans[cat]:\n                raw_data = self._dataset.get_raw_data(self._stage, id_scan)\n                self._full_res_scans[cat][id_scan] = SegmentationVoter(\n                    raw_data, self._num_classes, self._conv_type, class_seg_map=self._class_seg_map[cat]\n                )\n            self._full_res_scans[cat][id_scan].add_vote(data, logits, batch_mask)\n\n    def finalise(self, **kwargs):\n        # Check if at least one element has been created for full res interpolation\n        contains_elements = np.sum([bool(d) for d in list(self._full_res_scans.values())]) > 0\n        if not contains_elements:\n            return\n\n        for cat in self._full_res_scans.keys():\n            samples = self._full_res_scans[cat].values()\n            for sample in samples:\n                segl = sample.full_res_labels.numpy()\n                segp = sample.full_res_preds.numpy()\n                part_ious = self._compute_part_ious(segl, segp, cat)\n                self._full_shape_ious[cat].append(np.mean(part_ious))\n        self._full_miou_per_class, self._full_Cmiou, self._full_Imiou = ShapenetPartTracker._get_metrics_per_class(\n            self._full_shape_ious\n        )\n\n        self._full_res = True\n\n    def _compute_part_ious(self, segl, segp, cat):\n        part_ious = np.zeros(len(self._class_seg_map[cat]))\n        for l in self._class_seg_map[cat]:\n            if np.sum((segl == l) | (segp == l)) == 0:\n                # part is not present in this shape\n                part_ious[l - self._class_seg_map[cat][0]] = 1\n            else:\n                part_ious[l - self._class_seg_map[cat][0]] = float(np.sum((segl == l) & (segp == l))) / float(\n                    np.sum((segl == l) | (segp == l))\n                )\n        return part_ious\n\n    def get_metrics(self, verbose=False) -> Dict[str, float]:\n        """""" Returns a dictionnary of all metrics and losses being tracked\n        """"""\n        metrics = super().get_metrics(verbose)\n        metrics[""{}_Cmiou"".format(self._stage)] = self._Cmiou * 100\n        metrics[""{}_Imiou"".format(self._stage)] = self._Imiou * 100\n        if self._full_res:\n            metrics[""{}_full_Cmiou"".format(self._stage)] = self._full_Cmiou * 100\n            metrics[""{}_full_Imiou"".format(self._stage)] = self._full_Imiou * 100\n        if verbose:\n            metrics[""{}_Imiou_per_class"".format(self._stage)] = self._miou_per_class\n            if self._full_res:\n                metrics[""{}_full_Imiou_per_class"".format(self._stage)] = self._full_miou_per_class\n        return metrics\n\n    @property\n    def metric_func(self):\n        self._metric_func = {""Cmiou"": max, ""Imiou"": max, ""loss"": min}\n        return self._metric_func\n\n    @staticmethod\n    def _get_metrics_per_class(shape_ious):\n        instance_ious = []\n        cat_ious = {}\n        for cat in shape_ious.keys():\n            for iou in shape_ious[cat]:\n                instance_ious.append(iou)\n            if len(shape_ious[cat]):\n                cat_ious[cat] = np.mean(shape_ious[cat])\n        mean_class_ious = np.mean(list(cat_ious.values()))\n        return cat_ious, mean_class_ious, np.mean(instance_ious)\n'"
torch_points3d/models/__init__.py,0,b''
torch_points3d/models/base_model.py,14,"b'from collections import OrderedDict\nfrom abc import abstractmethod\nfrom typing import Optional, Dict, Any\nimport torch\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim.lr_scheduler import _LRScheduler\nimport logging\nfrom collections import defaultdict\nfrom torch_points3d.core.schedulers.lr_schedulers import instantiate_scheduler\nfrom torch_points3d.core.schedulers.bn_schedulers import instantiate_bn_scheduler\nfrom torch_points3d.utils.enums import SchedulerUpdateOn\n\nfrom torch_points3d.core.regularizer import *\nfrom torch_points3d.core.losses import instantiate_loss_or_miner\nfrom torch_points3d.utils.config import is_dict\nfrom torch_points3d.utils.colors import colored_print, COLORS\nfrom .model_interface import TrackerInterface, DatasetInterface, CheckpointInterface\n\nlog = logging.getLogger(__name__)\n\n\nclass BaseModel(torch.nn.Module, TrackerInterface, DatasetInterface, CheckpointInterface):\n    """"""This class is an abstract base class (ABC) for models.\n    To create a subclass, you need to implement the following five functions:\n        -- <__init__>:                      initialize the class; first call BaseModel.__init__(self, opt).\n        -- <set_input>:                     unpack data from dataset and apply preprocessing.\n        -- <forward>:                       produce intermediate results.\n        -- <optimize_parameters>:           calculate losses, gradients, and update network weights.\n    """"""\n\n    def __init__(self, opt):\n        """"""Initialize the BaseModel class.\n        Parameters:\n            opt (Option class)-- stores all the experiment flags; needs to be a subclass of BaseOptions\n        When creating your custom class, you need to implement your own initialization.\n        In this fucntion, you should first call <BaseModel.__init__(self, opt)>\n        Then, you need to define four lists:\n            -- self.loss_names (str list):          specify the training losses that you want to plot and save.\n            -- self.model_names (str list):         specify the images that you want to display and save.\n            -- self.visual_names (str list):        define networks used in our training.\n            -- self.optimizers (optimizer list):    define and initialize optimizers. You can define one optimizer for each network. If two networks are updated at the same time, you can use itertools.chain to group them. See cycle_gan_model.py for an example.\n        """"""\n        super(BaseModel, self).__init__()\n        self.opt = opt\n        self.loss_names = []\n        self.visual_names = []\n        self.output = None\n        self._conv_type = opt.conv_type\n        self._optimizer: Optional[Optimizer] = None\n        self._lr_scheduler: Optimizer[_LRScheduler] = None\n        self._bn_scheduler = None\n        self._spatial_ops_dict: Dict = {}\n        self._num_epochs = None\n        self._num_batches = 0\n        self._num_samples = -1\n        self._latest_metrics = None\n        self._latest_stage = None\n        self._latest_epoch = None\n        self._schedulers = {}\n        self._accumulated_gradient_step = None\n        self._grad_clip = -1\n        self._update_lr_scheduler_on = ""on_epoch""\n        self._update_bn_scheduler_on = ""on_epoch""\n\n    @property\n    def schedulers(self):\n        return self._schedulers\n\n    @schedulers.setter\n    def schedulers(self, schedulers):\n        if schedulers:\n            self._schedulers = schedulers\n            for scheduler_name, scheduler in schedulers.items():\n                setattr(self, ""_{}"".format(scheduler_name), scheduler)\n\n    def _add_scheduler(self, scheduler_name, scheduler):\n        setattr(self, ""_{}"".format(scheduler_name), scheduler)\n        self._schedulers[scheduler_name] = scheduler\n\n    @property\n    def optimizer(self):\n        return self._optimizer\n\n    @optimizer.setter\n    def optimizer(self, optimizer):\n        self._optimizer = optimizer\n\n    @property\n    def learning_rate(self):\n        for param_group in self.optimizer.param_groups:\n            return param_group[""lr""]\n\n    @property\n    def device(self):\n        return next(self.parameters()).device\n\n    @property\n    def conv_type(self):\n        return self._conv_type\n\n    def set_input(self, input, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input (dict): includes the data itself and its metadata information.\n        """"""\n        raise NotImplementedError\n\n    def get_labels(self):\n        """""" returns a trensor of size ``[N_points]`` where each value is the label of a point\n        """"""\n        return getattr(self, ""labels"", None)\n\n    def get_batch(self):\n        """""" returns a trensor of size ``[N_points]`` where each value is the batch index of a point\n        """"""\n        return getattr(self, ""batch_idx"", None)\n\n    def get_output(self):\n        """""" returns a trensor of size ``[N_points,...]`` where each value is the output\n        of the network for a point (output of the last layer in general)\n        """"""\n        return self.output\n\n    def get_input(self):\n        """""" returns the last input that was given to the model or raises error\n        """"""\n        return getattr(self, ""input"")\n\n    def forward(self, *args, **kwargs) -> Any:\n        """"""Run forward pass; called by both functions <optimize_parameters> and <test>.""""""\n        raise NotImplementedError(""You must implement your own forward"")\n\n    def _manage_optimizer_zero_grad(self):\n        if not self._accumulated_gradient_step:\n            self._optimizer.zero_grad()  # clear existing gradients\n            return True\n        else:\n            if self._accumulated_gradient_count == self._accumulated_gradient_step:\n                self._accumulated_gradient_count = 0\n                return True\n\n            if self._accumulated_gradient_count == 0:\n                self._optimizer.zero_grad()  # clear existing gradients\n            self._accumulated_gradient_count += 1\n            return False\n\n    def _collect_scheduler_step(self, update_scheduler_on):\n        if hasattr(self, update_scheduler_on):\n            update_scheduler_on = getattr(self, update_scheduler_on)\n            if update_scheduler_on is None:\n                raise Exception(""The function instantiate_optimizers doesn\'t look like called"")\n\n            if update_scheduler_on == SchedulerUpdateOn.ON_EPOCH.value:\n                return self._num_epochs\n            elif update_scheduler_on == SchedulerUpdateOn.ON_NUM_BATCH.value:\n                return self._num_batches\n            elif update_scheduler_on == SchedulerUpdateOn.ON_NUM_SAMPLE.value:\n                return self._num_samples\n        else:\n            raise Exception(""The attributes {} should be defined within self"".format(update_scheduler_on))\n\n    def optimize_parameters(self, epoch, batch_size):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        self._num_epochs = epoch\n        self._num_batches += 1\n        self._num_samples += batch_size\n\n        self.forward()  # first call forward to calculate intermediate results\n        make_optimizer_step = self._manage_optimizer_zero_grad()  # Accumulate gradient if option is up\n        self.backward()  # calculate gradients\n\n        if self._grad_clip > 0:\n            torch.nn.utils.clip_grad_value_(self.parameters(), self._grad_clip)\n\n        if make_optimizer_step:\n            self._optimizer.step()  # update parameters\n\n        if self._lr_scheduler:\n            lr_scheduler_step = self._collect_scheduler_step(""_update_lr_scheduler_on"")\n            self._lr_scheduler.step(lr_scheduler_step)\n\n        if self._bn_scheduler:\n            bn_scheduler_step = self._collect_scheduler_step(""_update_bn_scheduler_on"")\n            self._bn_scheduler.step(bn_scheduler_step)\n\n    def get_current_losses(self):\n        """"""Return traning losses / errors. train.py will print out these errors on console""""""\n        errors_ret = OrderedDict()\n        for name in self.loss_names:\n            if isinstance(name, str):\n                if hasattr(self, name):\n                    try:\n                        errors_ret[name] = float(getattr(self, name))\n                    except:\n                        errors_ret[name] = None\n        return errors_ret\n\n    def instantiate_optimizers(self, config):\n        # Optimiser\n        optimizer_opt = self.get_from_opt(\n            config,\n            [""training"", ""optim"", ""optimizer""],\n            msg_err=""optimizer needs to be defined within the training config"",\n        )\n        optmizer_cls_name = optimizer_opt.get(""class"")\n        optimizer_cls = getattr(torch.optim, optmizer_cls_name)\n        optimizer_params = {}\n        if hasattr(optimizer_opt, ""params""):\n            optimizer_params = optimizer_opt.params\n        self._optimizer = optimizer_cls(self.parameters(), **optimizer_params)\n\n        # LR Scheduler\n        scheduler_opt = self.get_from_opt(config, [""training"", ""optim"", ""lr_scheduler""])\n        if scheduler_opt:\n            update_lr_scheduler_on = config.update_lr_scheduler_on\n            if update_lr_scheduler_on:\n                self._update_lr_scheduler_on = update_lr_scheduler_on\n            scheduler_opt.update_scheduler_on = self._update_lr_scheduler_on\n            lr_scheduler = instantiate_scheduler(self._optimizer, scheduler_opt)\n            self._add_scheduler(""lr_scheduler"", lr_scheduler)\n\n        # BN Scheduler\n        bn_scheduler_opt = self.get_from_opt(config, [""training"", ""optim"", ""bn_scheduler""])\n        if bn_scheduler_opt:\n            update_bn_scheduler_on = config.update_bn_scheduler_on\n            if update_bn_scheduler_on:\n                self._update_bn_scheduler_on = update_bn_scheduler_on\n            bn_scheduler_opt.update_scheduler_on = self._update_bn_scheduler_on\n            bn_scheduler = instantiate_bn_scheduler(self, bn_scheduler_opt)\n            self._add_scheduler(""bn_scheduler"", bn_scheduler)\n\n        # Accumulated gradients\n        self._accumulated_gradient_step = self.get_from_opt(config, [""training"", ""optim"", ""accumulated_gradient""])\n        if self._accumulated_gradient_step:\n            if self._accumulated_gradient_step > 1:\n                self._accumulated_gradient_count = 0\n            else:\n                raise Exception(""When set, accumulated_gradient option should be an integer greater than 1"")\n\n        # Gradient clipping\n        self._grad_clip = self.get_from_opt(config, [""training"", ""optim"", ""grad_clip""], default_value=-1)\n\n    def get_regularization_loss(self, regularizer_type=""L2"", **kwargs):\n        loss = 0\n        regularizer_cls = RegularizerTypes[regularizer_type.upper()].value\n        regularizer = regularizer_cls(self, **kwargs)\n        return regularizer.regularized_all_param(loss)\n\n    def get_named_internal_losses(self):\n        """"""\n            Modules which have internal losses return a dict of the form\n            {<loss_name>: <loss>}\n            This method merges the dicts of all child modules with internal loss\n            and returns this merged dict\n        """"""\n        losses_global = defaultdict(list)\n\n        def search_from_key(modules, losses_global):\n            for _, module in modules.items():\n                if isinstance(module, BaseInternalLossModule):\n                    losses = module.get_internal_losses()\n                    for loss_name, loss_value in losses.items():\n                        if torch.is_tensor(loss_value):\n                            assert loss_value.dim() == 0\n                            losses_global[loss_name].append(loss_value)\n                        elif isinstance(loss_value, float):\n                            losses_global[loss_name].append(torch.tensor(loss_value).to(self.device))\n                        else:\n                            raise ValueError(""Unsupported value type for a loss: {}"".format(loss_value))\n                search_from_key(module._modules, losses_global)\n\n        search_from_key(self._modules, losses_global)\n        return losses_global\n\n    def collect_internal_losses(self, lambda_weight=1, aggr_func=torch.sum):\n        """"""\n            Collect internal loss of all child modules with\n            internal losses and set the losses\n        """"""\n        loss_out = 0\n        losses = self.get_named_internal_losses()\n        for loss_name, loss_values in losses.items():\n            if loss_name not in self.loss_names:\n                self.loss_names.append(loss_name)\n            item_loss = lambda_weight * aggr_func(torch.stack(loss_values))\n            loss_out += item_loss\n            setattr(self, loss_name, item_loss.item())\n        return loss_out\n\n    def get_internal_loss(self):\n        """"""\n            Returns the average internal loss of all child modules with\n            internal losses\n        """"""\n        loss = 0\n        c = 0\n        losses = self.get_named_internal_losses()\n        for loss_name, loss_values in losses.items():\n            loss += torch.mean(torch.stack(loss_values))\n            c += 1\n        if c == 0:\n            return loss\n        else:\n            return loss / c\n\n    @staticmethod\n    def get_metric_loss_and_miner(opt_loss, opt_miner):\n        """"""\n        instantiate the loss and the miner if it\'s available\n        in the yaml config:\n\n        example in the yaml config\n        metric_loss:\n            class: ""TripletMarginLoss""\n            params:\n                smooth_loss: True\n                triplets_per_anchors: \'all\'\n        """"""\n        loss = None\n        miner = None\n        if opt_loss is not None:\n            loss = instantiate_loss_or_miner(opt_loss, mode=""metric_loss"")\n        if opt_miner is not None:\n            miner = instantiate_loss_or_miner(opt_miner, mode=""miner"")\n\n        return loss, miner\n\n    def get_spatial_ops(self):\n        return self._spatial_ops_dict\n\n    def enable_dropout_in_eval(self):\n        def search_from_key(modules):\n            for _, m in modules.items():\n                if m.__class__.__name__.startswith(""Dropout""):\n                    m.train()\n                search_from_key(m._modules)\n\n        search_from_key(self._modules)\n\n    def get_from_opt(self, opt, keys=[], default_value=None, msg_err=None, silent=True):\n        if len(keys) == 0:\n            raise Exception(""Keys should not be empty"")\n        value_out = default_value\n\n        def search_with_keys(args, keys, value_out):\n            if len(keys) == 0:\n                value_out = args\n                return value_out\n            value = args[keys[0]]\n            return search_with_keys(value, keys[1:], value_out)\n\n        try:\n            value_out = search_with_keys(opt, keys, value_out)\n        except Exception as e:\n            if msg_err:\n                raise Exception(str(msg_err))\n            else:\n                if not silent:\n                    log.exception(e)\n            value_out = default_value\n        return value_out\n\n    def get_current_visuals(self):\n        """"""Return an OrderedDict containing associated tensors within visual_names""""""\n        visual_ret = OrderedDict()\n        for name in self.visual_names:\n            if isinstance(name, str):\n                visual_ret[name] = getattr(self, name)\n        return visual_ret\n\n    def log_optimizers(self):\n        colored_print(COLORS.Green, ""Optimizer: {}"".format(self._optimizer))\n        colored_print(COLORS.Green, ""Learning Rate Scheduler: {}"".format(self._lr_scheduler))\n        colored_print(COLORS.Green, ""BatchNorm Scheduler: {}"".format(self._bn_scheduler))\n        colored_print(COLORS.Green, ""Accumulated gradients: {}"".format(self._accumulated_gradient_step))\n\n    def to(self, *args, **kwargs):\n        super().to(*args, *kwargs)\n        if self.optimizer:\n            for state in self.optimizer.state.values():\n                for k, v in state.items():\n                    if isinstance(v, torch.Tensor):\n                        state[k] = v.to(*args, **kwargs)\n        return self\n\n    def cpu(self):\n        return self.to(torch.device(""cpu""))\n\n    def cuda(self):\n        return self.to(torch.device(""cuda""))\n\n\nclass BaseInternalLossModule(torch.nn.Module):\n    """"""ABC for modules which have internal loss(es)\n    """"""\n\n    @abstractmethod\n    def get_internal_losses(self) -> Dict[str, Any]:\n        pass\n'"
torch_points3d/models/model_factory.py,0,"b'import importlib\nimport hydra\n\nfrom .base_model import BaseModel\nfrom torch_points3d.utils.model_building_utils.model_definition_resolver import resolve_model\n\n\ndef instantiate_model(config, dataset) -> BaseModel:\n    """""" Creates a model given a datset and a training config. The config should contain the following:\n    - config.data.task: task that will be evaluated\n    - config.model_name: model to instantiate\n    - config.models: All models available\n    """"""\n\n    # Get task and model_name\n    task = config.data.task\n    tested_model_name = config.model_name\n\n    # Find configs\n    model_config = getattr(config.models, tested_model_name, None)\n    if model_config is None:\n        raise Exception(""The model_name {} isn t within {}"".format(tested_model_name, list(config.models.keys())))\n    resolve_model(model_config, dataset, task)\n\n    model_class = getattr(model_config, ""class"")\n    model_paths = model_class.split(""."")\n    module = ""."".join(model_paths[:-1])\n    class_name = model_paths[-1]\n    model_module = ""."".join([""torch_points3d.models"", task, module])\n    modellib = importlib.import_module(model_module)\n\n    model_cls = None\n    for name, cls in modellib.__dict__.items():\n        if name.lower() == class_name.lower():\n            model_cls = cls\n\n    if model_cls is None:\n        raise NotImplementedError(\n            ""In %s.py, there should be a subclass of BaseDataset with class name that matches %s in lowercase.""\n            % (model_module, class_name)\n        )\n    model = model_cls(model_config, ""dummy"", dataset, modellib)\n    return model\n'"
torch_points3d/models/model_interface.py,0,"b'from abc import abstractmethod, abstractproperty, ABC\n\n\nclass CheckpointInterface(ABC):\n    """"""This class is a minimal interface class for models.\n    """"""\n\n    @abstractproperty  # type: ignore\n    def schedulers(self):\n        pass\n\n    @schedulers.setter\n    def schedulers(self, schedulers):\n        pass\n\n    @abstractproperty  # type: ignore\n    def optimizer(self):\n        pass\n\n    @optimizer.setter\n    def optimizer(self, optimizer):\n        pass\n\n    @abstractmethod\n    def state_dict(self):\n        pass\n\n    @abstractmethod\n    def load_state_dict(self, state):\n        pass\n\n\nclass DatasetInterface(ABC):\n    @abstractproperty\n    def conv_type(self):\n        pass\n\n    def get_spatial_ops(self):\n        pass\n\n\nclass TrackerInterface(ABC):\n    @property\n    @abstractmethod\n    def conv_type(self):\n        pass\n\n    @abstractmethod\n    def get_labels(self):\n        """""" returns a trensor of size ``[N_points]`` where each value is the label of a point\n        """"""\n\n    @abstractmethod\n    def get_batch(self):\n        """""" returns a trensor of size ``[N_points]`` where each value is the batch index of a point\n        """"""\n\n    @abstractmethod\n    def get_output(self):\n        """""" returns a trensor of size ``[N_points,...]`` where each value is the output\n        of the network for a point (output of the last layer in general)\n        """"""\n\n    @abstractmethod\n    def get_input(self):\n        """""" returns the last input that was given to the model or raises error\n        """"""\n\n    @abstractmethod\n    def get_current_losses(self):\n        """"""Return traning losses / errors. train.py will print out these errors on console""""""\n\n    @abstractproperty\n    def device(self):\n        """""" Returns the device onto which the model leaves (cpu or gpu)\n        """"""\n'"
torch_points3d/modules/__init__.py,0,b''
torch_points3d/utils/__init__.py,0,b'from .colors import *\nfrom .config import *\nfrom .enums import *\nfrom .running_stats import *\nfrom .timer import *\nfrom .transform_utils import *\n'
torch_points3d/utils/colors.py,0,"b'import logging\n\nlog = logging.getLogger(__name__)\n\n\nclass COLORS:\n    """"""[This class is used to color the bash shell by using {} {} {} with \'COLORS.{}, text, COLORS.END_TOKEN\']\n    """"""\n\n    TRAIN_COLOR = ""\\033[0;92m""\n    VAL_COLOR = ""\\033[0;94m""\n    TEST_COLOR = ""\\033[0;93m""\n    BEST_COLOR = ""\\033[0;92m""\n\n    END_TOKEN = ""\\033[0m)""\n    END_NO_TOKEN = ""\\033[0m""\n\n    Black = ""\\033[0;30m""  # Black\n    Red = ""\\033[0;31m""  # Red\n    Green = ""\\033[0;32m""  # Green\n    Yellow = ""\\033[0;33m""  # Yellow\n    Blue = ""\\033[0;34m""  # Blue\n    Purple = ""\\033[0;35m""  # Purple\n    Cyan = ""\\033[0;36m""  # Cyan\n    White = ""\\033[0;37m""  # White\n\n    # Bold\n    BBlack = ""\\033[1;30m""  # Black\n    BRed = ""\\033[1;31m""  # Red\n    BGreen = ""\\033[1;32m""  # Green\n    BYellow = ""\\033[1;33m""  # Yellow\n    BBlue = ""\\033[1;34m""  # Blue\n    BPurple = ""\\033[1;35m""  # Purple\n    BCyan = ""\\033[1;36m""  # Cyan\n    BWhite = ""\\033[1;37m""  # White\n\n    # Underline\n    UBlack = ""\\033[4;30m""  # Black\n    URed = ""\\033[4;31m""  # Red\n    UGreen = ""\\033[4;32m""  # Green\n    UYellow = ""\\033[4;33m""  # Yellow\n    UBlue = ""\\033[4;34m""  # Blue\n    UPurple = ""\\033[4;35m""  # Purple\n    UCyan = ""\\033[4;36m""  # Cyan\n    UWhite = ""\\033[4;37m""  # White\n\n    # Background\n    On_Black = ""\\033[40m""  # Black\n    On_Red = ""\\033[41m""  # Red\n    On_Green = ""\\033[42m""  # Green\n    On_Yellow = ""\\033[43m""  # Yellow\n    On_Blue = ""\\033[44m""  # Blue\n    On_Purple = ""\\033[45m""  # Purple\n    On_Cyan = ""\\033[46m""  # Cyan\n    On_White = ""\\033[47m""  # White\n\n    # High Intensty\n    IBlack = ""\\033[0;90m""  # Black\n    IRed = ""\\033[0;91m""  # Red\n    IGreen = ""\\033[0;92m""  # Green\n    IYellow = ""\\033[0;93m""  # Yellow\n    IBlue = ""\\033[0;94m""  # Blue\n    IPurple = ""\\033[0;95m""  # Purple\n    ICyan = ""\\033[0;96m""  # Cyan\n    IWhite = ""\\033[0;97m""  # White\n\n    # Bold High Intensty\n    BIBlack = ""\\033[1;90m""  # Black\n    BIRed = ""\\033[1;91m""  # Red\n    BIGreen = ""\\033[1;92m""  # Green\n    BIYellow = ""\\033[1;93m""  # Yellow\n    BIBlue = ""\\033[1;94m""  # Blue\n    BIPurple = ""\\033[1;95m""  # Purple\n    BICyan = ""\\033[1;96m""  # Cyan\n    BIWhite = ""\\033[1;97m""  # White\n\n    # High Intensty backgrounds\n    On_IBlack = ""\\033[0;100m""  # Black\n    On_IRed = ""\\033[0;101m""  # Red\n    On_IGreen = ""\\033[0;102m""  # Green\n    On_IYellow = ""\\033[0;103m""  # Yellow\n    On_IBlue = ""\\033[0;104m""  # Blue\n    On_IPurple = ""\\033[10;95m""  # Purple\n    On_ICyan = ""\\033[0;106m""  # Cyan\n    On_IWhite = ""\\033[0;107m""  # White\n\n\ndef colored_print(color, msg):\n    log.info(color + msg + COLORS.END_NO_TOKEN)\n'"
torch_points3d/utils/config.py,0,"b'import numpy as np\nfrom typing import List\nimport shutil\nimport matplotlib.pyplot as plt\nimport os\nfrom os import path as osp\nimport torch\nimport logging\nfrom collections import namedtuple\nfrom omegaconf import OmegaConf\nfrom omegaconf.listconfig import ListConfig\nfrom omegaconf.dictconfig import DictConfig\nfrom .enums import ConvolutionFormat\nfrom torch_points3d.utils.debugging_vars import DEBUGGING_VARS\nfrom torch_points3d.utils.colors import COLORS, colored_print\nimport subprocess\n\nlog = logging.getLogger(__name__)\n\n\nclass ConvolutionFormatFactory:\n    @staticmethod\n    def check_is_dense_format(conv_type):\n        if (\n            conv_type.lower() == ConvolutionFormat.PARTIAL_DENSE.value.lower()\n            or conv_type.lower() == ConvolutionFormat.MESSAGE_PASSING.value.lower()\n            or conv_type.lower() == ConvolutionFormat.SPARSE.value.lower()\n        ):\n            return False\n        elif conv_type.lower() == ConvolutionFormat.DENSE.value.lower():\n            return True\n        else:\n            raise NotImplementedError(""Conv type {} not supported"".format(conv_type))\n\n\nclass Option:\n    """"""This class is used to enable accessing arguments as attributes without having OmaConf.\n       It is used along convert_to_base_obj function\n    """"""\n\n    def __init__(self, opt):\n        for key, value in opt.items():\n            setattr(self, key, value)\n\n\ndef convert_to_base_obj(opt):\n    return Option(OmegaConf.to_container(opt))\n\n\ndef set_debugging_vars_to_global(cfg):\n    for key in cfg.keys():\n        key_upper = key.upper()\n        if key_upper in DEBUGGING_VARS.keys():\n            DEBUGGING_VARS[key_upper] = cfg[key]\n    log.info(DEBUGGING_VARS)\n\n\ndef set_to_wandb_args(wandb_args, cfg, name):\n    var = getattr(cfg.wandb, name, None)\n    if var:\n        wandb_args[name] = var\n\n\ndef launch_wandb(cfg, launch: bool):\n    if launch:\n        import wandb\n\n        model_config = getattr(cfg.models, cfg.model_name, None)\n        model_class = getattr(model_config, ""class"")\n        tested_dataset_class = getattr(cfg.data, ""class"")\n        otimizer_class = getattr(cfg.training.optim.optimizer, ""class"")\n        scheduler_class = getattr(cfg.lr_scheduler, ""class"")\n        tags = [\n            cfg.model_name,\n            model_class.split(""."")[0],\n            tested_dataset_class,\n            otimizer_class,\n            scheduler_class,\n        ]\n\n        wandb_args = {}\n        wandb_args[""project""] = cfg.wandb.project\n        wandb_args[""tags""] = tags\n        try:\n            wandb_args[""config""] = {\n                ""run_path"": os.getcwd(),\n                ""commit"": subprocess.check_output([""git"", ""rev-parse"", ""HEAD""]).decode(""ascii"").strip(),\n            }\n        except:\n            wandb_args[""config""] = {\n                ""run_path"": os.getcwd(),\n                ""commit"": ""n/a"",\n            }\n        set_to_wandb_args(wandb_args, cfg, ""name"")\n        set_to_wandb_args(wandb_args, cfg, ""entity"")\n        set_to_wandb_args(wandb_args, cfg, ""notes"")\n\n        wandb.init(**wandb_args)\n        shutil.copyfile(\n            os.path.join(os.getcwd(), "".hydra/config.yaml""), os.path.join(os.getcwd(), "".hydra/hydra-config.yaml"")\n        )\n        wandb.save(os.path.join(os.getcwd(), "".hydra/hydra-config.yaml""))\n        wandb.save(os.path.join(os.getcwd(), "".hydra/overrides.yaml""))\n\n\ndef is_list(entity):\n    return isinstance(entity, list) or isinstance(entity, ListConfig)\n\n\ndef is_iterable(entity):\n    return isinstance(entity, list) or isinstance(entity, ListConfig) or isinstance(entity, tuple)\n\n\ndef is_dict(entity):\n    return isinstance(entity, dict) or isinstance(entity, DictConfig)\n\ndef create_symlink_from_eval_to_train(eval_checkpoint_dir):\n    root = os.path.join(os.getcwd(), ""evals"")\n    if not os.path.exists(root):\n        os.makedirs(root)\n    num_files = len(os.listdir(root)) + 1\n    os.symlink(eval_checkpoint_dir, os.path.join(root, ""eval_{}"".format(num_files)))'"
torch_points3d/utils/debugging_vars.py,0,"b'import numpy as np\n\nDEBUGGING_VARS = {""FIND_NEIGHBOUR_DIST"": False}\n\n\ndef extract_histogram(spatial_ops, normalize=True):\n    out = []\n    for idx, nf in enumerate(spatial_ops[""neighbour_finder""]):\n        dist_meters = nf.dist_meters\n        temp = {}\n        for dist_meter in dist_meters:\n            hist = dist_meter.histogram.copy()\n            if normalize:\n                hist /= hist.sum()\n            temp[str(dist_meter.radius)] = hist.tolist()\n            dist_meter.reset()\n        out.append(temp)\n    return out\n\n\nclass DistributionNeighbour(object):\n    def __init__(self, radius, bins=1000):\n        self._radius = radius\n        self._bins = bins\n        self._histogram = np.zeros(self._bins)\n\n    def reset(self):\n        self._histogram = np.zeros(self._bins)\n\n    @property\n    def radius(self):\n        return self._radius\n\n    @property\n    def histogram(self):\n        return self._histogram\n\n    @property\n    def histogram_non_zero(self):\n        idx = len(self._histogram) - np.cumsum(self._histogram[::-1]).nonzero()[0][0]\n        return self._histogram[:idx]\n\n    def add_valid_neighbours(self, points):\n        for num_valid in points:\n            self._histogram[num_valid] += 1\n\n    def __repr__(self):\n        return ""{}(radius={}, bins={})"".format(self.__class__.__name__, self._radius, self._bins)\n'"
torch_points3d/utils/enums.py,0,"b'import enum\n\n\nclass SchedulerUpdateOn(enum.Enum):\n    ON_EPOCH = ""on_epoch""\n    ON_NUM_BATCH = ""on_num_batch""\n    ON_NUM_SAMPLE = ""on_num_sample""\n\n\nclass ConvolutionFormat(enum.Enum):\n    DENSE = ""dense""\n    PARTIAL_DENSE = ""partial_dense""\n    MESSAGE_PASSING = ""message_passing""\n    SPARSE = ""sparse""\n'"
torch_points3d/utils/mock.py,7,"b'import torch\nfrom torch_geometric.data import Data, Batch\n\nfrom torch_points3d.datasets.batch import SimpleBatch\nfrom torch_points3d.core.data_transform import MultiScaleTransform\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\n\n\nclass MockDatasetConfig(object):\n    def __init__(self):\n        pass\n\n    def keys(self):\n        return []\n\n    def get(self, dataset_name, default):\n        return None\n\n\nclass MockDataset(torch.utils.data.Dataset):\n    def __init__(self, feature_size=0, transform=None, num_points=100):\n        self.feature_dimension = feature_size\n        self.num_classes = 10\n        self.num_points = num_points\n        self.batch_size = 2\n        self.weight_classes = None\n        if feature_size > 0:\n            self._feature = torch.tensor([range(feature_size) for i in range(self.num_points)], dtype=torch.float,)\n        else:\n            self._feature = None\n        self._y = torch.tensor([0 for i in range(self.num_points)], dtype=torch.long)\n        self._category = torch.ones((self.num_points,), dtype=torch.long)\n        self._ms_transform = None\n        self._transform = transform\n\n    def __len__(self):\n        return self.num_points\n\n    @property\n    def datalist(self):\n        torch.manual_seed(0)\n        torch.randn((self.num_points, 3))\n        datalist = [\n            Data(pos=torch.randn((self.num_points, 3)), x=self._feature, y=self._y, category=self._category)\n            for i in range(self.batch_size)\n        ]\n        if self._transform:\n            datalist = [self._transform(d.clone()) for d in datalist]\n        if self._ms_transform:\n            datalist = [self._ms_transform(d.clone()) for d in datalist]\n        return datalist\n\n    def __getitem__(self, index):\n        return SimpleBatch.from_data_list(self.datalist)\n\n    @property\n    def class_to_segments(self):\n        return {""class1"": [0, 1, 2, 3, 4, 5], ""class2"": [6, 7, 8, 9]}\n\n    def set_strategies(self, model):\n        strategies = model.get_spatial_ops()\n        transform = MultiScaleTransform(strategies)\n        self._ms_transform = transform\n\n\nclass MockDatasetGeometric(MockDataset):\n    def __getitem__(self, index):\n        if self._ms_transform:\n            return MultiScaleBatch.from_data_list(self.datalist)\n        else:\n            return Batch.from_data_list(self.datalist)\n'"
torch_points3d/utils/running_stats.py,0,"b'import numpy as np\n\n\nclass RunningStats:\n    def __init__(self):\n        self.n = 0\n        self.old_m = 0\n        self.new_m = 0\n        self.old_s = 0\n        self.new_s = 0\n\n    def clear(self):\n        self.n = 0\n\n    def push(self, x):\n        self.n += 1\n\n        if self.n == 1:\n            self.old_m = self.new_m = x\n            self.old_s = 0\n        else:\n            self.new_m = self.old_m + (x - self.old_m) / self.n\n            self.new_s = self.old_s + (x - self.old_m) * (x - self.new_m)\n\n            self.old_m = self.new_m\n            self.old_s = self.new_s\n\n    def mean(self):\n        return self.new_m if self.n else 0.0\n\n    def variance(self):\n        return self.new_s / (self.n - 1) if self.n > 1 else 0.0\n\n    def std(self):\n        return np.sqrt(self.variance())\n'"
torch_points3d/utils/timer.py,0,"b'from time import time\nfrom collections import defaultdict\nimport functools\nfrom .running_stats import RunningStats\n\nFunctionStats: defaultdict = defaultdict(RunningStats)\n\n\ndef time_func(*outer_args, **outer_kwargs):\n    print_rec = outer_kwargs.get(""print_rec"", 100)\n    measure_runtime = outer_kwargs.get(""measure_runtime"", False)\n    name = outer_kwargs.get(""name"", """")\n\n    def time_func_inner(func):\n        @functools.wraps(func)\n        def func_wrapper(*args, **kwargs):\n            if measure_runtime:\n                func_name = name if name else func.__name__\n                if FunctionStats.get(func_name, None) is not None:\n                    if FunctionStats[func_name].n % print_rec == 0:\n                        stats = FunctionStats[func_name]\n                        stats_mean = stats.mean()\n                        print(\n                            ""{} run in {} | {} over {} runs"".format(\n                                func_name, stats_mean, stats_mean * stats.n, stats.n\n                            )\n                        )\n                        # print(\'{} run in {} +/- {} over {} runs\'.format(func.__name__, stats.mean(), stats.std(), stats.n))\n                t0 = time()\n                out = func(*args, **kwargs)\n                diff = time() - t0\n                FunctionStats[func_name].push(diff)\n                return out\n            else:\n                return func(*args, **kwargs)\n\n        return func_wrapper\n\n    return time_func_inner\n\n\n@time_func(print_rec=50, measure_runtime=True)\ndef do_nothing():\n    pass\n\n\ndef iteration():\n    for _ in range(10000):\n        do_nothing()\n\n\nif __name__ == ""__main__"":\n    iteration()\n'"
torch_points3d/utils/transform_utils.py,0,"b'import numpy as np\n\n\nclass SamplingStrategy(object):\n\n    STRATEGIES = [""random"", ""freq_class_based""]\n    CLASS_WEIGHT_METHODS = [""sqrt""]\n\n    def __init__(self, strategy=""random"", class_weight_method=""sqrt""):\n\n        if strategy.lower() in self.STRATEGIES:\n            self._strategy = strategy.lower()\n\n        if class_weight_method.lower() in self.CLASS_WEIGHT_METHODS:\n            self._class_weight_method = class_weight_method.lower()\n\n    def __call__(self, data):\n\n        if self._strategy == ""random"":\n            random_center = np.random.randint(0, len(data.pos))\n\n        elif self._strategy == ""freq_class_based"":\n            labels = np.asarray(data.y)\n            uni, uni_counts = np.unique(np.asarray(data.y), return_counts=True)\n            uni_counts = uni_counts.mean() / uni_counts\n            if self._class_weight_method == ""sqrt"":\n                uni_counts = np.sqrt(uni_counts)\n            uni_counts /= np.sum(uni_counts)\n            chosen_label = np.random.choice(uni, p=uni_counts)\n            random_center = np.random.choice(np.argwhere(labels == chosen_label).flatten())\n        else:\n            raise NotImplementedError\n\n        return random_center\n\n    def __repr__(self):\n        return ""{}(strategy={}, class_weight_method={})"".format(\n            self.__class__.__name__, self._strategy, self._class_weight_method\n        )\n'"
torch_points3d/visualization/__init__.py,0,b'from .visualizer import *\nfrom .experiment_manager import ExperimentManager\n'
torch_points3d/visualization/experiment_manager.py,1,"b'import os\nfrom glob import glob\nfrom collections import defaultdict\nimport torch\nfrom plyfile import PlyData, PlyElement\nfrom numpy.lib import recfunctions as rfn\nfrom torch_points3d.utils.colors import COLORS\nimport numpy as np\n\n\ndef colored_print(color, msg):\n    print(color + msg + COLORS.END_NO_TOKEN)\n\n\nclass ExperimentFolder:\n\n    POS_KEYS = [""x"", ""y"", ""z""]\n\n    def __init__(self, run_path):\n        self._run_path = run_path\n        self._model_name = None\n        self._stats = None\n        self._find_files()\n\n    def _find_files(self):\n        self._files = os.listdir(self._run_path)\n\n    def __repr__(self):\n        return self._run_path.split(""outputs"")[1]\n\n    @property\n    def model_name(self):\n        return self._model_name\n\n    @property\n    def epochs(self):\n        return os.listdir(self._viz_path)\n\n    def get_splits(self, epoch):\n        return os.listdir(os.path.join(self._viz_path, str(epoch)))\n\n    def get_files(self, epoch, split):\n        return os.listdir(os.path.join(self._viz_path, str(epoch), split))\n\n    def load_ply(self, epoch, split, file):\n        self._data_name = ""data_{}_{}_{}"".format(epoch, split, file)\n        if not hasattr(self, self._data_name):\n            path_to_ply = os.path.join(self._viz_path, str(epoch), split, file)\n            if os.path.exists(path_to_ply):\n                plydata = PlyData.read(path_to_ply)\n                arr = np.asarray([e.data for e in plydata.elements])\n                names = list(arr.dtype.names)\n                pos_indices = [names.index(n) for n in self.POS_KEYS]\n                non_pos_indices = {n: names.index(n) for n in names if n not in self.POS_KEYS}\n                arr_ = rfn.structured_to_unstructured(arr).squeeze()\n                xyz = arr_[:, pos_indices]\n                data = {""xyz"": xyz, ""columns"": non_pos_indices.keys(), ""name"": self._data_name}\n                for n, i in non_pos_indices.items():\n                    data[n] = arr_[:, i]\n                setattr(self, self._data_name, data)\n            else:\n                print(""The file doesn\' t exist: Wierd !"")\n        else:\n            return getattr(self, self._data_name)\n\n    @property\n    def current_pointcloud(self):\n        return getattr(self, self._data_name)\n\n    @property\n    def contains_viz(self):\n        if not hasattr(self, ""_contains_viz""):\n            for f in self._files:\n                if ""viz"" in f:\n                    self._viz_path = os.path.join(self._run_path, ""viz"")\n                    vizs = os.listdir(self._viz_path)\n                    self._contains_viz = len(vizs) > 0\n                    return self._contains_viz\n            self._contains_viz = False\n            return self._contains_viz\n        else:\n            return self._contains_viz\n\n    @property\n    def contains_trained_model(self):\n        if not hasattr(self, ""_contains_trained_model""):\n            for f in self._files:\n                if "".pt"" in f:\n                    self._contains_trained_model = True\n                    self._model_name = f\n                    return self._contains_trained_model\n            self._contains_trained_model = False\n            return self._contains_trained_model\n        else:\n            return self._contains_trained_model\n\n    def extract_stats(self):\n        path_to_checkpoint = os.path.join(self._run_path, self.model_name)\n        stats = torch.load(path_to_checkpoint)[""stats""]\n        self._stats = stats\n        num_epoch = len(stats[""train""])\n        stats_dict = defaultdict(dict)\n        for split_name in stats.keys():\n            if len(stats[split_name]) > 0:\n                latest_epoch = stats[split_name][-1]\n                for metric_name in latest_epoch.keys():\n                    if ""best"" in metric_name:\n                        stats_dict[metric_name][split_name] = latest_epoch[metric_name]\n        return num_epoch, stats_dict\n\n\nclass ExperimentManager(object):\n    def __init__(self, experiments_root):\n        self._experiments_root = experiments_root\n        self._collect_experiments()\n\n    def _collect_experiments(self):\n        self._experiment_with_models = defaultdict(list)\n        run_paths = glob(os.path.join(self._experiments_root, ""outputs"", ""*"", ""*""))\n        for run_path in run_paths:\n            experiment = ExperimentFolder(run_path)\n            if experiment.contains_trained_model:\n                self._experiment_with_models[experiment.model_name].append(experiment)\n\n        self._find_experiments_with_viz()\n\n    def _find_experiments_with_viz(self):\n        if not hasattr(self, ""_experiment_with_viz""):\n            self._experiment_with_viz = defaultdict(list)\n            for model_name in self._experiment_with_models.keys():\n                for experiment in self._experiment_with_models[model_name]:\n                    if experiment.contains_viz:\n                        self._experiment_with_viz[experiment.model_name].append(experiment)\n\n    @property\n    def model_name_wviz(self):\n        keys = list(self._experiment_with_viz.keys())\n        return [k.replace("".pt"", """") for k in keys]\n\n    @property\n    def current_pointcloud(self):\n        return self._current_experiment.current_pointcloud\n\n    def load_ply_file(self, file):\n        if hasattr(self, ""_current_split""):\n            self._current_file = file\n            self._current_experiment.load_ply(self._current_epoch, self._current_split, self._current_file)\n        else:\n            return []\n\n    def from_split_to_file(self, split_name):\n        if hasattr(self, ""_current_epoch""):\n            self._current_split = split_name\n            return self._current_experiment.get_files(self._current_epoch, self._current_split)\n        else:\n            return []\n\n    def from_epoch_to_split(self, epoch):\n        if hasattr(self, ""_current_experiment""):\n            self._current_epoch = epoch\n            return self._current_experiment.get_splits(self._current_epoch)\n        else:\n            return []\n\n    def from_paths_to_epoch(self, run_path):\n        for exp in self._current_exps:\n            if str(run_path) == str(exp.__repr__()):\n                self._current_experiment = exp\n        return sorted(self._current_experiment.epochs)\n\n    def get_model_wviz_paths(self, model_path):\n        model_name = model_path + "".pt""\n        self._current_exps = self._experiment_with_viz[model_name]\n        return self._current_exps\n\n    def display_stats(self):\n        print("""")\n        for model_name in self._experiment_with_models.keys():\n            colored_print(COLORS.Green, str(model_name))\n            for experiment in self._experiment_with_models[model_name]:\n                print(experiment)\n                num_epoch, stats = experiment.extract_stats()\n                colored_print(COLORS.Red, ""Epoch: {}"".format(num_epoch))\n                for metric_name in stats:\n                    sentence = """"\n                    for split_name in stats[metric_name].keys():\n                        sentence += ""{}: {}, "".format(split_name, stats[metric_name][split_name])\n                    metric_sentence = metric_name + ""({})"".format(sentence[:-2])\n                    colored_print(COLORS.BBlue, metric_sentence)\n                print("""")\n            print("""")\n'"
torch_points3d/visualization/visualizer.py,3,"b'import os\nimport torch\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\nclass Visualizer(object):\n    """"""Initialize the Visualizer class.\n    Parameters:\n        viz_conf (OmegaConf Dictionnary) -- stores all config for the visualizer\n        num_batches (dict) -- This dictionnary maps stage_name to #batches\n        batch_size (int) -- Current batch size usef\n        save_dir (str) -- The path used by hydra to store the experiment\n\n    This class is responsible to save visual into .ply format\n    The configuration looks like that:\n        visualization:\n            activate: False # Wheter to activate the visualizer\n            format: ""pointcloud"" # image will come later\n            num_samples_per_epoch: 2 # If negative, it will save all elements\n            deterministic: True # False -> Randomly sample elements from epoch to epoch\n            saved_keys: # Mapping from Data Object to structured numpy\n                pos: [[\'x\', \'float\'], [\'y\', \'float\'], [\'z\', \'float\']]\n                y: [[\'l\', \'float\']]\n                pred: [[\'p\', \'float\']]\n            indices: # List of indices to be saved (support ""train"", ""test"", ""val"")\n                train: [0, 3]\n    """"""\n\n    def __init__(self, viz_conf, num_batches, batch_size, save_dir):\n        # From configuration and dataset\n        for stage_name, stage_num_sample in num_batches.items():\n            setattr(self, ""{}_num_batches"".format(stage_name), stage_num_sample)\n        self._batch_size = batch_size\n        self._activate = viz_conf.activate\n        self._format = viz_conf.format\n        self._num_samples_per_epoch = int(viz_conf.num_samples_per_epoch)\n        self._deterministic = viz_conf.deterministic\n\n        self._saved_keys = viz_conf.saved_keys\n\n        # Internal state\n        self._stage = None\n        self._current_epoch = None\n\n        # Current experiment path\n        self._save_dir = save_dir\n        self._viz_path = os.path.join(self._save_dir, ""viz"")\n        if not os.path.exists(self._viz_path):\n            os.makedirs(self._viz_path)\n\n        self._indices = {}\n        self._contains_indices = False\n\n        try:\n            indices = getattr(viz_conf, ""indices"", None)\n        except:\n            indices = None\n\n        if indices:\n            for split in [""train"", ""test"", ""val""]:\n                if hasattr(indices, split):\n                    indices = getattr(indices, split)\n                    self._indices[split] = np.asarray(indices)\n                    self._contains_indices = True\n\n    def get_indices(self, stage):\n        """"""This function is responsible to calculate the indices to be saved""""""\n        if self._contains_indices:\n            return\n        stage_num_batches = getattr(self, ""{}_num_batches"".format(stage))\n        total_items = (stage_num_batches - 1) * self._batch_size\n        if stage_num_batches > 0:\n            if self._num_samples_per_epoch < 0:  # All elements should be saved.\n                if stage_num_batches > 0:\n                    self._indices[stage] = np.arange(total_items)\n                else:\n                    self._indices[stage] = None\n            else:\n                if self._deterministic:\n                    if stage not in self._indices:\n                        if self._num_samples_per_epoch > total_items:\n                            log.warn(""Number of samples to save is higher than the number of available elements"")\n                        self._indices[stage] = np.random.permutation(total_items)[: self._num_samples_per_epoch]\n                else:\n                    if self._num_samples_per_epoch > total_items:\n                        log.warn(""Number of samples to save is higher than the number of available elements"")\n                    self._indices[stage] = np.random.permutation(total_items)[: self._num_samples_per_epoch]\n\n    @property\n    def is_active(self):\n        return self._activate\n\n    def reset(self, epoch, stage):\n        """"""This function is responsible to restore the visualizer\n            to start a new epoch on a new stage\n        """"""\n        self._current_epoch = epoch\n        self._seen_batch = 0\n        self._stage = stage\n        if self._activate:\n            self.get_indices(stage)\n\n    def _extract_from_PYG(self, item, pos_idx):\n        num_samples = item.batch.shape[0]\n        batch_mask = item.batch == pos_idx\n        out_data = {}\n        for k in item.keys:\n            if torch.is_tensor(item[k]) and k in self._saved_keys.keys():\n                if item[k].shape[0] == num_samples:\n                    out_data[k] = item[k][batch_mask]\n        return out_data\n\n    def _extract_from_dense(self, item, pos_idx):\n        assert (\n            item.y.shape[0] == item.pos.shape[0]\n        ), ""y and pos should have the same number of samples. Something is probably wrong with your data to visualise""\n        num_samples = item.y.shape[0]\n        out_data = {}\n        for k in item.keys:\n            if torch.is_tensor(item[k]) and k in self._saved_keys.keys():\n                if item[k].shape[0] == num_samples:\n                    out_data[k] = item[k][pos_idx]\n        return out_data\n\n    def _dict_to_structured_npy(self, item):\n        item.keys()\n        out = []\n        dtypes = []\n        for k, v in item.items():\n            v_npy = v.detach().cpu().numpy()\n            if len(v_npy.shape) == 1:\n                v_npy = v_npy[..., np.newaxis]\n            for dtype in self._saved_keys[k]:\n                dtypes.append(dtype)\n            out.append(v_npy)\n\n        out = np.concatenate(out, axis=-1)\n        dtypes = np.dtype([tuple(d) for d in dtypes])\n        return np.asarray([tuple(o) for o in out], dtype=dtypes)\n\n    def save_visuals(self, visuals):\n        """"""This function is responsible to save the data into .ply objects\n            Parameters:\n                visuals (Dict[Data(pos=torch.Tensor, ...)]) -- Contains a dictionnary of tensors\n            Make sure the saved_keys  within the config maps to the Data attributes.\n        """"""\n        if self._stage in self._indices:\n            batch_indices = self._indices[self._stage] // self._batch_size\n            pos_indices = self._indices[self._stage] % self._batch_size\n            for idx in np.argwhere(self._seen_batch == batch_indices).flatten():\n                pos_idx = pos_indices[idx]\n                for visual_name, item in visuals.items():\n                    if hasattr(item, ""batch""):  # The PYG dataloader has been used\n                        out_item = self._extract_from_PYG(item, pos_idx)\n                    else:\n                        out_item = self._extract_from_dense(item, pos_idx)\n                    out_item = self._dict_to_structured_npy(out_item)\n\n                    dir_path = os.path.join(self._viz_path, str(self._current_epoch), self._stage)\n                    if not os.path.exists(dir_path):\n                        os.makedirs(dir_path)\n\n                    filename = ""{}_{}.ply"".format(self._seen_batch, pos_idx)\n                    path_out = os.path.join(dir_path, filename)\n                    el = PlyElement.describe(out_item, visual_name)\n                    PlyData([el], byte_order="">"").write(path_out)\n            self._seen_batch += 1\n'"
torch_points3d/core/base_conv/__init__.py,0,b'from .base_conv import *\n'
torch_points3d/core/base_conv/base_conv.py,0,"b'from abc import ABC\n\nfrom torch_points3d.core.common_modules.base_modules import BaseModule\n\n\nclass BaseConvolution(ABC, BaseModule):\n    def __init__(self, sampler, neighbour_finder, *args, **kwargs):\n        BaseModule.__init__(self)\n        self.sampler = sampler\n        self.neighbour_finder = neighbour_finder\n'"
torch_points3d/core/base_conv/dense.py,9,"b'import numpy as np\nimport torch\nfrom torch.nn import (\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nfrom torch_geometric.nn import (\n    knn_interpolate,\n    fps,\n    radius,\n    global_max_pool,\n    global_mean_pool,\n    knn,\n)\nfrom torch_geometric.data import Data\nimport torch_points_kernels as tp\n\nfrom torch_points3d.core.spatial_ops import BaseMSNeighbourFinder\nfrom torch_points3d.core.base_conv import BaseConvolution\nfrom torch_points3d.core.common_modules.dense_modules import MLP2D\n\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.utils.model_building_utils.activation_resolver import get_activation\n\n#################### THOSE MODULES IMPLEMENTS THE BASE DENSE CONV API ############################\n\n\nclass BaseDenseConvolutionDown(BaseConvolution):\n    """""" Multiscale convolution down (also supports single scale). Convolution kernel is shared accross the scales\n\n        Arguments:\n            sampler  -- Strategy for sampling the input clouds\n            neighbour_finder -- Multiscale strategy for finding neighbours\n    """"""\n\n    CONV_TYPE = ConvolutionFormat.DENSE.value\n\n    def __init__(self, sampler, neighbour_finder: BaseMSNeighbourFinder, *args, **kwargs):\n        super(BaseDenseConvolutionDown, self).__init__(sampler, neighbour_finder, *args, **kwargs)\n        self._index = kwargs.get(""index"", None)\n        self._save_sampling_id = kwargs.get(""save_sampling_id"", None)\n\n    def conv(self, x, pos, new_pos, radius_idx, scale_idx):\n        """""" Implements a Dense convolution where radius_idx represents\n        the indexes of the points in x and pos to be agragated into the new feature\n        for each point in new_pos\n\n        Arguments:\n            x -- Previous features [B, C, N]\n            pos -- Previous positions [B, N, 3]\n            new_pos  -- Sampled positions [B, npoints, 3]\n            radius_idx -- Indexes to group [B, npoints, nsample]\n            scale_idx -- Scale index in multiscale convolutional layers\n        """"""\n        raise NotImplementedError\n\n    def forward(self, data, sample_idx=None, **kwargs):\n        """"""\n        Parameters\n        ----------\n        data: Data\n            x -- Previous features [B, C, N]\n            pos -- Previous positions [B, N, 3]\n        sample_idx: Optional[torch.Tensor]\n            can be used to shortcut the sampler [B,K]\n        """"""\n        x, pos = data.x, data.pos\n        if sample_idx:\n            idx = sample_idx\n        else:\n            idx = self.sampler(pos)\n        idx = idx.unsqueeze(-1).repeat(1, 1, pos.shape[-1]).long()\n        new_pos = pos.gather(1, idx)\n\n        ms_x = []\n        for scale_idx in range(self.neighbour_finder.num_scales):\n            radius_idx = self.neighbour_finder(pos, new_pos, scale_idx=scale_idx)\n            ms_x.append(self.conv(x, pos, new_pos, radius_idx, scale_idx))\n        new_x = torch.cat(ms_x, 1)\n\n        new_data = Data(pos=new_pos, x=new_x)\n        if self._save_sampling_id:\n            setattr(new_data, ""sampling_id_{}"".format(self._index), idx[:, :, 0])\n        return new_data\n\n\nclass BaseDenseConvolutionUp(BaseConvolution):\n\n    CONV_TYPE = ConvolutionFormat.DENSE.value\n\n    def __init__(self, neighbour_finder, *args, **kwargs):\n        super(BaseDenseConvolutionUp, self).__init__(None, neighbour_finder, *args, **kwargs)\n        self._index = kwargs.get(""index"", None)\n        self._skip = kwargs.get(""skip"", True)\n\n    def conv(self, pos, pos_skip, x):\n        raise NotImplementedError\n\n    def forward(self, data, **kwargs):\n        """""" Propagates features from one layer to the next.\n        data contains information from the down convs in data_skip\n\n        Arguments:\n            data -- (data, data_skip)\n        """"""\n        data, data_skip = data\n        pos, x = data.pos, data.x\n        pos_skip, x_skip = data_skip.pos, data_skip.x\n\n        new_features = self.conv(pos, pos_skip, x)\n\n        if x_skip is not None:\n            new_features = torch.cat([new_features, x_skip], dim=1)  # (B, C2 + C1, n)\n\n        new_features = new_features.unsqueeze(-1)\n\n        if hasattr(self, ""nn""):\n            new_features = self.nn(new_features)\n\n        return Data(x=new_features.squeeze(-1), pos=pos_skip)\n\n\nclass DenseFPModule(BaseDenseConvolutionUp):\n    def __init__(self, up_conv_nn, bn=True, bias=False, activation=torch.nn.LeakyReLU(negative_slope=0.01), **kwargs):\n        super(DenseFPModule, self).__init__(None, **kwargs)\n\n        self.nn = MLP2D(up_conv_nn, bn=bn, activation=activation, bias=False)\n\n    def conv(self, pos, pos_skip, x):\n        assert pos_skip.shape[2] == 3\n\n        if pos is not None:\n            dist, idx = tp.three_nn(pos_skip, pos)\n            dist_recip = 1.0 / (dist + 1e-8)\n            norm = torch.sum(dist_recip, dim=2, keepdim=True)\n            weight = dist_recip / norm\n            interpolated_feats = tp.three_interpolate(x, idx, weight)\n        else:\n            interpolated_feats = x.expand(*(x.size()[0:2] + (pos_skip.size(1),)))\n\n        return interpolated_feats\n\n    def __repr__(self):\n        return ""{}: {} ({})"".format(self.__class__.__name__, self.nb_params, self.nn)\n\n\nclass GlobalDenseBaseModule(torch.nn.Module):\n    def __init__(self, nn, aggr=""max"", bn=True, activation=torch.nn.LeakyReLU(negative_slope=0.01), **kwargs):\n        super(GlobalDenseBaseModule, self).__init__()\n        self.nn = MLP2D(nn, bn=bn, activation=activation, bias=False)\n        if aggr.lower() not in [""mean"", ""max""]:\n            raise Exception(""The aggregation provided is unrecognized {}"".format(aggr))\n        self._aggr = aggr.lower()\n\n    @property\n    def nb_params(self):\n        """"""[This property is used to return the number of trainable parameters for a given layer]\n        It is useful for debugging and reproducibility.\n        Returns:\n            [type] -- [description]\n        """"""\n        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n        self._nb_params = sum([np.prod(p.size()) for p in model_parameters])\n        return self._nb_params\n\n    def forward(self, data, **kwargs):\n        x, pos = data.x, data.pos\n        pos_flipped = pos.transpose(1, 2).contiguous()\n\n        x = self.nn(torch.cat([x, pos_flipped], dim=1).unsqueeze(-1))\n\n        if self._aggr == ""max"":\n            x = x.squeeze(-1).max(-1)[0]\n        elif self._aggr == ""mean"":\n            x = x.squeeze(-1).mean(-1)\n        else:\n            raise NotImplementedError(""The following aggregation {} is not recognized"".format(self._aggr))\n\n        pos = None  # pos.mean(1).unsqueeze(1)\n        x = x.unsqueeze(-1)\n        return Data(x=x, pos=pos)\n\n    def __repr__(self):\n        return ""{}: {} (aggr={}, {})"".format(self.__class__.__name__, self.nb_params, self._aggr, self.nn)\n'"
torch_points3d/core/base_conv/message_passing.py,11,"b'from abc import abstractmethod\nfrom typing import *\nimport torch\nfrom torch.nn import (\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nfrom torch_geometric.nn import (\n    knn_interpolate,\n    fps,\n    radius,\n    global_max_pool,\n    global_mean_pool,\n    knn,\n)\nfrom torch_geometric.data import Batch\nimport torch_points_kernels as tp\n\nfrom torch_points3d.core.base_conv.base_conv import *\nfrom torch_points3d.core.common_modules import *\nfrom torch_points3d.core.spatial_ops import *\n\n\ndef copy_from_to(data, batch):\n    for key in data.keys:\n        if key not in batch.keys:\n            setattr(batch, key, getattr(data, key, None))\n\n\n#################### THOSE MODULES IMPLEMENTS THE BASE MESSAGE_PASSING CONV API ############################\n\n\nclass BaseConvolutionDown(BaseConvolution):\n    def __init__(self, sampler, neighbour_finder, *args, **kwargs):\n        super(BaseConvolutionDown, self).__init__(sampler, neighbour_finder, *args, **kwargs)\n\n        self._index = kwargs.get(""index"", None)\n\n    def conv(self, x, pos, edge_index, batch):\n        raise NotImplementedError\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x, pos, batch = data.x, data.pos, data.batch\n        idx = self.sampler(pos, batch)\n        row, col = self.neighbour_finder(pos, pos[idx], batch_x=batch, batch_y=batch[idx])\n        edge_index = torch.stack([col, row], dim=0)\n        batch_obj.idx = idx\n        batch_obj.edge_index = edge_index\n\n        batch_obj.x = self.conv(x, (pos[idx], pos), edge_index, batch)\n\n        batch_obj.pos = pos[idx]\n        batch_obj.batch = batch[idx]\n        copy_from_to(data, batch_obj)\n        return batch_obj\n\n\nclass BaseMSConvolutionDown(BaseConvolution):\n    """""" Multiscale convolution down (also supports single scale). Convolution kernel is shared accross the scales\n\n        Arguments:\n            sampler  -- Strategy for sampling the input clouds\n            neighbour_finder -- Multiscale strategy for finding neighbours\n    """"""\n\n    def __init__(self, sampler, neighbour_finder: BaseMSNeighbourFinder, *args, **kwargs):\n        super(BaseMSConvolutionDown, self).__init__(sampler, neighbour_finder, *args, **kwargs)\n\n        self._index = kwargs.get(""index"", None)\n\n    def conv(self, x, pos, edge_index, batch):\n        raise NotImplementedError\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x, pos, batch = data.x, data.pos, data.batch\n        idx = self.sampler(pos, batch)\n        batch_obj.idx = idx\n\n        ms_x = []\n        for scale_idx in range(self.neighbour_finder.num_scales):\n            row, col = self.neighbour_finder(pos, pos[idx], batch_x=batch, batch_y=batch[idx], scale_idx=scale_idx,)\n            edge_index = torch.stack([col, row], dim=0)\n\n            ms_x.append(self.conv(x, (pos, pos[idx]), edge_index, batch))\n\n        batch_obj.x = torch.cat(ms_x, -1)\n        batch_obj.pos = pos[idx]\n        batch_obj.batch = batch[idx]\n        copy_from_to(data, batch_obj)\n        return batch_obj\n\n\nclass BaseConvolutionUp(BaseConvolution):\n    def __init__(self, neighbour_finder, *args, **kwargs):\n        super(BaseConvolutionUp, self).__init__(None, neighbour_finder, *args, **kwargs)\n\n        self._index = kwargs.get(""index"", None)\n        self._skip = kwargs.get(""skip"", True)\n\n    def conv(self, x, pos, pos_skip, batch, batch_skip, edge_index):\n        raise NotImplementedError\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        data, data_skip = data\n        x, pos, batch = data.x, data.pos, data.batch\n        x_skip, pos_skip, batch_skip = data_skip.x, data_skip.pos, data_skip.batch\n\n        if self.neighbour_finder is not None:\n            row, col = self.neighbour_finder(pos, pos_skip, batch, batch_skip)\n            edge_index = torch.stack([col, row], dim=0)\n        else:\n            edge_index = None\n\n        x = self.conv(x, pos, pos_skip, batch, batch_skip, edge_index)\n\n        if x_skip is not None and self._skip:\n            x = torch.cat([x, x_skip], dim=1)\n\n        if hasattr(self, ""nn""):\n            batch_obj.x = self.nn(x)\n        else:\n            batch_obj.x = x\n        copy_from_to(data_skip, batch_obj)\n        return batch_obj\n\n\nclass GlobalBaseModule(torch.nn.Module):\n    def __init__(self, nn, aggr=""max"", *args, **kwargs):\n        super(GlobalBaseModule, self).__init__()\n        self.nn = MLP(nn)\n        self.pool = global_max_pool if aggr == ""max"" else global_mean_pool\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x, pos, batch = data.x, data.pos, data.batch\n        x = self.nn(torch.cat([x, pos], dim=1))\n        x = self.pool(x, batch)\n        batch_obj.x = x\n        batch_obj.pos = pos.new_zeros((x.size(0), 3))\n        batch_obj.batch = torch.arange(x.size(0), device=batch.device)\n        copy_from_to(data, batch_obj)\n        return batch_obj\n\n\n#################### COMMON MODULE ########################\n\n\nclass FPModule(BaseConvolutionUp):\n    """""" Upsampling module from PointNet++\n\n    Arguments:\n        k [int] -- number of nearest neighboors used for the interpolation\n        up_conv_nn [List[int]] -- list of feature sizes for the uplconv mlp\n    """"""\n\n    def __init__(self, up_k, up_conv_nn, *args, **kwargs):\n        super(FPModule, self).__init__(None)\n\n        self.k = up_k\n        bn_momentum = kwargs.get(""bn_momentum"", 0.1)\n        self.nn = MLP(up_conv_nn, bn_momentum=bn_momentum, bias=False)\n\n    def conv(self, x, pos, pos_skip, batch, batch_skip, *args):\n        return knn_interpolate(x, pos, pos_skip, batch, batch_skip, k=self.k)\n\n    def extra_repr(self):\n        return ""Nb parameters: %i"" % self.nb_params\n\n\n########################## BASE RESIDUAL DOWN #####################\n\n\nclass BaseResnetBlockDown(BaseConvolutionDown):\n    def __init__(self, sampler, neighbour_finder, *args, **kwargs):\n        super(BaseResnetBlockDown, self).__init__(sampler, neighbour_finder, *args, **kwargs)\n\n        in_features, out_features, conv_features = kwargs.get(""down_conv_nn"", None)\n\n        self.in_features = in_features\n        self.out_features = out_features\n        self.conv_features = conv_features\n\n        self.features_downsample_nn = MLP([self.in_features, self.conv_features])\n\n        self.features_upsample_nn = MLP([self.conv_features, self.out_features])\n        self.shortcut_feature_resize_nn = MLP([self.in_features, self.out_features])\n\n    def convs(self, x, pos, edge_index):\n        raise NotImplementedError\n\n    def conv(self, x, pos, edge_index):\n        shortcut = x\n        x = self.features_downsample_nn(x)\n        x, pos, edge_index, idx = self.convs(x, pos, edge_index)\n        x = self.features_upsample_nn(x)\n        if idx is not None:\n            shortcut = shortcut[idx]\n        shortcut = self.shortcut_feature_resize_nn(shortcut)\n        x = shortcut + x\n        return x\n\n\nclass BaseResnetBlock(torch.nn.Module):\n    def __init__(self, indim, outdim, convdim):\n        """"""\n            indim: size of x at the input\n            outdim: desired size of x at the output\n            convdim: size of x following convolution\n        """"""\n        torch.nn.Module.__init__(self)\n\n        self.indim = indim\n        self.outdim = outdim\n        self.convdim = convdim\n\n        self.features_downsample_nn = MLP([self.indim, self.outdim // 4])\n        self.features_upsample_nn = MLP([self.convdim, self.outdim])\n\n        self.shortcut_feature_resize_nn = MLP([self.indim, self.outdim])\n\n        self.activation = ReLU()\n\n    @property\n    @abstractmethod\n    def convs(self):\n        pass\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x = data.x  # (N, indim)\n        shortcut = x  # (N, indim)\n        x = self.features_downsample_nn(x)  # (N, outdim//4)\n        # if this is an identity resnet block, idx will be None\n        data = self.convs(data)  # (N\', convdim)\n        x = data.x\n        idx = data.idx\n        x = self.features_upsample_nn(x)  # (N\', outdim)\n        if idx is not None:\n            shortcut = shortcut[idx]  # (N\', indim)\n        shortcut = self.shortcut_feature_resize_nn(shortcut)  # (N\', outdim)\n        x = shortcut + x\n        batch_obj.x = x\n        batch_obj.pos = data.pos\n        batch_obj.batch = data.batch\n        copy_from_to(data, batch_obj)\n        return batch_obj\n'"
torch_points3d/core/base_conv/partial_dense.py,10,"b'from typing import *\nimport torch\nfrom torch.nn import (\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nfrom torch_geometric.nn import (\n    knn_interpolate,\n    fps,\n    radius,\n    global_max_pool,\n    global_mean_pool,\n    knn,\n)\nfrom torch_geometric.data import Batch\nimport torch_points_kernels as tp\n\nfrom torch_points3d.core.spatial_ops import *\nfrom .base_conv import BaseConvolution\nfrom torch_points3d.core.common_modules.base_modules import BaseModule\nfrom torch_points3d.core.common_modules import MLP\n\n\n#################### THOSE MODULES IMPLEMENTS THE BASE PARTIAL_DENSE CONV API ############################\n\n\ndef copy_from_to(data, batch):\n    for key in data.keys:\n        if key not in batch.keys:\n            setattr(batch, key, getattr(data, key, None))\n\n\nclass BasePartialDenseConvolutionDown(BaseConvolution):\n\n    CONV_TYPE = ConvolutionFormat.PARTIAL_DENSE.value\n\n    def __init__(self, sampler, neighbour_finder, *args, **kwargs):\n        super(BasePartialDenseConvolutionDown, self).__init__(sampler, neighbour_finder, *args, **kwargs)\n\n        self._index = kwargs.get(""index"", None)\n\n    def conv(self, x, pos, x_neighbour, pos_centered_neighbour, idx_neighbour, idx_sampler):\n        """""" Generic down convolution for partial dense data\n\n        Arguments:\n            x [N, C] -- features\n            pos [N, 3] -- positions\n            x_neighbour [N, n_neighbours, C] -- features of the neighbours of each point in x\n            pos_centered_neighbour [N, n_neighbours, 3]  -- position of the neighbours of x_i centred on x_i\n            idx_neighbour [N, n_neighbours] -- indices of the neighbours of each point in pos\n            idx_sampler [n]  -- points to keep for the output\n\n        Raises:\n            NotImplementedError: [description]\n        """"""\n        raise NotImplementedError\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x, pos, batch = data.x, data.pos, data.batch\n        idx_sampler = self.sampler(pos=pos, x=x, batch=batch)\n\n        idx_neighbour = self.neighbour_finder(pos, pos, batch_x=batch, batch_y=batch)\n\n        shadow_x = torch.full((1,) + x.shape[1:], self.shadow_features_fill).to(x.device)\n        shadow_points = torch.full((1,) + pos.shape[1:], self.shadow_points_fill_).to(x.device)\n\n        x = torch.cat([x, shadow_x], dim=0)\n        pos = torch.cat([pos, shadow_points], dim=0)\n\n        x_neighbour = x[idx_neighbour]\n        pos_centered_neighbour = pos[idx_neighbour] - pos[:-1].unsqueeze(1)  # Centered the points, no shadow point\n\n        batch_obj.x = self.conv(x, pos, x_neighbour, pos_centered_neighbour, idx_neighbour, idx_sampler)\n\n        batch_obj.pos = pos[idx_sampler]\n        batch_obj.batch = batch[idx_sampler]\n        copy_from_to(data, batch_obj)\n        return batch_obj\n\n\nclass GlobalPartialDenseBaseModule(torch.nn.Module):\n    def __init__(self, nn, aggr=""max"", *args, **kwargs):\n        super(GlobalPartialDenseBaseModule, self).__init__()\n\n        self.nn = MLP(nn)\n        self.pool = global_max_pool if aggr == ""max"" else global_mean_pool\n\n    def forward(self, data, **kwargs):\n        batch_obj = Batch()\n        x, pos, batch = data.x, data.pos, data.batch\n        x = self.nn(torch.cat([x, pos], dim=1))\n        x = self.pool(x, batch)\n        batch_obj.x = x\n        batch_obj.pos = pos.new_zeros((x.size(0), 3))\n        batch_obj.batch = torch.arange(x.size(0), device=x.device)\n        copy_from_to(data, batch_obj)\n        return batch_obj\n\n\nclass FPModule_PD(BaseModule):\n    """""" Upsampling module from PointNet++\n    Arguments:\n        k [int] -- number of nearest neighboors used for the interpolation\n        up_conv_nn [List[int]] -- list of feature sizes for the uplconv mlp\n    """"""\n\n    def __init__(self, up_k, up_conv_nn, *args, **kwargs):\n        super(FPModule_PD, self).__init__()\n        self.upsample_op = KNNInterpolate(up_k)\n        bn_momentum = kwargs.get(""bn_momentum"", 0.1)\n        self.nn = MLP(up_conv_nn, bn_momentum=bn_momentum, bias=False)\n\n    def forward(self, data, precomputed=None, **kwargs):\n        data, data_skip = data\n        batch_out = data_skip.clone()\n        x_skip = data_skip.x\n\n        has_innermost = len(data.x) == data.batch.max() + 1\n\n        if precomputed and not has_innermost:\n            if not hasattr(data, ""up_idx""):\n                setattr(batch_out, ""up_idx"", 0)\n            else:\n                setattr(batch_out, ""up_idx"", data.up_idx)\n\n            pre_data = precomputed[batch_out.up_idx]\n            batch_out.up_idx = batch_out.up_idx + 1\n        else:\n            pre_data = None\n\n        if has_innermost:\n            x = torch.gather(data.x, 0, data_skip.batch.unsqueeze(-1).repeat((1, data.x.shape[-1])))\n        else:\n            x = self.upsample_op(data, data_skip, precomputed=pre_data)\n\n        if x_skip is not None:\n            x = torch.cat([x, x_skip], dim=1)\n\n        if hasattr(self, ""nn""):\n            batch_out.x = self.nn(x)\n        else:\n            batch_out.x = x\n        return batch_out\n'"
torch_points3d/core/common_modules/__init__.py,0,b'from .base_modules import *\nfrom .spatial_transform import *\n'
torch_points3d/core/common_modules/base_modules.py,9,"b'import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.nn.parameter import Parameter\n\n\nclass BaseModule(nn.Module):\n    """""" Base module class with some basic additions to the pytorch Module class\n    """"""\n\n    @property\n    def nb_params(self):\n        """"""This property is used to return the number of trainable parameters for a given layer\n        It is useful for debugging and reproducibility.\n        """"""\n        model_parameters = filter(lambda p: p.requires_grad, self.parameters())\n        self._nb_params = sum([np.prod(p.size()) for p in model_parameters])\n        return self._nb_params\n\n\ndef weight_variable(shape):\n    initial = torch.empty(shape, dtype=torch.float)\n    torch.nn.init.xavier_normal_(initial)\n    return initial\n\n\nclass Identity(BaseModule):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n    def forward(self, data):\n        return data\n\n\ndef MLP(channels, activation=nn.LeakyReLU(0.2), bn_momentum=0.1, bias=True):\n    return nn.Sequential(\n        *[\n            nn.Sequential(\n                nn.Linear(channels[i - 1], channels[i], bias=bias),\n                FastBatchNorm1d(channels[i], momentum=bn_momentum),\n                activation,\n            )\n            for i in range(1, len(channels))\n        ]\n    )\n\n\nclass UnaryConv(BaseModule):\n    def __init__(self, kernel_shape):\n        """"""\n        1x1 convolution on point cloud (we can even call it a mini pointnet)\n        """"""\n        super(UnaryConv, self).__init__()\n        self.weight = Parameter(weight_variable(kernel_shape))\n\n    def forward(self, features):\n        """"""\n        features(Torch Tensor): size N x d d is the size of inputs\n        """"""\n        return torch.matmul(features, self.weight)\n\n    def __repr__(self):\n        return ""UnaryConv {}"".format(self.weight.shape)\n\n\nclass MultiHeadClassifier(BaseModule):\n    """""" Allows segregated segmentation in case the category of an object is known. This is the case in ShapeNet\n    for example.\n\n        Arguments:\n            in_features -- size of the input channel\n            cat_to_seg {[type]} -- category to segment maps for example:\n                {\n                    \'Airplane\': [0,1,2],\n                    \'Table\': [3,4]\n                }\n\n        Keyword Arguments:\n            dropout_proba  (default: {0.5})\n            bn_momentum  -- batch norm momentum (default: {0.1})\n        """"""\n\n    def __init__(self, in_features, cat_to_seg, dropout_proba=0.5, bn_momentum=0.1):\n        super().__init__()\n        self._cat_to_seg = {}\n        self._num_categories = len(cat_to_seg)\n        self._max_seg_count = 0\n        self._max_seg = 0\n        self._shifts = torch.zeros((self._num_categories,), dtype=torch.long)\n        for i, seg in enumerate(cat_to_seg.values()):\n            self._max_seg_count = max(self._max_seg_count, len(seg))\n            self._max_seg = max(self._max_seg, max(seg))\n            self._shifts[i] = min(seg)\n            self._cat_to_seg[i] = seg\n\n        self.channel_rasing = MLP(\n            [in_features, self._num_categories * in_features], bn_momentum=bn_momentum, bias=False\n        )\n        if dropout_proba:\n            self.channel_rasing.add_module(""Dropout"", nn.Dropout(p=dropout_proba))\n\n        self.classifier = UnaryConv((self._num_categories, in_features, self._max_seg_count))\n        self._bias = Parameter(torch.zeros(self._max_seg_count,))\n\n    def forward(self, features, category_labels, **kwargs):\n        assert features.dim() == 2\n        self._shifts = self._shifts.to(features.device)\n        in_dim = features.shape[-1]\n        features = self.channel_rasing(features)\n        features = features.reshape((-1, self._num_categories, in_dim))\n        features = features.transpose(0, 1)  # [num_categories, num_points, in_dim]\n        features = self.classifier(features) + self._bias  # [num_categories, num_points, max_seg]\n        ind = category_labels.unsqueeze(-1).repeat(1, 1, features.shape[-1]).long()\n\n        logits = features.gather(0, ind).squeeze(0)\n        softmax = torch.nn.functional.log_softmax(logits, dim=-1)\n\n        output = torch.zeros(logits.shape[0], self._max_seg + 1).to(features.device)\n        cats_in_batch = torch.unique(category_labels)\n        for cat in cats_in_batch:\n            cat_mask = category_labels == cat\n            seg_indices = self._cat_to_seg[cat.item()]\n            probs = softmax[cat_mask, : len(seg_indices)]\n            output[cat_mask, seg_indices[0] : seg_indices[-1] + 1] = probs\n        return output\n\n\nclass FastBatchNorm1d(BaseModule):\n    def __init__(self, num_features, momentum=0.1):\n        super().__init__()\n        self.batch_norm = nn.BatchNorm1d(num_features, momentum=momentum)\n\n    def _forward_dense(self, x):\n        return self.batch_norm(x)\n\n    def _forward_sparse(self, x):\n        """""" Batch norm 1D is not optimised for 2D tensors. The first dimension is supposed to be\n        the batch and therefore not very large. So we introduce a custom version that leverages BatchNorm1D\n        in a more optimised way\n        """"""\n        x = x.unsqueeze(2)\n        x = x.transpose(0, 2)\n        x = self.batch_norm(x)\n        x = x.transpose(0, 2)\n        return x.squeeze()\n\n    def forward(self, x):\n        if x.dim() == 2:\n            return self._forward_sparse(x)\n        elif x.dim() == 3:\n            return self._forward_dense(x)\n        else:\n            raise ValueError(""Non supported number of dimensions {}"".format(x.dim()))\n\n\nclass Seq(nn.Sequential):\n    def __init__(self):\n        super().__init__()\n        self._num_modules = 0\n\n    def append(self, module):\n        self.add_module(str(self._num_modules), module)\n        self._num_modules += 1\n'"
torch_points3d/core/common_modules/dense_modules.py,1,"b'import torch.nn as nn\nfrom .base_modules import Seq\n\n\nclass Conv2D(Seq):\n    def __init__(self, in_channels, out_channels, bias=True, bn=True, activation=nn.LeakyReLU(negative_slope=0.01)):\n        super().__init__()\n        self.append(nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(1, 1), bias=bias))\n        if bn:\n            self.append(nn.BatchNorm2d(out_channels))\n        if activation:\n            self.append(activation)\n\n\nclass Conv1D(Seq):\n    def __init__(self, in_channels, out_channels, bias=True, bn=True, activation=nn.LeakyReLU(negative_slope=0.01)):\n        super().__init__()\n        self.append(nn.Conv1d(in_channels, out_channels, kernel_size=1, bias=bias))\n        if bn:\n            self.append(nn.BatchNorm1d(out_channels))\n        if activation:\n            self.append(activation)\n\n\nclass MLP2D(Seq):\n    def __init__(self, channels, bias=False, bn=True, activation=nn.LeakyReLU(negative_slope=0.01)):\n        super().__init__()\n        for i in range(len(channels) - 1):\n            self.append(Conv2D(channels[i], channels[i + 1], bn=bn, bias=bias, activation=activation))\n'"
torch_points3d/core/common_modules/spatial_transform.py,10,"b'import torch\nfrom torch.nn import Linear\n\n\nclass BaseLinearTransformSTNkD(torch.nn.Module):\n    """"""STN which learns a k-dimensional linear transformation\n\n    Arguments:\n        nn (torch.nn.Module) -- module which takes feat_x as input and regresses it to a global feature used to calculate the transform\n        nn_feat_size -- the size of the global feature\n        k -- the size of trans_x\n        batch_size -- the number of examples per batch\n    """"""\n\n    def __init__(self, nn, nn_feat_size, k=3, batch_size=1):\n        super().__init__()\n\n        self.nn = nn\n        self.k = k\n        self.batch_size = batch_size\n\n        # fully connected layer to regress the global feature to a k-d linear transform\n        # the transform is initialized to the identity\n        self.fc_layer = Linear(nn_feat_size, k * k)\n        torch.nn.init.constant_(self.fc_layer.weight, 0)\n        torch.nn.init.constant_(self.fc_layer.bias, 0)\n        self.identity = torch.eye(k).view(1, k * k).repeat(batch_size, 1)\n\n    def forward(self, feat_x, trans_x, batch):\n        """"""\n            Learns and applies a linear transformation to trans_x based on feat_x.\n            feat_x and trans_x may be the same or different.\n        """"""\n        global_feature = self.nn(feat_x, batch)\n        trans = self.fc_layer(global_feature)\n\n        # needed so that transform is initialized to identity\n        trans = trans + self.identity.to(feat_x.device)\n        trans = trans.view(-1, self.k, self.k)\n        self.trans = trans\n\n        # convert trans_x from (N, K) to (B, N, K) to do batched matrix multiplication\n        # batch_x = trans_x.view(self.batch_size, -1, trans_x.shape[1])\n        batch_x = trans_x.view(trans_x.shape[0], 1, trans_x.shape[1])\n        x_transformed = torch.bmm(batch_x, trans[batch])\n\n        return x_transformed.view(len(trans_x), trans_x.shape[1])\n\n    def get_orthogonal_regularization_loss(self):\n        loss = torch.mean(\n            torch.norm(\n                torch.bmm(self.trans, self.trans.transpose(2, 1))\n                - self.identity.to(self.trans.device).view(-1, self.k, self.k),\n                dim=(1, 2),\n            )\n        )\n\n        return loss\n'"
torch_points3d/core/data_transform/__init__.py,0,"b'import sys\n\nimport torch_geometric.transforms as T\nfrom .transforms import *\nfrom .grid_transform import *\nfrom .sparse_transforms import *\nfrom .inference_transforms import *\nfrom .feature_augment import *\nfrom .features import *\nfrom .filters import *\n\n_custom_transforms = sys.modules[__name__]\n_torch_geometric_transforms = sys.modules[""torch_geometric.transforms""]\n_intersection_names = set(_custom_transforms.__dict__) & set(_torch_geometric_transforms.__dict__)\n_intersection_names = set([module for module in _intersection_names if not module.startswith(""_"")])\nL_intersection_names = len(_intersection_names) > 0\n_intersection_cls = []\n\nfor transform_name in _intersection_names:\n    transform_cls = getattr(_custom_transforms, transform_name)\n    if not ""torch_geometric.transforms."" in str(transform_cls):\n        _intersection_cls.append(transform_cls)\nL_intersection_cls = len(_intersection_cls) > 0\n\nif L_intersection_names:\n    if L_intersection_cls:\n        raise Exception(\n            ""It seems that you are overiding a transform from pytorch gemetric, \\\n                this is forbiden, please rename your classes {} from {}"".format(\n                _intersection_names, _intersection_cls\n            )\n        )\n    else:\n        raise Exception(\n            ""It seems you are importing transforms {} from pytorch geometric within the current code base. \\\n             Please, remove them or add them within a class, function, etc."".format(\n                _intersection_names\n            )\n        )\n\n\ndef instantiate_transform(transform_option, attr=""transform""):\n    """""" Creates a transform from an OmegaConf dict such as\n    transform: GridSampling3D\n        params:\n            size: 0.01\n    """"""\n    tr_name = getattr(transform_option, attr, None)\n    try:\n        tr_params = transform_option.params\n    except KeyError:\n        tr_params = None\n    try:\n        lparams = transform_option.lparams\n    except KeyError:\n        lparams = None\n\n    cls = getattr(_custom_transforms, tr_name, None)\n    if not cls:\n        cls = getattr(_torch_geometric_transforms, tr_name, None)\n        if not cls:\n            raise ValueError(""Transform %s is nowhere to be found"" % tr_name)\n\n    if tr_params and lparams:\n        return cls(*lparams, **tr_params)\n\n    if tr_params:\n        return cls(**tr_params)\n\n    if lparams:\n        return cls(*lparams)\n\n    return cls()\n\n\ndef instantiate_transforms(transform_options):\n    """""" Creates a torch_geometric composite transform from an OmegaConf list such as\n    - transform: GridSampling3D\n        params:\n            size: 0.01\n    - transform: NormaliseScale\n    """"""\n    transforms = []\n    for transform in transform_options:\n        transforms.append(instantiate_transform(transform))\n    return T.Compose(transforms)\n\n\ndef instantiate_filters(filter_options):\n    filters = []\n    for filt in filter_options:\n        filters.append(instantiate_transform(filt, ""filter""))\n    return FCompose(filters)\n'"
torch_points3d/core/data_transform/feature_augment.py,5,"b'import random\nimport torch\n\n# Those Transformation are adapted from https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/transforms.py\n\n\nclass NormalizeRGB(object):\n    """"""Normalize rgb between 0 and 1\n\n    Parameters\n    ----------\n    normalize: bool: Whether to normalize the rgb attributes\n    """"""\n\n    def __init__(self, normalize=True):\n        self._normalize = normalize\n\n    def __call__(self, data):\n        assert hasattr(data, ""rgb"")\n        if not (data.rgb.max() <= 1 and data.rgb.min() >= 0):\n            data.rgb = data.rgb.float() / 255.0\n        return data\n\n    def __repr__(self):\n        return ""{}({})"".format(self.__class__.__name__, self._normalize)\n\n\nclass ChromaticTranslation(object):\n    """"""Add random color to the image, data must contain an rgb attribute between 0 and 1\n\n    Parameters\n    ----------\n    trans_range_ratio:\n        ratio of translation i.e. tramnslation = 2 * ratio * rand(-0.5, 0.5) (default: 1e-1)\n    """"""\n\n    def __init__(self, trans_range_ratio=1e-1):\n        self.trans_range_ratio = trans_range_ratio\n\n    def __call__(self, data):\n        assert hasattr(data, ""rgb"")\n        assert data.rgb.max() <= 1 and data.rgb.min() >= 0\n        if random.random() < 0.95:\n            tr = (torch.rand(1, 3) - 0.5) * 2 * self.trans_range_ratio\n            data.rgb = torch.clamp(tr + data.rgb, 0, 1)\n        return data\n\n    def __repr__(self):\n        return ""{}(trans_range_ratio={})"".format(self.__class__.__name__, self.trans_range_ratio)\n\n\nclass ChromaticAutoContrast(object):\n    """""" Rescale colors between 0 and 1 to enhance contrast\n\n    Parameters\n    ----------\n    randomize_blend_factor :\n        Blend factor is random\n    blend_factor:\n        Ratio of the original color that is kept\n    """"""\n\n    def __init__(self, randomize_blend_factor=True, blend_factor=0.5):\n        self.randomize_blend_factor = randomize_blend_factor\n        self.blend_factor = blend_factor\n\n    def __call__(self, data):\n        assert hasattr(data, ""rgb"")\n        assert data.rgb.max() <= 1 and data.rgb.min() >= 0\n        if random.random() < 0.2:\n            feats = data.rgb\n            lo = feats.min(0, keepdims=True)[0]\n            hi = feats.max(0, keepdims=True)[0]\n            assert hi.max() > 0, ""invalid color value. Color is supposed to be [0-255]""\n\n            scale = 1.0 / (hi - lo)\n\n            contrast_feats = (feats - lo) * scale\n\n            blend_factor = random.random() if self.randomize_blend_factor else self.blend_factor\n            data.rgb = (1 - blend_factor) * feats + blend_factor * contrast_feats\n        return data\n\n    def __repr__(self):\n        return ""{}(randomize_blend_factor={}, blend_factor={})"".format(\n            self.__class__.__name__, self.randomize_blend_factor, self.blend_factor\n        )\n\n\nclass ChromaticJitter:\n    """""" Jitter on the rgb attribute of data\n\n    Parameters\n    ----------\n    std :\n        standard deviation of the Jitter\n    """"""\n\n    def __init__(self, std=0.01):\n        self.std = std\n\n    def __call__(self, data):\n        assert hasattr(data, ""rgb"")\n        assert data.rgb.max() <= 1 and data.rgb.min() >= 0\n        if random.random() < 0.95:\n            noise = torch.randn(data.rgb.shape[0], 3)\n            noise *= self.std\n            data.rgb = torch.clamp(noise + data.rgb, 0, 1)\n        return data\n\n    def __repr__(self):\n        return ""{}(std={})"".format(self.__class__.__name__, self.std)\n\n\nclass DropFeature:\n    """""" Sets the given feature to 0 with a given probability\n\n    Parameters\n    ----------\n    drop_proba:\n        Probability that the feature gets dropped\n    feature_name:\n        Name of the feature to drop\n    """"""\n\n    def __init__(self, drop_proba=0.2, feature_name=""rgb""):\n        self._drop_proba = drop_proba\n        self._feature_name = feature_name\n\n    def __call__(self, data):\n        assert hasattr(data, self._feature_name)\n        if random.random() < self._drop_proba:\n            data[self._feature_name] = data[self._feature_name] * 0\n        return data\n\n    def __repr__(self):\n        return ""DropFeature: proba = {}, feature = {}"".format(self._drop_proba, self._feature_name)\n\n\nclass Jitter:\n    """"""\n    add a small gaussian noise to the feature.\n    Parameters\n    ----------\n    mu: float\n        mean of the gaussian noise\n    sigma: float\n        standard deviation of the gaussian noise\n    p: float\n        probability of noise\n    """"""\n\n    def __init__(self, mu=0, sigma=0.01, p=0.95):\n        self.mu = mu\n        self.sigma = sigma\n        self.p = p\n\n    def __call__(self, data):\n        if random.random() < self.p:\n            data.x += torch.randn_like(data.x) * self.sigma + self.mu\n        return data\n\n    def __repr__(self):\n        return ""Jitter(mu={}, sigma={})"".format(self.mu, self.sigma)\n'"
torch_points3d/core/data_transform/features.py,7,"b'from typing import List, Optional\nfrom tqdm.auto import tqdm as tq\nimport itertools\nimport numpy as np\nimport math\nimport re\nimport torch\nimport random\nfrom torch.nn import functional as F\nfrom sklearn.neighbors import NearestNeighbors, KDTree\nfrom functools import partial\n\nfrom torch_geometric.nn import fps, radius, knn, voxel_grid\nfrom torch_geometric.nn.pool.consecutive import consecutive_cluster\nfrom torch_geometric.nn.pool.pool import pool_pos, pool_batch\nfrom torch_scatter import scatter_add, scatter_mean\nfrom torch_geometric.data import Data, Batch\nfrom torch_points3d.datasets.multiscale_data import MultiScaleData\nfrom torch_points3d.utils.transform_utils import SamplingStrategy\nfrom torch_points3d.utils.config import is_list\nfrom torch_points3d.utils import is_iterable\nfrom torch_points3d.core.data_transform.transforms import euler_angles_to_rotation_matrix\n\nclass Random3AxisRotation(object):\n    """"""\n    Rotate pointcloud with random angles along x, y, z axis\n\n    The angles should be given `in degrees`.\n\n    Parameters\n    -----------\n    apply_rotation: bool:\n        Whether to apply the rotation\n    rot_x: float\n        Rotation angle in degrees on x axis\n    rot_y: float\n        Rotation anglei n degrees on y axis\n    rot_z: float\n        Rotation angle in degrees on z axis\n    """"""\n\n    def __init__(self, apply_rotation: bool = True, rot_x: float = None, rot_y: float = None, rot_z: float = None):\n        self._apply_rotation = apply_rotation\n        if apply_rotation:\n            if (rot_x is None) and (rot_y is None) and (rot_z is None):\n                raise Exception(""At least one rot_ should be defined"")\n\n        self._rot_x = np.abs(rot_x) if rot_x else 0\n        self._rot_y = np.abs(rot_y) if rot_y else 0\n        self._rot_z = np.abs(rot_z) if rot_z else 0\n\n        self._degree_angles = [self._rot_x, self._rot_y, self._rot_z]\n\n    def generate_random_rotation_matrix(self):\n        thetas = []\n        for axis_ind, deg_angle in enumerate(self._degree_angles):\n            if deg_angle > 0:\n                rand_deg_angle = random.random() * deg_angle\n                rand_radian_angle = float(rand_deg_angle * np.pi) / 180.0\n                thetas.append(torch.tensor(rand_radian_angle))\n            else:\n                thetas.append(torch.tensor(0.0))\n        return euler_angles_to_rotation_matrix(thetas)\n\n    def __call__(self, data):\n        if self._apply_rotation:\n            pos = data.pos\n            M = self.generate_random_rotation_matrix()\n            data.pos = pos @ M.T\n        return data\n\n    def __repr__(self):\n        return ""{}(apply_rotation={}, rot_x={}, rot_y={}, rot_z={})"".format(\n            self.__class__.__name__, self._apply_rotation, self._rot_x, self._rot_y, self._rot_z\n        )\n\n\nclass AddFeatsByKeys(object):\n    """"""This transform takes a list of attributes names and if allowed, add them to x\n\n    Example:\n\n        Before calling ""AddFeatsByKeys"", if data.x was empty\n\n        - transform: AddFeatsByKeys\n          params:\n              list_add_to_x: [False, True, True]\n              feat_names: [\'normal\', \'rgb\', ""elevation""]\n              input_nc_feats: [3, 3, 1]\n\n        After calling ""AddFeatsByKeys"", data.x contains ""rgb"" and ""elevation"". Its shape[-1] == 4 (rgb:3 + elevation:1)\n        If input_nc_feats was [4, 4, 1], it would raise an exception as rgb dimension is only 3.\n\n    Paremeters\n    ----------\n    list_add_to_x: List[bool]\n        For each boolean within list_add_to_x, control if the associated feature is going to be concatenated to x\n    feat_names: List[str]\n        The list of features within data to be added to x\n    input_nc_feats: List[int], optional\n        If provided, evaluate the dimension of the associated feature shape[-1] found using feat_names and this provided value. It allows to make sure feature dimension didn\'t change\n    stricts: List[bool], optional\n        Recommended to be set to list of True. If True, it will raise an Exception if feat isn\'t found or dimension doesn t match.\n    delete_feats: List[bool], optional\n        Wether we want to delete the feature from the data object. List length must match teh number of features added.\n    """"""\n\n    def __init__(\n        self,\n        list_add_to_x: List[bool],\n        feat_names: List[str],\n        input_nc_feats: List[Optional[int]] = None,\n        stricts: List[bool] = None,\n        delete_feats: List[bool] = None,\n    ):\n\n        self._feat_names = feat_names\n        self._list_add_to_x = list_add_to_x\n        self._delete_feats = delete_feats\n        if self._delete_feats:\n            assert len(self._delete_feats) == len(self._feat_names)\n        from torch_geometric.transforms import Compose\n\n        num_names = len(feat_names)\n        if num_names == 0:\n            raise Exception(""Expected to have at least one feat_names"")\n\n        assert len(list_add_to_x) == num_names\n\n        if input_nc_feats:\n            assert len(input_nc_feats) == num_names\n        else:\n            input_nc_feats = [None for _ in range(num_names)]\n\n        if stricts:\n            assert len(stricts) == num_names\n        else:\n            stricts = [True for _ in range(num_names)]\n\n        transforms = [\n            AddFeatByKey(add_to_x, feat_name, input_nc_feat=input_nc_feat, strict=strict)\n            for add_to_x, feat_name, input_nc_feat, strict in zip(list_add_to_x, feat_names, input_nc_feats, stricts)\n        ]\n\n        self.transform = Compose(transforms)\n\n    def __call__(self, data):\n        data = self.transform(data)\n        if self._delete_feats:\n            for feat_name, delete_feat in zip(self._feat_names, self._delete_feats):\n                if delete_feat:\n                    delattr(data, feat_name)\n        return data\n\n    def __repr__(self):\n        msg = """"\n        for f, a in zip(self._feat_names, self._list_add_to_x):\n            msg += ""{}={}, "".format(f, a)\n        return ""{}({})"".format(self.__class__.__name__, msg[:-2])\n\n\nclass AddFeatByKey(object):\n    """"""This transform is responsible to get an attribute under feat_name and add it to x if add_to_x is True\n\n    Paremeters\n    ----------\n    add_to_x: bool\n        Control if the feature is going to be added/concatenated to x\n    feat_name: str\n        The feature to be found within data to be added/concatenated to x\n    input_nc_feat: int, optional\n        If provided, check if feature last dimension maches provided value.\n    strict: bool, optional\n        Recommended to be set to True. If False, it won\'t break if feat isn\'t found or dimension doesn t match. (default: ``True``)\n    """"""\n\n    def __init__(self, add_to_x, feat_name, input_nc_feat=None, strict=True):\n\n        self._add_to_x: bool = add_to_x\n        self._feat_name: str = feat_name\n        self._input_nc_feat = input_nc_feat\n        self._strict: bool = strict\n\n    def __call__(self, data: Data):\n        if not self._add_to_x:\n            return data\n        feat = getattr(data, self._feat_name, None)\n        if feat is None:\n            if self._strict:\n                raise Exception(""Data should contain the attribute {}"".format(self._feat_name))\n            else:\n                return data\n        else:\n            if self._input_nc_feat:\n                feat_dim = 1 if feat.dim() == 1 else feat.shape[-1]\n                if self._input_nc_feat != feat_dim and self._strict:\n                    raise Exception(""The shape of feat: {} doesn t match {}"".format(feat.shape, self._input_nc_feat))\n            x = getattr(data, ""x"", None)\n            if x is None:\n                if self._strict and data.pos.shape[0] != feat.shape[0]:\n                    raise Exception(""We expected to have an attribute x"")\n                data.x = feat\n            else:\n                if x.shape[0] == feat.shape[0]:\n                    if x.dim() == 1:\n                        x = x.unsqueeze(-1)\n                    if feat.dim() == 1:\n                        feat = feat.unsqueeze(-1)\n                    data.x = torch.cat([x, feat], dim=-1)\n                else:\n                    raise Exception(\n                        ""The tensor x and {} can\'t be concatenated, x: {}, feat: {}"".format(\n                            self._feat_name, x.pos.shape[0], feat.pos.shape[0]\n                        )\n                    )\n        return data\n\n    def __repr__(self):\n        return ""{}(add_to_x: {}, feat_name: {}, strict: {})"".format(\n            self.__class__.__name__, self._add_to_x, self._feat_name, self._strict\n        )\n\n\ndef compute_planarity(eigenvalues):\n    r""""""\n    compute the planarity with respect to the eigenvalues of the covariance matrix of the pointcloud\n    let\n    :math:`\\lambda_1, \\lambda_2, \\lambda_3` be the eigenvalues st:\n\n    .. math::\n        \\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3\n\n    then planarity is defined as:\n\n    .. math::\n        planarity = \\frac{\\lambda_2 - \\lambda_1}{\\lambda_3}\n    """"""\n\n    return (eigenvalues[1] - eigenvalues[0]) / eigenvalues[2]\n\n\nclass NormalFeature(object):\n    """"""\n    add normal as feature. if it doesn\'t exist, compute normals\n    using PCA\n    """"""\n\n    def __call__(self, data):\n        if data.norm is None:\n            raise NotImplementedError(""TODO: Implement normal computation"")\n\n        norm = data.norm\n        if data.x is None:\n            data.x = norm\n        else:\n            data.x = torch.cat([data.x, norm], -1)\n        return data\n\n\nclass PCACompute(object):\n    r""""""\n    compute `Principal Component Analysis <https://en.wikipedia.org/wiki/Principal_component_analysis>`__ of a point cloud :math:`x_1,\\dots, x_n`.\n    It computes the eigenvalues and the eigenvectors of the matrix :math:`C` which is the covariance matrix of the point cloud:\n\n    .. math::\n        x_{centered} &= \\frac{1}{n} \\sum_{i=1}^n x_i\n\n        C &= \\frac{1}{n} \\sum_{i=1}^n (x_i - x_{centered})(x_i - x_{centered})^T\n\n    store the eigen values and the eigenvectors in data.\n    in eigenvalues attribute and eigenvectors attributes.\n    data.eigenvalues is a tensor :math:`(\\lambda_1, \\lambda_2, \\lambda_3)` such that :math:`\\lambda_1 \\leq \\lambda_2 \\leq \\lambda_3`.\n\n    data.eigenvectors is a 3 x 3 matrix such that the column are the eigenvectors associated to their eigenvalues\n    Therefore, the first column of data.eigenvectors estimates the normal at the center of the pointcloud.\n    """"""\n\n    def __call__(self, data):\n        pos_centered = data.pos - data.pos.mean(axis=0)\n        cov_matrix = pos_centered.T.mm(pos_centered) / len(pos_centered)\n        eig, v = torch.symeig(cov_matrix, eigenvectors=True)\n        data.eigenvalues = eig\n        data.eigenvectors = v\n        return data\n\n    def __repr__(self):\n        return ""{}()"".format(self.__class__.__name__)\n\n\nclass AddOnes(object):\n    """"""\n    Add ones tensor to data\n    """"""\n\n    def __call__(self, data):\n        num_nodes = data.pos.shape[0]\n        data.ones = torch.ones((num_nodes, 1)).float()\n        return data\n\n    def __repr__(self):\n        return ""{}()"".format(self.__class__.__name__)\n\n\nclass XYZFeature(object):\n    """"""\n    Add the X, Y and Z as a feature\n    Parameters\n    -----------\n    add_x: bool [default: False]\n        whether we add the x position or not\n    add_y: bool [default: False]\n        whether we add the y position or not\n    add_z: bool [default: True]\n        whether we add the z position or not\n    """"""\n\n    def __init__(self, add_x=False, add_y=False, add_z=True):\n        self._axis = []\n        axis_names = [""x"", ""y"", ""z""]\n        if add_x:\n            self._axis.append(0)\n        if add_y:\n            self._axis.append(1)\n        if add_z:\n            self._axis.append(2)\n\n        self._axis_names = [axis_names[idx_axis] for idx_axis in self._axis]\n\n    def __call__(self, data):\n        assert data.pos is not None\n        for axis_name, id_axis in zip(self._axis_names, self._axis):\n            f = data.pos[:, id_axis].clone()\n            setattr(data, ""pos_{}"".format(axis_name), f)\n        return data\n\n    def __repr__(self):\n        return ""{}(axis={})"".format(self.__class__.__name__, self._axis_names)\n'"
torch_points3d/core/data_transform/filters.py,0,"b'import numpy as np\nimport torch\nimport random\nfrom torch_points3d.core.data_transform.features import PCACompute, compute_planarity\n\n\nclass FCompose(object):\n    """"""\n    allow to compose different filters using the boolean operation\n\n    Parameter\n    ---------\n    list_filter: list\n        list of different filter functions we want to apply\n    boolean_operation: function, optional\n        boolean function to compose the filter (take a pair and return a boolean)\n    """"""\n    def __init__(self, list_filter, boolean_operation=np.logical_and):\n        self.list_filter = list_filter\n        self.boolean_operation = boolean_operation\n\n    def __call__(self, data):\n        assert len(self.list_filter) > 0\n        res = self.list_filter[0](data)\n        for filter_fn in self.list_filter:\n            res = self.boolean_operation(res, filter_fn(data))\n        return res\n\n    def __repr__(self):\n        rep = ""{}(["".format(self.__class__.__name__)\n        for filt in self.list_filter:\n            rep = rep + filt.__repr__() + "", ""\n        rep = rep + ""])""\n        return rep\n\n\nclass PlanarityFilter(object):\n    """"""\n    compute planarity and return false if the planarity of a pointcloud is above or below a threshold\n\n    Parameter\n    ---------\n    thresh: float, optional\n        threshold to filter low planar pointcloud\n    is_leq: bool, optional\n        choose whether planarity should be lesser or equal than the threshold or greater than the threshold.\n    """"""\n\n    def __init__(self, thresh=0.3, is_leq=True):\n        self.thresh = thresh\n        self.is_leq = is_leq\n\n    def __call__(self, data):\n        if(getattr(data, \'eigenvalues\', None) is None):\n            data = PCACompute()(data)\n        planarity = compute_planarity(data.eigenvalues)\n        if(self.is_leq):\n            return planarity <= self.thresh\n        else:\n            return planarity > self.thresh\n\n    def __repr__(self):\n        return ""{}(thresh={}, is_leq={})"".format(self.__class__.__name__, self.thresh, self.is_leq)\n\n\nclass RandomFilter(object):\n    """"""\n    Randomly select an elem of the dataset (to have smaller dataset) with a bernouilli distribution of parameter thresh.\n\n    Parameter\n    ---------\n    thresh: float, optional\n        the parameter of the bernouilli function\n    """"""\n\n    def __init__(self, thresh=0.3):\n        self.thresh = thresh\n\n    def __call__(self, data):\n        return random.random() < self.thresh\n\n    def __repr__(self):\n        return ""{}(thresh={})"".format(self.__class__.__name__, self.thresh)\n'"
torch_points3d/core/data_transform/grid_transform.py,11,"b'from typing import *\nimport numpy as np\nimport numpy\nimport scipy\nimport re\nimport torch\nimport logging\nimport torch.nn.functional as F\nfrom torch_scatter import scatter_mean, scatter_add\nfrom torch_geometric.nn.pool.consecutive import consecutive_cluster\nfrom torch_geometric.nn import voxel_grid\nfrom torch_geometric.data import Data\nfrom torch_cluster import grid_cluster\n\nlog = logging.getLogger(__name__)\n\n\n# Label will be the majority label in each voxel\n_INTEGER_LABEL_KEYS = [""y"", ""instance_labels""]\n\n\ndef shuffle_data(data):\n    num_points = data.pos.shape[0]\n    shuffle_idx = torch.randperm(num_points)\n    for key in set(data.keys):\n        item = data[key]\n        if torch.is_tensor(item) and num_points == item.shape[0]:\n            data[key] = item[shuffle_idx]\n    return data\n\n\ndef group_data(data, cluster=None, unique_pos_indices=None, mode=""last"", skip_keys=[]):\n    """""" Group data based on indices in cluster.\n    The option ``mode`` controls how data gets agregated within each cluster.\n\n    Parameters\n    ----------\n    data : Data\n        [description]\n    cluster : torch.Tensor\n        Tensor of the same size as the number of points in data. Each element is the cluster index of that point.\n    unique_pos_indices : torch.tensor\n        Tensor containing one index per cluster, this index will be used to select features and labels\n    mode : str\n        Option to select how the features and labels for each voxel is computed. Can be ``last`` or ``mean``.\n        ``last`` selects the last point falling in a voxel as the representent, ``mean`` takes the average.\n    skip_keys: list\n        Keys of attributes to skip in the grouping\n    """"""\n\n    assert mode in [""mean"", ""last""]\n    if mode == ""mean"" and cluster is None:\n        raise ValueError(""In mean mode the cluster argument needs to be specified"")\n    if mode == ""last"" and unique_pos_indices is None:\n        raise ValueError(""In last mode the unique_pos_indices argument needs to be specified"")\n\n    num_nodes = data.num_nodes\n    for key, item in data:\n        if bool(re.search(""edge"", key)):\n            raise ValueError(""Edges not supported. Wrong data type."")\n        if key in skip_keys:\n            continue\n\n        if torch.is_tensor(item) and item.size(0) == num_nodes:\n            if mode == ""last"" or key == ""batch"" or key == SaveOriginalPosId.KEY:\n                data[key] = item[unique_pos_indices]\n            elif mode == ""mean"":\n                if key in _INTEGER_LABEL_KEYS:\n                    item_min = item.min()\n                    item = F.one_hot(item - item_min)\n                    item = scatter_add(item, cluster, dim=0)\n                    data[key] = item.argmax(dim=-1) + item_min\n                else:\n                    data[key] = scatter_mean(item, cluster, dim=0)\n    return data\n\n\nclass GridSampling3D:\n    """""" Clusters points into voxels with size :attr:`size`.\n    Parameters\n    ----------\n    size: float\n        Size of a voxel (in each dimension).\n    quantize_coords: bool\n            If True, it will convert the points into their associated sparse coordinates within the grid. \\\n    mode: string:\n        The mode can be either `last` or `mean`.\n        If mode is `mean`, all the points and their features within a cell will be averaged\n        If mode is `last`, one random points per cell will be selected with its associated features\n    """"""\n\n    def __init__(self, size, quantize_coords=False, mode=""mean"", verbose=False):\n        self._grid_size = size\n        self._quantize_coords = quantize_coords\n        self._mode = mode\n        if verbose:\n            log.warning(\n                ""If you need to keep track of the position of your points, use SaveOriginalPosId transform before using GridSampling3D""\n            )\n\n            if self._mode == ""last"":\n                log.warning(\n                    ""The tensors within data will be shuffled each time this transform is applied. Be careful that if an attribute doesn\'t have the size of num_points, it won\'t be shuffled""\n                )\n\n    def _process(self, data):\n        if self._mode == ""last"":\n            data = shuffle_data(data)\n\n        coords = ((data.pos) / self._grid_size).int()\n        if ""batch"" not in data:\n            cluster = grid_cluster(coords, torch.tensor([1, 1, 1]))\n        else:\n            cluster = voxel_grid(coords, data.batch, 1)\n        cluster, unique_pos_indices = consecutive_cluster(cluster)\n\n        skip_keys = []\n        if self._quantize_coords:\n            skip_keys.append(""pos"")\n\n        data = group_data(data, cluster, unique_pos_indices, mode=self._mode, skip_keys=skip_keys)\n\n        if self._quantize_coords:\n            data.pos = coords[unique_pos_indices]\n\n        return data\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in data]\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(grid_size={}, quantize_coords={}, mode={})"".format(\n            self.__class__.__name__, self._grid_size, self._quantize_coords, self._mode\n        )\n\n\nclass SaveOriginalPosId:\n    """""" Transform that adds the index of the point to the data object\n    This allows us to track this point from the output back to the input data object\n    """"""\n\n    KEY = ""origin_id""\n\n    def _process(self, data):\n        if hasattr(data, self.KEY):\n            return data\n\n        setattr(data, self.KEY, torch.arange(0, data.pos.shape[0]))\n        return data\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in data]\n        else:\n            data = self._process(data)\n        return data\n\n\nclass ElasticDistortion:\n    """"""Apply elastic distortion on sparse coordinate space.\n    Parameters\n    ----------\n    granularity: float\n        Size of the noise grid (in same scale[m/cm] as the voxel grid)\n    magnitude: bool\n        Noise multiplier\n    Returns\n    -------\n    data: Data\n        Returns the same data object with distorted grid\n    """"""\n\n    def __init__(self, apply_distorsion: bool = True, granularity: List = [0.2, 0.4]):\n        self._apply_distorsion = apply_distorsion\n        self._granularity = list(granularity)\n\n    @staticmethod\n    def elastic_distortion(coords, granularity):\n        blurx = np.ones((3, 1, 1, 1)).astype(""float32"") / 3\n        blury = np.ones((1, 3, 1, 1)).astype(""float32"") / 3\n        blurz = np.ones((1, 1, 3, 1)).astype(""float32"") / 3\n        coords_min = coords.min(0)[0]\n\n        # Create Gaussian noise tensor of the size given by granularity.\n        dim = coords.shape[-1]\n        denom = torch.Tensor([np.random.uniform(granularity[0], granularity[1]) for _ in range(dim)])\n        noise_dim = ((coords - coords_min).max(0)[0] // denom).int() + 3\n        noise = np.random.randn(*noise_dim, 3).astype(np.float32)\n\n        # Smoothing.\n        for _ in range(2):\n            noise = scipy.ndimage.filters.convolve(noise, blurx, mode=""constant"", cval=0)\n            noise = scipy.ndimage.filters.convolve(noise, blury, mode=""constant"", cval=0)\n            noise = scipy.ndimage.filters.convolve(noise, blurz, mode=""constant"", cval=0)\n\n        # Trilinear interpolate noise filters for each spatial dimensions.\n        granularity_shift = granularity[1]\n        ax = [\n            np.linspace(d_min, d_max, d)\n            for d_min, d_max, d in zip(\n                coords_min - granularity_shift, coords_min + granularity_shift * (noise_dim - 2), noise_dim\n            )\n        ]\n        interp = scipy.interpolate.RegularGridInterpolator(ax, noise, bounds_error=0, fill_value=0)\n        return (coords + torch.Tensor(interp(coords))).int()\n\n    def __call__(self, data):\n        if self._apply_distorsion:\n            if np.random.uniform(0, 1) < 0.5:\n                data.pos = ElasticDistortion.elastic_distortion(data.pos, torch.Tensor(self._granularity))\n        return data\n\n    def __repr__(self):\n        return ""{}(apply_distorsion={}, granularity={})"".format(\n            self.__class__.__name__, self._apply_distorsion, self._granularity\n        )\n'"
torch_points3d/core/data_transform/inference_transforms.py,0,"b'import os\nimport sys\nimport logging\n\nROOT = os.path.join(os.path.dirname(os.path.realpath(__file__)), "".."", "".."", "".."")\nsys.path.insert(0, os.path.join(ROOT))\n\nlog = logging.getLogger(__name__)\n\n\nclass ModelInference(object):\n    """""" Base class transform for performing a point cloud inference using a pre_trained model\n    Subclass and implement the ``__call__`` method with your own forward. \n    See ``PointNetForward`` for an example implementation.\n    \n    Parameters\n    ----------\n    checkpoint_dir: str\n        Path to a checkpoint directory\n    model_name: str\n        Model name, the file ``checkpoint_dir/model_name.pt`` must exist\n    """"""\n\n    def __init__(self, checkpoint_dir, model_name, weight_name, feat_name, num_classes=None, mock_dataset=True):\n        # Checkpoint\n        from torch_points3d.datasets.base_dataset import BaseDataset\n        from torch_points3d.datasets.dataset_factory import instantiate_dataset\n        from torch_points3d.utils.mock import MockDataset\n        import torch_points3d.metrics.model_checkpoint as model_checkpoint\n\n        checkpoint = model_checkpoint.ModelCheckpoint(checkpoint_dir, model_name, weight_name, strict=True)\n        if mock_dataset:\n            dataset = MockDataset(num_classes)\n            dataset.num_classes = num_classes\n        else:\n            dataset = instantiate_dataset(checkpoint.data_config)\n        BaseDataset.set_transform(self, checkpoint.data_config)\n        self.model = checkpoint.create_model(dataset, weight_name=weight_name)\n        self.model.eval()\n\n    def __call__(self, data):\n        raise NotImplementedError\n\n\nclass PointNetForward(ModelInference):\n    """""" Transform for running a PointNet inference on a Data object. It assumes that the\n    model has been trained for segmentation.\n    \n    Parameters\n    ----------\n    checkpoint_dir: str\n        Path to a checkpoint directory\n    model_name: str\n        Model name, the file ``checkpoint_dir/model_name.pt`` must exist\n    weight_name: str\n        Type of weights to load (best for iou, best for loss etc...)\n    feat_name: str\n        Name of the key in Data that will hold the output of the forward\n    num_classes: int\n        Number of classes that the model was trained on\n    """"""\n\n    def __init__(self, checkpoint_dir, model_name, weight_name, feat_name, num_classes, mock_dataset=True):\n        super(PointNetForward, self).__init__(\n            checkpoint_dir, model_name, weight_name, feat_name, num_classes=num_classes, mock_dataset=mock_dataset\n        )\n        self.feat_name = feat_name\n\n        from torch_points3d.datasets.base_dataset import BaseDataset\n        from torch_geometric.transforms import FixedPoints, GridSampling3D\n\n        self.inference_transform = BaseDataset.remove_transform(self.inference_transform, [GridSampling3D, FixedPoints])\n\n    def __call__(self, data):\n        data_c = data.clone()\n        data_c.pos = data_c.pos.float()\n        if self.inference_transform:\n            data_c = self.inference_transform(data_c)\n        self.model.set_input(data_c, data.pos.device)\n        feat = self.model.get_local_feat().detach()\n        setattr(data, str(self.feat_name), feat)\n        return data\n\n    def __repr__(self):\n        return ""{}(model: {}, transform: {})"".format(\n            self.__class__.__name__, self.model.__class__.__name__, self.inference_transform\n        )\n'"
torch_points3d/core/data_transform/sparse_transforms.py,3,"b'\nfrom typing import List\nimport itertools\nimport numpy as np\nimport math\nimport re\nimport torch\nimport scipy\nimport random\nfrom tqdm.auto import tqdm as tq\nfrom torch.nn import functional as F\nfrom sklearn.neighbors import NearestNeighbors, KDTree\nfrom functools import partial\nfrom torch_geometric.nn import fps, radius, knn, voxel_grid\nfrom torch_geometric.nn.pool.consecutive import consecutive_cluster\nfrom torch_geometric.nn.pool.pool import pool_pos, pool_batch\nfrom torch_scatter import scatter_add, scatter_mean\nfrom torch_cluster import grid_cluster\n\nfrom torch_points3d.datasets.multiscale_data import MultiScaleData\nfrom torch_points3d.utils.config import is_list\nfrom torch_points3d.utils import is_iterable\nfrom .grid_transform import group_data, GridSampling3D, shuffle_data\n\n\nclass RemoveDuplicateCoords(object):\n    """""" This transform allow sto remove duplicated coords within ``indices`` from data.\n    Takes the average or selects the last point to set the features and labels of each voxel\n\n    Parameters\n    ----------\n    mode : str\n        Option to select how the features and labels for each voxel is computed. Can be ``last`` or ``mean``.\n        ``last`` selects the last point falling in a voxel as the representent, ``mean`` takes the average.\n    """"""\n\n    def __init__(self, mode=""last""):\n        self._mode = mode\n\n    def _process(self, data):\n        if self._mode == ""last"":\n            data = shuffle_data(data)\n\n        coords = data.pos\n        if ""batch"" not in data:\n            cluster = grid_cluster(coords, torch.tensor([1, 1, 1]))\n        else:\n            cluster = voxel_grid(coords, data.batch, 1)\n        cluster, unique_pos_indices = consecutive_cluster(cluster)\n\n        skip_keys=[]\n        if self._mode == ""last"":\n            skip_keys.append(""pos"")\n            data.pos = coords[unique_pos_indices]\n        data = group_data(data, cluster, unique_pos_indices, mode=self._mode, skip_keys=skip_keys)\n        return data\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in tq(data)]\n            data = list(itertools.chain(*data))  # 2d list needs to be flatten\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(mode={})"".format(self.__class__.__name__, self._mode)\n\nclass ToSparseInput(object):\n    """"""This transform allows to prepare data for sparse model as SparseConv / Minkowski Engine.\n    It does the following things:\n\n    - Puts ``pos`` on a fixed integer grid based on grid size\n    - Keeps one point per grid cell. The strategy for defining the feature nad label at that point depends on the ``mode`` option\n\n    Parameters\n    ----------\n    grid_size: float\n        Grid voxel size\n    mode : str\n        Option to select how the features and labels for each voxel is computed. Can be ``last`` or ``mean``.\n        ``last`` selects the last point falling in a voxel as the representent, ``mean`` takes the average.\n\n    Returns\n    -------\n    data: Data\n        Returns the same data object with only one point per voxel\n    """"""\n\n    def __init__(self, grid_size=None, mode=""last""):\n\n        self._grid_size = grid_size\n        self._mode = mode\n\n        self._transform = GridSampling3D(grid_size, quantize_coords=True, mode=mode)\n\n    def _process(self, data):\n        return self._transform(data)\n\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in tq(data)]\n            data = list(itertools.chain(*data))  # 2d list needs to be flatten\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(grid_size={}, mode={})""\\\n            .format(self.__class__.__name__, self._grid_size, self._mode)\n\n\nclass RandomCoordsFlip(object):\n\n    def __init__(self, ignored_axis, is_temporal=False, p=0.95):\n        """"""This transform is used to flip sparse coords using a given axis. Usually, it would be x or y\n\n        Parameters\n        ----------\n        ignored_axis: str\n            Axis to be chosen between x, y, z\n        is_temporal : bool\n            Used to indicate if the pointcloud is actually 4 dimensional\n\n        Returns\n        -------\n        data: Data\n            Returns the same data object with only one point per voxel\n        """"""\n        assert 0 <= p <= 1, ""p should be within 0 and 1. Higher probability reduce chance of flipping""\n        self._is_temporal = is_temporal\n        self._D = 4 if is_temporal else 3\n        mapping = {\'x\': 0, \'y\': 1, \'z\': 2}\n        self._ignored_axis = [mapping[axis] for axis in ignored_axis]\n        # Use the rest of axes for flipping.\n        self._horz_axes = set(range(self._D)) - set(self._ignored_axis)\n        self._p = p\n\n    def __call__(self, data):\n        for curr_ax in self._horz_axes:\n            if random.random() < self._p:\n                coords = data.pos\n                coord_max = torch.max(coords[:, curr_ax])\n                data.pos[:, curr_ax] = coord_max - coords[:, curr_ax]\n        return data\n\n    def __repr__(self):\n        return ""{}(flip_axis={}, prob={}, is_temporal={})""\\\n            .format(self.__class__.__name__, self._horz_axes, self._p, self._is_temporal)\n'"
torch_points3d/core/data_transform/transforms.py,24,"b'from typing import List\nimport itertools\nimport numpy as np\nimport math\nimport re\nimport torch\nimport random\nfrom tqdm.auto import tqdm as tq\nfrom sklearn.neighbors import NearestNeighbors, KDTree\nfrom functools import partial\nfrom torch.nn import functional as F\nfrom torch_geometric.nn.pool.pool import pool_pos, pool_batch\nfrom torch_geometric.data import Data, Batch\nfrom torch_scatter import scatter_add, scatter_mean\nfrom torch_geometric.transforms import FixedPoints as FP\n\nfrom torch_points3d.datasets.multiscale_data import MultiScaleData\nfrom torch_points3d.datasets.registration.pair import Pair\nfrom torch_points3d.utils.transform_utils import SamplingStrategy\nfrom torch_points3d.utils.config import is_list\nfrom torch_points3d.utils import is_iterable\nfrom .grid_transform import group_data, GridSampling3D, shuffle_data\n\n\nclass RemoveAttributes(object):\n    """"""This transform allows to remove unnecessary attributes from data for optimization purposes\n\n    Parameters\n    ----------\n    attr_names: list\n        Remove the attributes from data using the provided `attr_name` within attr_names\n    strict: bool=False\n        Wether True, it will raise an execption if the provided attr_name isn t within data keys.\n    """"""\n\n    def __init__(self, attr_names=[], strict=False):\n        self._attr_names = attr_names\n        self._strict = strict\n\n    def __call__(self, data):\n        keys = set(data.keys)\n        for attr_name in self._attr_names:\n            if attr_name not in keys and self._strict:\n                raise Exception(""attr_name: {} isn t within keys: {}"".format(attr_name, keys))\n        for attr_name in self._attr_names:\n            delattr(data, attr_name)\n        return data\n\n    def __repr__(self):\n        return ""{}(attr_names={}, strict={})"".format(self.__class__.__name__, self._attr_names, self._strict)\n\n\nclass PointCloudFusion(object):\n\n    """"""This transform is responsible to perform a point cloud fusion from a list of data\n\n    - If a list of data is provided -> Create one Batch object with all data\n    - If a list of list of data is provided -> Create a list of fused point cloud\n    """"""\n\n    def _process(self, data_list):\n        if len(data_list) == 0:\n            return Data()\n        data = Batch.from_data_list(data_list)\n        delattr(data, ""batch"")\n        return data\n\n    def __call__(self, data_list: List[Data]):\n        if len(data_list) == 0:\n            raise Exception(""A list of data should be provided"")\n        elif len(data_list) == 1:\n            return data_list[0]\n        else:\n            if isinstance(data_list[0], list):\n                data = [self._process(d) for d in data_list]\n            else:\n                data = self._process(data_list)\n        return data\n\n    def __repr__(self):\n        return ""{}()"".format(self.__class__.__name__)\n\n\nclass GridSphereSampling(object):\n    """"""Fits the point cloud to a grid and for each point in this grid,\n    create a sphere with a radius r\n\n    Parameters\n    ----------\n    radius: float\n        Radius of the sphere to be sampled.\n    grid_size: float, optional\n        Grid_size to be used with GridSampling3D to select spheres center. If None, radius will be used\n    delattr_kd_tree: bool, optional\n        If True, KDTREE_KEY should be deleted as an attribute if it exists\n    center: bool, optional\n        If True, a centre transform is apply on each sphere.\n    """"""\n\n    KDTREE_KEY = ""kd_tree""\n\n    def __init__(self, radius, grid_size=None, delattr_kd_tree=True, center=True):\n        self._radius = eval(radius) if isinstance(radius, str) else float(radius)\n        grid_size = eval(grid_size) if isinstance(grid_size, str) else float(grid_size)\n        self._grid_sampling = GridSampling3D(size=grid_size if grid_size else self._radius)\n        self._delattr_kd_tree = delattr_kd_tree\n        self._center = center\n\n    def _process(self, data):\n        if not hasattr(data, self.KDTREE_KEY):\n            tree = KDTree(np.asarray(data.pos), leaf_size=50)\n        else:\n            tree = getattr(data, self.KDTREE_KEY)\n\n        # The kdtree has bee attached to data for optimization reason.\n        # However, it won\'t be used for down the transform pipeline and should be removed before any collate func call.\n        if hasattr(data, self.KDTREE_KEY) and self._delattr_kd_tree:\n            delattr(data, self.KDTREE_KEY)\n\n        # apply grid sampling\n        grid_data = self._grid_sampling(data.clone())\n\n        datas = []\n        for grid_center in np.asarray(grid_data.pos):\n            pts = np.asarray(grid_center)[np.newaxis]\n\n            # Find closest point within the original data\n            ind = torch.LongTensor(tree.query(pts, k=1)[1][0])\n            grid_label = data.y[ind]\n\n            # Find neighbours within the original data\n            ind = torch.LongTensor(tree.query_radius(pts, r=self._radius)[0])\n            sampler = SphereSampling(self._radius, grid_center, align_origin=self._center)\n            new_data = sampler(data)\n            new_data.center_label = grid_label\n\n            datas.append(new_data)\n        return datas\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in tq(data)]\n            data = list(itertools.chain(*data))  # 2d list needs to be flatten\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(radius={}, center={})"".format(self.__class__.__name__, self._radius, self._center)\n\n\nclass ComputeKDTree(object):\n    """"""Calculate the KDTree and saves it within data\n\n    Parameters\n    -----------\n    leaf_size:int\n        Size of the leaf node.\n    """"""\n\n    def __init__(self, leaf_size):\n        self._leaf_size = leaf_size\n\n    def _process(self, data):\n        data.kd_tree = KDTree(np.asarray(data.pos), leaf_size=self._leaf_size)\n        return data\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in data]\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(leaf_size={})"".format(self.__class__.__name__, self._leaf_size)\n\n\nclass RandomSphere(object):\n    """"""Select points within a sphere of a given radius. The centre is chosen randomly within the point cloud.\n\n    Parameters\n    ----------\n    radius: float\n        Radius of the sphere to be sampled.\n    strategy: str\n        choose between `random` and `freq_class_based`. The `freq_class_based` \\\n        favors points with low frequency class. This can be used to balance unbalanced datasets\n    center: bool\n        if True then the sphere will be moved to the origin\n    """"""\n\n    def __init__(self, radius, strategy=""random"", class_weight_method=""sqrt"", center=True):\n        self._radius = eval(radius) if isinstance(radius, str) else float(radius)\n        self._sampling_strategy = SamplingStrategy(strategy=strategy, class_weight_method=class_weight_method)\n        self._center = center\n\n    def _process(self, data):\n        # apply sampling strategy\n        random_center = self._sampling_strategy(data)\n        random_center = np.asarray(data.pos[random_center])[np.newaxis]\n        sphere_sampling = SphereSampling(self._radius, random_center, align_origin=self._center)\n        return sphere_sampling(data)\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in data]\n        else:\n            data = self._process(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(radius={}, center={}, sampling_strategy={})"".format(\n            self.__class__.__name__, self._radius, self._center, self._sampling_strategy\n        )\n\n\nclass SphereSampling:\n    """""" Samples points within a sphere\n\n    Parameters\n    ----------\n    radius : float\n        Radius of the sphere\n    sphere_centre : torch.Tensor or np.array\n        Centre of the sphere (1D array that contains (x,y,z))\n    align_origin : bool, optional\n        move resulting point cloud to origin\n    """"""\n\n    KDTREE_KEY = ""kd_tree""\n\n    def __init__(self, radius, sphere_centre, align_origin=True):\n        self._radius = radius\n        self._centre = np.asarray(sphere_centre)\n        if len(self._centre.shape) == 1:\n            self._centre = np.expand_dims(self._centre, 0)\n        self._align_origin = align_origin\n\n    def __call__(self, data):\n        num_points = data.pos.shape[0]\n        if not hasattr(data, self.KDTREE_KEY):\n            tree = KDTree(np.asarray(data.pos), leaf_size=50)\n            setattr(data, self.KDTREE_KEY, tree)\n        else:\n            tree = getattr(data, self.KDTREE_KEY)\n\n        t_center = torch.FloatTensor(self._centre)\n        ind = torch.LongTensor(tree.query_radius(self._centre, r=self._radius)[0])\n        new_data = Data()\n        for key in set(data.keys):\n            if key == self.KDTREE_KEY:\n                continue\n            item = data[key]\n            if torch.is_tensor(item) and num_points == item.shape[0]:\n                item = item[ind]\n                if self._align_origin and key == ""pos"":  # Center the sphere.\n                    item -= t_center\n            elif torch.is_tensor(item):\n                item = item.clone()\n            setattr(new_data, key, item)\n        return new_data\n\n    def __repr__(self):\n        return ""{}(radius={}, center={}, align_origin={})"".format(\n            self.__class__.__name__, self._radius, self._centre, self._align_origin\n        )\n\n\nclass RandomSymmetry(object):\n    """""" Apply a random symmetry transformation on the data\n\n    Parameters\n    ----------\n    axis: Tuple[bool,bool,bool], optional\n        axis along which the symmetry is applied\n    """"""\n\n    def __init__(self, axis=[False, False, False]):\n        self.axis = axis\n\n    def __call__(self, data):\n\n        for i, ax in enumerate(self.axis):\n            if ax:\n                if torch.rand(1) < 0.5:\n                    data.pos[:, i] *= -1\n        return data\n\n    def __repr__(self):\n        return ""Random symmetry of axes: x={}, y={}, z={}"".format(*self.axis)\n\n\nclass RandomNoise(object):\n    """""" Simple isotropic additive gaussian noise (Jitter)\n\n    Parameters\n    ----------\n    sigma:\n        Variance of the noise\n    clip:\n        Maximum amplitude of the noise\n    """"""\n\n    def __init__(self, sigma=0.01, clip=0.05):\n        self.sigma = sigma\n        self.clip = clip\n\n    def __call__(self, data):\n        noise = self.sigma * torch.randn(data.pos.shape)\n        noise = noise.clamp(-self.clip, self.clip)\n        data.pos = data.pos + noise\n        return data\n\n    def __repr__(self):\n        return ""{}(sigma={}, clip={})"".format(self.__class__.__name__, self.sigma, self.clip)\n\n\nclass ScalePos:\n    def __init__(self, scale=None):\n        self.scale = scale\n\n    def __call__(self, data):\n        data.pos *= self.scale\n        return data\n\n    def __repr__(self):\n        return ""{}(scale={})"".format(self.__class__.__name__, self.scale)\n\n\nclass RandomScaleAnisotropic:\n    r"""""" Scales node positions by a randomly sampled factor ``s1, s2, s3`` within a\n    given interval, *e.g.*, resulting in the transformation matrix\n\n    .. math::\n        \\left[\n        \\begin{array}{ccc}\n            s1 & 0 & 0 \\\\\n            0 & s2 & 0 \\\\\n            0 & 0 & s3 \\\\\n        \\end{array}\n        \\right]\n\n\n    for three-dimensional positions.\n\n    Parameter\n    ---------\n    scales:\n        scaling factor interval, e.g. ``(a, b)``, then scale \\\n        is randomly sampled from the range \\\n        ``a <=  b``. \\\n    """"""\n\n    def __init__(self, scales=None, anisotropic=True):\n        assert is_iterable(scales) and len(scales) == 2\n        assert scales[0] <= scales[1]\n        self.scales = scales\n\n    def __call__(self, data):\n        scale = self.scales[0] + torch.rand((3,)) * (self.scales[1] - self.scales[0])\n        data.pos = data.pos * scale\n        return data\n\n    def __repr__(self):\n        return ""{}({})"".format(self.__class__.__name__, self.scales)\n\n\ndef euler_angles_to_rotation_matrix(theta):\n    R_x = torch.tensor(\n        [[1, 0, 0], [0, torch.cos(theta[0]), -torch.sin(theta[0])], [0, torch.sin(theta[0]), torch.cos(theta[0])]]\n    )\n\n    R_y = torch.tensor(\n        [[torch.cos(theta[1]), 0, torch.sin(theta[1])], [0, 1, 0], [-torch.sin(theta[1]), 0, torch.cos(theta[1])]]\n    )\n\n    R_z = torch.tensor(\n        [[torch.cos(theta[2]), -torch.sin(theta[2]), 0], [torch.sin(theta[2]), torch.cos(theta[2]), 0], [0, 0, 1]]\n    )\n\n    R = torch.mm(R_z, torch.mm(R_y, R_x))\n    return R\n\n\nclass MeshToNormal(object):\n    """""" Computes mesh normals (IN PROGRESS)\n    """"""\n\n    def __init__(self):\n        pass\n\n    def __call__(self, data):\n        if hasattr(data, ""face""):\n            pos = data.pos\n            face = data.face\n            vertices = [pos[f] for f in face]\n            normals = torch.cross(vertices[0] - vertices[1], vertices[0] - vertices[2], dim=1)\n            normals = F.normalize(normals)\n            data.normals = normals\n        return data\n\n    def __repr__(self):\n        return ""{}"".format(self.__class__.__name__)\n\n\nclass MultiScaleTransform(object):\n    """""" Pre-computes a sequence of downsampling / neighboorhood search on the CPU.\n    This currently only works on PARTIAL_DENSE formats\n\n    Parameters\n    -----------\n    strategies: Dict[str, object]\n        Dictionary that contains the samplers and neighbour_finder\n    """"""\n\n    def __init__(self, strategies):\n        self.strategies = strategies\n        self.num_layers = len(self.strategies[""sampler""])\n\n    @staticmethod\n    def __inc__wrapper(func, special_params):\n        def new__inc__(key, num_nodes, special_params=None, func=None):\n            if key in special_params:\n                return special_params[key]\n            else:\n                return func(key, num_nodes)\n\n        return partial(new__inc__, special_params=special_params, func=func)\n\n    def __call__(self, data: Data) -> MultiScaleData:\n        # Compute sequentially multi_scale indexes on cpu\n        data.contiguous()\n        ms_data = MultiScaleData.from_data(data)\n        precomputed = [Data(pos=data.pos)]\n        upsample = []\n        upsample_index = 0\n        for index in range(self.num_layers):\n            sampler, neighbour_finder = self.strategies[""sampler""][index], self.strategies[""neighbour_finder""][index]\n            support = precomputed[index]\n            new_data = Data(pos=support.pos)\n            if sampler:\n                query = sampler(new_data.clone())\n                query.contiguous()\n\n                if len(self.strategies[""upsample_op""]):\n                    if upsample_index >= len(self.strategies[""upsample_op""]):\n                        raise ValueError(""You are missing some upsample blocks in your network"")\n\n                    upsampler = self.strategies[""upsample_op""][upsample_index]\n                    upsample_index += 1\n                    pre_up = upsampler.precompute(query, support)\n                    upsample.append(pre_up)\n                    special_params = {}\n                    special_params[""x_idx""] = query.num_nodes\n                    special_params[""y_idx""] = support.num_nodes\n                    setattr(pre_up, ""__inc__"", self.__inc__wrapper(pre_up.__inc__, special_params))\n            else:\n                query = new_data\n\n            s_pos, q_pos = support.pos, query.pos\n            if hasattr(query, ""batch""):\n                s_batch, q_batch = support.batch, query.batch\n            else:\n                s_batch, q_batch = (\n                    torch.zeros((s_pos.shape[0]), dtype=torch.long),\n                    torch.zeros((q_pos.shape[0]), dtype=torch.long),\n                )\n\n            idx_neighboors = neighbour_finder(s_pos, q_pos, batch_x=s_batch, batch_y=q_batch)\n            special_params = {}\n            special_params[""idx_neighboors""] = s_pos.shape[0]\n            setattr(query, ""idx_neighboors"", idx_neighboors)\n            setattr(query, ""__inc__"", self.__inc__wrapper(query.__inc__, special_params))\n            precomputed.append(query)\n        ms_data.multiscale = precomputed[1:]\n        upsample.reverse()  # Switch to inner layer first\n        ms_data.upsample = upsample\n        return ms_data\n\n    def __repr__(self):\n        return ""{}"".format(self.__class__.__name__)\n\n\nclass ShuffleData(object):\n    """""" This transform allow to shuffle feature, pos and label tensors within data\n    """"""\n\n    def _process(self, data):\n        return shuffle_data(data)\n\n    def __call__(self, data):\n        if isinstance(data, list):\n            data = [self._process(d) for d in tq(data)]\n            data = list(itertools.chain(*data))  # 2d list needs to be flatten\n        else:\n            data = self._process(data)\n        return data\n\n\nclass PairTransform(object):\n    def __init__(self, transform):\n        """"""\n        apply the transform for a pair of data\n        (as defined in torch_points3d/datasets/registration/pair.py)\n        """"""\n        self.transform = transform\n\n    def __call__(self, data):\n        data_source, data_target = data.to_data()\n        data_source = self.transform(data_source)\n        data_target = self.transform(data_target)\n        return Pair.make_pair(data_source, data_target)\n\n    def __repr__(self):\n        return ""{}()"".format(self.__class__.__name__)\n\n\nclass ShiftVoxels:\n    """""" Trick to make Sparse conv invariant to even and odds coordinates\n    https://github.com/chrischoy/SpatioTemporalSegmentation/blob/master/lib/train.py#L78\n    Parameters\n    -----------\n    apply_shift: bool:\n        Whether to apply the shift on indices\n    """"""\n\n    def __init__(self, apply_shift=True):\n        self._apply_shift = apply_shift\n\n    def __call__(self, data):\n        if self._apply_shift:\n            if not isinstance(data.pos, torch.IntTensor):\n                raise Exception(""The pos are expected to be coordinates, so torch.IntTensor"")\n            data.pos[:, :3] += (torch.rand(3) * 100).type_as(data.pos)\n        return data\n\n    def __repr__(self):\n        return ""{}(apply_shift={})"".format(self.__class__.__name__, self._apply_shift)\n\n\nclass RandomDropout:\n    """""" Randomly drop points from the input data\n    Parameters\n    ----------\n    dropout_ratio : float, optional\n        Ratio that gets dropped\n    dropout_application_ratio   : float, optional\n        chances of the dropout to be applied\n    """"""\n\n    def __init__(self, dropout_ratio=0.2, dropout_application_ratio=0.5):\n        self.dropout_ratio = dropout_ratio\n        self.dropout_application_ratio = dropout_application_ratio\n\n    def __call__(self, data):\n        if random.random() < self.dropout_application_ratio:\n            N = len(data.pos)\n            data = FP(int(N * (1 - self.dropout_ratio)))(data)\n        return data\n\n    def __repr__(self):\n        return ""{}(dropout_ratio={}, dropout_application_ratio={})"".format(\n            self.__class__.__name__, self.dropout_ratio, self.dropout_application_ratio\n        )\n'"
torch_points3d/core/initializer/__init__.py,0,b'from .initializer import *\n'
torch_points3d/core/initializer/initializer.py,3,"b'import torch\nfrom torch.nn import init\n\n\ndef init_weights(net, init_type=""normal"", gain=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, ""weight"") and (classname.find(""Conv"") != -1 or classname.find(""Linear"") != -1):\n            if init_type == ""normal"":\n                init.normal_(m.weight.data, 0.0, gain)\n            elif init_type == ""xavier"":\n                init.xavier_normal_(m.weight.data, gain=gain)\n            elif init_type == ""kaiming"":\n                init.kaiming_normal_(m.weight.data, a=0, mode=""fan_in"")\n            elif init_type == ""orthogonal"":\n                init.orthogonal_(m.weight.data, gain=gain)\n            else:\n                raise NotImplementedError(""initialization method [%s] is not implemented"" % init_type)\n            if hasattr(m, ""bias"") and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find(""BatchNorm2d"") != -1:\n            init.normal_(m.weight.data, 1.0, gain)\n            init.constant_(m.bias.data, 0.0)\n\n    print(""initialize network with %s"" % init_type)\n    net.apply(init_func)\n\n\ndef init_net(net, init_type=""normal"", init_gain=0.02, gpu_ids=[]):\n    if len(gpu_ids) > 0:\n        assert torch.cuda.is_available()\n        net.to(gpu_ids[0])\n        net = torch.nn.DataParallel(net, gpu_ids)\n    init_weights(net, init_type, gain=init_gain)\n    return net\n'"
torch_points3d/core/losses/__init__.py,0,"b'import sys\n\nfrom .losses import *\nfrom .huber_loss import *\nfrom pytorch_metric_learning.miners import *\nfrom pytorch_metric_learning.losses import *\n\n_custom_losses = sys.modules[""torch_points3d.core.losses.losses""]\n_torch_metric_learning_losses = sys.modules[""pytorch_metric_learning.losses""]\n_torch_metric_learning_miners = sys.modules[""pytorch_metric_learning.miners""]\n_intersection = set(_custom_losses.__dict__) & set(_torch_metric_learning_losses.__dict__)\n_intersection = set([module for module in _intersection if not module.startswith(""_"")])\nif _intersection:\n    raise Exception(\n        ""It seems that you are overiding a transform from pytorch metric learning, \\\n            this is forbiden, please rename your classes {}"".format(\n            _intersection\n        )\n    )\n\n\ndef instantiate_loss_or_miner(option, mode=""loss""):\n    """"""\n    create a loss from an OmegaConf dict such as\n    TripletMarginLoss.\n    params:\n        margin=0.1\n    It can also instantiate a miner to better learn a loss\n    """"""\n    class_ = getattr(option, ""class"", None)\n    try:\n        params = option.params\n    except KeyError:\n        params = None\n\n    try:\n        lparams = option.lparams\n    except KeyError:\n        lparams = None\n\n    if ""loss"" in mode:\n        cls = getattr(_custom_losses, class_, None)\n        if not cls:\n            cls = getattr(_torch_metric_learning_losses, class_, None)\n            if not cls:\n                raise ValueError(""loss %s is nowhere to be found"" % class_)\n    elif mode == ""miner"":\n        cls = getattr(_torch_metric_learning_miners, class_, None)\n        if not cls:\n            raise ValueError(""miner %s is nowhere to be found"" % class_)\n    else:\n        raise NotImplementedError(""Cannot instantiate this mode {}"".format(mode))\n\n    if params and lparams:\n        return cls(*lparams, **params)\n    if params:\n        return cls(**params)\n    if lparams:\n        return cls(*params)\n    return cls()\n'"
torch_points3d/core/losses/dirichlet_loss.py,5,"b'import torch\nimport torch_points_kernels as tp\nfrom torch_cluster import radius\nfrom torch_scatter import scatter_add\n\n_MAX_NEIGHBOURS = 32\n\n\nclass DirichletLoss(torch.nn.Module):\n    """""" L2 norm of the gradient estimated as the average change of a field value f\n    accross neighbouring points within a radius r\n    """"""\n\n    def __init__(self, r, aggr=torch.mean):\n        super().__init__()\n        self._r = r\n        self._aggr = aggr\n\n    def forward(self, pos, f, batch_idx=None):\n        """""" Computes the Dirichlet loss (or L2 norm of the gradient) of f\n        Arguments:\n            pos -- [N,3] (or [B,N,3] for dense format)  location of each point\n            f -- [N] (or [B,N] for dense format)  Value of a function at each points\n            batch_idx -- [N] Batch id of each point (Only for sparse format)\n        """"""\n        return dirichlet_loss(self._r, pos, f, batch_idx=batch_idx, aggr=self._aggr)\n\n\ndef dirichlet_loss(r, pos, f, batch_idx=None, aggr=torch.mean):\n    """""" Computes the Dirichlet loss (or L2 norm of the gradient) of f\n    Arguments:\n        r -- Radius for the beighbour search\n        pos -- [N,3] (or [B,N,3] for dense format)  location of each point\n        f -- [N] (or [B,N] for dense format)  Value of a function at each points\n        batch_idx -- [N] Batch id of each point (Only for sparse format)\n        aggr -- aggregation function for the final loss value\n    """"""\n    if batch_idx is None:\n        assert f.dim() == 2 and pos.dim() == 3\n        return _dirichlet_dense(r, pos, f, aggr)\n    else:\n        assert f.dim() == 1 and pos.dim() == 2\n        return _dirichlet_sparse(r, pos, f, batch_idx, aggr)\n\n\ndef _dirichlet_dense(r, pos, f, aggr):\n    variances = _variance_estimator_dense(r, pos, f)\n    return 1 / 2.0 * aggr(variances)\n\n\ndef _variance_estimator_dense(r, pos, f):\n    nei_idx = tp.ball_query(r, _MAX_NEIGHBOURS, pos, pos, sort=True)[0].reshape(pos.shape[0], -1).long()  # [B,N * nei]\n    f_neighboors = f.gather(1, nei_idx).reshape(f.shape[0], f.shape[1], -1)  # [B,N , nei]\n    gradient = (f.unsqueeze(-1).repeat(1, 1, f_neighboors.shape[-1]) - f_neighboors) ** 2  # [B,N,nei]\n    return gradient.sum(-1)\n\n\ndef _dirichlet_sparse(r, pos, f, batch_idx, aggr):\n    variances = _variance_estimator_sparse(r, pos, f, batch_idx)\n    return 1 / 2.0 * aggr(variances)\n\n\ndef _variance_estimator_sparse(r, pos, f, batch_idx):\n    with torch.no_grad():\n        assign_index = radius(pos, pos, r, batch_x=batch_idx, batch_y=batch_idx)\n        y_idx, x_idx = assign_index\n        # diff = pos[x_idx] - pos[y_idx]\n        # squared_distance = (diff * diff).sum(dim=-1, keepdim=True)\n        # weights = 1.0 / torch.clamp(squared_distance, min=1e-16)\n\n        grad_f = (f[x_idx] - f[y_idx]) ** 2\n    y = scatter_add(grad_f, y_idx, dim=0, dim_size=pos.size(0))\n    return y\n'"
torch_points3d/core/losses/huber_loss.py,4,"b'import torch\n\n\ndef huber_loss(error, delta=1.0):\n    """"""\n    Args:\n        error: Torch tensor (d1,d2,...,dk)\n    Returns:\n        loss: Torch tensor (d1,d2,...,dk)\n\n    x = error = pred - gt or dist(pred,gt)\n    0.5 * |x|^2                 if |x|<=d\n    0.5 * d^2 + d * (|x|-d)     if |x|>d\n    Ref: https://github.com/charlesq34/frustum-pointnets/blob/master/models/model_util.py\n    """"""\n    abs_error = torch.abs(error)\n    # quadratic = torch.min(abs_error, torch.FloatTensor([delta]))\n    quadratic = torch.clamp(abs_error, max=delta)\n    linear = abs_error - quadratic\n    loss = 0.5 * quadratic ** 2 + delta * linear\n    return loss\n\n\nclass HuberLoss(torch.nn.Module):\n    def __init__(self, delta=0.1):\n        super().__init__()\n        self._delta = delta\n\n    def forward(self, error):\n        return huber_loss(error, self._delta)\n'"
torch_points3d/core/losses/losses.py,10,"b'from typing import Any\nfrom collections import OrderedDict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\nfrom .metric_losses import *\n\n\ndef filter_valid(output, target, ignore_label=IGNORE_LABEL, other=None):\n    """""" Removes predictions for nodes without ground truth """"""\n    idx = target != ignore_label\n    if other is not None:\n        return output[idx, :], target[idx], other[idx, ...]\n    return output[idx, :], target[idx]\n\n\nclass LossAnnealer(torch.nn.modules.loss._Loss):\n    """"""\n    This class will be used to perform annealing between two losses\n    """"""\n\n    def __init__(self, args):\n        super(LossAnnealer, self).__init__()\n        self._coeff = 0.5  # args.coefficient\n        self.normalized_loss = True\n\n    def forward(self, loss_1, loss_2, **kwargs):\n        annealing_alpha = kwargs.get(""annealing_alpha"", None)\n        if annealing_alpha is None:\n            return self._coeff * loss_1 + (1 - self._coeff) * loss_2\n        else:\n            return (1 - annealing_alpha) * loss_1 + annealing_alpha * loss_2\n\n\nclass LossFactory(torch.nn.modules.loss._Loss):\n    def __init__(self, loss, dbinfo):\n        super(LossFactory, self).__init__()\n\n        self._loss = loss\n        self.special_args = {}\n        self.search_for_args = []\n        if self._loss == ""cross_entropy"":\n            self._loss_func = nn.functional.cross_entropy\n            self.special_args = {""weight"": dbinfo[""class_weights""]}\n            # self.search_for_args = [\'cloug_flag\']\n\n        elif self._loss == ""focal_loss"":\n            self._loss_func = FocalLoss(alphas=dbinfo[""class_weights""])\n\n        elif self._loss == ""KLDivLoss"":\n            self._loss_func = WrapperKLDivLoss()\n            self.search_for_args = [""segm_size"", ""label_vec""]\n\n        else:\n            raise NotImplementedError\n\n    def forward(self, input, target, **kwargs):\n        added_arguments = OrderedDict()\n        for key in self.search_for_args:\n            added_arguments[key] = kwargs.get(key, None)\n        input, target = filter_valid(input, target)\n        return self._loss_func(input, target, **added_arguments, **self.special_args)\n\n\nclass FocalLoss(torch.nn.modules.loss._Loss):\n    def __init__(\n        self, gamma: float = 2, alphas: Any = None, size_average: bool = True, normalized: bool = True,\n    ):\n        super(FocalLoss, self).__init__()\n        self._gamma = gamma\n        self._alphas = alphas\n        self.size_average = size_average\n        self.normalized = normalized\n\n    def forward(self, input, target):\n        logpt = F.log_softmax(input, dim=-1)\n        logpt = torch.gather(logpt, -1, target.unsqueeze(-1))\n        logpt = logpt.view(-1)\n        pt = Variable(logpt.data.exp())\n\n        if self._alphas is not None:\n            at = self._alphas.gather(0, target)\n            logpt = logpt * Variable(at)\n\n        if self.normalized:\n            sum_ = 1 / torch.sum((1 - pt) ** self._gamma)\n        else:\n            sum_ = 1\n\n        loss = -1 * sum_ * (1 - pt) ** self._gamma * logpt\n        return loss.sum()\n\n\nclass WrapperKLDivLoss(torch.nn.modules.loss._Loss):\n    def __init__(self, size_average=None, reduce=None, reduction=""mean""):\n        super(WrapperKLDivLoss, self).__init__(size_average, reduce, reduction)\n\n    def forward(self, input, target, label_vec=None, segm_size=None):\n        label_vec = Variable(label_vec).float() / segm_size.unsqueeze(-1).float()\n        input = F.log_softmax(input, dim=-1)\n        loss = torch.nn.modules.loss.KLDivLoss()(input, label_vec)\n        return loss\n'"
torch_points3d/core/losses/metric_losses.py,7,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\n\ndef _hash(arr, M):\n    if isinstance(arr, np.ndarray):\n        N, D = arr.shape\n    else:\n        N, D = len(arr[0]), len(arr)\n\n    hash_vec = np.zeros(N, dtype=np.int64)\n    for d in range(D):\n        if isinstance(arr, np.ndarray):\n            hash_vec += arr[:, d] * M ** d\n        else:\n            hash_vec += arr[d] * M ** d\n    return hash_vec\n\n\ndef pdist(A, B, dist_type=""L2""):\n    if dist_type == ""L2"":\n        D2 = torch.sum((A.unsqueeze(1) - B.unsqueeze(0)).pow(2), 2)\n        return torch.sqrt(D2 + 1e-7)\n    elif dist_type == ""SquareL2"":\n        return torch.sum((A.unsqueeze(1) - B.unsqueeze(0)).pow(2), 2)\n    else:\n        raise NotImplementedError(""Not implemented"")\n\n\nclass ContrastiveHardestNegativeLoss(nn.Module):\n    r""""""\n    Compute contrastive loss between positive pairs and mine negative pairs which are not in the intersection of the two point clouds (taken from https://github.com/chrischoy/FCGF)\n    Let :math:`(f_i, f^{+}_i)_{i=1 \\dots N}` set of positive_pairs and :math:`(f_i, f^{-}_i)_{i=1 \\dots M}` a set of negative pairs\n    The loss is computed as:\n    .. math::\n        L = \\frac{1}{N^2} \\sum_{i=1}^N \\sum_{j=1}^N [d^{+}_{ij} - \\lambda_+]_+ + \\frac{1}{M} \\sum_{i=1}^M [\\lambda_{-} - d^{-}_i]_+\n\n    where:\n    .. math::\n        d^{+}_{ij} = ||f_{i} - f^{+}_{j}||\n\n    and\n    .. math::\n        d^{-}_{i} = \\min_{j}(||f_{i} - f^{-}_{j}||)\n\n    In this loss, we only mine the negatives\n    Parameters\n    ----------\n\n    pos_thresh:\n        positive threshold of the positive distance\n    neg_thresh:\n        negative threshold of the negative distance\n    num_pos:\n        number of positive pairs\n    num_hn_samples:\n        number of negative point we mine.\n    """"""\n\n    def __init__(self, pos_thresh, neg_thresh, num_pos=5192, num_hn_samples=2048):\n        nn.Module.__init__(self)\n        self.pos_thresh = pos_thresh\n        self.neg_thresh = neg_thresh\n        self.num_pos = num_pos\n        self.num_hn_samples = num_hn_samples\n\n    def contrastive_hardest_negative_loss(self, F0, F1, positive_pairs, thresh=None):\n        """"""\n        Generate negative pairs\n        """"""\n        N0, N1 = len(F0), len(F1)\n        N_pos_pairs = len(positive_pairs)\n        hash_seed = max(N0, N1)\n        sel0 = np.random.choice(N0, min(N0, self.num_hn_samples), replace=False)\n        sel1 = np.random.choice(N1, min(N1, self.num_hn_samples), replace=False)\n\n        if N_pos_pairs > self.num_pos:\n            pos_sel = np.random.choice(N_pos_pairs, self.num_pos, replace=False)\n            sample_pos_pairs = positive_pairs[pos_sel]\n        else:\n            sample_pos_pairs = positive_pairs\n\n        # Find negatives for all F1[positive_pairs[:, 1]]\n        subF0, subF1 = F0[sel0], F1[sel1]\n\n        pos_ind0 = sample_pos_pairs[:, 0].long()\n        pos_ind1 = sample_pos_pairs[:, 1].long()\n        posF0, posF1 = F0[pos_ind0], F1[pos_ind1]\n\n        D01 = pdist(posF0, subF1, dist_type=""L2"")\n        D10 = pdist(posF1, subF0, dist_type=""L2"")\n\n        D01min, D01ind = D01.min(1)\n        D10min, D10ind = D10.min(1)\n\n        if not isinstance(positive_pairs, np.ndarray):\n            positive_pairs = np.array(positive_pairs, dtype=np.int64)\n\n        pos_keys = _hash(positive_pairs, hash_seed)\n\n        D01ind = sel1[D01ind.cpu().numpy()]\n        D10ind = sel0[D10ind.cpu().numpy()]\n        neg_keys0 = _hash([pos_ind0.numpy(), D01ind], hash_seed)\n        neg_keys1 = _hash([D10ind, pos_ind1.numpy()], hash_seed)\n\n        mask0 = torch.from_numpy(np.logical_not(np.isin(neg_keys0, pos_keys, assume_unique=False)))\n        mask1 = torch.from_numpy(np.logical_not(np.isin(neg_keys1, pos_keys, assume_unique=False)))\n        pos_loss = F.relu((posF0 - posF1).pow(2).sum(1) - self.pos_thresh)\n        neg_loss0 = F.relu(self.neg_thresh - D01min[mask0]).pow(2)\n        neg_loss1 = F.relu(self.neg_thresh - D10min[mask1]).pow(2)\n        return pos_loss.mean(), (neg_loss0.mean() + neg_loss1.mean()) / 2\n\n    def forward(self, F0, F1, matches, xyz0=None, xyz1=None):\n\n        pos_loss, neg_loss = self.contrastive_hardest_negative_loss(F0, F1, matches.detach().cpu())\n\n        return pos_loss + neg_loss\n\n\nclass BatchHardContrastiveLoss(nn.Module):\n    """"""\n        apply contrastive loss but mine the negative sample in the batch.\n    apply a mask if the distance between negative pair is too close.\n    Parameters\n    ----------\n    pos_thresh:\n        positive threshold of the positive distance\n    neg_thresh:\n        negative threshold of the negative distance\n    min_dist:\n        minimum distance to be in the negative sample\n    """"""\n\n    def __init__(self, pos_thresh, neg_thresh, min_dist=0.15):\n        nn.Module.__init__(self)\n        self.pos_thresh = pos_thresh\n        self.neg_thresh = neg_thresh\n        self.min_dist = min_dist\n\n    def forward(self, F0, F1, positive_pairs, xyz0=None, xyz1=None):\n\n        posF0 = F0[positive_pairs[:, 0]]\n        posF1 = F1[positive_pairs[:, 1]]\n\n        subxyz0 = xyz0[positive_pairs[:, 0]]\n        false_negative = pdist(subxyz0, subxyz0, dist_type=""L2"") > self.min_dist\n        # dists = pdist(posF0, posF1, dist_type=""L2"").view(-1)\n        furthest_pos, _ = (posF0 - posF1).pow(2).max(1)\n        neg_loss = F.relu(self.neg_thresh - (posF0[0] - posF1[false_negative[0]]).pow(2).sum(1).min()).pow(2) / len(\n            posF0\n        )\n\n        for i in range(1, len(posF0)):\n            neg_loss += F.relu(self.neg_thresh - (posF0[i] - posF1[false_negative[i]]).pow(2).sum(1).min()).pow(\n                2\n            ) / len(posF0)\n\n        pos_loss = F.relu(furthest_pos - self.pos_thresh).pow(2)\n        # neg_loss = F.relu(self.neg_thresh - closest_neg)\n        return pos_loss.mean() + neg_loss.mean()\n'"
torch_points3d/core/regularizer/__init__.py,0,b'from .regularizers import *\n'
torch_points3d/core/regularizer/regularizers.py,0,"b'from enum import Enum\n\n\nclass _Regularizer(object):\n    """"""\n    Parent class of Regularizers\n    """"""\n\n    def __init__(self, model):\n        super(_Regularizer, self).__init__()\n        self.model = model\n\n    def regularized_param(self, param_weights, reg_loss_function):\n        raise NotImplementedError\n\n    def regularized_all_param(self, reg_loss_function):\n        raise NotImplementedError\n\n\nclass L1Regularizer(_Regularizer):\n    """"""\n    L1 regularized loss\n    """"""\n\n    def __init__(self, model, lambda_reg=0.01):\n        super(L1Regularizer, self).__init__(model=model)\n        self.lambda_reg = lambda_reg\n\n    def regularized_param(self, param_weights, reg_loss_function):\n        reg_loss_function += self.lambda_reg * L1Regularizer.__add_l1(var=param_weights)\n        return reg_loss_function\n\n    def regularized_all_param(self, reg_loss_function):\n        for model_param_name, model_param_value in self.model.named_parameters():\n            if (\n                model_param_name.endswith(""weight"")\n                and ""1.weight"" not in model_param_name\n                and ""bn"" not in model_param_name\n            ):\n                reg_loss_function += self.lambda_reg * L1Regularizer.__add_l1(var=model_param_value)\n        return reg_loss_function\n\n    @staticmethod\n    def __add_l1(var):\n        return var.abs().sum()\n\n\nclass L2Regularizer(_Regularizer):\n    """"""\n       L2 regularized loss\n    """"""\n\n    def __init__(self, model, lambda_reg=0.01):\n        super(L2Regularizer, self).__init__(model=model)\n        self.lambda_reg = lambda_reg\n\n    def regularized_param(self, param_weights, reg_loss_function):\n        reg_loss_function += self.lambda_reg * L2Regularizer.__add_l2(var=param_weights)\n        return reg_loss_function\n\n    def regularized_all_param(self, reg_loss_function):\n        for model_param_name, model_param_value in self.model.named_parameters():\n            if (\n                model_param_name.endswith(""weight"")\n                and ""1.weight"" not in model_param_name\n                and ""bn"" not in model_param_name\n            ):\n                reg_loss_function += self.lambda_reg * L2Regularizer.__add_l2(var=model_param_value)\n        return reg_loss_function\n\n    @staticmethod\n    def __add_l2(var):\n        return var.pow(2).sum()\n\n\nclass ElasticNetRegularizer(_Regularizer):\n    """"""\n    Elastic Net Regularizer\n    """"""\n\n    def __init__(self, model, lambda_reg=0.01, alpha_reg=0.01):\n        super(ElasticNetRegularizer, self).__init__(model=model)\n        self.lambda_reg = lambda_reg\n        self.alpha_reg = alpha_reg\n\n    def regularized_param(self, param_weights, reg_loss_function):\n        reg_loss_function += self.lambda_reg * (\n            ((1 - self.alpha_reg) * ElasticNetRegularizer.__add_l2(var=param_weights))\n            + (self.alpha_reg * ElasticNetRegularizer.__add_l1(var=param_weights))\n        )\n        return reg_loss_function\n\n    def regularized_all_param(self, reg_loss_function):\n        for model_param_name, model_param_value in self.model.named_parameters():\n            if model_param_name.endswith(""weight""):\n                reg_loss_function += self.lambda_reg * (\n                    ((1 - self.alpha_reg) * ElasticNetRegularizer.__add_l2(var=model_param_value))\n                    + (self.alpha_reg * ElasticNetRegularizer.__add_l1(var=model_param_value))\n                )\n        return reg_loss_function\n\n    @staticmethod\n    def __add_l1(var):\n        return var.abs().sum()\n\n    @staticmethod\n    def __add_l2(var):\n        return var.pow(2).sum()\n\n\nclass GroupSparseLassoRegularizer(_Regularizer):\n    """"""\n    Group Sparse Lasso Regularizer\n    """"""\n\n    def __init__(self, model, lambda_reg=0.01):\n        super(GroupSparseLassoRegularizer, self).__init__(model=model)\n        self.lambda_reg = lambda_reg\n        self.reg_l2_l1 = GroupLassoRegularizer(model=self.model, lambda_reg=self.lambda_reg)\n        self.reg_l1 = L1Regularizer(model=self.model, lambda_reg=self.lambda_reg)\n\n    def regularized_param(self, param_weights, reg_loss_function):\n        reg_loss_function = self.lambda_reg * (\n            self.reg_l2_l1.regularized_param(param_weights=param_weights, reg_loss_function=reg_loss_function)\n            + self.reg_l1.regularized_param(param_weights=param_weights, reg_loss_function=reg_loss_function)\n        )\n\n        return reg_loss_function\n\n    def regularized_all_param(self, reg_loss_function):\n        reg_loss_function = self.lambda_reg * (\n            self.reg_l2_l1.regularized_all_param(reg_loss_function=reg_loss_function)\n            + self.reg_l1.regularized_all_param(reg_loss_function=reg_loss_function)\n        )\n\n        return reg_loss_function\n\n\nclass GroupLassoRegularizer(_Regularizer):\n    """"""\n    GroupLasso Regularizer:\n    The first dimension represents the input layer and the second dimension represents the output layer.\n    The groups are defined by the column in the matrix W\n    """"""\n\n    def __init__(self, model, lambda_reg=0.01):\n        super(GroupLassoRegularizer, self).__init__(model=model)\n        self.lambda_reg = lambda_reg\n\n    def regularized_param(self, param_weights, reg_loss_function, group_name=""input_group""):\n        if group_name == ""input_group"":\n            reg_loss_function += self.lambda_reg * GroupLassoRegularizer.__inputs_groups_reg(\n                layer_weights=param_weights\n            )  # apply the group norm on the input value\n        elif group_name == ""hidden_group"":\n            reg_loss_function += self.lambda_reg * GroupLassoRegularizer.__inputs_groups_reg(\n                layer_weights=param_weights\n            )  # apply the group norm on every hidden layer\n        elif group_name == ""bias_group"":\n            reg_loss_function += self.lambda_reg * GroupLassoRegularizer.__bias_groups_reg(\n                bias_weights=param_weights\n            )  # apply the group norm on the bias\n        else:\n            print(\n                ""The group {} is not supported yet. Please try one of this: [input_group, hidden_group, bias_group]"".format(\n                    group_name\n                )\n            )\n        return reg_loss_function\n\n    def regularized_all_param(self, reg_loss_function):\n        for model_param_name, model_param_value in self.model.named_parameters():\n            if model_param_name.endswith(""weight""):\n                reg_loss_function += self.lambda_reg * GroupLassoRegularizer.__inputs_groups_reg(\n                    layer_weights=model_param_value\n                )\n            if model_param_name.endswith(""bias""):\n                reg_loss_function += self.lambda_reg * GroupLassoRegularizer.__bias_groups_reg(\n                    bias_weights=model_param_value\n                )\n        return reg_loss_function\n\n    @staticmethod\n    def __grouplasso_reg(groups, dim):\n        if dim == -1:\n            # We only have single group\n            return groups.norm(2)\n        return groups.norm(2, dim=dim).sum()\n\n    @staticmethod\n    def __inputs_groups_reg(layer_weights):\n        return GroupLassoRegularizer.__grouplasso_reg(groups=layer_weights, dim=1)\n\n    @staticmethod\n    def __bias_groups_reg(bias_weights):\n        return GroupLassoRegularizer.__grouplasso_reg(groups=bias_weights, dim=-1)  # ou 0 i dont know yet\n\n\nclass RegularizerTypes(Enum):\n    L1 = L1Regularizer\n    L2 = L2Regularizer\n    ELASTIC = ElasticNetRegularizer\n'"
torch_points3d/core/schedulers/__init__.py,0,b'from .lr_schedulers import *\nfrom .bn_schedulers import *\n'
torch_points3d/core/schedulers/bn_schedulers.py,1,"b'from typing import *\nfrom torch import nn\nimport logging\n\ntry:\n    import MinkowskiEngine as ME\n\n    BATCH_NORM_MODULES: Any = (\n        nn.BatchNorm1d,\n        nn.BatchNorm2d,\n        nn.BatchNorm3d,\n        ME.MinkowskiBatchNorm,\n        ME.MinkowskiInstanceNorm,\n    )\nexcept:\n    BATCH_NORM_MODULES = (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)\n\n\nlog = logging.getLogger(__name__)\n\n\ndef set_bn_momentum_default(bn_momentum):\n    """"""\n    This function return a function which will assign `bn_momentum` to every module instance within `BATCH_NORM_MODULES`.\n    """"""\n\n    def fn(m):\n        if isinstance(m, BATCH_NORM_MODULES):\n            m.momentum = bn_momentum\n\n    return fn\n\n\nclass BNMomentumScheduler(object):\n    def __init__(self, model, bn_lambda, update_scheduler_on, last_epoch=-1, setter=set_bn_momentum_default):\n        if not isinstance(model, nn.Module):\n            raise RuntimeError(""Class \'{}\' is not a PyTorch nn Module"".format(type(model).__name__))\n\n        self.model = model\n        self.setter = setter\n        self.bn_lambda = bn_lambda\n        self.step(last_epoch + 1)\n        self.last_epoch = last_epoch\n        self._scheduler_opt = None\n        self._update_scheduler_on = update_scheduler_on\n\n    @property\n    def update_scheduler_on(self):\n        return self._update_scheduler_on\n\n    @property\n    def scheduler_opt(self):\n        return self._scheduler_opt\n\n    @scheduler_opt.setter\n    def scheduler_opt(self, scheduler_opt):\n        self._scheduler_opt = scheduler_opt\n\n    def step(self, epoch=None):\n\n        if epoch is None:\n            epoch = self.last_epoch + 1\n\n        self.last_epoch = epoch\n        current_momemtum = self.bn_lambda(epoch)\n        if not hasattr(self, ""current_momemtum""):\n            self._current_momemtum = current_momemtum\n        else:\n            if self._current_momemtum != current_momemtum:\n                self._current_momemtum = current_momemtum\n                log.info(""Setting batchnorm momentum at {}"".format(current_momemtum))\n        self.model.apply(self.setter(current_momemtum))\n\n    def state_dict(self):\n        return {\n            ""current_momemtum"": self.bn_lambda(self.last_epoch),\n            ""last_epoch"": self.last_epoch,\n        }\n\n    def load_state_dict(self, state_dict):\n        self.last_epoch = state_dict[""last_epoch""]\n        self.current_momemtum = state_dict[""current_momemtum""]\n\n    def __repr__(self):\n        return ""{}(base_momentum: {}, update_scheduler_on={})"".format(\n            self.__class__.__name__, self.bn_lambda(self.last_epoch), self._update_scheduler_on\n        )\n\n\ndef instantiate_bn_scheduler(model, bn_scheduler_opt):\n    """"""Return a batch normalization scheduler\n    Parameters:\n        model          -- the nn network\n        bn_scheduler_opt (option class) -- dict containing all the params to build the scheduler\xe3\x80\x80\n                              opt.bn_policy is the name of learning rate policy: lambda_rule | step | plateau | cosine\n                              opt.params contains the scheduler_params to construct the scheduler\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    """"""\n    update_scheduler_on = bn_scheduler_opt.update_scheduler_on\n    bn_scheduler_params = bn_scheduler_opt.params\n    if bn_scheduler_opt.bn_policy == ""step_decay"":\n        bn_lambda = lambda e: max(\n            bn_scheduler_params.bn_momentum\n            * bn_scheduler_params.bn_decay ** (int(e // bn_scheduler_params.decay_step)),\n            bn_scheduler_params.bn_clip,\n        )\n\n    else:\n        return NotImplementedError(""bn_policy [%s] is not implemented"", bn_scheduler_opt.bn_policy)\n\n    bn_scheduler = BNMomentumScheduler(model, bn_lambda, update_scheduler_on)\n    bn_scheduler.scheduler_opt = bn_scheduler_opt\n    return bn_scheduler\n'"
torch_points3d/core/schedulers/lr_schedulers.py,3,"b'import sys\nfrom torch.optim import lr_scheduler\nfrom omegaconf.dictconfig import DictConfig\nimport logging\nfrom torch.optim.lr_scheduler import LambdaLR\n\nfrom torch_points3d.utils.enums import SchedulerUpdateOn\n\nlog = logging.getLogger(__name__)\n\n_custom_lr_scheduler = sys.modules[__name__]\n\n\ndef collect_params(params, update_scheduler_on):\n    """"""\n    This function enable to handle if params contains on_epoch and on_iter or not.\n    """"""\n    on_epoch_params = params.get(""on_epoch"")\n    on_batch_params = params.get(""on_num_batch"")\n    on_sample_params = params.get(""on_num_sample"")\n\n    def check_params(params):\n        if params is not None:\n            return params\n        else:\n            raise Exception(\n                ""The lr_scheduler doesn\'t have policy {}. Options: {}"".format(update_scheduler_on, SchedulerUpdateOn)\n            )\n\n    if on_epoch_params or on_batch_params or on_sample_params:\n        if update_scheduler_on == SchedulerUpdateOn.ON_EPOCH.value:\n            return check_params(on_epoch_params)\n        elif update_scheduler_on == SchedulerUpdateOn.ON_NUM_BATCH.value:\n            return check_params(on_batch_params)\n        elif update_scheduler_on == SchedulerUpdateOn.ON_NUM_SAMPLE.value:\n            return check_params(on_sample_params)\n        else:\n            raise Exception(\n                ""The provided update_scheduler_on {} isn\'t within {}"".format(update_scheduler_on, SchedulerUpdateOn)\n            )\n    else:\n        return params\n\n\nclass LambdaStepLR(LambdaLR):\n    def __init__(self, optimizer, lr_lambda, last_step=-1):\n        super(LambdaStepLR, self).__init__(optimizer, lr_lambda, last_step)\n\n    @property\n    def last_step(self):\n        """"""Use last_epoch for the step counter""""""\n        return self.last_epoch\n\n    @last_step.setter\n    def last_step(self, v):\n        self.last_epoch = v\n\n\nclass PolyLR(LambdaStepLR):\n    """"""DeepLab learning rate policy""""""\n\n    def __init__(self, optimizer, max_iter, power=0.9, last_step=-1):\n        lambda_func = lambda s: (1 - s / (max_iter + 1)) ** power\n        composite_func = lambda s: lambda_func(max_iter) if s > max_iter else lambda_func(s)\n        super(PolyLR, self).__init__(optimizer, lambda s: composite_func(s), last_step)\n\n\nclass SquaredLR(LambdaStepLR):\n    """""" Used for SGD Lars""""""\n\n    def __init__(self, optimizer, max_iter, last_step=-1):\n        super(SquaredLR, self).__init__(optimizer, lambda s: (1 - s / (max_iter + 1)) ** 2, last_step)\n\n\nclass ExpLR(LambdaStepLR):\n    def __init__(self, optimizer, step_size, gamma=0.9, last_step=-1):\n        # (0.9 ** 21.854) = 0.1, (0.95 ** 44.8906) = 0.1\n        # To get 0.1 every N using gamma 0.9, N * log(0.9)/log(0.1) = 0.04575749 N\n        # To get 0.1 every N using gamma g, g ** N = 0.1 -> N * log(g) = log(0.1) -> g = np.exp(log(0.1) / N)\n        super(ExpLR, self).__init__(optimizer, lambda s: gamma ** (s / step_size), last_step)\n\n\ndef repr(self, scheduler_params={}):\n    return ""{}({})"".format(self.__class__.__name__, scheduler_params)\n\n\nclass LRScheduler:\n    def __init__(self, scheduler, scheduler_params, update_scheduler_on):\n        self._scheduler = scheduler\n        self._scheduler_params = scheduler_params\n        self._update_scheduler_on = update_scheduler_on\n\n    @property\n    def scheduler(self):\n        return self._scheduler\n\n    @property\n    def scheduler_opt(self):\n        return self._scheduler._scheduler_opt\n\n    def __repr__(self):\n        return ""{}({}, update_scheduler_on={})"".format(\n            self._scheduler.__class__.__name__, self._scheduler_params, self._update_scheduler_on\n        )\n\n    def step(self, *args, **kwargs):\n        self._scheduler.step(*args, **kwargs)\n\n    def state_dict(self):\n        return self._scheduler.state_dict()\n\n    def load_state_dict(self, state_dict):\n        self._scheduler.load_state_dict(state_dict)\n\n\ndef instantiate_scheduler(optimizer, scheduler_opt):\n    """"""Return a learning rate scheduler\n    Parameters:\n        optimizer          -- the optimizer of the network\n        scheduler_opt (option class) -- dict containing all the params to build the scheduler\xe3\x80\x80\n                              opt.lr_policy is the name of learning rate policy: lambda_rule | step | plateau | cosine\n                              opt.params contains the scheduler_params to construct the scheduler\n    See https://pytorch.org/docs/stable/optim.html for more details.\n    """"""\n\n    update_scheduler_on = scheduler_opt.update_scheduler_on\n    scheduler_cls_name = getattr(scheduler_opt, ""class"")\n    scheduler_params = collect_params(scheduler_opt.params, update_scheduler_on)\n\n    try:\n        scheduler_cls = getattr(lr_scheduler, scheduler_cls_name)\n    except:\n        scheduler_cls = getattr(_custom_lr_scheduler, scheduler_cls_name)\n        log.info(""Created custom lr scheduler"")\n\n    if scheduler_cls_name.lower() == ""ReduceLROnPlateau"".lower():\n        raise NotImplementedError(""This scheduler is not fully supported yet"")\n\n    scheduler = scheduler_cls(optimizer, **scheduler_params)\n    # used to re_create the scheduler\n\n    setattr(scheduler, ""_scheduler_opt"", scheduler_opt)\n    return LRScheduler(scheduler, scheduler_params, update_scheduler_on)\n'"
torch_points3d/core/spatial_ops/__init__.py,0,b'from .neighbour_finder import *\nfrom .sampling import *\nfrom .interpolate import *\n'
torch_points3d/core/spatial_ops/interpolate.py,7,"b'import torch\nfrom torch_geometric.nn import knn_interpolate, knn\nfrom torch_scatter import scatter_add\nfrom torch_geometric.data import Data\n\n\nclass KNNInterpolate:\n    def __init__(self, k):\n        self.k = k\n\n    def precompute(self, query, support):\n        """""" Precomputes a data structure that can be used in the transform itself to speed things up\n        """"""\n        pos_x, pos_y = query.pos, support.pos\n        if hasattr(support, ""batch""):\n            batch_y = support.batch\n        else:\n            batch_y = torch.zeros((support.num_nodes,), dtype=torch.long)\n        if hasattr(query, ""batch""):\n            batch_x = query.batch\n        else:\n            batch_x = torch.zeros((query.num_nodes,), dtype=torch.long)\n\n        with torch.no_grad():\n            assign_index = knn(pos_x, pos_y, self.k, batch_x=batch_x, batch_y=batch_y)\n            y_idx, x_idx = assign_index\n            diff = pos_x[x_idx] - pos_y[y_idx]\n            squared_distance = (diff * diff).sum(dim=-1, keepdim=True)\n            weights = 1.0 / torch.clamp(squared_distance, min=1e-16)\n        normalisation = scatter_add(weights, y_idx, dim=0, dim_size=pos_y.size(0))\n\n        return Data(num_nodes=support.num_nodes, x_idx=x_idx, y_idx=y_idx, weights=weights, normalisation=normalisation)\n\n    def __call__(self, query, support, precomputed: Data = None):\n        """""" Computes a new set of features going from the query resolution position to the support\n        resolution position\n        Args:\n            - query: data structure that holds the low res data (position + features)\n            - support: data structure that holds the position to which we will interpolate\n        Returns:\n            - torch.tensor: interpolated features\n        """"""\n        if precomputed:\n            num_points = support.pos.size(0)\n            if num_points != precomputed.num_nodes:\n                raise ValueError(""Precomputed indices do not match with the data given to the transform"")\n\n            x = query.x\n            x_idx, y_idx, weights, normalisation = (\n                precomputed.x_idx,\n                precomputed.y_idx,\n                precomputed.weights,\n                precomputed.normalisation,\n            )\n            y = scatter_add(x[x_idx] * weights, y_idx, dim=0, dim_size=num_points)\n            y = y / normalisation\n            return y\n\n        x, pos = query.x, query.pos\n        pos_support = support.pos\n        if hasattr(support, ""batch""):\n            batch_support = support.batch\n        else:\n            batch_support = torch.zeros((support.num_nodes,), dtype=torch.long)\n        if hasattr(query, ""batch""):\n            batch = query.batch\n        else:\n            batch = torch.zeros((query.num_nodes,), dtype=torch.long)\n\n        return knn_interpolate(x, pos, pos_support, batch, batch_support, k=self.k)\n'"
torch_points3d/core/spatial_ops/neighbour_finder.py,2,"b'from abc import ABC, abstractmethod\nfrom typing import List, Union, cast\nimport torch\nfrom torch_geometric.nn import knn, radius\nimport torch_points_kernels as tp\n\nfrom torch_points3d.utils.config import is_list\nfrom torch_points3d.utils.enums import ConvolutionFormat\n\nfrom torch_points3d.utils.debugging_vars import DEBUGGING_VARS, DistributionNeighbour\n\n\nclass BaseNeighbourFinder(ABC):\n    def __call__(self, x, y, batch_x, batch_y):\n        return self.find_neighbours(x, y, batch_x, batch_y)\n\n    @abstractmethod\n    def find_neighbours(self, x, y, batch_x, batch_y):\n        pass\n\n    def __repr__(self):\n        return str(self.__class__.__name__) + "" "" + str(self.__dict__)\n\n\nclass RadiusNeighbourFinder(BaseNeighbourFinder):\n    def __init__(self, radius: float, max_num_neighbors: int = 64, conv_type=ConvolutionFormat.MESSAGE_PASSING.value):\n        self._radius = radius\n        self._max_num_neighbors = max_num_neighbors\n        self._conv_type = conv_type.lower()\n\n    def find_neighbours(self, x, y, batch_x=None, batch_y=None):\n        if self._conv_type == ConvolutionFormat.MESSAGE_PASSING.value:\n            return radius(x, y, self._radius, batch_x, batch_y, max_num_neighbors=self._max_num_neighbors)\n        elif self._conv_type == ConvolutionFormat.DENSE.value or ConvolutionFormat.PARTIAL_DENSE.value:\n            return tp.ball_query(\n                self._radius, self._max_num_neighbors, x, y, mode=self._conv_type, batch_x=batch_x, batch_y=batch_y\n            )[0]\n        else:\n            raise NotImplementedError\n\n\nclass KNNNeighbourFinder(BaseNeighbourFinder):\n    def __init__(self, k):\n        self.k = k\n\n    def find_neighbours(self, x, y, batch_x, batch_y):\n        return knn(x, y, self.k, batch_x, batch_y)\n\n\nclass DilatedKNNNeighbourFinder(BaseNeighbourFinder):\n    def __init__(self, k, dilation):\n        self.k = k\n        self.dilation = dilation\n        self.initialFinder = KNNNeighbourFinder(k * dilation)\n\n    def find_neighbours(self, x, y, batch_x, batch_y):\n        # find the self.k * self.dilation closest neighbours in x for each y\n        row, col = self.initialFinder.find_neighbours(x, y, batch_x, batch_y)\n\n        # for each point in y, randomly select k of its neighbours\n        index = torch.randint(self.k * self.dilation, (len(y), self.k), device=row.device, dtype=torch.long,)\n\n        arange = torch.arange(len(y), dtype=torch.long, device=row.device)\n        arange = arange * (self.k * self.dilation)\n        index = (index + arange.view(-1, 1)).view(-1)\n        row, col = row[index], col[index]\n\n        return row, col\n\n\nclass BaseMSNeighbourFinder(ABC):\n    def __call__(self, x, y, batch_x=None, batch_y=None, scale_idx=0):\n        return self.find_neighbours(x, y, batch_x=batch_x, batch_y=batch_y, scale_idx=scale_idx)\n\n    @abstractmethod\n    def find_neighbours(self, x, y, batch_x=None, batch_y=None, scale_idx=0):\n        pass\n\n    @property\n    @abstractmethod\n    def num_scales(self):\n        pass\n\n    @property\n    def dist_meters(self):\n        return getattr(self, ""_dist_meters"", None)\n\n\nclass MultiscaleRadiusNeighbourFinder(BaseMSNeighbourFinder):\n    """""" Radius search with support for multiscale for sparse graphs\n\n        Arguments:\n            radius {Union[float, List[float]]}\n\n        Keyword Arguments:\n            max_num_neighbors {Union[int, List[int]]}  (default: {64})\n\n        Raises:\n            ValueError: [description]\n    """"""\n\n    def __init__(\n        self, radius: Union[float, List[float]], max_num_neighbors: Union[int, List[int]] = 64,\n    ):\n        if DEBUGGING_VARS[""FIND_NEIGHBOUR_DIST""]:\n            if not isinstance(radius, list):\n                radius = [radius]\n            self._dist_meters = [DistributionNeighbour(r) for r in radius]\n            if not isinstance(max_num_neighbors, list):\n                max_num_neighbors = [max_num_neighbors]\n            max_num_neighbors = [256 for _ in max_num_neighbors]\n\n        if not is_list(max_num_neighbors) and is_list(radius):\n            self._radius = cast(list, radius)\n            max_num_neighbors = cast(int, max_num_neighbors)\n            self._max_num_neighbors = [max_num_neighbors for i in range(len(self._radius))]\n            return\n\n        if not is_list(radius) and is_list(max_num_neighbors):\n            self._max_num_neighbors = cast(list, max_num_neighbors)\n            radius = cast(int, radius)\n            self._radius = [radius for i in range(len(self._max_num_neighbors))]\n            return\n\n        if is_list(max_num_neighbors):\n            max_num_neighbors = cast(list, max_num_neighbors)\n            radius = cast(list, radius)\n            if len(max_num_neighbors) != len(radius):\n                raise ValueError(""Both lists max_num_neighbors and radius should be of the same length"")\n            self._max_num_neighbors = max_num_neighbors\n            self._radius = radius\n            return\n\n        self._max_num_neighbors = [cast(int, max_num_neighbors)]\n        self._radius = [cast(int, radius)]\n\n    def find_neighbours(self, x, y, batch_x=None, batch_y=None, scale_idx=0):\n        if scale_idx >= self.num_scales:\n            raise ValueError(""Scale %i is out of bounds %i"" % (scale_idx, self.num_scales))\n\n        radius_idx = radius(\n            x, y, self._radius[scale_idx], batch_x, batch_y, max_num_neighbors=self._max_num_neighbors[scale_idx]\n        )\n        return radius_idx\n\n    @property\n    def num_scales(self):\n        return len(self._radius)\n\n    def __call__(self, x, y, batch_x=None, batch_y=None, scale_idx=0):\n        """""" Sparse interface of the neighboorhood finder\n        """"""\n        return self.find_neighbours(x, y, batch_x, batch_y, scale_idx)\n\n\nclass DenseRadiusNeighbourFinder(MultiscaleRadiusNeighbourFinder):\n    """""" Multiscale radius search for dense graphs\n    """"""\n\n    def find_neighbours(self, x, y, scale_idx=0):\n        if scale_idx >= self.num_scales:\n            raise ValueError(""Scale %i is out of bounds %i"" % (scale_idx, self.num_scales))\n        num_neighbours = self._max_num_neighbors[scale_idx]\n        neighbours = tp.ball_query(self._radius[scale_idx], num_neighbours, x, y)[0]\n\n        if DEBUGGING_VARS[""FIND_NEIGHBOUR_DIST""]:\n            for i in range(neighbours.shape[0]):\n                start = neighbours[i, :, 0]\n                valid_neighbours = (neighbours[i, :, 1:] != start.view((-1, 1)).repeat(1, num_neighbours - 1)).sum(\n                    1\n                ) + 1\n                self._dist_meters[scale_idx].add_valid_neighbours(valid_neighbours)\n        return neighbours\n\n    def __call__(self, x, y, scale_idx=0, **kwargs):\n        """""" Dense interface of the neighboorhood finder\n        """"""\n        return self.find_neighbours(x, y, scale_idx)\n'"
torch_points3d/core/spatial_ops/sampling.py,2,"b'from abc import ABC, abstractmethod\nimport math\nimport torch\nfrom torch_geometric.nn import voxel_grid\nfrom torch_geometric.nn.pool.consecutive import consecutive_cluster\nfrom torch_geometric.nn.pool.pool import pool_pos, pool_batch\nimport torch_points_kernels as tp\n\nfrom torch_points3d.utils.config import is_list\nfrom torch_points3d.utils.enums import ConvolutionFormat\n\n\nclass BaseSampler(ABC):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n    """"""\n\n    def __init__(self, ratio=None, num_to_sample=None, subsampling_param=None):\n        if num_to_sample is not None:\n            if (ratio is not None) or (subsampling_param is not None):\n                raise ValueError(""Can only specify ratio or num_to_sample or subsampling_param, not several !"")\n            self._num_to_sample = num_to_sample\n\n        elif ratio is not None:\n            self._ratio = ratio\n\n        elif subsampling_param is not None:\n            self._subsampling_param = subsampling_param\n\n        else:\n            raise Exception(\'At least [""ratio, num_to_sample, subsampling_param""] should be defined\')\n\n    def __call__(self, pos, x=None, batch=None):\n        return self.sample(pos, batch=batch, x=x)\n\n    def _get_num_to_sample(self, batch_size) -> int:\n        if hasattr(self, ""_num_to_sample""):\n            return self._num_to_sample\n        else:\n            return math.floor(batch_size * self._ratio)\n\n    def _get_ratio_to_sample(self, batch_size) -> float:\n        if hasattr(self, ""_ratio""):\n            return self._ratio\n        else:\n            return self._num_to_sample / float(batch_size)\n\n    @abstractmethod\n    def sample(self, pos, x=None, batch=None):\n        pass\n\n\nclass FPSSampler(BaseSampler):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n    """"""\n\n    def sample(self, pos, batch, **kwargs):\n        from torch_geometric.nn import fps\n\n        if len(pos.shape) != 2:\n            raise ValueError("" This class is for sparse data and expects the pos tensor to be of dimension 2"")\n        return fps(pos, batch, ratio=self._get_ratio_to_sample(pos.shape[0]))\n\n\nclass GridSampler(BaseSampler):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n    """"""\n\n    def sample(self, pos=None, x=None, batch=None):\n        if len(pos.shape) != 2:\n            raise ValueError(""This class is for sparse data and expects the pos tensor to be of dimension 2"")\n\n        pool = voxel_grid(pos, batch, self._subsampling_param)\n        pool, perm = consecutive_cluster(pool)\n        batch = pool_batch(perm, batch)\n        if x is not None:\n            return pool_pos(pool, x), pool_pos(pool, pos), batch\n        else:\n            return None, pool_pos(pool, pos), batch\n\n\nclass DenseFPSSampler(BaseSampler):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n    """"""\n\n    def sample(self, pos, **kwargs):\n        """""" Sample pos\n\n        Arguments:\n            pos -- [B, N, 3]\n\n        Returns:\n            indexes -- [B, num_sample]\n        """"""\n        if len(pos.shape) != 3:\n            raise ValueError("" This class is for dense data and expects the pos tensor to be of dimension 2"")\n        return tp.furthest_point_sample(pos, self._get_num_to_sample(pos.shape[1]))\n\n\nclass RandomSampler(BaseSampler):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n    """"""\n\n    def sample(self, pos, batch, **kwargs):\n        if len(pos.shape) != 2:\n            raise ValueError("" This class is for sparse data and expects the pos tensor to be of dimension 2"")\n        idx = torch.randint(0, pos.shape[0], (self._get_num_to_sample(pos.shape[0]),))\n        return idx\n\n\nclass DenseRandomSampler(BaseSampler):\n    """"""If num_to_sample is provided, sample exactly\n        num_to_sample points. Otherwise sample floor(pos[0] * ratio) points\n        Arguments:\n            pos -- [B, N, 3]\n    """"""\n\n    def sample(self, pos, **kwargs):\n        if len(pos.shape) != 3:\n            raise ValueError("" This class is for dense data and expects the pos tensor to be of dimension 2"")\n        idx = torch.randint(0, pos.shape[1], (self._get_num_to_sample(pos.shape[1]),))\n        return idx\n'"
torch_points3d/datasets/classification/__init__.py,0,b''
torch_points3d/datasets/classification/modelnet.py,4,"b'import os.path as osp\nimport os\nimport shutil\nimport torch\n\nfrom torch_geometric.datasets import ModelNet\nfrom torch_geometric.data import DataLoader, InMemoryDataset, download_url, extract_zip, Data\nimport torch_geometric.transforms as T\nfrom torch_geometric.io import read_txt_array\n\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.metrics.classification_tracker import ClassificationTracker\n\n\nclass SampledModelNet(InMemoryDataset):\n    r""""""The ModelNet10/40 dataset from the `""3D ShapeNets: A Deep\n    Representation for Volumetric Shapes""\n    <https://people.csail.mit.edu/khosla/papers/cvpr2015_wu.pdf>`_ paper,\n    containing sampled CAD models of 40 categories. Each sample contains 10,000\n    points uniformly sampled with their normal vector.\n\n    .. note::\n\n        Data objects hold mesh faces instead of edge indices.\n        To convert the mesh to a graph, use the\n        :obj:`torch_geometric.transforms.FaceToEdge` as :obj:`pre_transform`.\n        To convert the mesh to a point cloud, use the\n        :obj:`torch_geometric.transforms.SamplePoints` as :obj:`transform` to\n        sample a fixed number of points on the mesh faces according to their\n        face area.\n\n    Parameters:\n    ------------\n    root (string): Root directory where the dataset should be saved.\n    name (string, optional): The name of the dataset (:obj:`""10""` for\n        ModelNet10, :obj:`""40""` for ModelNet40). (default: :obj:`""10""`)\n    train (bool, optional): If :obj:`True`, loads the training dataset,\n        otherwise the test dataset. (default: :obj:`True`)\n    transform (callable, optional): A function/transform that takes in an\n        :obj:`torch_geometric.data.Data` object and returns a transformed\n        version. The data object will be transformed before every access.\n        (default: :obj:`None`)\n    pre_transform (callable, optional): A function/transform that takes in\n        an :obj:`torch_geometric.data.Data` object and returns a\n        transformed version. The data object will be transformed before\n        being saved to disk. (default: :obj:`None`)\n    pre_filter (callable, optional): A function that takes in an\n        :obj:`torch_geometric.data.Data` object and returns a boolean\n        value, indicating whether the data object should be included in the\n        final dataset. (default: :obj:`None`)\n    """"""\n\n    url = ""https://shapenet.cs.stanford.edu/media/modelnet40_normal_resampled.zip""\n\n    def __init__(self, root, name=""10"", train=True, transform=None, pre_transform=None, pre_filter=None):\n        assert name in [""10"", ""40""]\n        self.name = name\n        super(SampledModelNet, self).__init__(root, transform, pre_transform, pre_filter)\n        path = self.processed_paths[0] if train else self.processed_paths[1]\n        self.data, self.slices = torch.load(path)\n\n    @property\n    def raw_file_names(self):\n        return [""bathtub"", ""bed"", ""chair"", ""desk"", ""dresser"", ""monitor"", ""night_stand"", ""sofa"", ""table"", ""toilet""]\n\n    @property\n    def processed_file_names(self):\n        return [""training_{}.pt"".format(self.name), ""test_{}.pt"".format(self.name)]\n\n    def download(self):\n        path = download_url(self.url, self.root)\n        extract_zip(path, self.root)\n        os.unlink(path)\n        folder = osp.join(self.root, ""modelnet40_normal_resampled"")\n        shutil.rmtree(self.raw_dir)\n        os.rename(folder, self.raw_dir)\n\n    def process(self):\n        torch.save(self.process_set(""train""), self.processed_paths[0])\n        torch.save(self.process_set(""test""), self.processed_paths[1])\n\n    def process_set(self, dataset):\n        with open(osp.join(self.raw_dir, ""modelnet{}_shape_names.txt"".format(self.name)), ""r"") as f:\n            categories = f.read().splitlines()\n            categories = sorted(categories)\n        with open(osp.join(self.raw_dir, ""modelnet{}_{}.txt"".format(self.name, dataset)), ""r"") as f:\n            split_objects = f.read().splitlines()\n\n        data_list = []\n        for target, category in enumerate(categories):\n            folder = osp.join(self.raw_dir, category)\n            category_ojects = filter(lambda o: category in o, split_objects)\n            paths = [""{}/{}.txt"".format(folder, o.strip()) for o in category_ojects]\n            for path in paths:\n                raw = read_txt_array(path, sep="","")\n                data = Data(pos=raw[:, :3], norm=raw[:, 3:], y=torch.tensor([target]))\n                data_list.append(data)\n\n        if self.pre_filter is not None:\n            data_list = [d for d in data_list if self.pre_filter(d)]\n\n        if self.pre_transform is not None:\n            data_list = [self.pre_transform(d) for d in data_list]\n\n        return self.collate(data_list)\n\n    def __repr__(self):\n        return ""{}{}({})"".format(self.__class__.__name__, self.name, len(self))\n\n\nclass ModelNetDataset(BaseDataset):\n\n    AVAILABLE_NUMBERS = [""10"", ""40""]\n\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n\n        number = dataset_opt.number\n        if str(number) not in self.AVAILABLE_NUMBERS:\n            raise Exception(""Only ModelNet10 and ModelNet40 are available"")\n        self.train_dataset = SampledModelNet(\n            self._data_path,\n            name=str(number),\n            train=True,\n            transform=self.train_transform,\n            pre_transform=self.pre_transform,\n        )\n        self.test_dataset = SampledModelNet(\n            self._data_path,\n            name=str(number),\n            train=False,\n            transform=self.test_transform,\n            pre_transform=self.pre_transform,\n        )\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        return ClassificationTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n'"
torch_points3d/datasets/object_detection/__init__.py,0,b''
torch_points3d/datasets/object_detection/scannet.py,11,"b'import os\nimport numpy as np\nimport torch\nimport logging\nfrom torch_geometric.data import InMemoryDataset\nfrom torch_points3d.datasets.segmentation.scannet import Scannet, NUM_CLASSES, IGNORE_LABEL\nfrom torch_points3d.metrics.object_detection_tracker import ObjectDetectionTracker\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\nDIR = os.path.dirname(os.path.realpath(__file__))\n\nlog = logging.getLogger(__name__)\n\n\nclass ScannetObjectDetection(Scannet):\n\n    MAX_NUM_OBJ = 64\n    NUM_HEADING_BIN = 1\n    TYPE2CLASS = {\n        ""cabinet"": 0,\n        ""bed"": 1,\n        ""chair"": 2,\n        ""sofa"": 3,\n        ""table"": 4,\n        ""door"": 5,\n        ""window"": 6,\n        ""bookshelf"": 7,\n        ""picture"": 8,\n        ""counter"": 9,\n        ""desk"": 10,\n        ""curtain"": 11,\n        ""refrigerator"": 12,\n        ""showercurtrain"": 13,\n        ""toilet"": 14,\n        ""sink"": 15,\n        ""bathtub"": 16,\n        ""garbagebin"": 17,\n    }\n    NYU40IDS = np.array([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36, 39])\n    MEAN_COLOR_RGB = np.array([109.8, 97.2, 83.8])\n\n    def __init__(self, *args, **kwargs):\n        super(ScannetObjectDetection, self).__init__(*args, **kwargs)\n\n        self.CLASS2TYPE = {self.TYPE2CLASS[t]: t for t in self.TYPE2CLASS}\n        self.NYU40ID2CLASS = {nyu40id: i for i, nyu40id in enumerate(list(self.NYU40IDS))}\n        self.MEAN_SIZE_ARR = np.load(os.path.join(DIR, ""scannet_metadata/scannet_means.npz""))[""arr_0""]\n        self.TYPE_MEAN_SIZE = {}\n        for i in range(len(self.NYU40IDS)):\n            self.TYPE_MEAN_SIZE[self.CLASS2TYPE[i]] = self.MEAN_SIZE_ARR[i, :]\n\n    def __getitem__(self, idx):\n        """"""\n        Data object contains:\n            pos - points\n            x - features\n        """"""\n        if not isinstance(idx, int):\n            raise ValueError(""Only integer indices supported"")\n\n        # Get raw data and apply transforms\n        data = self.get(idx)\n        data = data if self.transform is None else self.transform(data)\n\n        # Extract instance and box labels\n        self._set_extra_labels(data)\n        return data\n\n    def _set_extra_labels(self, data):\n        """""" Adds extra labels for the instance and object segmentation tasks\n\n        center_label: (MAX_NUM_OBJ,3) for GT box center XYZ\n        sem_cls_label: (MAX_NUM_OBJ,) semantic class index\n        angle_residual_label: (MAX_NUM_OBJ,)\n        size_residual_label: (MAX_NUM_OBJ,3)\n        box_label_mask: (MAX_NUM_OBJ) as 0/1 with 1 indicating a unique box\n        vote_label: (N,3) with votes XYZ\n        vote_label_mask: (N,) with 0/1 with 1 indicating the point is in one of the object\'s OBB.\n        """"""\n        # Initaliase variables\n        num_points = data.pos.shape[0]\n        semantic_labels = data.y\n        instance_labels = data.instance_labels\n        instance_bboxes = data.instance_bboxes\n\n        target_bboxes = torch.zeros((self.MAX_NUM_OBJ, 6))\n        target_bboxes_mask = torch.zeros((self.MAX_NUM_OBJ), dtype=torch.bool)\n        angle_residuals = torch.zeros((self.MAX_NUM_OBJ,))\n        size_classes = torch.zeros((self.MAX_NUM_OBJ,))\n        size_residuals = torch.zeros((self.MAX_NUM_OBJ, 3))\n\n        # Keep only boxes with valid ids\n        bbox_mask = np.in1d(instance_bboxes[:, -1], self.NYU40IDS)\n        instance_bboxes = instance_bboxes[bbox_mask, :]\n        target_bboxes_mask[0 : instance_bboxes.shape[0]] = True\n        target_bboxes[0 : instance_bboxes.shape[0], :] = instance_bboxes[:, 0:6]  # TODO handle data augmentation\n\n        # compute votes *AFTER* augmentation\n        # generate votes\n        # Note: since there\'s no map between bbox instance labels and\n        # pc instance_labels (it had been filtered\n        # in the data preparation step) we\'ll compute the instance bbox\n        # from the points sharing the same instance label.\n        point_votes = torch.zeros([num_points, 3])\n        point_votes_mask = torch.zeros(num_points, dtype=torch.bool)\n        for i_instance in np.unique(instance_labels):\n            # find all points belong to that instance\n            ind = np.where(instance_labels == i_instance)[0]\n            # find the semantic label\n            instance_class = semantic_labels[ind[0]].item()\n            if instance_class in self.NYU40IDS:\n                x = data.pos[ind, :3]\n                center = 0.5 * (x.min(0)[0] + x.max(0)[0])\n                point_votes[ind, :] = center - x\n                point_votes_mask[ind] = True\n        point_votes = point_votes.repeat((1, 3))  # make 3 votes identical\n\n        # NOTE: set size class as semantic class. Consider use size2class.\n        class_ind = np.asarray([np.where(self.NYU40IDS == x)[0][0] for x in np.asarray(instance_bboxes[:, -1])])\n        size_classes[0 : instance_bboxes.shape[0]] = torch.from_numpy(class_ind)\n        if len(class_ind):\n            size_residuals[0 : instance_bboxes.shape[0], :] = target_bboxes[\n                0 : instance_bboxes.shape[0], 3:6\n            ] - torch.from_numpy(self.MEAN_SIZE_ARR[class_ind, :])\n\n        target_bboxes_semcls = np.zeros((self.MAX_NUM_OBJ))\n        target_bboxes_semcls[0 : instance_bboxes.shape[0]] = [\n            self.NYU40ID2CLASS[x.item()] for x in instance_bboxes[:, -1][0 : instance_bboxes.shape[0]]\n        ]\n\n        data.center_label = target_bboxes.float()[:, 0:3]\n        data.heading_class_label = torch.zeros((self.MAX_NUM_OBJ,))\n        data.heading_residual_label = angle_residuals.float()\n        data.size_class_label = size_classes\n        data.size_residual_label = size_residuals.float()\n        data.sem_cls_label = torch.from_numpy(target_bboxes_semcls).int()\n        data.box_label_mask = target_bboxes_mask\n        data.vote_label = point_votes.float()\n        data.vote_label_mask = point_votes_mask\n        delattr(data, ""instance_bboxes"")\n        delattr(data, ""instance_labels"")\n        delattr(data, ""y"")\n        return data\n\n    def _remap_labels(self, data):\n        log.info(""Keeping original labels in y. Please do not use data.y in your network."")\n        return data\n\n    def process(self):\n        super().process()\n\n    def download(self):\n        super().download()\n\n\nclass ScannetDataset(BaseDataset):\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n\n        use_instance_labels: bool = dataset_opt.use_instance_labels\n        use_instance_bboxes: bool = dataset_opt.use_instance_bboxes\n        donotcare_class_ids: [] = dataset_opt.donotcare_class_ids if dataset_opt.donotcare_class_ids else []\n        max_num_point: int = dataset_opt.max_num_point if dataset_opt.max_num_point != ""None"" else None\n\n        self.train_dataset = ScannetObjectDetection(\n            self._data_path,\n            split=""train"",\n            pre_transform=self.pre_transform,\n            transform=self.train_transform,\n            version=dataset_opt.version,\n            use_instance_labels=use_instance_labels,\n            use_instance_bboxes=use_instance_bboxes,\n            donotcare_class_ids=donotcare_class_ids,\n            max_num_point=max_num_point,\n        )\n\n        self.val_dataset = ScannetObjectDetection(\n            self._data_path,\n            split=""val"",\n            transform=self.val_transform,\n            pre_transform=self.pre_transform,\n            version=dataset_opt.version,\n            use_instance_labels=use_instance_labels,\n            use_instance_bboxes=use_instance_bboxes,\n            donotcare_class_ids=donotcare_class_ids,\n            max_num_point=max_num_point,\n        )\n\n    @property\n    def mean_size_arr(self):\n        return self.train_dataset.MEAN_SIZE_ARR.copy()\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n\n        Arguments:\n            wandb_log - Log using weight and biases\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        return ObjectDetectionTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n'"
torch_points3d/datasets/registration/base3dmatch.py,11,"b'import json\nimport logging\nimport numpy as np\nimport os\nimport os.path as osp\nfrom plyfile import PlyData\nimport shutil\nimport torch\n\nfrom torch_geometric.data import Dataset, download_url, extract_zip\nfrom torch_geometric.data import Data\nfrom torch_points3d.datasets.registration.detector import RandomDetector\nfrom torch_points3d.datasets.registration.utils import rgbd2fragment_rough\nfrom torch_points3d.datasets.registration.utils import rgbd2fragment_fine\nfrom torch_points3d.datasets.registration.utils import compute_overlap_and_matches\nfrom torch_points3d.datasets.registration.utils import to_list\nfrom torch_points3d.datasets.registration.utils import files_exist\nfrom torch_points3d.datasets.registration.utils import makedirs\nfrom torch_points3d.datasets.registration.utils import get_urls\nfrom torch_points3d.datasets.registration.utils import PatchExtractor\n\nlog = logging.getLogger(__name__)\n\n\nclass Base3DMatch(Dataset):\n\n    base = osp.abspath(osp.join(osp.realpath(__file__),\n                                \'..\'))\n    list_urls_train = get_urls(osp.join(base, \'urls\', \'url_train.txt\'))\n    list_urls_train_small = get_urls(osp.join(base, \'urls\', \'url_train_small.txt\'))\n    list_urls_train_tiny = get_urls(osp.join(base, \'urls\', \'url_train_tiny.txt\'))\n    list_urls_val = get_urls(osp.join(base, \'urls\', \'url_val.txt\'))\n    dict_urls = dict(train=list_urls_train,\n                     train_small=list_urls_train_small,\n                     train_tiny=list_urls_train_tiny,\n                     val=list_urls_val)\n\n    def __init__(self, root,\n                 num_frame_per_fragment=50,\n                 mode=\'train_small\',\n                 min_overlap_ratio=0.3,\n                 max_overlap_ratio=1.0,\n                 max_dist_overlap=0.01,\n                 tsdf_voxel_size=0.01,\n                 limit_size=700,\n                 depth_thresh=6,\n                 is_fine=True,\n                 transform=None,\n                 pre_transform=None,\n                 pre_filter=None,\n                 verbose=False,\n                 debug=False,\n                 num_random_pt=5000,\n                 is_offline=False,\n                 radius_patch=None,\n                 pre_transform_patch=None):\n        r""""""\n        the Princeton 3DMatch dataset from the\n        `""3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions""\n        <https://arxiv.org/pdf/1603.08182.pdf>`_\n        paper, containing rgbd frames of the following dataset:\n        `"" SUN3D: A Database of Big Spaces Reconstructed using SfM and Object Labels\n        ""<http://sun3d.cs.princeton.edu/>`\n        `""Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n        ""<https://www.microsoft.com/en-us/research/publication/scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images/>`\n        `""Unsupervised Feature Learning for 3D Scene Labeling\n        ""<http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/>`\n        `""BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online Surface Re-integration\n        ""<http://graphics.stanford.edu/projects/bundlefusion/>`\n        `""Learning to Navigate the Energy Landscape\n        ""<http://graphics.stanford.edu/projects/reloc/>`\n\n        Args:\n            root (string): Root directory where the dataset should be saved\n\n            num_frame_per_fragment (int, optional): indicate the number of frames\n                we use to build fragments. If it is equal to 0, then we don\'t\n                build fragments and use the raw frames.\n\n            mode (string, optional): If :obj:`True`, loads the training dataset,\n            otherwise the test dataset. (default: :obj:`True`)\n\n            transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                every access. (default: :obj:`None`)\n\n            pre_transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                being saved to disk. (default: :obj:`None`)\n            pre_filter (callable, optional): A function that takes in an\n                :obj:`torch_geometric.data.Data` object and returns a boolean\n                value, indicating whether the data object should be included in the\n                final dataset. (default: :obj:`None`)\n        """"""\n\n        self.verbose = verbose\n        self.debug = debug\n        self.is_fine = is_fine\n        self.num_frame_per_fragment = num_frame_per_fragment\n        self.tsdf_voxel_size = tsdf_voxel_size\n        self.limit_size = limit_size\n        self.depth_thresh = depth_thresh\n        self.mode = mode\n        self.num_random_pt = num_random_pt\n        # select points for testing\n        self.detector = RandomDetector(num_points=self.num_random_pt)\n        # constant to compute overlap\n        self.min_overlap_ratio = min_overlap_ratio\n        self.max_overlap_ratio = max_overlap_ratio\n        self.max_dist_overlap = max_dist_overlap\n\n        self.is_offline = is_offline\n        self.num_random_pt = num_random_pt\n        self.radius_patch = radius_patch\n        self.pre_transform_patch = pre_transform_patch\n        if mode not in self.dict_urls.keys():\n            raise RuntimeError(\'this mode {} does \'\n                               \'not exist\'\n                               \'(train_small|train_tiny|train|val|test)\'.format(mode))\n        super(Base3DMatch, self).__init__(root,\n                                          transform,\n                                          pre_transform,\n                                          pre_filter)\n\n        # path = self.processed_paths[0] if train else self.processed_paths[1]\n        # self.data, self.slices = torch.load(path)\n\n    @property\n    def raw_file_names(self):\n        return [self.mode]\n\n    @property\n    def processed_file_names(self):\n        res =  [osp.join(self.mode, \'raw_fragment\'),\n                osp.join(self.mode, \'matches\'),\n                osp.join(self.mode, \'fragment\')]\n        if self.is_offline:\n            res.append(osp.join(self.mode, \'patches\'))\n        return res\n\n    def download(self):\n        # we download the raw RGBD file for the train and the validation data\n        folder = osp.join(self.raw_dir, self.mode)\n        log.info(""Download elements in the file {}..."".format(folder))\n        for url in self.dict_urls[self.mode]:\n            path = download_url(url, folder, self.verbose)\n            extract_zip(path, folder, self.verbose)\n            os.unlink(path)\n\n\n    def _create_fragment(self, mod):\n        r""""""\n        create fragments from the rgbd frames ie a partial reconstruction of\n        the scene with some frames(usually 50).\n        We will only use the first sequence for each scene\n        """"""\n\n        print(""Create fragment from RGBD frames..."")\n        if files_exist([osp.join(self.processed_dir, mod, \'raw_fragment\')]):  # pragma: no cover\n            log.warning(""the fragments on mode {} already exists"".format(mod))\n            return\n        for scene_path in os.listdir(osp.join(self.raw_dir, mod)):\n            # TODO list the right sequences.\n            list_seq = [f for f in os.listdir(osp.join(self.raw_dir, mod,\n                                                       scene_path)) if \'seq\' in f]\n            for seq in list_seq:\n                frames_dir = osp.join(self.raw_dir, self.mode,\n                                      scene_path, seq)\n                out_dir = osp.join(self.processed_dir,\n                                   mod, \'raw_fragment\',\n                                   scene_path, seq)\n                makedirs(out_dir)\n                path_intrinsic = osp.join(self.raw_dir,\n                                          self.mode, scene_path,\n                                          \'camera-intrinsics.txt\')\n                list_path_frames = sorted([osp.join(frames_dir, f)\n                                           for f in os.listdir(frames_dir)\n                                           if \'png\' in f and \'depth\' in f])\n                # list_path_color = sorted([osp.join(frames_dir, f)\n                #                          for f in os.listdir(frames_dir)\n                #                          if \'png\' in f and \'color\' in f])\n                list_path_trans = sorted([osp.join(frames_dir, f)\n                                          for f in os.listdir(frames_dir)\n                                          if \'pose\' in f and \'txt\' in f])\n                # compute each fragment and save it\n                if(not self.is_fine):\n                    rgbd2fragment_rough(list_path_frames, path_intrinsic,\n                                        list_path_trans, out_dir,\n                                        self.num_frame_per_fragment,\n                                        pre_transform=None)\n                else:\n                    assert len(list_path_frames) == len(list_path_trans), \\\n                        log.error(""For the sequence {},""\n                                  ""the number of frame ""\n                                  ""and the number of ""\n                                  ""pose is different"".format(frames_dir))\n                    rgbd2fragment_fine(list_path_frames,\n                                       path_intrinsic,\n                                       list_path_trans,\n                                       out_dir, self.num_frame_per_fragment,\n                                       voxel_size=self.tsdf_voxel_size,\n                                       pre_transform=None,\n                                       depth_thresh=self.depth_thresh,\n                                       limit_size=self.limit_size)\n\n    def _pre_transform_fragment(self, mod):\n        """"""\n        pre_transform raw fragments and save it into fragments\n        """"""\n        out_dir = osp.join(self.processed_dir,\n                           mod, \'fragment\')\n        if files_exist([out_dir]):  # pragma: no cover\n            return\n        makedirs(out_dir)\n        for scene_path in os.listdir(osp.join(self.raw_dir, mod)):\n            # TODO list the right sequences.\n            list_seq = [f for f in os.listdir(osp.join(self.raw_dir, mod,\n                                                       scene_path)) if \'seq\' in f]\n            for seq in list_seq:\n                in_dir = osp.join(self.processed_dir,\n                                  mod, \'raw_fragment\',\n                                  scene_path, seq)\n                out_dir = osp.join(self.processed_dir,\n                                   mod, \'fragment\',\n                                   scene_path, seq)\n                makedirs(out_dir)\n                list_fragment_path = sorted([f\n                                             for f in os.listdir(in_dir)\n                                             if \'fragment\' in f])\n                for path in list_fragment_path:\n                    data = torch.load(osp.join(in_dir, path))\n                    if(self.pre_transform is not None):\n                        data = self.pre_transform(data)\n                    torch.save(data, osp.join(out_dir, path))\n\n    def _compute_matches_between_fragments(self, mod):\n\n        out_dir = osp.join(self.processed_dir,\n                           mod, \'matches\')\n        if files_exist([out_dir]):  # pragma: no cover\n            return\n        makedirs(out_dir)\n        ind = 0\n        for scene_path in os.listdir(osp.join(self.raw_dir, mod)):\n\n            list_seq = sorted([f for f in os.listdir(osp.join(self.raw_dir, mod,\n                                                              scene_path)) if \'seq\' in f])\n            for seq in list_seq:\n                log.info(""{}, {}"".format(scene_path, seq))\n                fragment_dir = osp.join(self.processed_dir,\n                                        mod, \'fragment\',\n                                        scene_path, seq)\n                list_fragment_path = sorted([osp.join(fragment_dir, f)\n                                             for f in os.listdir(fragment_dir)\n                                             if \'fragment\' in f])\n                log.info(""compute_overlap_and_matches"")\n\n                for path1 in list_fragment_path:\n                    for path2 in list_fragment_path:\n                        if path1 < path2:\n                            out_path = osp.join(out_dir,\n                                                \'matches{:06d}.npy\'.format(ind))\n                            data1 = torch.load(path1)\n                            data2 = torch.load(path2)\n                            match = compute_overlap_and_matches(\n                                data1, data2, self.max_dist_overlap)\n                            match[\'path_source\'] = path1\n                            match[\'path_target\'] = path2\n\n                            if(np.max(match[\'overlap\']) > self.min_overlap_ratio and\n                               np.max(match[\'overlap\']) < self.max_overlap_ratio):\n                                np.save(out_path, match)\n                                ind += 1\n\n    def _save_patches(self, mod):\n        """"""\n        save patch to load it offline for the training\n        """"""\n        p_extractor = PatchExtractor(self.radius_patch)\n        out_dir = osp.join(self.processed_dir,\n                           mod, \'patches\')\n        if files_exist([out_dir]):  # pragma: no cover\n            return\n        makedirs(out_dir)\n        match_dir = osp.join(self.processed_dir,\n                             mod, \'matches\')\n        idx = 0\n        for i in range(len(os.listdir(match_dir))):\n            match = np.load(\n                osp.join(match_dir,\n                         \'matches{:06d}.npy\'.format(i)),\n                allow_pickle=True).item()\n\n            for _ in range(self.num_random_pt):\n                data_source = torch.load(match[\'path_source\'])\n                data_target = torch.load(match[\'path_target\'])\n                rand = np.random.randint(0, len(match[\'pair\']))\n                data_source = p_extractor(data_source, match[\'pair\'][rand][0])\n                data_target = p_extractor(data_target, match[\'pair\'][rand][1])\n                if(self.pre_transform_patch is not None):\n                    data_source = self.pre_transform_patch(data_source)\n                    data_target = self.pre_transform_patch(data_target)\n                if(self.pre_filter is not None):\n                    if(self.pre_filter(data_source) and self.pre_filter(data_target)):\n\n                        torch.save(data_source,\n                                   osp.join(out_dir,\n                                            \'patches_source{:06d}.pt\'.format(idx)))\n                        torch.save(data_target,\n                                   osp.join(out_dir,\n                                            \'patches_target{:06d}.pt\'.format(idx)))\n                        idx += 1\n                else:\n                    torch.save(data_source,\n                               osp.join(out_dir,\n                                        \'patches_source{:06d}.pt\'.format(idx)))\n                    torch.save(data_target,\n                               osp.join(out_dir,\n                                        \'patches_target{:06d}.pt\'.format(idx)))\n                    idx += 1\n\n    def process(self):\n        log.info(""create fragments"")\n        self._create_fragment(self.mode)\n        log.info(""pre_transform those fragments"")\n        self._pre_transform_fragment(self.mode)\n        log.info(""compute matches"")\n        self._compute_matches_between_fragments(self.mode)\n        if(self.is_offline):\n            log.info(""precompute patches and save it"")\n            self._save_patches(self.mode)\n\n\n    def get(self, idx):\n        raise NotImplementedError(""implement class to get patch or fragment or more"")\n\n    def __getitem__(self, idx):\n        r""""""Gets the data object at index :obj:`idx` and transforms it (in case\n        a :obj:`self.transform` is given).\n        In case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\n        tuple, a  LongTensor or a BoolTensor, will return a subset of the\n        dataset at the specified indices.""""""\n\n        data = self.get(idx)\n        return data\n'"
torch_points3d/datasets/registration/base_siamese_dataset.py,0,"b'from torch_points3d.core.data_transform import MultiScaleTransform\nfrom torch_points3d.core.data_transform import PairTransform\nfrom torch_points3d.datasets.registration.pair import DensePairBatch\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.utils.config import ConvolutionFormatFactory\nfrom torch_points3d.datasets.registration.pair import PairMultiScaleBatch, PairBatch\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n\n\nclass BaseSiameseDataset(BaseDataset):\n    def __init__(self, dataset_opt):\n        """"""\n        base dataset for siamese inputs\n        """"""\n        super().__init__(dataset_opt)\n\n    @staticmethod\n    def _get_collate_function(conv_type, is_multiscale):\n\n        is_dense = ConvolutionFormatFactory.check_is_dense_format(conv_type)\n\n        if is_multiscale:\n            if conv_type.lower() == ConvolutionFormat.PARTIAL_DENSE.value.lower():\n                return lambda datalist: PairMultiScaleBatch.from_data_list(datalist)\n            else:\n                raise NotImplementedError(\n                    ""MultiscaleTransform is activated and supported only for partial_dense format""\n                )\n\n        if is_dense:\n            return lambda datalist: DensePairBatch.from_data_list(datalist)\n        else:\n            return lambda datalist: PairBatch.from_data_list(datalist)\n'"
torch_points3d/datasets/registration/basetest.py,4,"b'import json\nimport logging\nimport numpy as np\nimport os\nimport os.path as osp\nfrom plyfile import PlyData\nimport shutil\nimport torch\n\nfrom torch_geometric.data import Dataset, download_url, extract_zip\nfrom torch_geometric.data import Data\n\nfrom torch_points3d.datasets.registration.utils import rgbd2fragment_rough\nfrom torch_points3d.datasets.registration.utils import rgbd2fragment_fine\nfrom torch_points3d.datasets.registration.utils import compute_overlap_and_matches\nfrom torch_points3d.datasets.registration.utils import to_list\nfrom torch_points3d.datasets.registration.utils import files_exist\nfrom torch_points3d.datasets.registration.utils import makedirs\nfrom torch_points3d.datasets.registration.utils import get_urls\nfrom torch_points3d.datasets.registration.utils import PatchExtractor\n\n\nlog = logging.getLogger(__name__)\n\n\nclass SimplePatch(torch.utils.data.Dataset):\n\n    def __init__(self, list_patches, transform=None):\n        """"""\n        transform a list of Data into a dataset(and apply transform)\n        """"""\n        if(transform is None):\n            self.list_patches = list_patches\n        else:\n            self.list_patches = [transform(p) for p in list_patches]\n        self.transform = transform\n\n\n    def __len__(self):\n        return len(self.list_patches)\n\n    def __getitem__(self, idx):\n        data = self.list_patches[idx]\n        return data\n\n    @property\n    def num_features(self):\n        if self[0].x is None:\n            return 0\n        return 1 if self[0].x.dim() == 1 else self[0].x.size(1)\n\n\nclass BaseTest(Dataset):\n\n    def __init__(self, root,\n                 transform=None,\n                 pre_transform=None,\n                 pre_filter=None,\n                 verbose=False,\n                 debug=False,\n                 num_random_pt=5000):\n        """"""\n        a baseDataset that download a dataset,\n        apply preprocessing, and compute keypoints\n        """"""\n\n        self.num_random_pt = num_random_pt\n        super(BaseTest, self).__init__(root,\n                                       transform,\n                                       pre_transform,\n                                       pre_filter)\n\n    @property\n    def raw_file_names(self):\n        return [""raw_fragment""]\n\n    @property\n    def processed_file_names(self):\n        return [""fragment""]\n\n    def download(self):\n        raise NotImplementedError(\'need to download the dataset\')\n\n    def _pre_transform_fragments_ply(self):\n        """"""\n        apply pre_transform on fragments (ply) and save the results\n        """"""\n        out_dir = osp.join(self.processed_dir,\n                           \'fragment\')\n        if files_exist([out_dir]):  # pragma: no cover\n            return\n        makedirs(out_dir)\n        ind = 0\n        # table to map fragment numper with\n        self.table = dict()\n\n        for scene_path in os.listdir(osp.join(self.raw_dir, ""raw_fragment"")):\n\n            fragment_dir = osp.join(self.raw_dir,\n                                    ""raw_fragment"",\n                                    scene_path)\n            list_fragment_path = sorted([f\n                                         for f in os.listdir(fragment_dir)\n                                         if \'ply\' in f])\n\n            for i, f_p in enumerate(list_fragment_path):\n                fragment_path = osp.join(fragment_dir, f_p)\n                out_path = osp.join(out_dir, \'fragment_{:06d}.pt\'.format(ind))\n\n                # read ply file\n                with open(fragment_path, \'rb\') as f:\n                    data = PlyData.read(f)\n                pos = ([torch.tensor(data[\'vertex\'][axis]) for axis in [\'x\', \'y\', \'z\']])\n                pos = torch.stack(pos, dim=-1)\n                data = Data(pos=pos)\n                if(self.pre_transform is not None):\n                    data = self.pre_transform(data)\n                torch.save(data, out_path)\n                self.table[ind] = {\'in_path\': fragment_path,\n                                   \'scene_path\': scene_path,\n                                   \'fragment_name\': f_p,\n                                   \'out_path\': out_path}\n                ind += 1\n\n        # save this file into json\n        with open(osp.join(out_dir, \'table.json\'), \'w\') as f:\n            json.dump(self.table, f)\n\n    def process(self):\n        self._pre_transform_fragments_ply()\n\n    def __getitem__(self, idx):\n        raise NotImplementedError(""implement class to get patch or fragment or more"")\n\n\nclass Base3DMatchTest(BaseTest):\n\n    def __init__(self, root,\n                 transform=None,\n                 pre_transform=None,\n                 pre_filter=None,\n                 verbose=False,\n                 debug=False,\n                 num_random_pt=5000):\n        """"""\n        Base 3D Match but for testing\n        """"""\n        base = osp.abspath(osp.join(osp.realpath(__file__),\n                                    \'..\'))\n        self.list_urls_test = get_urls(osp.join(base, \'urls\', \'url_test.txt\'))\n        super(Base3DMatchTest, self).__init__(root,\n                                              transform,\n                                              pre_transform,\n                                              pre_filter,\n                                              verbose,\n                                              debug,\n                                              num_random_pt)\n\n    def download(self):\n        folder_test = osp.join(self.raw_dir, \'raw_fragment\')\n        for url_raw in self.list_urls_test:\n            url = url_raw.split(\'\\n\')[0]\n            path = download_url(url, folder_test)\n            extract_zip(path, folder_test)\n            log.info(path)\n            folder = path.split(\'.zip\')[0]\n            os.unlink(path)\n            path_eval = download_url(url.split(\'.zip\')[0]+\'-evaluation.zip\',\n                                     folder)\n            extract_zip(path_eval, folder)\n            os.unlink(path_eval)\n            folder_eval = path_eval.split(\'.zip\')[0]\n            for f in os.listdir(folder_eval):\n                os.rename(osp.join(folder_eval, f), osp.join(folder, f))\n            shutil.rmtree(folder_eval)\n\n\nclass BaseETHTest(BaseTest):\n\n    def __init__(self, root,\n                 transform=None,\n                 pre_transform=None,\n                 pre_filter=None,\n                 verbose=False,\n                 debug=False,\n                 num_random_pt=5000):\n        """"""\n        Base for ETH Dataset. The main goal is to see\n        if the descriptors generalize well.\n        """"""\n\n        self.list_urls_test = [""url""]\n        super(BaseTest, self).__init__(root,\n                                       transform,\n                                       pre_transform,\n                                       pre_filter,\n                                       verbose,\n                                       debug,\n                                       num_random_pt)\n\n    def download(self):\n        raise NotImplementedError(""need to implement test for this dataset"")\n'"
torch_points3d/datasets/registration/detector.py,1,"b'import torch\n\n\nclass RandomDetector(object):\n    """"""\n    Random selector for test points\n    """"""\n\n    def __init__(self, num_points=5000):\n        self.num_points = num_points\n\n    def __call__(self, data):\n        keypoints_idx = torch.randint(0,\n                                      data.pos.shape[0],\n                                      (self.num_points, ))\n        data.keypoints = keypoints_idx\n        return data\n'"
torch_points3d/datasets/registration/fusion.py,0,"b'# Copyright (c) 2018 Andy Zeng\n# Taken from https://github.com/andyzeng/tsdf-fusion-python\n\nimport numpy as np\n\nfrom numba import njit, prange\nfrom skimage import measure\n\ntry:\n    import pycuda.driver as cuda\n    import pycuda.autoinit\n    from pycuda.compiler import SourceModule\n    FUSION_GPU_MODE = 1\nexcept Exception as err:\n    print(\'Warning: {}\'.format(err))\n    print(\'Failed to import PyCUDA. Running fusion in CPU mode.\')\n    FUSION_GPU_MODE = 0\n\n\nclass TSDFVolume:\n    """"""Volumetric TSDF Fusion of RGB-D Images.\n    """"""\n    def __init__(self, vol_bnds, voxel_size, use_gpu=True):\n        """"""Constructor.\n        Args:\n        vol_bnds (ndarray): An ndarray of shape (3, 2). Specifies the\n        xyz bounds (min/max) in meters.\n        voxel_size (float): The volume discretization in meters.\n        """"""\n        vol_bnds = np.asarray(vol_bnds)\n        assert vol_bnds.shape == (3, 2), ""[!] `vol_bnds` should be of shape (3, 2).""\n\n        # Define voxel volume parameters\n        self._vol_bnds = vol_bnds\n        self._voxel_size = float(voxel_size)\n        self._trunc_margin = 5 * self._voxel_size  # truncation on SDF\n\n\n        # Adjust volume bounds and ensure C-order contiguous\n        self._vol_dim = np.ceil(\n            (self._vol_bnds[:, 1] - self._vol_bnds[:, 0]) /\n            self._voxel_size).copy(order=\'C\').astype(int)\n        self._vol_bnds[:, 1] = self._vol_bnds[:, 0]+self._vol_dim*self._voxel_size\n        self._vol_origin = self._vol_bnds[:, 0].copy(order=\'C\').astype(np.float32)\n\n        print(""Voxel volume size: {} x {} x {} - # points: {:,}"".format(\n            self._vol_dim[0], self._vol_dim[1], self._vol_dim[2],\n            self._vol_dim[0]*self._vol_dim[1]*self._vol_dim[2])\n        )\n\n        # Initialize pointers to voxel volume in CPU memory\n        self._tsdf_vol_cpu = np.ones(self._vol_dim).astype(np.float32)\n        # for computing the cumulative moving average of observations per voxel\n        self._weight_vol_cpu = np.zeros(self._vol_dim).astype(np.float32)\n        self.gpu_mode = use_gpu and FUSION_GPU_MODE\n\n        # Copy voxel volumes to GPU\n        if self.gpu_mode:\n            self._tsdf_vol_gpu = cuda.mem_alloc(self._tsdf_vol_cpu.nbytes)\n            cuda.memcpy_htod(self._tsdf_vol_gpu, self._tsdf_vol_cpu)\n            self._weight_vol_gpu = cuda.mem_alloc(self._weight_vol_cpu.nbytes)\n            cuda.memcpy_htod(self._weight_vol_gpu, self._weight_vol_cpu)\n\n            # Cuda kernel function (C++)\n            self._cuda_src_mod = SourceModule(""""""\n            __global__ void integrate(float * tsdf_vol,\n                                  float * weight_vol,\n                                  float * vol_dim,\n                                  float * vol_origin,\n                                  float * cam_intr,\n                                  float * cam_pose,\n                                  float * other_params,\n                                  float * depth_im) {\n          // Get voxel index\n          int gpu_loop_idx = (int) other_params[0];\n          int max_threads_per_block = blockDim.x;\n          int block_idx = blockIdx.z*gridDim.y*gridDim.x+blockIdx.y*gridDim.x+blockIdx.x;\n          int voxel_idx = gpu_loop_idx*gridDim.x*gridDim.y*gridDim.z*max_threads_per_block+block_idx*max_threads_per_block+threadIdx.x;\n          int vol_dim_x = (int) vol_dim[0];\n          int vol_dim_y = (int) vol_dim[1];\n          int vol_dim_z = (int) vol_dim[2];\n          if (voxel_idx > vol_dim_x*vol_dim_y*vol_dim_z)\n              return;\n          // Get voxel grid coordinates (note: be careful when casting)\n          float voxel_x = floorf(((float)voxel_idx)/((float)(vol_dim_y*vol_dim_z)));\n          float voxel_y = floorf(((float)(voxel_idx-((int)voxel_x)*vol_dim_y*vol_dim_z))/((float)vol_dim_z));\n          float voxel_z = (float)(voxel_idx-((int)voxel_x)*vol_dim_y*vol_dim_z-((int)voxel_y)*vol_dim_z);\n          // Voxel grid coordinates to world coordinates\n          float voxel_size = other_params[1];\n          float pt_x = vol_origin[0]+voxel_x*voxel_size;\n          float pt_y = vol_origin[1]+voxel_y*voxel_size;\n          float pt_z = vol_origin[2]+voxel_z*voxel_size;\n          // World coordinates to camera coordinates\n          float tmp_pt_x = pt_x-cam_pose[0*4+3];\n          float tmp_pt_y = pt_y-cam_pose[1*4+3];\n          float tmp_pt_z = pt_z-cam_pose[2*4+3];\n          float cam_pt_x = cam_pose[0*4+0]*tmp_pt_x+cam_pose[1*4+0]*tmp_pt_y+cam_pose[2*4+0]*tmp_pt_z;\n          float cam_pt_y = cam_pose[0*4+1]*tmp_pt_x+cam_pose[1*4+1]*tmp_pt_y+cam_pose[2*4+1]*tmp_pt_z;\n          float cam_pt_z = cam_pose[0*4+2]*tmp_pt_x+cam_pose[1*4+2]*tmp_pt_y+cam_pose[2*4+2]*tmp_pt_z;\n          // Camera coordinates to image pixels\n          int pixel_x = (int) roundf(cam_intr[0*3+0]*(cam_pt_x/cam_pt_z)+cam_intr[0*3+2]);\n          int pixel_y = (int) roundf(cam_intr[1*3+1]*(cam_pt_y/cam_pt_z)+cam_intr[1*3+2]);\n          // Skip if outside view frustum\n          int im_h = (int) other_params[2];\n          int im_w = (int) other_params[3];\n          if (pixel_x < 0 || pixel_x >= im_w || pixel_y < 0 || pixel_y >= im_h || cam_pt_z<0)\n              return;\n          // Skip invalid depth\n          float depth_value = depth_im[pixel_y*im_w+pixel_x];\n          if (depth_value == 0)\n              return;\n          // Integrate TSDF\n          float trunc_margin = other_params[4];\n          float depth_diff = depth_value-cam_pt_z;\n          if (depth_diff < -trunc_margin)\n              return;\n          float dist = fmin(1.0f,depth_diff/trunc_margin);\n          float w_old = weight_vol[voxel_idx];\n          float obs_weight = other_params[5];\n          float w_new = w_old + obs_weight;\n          weight_vol[voxel_idx] = w_new;\n          tsdf_vol[voxel_idx] = (tsdf_vol[voxel_idx]*w_old+obs_weight*dist)/w_new;\n\n            }"""""")\n\n            self._cuda_integrate = self._cuda_src_mod.get_function(""integrate"")\n\n            # Determine block/grid size on GPU\n            gpu_dev = cuda.Device(0)\n            self._max_gpu_threads_per_block = gpu_dev.MAX_THREADS_PER_BLOCK\n            n_blocks = int(np.ceil(\n                float(np.prod(self._vol_dim)) /\n                float(self._max_gpu_threads_per_block)))\n            grid_dim_x = min(gpu_dev.MAX_GRID_DIM_X,\n                             int(np.floor(np.cbrt(n_blocks))))\n            grid_dim_y = min(gpu_dev.MAX_GRID_DIM_Y,\n                             int(np.floor(np.sqrt(n_blocks/grid_dim_x))))\n            grid_dim_z = min(gpu_dev.MAX_GRID_DIM_Z,\n                             int(np.ceil(float(n_blocks) /\n                                         float(grid_dim_x * grid_dim_y))))\n            self._max_gpu_grid_dim = np.array(\n                [grid_dim_x, grid_dim_y, grid_dim_z]).astype(int)\n            self._n_gpu_loops = int(np.ceil(\n                float(np.prod(self._vol_dim)) /\n                float(np.prod(self._max_gpu_grid_dim)*self._max_gpu_threads_per_block)))\n        else:\n            # Get voxel grid coordinates\n            xv, yv, zv = np.meshgrid(\n                range(self._vol_dim[0]),\n                range(self._vol_dim[1]),\n                range(self._vol_dim[2]),\n                indexing=\'ij\'\n            )\n            self.vox_coords = np.concatenate([\n                xv.reshape(1, -1),\n                yv.reshape(1, -1),\n                zv.reshape(1, -1)\n            ], axis=0).astype(int).T\n\n    @staticmethod\n    @njit(parallel=True)\n    def vox2world(vol_origin, vox_coords, vox_size):\n        """"""Convert voxel grid coordinates to world coordinates.\n        """"""\n        vol_origin = vol_origin.astype(np.float32)\n        vox_coords = vox_coords.astype(np.float32)\n        cam_pts = np.empty_like(vox_coords, dtype=np.float32)\n        for i in prange(vox_coords.shape[0]):\n            for j in range(3):\n                cam_pts[i, j] = vol_origin[j] + (vox_size * vox_coords[i, j])\n        return cam_pts\n\n    @staticmethod\n    @njit(parallel=True)\n    def cam2pix(cam_pts, intr):\n        """"""Convert camera coordinates to pixel coordinates.\n        """"""\n        intr = intr.astype(np.float32)\n        fx, fy = intr[0, 0], intr[1, 1]\n        cx, cy = intr[0, 2], intr[1, 2]\n        pix = np.empty((cam_pts.shape[0], 2), dtype=np.int64)\n        for i in prange(cam_pts.shape[0]):\n            pix[i, 0] = int(np.round((cam_pts[i, 0] * fx / cam_pts[i, 2]) + cx))\n            pix[i, 1] = int(np.round((cam_pts[i, 1] * fy / cam_pts[i, 2]) + cy))\n        return pix\n\n    @staticmethod\n    @njit(parallel=True)\n    def integrate_tsdf(tsdf_vol, dist, w_old, obs_weight):\n        """"""Integrate the TSDF volume.\n        """"""\n        tsdf_vol_int = np.empty_like(tsdf_vol, dtype=np.float32)\n        w_new = np.empty_like(w_old, dtype=np.float32)\n        for i in prange(len(tsdf_vol)):\n            w_new[i] = w_old[i] + obs_weight\n            tsdf_vol_int[i] = (w_old[i] * tsdf_vol[i] + obs_weight * dist[i]) / w_new[i]\n        return tsdf_vol_int, w_new\n\n    def integrate(self, depth_im, cam_intr, cam_pose, obs_weight=1.):\n        """"""Integrate an RGB-D frame into the TSDF volume.\n        Args:\n        depth_im (ndarray): A depth image of shape (H, W).\n        cam_intr (ndarray): The camera intrinsics matrix of shape (3, 3).\n        cam_pose (ndarray): The camera pose (i.e. extrinsics) of shape (4, 4).\n        obs_weight (float): The weight to assign for the current observation. A higher\n        value\n        """"""\n        im_h, im_w = depth_im.shape\n        if self.gpu_mode:  # GPU mode: integrate voxel volume (calls CUDA kernel)\n            for gpu_loop_idx in range(self._n_gpu_loops):\n                self._cuda_integrate(self._tsdf_vol_gpu,\n                                     self._weight_vol_gpu,\n                                     cuda.InOut(self._vol_dim.astype(np.float32)),\n                                     cuda.InOut(self._vol_origin.astype(np.float32)),\n                                     cuda.InOut(cam_intr.reshape(-1).astype(np.float32)),\n                                     cuda.InOut(cam_pose.reshape(-1).astype(np.float32)),\n                                     cuda.InOut(np.asarray([\n                                         gpu_loop_idx,\n                                         self._voxel_size,\n                                         im_h,\n                                         im_w,\n                                         self._trunc_margin,\n                                         obs_weight\n                                     ], np.float32)),\n                                     cuda.InOut(depth_im.reshape(-1).astype(np.float32)),\n                                     block=(self._max_gpu_threads_per_block,1,1),\n                                     grid=(\n                                         int(self._max_gpu_grid_dim[0]),\n                                         int(self._max_gpu_grid_dim[1]),\n                                         int(self._max_gpu_grid_dim[2]),\n                                     )\n                )\n        else:  # CPU mode: integrate voxel volume (vectorized implementation)\n            # Convert voxel grid coordinates to pixel coordinates\n            cam_pts = self.vox2world(self._vol_origin, self.vox_coords, self._voxel_size)\n            cam_pts = rigid_transform(cam_pts, np.linalg.inv(cam_pose))\n            pix_z = cam_pts[:, 2]\n            pix = self.cam2pix(cam_pts, cam_intr)\n            pix_x, pix_y = pix[:, 0], pix[:, 1]\n\n            # Eliminate pixels outside view frustum\n            valid_pix = np.logical_and(\n                pix_x >= 0,\n                np.logical_and(pix_x < im_w,\n                               np.logical_and(pix_y >= 0,\n                                              np.logical_and(pix_y < im_h,\n                                                             pix_z > 0))))\n            depth_val = np.zeros(pix_x.shape)\n            depth_val[valid_pix] = depth_im[pix_y[valid_pix], pix_x[valid_pix]]\n\n            # Integrate TSDF\n            depth_diff = depth_val - pix_z\n            valid_pts = np.logical_and(depth_val > 0, depth_diff >= -self._trunc_margin)\n            dist = np.minimum(1, depth_diff / self._trunc_margin)\n            valid_vox_x = self.vox_coords[valid_pts, 0]\n            valid_vox_y = self.vox_coords[valid_pts, 1]\n            valid_vox_z = self.vox_coords[valid_pts, 2]\n            w_old = self._weight_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z]\n            tsdf_vals = self._tsdf_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z]\n            valid_dist = dist[valid_pts]\n            tsdf_vol_new, w_new = self.integrate_tsdf(tsdf_vals, valid_dist, w_old, obs_weight)\n            self._weight_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z] = w_new\n            self._tsdf_vol_cpu[valid_vox_x, valid_vox_y, valid_vox_z] = tsdf_vol_new\n\n    def get_volume(self):\n        if self.gpu_mode:\n            cuda.memcpy_dtoh(self._tsdf_vol_cpu, self._tsdf_vol_gpu)\n            cuda.memcpy_dtoh(self._weight_vol_cpu, self._weight_vol_gpu)\n\n        return self._tsdf_vol_cpu, self._weight_vol_cpu\n\n    def get_mesh(self):\n        """"""Compute a mesh from the voxel volume using marching cubes.\n        """"""\n        tsdf_vol, _ = self.get_volume()\n\n        # Marching cubes\n        verts, faces, norms, vals = measure.marching_cubes_lewiner(tsdf_vol,\n                                                                   level=0)\n\n        verts = verts*self._voxel_size+self._vol_origin\n        # voxel grid coordinates to world coordinates\n        return verts, faces, norms\n\n    def get_point_cloud(self, tsdf_thresh, weight_thresh):\n        """"""\n        compute the surface pointcloud from the voxel volume\n        """"""\n        tsdf_vol, weight_vol = self.get_volume()\n\n        mask = np.logical_and(np.abs(tsdf_vol) < tsdf_thresh,\n                              weight_vol > weight_thresh)\n\n        xv, yv, zv = np.meshgrid(\n            range(self._vol_dim[0]),\n            range(self._vol_dim[1]),\n            range(self._vol_dim[2]),\n            indexing=\'ij\'\n        )\n        xv = xv[mask]\n        yv = yv[mask]\n        zv = zv[mask]\n        pcd_ind = np.concatenate([\n            xv.reshape(1, -1),\n            yv.reshape(1, -1),\n            zv.reshape(1, -1)\n        ], axis=0).astype(int).T\n\n        pcd = pcd_ind.astype(float) * self._voxel_size+self._vol_origin\n        pcd_ind = pcd_ind.astype(int)\n\n        return pcd\n\n\ndef rigid_transform(xyz, transform):\n    """"""Applies a rigid transform to an (N, 3) pointcloud.\n        """"""\n    xyz_h = np.hstack([xyz, np.ones((len(xyz), 1), dtype=np.float32)])\n    xyz_t_h = np.dot(transform, xyz_h.T).T\n    return xyz_t_h[:, :3]\n\n\ndef get_view_frustum(depth_im, cam_intr, cam_pose):\n    """"""Get corners of 3D camera view frustum of depth image\n        """"""\n    im_h = depth_im.shape[0]\n    im_w = depth_im.shape[1]\n    max_depth = np.max(depth_im)\n    view_frust_pts = np.array([\n        (np.array([0, 0, 0, im_w, im_w])-cam_intr[0, 2]) *\n        np.array([0, max_depth, max_depth, max_depth, max_depth]) /\n        cam_intr[0, 0],\n        (np.array([0, 0, im_h, 0, im_h]) - cam_intr[1, 2]) *\n        np.array([0, max_depth, max_depth, max_depth, max_depth]) /\n        cam_intr[1, 1],\n        np.array([0, max_depth, max_depth, max_depth, max_depth])\n    ])\n    view_frust_pts = rigid_transform(view_frust_pts.T, cam_pose).T\n    return view_frust_pts\n'"
torch_points3d/datasets/registration/general3dmatch.py,10,"b'import numpy as np\nimport os\nimport os.path as osp\nimport torch\nfrom torch_geometric.data import Data\n\nfrom torch_points3d.datasets.registration.base3dmatch import Base3DMatch\nfrom torch_points3d.datasets.registration.utils import PatchExtractor\nfrom torch_points3d.datasets.registration.utils import tracked_matches\nfrom torch_points3d.datasets.registration.pair import Pair, MultiScalePair\nfrom torch_points3d.metrics.registration_tracker import PatchRegistrationTracker\nfrom torch_points3d.metrics.registration_tracker import FragmentRegistrationTracker\n\nfrom torch_points3d.datasets.registration.base_siamese_dataset import BaseSiameseDataset\nfrom torch_points3d.datasets.registration.utils import compute_overlap_and_matches\n\n\n\nclass Patch3DMatch(Base3DMatch):\n    def __init__(\n        self,\n        root,\n        radius_patch=0.3,\n        num_frame_per_fragment=50,\n        mode=""train_small"",\n        min_overlap_ratio=0.3,\n        max_overlap_ratio=1.0,\n        max_dist_overlap=0.01,\n        tsdf_voxel_size=0.02,\n        limit_size=700,\n        depth_thresh=6,\n        is_fine=True,\n        transform=None,\n        pre_transform=None,\n        pre_filter=None,\n        verbose=False,\n        debug=False,\n        num_random_pt=5000,\n        is_offline=False,\n        pre_transform_patch=None,\n    ):\n        r""""""\n        Patch extracted from :the Princeton 3DMatch dataset\\n\n        `""3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions""\n        <https://arxiv.org/pdf/1603.08182.pdf>`_\n        paper, containing rgbd frames of the following dataset:\n        `"" SUN3D: A Database of Big Spaces Reconstructed using SfM and Object Labels\n        ""<http://sun3d.cs.princeton.edu/>`\n        `""Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n        ""<https://www.microsoft.com/en-us/research/publication/scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images/>`\n        `""Unsupervised Feature Learning for 3D Scene Labeling\n        ""<http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/>`\n        `""BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online\n        Surface Re-integration\n        ""<http://graphics.stanford.edu/projects/bundlefusion/>`\n        `""Learning to Navigate the Energy Landscape\n        ""<http://graphics.stanford.edu/projects/reloc/>`\n\n        Args:\n\n            root (string): Root directory where the dataset should be saved\n\n            radius_patch(float, optional): the size of the patch\n\n            num_frame_per_fragment (int, optional): indicate the number of frames\n                we use to build fragments. If it is equal to 0, then we don\'t\n                build fragments and use the raw frames.\n\n            mode (string, optional): If :obj:`True`, loads the training dataset,\n            otherwise the test dataset. (default: :obj:`True`)\n\n            min_overlap_ratio(float, optional): the minimum overlap we should have to match two fragments (overlap is the number of points in a fragment that matches in an other fragment divided by the number of points)\n\n            max_dist_overlap(float, optional): minimum distance to consider that a point match with an other.\n            tsdf_voxel_size(float, optional): the size of the tsdf voxel grid to perform fine RGBD fusion to create fine fragments\n            depth_thresh: threshold to remove depth pixel that are two far.\n\n            is_fine: fine mode for the fragment fusion\n\n            limit_size : limit the number of pixel at each direction to abvoid to heavy tsdf voxel\n\n            transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                every access. (default: :obj:`None`)\n\n            pre_transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                being saved to disk. (default: :obj:`None`)\n            pre_filter (callable, optional): A function that takes in an\n                :obj:`torch_geometric.data.Data` object and returns a boolean\n                value, indicating whether the data object should be included in the\n                final dataset. (default: :obj:`None`)\n            num_random_pt: number of point we select\n        """"""\n        self.is_patch = True\n        super(Patch3DMatch, self).__init__(\n            root,\n            num_frame_per_fragment,\n            mode,\n            min_overlap_ratio,\n            max_overlap_ratio,\n            max_dist_overlap,\n            tsdf_voxel_size,\n            limit_size,\n            depth_thresh,\n            is_fine,\n            transform,\n            pre_transform,\n            pre_filter,\n            verbose,\n            debug,\n            num_random_pt,\n            is_offline,\n            radius_patch,\n            pre_transform_patch,\n        )\n\n        self.radius_patch = radius_patch\n        self.is_offline = is_offline\n        self.path_data = osp.join(self.processed_dir, self.mode, ""matches"")\n        if self.is_offline:\n            self.path_data = osp.join(self.processed_dir, self.mode, ""patches"")\n\n    def get_patch_online(self, idx):\n        p_extractor = PatchExtractor(self.radius_patch)\n\n        match = np.load(osp.join(self.path_data, ""matches{:06d}.npy"".format(idx)), allow_pickle=True).item()\n        data_source = torch.load(match[""path_source""]).to(torch.float)\n        data_target = torch.load(match[""path_target""]).to(torch.float)\n\n        # select a random match on the list of match.\n        # It cannot be 0 because matches are filtered.\n        rand = np.random.randint(0, len(match[""pair""]))\n\n        data_source = p_extractor(data_source, match[""pair""][rand][0])\n        data_target = p_extractor(data_target, match[""pair""][rand][1])\n\n        if self.transform is not None:\n            data_source = self.transform(data_source)\n            data_target = self.transform(data_target)\n        batch = Pair.make_pair(data_source, data_target)\n        batch = batch.contiguous()\n        return batch\n\n    def get_patch_offline(self, idx):\n        data_source = torch.load(osp.join(self.path_data, ""patches_source{:06d}.pt"".format(idx)))\n        data_target = torch.load(osp.join(self.path_data, ""patches_target{:06d}.pt"".format(idx)))\n        if self.transform is not None:\n            data_source = self.transform(data_source)\n            data_target = self.transform(data_target)\n\n        if(hasattr(data_source, ""multiscale"")):\n            batch = MultiScalePair.make_pair(data_source, data_target)\n        else:\n            batch = Pair.make_pair(data_source, data_target)\n        return batch.contiguous()\n\n    def get(self, idx):\n        if self.is_offline:\n            return self.get_patch_offline(idx)\n        else:\n            return self.get_patch_online(idx)\n\n    def __len__(self):\n        size_dataset = len(os.listdir(self.path_data))\n        if self.is_offline:\n            size_dataset = size_dataset // 2\n        return size_dataset\n\n\nclass Fragment3DMatch(Base3DMatch):\n    r""""""\n        Fragment extracted from :the Princeton 3DMatch dataset\\n\n        `""3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions""\n        <https://arxiv.org/pdf/1603.08182.pdf>`_\n        paper, containing rgbd frames of the following dataset:\n        `"" SUN3D: A Database of Big Spaces Reconstructed using SfM and Object Labels\n        ""<http://sun3d.cs.princeton.edu/>`\n        `""Scene Coordinate Regression Forests for Camera Relocalization in RGB-D Images\n        ""<https://www.microsoft.com/en-us/research/publication/scene-coordinate-regression-forests-for-camera-relocalization-in-rgb-d-images/>`\n        `""Unsupervised Feature Learning for 3D Scene Labeling\n        ""<http://rgbd-dataset.cs.washington.edu/dataset/rgbd-scenes-v2/>`\n        `""BundleFusion: Real-time Globally Consistent 3D Reconstruction using Online\n        Surface Re-integration\n        ""<http://graphics.stanford.edu/projects/bundlefusion/>`\n        `""Learning to Navigate the Energy Landscape\n        ""<http://graphics.stanford.edu/projects/reloc/>`\n\n        Args:\n\n            root (string): Root directory where the dataset should be saved\n\n            num_frame_per_fragment (int, optional): indicate the number of frames\n                we use to build fragments. If it is equal to 0, then we don\'t\n                build fragments and use the raw frames.\n\n            mode (string, optional): If :obj:`True`, loads the training dataset,\n            otherwise the test dataset. (default: :obj:`True`)\n\n            min_overlap_ratio(float, optional): the minimum overlap we should have to match two fragments (overlap is the number of points in a fragment that matches in an other fragment divided by the number of points)\n            max_overlap_ratio(float, optional): the maximum overlap we should have to match two fragments\n            max_dist_overlap(float, optional): minimum distance to consider that a point match with an other.\n            tsdf_voxel_size(float, optional): the size of the tsdf voxel grid to perform fine RGBD fusion to create fine fragments\n            depth_thresh: threshold to remove depth pixel that are two far.\n\n            is_fine: fine mode for the fragment fusion\n\n\n            transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                every access. (default: :obj:`None`)\n\n            pre_transform (callable, optional): A function/transform that takes in\n                an :obj:`torch_geometric.data.Data` object and returns a\n                transformed version. The data object will be transformed before\n                being saved to disk. (default: :obj:`None`)\n            pre_filter (callable, optional): A function that takes in an\n                :obj:`torch_geometric.data.Data` object and returns a boolean\n                value, indicating whether the data object should be included in the\n                final dataset. (default: :obj:`None`)\n            num_random_pt: number of point we select when we test\n        """"""\n    def __init__(\n        self,\n        root,\n        num_frame_per_fragment=50,\n        mode=""train_small"",\n        min_overlap_ratio=0.3,\n        max_overlap_ratio=1.0,\n        max_dist_overlap=0.01,\n        tsdf_voxel_size=0.02,\n        limit_size=700,\n        depth_thresh=6,\n        is_fine=True,\n        transform=None,\n        pre_transform=None,\n        pre_transform_fragment=None,\n        pre_filter=None,\n        verbose=False,\n        debug=False,\n        is_online_matching=False,\n        num_pos_pairs=1024,\n    ):\n\n\n        self.is_patch = False\n        super(Fragment3DMatch, self).__init__(\n            root,\n            num_frame_per_fragment,\n            mode,\n            min_overlap_ratio,\n            max_overlap_ratio,\n            max_dist_overlap,\n            tsdf_voxel_size,\n            limit_size,\n            depth_thresh,\n            is_fine,\n            transform,\n            pre_transform,\n            pre_transform_fragment,\n            pre_filter,\n            verbose,\n            debug,\n        )\n        self.path_match = osp.join(self.processed_dir, self.mode, ""matches"")\n        self.list_fragment = [f for f in os.listdir(self.path_match) if ""matches"" in f]\n        self.is_online_matching = is_online_matching\n        self.num_pos_pairs = num_pos_pairs\n\n    def get_fragment(self, idx):\n\n        match = np.load(osp.join(self.path_match, ""matches{:06d}.npy"".format(idx)), allow_pickle=True).item()\n        data_source = torch.load(match[""path_source""]).to(torch.float)\n        data_target = torch.load(match[""path_target""]).to(torch.float)\n        new_pair = torch.from_numpy(match[""pair""])\n\n        if self.transform is not None:\n            data_source = self.transform(data_source)\n            data_target = self.transform(data_target)\n\n        if(hasattr(data_source, ""multiscale"")):\n            batch = MultiScalePair.make_pair(data_source, data_target)\n        else:\n            batch = Pair.make_pair(data_source, data_target)\n        if self.is_online_matching:\n            new_match = compute_overlap_and_matches(\n                Data(pos=data_source.pos), Data(pos=data_target.pos), self.max_dist_overlap\n            )\n            batch.pair_ind = torch.from_numpy(new_match[""pair""].copy())\n        else:\n            pair = tracked_matches(data_source, data_target, new_pair)\n            batch.pair_ind = pair\n\n        num_pos_pairs = len(batch.pair_ind)\n        if self.num_pos_pairs < len(batch.pair_ind):\n            num_pos_pairs = self.num_pos_pairs\n\n        rand_ind = torch.randperm(len(batch.pair_ind))[:num_pos_pairs]\n        batch.pair_ind = batch.pair_ind[rand_ind]\n        batch.size_pair_ind = torch.tensor([num_pos_pairs])\n        return batch.contiguous()\n\n    def get(self, idx):\n        return self.get_fragment(idx)\n\n    def __len__(self):\n        return len(self.list_fragment)\n\n\nclass General3DMatchDataset(BaseSiameseDataset):\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n        pre_transform = self.pre_transform\n        train_transform = self.train_transform\n        test_transform = self.test_transform\n        pre_filter = self.pre_filter\n        test_pre_filter = getattr(self, ""test_pre_filter"", None)\n        self.is_patch = dataset_opt.is_patch\n\n        if dataset_opt.is_patch:\n            self.train_dataset = Patch3DMatch(\n                root=self._data_path,\n                mode=""train"",\n                radius_patch=dataset_opt.radius_patch,\n                num_frame_per_fragment=dataset_opt.num_frame_per_fragment,\n                max_dist_overlap=dataset_opt.max_dist_overlap,\n                min_overlap_ratio=dataset_opt.min_overlap_ratio,\n                tsdf_voxel_size=dataset_opt.tsdf_voxel_size,\n                limit_size=dataset_opt.limit_size,\n                depth_thresh=dataset_opt.depth_thresh,\n                pre_transform=pre_transform,\n                transform=train_transform,\n                num_random_pt=dataset_opt.num_random_pt,\n                is_offline=dataset_opt.is_offline,\n                pre_filter=pre_filter,\n            )\n\n            self.test_dataset = Patch3DMatch(\n                root=self._data_path,\n                mode=""val"",\n                radius_patch=dataset_opt.radius_patch,\n                num_frame_per_fragment=dataset_opt.num_frame_per_fragment,\n                max_dist_overlap=dataset_opt.max_dist_overlap,\n                min_overlap_ratio=dataset_opt.min_overlap_ratio,\n                tsdf_voxel_size=dataset_opt.tsdf_voxel_size,\n                limit_size=dataset_opt.limit_size,\n                depth_thresh=dataset_opt.depth_thresh,\n                pre_transform=pre_transform,\n                transform=test_transform,\n                num_random_pt=dataset_opt.num_random_pt,\n                is_offline=dataset_opt.is_offline,\n                pre_filter=test_pre_filter,\n            )\n        else:\n\n            self.train_dataset = Fragment3DMatch(\n                root=self._data_path,\n                mode=""train"",\n                num_frame_per_fragment=dataset_opt.num_frame_per_fragment,\n                max_dist_overlap=dataset_opt.max_dist_overlap,\n                min_overlap_ratio=dataset_opt.min_overlap_ratio,\n                tsdf_voxel_size=dataset_opt.tsdf_voxel_size,\n                limit_size=dataset_opt.limit_size,\n                depth_thresh=dataset_opt.depth_thresh,\n                pre_transform=pre_transform,\n                transform=train_transform,\n                pre_filter=pre_filter,\n                is_online_matching=dataset_opt.is_online_matching,\n                num_pos_pairs=dataset_opt.num_pos_pairs)\n\n            self.test_dataset = Fragment3DMatch(\n                root=self._data_path,\n                mode=""val"",\n                num_frame_per_fragment=dataset_opt.num_frame_per_fragment,\n                max_dist_overlap=dataset_opt.max_dist_overlap,\n                min_overlap_ratio=dataset_opt.min_overlap_ratio,\n                tsdf_voxel_size=dataset_opt.tsdf_voxel_size,\n                limit_size=dataset_opt.limit_size,\n                depth_thresh=dataset_opt.depth_thresh,\n                pre_transform=pre_transform,\n                transform=test_transform,\n                is_online_matching=False,\n                num_pos_pairs=dataset_opt.num_pos_pairs,\n            )\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""\n        Factory method for the tracker\n\n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        if self.is_patch:\n            return PatchRegistrationTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n        else:\n            return FragmentRegistrationTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n'"
torch_points3d/datasets/registration/pair.py,9,"b'import torch\nfrom typing import List, Optional\nfrom torch_geometric.data import Data\nfrom torch_geometric.data import Batch\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch, MultiScaleData\nimport re\n\nclass Pair(Data):\n\n    def __init__(\n            self,\n            x=None,\n            y=None,\n            pos=None,\n            x_target=None,\n            pos_target=None,\n            **kwargs,\n    ):\n        self.__data_class__ = Data\n        super(Pair, self).__init__(x=x, pos=pos,\n                                   x_target=x_target, pos_target=pos_target, **kwargs)\n\n\n    @classmethod\n    def make_pair(cls, data_source, data_target):\n        """"""\n        add in a Data object the source elem, the target elem.\n        """"""\n        # add concatenation of the point cloud\n        batch = cls()\n        for key in data_source.keys:\n            batch[key] = data_source[key]\n        for key_target in data_target.keys:\n            batch[key_target+""_target""] = data_target[key_target]\n        if(batch.x is None):\n            batch[""x_target""] = None\n        return batch.contiguous()\n\n    def to_data(self):\n        data_source = self.__data_class__()\n        data_target = self.__data_class__()\n        for key in self.keys:\n            match = re.search(r""(.+)_target$"", key)\n            if match is None:\n                data_source[key] = self[key]\n            else:\n                new_key = match.groups()[0]\n                data_target[new_key] = self[key]\n        return data_source, data_target\n\n    @property\n    def num_nodes_target(self):\n        for key, item in self(\'x_target\', \'pos_target\', \'norm_target\', \'batch_target\'):\n            return item.size(self.__cat_dim__(key, item))\n        return None\n\n\nclass MultiScalePair(Pair):\n    def __init__(\n            self,\n            x=None,\n            y=None,\n            pos=None,\n            multiscale: Optional[List[Data]] = None,\n            upsample: Optional[List[Data]] = None,\n            x_target=None,\n            pos_target=None,\n            multiscale_target: Optional[List[Data]] = None,\n            upsample_target: Optional[List[Data]] = None,\n            **kwargs,\n    ):\n        super(MultiScalePair, self).__init__(x=x, pos=pos,\n                                             multiscale=multiscale,\n                                             upsample=upsample,\n                                             x_target=x_target, pos_target=pos_target,\n                                             multiscale_target=multiscale_target,\n                                             upsample_target=upsample_target,\n                                             **kwargs)\n        self.__data_class__ = MultiScaleData\n\n    def apply(self, func, *keys):\n        r""""""Applies the function :obj:`func` to all tensor and Data attributes\n        :obj:`*keys`. If :obj:`*keys` is not given, :obj:`func` is applied to\n        all present attributes.\n        """"""\n        for key, item in self(*keys):\n            if torch.is_tensor(item):\n                self[key] = func(item)\n        for scale in range(self.num_scales):\n            self.multiscale[scale] = self.multiscale[scale].apply(func)\n            self.multiscale_target[scale] = self.multiscale_target[scale].apply(func)\n\n        for up in range(self.num_upsample):\n            self.upsample[up] = self.upsample[up].apply(func)\n            self.upsample_target[up] = self.upsample_target[up].apply(func)\n        return self\n\n    @property\n    def num_scales(self):\n        """""" Number of scales in the multiscale array\n        """"""\n        return len(self.multiscale) if self.multiscale else 0\n\n    @property\n    def num_upsample(self):\n        """""" Number of upsample operations\n        """"""\n        return len(self.upsample) if self.upsample else 0\n\n    @classmethod\n    def from_data(cls, data):\n        ms_data = cls()\n        for k, item in data:\n            ms_data[k] = item\n        return ms_data\n\n\nclass PairBatch(Pair):\n\n    def __init__(self, batch=None, batch_target=None, **kwargs):\n        r""""""\n        Pair batch for message passing\n        """"""\n        self.batch_target = batch_target\n        self.batch = None\n        super(PairBatch, self).__init__(**kwargs)\n        self.__data_class__ = Batch\n\n    @staticmethod\n    def from_data_list(data_list):\n        r""""""\n        from a list of torch_points3d.datasets.registation.pair.Pair objects, create\n        a batch\n        Warning : follow_batch is not here yet...\n        """"""\n        assert isinstance(data_list[0], Pair)\n        data_list_s, data_list_t = list(map(list, zip(*[data.to_data() for data in data_list])))\n        if hasattr(data_list_s[0], \'pair_ind\'):\n            pair_ind = concatenate_pair_ind(data_list_s, data_list_t)\n        else:\n            pair_ind = None\n        batch_s = Batch.from_data_list(data_list_s)\n        batch_t = Batch.from_data_list(data_list_t)\n        pair = PairBatch.make_pair(batch_s, batch_t)\n        pair.pair_ind = pair_ind\n        return pair.contiguous()\n\nclass PairMultiScaleBatch(MultiScalePair):\n\n    def __init__(self, batch=None, batch_target=None, **kwargs):\n        self.batch = batch\n        self.batch_target = batch_target\n        super(PairMultiScaleBatch, self).__init__(**kwargs)\n        self.__data_class__ = MultiScaleBatch\n\n    @staticmethod\n    def from_data_list(data_list):\n        r""""""\n        from a list of torch_points3d.datasets.registation.pair.Pair objects, create\n        a batch\n        Warning : follow_batch is not here yet...\n        """"""\n        data_list_s, data_list_t = list(map(list, zip(*[data.to_data() for data in data_list])))\n        if hasattr(data_list_s[0], \'pair_ind\'):\n            pair_ind = concatenate_pair_ind(data_list_s, data_list_t).to(torch.long)\n        else:\n            pair_ind = None\n        batch_s = MultiScaleBatch.from_data_list(data_list_s)\n        batch_t = MultiScaleBatch.from_data_list(data_list_t)\n        pair = PairMultiScaleBatch.make_pair(batch_s, batch_t)\n        pair.pair_ind = pair_ind\n        return pair.contiguous()\n\n\nclass DensePairBatch(Pair):\n    r"""""" A classic batch object wrapper with :class:`Pair`. Used for Dense Pair Batch (ie pointcloud with fixed size).\n    """"""\n\n    def __init__(self, batch=None, **kwargs):\n        super(DensePairBatch, self).__init__(**kwargs)\n\n        self.batch = batch\n        self.__data_class__ = Data\n\n    @staticmethod\n    def from_data_list(data_list):\n        r""""""Constructs a batch object from a python list holding\n        :class:`torch_geometric.data.Data` objects.\n        """"""\n        keys = [set(data.keys) for data in data_list]\n        keys = list(set.union(*keys))\n\n        # Check if all dimensions matches and we can concatenate data\n        # if len(data_list) > 0:\n        #    for data in data_list[1:]:\n        #        for key in keys:\n        #            assert data_list[0][key].shape == data[key].shape\n\n        batch = DensePairBatch()\n        batch.__data_class__ = data_list[0].__class__\n\n        for key in keys:\n            batch[key] = []\n\n        for _, data in enumerate(data_list):\n            for key in data.keys:\n                item = data[key]\n                batch[key].append(item)\n\n        for key in batch.keys:\n            item = batch[key][0]\n            if (\n                torch.is_tensor(item)\n                or isinstance(item, int)\n                or isinstance(item, float)\n            ):\n                if key != ""pair_ind"":\n                    batch[key] = torch.stack(batch[key])\n            else:\n                raise ValueError(""Unsupported attribute type"")\n        # add pair_ind for dense data too\n        if hasattr(data_list[0], \'pair_ind\'):\n            pair_ind = concatenate_pair_ind(data_list, data_list).to(torch.long)\n        else:\n            pair_ind = None\n        batch.pair_ind = pair_ind\n        return batch.contiguous()\n        # return [batch.x.transpose(1, 2).contiguous(), batch.pos, batch.y.view(-1)]\n\n    @property\n    def num_graphs(self):\n        """"""Returns the number of graphs in the batch.""""""\n        return self.batch[-1].item() + 1\n\n\ndef concatenate_pair_ind(list_data_source, list_data_target):\n    """"""\n    for a list of pair of indices batched, change the index it refers to wrt the batch index\n    Parameters\n    ----------\n    list_data_source: list[Data]\n    list_data_target: list[Data]\n    Returns\n    -------\n    torch.Tensor\n        indices of y corrected wrt batch indices\n\n\n    """"""\n\n    assert len(list_data_source) == len(list_data_target)\n    assert hasattr(list_data_source[0], ""pair_ind"")\n    list_pair_ind = []\n    cum_size = torch.zeros(2)\n    for i in range(len(list_data_source)):\n        size = torch.tensor([len(list_data_source[i].pos),\n                             len(list_data_target[i].pos)])\n        list_pair_ind.append(list_data_source[i].pair_ind + cum_size)\n        cum_size = cum_size + size\n    return torch.cat(list_pair_ind, 0)\n'"
torch_points3d/datasets/registration/test3dmatch.py,2,"b'import numpy as np\nimport os\nimport os.path as osp\nimport torch\nimport json\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.datasets.registration.basetest import Base3DMatchTest\nfrom torch_points3d.datasets.registration.basetest import SimplePatch\nfrom torch_points3d.datasets.registration.utils import PatchExtractor\nfrom torch_points3d.datasets.registration.detector import RandomDetector\n\n\nclass Test3DMatch(Base3DMatchTest):\n\n    def __init__(self,\n                 root,\n                 radius_patch=0.3,\n                 pre_transform=None,\n                 pre_filter=None,\n                 transform=None,\n                 verbose=False,\n                 debug=False,\n                 num_random_pt=5000):\n\n        super(Test3DMatch, self).__init__(root,\n                                          transform,\n                                          pre_transform,\n                                          pre_filter,\n                                          verbose, debug,\n                                          num_random_pt)\n\n        self.radius_patch = radius_patch\n        self.patch_extractor = PatchExtractor(self.radius_patch)\n        self.path_table = osp.join(self.processed_dir, \'fragment\')\n        with open(osp.join(self.path_table, \'table.json\'), \'r\') as f:\n            self.table = json.load(f)\n\n    def __getitem__(self, idx):\n        r""""""Gets the data object at index :obj:`idx` and transforms it (in case\n        a :obj:`self.transform` is given).\n        In case :obj:`idx` is a slicing object, *e.g.*, :obj:`[2:5]`, a list, a\n        tuple, a  LongTensor or a BoolTensor, will return a subset of the\n        dataset at the specified indices.""""""\n        data = torch.load(\n            osp.join(self.path_table, \'fragment_{:06d}.pt\'.format(idx)))\n        if(self.transform is not None):\n            data = self.transform(data)\n        if(self.num_random_pt > 0):\n            detector = RandomDetector(self.num_random_pt)\n            data = detector(data)\n        return data\n\n    def get_patches(self, idx):\n        fragment = torch.load(\n            osp.join(self.path_table, \'fragment_{:06d}.pt\'.format(idx)))\n        patch_dataset = [self.patch_extractor(fragment, fragment.keypoints[i])\n                         for i in range(self.num_random_pt)]\n\n        simple_patch = SimplePatch(patch_dataset, self.transform)\n        return simple_patch\n\n    def __len__(self):\n        return len(self.table)\n\n    def get_table(self):\n        return self.table\n\n\nclass Test3DMatchDataset(BaseDataset):\n    """"""\n    this class is a dataset just for testing.\n    if we compute descriptors on patches,  at each iteration,\n    the test dataset must change\n    """"""\n\n    def __init__(self, dataset_opt):\n\n        super().__init__(dataset_opt)\n        pre_transform = self.pre_transform\n        test_transform = self.test_transform\n\n        self.base_dataset = Test3DMatch(root=self._data_path,\n                                        radius_patch=dataset_opt.radius_patch,\n                                        pre_transform=pre_transform,\n                                        transform=test_transform,\n                                        num_random_pt=dataset_opt.num_random_pt)\n\n        if(dataset_opt.is_patch):\n            self.test_dataset = self.base_dataset.get_patches(0)\n        else:\n            self.test_dataset = self.base_dataset\n\n    def set_patches(self, idx):\n        self.test_dataset = self.base_dataset.get_patches(idx)\n\n    def get_name(self, idx):\n        """"""\n        return a pair of string which indicate the name of the scene and\n        the name of the point cloud\n        """"""\n        table = self.base_dataset.get_table()[str(idx)]\n        return table[\'scene_path\'], table[\'fragment_name\']\n\n    @property\n    def num_fragment(self):\n        return len(self.base_dataset)\n'"
torch_points3d/datasets/registration/utils.py,15,"b'import collections\nimport errno\nimport numpy as np\nimport os\nimport os.path as osp\nimport torch\nfrom torch_geometric.data import Data\nfrom torch_points_kernels.points_cpu import ball_query\nimport imageio\nfrom tqdm.auto import tqdm\n\nfrom torch_points3d.core.data_transform import GridSampling3D, SaveOriginalPosId\nfrom torch_geometric.transforms import Compose\nimport torch_points3d.datasets.registration.fusion as fusion\n\n\ndef to_list(x):\n    """"""\n    taken from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/dataset.html#Dataset\n    """"""\n    if not isinstance(x, collections.Iterable) or isinstance(x, str):\n        x = [x]\n    return x\n\n\ndef files_exist(files):\n    """"""\n    taken from https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/data/dataset.html#Dataset\n    """"""\n\n    return all([osp.exists(f) for f in files])\n\n\ndef makedirs(path):\n    """"""\n    taken from https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/data/makedirs.py\n    """"""\n    try:\n        os.makedirs(osp.expanduser(osp.normpath(path)))\n    except OSError as e:\n        if e.errno != errno.EEXIST and osp.isdir(path):\n            raise e\n\n\ndef get_urls(filename):\n    res = []\n    with open(filename, ""r"") as f:\n        res = f.readlines()\n    return res\n\n\ndef extract_pcd(depth_image, K, color_image=None):\n    """"""\n    transform a depth image into a pointcloud (here a numpy array)\n    """"""\n\n    Z = (depth_image / 1000).ravel()\n    mask_z = (Z < 6) * (Z > 0)\n    X, Y = np.meshgrid(np.arange(depth_image.shape[1]), np.arange(depth_image.shape[0]))\n\n    Xworld = (X.ravel()[mask_z] + 0.5 - K[0, 2]) * Z[mask_z] / K[0, 0]\n    Yworld = (Y.ravel()[mask_z] + 0.5 - K[1, 2]) * Z[mask_z] / K[1, 1]\n\n    pcd = np.vstack((Xworld, Yworld, Z[mask_z])).T\n    if color_image is None:\n        return pcd\n    else:\n        color = color_image.reshape(-1, 3)[mask_z, :]\n        return pcd, color\n\n\ndef rgbd2pcd(path_img, path_intrinsic, path_trans, path_color=None):\n\n    # read imageio\n    depth = imageio.imread(path_img)\n\n    intrinsic = np.loadtxt(path_intrinsic)\n    trans = np.loadtxt(path_trans)\n    if path_color is not None:\n        color_image = imageio.imread(path_color)\n        pcd, color = extract_pcd(depth, intrinsic, color_image)\n        pcd = pcd.dot(trans[:3, :3].T) + trans[:3, 3]\n        return pcd, color\n    else:\n        pcd = extract_pcd(depth, intrinsic)\n        pcd = pcd.dot(trans[:3, :3].T) + trans[:3, 3]\n        return pcd\n\n\ndef rgbd2fragment_rough(\n    list_path_img,\n    path_intrinsic,\n    list_path_trans,\n    out_path,\n    num_frame_per_fragment=5,\n    pre_transform=None,\n    list_path_color=None,\n):\n\n    one_fragment = []\n    one_color = []\n    ind = 0\n    for i, path_img in tqdm(enumerate(list_path_img), total=len(list_path_img)):\n        path_trans = list_path_trans[i]\n        path_color = None\n        if list_path_color is not None:\n            path_color = list_path_color[i]\n            pcd, color = rgbd2pcd(path_img, path_intrinsic, path_trans, path_color=path_color)\n            one_fragment.append(pcd)\n            one_color.append(color)\n        else:\n            pcd = rgbd2pcd(path_img, path_intrinsic, path_trans, path_color=path_color)\n            one_fragment.append(pcd)\n        if (i + 1) % num_frame_per_fragment == 0:\n            pos = torch.from_numpy(np.concatenate(one_fragment, axis=0))\n            if list_path_color is None:\n                torch_data = Data(pos=pos)\n            else:\n                color = torch.from_numpy(np.concatenate(one_color, axis=0))\n                torch_data = Data(pos=pos, color=color)\n            if pre_transform is not None:\n                torch_data = pre_transform(torch_data)\n            torch.save(torch_data, osp.join(out_path, ""fragment_{:06d}.pt"".format(ind)))\n            ind += 1\n            one_fragment = []\n            one_color = []\n    # concatenate all fragment\n\n    # create batches\n    # save fragments for each batches using a simple batch\n\n\ndef filter_pair(pair, dist):\n    """"""\n    give a pair of indices where the distance is positive\n    """"""\n    pair = pair[dist[:, 0] >= 0]\n    if len(pair) > 0:\n        pair = pair.numpy()[:, ::-1]\n    else:\n        pair = np.array([])\n    return pair\n\n\ndef compute_overlap_and_matches(data1, data2, max_distance_overlap, reciprocity=False, num_pos=1, rot_gt=torch.eye(3)):\n\n    # we can use ball query on cpu because the points are sorted\n    # print(len(data1.pos), len(data2.pos), max_distance_overlap)\n    pair, dist = ball_query(data2.pos.to(torch.float),\n                            data1.pos.to(torch.float) @ rot_gt.T,\n                            radius=max_distance_overlap,\n                            max_num=num_pos, mode=1, sorted=True)\n    pair = filter_pair(pair, dist)\n    pair2 = []\n    overlap = [pair.shape[0] / len(data1.pos)]\n    if reciprocity:\n        pair2, dist2 = ball_query(data1.pos.to(torch.float) @ rot_gt.T,\n                                  data2.pos.to(torch.float),\n                                  radius=max_distance_overlap,\n                                  max_num=num_pos, mode=1, sorted=True)\n        pair2 = filter_pair(pair2, dist2)\n        overlap.append(pair2.shape[0] / len(data2.pos))\n    # overlap = pair.shape[0] / \\\n    #    (len(data1.pos) + len(data2.pos) - pair.shape[0])\n    # print(pair)\n\n    # print(path1, path2, ""overlap="", overlap)\n    output = dict(pair=pair, pair2=pair2, overlap=overlap)\n    return output\n\ndef compute_subsampled_matches(data1, data2, voxel_size=0.1, max_distance_overlap=0.02):\n    """"""\n    compute matches on subsampled version of data and track ind\n    """"""\n    grid_sampling = Compose([SaveOriginalPosId(), GridSampling3D(voxel_size, mode=\'last\')])\n    subsampled_data = grid_sampling(data1.clone())\n    origin_id = subsampled_data.origin_id.numpy()\n    pair = compute_overlap_and_matches(subsampled_data, data2, max_distance_overlap)[\'pair\']\n    pair[:, 0] = origin_id[pair[:, 0]]\n    return torch.from_numpy(pair.copy())\n\ndef get_3D_bound(list_path_img, path_intrinsic, list_path_trans, depth_thresh, limit_size=600, voxel_size=0.01):\n    vol_bnds = np.zeros((3, 2))\n    list_min = np.zeros((3, len(list_path_img)))\n    list_max = np.zeros((3, len(list_path_img)))\n    for i, path_img in tqdm(enumerate(list_path_img), total=len(list_path_img)):\n        # read imageio\n        depth = imageio.imread(path_img) / 1000.0\n        depth[depth > depth_thresh] = 0\n        intrinsic = np.loadtxt(path_intrinsic)\n        pose = np.loadtxt(list_path_trans[i])\n        view_frust_pts = fusion.get_view_frustum(depth, intrinsic, pose)\n        list_min[:, i] = np.amin(view_frust_pts, axis=1)\n        list_max[:, i] = np.amax(view_frust_pts, axis=1)\n    # take the quantile instead of the min to be more robust to outilers frames\n\n    vol_bnds[:, 0] = np.quantile(list_min, 0.1, axis=1)\n    vol_bnds[:, 1] = np.quantile(list_max, 0.9, axis=1)\n\n    # remove some voxel that are on the edge to control the size of the tsdf.\n    vol_dim = (vol_bnds[:, 1] - vol_bnds[:, 0]) / voxel_size\n    for i in range(3):\n        # add and substract delta to limit the size\n        if(vol_dim[i] > limit_size):\n            delta = voxel_size * (vol_dim[i] - limit_size) * 0.5\n            vol_bnds[i][0] += delta\n            vol_bnds[i][1] -= delta\n    return vol_bnds\n\n\ndef rgbd2fragment_fine(\n    list_path_img,\n    path_intrinsic,\n    list_path_trans,\n    out_path,\n    num_frame_per_fragment=5,\n    voxel_size=0.01,\n    pre_transform=None,\n    depth_thresh=6,\n    save_pc=True,\n    limit_size=600\n):\n    """"""\n    fuse rgbd frame with a tsdf volume and get the mesh using marching cube.\n    """"""\n\n    ind = 0\n    begin = 0\n    end = num_frame_per_fragment\n\n    vol_bnds = get_3D_bound(list_path_img[begin:end], path_intrinsic, list_path_trans[begin:end], depth_thresh, voxel_size=voxel_size, limit_size=limit_size)\n\n    print(vol_bnds)\n    tsdf_vol = fusion.TSDFVolume(vol_bnds, voxel_size=voxel_size)\n    for i, path_img in tqdm(enumerate(list_path_img), total=len(list_path_img)):\n\n        depth = imageio.imread(path_img).astype(float) / 1000.0\n        depth[depth > depth_thresh] = 0\n        depth[depth <= 0] = 0\n        intrinsic = np.loadtxt(path_intrinsic)\n        pose = np.loadtxt(list_path_trans[i])\n        tsdf_vol.integrate(depth, intrinsic, pose, obs_weight=1.0)\n        if (i + 1) % num_frame_per_fragment == 0:\n\n            if save_pc:\n                pcd = tsdf_vol.get_point_cloud(0.35, 0.0)\n                torch_data = Data(pos=torch.from_numpy(pcd.copy()))\n            else:\n                verts, faces, norms = tsdf_vol.get_mesh()\n                torch_data = Data(pos=torch.from_numpy(verts.copy()), norm=torch.from_numpy(norms.copy()))\n            if pre_transform is not None:\n                torch_data = pre_transform(torch_data)\n            torch.save(torch_data, osp.join(out_path, ""fragment_{:06d}.pt"".format(ind)))\n            ind += 1\n\n            if i + 1 < len(list_path_img):\n                begin = i + 1\n                if begin + num_frame_per_fragment < len(list_path_img):\n                    end = begin + num_frame_per_fragment\n                    vol_bnds = get_3D_bound(\n                        list_path_img[begin:end], path_intrinsic, list_path_trans[begin:end], depth_thresh, voxel_size=voxel_size, limit_size=limit_size\n                    )\n                else:\n                    vol_bnds = get_3D_bound(\n                        list_path_img[begin:], path_intrinsic, list_path_trans[begin:],\n                        depth_thresh, voxel_size=voxel_size, limit_size=limit_size\n                    )\n                tsdf_vol = fusion.TSDFVolume(vol_bnds, voxel_size=voxel_size)\n\n\nclass PatchExtractor:\n    r""""""\n    Extract patches on a point cloud\n    """"""\n\n    def __init__(self, radius_patch):\n        self.radius_patch = radius_patch\n\n    def __call__(self, data: Data, ind):\n\n        pos = data.pos\n        point = pos[ind].view(1, 3)\n        ind, dist = ball_query(point, pos, radius=self.radius_patch, max_num=-1, mode=1)\n\n        row, col = ind[dist[:, 0] > 0].t()\n        patch = Data()\n        for key in data.keys:\n            if torch.is_tensor(data[key]):\n                if torch.all(col < data[key].shape[0]):\n                    patch[key] = data[key][col]\n\n        return patch\n\n\ndef tracked_matches(data_s, data_t, pair):\n    """"""\n    allow to keep the index that are still present after a sparse input\n    Parameters:\n    pair : P x 2 indices of the matched points before any transformation\n    """"""\n\n    pair_np = pair.numpy()\n    mask_s = np.isin(pair_np[:, 0], data_s.origin_id.numpy())\n    mask_t = np.isin(pair_np[:, 1], data_t.origin_id.numpy())\n    # print(data_s.origin_id.shape)\n    # print(data_s.pos.shape)\n    # print(data_s.xyz.shape)\n    mask = np.logical_and(mask_s, mask_t)\n    filtered_pair = pair_np[mask]\n\n    table_s = dict(zip(data_s.origin_id.numpy(),\n                       np.arange(0, len(data_s.pos))))\n    table_t = dict(zip(data_t.origin_id.numpy(),\n                       np.arange(0, len(data_t.pos))))\n    res = torch.tensor([[table_s[p[0]], table_t[p[1]]] for p in filtered_pair]).to(torch.long)\n    return res\n'"
torch_points3d/datasets/segmentation/__init__.py,0,"b'IGNORE_LABEL: int = -1\n\nfrom .shapenet import ShapeNet, ShapeNetDataset\nfrom .s3dis import S3DISFusedDataset, S3DIS1x1Dataset, S3DISOriginalFused, S3DISSphere\nfrom .scannet import ScannetDataset, Scannet\n'"
torch_points3d/datasets/segmentation/s3dis.py,29,"b'import os\nimport os.path as osp\nfrom itertools import repeat, product\nimport numpy as np\nimport h5py\nimport torch\nimport random\nimport glob\nfrom plyfile import PlyData, PlyElement\nfrom torch_geometric.data import InMemoryDataset, Data, download_url, extract_zip, Dataset\nfrom torch_geometric.data.dataset import files_exist\nfrom torch_geometric.data import DataLoader\nfrom torch_geometric.datasets import S3DIS as S3DIS1x1\nimport torch_geometric.transforms as T\nimport logging\nfrom sklearn.neighbors import NearestNeighbors, KDTree\nfrom tqdm.auto import tqdm as tq\nimport csv\nimport pandas as pd\nimport pickle\n\nfrom torch_points3d.datasets.samplers import BalancedRandomSampler\nimport torch_points3d.core.data_transform as cT\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\nlog = logging.getLogger(__name__)\n\nS3DIS_NUM_CLASSES = 13\n\nINV_OBJECT_LABEL = {\n    0: ""ceiling"",\n    1: ""floor"",\n    2: ""wall"",\n    3: ""beam"",\n    4: ""column"",\n    5: ""window"",\n    6: ""door"",\n    7: ""chair"",\n    8: ""table"",\n    9: ""bookcase"",\n    10: ""sofa"",\n    11: ""board"",\n    12: ""clutter"",\n}\n\nOBJECT_COLOR = np.asarray(\n    [\n        [233, 229, 107],  #\'ceiling\' .-> .yellow\n        [95, 156, 196],  #\'floor\' .-> . blue\n        [179, 116, 81],  #\'wall\'  ->  brown\n        [241, 149, 131],  #\'beam\'  ->  salmon\n        [81, 163, 148],  #\'column\'  ->  bluegreen\n        [77, 174, 84],  #\'window\'  ->  bright green\n        [108, 135, 75],  #\'door\'   ->  dark green\n        [41, 49, 101],  #\'chair\'  ->  darkblue\n        [79, 79, 76],  #\'table\'  ->  dark grey\n        [223, 52, 52],  #\'bookcase\'  ->  red\n        [89, 47, 95],  #\'sofa\'  ->  purple\n        [81, 109, 114],  #\'board\'   ->  grey\n        [233, 233, 229],  #\'clutter\'  ->  light grey\n        [0, 0, 0],  # unlabelled .->. black\n    ]\n)\n\nOBJECT_LABEL = {name: i for i, name in INV_OBJECT_LABEL.items()}\n\nVALIDATION_ROOMS = [\n    ""hallway_1"",\n    ""hallway_6"",\n    ""hallway_11"",\n    ""office_1"",\n    ""office_6"",\n    ""office_11"",\n    ""office_16"",\n    ""office_21"",\n    ""office_26"",\n    ""office_31"",\n    ""office_36"",\n    ""WC_2"",\n    ""storage_1"",\n    ""storage_5"",\n    ""conferenceRoom_2"",\n    ""auditorium_1"",\n]\n\n################################### UTILS #######################################\n\n\ndef object_name_to_label(object_class):\n    """"""convert from object name in S3DIS to an int""""""\n    object_label = OBJECT_LABEL.get(object_class, OBJECT_LABEL[""clutter""])\n    return object_label\n\n\ndef read_s3dis_format(train_file, room_name, label_out=True, verbose=False, debug=False):\n    """"""extract data from a room folder""""""\n    raw_path = osp.join(train_file, ""{}.txt"".format(room_name))\n    if debug:\n        reader = pd.read_csv(raw_path, delimiter=""\\n"")\n        RECOMMENDED = 6\n        for idx, row in enumerate(reader.values):\n            row = row[0].split("" "")\n            if len(row) != RECOMMENDED:\n                log.info(""1: {} row {}: {}"".format(raw_path, idx, row))\n\n            try:\n                for r in row:\n                    r = float(r)\n            except:\n                log.info(""2: {} row {}: {}"".format(raw_path, idx, row))\n\n        return True\n    else:\n        room_ver = pd.read_csv(raw_path, sep="" "", header=None).values\n        xyz = np.ascontiguousarray(room_ver[:, 0:3], dtype=""float32"")\n        try:\n            rgb = np.ascontiguousarray(room_ver[:, 3:6], dtype=""uint8"")\n        except ValueError:\n            rgb = np.zeros((room_ver.shape[0], 3), dtype=""uint8"")\n            log.warning(""WARN - corrupted rgb data for file %s"" % raw_path)\n        if not label_out:\n            return xyz, rgb\n        n_ver = len(room_ver)\n        del room_ver\n        nn = NearestNeighbors(1, algorithm=""kd_tree"").fit(xyz)\n        room_labels = np.zeros((n_ver,), dtype=""int64"")\n        room_object_indices = np.zeros((n_ver,), dtype=""int64"")\n        objects = glob.glob(osp.join(train_file, ""Annotations/*.txt""))\n        i_object = 1\n        for single_object in objects:\n            object_name = os.path.splitext(os.path.basename(single_object))[0]\n            if verbose:\n                log.debug(""adding object "" + str(i_object) + "" : "" + object_name)\n            object_class = object_name.split(""_"")[0]\n            object_label = object_name_to_label(object_class)\n            obj_ver = pd.read_csv(single_object, sep="" "", header=None).values\n            _, obj_ind = nn.kneighbors(obj_ver[:, 0:3])\n            room_labels[obj_ind] = object_label\n            room_object_indices[obj_ind] = i_object\n            i_object = i_object + 1\n\n        return (\n            torch.from_numpy(xyz),\n            torch.from_numpy(rgb),\n            torch.from_numpy(room_labels),\n            torch.from_numpy(room_object_indices),\n        )\n\n\ndef to_ply(pos, label, file):\n    assert len(label.shape) == 1\n    assert pos.shape[0] == label.shape[0]\n    pos = np.asarray(pos)\n    colors = OBJECT_COLOR[np.asarray(label)]\n    ply_array = np.ones(\n        pos.shape[0], dtype=[(""x"", ""f4""), (""y"", ""f4""), (""z"", ""f4""), (""red"", ""u1""), (""green"", ""u1""), (""blue"", ""u1"")]\n    )\n    ply_array[""x""] = pos[:, 0]\n    ply_array[""y""] = pos[:, 1]\n    ply_array[""z""] = pos[:, 2]\n    ply_array[""red""] = colors[:, 0]\n    ply_array[""green""] = colors[:, 1]\n    ply_array[""blue""] = colors[:, 2]\n    el = PlyElement.describe(ply_array, ""S3DIS"")\n    PlyData([el], byte_order="">"").write(file)\n\n\ndef add_weights(dataset, train, class_weight_method):\n    L = len(INV_OBJECT_LABEL.keys())\n    if train:\n        weights = torch.ones(L)\n        if class_weight_method is not None:\n\n            idx_classes, counts = torch.unique(dataset.data.y, return_counts=True)\n\n            dataset.idx_classes = torch.arange(L).long()\n            weights[idx_classes] = counts.float()\n            weights = weights.float()\n            weights = weights.mean() / weights\n            if class_weight_method == ""sqrt"":\n                weights = torch.sqrt(weights)\n            elif str(class_weight_method).startswith(""log""):\n                weights = 1 / torch.log(1.1 + weights / weights.sum())\n\n            weights /= torch.sum(weights)\n        log.info(\n            ""CLASS WEIGHT : {}"".format(\n                {name: np.round(weights[index].item(), 4) for index, name in INV_OBJECT_LABEL.items()}\n            )\n        )\n        setattr(dataset, ""weight_classes"", weights)\n    else:\n        setattr(dataset, ""weight_classes"", torch.ones((len(INV_OBJECT_LABEL.keys()))))\n\n    return dataset\n\n\n################################### 1m cylinder s3dis ###################################\n\n\nclass S3DIS1x1Dataset(BaseDataset):\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n\n        pre_transform = self.pre_transform\n        train_dataset = S3DIS1x1(\n            self._data_path,\n            test_area=self.dataset_opt.fold,\n            train=True,\n            pre_transform=self.pre_transform,\n            transform=self.train_transform,\n        )\n        self.test_dataset = S3DIS1x1(\n            self._data_path,\n            test_area=self.dataset_opt.fold,\n            train=False,\n            pre_transform=pre_transform,\n            transform=self.test_transform,\n        )\n\n        self.train_dataset = add_weights(train_dataset, True, dataset_opt.class_weight_method)\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n\n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        from torch_points3d.metrics.segmentation_tracker import SegmentationTracker\n\n        return SegmentationTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n\n\n################################### Used for fused s3dis radius sphere ###################################\n\n\nclass S3DISOriginalFused(InMemoryDataset):\n    """""" Original S3DIS dataset. Each area is loaded individually and can be processed using a pre_collate transform. \n    This transform can be used for example to fuse the area into a single space and split it into \n    spheres or smaller regions. If no fusion is applied, each element in the dataset is a single room by default.\n\n    http://buildingparser.stanford.edu/dataset.html\n\n    Parameters\n    ----------\n    root: str\n        path to the directory where the data will be saved\n    test_area: int\n        number between 1 and 6 that denotes the area used for testing\n    split: str\n        can be one of train, trainval, val or test\n    pre_collate_transform:\n        Transforms to be applied before the data is assembled into samples (apply fusing here for example)\n    keep_instance: bool\n        set to True if you wish to keep instance data\n    pre_transform\n    transform\n    pre_filter\n    """"""\n\n    url = ""https://docs.google.com/forms/d/e/1FAIpQLScDimvNMCGhy_rmBA2gHfDu3naktRm6A8BPwAWWDv-Uhm6Shw/viewform?c=0&w=1""\n    zip_name = ""Stanford3dDataset_v1.2_Version.zip""\n    folders = [""Area_{}"".format(i) for i in range(1, 7)]\n    num_classes = S3DIS_NUM_CLASSES\n\n    def __init__(\n        self,\n        root,\n        test_area=6,\n        split=""train"",\n        transform=None,\n        pre_transform=None,\n        pre_collate_transform=None,\n        pre_filter=None,\n        keep_instance=False,\n        verbose=False,\n        debug=False,\n    ):\n        assert test_area >= 1 and test_area <= 6\n        self.transform = transform\n        self.pre_collate_transform = pre_collate_transform\n        self.test_area = test_area\n        self.keep_instance = keep_instance\n        self.verbose = verbose\n        self.debug = debug\n        self._split = split\n        super(S3DISOriginalFused, self).__init__(root, transform, pre_transform, pre_filter)\n        if split == ""train"":\n            path = self.processed_paths[0]\n        elif split == ""val"":\n            path = self.processed_paths[1]\n        elif split == ""test"":\n            path = self.processed_paths[2]\n        elif split == ""trainval"":\n            path = self.processed_paths[3]\n        else:\n            raise ValueError((f""Split {split} found, but expected either "" ""train, val, trainval or test""))\n        self._load_data(path)\n\n        if split == ""test"":\n            self.raw_test_data = torch.load(self.raw_areas_paths[test_area - 1])\n\n    @property\n    def center_labels(self):\n        if hasattr(self.data, ""center_label""):\n            return self.data.center_label\n        else:\n            return None\n\n    @property\n    def raw_file_names(self):\n        return self.folders\n\n    @property\n    def pre_processed_path(self):\n        pre_processed_file_names = ""preprocessed.pt""\n        return os.path.join(self.processed_dir, pre_processed_file_names)\n\n    @property\n    def raw_areas_paths(self):\n        return [os.path.join(self.processed_dir, ""raw_area_%i.pt"" % i) for i in range(6)]\n\n    @property\n    def processed_file_names(self):\n        test_area = self.test_area\n        return (\n            [""{}_{}.pt"".format(s, test_area) for s in [""train"", ""val"", ""test"", ""trainval""]]\n            + self.raw_areas_paths\n            + [self.pre_processed_path]\n        )\n\n    @property\n    def raw_test_data(self):\n        return self._raw_test_data\n\n    @raw_test_data.setter\n    def raw_test_data(self, value):\n        self._raw_test_data = value\n\n    def download(self):\n        raw_folders = os.listdir(self.raw_dir)\n        if len(raw_folders) == 0:\n            raise RuntimeError(\n                ""Dataset not found. Please download {} from {} and move it to {} with {}"".format(\n                    self.zip_name, self.url, self.raw_dir, self.folders\n                )\n            )\n        else:\n            intersection = len(set(self.folders).intersection(set(raw_folders)))\n            if intersection == 0:\n                log.info(""The data seems properly downloaded"")\n            else:\n                raise RuntimeError(\n                    ""Dataset not found. Please download {} from {} and move it to {} with {}"".format(\n                        self.zip_name, self.url, self.raw_dir, self.folders\n                    )\n                )\n\n    def process(self):\n        if not os.path.exists(self.pre_processed_path):\n            train_areas = [f for f in self.folders if str(self.test_area) not in f]\n            test_areas = [f for f in self.folders if str(self.test_area) in f]\n\n            train_files = [\n                (f, room_name, osp.join(self.raw_dir, f, room_name))\n                for f in train_areas\n                for room_name in os.listdir(osp.join(self.raw_dir, f))\n                if os.path.isdir(osp.join(self.raw_dir, f, room_name))\n            ]\n\n            test_files = [\n                (f, room_name, osp.join(self.raw_dir, f, room_name))\n                for f in test_areas\n                for room_name in os.listdir(osp.join(self.raw_dir, f))\n                if os.path.isdir(osp.join(self.raw_dir, f, room_name))\n            ]\n\n            # Gather data per area\n            data_list = [[] for _ in range(6)]\n            for (area, room_name, file_path) in tq(train_files + test_files):\n\n                area_num = int(area[-1]) - 1\n                if self.debug:\n                    read_s3dis_format(file_path, room_name, label_out=True, verbose=self.verbose, debug=self.debug)\n                    continue\n                else:\n                    xyz, rgb, room_labels, room_object_indices = read_s3dis_format(\n                        file_path, room_name, label_out=True, verbose=self.verbose, debug=self.debug\n                    )\n\n                    rgb_norm = rgb.float() / 255.0\n                    data = Data(pos=xyz, y=room_labels, rgb=rgb_norm)\n                    if room_name in VALIDATION_ROOMS:\n                        data.validation_set = True\n                    else:\n                        data.validation_set = False\n\n                    if self.keep_instance:\n                        data.room_object_indices = room_object_indices\n\n                    if self.pre_filter is not None and not self.pre_filter(data):\n                        continue\n\n                    data_list[area_num].append(data)\n\n            raw_areas = cT.PointCloudFusion()(data_list)\n            for i, area in enumerate(raw_areas):\n                torch.save(area, self.raw_areas_paths[i])\n\n            for area_datas in data_list:\n                # Apply pre_transform\n                if self.pre_transform is not None:\n                    for data in area_datas:\n                        data = self.pre_transform(data)\n            torch.save(data_list, self.pre_processed_path)\n        else:\n            data_list = torch.load(self.pre_processed_path)\n\n        if self.debug:\n            return\n\n        train_data_list = {}\n        val_data_list = {}\n        trainval_data_list = {}\n        for i in range(6):\n            if i != self.test_area - 1:\n                train_data_list[i] = []\n                val_data_list[i] = []\n                for data in data_list[i]:\n                    validation_set = data.validation_set\n                    del data.validation_set\n                    if validation_set:\n                        val_data_list[i].append(data)\n                    else:\n                        train_data_list[i].append(data)\n                trainval_data_list[i] = val_data_list[i] + train_data_list[i]\n\n        train_data_list = list(train_data_list.values())\n        val_data_list = list(val_data_list.values())\n        trainval_data_list = list(trainval_data_list.values())\n        test_data_list = data_list[self.test_area - 1]\n\n        if self.pre_collate_transform:\n            log.info(""pre_collate_transform ..."")\n            log.info(self.pre_collate_transform)\n            train_data_list = self.pre_collate_transform(train_data_list)\n            val_data_list = self.pre_collate_transform(val_data_list)\n            test_data_list = self.pre_collate_transform(test_data_list)\n            trainval_data_list = self.pre_collate_transform(trainval_data_list)\n\n        self._save_data(train_data_list, val_data_list, test_data_list, trainval_data_list)\n\n    def _save_data(self, train_data_list, val_data_list, test_data_list, trainval_data_list):\n        torch.save(self.collate(train_data_list), self.processed_paths[0])\n        torch.save(self.collate(val_data_list), self.processed_paths[1])\n        torch.save(self.collate(test_data_list), self.processed_paths[2])\n        torch.save(self.collate(trainval_data_list), self.processed_paths[3])\n\n    def _load_data(self, path):\n        self.data, self.slices = torch.load(path)\n\n\nclass S3DISSphere(S3DISOriginalFused):\n    """""" Small variation of S3DISOriginalFused that allows random sampling of spheres \n    within an Area during training and validation. Spheres have a radius of 2m. If sample_per_epoch is not specified, spheres\n    are taken on a 2m grid.\n\n    http://buildingparser.stanford.edu/dataset.html\n\n    Parameters\n    ----------\n    root: str\n        path to the directory where the data will be saved\n    test_area: int\n        number between 1 and 6 that denotes the area used for testing\n    train: bool\n        Is this a train split or not\n    pre_collate_transform:\n        Transforms to be applied before the data is assembled into samples (apply fusing here for example)\n    keep_instance: bool\n        set to True if you wish to keep instance data\n    sample_per_epoch\n        Number of spheres that are randomly sampled at each epoch (-1 for fixed grid)\n    radius\n        radius of each sphere\n    pre_transform\n    transform\n    pre_filter\n    """"""\n\n    def __init__(self, root, sample_per_epoch=100, radius=2, *args, **kwargs):\n        self._sample_per_epoch = sample_per_epoch\n        self._radius = radius\n        self._grid_sphere_sampling = cT.GridSampling3D(size=radius / 10.0)\n        super().__init__(root, *args, **kwargs)\n\n    def __len__(self):\n        if self._sample_per_epoch > 0:\n            return self._sample_per_epoch\n        else:\n            return len(self._test_spheres)\n\n    def get(self, idx):\n        if self._sample_per_epoch > 0:\n            return self._get_random()\n        else:\n            return self._test_spheres[idx]\n\n    def process(self): # We have to include this method, otherwise the parent class skips processing\n        super().process()\n\n    def download(self): # We have to include this method, otherwise the parent class skips download\n        super().download()\n\n    def _get_random(self):\n        # Random spheres biased towards getting more low frequency classes\n        chosen_label = np.random.choice(self._labels, p=self._label_counts)\n        valid_centres = self._centres_for_sampling[self._centres_for_sampling[:, 4] == chosen_label]\n        centre_idx = int(random.random() * (valid_centres.shape[0] - 1))\n        centre = valid_centres[centre_idx]\n        area_data = self._datas[centre[3].int()]\n        sphere_sampler = cT.SphereSampling(self._radius, centre[:3], align_origin=False)\n        return sphere_sampler(area_data)\n\n    def _save_data(self, train_data_list, val_data_list, test_data_list, trainval_data_list):\n        torch.save(train_data_list, self.processed_paths[0])\n        torch.save(val_data_list, self.processed_paths[1])\n        torch.save(test_data_list, self.processed_paths[2])\n        torch.save(trainval_data_list, self.processed_paths[3])\n\n    def _load_data(self, path):\n        self._datas = torch.load(path)\n        if not isinstance(self._datas, list):\n            self._datas = [self._datas]\n        if self._sample_per_epoch > 0:\n            self._centres_for_sampling = []\n            for i, data in enumerate(self._datas):\n                assert not hasattr(\n                    data, cT.SphereSampling.KDTREE_KEY\n                )  # Just to make we don\'t have some out of date data in there\n                low_res = self._grid_sphere_sampling(data.clone())\n                centres = torch.empty((low_res.pos.shape[0], 5), dtype=torch.float)\n                centres[:, :3] = low_res.pos\n                centres[:, 3] = i\n                centres[:, 4] = low_res.y\n                self._centres_for_sampling.append(centres)\n                tree = KDTree(np.asarray(data.pos), leaf_size=10)\n                setattr(data, cT.SphereSampling.KDTREE_KEY, tree)\n\n            self._centres_for_sampling = torch.cat(self._centres_for_sampling, 0)\n            uni, uni_counts = np.unique(np.asarray(self._centres_for_sampling[:, -1]), return_counts=True)\n            uni_counts = np.sqrt(uni_counts.mean() / uni_counts)\n            self._label_counts = uni_counts / np.sum(uni_counts)\n            self._labels = uni\n        else:\n            grid_sampler = cT.GridSphereSampling(self._radius, self._radius, center=False)\n            self._test_spheres = grid_sampler(self._datas)\n\n\nclass S3DISFusedDataset(BaseDataset):\n    """""" Wrapper around S3DISSphere that creates train and test datasets.\n\n    http://buildingparser.stanford.edu/dataset.html\n\n    Parameters\n    ----------\n    dataset_opt: omegaconf.DictConfig\n        Config dictionary that should contain\n\n            - dataroot\n            - fold: test_area parameter\n            - pre_collate_transform\n            - train_transforms\n            - test_transforms\n    """"""\n\n    INV_OBJECT_LABEL = INV_OBJECT_LABEL\n\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n\n        self.train_dataset = S3DISSphere(\n            self._data_path,\n            sample_per_epoch=3000,\n            test_area=self.dataset_opt.fold,\n            split=""train"",\n            pre_collate_transform=self.pre_collate_transform,\n            transform=self.train_transform,\n        )\n\n        self.val_dataset = S3DISSphere(\n            self._data_path,\n            sample_per_epoch=-1,\n            test_area=self.dataset_opt.fold,\n            split=""val"",\n            pre_collate_transform=self.pre_collate_transform,\n            transform=self.train_transform,\n        )\n        self.test_dataset = S3DISSphere(\n            self._data_path,\n            sample_per_epoch=-1,\n            test_area=self.dataset_opt.fold,\n            split=""test"",\n            pre_collate_transform=self.pre_collate_transform,\n            transform=self.test_transform,\n        )\n\n        if dataset_opt.class_weight_method:\n            self.train_dataset = add_weights(self.train_dataset, True, dataset_opt.class_weight_method)\n\n    @property\n    def test_data(self):\n        return self.test_dataset[0].raw_test_data\n\n    @staticmethod\n    def to_ply(pos, label, file):\n        """""" Allows to save s3dis predictions to disk using s3dis color scheme\n\n        Parameters\n        ----------\n        pos : torch.Tensor\n            tensor that contains the positions of the points\n        label : torch.Tensor\n            predicted label\n        file : string\n            Save location\n        """"""\n        to_ply(pos, label, file)\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n\n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        from torch_points3d.metrics.s3dis_tracker import S3DISTracker\n\n        return S3DISTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n'"
torch_points3d/datasets/segmentation/scannet.py,14,"b'import os\nimport os.path as osp\nimport shutil\nimport json\nimport torch\nfrom glob import glob\nimport sys\nimport csv\nimport logging\nimport numpy as np\nfrom plyfile import PlyData, PlyElement\nfrom torch_geometric.data import Data, InMemoryDataset, download_url, extract_zip\nimport torch_geometric.transforms as T\nimport multiprocessing\nimport pandas as pd\n\nimport tempfile\nimport urllib\nfrom urllib.request import urlopen\n\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nimport torch_points3d.core.data_transform as cT\nfrom . import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n# Ref: https://github.com/xjwang-cs/TSDF_utils/blob/master/download-scannet.py\n########################################################################################\n#                                                                                      #\n#                                      Download script                                 #\n#                                                                                      #\n########################################################################################\n\nBASE_URL = ""http://kaldir.vc.in.tum.de/scannet/""\nTOS_URL = BASE_URL + ""ScanNet_TOS.pdf""\nFILETYPES = [\n    "".aggregation.json"",\n    "".sens"",\n    "".txt"",\n    ""_vh_clean.ply"",\n    ""_vh_clean_2.0.010000.segs.json"",\n    ""_vh_clean_2.ply"",\n    ""_vh_clean.segs.json"",\n    ""_vh_clean.aggregation.json"",\n    ""_vh_clean_2.labels.ply"",\n    ""_2d-instance.zip"",\n    ""_2d-instance-filt.zip"",\n    ""_2d-label.zip"",\n    ""_2d-label-filt.zip"",\n]\nFILETYPES_TEST = ["".sens"", "".txt"", ""_vh_clean.ply"", ""_vh_clean_2.ply""]\nPREPROCESSED_FRAMES_FILE = [""scannet_frames_25k.zip"", ""5.6GB""]\nTEST_FRAMES_FILE = [""scannet_frames_test.zip"", ""610MB""]\nLABEL_MAP_FILES = [""scannetv2-labels.combined.tsv"", ""scannet-labels.combined.tsv""]\nRELEASES = [""v2/scans"", ""v1/scans""]\nRELEASES_TASKS = [""v2/tasks"", ""v1/tasks""]\nRELEASES_NAMES = [""v2"", ""v1""]\nRELEASE = RELEASES[0]\nRELEASE_TASKS = RELEASES_TASKS[0]\nRELEASE_NAME = RELEASES_NAMES[0]\nLABEL_MAP_FILE = LABEL_MAP_FILES[0]\nRELEASE_SIZE = ""1.2TB""\nV1_IDX = 1\nNUM_CLASSES = 41\nCLASS_LABELS = (\n    ""wall"",\n    ""floor"",\n    ""cabinet"",\n    ""bed"",\n    ""chair"",\n    ""sofa"",\n    ""table"",\n    ""door"",\n    ""window"",\n    ""bookshelf"",\n    ""picture"",\n    ""counter"",\n    ""desk"",\n    ""curtain"",\n    ""refrigerator"",\n    ""shower curtain"",\n    ""toilet"",\n    ""sink"",\n    ""bathtub"",\n    ""otherfurniture"",\n)\nURLS_METADATA = [\n    ""https://raw.githubusercontent.com/facebookresearch/votenet/master/scannet/meta_data/scannetv2-labels.combined.tsv"",\n    ""https://raw.githubusercontent.com/facebookresearch/votenet/master/scannet/meta_data/scannetv2_train.txt"",\n    ""https://raw.githubusercontent.com/facebookresearch/votenet/master/scannet/meta_data/scannetv2_test.txt"",\n    ""https://raw.githubusercontent.com/facebookresearch/votenet/master/scannet/meta_data/scannetv2_val.txt"",\n]\nVALID_CLASS_IDS = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 14, 16, 24, 28, 33, 34, 36, 39]\n\nSCANNET_COLOR_MAP = {\n    0: (0.0, 0.0, 0.0),\n    1: (174.0, 199.0, 232.0),\n    2: (152.0, 223.0, 138.0),\n    3: (31.0, 119.0, 180.0),\n    4: (255.0, 187.0, 120.0),\n    5: (188.0, 189.0, 34.0),\n    6: (140.0, 86.0, 75.0),\n    7: (255.0, 152.0, 150.0),\n    8: (214.0, 39.0, 40.0),\n    9: (197.0, 176.0, 213.0),\n    10: (148.0, 103.0, 189.0),\n    11: (196.0, 156.0, 148.0),\n    12: (23.0, 190.0, 207.0),\n    14: (247.0, 182.0, 210.0),\n    15: (66.0, 188.0, 102.0),\n    16: (219.0, 219.0, 141.0),\n    17: (140.0, 57.0, 197.0),\n    18: (202.0, 185.0, 52.0),\n    19: (51.0, 176.0, 203.0),\n    20: (200.0, 54.0, 131.0),\n    21: (92.0, 193.0, 61.0),\n    22: (78.0, 71.0, 183.0),\n    23: (172.0, 114.0, 82.0),\n    24: (255.0, 127.0, 14.0),\n    25: (91.0, 163.0, 138.0),\n    26: (153.0, 98.0, 156.0),\n    27: (140.0, 153.0, 101.0),\n    28: (158.0, 218.0, 229.0),\n    29: (100.0, 125.0, 154.0),\n    30: (178.0, 127.0, 135.0),\n    32: (146.0, 111.0, 194.0),\n    33: (44.0, 160.0, 44.0),\n    34: (112.0, 128.0, 144.0),\n    35: (96.0, 207.0, 209.0),\n    36: (227.0, 119.0, 194.0),\n    37: (213.0, 92.0, 176.0),\n    38: (94.0, 106.0, 211.0),\n    39: (82.0, 84.0, 163.0),\n    40: (100.0, 85.0, 144.0),\n}\n\nSPLITS = [""train"", ""val"", ""test""]\n\nMAX_NUM_POINTS = 1200000\n\n\ndef get_release_scans(release_file):\n    scan_lines = urlopen(release_file)\n    scans = []\n    for scan_line in scan_lines:\n        scan_id = scan_line.decode(""utf8"").rstrip(""\\n"")\n        scans.append(scan_id)\n    return scans\n\n\ndef download_release(release_scans, out_dir, file_types, use_v1_sens):\n    if len(release_scans) == 0:\n        return\n    log.info(""Downloading ScanNet "" + RELEASE_NAME + "" release to "" + out_dir + ""..."")\n    failed = []\n    for scan_id in release_scans:\n        scan_out_dir = os.path.join(out_dir, scan_id)\n        try:\n            download_scan(scan_id, scan_out_dir, file_types, use_v1_sens)\n        except:\n            failed.append(scan_id)\n    log.info(""Downloaded ScanNet "" + RELEASE_NAME + "" release."")\n    if len(failed):\n        log.warning(""Failed downloads: {}"".format(failed))\n\n\ndef download_file(url, out_file):\n    out_dir = os.path.dirname(out_file)\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    if not os.path.isfile(out_file):\n        log.info(""\\t"" + url + "" > "" + out_file)\n        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n        f = os.fdopen(fh, ""w"")\n        f.close()\n        urllib.request.urlretrieve(url, out_file_tmp)\n        # urllib.urlretrieve(url, out_file_tmp)\n        os.rename(out_file_tmp, out_file)\n    else:\n        pass\n        # log.warning(""WARNING Skipping download of existing file "" + out_file)\n\n\ndef download_scan(scan_id, out_dir, file_types, use_v1_sens):\n    # log.info(""Downloading ScanNet "" + RELEASE_NAME + "" scan "" + scan_id + "" ..."")\n    if not os.path.isdir(out_dir):\n        os.makedirs(out_dir)\n    for ft in file_types:\n        v1_sens = use_v1_sens and ft == "".sens""\n        url = (\n            BASE_URL + RELEASE + ""/"" + scan_id + ""/"" + scan_id + ft\n            if not v1_sens\n            else BASE_URL + RELEASES[V1_IDX] + ""/"" + scan_id + ""/"" + scan_id + ft\n        )\n        out_file = out_dir + ""/"" + scan_id + ft\n        download_file(url, out_file)\n    # log.info(""Downloaded scan "" + scan_id)\n\n\ndef download_label_map(out_dir):\n    log.info(""Downloading ScanNet "" + RELEASE_NAME + "" label mapping file..."")\n    files = [LABEL_MAP_FILE]\n    for file in files:\n        url = BASE_URL + RELEASE_TASKS + ""/"" + file\n        localpath = os.path.join(out_dir, file)\n        localdir = os.path.dirname(localpath)\n        if not os.path.isdir(localdir):\n            os.makedirs(localdir)\n        download_file(url, localpath)\n    log.info(""Downloaded ScanNet "" + RELEASE_NAME + "" label mapping file."")\n\n\n# REFERENCE TO https://github.com/facebookresearch/votenet/blob/master/scannet/load_scannet_data.py\n########################################################################################\n#                                                                                      #\n#                                      UTILS                                           #\n#                                                                                      #\n########################################################################################\n\n\ndef represents_int(s):\n    """""" if string s represents an int. """"""\n    try:\n        int(s)\n        return True\n    except ValueError:\n        return False\n\n\ndef read_label_mapping(filename, label_from=""raw_category"", label_to=""nyu40id""):\n    assert os.path.isfile(filename)\n    mapping = dict()\n    with open(filename) as csvfile:\n        reader = csv.DictReader(csvfile, delimiter=""\\t"")\n        for row in reader:\n            mapping[row[label_from]] = int(row[label_to])\n    if represents_int(list(mapping.keys())[0]):\n        mapping = {int(k): v for k, v in mapping.items()}\n    return mapping\n\n\ndef read_mesh_vertices(filename):\n    """""" read XYZ for each vertex.\n    """"""\n    assert os.path.isfile(filename)\n    with open(filename, ""rb"") as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata[""vertex""].count\n        vertices = np.zeros(shape=[num_verts, 3], dtype=np.float32)\n        vertices[:, 0] = plydata[""vertex""].data[""x""]\n        vertices[:, 1] = plydata[""vertex""].data[""y""]\n        vertices[:, 2] = plydata[""vertex""].data[""z""]\n    return vertices\n\n\ndef read_mesh_vertices_rgb(filename):\n    """""" read XYZ RGB for each vertex.\n    Note: RGB values are in 0-255\n    """"""\n    assert os.path.isfile(filename)\n    with open(filename, ""rb"") as f:\n        plydata = PlyData.read(f)\n        num_verts = plydata[""vertex""].count\n        vertices = np.zeros(shape=[num_verts, 6], dtype=np.float32)\n        vertices[:, 0] = plydata[""vertex""].data[""x""]\n        vertices[:, 1] = plydata[""vertex""].data[""y""]\n        vertices[:, 2] = plydata[""vertex""].data[""z""]\n        vertices[:, 3] = plydata[""vertex""].data[""red""]\n        vertices[:, 4] = plydata[""vertex""].data[""green""]\n        vertices[:, 5] = plydata[""vertex""].data[""blue""]\n    return vertices\n\n\ndef read_aggregation(filename):\n    assert os.path.isfile(filename)\n    object_id_to_segs = {}\n    label_to_segs = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_objects = len(data[""segGroups""])\n        for i in range(num_objects):\n            object_id = data[""segGroups""][i][""objectId""] + 1  # instance ids should be 1-indexed\n            label = data[""segGroups""][i][""label""]\n            segs = data[""segGroups""][i][""segments""]\n            object_id_to_segs[object_id] = segs\n            if label in label_to_segs:\n                label_to_segs[label].extend(segs)\n            else:\n                label_to_segs[label] = segs\n    return object_id_to_segs, label_to_segs\n\n\ndef read_segmentation(filename):\n    assert os.path.isfile(filename)\n    seg_to_verts = {}\n    with open(filename) as f:\n        data = json.load(f)\n        num_verts = len(data[""segIndices""])\n        for i in range(num_verts):\n            seg_id = data[""segIndices""][i]\n            if seg_id in seg_to_verts:\n                seg_to_verts[seg_id].append(i)\n            else:\n                seg_to_verts[seg_id] = [i]\n    return seg_to_verts, num_verts\n\n\ndef export(mesh_file, agg_file, seg_file, meta_file, label_map_file, output_file=None):\n    """""" points are XYZ RGB (RGB in 0-255),\n    semantic label as nyu40 ids,\n    instance label as 1-#instance,\n    box as (cx,cy,cz,dx,dy,dz,semantic_label)\n    """"""\n    label_map = read_label_mapping(label_map_file, label_from=""raw_category"", label_to=""nyu40id"")\n    mesh_vertices = read_mesh_vertices_rgb(mesh_file)\n\n    # Load scene axis alignment matrix\n    lines = open(meta_file).readlines()\n    for line in lines:\n        if ""axisAlignment"" in line:\n            axis_align_matrix = [float(x) for x in line.rstrip().strip(""axisAlignment = "").split("" "")]\n            break\n    axis_align_matrix = np.array(axis_align_matrix).reshape((4, 4))\n    pts = np.ones((mesh_vertices.shape[0], 4))\n    pts[:, 0:3] = mesh_vertices[:, 0:3]\n    pts = np.dot(pts, axis_align_matrix.transpose())  # Nx4\n    mesh_vertices[:, 0:3] = pts[:, 0:3]\n\n    # Load semantic and instance labels\n    object_id_to_segs, label_to_segs = read_aggregation(agg_file)\n    seg_to_verts, num_verts = read_segmentation(seg_file)\n    label_ids = np.zeros(shape=(num_verts), dtype=np.uint32)  # 0: unannotated\n    object_id_to_label_id = {}\n    for label, segs in label_to_segs.items():\n        label_id = label_map[label]\n        for seg in segs:\n            verts = seg_to_verts[seg]\n            label_ids[verts] = label_id\n    instance_ids = np.zeros(shape=(num_verts), dtype=np.uint32)  # 0: unannotated\n    num_instances = len(np.unique(list(object_id_to_segs.keys())))\n    for object_id, segs in object_id_to_segs.items():\n        for seg in segs:\n            verts = seg_to_verts[seg]\n            instance_ids[verts] = object_id\n            if object_id not in object_id_to_label_id:\n                object_id_to_label_id[object_id] = label_ids[verts][0]\n    instance_bboxes = np.zeros((num_instances, 7))\n    for obj_id in object_id_to_segs:\n        label_id = object_id_to_label_id[obj_id]\n        obj_pc = mesh_vertices[instance_ids == obj_id, 0:3]\n        if len(obj_pc) == 0:\n            continue\n        # Compute axis aligned box\n        # An axis aligned bounding box is parameterized by\n        # (cx,cy,cz) and (dx,dy,dz) and label id\n        # where (cx,cy,cz) is the center point of the box,\n        # dx is the x-axis length of the box.\n        xmin = np.min(obj_pc[:, 0])\n        ymin = np.min(obj_pc[:, 1])\n        zmin = np.min(obj_pc[:, 2])\n        xmax = np.max(obj_pc[:, 0])\n        ymax = np.max(obj_pc[:, 1])\n        zmax = np.max(obj_pc[:, 2])\n        bbox = np.array(\n            [\n                (xmin + xmax) / 2.0,\n                (ymin + ymax) / 2.0,\n                (zmin + zmax) / 2.0,\n                xmax - xmin,\n                ymax - ymin,\n                zmax - zmin,\n                label_id,\n            ]\n        )\n        # NOTE: this assumes obj_id is in 1,2,3,.,,,.NUM_INSTANCES\n        instance_bboxes[obj_id - 1, :] = bbox\n\n    return (\n        mesh_vertices.astype(np.float32),\n        label_ids.astype(np.int),\n        instance_ids.astype(np.int),\n        instance_bboxes.astype(np.float32),\n        object_id_to_label_id,\n    )\n\n\n########################################################################################\n#                                                                                      #\n#                          SCANNET InMemoryDataset DATASET                             #\n#                                                                                      #\n########################################################################################\n\n\nclass Scannet(InMemoryDataset):\n    """""" Scannet dataset, you will have to agree to terms and conditions by hitting enter\n    so that it downloads the dataset.\n\n    http://www.scan-net.org/\n\n    Parameters\n    ----------\n    root : str\n        Path to the data\n    split : str, optional\n        Split used (train, val or test)\n    transform (callable, optional):\n        A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a transformed\n        version. The data object will be transformed before every access.\n    pre_transform (callable, optional): \n        A function/transform that takes in an :obj:`torch_geometric.data.Data` object and returns a \n        transformed version. The data object will be transformed before being saved to disk.\n    pre_filter (callable, optional): \n        A function that takes in an :obj:`torch_geometric.data.Data` object and returns a boolean\n        value, indicating whether the data object should be included in the final dataset. \n    version : str, optional\n        version of scannet, by default ""v2""\n    use_instance_labels : bool, optional\n        Wether we use instance labels or not, by default False\n    use_instance_bboxes : bool, optional\n        Wether we use bounding box labels or not, by default False\n    donotcare_class_ids : list, optional\n        Class ids to be discarded\n    max_num_point : [type], optional\n        Max number of points to keep during the pre processing step\n    use_multiprocessing : bool, optional\n        Wether we use multiprocessing or not\n    process_workers : int, optional\n        Number of process workers\n    normalize_rgb : bool, optional\n        Normalise rgb values, by default True\n    """"""\n\n    CLASS_LABELS = CLASS_LABELS\n    URLS_METADATA = URLS_METADATA\n    VALID_CLASS_IDS = VALID_CLASS_IDS\n    SCANNET_COLOR_MAP = SCANNET_COLOR_MAP\n    SPLITS = SPLITS\n\n    def __init__(\n        self,\n        root,\n        split=""train"",\n        transform=None,\n        pre_transform=None,\n        pre_filter=None,\n        version=""v2"",\n        use_instance_labels=False,\n        use_instance_bboxes=False,\n        donotcare_class_ids=[],\n        max_num_point=None,\n        process_workers=4,\n        types=["".txt"", ""_vh_clean_2.ply"", ""_vh_clean_2.0.010000.segs.json"", "".aggregation.json""],\n        normalize_rgb=True,\n    ):\n\n        assert self.SPLITS == [""train"", ""val"", ""test""]\n        if not isinstance(donotcare_class_ids, list):\n            raise Exception(""donotcare_class_ids should be list with indices of class to ignore"")\n        self.donotcare_class_ids = donotcare_class_ids\n\n        self.valid_class_idx = [idx for idx in self.VALID_CLASS_IDS if idx not in donotcare_class_ids]\n\n        assert version in [""v2"", ""v1""], ""The version should be either v1 or v2""\n        self.version = version\n        self.max_num_point = max_num_point\n        self.use_instance_labels = use_instance_labels\n        self.use_instance_bboxes = use_instance_bboxes\n        self.use_multiprocessing = process_workers > 1\n        self.process_workers = process_workers\n        self.types = types\n        self.normalize_rgb = normalize_rgb\n\n        super().__init__(root, transform, pre_transform, pre_filter)\n        if split == ""train"":\n            path = self.processed_paths[0]\n        elif split == ""val"":\n            path = self.processed_paths[1]\n        elif split == ""test"":\n            path = self.processed_paths[2]\n        else:\n            raise ValueError((f""Split {split} found, but expected either "" ""train, val, or test""))\n\n        self.data, self.slices = torch.load(path)\n\n        if split != ""test"":\n            if not use_instance_bboxes:\n                delattr(self.data, ""instance_bboxes"")\n            if not use_instance_labels:\n                delattr(self.data, ""instance_labels"")\n            self.data = self._remap_labels(self.data)\n            self.has_labels = True\n        else:\n            self.has_labels = False\n\n        self.read_from_metadata()\n\n    def get_raw_data(self, id_scan, remap_labels=True) -> Data:\n        """""" Grabs the raw data associated with a scan index\n\n        Parameters\n        ----------\n        id_scan : int or torch.Tensor\n            id of the scan\n        remap_labels : bool, optional\n            If True then labels are mapped to the range [IGNORE_LABELS:number of labels]. \n            If using this method to compare ground truth against prediction then set remap_labels to True\n        """"""\n        stage = self.name\n        if torch.is_tensor(id_scan):\n            id_scan = int(id_scan.item())\n        assert stage in self.SPLITS\n        mapping_idx_to_scan_names = getattr(self, ""MAPPING_IDX_TO_SCAN_{}_NAMES"".format(stage.upper()))\n        scan_name = mapping_idx_to_scan_names[id_scan]\n        path_to_raw_scan = os.path.join(\n            self.processed_raw_paths[self.SPLITS.index(stage.lower())], ""{}.pt"".format(scan_name)\n        )\n        data = torch.load(path_to_raw_scan)\n        data.scan_name = scan_name\n        data.path_to_raw_scan = path_to_raw_scan\n        if self.has_labels and remap_labels:\n            data = self._remap_labels(data)\n        return data\n\n    @property\n    def raw_file_names(self):\n        return [""metadata"", ""scans"", ""scannetv2-labels.combined.tsv""]\n\n    @property\n    def processed_file_names(self):\n        return [""{}.pt"".format(s,) for s in Scannet.SPLITS]\n\n    @property\n    def processed_raw_paths(self):\n        processed_raw_paths = [os.path.join(self.processed_dir, ""raw_{}"".format(s)) for s in Scannet.SPLITS]\n        for p in processed_raw_paths:\n            if not os.path.exists(p):\n                os.makedirs(p)\n        return processed_raw_paths\n\n    @property\n    def path_to_submission(self):\n        root = os.getcwd()\n        path_to_submission = os.path.join(root, ""submission_labels"")\n        if not os.path.exists(path_to_submission):\n            os.makedirs(path_to_submission)\n        return path_to_submission\n\n    @property\n    def num_classes(self):\n        return len(Scannet.VALID_CLASS_IDS)\n\n    def download_scans(self):\n        release_file = BASE_URL + RELEASE + "".txt""\n        release_scans = get_release_scans(release_file)\n        file_types = FILETYPES\n        release_test_file = BASE_URL + RELEASE + ""_test.txt""\n        release_test_scans = get_release_scans(release_test_file)\n        file_types_test = FILETYPES_TEST\n        out_dir_scans = os.path.join(self.raw_dir, ""scans"")\n        out_dir_test_scans = os.path.join(self.raw_dir, ""scans_test"")\n\n        if self.types:  # download file type\n            file_types = self.types\n            for file_type in file_types:\n                if file_type not in FILETYPES:\n                    log.error(""ERROR: Invalid file type: "" + file_type)\n                    return\n            file_types_test = []\n            for file_type in file_types:\n                if file_type in FILETYPES_TEST:\n                    file_types_test.append(file_type)\n        download_label_map(self.raw_dir)\n        log.info(""WARNING: You are downloading all ScanNet "" + RELEASE_NAME + "" scans of type "" + file_types[0])\n        log.info(\n            ""Note that existing scan directories will be skipped. Delete partially downloaded directories to re-download.""\n        )\n        log.info(""***"")\n        log.info(""Press any key to continue, or CTRL-C to exit."")\n        input("""")\n        if self.version == ""v2"" and "".sens"" in file_types:\n            log.info(\n                ""Note: ScanNet v2 uses the same .sens files as ScanNet v1: Press \'n\' to exclude downloading .sens files for each scan""\n            )\n            key = input("""")\n            if key.strip().lower() == ""n"":\n                file_types.remove("".sens"")\n        download_release(release_scans, out_dir_scans, file_types, use_v1_sens=True)\n        if self.version == ""v2"":\n            download_label_map(self.raw_dir)\n            download_release(release_test_scans, out_dir_test_scans, file_types_test, use_v1_sens=True)\n            # download_file(os.path.join(BASE_URL, RELEASE_TASKS, TEST_FRAMES_FILE[0]), os.path.join(out_dir_tasks, TEST_FRAMES_FILE[0]))\n\n    def download(self):\n        log.info(\n            ""By pressing any key to continue you confirm that you have agreed to the ScanNet terms of use as described at:""\n        )\n        log.info(TOS_URL)\n        log.info(""***"")\n        log.info(""Press any key to continue, or CTRL-C to exit."")\n        input("""")\n        self.download_scans()\n        metadata_path = osp.join(self.raw_dir, ""metadata"")\n        if not os.path.exists(metadata_path):\n            os.makedirs(metadata_path)\n        for url in self.URLS_METADATA:\n            _ = download_url(url, metadata_path)\n\n    @staticmethod\n    def read_one_test_scan(scannet_dir, scan_name, normalize_rgb):\n        mesh_file = osp.join(scannet_dir, scan_name, scan_name + ""_vh_clean_2.ply"")\n        mesh_vertices = read_mesh_vertices_rgb(mesh_file)\n\n        data = {}\n        data[""pos""] = torch.from_numpy(mesh_vertices[:, :3])\n        data[""rgb""] = torch.from_numpy(mesh_vertices[:, 3:])\n        if normalize_rgb:\n            data[""rgb""] /= 255.0\n        return Data(**data)\n\n    @staticmethod\n    def read_one_scan(\n        scannet_dir, scan_name, label_map_file, donotcare_class_ids, max_num_point, obj_class_ids, normalize_rgb,\n    ):\n        mesh_file = osp.join(scannet_dir, scan_name, scan_name + ""_vh_clean_2.ply"")\n        agg_file = osp.join(scannet_dir, scan_name, scan_name + "".aggregation.json"")\n        seg_file = osp.join(scannet_dir, scan_name, scan_name + ""_vh_clean_2.0.010000.segs.json"")\n        meta_file = osp.join(\n            scannet_dir, scan_name, scan_name + "".txt""\n        )  # includes axisAlignment info for the train set scans.\n        mesh_vertices, semantic_labels, instance_labels, instance_bboxes, instance2semantic = export(\n            mesh_file, agg_file, seg_file, meta_file, label_map_file, None\n        )\n\n        # Discard unwanted classes\n        mask = np.logical_not(np.in1d(semantic_labels, donotcare_class_ids))\n        mesh_vertices = mesh_vertices[mask, :]\n        semantic_labels = semantic_labels[mask]\n        instance_labels = instance_labels[mask]\n\n        bbox_mask = np.in1d(instance_bboxes[:, -1], obj_class_ids)\n        instance_bboxes = instance_bboxes[bbox_mask, :]\n\n        # Subsample\n        N = mesh_vertices.shape[0]\n        if max_num_point:\n            if N > max_num_point:\n                choices = np.random.choice(N, max_num_point, replace=False)\n                mesh_vertices = mesh_vertices[choices, :]\n                semantic_labels = semantic_labels[choices]\n                instance_labels = instance_labels[choices]\n\n        # Build data container\n        data = {}\n        data[""pos""] = torch.from_numpy(mesh_vertices[:, :3])\n        data[""rgb""] = torch.from_numpy(mesh_vertices[:, 3:])\n        if normalize_rgb:\n            data[""rgb""] /= 255.0\n        data[""y""] = torch.from_numpy(semantic_labels)\n        data[""x""] = None\n        data[""instance_labels""] = torch.from_numpy(instance_labels)\n        data[""instance_bboxes""] = torch.from_numpy(instance_bboxes)\n\n        return Data(**data)\n\n    def read_from_metadata(self):\n        metadata_path = osp.join(self.raw_dir, ""metadata"")\n        self.label_map_file = osp.join(metadata_path, LABEL_MAP_FILE)\n        split_files = [""scannetv2_{}.txt"".format(s) for s in Scannet.SPLITS]\n        self.scan_names = [sorted([line.rstrip() for line in open(osp.join(metadata_path, sf))]) for sf in split_files]\n\n        for idx_split, split in enumerate(Scannet.SPLITS):\n            idx_mapping = {idx: scan_name for idx, scan_name in enumerate(self.scan_names[idx_split])}\n            setattr(self, ""MAPPING_IDX_TO_SCAN_{}_NAMES"".format(split.upper()), idx_mapping)\n\n    @staticmethod\n    def process_func(\n        id_scan,\n        total,\n        scannet_dir,\n        scan_name,\n        label_map_file,\n        donotcare_class_ids,\n        max_num_point,\n        obj_class_ids,\n        normalize_rgb,\n        split,\n    ):\n        if split == ""test"":\n            data = Scannet.read_one_test_scan(scannet_dir, scan_name, normalize_rgb)\n        else:\n            data = Scannet.read_one_scan(\n                scannet_dir,\n                scan_name,\n                label_map_file,\n                donotcare_class_ids,\n                max_num_point,\n                obj_class_ids,\n                normalize_rgb,\n            )\n        log.info(""{}/{}| scan_name: {}, data: {}"".format(id_scan, total, scan_name, data))\n\n        data[""id_scan""] = torch.from_numpy(np.asarray([id_scan]))\n\n        return cT.SaveOriginalPosId()(data)\n\n    def process(self):\n        self.read_from_metadata()\n\n        scannet_dir = osp.join(self.raw_dir, ""scans"")\n        for i, (scan_names, split) in enumerate(zip(self.scan_names, self.SPLITS)):\n            if not os.path.exists(self.processed_paths[i]):\n                mapping_idx_to_scan_names = getattr(self, ""MAPPING_IDX_TO_SCAN_{}_NAMES"".format(split.upper()))\n                scannet_dir = osp.join(self.raw_dir, ""scans"" if split in [""train"", ""val""] else ""scans_test"")\n                total = len(scan_names)\n                args = [\n                    (\n                        id,\n                        total,\n                        scannet_dir,\n                        scan_name,\n                        self.label_map_file,\n                        self.donotcare_class_ids,\n                        self.max_num_point,\n                        self.VALID_CLASS_IDS,\n                        self.normalize_rgb,\n                        split,\n                    )\n                    for id, scan_name in enumerate(scan_names)\n                ]\n                if self.use_multiprocessing:\n                    with multiprocessing.Pool(processes=self.process_workers) as pool:\n                        datas = pool.starmap(Scannet.process_func, args)\n                else:\n                    datas = []\n                    for arg in args:\n                        data = Scannet.process_func(*arg)\n                        datas.append(data)\n\n                for data in datas:\n                    id_scan = int(data.id_scan.item())\n                    scan_name = mapping_idx_to_scan_names[id_scan]\n                    path_to_raw_scan = os.path.join(self.processed_raw_paths[i], ""{}.pt"".format(scan_name))\n                    torch.save(data, path_to_raw_scan)\n\n                datas = [self.pre_transform(data) for data in datas]\n\n                log.info(""SAVING TO {}"".format(self.processed_paths[i]))\n                torch.save(self.collate(datas), self.processed_paths[i])\n\n    def _remap_labels(self, data):\n        """""" Remaps labels to [0 ; num_labels -1]. Can be overriden.\n        """"""\n        mapping_dict = {indice: idx for idx, indice in enumerate(self.valid_class_idx)}\n        for idx in range(NUM_CLASSES):\n            if idx not in mapping_dict:\n                mapping_dict[idx] = IGNORE_LABEL\n        for idx in self.donotcare_class_ids:\n            mapping_dict[idx] = IGNORE_LABEL\n        for source, target in mapping_dict.items():\n            mask = data.y == source\n            data.y[mask] = target\n        return data\n\n    def __repr__(self):\n        return ""{}({})"".format(self.__class__.__name__, len(self))\n\n\nclass ScannetDataset(BaseDataset):\n    """""" Wrapper around Scannet that creates train and test datasets.\n\n    Parameters\n    ----------\n    dataset_opt: omegaconf.DictConfig\n        Config dictionary that should contain\n\n            - dataroot\n            - version\n            - max_num_point (optional)\n            - use_instance_labels (optional)\n            - use_instance_bboxes (optional)\n            - donotcare_class_ids (optional)\n            - pre_transforms (optional)\n            - train_transforms (optional)\n            - val_transforms (optional)\n    """"""\n\n    SPLITS = SPLITS\n\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n\n        use_instance_labels: bool = dataset_opt.use_instance_labels\n        use_instance_bboxes: bool = dataset_opt.use_instance_bboxes\n        donotcare_class_ids: [] = dataset_opt.donotcare_class_ids if dataset_opt.donotcare_class_ids else []\n        max_num_point: int = dataset_opt.max_num_point if dataset_opt.max_num_point is not None else None\n        process_workers: int = dataset_opt.process_workers if dataset_opt.process_workers else 0\n\n        self.train_dataset = Scannet(\n            self._data_path,\n            split=""train"",\n            pre_transform=self.pre_transform,\n            transform=self.train_transform,\n            version=dataset_opt.version,\n            use_instance_labels=use_instance_labels,\n            use_instance_bboxes=use_instance_bboxes,\n            donotcare_class_ids=donotcare_class_ids,\n            max_num_point=max_num_point,\n            process_workers=process_workers,\n        )\n\n        self.val_dataset = Scannet(\n            self._data_path,\n            split=""val"",\n            transform=self.val_transform,\n            pre_transform=self.pre_transform,\n            version=dataset_opt.version,\n            use_instance_labels=use_instance_labels,\n            use_instance_bboxes=use_instance_bboxes,\n            donotcare_class_ids=donotcare_class_ids,\n            max_num_point=max_num_point,\n            process_workers=process_workers,\n        )\n\n        self.test_dataset = Scannet(\n            self._data_path,\n            split=""test"",\n            transform=self.val_transform,\n            pre_transform=self.pre_transform,\n            version=dataset_opt.version,\n            use_instance_labels=use_instance_labels,\n            use_instance_bboxes=use_instance_bboxes,\n            donotcare_class_ids=donotcare_class_ids,\n            max_num_point=max_num_point,\n            process_workers=process_workers,\n        )\n\n    @property\n    def path_to_submission(self):\n        return self.train_dataset.path_to_submission\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n\n        Arguments:\n            dataset {[type]}\n            wandb_log - Log using weight and biases\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        from torch_points3d.metrics.scannet_segmentation_tracker import ScannetSegmentationTracker\n\n        return ScannetSegmentationTracker(\n            self, wandb_log=wandb_log, use_tensorboard=tensorboard_log, ignore_label=IGNORE_LABEL\n        )\n'"
torch_points3d/datasets/segmentation/shapenet.py,7,"b'import os\nimport os.path as osp\nimport shutil\nimport json\nfrom tqdm.auto import tqdm as tq\nfrom itertools import repeat, product\nimport numpy as np\nimport torch\n\nfrom torch_geometric.data import Data, InMemoryDataset, download_url, extract_zip\nfrom torch_geometric.io import read_txt_array\nimport torch_geometric.transforms as T\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.metrics.shapenet_part_tracker import ShapenetPartTracker\nfrom torch_points3d.datasets.base_dataset import BaseDataset\n\n\nclass ShapeNet(InMemoryDataset):\n    r""""""The ShapeNet part level segmentation dataset from the `""A Scalable\n    Active Framework for Region Annotation in 3D Shape Collections""\n    <http://web.stanford.edu/~ericyi/papers/part_annotation_16_small.pdf>`_\n    paper, containing about 17,000 3D shape point clouds from 16 shape\n    categories.\n    Each category is annotated with 2 to 6 parts.\n\n    Args:\n        root (string): Root directory where the dataset should be saved.\n        categories (string or [string], optional): The category of the CAD\n            models (one or a combination of :obj:`""Airplane""`, :obj:`""Bag""`,\n            :obj:`""Cap""`, :obj:`""Car""`, :obj:`""Chair""`, :obj:`""Earphone""`,\n            :obj:`""Guitar""`, :obj:`""Knife""`, :obj:`""Lamp""`, :obj:`""Laptop""`,\n            :obj:`""Motorbike""`, :obj:`""Mug""`, :obj:`""Pistol""`, :obj:`""Rocket""`,\n            :obj:`""Skateboard""`, :obj:`""Table""`).\n            Can be explicitly set to :obj:`None` to load all categories.\n            (default: :obj:`None`)\n        include_normals (bool, optional): If set to :obj:`False`, will not\n            include normal vectors as input features. (default: :obj:`True`)\n        split (string, optional): If :obj:`""train""`, loads the training\n            dataset.\n            If :obj:`""val""`, loads the validation dataset.\n            If :obj:`""trainval""`, loads the training and validation dataset.\n            If :obj:`""test""`, loads the test dataset.\n            (default: :obj:`""trainval""`)\n        transform (callable, optional): A function/transform that takes in an\n            :obj:`torch_geometric.data.Data` object and returns a transformed\n            version. The data object will be transformed before every access.\n            (default: :obj:`None`)\n        pre_transform (callable, optional): A function/transform that takes in\n            an :obj:`torch_geometric.data.Data` object and returns a\n            transformed version. The data object will be transformed before\n            being saved to disk. (default: :obj:`None`)\n        pre_filter (callable, optional): A function that takes in an\n            :obj:`torch_geometric.data.Data` object and returns a boolean\n            value, indicating whether the data object should be included in the\n            final dataset. (default: :obj:`None`)\n    """"""\n\n    url = ""https://shapenet.cs.stanford.edu/media/"" ""shapenetcore_partanno_segmentation_benchmark_v0_normal.zip""\n\n    category_ids = {\n        ""Airplane"": ""02691156"",\n        ""Bag"": ""02773838"",\n        ""Cap"": ""02954340"",\n        ""Car"": ""02958343"",\n        ""Chair"": ""03001627"",\n        ""Earphone"": ""03261776"",\n        ""Guitar"": ""03467517"",\n        ""Knife"": ""03624134"",\n        ""Lamp"": ""03636649"",\n        ""Laptop"": ""03642806"",\n        ""Motorbike"": ""03790512"",\n        ""Mug"": ""03797390"",\n        ""Pistol"": ""03948459"",\n        ""Rocket"": ""04099429"",\n        ""Skateboard"": ""04225987"",\n        ""Table"": ""04379243"",\n    }\n\n    seg_classes = {\n        ""Airplane"": [0, 1, 2, 3],\n        ""Bag"": [4, 5],\n        ""Cap"": [6, 7],\n        ""Car"": [8, 9, 10, 11],\n        ""Chair"": [12, 13, 14, 15],\n        ""Earphone"": [16, 17, 18],\n        ""Guitar"": [19, 20, 21],\n        ""Knife"": [22, 23],\n        ""Lamp"": [24, 25, 26, 27],\n        ""Laptop"": [28, 29],\n        ""Motorbike"": [30, 31, 32, 33, 34, 35],\n        ""Mug"": [36, 37],\n        ""Pistol"": [38, 39, 40],\n        ""Rocket"": [41, 42, 43],\n        ""Skateboard"": [44, 45, 46],\n        ""Table"": [47, 48, 49],\n    }\n\n    def __init__(\n        self,\n        root,\n        categories=None,\n        include_normals=True,\n        split=""trainval"",\n        transform=None,\n        pre_transform=None,\n        pre_filter=None,\n    ):\n        if categories is None:\n            categories = list(self.category_ids.keys())\n        if isinstance(categories, str):\n            categories = [categories]\n        assert all(category in self.category_ids for category in categories)\n        self.categories = categories\n        super(ShapeNet, self).__init__(root, transform, pre_transform, pre_filter)\n\n        if split == ""train"":\n            path = self.processed_paths[0]\n            raw_path = self.processed_raw_paths[0]\n        elif split == ""val"":\n            path = self.processed_paths[1]\n            raw_path = self.processed_raw_paths[1]\n        elif split == ""test"":\n            path = self.processed_paths[2]\n            raw_path = self.processed_raw_paths[2]\n        elif split == ""trainval"":\n            path = self.processed_paths[3]\n            raw_path = self.processed_raw_paths[3]\n        else:\n            raise ValueError((f""Split {split} found, but expected either "" ""train, val, trainval or test""))\n\n        self.data, self.slices, self.y_mask = self.load_data(path, include_normals)\n\n        # We have perform a slighly optimzation on memory space of no pre-transform was used.\n        # c.f self._process_filenames\n        if os.path.exists(raw_path):\n            self.raw_data, self.raw_slices, _ = self.load_data(raw_path, include_normals)\n        else:\n            self.get_raw_data = self.get\n\n    def load_data(self, path, include_normals):\n        \'\'\'This function is used twice to load data for both raw and pre_transformed\n        \'\'\'\n        data, slices = torch.load(path)\n        data.x = data.x if include_normals else None\n\n        y_mask = torch.zeros((len(self.seg_classes.keys()), 50), dtype=torch.bool)\n        for i, labels in enumerate(self.seg_classes.values()):\n            y_mask[i, labels] = 1\n\n        return data, slices, y_mask\n\n    @property\n    def raw_file_names(self):\n        return list(self.category_ids.values()) + [""train_test_split""]\n\n    @property\n    def processed_raw_paths(self):\n        cats = ""_"".join([cat[:3].lower() for cat in self.categories])\n        processed_raw_paths = [os.path.join(self.processed_dir, ""raw_{}_{}"".format(cats, s)) for s in [""train"", ""val"", ""test"", ""trainval""]]\n        return processed_raw_paths\n        \n    @property\n    def processed_file_names(self):\n        cats = ""_"".join([cat[:3].lower() for cat in self.categories])\n        return [os.path.join(""{}_{}.pt"".format(cats, split)) for split in [""train"", ""val"", ""test"", ""trainval""]]\n\n    def download(self):\n        path = download_url(self.url, self.root)\n        extract_zip(path, self.root)\n        os.unlink(path)\n        shutil.rmtree(self.raw_dir)\n        name = self.url.split(""/"")[-1].split(""."")[0]\n        os.rename(osp.join(self.root, name), self.raw_dir)\n\n    def get_raw_data(self, idx, **kwargs):\n        data = self.raw_data.__class__()\n\n        if hasattr(self.raw_data, \'__num_nodes__\'):\n            data.num_nodes = self.raw_data.__num_nodes__[idx]\n\n        for key in self.raw_data.keys:\n            item, slices = self.raw_data[key], self.raw_slices[key]\n            start, end = slices[idx].item(), slices[idx + 1].item()\n            # print(slices[idx], slices[idx + 1])\n            if torch.is_tensor(item):\n                s = list(repeat(slice(None), item.dim()))\n                s[self.raw_data.__cat_dim__(key, item)] = slice(start, end)\n            elif start + 1 == end:\n                s = slices[start]\n            else:\n                s = slice(start, end)\n            data[key] = item[s]\n        return data\n\n    def _process_filenames(self, filenames):\n        data_raw_list = []\n        data_list = []\n        categories_ids = [self.category_ids[cat] for cat in self.categories]\n        cat_idx = {categories_ids[i]: i for i in range(len(categories_ids))}\n\n        has_pre_transform = self.pre_transform is not None\n        \n        id_scan = -1\n        for name in tq(filenames):\n            cat = name.split(osp.sep)[0]\n            if cat not in categories_ids:\n                continue\n            id_scan += 1\n            data = read_txt_array(osp.join(self.raw_dir, name))\n            pos = data[:, :3]\n            x = data[:, 3:6]\n            y = data[:, -1].type(torch.long)\n            category = torch.ones(x.shape[0], dtype=torch.long) * cat_idx[cat]\n            id_scan_tensor = torch.from_numpy(np.asarray([id_scan])).clone()\n            data = Data(pos=pos, x=x, y=y, category=category, id_scan=id_scan_tensor)\n            data = SaveOriginalPosId()(data)\n            if self.pre_filter is not None and not self.pre_filter(data):\n                continue\n            data_raw_list.append(data.clone() if has_pre_transform else data)\n            if has_pre_transform:\n                data = self.pre_transform(data)\n                data_list.append(data)\n        if not has_pre_transform:\n            return [], data_raw_list\n        return data_raw_list, data_list\n\n    def _save_data_list(self, datas, path_to_datas, save_bool=True):\n        if save_bool:\n            torch.save(self.collate(datas), path_to_datas)\n\n    def _re_index_trainval(self, trainval):\n        if len(trainval) == 0:\n            return trainval\n        train, val = trainval\n        for v in val:\n            v.id_scan += len(train)\n        assert (train[-1].id_scan + 1 == val[0].id_scan).item(), (train[-1].id_scan, val[0].id_scan)\n        return train + val\n\n    def process(self):\n        raw_trainval = []\n        trainval = []\n        for i, split in enumerate([""train"", ""val"", ""test""]):\n            path = osp.join(self.raw_dir, ""train_test_split"", f""shuffled_{split}_file_list.json"")\n            with open(path, ""r"") as f:\n                filenames = [\n                    osp.sep.join(name.split(osp.sep)[1:]) + "".txt"" for name in json.load(f)\n                ]  # Removing first directory.\n            data_raw_list, data_list = self._process_filenames(sorted(filenames))\n            if split == ""train"" or split == ""val"":\n                if len(data_raw_list) > 0: raw_trainval.append(data_raw_list)\n                trainval.append(data_list)\n\n            self._save_data_list(data_list, self.processed_paths[i])\n            self._save_data_list(data_raw_list, self.processed_raw_paths[i], save_bool=len(data_raw_list) > 0)\n\n        self._save_data_list(self._re_index_trainval(trainval), self.processed_paths[3])\n        self._save_data_list(self._re_index_trainval(raw_trainval), self.processed_raw_paths[3], save_bool=len(raw_trainval) > 0)\n\n    def __repr__(self):\n        return ""{}({}, categories={})"".format(self.__class__.__name__, len(self), self.categories)\n\n\nclass ShapeNetDataset(BaseDataset):\n    """""" Wrapper around ShapeNet that creates train and test datasets.\n\n    Parameters\n    ----------\n    dataset_opt: omegaconf.DictConfig\n        Config dictionary that should contain\n\n            - dataroot\n            - category: List of categories or All\n            - normal: bool, include normals or not\n            - pre_transforms\n            - train_transforms\n            - test_transforms\n            - val_transforms\n    """"""\n\n    FORWARD_CLASS = ""forward.shapenet.ForwardShapenetDataset""\n\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n        try:\n            self._category = dataset_opt.category\n        except KeyError:\n            self._category = None\n\n        self.train_dataset = ShapeNet(\n            self._data_path,\n            self._category,\n            include_normals=dataset_opt.normal,\n            split=""train"",\n            pre_transform=self.pre_transform,\n            transform=self.train_transform,\n        )\n\n        self.val_dataset = ShapeNet(\n            self._data_path,\n            self._category,\n            include_normals=dataset_opt.normal,\n            split=""val"",\n            pre_transform=self.pre_transform,\n            transform=self.val_transform,\n        )\n\n        self.test_dataset = ShapeNet(\n            self._data_path,\n            self._category,\n            include_normals=dataset_opt.normal,\n            split=""test"",\n            transform=self.test_transform,\n            pre_transform=self.pre_transform,\n        )\n        self._categories = self.train_dataset.categories\n\n    @property\n    def class_to_segments(self):\n        classes_to_segment = {}\n        for key in self._categories:\n            classes_to_segment[key] = ShapeNet.seg_classes[key]\n        return classes_to_segment\n\n    @property\n    def is_hierarchical(self):\n        return len(self._categories) > 1\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n\n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        return ShapenetPartTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n'"
torch_points3d/models/base_architectures/__init__.py,0,b'from .unet import *\nfrom .backbone import *\n'
torch_points3d/models/base_architectures/backbone.py,1,"b'from torch import nn\nfrom torch_geometric.nn import (\n    global_max_pool,\n    global_mean_pool,\n    fps,\n    radius,\n    knn_interpolate,\n)\nfrom torch.nn import (\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nfrom omegaconf.listconfig import ListConfig\nfrom omegaconf.dictconfig import DictConfig\nimport logging\n\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.models.base_architectures import BaseFactory\nfrom torch_points3d.core.common_modules.base_modules import Identity\nfrom torch_points3d.core.losses import instantiate_loss_or_miner\nfrom torch_points3d.utils.config import is_list\n\nlog = logging.getLogger(__name__)\n\nSPECIAL_NAMES = [""radius"", ""max_num_neighbors"", ""block_names""]\n\n\n############################# Backbone Base ###################################\n\n\nclass BackboneBasedModel(BaseModel):\n    """"""\n    create a backbone-based generator:\n    This is simply an encoder\n    (can be used in classification, regression, metric learning and so one)\n    """"""\n\n    def _save_sampling_and_search(self, down_conv):\n        sampler = getattr(down_conv, ""sampler"", None)\n        if is_list(sampler):\n            self._spatial_ops_dict[""sampler""] = sampler + self._spatial_ops_dict[""sampler""]\n        else:\n            self._spatial_ops_dict[""sampler""] = [sampler] + self._spatial_ops_dict[""sampler""]\n\n        neighbour_finder = getattr(down_conv, ""neighbour_finder"", None)\n        if is_list(neighbour_finder):\n            self._spatial_ops_dict[""neighbour_finder""] = neighbour_finder + self._spatial_ops_dict[""neighbour_finder""]\n        else:\n            self._spatial_ops_dict[""neighbour_finder""] = [neighbour_finder] + self._spatial_ops_dict[""neighbour_finder""]\n\n    def __init__(self, opt, model_type, dataset: BaseDataset, modules_lib):\n\n        """"""Construct a backbone generator (It is a simple down module)\n        Parameters:\n            opt - options for the network generation\n            model_type - type of the model to be generated\n            modules_lib - all modules that can be used in the backbone\n\n\n        opt is expected to contains the following keys:\n        * down_conv\n        """"""\n\n        super(BackboneBasedModel, self).__init__(opt)\n        self._spatial_ops_dict = {""neighbour_finder"": [], ""sampler"": []}\n\n        # detect which options format has been used to define the model\n        if is_list(opt.down_conv) or ""down_conv_nn"" not in opt.down_conv:\n            raise NotImplementedError\n        else:\n            self._init_from_compact_format(opt, model_type, dataset, modules_lib)\n\n    def _get_from_kwargs(self, kwargs, name):\n        module = kwargs[name]\n        kwargs.pop(name)\n        return module\n\n    def _init_from_compact_format(self, opt, model_type, dataset, modules_lib):\n        """"""Create a backbonebasedmodel from the compact options format - where the\n        same convolution is given for each layer, and arguments are given\n        in lists\n        """"""\n        num_convs = len(opt.down_conv.down_conv_nn)\n        self.down_modules = nn.ModuleList()\n        factory_module_cls = self._get_factory(model_type, modules_lib)\n        down_conv_cls_name = opt.down_conv.module_name\n        self._factory_module = factory_module_cls(down_conv_cls_name, None, modules_lib)\n        # Down modules\n        for i in range(num_convs):\n            args = self._fetch_arguments(opt.down_conv, i, ""DOWN"")\n            conv_cls = self._get_from_kwargs(args, ""conv_cls"")\n            down_module = conv_cls(**args)\n            self._save_sampling_and_search(down_module)\n            self.down_modules.append(down_module)\n\n        self.metric_loss_module, self.miner_module = BaseModel.get_metric_loss_and_miner(\n            getattr(opt, ""metric_loss"", None), getattr(opt, ""miner"", None)\n        )\n\n    def _get_factory(self, model_name, modules_lib) -> BaseFactory:\n        factory_module_cls = getattr(modules_lib, ""{}Factory"".format(model_name), None)\n        if factory_module_cls is None:\n            factory_module_cls = BaseFactory\n        return factory_module_cls\n\n    def _fetch_arguments_from_list(self, opt, index):\n        """"""Fetch the arguments for a single convolution from multiple lists\n        of arguments - for models specified in the compact format.\n        """"""\n        args = {}\n        for o, v in opt.items():\n            name = str(o)\n            if is_list(v) and len(getattr(opt, o)) > 0:\n                if name[-1] == ""s"" and name not in SPECIAL_NAMES:\n                    name = name[:-1]\n                v_index = v[index]\n                if is_list(v_index):\n                    v_index = list(v_index)\n                args[name] = v_index\n            else:\n                if is_list(v):\n                    v = list(v)\n                args[name] = v\n        return args\n\n    def _fetch_arguments(self, conv_opt, index, flow=""DOWN""):\n        """""" Fetches arguments for building a convolution down\n\n        Arguments:\n            conv_opt\n            index in sequential order (as they come in the config)\n            flow ""DOWN""\n        """"""\n        args = self._fetch_arguments_from_list(conv_opt, index)\n        args[""conv_cls""] = self._factory_module.get_module(flow)\n        args[""index""] = index\n        return args\n\n    def _flatten_compact_options(self, opt):\n        """"""Converts from a dict of lists, to a list of dicts\n        """"""\n        flattenedOpts = []\n\n        for index in range(int(1e6)):\n            try:\n                flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))\n            except IndexError:\n                break\n\n        return flattenedOpts\n'"
torch_points3d/models/base_architectures/unet.py,4,"b'from torch import nn\nfrom torch_geometric.nn import (\n    global_max_pool,\n    global_mean_pool,\n    fps,\n    radius,\n    knn_interpolate,\n)\nfrom torch.nn import (\n    Linear as Lin,\n    ReLU,\n    LeakyReLU,\n    BatchNorm1d as BN,\n    Dropout,\n)\nfrom omegaconf.listconfig import ListConfig\nfrom omegaconf.dictconfig import DictConfig\nimport logging\n\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.core.common_modules.base_modules import Identity\nfrom torch_points3d.utils.config import is_list\n\n\nlog = logging.getLogger(__name__)\n\n\nSPECIAL_NAMES = [""radius"", ""max_num_neighbors"", ""block_names""]\n\n\nclass BaseFactory:\n    def __init__(self, module_name_down, module_name_up, modules_lib):\n        self.module_name_down = module_name_down\n        self.module_name_up = module_name_up\n        self.modules_lib = modules_lib\n\n    def get_module(self, flow):\n        if flow.upper() == ""UP"":\n            return getattr(self.modules_lib, self.module_name_up, None)\n        else:\n            return getattr(self.modules_lib, self.module_name_down, None)\n\n\n############################# UNET BASE ###################################\n\n\nclass UnetBasedModel(BaseModel):\n    """"""Create a Unet-based generator""""""\n\n    def _save_sampling_and_search(self, submodule):\n        sampler = getattr(submodule.down, ""sampler"", None)\n        if is_list(sampler):\n            self._spatial_ops_dict[""sampler""] = sampler + self._spatial_ops_dict[""sampler""]\n        else:\n            self._spatial_ops_dict[""sampler""] = [sampler] + self._spatial_ops_dict[""sampler""]\n\n        neighbour_finder = getattr(submodule.down, ""neighbour_finder"", None)\n        if is_list(neighbour_finder):\n            self._spatial_ops_dict[""neighbour_finder""] = neighbour_finder + self._spatial_ops_dict[""neighbour_finder""]\n        else:\n            self._spatial_ops_dict[""neighbour_finder""] = [neighbour_finder] + self._spatial_ops_dict[""neighbour_finder""]\n\n        upsample_op = getattr(submodule.up, ""upsample_op"", None)\n        if upsample_op:\n            self._spatial_ops_dict[""upsample_op""].append(upsample_op)\n\n    def __init__(self, opt, model_type, dataset: BaseDataset, modules_lib):\n        """"""Construct a Unet generator\n        Parameters:\n            opt - options for the network generation\n            model_type - type of the model to be generated\n            num_class - output of the network\n            modules_lib - all modules that can be used in the UNet\n        We construct the U-Net from the innermost layer to the outermost layer.\n        It is a recursive process.\n\n        opt is expected to contains the following keys:\n        * down_conv\n        * up_conv\n        * OPTIONAL: innermost\n        """"""\n        super(UnetBasedModel, self).__init__(opt)\n        self._spatial_ops_dict = {""neighbour_finder"": [], ""sampler"": [], ""upsample_op"": []}\n        # detect which options format has been used to define the model\n        if type(opt.down_conv) is ListConfig or ""down_conv_nn"" not in opt.down_conv:\n            self._init_from_layer_list_format(opt, model_type, dataset, modules_lib)\n        else:\n            self._init_from_compact_format(opt, model_type, dataset, modules_lib)\n\n    def _init_from_compact_format(self, opt, model_type, dataset, modules_lib):\n        """"""Create a unetbasedmodel from the compact options format - where the\n        same convolution is given for each layer, and arguments are given\n        in lists\n        """"""\n        num_convs = len(opt.down_conv.down_conv_nn)\n\n        # Factory for creating up and down modules\n        factory_module_cls = self._get_factory(model_type, modules_lib)\n        down_conv_cls_name = opt.down_conv.module_name\n        up_conv_cls_name = opt.up_conv.module_name\n        self._factory_module = factory_module_cls(\n            down_conv_cls_name, up_conv_cls_name, modules_lib\n        )  # Create the factory object\n        # construct unet structure\n        contains_global = hasattr(opt, ""innermost"") and opt.innermost is not None\n        if contains_global:\n            assert len(opt.down_conv.down_conv_nn) + 1 == len(opt.up_conv.up_conv_nn)\n\n            args_up = self._fetch_arguments_from_list(opt.up_conv, 0)\n            args_up[""up_conv_cls""] = self._factory_module.get_module(""UP"")\n\n            unet_block = UnetSkipConnectionBlock(\n                args_up=args_up, args_innermost=opt.innermost, modules_lib=modules_lib, submodule=None, innermost=True,\n            )  # add the innermost layer\n        else:\n            unet_block = Identity()\n\n        if num_convs > 1:\n            for index in range(num_convs - 1, 0, -1):\n                args_up, args_down = self._fetch_arguments_up_and_down(opt, index)\n                unet_block = UnetSkipConnectionBlock(args_up=args_up, args_down=args_down, submodule=unet_block)\n                self._save_sampling_and_search(unet_block)\n        else:\n            index = num_convs\n\n        index -= 1\n        args_up, args_down = self._fetch_arguments_up_and_down(opt, index)\n        self.model = UnetSkipConnectionBlock(\n            args_up=args_up, args_down=args_down, submodule=unet_block, outermost=True\n        )  # add the outermost layer\n        self._save_sampling_and_search(self.model)\n\n    def _init_from_layer_list_format(self, opt, model_type, dataset, modules_lib):\n        """"""Create a unetbasedmodel from the layer list options format - where\n        each layer of the unet is specified separately\n        """"""\n\n        self._get_factory(model_type, modules_lib)\n\n        down_conv_layers = (\n            opt.down_conv if type(opt.down_conv) is ListConfig else self._flatten_compact_options(opt.down_conv)\n        )\n        up_conv_layers = opt.up_conv if type(opt.up_conv) is ListConfig else self._flatten_compact_options(opt.up_conv)\n        num_convs = len(down_conv_layers)\n\n        unet_block = []\n        contains_global = hasattr(opt, ""innermost"") and opt.innermost is not None\n        if contains_global:\n            assert len(down_conv_layers) + 1 == len(up_conv_layers)\n\n            up_layer = dict(up_conv_layers[0])\n            up_layer[""up_conv_cls""] = getattr(modules_lib, up_layer[""module_name""])\n\n            unet_block = UnetSkipConnectionBlock(\n                args_up=up_layer, args_innermost=opt.innermost, modules_lib=modules_lib, innermost=True,\n            )\n\n        for index in range(num_convs - 1, 0, -1):\n            down_layer = dict(down_conv_layers[index])\n            up_layer = dict(up_conv_layers[num_convs - index])\n\n            down_layer[""down_conv_cls""] = getattr(modules_lib, down_layer[""module_name""])\n            up_layer[""up_conv_cls""] = getattr(modules_lib, up_layer[""module_name""])\n\n            unet_block = UnetSkipConnectionBlock(\n                args_up=up_layer, args_down=down_layer, modules_lib=modules_lib, submodule=unet_block,\n            )\n\n        up_layer = dict(up_conv_layers[-1])\n        down_layer = dict(down_conv_layers[0])\n        down_layer[""down_conv_cls""] = getattr(modules_lib, down_layer[""module_name""])\n        up_layer[""up_conv_cls""] = getattr(modules_lib, up_layer[""module_name""])\n        self.model = UnetSkipConnectionBlock(\n            args_up=up_layer, args_down=down_layer, submodule=unet_block, outermost=True\n        )\n\n        self._save_sampling_and_search(self.model)\n\n    def _get_factory(self, model_name, modules_lib) -> BaseFactory:\n        factory_module_cls = getattr(modules_lib, ""{}Factory"".format(model_name), None)\n        if factory_module_cls is None:\n            factory_module_cls = BaseFactory\n        return factory_module_cls\n\n    def _fetch_arguments_from_list(self, opt, index):\n        """"""Fetch the arguments for a single convolution from multiple lists\n        of arguments - for models specified in the compact format.\n        """"""\n        args = {}\n        for o, v in opt.items():\n            name = str(o)\n            if is_list(v) and len(getattr(opt, o)) > 0:\n                if name[-1] == ""s"" and name not in SPECIAL_NAMES:\n                    name = name[:-1]\n                v_index = v[index]\n                if is_list(v_index):\n                    v_index = list(v_index)\n                args[name] = v_index\n            else:\n                if is_list(v):\n                    v = list(v)\n                args[name] = v\n        return args\n\n    def _fetch_arguments_up_and_down(self, opt, index):\n        # Defines down arguments\n        args_down = self._fetch_arguments_from_list(opt.down_conv, index)\n        args_down[""index""] = index\n        args_down[""down_conv_cls""] = self._factory_module.get_module(""DOWN"")\n\n        # Defines up arguments\n        idx = len(getattr(opt.up_conv, ""up_conv_nn"")) - index - 1\n        args_up = self._fetch_arguments_from_list(opt.up_conv, idx)\n        args_up[""index""] = index\n        args_up[""up_conv_cls""] = self._factory_module.get_module(""UP"")\n        return args_up, args_down\n\n    def _flatten_compact_options(self, opt):\n        """"""Converts from a dict of lists, to a list of dicts\n        """"""\n        flattenedOpts = []\n\n        for index in range(int(1e6)):\n            try:\n                flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))\n            except IndexError:\n                break\n\n        return flattenedOpts\n\n\nclass UnetSkipConnectionBlock(nn.Module):\n    """"""Defines the Unet submodule with skip connection.\n        X -------------------identity----------------------\n        |-- downsampling -- |submodule| -- upsampling --|\n\n    """"""\n\n    def get_from_kwargs(self, kwargs, name):\n        module = kwargs[name]\n        kwargs.pop(name)\n        return module\n\n    def __init__(\n        self,\n        args_up=None,\n        args_down=None,\n        args_innermost=None,\n        modules_lib=None,\n        submodule=None,\n        outermost=False,\n        innermost=False,\n    ):\n        """"""Construct a Unet submodule with skip connections.\n        Parameters:\n            args_up -- arguments for up convs\n            args_down -- arguments for down convs\n            args_innermost -- arguments for innermost\n            submodule (UnetSkipConnectionBlock) -- previously defined submodules\n            outermost (bool)    -- if this module is the outermost module\n            innermost (bool)    -- if this module is the innermost module\n        """"""\n        super(UnetSkipConnectionBlock, self).__init__()\n\n        self.outermost = outermost\n        self.innermost = innermost\n\n        if innermost:\n            assert outermost == False\n            module_name = self.get_from_kwargs(args_innermost, ""module_name"")\n            inner_module_cls = getattr(modules_lib, module_name)\n            self.inner = inner_module_cls(**args_innermost)\n            upconv_cls = self.get_from_kwargs(args_up, ""up_conv_cls"")\n            self.up = upconv_cls(**args_up)\n        else:\n            downconv_cls = self.get_from_kwargs(args_down, ""down_conv_cls"")\n            upconv_cls = self.get_from_kwargs(args_up, ""up_conv_cls"")\n            downconv = downconv_cls(**args_down)\n            upconv = upconv_cls(**args_up)\n\n            self.down = downconv\n            self.submodule = submodule\n            self.up = upconv\n\n    def forward(self, data, **kwargs):\n        if self.innermost:\n            data_out = self.inner(data, **kwargs)\n            data = (data_out, data)\n            return self.up(data, **kwargs)\n        else:\n            data_out = self.down(data, **kwargs)\n            data_out2 = self.submodule(data_out, **kwargs)\n            data = (data_out2, data)\n            return self.up(data, **kwargs)\n\n\n############################# UNWRAPPED UNET BASE ###################################\n\n\nclass UnwrappedUnetBasedModel(BaseModel):\n    """"""Create a Unet unwrapped generator""""""\n\n    def _save_sampling_and_search(self, down_conv):\n        sampler = getattr(down_conv, ""sampler"", None)\n        if is_list(sampler):\n            self._spatial_ops_dict[""sampler""] += sampler\n        else:\n            self._spatial_ops_dict[""sampler""].append(sampler)\n\n        neighbour_finder = getattr(down_conv, ""neighbour_finder"", None)\n        if is_list(neighbour_finder):\n            self._spatial_ops_dict[""neighbour_finder""] += neighbour_finder\n        else:\n            self._spatial_ops_dict[""neighbour_finder""].append(neighbour_finder)\n\n    def _save_upsample(self, up_conv):\n        upsample_op = getattr(up_conv, ""upsample_op"", None)\n        if upsample_op:\n            self._spatial_ops_dict[""upsample_op""].append(upsample_op)\n\n    def __init__(self, opt, model_type, dataset: BaseDataset, modules_lib):\n        """"""Construct a Unet unwrapped generator\n\n        The layers will be appended within lists with the following names\n        * down_modules : Contains all the down module\n        * inner_modules : Contain one or more inner modules\n        * up_modules: Contains all the up module\n\n        Parameters:\n            opt - options for the network generation\n            model_type - type of the model to be generated\n            num_class - output of the network\n            modules_lib - all modules that can be used in the UNet\n\n        For a recursive implementation. See UnetBaseModel.\n\n        opt is expected to contains the following keys:\n        * down_conv\n        * up_conv\n        * OPTIONAL: innermost\n\n        """"""\n        super(UnwrappedUnetBasedModel, self).__init__(opt)\n        # detect which options format has been used to define the model\n        self._spatial_ops_dict = {""neighbour_finder"": [], ""sampler"": [], ""upsample_op"": []}\n\n        if is_list(opt.down_conv) or ""down_conv_nn"" not in opt.down_conv:\n            raise NotImplementedError\n        else:\n            self._init_from_compact_format(opt, model_type, dataset, modules_lib)\n\n    def _collect_sampling_ids(self, list_data):\n        def extract_matching_key(keys, start_token):\n            for key in keys:\n                if key.startswith(start_token):\n                    return key\n            return None\n\n        d = {}\n        if self.save_sampling_id:\n            for idx, data in enumerate(list_data):\n                key = extract_matching_key(data.keys, ""sampling_id"")\n                if key:\n                    d[key] = getattr(data, key)\n        return d\n\n    def _get_from_kwargs(self, kwargs, name):\n        module = kwargs[name]\n        kwargs.pop(name)\n        return module\n\n    def _create_inner_modules(self, args_innermost, modules_lib):\n        inners = []\n        if is_list(args_innermost):\n            for inner_opt in args_innermost:\n                module_name = self._get_from_kwargs(inner_opt, ""module_name"")\n                inner_module_cls = getattr(modules_lib, module_name)\n                inners.append(inner_module_cls(**inner_opt))\n\n        else:\n            module_name = self._get_from_kwargs(args_innermost, ""module_name"")\n            inner_module_cls = getattr(modules_lib, module_name)\n            inners.append(inner_module_cls(**args_innermost))\n\n        return inners\n\n    def _init_from_compact_format(self, opt, model_type, dataset, modules_lib):\n        """"""Create a unetbasedmodel from the compact options format - where the\n        same convolution is given for each layer, and arguments are given\n        in lists\n        """"""\n\n        self.down_modules = nn.ModuleList()\n        self.inner_modules = nn.ModuleList()\n        self.up_modules = nn.ModuleList()\n\n        self.save_sampling_id = opt.down_conv.save_sampling_id\n\n        # Factory for creating up and down modules\n        factory_module_cls = self._get_factory(model_type, modules_lib)\n        down_conv_cls_name = opt.down_conv.module_name\n        up_conv_cls_name = opt.up_conv.module_name if opt.up_conv is not None else None\n        self._factory_module = factory_module_cls(\n            down_conv_cls_name, up_conv_cls_name, modules_lib\n        )  # Create the factory object\n\n        # Loal module\n        contains_global = hasattr(opt, ""innermost"") and opt.innermost is not None\n        if contains_global:\n            inners = self._create_inner_modules(opt.innermost, modules_lib)\n            for inner in inners:\n                self.inner_modules.append(inner)\n        else:\n            self.inner_modules.append(Identity())\n\n        # Down modules\n        for i in range(len(opt.down_conv.down_conv_nn)):\n            args = self._fetch_arguments(opt.down_conv, i, ""DOWN"")\n            conv_cls = self._get_from_kwargs(args, ""conv_cls"")\n            down_module = conv_cls(**args)\n            self._save_sampling_and_search(down_module)\n            self.down_modules.append(down_module)\n\n        # Up modules\n        if up_conv_cls_name:\n            for i in range(len(opt.up_conv.up_conv_nn)):\n                args = self._fetch_arguments(opt.up_conv, i, ""UP"")\n                conv_cls = self._get_from_kwargs(args, ""conv_cls"")\n                up_module = conv_cls(**args)\n                self._save_upsample(up_module)\n                self.up_modules.append(up_module)\n\n        self.metric_loss_module, self.miner_module = BaseModel.get_metric_loss_and_miner(\n            getattr(opt, ""metric_loss"", None), getattr(opt, ""miner"", None)\n        )\n\n    def _get_factory(self, model_name, modules_lib) -> BaseFactory:\n        factory_module_cls = getattr(modules_lib, ""{}Factory"".format(model_name), None)\n        if factory_module_cls is None:\n            factory_module_cls = BaseFactory\n        return factory_module_cls\n\n    def _fetch_arguments_from_list(self, opt, index):\n        """"""Fetch the arguments for a single convolution from multiple lists\n        of arguments - for models specified in the compact format.\n        """"""\n        args = {}\n        for o, v in opt.items():\n            name = str(o)\n            if is_list(v) and len(getattr(opt, o)) > 0:\n                if name[-1] == ""s"" and name not in SPECIAL_NAMES:\n                    name = name[:-1]\n                v_index = v[index]\n                if is_list(v_index):\n                    v_index = list(v_index)\n                args[name] = v_index\n            else:\n                if is_list(v):\n                    v = list(v)\n                args[name] = v\n        return args\n\n    def _fetch_arguments(self, conv_opt, index, flow):\n        """""" Fetches arguments for building a convolution (up or down)\n\n        Arguments:\n            conv_opt\n            index in sequential order (as they come in the config)\n            flow ""UP"" or ""DOWN""\n        """"""\n        args = self._fetch_arguments_from_list(conv_opt, index)\n        args[""conv_cls""] = self._factory_module.get_module(flow)\n        args[""index""] = index\n        return args\n\n    def _flatten_compact_options(self, opt):\n        """"""Converts from a dict of lists, to a list of dicts\n        """"""\n        flattenedOpts = []\n\n        for index in range(int(1e6)):\n            try:\n                flattenedOpts.append(DictConfig(self._fetch_arguments_from_list(opt, index)))\n            except IndexError:\n                break\n\n        return flattenedOpts\n\n    def forward(self, data, precomputed_down=None, precomputed_up=None):\n        """""" This method does a forward on the Unet assuming symmetrical skip connections\n\n        Parameters\n        ----------\n        data: torch.geometric.Data\n            Data object that contains all info required by the modules\n        precomputed_down: torch.geometric.Data\n            Precomputed data that will be passed to the down convs\n        precomputed_up: torch.geometric.Data\n            Precomputed data that will be passed to the up convs\n        """"""\n        stack_down = []\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data, precomputed=precomputed_down)\n            stack_down.append(data)\n        data = self.down_modules[-1](data, precomputed=precomputed_down)\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n\n        sampling_ids = self._collect_sampling_ids(stack_down)\n\n        for i in range(len(self.up_modules)):\n            data = self.up_modules[i]((data, stack_down.pop()), precomputed=precomputed_up)\n\n        for key, value in sampling_ids.items():\n            setattr(data, key, value)\n        return data\n'"
torch_points3d/models/object_detection/__init__.py,0,b''
torch_points3d/models/object_detection/votenet.py,1,"b'import logging\nimport torch\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.applications import models\nimport torch_points3d.modules.VoteNet as votenet_module\nfrom torch_points3d.models.base_architectures import UnetBasedModel\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n\nclass VoteNetModel(BaseModel):\n    def __init__(self, option, model_type, dataset, modules):\n        """"""Initialize this model class.\n        Parameters:\n            opt -- training/test options\n        A few things can be done here.\n        - (required) call the initialization function of BaseModel\n        - define loss function, visualization images, model names, and optimizers\n        """"""\n        super(VoteNetModel, self).__init__(option)\n\n        # 1 - CREATE BACKBONE MODEL\n        input_nc = dataset.feature_dimension\n        backbone_option = option.backbone\n        backbone_cls = getattr(models, backbone_option.model_type)\n        self.backbone_model = backbone_cls(architecture=""unet"", input_nc=input_nc, config=backbone_option)\n\n        # 2 - CREATE VOTING MODEL\n        voting_option = option.voting\n        voting_cls = getattr(votenet_module, voting_option.module_name)\n        self.voting_module = voting_cls(vote_factor=voting_option.vote_factor, seed_feature_dim=voting_option.feat_dim)\n\n        # 3 - CREATE PROPOSAL MODULE\n        proposal_option = option.proposal\n        proposal_cls = getattr(votenet_module, proposal_option.module_name)\n        self.proposal_cls_module = proposal_cls(\n            num_class=proposal_option.num_class,\n            vote_aggregation_config=proposal_option.vote_aggregation,\n            num_heading_bin=proposal_option.num_heading_bin,\n            num_size_cluster=proposal_option.num_size_cluster,\n            mean_size_arr=dataset.mean_size_arr,\n            num_proposal=proposal_option.num_proposal,\n            sampling=proposal_option.sampling,\n        )\n\n        self.loss_params = option.loss_params\n        self.loss_params.num_heading_bin = proposal_option.num_heading_bin\n        self.loss_params.num_size_cluster = proposal_option.num_size_cluster\n        self.loss_params.mean_size_arr = dataset.mean_size_arr.tolist()\n\n        self.losses_has_been_added = False\n        self.loss_names = []\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        """"""\n        # Forward through backbone model\n        self.input = data.to(device)\n\n    def forward(self):\n        """"""Run forward pass. This will be called by both functions <optimize_parameters> and <test>.""""""\n\n        data_features = self.backbone_model.forward(self.input)\n        data_votes = self.voting_module(data_features)\n        outputs = self.proposal_cls_module(data_votes)\n\n        sampling_id_key = ""sampling_id_0""\n        num_seeds = data_features.pos.shape[1]\n        seed_inds = getattr(data_features, sampling_id_key, None)[:, :num_seeds]\n        setattr(outputs, ""seed_inds"", seed_inds)  # [B,num_seeds]\n\n        self.output = outputs\n        self._compute_losses()\n\n    def _compute_losses(self):\n        losses, metrics, labels = votenet_module.get_loss(self.input, self.output, self.loss_params)\n        self.labels = labels\n        for loss_name, loss in losses.items():\n            if torch.is_tensor(loss):\n                if not self.losses_has_been_added:\n                    self.loss_names += [loss_name]\n                setattr(self, loss_name, loss)\n        self.losses_has_been_added = True\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        self.loss.backward()\n'"
torch_points3d/models/registration/base.py,4,"b'import logging\nimport torch\nimport torch.nn.functional as F\nfrom typing import Any\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.models.base_architectures import BackboneBasedModel\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\n\nlog = logging.getLogger(__name__)\n\n\ndef create_batch_siamese(pair, batch):\n    """"""\n    create a batch with siamese input\n    """"""\n    return 2 * batch + pair\n\n\nclass PatchSiamese(BackboneBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        """"""\n        Initialize this model class\n        Parameters:\n            opt -- training/test options\n        A few things can be done here.\n        - (required) call the initialization function of BaseModel\n        - define loss function, visualization images, model names, and optimizers\n        """"""\n\n        BackboneBasedModel.__init__(self, option, model_type, dataset, modules)\n        self.set_last_mlp(option.mlp_cls)\n        self.loss_names = [""loss_reg""]\n\n    def set_input(self, data, device):\n        data = data.to(device)\n        self.input = data\n        # TODO multiscale data pre_computed...\n        if isinstance(data, MultiScaleBatch):\n            self.pre_computed = data.multiscale\n            del data.multiscale\n        else:\n            self.pre_computed = None\n        # batch siamese\n        self.batch_idx = create_batch_siamese(data.pair, data.batch)\n\n    def set_last_mlp(self, last_mlp_opt):\n        self.FC_layer = Seq()\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))\n\n    def set_loss(self):\n        raise NotImplementedError(""Choose a loss for the metric learning"")\n\n    def forward(self) -> Any:\n        """"""Run forward pass. This will be called by both functions <optimize_parameters> and <test>.""""""\n\n        data = self.input\n        for i in range(len(self.down_modules)):\n            data = self.down_modules[i](data, precomputed=self.pre_computed)\n\n        x = F.relu(self.lin1(data.x))\n        x = F.dropout(x, p=self.dropout, training=self.training)\n        self.output = self.lin2(x)\n\n        self.loss_reg = self.loss_module(self.output) + self.get_internal_loss()\n        return self.output\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss_reg.backward()  # calculate gradients of network G w.r.t. loss_G\n\n\nclass FragmentBaseModel(BaseModel):\n    def __init__(self, option):\n        BaseModel.__init__(self, option)\n\n    def set_input(self, data, device):\n        raise NotImplementedError(""need to define set_input"")\n\n    def compute_loss_match(self):\n        if hasattr(self, ""xyz""):\n            xyz = self.xyz\n            xyz_target = self.xyz_target\n        else:\n            xyz = self.input.pos\n            xyz_target = self.input_target.pos\n        loss_reg = self.metric_loss_module(self.output, self.output_target, self.match[:, :2], xyz, xyz_target)\n        return loss_reg\n\n    def compute_loss_label(self):\n        """"""\n        compute the loss separating the miner and the loss\n        each point correspond to a labels\n        """"""\n        output = torch.cat([self.output[self.match[:, 0]], self.output_target[self.match[:, 1]]], 0)\n        rang = torch.arange(0, len(self.match), dtype=torch.long, device=self.match.device)\n        labels = torch.cat([rang, rang], 0)\n        hard_pairs = None\n        if self.miner_module is not None:\n            hard_pairs = self.miner_module(output, labels)\n        # loss\n        loss_reg = self.metric_loss_module(output, labels, hard_pairs)\n        return loss_reg\n\n    def compute_loss(self):\n        if self.mode == ""match"":\n            self.loss = self.compute_loss_match()\n        elif self.mode == ""label"":\n            self.loss = self.compute_loss_label()\n        else:\n            raise NotImplementedError(""The mode for the loss is incorrect"")\n\n    def apply_nn(self, input):\n        raise NotImplementedError(""Model still not defined"")\n\n    def forward(self):\n        self.output = self.apply_nn(self.input)\n        if self.match is None:\n            return self.output\n\n        self.output_target = self.apply_nn(self.input_target)\n        self.compute_loss()\n\n        return self.output\n\n    def backward(self):\n        if hasattr(self, ""loss""):\n            self.loss.backward()\n\n    def get_output(self):\n        if self.match is not None:\n            return self.output, self.output_target\n        else:\n            return self.output, None\n\n    def get_batch(self):\n        raise NotImplementedError(""Need to define get_batch"")\n\n    def get_input(self):\n        raise NotImplementedError(""Need to define get_input"")\n'"
torch_points3d/models/registration/kpconv.py,12,"b'from typing import Any\nimport torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear as Lin\n\nfrom torch.nn import Sequential as Seq\nfrom torch.nn import Identity\nfrom torch_geometric.data import Data\nfrom torch_geometric.nn import global_mean_pool\n\n\nimport logging\n\nfrom torch_points3d.core.losses import *\nfrom torch_points3d.core.common_modules import MLP\nfrom torch_points3d.core.common_modules import FastBatchNorm1d\nfrom torch_points3d.core.base_conv.partial_dense import *\n\nfrom torch_points3d.models.registration.base import FragmentBaseModel\nfrom torch_points3d.models.base_architectures import BackboneBasedModel\nfrom torch_points3d.models.registration.base import create_batch_siamese\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.models.base_architectures.unet import UnwrappedUnetBasedModel\n\nfrom torch_points3d.modules.KPConv import *\n\nfrom torch_points3d.datasets.registration.pair import PairMultiScaleBatch\n\nlog = logging.getLogger(__name__)\n\n\nclass PatchKPConv(BackboneBasedModel):\n    r""""""\n    siamese neural network using Kernel Point\n    Convolution to learn descriptors on patch(for registration).\n    """"""\n\n    def __init__(self, option, model_type, dataset, modules):\n\n        BackboneBasedModel.__init__(self, option, model_type, dataset, modules)\n        self.set_last_mlp(option.mlp_cls)\n        self.loss_names = [""loss_reg"", ""loss"", ""internal""]\n\n    def set_last_mlp(self, last_mlp_opt):\n\n        if len(last_mlp_opt.nn) > 2:\n\n            self.FC_layer = MLP(last_mlp_opt.nn[: len(last_mlp_opt.nn) - 1])\n            self.FC_layer.add_module(""last"", Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))\n        elif len(last_mlp_opt.nn) == 2:\n            self.FC_layer = Seq(Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))\n        else:\n            self.FC_layer = torch.nn.Identity()\n\n    def set_input(self, data, device):\n        data = data.to(device)\n        data.x = add_ones(data.pos, data.x, True)\n        self.batch_idx = data.batch\n        if isinstance(data, PairMultiScaleBatch):\n            self.pre_computed = data.multiscale\n        else:\n            self.pre_computed = None\n        if getattr(data, ""pos_target"", None) is not None:\n            data.x_target = add_ones(data.pos_target, data.x_target, True)\n            if isinstance(data, PairMultiScaleBatch):\n                self.pre_computed_target = data.multiscale_target\n                del data.multiscale_target\n            else:\n                self.pre_computed_target = None\n\n            self.input, self.input_target = data.to_data()\n            self.batch_idx_target = data.batch_target\n            rang = torch.arange(0, data.batch_idx[-1])\n            rang_target = torch.arange(0, data.batch_idx_target[-1])\n            assert len(rang) == len(rang_target)\n            self.labels = torch.cat([rang, rang_target], 0).to(device)\n        else:\n            self.input = data\n            self.labels = None\n\n    def apply_nn(self, input, pre_computed, batch):\n        data = input\n        for i in range(len(self.down_modules)):\n            data = self.down_modules[i](data, precomputed=pre_computed)\n\n        last_feature = global_mean_pool(data.x, batch)\n        output = self.FC_layer(last_feature)\n        return F.normalize(output, p=2, dim=1)\n\n    def forward(self) -> Any:\n\n        self.output = self.apply_nn(self.input, self.pre_computed, self.batch_idx)\n        if self.labels is None:\n            return self.output\n        else:\n            output_target = self.apply_nn(self.input_target, self.pre_computed_target, self.batch_idx_target)\n            self.output = torch.cat([self.output, output_target], 0)\n            self.compute_loss()\n            return self.output\n\n    def compute_loss(self):\n        self.loss = 0\n\n        # Get regularization on weights\n        if self.lambda_reg:\n            self.loss_reg = self.get_regularization_loss(regularizer_type=""l2"", lambda_reg=self.lambda_reg)\n            self.loss += self.loss_reg\n\n        # Collect internal losses and set them with self and them to self for later tracking\n        if self.lambda_internal_losses:\n            self.loss += self.collect_internal_losses(lambda_weight=self.lambda_internal_losses)\n\n        hard_pairs = None\n        if self.miner_module is not None:\n            hard_pairs = self.miner_module(self.output, self.labels)\n        self.loss_reg = self.metric_loss_module(self.output, self.labels, hard_pairs)\n\n        self.loss += self.loss_reg\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        if hasattr(self, ""loss""):\n            self.loss.backward()  # calculate gradients of network G w.r.t. loss_G\n\n\nclass FragmentKPConv(FragmentBaseModel, UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        # Extract parameters from the dataset\n        # Assemble encoder / decoder\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n\n        # Build final MLP\n        last_mlp_opt = option.mlp_cls\n\n        self.out_channels = option.out_channels\n        in_feat = last_mlp_opt.nn[0]\n        self.FC_layer = Seq()\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.add_module(\n                str(i),\n                Seq(\n                    *[\n                        Lin(in_feat, last_mlp_opt.nn[i], bias=False),\n                        FastBatchNorm1d(last_mlp_opt.nn[i], momentum=last_mlp_opt.bn_momentum),\n                        LeakyReLU(0.2),\n                    ]\n                ),\n            )\n            in_feat = last_mlp_opt.nn[i]\n\n        if last_mlp_opt.dropout:\n            self.FC_layer.add_module(""Dropout"", Dropout(p=last_mlp_opt.dropout))\n\n        self.FC_layer.add_module(""Last"", Lin(in_feat, self.out_channels, bias=False))\n        self.mode = option.loss_mode\n        self.normalize_feature = option.normalize_feature\n        self.loss_names = [""loss_reg"", ""loss""]\n\n        self.lambda_reg = self.get_from_opt(option, [""loss_weights"", ""lambda_reg""])\n        if self.lambda_reg:\n            self.loss_names += [""loss_regul""]\n\n        self.lambda_internal_losses = self.get_from_opt(option, [""loss_weights"", ""lambda_internal_losses""])\n\n        self.visual_names = [""data_visual""]\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        """"""\n        # data = data.to(device)\n        if isinstance(data, PairMultiScaleBatch):\n            self.pre_computed = [f.to(device) for f in data.multiscale]\n            self.upsample = [f.to(device) for f in data.upsample]\n        else:\n            self.upsample = None\n            self.pre_computed = None\n\n        self.input = Data(pos=data.pos, x=data.x, batch=data.batch).to(device)\n        if hasattr(data, ""pos_target""):\n            if isinstance(data, PairMultiScaleBatch):\n                self.pre_computed_target = [f.to(device) for f in data.multiscale_target]\n                self.upsample_target = [f.to(device) for f in data.upsample_target]\n            else:\n                self.upsample_target = None\n                self.pre_computed_target = None\n            self.input_target = Data(pos=data.pos_target, x=data.x_target, batch=data.batch_target).to(device)\n            self.match = data.pair_ind.to(torch.long).to(device)\n            self.size_match = data.size_pair_ind.to(torch.long).to(device)\n        else:\n            self.match = None\n\n    def apply_nn(self, input, pre_computed, upsample):\n        stack_down = []\n        data = input\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data, precomputed=pre_computed)\n            stack_down.append(data)\n\n        data = self.down_modules[-1](data, precomputed=pre_computed)\n        innermost = False\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n            innermost = True\n\n        for i in range(len(self.up_modules)):\n            if i == 0 and innermost:\n                data = self.up_modules[i]((data, stack_down.pop()))\n            else:\n                data = self.up_modules[i]((data, stack_down.pop()), precomputed=upsample)\n\n        output = self.FC_layer(data.x)\n        if self.normalize_feature:\n            return output / (torch.norm(output, p=2, dim=1, keepdim=True) + 1e-3)\n        else:\n            return output\n\n    def forward(self):\n        self.output = self.apply_nn(self.input, self.pre_computed, self.upsample)\n        if self.match is None:\n            return self.output\n\n        self.output_target = self.apply_nn(self.input_target, self.pre_computed_target, self.upsample_target)\n        self.compute_loss()\n\n        return self.output\n\n    def compute_loss(self):\n        self.loss = 0\n\n        # Collect internal losses and set them with self and them to self for later tracking\n        if self.lambda_internal_losses:\n            self.loss_internal = self.collect_internal_losses(lambda_weight=self.lambda_internal_losses)\n            self.loss += self.loss_internal\n\n        if self.mode == ""match"":\n            self.loss_reg = self.compute_loss_match()\n        elif self.mode == ""label"":\n            self.loss_reg = self.compute_loss_label()\n\n        self.loss += self.loss_reg\n\n    def get_batch(self):\n        if self.match is not None:\n            batch = self.input.batch\n            batch_target = self.input_target.batch\n            return batch, batch_target\n        else:\n            return None\n\n    def get_input(self):\n        if self.match is not None:\n            input = Data(pos=self.input.pos, ind=self.match[:, 0], size=self.size_match)\n            input_target = Data(pos=self.input_target.pos, ind=self.match[:, 1], size=self.size_match)\n            return input, input_target\n        else:\n            input = Data(pos=self.xyz)\n            return input\n'"
torch_points3d/models/registration/minkowski.py,11,"b'import logging\nimport torch\nfrom torch_geometric.data import Data\n\nfrom torch_points3d.modules.MinkowskiEngine import *\nfrom torch_points3d.models.base_architectures import UnwrappedUnetBasedModel\nfrom torch_points3d.models.registration.base import FragmentBaseModel\nfrom torch.nn import Sequential, Linear, LeakyReLU, Dropout\nfrom torch_points3d.core.common_modules import FastBatchNorm1d, Seq\n\n\nlog = logging.getLogger(__name__)\n\n\nclass BaseMinkowski(FragmentBaseModel):\n    def __init__(self, option, model_type, dataset, modules):\n        FragmentBaseModel.__init__(self, option)\n        self.mode = option.loss_mode\n        self.normalize_feature = option.normalize_feature\n        self.loss_names = [""loss_reg"", ""loss""]\n        self.metric_loss_module, self.miner_module = FragmentBaseModel.get_metric_loss_and_miner(\n            getattr(option, ""metric_loss"", None), getattr(option, ""miner"", None)\n        )\n        # Last Layer\n\n        if option.mlp_cls is not None:\n            last_mlp_opt = option.mlp_cls\n            in_feat = last_mlp_opt.nn[0]\n            self.FC_layer = Seq()\n            for i in range(1, len(last_mlp_opt.nn)):\n                self.FC_layer.append(\n                    str(i),\n                    Sequential(\n                        *[\n                            Linear(in_feat, last_mlp_opt.nn[i], bias=False),\n                            FastBatchNorm1d(last_mlp_opt.nn[i], momentum=last_mlp_opt.bn_momentum),\n                            LeakyReLU(0.2),\n                        ]\n                    ),\n                )\n                in_feat = last_mlp_opt.nn[i]\n\n            if last_mlp_opt.dropout:\n                self.FC_layer.append(Dropout(p=last_mlp_opt.dropout))\n\n            self.FC_layer.append(Linear(in_feat, in_feat, bias=False))\n        else:\n            self.FC_layer = torch.nn.Identity()\n\n    def set_input(self, data, device):\n        coords = torch.cat([data.batch.unsqueeze(-1).int(), data.pos.int()], -1)\n        self.input = ME.SparseTensor(data.x, coords=coords).to(device)\n        self.xyz = torch.stack((data.pos_x, data.pos_y, data.pos_z), 0).T.to(device)\n        if hasattr(data, ""pos_target""):\n            coords_target = torch.cat([data.batch_target.unsqueeze(-1).int(), data.pos_target.int()], -1)\n            self.input_target = ME.SparseTensor(data.x_target, coords=coords_target).to(device)\n            self.xyz_target = torch.stack((data.pos_x_target, data.pos_y_target, data.pos_z_target), 0).T.to(device)\n            self.match = data.pair_ind.to(torch.long).to(device)\n            self.size_match = data.size_pair_ind.to(torch.long).to(device)\n        else:\n            self.match = None\n\n    def get_batch(self):\n        if self.match is not None:\n            batch = self.input.C[:, 0]\n            batch_target = self.input_target.C[:, 0]\n            return batch, batch_target\n        else:\n            return None, None\n\n    def get_input(self):\n        if self.match is not None:\n            input = Data(pos=self.xyz, ind=self.match[:, 0], size=self.size_match)\n            input_target = Data(pos=self.xyz_target, ind=self.match[:, 1], size=self.size_match)\n            return input, input_target\n        else:\n            input = Data(pos=self.xyz)\n            return input, None\n\n\nclass Minkowski_Baseline_Model_Fragment(BaseMinkowski):\n    def __init__(self, option, model_type, dataset, modules):\n        BaseMinkowski.__init__(self, option, model_type, dataset, modules)\n\n        self.model = initialize_minkowski_unet(\n            option.model_name,\n            in_channels=dataset.feature_dimension,\n            out_channels=option.out_channels,\n            D=option.D,\n            conv1_kernel_size=option.conv1_kernel_size,\n        )\n\n    def apply_nn(self, input):\n        output = self.model(input).F\n        output = self.FC_layer(output)\n        if self.normalize_feature:\n            return output / (torch.norm(output, p=2, dim=1, keepdim=True) + 1e-3)\n        else:\n            return output\n\n\nclass MinkowskiFragment(BaseMinkowski, UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n        self.mode = option.loss_mode\n        self.normalize_feature = option.normalize_feature\n        self.loss_names = [""loss_reg"", ""loss""]\n        self.metric_loss_module, self.miner_module = FragmentBaseModel.get_metric_loss_and_miner(\n            getattr(option, ""metric_loss"", None), getattr(option, ""miner"", None)\n        )\n        # Last Layer\n\n        if option.mlp_cls is not None:\n            last_mlp_opt = option.mlp_cls\n            in_feat = last_mlp_opt.nn[0]\n            self.FC_layer = Seq()\n            for i in range(1, len(last_mlp_opt.nn)):\n                self.FC_layer.append(\n                    str(i),\n                    Sequential(\n                        *[\n                            Linear(in_feat, last_mlp_opt.nn[i], bias=False),\n                            FastBatchNorm1d(last_mlp_opt.nn[i], momentum=last_mlp_opt.bn_momentum),\n                            LeakyReLU(0.2),\n                        ]\n                    ),\n                )\n                in_feat = last_mlp_opt.nn[i]\n\n            if last_mlp_opt.dropout:\n                self.FC_layer.append(Dropout(p=last_mlp_opt.dropout))\n\n            self.FC_layer.append(Linear(in_feat, in_feat, bias=False))\n        else:\n            self.FC_layer = torch.nn.Identity()\n\n    def apply_nn(self, input):\n        x = input\n        stack_down = []\n        for i in range(len(self.down_modules) - 1):\n            x = self.down_modules[i](x)\n            stack_down.append(x)\n\n        x = self.down_modules[-1](x)\n        stack_down.append(None)\n\n        for i in range(len(self.up_modules)):\n            x = self.up_modules[i](x, stack_down.pop())\n        out_feat = self.FC_layer(x.F)\n        # out_feat = x.F\n        if self.normalize_feature:\n            return out_feat / (torch.norm(out_feat, p=2, dim=1, keepdim=True) + 1e-20)\n        else:\n            return out_feat\n'"
torch_points3d/models/registration/pointnet.py,0,"b'from torch_geometric.data import Data\nimport logging\n\nfrom torch_points3d.modules.pointnet2 import *\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom torch_points3d.core.base_conv.dense import DenseFPModule\nfrom torch_points3d.models.base_architectures import BackboneBasedModel\nfrom torch_points3d.models.registration.base import create_batch_siamese\n\nlog = logging.getLogger(__name__)\n\n\nclass SiamesePointNet2_D(BackboneBasedModel):\n    r""""""\n    PointNet2 with multi-scale grouping\n    metric learning siamese network that uses feature propogation layers\n    """"""\n\n    def __init__(self, option, model_type, dataset, modules):\n        BackboneBasedModel.__init__(self, option, model_type, dataset, modules)\n\n        # Last MLP\n        last_mlp_opt = option.mlp_cls\n        self._dim_output = last_mlp_opt.nn[-1]\n\n        self.FC_layer = Seq()\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))\n\n        self.loss_names = [""loss_patch_desc""]\n\n    def set_input(self, data, device):\n        assert len(data.pos.shape) == 3\n        data = data.to(device)\n        self.input = Data(x=data.x.transpose(1, 2).contiguous(), pos=data.pos)\n\n    def forward(self):\n        r""""""\n        forward pass of the network\n        """"""\n        data = self.input\n        for i in range(len(self.down_modules)):\n            data = self.down_modules[i](data)\n        last_feature = data.x\n        self.output = self.FC_layer(last_feature).transpose(1, 2).contiguous().view((-1, self._dim_output))\n\n        self.loss_reg = self.loss_module(self.output) + self.get_internal_loss()\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss_reg.backward()  # calculate gradients of network G w.r.t. loss_G\n'"
torch_points3d/models/registration/pointnet2.py,15,"b'import torch\nimport torch.nn.functional as F\nfrom torch.nn import Linear as Lin\nfrom torch.nn import Sequential\nfrom torch_geometric.data import Data\nimport logging\n\nfrom torch_points3d.core.losses import *\nfrom torch_points3d.modules.pointnet2 import *\nfrom torch_points3d.core.common_modules import MLP\nfrom torch_points3d.models.base_architectures import BackboneBasedModel\nfrom torch_points3d.models.base_architectures import UnetBasedModel\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom torch_points3d.models.registration.base import FragmentBaseModel\n\nlog = logging.getLogger(__name__)\n\n\nclass PatchPointNet2_D(BackboneBasedModel):\n    r""""""\n    PointNet2 with multi-scale grouping\n    metric learning siamese network that uses feature propogation layers\n\n\n    """"""\n\n    def __init__(self, option, model_type, dataset, modules):\n        BackboneBasedModel.__init__(self, option, model_type, dataset, modules)\n\n        # Last MLP\n        self.set_last_mlp(option.mlp_cls)\n        self.loss_names = [""loss_reg"", ""loss"", ""internal""]\n\n    def set_last_mlp(self, last_mlp_opt):\n\n        if len(last_mlp_opt.nn) > 2:\n            self.FC_layer = MLP(last_mlp_opt.nn[: len(last_mlp_opt.nn) - 1])\n            self.FC_layer.add_module(""last"", Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))\n        elif len(last_mlp_opt.nn) == 2:\n            self.FC_layer = Sequential(Lin(last_mlp_opt.nn[-2], last_mlp_opt.nn[-1]))\n        else:\n            self.FC_layer = torch.nn.Identity()\n\n    def set_input(self, data, device):\n        # Size : B x N x 3\n        # manually concatenate the b\n\n        if getattr(data, ""pos_target"", None) is not None:\n            assert len(data.pos.shape) == 3 and len(data.pos_target.shape) == 3\n            if data.x is not None:\n                x = torch.cat([data.x, data.x_target], 0)\n            else:\n                x = None\n            pos = torch.cat([data.pos, data.pos_target], 0)\n            rang = torch.arange(0, data.pos.shape[0])\n\n            labels = torch.cat([rang, rang], 0)\n        else:\n            x = data.x\n            pos = data.pos\n            labels = None\n\n        if x is not None:\n            x = x.transpose(1, 2).contiguous()\n\n        self.input = Data(x=x, pos=pos, y=labels).to(device)\n\n    def forward(self):\n        r""""""\n        forward pass of the network\n        """"""\n        data = self.input\n        labels = data.y\n        for i in range(len(self.down_modules)):\n            data = self.down_modules[i](data)\n\n        # size after pointnet B x D x N\n        last_feature = torch.mean(data.x, dim=-1)\n        # size after global pooling B x D\n        self.output = self.FC_layer(last_feature)\n        self.output = F.normalize(self.output, p=2, dim=1)\n        if labels is None:\n            return self.output\n        hard_pairs = None\n        if self.miner_module is not None:\n            hard_pairs = self.miner_module(self.output, labels)\n        self.loss_reg = self.metric_loss_module(self.output, labels, hard_pairs)\n        self.internal = self.get_internal_loss()\n        self.loss = self.loss_reg + self.internal\n        return self.output\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        if hasattr(self, ""loss""):\n            self.loss.backward()  # calculate gradients of network G w.r.t. loss_G\n\n\nclass FragmentPointNet2_D(UnetBasedModel, FragmentBaseModel):\n\n    r""""""\n        PointNet2 with multi-scale grouping\n        descriptors network for registration that uses feature propogation layers\n\n        Parameters\n        ----------\n        num_classes: int\n            Number of semantics classes to predict over -- size of softmax classifier that run for each point\n        input_channels: int = 6\n            Number of input channels in the feature descriptor for each point.  If the point cloud is Nx9, this\n            value should be 6 as in an Nx9 point cloud, 3 of the channels are xyz, and 6 are feature descriptors\n        use_xyz: bool = True\n            Whether or not to use the xyz position of a point as a feature\n    """"""\n\n    def __init__(self, option, model_type, dataset, modules):\n        # call the initialization method of UnetBasedModel\n        UnetBasedModel.__init__(self, option, model_type, dataset, modules)\n        # Last MLP\n        self.mode = option.loss_mode\n        self.normalize_feature = option.normalize_feature\n        self.out_channels = option.out_channels\n        self.loss_names = [""loss_reg"", ""loss""]\n        self.metric_loss_module, self.miner_module = UnetBasedModel.get_metric_loss_and_miner(\n            getattr(option, ""metric_loss"", None), getattr(option, ""miner"", None)\n        )\n        last_mlp_opt = option.mlp_cls\n\n        self.FC_layer = Seq()\n        last_mlp_opt.nn[0]\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))\n        if last_mlp_opt.dropout:\n            self.FC_layer.append(torch.nn.Dropout(p=last_mlp_opt.dropout))\n\n        self.FC_layer.append(Conv1D(last_mlp_opt.nn[-1], self.out_channels, activation=None, bias=True, bn=False))\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        Sets:\n            self.input:\n                x -- Features [B, C, N]\n                pos -- Points [B, N, 3]\n        """"""\n        assert len(data.pos.shape) == 3\n\n        if data.x is not None:\n            x = data.x.transpose(1, 2).contiguous()\n        else:\n            x = None\n        self.input = Data(x=x, pos=data.pos).to(device)\n\n        if hasattr(data, ""pos_target""):\n            if data.x_target is not None:\n                x = data.x_target.transpose(1, 2).contiguous()\n            else:\n                x = None\n            self.input_target = Data(x=x, pos=data.pos_target).to(device)\n            self.match = data.pair_ind.to(torch.long).to(device)\n            self.size_match = data.size_pair_ind.to(torch.long).to(device)\n        else:\n            self.match = None\n\n    def apply_nn(self, input):\n        last_feature = self.model(input).x\n        output = self.FC_layer(last_feature).transpose(1, 2).contiguous().view((-1, self.out_channels))\n        if self.normalize_feature:\n            return output / (torch.norm(output, p=2, dim=1, keepdim=True) + 1e-5)\n        else:\n            return output\n\n    def get_input(self):\n        if self.match is not None:\n            input = Data(pos=self.input.pos.view(-1, 3), ind=self.match[:, 0], size=self.size_match)\n            input_target = Data(pos=self.input_target.pos.view(-1, 3), ind=self.match[:, 1], size=self.size_match)\n            return input, input_target\n        else:\n            input = Data(pos=self.input.pos.view(-1, 3))\n            return input\n\n    def get_batch(self):\n        if self.match is not None:\n            batch = (\n                torch.arange(0, self.input.pos.shape[0])\n                .view(-1, 1)\n                .repeat(1, self.input.pos.shape[1])\n                .view(-1)\n                .to(self.input.pos.device)\n            )\n            batch_target = (\n                torch.arange(0, self.input_target.pos.shape[0])\n                .view(-1, 1)\n                .repeat(1, self.input_target.pos.shape[1])\n                .view(-1)\n                .to(self.input.pos.device)\n            )\n            return batch, batch_target\n        else:\n            return None, None\n'"
torch_points3d/models/segmentation/__init__.py,0,b''
torch_points3d/models/segmentation/base.py,4,"b'import logging\nimport torch\nimport torch.nn.functional as F\nfrom typing import Any\n\nfrom torch_points3d.models.base_architectures import UnetBasedModel\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n\nclass Segmentation_MP(UnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        """"""Initialize this model class.\n        Parameters:\n            opt -- training/test options\n        A few things can be done here.\n        - (required) call the initialization function of BaseModel\n        - define loss function, visualization images, model names, and optimizers\n        """"""\n        UnetBasedModel.__init__(\n            self, option, model_type, dataset, modules\n        )  # call the initialization method of UnetBasedModel\n\n        self._weight_classes = dataset.weight_classes\n\n        nn = option.mlp_cls.nn\n        self.dropout = option.mlp_cls.get(""dropout"")\n        self.lin1 = torch.nn.Linear(nn[0], nn[1])\n        self.lin2 = torch.nn.Linear(nn[2], nn[3])\n        self.lin3 = torch.nn.Linear(nn[4], dataset.num_classes)\n\n        self.loss_names = [""loss_seg""]\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        """"""\n        data = data.to(device)\n        self.input = data\n        self.labels = data.y\n        self.batch_idx = data.batch\n\n    def forward(self, *args, **kwargs) -> Any:\n        """"""Run forward pass. This will be called by both functions <optimize_parameters> and <test>.""""""\n        data = self.model(self.input)\n        x = F.relu(self.lin1(data.x))\n        x = F.dropout(x, p=self.dropout, training=bool(self.training))\n        x = self.lin2(x)\n        x = F.dropout(x, p=self.dropout, training=bool(self.training))\n        x = self.lin3(x)\n        self.output = F.log_softmax(x, dim=-1)\n\n        if self.labels is not None:\n            self.loss_seg = F.nll_loss(self.output, self.labels, ignore_index=IGNORE_LABEL) + self.get_internal_loss()\n\n        return self.output\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss_seg.backward()  # calculate gradients of network G w.r.t. loss_G\n'"
torch_points3d/models/segmentation/kpconv.py,3,"b'from typing import Any\nimport logging\nfrom omegaconf.dictconfig import DictConfig\nfrom omegaconf.listconfig import ListConfig\nfrom torch.nn import Sequential, Dropout, Linear\nimport torch.nn.functional as F\nfrom torch import nn\n\nfrom .base import Segmentation_MP\nfrom torch_points3d.core.common_modules import FastBatchNorm1d\nfrom torch_points3d.modules.KPConv import *\nfrom torch_points3d.core.base_conv.partial_dense import *\nfrom torch_points3d.core.common_modules import MultiHeadClassifier\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.models.base_architectures.unet import UnwrappedUnetBasedModel\nfrom torch_points3d.datasets.multiscale_data import MultiScaleBatch\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n\nclass KPConvPaper(UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        # Extract parameters from the dataset\n        self._num_classes = dataset.num_classes\n        self._weight_classes = dataset.weight_classes\n        self._use_category = getattr(option, ""use_category"", False)\n        if self._use_category:\n            if not dataset.class_to_segments:\n                raise ValueError(\n                    ""The dataset needs to specify a class_to_segments property when using category information for segmentation""\n                )\n            self._class_to_seg = dataset.class_to_segments\n            self._num_categories = len(self._class_to_seg)\n            log.info(""Using category information for the predictions with %i categories"", self._num_categories)\n        else:\n            self._num_categories = 0\n\n        # Assemble encoder / decoder\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n\n        # Build final MLP\n        last_mlp_opt = option.mlp_cls\n        if self._use_category:\n            self.FC_layer = MultiHeadClassifier(\n                last_mlp_opt.nn[0],\n                self._class_to_seg,\n                dropout_proba=last_mlp_opt.dropout,\n                bn_momentum=last_mlp_opt.bn_momentum,\n            )\n        else:\n            in_feat = last_mlp_opt.nn[0] + self._num_categories\n            self.FC_layer = Sequential()\n            for i in range(1, len(last_mlp_opt.nn)):\n                self.FC_layer.add_module(\n                    str(i),\n                    Sequential(\n                        *[\n                            Linear(in_feat, last_mlp_opt.nn[i], bias=False),\n                            FastBatchNorm1d(last_mlp_opt.nn[i], momentum=last_mlp_opt.bn_momentum),\n                            LeakyReLU(0.2),\n                        ]\n                    ),\n                )\n                in_feat = last_mlp_opt.nn[i]\n\n            if last_mlp_opt.dropout:\n                self.FC_layer.add_module(""Dropout"", Dropout(p=last_mlp_opt.dropout))\n\n            self.FC_layer.add_module(""Class"", Lin(in_feat, self._num_classes, bias=False))\n            self.FC_layer.add_module(""Softmax"", nn.LogSoftmax(-1))\n        self.loss_names = [""loss_seg""]\n\n        self.lambda_reg = self.get_from_opt(option, [""loss_weights"", ""lambda_reg""])\n        if self.lambda_reg:\n            self.loss_names += [""loss_reg""]\n\n        self.lambda_internal_losses = self.get_from_opt(option, [""loss_weights"", ""lambda_internal_losses""])\n\n        self.visual_names = [""data_visual""]\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        """"""\n        data = data.to(device)\n        data.x = add_ones(data.pos, data.x, True)\n\n        if isinstance(data, MultiScaleBatch):\n            self.pre_computed = data.multiscale\n            self.upsample = data.upsample\n            del data.upsample\n            del data.multiscale\n        else:\n            self.upsample = None\n            self.pre_computed = None\n\n        self.input = data\n        self.labels = data.y\n        self.batch_idx = data.batch\n\n        if self._use_category:\n            self.category = data.category\n\n    def forward(self, *args, **kwargs) -> Any:\n        """"""Run forward pass. This will be called by both functions <optimize_parameters> and <test>.""""""\n        stack_down = []\n\n        data = self.input\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data, precomputed=self.pre_computed)\n            stack_down.append(data)\n\n        data = self.down_modules[-1](data, precomputed=self.pre_computed)\n        innermost = False\n\n        if not isinstance(self.inner_modules[0], Identity):\n            stack_down.append(data)\n            data = self.inner_modules[0](data)\n            innermost = True\n\n        for i in range(len(self.up_modules)):\n            if i == 0 and innermost:\n                data = self.up_modules[i]((data, stack_down.pop()))\n            else:\n                data = self.up_modules[i]((data, stack_down.pop()), precomputed=self.upsample)\n\n        last_feature = data.x\n        if self._use_category:\n            self.output = self.FC_layer(last_feature, self.category)\n        else:\n            self.output = self.FC_layer(last_feature)\n\n        if self.labels is not None:\n            self.compute_loss()\n\n        self.data_visual = self.input\n        self.data_visual.pred = torch.max(self.output, -1)[1]\n        return self.output\n\n    def compute_loss(self):\n        if self._weight_classes is not None:\n            self._weight_classes = self._weight_classes.to(self.output.device)\n\n        self.loss = 0\n\n        # Get regularization on weights\n        if self.lambda_reg:\n            self.loss_reg = self.get_regularization_loss(regularizer_type=""l2"", lambda_reg=self.lambda_reg)\n            self.loss += self.loss_reg\n\n        # Collect internal losses and set them with self and them to self for later tracking\n        if self.lambda_internal_losses:\n            self.loss += self.collect_internal_losses(lambda_weight=self.lambda_internal_losses)\n\n        # Final cross entrop loss\n        self.loss_seg = F.nll_loss(self.output, self.labels, weight=self._weight_classes, ignore_index=IGNORE_LABEL)\n        self.loss += self.loss_seg\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss.backward()  # calculate gradients of network G w.r.t. loss_G\n'"
torch_points3d/models/segmentation/minkowski.py,3,"b'import logging\nimport torch.nn.functional as F\nimport torch\n\nfrom torch_points3d.modules.MinkowskiEngine import *\nfrom torch_points3d.models.base_architectures import UnwrappedUnetBasedModel\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\n\nlog = logging.getLogger(__name__)\n\n\nclass Minkowski_Baseline_Model(BaseModel):\n    def __init__(self, option, model_type, dataset, modules):\n        super(Minkowski_Baseline_Model, self).__init__(option)\n        self.model = initialize_minkowski_unet(\n            option.model_name, dataset.feature_dimension, dataset.num_classes, option.D\n        )\n        self.loss_names = [""loss_seg""]\n\n    def set_input(self, data, device):\n\n        self.batch_idx = data.batch.squeeze()\n        coords = torch.cat([data.batch.unsqueeze(-1).int(), data.pos.int()], -1)\n        self.input = ME.SparseTensor(data.x, coords=coords).to(device)\n        self.labels = data.y.to(device)\n\n    def forward(self):\n        self.output = F.log_softmax(self.model(self.input).feats, dim=-1)\n        self.loss_seg = F.nll_loss(self.output, self.labels, ignore_index=IGNORE_LABEL)\n\n    def backward(self):\n        self.loss_seg.backward()\n\n\n# This model still doesn\'t fully work yet.\nclass Minkowski_Model(UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        # call the initialization method of UnetBasedModel\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n        self.loss_names = [""loss_seg""]\n\n    def set_input(self, data, device):\n        coords = torch.cat([data.batch.unsqueeze(-1).int(), data.pos.int()], -1)\n        self.input = ME.SparseTensor(data.x, coords=coords).to(device)\n        self.labels = data.y\n\n    def forward(self):\n\n        stack_down = []\n\n        x = self.input\n        for i in range(len(self.down_modules) - 1):\n            print(x.shape)\n            x = self.down_modules[i](x)\n            stack_down.append(x)\n\n        x = self.down_modules[-1](x)\n\n        for i in range(len(self.up_modules)):\n            x = self.up_modules[i](x, stack_down.pop())\n\n    def backward(self):\n        pass\n'"
torch_points3d/models/segmentation/pointcnn.py,0,"b'from .base import Segmentation_MP\nfrom torch_points3d.modules.PointCNN import *\n\n\nclass PointCNNSeg(Segmentation_MP):\n    """""" Unet base implementation of PointCNN\n    https://arxiv.org/abs/1801.07791\n    """"""\n'"
torch_points3d/models/segmentation/pointnet.py,2,"b'import torch.nn.functional as F\nimport logging\n\nfrom torch_points3d.core.base_conv.base_conv import *\nfrom torch_points3d.core.common_modules.base_modules import *\nfrom torch_points3d.utils.config import ConvolutionFormatFactory\nfrom torch_points3d.modules.PointNet import *\nfrom torch_points3d.models.base_model import BaseModel\nfrom torch_points3d.utils.model_building_utils.resolver_utils import flatten_dict\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n\nclass PointNet(BaseModel):\n    def __init__(self, opt, type, dataset, modules_lib):\n        super().__init__(opt)\n        self.pointnet_seg = PointNetSeg(**flatten_dict(opt))\n        self._is_dense = ConvolutionFormatFactory.check_is_dense_format(self.conv_type)\n\n    def set_input(self, data, device):\n        data = data.to(device)\n        self.input = data\n        self.labels = data.y\n\n        self.pointnet_seg.set_scatter_pooling(not self._is_dense)\n\n    def forward(self):\n\n        x = self.pointnet_seg(self.input.pos, self.input.batch)\n        self.output = F.log_softmax(x, dim=-1)\n        internal_loss = self.get_internal_loss()\n        if self.labels is not None:\n            self.loss = (\n                F.nll_loss(self.output, self.labels, ignore_index=IGNORE_LABEL)\n                + (internal_loss if internal_loss.item() != 0 else 0) * 0.001\n            )\n        return self.output\n\n    def backward(self):\n        self.loss.backward()\n\n\nclass SegPointNetModel(BaseModel):\n    def __init__(self, opt, type, dataset, modules_lib):\n        super().__init__(opt)\n        self.pointnet_seg = MiniPointNet(\n            opt.pointnet.local_nn,\n            opt.pointnet.global_nn,\n            aggr=opt.pointnet.aggr,\n            return_local_out=opt.pointnet.return_local_out,\n        )\n        self.seg_nn = MLP(opt.seg_nn)\n\n    def set_input(self, data, device):\n        data = data.to(device)\n        self.pos = data.pos\n        self.labels = data.y\n        if not hasattr(data, ""batch""):\n            self.batch_idx = torch.zeros(self.labels.shape[0]).long()\n        else:\n            self.batch_idx = data.batch\n\n    def get_local_feat(self):\n        return self.pointnet_seg.local_nn(self.pos)\n\n    def forward(self):\n        x = self.pointnet_seg.forward_embedding(self.pos, self.batch_idx)\n        x = self.seg_nn(x)\n        self.output = F.log_softmax(x, dim=-1)\n        self.loss = F.nll_loss(self.output, self.labels)\n        return self.output\n\n    def backward(self):\n        self.loss.backward()\n'"
torch_points3d/models/segmentation/pointnet2.py,5,"b'import torch\n\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\nimport logging\n\nfrom torch_points3d.modules.pointnet2 import *\nfrom torch_points3d.core.base_conv.dense import DenseFPModule\nfrom torch_points3d.models.base_architectures import UnetBasedModel\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom .base import Segmentation_MP\nfrom torch_points3d.datasets.segmentation import IGNORE_LABEL\n\nlog = logging.getLogger(__name__)\n\n\nclass PointNet2_D(UnetBasedModel):\n    r""""""\n        PointNet2 with multi-scale grouping\n        Semantic segmentation network that uses feature propogation layers\n\n        Parameters\n        ----------\n        num_classes: int\n            Number of semantics classes to predict over -- size of softmax classifier that run for each point\n        input_channels: int = 6\n            Number of input channels in the feature descriptor for each point.  If the point cloud is Nx9, this\n            value should be 6 as in an Nx9 point cloud, 3 of the channels are xyz, and 6 are feature descriptors\n        use_xyz: bool = True\n            Whether or not to use the xyz position of a point as a feature\n    """"""\n\n    def __init__(self, option, model_type, dataset, modules):\n        # call the initialization method of UnetBasedModel\n        UnetBasedModel.__init__(self, option, model_type, dataset, modules)\n        self._num_classes = dataset.num_classes\n        self._weight_classes = dataset.weight_classes\n        self._use_category = getattr(option, ""use_category"", False)\n        if self._use_category:\n            if not dataset.class_to_segments:\n                raise ValueError(\n                    ""The dataset needs to specify a class_to_segments property when using category information for segmentation""\n                )\n            self._num_categories = len(dataset.class_to_segments.keys())\n            log.info(""Using category information for the predictions with %i categories"", self._num_categories)\n        else:\n            self._num_categories = 0\n\n        # Last MLP\n        last_mlp_opt = option.mlp_cls\n\n        self.FC_layer = Seq()\n        last_mlp_opt.nn[0] += self._num_categories\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))\n        if last_mlp_opt.dropout:\n            self.FC_layer.append(torch.nn.Dropout(p=last_mlp_opt.dropout))\n\n        self.FC_layer.append(Conv1D(last_mlp_opt.nn[-1], self._num_classes, activation=None, bias=True, bn=False))\n        self.loss_names = [""loss_seg""]\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        Sets:\n            self.input:\n                x -- Features [B, C, N]\n                pos -- Points [B, N, 3]\n        """"""\n        assert len(data.pos.shape) == 3\n        data = data.to(device)\n        if data.x is not None:\n            x = data.x.transpose(1, 2).contiguous()\n        else:\n            x = None\n        self.input = Data(x=x, pos=data.pos)\n        if data.y is not None:\n            self.labels = torch.flatten(data.y).long()  # [B * N]\n        else:\n            self.labels = None\n        self.batch_idx = torch.arange(0, data.pos.shape[0]).view(-1, 1).repeat(1, data.pos.shape[1]).view(-1)\n        if self._use_category:\n            self.category = data.category\n\n    def forward(self):\n        r""""""\n            Forward pass of the network\n            self.input:\n                x -- Features [B, C, N]\n                pos -- Points [B, N, 3]\n        """"""\n        data = self.model(self.input)\n        last_feature = data.x\n        if self._use_category:\n            cat_one_hot = F.one_hot(self.category, self._num_categories).float().transpose(1, 2)\n            last_feature = torch.cat((last_feature, cat_one_hot), dim=1)\n\n        self.output = self.FC_layer(last_feature).transpose(1, 2).contiguous().view((-1, self._num_classes))\n\n        if self._weight_classes is not None:\n            self._weight_classes = self._weight_classes.to(self.output.device)\n        if self.labels is not None:\n            self.loss_seg = F.cross_entropy(\n                self.output, self.labels, weight=self._weight_classes, ignore_index=IGNORE_LABEL\n            )\n        return self.output\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss_seg.backward()\n\n\nclass PointNet2_MP(Segmentation_MP):\n    """""" Message passing version of PN2""""""\n'"
torch_points3d/models/segmentation/randlanet.py,0,"b'from .base import Segmentation_MP\nfrom torch_points3d.modules.RandLANet import *\n\n\nclass RandLANetSeg(Segmentation_MP):\n    """""" Unet base implementation of RandLANet\n    """"""\n'"
torch_points3d/models/segmentation/rsconv.py,6,"b'import logging\nimport queue\n\nimport torch\nimport torch.nn.functional as F\n\nfrom torch_points3d.models.base_architectures import UnwrappedUnetBasedModel\nfrom torch_points3d.modules.RSConv import *\nfrom torch_points3d.core.common_modules.dense_modules import Conv1D\nfrom torch_points3d.core.common_modules.base_modules import Seq\nfrom .base import Segmentation_MP\n\nlog = logging.getLogger(__name__)\n\n\nclass RSConvLogicModel(UnwrappedUnetBasedModel):\n    def __init__(self, option, model_type, dataset, modules):\n        # call the initialization method of UnwrappedUnetBasedModel\n        UnwrappedUnetBasedModel.__init__(self, option, model_type, dataset, modules)\n        self._num_classes = dataset.num_classes\n        self._weight_classes = dataset.weight_classes\n        self._use_category = getattr(option, ""use_category"", False)\n        if self._use_category:\n            if not dataset.class_to_segments:\n                raise ValueError(\n                    ""The dataset needs to specify a class_to_segments property when using category information for segmentation""\n                )\n            self._num_categories = len(dataset.class_to_segments.keys())\n            log.info(""Using category information for the predictions with %i categories"", self._num_categories)\n        else:\n            self._num_categories = 0\n\n        # Last MLP\n        last_mlp_opt = option.mlp_cls\n\n        self.FC_layer = Seq()\n        last_mlp_opt.nn[0] += self._num_categories\n        for i in range(1, len(last_mlp_opt.nn)):\n            self.FC_layer.append(Conv1D(last_mlp_opt.nn[i - 1], last_mlp_opt.nn[i], bn=True, bias=False))\n        if last_mlp_opt.dropout:\n            self.FC_layer.append(torch.nn.Dropout(p=last_mlp_opt.dropout))\n\n        self.FC_layer.append(Conv1D(last_mlp_opt.nn[-1], self._num_classes, activation=None, bias=True, bn=False))\n        self.loss_names = [""loss_seg""]\n\n    def set_input(self, data, device):\n        """"""Unpack input data from the dataloader and perform necessary pre-processing steps.\n        Parameters:\n            input: a dictionary that contains the data itself and its metadata information.\n        Sets:\n            self.data:\n                x -- Features [B, C, N]\n                pos -- Features [B, 3, N]\n        """"""\n        data = data.to(device)\n        if data.x is not None:\n            data.x = data.x.transpose(1, 2).contiguous()\n        self.input = data\n        if data.y is not None:\n            self.labels = torch.flatten(data.y).long()  # [B,N]\n        else:\n            self.labels = data.y\n        self.batch_idx = torch.arange(0, data.pos.shape[0]).view(-1, 1).repeat(1, data.pos.shape[1]).view(-1)\n        if self._use_category:\n            self.category = data.category\n\n    def forward(self):\n        r""""""\n            Forward pass of the network\n            self.data:\n                x -- Features [B, C, N]\n                pos -- Features [B, N, 3]\n        """"""\n        stack_down = []\n        queue_up = queue.Queue()\n\n        data = self.input\n        stack_down.append(data)\n\n        for i in range(len(self.down_modules) - 1):\n            data = self.down_modules[i](data)\n            stack_down.append(data)\n\n        data = self.down_modules[-1](data)\n        queue_up.put(data)\n\n        assert len(self.inner_modules) == 2, ""For this segmentation model, we except 2 distinct inner""\n        data_inner = self.inner_modules[0](data)\n        data_inner_2 = self.inner_modules[1](stack_down[3])\n\n        for i in range(len(self.up_modules) - 1):\n            data = self.up_modules[i]((queue_up.get(), stack_down.pop()))\n            queue_up.put(data)\n\n        last_feature = torch.cat(\n            [data.x, data_inner.x.repeat(1, 1, data.x.shape[-1]), data_inner_2.x.repeat(1, 1, data.x.shape[-1])], dim=1\n        )\n\n        if self._use_category:\n            cat_one_hot = F.one_hot(self.category, self._num_categories).float().transpose(1, 2)\n            last_feature = torch.cat((last_feature, cat_one_hot), dim=1)\n\n        self.output = self.FC_layer(last_feature).transpose(1, 2).contiguous().view((-1, self._num_classes))\n\n        # Compute loss\n        if self._weight_classes is not None:\n            self._weight_classes = self._weight_classes.to(self.output.device)\n\n        if self.labels is not None:\n            self.loss_seg = F.cross_entropy(self.output, self.labels, weight=self._weight_classes)\n\n        return self.output\n\n    def backward(self):\n        """"""Calculate losses, gradients, and update network weights; called in every training iteration""""""\n        # caculate the intermediate results if necessary; here self.output has been computed during function <forward>\n        # calculate loss given the input and intermediate results\n        self.loss_seg.backward()\n\n\nclass RSConv_MP(Segmentation_MP):\n    """""" Message passing version of RSConv""""""\n'"
torch_points3d/modules/KPConv/__init__.py,0,b'from .blocks import *\nfrom .kernels import *\n'
torch_points3d/modules/KPConv/blocks.py,12,"b'import torch\nimport sys\nfrom torch.nn import Linear as Lin\n\nfrom .kernels import KPConvLayer, KPConvDeformableLayer\nfrom torch_points3d.core.common_modules.base_modules import BaseModule, FastBatchNorm1d\nfrom torch_points3d.core.spatial_ops import RadiusNeighbourFinder\nfrom torch_points3d.core.data_transform import GridSampling3D\nfrom torch_points3d.utils.enums import ConvolutionFormat\nfrom torch_points3d.core.base_conv.message_passing import GlobalBaseModule\nfrom torch_points3d.core.common_modules.base_modules import Identity\nfrom torch_points3d.utils.config import is_list\n\n\nclass SimpleBlock(BaseModule):\n    """"""\n    simple layer with KPConv convolution -> activation -> BN\n    we can perform a stride version (just change the query and the neighbors)\n    """"""\n\n    CONV_TYPE = ConvolutionFormat.PARTIAL_DENSE.value\n    DEFORMABLE_DENSITY = 5.0\n    RIGID_DENSITY = 2.5\n\n    def __init__(\n        self,\n        down_conv_nn=None,\n        grid_size=None,\n        prev_grid_size=None,\n        sigma=1.0,\n        max_num_neighbors=16,\n        activation=torch.nn.LeakyReLU(negative_slope=0.1),\n        bn_momentum=0.02,\n        bn=FastBatchNorm1d,\n        deformable=False,\n        add_one=False,\n        **kwargs,\n    ):\n        super(SimpleBlock, self).__init__()\n        assert len(down_conv_nn) == 2\n        num_inputs, num_outputs = down_conv_nn\n        if deformable:\n            density_parameter = self.DEFORMABLE_DENSITY\n            self.kp_conv = KPConvDeformableLayer(\n                num_inputs, num_outputs, point_influence=prev_grid_size * sigma, add_one=add_one\n            )\n        else:\n            density_parameter = self.RIGID_DENSITY\n            self.kp_conv = KPConvLayer(num_inputs, num_outputs, point_influence=prev_grid_size * sigma, add_one=add_one)\n        search_radius = density_parameter * sigma * prev_grid_size\n        self.neighbour_finder = RadiusNeighbourFinder(search_radius, max_num_neighbors, conv_type=self.CONV_TYPE)\n\n        if bn:\n            self.bn = bn(num_outputs, momentum=bn_momentum)\n        else:\n            self.bn = None\n        self.activation = activation\n\n        is_strided = prev_grid_size != grid_size\n        if is_strided:\n            self.sampler = GridSampling3D(grid_size)\n        else:\n            self.sampler = None\n\n    def forward(self, data, precomputed=None, **kwargs):\n        if not hasattr(data, ""block_idx""):\n            setattr(data, ""block_idx"", 0)\n\n        if precomputed:\n            query_data = precomputed[data.block_idx]\n        else:\n            if self.sampler:\n                query_data = self.sampler(data.clone())\n            else:\n                query_data = data.clone()\n\n        if precomputed:\n            idx_neighboors = query_data.idx_neighboors\n            q_pos = query_data.pos\n        else:\n            q_pos, q_batch = query_data.pos, query_data.batch\n            idx_neighboors = self.neighbour_finder(data.pos, q_pos, batch_x=data.batch, batch_y=q_batch)\n            query_data.idx_neighboors = idx_neighboors\n\n        x = self.kp_conv(q_pos, data.pos, idx_neighboors, data.x,)\n        if self.bn:\n            x = self.bn(x)\n        x = self.activation(x)\n\n        query_data.x = x\n        query_data.block_idx = data.block_idx + 1\n        return query_data\n\n    def extra_repr(self):\n        return ""Nb parameters: {}; {}; {}"".format(self.nb_params, self.sampler, self.neighbour_finder)\n\n\nclass ResnetBBlock(BaseModule):\n    """""" Resnet block with optional bottleneck activated by default\n    Arguments:\n        down_conv_nn (len of 2 or 3) :\n                        sizes of input, intermediate, output.\n                        If length == 2 then intermediate =  num_outputs // 4\n        radius : radius of the conv kernel\n        sigma :\n        density_parameter : density parameter for the kernel\n        max_num_neighbors : maximum number of neighboors for the neighboor search\n        activation : activation function\n        has_bottleneck: wether to use the bottleneck or not\n        bn_momentum\n        bn : batch norm (can be None -> no batch norm)\n        grid_size : size of the grid,\n        prev_grid_size : size of the grid at previous step.\n                        In case of a strided block, this is different than grid_size\n    """"""\n\n    CONV_TYPE = ConvolutionFormat.PARTIAL_DENSE.value\n\n    def __init__(\n        self,\n        down_conv_nn=None,\n        grid_size=None,\n        prev_grid_size=None,\n        sigma=1,\n        max_num_neighbors=16,\n        activation=torch.nn.LeakyReLU(negative_slope=0.1),\n        has_bottleneck=True,\n        bn_momentum=0.02,\n        bn=FastBatchNorm1d,\n        deformable=False,\n        add_one=False,\n        **kwargs,\n    ):\n        super(ResnetBBlock, self).__init__()\n        assert len(down_conv_nn) == 2 or len(down_conv_nn) == 3, ""down_conv_nn should be of size 2 or 3""\n        if len(down_conv_nn) == 2:\n            num_inputs, num_outputs = down_conv_nn\n            d_2 = num_outputs // 4\n        else:\n            num_inputs, d_2, num_outputs = down_conv_nn\n        self.is_strided = prev_grid_size != grid_size\n        self.has_bottleneck = has_bottleneck\n\n        # Main branch\n        if self.has_bottleneck:\n            kp_size = [d_2, d_2]\n        else:\n            kp_size = [num_inputs, num_outputs]\n\n        self.kp_conv = SimpleBlock(\n            down_conv_nn=kp_size,\n            grid_size=grid_size,\n            prev_grid_size=prev_grid_size,\n            sigma=sigma,\n            max_num_neighbors=max_num_neighbors,\n            activation=activation,\n            bn_momentum=bn_momentum,\n            bn=bn,\n            deformable=deformable,\n            add_one=add_one,\n        )\n\n        if self.has_bottleneck:\n            if bn:\n                self.unary_1 = torch.nn.Sequential(\n                    Lin(num_inputs, d_2, bias=False), bn(d_2, momentum=bn_momentum), activation\n                )\n                self.unary_2 = torch.nn.Sequential(\n                    Lin(d_2, num_outputs, bias=False), bn(num_outputs, momentum=bn_momentum), activation\n                )\n            else:\n                self.unary_1 = torch.nn.Sequential(Lin(num_inputs, d_2, bias=False), activation)\n                self.unary_2 = torch.nn.Sequential(Lin(d_2, num_outputs, bias=False), activation)\n\n        # Shortcut\n        if num_inputs != num_outputs:\n            if bn:\n                self.shortcut_op = torch.nn.Sequential(\n                    Lin(num_inputs, num_outputs, bias=False), bn(num_outputs, momentum=bn_momentum)\n                )\n            else:\n                self.shortcut_op = Lin(num_inputs, num_outputs, bias=False)\n        else:\n            self.shortcut_op = torch.nn.Identity()\n\n        # Final activation\n        self.activation = activation\n\n    def forward(self, data, precomputed=None, **kwargs):\n        """"""\n            data: x, pos, batch_idx and idx_neighbour when the neighboors of each point in pos have already been computed\n        """"""\n        # Main branch\n        output = data.clone()\n        shortcut_x = data.x\n        if self.has_bottleneck:\n            output.x = self.unary_1(output.x)\n        output = self.kp_conv(output, precomputed=precomputed)\n        if self.has_bottleneck:\n            output.x = self.unary_2(output.x)\n\n        # Shortcut\n        if self.is_strided:\n            idx_neighboors = output.idx_neighboors\n            shortcut_x = torch.cat([shortcut_x, torch.zeros_like(shortcut_x[:1, :])], axis=0)  # Shadow feature\n            neighborhood_features = shortcut_x[idx_neighboors]\n            shortcut_x = torch.max(neighborhood_features, dim=1, keepdim=False)[0]\n\n        shortcut = self.shortcut_op(shortcut_x)\n        output.x += shortcut\n        return output\n\n    @property\n    def sampler(self):\n        return self.kp_conv.sampler\n\n    @property\n    def neighbour_finder(self):\n        return self.kp_conv.neighbour_finder\n\n    def extra_repr(self):\n        return ""Nb parameters: %i"" % self.nb_params\n\n\nclass KPDualBlock(BaseModule):\n    """""" Dual KPConv block (usually strided + non strided)\n\n    Arguments: Accepted kwargs\n        block_names: Name of the blocks to be used as part of this dual block\n        down_conv_nn: Size of the convs e.g. [64,128],\n        grid_size: Size of the grid for each block,\n        prev_grid_size: Size of the grid in the previous KPConv\n        has_bottleneck: Wether a block should implement the bottleneck\n        max_num_neighbors: Max number of neighboors for the radius search,\n        deformable: Is deformable,\n        add_one: Add one as a feature,\n    """"""\n\n    def __init__(\n        self,\n        block_names=None,\n        down_conv_nn=None,\n        grid_size=None,\n        prev_grid_size=None,\n        has_bottleneck=None,\n        max_num_neighbors=None,\n        deformable=False,\n        add_one=False,\n        **kwargs,\n    ):\n        super(KPDualBlock, self).__init__()\n\n        assert len(block_names) == len(down_conv_nn)\n        self.blocks = torch.nn.ModuleList()\n        for i, class_name in enumerate(block_names):\n            # Constructing extra keyword arguments\n            block_kwargs = {}\n            for key, arg in kwargs.items():\n                block_kwargs[key] = arg[i] if is_list(arg) else arg\n\n            # Building the block\n            kpcls = getattr(sys.modules[__name__], class_name)\n            block = kpcls(\n                down_conv_nn=down_conv_nn[i],\n                grid_size=grid_size[i],\n                prev_grid_size=prev_grid_size[i],\n                has_bottleneck=has_bottleneck[i],\n                max_num_neighbors=max_num_neighbors[i],\n                deformable=deformable[i] if is_list(deformable) else deformable,\n                add_one=add_one[i] if is_list(add_one) else add_one,\n                **block_kwargs,\n            )\n            self.blocks.append(block)\n\n    def forward(self, data, precomputed=None, **kwargs):\n        for block in self.blocks:\n            data = block(data, precomputed=precomputed)\n        return data\n\n    @property\n    def sampler(self):\n        return [b.sampler for b in self.blocks]\n\n    @property\n    def neighbour_finder(self):\n        return [b.neighbour_finder for b in self.blocks]\n\n    def extra_repr(self):\n        return ""Nb parameters: %i"" % self.nb_params\n'"
torch_points3d/modules/KPConv/convolution_ops.py,27,"b'# defining KPConv using torch ops\n# Adaptation of https://github.com/HuguesTHOMAS/KPConv/\n# Adaption from https://github.com/humanpose1/KPConvTorch/\n\nimport torch\n\n\ndef gather(x, idx, method=2):\n    """"""\n    https://github.com/pytorch/pytorch/issues/15245\n    implementation of a custom gather operation for faster backwards.\n    :param x: input with shape [N, D_1, ... D_d]\n    :param idx: indexing with shape [n_1, ..., n_m]\n    :param method: Choice of the method\n    :return: x[idx] with shape [n_1, ..., n_m, D_1, ... D_d]\n    """"""\n    idx[idx == -1] = x.shape[0] - 1  # Shadow point\n    if method == 0:\n        return x[idx]\n    elif method == 1:\n        x = x.unsqueeze(1)\n        x = x.expand((-1, idx.shape[-1], -1))\n        idx = idx.unsqueeze(2)\n        idx = idx.expand((-1, -1, x.shape[-1]))\n        return x.gather(0, idx)\n    elif method == 2:\n        for i, ni in enumerate(idx.size()[1:]):\n            x = x.unsqueeze(i + 1)\n            new_s = list(x.size())\n            new_s[i + 1] = ni\n            x = x.expand(new_s)\n        n = len(idx.size())\n        for i, di in enumerate(x.size()[n:]):\n            idx = idx.unsqueeze(i + n)\n            new_s = list(idx.size())\n            new_s[i + n] = di\n            idx = idx.expand(new_s)\n        return x.gather(0, idx)\n    else:\n        raise ValueError(""Unkown method"")\n\n\ndef radius_gaussian(sq_r, sig, eps=1e-9):\n    """"""\n    Compute a radius gaussian (gaussian of distance)\n    :param sq_r: input radiuses [dn, ..., d1, d0]\n    :param sig: extents of gaussians [d1, d0] or [d0] or float\n    :return: gaussian of sq_r [dn, ..., d1, d0]\n    """"""\n    return torch.exp(-sq_r / (2 * sig ** 2 + eps))\n\n\ndef KPConv_ops(\n    query_points,\n    support_points,\n    neighbors_indices,\n    features,\n    K_points,\n    K_values,\n    KP_extent,\n    KP_influence,\n    aggregation_mode,\n):\n    """"""\n    This function creates a graph of operations to define Kernel Point Convolution in tensorflow. See KPConv function\n    above for a description of each parameter\n    :param query_points: float32[n_points, dim] - input query points (center of neighborhoods)\n    :param support_points: float32[n0_points, dim] - input support points (from which neighbors are taken)\n    :param neighbors_indices: int32[n_points, n_neighbors] - indices of neighbors of each point\n    :param features: float32[n0_points, in_fdim] - input features\n    :param K_values: float32[n_kpoints, in_fdim, out_fdim] - weights of the kernel\n    :param fixed: string in (\'none\', \'center\' or \'verticals\') - fix position of certain kernel points\n    :param KP_extent: float32 - influence radius of each kernel point\n    :param KP_influence: string in (\'constant\', \'linear\', \'gaussian\') - influence function of the kernel points\n    :param aggregation_mode: string in (\'closest\', \'sum\') - whether to sum influences, or only keep the closest\n    :return:                    [n_points, out_fdim]\n    """"""\n\n    # Get variables\n    int(K_points.shape[0])\n\n    # Add a fake point in the last row for shadow neighbors\n    shadow_point = torch.ones_like(support_points[:1, :]) * 1e6\n    support_points = torch.cat([support_points, shadow_point], dim=0)\n\n    # Get neighbor points [n_points, n_neighbors, dim]\n    neighbors = gather(support_points, neighbors_indices)\n\n    # Center every neighborhood\n    neighbors = neighbors - query_points.unsqueeze(1)\n\n    # Get all difference matrices [n_points, n_neighbors, n_kpoints, dim]\n    neighbors.unsqueeze_(2)\n    differences = neighbors - K_points\n\n    # Get the square distances [n_points, n_neighbors, n_kpoints]\n    sq_distances = torch.sum(differences ** 2, dim=3)\n\n    # Get Kernel point influences [n_points, n_kpoints, n_neighbors]\n    if KP_influence == ""constant"":\n        # Every point get an influence of 1.\n        all_weights = torch.ones_like(sq_distances)\n        all_weights = all_weights.transpose(2, 1)\n\n    elif KP_influence == ""linear"":\n        # Influence decrease linearly with the distance, and get to zero when d = KP_extent.\n        all_weights = torch.clamp(1 - torch.sqrt(sq_distances) / KP_extent, min=0.0)\n        all_weights = all_weights.transpose(2, 1)\n\n    elif KP_influence == ""gaussian"":\n        # Influence in gaussian of the distance.\n        sigma = KP_extent * 0.3\n        all_weights = radius_gaussian(sq_distances, sigma)\n        all_weights = all_weights.transpose(2, 1)\n    else:\n        raise ValueError(""Unknown influence function type (config.KP_influence)"")\n\n    # In case of closest mode, only the closest KP can influence each point\n    if aggregation_mode == ""closest"":\n        neighbors_1nn = torch.argmin(sq_distances, dim=-1)\n        all_weights *= torch.transpose(torch.nn.functional.one_hot(neighbors_1nn, K_points.shape[0]), 1, 2)\n\n    elif aggregation_mode != ""sum"":\n        raise ValueError(""Unknown convolution mode. Should be \'closest\' or \'sum\'"")\n\n    features = torch.cat([features, torch.zeros_like(features[:1, :])], dim=0)\n\n    # Get the features of each neighborhood [n_points, n_neighbors, in_fdim]\n    neighborhood_features = gather(features, neighbors_indices)\n\n    # Apply distance weights [n_points, n_kpoints, in_fdim]\n    weighted_features = torch.matmul(all_weights, neighborhood_features)\n\n    # Apply network weights [n_kpoints, n_points, out_fdim]\n    weighted_features = weighted_features.permute(1, 0, 2)\n    kernel_outputs = torch.matmul(weighted_features, K_values)\n\n    # Convolution sum to get [n_points, out_fdim]\n    output_features = torch.sum(kernel_outputs, dim=0)\n\n    return output_features\n\n\ndef KPConv_deform_ops(\n    query_points,\n    support_points,\n    neighbors_indices,\n    features,\n    K_points,\n    offsets,\n    modulations,\n    K_values,\n    KP_extent,\n    KP_influence,\n    aggregation_mode,\n):\n    """"""\n    This function creates a graph of operations to define Deformable Kernel Point Convolution in tensorflow. See\n    KPConv_deformable function above for a description of each parameter\n    :param query_points:        [n_points, dim]\n    :param support_points:      [n0_points, dim]\n    :param neighbors_indices:   [n_points, n_neighbors]\n    :param features:            [n0_points, in_fdim]\n    :param K_points:            [n_kpoints, dim]\n    :param offsets:             [n_points, n_kpoints, dim]\n    :param modulations:         [n_points, n_kpoints] or None\n    :param K_values:            [n_kpoints, in_fdim, out_fdim]\n    :param KP_extent:           float32\n    :param KP_influence:        string\n    :param aggregation_mode:    string in (\'closest\', \'sum\') - whether to sum influences, or only keep the closest\n\n    :return features, square_distances, deformed_K_points\n    """"""\n\n    # Get variables\n    n_kp = int(K_points.shape[0])\n    shadow_ind = support_points.shape[0]\n\n    # Add a fake point in the last row for shadow neighbors\n    shadow_point = torch.ones_like(support_points[:1, :]) * 1e6\n    support_points = torch.cat([support_points, shadow_point], axis=0)\n\n    # Get neighbor points [n_points, n_neighbors, dim]\n    neighbors = support_points[neighbors_indices]\n\n    # Center every neighborhood\n    neighbors = neighbors - query_points.unsqueeze(1)\n\n    # Apply offsets to kernel points [n_points, n_kpoints, dim]\n    deformed_K_points = torch.add(offsets, K_points)\n\n    # Get all difference matrices [n_points, n_neighbors, n_kpoints, dim]\n    neighbors = neighbors.unsqueeze(2)\n    neighbors = neighbors.repeat([1, 1, n_kp, 1])\n    differences = neighbors - deformed_K_points.unsqueeze(1)\n\n    # Get the square distances [n_points, n_neighbors, n_kpoints]\n    sq_distances = torch.sum(differences ** 2, axis=3)\n\n    # Boolean of the neighbors in range of a kernel point [n_points, n_neighbors]\n    in_range = (sq_distances < KP_extent ** 2).any(2).to(torch.long)\n\n    # New value of max neighbors\n    new_max_neighb = torch.max(torch.sum(in_range, axis=1))\n    # print(new_max_neighb)\n\n    # For each row of neighbors, indices of the ones that are in range [n_points, new_max_neighb]\n    new_neighb_bool, new_neighb_inds = torch.topk(in_range, k=new_max_neighb)\n\n    # Gather new neighbor indices [n_points, new_max_neighb]\n    new_neighbors_indices = neighbors_indices.gather(1, new_neighb_inds)\n\n    # Gather new distances to KP [n_points, new_max_neighb, n_kpoints]\n    new_neighb_inds_sq = new_neighb_inds.unsqueeze(-1)\n    new_sq_distances = sq_distances.gather(1, new_neighb_inds_sq.repeat((1, 1, sq_distances.shape[-1])))\n\n    # New shadow neighbors have to point to the last shadow point\n    new_neighbors_indices *= new_neighb_bool\n    new_neighbors_indices += (1 - new_neighb_bool) * shadow_ind\n\n    # Get Kernel point influences [n_points, n_kpoints, n_neighbors]\n    if KP_influence == ""constant"":\n        # Every point get an influence of 1.\n        all_weights = (new_sq_distances < KP_extent ** 2).to(torch.float32)\n        all_weights = all_weights.permute(0, 2, 1)\n\n    elif KP_influence == ""linear"":\n        # Influence decrease linearly with the distance, and get to zero when d = KP_extent.\n        all_weights = torch.relu(1 - torch.sqrt(new_sq_distances) / KP_extent)\n        all_weights = all_weights.permute(0, 2, 1)\n\n    elif KP_influence == ""gaussian"":\n        # Influence in gaussian of the distance.\n        sigma = KP_extent * 0.3\n        all_weights = radius_gaussian(new_sq_distances, sigma)\n        all_weights = all_weights.permute(0, 2, 1)\n    else:\n        raise ValueError(""Unknown influence function type (config.KP_influence)"")\n\n    # In case of closest mode, only the closest KP can influence each point\n    if aggregation_mode == ""closest"":\n        neighbors_1nn = torch.argmin(new_sq_distances, axis=2, output_type=torch.long)\n        all_weights *= torch.zeros_like(all_weights, dtype=torch.float32).scatter_(1, neighbors_1nn, 1)\n\n    elif aggregation_mode != ""sum"":\n        raise ValueError(""Unknown convolution mode. Should be \'closest\' or \'sum\'"")\n\n    features = torch.cat([features, torch.zeros_like(features[:1, :])], axis=0)\n\n    # Get the features of each neighborhood [n_points, new_max_neighb, in_fdim]\n    neighborhood_features = features[new_neighbors_indices]\n\n    # Apply distance weights [n_points, n_kpoints, in_fdim]\n    # print(all_weights.shape, neighborhood_features.shape)\n    weighted_features = torch.matmul(all_weights, neighborhood_features)\n\n    # Apply modulations\n    if modulations is not None:\n        weighted_features *= modulations.unsqueeze(2)\n\n    # Apply network weights [n_kpoints, n_points, out_fdim]\n    weighted_features = weighted_features.permute(1, 0, 2)\n    kernel_outputs = torch.matmul(weighted_features, K_values)\n\n    # Convolution sum [n_points, out_fdim]\n    output_features = torch.sum(kernel_outputs, axis=0)\n\n    # we need regularization\n    return output_features, sq_distances, deformed_K_points\n'"
torch_points3d/modules/KPConv/kernel_utils.py,0,"b'#\n#\n#      0=================================0\n#      |    Kernel Point Convolutions    |\n#      0=================================0\n#\n#\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#      Functions handling the disposition of kernel points.\n#\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#      Hugues THOMAS - 11/06/2018\n#\n\n\n# ------------------------------------------------------------------------------------------\n#\n#          Imports and global variables\n#      \\**********************************/\n#\n\n\n# Import numpy package and name it ""np""\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom os import makedirs\nfrom os.path import join, exists\nimport os\nimport logging\n\nfrom .plyutils import read_ply, write_ply\n\n\n# ------------------------------------------------------------------------------------------\n#\n#           Functions\n#       \\***************/\n#\n#\nlog = logging.getLogger(__name__)\nDIR = os.path.dirname(os.path.realpath(__file__))\n\n\ndef kernel_point_optimization_debug(\n    radius, num_points, num_kernels=1, dimension=3, fixed=""center"", ratio=1.0, verbose=0\n):\n    """"""\n    Creation of kernel point via optimization of potentials.\n    :param radius: Radius of the kernels\n    :param num_points: points composing kernels\n    :param num_kernels: number of wanted kernels\n    :param dimension: dimension of the space\n    :param fixed: fix position of certain kernel points (\'none\', \'center\' or \'verticals\')\n    :param ratio: ratio of the radius where you want the kernels points to be placed\n    :param verbose: display option\n    :return: points [num_kernels, num_points, dimension]\n    """"""\n\n    #######################\n    # Parameters definition\n    #######################\n\n    # Radius used for optimization (points are rescaled afterwards)\n    radius0 = 1\n    diameter0 = 2\n\n    # Factor multiplicating gradients for moving points (~learning rate)\n    moving_factor = 1e-2\n    continuous_moving_decay = 0.9995\n\n    # Gradient threshold to stop optimization\n    thresh = 1e-5\n\n    # Gradient clipping value\n    clip = 0.05 * radius0\n\n    #######################\n    # Kernel initialization\n    #######################\n\n    # Random kernel points\n    kernel_points = np.random.rand(num_kernels * num_points - 1, dimension) * diameter0 - radius0\n    while kernel_points.shape[0] < num_kernels * num_points:\n        new_points = np.random.rand(num_kernels * num_points - 1, dimension) * diameter0 - radius0\n        kernel_points = np.vstack((kernel_points, new_points))\n        d2 = np.sum(np.power(kernel_points, 2), axis=1)\n        kernel_points = kernel_points[d2 < 0.5 * radius0 * radius0, :]\n    kernel_points = kernel_points[: num_kernels * num_points, :].reshape((num_kernels, num_points, -1))\n\n    # Optionnal fixing\n    if fixed == ""center"":\n        kernel_points[:, 0, :] *= 0\n    if fixed == ""verticals"":\n        kernel_points[:, :3, :] *= 0\n        kernel_points[:, 1, -1] += 2 * radius0 / 3\n        kernel_points[:, 2, -1] -= 2 * radius0 / 3\n\n    #####################\n    # Kernel optimization\n    #####################\n\n    # Initiate figure\n    if verbose > 1:\n        fig = plt.figure()\n\n    saved_gradient_norms = np.zeros((10000, num_kernels))\n    old_gradient_norms = np.zeros((num_kernels, num_points))\n    for iter in range(10000):\n\n        # Compute gradients\n        # *****************\n\n        # Derivative of the sum of potentials of all points\n        A = np.expand_dims(kernel_points, axis=2)\n        B = np.expand_dims(kernel_points, axis=1)\n        interd2 = np.sum(np.power(A - B, 2), axis=-1)\n        inter_grads = (A - B) / (np.power(np.expand_dims(interd2, -1), 3 / 2) + 1e-6)\n        inter_grads = np.sum(inter_grads, axis=1)\n\n        # Derivative of the radius potential\n        circle_grads = 10 * kernel_points\n\n        # All gradients\n        gradients = inter_grads + circle_grads\n\n        if fixed == ""verticals"":\n            gradients[:, 1:3, :-1] = 0\n\n        # Stop condition\n        # **************\n\n        # Compute norm of gradients\n        gradients_norms = np.sqrt(np.sum(np.power(gradients, 2), axis=-1))\n        saved_gradient_norms[iter, :] = np.max(gradients_norms, axis=1)\n\n        # Stop if all moving points are gradients fixed (low gradients diff)\n\n        if fixed == ""center"" and np.max(np.abs(old_gradient_norms[:, 1:] - gradients_norms[:, 1:])) < thresh:\n            break\n        elif fixed == ""verticals"" and np.max(np.abs(old_gradient_norms[:, 3:] - gradients_norms[:, 3:])) < thresh:\n            break\n        elif np.max(np.abs(old_gradient_norms - gradients_norms)) < thresh:\n            break\n        old_gradient_norms = gradients_norms\n\n        # Move points\n        # ***********\n\n        # Clip gradient to get moving dists\n        moving_dists = np.minimum(moving_factor * gradients_norms, clip)\n\n        # Fix central point\n        if fixed == ""center"":\n            moving_dists[:, 0] = 0\n        if fixed == ""verticals"":\n            moving_dists[:, 0] = 0\n\n        # Move points\n        kernel_points -= np.expand_dims(moving_dists, -1) * gradients / np.expand_dims(gradients_norms + 1e-6, -1)\n\n        if verbose:\n            log.info(""iter {:5d} / max grad = {:f}"".format(iter, np.max(gradients_norms[:, 3:])))\n        if verbose > 1:\n            plt.clf()\n            plt.plot(kernel_points[0, :, 0], kernel_points[0, :, 1], ""."")\n            circle = plt.Circle((0, 0), radius, color=""r"", fill=False)\n            fig.axes[0].add_artist(circle)\n            fig.axes[0].set_xlim((-radius * 1.1, radius * 1.1))\n            fig.axes[0].set_ylim((-radius * 1.1, radius * 1.1))\n            fig.axes[0].set_aspect(""equal"")\n            plt.draw()\n            plt.pause(0.001)\n            plt.show(block=False)\n            log.info(moving_factor)\n\n        # moving factor decay\n        moving_factor *= continuous_moving_decay\n\n    # Rescale radius to fit the wanted ratio of radius\n    r = np.sqrt(np.sum(np.power(kernel_points, 2), axis=-1))\n    kernel_points *= ratio / np.mean(r[:, 1:])\n\n    # Rescale kernels with real radius\n    return kernel_points * radius, saved_gradient_norms\n\n\ndef load_kernels(radius, num_kpoints, num_kernels, dimension, fixed):\n\n    # Number of tries in the optimization process, to ensure we get the most stable disposition\n    num_tries = 100\n\n    # Kernel directory\n    kernel_dir = join(DIR, ""kernels/dispositions"")\n    if not exists(kernel_dir):\n        makedirs(kernel_dir)\n\n    # Kernel_file\n    if dimension == 3:\n        kernel_file = join(kernel_dir, ""k_{:03d}_{:s}.ply"".format(num_kpoints, fixed))\n    elif dimension == 2:\n        kernel_file = join(kernel_dir, ""k_{:03d}_{:s}_2D.ply"".format(num_kpoints, fixed))\n    else:\n        raise ValueError(""Unsupported dimpension of kernel : "" + str(dimension))\n\n    # Check if already done\n    if not exists(kernel_file):\n\n        # Create kernels\n        kernel_points, grad_norms = kernel_point_optimization_debug(\n            1.0, num_kpoints, num_kernels=num_tries, dimension=dimension, fixed=fixed, verbose=0,\n        )\n\n        # Find best candidate\n        best_k = np.argmin(grad_norms[-1, :])\n\n        # Save points\n        original_kernel = kernel_points[best_k, :, :]\n        write_ply(kernel_file, original_kernel, [""x"", ""y"", ""z""])\n\n    else:\n        data = read_ply(kernel_file)\n        original_kernel = np.vstack((data[""x""], data[""y""], data[""z""])).T\n\n    # N.B. 2D kernels are not supported yet\n    if dimension == 2:\n        return original_kernel\n\n    # Random rotations depending of the fixed points\n    if fixed == ""verticals"":\n\n        # Create random rotations\n        thetas = np.random.rand(num_kernels) * 2 * np.pi\n        c, s = np.cos(thetas), np.sin(thetas)\n        R = np.zeros((num_kernels, 3, 3), dtype=np.float32)\n        R[:, 0, 0] = c\n        R[:, 1, 1] = c\n        R[:, 2, 2] = 1\n        R[:, 0, 1] = s\n        R[:, 1, 0] = -s\n\n        # Scale kernels\n        original_kernel = radius * np.expand_dims(original_kernel, 0)\n\n        # Rotate kernels\n        kernels = np.matmul(original_kernel, R)\n\n    else:\n\n        # Create random rotations\n        u = np.ones((num_kernels, 3))\n        v = np.ones((num_kernels, 3))\n        wrongs = np.abs(np.sum(u * v, axis=1)) > 0.99\n        while np.any(wrongs):\n            new_u = np.random.rand(num_kernels, 3) * 2 - 1\n            new_u = new_u / np.expand_dims(np.linalg.norm(new_u, axis=1) + 1e-9, -1)\n            u[wrongs, :] = new_u[wrongs, :]\n            new_v = np.random.rand(num_kernels, 3) * 2 - 1\n            new_v = new_v / np.expand_dims(np.linalg.norm(new_v, axis=1) + 1e-9, -1)\n            v[wrongs, :] = new_v[wrongs, :]\n            wrongs = np.abs(np.sum(u * v, axis=1)) > 0.99\n\n        # Make v perpendicular to u\n        v -= np.expand_dims(np.sum(u * v, axis=1), -1) * u\n        v = v / np.expand_dims(np.linalg.norm(v, axis=1) + 1e-9, -1)\n\n        # Last rotation vector\n        w = np.cross(u, v)\n        R = np.stack((u, v, w), axis=-1)\n\n        # Scale kernels\n        original_kernel = radius * np.expand_dims(original_kernel, 0)\n\n        # Rotate kernels\n        kernels = np.matmul(original_kernel, R)\n\n        # Add a small noise\n        kernels = kernels\n        kernels = kernels + np.random.normal(scale=radius * 0.01, size=kernels.shape)\n\n    return kernels\n'"
torch_points3d/modules/KPConv/kernels.py,14,"b'import torch\nfrom torch.nn.parameter import Parameter\n\nfrom .kernel_utils import kernel_point_optimization_debug, load_kernels\nfrom .losses import fitting_loss, repulsion_loss, permissive_loss\nfrom .convolution_ops import *\nfrom torch_points3d.models.base_model import BaseInternalLossModule\n\n\ndef add_ones(query_points, x, add_one):\n    if add_one:\n        ones = torch.ones(query_points.shape[0], dtype=torch.float).unsqueeze(-1).to(query_points.device)\n        if x is not None:\n            x = torch.cat([ones.to(x.dtype), x], dim=-1)\n        else:\n            x = ones\n    return x\n\n\nclass KPConvLayer(torch.nn.Module):\n    """"""\n    apply the kernel point convolution on a point cloud\n    NB : it is the original version of KPConv, it is not the message passing version\n    attributes:\n    num_inputs : dimension of the input feature\n    num_outputs : dimension of the output feature\n    point_influence: influence distance of a single point (sigma * grid_size)\n    n_kernel_points=15\n    fixed=""center""\n    KP_influence=""linear""\n    aggregation_mode=""sum""\n    dimension=3\n    """"""\n\n    _INFLUENCE_TO_RADIUS = 1.5\n\n    def __init__(\n        self,\n        num_inputs,\n        num_outputs,\n        point_influence,\n        n_kernel_points=15,\n        fixed=""center"",\n        KP_influence=""linear"",\n        aggregation_mode=""sum"",\n        dimension=3,\n        add_one=False,\n    ):\n        super(KPConvLayer, self).__init__()\n        self.kernel_radius = self._INFLUENCE_TO_RADIUS * point_influence\n        self.point_influence = point_influence\n        self.add_one = add_one\n        self.num_inputs = num_inputs + self.add_one * 1\n        self.num_outputs = num_outputs\n\n        self.KP_influence = KP_influence\n        self.n_kernel_points = n_kernel_points\n        self.aggregation_mode = aggregation_mode\n\n        # Initial kernel extent for this layer\n        K_points_numpy = load_kernels(\n            self.kernel_radius, n_kernel_points, num_kernels=1, dimension=dimension, fixed=fixed,\n        )\n\n        self.K_points = Parameter(\n            torch.from_numpy(K_points_numpy.reshape((n_kernel_points, dimension))).to(torch.float), requires_grad=False,\n        )\n\n        weights = torch.empty([n_kernel_points, self.num_inputs, num_outputs], dtype=torch.float)\n        torch.nn.init.xavier_normal_(weights)\n        self.weight = Parameter(weights)\n\n    def forward(self, query_points, support_points, neighbors, x):\n        """"""\n        - query_points(torch Tensor): query of size N x 3\n        - support_points(torch Tensor): support points of size N0 x 3\n        - neighbors(torch Tensor): neighbors of size N x M\n        - features : feature of size N0 x d (d is the number of inputs)\n        """"""\n        x = add_ones(support_points, x, self.add_one)\n\n        new_feat = KPConv_ops(\n            query_points,\n            support_points,\n            neighbors,\n            x,\n            self.K_points,\n            self.weight,\n            self.point_influence,\n            self.KP_influence,\n            self.aggregation_mode,\n        )\n        return new_feat\n\n    def __repr__(self):\n        return ""KPConvLayer(InF: %i, OutF: %i, kernel_pts: %i, radius: %.2f, KP_influence: %s, Add_one: %s)"" % (\n            self.num_inputs,\n            self.num_outputs,\n            self.n_kernel_points,\n            self.kernel_radius,\n            self.KP_influence,\n            self.add_one,\n        )\n\n\nclass KPConvDeformableLayer(BaseInternalLossModule):\n    """"""\n    apply the deformable kernel point convolution on a point cloud\n    NB : it is the original version of KPConv, it is not the message passing version\n    attributes:\n    num_inputs : dimension of the input feature\n    num_outputs : dimension of the output feature\n    point_influence: influence distance of a single point (sigma * grid_size)\n    n_kernel_points=15\n    fixed=""center""\n    KP_influence=""linear""\n    aggregation_mode=""sum""\n    dimension=3\n    modulated = False :   If deformable conv should be modulated\n    """"""\n\n    PERMISSIVE_LOSS_KEY = ""permissive_loss""\n    FITTING_LOSS_KEY = ""fitting_loss""\n    REPULSION_LOSS_KEY = ""repulsion_loss""\n\n    _INFLUENCE_TO_RADIUS = 1.5\n\n    def __init__(\n        self,\n        num_inputs,\n        num_outputs,\n        point_influence,\n        n_kernel_points=15,\n        fixed=""center"",\n        KP_influence=""linear"",\n        aggregation_mode=""sum"",\n        dimension=3,\n        modulated=False,\n        loss_mode=""fitting"",\n        add_one=False,\n    ):\n        super(KPConvDeformableLayer, self).__init__()\n        self.kernel_radius = self._INFLUENCE_TO_RADIUS * point_influence\n        self.point_influence = point_influence\n        self.add_one = add_one\n        self.num_inputs = num_inputs + self.add_one * 1\n        self.num_outputs = num_outputs\n\n        self.KP_influence = KP_influence\n        self.n_kernel_points = n_kernel_points\n        self.aggregation_mode = aggregation_mode\n        self.modulated = modulated\n        self.internal_losses = {self.PERMISSIVE_LOSS_KEY: 0.0, self.FITTING_LOSS_KEY: 0.0, self.REPULSION_LOSS_KEY: 0.0}\n        self.loss_mode = loss_mode\n\n        # Initial kernel extent for this layer\n        K_points_numpy = load_kernels(\n            self.kernel_radius, n_kernel_points, num_kernels=1, dimension=dimension, fixed=fixed,\n        )\n        self.K_points = Parameter(\n            torch.from_numpy(K_points_numpy.reshape((n_kernel_points, dimension))).to(torch.float), requires_grad=False,\n        )\n\n        # Create independant weight for the first convolution and a bias term as no batch normalization happen\n        if modulated:\n            offset_dim = (dimension + 1) * self.n_kernel_points\n        else:\n            offset_dim = dimension * self.n_kernel_points\n        offset_weights = torch.empty([n_kernel_points, self.num_inputs, offset_dim], dtype=torch.float)\n        torch.nn.init.xavier_normal_(offset_weights)\n        self.offset_weights = Parameter(offset_weights)\n        self.offset_bias = Parameter(torch.zeros(offset_dim, dtype=torch.float))\n\n        # Main deformable weights\n        weights = torch.empty([n_kernel_points, self.num_inputs, num_outputs], dtype=torch.float)\n        torch.nn.init.xavier_normal_(weights)\n        self.weight = Parameter(weights)\n\n    def forward(self, query_points, support_points, neighbors, x):\n        """"""\n        - query_points(torch Tensor): query of size N x 3\n        - support_points(torch Tensor): support points of size N0 x 3\n        - neighbors(torch Tensor): neighbors of size N x M\n        - features : feature of size N0 x d (d is the number of inputs)\n        """"""\n\n        x = add_ones(support_points, x, self.add_one)\n\n        offset_feat = (\n            KPConv_ops(\n                query_points,\n                support_points,\n                neighbors,\n                x,\n                self.K_points,\n                self.offset_weights,\n                self.point_influence,\n                self.KP_influence,\n                self.aggregation_mode,\n            )\n            + self.offset_bias\n        )\n        points_dim = query_points.shape[-1]\n        if self.modulated:\n            # Get offset (in normalized scale) from features\n            offsets = offset_feat[:, : points_dim * self.n_kernel_points]\n            offsets = offsets.reshape((-1, self.n_kernel_points, points_dim))\n\n            # Get modulations\n            modulations = 2 * torch.nn.functional.sigmoid(offset_feat[:, points_dim * self.n_kernel_points :])\n        else:\n            # Get offset (in normalized scale) from features\n            offsets = offset_feat.reshape((-1, self.n_kernel_points, points_dim))\n            # No modulations\n            modulations = None\n        offsets *= self.point_influence\n\n        # Apply deformable kernel\n        new_feat, sq_distances, K_points_deformed = KPConv_deform_ops(\n            query_points,\n            support_points,\n            neighbors,\n            x,\n            self.K_points,\n            offsets,\n            modulations,\n            self.weight,\n            self.point_influence,\n            self.KP_influence,\n            self.aggregation_mode,\n        )\n\n        if self.loss_mode == ""fitting"":\n            self.internal_losses[self.FITTING_LOSS_KEY] = fitting_loss(sq_distances, self.kernel_radius)\n            self.internal_losses[self.REPULSION_LOSS_KEY] = repulsion_loss(K_points_deformed, self.point_influence)\n        elif self.loss_mode == ""permissive"":\n            self.internal_losses[self.PERMISSIVE_LOSS_KEY] = permissive_loss(K_points_deformed, self.kernel_radius)\n        else:\n            raise NotImplementedError(\n                ""Loss mode %s not recognised. Only permissive and fitting are valid"" % self.loss_mode\n            )\n        return new_feat\n\n    def get_internal_losses(self):\n        return self.internal_losses\n\n    def __repr__(self):\n        return ""KPConvDeformableLayer(InF: %i, OutF: %i, kernel_pts: %i, radius: %.2f, KP_influence: %s)"" % (\n            self.num_inputs,\n            self.num_outputs,\n            self.n_kernel_points,\n            self.kernel_radius,\n            self.KP_influence,\n        )\n'"
torch_points3d/modules/KPConv/losses.py,8,"b'import torch\n\n\ndef fitting_loss(sq_distance, radius):\n    """""" KPConv fitting loss. For each query point it ensures that at least one neighboor is\n    close to each kernel point\n\n    Arguments:\n        sq_distance - For each querry point, from all neighboors to all KP points [N_querry, N_neighboors, N_KPoints]\n        radius - Radius of the convolution\n    """"""\n    kpmin = sq_distance.min(dim=1)[0]\n    normalised_kpmin = kpmin / (radius ** 2)\n    return torch.mean(normalised_kpmin)\n\n\ndef repulsion_loss(deformed_kpoints, radius):\n    """""" Ensures that the deformed points within the kernel remain equidistant\n\n    Arguments:\n        deformed_kpoints - deformed points for each query point\n        radius - Radius of the kernel\n    """"""\n    deformed_kpoints / float(radius)\n    n_points = deformed_kpoints.shape[1]\n    repulsive_loss = 0\n    for i in range(n_points):\n        with torch.no_grad():\n            other_points = torch.cat([deformed_kpoints[:, :i, :], deformed_kpoints[:, i + 1 :, :]], dim=1)\n        distances = torch.sqrt(torch.sum((other_points - deformed_kpoints[:, i : i + 1, :]) ** 2, dim=-1))\n        repulsion_force = torch.sum(torch.pow(torch.relu(1.5 - distances), 2), dim=1)\n        repulsive_loss += torch.mean(repulsion_force)\n    return repulsive_loss\n\n\ndef permissive_loss(deformed_kpoints, radius):\n    """"""This loss is responsible to penalize deformed_kpoints to\n    move outside from the radius defined for the convolution\n    """"""\n    norm_deformed_normalized = torch.norm(deformed_kpoints, p=2, dim=-1) / float(radius)\n    permissive_loss = torch.mean(norm_deformed_normalized[norm_deformed_normalized > 1.0])\n    return permissive_loss\n'"
torch_points3d/modules/KPConv/plyutils.py,0,"b'#\n#      0===============================0\n#      |    PLY files reader/writer    |\n#      0===============================0\n#\n#\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#      function to read/write .ply files\n#\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#      Hugues THOMAS - 10/02/2017\n#\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#          Imports and global variables\n#      \\**********************************/\n#\n\n\n# Basic libs\nimport numpy as np\nimport sys\nimport logging\n\nlog = logging.getLogger(__name__)\n\n\n# Define PLY types\nply_dtypes = dict(\n    [\n        (b""int8"", ""i1""),\n        (b""char"", ""i1""),\n        (b""uint8"", ""u1""),\n        (b""uchar"", ""u1""),\n        (b""int16"", ""i2""),\n        (b""short"", ""i2""),\n        (b""uint16"", ""u2""),\n        (b""ushort"", ""u2""),\n        (b""int32"", ""i4""),\n        (b""int"", ""i4""),\n        (b""uint32"", ""u4""),\n        (b""uint"", ""u4""),\n        (b""float32"", ""f4""),\n        (b""float"", ""f4""),\n        (b""float64"", ""f8""),\n        (b""double"", ""f8""),\n    ]\n)\n\n# Numpy reader format\nvalid_formats = {""ascii"": """", ""binary_big_endian"": "">"", ""binary_little_endian"": ""<""}\n\n\n# ----------------------------------------------------------------------------------------------------------------------\n#\n#           Functions\n#       \\***************/\n#\n\n\ndef parse_header(plyfile, ext):\n    # Variables\n    line = []\n    properties = []\n    num_points = None\n\n    while b""end_header"" not in line and line != b"""":\n        line = plyfile.readline()\n\n        if b""element"" in line:\n            line = line.split()\n            num_points = int(line[2])\n\n        elif b""property"" in line:\n            line = line.split()\n            properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))\n\n    return num_points, properties\n\n\ndef parse_mesh_header(plyfile, ext):\n    # Variables\n    line = []\n    vertex_properties = []\n    num_points = None\n    num_faces = None\n    current_element = None\n\n    while b""end_header"" not in line and line != b"""":\n        line = plyfile.readline()\n\n        # Find point element\n        if b""element vertex"" in line:\n            current_element = ""vertex""\n            line = line.split()\n            num_points = int(line[2])\n\n        elif b""element face"" in line:\n            current_element = ""face""\n            line = line.split()\n            num_faces = int(line[2])\n\n        elif b""property"" in line:\n            if current_element == ""vertex"":\n                line = line.split()\n                vertex_properties.append((line[2].decode(), ext + ply_dtypes[line[1]]))\n            elif current_element == ""vertex"":\n                if not line.startswith(""property list uchar int""):\n                    raise ValueError(""Unsupported faces property : "" + line)\n\n    return num_points, num_faces, vertex_properties\n\n\ndef read_ply(filename, triangular_mesh=False):\n    """"""\n    Read "".ply"" files\n    Parameters\n    ----------\n    filename : string\n        the name of the file to read.\n    Returns\n    -------\n    result : array\n        data stored in the file\n    Examples\n    --------\n    Store data in file\n    >>> points = np.random.rand(5, 3)\n    >>> values = np.random.randint(2, size=10)\n    >>> write_ply(\'example.ply\', [points, values], [\'x\', \'y\', \'z\', \'values\'])\n    Read the file\n    >>> data = read_ply(\'example.ply\')\n    >>> values = data[\'values\']\n    array([0, 0, 1, 1, 0])\n\n    >>> points = np.vstack((data[\'x\'], data[\'y\'], data[\'z\'])).T\n    array([[ 0.466  0.595  0.324]\n           [ 0.538  0.407  0.654]\n           [ 0.850  0.018  0.988]\n           [ 0.395  0.394  0.363]\n           [ 0.873  0.996  0.092]])\n    """"""\n\n    with open(filename, ""rb"") as plyfile:\n\n        # Check if the file start with ply\n        if b""ply"" not in plyfile.readline():\n            raise ValueError(""The file does not start whith the word ply"")\n\n        # get binary_little/big or ascii\n        fmt = plyfile.readline().split()[1].decode()\n        if fmt == ""ascii"":\n            raise ValueError(""The file is not binary"")\n\n        # get extension for building the numpy dtypes\n        ext = valid_formats[fmt]\n\n        # PointCloud reader vs mesh reader\n        if triangular_mesh:\n\n            # Parse header\n            num_points, num_faces, properties = parse_mesh_header(plyfile, ext)\n\n            # Get point data\n            vertex_data = np.fromfile(plyfile, dtype=properties, count=num_points)\n\n            # Get face data\n            face_properties = [\n                (""k"", ext + ""u1""),\n                (""v1"", ext + ""i4""),\n                (""v2"", ext + ""i4""),\n                (""v3"", ext + ""i4""),\n            ]\n            faces_data = np.fromfile(plyfile, dtype=face_properties, count=num_faces)\n\n            # Return vertex data and concatenated faces\n            faces = np.vstack((faces_data[""v1""], faces_data[""v2""], faces_data[""v3""])).T\n            data = [vertex_data, faces]\n\n        else:\n\n            # Parse header\n            num_points, properties = parse_header(plyfile, ext)\n\n            # Get data\n            data = np.fromfile(plyfile, dtype=properties, count=num_points)\n\n    return data\n\n\ndef header_properties(field_list, field_names):\n\n    # List of lines to write\n    lines = []\n\n    # First line describing element vertex\n    lines.append(""element vertex %d"" % field_list[0].shape[0])\n\n    # Properties lines\n    i = 0\n    for fields in field_list:\n        for field in fields.T:\n            lines.append(""property %s %s"" % (field.dtype.name, field_names[i]))\n            i += 1\n\n    return lines\n\n\ndef write_ply(filename, field_list, field_names, triangular_faces=None):\n    """"""\n    Write "".ply"" files\n    Parameters\n    ----------\n    filename : string\n        the name of the file to which the data is saved. A \'.ply\' extension will be appended to the\n        file name if it does no already have one.\n    field_list : list, tuple, numpy array\n        the fields to be saved in the ply file. Either a numpy array, a list of numpy arrays or a\n        tuple of numpy arrays. Each 1D numpy array and each column of 2D numpy arrays are considered\n        as one field.\n    field_names : list\n        the name of each fields as a list of strings. Has to be the same length as the number of\n        fields.\n    Examples\n    --------\n    >>> points = np.random.rand(10, 3)\n    >>> write_ply(\'example1.ply\', points, [\'x\', \'y\', \'z\'])\n    >>> values = np.random.randint(2, size=10)\n    >>> write_ply(\'example2.ply\', [points, values], [\'x\', \'y\', \'z\', \'values\'])\n    >>> colors = np.random.randint(255, size=(10,3), dtype=np.uint8)\n    >>> field_names = [\'x\', \'y\', \'z\', \'red\', \'green\', \'blue\', values\']\n    >>> write_ply(\'example3.ply\', [points, colors, values], field_names)\n    """"""\n\n    # Format list input to the right form\n    field_list = list(field_list) if (type(field_list) == list or type(field_list) == tuple) else list((field_list,))\n    for i, field in enumerate(field_list):\n        if field.ndim < 2:\n            field_list[i] = field.reshape(-1, 1)\n        if field.ndim > 2:\n            log.info(""fields have more than 2 dimensions"")\n            return False\n\n    # check all fields have the same number of data\n    n_points = [field.shape[0] for field in field_list]\n    if not np.all(np.equal(n_points, n_points[0])):\n        log.info(""wrong field dimensions"")\n        return False\n\n    # Check if field_names and field_list have same nb of column\n    n_fields = np.sum([field.shape[1] for field in field_list])\n    if n_fields != len(field_names):\n        log.info(""wrong number of field names"")\n        return False\n\n    # Add extension if not there\n    if not filename.endswith("".ply""):\n        filename += "".ply""\n\n    # open in text mode to write the header\n    with open(filename, ""w"") as plyfile:\n\n        # First magical word\n        header = [""ply""]\n\n        # Encoding format\n        header.append(""format binary_"" + sys.byteorder + ""_endian 1.0"")\n\n        # Points properties description\n        header.extend(header_properties(field_list, field_names))\n\n        # Add faces if needded\n        if triangular_faces is not None:\n            header.append(""element face {:d}"".format(triangular_faces.shape[0]))\n            header.append(""property list uchar int vertex_indices"")\n\n        # End of header\n        header.append(""end_header"")\n\n        # Write all lines\n        for line in header:\n            plyfile.write(""%s\\n"" % line)\n\n    # open in binary/append to use tofile\n    with open(filename, ""ab"") as plyfile:\n\n        # Create a structured array\n        i = 0\n        type_list = []\n        for fields in field_list:\n            for field in fields.T:\n                type_list += [(field_names[i], field.dtype.str)]\n                i += 1\n        data = np.empty(field_list[0].shape[0], dtype=type_list)\n        i = 0\n        for fields in field_list:\n            for field in fields.T:\n                data[field_names[i]] = field\n                i += 1\n\n        data.tofile(plyfile)\n\n        if triangular_faces is not None:\n            triangular_faces = triangular_faces.astype(np.int32)\n            type_list = [(""k"", ""uint8"")] + [(str(ind), ""int32"") for ind in range(3)]\n            data = np.empty(triangular_faces.shape[0], dtype=type_list)\n            data[""k""] = np.full((triangular_faces.shape[0],), 3, dtype=np.uint8)\n            data[""0""] = triangular_faces[:, 0]\n            data[""1""] = triangular_faces[:, 1]\n            data[""2""] = triangular_faces[:, 2]\n            data.tofile(plyfile)\n\n    return True\n\n\ndef describe_element(name, df):\n    """""" Takes the columns of the dataframe and builds a ply-like description\n    Parameters\n    ----------\n    name: str\n    df: pandas DataFrame\n    Returns\n    -------\n    element: list[str]\n    """"""\n    property_formats = {""f"": ""float"", ""u"": ""uchar"", ""i"": ""int""}\n    element = [""element "" + name + "" "" + str(len(df))]\n\n    if name == ""face"":\n        element.append(""property list uchar int points_indices"")\n\n    else:\n        for i in range(len(df.columns)):\n            # get first letter of dtype to infer format\n            f = property_formats[str(df.dtypes[i])[0]]\n            element.append(""property "" + f + "" "" + df.columns.values[i])\n\n    return element\n'"
torch_points3d/modules/MinkowskiEngine/__init__.py,0,"b'import sys\n\ntry:\n    from .networks import *\n    from .res16unet import *\n    from .resunet import *\n    from .modules import *\n\n    _custom_models = sys.modules[__name__]\n\n    def initialize_minkowski_unet(\n        model_name, in_channels, out_channels, D, conv1_kernel_size=3, dilations=[1, 1, 1, 1], **kwargs\n    ):\n        net_cls = getattr(_custom_models, model_name)\n        return net_cls(\n            in_channels=in_channels, out_channels=out_channels, D=D, conv1_kernel_size=conv1_kernel_size, **kwargs\n        )\n\n\nexcept:\n    import logging\n\n    log = logging.getLogger(__name__)\n    log.warning(""Could not load Minkowski Engine, please check that it is installed correctly"")\n'"
torch_points3d/modules/MinkowskiEngine/common.py,1,"b'import collections\nfrom enum import Enum\nimport torch.nn as nn\n\nimport MinkowskiEngine as ME\n\n\nclass NormType(Enum):\n    BATCH_NORM = 0\n    INSTANCE_NORM = 1\n    INSTANCE_BATCH_NORM = 2\n\n\ndef get_norm(norm_type, n_channels, D, bn_momentum=0.1):\n    if norm_type == NormType.BATCH_NORM:\n        return ME.MinkowskiBatchNorm(n_channels, momentum=bn_momentum)\n    elif norm_type == NormType.INSTANCE_NORM:\n        return ME.MinkowskiInstanceNorm(n_channels)\n    elif norm_type == NormType.INSTANCE_BATCH_NORM:\n        return nn.Sequential(\n            ME.MinkowskiInstanceNorm(n_channels), ME.MinkowskiBatchNorm(n_channels, momentum=bn_momentum)\n        )\n    else:\n        raise ValueError(f""Norm type: {norm_type} not supported"")\n\n\nclass ConvType(Enum):\n    """"""\n  Define the kernel region type\n  """"""\n\n    HYPERCUBE = 0, ""HYPERCUBE""\n    SPATIAL_HYPERCUBE = 1, ""SPATIAL_HYPERCUBE""\n    SPATIO_TEMPORAL_HYPERCUBE = 2, ""SPATIO_TEMPORAL_HYPERCUBE""\n    HYPERCROSS = 3, ""HYPERCROSS""\n    SPATIAL_HYPERCROSS = 4, ""SPATIAL_HYPERCROSS""\n    SPATIO_TEMPORAL_HYPERCROSS = 5, ""SPATIO_TEMPORAL_HYPERCROSS""\n    SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS = 6, ""SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS ""\n\n    def __new__(cls, value, name):\n        member = object.__new__(cls)\n        member._value_ = value\n        member.fullname = name\n        return member\n\n    def __int__(self):\n        return self.value\n\n\n# Covert the ConvType var to a RegionType var\nconv_to_region_type = {\n    # kernel_size = [k, k, k, 1]\n    ConvType.HYPERCUBE: ME.RegionType.HYPERCUBE,\n    ConvType.SPATIAL_HYPERCUBE: ME.RegionType.HYPERCUBE,\n    ConvType.SPATIO_TEMPORAL_HYPERCUBE: ME.RegionType.HYPERCUBE,\n    ConvType.HYPERCROSS: ME.RegionType.HYPERCROSS,\n    ConvType.SPATIAL_HYPERCROSS: ME.RegionType.HYPERCROSS,\n    ConvType.SPATIO_TEMPORAL_HYPERCROSS: ME.RegionType.HYPERCROSS,\n    ConvType.SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS: ME.RegionType.HYBRID,\n}\n\nint_to_region_type = {m.value: m for m in ME.RegionType}\n\n\ndef convert_region_type(region_type):\n    """"""\n  Convert the integer region_type to the corresponding RegionType enum object.\n  """"""\n    return int_to_region_type[region_type]\n\n\ndef convert_conv_type(conv_type, kernel_size, D):\n    assert isinstance(conv_type, ConvType), ""conv_type must be of ConvType""\n    region_type = conv_to_region_type[conv_type]\n    axis_types = None\n    if conv_type == ConvType.SPATIAL_HYPERCUBE:\n        # No temporal convolution\n        if isinstance(kernel_size, collections.Sequence):\n            kernel_size = kernel_size[:3]\n        else:\n            kernel_size = [kernel_size,] * 3\n        if D == 4:\n            kernel_size.append(1)\n    elif conv_type == ConvType.SPATIO_TEMPORAL_HYPERCUBE:\n        # conv_type conversion already handled\n        assert D == 4\n    elif conv_type == ConvType.HYPERCUBE:\n        # conv_type conversion already handled\n        pass\n    elif conv_type == ConvType.SPATIAL_HYPERCROSS:\n        if isinstance(kernel_size, collections.Sequence):\n            kernel_size = kernel_size[:3]\n        else:\n            kernel_size = [kernel_size,] * 3\n        if D == 4:\n            kernel_size.append(1)\n    elif conv_type == ConvType.HYPERCROSS:\n        # conv_type conversion already handled\n        pass\n    elif conv_type == ConvType.SPATIO_TEMPORAL_HYPERCROSS:\n        # conv_type conversion already handled\n        assert D == 4\n    elif conv_type == ConvType.SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS:\n        # Define the CUBIC conv kernel for spatial dims and CROSS conv for temp dim\n        axis_types = [ME.RegionType.HYPERCUBE,] * 3\n        if D == 4:\n            axis_types.append(ME.RegionType.HYPERCROSS)\n    return region_type, axis_types, kernel_size\n\n\ndef conv(in_planes, out_planes, kernel_size, stride=1, dilation=1, bias=False, conv_type=ConvType.HYPERCUBE, D=-1):\n    assert D > 0, ""Dimension must be a positive integer""\n    region_type, axis_types, kernel_size = convert_conv_type(conv_type, kernel_size, D)\n    kernel_generator = ME.KernelGenerator(\n        kernel_size, stride, dilation, region_type=region_type, axis_types=axis_types, dimension=D\n    )\n\n    return ME.MinkowskiConvolution(\n        in_channels=in_planes,\n        out_channels=out_planes,\n        kernel_size=kernel_size,\n        stride=stride,\n        dilation=dilation,\n        has_bias=bias,\n        kernel_generator=kernel_generator,\n        dimension=D,\n    )\n\n\ndef conv_tr(\n    in_planes, out_planes, kernel_size, upsample_stride=1, dilation=1, bias=False, conv_type=ConvType.HYPERCUBE, D=-1\n):\n    assert D > 0, ""Dimension must be a positive integer""\n    region_type, axis_types, kernel_size = convert_conv_type(conv_type, kernel_size, D)\n    kernel_generator = ME.KernelGenerator(\n        kernel_size, upsample_stride, dilation, region_type=region_type, axis_types=axis_types, dimension=D\n    )\n\n    return ME.MinkowskiConvolutionTranspose(\n        in_channels=in_planes,\n        out_channels=out_planes,\n        kernel_size=kernel_size,\n        stride=upsample_stride,\n        dilation=dilation,\n        has_bias=bias,\n        kernel_generator=kernel_generator,\n        dimension=D,\n    )\n\n\ndef avg_pool(kernel_size, stride=1, dilation=1, conv_type=ConvType.HYPERCUBE, in_coords_key=None, D=-1):\n    assert D > 0, ""Dimension must be a positive integer""\n    region_type, axis_types, kernel_size = convert_conv_type(conv_type, kernel_size, D)\n    kernel_generator = ME.KernelGenerator(\n        kernel_size, stride, dilation, region_type=region_type, axis_types=axis_types, dimension=D\n    )\n\n    return ME.MinkowskiAvgPooling(\n        kernel_size=kernel_size, stride=stride, dilation=dilation, kernel_generator=kernel_generator, dimension=D\n    )\n\n\ndef avg_unpool(kernel_size, stride=1, dilation=1, conv_type=ConvType.HYPERCUBE, D=-1):\n    assert D > 0, ""Dimension must be a positive integer""\n    region_type, axis_types, kernel_size = convert_conv_type(conv_type, kernel_size, D)\n    kernel_generator = ME.KernelGenerator(\n        kernel_size, stride, dilation, region_type=region_type, axis_types=axis_types, dimension=D\n    )\n\n    return ME.MinkowskiAvgUnpooling(\n        kernel_size=kernel_size, stride=stride, dilation=dilation, kernel_generator=kernel_generator, dimension=D\n    )\n\n\ndef sum_pool(kernel_size, stride=1, dilation=1, conv_type=ConvType.HYPERCUBE, D=-1):\n    assert D > 0, ""Dimension must be a positive integer""\n    region_type, axis_types, kernel_size = convert_conv_type(conv_type, kernel_size, D)\n    kernel_generator = ME.KernelGenerator(\n        kernel_size, stride, dilation, region_type=region_type, axis_types=axis_types, dimension=D\n    )\n\n    return ME.MinkowskiSumPooling(\n        kernel_size=kernel_size, stride=stride, dilation=dilation, kernel_generator=kernel_generator, dimension=D\n    )\n'"
torch_points3d/modules/MinkowskiEngine/modules.py,1,"b'import torch.nn as nn\nimport MinkowskiEngine as ME\nfrom .common import ConvType, NormType\n\nfrom torch_points3d.utils.config import is_list\n\n\nclass BasicBlock(nn.Module):\n    """"""This module implements a basic residual convolution block using MinkowskiEngine\n\n    Parameters\n    ----------\n    inplanes: int\n        Input dimension\n    planes: int\n        Output dimension\n    dilation: int\n        Dilation value\n    downsample: nn.Module\n        If provided, downsample will be applied on input before doing residual addition\n    bn_momentum: float\n        Input dimension\n    """"""\n\n    EXPANSION = 1\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, bn_momentum=0.1, dimension=-1):\n        super(BasicBlock, self).__init__()\n        assert dimension > 0\n\n        self.conv1 = ME.MinkowskiConvolution(\n            inplanes, planes, kernel_size=3, stride=stride, dilation=dilation, dimension=dimension\n        )\n        self.norm1 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n        self.conv2 = ME.MinkowskiConvolution(\n            planes, planes, kernel_size=3, stride=1, dilation=dilation, dimension=dimension\n        )\n        self.norm2 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    EXPANSION = 4\n\n    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None, bn_momentum=0.1, dimension=-1):\n        super(Bottleneck, self).__init__()\n        assert dimension > 0\n\n        self.conv1 = ME.MinkowskiConvolution(inplanes, planes, kernel_size=1, dimension=dimension)\n        self.norm1 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n\n        self.conv2 = ME.MinkowskiConvolution(\n            planes, planes, kernel_size=3, stride=stride, dilation=dilation, dimension=dimension\n        )\n        self.norm2 = ME.MinkowskiBatchNorm(planes, momentum=bn_momentum)\n\n        self.conv3 = ME.MinkowskiConvolution(planes, planes * self.EXPANSION, kernel_size=1, dimension=dimension)\n        self.norm3 = ME.MinkowskiBatchNorm(planes * self.EXPANSION, momentum=bn_momentum)\n\n        self.relu = ME.MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.norm3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BaseResBlock(nn.Module):\n    def __init__(\n        self,\n        feat_in,\n        feat_mid,\n        feat_out,\n        kernel_sizes=[],\n        strides=[],\n        dilations=[],\n        has_biases=[],\n        kernel_generators=[],\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        has_bias=False,\n        kernel_generator=None,\n        norm_layer=ME.MinkowskiBatchNorm,\n        activation=ME.MinkowskiReLU,\n        bn_momentum=0.1,\n        dimension=-1,\n        **kwargs\n    ):\n\n        super(BaseResBlock, self).__init__()\n        assert dimension > 0\n\n        modules = []\n\n        convolutions_dim = [[feat_in, feat_mid], [feat_mid, feat_mid], [feat_mid, feat_out]]\n\n        kernel_sizes = self.create_arguments_list(kernel_sizes, kernel_size)\n        strides = self.create_arguments_list(strides, stride)\n        dilations = self.create_arguments_list(dilations, dilation)\n        has_biases = self.create_arguments_list(has_biases, has_bias)\n        kernel_generators = self.create_arguments_list(kernel_generators, kernel_generator)\n\n        for conv_dim, kernel_size, stride, dilation, has_bias, kernel_generator in zip(\n            convolutions_dim, kernel_sizes, strides, dilations, has_biases, kernel_generators\n        ):\n\n            modules.append(\n                ME.MinkowskiConvolution(\n                    conv_dim[0],\n                    conv_dim[1],\n                    kernel_size=kernel_size,\n                    stride=stride,\n                    dilation=dilation,\n                    has_bias=has_bias,\n                    kernel_generator=kernel_generator,\n                    dimension=dimension,\n                )\n            )\n\n            if norm_layer:\n                modules.append(norm_layer(conv_dim[1], momentum=bn_momentum))\n\n            if activation:\n                modules.append(activation(inplace=True))\n\n        self.conv = nn.Sequential(*modules)\n\n    @staticmethod\n    def create_arguments_list(arg_list, arg):\n        if len(arg_list) == 3:\n            return arg_list\n        return [arg for _ in range(3)]\n\n    def forward(self, x):\n        return x, self.conv(x)\n\n\nclass ResnetBlockDown(BaseResBlock):\n    def __init__(\n        self,\n        down_conv_nn=[],\n        kernel_sizes=[],\n        strides=[],\n        dilations=[],\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        norm_layer=ME.MinkowskiBatchNorm,\n        activation=ME.MinkowskiReLU,\n        bn_momentum=0.1,\n        dimension=-1,\n        down_stride=2,\n        **kwargs\n    ):\n\n        super(ResnetBlockDown, self).__init__(\n            down_conv_nn[0],\n            down_conv_nn[1],\n            down_conv_nn[2],\n            kernel_sizes=kernel_sizes,\n            strides=strides,\n            dilations=dilations,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            norm_layer=norm_layer,\n            activation=activation,\n            bn_momentum=bn_momentum,\n            dimension=dimension,\n        )\n\n        self.downsample = nn.Sequential(\n            ME.MinkowskiConvolution(\n                down_conv_nn[0], down_conv_nn[2], kernel_size=2, stride=down_stride, dimension=dimension\n            ),\n            ME.MinkowskiBatchNorm(down_conv_nn[2]),\n        )\n\n    def forward(self, x):\n\n        residual, x = super().forward(x)\n\n        return self.downsample(residual) + x\n\n\nclass ResnetBlockUp(BaseResBlock):\n    def __init__(\n        self,\n        up_conv_nn=[],\n        kernel_sizes=[],\n        strides=[],\n        dilations=[],\n        kernel_size=3,\n        stride=1,\n        dilation=1,\n        norm_layer=ME.MinkowskiBatchNorm,\n        activation=ME.MinkowskiReLU,\n        bn_momentum=0.1,\n        dimension=-1,\n        up_stride=2,\n        skip=True,\n        **kwargs\n    ):\n\n        self.skip = skip\n\n        super(ResnetBlockUp, self).__init__(\n            up_conv_nn[0],\n            up_conv_nn[1],\n            up_conv_nn[2],\n            kernel_sizes=kernel_sizes,\n            strides=strides,\n            dilations=dilations,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            norm_layer=norm_layer,\n            activation=activation,\n            bn_momentum=bn_momentum,\n            dimension=dimension,\n        )\n\n        self.upsample = ME.MinkowskiConvolutionTranspose(\n            up_conv_nn[0], up_conv_nn[2], kernel_size=2, stride=up_stride, dimension=dimension\n        )\n\n    def forward(self, x, x_skip):\n        residual, x = super().forward(x)\n\n        x = self.upsample(residual) + x\n\n        if self.skip:\n            return ME.cat(x, x_skip)\n        else:\n            return x\n\n\nclass SELayer(nn.Module):\n    def __init__(self, channel, reduction=16, D=-1):\n        # Global coords does not require coords_key\n        super(SELayer, self).__init__()\n        self.fc = nn.Sequential(\n            ME.MinkowskiLinear(channel, channel // reduction),\n            ME.MinkowskiReLU(inplace=True),\n            ME.MinkowskiLinear(channel // reduction, channel),\n            ME.MinkowskiSigmoid(),\n        )\n        self.pooling = ME.MinkowskiGlobalPooling(dimension=D)\n        self.broadcast_mul = ME.MinkowskiBroadcastMultiplication(dimension=D)\n\n    def forward(self, x):\n        y = self.pooling(x)\n        y = self.fc(y)\n        return self.broadcast_mul(x, y)\n\n\nclass SEBasicBlock(BasicBlock):\n    def __init__(\n        self, inplanes, planes, stride=1, dilation=1, downsample=None, conv_type=ConvType.HYPERCUBE, reduction=16, D=-1\n    ):\n        super(SEBasicBlock, self).__init__(\n            inplanes, planes, stride=stride, dilation=dilation, downsample=downsample, conv_type=conv_type, D=D\n        )\n        self.se = SELayer(planes, reduction=reduction, D=D)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBasicBlockBN(SEBasicBlock):\n    NORM_TYPE = NormType.BATCH_NORM\n\n\nclass SEBasicBlockIN(SEBasicBlock):\n    NORM_TYPE = NormType.INSTANCE_NORM\n\n\nclass SEBasicBlockIBN(SEBasicBlock):\n    NORM_TYPE = NormType.INSTANCE_BATCH_NORM\n\n\nclass SEBottleneck(Bottleneck):\n    def __init__(\n        self, inplanes, planes, stride=1, dilation=1, downsample=None, conv_type=ConvType.HYPERCUBE, D=3, reduction=16\n    ):\n        super(SEBottleneck, self).__init__(\n            inplanes, planes, stride=stride, dilation=dilation, downsample=downsample, conv_type=conv_type, D=D\n        )\n        self.se = SELayer(planes * self.expansion, reduction=reduction, D=D)\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.norm3(out)\n        out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneckBN(SEBottleneck):\n    NORM_TYPE = NormType.BATCH_NORM\n\n\nclass SEBottleneckIN(SEBottleneck):\n    NORM_TYPE = NormType.INSTANCE_NORM\n\n\nclass SEBottleneckIBN(SEBottleneck):\n    NORM_TYPE = NormType.INSTANCE_BATCH_NORM\n'"
torch_points3d/modules/MinkowskiEngine/networks.py,1,"b'import torch.nn as nn\n\nimport MinkowskiEngine as ME\nfrom .modules import BasicBlock, Bottleneck\n\n\nclass ResNetBase(nn.Module):\n    BLOCK = None\n    LAYERS = ()\n    INIT_DIM = 64\n    PLANES = (64, 128, 256, 512)\n\n    def __init__(self, in_channels, out_channels, D=3, **kwargs):\n        nn.Module.__init__(self)\n        self.D = D\n        assert self.BLOCK is not None, ""BLOCK is not defined""\n        assert self.PLANES is not None, ""PLANES is not defined""\n        self.network_initialization(in_channels, out_channels, D)\n        self.weight_initialization()\n\n    def network_initialization(self, in_channels, out_channels, D):\n\n        self.inplanes = self.INIT_DIM\n        self.conv1 = ME.MinkowskiConvolution(in_channels, self.inplanes, kernel_size=5, stride=2, dimension=D)\n\n        self.bn1 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n        self.pool = ME.MinkowskiAvgPooling(kernel_size=2, stride=2, dimension=D)\n\n        self.layer1 = self._make_layer(self.BLOCK, self.PLANES[0], self.LAYERS[0], stride=2)\n        self.layer2 = self._make_layer(self.BLOCK, self.PLANES[1], self.LAYERS[1], stride=2)\n        self.layer3 = self._make_layer(self.BLOCK, self.PLANES[2], self.LAYERS[2], stride=2)\n        self.layer4 = self._make_layer(self.BLOCK, self.PLANES[3], self.LAYERS[3], stride=2)\n\n        self.conv5 = ME.MinkowskiConvolution(self.inplanes, self.inplanes, kernel_size=3, stride=3, dimension=D)\n        self.bn5 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.glob_avg = ME.MinkowskiGlobalMaxPooling(dimension=D)\n\n        self.final = ME.MinkowskiLinear(self.inplanes, out_channels, bias=True)\n\n    def weight_initialization(self):\n        for m in self.modules():\n            if isinstance(m, ME.MinkowskiConvolution):\n                ME.utils.kaiming_normal_(m.kernel, mode=""fan_out"", nonlinearity=""relu"")\n\n            if isinstance(m, ME.MinkowskiBatchNorm):\n                nn.init.constant_(m.bn.weight, 1)\n                nn.init.constant_(m.bn.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, bn_momentum=0.1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.EXPANSION:\n            downsample = nn.Sequential(\n                ME.MinkowskiConvolution(\n                    self.inplanes, planes * block.EXPANSION, kernel_size=1, stride=stride, dimension=self.D\n                ),\n                ME.MinkowskiBatchNorm(planes * block.EXPANSION),\n            )\n        layers = []\n        layers.append(\n            block(self.inplanes, planes, stride=stride, dilation=dilation, downsample=downsample, dimension=self.D)\n        )\n        self.inplanes = planes * block.EXPANSION\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, stride=1, dilation=dilation, dimension=self.D))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.conv5(x)\n        x = self.bn5(x)\n        x = self.relu(x)\n\n        x = self.glob_avg(x)\n        return self.final(x)\n\n\nclass ResNet14(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (1, 1, 1, 1)\n\n\nclass ResNet18(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 2, 2, 2)\n\n\nclass ResNet34(ResNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (3, 4, 6, 3)\n\n\nclass ResNet50(ResNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (3, 4, 6, 3)\n\n\nclass ResNet101(ResNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (3, 4, 23, 3)\n\n\nclass MinkUNetBase(ResNetBase):\n    BLOCK = None\n    PLANES = None\n    DILATIONS = (1, 1, 1, 1, 1, 1, 1, 1)\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n    INIT_DIM = 32\n    OUT_TENSOR_STRIDE = 1\n\n    # To use the model, must call initialize_coords before forward pass.\n    # Once data is processed, call clear to reset the model before calling\n    # initialize_coords\n    def __init__(self, in_channels, out_channels, D=3, **kwargs):\n        ResNetBase.__init__(self, in_channels, out_channels, D)\n\n    def network_initialization(self, in_channels, out_channels, D):\n        # Output of the first conv concated to conv6\n        self.inplanes = self.INIT_DIM\n        self.conv0p1s1 = ME.MinkowskiConvolution(in_channels, self.inplanes, kernel_size=5, dimension=D)\n\n        self.bn0 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.conv1p1s2 = ME.MinkowskiConvolution(self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn1 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.block1 = self._make_layer(self.BLOCK, self.PLANES[0], self.LAYERS[0])\n\n        self.conv2p2s2 = ME.MinkowskiConvolution(self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn2 = ME.MinkowskiBatchNorm(self.inplanes)\n\n        self.block2 = self._make_layer(self.BLOCK, self.PLANES[1], self.LAYERS[1])\n\n        self.conv3p4s2 = ME.MinkowskiConvolution(self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n\n        self.bn3 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.block3 = self._make_layer(self.BLOCK, self.PLANES[2], self.LAYERS[2])\n\n        self.conv4p8s2 = ME.MinkowskiConvolution(self.inplanes, self.inplanes, kernel_size=2, stride=2, dimension=D)\n        self.bn4 = ME.MinkowskiBatchNorm(self.inplanes)\n        self.block4 = self._make_layer(self.BLOCK, self.PLANES[3], self.LAYERS[3])\n\n        self.convtr4p16s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[4], kernel_size=2, stride=2, dimension=D\n        )\n        self.bntr4 = ME.MinkowskiBatchNorm(self.PLANES[4])\n\n        self.inplanes = self.PLANES[4] + self.PLANES[2] * self.BLOCK.EXPANSION\n        self.block5 = self._make_layer(self.BLOCK, self.PLANES[4], self.LAYERS[4])\n        self.convtr5p8s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[5], kernel_size=2, stride=2, dimension=D\n        )\n        self.bntr5 = ME.MinkowskiBatchNorm(self.PLANES[5])\n\n        self.inplanes = self.PLANES[5] + self.PLANES[1] * self.BLOCK.EXPANSION\n        self.block6 = self._make_layer(self.BLOCK, self.PLANES[5], self.LAYERS[5])\n        self.convtr6p4s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[6], kernel_size=2, stride=2, dimension=D\n        )\n        self.bntr6 = ME.MinkowskiBatchNorm(self.PLANES[6])\n\n        self.inplanes = self.PLANES[6] + self.PLANES[0] * self.BLOCK.EXPANSION\n        self.block7 = self._make_layer(self.BLOCK, self.PLANES[6], self.LAYERS[6])\n        self.convtr7p2s2 = ME.MinkowskiConvolutionTranspose(\n            self.inplanes, self.PLANES[7], kernel_size=2, stride=2, dimension=D\n        )\n        self.bntr7 = ME.MinkowskiBatchNorm(self.PLANES[7])\n\n        self.inplanes = self.PLANES[7] + self.INIT_DIM\n        self.block8 = self._make_layer(self.BLOCK, self.PLANES[7], self.LAYERS[7])\n\n        self.final = ME.MinkowskiConvolution(self.PLANES[7], out_channels, kernel_size=1, has_bias=True, dimension=D)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv0p1s1(x)\n        out = self.bn0(out)\n        out_p1 = self.relu(out)\n\n        out = self.conv1p1s2(out_p1)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out_b1p2 = self.block1(out)\n\n        out = self.conv2p2s2(out_b1p2)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out_b2p4 = self.block2(out)\n\n        out = self.conv3p4s2(out_b2p4)\n        out = self.bn3(out)\n        out = self.relu(out)\n        out_b3p8 = self.block3(out)\n\n        # tensor_stride=16\n        out = self.conv4p8s2(out_b3p8)\n        out = self.bn4(out)\n        out = self.relu(out)\n        out = self.block4(out)\n\n        # tensor_stride=8\n        out = self.convtr4p16s2(out)\n        out = self.bntr4(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b3p8)\n        out = self.block5(out)\n\n        # tensor_stride=4\n        out = self.convtr5p8s2(out)\n        out = self.bntr5(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b2p4)\n        out = self.block6(out)\n\n        # tensor_stride=2\n        out = self.convtr6p4s2(out)\n        out = self.bntr6(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_b1p2)\n        out = self.block7(out)\n\n        # tensor_stride=1\n        out = self.convtr7p2s2(out)\n        out = self.bntr7(out)\n        out = self.relu(out)\n\n        out = ME.cat(out, out_p1)\n        out = self.block8(out)\n\n        return self.final(out)\n\n\nclass MinkUNet14(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (1, 1, 1, 1, 1, 1, 1, 1)\n\n\nclass MinkUNet18(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n\n\nclass MinkUNet34(MinkUNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass MinkUNet50(MinkUNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass MinkUNet101(MinkUNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 23, 2, 2, 2, 2)\n\n\nclass MinkUNet14A(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass MinkUNet14B(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass MinkUNet14C(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 192, 192, 128, 128)\n\n\nclass MinkUNet14D(MinkUNet14):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass MinkUNet18A(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass MinkUNet18B(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass MinkUNet18D(MinkUNet18):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass MinkUNet34A(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 64)\n\n\nclass MinkUNet34B(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 32)\n\n\nclass MinkUNet34C(MinkUNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 96, 96)\n'"
torch_points3d/modules/MinkowskiEngine/res16unet.py,1,"b'import torch.nn as nn\nimport MinkowskiEngine as ME\nfrom MinkowskiEngine import MinkowskiNetwork\nfrom MinkowskiEngine import MinkowskiReLU\nimport MinkowskiEngine.MinkowskiOps as me\n\nfrom .common import ConvType, NormType, conv, conv_tr, get_norm, sum_pool\n\n\nclass BasicBlockBase(nn.Module):\n    expansion = 1\n    NORM_TYPE = NormType.BATCH_NORM\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        dilation=1,\n        downsample=None,\n        conv_type=ConvType.HYPERCUBE,\n        bn_momentum=0.1,\n        D=3,\n    ):\n        super(BasicBlockBase, self).__init__()\n\n        self.conv1 = conv(inplanes, planes, kernel_size=3, stride=stride, dilation=dilation, conv_type=conv_type, D=D)\n        self.norm1 = get_norm(self.NORM_TYPE, planes, D, bn_momentum=bn_momentum)\n        self.conv2 = conv(\n            planes, planes, kernel_size=3, stride=1, dilation=dilation, bias=False, conv_type=conv_type, D=D\n        )\n        self.norm2 = get_norm(self.NORM_TYPE, planes, D, bn_momentum=bn_momentum)\n        self.relu = MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass BasicBlock(BasicBlockBase):\n    NORM_TYPE = NormType.BATCH_NORM\n\n\nclass BasicBlockIN(BasicBlockBase):\n    NORM_TYPE = NormType.INSTANCE_NORM\n\n\nclass BasicBlockINBN(BasicBlockBase):\n    NORM_TYPE = NormType.INSTANCE_BATCH_NORM\n\n\nclass BottleneckBase(nn.Module):\n    expansion = 4\n    NORM_TYPE = NormType.BATCH_NORM\n\n    def __init__(\n        self,\n        inplanes,\n        planes,\n        stride=1,\n        dilation=1,\n        downsample=None,\n        conv_type=ConvType.HYPERCUBE,\n        bn_momentum=0.1,\n        D=3,\n    ):\n        super(BottleneckBase, self).__init__()\n        self.conv1 = conv(inplanes, planes, kernel_size=1, D=D)\n        self.norm1 = get_norm(self.NORM_TYPE, planes, D, bn_momentum=bn_momentum)\n\n        self.conv2 = conv(planes, planes, kernel_size=3, stride=stride, dilation=dilation, conv_type=conv_type, D=D)\n        self.norm2 = get_norm(self.NORM_TYPE, planes, D, bn_momentum=bn_momentum)\n\n        self.conv3 = conv(planes, planes * self.expansion, kernel_size=1, D=D)\n        self.norm3 = get_norm(self.NORM_TYPE, planes * self.expansion, D, bn_momentum=bn_momentum)\n\n        self.relu = MinkowskiReLU(inplace=True)\n        self.downsample = downsample\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.norm1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.norm2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.norm3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass Bottleneck(BottleneckBase):\n    NORM_TYPE = NormType.BATCH_NORM\n\n\nclass BottleneckIN(BottleneckBase):\n    NORM_TYPE = NormType.INSTANCE_NORM\n\n\nclass BottleneckINBN(BottleneckBase):\n    NORM_TYPE = NormType.INSTANCE_BATCH_NORM\n\n\nclass ResNetBase(MinkowskiNetwork):\n    BLOCK = None\n    LAYERS = ()\n    INIT_DIM = 64\n    PLANES = (64, 128, 256, 512)\n    OUT_PIXEL_DIST = 32\n    HAS_LAST_BLOCK = False\n    CONV_TYPE = ConvType.HYPERCUBE\n\n    def __init__(self, in_channels, out_channels, D, conv1_kernel_size=3, dilations=[1, 1, 1, 1], **kwargs):\n        super(ResNetBase, self).__init__(D)\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.conv1_kernel_size = conv1_kernel_size\n        self.dilations = dilations\n        assert self.BLOCK is not None\n        assert self.OUT_PIXEL_DIST > 0\n\n        self.network_initialization(in_channels, out_channels, D)\n        self.weight_initialization()\n\n    def network_initialization(self, in_channels, out_channels, D):\n        def space_n_time_m(n, m):\n            return n if D == 3 else [n, n, n, m]\n\n        if D == 4:\n            self.OUT_PIXEL_DIST = space_n_time_m(self.OUT_PIXEL_DIST, 1)\n\n        dilations = self.dilations\n        bn_momentum = 1\n        self.inplanes = self.INIT_DIM\n        self.conv1 = conv(\n            in_channels, self.inplanes, kernel_size=space_n_time_m(self.conv1_kernel_size, 1), stride=1, D=D\n        )\n\n        self.bn1 = get_norm(NormType.BATCH_NORM, self.inplanes, D=self.D, bn_momentum=bn_momentum)\n        self.relu = ME.MinkowskiReLU(inplace=True)\n        self.pool = sum_pool(kernel_size=space_n_time_m(2, 1), stride=space_n_time_m(2, 1), D=D)\n\n        self.layer1 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[0],\n            self.LAYERS[0],\n            stride=space_n_time_m(2, 1),\n            dilation=space_n_time_m(dilations[0], 1),\n        )\n        self.layer2 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[1],\n            self.LAYERS[1],\n            stride=space_n_time_m(2, 1),\n            dilation=space_n_time_m(dilations[1], 1),\n        )\n        self.layer3 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[2],\n            self.LAYERS[2],\n            stride=space_n_time_m(2, 1),\n            dilation=space_n_time_m(dilations[2], 1),\n        )\n        self.layer4 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[3],\n            self.LAYERS[3],\n            stride=space_n_time_m(2, 1),\n            dilation=space_n_time_m(dilations[3], 1),\n        )\n\n        self.final = conv(self.PLANES[3] * self.BLOCK.expansion, out_channels, kernel_size=1, bias=True, D=D)\n\n    def weight_initialization(self):\n        for m in self.modules():\n            if isinstance(m, ME.MinkowskiBatchNorm):\n                nn.init.constant_(m.bn.weight, 1)\n                nn.init.constant_(m.bn.bias, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1, dilation=1, norm_type=NormType.BATCH_NORM, bn_momentum=0.1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False, D=self.D),\n                get_norm(norm_type, planes * block.expansion, D=self.D, bn_momentum=bn_momentum),\n            )\n        layers = []\n        layers.append(\n            block(\n                self.inplanes,\n                planes,\n                stride=stride,\n                dilation=dilation,\n                downsample=downsample,\n                conv_type=self.CONV_TYPE,\n                D=self.D,\n            )\n        )\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, stride=1, dilation=dilation, conv_type=self.CONV_TYPE, D=self.D))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.pool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n\n        x = self.final(x)\n        return x\n\n\nclass Res16UNetBase(ResNetBase):\n    BLOCK = None\n    PLANES = (32, 64, 128, 256, 256, 256, 256, 256)\n    DILATIONS = (1, 1, 1, 1, 1, 1, 1, 1)\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n    INIT_DIM = 32\n    OUT_PIXEL_DIST = 1\n    NORM_TYPE = NormType.BATCH_NORM\n    NON_BLOCK_CONV_TYPE = ConvType.SPATIAL_HYPERCUBE\n    CONV_TYPE = ConvType.SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS\n\n    # To use the model, must call initialize_coords before forward pass.\n    # Once data is processed, call clear to reset the model before calling initialize_coords\n    def __init__(self, in_channels, out_channels, D=3, conv1_kernel_size=3, **kwargs):\n        super(Res16UNetBase, self).__init__(in_channels, out_channels, D, conv1_kernel_size)\n\n    def network_initialization(self, in_channels, out_channels, D):\n        # Setup net_metadata\n        dilations = self.DILATIONS\n        bn_momentum = 0.02\n\n        def space_n_time_m(n, m):\n            return n if D == 3 else [n, n, n, m]\n\n        if D == 4:\n            self.OUT_PIXEL_DIST = space_n_time_m(self.OUT_PIXEL_DIST, 1)\n\n        # Output of the first conv concated to conv6\n        self.inplanes = self.INIT_DIM\n        self.conv0p1s1 = conv(\n            in_channels,\n            self.inplanes,\n            kernel_size=space_n_time_m(3, 1),\n            stride=1,\n            dilation=1,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n\n        self.bn0 = get_norm(self.NORM_TYPE, self.inplanes, D, bn_momentum=bn_momentum)\n\n        self.conv1p1s2 = conv(\n            self.inplanes,\n            self.inplanes,\n            kernel_size=space_n_time_m(2, 1),\n            stride=space_n_time_m(2, 1),\n            dilation=1,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bn1 = get_norm(self.NORM_TYPE, self.inplanes, D, bn_momentum=bn_momentum)\n        self.block1 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[0],\n            self.LAYERS[0],\n            dilation=dilations[0],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n\n        self.conv2p2s2 = conv(\n            self.inplanes,\n            self.inplanes,\n            kernel_size=space_n_time_m(2, 1),\n            stride=space_n_time_m(2, 1),\n            dilation=1,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bn2 = get_norm(self.NORM_TYPE, self.inplanes, D, bn_momentum=bn_momentum)\n        self.block2 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[1],\n            self.LAYERS[1],\n            dilation=dilations[1],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n\n        self.conv3p4s2 = conv(\n            self.inplanes,\n            self.inplanes,\n            kernel_size=space_n_time_m(2, 1),\n            stride=space_n_time_m(2, 1),\n            dilation=1,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bn3 = get_norm(self.NORM_TYPE, self.inplanes, D, bn_momentum=bn_momentum)\n        self.block3 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[2],\n            self.LAYERS[2],\n            dilation=dilations[2],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n\n        self.conv4p8s2 = conv(\n            self.inplanes,\n            self.inplanes,\n            kernel_size=space_n_time_m(2, 1),\n            stride=space_n_time_m(2, 1),\n            dilation=1,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bn4 = get_norm(self.NORM_TYPE, self.inplanes, D, bn_momentum=bn_momentum)\n        self.block4 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[3],\n            self.LAYERS[3],\n            dilation=dilations[3],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n        self.convtr4p16s2 = conv_tr(\n            self.inplanes,\n            self.PLANES[4],\n            kernel_size=space_n_time_m(2, 1),\n            upsample_stride=space_n_time_m(2, 1),\n            dilation=1,\n            bias=False,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bntr4 = get_norm(self.NORM_TYPE, self.PLANES[4], D, bn_momentum=bn_momentum)\n\n        self.inplanes = self.PLANES[4] + self.PLANES[2] * self.BLOCK.expansion\n        self.block5 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[4],\n            self.LAYERS[4],\n            dilation=dilations[4],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n        self.convtr5p8s2 = conv_tr(\n            self.inplanes,\n            self.PLANES[5],\n            kernel_size=space_n_time_m(2, 1),\n            upsample_stride=space_n_time_m(2, 1),\n            dilation=1,\n            bias=False,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bntr5 = get_norm(self.NORM_TYPE, self.PLANES[5], D, bn_momentum=bn_momentum)\n\n        self.inplanes = self.PLANES[5] + self.PLANES[1] * self.BLOCK.expansion\n        self.block6 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[5],\n            self.LAYERS[5],\n            dilation=dilations[5],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n        self.convtr6p4s2 = conv_tr(\n            self.inplanes,\n            self.PLANES[6],\n            kernel_size=space_n_time_m(2, 1),\n            upsample_stride=space_n_time_m(2, 1),\n            dilation=1,\n            bias=False,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bntr6 = get_norm(self.NORM_TYPE, self.PLANES[6], D, bn_momentum=bn_momentum)\n\n        self.inplanes = self.PLANES[6] + self.PLANES[0] * self.BLOCK.expansion\n        self.block7 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[6],\n            self.LAYERS[6],\n            dilation=dilations[6],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n        self.convtr7p2s2 = conv_tr(\n            self.inplanes,\n            self.PLANES[7],\n            kernel_size=space_n_time_m(2, 1),\n            upsample_stride=space_n_time_m(2, 1),\n            dilation=1,\n            bias=False,\n            conv_type=self.NON_BLOCK_CONV_TYPE,\n            D=D,\n        )\n        self.bntr7 = get_norm(self.NORM_TYPE, self.PLANES[7], D, bn_momentum=bn_momentum)\n\n        self.inplanes = self.PLANES[7] + self.INIT_DIM\n        self.block8 = self._make_layer(\n            self.BLOCK,\n            self.PLANES[7],\n            self.LAYERS[7],\n            dilation=dilations[7],\n            norm_type=self.NORM_TYPE,\n            bn_momentum=bn_momentum,\n        )\n\n        self.final = conv(self.PLANES[7], out_channels, kernel_size=1, stride=1, bias=True, D=D)\n        self.relu = MinkowskiReLU(inplace=True)\n\n    def forward(self, x):\n        out = self.conv0p1s1(x)\n        out = self.bn0(out)\n        out_p1 = self.relu(out)\n\n        out = self.conv1p1s2(out_p1)\n        out = self.bn1(out)\n        out = self.relu(out)\n        out_b1p2 = self.block1(out)\n\n        out = self.conv2p2s2(out_b1p2)\n        out = self.bn2(out)\n        out = self.relu(out)\n        out_b2p4 = self.block2(out)\n\n        out = self.conv3p4s2(out_b2p4)\n        out = self.bn3(out)\n        out = self.relu(out)\n        out_b3p8 = self.block3(out)\n\n        # pixel_dist=16\n        out = self.conv4p8s2(out_b3p8)\n        out = self.bn4(out)\n        out = self.relu(out)\n        out = self.block4(out)\n\n        # pixel_dist=8\n        out = self.convtr4p16s2(out)\n        out = self.bntr4(out)\n        out = self.relu(out)\n\n        out = me.cat(out, out_b3p8)\n        out = self.block5(out)\n\n        # pixel_dist=4\n        out = self.convtr5p8s2(out)\n        out = self.bntr5(out)\n        out = self.relu(out)\n\n        out = me.cat(out, out_b2p4)\n        out = self.block6(out)\n\n        # pixel_dist=2\n        out = self.convtr6p4s2(out)\n        out = self.bntr6(out)\n        out = self.relu(out)\n\n        out = me.cat(out, out_b1p2)\n        out = self.block7(out)\n\n        # pixel_dist=1\n        out = self.convtr7p2s2(out)\n        out = self.bntr7(out)\n        out = self.relu(out)\n\n        out = me.cat(out, out_p1)\n        out = self.block8(out)\n\n        return self.final(out)\n\n\nclass Res16UNet14(Res16UNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (1, 1, 1, 1, 1, 1, 1, 1)\n\n\nclass Res16UNet18(Res16UNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 2, 2, 2, 2, 2, 2, 2)\n\n\nclass Res16UNet34(Res16UNetBase):\n    BLOCK = BasicBlock\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass Res16UNet50(Res16UNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 6, 2, 2, 2, 2)\n\n\nclass Res16UNet101(Res16UNetBase):\n    BLOCK = Bottleneck\n    LAYERS = (2, 3, 4, 23, 2, 2, 2, 2)\n\n\nclass Res16UNet14A(Res16UNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass Res16UNet14A2(Res16UNet14A):\n    LAYERS = (1, 1, 1, 1, 2, 2, 2, 2)\n\n\nclass Res16UNet14B(Res16UNet14):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass Res16UNet14B2(Res16UNet14B):\n    LAYERS = (1, 1, 1, 1, 2, 2, 2, 2)\n\n\nclass Res16UNet14B3(Res16UNet14B):\n    LAYERS = (2, 2, 2, 2, 1, 1, 1, 1)\n\n\nclass Res16UNet14C(Res16UNet14):\n    PLANES = (32, 64, 128, 256, 192, 192, 128, 128)\n\n\nclass Res16UNet14D(Res16UNet14):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass Res16UNet18A(Res16UNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass Res16UNet18B(Res16UNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 128, 128)\n\n\nclass Res16UNet18D(Res16UNet18):\n    PLANES = (32, 64, 128, 256, 384, 384, 384, 384)\n\n\nclass Res16UNet32B(Res16UNet34):\n    PLANES = (32, 64, 128, 256, 256, 64, 64, 64)\n\n\nclass Res16UNet34A(Res16UNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 64)\n\n\nclass Res16UNet34B(Res16UNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 64, 32)\n\n\nclass Res16UNet34C(Res16UNet34):\n    PLANES = (32, 64, 128, 256, 256, 128, 96, 96)\n\n\nclass STRes16UNetBase(Res16UNetBase):\n\n    CONV_TYPE = ConvType.SPATIAL_HYPERCUBE_TEMPORAL_HYPERCROSS\n\n    def __init__(self, in_channels, out_channels, config, D=4, **kwargs):\n        super(STRes16UNetBase, self).__init__(in_channels, out_channels, config, D, **kwargs)\n\n\nclass STRes16UNet14(STRes16UNetBase, Res16UNet14):\n    pass\n\n\nclass STRes16UNet14A(STRes16UNetBase, Res16UNet14A):\n    pass\n\n\nclass STRes16UNet18(STRes16UNetBase, Res16UNet18):\n    pass\n\n\nclass STRes16UNet34(STRes16UNetBase, Res16UNet34):\n    pass\n\n\nclass STRes16UNet50(STRes16UNetBase, Res16UNet50):\n    pass\n\n\nclass STRes16UNet101(STRes16UNetBase, Res16UNet101):\n    pass\n\n\nclass STRes16UNet18A(STRes16UNet18):\n    PLANES = (32, 64, 128, 256, 128, 128, 96, 96)\n\n\nclass STResTesseract16UNetBase(STRes16UNetBase):\n    CONV_TYPE = ConvType.HYPERCUBE\n\n\nclass STResTesseract16UNet18A(STRes16UNet18A, STResTesseract16UNetBase):\n    pass\n\n\ndef get_block(norm_type, inplanes, planes, stride=1, dilation=1, downsample=None, bn_momentum=0.1, D=3):\n    if norm_type == NormType.BATCH_NORM:\n        return BasicBlock(\n            inplanes=inplanes,\n            planes=planes,\n            stride=stride,\n            dilation=dilation,\n            downsample=downsample,\n            bn_momentum=bn_momentum,\n            D=D,\n        )\n    elif norm_type == NormType.INSTANCE_NORM:\n        return BasicBlockIN(inplanes, planes, stride, dilation, downsample, bn_momentum, D)\n    else:\n        raise ValueError(f""Type {norm_type}, not defined"")\n'"
torch_points3d/modules/MinkowskiEngine/resunet.py,1,"b'import torch\nimport MinkowskiEngine as ME\nimport MinkowskiEngine.MinkowskiFunctional as MEF\nfrom .common import get_norm\n\nfrom .res16unet import get_block\nfrom .common import NormType\n\n\nclass ResUNet2(ME.MinkowskiNetwork):\n    NORM_TYPE = None\n    BLOCK_NORM_TYPE = NormType.BATCH_NORM\n    CHANNELS = [None, 32, 64, 128, 256]\n    TR_CHANNELS = [None, 32, 64, 64, 128]\n\n    # To use the model, must call initialize_coords before forward pass.\n    # Once data is processed, call clear to reset the model before calling initialize_coords\n    def __init__(\n        self, in_channels=3, out_channels=32, bn_momentum=0.01, normalize_feature=True, conv1_kernel_size=5, D=3\n    ):\n        ME.MinkowskiNetwork.__init__(self, D)\n        NORM_TYPE = self.NORM_TYPE\n        BLOCK_NORM_TYPE = self.BLOCK_NORM_TYPE\n        CHANNELS = self.CHANNELS\n        TR_CHANNELS = self.TR_CHANNELS\n        # print(D, in_channels, out_channels, conv1_kernel_size)\n        self.normalize_feature = normalize_feature\n        self.conv1 = ME.MinkowskiConvolution(\n            in_channels=in_channels,\n            out_channels=CHANNELS[1],\n            kernel_size=conv1_kernel_size,\n            stride=1,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm1 = get_norm(NORM_TYPE, CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n        self.block1 = get_block(BLOCK_NORM_TYPE, CHANNELS[1], CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n        self.conv2 = ME.MinkowskiConvolution(\n            in_channels=CHANNELS[1],\n            out_channels=CHANNELS[2],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm2 = get_norm(NORM_TYPE, CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n        self.block2 = get_block(BLOCK_NORM_TYPE, CHANNELS[2], CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n        self.conv3 = ME.MinkowskiConvolution(\n            in_channels=CHANNELS[2],\n            out_channels=CHANNELS[3],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm3 = get_norm(NORM_TYPE, CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n        self.block3 = get_block(BLOCK_NORM_TYPE, CHANNELS[3], CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n        self.conv4 = ME.MinkowskiConvolution(\n            in_channels=CHANNELS[3],\n            out_channels=CHANNELS[4],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm4 = get_norm(NORM_TYPE, CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n        self.block4 = get_block(BLOCK_NORM_TYPE, CHANNELS[4], CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n        self.conv4_tr = ME.MinkowskiConvolutionTranspose(\n            in_channels=CHANNELS[4],\n            out_channels=TR_CHANNELS[4],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm4_tr = get_norm(NORM_TYPE, TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n        self.block4_tr = get_block(BLOCK_NORM_TYPE, TR_CHANNELS[4], TR_CHANNELS[4], bn_momentum=bn_momentum, D=D)\n\n        self.conv3_tr = ME.MinkowskiConvolutionTranspose(\n            in_channels=CHANNELS[3] + TR_CHANNELS[4],\n            out_channels=TR_CHANNELS[3],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm3_tr = get_norm(NORM_TYPE, TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n        self.block3_tr = get_block(BLOCK_NORM_TYPE, TR_CHANNELS[3], TR_CHANNELS[3], bn_momentum=bn_momentum, D=D)\n\n        self.conv2_tr = ME.MinkowskiConvolutionTranspose(\n            in_channels=CHANNELS[2] + TR_CHANNELS[3],\n            out_channels=TR_CHANNELS[2],\n            kernel_size=3,\n            stride=2,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n        self.norm2_tr = get_norm(NORM_TYPE, TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n        self.block2_tr = get_block(BLOCK_NORM_TYPE, TR_CHANNELS[2], TR_CHANNELS[2], bn_momentum=bn_momentum, D=D)\n\n        self.conv1_tr = ME.MinkowskiConvolution(\n            in_channels=CHANNELS[1] + TR_CHANNELS[2],\n            out_channels=TR_CHANNELS[1],\n            kernel_size=1,\n            stride=1,\n            dilation=1,\n            has_bias=False,\n            dimension=D,\n        )\n\n        # self.block1_tr = BasicBlockBN(TR_CHANNELS[1], TR_CHANNELS[1], bn_momentum=bn_momentum, D=D)\n\n        self.final = ME.MinkowskiConvolution(\n            in_channels=TR_CHANNELS[1],\n            out_channels=out_channels,\n            kernel_size=1,\n            stride=1,\n            dilation=1,\n            has_bias=True,\n            dimension=D,\n        )\n\n    def forward(self, x):\n        out_s1 = self.conv1(x)\n        out_s1 = self.norm1(out_s1)\n        out_s1 = self.block1(out_s1)\n        out = MEF.relu(out_s1)\n\n        out_s2 = self.conv2(out)\n        out_s2 = self.norm2(out_s2)\n        out_s2 = self.block2(out_s2)\n        out = MEF.relu(out_s2)\n\n        out_s4 = self.conv3(out)\n        out_s4 = self.norm3(out_s4)\n        out_s4 = self.block3(out_s4)\n        out = MEF.relu(out_s4)\n\n        out_s8 = self.conv4(out)\n        out_s8 = self.norm4(out_s8)\n        out_s8 = self.block4(out_s8)\n        out = MEF.relu(out_s8)\n\n        out = self.conv4_tr(out)\n        out = self.norm4_tr(out)\n        out = self.block4_tr(out)\n        out_s4_tr = MEF.relu(out)\n\n        out = ME.cat(out_s4_tr, out_s4)\n\n        out = self.conv3_tr(out)\n        out = self.norm3_tr(out)\n        out = self.block3_tr(out)\n        out_s2_tr = MEF.relu(out)\n\n        out = ME.cat(out_s2_tr, out_s2)\n\n        out = self.conv2_tr(out)\n        out = self.norm2_tr(out)\n        out = self.block2_tr(out)\n        out_s1_tr = MEF.relu(out)\n\n        out = ME.cat(out_s1_tr, out_s1)\n        out = self.conv1_tr(out)\n        out = MEF.relu(out)\n        out = self.final(out)\n\n        if self.normalize_feature:\n            return ME.SparseTensor(\n                out.F / torch.norm(out.F, p=2, dim=1, keepdim=True),\n                coords_key=out.coords_key,\n                coords_manager=out.coords_man,\n            )\n        else:\n            return out\n\n\nclass ResUNetBN2(ResUNet2):\n    NORM_TYPE = NormType.BATCH_NORM\n\n\nclass ResUNetBN2B(ResUNet2):\n    NORM_TYPE = NormType.BATCH_NORM\n    CHANNELS = [None, 32, 64, 128, 256]\n    TR_CHANNELS = [None, 64, 64, 64, 64]\n\n\nclass ResUNetBN2C(ResUNet2):\n    NORM_TYPE = NormType.BATCH_NORM\n    CHANNELS = [None, 32, 64, 128, 256]\n    TR_CHANNELS = [None, 64, 64, 64, 128]\n\n\nclass ResUNetBN2D(ResUNet2):\n    NORM_TYPE = NormType.BATCH_NORM\n    CHANNELS = [None, 32, 64, 128, 256]\n    TR_CHANNELS = [None, 64, 64, 128, 128]\n\n\nclass ResUNetBN2E(ResUNet2):\n    NORM_TYPE = NormType.BATCH_NORM\n    CHANNELS = [None, 128, 128, 128, 256]\n    TR_CHANNELS = [None, 64, 128, 128, 128]\n\n\nclass Res2BlockDown(ME.MinkowskiNetwork):\n\n    """"""\n    block for unwrapped Resnet\n    """"""\n\n    def __init__(\n        self,\n        down_conv_nn,\n        kernel_size,\n        stride,\n        dilation,\n        dimension=3,\n        bn_momentum=0.01,\n        norm_type=NormType.BATCH_NORM,\n        block_norm_type=NormType.BATCH_NORM,\n        **kwargs\n    ):\n        ME.MinkowskiNetwork.__init__(self, dimension)\n        self.conv = ME.MinkowskiConvolution(\n            in_channels=down_conv_nn[0],\n            out_channels=down_conv_nn[1],\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            has_bias=False,\n            dimension=dimension,\n        )\n        self.norm = get_norm(norm_type, down_conv_nn[1], bn_momentum=bn_momentum, D=dimension)\n        self.block = get_block(block_norm_type, down_conv_nn[1], down_conv_nn[1], bn_momentum=bn_momentum, D=dimension)\n\n    def forward(self, x):\n\n        out_s = self.conv(x)\n        out_s = self.norm(out_s)\n        out_s = self.block(out_s)\n        out = MEF.relu(out_s)\n        return out\n\n\nclass Res2BlockUp(ME.MinkowskiNetwork):\n\n    """"""\n    block for unwrapped Resnet\n    """"""\n\n    def __init__(\n        self,\n        up_conv_nn,\n        kernel_size,\n        stride,\n        dilation,\n        dimension=3,\n        bn_momentum=0.01,\n        norm_type=NormType.BATCH_NORM,\n        block_norm_type=NormType.BATCH_NORM,\n        **kwargs\n    ):\n        ME.MinkowskiNetwork.__init__(self, dimension)\n        self.conv = ME.MinkowskiConvolutionTranspose(\n            in_channels=up_conv_nn[0],\n            out_channels=up_conv_nn[1],\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            has_bias=False,\n            dimension=dimension,\n        )\n        if len(up_conv_nn) == 3:\n            self.final = ME.MinkowskiConvolution(\n                in_channels=up_conv_nn[1],\n                out_channels=up_conv_nn[2],\n                kernel_size=kernel_size,\n                stride=stride,\n                dilation=dilation,\n                has_bias=True,\n                dimension=dimension,\n            )\n        else:\n            self.norm = get_norm(norm_type, up_conv_nn[1], bn_momentum=bn_momentum, D=dimension)\n            self.block = get_block(block_norm_type, up_conv_nn[1], up_conv_nn[1], bn_momentum=bn_momentum, D=dimension)\n            self.final = None\n\n    def forward(self, x, x_skip):\n        if x_skip is not None:\n            x = ME.cat(x, x_skip)\n        out_s = self.conv(x)\n        if self.final is None:\n            out_s = self.norm(out_s)\n            out_s = self.block(out_s)\n            out = MEF.relu(out_s)\n            return out\n        else:\n            out_s = MEF.relu(out_s)\n            out = self.final(out_s)\n            return out\n'"
torch_points3d/modules/PointCNN/__init__.py,0,b'from .modules import *\n'
torch_points3d/modules/PointCNN/modules.py,5,"b'from math import ceil\n\nimport torch\nfrom torch.nn import Sequential as S, Linear as L, BatchNorm1d as BN\nfrom torch.nn import ELU, Conv1d\nfrom torch_geometric.nn import Reshape\nfrom torch_geometric.nn.inits import reset\n\nfrom torch_points3d.core.spatial_ops import RandomSampler, FPSSampler, DilatedKNNNeighbourFinder\nfrom torch_points3d.core.base_conv.message_passing import *\n\n\n# XConv from torch geometric, modified for this framework\n# https://github.com/rusty1s/pytorch_geometric/blob/master/torch_geometric/nn/conv/x_conv.py\n\n\nclass XConv(torch.nn.Module):\n    r""""""The convolutional operator on :math:`\\mathcal{X}`-transformed points\n    from the `""PointCNN: Convolution On X-Transformed Points""\n    <https://arxiv.org/abs/1801.07791>`_ paper\n    .. math::\n        \\mathbf{x}^{\\prime}_i = \\mathrm{Conv}\\left(\\mathbf{K},\n        \\gamma_{\\mathbf{\\Theta}}(\\mathbf{P}_i - \\mathbf{p}_i) \\times\n        \\left( h_\\mathbf{\\Theta}(\\mathbf{P}_i - \\mathbf{p}_i) \\, \\Vert \\,\n        \\mathbf{x}_i \\right) \\right),\n    where :math:`\\mathbf{K}` and :math:`\\mathbf{P}_i` denote the trainable\n    filter and neighboring point positions of :math:`\\mathbf{x}_i`,\n    respectively.\n    :math:`\\gamma_{\\mathbf{\\Theta}}` and :math:`h_{\\mathbf{\\Theta}}` describe\n    neural networks, *i.e.* MLPs, where :math:`h_{\\mathbf{\\Theta}}`\n    individually lifts each point into a higher-dimensional space, and\n    :math:`\\gamma_{\\mathbf{\\Theta}}` computes the :math:`\\mathcal{X}`-\n    transformation matrix based on *all* points in a neighborhood.\n    Args:\n        in_channels (int): Size of each input sample.\n        out_channels (int): Size of each output sample.\n        dim (int): Point cloud dimensionality.\n        kernel_size (int): Size of the convolving kernel, *i.e.* number of\n            neighbors including self-loops.\n        hidden_channels (int, optional): Output size of\n            :math:`h_{\\mathbf{\\Theta}}`, *i.e.* dimensionality of lifted\n            points. If set to :obj:`None`, will be automatically set to\n            :obj:`in_channels / 4`. (default: :obj:`None`)\n        dilation (int, optional): The factor by which the neighborhood is\n            extended, from which :obj:`kernel_size` neighbors are then\n            uniformly sampled. Can be interpreted as the dilation rate of\n            classical convolutional operators. (default: :obj:`1`)\n        bias (bool, optional): If set to :obj:`False`, the layer will not learn\n            an additive bias. (default: :obj:`True`)\n        **kwargs (optional): Additional arguments of\n            :class:`torch_cluster.knn_graph`.\n    """"""\n\n    def __init__(\n        self, in_channels, out_channels, dim, kernel_size, hidden_channels=None, dilation=1, bias=True, **kwargs,\n    ):\n        super(XConv, self).__init__()\n\n        self.in_channels = in_channels\n        if hidden_channels is None:\n            hidden_channels = in_channels // 4\n        assert hidden_channels > 0\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.dim = dim\n        self.kernel_size = kernel_size\n        self.dilation = dilation\n        self.kwargs = kwargs\n\n        C_in, C_delta, C_out = in_channels, hidden_channels, out_channels\n        D, K = dim, kernel_size\n\n        self.mlp1 = S(\n            L(dim, C_delta), ELU(), BN(C_delta), L(C_delta, C_delta), ELU(), BN(C_delta), Reshape(-1, K, C_delta),\n        )\n\n        self.mlp2 = S(\n            L(D * K, K ** 2),\n            ELU(),\n            BN(K ** 2),\n            Reshape(-1, K, K),\n            Conv1d(K, K ** 2, K, groups=K),\n            ELU(),\n            BN(K ** 2),\n            Reshape(-1, K, K),\n            Conv1d(K, K ** 2, K, groups=K),\n            BN(K ** 2),\n            Reshape(-1, K, K),\n        )\n\n        C_in = C_in + C_delta\n        depth_multiplier = int(ceil(C_out / C_in))\n        self.conv = S(\n            Conv1d(C_in, C_in * depth_multiplier, K, groups=C_in),\n            Reshape(-1, C_in * depth_multiplier),\n            L(C_in * depth_multiplier, C_out, bias=bias),\n        )\n\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        reset(self.mlp1)\n        reset(self.mlp2)\n        reset(self.conv)\n\n    def forward(self, x, pos, edge_index):\n\n        # posTo = the points that will be centers of convolutions\n        # posFrom = points that have edges to the centers of convolutions\n        # For a down conv, posFrom = pos, posTo = pos[idx]\n        # For an up conv, posFrom = pos, posTo = pos_skip\n        posFrom, posTo = pos\n\n        (N, D), K = posTo.size(), self.kernel_size\n\n        idxFrom, idxTo = edge_index\n\n        relPos = posTo[idxTo] - posFrom[idxFrom]\n\n        x_star = self.mlp1(relPos)\n        # x_star = self.mlp1(relPos.view(len(row), D))\n        if x is not None:\n            x = x.unsqueeze(-1) if x.dim() == 1 else x\n            x = x[idxFrom].view(N, K, self.in_channels)\n            x_star = torch.cat([x_star, x], dim=-1)\n        x_star = x_star.transpose(1, 2).contiguous()\n        x_star = x_star.view(N, self.in_channels + self.hidden_channels, K, 1)\n\n        transform_matrix = self.mlp2(relPos.view(N, K * D))\n        transform_matrix = transform_matrix.view(N, 1, K, K)\n\n        x_transformed = torch.matmul(transform_matrix, x_star)\n        x_transformed = x_transformed.view(N, -1, K)\n\n        out = self.conv(x_transformed)\n\n        return out\n\n    def __repr__(self):\n        return ""{}({}, {})"".format(self.__class__.__name__, self.in_channels, self.out_channels)\n\n\nclass PointCNNConvDown(BaseConvolutionDown):\n    def __init__(\n        self, inN=None, outN=None, K=None, D=None, C1=None, C2=None, hidden_channel=None, *args, **kwargs,\n    ):\n        super(PointCNNConvDown, self).__init__(FPSSampler(outN / inN), DilatedKNNNeighbourFinder(K, D))\n\n        self._conv = XConv(C1, C2, 3, K, hidden_channels=hidden_channel)\n\n    def conv(self, x, pos, edge_index, batch):\n        return self._conv.forward(x, pos, edge_index)\n\n\nclass PointCNNConvUp(BaseConvolutionUp):\n    def __init__(self, K=None, D=None, C1=None, C2=None, *args, **kwargs):\n        super(PointCNNConvUp, self).__init__(DilatedKNNNeighbourFinder(K, D))\n\n        self._conv = XConv(C1, C2, 3, K)\n\n    def conv(self, x, pos, pos_skip, batch, batch_skip, edge_index):\n        return self._conv.forward(x, (pos, pos_skip), edge_index)\n'"
torch_points3d/modules/PointNet/__init__.py,0,b'from .modules import *\n'
torch_points3d/modules/PointNet/modules.py,5,"b'import torch\nfrom torch_geometric.nn import global_max_pool, global_mean_pool\n\nfrom torch_points3d.core.common_modules.base_modules import *\nfrom torch_points3d.core.common_modules.spatial_transform import BaseLinearTransformSTNkD\nfrom torch_points3d.models.base_model import BaseInternalLossModule\n\n\nclass MiniPointNet(torch.nn.Module):\n    def __init__(self, local_nn, global_nn, aggr=""max"", return_local_out=False):\n        super().__init__()\n\n        self.local_nn = MLP(local_nn)\n        self.global_nn = MLP(global_nn) if global_nn else None\n        self.g_pool = global_max_pool if aggr == ""max"" else global_mean_pool\n        self.return_local_out = return_local_out\n\n    def forward(self, x, batch):\n        y = x = self.local_nn(x)  # [num_points, in_dim] -> [num_points, local_out_nn]\n        x = self.g_pool(x, batch)  # [num_points, local_out_nn] -> [local_out_nn]\n        if self.global_nn:\n            x = self.global_nn(x)  # [local_out_nn] -> [global_out_nn]\n        if self.return_local_out:\n            return x, y\n        return x\n\n    def forward_embedding(self, pos, batch):\n        global_feat, local_feat = self.forward(pos, batch)\n        indices = batch.unsqueeze(-1).repeat((1, global_feat.shape[-1]))\n        gathered_global_feat = torch.gather(global_feat, 0, indices)\n        x = torch.cat([local_feat, gathered_global_feat], -1)\n        return x\n\n\nclass PointNetSTN3D(BaseLinearTransformSTNkD):\n    def __init__(self, local_nn=[3, 64, 128, 1024], global_nn=[1024, 512, 256], batch_size=1):\n        super().__init__(MiniPointNet(local_nn, global_nn), global_nn[-1], 3, batch_size)\n\n    def forward(self, x, batch):\n        return super().forward(x, x, batch)\n\n\nclass PointNetSTNkD(BaseLinearTransformSTNkD, BaseInternalLossModule):\n    def __init__(self, k=64, local_nn=[64, 64, 128, 1024], global_nn=[1024, 512, 256], batch_size=1):\n        super().__init__(MiniPointNet(local_nn, global_nn), global_nn[-1], k, batch_size)\n\n    def forward(self, x, batch):\n        return super().forward(x, x, batch)\n\n    def get_internal_losses(self):\n        return {""orthogonal_regularization_loss"": self.get_orthogonal_regularization_loss()}\n\n\nclass PointNetSeg(torch.nn.Module):\n    def __init__(\n        self,\n        input_stn_local_nn=[3, 64, 128, 1024],\n        input_stn_global_nn=[1024, 512, 256],\n        local_nn_1=[3, 64, 64],\n        feat_stn_k=64,\n        feat_stn_local_nn=[64, 64, 128, 1024],\n        feat_stn_global_nn=[1024, 512, 256],\n        local_nn_2=[64, 64, 128, 1024],\n        seg_nn=[1088, 512, 256, 128, 4],\n        batch_size=1,\n        *args,\n        **kwargs\n    ):\n        super().__init__()\n\n        self.batch_size = batch_size\n\n        self.input_stn = PointNetSTN3D(input_stn_local_nn, input_stn_global_nn, batch_size)\n        self.local_nn_1 = MLP(local_nn_1)\n        self.feat_stn = PointNetSTNkD(feat_stn_k, feat_stn_local_nn, feat_stn_global_nn, batch_size)\n        self.local_nn_2 = MLP(local_nn_2)\n        self.seg_nn = MLP(seg_nn)\n\n        self._use_scatter_pooling = True\n\n    def set_scatter_pooling(self, use_scatter_pooling):\n        self._use_scatter_pooling = use_scatter_pooling\n\n    def func_global_max_pooling(self, x3, batch):\n        if self._use_scatter_pooling:\n            return global_max_pool(x3, batch)\n        else:\n            global_feature = x3.max(1)\n            return global_feature[0]\n\n    def forward(self, x, batch):\n\n        # apply pointnet classification network to get per-point\n        # features and global feature\n        x = self.input_stn(x, batch)\n        x = self.local_nn_1(x)\n        x_feat_trans = self.feat_stn(x, batch)\n        x3 = self.local_nn_2(x_feat_trans)\n\n        global_feature = self.func_global_max_pooling(x3, batch)\n        # concat per-point and global feature and regress to get\n        # per-point scores\n        feat_concat = torch.cat([x_feat_trans, global_feature[batch]], dim=1)\n        out = self.seg_nn(feat_concat)\n\n        return out\n'"
torch_points3d/modules/RSConv/__init__.py,0,b'from .dense import *\nfrom .message_passing import *\n'
torch_points3d/modules/RSConv/dense.py,22,"b'import logging\n\nimport torch\nfrom torch.nn import Sequential\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import Conv1d\nimport torch_points_kernels as tp\n\nfrom torch_points3d.core.base_conv.dense import *\nfrom torch_points3d.core.common_modules.dense_modules import MLP2D\nfrom torch_points3d.core.spatial_ops import DenseFPSSampler, DenseRadiusNeighbourFinder\nfrom torch_points3d.utils.colors import COLORS\n\nlog = logging.getLogger(__name__)\n\n\nclass RSConvMapper(nn.Module):\n    """"""[This class handles the special mechanism between the msg\n        and the features of RSConv]\n    """"""\n\n    def __init__(self, down_conv_nn, use_xyz, bn=True, activation=nn.LeakyReLU(negative_slope=0.01), *args, **kwargs):\n        super(RSConvMapper, self).__init__()\n\n        self._down_conv_nn = down_conv_nn\n        self._use_xyz = use_xyz\n\n        self.nn = nn.ModuleDict()\n\n        if len(self._down_conv_nn) == 2:  # First layer\n            self._first_layer = True\n            f_in, f_intermediate, f_out = self._down_conv_nn[0]\n            self.nn[""features_nn""] = MLP2D(self._down_conv_nn[1], bn=bn, bias=False)\n\n        else:\n            self._first_layer = False\n            f_in, f_intermediate, f_out = self._down_conv_nn\n\n        self.nn[""mlp_msg""] = MLP2D([f_in, f_intermediate, f_out], bn=bn, bias=False)\n\n        self.nn[""norm""] = Sequential(*[nn.BatchNorm2d(f_out), activation])\n\n        self._f_out = f_out\n\n    @property\n    def f_out(self):\n        return self._f_out\n\n    def forward(self, features, msg):\n        """"""\n        features  -- [B, C, num_points, nsamples]\n        msg  -- [B, 10, num_points, nsamples]\n\n        The 10 features comes from [distance: 1,\n                                    coord_origin:3,\n                                    coord_target:3,\n                                    delta_origin_target:3]\n        """"""\n\n        # Transform msg\n        msg = self.nn[""mlp_msg""](msg)\n\n        # If first_layer, augment features_size\n        if self._first_layer:\n            features = self.nn[""features_nn""](features)\n\n        return self.nn[""norm""](torch.mul(features, msg))\n\n\nclass SharedRSConv(nn.Module):\n    """"""\n    Input shape: (B, C_in, npoint, nsample)\n    Output shape: (B, C_out, npoint)\n    """"""\n\n    def __init__(self, mapper: RSConvMapper, radius):\n        super(SharedRSConv, self).__init__()\n\n        self._mapper = mapper\n        self._radius = radius\n\n    def forward(self, aggr_features, centroids):\n        """"""\n        aggr_features  -- [B, 3 + 3 + C, num_points, nsamples]\n        centroids  -- [B, 3, num_points, 1]\n        """"""\n        # Extract information to create message\n        abs_coord = aggr_features[:, :3]  # absolute coordinates\n        delta_x = aggr_features[:, 3:6]  # normalized coordinates\n        features = aggr_features[:, 3:]\n\n        nsample = abs_coord.shape[-1]\n        coord_xi = centroids.repeat(1, 1, 1, nsample)  # (B, 3, npoint, nsample) centroid points\n\n        distance = torch.norm(delta_x, p=2, dim=1).unsqueeze(1)  # Calculate distance\n\n        # Create message by contenating distance, origin / target coords, delta coords\n        h_xi_xj = torch.cat((distance, coord_xi, abs_coord, delta_x), dim=1)\n\n        return self._mapper(features, h_xi_xj)\n\n    def __repr__(self):\n        return ""{}(radius={})"".format(self.__class__.__name__, self._radius)\n\n\nclass RSConvSharedMSGDown(BaseDenseConvolutionDown):\n    def __init__(\n        self,\n        npoint=None,\n        radii=None,\n        nsample=None,\n        down_conv_nn=None,\n        channel_raising_nn=None,\n        bn=True,\n        use_xyz=True,\n        activation=nn.ReLU(),\n        **kwargs\n    ):\n        assert len(radii) == len(nsample)\n        if len(radii) != len(down_conv_nn):\n            log.warn(""The down_conv_nn has a different size as radii. Make sure of have SharedRSConv"")\n        super(RSConvSharedMSGDown, self).__init__(\n            DenseFPSSampler(num_to_sample=npoint), DenseRadiusNeighbourFinder(radii, nsample), **kwargs\n        )\n\n        self.use_xyz = use_xyz\n        self.npoint = npoint\n        self.mlps = nn.ModuleList()\n\n        # https://github.com/Yochengliu/Relation-Shape-CNN/blob/6464eb8bb4efc686adec9da437112ef888e55684/utils/pointnet2_modules.py#L106\n        self._mapper = RSConvMapper(down_conv_nn, activation=activation, use_xyz=self.use_xyz)\n\n        self.mlp_out = Sequential(\n            *[\n                Conv1d(channel_raising_nn[0], channel_raising_nn[-1], kernel_size=1, stride=1, bias=True),\n                nn.BatchNorm1d(channel_raising_nn[-1]),\n                activation,\n            ]\n        )\n\n        for i in range(len(radii)):\n            self.mlps.append(SharedRSConv(self._mapper, radii[i]))\n\n    def _prepare_features(self, x, pos, new_pos, idx):\n        new_pos_trans = pos.transpose(1, 2).contiguous()\n        grouped_pos_absolute = tp.grouping_operation(new_pos_trans, idx)  # (B, 3, npoint, nsample)\n        centroids = new_pos.transpose(1, 2).unsqueeze(-1)\n        grouped_pos_normalized = grouped_pos_absolute - centroids\n\n        if x is not None:\n            grouped_features = tp.grouping_operation(x, idx)\n            if self.use_xyz:\n                new_features = torch.cat(\n                    [grouped_pos_absolute, grouped_pos_normalized, grouped_features], dim=1\n                )  # (B, 3 + 3 + C, npoint, nsample)\n            else:\n                new_features = grouped_features\n        else:\n            assert self.use_xyz, ""Cannot have not features and not use xyz as a feature!""\n            new_features = torch.cat(\n                [grouped_pos_absolute, grouped_pos_normalized], dim=1\n            )  # (B, 3 + 3 npoint, nsample)\n\n        return new_features, centroids\n\n    def conv(self, x, pos, new_pos, radius_idx, scale_idx):\n        """""" Implements a Dense convolution where radius_idx represents\n        the indexes of the points in x and pos to be agragated into the new feature\n        for each point in new_pos\n\n        Arguments:\n            x -- Previous features [B, N, C]\n            pos -- Previous positions [B, N, 3]\n            new_pos  -- Sampled positions [B, npoints, 3]\n            radius_idx -- Indexes to group [B, npoints, nsample]\n            scale_idx -- Scale index in multiscale convolutional layers\n        Returns:\n            new_x -- Features after passing trhough the MLP [B, mlp[-1], npoints]\n        """"""\n        assert scale_idx < len(self.mlps)\n        aggr_features, centroids = self._prepare_features(x, pos, new_pos, radius_idx)\n        new_features = self.mlps[scale_idx](aggr_features, centroids)  # (B, mlp[-1], npoint, nsample)\n        new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        new_features = self.mlp_out(new_features.squeeze(-1))  # (B, mlp[-1], npoint)\n        return new_features\n\n    def __repr__(self):\n        return ""{}({}, shared: {} {}, {} {})"".format(\n            self.__class__.__name__,\n            self.mlps.__repr__(),\n            COLORS.Cyan,\n            self.mlp_out.__repr__(),\n            self._mapper.__repr__(),\n            COLORS.END_TOKEN,\n        )\n\n\n######################################################################\n\n\nclass OriginalRSConv(nn.Module):\n    """"""\n    Input shape: (B, C_in, npoint, nsample)\n    Output shape: (B, C_out, npoint)\n    """"""\n\n    def __init__(self, mapping=None, first_layer=False, radius=None, activation=nn.ReLU(inplace=True)):\n        super(OriginalRSConv, self).__init__()\n\n        self.nn = nn.ModuleList()\n\n        self._radius = radius\n\n        self.mapping_func1 = mapping[0]\n        self.mapping_func2 = mapping[1]\n        self.cr_mapping = mapping[2]\n\n        self.first_layer = first_layer\n\n        if first_layer:\n            self.xyz_raising = mapping[3]\n            self.bn_xyz_raising = nn.BatchNorm2d(self.xyz_raising.out_channels)\n            self.nn.append(self.bn_xyz_raising)\n\n        self.bn_mapping = nn.BatchNorm2d(self.mapping_func1.out_channels)\n        self.bn_rsconv = nn.BatchNorm2d(self.cr_mapping.in_channels)\n        self.bn_channel_raising = nn.BatchNorm1d(self.cr_mapping.out_channels)\n\n        self.nn.append(self.bn_mapping)\n        self.nn.append(self.bn_rsconv)\n        self.nn.append(self.bn_channel_raising)\n\n        self.activation = activation\n\n    def forward(self, input):  # input: (B, 3 + 3 + C_in, npoint, centroid + nsample)\n\n        x = input[:, 3:, :, :]  # (B, C_in, npoint, nsample+1), input features\n        nsample = x.size()[3]\n        abs_coord = input[:, 0:3, :, :]  # (B, 3, npoint, nsample+1), absolute coordinates\n        delta_x = input[:, 3:6, :, :]  # (B, 3, npoint, nsample+1), normalized coordinates\n\n        coord_xi = abs_coord[:, :, :, 0:1].repeat(1, 1, 1, nsample)  # (B, 3, npoint, nsample),  centroid point\n        h_xi_xj = torch.norm(delta_x, p=2, dim=1).unsqueeze(1)\n        h_xi_xj = torch.cat((h_xi_xj, coord_xi, abs_coord, delta_x), dim=1)\n\n        h_xi_xj = self.mapping_func2(self.activation(self.bn_mapping(self.mapping_func1(h_xi_xj))))\n\n        if self.first_layer:\n            x = self.activation(self.bn_xyz_raising(self.xyz_raising(x)))\n        x = F.max_pool2d(self.activation(self.bn_rsconv(torch.mul(h_xi_xj, x))), kernel_size=(1, nsample)).squeeze(\n            3\n        )  # (B, C_in, npoint)\n        x = self.activation(self.bn_channel_raising(self.cr_mapping(x)))\n        return x\n\n    def __repr__(self):\n        return ""{}({})"".format(self.__class__.__name__, self.nn.__repr__())\n\n\nclass RSConvOriginalMSGDown(BaseDenseConvolutionDown):\n    def __init__(\n        self,\n        npoint=None,\n        radii=None,\n        nsample=None,\n        down_conv_nn=None,\n        channel_raising_nn=None,\n        bn=True,\n        bias=True,\n        use_xyz=True,\n        activation=nn.ReLU(),\n        **kwargs\n    ):\n        assert len(radii) == len(nsample)\n        if len(radii) != len(down_conv_nn):\n            log.warning(""The down_conv_nn has a different size as radii. Make sure of have SharedRSConv"")\n        super(RSConvOriginalMSGDown, self).__init__(\n            DenseFPSSampler(num_to_sample=npoint), DenseRadiusNeighbourFinder(radii, nsample), **kwargs\n        )\n\n        self.use_xyz = use_xyz\n        self.mlps = nn.ModuleList()\n        self.mappings = nn.ModuleList()\n\n        self._first_layer = True if len(down_conv_nn) == 2 else False\n\n        if self._first_layer:\n            C_in, C_intermediate, C_out = down_conv_nn[0]\n            feat_in, f_out = down_conv_nn[-1]\n            xyz_raising = nn.Conv2d(\n                in_channels=feat_in, out_channels=f_out, kernel_size=(1, 1), stride=(1, 1), bias=bias,\n            )\n            nn.init.kaiming_normal_(xyz_raising.weight)\n            if bias:\n                nn.init.constant_(xyz_raising.bias, 0)\n        else:\n            C_in, C_intermediate, C_out = down_conv_nn\n\n        mapping_func1 = nn.Conv2d(\n            in_channels=C_in, out_channels=C_intermediate, kernel_size=(1, 1), stride=(1, 1), bias=bias,\n        )\n        mapping_func2 = nn.Conv2d(\n            in_channels=C_intermediate, out_channels=C_out, kernel_size=(1, 1), stride=(1, 1), bias=bias,\n        )\n\n        nn.init.kaiming_normal_(mapping_func1.weight)\n        nn.init.kaiming_normal_(mapping_func2.weight)\n        if bias:\n            nn.init.constant_(mapping_func1.bias, 0)\n            nn.init.constant_(mapping_func2.bias, 0)\n\n        # channel raising mapping\n        cr_mapping = nn.Conv1d(\n            in_channels=channel_raising_nn[0], out_channels=channel_raising_nn[1], kernel_size=1, stride=1, bias=bias,\n        )\n        nn.init.kaiming_normal_(cr_mapping.weight)\n        nn.init.constant_(cr_mapping.bias, 0)\n\n        if self._first_layer:\n            mapping = [mapping_func1, mapping_func2, cr_mapping, xyz_raising]\n        elif npoint is not None:\n            mapping = [mapping_func1, mapping_func2, cr_mapping]\n\n        for m in mapping:\n            self.mappings.append(m)\n\n        for radius in radii:\n            self.mlps.append(OriginalRSConv(mapping=mapping, first_layer=self._first_layer, radius=radius))\n\n    def _prepare_features(\n        self, xyz: torch.Tensor, new_xyz: torch.Tensor, features: torch.Tensor = None, idx: torch.Tensor = None\n    ) -> torch.Tensor:\n        """"""\n        Parameters\n        ----------\n        xyz : torch.Tensor\n            xyz coordinates of the features (B, N, 3)\n        new_xyz : torch.Tensor\n            centriods (B, npoint, 3)\n        features : torch.Tensor\n            Descriptors of the features (B, C, N)\n\n        Returns\n        -------\n        new_features : torch.Tensor\n            (B, 3 + C, npoint, nsample) tensor\n        """"""\n        xyz_trans = xyz.transpose(1, 2).contiguous()\n        grouped_xyz = tp.grouping_operation(xyz_trans, idx)  # (B, 3, npoint, nsample)\n        raw_grouped_xyz = grouped_xyz\n        grouped_xyz -= new_xyz.transpose(1, 2).unsqueeze(-1)\n\n        if features is not None:\n            grouped_features = tp.grouping_operation(features, idx)\n            if self.use_xyz:\n                new_features = torch.cat(\n                    [raw_grouped_xyz, grouped_xyz, grouped_features], dim=1\n                )  # (B, 3 + 3 + C, npoint, nsample)\n            else:\n                new_features = grouped_features\n        else:\n            assert self.use_xyz, ""Cannot have not features and not use xyz as a feature!""\n            new_features = torch.cat([raw_grouped_xyz, grouped_xyz], dim=1)\n\n        return new_features\n\n    def conv(self, x, pos, new_pos, radius_idx, scale_idx):\n        """""" Implements a Dense convolution where radius_idx represents\n        the indexes of the points in x and pos to be agragated into the new feature\n        for each point in new_pos\n\n        Arguments:\n            x -- Previous features [B, N, C]\n            pos -- Previous positions [B, N, 3]\n            new_pos  -- Sampled positions [B, npoints, 3]\n            radius_idx -- Indexes to group [B, npoints, nsample]\n            scale_idx -- Scale index in multiscale convolutional layers\n        Returns:\n            new_x -- Features after passing trhough the MLP [B, mlp[-1], npoints]\n        """"""\n        assert scale_idx < len(self.mlps)\n        aggr_features = self._prepare_features(pos, new_pos, x, radius_idx)\n        new_features = self.mlps[scale_idx](\n            aggr_features\n        )  # (B, 3 + 3 + C, npoint, nsample) -> (B, mlp[-1], npoint, nsample)\n        return new_features\n\n    def __repr__(self):\n        return ""{}: {} ({}, shared: {} {} {})"".format(\n            self.__class__.__name__,\n            self.nb_params,\n            self.mlps.__repr__(),\n            COLORS.Cyan,\n            self.mappings.__repr__(),\n            COLORS.END_TOKEN,\n        )\n\n\nclass RSConvMSGDown(BaseDenseConvolutionDown):\n    def __init__(\n        self,\n        npoint=None,\n        radii=None,\n        nsample=None,\n        down_conv_nn=None,\n        channel_raising_nn=None,\n        bn=True,\n        bias=True,\n        use_xyz=True,\n        activation=nn.ReLU(),\n        **kwargs\n    ):\n        assert len(radii) == len(nsample)\n        if len(radii) != len(down_conv_nn):\n            log.warning(""The down_conv_nn has a different size as radii. Make sure to have sharedMLP"")\n        super(RSConvMSGDown, self).__init__(\n            DenseFPSSampler(num_to_sample=npoint), DenseRadiusNeighbourFinder(radii, nsample), **kwargs\n        )\n\n        self.use_xyz = use_xyz\n        self.npoint = npoint\n        self.mlps = nn.ModuleList()\n\n        # https://github.com/Yochengliu/Relation-Shape-CNN/blob/6464eb8bb4efc686adec9da437112ef888e55684/utils/pointnet2_modules.py#L106\n\n        self.mlp_out = Sequential(\n            *[\n                Conv1d(channel_raising_nn[0], channel_raising_nn[-1], kernel_size=1, stride=1, bias=True),\n                nn.BatchNorm1d(channel_raising_nn[-1]),\n                activation,\n            ]\n        )\n\n        for i in range(len(radii)):\n            mapper = RSConvMapper(down_conv_nn, activation=activation, use_xyz=self.use_xyz)\n            self.mlps.append(SharedRSConv(mapper, radii[i]))\n\n        self._mapper = mapper\n\n    def _prepare_features(self, x, pos, new_pos, idx):\n        new_pos_trans = pos.transpose(1, 2).contiguous()\n        grouped_pos_absolute = tp.grouping_operation(new_pos_trans, idx)  # (B, 3, npoint, nsample)\n        centroids = new_pos.transpose(1, 2).unsqueeze(-1)\n        grouped_pos_normalized = grouped_pos_absolute - centroids\n\n        if x is not None:\n            grouped_features = tp.grouping_operation(x, idx)\n            if self.use_xyz:\n                new_features = torch.cat(\n                    [grouped_pos_absolute, grouped_pos_normalized, grouped_features], dim=1\n                )  # (B, 3 + 3 + C, npoint, nsample)\n            else:\n                new_features = grouped_features\n        else:\n            assert self.use_xyz, ""Cannot have not features and not use xyz as a feature!""\n            new_features = torch.cat(\n                [grouped_pos_absolute, grouped_pos_normalized], dim=1\n            )  # (B, 3 + 3 npoint, nsample)\n\n        return new_features, centroids\n\n    def conv(self, x, pos, new_pos, radius_idx, scale_idx):\n        """""" Implements a Dense convolution where radius_idx represents\n        the indexes of the points in x and pos to be agragated into the new feature\n        for each point in new_pos\n\n        Arguments:\n            x -- Previous features [B, N, C]\n            pos -- Previous positions [B, N, 3]\n            new_pos  -- Sampled positions [B, npoints, 3]\n            radius_idx -- Indexes to group [B, npoints, nsample]\n            scale_idx -- Scale index in multiscale convolutional layers\n        Returns:\n            new_x -- Features after passing trhough the MLP [B, mlp[-1], npoints]\n        """"""\n        assert scale_idx < len(self.mlps)\n        aggr_features, centroids = self._prepare_features(x, pos, new_pos, radius_idx)\n        new_features = self.mlps[scale_idx](aggr_features, centroids)  # (B, mlp[-1], npoint, nsample)\n        new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        new_features = self.mlp_out(new_features.squeeze(-1))  # (B, mlp[-1], npoint)\n        return new_features\n\n    def __repr__(self):\n        return ""{}({}, shared: {} {}, {} {})"".format(\n            self.__class__.__name__,\n            self.mlps.__repr__(),\n            COLORS.Cyan,\n            self.mlp_out.__repr__(),\n            self._mapper.__repr__(),\n            COLORS.END_TOKEN,\n        )\n'"
torch_points3d/modules/RSConv/message_passing.py,3,"b'import torch\nfrom torch.nn import ReLU\nfrom torch_geometric.nn import MessagePassing\n\n\nfrom torch_points3d.core.base_conv.message_passing import *\nfrom torch_points3d.core.spatial_ops import *\n\n\nclass Convolution(MessagePassing):\n    r""""""The Relation Shape Convolution layer from ""Relation-Shape Convolutional Neural Network for Point Cloud Analysis""\n    https://arxiv.org/pdf/1904.07601\n\n    local_nn - an MLP which is applied to the relation vector h_ij between points i and j to determine\n    the weights applied to each element of the feature for x_j\n\n    global_nn - an optional MPL for channel-raising following the convolution\n\n    """"""\n\n    def __init__(self, local_nn, activation=ReLU(), global_nn=None, aggr=""max"", **kwargs):\n        super(Convolution, self).__init__(aggr=aggr)\n        self.local_nn = MLP(local_nn)\n        self.activation = activation\n        self.global_nn = MLP(global_nn) if global_nn is not None else None\n\n    def forward(self, x, pos, edge_index):\n        return self.propagate(edge_index, x=x, pos=pos)\n\n    def message(self, pos_i, pos_j, x_j):\n\n        if x_j is None:\n            x_j = pos_j\n\n        vij = pos_i - pos_j\n        dij = torch.norm(vij, dim=1).unsqueeze(1)\n\n        hij = torch.cat([dij, vij, pos_i, pos_j,], dim=1)\n\n        M_hij = self.local_nn(hij)\n\n        msg = M_hij * x_j\n\n        return msg\n\n    def update(self, aggr_out):\n        x = self.activation(aggr_out)\n        if self.global_nn is not None:\n            x = self.global_nn(x)\n        return x\n\n\nclass RSConvDown(BaseConvolutionDown):\n    def __init__(self, ratio=None, radius=None, local_nn=None, down_conv_nn=None, *args, **kwargs):\n        super(RSConvDown, self).__init__(FPSSampler(ratio), RadiusNeighbourFinder(radius), *args, **kwargs)\n\n        self._conv = Convolution(local_nn=local_nn, global_nn=down_conv_nn)\n\n    def conv(self, x, pos, edge_index, batch):\n        return self._conv(x, pos, edge_index)\n'"
torch_points3d/modules/RandLANet/__init__.py,0,b'from .modules import *\n'
torch_points3d/modules/RandLANet/modules.py,5,"b'import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import MessagePassing\n\nfrom torch_points3d.core.spatial_ops import *\nfrom torch_points3d.core.base_conv.message_passing import *\n\n\nclass RandlaKernel(MessagePassing):\n    """"""\n        Implements both the Local Spatial Encoding and Attentive Pooling blocks from\n        RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds\n        https://arxiv.org/pdf/1911.11236\n\n    """"""\n\n    def __init__(self, point_pos_nn=None, attention_nn=None, global_nn=None, *args, **kwargs):\n        MessagePassing.__init__(self, aggr=""add"")\n\n        self.point_pos_nn = MLP(point_pos_nn)\n        self.attention_nn = MLP(attention_nn)\n        self.global_nn = MLP(global_nn)\n\n    def forward(self, x, pos, edge_index):\n        x = self.propagate(edge_index, x=x, pos=pos)\n        return x\n\n    def message(self, x_j, pos_i, pos_j):\n\n        if x_j is None:\n            x_j = pos_j\n\n        # compute relative position encoding\n        vij = pos_i - pos_j\n\n        dij = torch.norm(vij, dim=1).unsqueeze(1)\n\n        relPointPos = torch.cat([pos_i, pos_j, vij, dij], dim=1)\n\n        rij = self.point_pos_nn(relPointPos)\n\n        # concatenate position encoding with feature vector\n        fij_hat = torch.cat([x_j, rij], dim=1)\n\n        # attentative pooling\n        g_fij = self.attention_nn(fij_hat)\n        s_ij = F.softmax(g_fij, -1)\n\n        msg = s_ij * fij_hat\n\n        return msg\n\n    def update(self, aggr_out):\n        return self.global_nn(aggr_out)\n\n\nclass RandlaConv(BaseConvolutionDown):\n    def __init__(self, ratio=None, k=None, *args, **kwargs):\n        super(RandlaConv, self).__init__(RandomSampler(ratio), KNNNeighbourFinder(k), *args, **kwargs)\n        if kwargs.get(""index"") == 0 and kwargs.get(""nb_feature"") is not None:\n            kwargs[""point_pos_nn""][-1] = kwargs.get(""nb_feature"")\n            kwargs[""attention_nn""][0] = kwargs[""attention_nn""][-1] = kwargs.get(""nb_feature"") * 2\n            kwargs[""down_conv_nn""][0] = kwargs.get(""nb_feature"") * 2\n        self._conv = RandlaKernel(*args, global_nn=kwargs[""down_conv_nn""], **kwargs)\n\n    def conv(self, x, pos, edge_index, batch):\n        return self._conv(x, pos, edge_index)\n\n\nclass DilatedResidualBlock(BaseResnetBlock):\n    def __init__(\n        self,\n        indim,\n        outdim,\n        ratio1,\n        ratio2,\n        point_pos_nn1,\n        point_pos_nn2,\n        attention_nn1,\n        attention_nn2,\n        global_nn1,\n        global_nn2,\n        *args,\n        **kwargs\n    ):\n        if kwargs.get(""index"") == 0 and kwargs.get(""nb_feature"") is not None:\n            indim = kwargs.get(""nb_feature"")\n        super(DilatedResidualBlock, self).__init__(indim, outdim, outdim)\n        self.conv1 = RandlaConv(\n            ratio1, 16, point_pos_nn=point_pos_nn1, attention_nn=attention_nn1, down_conv_nn=global_nn1, *args, **kwargs\n        )\n        kwargs[""nb_feature""] = None\n        self.conv2 = RandlaConv(\n            ratio2, 16, point_pos_nn=point_pos_nn2, attention_nn=attention_nn2, down_conv_nn=global_nn2, *args, **kwargs\n        )\n\n    def convs(self, data):\n        data = self.conv1(data)\n        data = self.conv2(data)\n        return data\n\n\nclass RandLANetRes(torch.nn.Module):\n    def __init__(self, indim, outdim, ratio, point_pos_nn, attention_nn, down_conv_nn, *args, **kwargs):\n        super(RandLANetRes, self).__init__()\n\n        self._conv = DilatedResidualBlock(\n            indim,\n            outdim,\n            ratio[0],\n            ratio[1],\n            point_pos_nn[0],\n            point_pos_nn[1],\n            attention_nn[0],\n            attention_nn[1],\n            down_conv_nn[0],\n            down_conv_nn[1],\n            *args,\n            **kwargs\n        )\n\n    def forward(self, data):\n        return self._conv.forward(data)\n'"
torch_points3d/modules/VoteNet/__init__.py,0,b'from .voting_module import *\nfrom .proposal_module import *\nfrom .loss_helper import get_loss\n'
torch_points3d/modules/VoteNet/loss_helper.py,42,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\n\nFAR_THRESHOLD = 0.6\nNEAR_THRESHOLD = 0.3\nGT_VOTE_FACTOR = 3  # number of GT votes per point TODO should not be hardcoded\nOBJECTNESS_CLS_WEIGHTS = [0.2, 0.8]  # put larger weights on positive objectness\n\nfrom torch_points3d.core.losses import huber_loss\n\n\ndef nn_distance(pc1, pc2, l1smooth=False, delta=1.0, l1=False):\n    """"""\n    Input:\n        pc1: (B,N,C) torch tensor\n        pc2: (B,M,C) torch tensor\n        l1smooth: bool, whether to use l1smooth loss\n        delta: scalar, the delta used in l1smooth loss\n    Output:\n        dist1: (B,N) torch float32 tensor\n        idx1: (B,N) torch int64 tensor\n        dist2: (B,M) torch float32 tensor\n        idx2: (B,M) torch int64 tensor\n    """"""\n    N = pc1.shape[1]\n    M = pc2.shape[1]\n    pc1_expand_tile = pc1.unsqueeze(2).repeat(1, 1, M, 1)\n    pc2_expand_tile = pc2.unsqueeze(1).repeat(1, N, 1, 1)\n    pc_diff = pc1_expand_tile - pc2_expand_tile\n\n    if l1smooth:\n        pc_dist = torch.sum(huber_loss(pc_diff, delta), dim=-1)  # (B,N,M)\n    elif l1:\n        pc_dist = torch.sum(torch.abs(pc_diff), dim=-1)  # (B,N,M)\n    else:\n        pc_dist = torch.sum(pc_diff ** 2, dim=-1)  # (B,N,M)\n    dist1, idx1 = torch.min(pc_dist, dim=2)  # (B,N)\n    dist2, idx2 = torch.min(pc_dist, dim=1)  # (B,M)\n    return dist1, idx1, dist2, idx2\n\n\ndef compute_vote_loss(input, output):\n    """""" Compute vote loss: Match predicted votes to GT votes.\n\n    Args:\n        end_points: dict (read-only)\n\n    Returns:\n        vote_loss: scalar Tensor\n\n    Overall idea:\n        If the seed point belongs to an object (votes_label_mask == 1),\n        then we require it to vote for the object center.\n\n        Each seed point may vote for multiple translations v1,v2,v3\n        A seed point may also be in the boxes of multiple objects:\n        o1,o2,o3 with corresponding GT votes c1,c2,c3\n\n        Then the loss for this seed point is:\n            min(d(v_i,c_j)) for i=1,2,3 and j=1,2,3\n    """"""\n\n    # Load ground truth votes and assign them to seed points\n    batch_size = output[""seed_pos""].shape[0]\n    num_seed = output[""seed_pos""].shape[1]  # B,num_seed,3\n    vote_xyz = output[""pos""]  # B,num_seed*vote_factor,3\n    seed_inds = output[""seed_inds""].long()  # B,num_seed in [0,num_points-1]\n\n    # Get groundtruth votes for the seed points\n    # vote_label_mask: Use gather to select B,num_seed from B,num_point\n    #   non-object point has no GT vote mask = 0, object point has mask = 1\n    # vote_label: Use gather to select B,num_seed,9 from B,num_point,9\n    #   with inds in shape B,num_seed,9 and 9 = GT_VOTE_FACTOR * 3\n    seed_gt_votes_mask = torch.gather(input[""vote_label_mask""], 1, seed_inds)\n    seed_inds_expand = seed_inds.view(batch_size, num_seed, 1).repeat(1, 1, 3 * GT_VOTE_FACTOR)\n\n    seed_gt_votes = torch.gather(input[""vote_label""], 1, seed_inds_expand)\n    seed_gt_votes += output[""seed_pos""].repeat(1, 1, 3)\n\n    # Compute the min of min of distance\n    vote_xyz_reshape = vote_xyz.view(\n        batch_size * num_seed, -1, 3\n    )  # from B,num_seed*vote_factor,3 to B*num_seed,vote_factor,3\n    seed_gt_votes_reshape = seed_gt_votes.view(\n        batch_size * num_seed, GT_VOTE_FACTOR, 3\n    )  # from B,num_seed,3*GT_VOTE_FACTOR to B*num_seed,GT_VOTE_FACTOR,3\n    # A predicted vote to no where is not penalized as long as there is a good vote near the GT vote.\n    dist1, _, dist2, _ = nn_distance(vote_xyz_reshape, seed_gt_votes_reshape, l1=True)\n    votes_dist, _ = torch.min(dist2, dim=1)  # (B*num_seed,vote_factor) to (B*num_seed,)\n    votes_dist = votes_dist.view(batch_size, num_seed)\n    vote_loss = torch.sum(votes_dist * seed_gt_votes_mask.float()) / (torch.sum(seed_gt_votes_mask.float()) + 1e-6)\n    return vote_loss\n\n\ndef compute_objectness_loss(inputs, outputs, loss_params):\n    """""" Compute objectness loss for the proposals.\n\n    Args:\n        end_points: dict (read-only)\n\n    Returns:\n        objectness_loss: scalar Tensor\n        objectness_label: (batch_size, num_seed) Tensor with value 0 or 1\n        objectness_mask: (batch_size, num_seed) Tensor with value 0 or 1\n        object_assignment: (batch_size, num_seed) Tensor with long int\n            within [0,num_gt_object-1]\n    """"""\n    # Associate proposal and GT objects by point-to-point distances\n    aggregated_vote_xyz = outputs[""aggregated_vote_xyz""]\n    gt_center = inputs[""center_label""][:, :, 0:3]\n    B = gt_center.shape[0]\n    K = aggregated_vote_xyz.shape[1]\n    gt_center.shape[1]\n    dist1, ind1, dist2, _ = nn_distance(\n        aggregated_vote_xyz, gt_center\n    )  # dist1: BxK, dist2: BxK2 TODO Optimise this nn_distance function, does a lot of useless stuff\n\n    # Generate objectness label and mask\n    # objectness_label: 1 if pred object center is within NEAR_THRESHOLD of any GT object\n    # objectness_mask: 0 if pred object center is in gray zone (DONOTCARE), 1 otherwise\n    euclidean_dist1 = torch.sqrt(dist1 + 1e-6)\n    objectness_label = torch.zeros((B, K), dtype=torch.long).to(inputs.pos.device)\n    objectness_mask = torch.zeros((B, K)).to(inputs.pos.device)\n    objectness_label[euclidean_dist1 < loss_params.near_threshold] = 1\n    objectness_mask[euclidean_dist1 < loss_params.near_threshold] = 1\n    objectness_mask[euclidean_dist1 > loss_params.far_threshold] = 1\n\n    # Compute objectness loss\n    objectness_scores = outputs[""objectness_scores""]\n    weights = torch.Tensor(loss_params.objectness_cls_weights).to(inputs.pos.device)\n    criterion = nn.CrossEntropyLoss(weights, reduction=""none"")\n    objectness_loss = criterion(objectness_scores.transpose(2, 1), objectness_label)\n    objectness_loss = torch.sum(objectness_loss * objectness_mask) / (torch.sum(objectness_mask) + 1e-6)\n\n    # Set assignment\n    object_assignment = ind1  # (B,K) with values in 0,1,...,K2-1\n\n    return objectness_loss, objectness_label, objectness_mask, object_assignment\n\n\ndef compute_box_and_sem_cls_loss(inputs, outputs, loss_params):\n    """""" Compute 3D bounding box and semantic classification loss.\n\n    Args:\n        end_points: dict (read-only)\n\n    Returns:\n        center_loss\n        heading_cls_loss\n        heading_reg_loss\n        size_cls_loss\n        size_reg_loss\n        sem_cls_loss\n    """"""\n\n    num_heading_bin = loss_params.num_heading_bin\n    num_size_cluster = loss_params.num_size_cluster\n    num_size_cluster = loss_params.num_size_cluster\n    mean_size_arr = np.asarray(loss_params.mean_size_arr)\n\n    object_assignment = inputs[""object_assignment""]\n    batch_size = object_assignment.shape[0]\n\n    # Compute center loss\n    pred_center = outputs[""center""]\n    gt_center = inputs[""center_label""][:, :, 0:3]\n    dist1, ind1, dist2, _ = nn_distance(pred_center, gt_center)  # dist1: BxK, dist2: BxK2\n    box_label_mask = inputs[""box_label_mask""]\n    objectness_label = inputs[""objectness_label""].float()\n    centroid_reg_loss1 = torch.sum(dist1 * objectness_label) / (torch.sum(objectness_label) + 1e-6)\n    centroid_reg_loss2 = torch.sum(dist2 * box_label_mask) / (torch.sum(box_label_mask) + 1e-6)\n    center_loss = centroid_reg_loss1 + centroid_reg_loss2\n\n    # Compute heading loss\n    heading_class_label = torch.gather(inputs[""heading_class_label""], 1, object_assignment)  # select (B,K) from (B,K2)\n    criterion_heading_class = nn.CrossEntropyLoss(reduction=""none"")\n\n    heading_class_loss = criterion_heading_class(\n        outputs[""heading_scores""].transpose(2, 1), heading_class_label.long()\n    )  # (B,K)\n    heading_class_loss = torch.sum(heading_class_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6)\n\n    heading_residual_label = torch.gather(\n        inputs[""heading_residual_label""], 1, object_assignment\n    )  # select (B,K) from (B,K2)\n    heading_residual_normalized_label = heading_residual_label / (np.pi / num_heading_bin)\n\n    # Ref: https://discuss.pytorch.org/t/convert-int-into-one-hot-format/507/3\n    heading_label_one_hot = torch.zeros(batch_size, heading_class_label.shape[1], num_heading_bin).to(inputs.pos.device)\n    heading_label_one_hot.scatter_(\n        2, heading_class_label.unsqueeze(-1).long(), 1\n    )  # src==1 so it\'s *one-hot* (B,K,num_heading_bin) TODO change that for pytorch OneHot\n    heading_residual_normalized_loss = huber_loss(\n        torch.sum(outputs[""heading_residuals_normalized""] * heading_label_one_hot, -1)\n        - heading_residual_normalized_label,\n        delta=1.0,\n    )  # (B,K)\n    heading_residual_normalized_loss = torch.sum(heading_residual_normalized_loss * objectness_label) / (\n        torch.sum(objectness_label) + 1e-6\n    )\n\n    # Compute size loss\n    size_class_label = torch.gather(inputs[""size_class_label""], 1, object_assignment)  # select (B,K) from (B,K2)\n    criterion_size_class = nn.CrossEntropyLoss(reduction=""none"")\n    size_class_loss = criterion_size_class(outputs[""size_scores""].transpose(2, 1), size_class_label.long())  # (B,K)\n    size_class_loss = torch.sum(size_class_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6)\n\n    size_residual_label = torch.gather(\n        inputs[""size_residual_label""], 1, object_assignment.unsqueeze(-1).repeat(1, 1, 3)\n    )  # select (B,K,3) from (B,K2,3)\n\n    size_label_one_hot = torch.zeros(batch_size, size_class_label.shape[1], num_size_cluster).to(inputs.pos.device)\n    size_label_one_hot.scatter_(\n        2, size_class_label.unsqueeze(-1).long(), 1\n    )  # src==1 so it\'s *one-hot* (B,K,num_size_cluster)\n    size_label_one_hot_tiled = size_label_one_hot.unsqueeze(-1).repeat(1, 1, 1, 3)  # (B,K,num_size_cluster,3)\n    predicted_size_residual_normalized = torch.sum(\n        outputs[""size_residuals_normalized""] * size_label_one_hot_tiled, 2\n    )  # (B,K,3)\n\n    mean_size_arr_expanded = (\n        torch.from_numpy(mean_size_arr.astype(np.float32)).unsqueeze(0).unsqueeze(0).to(inputs.pos.device)\n    )  # (1,1,num_size_cluster,3)\n    mean_size_label = torch.sum(size_label_one_hot_tiled * mean_size_arr_expanded, 2)  # (B,K,3)\n    size_residual_label_normalized = size_residual_label / mean_size_label  # (B,K,3)\n    size_residual_normalized_loss = torch.mean(\n        huber_loss(predicted_size_residual_normalized - size_residual_label_normalized, delta=1.0), -1\n    )  # (B,K,3) -> (B,K)\n    size_residual_normalized_loss = torch.sum(size_residual_normalized_loss * objectness_label) / (\n        torch.sum(objectness_label) + 1e-6\n    )\n\n    # 3.4 Semantic cls loss\n    sem_cls_label = torch.gather(inputs[""sem_cls_label""], 1, object_assignment)  # select (B,K) from (B,K2)\n    criterion_sem_cls = nn.CrossEntropyLoss(reduction=""none"")\n    sem_cls_loss = criterion_sem_cls(outputs[""sem_cls_scores""].transpose(2, 1), sem_cls_label.long())  # (B,K)\n    sem_cls_loss = torch.sum(sem_cls_loss * objectness_label) / (torch.sum(objectness_label) + 1e-6)\n\n    return (\n        center_loss,\n        heading_class_loss,\n        heading_residual_normalized_loss,\n        size_class_loss,\n        size_residual_normalized_loss,\n        sem_cls_loss,\n    )\n\n\ndef get_loss(inputs, outputs, loss_params):\n\n    losses = {}\n    metrics = {}\n    labels = {}\n\n    # Vote loss\n    vote_loss = compute_vote_loss(inputs, outputs)\n    losses[""vote_loss""] = vote_loss\n\n    # Obj loss\n    objectness_loss, objectness_label, objectness_mask, object_assignment = compute_objectness_loss(\n        inputs, outputs, loss_params\n    )\n    losses[""objectness_loss""] = objectness_loss\n    inputs[""objectness_label""] = objectness_label\n    inputs[""objectness_mask""] = objectness_mask\n    inputs[""object_assignment""] = object_assignment\n    total_num_proposal = objectness_label.shape[0] * objectness_label.shape[1]\n    metrics[""pos_ratio""] = torch.sum(objectness_label.float()) / float(total_num_proposal)\n    metrics[""neg_ratio""] = torch.sum(objectness_mask.float()) / float(total_num_proposal) - metrics[""pos_ratio""]\n\n    # Box loss and sem cls loss\n    (\n        center_loss,\n        heading_cls_loss,\n        heading_reg_loss,\n        size_cls_loss,\n        size_reg_loss,\n        sem_cls_loss,\n    ) = compute_box_and_sem_cls_loss(inputs, outputs, loss_params)\n    losses[""center_loss""] = center_loss\n    losses[""heading_cls_loss""] = heading_cls_loss\n    losses[""heading_reg_loss""] = heading_reg_loss\n    losses[""size_cls_loss""] = size_cls_loss\n    losses[""size_reg_loss""] = size_reg_loss\n    losses[""sem_cls_loss""] = sem_cls_loss\n    box_loss = center_loss + 0.1 * heading_cls_loss + heading_reg_loss + 0.1 * size_cls_loss + size_reg_loss\n    losses[""box_loss""] = box_loss\n\n    # Final loss function\n    loss = vote_loss + 0.5 * objectness_loss + box_loss + 0.1 * sem_cls_loss\n    loss *= 10  # TODO WHY???\n    losses[""loss""] = loss\n\n    # --------------------------------------------\n    # Some other statistics\n    obj_pred_val = torch.argmax(outputs[""objectness_scores""], 2)  # B,K\n    obj_acc = torch.sum((obj_pred_val == objectness_label.long()).float() * objectness_mask) / (\n        torch.sum(objectness_mask) + 1e-6\n    )\n    metrics[""obj_acc""] = obj_acc\n    # TODO move metrics to the tracker\n    labels[""objectness_label""] = objectness_label\n    labels[""objectness_mask""] = objectness_mask\n\n    return losses, metrics, labels\n'"
torch_points3d/modules/VoteNet/proposal_module.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom omegaconf import OmegaConf\n\nfrom torch_points3d.modules.pointnet2 import PointNetMSGDown\nimport torch_points_kernels as tp\n\n\ndef decode_scores(data, x, num_class, num_heading_bin, num_size_cluster, mean_size_arr):\n    """""" Returns a data object with:\n        - objectness_scores - [B,N,2]\n        - center - corrected centre of the box [B,N,3]\n        - heading_scores - [B, N, num_heading_bin]\n        - heading_residuals_normalized - between -1 and 1 [B, N, num_heading_bin]\n        - heading_residual - between -PI and PI [B, N, num_heading_bin]\n        - size_scores - [B,N,num_size_cluster]\n        - size_residuals_normalized - [B,N,num_size_cluster, 3]\n        - size_residuals - [B,N,num_size_cluster, 3]\n        - sem_cls_scores - [B,N,num_classes]\n    """"""\n    x_transposed = x.transpose(2, 1)  # (batch_size, num_proposal, features)\n    batch_size = x_transposed.shape[0]\n    num_proposal = x_transposed.shape[1]\n\n    objectness_scores = x_transposed[:, :, 0:2]\n    data.objectness_scores = objectness_scores\n\n    base_xyz = data.aggregated_vote_xyz  # (batch_size, num_proposal, 3)\n    center = base_xyz + x_transposed[:, :, 2:5]  # (batch_size, num_proposal, 3)\n    data.center = center\n\n    heading_scores = x_transposed[:, :, 5 : 5 + num_heading_bin]\n    heading_residuals_normalized = x_transposed[:, :, 5 + num_heading_bin : 5 + num_heading_bin * 2]\n    data.heading_scores = heading_scores  # Bxnum_proposalxnum_heading_bin\n    data.heading_residuals_normalized = (\n        heading_residuals_normalized  # Bxnum_proposalxnum_heading_bin (should be -1 to 1) TODO check that!\n    )\n    data.heading_residuals = heading_residuals_normalized * (np.pi / num_heading_bin)  # Bxnum_proposalxnum_heading_bin\n\n    size_scores = x_transposed[:, :, 5 + num_heading_bin * 2 : 5 + num_heading_bin * 2 + num_size_cluster]\n    size_residuals_normalized = x_transposed[\n        :, :, 5 + num_heading_bin * 2 + num_size_cluster : 5 + num_heading_bin * 2 + num_size_cluster * 4\n    ].view(\n        [batch_size, num_proposal, num_size_cluster, 3]\n    )  # Bxnum_proposalxnum_size_clusterx3\n    data.size_scores = size_scores\n    data.size_residuals_normalized = size_residuals_normalized\n    data.size_residuals = size_residuals_normalized * mean_size_arr.unsqueeze(0).unsqueeze(0)\n\n    sem_cls_scores = x_transposed[:, :, 5 + num_heading_bin * 2 + num_size_cluster * 4 :]  # Bxnum_proposalx10\n    data.sem_cls_scores = sem_cls_scores\n    return data\n\n\nclass ProposalModule(nn.Module):\n    def __init__(\n        self,\n        num_class,\n        vote_aggregation_config,\n        num_heading_bin,\n        num_size_cluster,\n        mean_size_arr,\n        num_proposal,\n        sampling,\n        seed_feat_dim=256,\n    ):\n        super().__init__()\n\n        self.num_class = num_class\n        self.num_heading_bin = num_heading_bin\n        self.num_size_cluster = num_size_cluster\n        self.mean_size_arr = nn.Parameter(torch.Tensor(mean_size_arr), requires_grad=False)\n        self.num_proposal = num_proposal\n        self.sampling = sampling\n        self.seed_feat_dim = seed_feat_dim\n\n        assert (\n            vote_aggregation_config.module_name == ""PointNetMSGDown""\n        ), ""Proposal Module support only PointNet2 for now""\n        params = OmegaConf.to_container(vote_aggregation_config)\n        self.vote_aggregation = PointNetMSGDown(**params)\n\n        # Object proposal/detection\n        # Objectness scores (2), center residual (3),\n        # heading class+residual (num_heading_bin*2), size class+residual(num_size_cluster*4)\n        self.conv1 = torch.nn.Conv1d(128, 128, 1)\n        self.conv2 = torch.nn.Conv1d(128, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 2 + 3 + num_heading_bin * 2 + num_size_cluster * 4 + self.num_class, 1)\n        self.bn1 = torch.nn.BatchNorm1d(128)\n        self.bn2 = torch.nn.BatchNorm1d(128)\n\n    def forward(self, data):\n        """"""\n        Args:\n            pos: (B,K,3)\n            features: (B,C,K)\n            seed_pos (B,N,3)\n        Returns:\n            scores: (B,num_proposal,2+3+NH*2+NS*4)\n        """"""\n        if data.pos.dim() != 3:\n            raise ValueError(""This method only supports dense convolutions for now"")\n        if self.sampling == ""seed_fps"":\n            sample_idx = tp.furthest_point_sample(data.seed_pos, self.num_proposal)\n        else:\n            raise ValueError(""Unknown sampling strategy: %s. Exiting!"" % (self.sampling))\n\n        data_features = self.vote_aggregation(data, sampled_idx=sample_idx)\n        data.aggregated_vote_xyz = data_features.pos  # (batch_size, num_proposal, 3)\n        data.aggregated_vote_inds = sample_idx  # (batch_size, num_proposal,) # should be 0,1,2,...,num_proposal\n\n        # --------- PROPOSAL GENERATION ---------\n        x = F.relu(self.bn1(self.conv1(data_features.x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.conv3(x)  # (batch_size, 2+3+num_heading_bin*2+num_size_cluster*4, num_proposal)\n\n        return decode_scores(data, x, self.num_class, self.num_heading_bin, self.num_size_cluster, self.mean_size_arr)\n'"
torch_points3d/modules/VoteNet/voting_module.py,8,"b'# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n"""""" Voting module: generate votes from XYZ and features of seed points.\n\nDate: July, 2019\nAuthor: Charles R. Qi and Or Litany\n""""""\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.data import Data\n\n\nclass VotingModule(nn.Module):\n    def __init__(self, vote_factor, seed_feature_dim):\n        """""" Votes generation from seed point features.\n\n        Args:\n            vote_facotr: int\n                number of votes generated from each seed point\n            seed_feature_dim: int\n                number of channels of seed point features\n            vote_feature_dim: int\n                number of channels of vote features\n        """"""\n        super().__init__()\n        self.vote_factor = vote_factor\n        self.in_dim = seed_feature_dim\n        self.out_dim = self.in_dim  # due to residual feature, in_dim has to be == out_dim\n        self.conv1 = torch.nn.Conv1d(self.in_dim, self.in_dim, 1)\n        self.conv2 = torch.nn.Conv1d(self.in_dim, self.in_dim, 1)\n        self.conv3 = torch.nn.Conv1d(self.in_dim, (3 + self.out_dim) * self.vote_factor, 1)\n        self.bn1 = torch.nn.BatchNorm1d(self.in_dim)\n        self.bn2 = torch.nn.BatchNorm1d(self.in_dim)\n\n    def forward(self, data):\n        """""" Votes for centres using a PN++ like architecture\n        Returns\n        -------\n        data:\n            - pos: position of the vote (centre of the box)\n            - x: feature of the vote (original feature + processed feature)\n            - seed_pos: position of the original point\n        """"""\n        if data.pos.dim() != 3:\n            raise ValueError(""This method only supports dense convolutions for now"")\n\n        batch_size = data.pos.shape[0]\n        num_points = data.pos.shape[1]\n        num_votes = num_points * self.vote_factor\n        x = F.relu(self.bn1(self.conv1(data.x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.conv3(x)  # (batch_size, (3+out_dim)*vote_factor, num_seed)\n\n        x = x.transpose(2, 1).view(batch_size, num_points, self.vote_factor, 3 + self.out_dim)\n        offset = x[:, :, :, 0:3]\n        vote_pos = data.pos.unsqueeze(2) + offset\n        vote_pos = vote_pos.contiguous().view(batch_size, num_votes, 3)\n\n        res_x = x[:, :, :, 3:]  # (batch_size, num_seed, vote_factor, out_dim)\n        vote_x = data.x.transpose(2, 1).unsqueeze(2) + res_x\n        vote_x = vote_x.contiguous().view(batch_size, num_votes, self.out_dim)\n        vote_x = vote_x.transpose(2, 1).contiguous()\n\n        return Data(pos=vote_pos, x=vote_x, seed_pos=data.pos)\n\n\nif __name__ == ""__main__"":\n    net = VotingModule(2, 256)\n    data_votes = net(Data(pos=torch.rand(8, 1024, 3), x=torch.rand(8, 256, 1024)))\n    print(""vote_pos"", data_votes.pos.shape)\n    print(""vote_x"", data_votes.x.shape)\n'"
torch_points3d/modules/pointnet2/__init__.py,0,b'from .dense import *\nfrom .message_passing import *\n'
torch_points3d/modules/pointnet2/dense.py,4,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch_points_kernels as tp\n\nfrom torch_points3d.core.base_conv.dense import *\nfrom torch_points3d.core.spatial_ops import DenseRadiusNeighbourFinder, DenseFPSSampler\nfrom torch_points3d.utils.model_building_utils.activation_resolver import get_activation\n\n\nclass PointNetMSGDown(BaseDenseConvolutionDown):\n    def __init__(\n        self,\n        npoint=None,\n        radii=None,\n        nsample=None,\n        down_conv_nn=None,\n        bn=True,\n        activation=torch.nn.LeakyReLU(negative_slope=0.01),\n        use_xyz=True,\n        **kwargs\n    ):\n        assert len(radii) == len(nsample) == len(down_conv_nn)\n        super(PointNetMSGDown, self).__init__(\n            DenseFPSSampler(num_to_sample=npoint), DenseRadiusNeighbourFinder(radii, nsample), **kwargs\n        )\n        self.use_xyz = use_xyz\n        self.npoint = npoint\n        self.mlps = nn.ModuleList()\n        for i in range(len(radii)):\n            self.mlps.append(MLP2D(down_conv_nn[i], bn=bn, activation=activation, bias=False))\n\n    def _prepare_features(self, x, pos, new_pos, idx):\n        new_pos_trans = pos.transpose(1, 2).contiguous()\n        grouped_pos = tp.grouping_operation(new_pos_trans, idx)  # (B, 3, npoint, nsample)\n        grouped_pos -= new_pos.transpose(1, 2).unsqueeze(-1)\n\n        if x is not None:\n            grouped_features = tp.grouping_operation(x, idx)\n            if self.use_xyz:\n                new_features = torch.cat([grouped_pos, grouped_features], dim=1)  # (B, C + 3, npoint, nsample)\n            else:\n                new_features = grouped_features\n        else:\n            assert self.use_xyz, ""Cannot have not features and not use xyz as a feature!""\n            new_features = grouped_pos\n\n        return new_features\n\n    def conv(self, x, pos, new_pos, radius_idx, scale_idx):\n        """""" Implements a Dense convolution where radius_idx represents\n        the indexes of the points in x and pos to be agragated into the new feature\n        for each point in new_pos\n\n        Arguments:\n            x -- Previous features [B, N, C]\n            pos -- Previous positions [B, N, 3]\n            new_pos  -- Sampled positions [B, npoints, 3]\n            radius_idx -- Indexes to group [B, npoints, nsample]\n            scale_idx -- Scale index in multiscale convolutional layers\n        Returns:\n            new_x -- Features after passing trhough the MLP [B, mlp[-1], npoints]\n        """"""\n        assert scale_idx < len(self.mlps)\n        new_features = self._prepare_features(x, pos, new_pos, radius_idx)\n        new_features = self.mlps[scale_idx](new_features)  # (B, mlp[-1], npoint, nsample)\n        new_features = F.max_pool2d(new_features, kernel_size=[1, new_features.size(3)])  # (B, mlp[-1], npoint, 1)\n        new_features = new_features.squeeze(-1)  # (B, mlp[-1], npoint)\n        return new_features\n'"
torch_points3d/modules/pointnet2/message_passing.py,0,"b'from torch_geometric.nn import PointConv\n\nfrom torch_points3d.core.base_conv.base_conv import *\nfrom torch_points3d.core.base_conv.message_passing import *\nfrom torch_points3d.core.common_modules.base_modules import *\nfrom torch_points3d.core.spatial_ops import FPSSampler, RandomSampler, MultiscaleRadiusNeighbourFinder\n\n\nclass SAModule(BaseMSConvolutionDown):\n    def __init__(self, ratio=None, radius=None, radius_num_point=None, down_conv_nn=None, *args, **kwargs):\n        super(SAModule, self).__init__(\n            FPSSampler(ratio=ratio),\n            MultiscaleRadiusNeighbourFinder(radius, max_num_neighbors=radius_num_point),\n            *args,\n            **kwargs\n        )\n\n        local_nn = MLP(down_conv_nn) if down_conv_nn is not None else None\n\n        self._conv = PointConv(local_nn=local_nn, global_nn=None)\n        self._radius = radius\n        self._ratio = ratio\n        self._num_points = radius_num_point\n\n    def conv(self, x, pos, edge_index, batch):\n        return self._conv(x, pos, edge_index)\n\n    def extra_repr(self):\n        return ""{}(ratio {}, radius {}, radius_points {})"".format(\n            self.__class__.__name__, self._ratio, self._radius, self._num_points\n        )\n'"
torch_points3d/utils/model_building_utils/activation_resolver.py,3,"b'import torch.nn\n\nfrom torch_points3d.utils.config import is_dict\n\n\ndef get_activation(act_opt, create_cls=True):\n    if is_dict(act_opt):\n        act_opt = dict(act_opt)\n        act = getattr(torch.nn, act_opt[""name""])\n        del act_opt[""name""]\n        args = dict(act_opt)\n    else:\n        act = getattr(torch.nn, act_opt)\n        args = {}\n\n    if create_cls:\n        return act(**args)\n    else:\n        return act\n'"
torch_points3d/utils/model_building_utils/model_definition_resolver.py,0,"b'from omegaconf.dictconfig import DictConfig\nfrom omegaconf.listconfig import ListConfig\n\n\ndef resolve_model(model_config, dataset, tested_task):\n    """""" Parses the model config and evaluates any expression that may contain constants\n    """"""\n    # placeholders to subsitute\n    constants = {\n        ""FEAT"": max(dataset.feature_dimension, 0),\n        ""TASK"": tested_task,\n        ""N_CLS"": dataset.num_classes if hasattr(dataset, ""num_classes"") else None,\n    }\n\n    # user defined contants to subsitute\n    if ""define_constants"" in model_config.keys():\n        constants.update(dict(model_config.define_constants))\n\n    resolve(model_config, constants)\n\n\ndef resolve(obj, constants):\n    """""" Resolves expressions and constants in obj.\n    returns False if obj is a ListConfig or DictConfig, True is obj is a primative type.\n    """"""\n    if type(obj) == DictConfig:\n        it = (k for k in obj)\n    elif type(obj) == ListConfig:\n        it = range(len(obj))\n    else:\n        # obj is a single element\n        return True\n\n    # recursively resolve all children of obj\n    for k in it:\n\n        # if obj[k] is a primative type, evalulate it\n        if resolve(obj[k], constants):\n            if type(obj[k]) is str:\n                try:\n                    obj[k] = eval(obj[k], constants)\n                except NameError:\n                    # we tried to resolve a string which isn\'t an expression\n                    pass\n                except ValueError:\n                    # we tried to resolve a string which is also a builtin (e.g. max)\n                    pass\n                except Exception as e:\n                    print(e)\n\n    return False\n'"
torch_points3d/utils/model_building_utils/resolver_utils.py,0,"b'import collections\n\n# from https://stackoverflow.com/questions/6027558/flatten-nested-dictionaries-compressing-keys\n# flattens nested dicts to a single dict, with keys concatenated\n# e.g. flatten_dict({\'a\': 1, \'c\': {\'a\': 2, \'b\': {\'x\': 5, \'y\' : 10}}, \'d\': [1, 2, 3]})\n# {\'a\': 1, \'c_a\': 2, \'c_b_x\': 5, \'d\': [1, 2, 3], \'c_b_y\': 10}\ndef flatten_dict(d, parent_key="""", sep=""_""):\n    items = []\n    for k, v in d.items():\n        new_key = parent_key + sep + k if parent_key else k\n        if isinstance(v, collections.abc.MutableMapping):\n            items.extend(flatten_dict(v, new_key, sep=sep).items())\n        else:\n            items.append((new_key, v))\n    return dict(items)\n'"
torch_points3d/datasets/segmentation/forward/__init__.py,0,b''
torch_points3d/datasets/segmentation/forward/shapenet.py,4,"b'import torch\nimport glob\nimport os\nfrom torch_geometric.io import read_txt_array\nfrom torch_geometric.data.data import Data\nimport torch_geometric.transforms as T\nfrom torch_geometric.nn import knn_interpolate\nimport numpy as np\nimport logging\n\nfrom torch_points3d.core.data_transform import SaveOriginalPosId\nfrom torch_points3d.utils import is_list\nfrom torch_points3d.datasets.base_dataset import BaseDataset\nfrom torch_points3d.metrics.shapenet_part_tracker import ShapenetPartTracker\nfrom torch_points3d.datasets.segmentation.shapenet import ShapeNet\n\nlog = logging.getLogger(__name__)\n\n\nclass _ForwardShapenet(torch.utils.data.Dataset):\n    """""" Dataset to run forward inference on Shapenet kind of data data. Runs on a whole folder.\n    Arguments:\n        path: folder that contains a set of files of a given category\n        category: index of the category to use for forward inference. This value depends on how many categories the model has been trained one.\n        transforms: transforms to be applied to the data\n        include_normals: wether to include normals for the forward inference\n    """"""\n\n    def __init__(self, path, category: int, transforms=None, include_normals=True):\n        super().__init__()\n        self._category = category\n        self._path = path\n        self._files = sorted(glob.glob(os.path.join(self._path, ""*.txt"")))\n        self._transforms = transforms\n        self._include_normals = include_normals\n        assert os.path.exists(self._path)\n        if self.__len__() == 0:\n            raise ValueError(""Empty folder %s"" % path)\n\n    def __len__(self):\n        return len(self._files)\n\n    def _read_file(self, filename):\n        raw = read_txt_array(filename)\n        pos = raw[:, :3]\n        x = raw[:, 3:6]\n        if raw.shape[1] == 7:\n            y = raw[:, 6].type(torch.long)\n        else:\n            y = None\n        return Data(pos=pos, x=x, y=y)\n\n    def get_raw(self, index):\n        """""" returns the untransformed data associated with an element\n        """"""\n        return self._read_file(self._files[index])\n\n    @property\n    def num_features(self):\n        feats = self[0].x\n        if feats is not None:\n            return feats.shape[-1]\n        return 0\n\n    def get_filename(self, index):\n        return os.path.basename(self._files[index])\n\n    def __getitem__(self, index):\n        data = self._read_file(self._files[index])\n        category = torch.ones(data.pos.shape[0], dtype=torch.long) * self._category\n        setattr(data, ""category"", category)\n        setattr(data, ""sampleid"", torch.tensor([index]))\n        if not self._include_normals:\n            data.x = None\n        if self._transforms is not None:\n            data = self._transforms(data)\n        return data\n\n\nclass ForwardShapenetDataset(BaseDataset):\n    def __init__(self, dataset_opt):\n        super().__init__(dataset_opt)\n        forward_category = dataset_opt.forward_category\n        if not isinstance(forward_category, str):\n            raise ValueError(\n                ""dataset_opt.forward_category is not set or is not a string. Current value: {}"".format(\n                    dataset_opt.forward_category\n                )\n            )\n        self._train_categories = dataset_opt.category\n        if not is_list(self._train_categories):\n            self._train_categories = [self._train_categories]\n\n        # Sets the index of the category with respect to the categories in the trained model\n        self._cat_idx = None\n        for i, train_category in enumerate(self._train_categories):\n            if forward_category.lower() == train_category.lower():\n                self._cat_idx = i\n                break\n        if self._cat_idx is None:\n            raise ValueError(\n                ""Cannot run an inference on category {} with a network trained on {}"".format(\n                    forward_category, self._train_categories\n                )\n            )\n        log.info(\n            ""Running an inference on category {} with a network trained on {}"".format(\n                forward_category, self._train_categories\n            )\n        )\n\n        self._data_path = dataset_opt.dataroot\n        include_normals = dataset_opt.include_normals if dataset_opt.include_normals else True\n\n        transforms = SaveOriginalPosId()\n        for t in [self.pre_transform, self.test_transform]:\n            if t:\n                transforms = T.Compose([transforms, t])\n        self.test_dataset = _ForwardShapenet(\n            self._data_path, self._cat_idx, transforms=transforms, include_normals=include_normals\n        )\n\n    def get_tracker(self, wandb_log: bool, tensorboard_log: bool):\n        """"""Factory method for the tracker\n        \n        Arguments:\n            wandb_log - Log using weight and biases\n            tensorboard_log - Log using tensorboard\n        Returns:\n            [BaseTracker] -- tracker\n        """"""\n        return ShapenetPartTracker(self, wandb_log=wandb_log, use_tensorboard=tensorboard_log)\n\n    def predict_original_samples(self, batch, conv_type, output):\n        """""" Takes the output generated by the NN and upsamples it to the original data\n        Arguments:\n            batch -- processed batch\n            conv_type -- Type of convolutio (DENSE, PARTIAL_DENSE, etc...)\n            output -- output predicted by the model\n        """"""\n        full_res_results = {}\n        num_sample = BaseDataset.get_num_samples(batch, conv_type)\n        if conv_type == ""DENSE"":\n            output = output.reshape(num_sample, -1, output.shape[-1])  # [B,N,L]\n\n        setattr(batch, ""_pred"", output)\n        for b in range(num_sample):\n            sampleid = batch.sampleid[b]\n            sample_raw_pos = self.test_dataset[0].get_raw(sampleid).pos.to(output.device)\n            predicted = BaseDataset.get_sample(batch, ""_pred"", b, conv_type)\n            origindid = BaseDataset.get_sample(batch, SaveOriginalPosId.KEY, b, conv_type)\n            full_prediction = knn_interpolate(predicted, sample_raw_pos[origindid], sample_raw_pos, k=3)\n            labels = full_prediction.max(1)[1].unsqueeze(-1)\n            full_res_results[self.test_dataset[0].get_filename(sampleid)] = np.hstack(\n                (sample_raw_pos.cpu().numpy(), labels.cpu().numpy(),)\n            )\n        return full_res_results\n\n    @property\n    def class_to_segments(self):\n        classes_to_segment = {}\n        for key in self._train_categories:\n            classes_to_segment[key] = ShapeNet.seg_classes[key]\n        return classes_to_segment\n\n    @property\n    def num_classes(self):\n        segments = self.class_to_segments.values()\n        num = 0\n        for seg in segments:\n            num = max(num, max(seg))\n        return num + 1\n'"
