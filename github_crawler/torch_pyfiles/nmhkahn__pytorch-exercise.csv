file_path,api_count,code
codes/cdcgan/net.py,2,"b'import torch\nimport torch.nn as nn\n\ndef transpose_conv(in_channels, out_channels, \n                   kernel_size, stride=2, padding=1, \n                   act=nn.LeakyReLU(0.05, True),\n                   bn=True):\n    layers = list()\n\n    layers += [nn.ConvTranspose2d(\n        in_channels, out_channels, kernel_size, stride, padding)\n    ]\n    if bn:\n        layers += [nn.BatchNorm2d(out_channels)]\n    if act:\n        layers += [act]\n\n    return nn.Sequential(*layers)\n    \n\ndef conv(in_channels, out_channels, \n         kernel_size, stride=2, padding=1, \n         act=nn.ReLU(True),\n         bn=True):\n    layers = list()\n\n    layers += [nn.Conv2d(\n        in_channels, out_channels, kernel_size, stride, padding)\n    ]\n    if bn:\n        layers += [nn.BatchNorm2d(out_channels)]\n    if act:\n        layers += [act]\n\n    return nn.Sequential(*layers)\n\n\nclass Generator(nn.Module):\n    def __init__(self, num_class=10, z_dim=100):\n        super(Generator, self).__init__()\n\n        self.fc = transpose_conv(num_class+z_dim, 32, 7, 1, 0, bn=False)\n        self.t_conv1 = transpose_conv(32, 64, 4)\n        self.t_conv2 = transpose_conv(64, 128, 4)\n        self.conv3   = conv(128, 1, 3, 1, 1, act=nn.Tanh(), bn=False)\n\n    def forward(self, y, z):\n        latent = torch.cat([y, z], dim=1)\n        latent = latent.view(latent.size(0), latent.size(1), 1, 1)\n        \n        out = self.fc(latent)\n        out = self.t_conv1(out)\n        out = self.t_conv2(out)\n        out = self.conv3(out)\n\n        return out\n    \n\nclass Discriminator(nn.Module):\n    def __init__(self, num_class=10):\n        super(Discriminator, self).__init__()\n\n        self.conv1 = conv(1, 32, 4, bn=False)\n        self.conv2 = conv(32, 64, 4)\n        self.fc_adv = conv(64, 2, 7, 1, 0, act=None, bn=False)\n        self.fc_cls = conv(64, num_class, 7, 1, 0, act=None, bn=False)\n        \n    def forward(self, x):\n        out = x\n        out = self.conv1(out)\n        out = self.conv2(out)\n\n        adv = self.fc_adv(out).squeeze()\n        cls = self.fc_cls(out).squeeze()\n\n        return adv, cls\n'"
codes/cdcgan/solver.py,16,"b'import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torchvision.utils import save_image\nfrom net import *\n\nclass Solver():\n    def __init__(self, args):\n        # define normalize transformation\n        transform = transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize(mean=(0.5, 0.5, 0.5), \n                                     std=(0.5, 0.5, 0.5))\n        ])\n\n        # prepare fashion MNIST dataset\n        self.train_dataset = datasets.FashionMNIST(\n            root=args.data_root,\n            train=True, \n            transform=transform, \n            download=True)\n \n        self.train_loader = torch.utils.data.DataLoader(\n            dataset=self.train_dataset,\n            batch_size=args.batch_size, \n            shuffle=True)\n        \n        self.G = Generator(z_dim=args.z_dim)\n        self.D = Discriminator()\n        \n        # cudafy if available\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.G = self.G.to(self.device)\n        self.D = self.D.to(self.device)\n\n        self.loss_fn = nn.CrossEntropyLoss()\n        self.optim_G = torch.optim.Adam(self.G.parameters(), args.lr)\n        self.optim_D = torch.optim.Adam(self.D.parameters(), args.lr)\n        \n        self.args = args\n        \n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        if not os.path.exists(args.result_dir):\n            os.makedirs(args.result_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.G.train()\n            self.D.train()\n            for step, inputs in enumerate(self.train_loader):\n                batch_size = inputs[0].size(0)\n\n                images = inputs[0].to(self.device)\n                labels = inputs[1].to(self.device)\n                \n                # create the labels used to distingush real or fake\n                real_labels = torch.ones(batch_size, dtype=torch.int64).to(self.device)\n                fake_labels = torch.zeros(batch_size, dtype=torch.int64).to(self.device)\n                \n                # train the discriminator\n                \n                # discriminator <- real image\n                D_real, D_real_cls = self.D(images)\n                D_loss_real = self.loss_fn(D_real, real_labels)\n                D_loss_real_cls = self.loss_fn(D_real_cls, labels)\n                \n                # noise vector\n                z = torch.randn(batch_size, args.z_dim).to(self.device)\n\n                # make label to onehot vector\n                y_onehot = torch.zeros((batch_size, 10)).to(self.device)\n                y_onehot.scatter_(1, labels.unsqueeze(1), 1)\n                y_onehot.requires_grad_(False)\n                \n                # discriminator <- fake image\n                G_fake = self.G(y_onehot, z)\n                D_fake, D_fake_cls = self.D(G_fake)\n                D_loss_fake = self.loss_fn(D_fake, fake_labels)\n                D_loss_fake_cls = self.loss_fn(D_fake_cls, labels)\n                \n                D_loss = D_loss_real + D_loss_fake + \\\n                         D_loss_real_cls + D_loss_fake_cls\n                self.D.zero_grad()\n                D_loss.backward()\n                self.optim_D.step()\n                \n                # train the generator\n\n                z = torch.randn(batch_size, args.z_dim).to(self.device)\n                G_fake = self.G(y_onehot, z)\n                D_fake, D_fake_cls = self.D(G_fake)\n                \n                G_loss = self.loss_fn(D_fake, real_labels) + \\\n                         self.loss_fn(D_fake_cls, labels)\n                self.G.zero_grad()\n                G_loss.backward()\n                self.optim_G.step()\n\n            if (epoch+1) % args.print_every == 0:\n                print(""Epoch [{}/{}] Loss_D: {:.3f}, Loss_G: {:.3f}"".\n                    format(epoch+1, args.max_epochs, D_loss.item(), G_loss.item()))\n                self.save(args.ckpt_dir, epoch+1)\n                self.sample(epoch+1)\n\n                \n    def sample(self, global_step=0):\n        self.G.eval()\n        self.D.eval()\n        \n        args = self.args\n        batch_size = args.batch_size\n                \n        # produce the samples among 10-classes\n        with torch.no_grad():\n            for i in range(10):\n                z = torch.randn(batch_size, args.z_dim).to(self.device)\n                labels = torch.full((batch_size,), i, dtype=torch.int64).to(self.device)\n            \n                # make label to onehot vector\n                y_onehot = torch.zeros((batch_size, 10)).to(self.device)\n                y_onehot.scatter_(1, labels.unsqueeze(1), 1)\n                y_onehot.requires_grad_(False)\n\n                G_fake = self.G(y_onehot, z)\n\n                # save the results\n                save_image(denormalize(G_fake.detach()),\n                    os.path.join(args.result_dir, ""fake_{}_{}.png"".format(global_step, i)))\n\n    def save(self, ckpt_dir, global_step):\n        D_path = os.path.join(\n            ckpt_dir, ""discriminator_{}.pth"".format(global_step))\n        G_path = os.path.join(\n            ckpt_dir, ""generator_{}.pth"".format(global_step))\n\n        torch.save(self.D.state_dict(), D_path)\n        torch.save(self.G.state_dict(), G_path)\n\n\ndef denormalize(tensor):\n    out = (tensor + 1) / 2\n    return out.clamp(0, 1)\n\n'"
codes/cdcgan/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.0003)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--max_epochs"", type=int, default=100)\n    parser.add_argument(""--z_dim"", type=int, default=100)\n    \n    parser.add_argument(""--print_every"", type=int, default=1)\n    parser.add_argument(""--result_dir"", type=str, default=""./result"")\n    parser.add_argument(""--ckpt_dir"", type=str, default=""./checkpoint"")\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n\n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/char_rnn/net.py,2,"b""import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self,\n                 vocab_size, embed_dim=300,\n\t\t         hidden_dim=512, num_layers=1):\n        super().__init__()\n\n        self.hidden_dim = hidden_dim\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.encoder = nn.LSTM(embed_dim, hidden_dim, \n                               num_layers=num_layers)\n\n        self.decoder = nn.Linear(hidden_dim, vocab_size)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        embed = self.embedding(x)\n        out, hidden = self.encoder(embed)\n\n        out = self.decoder(out)\n        out = out.view(-1, out.size(2))\n\n        return out, hidden\n    \n    def sample(self, prime, length):\n        # NOTE:\n        # For char-RNN, behaviors of train and sample phase are different.\n        # e.g. The model can't see the the ground-truth sentence in the sample phase.\n        #      So, the input of each time-step has to be the output of previous time.\n        # To handle it, RNN model is implemented by for-loop to unroll it.\n        indices = list()\n        \n        # prepare the first hidden state\n        out, hidden = self.forward(prime)\n\n        # hidden state of last step\n        h_0 = hidden[0][:,-1,:].contiguous().view(-1, 1, self.hidden_dim)\n        c_0 = hidden[1][:,-1,:].contiguous().view(-1, 1, self.hidden_dim)\n        hidden = (h_0, c_0)\n\n        x = prime[:, -1]\n        for t in range(length):\n            embed = self.embedding(x)\n            embed = embed.view(1, 1, -1)\n\n            out, hidden = self.encoder(embed, hidden) \n            out = self.decoder(out)\n            out = out.view(-1, out.size(2))\n            \n            _, argmax = torch.max(out, 1)\n            indices.append(argmax)\n\n            x = argmax # previous output is current output\n\n        return indices\n"""
codes/char_rnn/solver.py,5,"b'import os\nimport numpy as np\nimport torch\nfrom net import Net\nfrom utils import *\n\nclass Solver():\n    def __init__(self, args):\n        # prepare shakespeare dataset\n        train_iter, data_info = load_shakespeare(args.batch_size, args.bptt_len)\n        self.vocab_size = data_info[""vocab_size""]\n        self.TEXT = data_info[""TEXT""]\n        \n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        \n        self.net = Net(self.vocab_size, args.embed_dim, \n                       args.hidden_dim, args.num_layers).to(self.device)\n        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1) # <pad>: 1\n        self.optim   = torch.optim.Adam(self.net.parameters(), args.lr)\n        \n        self.args = args\n        self.train_iter = train_iter\n        \n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_iter):\n                X = inputs.text.to(self.device)\n                y = inputs.target.to(self.device)\n\n                out, _ = self.net(X)\n                loss = self.loss_fn(out, y.view(-1))\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n            \n            if (epoch+1) % args.print_every == 0:\n                text = self.sample(args.sample_length, args.sample_prime)\n                print(""Epoch [{}/{}] loss: {:.3f}""\n                    .format(epoch+1, args.max_epochs, loss.item()/args.bptt_len))\n                print(text, ""\\n"")\n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def sample(self, length, prime=""The""):\n        args = self.args\n\n        self.net.eval()\n        samples = list(prime)\n\n        # convert prime string to torch.LongTensor type\n        prime = self.TEXT.process(prime, device=self.device, train=False)\n        \n        # sample character indices\n        indices = self.net.sample(prime, length)\n\n        # convert char indices to string type\n        for index in indices:\n            out = self.TEXT.vocab.itos[index.item()]\n            samples.append(out.replace(""<eos>"", ""\\n""))\n        \n        self.TEXT.sequential = True\n\n        return """".join(samples)\n                \n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/char_rnn/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--batch_size"", type=int, default=128)\n    parser.add_argument(""--max_epochs"", type=int, default=200)\n    parser.add_argument(""--bptt_len"", type=int, default=30)\n    \n    parser.add_argument(""--embed_dim"", type=int, default=300)\n    parser.add_argument(""--hidden_dim"", type=int, default=512)\n    parser.add_argument(""--num_layers"", type=int, default=1)\n    \n    parser.add_argument(""--sample_prime"", type=str, default=""All:"")\n    parser.add_argument(""--sample_length"", type=int, default=500)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""char-rnn"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/char_rnn/utils.py,0,"b'import torch\nimport torchtext.data as data\nimport torchtext.datasets as datasets\n\nclass Shakespeare(datasets.LanguageModelingDataset):\n    urls = [""https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt""]\n    name = ""tinyshakespeare""\n    dirname = ""./""\n\n    @classmethod\n    def splits(cls, \n               text_field, \n               root=""./data"", train=""input.txt"",\n               **kwargs):\n        return super(Shakespeare, cls).splits(\n            root=root, train=train,\n            text_field=text_field, **kwargs)\n\n    @classmethod\n    def iters(cls, \n              batch_size=32, bptt_len=35, \n              root=""./data"", \n              repeat=False,\n              **kwargs):\n        TEXT = data.Field(sequential=True, tokenize=list)\n\n        train, = cls.splits(TEXT, root=root, **kwargs)\n        TEXT.build_vocab(train)\n\n        return TEXT, data.BPTTIterator.splits(\n            (train, ), batch_size=batch_size, bptt_len=bptt_len, repeat=repeat)\n\n\ndef load_shakespeare(batch_size, bptt_len):\n    TEXT, (train_iter,) = Shakespeare.iters(\n        batch_size=batch_size,\n        bptt_len=bptt_len,\n        repeat=False)\n\n    data_info = {\n        ""vocab_size"": len(TEXT.vocab),\n        ""TEXT"": TEXT\n    }\n\n    return train_iter, data_info\n'"
codes/colorization/dataset.py,3,"b'import os\nimport glob\nimport numpy as np\nfrom PIL import Image\nfrom skimage.color import rgb2lab, rgb2gray\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom utils import *\n\nclass Dataset(data.Dataset):\n    def __init__(self, train, **kwargs):\n        super(Dataset, self).__init__()\n\n        self.size = kwargs.get(""size"", None)\n        self.data_root = kwargs.get(""data_root"", ""./data"")\n        \n        self._prepare_dataset(self.data_root)\n        \n        phase = ""train"" if train else ""test""\n        dirname = os.path.join(self.data_root, ""flower/{}"".format(phase))\n\n        self.paths = glob.glob(os.path.join(dirname, ""*.jpg""))\n  \n    def __getitem__(self, index):\n        image_raw = Image.open(self.paths[index])\n        \n        # resize original images\n        if self.size:\n            image_raw = image_raw.resize((self.size, self.size), Image.BICUBIC)\n            \n        image_raw = np.array(image_raw)\n        \n        # convert RGB image to Lab space\n        image_lab = rgb2lab(image_raw).astype(np.float32)\n        image_lab = (image_lab + 128) / 255\n        \n        image_ab = image_lab[:, :, 1:]\n        image_ab = torch.from_numpy(image_ab.transpose((2, 0, 1)))\n        \n        image_gray = rgb2gray(image_raw).astype(np.float32)\n        image_gray = torch.from_numpy(image_gray)\n\n        return image_gray, image_ab\n\n    def __len__(self):\n        return len(self.paths)\n\n    def _prepare_dataset(self, data_root):\n        check = os.path.join(data_root, ""flower"")\n        if not os.path.isdir(check):\n            download_and_convert(data_root)'"
codes/colorization/net.py,2,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        layers = list()\n        layers += [nn.Conv2d(1, 64, 3, 2, 1), nn.BatchNorm2d(64), nn.ReLU()]\n        layers += [nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()]\n        \n        layers += [nn.Conv2d(128, 128, 3, 2, 1), nn.BatchNorm2d(128), nn.ReLU()]\n        layers += [nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU()]\n        \n        layers += [nn.Conv2d(256, 256, 3, 2, 1), nn.BatchNorm2d(256), nn.ReLU()]\n        layers += [nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU()]\n        \n        layers += [nn.ConvTranspose2d(512, 256, 4, 2, 1), nn.BatchNorm2d(256), nn.ReLU()]\n        layers += [nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU()]\n        \n        layers += [nn.ConvTranspose2d(256, 128, 4, 2, 1), nn.BatchNorm2d(128), nn.ReLU()]\n        layers += [nn.Conv2d(128, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU()]\n        \n        layers += [nn.ConvTranspose2d(128, 64, 4, 2, 1), nn.BatchNorm2d(64), nn.ReLU()]\n        layers += [nn.Conv2d(64, 2, 3, 1, 1)]\n        \n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.layers(x.unsqueeze(1))\n\n        return out'"
codes/colorization/solver.py,5,"b'import os\nimport numpy as np\nfrom skimage.color import lab2rgb\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom net import Net\nfrom dataset import Dataset\n\nclass Solver():\n    def __init__(self, args):      \n        # prepare a datasets\n        self.train_data = Dataset(train=True,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.test_data  = Dataset(train=False,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.train_loader = DataLoader(self.train_data,\n                                       batch_size=args.batch_size,\n                                       num_workers=1,\n                                       shuffle=True, drop_last=True)\n        \n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net     = Net().to(self.device)\n        self.loss_fn = torch.nn.L1Loss()\n        self.optim   = torch.optim.Adam(self.net.parameters(), args.lr)\n        \n        self.args = args\n        \n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_loader):\n                gt_gray = inputs[0].to(self.device)\n                gt_ab   = inputs[1].to(self.device)\n                \n                pred_ab = self.net(gt_gray)\n                loss = self.loss_fn(pred_ab, gt_ab)\n                \n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n            if (epoch+1) % args.print_every == 0:\n                print(""Epoch [{}/{}] loss: {:.6f}"".format(epoch+1, args.max_epochs, loss.item()))\n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/colorization/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.0001)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--max_epochs"", type=int, default=200)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""./checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""color"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    parser.add_argument(""--image_size"", type=int, default=128)\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n\n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/colorization/utils.py,0,"b'import os\nimport glob\nimport shutil\nimport tarfile\nimport urllib.request\nimport scipy.misc as misc\n\nurl = ""http://download.tensorflow.org/example_images/flower_photos.tgz""\ndirnames = [""daisy"", ""dandelion"", ""roses"", ""sunflowers"", ""tulips""]\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath)\n    tarfile.open(filepath, ""r:gz"").extractall(dataset_dir)\n\n\ndef download_and_convert(data_root):\n    if not os.path.exists(data_root):\n        os.makedirs(data_root)\n\n    if not os.path.exists(os.path.join(data_root, ""flower_photos"")):\n        print(""[!] Downloading images..."")\n        download_and_uncompress_tarball(url, data_root)\n\n    data_dir = os.path.join(data_root, ""flower"")\n    \n    if not os.path.exists(os.path.join(data_dir, ""train"")):\n        os.makedirs(os.path.join(data_dir, ""train""))\n    if not os.path.exists(os.path.join(data_dir, ""test"")):\n        os.makedirs(os.path.join(data_dir, ""test""))\n    \n    print(""[!] Converting images..."")\n    for dirname in dirnames:\n        paths = glob.glob(os.path.join(\n            data_root, ""flower_photos"", dirname, ""*.jpg""))\n        \n        # training data\n        for path in paths[:-10]:\n            new_path = os.path.join(data_dir, ""train"",\n                ""{}_{}"".format(dirname, path.split(""/"")[-1]))\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n\n        # test data\n        for path in paths[-10:]:\n            new_path = os.path.join(data_dir, ""test"",\n                ""{}_{}"".format(dirname, path.split(""/"")[-1]))\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n'"
codes/flower_cls/dataset.py,1,"b'import os\nimport csv\nimport glob\nimport random\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom utils import *\n\nclass Dataset(data.Dataset):\n    # mapping table of label and index\n    str2label = {""daisy"": 0, ""dandelion"": 1, ""roses"": 2, ""sunflowers"": 3, ""tulips"": 4}\n    label2str = {0: ""daisy"", 1: ""dandelion"", 2: ""roses"", 3: ""sunflowers"", 4: ""tulips""}\n\n    def __init__(self, train, **kwargs):\n        super(Dataset, self).__init__()\n\n        self.data = list()\n        self.size = kwargs.get(""size"", None)\n        self.data_root = kwargs.get(""data_root"", ""./data"")\n        \n        self._prepare_dataset(self.data_root)\n        \n        # load csv file and import file paths\n        csv_name = ""train.csv"" if train else ""test.csv""\n        with open(os.path.join(self.data_root, ""flower"", csv_name)) as f:\n            reader = csv.reader(f)\n            for line in reader:\n                self.data.append(line)\n\n        self.transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n\n    def __getitem__(self, index):\n        path, label = self.data[index]\n        image = Image.open(path)\n        \n        # resize input images\n        if self.size:\n            image = image.resize((self.size, self.size), Image.BICUBIC)\n\n        label = self.str2label[label]\n\n        return self.transform(image), label\n\n    def __len__(self):\n        return len(self.data)\n\n    def _prepare_dataset(self, data_root):\n        check = os.path.join(data_root, ""flower"")\n        if not os.path.isdir(check):\n            download_and_convert(data_root)\n'"
codes/flower_cls/net.py,1,"b'import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(3, 64, 3, 1, 1),\n            nn.ReLU(),\n        )\n        self.block1 = self._make_block(64, 128, 2)\n        self.block2 = self._make_block(128, 256, 2)\n        self.block3 = self._make_block(256, 512, 2)\n\n        self.max_pool = nn.MaxPool2d(2)\n        self.fc1 = nn.Sequential(\n            nn.Linear(512*8*8, 512),\n            nn.BatchNorm1d(512),\n            nn.ReLU(inplace=True)\n        )\n        self.fc2 = nn.Linear(512, 5)\n\n    def _make_block(self, \n                    in_channels, out_channels, \n                    num_layers=2):\n        layers = list()\n        for i in range(num_layers):\n            layers += [\n                nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n                nn.BatchNorm2d(out_channels),\n                nn.ReLU(inplace=True),\n            ]\n            in_channels = out_channels\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.max_pool(out)\n        \n        out = self.block1(out)\n        out = self.max_pool(out)\n        \n        out = self.block2(out)\n        out = self.max_pool(out)\n        \n        out = self.block3(out)\n        out = self.max_pool(out)\n        \n        out = out.view(out.size(0), -1)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n'"
codes/flower_cls/solver.py,7,"b'import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom net import Net\nfrom dataset import Dataset\n\nclass Solver():\n    def __init__(self, args):\n        # prepare a dataset\n        self.train_data = Dataset(train=True,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.test_data  = Dataset(train=False,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.train_loader = DataLoader(self.train_data,\n                                       batch_size=args.batch_size,\n                                       num_workers=1,\n                                       shuffle=True, drop_last=True)\n        \n        # turn on the CUDA if available\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net     = Net().to(self.device)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n        self.optim   = torch.optim.Adam(self.net.parameters(), args.lr)\n        \n        self.args = args\n\n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_loader):\n                images = inputs[0].to(self.device)\n                labels = inputs[1].to(self.device)\n                \n                preds = self.net(images)\n                loss = self.loss_fn(preds, labels)\n                \n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n            \n            if (epoch+1) % args.print_every == 0:\n                train_acc = self.evaluate(self.train_data)\n                test_acc  = self.evaluate(self.test_data)\n\n                print(""Epoch [{}/{}] Loss: {:.3f} Train Acc: {:.3f}, Test Acc: {:.3f}"".\n                    format(epoch+1, args.max_epochs, loss.item(), train_acc, test_acc))\n                \n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def evaluate(self, data):\n        args = self.args\n        loader = DataLoader(data,\n                            batch_size=args.batch_size,\n                            num_workers=1,\n                            shuffle=False)\n\n        self.net.eval()\n        num_correct, num_total = 0, 0\n        \n        with torch.no_grad():\n            for inputs in loader:\n                images = inputs[0].to(self.device)\n                labels = inputs[1].to(self.device)\n\n                outputs = self.net(images)\n                _, preds = torch.max(outputs.detach(), 1)\n\n                num_correct += (preds == labels).sum().item()\n                num_total += labels.size(0)\n\n        return num_correct / num_total\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/flower_cls/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--batch_size"", type=int, default=32)\n    parser.add_argument(""--max_epochs"", type=int, default=30)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""./checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""flower"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    # if you change image size, you must change all the network channels\n    parser.add_argument(""--image_size"", type=int, default=128)\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n\n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/flower_cls/utils.py,0,"b'import os\nimport csv\nimport glob\nimport shutil\nimport tarfile\nimport urllib.request\nimport scipy.misc as misc\n\nurl = ""http://download.tensorflow.org/example_images/flower_photos.tgz""\ndirnames = [""daisy"", ""dandelion"", ""roses"", ""sunflowers"", ""tulips""]\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath)\n    tarfile.open(filepath, ""r:gz"").extractall(dataset_dir)\n\n\ndef download_and_convert(data_root):\n    if not os.path.exists(data_root):\n        os.makedirs(data_root)\n    \n    if not os.path.exists(os.path.join(data_root, ""flower_photos"")):\n        print(""[!] Downloading images..."")\n        download_and_uncompress_tarball(url, data_root)\n\n    data_dir = os.path.join(data_root, ""flower"")\n    if os.path.exists(os.path.join(data_dir, ""train.csv"")) or \\\n       os.path.exists(os.path.join(data_dir, ""test.csv"")):\n       return\n    \n    if not os.path.exists(os.path.join(data_dir, ""train"")):\n        os.makedirs(os.path.join(data_dir, ""train""))\n    if not os.path.exists(os.path.join(data_dir, ""test"")):\n        os.makedirs(os.path.join(data_dir, ""test""))\n    \n    print(""[!] Converting images..."")\n    # manage data with csv files\n    train_f = open(os.path.join(data_dir, ""train.csv""), ""w"")\n    test_f  = open(os.path.join(data_dir, ""test.csv""), ""w"")\n    train_writer, test_writer = csv.writer(train_f), csv.writer(test_f)\n    \n    for dirname in dirnames:\n        paths = glob.glob(os.path.join(\n            data_root, ""flower_photos"", dirname, ""*.jpg""))\n\n        num_test = int(len(paths)*0.2)\n\n        if not os.path.exists(os.path.join(data_dir, ""train"", dirname)):\n            os.makedirs(os.path.join(data_dir, ""train"", dirname))\n        if not os.path.exists(os.path.join(data_dir, ""test"", dirname)):\n            os.makedirs(os.path.join(data_dir, ""test"", dirname))\n        \n        # prepare a training data\n        for path in paths[:-num_test]:\n            new_path = os.path.join(data_dir, ""train"",\n                dirname, path.split(""/"")[-1])\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n            train_writer.writerow([new_path, dirname])\n\n        # prepare a test data\n        for path in paths[-num_test:]:\n            new_path = os.path.join(data_dir, ""test"",\n                dirname, path.split(""/"")[-1])\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n            test_writer.writerow([new_path, dirname])\n\n    train_f.close()\n    test_f.close()\n'"
codes/mnist/net.py,1,"b'import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc = nn.Linear(7*7*64, 10)\n        \n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        flat  = conv2.view(conv2.size(0), -1)\n        out = self.fc(flat)\n        return out\n'"
codes/mnist/train.py,7,"b'import torch\nimport torch.nn as nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom net import Net\n\ndef evaluate(net, loader, device):\n    net.eval()\n    num_correct, num_total = 0, 0\n\n    # same as volatile=True of the v0.3\n    with torch.no_grad():\n        for inputs in loader:\n            images = inputs[0].to(device)\n            labels = inputs[1].to(device)\n\n            outputs = net(images)\n            _, preds = torch.max(outputs.detach(), 1)\n\n            num_correct += (preds == labels).sum().item()\n            num_total += labels.size(0)\n\n    return num_correct / num_total\n\n\ndef train(args):\n    # prepare the MNIST dataset\n    train_dataset = datasets.MNIST(root=""./data/"",\n                                   train=True, \n                                   transform=transforms.ToTensor(),\n                                   download=True)\n\n    test_dataset = datasets.MNIST(root=""./data/"",\n                                  train=False, \n                                  transform=transforms.ToTensor())\n\n    # create the data loader\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size, \n                              shuffle=True, drop_last=True)\n\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size, \n                             shuffle=False)\n\n    \n    # turn on the CUDA if available\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    \n    net = Net().to(device)\n    loss_op = nn.CrossEntropyLoss()\n    optim   = torch.optim.Adam(net.parameters(), lr=args.lr)\n\n    for epoch in range(args.max_epochs):\n        net.train()\n        for step, inputs in enumerate(train_loader):\n            images = inputs[0].to(device)\n            labels = inputs[1].to(device)\n            \n            # forward-propagation\n            outputs = net(images)\n            loss = loss_op(outputs, labels)\n            \n            # back-propagation\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n        acc = evaluate(net, test_loader, device)\n        print(""Epoch [{}/{}] loss: {:.5f} test acc: {:.3f}""\n              .format(epoch+1, args.max_epochs, loss.item(), acc))\n\n    torch.save(net.state_dict(), ""mnist-final.pth"")\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--max_epochs"", type=int, default=5)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    args = parser.parse_args()\n\n    train(args)\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/show_and_tell/dataset.py,5,"b'import os\nimport csv\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchtext\nimport torchvision.transforms as transforms\n\nclass CaptionDataset(torch.utils.data.Dataset):\n    # to preprocess and numerlicalize text\n    TEXT = torchtext.data.Field(sequential=True, tokenize=""spacy"",\n                                init_token=""<start>"", eos_token=""<end>"",\n                                include_lengths=True,\n                                batch_first=True)\n    \n    def __init__(self, train, **kwargs):\n        super().__init__()\n\n        self.image_size = kwargs.get(""image_size"", None)\n        self.data_root = kwargs.get(""data_root"", ""./data"")\n        \n        phase = ""train"" if train else ""test""\n        \n        self.path, self.text = list(), list()\n        with open(os.path.join(self.data_root, ""{}.csv"".format(phase))) as f:\n            reader = csv.reader(f)\n            for line in reader:\n                self.path.append(line[0])\n                self.text.append(line[1])\n        \n        # preprocess (tokenize) text\n        for i, t in enumerate(self.text):\n            self.text[i] = CaptionDataset.TEXT.preprocess(t)\n        \n        # build vocab with GLOVE\n        # NOTE: only performed in training phase\n        if phase == ""train"":\n            CaptionDataset.TEXT.build_vocab(self.text, vectors=""glove.6B.300d"")\n        \n        # image transform function\n        self.transform = transforms.Compose([ \n            transforms.RandomHorizontalFlip(), \n            transforms.ToTensor(), \n            transforms.Normalize((0.485, 0.456, 0.406), \n                                 (0.229, 0.224, 0.225))])\n    \n    def __getitem__(self, index):\n        path, text = self.path[index], self.text[index]\n        \n        image = Image.open(os.path.join(self.data_root, path))\n        # some grayscale images are in the dataset\n        image = image.convert(""RGB"")\n\n        if self.image_size:\n            image = image.resize(\n                (self.image_size, self.image_size), \n                Image.BICUBIC)\n        \n        return self.transform(image), text\n    \n    def __len__(self):\n        return len(self.path)\n    \n    def indices_to_string(self, indices, words=False):\n        """"""Convert word indices (torch.Tensor) to sentence (string).\n\n        Args:\n            indices: torch.tensor or numpy.array of shape (T) or (T, 1)\n            words: boolean, wheter return list of words\n        Returns:\n            sentence: string type of converted sentence\n            words: (optional) list[string] type of words list\n        """"""\n        sentence = list()    \n        for idx in indices:\n            word = CaptionDataset.TEXT.vocab.itos[idx.item()]\n\n            if word in [""<pad>"", ""<start>""]: continue\n            if word in [""<end>""]: break\n\n            # no needs of space between the special symbols\n            if len(sentence) and word in [""\'"", ""."", ""?"", ""!"", "",""]:\n                sentence[-1] += word\n            else:\n                sentence.append(word)\n\n        if words:\n            return "" "".join(sentence), sentence\n        return "" "".join(sentence)\n    \n    \ndef collate_fn(data):\n    """"""Creates mini-batch tensors from the list of tuples (image, caption).\n    Reference: https://github.com/yunjey/pytorch-tutorial\n\n    Args:\n        data: list of tuple (image, caption). \n            - image: torch tensor of shape (3, size, size).\n            - caption: torch tensor of shape (?); variable length.\n    Returns:\n        images: torch tensor of shape (N, 3, image_size, image_size).\n        captions: torch tensor of shape (N, padded_length).\n        lengths: list; valid length for each padded caption.\n    """"""\n    # sort a data list by caption length (descending order).\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions = zip(*data)\n    \n    # merge images (from tuple of 3D tensor to 4D tensor).\n    images = torch.stack(images, 0)\n    \n    # add padding to match max length and numericalize caption\n    captions, lengths = CaptionDataset.TEXT.process(captions, \n        device=-1, train=True)\n    \n    return images, captions, lengths\n\n\ndef get_caption_dataset(train, \n                        data_root=""./data"", \n                        batch_size=32, image_size=224,\n                        num_workers=4, shuffle=True,\n                        text_field=False):\n    dataset = CaptionDataset(train=train, \n        image_size=image_size, data_root=data_root)\n\n    loader = torch.utils.data.DataLoader(dataset=dataset, \n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        collate_fn=collate_fn)\n    \n    if text_field:\n        return loader, dataset, CaptionDataset.TEXT\n    return loader, dataset\n'"
codes/show_and_tell/net.py,5,"b'import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\nclass Net(nn.Module):\n    def __init__(self, TEXT,\n                 hidden_dim=512, num_layers=2):\n        super().__init__()\n\n        vocab_size = TEXT.vocab.vectors.size(0)\n        embed_dim = TEXT.vocab.vectors.size(1)\n        \n        self.encoder = Encoder(embed_dim)\n        self.decoder = Decoder(TEXT,\n                               vocab_size, embed_dim,\n                               hidden_dim, num_layers)\n\n    def forward(self, image, caption, lengths):\n        feature = self.encoder(image)\n        out = self.decoder(feature, caption, lengths)\n\n        return out\n\n    def sample(self, image):\n        feature = self.encoder(image)\n        out = self.decoder.sample(feature)\n        \n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n\n        self.body = models.resnet50(pretrained=True)\n\n        for param in self.body.parameters():\n            param.requires_grad_(False)\n\n        # modify last fc layer\n        self.body.fc = nn.Linear(self.body.fc.in_features, embed_dim)\n\n    def forward(self, x):\n        return self.body(x)\n\n\nclass Decoder(nn.Module):\n    def __init__(self, TEXT,\n                 vocab_size, embed_dim,\n                 hidden_dim, num_layers):\n        super().__init__()\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.LSTM(embed_dim, hidden_dim, \n                           num_layers=num_layers,\n                           batch_first=True)\n        self.linear = nn.Linear(hidden_dim, vocab_size)\n\n        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n        self.embedding.weight.requires_grad_(False)\n\n    def forward(self, feature, caption, lengths):\n        embed = self.embedding(caption)\n        embed = torch.cat((feature.unsqueeze(1), embed), 1)\n        \n        embed = pack_padded_sequence(embed, lengths, batch_first=True)        \n        out, _ = self.rnn(embed)\n        out, _ = pad_packed_sequence(out, batch_first=True)\n   \n        out = self.linear(out)\n        out = out.view(-1, out.size(2))\n\n        return out\n\n    def sample(self, feature):\n        batch_size = feature.size(0)\n      \n        hidden = None\n        embed = feature.unsqueeze(1)\n                \n        indices = list()\n        for t in range(50):\n            out, hidden = self.rnn(embed, hidden)\n            out = self.linear(out.squeeze(1))\n\n            _, argmax = torch.max(out, 1)\n            indices.append(argmax)\n                        \n            # previous output is current input\n            embed = self.embedding(argmax).unsqueeze(1)\n                                          \n        return torch.stack(indices, 1).cpu().numpy()\n'"
codes/show_and_tell/solver.py,6,"b'import os\nimport torch\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom net import Net\nfrom dataset import get_caption_dataset\nfrom torch.nn.utils.rnn import pack_padded_sequence\nfrom visdomX import VisdomX\n\nclass Solver():\n    def __init__(self, args):\n        self.train_loader, self.train_data, TEXT = get_caption_dataset(\n            train=True,\n            data_root=args.data_root,\n            batch_size=args.batch_size, image_size=args.image_size, \n            text_field=True)\n        self.test_loader, _ = get_caption_dataset(\n            train=False,\n            data_root=args.data_root,\n            batch_size=args.batch_size, image_size=args.image_size)\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net = Net(TEXT, args.hidden_dim, args.num_layers).to(self.device)\n        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1) # <pad>: 1\n        self.optim   = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, self.net.parameters()),\n            args.lr)\n\n        self.vis = VisdomX()\n\n        self.args = args\n\n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n\n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_loader):\n                image   = inputs[0].to(self.device)\n                caption = inputs[1].to(self.device)\n                lengths = inputs[2].to(self.device)\n\n                out = self.net(image, caption, lengths)\n                loss = self.loss_fn(out, caption.view(-1))\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n            if (epoch+1) % args.print_every == 0:\n                perplexity = torch.exp(loss).item()\n                self.vis.add_scalars(perplexity, epoch,\n                                     title=""Perplexity"",\n                                     ylabel=""Perplexity"", xlabel=""Epoch"")\n\n                print(""Epoch [{}/{}] Perplexity: {:5.3f}""\n                    .format(epoch+1, args.max_epochs, perplexity))\n\n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/show_and_tell/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--batch_size"", type=int, default=128)\n    parser.add_argument(""--max_epochs"", type=int, default=200)\n    \n    parser.add_argument(""--embed_dim"", type=int, default=300)\n    parser.add_argument(""--hidden_dim"", type=int, default=512)\n    parser.add_argument(""--num_layers"", type=int, default=1)\n\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n    parser.add_argument(""--image_size"", type=int, default=224)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""caption"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/show_and_tell/visdomX.py,0,"b'import numpy as np\nfrom visdom import Visdom\n\nclass VisdomX:\n    def __init__(self):\n        self.vis = Visdom()\n        self.wins = dict()\n\n    def add_scalars(self, \n                    value, step,\n                    title="""", \n                    ylabel=""Loss"", xlabel=""Epoch""):\n        # visdom only get np.array or tensor\n        value = np.array([value])\n        step = np.array([step], dtype=np.int64)\n\n        if not self.wins.get(title):\n            win = self.vis.line(\n                Y=value, X=step,\n                opts=dict(\n                    title=title, \n                    ylabel=ylabel, \n                    xlabel=xlabel\n                )\n            )\n            self.wins[title] = win\n        else:\n            self.vis.line(\n                Y=value, X=step,\n                win=self.wins.get(title), \n                update=""append""\n            )\n\n    def add_text(self, text):\n        self.vis.text(text)\n'"
codes/show_attend_and_tell/dataset.py,5,"b'import os\nimport csv\nimport numpy as np\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nimport torch\nimport torchtext\nimport torchvision.transforms as transforms\n\nclass CaptionDataset(torch.utils.data.Dataset):\n    # to preprocess and numerlicalize text\n    TEXT = torchtext.data.Field(sequential=True, tokenize=""spacy"",\n                                init_token=""<start>"", eos_token=""<end>"",\n                                include_lengths=True,\n                                batch_first=True)\n    \n    def __init__(self, train, **kwargs):\n        super().__init__()\n\n        self.image_size = kwargs.get(""image_size"", None)\n        self.data_root = kwargs.get(""data_root"", ""./data"")\n        self.max_vocab = kwargs.get(""max_vocab"", ""10000"")\n\n        if self.max_vocab == -1:\n            self.max_vocab = None\n        \n        self.phase = ""train"" if train else ""val""\n        json_path = ""{}/annotations/captions_{}2014.json"".format(self.data_root, self.phase)\n        \n        # load data with COCO API\n        print(""[!] Prepare COCO {} dataset"".format(self.phase))\n\n        coco = COCO(json_path)\n        self.path, self.text = list(), list()\n        for key, value in coco.anns.items():\n            image_idx = value[""image_id""]\n            path = coco.loadImgs(image_idx)[0][""file_name""]\n            \n            self.text.append(value[""caption""])\n            self.path.append(path)\n\n        # preprocess (tokenize) text\n        for i, t in enumerate(self.text):\n            self.text[i] = CaptionDataset.TEXT.preprocess(t)\n        \n        # build vocab with GLOVE\n        # NOTE: only performed in training phase\n        if self.phase == ""train"":\n            CaptionDataset.TEXT.build_vocab(\n                self.text, \n                vectors=""glove.6B.300d"", \n                max_size=self.max_vocab)\n        \n        print(""[!] Dataset preparation done!"")\n        print(""\\t# of data: {}"".format(len(self.text)))\n        print(""\\tVocab size: {}\\n"".format(len(CaptionDataset.TEXT.vocab)))\n        \n        # image transform function\n        self.transform = transforms.Compose([ \n            transforms.RandomHorizontalFlip(), \n            transforms.ToTensor(), \n            transforms.Normalize((0.485, 0.456, 0.406), \n                                 (0.229, 0.224, 0.225))])\n    \n    def __getitem__(self, index):\n        path, text = self.path[index], self.text[index]\n        path = os.path.join(self.data_root, ""{}2014"".format(self.phase), path)\n        \n        # some grayscale images are in the dataset\n        image = Image.open(path).convert(""RGB"")\n\n        if self.image_size:\n            image = image.resize(\n                (self.image_size, self.image_size), \n                Image.BICUBIC)\n\n        return self.transform(image), text\n    \n    def __len__(self):\n        return len(self.path)\n    \n    def indices_to_string(self, indices, words=False):\n        """"""Convert word indices (torch.Tensor) to sentence (string).\n\n        Args:\n            indices: torch.tensor or numpy.array of shape (T) or (T, 1)\n            words: boolean, wheter return list of words\n        Returns:\n            sentence: string type of converted sentence\n            words: (optional) list[string] type of words list\n        """"""\n        sentence = list()    \n        for idx in indices:\n            word = CaptionDataset.TEXT.vocab.itos[idx.item()]\n\n            if word in [""<pad>"", ""<start>""]: continue\n            if word in [""<end>""]: break\n\n            # no needs of space between the special symbols\n            if len(sentence) and word in [""\'"", ""."", ""?"", ""!"", "",""]:\n                sentence[-1] += word\n            else:\n                sentence.append(word)\n\n        if words:\n            return "" "".join(sentence), sentence\n        return "" "".join(sentence)\n    \n    \ndef collate_fn(data):\n    """"""Creates mini-batch tensors from the list of tuples (image, caption).\n    Reference: https://github.com/yunjey/pytorch-tutorial\n\n    Args:\n        data: list of tuple (image, caption). \n            - image: torch tensor of shape (3, size, size).\n            - caption: torch tensor of shape (?); variable length.\n    Returns:\n        images: torch tensor of shape (N, 3, image_size, image_size).\n        captions: torch tensor of shape (N, padded_length).\n        lengths: list; valid length for each padded caption.\n    """"""\n    # sort a data list by caption length (descending order).\n    data.sort(key=lambda x: len(x[1]), reverse=True)\n    images, captions = zip(*data)\n    \n    # merge images (from tuple of 3D tensor to 4D tensor).\n    images = torch.stack(images, 0)\n    \n    # add padding to match max length and numericalize caption\n    captions, lengths = CaptionDataset.TEXT.process(captions, \n        device=-1, train=True)\n    \n    return images, captions, lengths\n\n\ndef get_caption_dataset(train, \n                        data_root=""./data"", \n                        max_vocab=10000,\n                        batch_size=32, image_size=224,\n                        num_workers=4, shuffle=True,\n                        text_field=False):\n    dataset = CaptionDataset(train=train,\n        max_vocab=max_vocab,\n        image_size=image_size, \n        data_root=data_root)\n\n    loader = torch.utils.data.DataLoader(dataset=dataset, \n        batch_size=batch_size,\n        shuffle=shuffle,\n        num_workers=num_workers,\n        collate_fn=collate_fn)\n    \n    if text_field:\n        return loader, dataset, CaptionDataset.TEXT\n    return loader, dataset\n'"
codes/show_attend_and_tell/net.py,10,"b'import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models\n\nclass Net(nn.Module):\n    def __init__(self, TEXT,\n                 hidden_dim=512, attn_dim=512,\n                 num_layers=1):\n        super().__init__()\n\n        vocab_size = TEXT.vocab.vectors.size(0)\n        embed_dim = TEXT.vocab.vectors.size(1)\n        \n        self.encoder = Encoder(embed_dim)\n        self.decoder = AttentionDecoder(TEXT,\n            vocab_size, embed_dim,\n            hidden_dim, attn_dim,\n            num_layers)\n\n    def forward(self, image, caption, lengths):\n        feature = self.encoder(image)\n        out = self.decoder(feature, caption, lengths)\n\n        return out\n\n    def sample(self, image):\n        feature = self.encoder(image)\n        out = self.decoder.sample(feature)\n        \n        return out\n\n\nclass Encoder(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        \n        # exclude last max_pool to maintain output shape as [512, 14, 14]\n        self.body = models.vgg19_bn(pretrained=True).features[:-1]\n\n        for param in self.body.parameters():\n            param.requires_grad_(False)\n\n    def forward(self, x):\n        return self.body(x)\n\n\nclass AttentionDecoder(nn.Module):\n    def __init__(self, TEXT,\n                 vocab_size, embed_dim=300,\n                 hidden_dim=512, attn_dim=512, \n                 num_layers=1):\n        super().__init__()\n        \n        self.vocab_size = vocab_size\n        self.num_layers = num_layers\n        self.hidden_dim = hidden_dim\n        \n        # RNN layers\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.rnn = nn.LSTM(embed_dim+attn_dim, hidden_dim, \n                           num_layers=num_layers,\n                           batch_first=True)\n        self.exit = nn.Linear(hidden_dim, vocab_size)\n\n        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n        self.embedding.weight.requires_grad_(False)\n\n        # projection and attention layers\n        self.proj_feature = nn.Linear(512, attn_dim)\n        self.proj_hidden  = nn.Linear(hidden_dim, attn_dim)\n        self.attn_hidden  = nn.Linear(attn_dim, 1)\n        \n    def _attention(self, feature, feature_proj, hx):\n        # (num_layers, N, 512) -> (N, 1, num_layers*512)\n        hx = hx.permute(1, 0, 2).view(-1, 1, self.num_layers*self.hidden_dim)\n        hx_attn = self.proj_hidden(hx)\n\n        hx_attn = F.relu(feature_proj + hx_attn)       # (N, 196, 512)\n        hx_attn = self.attn_hidden(hx_attn).squeeze(2) # (N, 196)\n        \n        alpha = F.softmax(hx_attn, dim=1)\n        context = torch.sum(feature * alpha.unsqueeze(2), 1)\n\n        return context, alpha\n\n    def forward(self, feature, caption, lengths):\n        batch_size = feature.size(0)\n\n        # initial hidden state\n        hx = feature.new_zeros(\n            self.num_layers, batch_size, self.hidden_dim,\n            requires_grad=False)\n        cx = hx.clone()\n        \n        # (N, 512, 14, 14) -> (N, 196, 512)\n        feature = feature.permute(0, 2, 3, 1).view(-1, 196, 512)\n        feature_proj = self.proj_feature(feature)\n        \n        predicts = feature.new_zeros((batch_size, lengths[0], self.vocab_size))\n        for t in range(lengths[0]):\n            # do not operate the sequence which shorter then current length\n            # sequences have to be sorted as descending order\n            batch_size = sum(i >= t for i in lengths)\n            \n            embed = self.embedding(caption[:batch_size, t])\n            context, alpha = self._attention(\n                feature[:batch_size],\n                feature_proj[:batch_size], \n                hx[:, :batch_size])\n            \n            joint_embed = torch.cat([context, embed], 1).unsqueeze(1)\n            out, (hx, cx) = self.rnn(\n                joint_embed, \n                (hx[:, :batch_size], cx[:, :batch_size]))\n\n            out = self.exit(out)\n            out = out.view(-1, out.size(2))\n            \n            predicts[:batch_size, t, :] = out\n        \n        return predicts.view(-1, predicts.size(2))\n\n    def sample(self, feature):\n        batch_size = feature.size(0)\n      \n        # initial hidden state\n        hx = feature.new_zeros(\n            self.num_layers, batch_size, self.hidden_dim,\n            requires_grad=False)\n        cx = hx.clone()\n        \n        # (N, 512, 14, 14) -> (N, 196, 512)\n        feature = feature.permute(0, 2, 3, 1).view(-1, 196, 512)\n        feature_proj = self.proj_feature(feature)\n        \n        # initial embed (<start> token)\n        device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n        embed = torch.full((batch_size,), fill_value=2, dtype=torch.int64).to(device)\n\n        alphas, indices = list(), list()\n        for t in range(30):\n            embed = self.embedding(embed)\n            context, alpha = self._attention(feature, feature_proj, hx)\n            \n            joint_embed = torch.cat([context, embed], 1).unsqueeze(1)\n            out, (hx, cx) = self.rnn(joint_embed, (hx, cx))\n\n            out = self.exit(out)\n            out = out.view(-1, out.size(2))\n            _, argmax = torch.max(out, 1)\n\n            alphas.append(alpha)\n            indices.append(argmax)\n                        \n            # previous output is current input\n            embed = argmax\n                  \n        alphas  = torch.stack(alphas, 1).cpu().numpy()\n        indices = torch.stack(indices, 1).cpu().numpy()\n\n        return alphas, indices\n'"
codes/show_attend_and_tell/solver.py,5,"b'import os\nimport torch\nfrom net import Net\nfrom dataset import get_caption_dataset\nfrom visdomX import VisdomX\n\nclass Solver():\n    def __init__(self, args):\n        self.train_loader, self.train_data, TEXT = get_caption_dataset(\n            train=True,\n            max_vocab=args.max_vocab,\n            data_root=args.data_root,\n            batch_size=args.batch_size, \n            image_size=args.image_size, \n            text_field=True)\n        self.val_loader, _ = get_caption_dataset(\n            train=False,\n            data_root=args.data_root,\n            batch_size=args.batch_size, \n            image_size=args.image_size)\n\n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net = Net(TEXT, \n            args.hidden_dim, args.attn_dim, \n            args.num_layers).to(self.device)\n        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=1) # <pad>: 1\n        self.optim   = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, self.net.parameters()),\n            args.lr)\n\n        self.vis = VisdomX()\n\n        self.args = args\n\n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n\n    def fit(self):\n        args = self.args\n        \n        global_step = 0\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_loader):\n                image   = inputs[0].to(self.device)\n                caption = inputs[1].to(self.device)\n                lengths = inputs[2].to(self.device)\n                \n                # e.g.\n                # input: <start> this is caption\n                # gt:    this is caption <end>\n                gt  = caption[:, 1:].contiguous().view(-1)\n\n                out = self.net(image, caption[:, :-1], lengths-1)\n                loss = self.loss_fn(out, gt)\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n                if (global_step+1) % args.print_every == 0:\n                    perplexity = torch.exp(loss).item()\n                    self.vis.add_scalars(perplexity, global_step+1,\n                                         title=""Attention-Perplexity"",\n                                         ylabel=""Perplexity"", xlabel=""step"")\n\n                    print(""Epoch [{}/{}] Global Step: [{}K/{}K] Perplexity: {:5.3f}""\n                        .format(epoch+1, args.max_epochs, \n                                int((global_step+1)/1000), \n                                int(args.max_epochs*len(self.train_loader)/1000),\n                                perplexity))\n\n                    self.save(args.ckpt_dir, args.ckpt_name, global_step+1)\n\n                global_step += 1\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/show_attend_and_tell/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--max_epochs"", type=int, default=20)\n    \n    parser.add_argument(""--max_vocab"", type=int, default=10000)\n    parser.add_argument(""--embed_dim"", type=int, default=300)\n    parser.add_argument(""--hidden_dim"", type=int, default=512)\n    parser.add_argument(""--attn_dim"", type=int, default=512)\n    parser.add_argument(""--num_layers"", type=int, default=1)\n\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n    parser.add_argument(""--image_size"", type=int, default=224)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""caption"")\n    parser.add_argument(""--print_every"", type=int, default=1000) # 1k step not epoch\n    \n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/show_attend_and_tell/visdomX.py,0,"b'import numpy as np\nfrom visdom import Visdom\n\nclass VisdomX:\n    def __init__(self):\n        self.vis = Visdom()\n        self.wins = dict()\n\n    def add_scalars(self, \n                    value, step,\n                    title="""", \n                    ylabel=""Loss"", xlabel=""Epoch""):\n        # visdom only get np.array or tensor\n        value = np.array([value])\n        step = np.array([step], dtype=np.int64)\n\n        if not self.wins.get(title):\n            win = self.vis.line(\n                Y=value, X=step,\n                opts=dict(\n                    title=title, \n                    ylabel=ylabel, \n                    xlabel=xlabel\n                )\n            )\n            self.wins[title] = win\n        else:\n            self.vis.line(\n                Y=value, X=step,\n                win=self.wins.get(title), \n                update=""append""\n            )\n\n    def add_text(self, text):\n        self.vis.text(text)\n'"
codes/style_transfer/train.py,6,"b'import os\nimport numpy as np\nimport torch\nfrom utils import *\nfrom vgg import *\n\ndef mse(feat1, feat2):\n    return torch.mean((feat1-feat2)**2)\n\n\ndef gram_matrix(matrix):\n    _, c, h, w = matrix.size()\n    matrix = matrix.view(c, h*w)\n\n    return torch.mm(matrix, matrix.t())\n\n\ndef single_layer_style_loss(X_feat, style_feat):\n    _, c, h, w = X_feat.size()\n\n    X_gram = gram_matrix(X_feat)\n    style_gram = gram_matrix(style_feat)\n\n    return mse(X_gram, style_gram) / (c*h*w)\n\n\ndef single_layer_content_loss(X_feat, content_feat):\n    return mse(X_feat, content_feat)\n\n\ndef total_variance_loss(image):\n    w_variance = torch.sum((image[:,:,:,1:] - image[:,:,:,:-1])**2)\n    h_variance = torch.sum((image[:,:,1:,:] - image[:,:,:-1,:])**2)\n\n    return w_variance + h_variance\n\n\ndef fit(X, content, style, device, args):\n    style_weights = [4.0, 3.0, 1.5, 1.0, 0.5]\n\n    print(""[!] Prepare the pretrained VGGNet"")\n    vgg = VGGNet().to(device)\n    optim = torch.optim.Adam([X], args.lr, betas=[0.5, 0.999])\n\n    print(""[!] Start training"")        \n    for step in range(args.max_steps):\n        style_loss, content_loss = 0, 0\n        \n        style_feats     = vgg(style, phase=""style"")\n        X_style_feats   = vgg(X, phase=""style"")\n        content_feats   = vgg(content, phase=""content"")\n        X_content_feats = vgg(X, phase=""content"")\n        \n        for feat in zip(X_content_feats, content_feats):\n            X_feat, content_feat = feat\n            content_loss += single_layer_content_loss(X_feat, content_feat)\n\n        for i, feat in enumerate(zip(X_style_feats, style_feats)):\n            X_feat, style_feat = feat\n            style_loss += single_layer_style_loss(X_feat, style_feat)\n\n        tv_loss = total_variance_loss(X)\n       \n        loss = args.alpha * content_loss + \\\n               args.beta * style_loss + \\\n               args.tv * tv_loss\n\n        optim.zero_grad()\n        loss.backward()\n        optim.step()\n        \n        if (step+1) % args.print_every == 0:\n            print(""[{}/{}] Style Loss: {:.3f} Content Loss: {:.3f}""\n                .format(step+1, args.max_steps, style_loss.item(), content_loss.item()))\n\n            save_image(X.detach()[0], os.path.join(args.result_dir, ""result_{}.png"".format(step+1)))\n\n\ndef main(args):\n    print(""[!] Prepare the content and style images"")\n    content, style = prepare_images(\n        args.content, args.style, args.resize_side_max)\n\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n    # cudafy if available\n    content = content.to(device)\n    style   = style.to(device)\n    \n    X = content.clone()\n    X.requires_grad_(True)\n\n    content.requires_grad_(False)\n    style.requires_grad_(False)\n\n    fit(X, content, style, device, args)\n        \n \nif __name__ == ""__main__"":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--result_dir"", type=str, default=""./result"")\n    parser.add_argument(""--content"", type=str, default=""./images/content.jpg"")\n    parser.add_argument(""--style"", type=str, default=""./images/style.jpg"")\n    parser.add_argument(""--resize_side_max"", type=int, default=600)\n\n    parser.add_argument(""--max_steps"", type=int, default=2000)\n    parser.add_argument(""--print_every"", type=int, default=100)\n    \n    parser.add_argument(""--alpha"", type=float, default=1.0)\n    parser.add_argument(""--beta"", type=float, default=100.0)\n    parser.add_argument(""--tv"", type=float, default=0.001)\n    parser.add_argument(""--lr"", type=float, default=0.01)\n    args = parser.parse_args()\n\n    if not os.path.exists(args.result_dir):\n        os.makedirs(args.result_dir)\n\n    main(args)\n'"
codes/style_transfer/utils.py,0,"b'import numpy as np\nimport PIL\nfrom PIL import Image\nimport torch\nimport torchvision\nfrom torchvision import transforms\n\ndef prepare_images(content_path, style_path,\n                   resize_side_max=None, \n                   noise_ratio=0.6):\n    content = Image.open(content_path)\n    style   = Image.open(style_path)\n        \n    w, h = content.size\n    # resize content image by maintaining aspect ratio\n    if resize_side_max:\n        scale = max(1, resize_side_max / max([w, h]))\n        w, h = int(w*scale), int(h*scale)\n        content = content.resize((w, h), PIL.Image.BICUBIC)\n\n    # match style image size to content image\n    style = style.resize((w, h), PIL.Image.BICUBIC)\n\n    # transform images and add batch dimension\n    transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.485, 0.456, 0.406),\n                             (0.229, 0.224, 0.225))\n    ])\n    content = transform(content).unsqueeze(0)\n    style = transform(style).unsqueeze(0)\n\n    return content, style\n\n\ndef save_image(tensor, filename):\n    transform = transforms.Normalize((-2.12, -2.04, -1.80), (4.37, 4.46, 4.44))\n    image = tensor.clone()\n    image = transform(image).clamp_(0, 1)\n    \n    torchvision.utils.save_image(image, filename)\n'"
codes/style_transfer/vgg.py,1,"b'import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nclass VGGNet(nn.Module):\n    feature_table = {\n        ""content"": [""25""],\n        ""style"": [""0"", ""5"", ""10"", ""19"", ""28""]\n    }\n    \n    def __init__(self):\n        super(VGGNet, self).__init__()\n        self.vgg = models.vgg19(pretrained=True).features\n        \n    def forward(self, x, phase):\n        if phase not in [""content"", ""style""]:\n            raise ValueError(""phase argument must be in [\\""content\\"", \\""style\\""]"")\n\n        features = list()\n        table = self.feature_table[phase]\n\n        for name, layer in self.vgg._modules.items():\n            x = layer(x)\n            if name in table:\n                features.append(x)\n                if len(features) >= len(table):\n                    break\n        return features\n'"
codes/super_resolution/dataset.py,1,"b'import os\nimport glob\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torch.utils.data as data\nimport torchvision.transforms as transforms\nfrom utils import *\n\nclass Dataset(data.Dataset):\n    def __init__(self, scale, train, **kwargs):\n        super(Dataset, self).__init__()\n\n        self.scale = scale\n        self.size = kwargs.get(""size"", None)\n        self.data_root = kwargs.get(""data_root"", ""./data"")\n        \n        self._prepare_dataset(self.data_root)\n        \n        phase = ""train"" if train else ""test""\n        dirname = os.path.join(self.data_root, ""flower/{}"".format(phase))\n\n        self.paths = glob.glob(os.path.join(dirname, ""*.jpg""))\n        self.transform = transforms.Compose([\n            transforms.ToTensor()\n        ])\n\n    def __getitem__(self, index):\n        hr_image = Image.open(self.paths[index])\n\n        # resize HR images\n        if self.size:\n            hr_image = hr_image.resize((self.size, self.size), Image.BICUBIC)\n\n        w, h = hr_image.size\n        lr_image = hr_image.resize((int(w/self.scale), int(h/self.scale)), \n                                   Image.BICUBIC)\n\n        return self.transform(hr_image), self.transform(lr_image)\n\n    def __len__(self):\n        return len(self.paths)\n\n    def _prepare_dataset(self, data_root):\n        check = os.path.join(data_root, ""flower"")\n        if not os.path.isdir(check):\n            download_and_convert(data_root)\n'"
codes/super_resolution/net.py,2,"b'import math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    def __init__(self, scale):\n        super(Net, self).__init__()\n        \n        self.entry = nn.Conv2d(3, 64, 3, 1, 1)\n        self.exit  = nn.Conv2d(64, 3, 3, 1, 1)\n\n        body = list()\n        for i in range(5):\n            body += [nn.Conv2d(64, 64, 3, 1, 1), nn.ReLU()]\n        self.body = nn.Sequential(*body) \n        \n        # support only x2, x4, ...\n        upsample = list()\n        for _ in range(int(math.log(scale, 2))):\n            upsample += [nn.ConvTranspose2d(64, 64, 4, 2, 1), nn.ReLU()]\n        self.upsample = nn.Sequential(*upsample)\n\n    def forward(self, x):\n        x = self.entry(x)\n        out = self.body(x)\n        out += x\n        \n        out = self.upsample(out)\n        out = self.exit(out)\n\n        return out\n\n'"
codes/super_resolution/solver.py,6,"b'import os\nimport numpy as np\nimport torch\nfrom torch.utils.data import DataLoader\nfrom net import Net\nfrom dataset import Dataset\n\nclass Solver():\n    def __init__(self, args):\n        # prepare a datasets\n        self.train_data = Dataset(args.scale, train=True,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.test_data  = Dataset(args.scale, train=False,\n                                  data_root=args.data_root,\n                                  size=args.image_size)\n        self.train_loader = DataLoader(self.train_data,\n                                       batch_size=args.batch_size,\n                                       num_workers=1,\n                                       shuffle=True, drop_last=True)\n        \n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net     = Net(args.scale).to(self.device)\n        self.loss_fn = torch.nn.L1Loss()\n        self.optim   = torch.optim.Adam(self.net.parameters(), args.lr)\n        \n        self.args = args\n        \n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        if not os.path.exists(args.result_dir):\n            os.makedirs(args.result_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_loader):\n                image_hr = inputs[0].to(self.device)\n                image_lr = inputs[1].to(self.device)\n                \n                image_sr = self.net(image_lr)\n                loss = self.loss_fn(image_sr, image_hr)\n                \n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n\n            if (epoch+1) % args.print_every == 0:\n                psnr = self.evaluate(epoch+1)\n                print(""Epoch [{}/{}] PSNR: {:.3f}"".format(epoch+1, args.max_epochs, psnr))\n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def evaluate(self, global_step):\n        args = self.args\n        loader = DataLoader(self.test_data,\n                            batch_size=args.batch_size,\n                            num_workers=1,\n                            shuffle=False, drop_last=False)\n\n        self.net.eval()\n        mean_psnr = 0\n\n        with torch.no_grad():\n            for step, inputs in enumerate(loader):\n                image_hr = inputs[0].to(self.device)\n                image_lr = inputs[1].to(self.device)\n                \n                image_sr = self.net(image_lr)\n\n                for hr, sr in zip(image_hr, image_sr):\n                    hr = hr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).data.numpy()\n                    sr = sr.cpu().mul(255).clamp(0, 255).byte().permute(1, 2, 0).data.numpy()\n                    \n                    mean_psnr += psnr(hr, sr) / len(self.test_data)\n        \n        return mean_psnr\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n\n\ndef psnr(im1, im2):\n    def im2double(im):\n        min_val, max_val = 0, 255\n        out = (im.astype(np.float64)-min_val) / (max_val-min_val)\n        return out\n        \n    im1 = im2double(im1)\n    im2 = im2double(im2)\n\n    mse = np.mean((im1-im2)**2)\n    return 20 * np.log10(1.0 / np.sqrt(mse))'"
codes/super_resolution/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.0001)\n    parser.add_argument(""--scale"", type=int, default=4)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--max_epochs"", type=int, default=100)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""./checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""sr"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    parser.add_argument(""--image_size"", type=int, default=100)\n    parser.add_argument(""--data_root"", type=str, default=""./data"")\n\n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/super_resolution/utils.py,0,"b'import os\nimport glob\nimport shutil\nimport tarfile\nimport urllib.request\nimport scipy.misc as misc\n\nurl = ""http://download.tensorflow.org/example_images/flower_photos.tgz""\ndirnames = [""daisy"", ""dandelion"", ""roses"", ""sunflowers"", ""tulips""]\n\ndef download_and_uncompress_tarball(tarball_url, dataset_dir):\n    filename = tarball_url.split(\'/\')[-1]\n    filepath = os.path.join(dataset_dir, filename)\n\n    filepath, _ = urllib.request.urlretrieve(tarball_url, filepath)\n    tarfile.open(filepath, ""r:gz"").extractall(dataset_dir)\n\n\ndef download_and_convert(data_root):\n    if not os.path.exists(data_root):\n        os.makedirs(data_root)\n\n    if not os.path.exists(os.path.join(data_root, ""flower_photos"")):\n        print(""[!] Downloading images..."")\n        download_and_uncompress_tarball(url, data_root)\n\n    data_dir = os.path.join(data_root, ""flower"")\n    \n    if not os.path.exists(os.path.join(data_dir, ""train"")):\n        os.makedirs(os.path.join(data_dir, ""train""))\n    if not os.path.exists(os.path.join(data_dir, ""test"")):\n        os.makedirs(os.path.join(data_dir, ""test""))\n    \n    print(""[!] Converting images..."")\n    for dirname in dirnames:\n        paths = glob.glob(os.path.join(\n            data_root, ""flower_photos"", dirname, ""*.jpg""))\n        \n        # training data\n        for path in paths[:-10]:\n            new_path = os.path.join(data_dir, ""train"",\n                ""{}_{}"".format(dirname, path.split(""/"")[-1]))\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n\n        # test data\n        for path in paths[-10:]:\n            new_path = os.path.join(data_dir, ""test"",\n                ""{}_{}"".format(dirname, path.split(""/"")[-1]))\n\n            im = misc.imread(path)\n            misc.imsave(new_path, im)\n'"
codes/text_cls/net.py,1,"b'import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self, TEXT,\n                 hidden_dim=512, num_layers=2,\n                 num_class=5):\n        super().__init__()\n\n        vocab_size = TEXT.vocab.vectors.size(0)\n        embed_dim = TEXT.vocab.vectors.size(1)\n\n        self.embedding = nn.Embedding(vocab_size, embed_dim)\n        self.encoder = nn.GRU(embed_dim, hidden_dim, \n                              num_layers=num_layers, \n                              dropout=0.5, bidirectional=True)\n\n        self.embedding.weight.data.copy_(TEXT.vocab.vectors)\n        self.embedding.weight.requires_grad=False\n\n        self.linear = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.Linear(hidden_dim*2, num_class)\n        )\n    \n    def forward(self, x):\n        embed = self.embedding(x)\n        out, _ = self.encoder(embed)\n        \n        out = self.linear(out[-1])\n        return out\n'"
codes/text_cls/solver.py,6,"b'import os\nimport numpy as np\nimport torch\nfrom net import Net\nfrom utils import *\n\nclass Solver():\n    def __init__(self, args):\n        \n        # prepare SST dataset\n        train_iter, val_iter, test_iter, sst_info = load_sst(args.batch_size, args.max_vocab)\n        vocab_size = sst_info[""vocab_size""]\n        num_class  = sst_info[""num_class""]\n        TEXT = sst_info[""TEXT""]\n        print(""[!] vocab_size: {}, num_class: {}"".format(vocab_size, num_class))\n        \n        self.device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n\n        self.net = Net(TEXT, \n                       args.hidden_dim,\n                       args.num_layers, num_class).to(self.device)\n        self.optim = torch.optim.Adam(\n            filter(lambda p: p.requires_grad, self.net.parameters()),\n            args.lr, weight_decay=args.weight_decay)\n        self.loss_fn = torch.nn.CrossEntropyLoss()\n        \n        self.args = args\n        self.train_iter = train_iter\n        self.val_iter   = val_iter\n        self.test_iter  = test_iter\n        \n        if not os.path.exists(args.ckpt_dir):\n            os.makedirs(args.ckpt_dir)\n        \n    def fit(self):\n        args = self.args\n\n        for epoch in range(args.max_epochs):\n            self.net.train()\n            for step, inputs in enumerate(self.train_iter):\n                X = inputs.text.to(self.device)\n                y = inputs.label.to(self.device)\n\n                pred_y = self.net(X)\n                loss = self.loss_fn(pred_y, y)\n\n                self.optim.zero_grad()\n                loss.backward()\n                self.optim.step()\n            \n            if (epoch+1) % args.print_every == 0:\n                train_acc = self.evaluate(self.train_iter)\n                val_acc   = self.evaluate(self.val_iter)\n\n                print(""Epoch [{}/{}] train_acc: {:.3f}, val_acc: {:.3f}""\n                    .format(epoch+1, args.max_epochs, train_acc, val_acc))\n                self.save(args.ckpt_dir, args.ckpt_name, epoch+1)\n\n    def evaluate(self, iters):\n        args = self.args\n\n        self.net.eval()\n        num_correct, num_total = 0, 0\n\n        with torch.no_grad():\n            for step, inputs in enumerate(iters):\n                X = inputs.text.to(self.device)\n                y = inputs.label.to(self.device)\n\n                pred_y = self.net(X)\n                _, pred_y = torch.max(pred_y.detach(), 1)\n                \n                num_correct += (pred_y == y.detach()).sum().item()\n                num_total += y.size(0)\n\n        return num_correct / num_total\n\n    def save(self, ckpt_dir, ckpt_name, global_step):\n        save_path = os.path.join(\n            ckpt_dir, ""{}_{}.pth"".format(ckpt_name, global_step))\n        torch.save(self.net.state_dict(), save_path)\n'"
codes/text_cls/train.py,0,"b'import os\nimport argparse\nfrom solver import Solver\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    parser.add_argument(""--weight_decay"", type=float, default=0.0001)\n    parser.add_argument(""--batch_size"", type=int, default=128)\n    parser.add_argument(""--max_epochs"", type=int, default=20)\n    \n    parser.add_argument(""--hidden_dim"", type=int, default=128)\n    parser.add_argument(""--num_layers"", type=int, default=2)\n    parser.add_argument(""--max_vocab"", type=int, default=-1)\n    \n    parser.add_argument(""--ckpt_dir"", type=str, default=""checkpoint"")\n    parser.add_argument(""--ckpt_name"", type=str, default=""sst"")\n    parser.add_argument(""--print_every"", type=int, default=1)\n    \n    args = parser.parse_args()\n    solver = Solver(args)\n    solver.fit()\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/text_cls/utils.py,0,"b'import torch\nimport torchtext.data as data\nimport torchtext.datasets as datasets\n\ndef _iters(batch_size,\n           max_vocab,\n           fine_grained, \n           repeat):\n    TEXT = data.Field(sequential=True)\n    LABEL = data.LabelField()\n\n    train, val, test = datasets.SST.splits(\n        TEXT, LABEL,\n        root=""./data"", fine_grained=fine_grained)\n\n    if max_vocab == -1:\n        max_vocab = None\n\n    TEXT.build_vocab(train, vectors=""glove.6B.300d"", max_size=max_vocab)\n    LABEL.build_vocab(train)\n\n    return TEXT, LABEL, data.BucketIterator.splits(\n        (train, val, test),\n        batch_size=batch_size, \n        repeat=repeat)\n\n\ndef load_sst(batch_size, max_vocab, fine_grained=True):\n    TEXT, LABEL, (train_iter, val_iter, test_iter) = _iters(\n        batch_size=batch_size,\n        max_vocab=max_vocab,\n        fine_grained=fine_grained,\n        repeat=False)\n\n    sst_info = {\n        ""vocab_size"": len(TEXT.vocab),\n        ""num_class"": 5 if fine_grained else 3,\n        ""TEXT"": TEXT\n    }\n\n    return train_iter, val_iter, test_iter, sst_info\n'"
codes/utilities/net.py,1,"b'import torch\nimport torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.fc = nn.Linear(7*7*64, 10)\n        \n    def forward(self, x):\n        conv1 = self.conv1(x)\n        conv2 = self.conv2(conv1)\n        flat  = conv2.view(conv2.size(0), -1)\n        out = self.fc(flat)\n        return out\n'"
codes/utilities/train.py,7,"b'import numpy as np\nimport torch\nimport torch.nn as nn\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom net import Net\n# pacakges to log summary\nfrom visdomX import VisdomX\nfrom torchsummary import summary\n\ndef evaluate(net, loader, device):\n    net.eval()\n    num_correct, num_total = 0, 0\n\n    # same as volatile=True of the v0.3\n    with torch.no_grad():\n        for inputs in loader:\n            images = inputs[0].to(device)\n            labels = inputs[1].to(device)\n\n            outputs = net(images)\n            _, preds = torch.max(outputs.detach(), 1)\n\n            num_correct += (preds == labels).sum().item()\n            num_total += labels.size(0)\n\n    return num_correct / num_total\n\n\ndef train(args):\n    # initialize visdomX\n    vis = VisdomX()\n    \n    # prepare the MNIST dataset\n    train_dataset = datasets.MNIST(root=""./data/"",\n                                   train=True, \n                                   transform=transforms.ToTensor(),\n                                   download=True)\n\n    test_dataset = datasets.MNIST(root=""./data/"",\n                                  train=False, \n                                  transform=transforms.ToTensor())\n\n    # create the data loader\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=args.batch_size, \n                              shuffle=True, drop_last=True)\n\n    test_loader = DataLoader(dataset=test_dataset,\n                             batch_size=args.batch_size, \n                             shuffle=False)\n\n    \n    # turn on the CUDA if available\n    device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")\n    \n    net = Net().to(device)\n    loss_op = nn.CrossEntropyLoss()\n    optim   = torch.optim.Adam(net.parameters(), lr=args.lr)\n    \n    # model summary via torchsummary\n    summary(net, (1, 28, 28))\n\n    for epoch in range(args.max_epochs):\n        net.train()\n        for step, inputs in enumerate(train_loader):\n            images = inputs[0].to(device)\n            labels = inputs[1].to(device)\n            \n            # forward-propagation\n            outputs = net(images)\n            loss = loss_op(outputs, labels)\n            \n            # back-propagation\n            optim.zero_grad()\n            loss.backward()\n            optim.step()\n\n        acc = evaluate(net, test_loader, device)\n\n        # plot loss and acc by visdom\n        vis.add_scalars(loss.item(), epoch,\n                        title=""Loss"",\n                        ylabel=""Loss"", xlabel=""Epoch"")\n        vis.add_scalars(acc, epoch,\n                        title=""Accuracy"",\n                        ylabel=""Accuracy"", xlabel=""Epoch"")\n        \n        print(""Epoch [{}/{}] loss: {:.5f} test acc: {:.3f}""\n              .format(epoch+1, args.max_epochs, loss.item(), acc))\n\n    torch.save(net.state_dict(), ""mnist-final.pth"")\n\n\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(""--max_epochs"", type=int, default=5)\n    parser.add_argument(""--batch_size"", type=int, default=64)\n    parser.add_argument(""--lr"", type=float, default=0.001)\n    args = parser.parse_args()\n\n    train(args)\n\nif __name__ == ""__main__"":\n    main()\n'"
codes/utilities/visdomX.py,0,"b'import numpy as np\nfrom visdom import Visdom\n\nclass VisdomX:\n    def __init__(self):\n        self.vis = Visdom()\n        self.wins = dict()\n\n    def add_scalars(self, \n                    value, step,\n                    title="""", \n                    ylabel=""Loss"", xlabel=""Epoch""):\n        # visdom only get np.array or tensor\n        value = np.array([value])\n        step = np.array([step], dtype=np.int64)\n\n        if not self.wins.get(title):\n            win = self.vis.line(\n                Y=value, X=step,\n                opts=dict(\n                    title=title, \n                    ylabel=ylabel, \n                    xlabel=xlabel\n                )\n            )\n            self.wins[title] = win\n        else:\n            self.vis.line(\n                Y=value, X=step,\n                win=self.wins.get(title), \n                update=""append""\n            )\n\n    def add_text(self, text):\n        self.vis.text(text)\n'"
